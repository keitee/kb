*kt_dev_08*                                                                tw=100

kt.dev.py

/^[#=]{
Use #{ for a group and ={ for a item

|kt_dev_py_0001| py-book
|kt_dev_py_0001| py-check-version
|kt_dev_py_0001| py-base-indentation
|kt_dev_py_0001| py-base-basic
|kt_dev_py_0001| py-base-type
|kt_dev_py_0001| py-base-none-type
|kt_dev_py_0001| py-base-control
|kt_dev_py_0001| py-base-loop
|kt_dev_py_0001| py-base-what-evaluate
|kt_dev_py_0001| py-base-comment

|kt_dev_py_0001| py-gotchas
|kt_dev_py_0001| py-types
|kt_dev_py_0001| py-variable py-dynamic-type
|kt_dev_py_0001| py-number
|kt_dev_py_0001| py-string
|kt_dev_py_0001| py-tuple
|kt_dev_py_0001| py-list
|kt_dev_py_0001| py-slice
|kt_dev_py_0001| py-reverse
|kt_dev_py_0001| py-dict 
|kt_dev_py_0001| py-set
|kt_dev_py_0001| py-reference
|kt_dev_py_0001| py-if
|kt_dev_py_0001| py-iter py-comprehension
|kt_dev_py_0001| py-module
|kt_dev_py_0001| py-module-namespace
|kt_dev_py_0001| py-module-package
|kt_dev_py_0001| py-module-advanced
|kt_dev_py_0001| py-module-main-name
|kt_dev_py_0001| py-function
|kt_dev_py_0001| py-function-polymorphism
|kt_dev_py_0001| py-function-scope
|kt_dev_py_0001| py-function-scope
|kt_dev_py_0001| py-function-arguments
|kt_dev_py_0001| py-class
|kt_dev_py_0001| py-class-static py-decorator
|kt_dev_py_0001| py-exception

|kt_dev_py_0001| py-sys
|kt_dev_py_0001| py-subprocess
|kt_dev_py_0001| py-shlex
|kt_dev_py_0001| py-builtin-function
|kt_dev_py_0001| py-os
|kt_dev_py_0001| py-urllib
|kt_dev_py_0001| py-file
|kt_dev_py_0001| py-itertools
|kt_dev_py_0001| py-cvs
|kt_dev_py_0001| py-re
|kt_dev_py_0001| py-pip
|kt_dev_py_0001| py-dropbox
|kt_dev_py_0001| py-imaplib
|kt_dev_py_0001| py-time
|kt_dev_py_0001| py-time-benchmark
|kt_dev_py_0001| py-cvs
|kt_dev_py_0001| py-stmplib
|kt_dev_py_0001| py-parser-option
|kt_dev_py_0001| py-xml
|kt_dev_py_0001| py-logging
|kt_dev_py_0001| py-error-encoding
|kt_dev_py_0001| py-six

|kt_dev_py_0001| py-flask-doc
|kt_dev_py_0001| py-flask-apache
|kt_dev_py_0001| py-flask
|kt_dev_py_0001| py-flask-template
|kt_dev_py_0001| py-flask-form


={============================================================================
|kt_dev_py_0001| py-book

LPY. Learning Python, Fifth Edition


# ============================================================================
#{
={============================================================================
|kt_dev_py_0001| py-check-version

10:27:35 ~$ python -V
Python 2.7.3

10:27:40 ~$ python --version
Python 2.7.3

>>> import sys
>>> print (sys.version)
2.7.3 (default, Mar 14 2014, 11:57:14) 
[GCC 4.7.2]

>>> sys.version_info
sys.version_info(major=2, minor=7, micro=3, releaselevel='final', serial=0)
>>> sys.hexversion
34014192

# `tuple` and `tuple` comparison?
>>> sys.version_info >= (2,5)
True


={============================================================================
|kt_dev_py_0001| py-base-indentation

Chapter 12: if Tests and Syntax Rules, Python Syntax Revisited

Block and statement boundaries are detected automatically. 

As we’ve seen, there are no braces or “begin/end” delimiters around blocks of
code in Python; instead, Python uses the indentation of statements under a
header to group the statements in a nested block. Similarly, Python statements
are not normally terminated with semicolons; rather, the end of a line usually
marks the end of the statement coded on that line. As a special case,
statements can span lines and be combined on a line with special syntax.

Block Delimiters: Indentation Rules

As introduced in Chapter 10, Python detects block boundaries automatically, by
`line indentation` - that is, the empty space to the left of your code. 

All statements indented the same distance to the right belong to the same
block of code. In other words, the statements within a block line up
vertically, as in a column. The block ends when the end of the file or a
lesser-indented line is encountered, and more deeply nested blocks are simply
indented further to the right than the statements in the enclosing block.


<indentation-not-brace>
Python uses whitespace (tabs or spaces) to structure code

for x in array:
  if x < pivot:
    less.append(x)
  else:
    greater.append(x)

A `colon` denotes the start of an `indented code block` after which all of the
code must be indented by the same amount until the end of the block.


<ex>
class Employee:
   empCount = 0

   def __init__(self, name, salary):
      self.name = name

     def setvalue(self, val):
        empCount=val

Error:
IndentationError: unindent does not match any outer indentation level

class Employee:
   empCount = 0

   def __init__(self, name, salary):
      self.name = name

   def setvalue(self, val):
      empCount=val


<ex>
    # ok
    for i in range(settings.secondsToWait):
      try:
        time.sleep(1)
      except KeyboardInterrupt:
        break

      valids = devices.getValidDevices()
      if len(valids) >= settings.numSTBs:
        break

    print 'Found %d STBs in %ds' % (len(valids), i)


    # nk
    for i in range(settings.secondsToWait):
      try:
        time.sleep(1)
      except KeyboardInterrupt:
        break

    valids = devices.getValidDevices()
    if len(valids) >= settings.numSTBs:
      break

    print 'Found %d STBs in %ds' % (len(valids), i)

SyntaxError: 'break' outside loop


Statement Delimiters: Lines and Continuations

A statement in Python normally ends at the end of the line on which it
appears. When a statement is too long to fit on a single line, though, a few
special rules may be used to make it span multiple lines:

Statements may span multiple lines if you're continuing an open syntactic
pair. 

Python lets you continue typing a statement on the next line if you're coding
something enclosed in a (), {}, or [] `pair.` For instance, expressions in
parentheses and dictionary and list literals can span any number of lines;
your statement doesn’t end until the Python interpreter reaches the line on
  which you type the closing part of the pair (a ), }, or ]). 
  
Continuation lines-lines 2 and beyond of the statement can start at any
indentation level you like, but you should try to make them align vertically
for readability if possible. This open pairs rule also covers set and
  dictionary comprehensions in Python 3.X and 2.7.


Statements may span multiple lines if they end in a backslash. 

This is a somewhat outdated feature that's `not generally recommended,` but if a
statement needs to span multiple lines, you can also add a backslash (a \ not
    embedded in a string literal or comment) at the end of the prior line to
indicate you're continuing on the next line. 

Because you can also `continue by adding parentheses around most constructs`,
        backslashes are rarely used today. 

<ex>
print "  %s, %s, %s - %s -> %s, %s" % (getBoxInfo(), getBoxInfo(), getBoxInfo(), getBoxInfo(), getBoxInfo(), getBoxInfo())

print ("  %s, %s, %s - %s -> %s, %s" % 
        (getBoxInfo(), 
         getBoxInfo(), 
         getBoxInfo(), 
         getBoxInfo(), 
         getBoxInfo(), 
         getBoxInfo()))

This approach is also error-prone: accidentally forgetting a \ usually
generates a syntax error and might even cause the next line to be silently
mistaken (i.e., without warning) for a new statement, with unexpected results.


Special rules for string literals. triple-quoted string


={============================================================================
|kt_dev_py_0001| py-base-basic

<everything-is-object>
An important characteristic of the Python language is the consistency of its
object model. 

Every number, string, data structure, function, class, module, and so on
exists in the Python interpreter in its own "box" which is referred to as a
`Python object` Each object has an associated `type` (for example, string or
    function) and internal `data`. In practice this makes the language very
flexible, as even functions can be treated just like any other object.


<comment>
Any text preceded by the hash mark (pound sign) # is ignored by the Python
interpreter.


<function>
Functions are called using parentheses and passing zero or more arguments,
          optionally assigning the returned value to a variable:

result = f(x, y, z)
g()

Almost every object in Python has attached functions, known as methods, that
have access to the object's internal contents. They can be called using the
syntax:

obj.some_method(x, y, z)


<pass-by-reference>
When assigning a variable (or name) in Python, you are creating a `reference` to
the object on the right hand side of the equals sign.

In some languages, this assignment would cause the data [1, 2, 3] to be
copied. In Python, a and b actually now refer to the same object, the original
list

a = [1,2,3]
b = a
a.append(4)
b
[1,2,3,4]

note:
Assignment is also referred to as `binding`, as we are binding a name to an
object. Variables names that have been assigned may occasionally be referred
to as bound variables.

When you pass objects as arguments to a function, you are only passing
references; no copying occurs.

Understanding the semantics of references in Python and when, how, and why
  data is copied is especially critical when working with larger data sets in
  Python.


<typed-language> but object has a type
In contrast with many compiled languages, such as Java and C++, 
`object references` in Python `have no type` associated with them. There is no
  problem with the following:

>>> a = 5
>>> type(a)
<type 'int'>

>>> a = 'foo'
>>> type(a)
<type 'str'>

>>> a = "foo"
>>> type(a)
<type 'str'>


Variables are names for objects within a particular namespace; the type
information is stored in the `object itself` Some observers might hastily
conclude that Python `is not a typed language` This is `not true`; consider
this example:

>>> '5'+5
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
`TypeError`: cannot concatenate 'str' and 'int' objects

In some languages, such as Visual Basic, the string '5' might get implicitly
  converted (or casted) to an integer, thus yielding 10. Yet in other
  languages, such as JavaScript, the integer 5 might be casted to a string,
  yielding the concatenated string '55'. 

In this regard Python is considered a strongly-typed language, which means
that every object has a specific type (or class), and implicit conversions
will occur only in `certain obvious circumstances`, such as the following:

>>> a = 4.5
>>> b = 2
>>> print "a is %s, b is %s" % (type(a), type(b))
a is <type 'float'>, b is <type 'int'>
>>> a/b
2.25


Knowing the type of an object is important and can check that an object is an
instance of a particular type using the `isinstance` function:

>>> a = 5
>>> isinstance(a,int)
True
>>> b = 4.5
>>> isinstance(b,int)
False
>>> isinstance(b,float)
True

isinstance can accept a tuple of types if you want to check that an object's
type is among those present in the tuple:

>>> isinstance(b,(int,float))
True


<attribute-and-method>
Objects in Python typically have both `attributes`, other Python objects
stored "inside" the object, and `methods`, functions associated with an object
which can have access to the object's internal data.

Both of them are accessed via the syntax `obj.attribute_name` Attributes and
methods can also be accessed by name using the `getattr` function:

>>> a='foo'
>>> type(a)
<type 'str'>
>>> getattr(a, 'split')
<built-in method split of str object at 0xb744dc38>

While we will not extensively use the functions getattr and related functions
`hasattr` and setattr in this book, they can be used very effectively to write
generic, reusable code.


<iterable>
In a nutshell, an object is `iterable` if it is either a physically stored
sequence in memory, or an object that generates one item at a time in the
context of an iteration operation - a sort of "virtual" sequence.


Can verify that an object is iterable if it implemented the iterator protocol.

>>> def isiterable(obj):
...     try:
...             iter(obj)
...             return True
...     except TypeError:
...             return False
... 
>>> isiterable('a string')
True
>>> isiterable([1,2,3])
True
>>> isiterable(4)
False


iter(object[, sentinel])

Return an iterator object. Without a second argument, object must be a
collection object which supports the `iteration protocol` (the __iter__()
    method), or it must support the sequence protocol (the __getitem__()
      method with integer arguments starting at 0). If it does not support
    either of those protocols, TypeError is `raised.`

# exception is raised that's why use try

A common case is writing a function that can accept any kind of sequence
(list, tuple, ndarray) or even an iterator. If it is not, convert it to be
one:

if not isinstance(x, list) and isiterable(x):
  x = list(x)


<comparison>
To check if two `references` refer to the same object, use the `is` and `is not`
keyword. Not the object itself. 

>>> a=[1,2,3]
>>> b=a
>>> c=list(a)
>>> a is b
True
>>> a is not c
True
>>> a is c
False
>>> a == c
True
>>> b == c
True

A very common use of is and is not is to check if a variable is None

>>> a = None
>>> a is None
True


<py-equility> <py-is-keyword> <py-comparison>
>>> L = [1, 2, 3]
>>> M = L         # M and L reference the same object
>>> L == M        # Same values
True
>>> L is M        # Same objects
True

# from shared-reference

>>> L = [1, 2, 3]
>>> M = [1, 2, 3]   # M and L reference different objects
>>> L == M          # Same values
True
>>> L is M          # Different objects
False


The first technique here, the == operator, tests whether the two referenced
objects have the `same values`; this is the method almost always used for
equality checks in Python.  

The second method, the `is operator`, instead tests for `object identity.` it
returns True only if both names point to the exact `same object`, so it is a
much `stronger form` of equality testing.


Learning Python 5E, 9, Tuples, Files, and Everything Else

When nested objects are present, Python automatically traverses data
structures to apply `comparisons from left to right`, and as deeply as needed.

The first difference found along the way determines the comparison result.
This is sometimes called a `recursive comparison`

*py-is-keyword*

>>> L1 = [1, ('a', 3)]  # Same value, unique objects
>>> L2 = [1, ('a', 3)]
>>> L1 == L2, L1 is L2  # Equivalent? Same object?
(True, False)

L1 and L2 are assigned lists that are equivalent but distinct objects.

  The == operator tests `value equivalence` Python performs an equivalence
  test, comparing all nested objects recursively.

  The is operator tests `object identity` Python tests whether the two are
  really the same object (i.e., live at the same address in memory).


Relative magnitude comparisons are also applied recursively to nested data
structures:

>>> L1 = [1, ('a', 3)]
>>> L2 = [1, ('a', 2)]
>>> L1 < L2, L1 == L2, L1 > L2    # Less, equal, greater: tuple of results
(False, False, True)

<ex>
#!/usr/bin/python

# returns 1 if greater is bigger than lesser
#        -1 is lesser  is bigger than greater
#         0 if they are equal
def compareVersions(greater, lesser):
    greater_array = [int(i) for i in greater.split('.')]
    lesser_array = [int(i) for i in lesser.split('.')]
    len_lesser = len(lesser_array)

    for i in range(len(greater_array)):
        try:
            if i >= len_lesser or greater_array[i] > lesser_array[i]:
                return 1
            elif greater_array[i] < lesser_array[i]:
                return -1
        except IndexError:
            # One of the versions strings is too short, assume they are the same
            print 'Warning unmatched version string lengths', greater, lesser                

    # if we get here they should be the same
    return 0

# returns 1 if greater is bigger than lesser
#        -1 is lesser  is bigger than greater
#         0 if they are equal
def compareVersions2(greater, lesser):
    greater_array = [int(i) for i in greater.split('.')]
    lesser_array = [int(i) for i in lesser.split('.')]

    return 1 if greater_array > lesser_array else -1

version1 = "40.65.00"
version2 = '40.66.00'

print 'version 1 is :', version1
print 'version 2 is :', version1

print 'compare result :', compareVersions( version1, version2 )
print 'compare result :', compareVersions2( version1, version2 )

print 'compare result :', compareVersions( version2, version1 )
print 'compare result :', compareVersions2( version2, version1 )

$ ./py-03.py
version 1 is : 40.65.00
version 2 is : 40.65.00
compare result : -1
compare result : -1
compare result : 1
compare result : 1

note: have to?
>>> s1 = '40.65.00'
>>> s2 = '40.66.00'
>>> s1 > s2
False
>>> s1 < s2
True
>>> s3 = '40.65.00'
>>> s1 == s3
True


More specifically, Python compares types as follows:

  Numbers are compared by relative magnitude, after conversion to the common
  highest type if needed.

  Strings are compared lexicographically (by the character set code point
      values returned by ord), and character by character until the end or
  first mismatch ("abc" < "ac").

  Lists and tuples are compared by comparing each component from left to
  right, and recursively for nested structures, until the end or first
  mismatch ([2] > [1, 2]).

  Sets are equal if both contain the same items (formally, if each is a subset
      of the other), and set relative magnitude comparisons apply subset and
  superset tests.

  Dictionaries compare as equal if their sorted (key, value) lists are equal.
  Relative magnitude comparisons are not supported for dictionaries in Python
  3.X, but they work in 2.X as though comparing sorted (key, value) lists.


Python 2.X and 3.X dictionary comparisons

In Python 2.X, dictionaries support magnitude comparisons, as though you were
comparing sorted key/value lists:

C:\code> c:\python27\python
>>> D1 = {'a':1, 'b':2}
>>> D2 = {'a':1, 'b':3}
>>> D1 == D2 # Dictionary equality: 2.X + 3.X
False
>>> D1 < D2 # Dictionary magnitude: 2.X only
True

magnitude comparisons for dictionaries are removed in Python 3.X because they
incur too much overhead when equality is desired (equality uses an optimized
    scheme in 3.X that doesn't literally compare sorted key/ value lists):

C:\code> c:\python33\python
>>> D1 = {'a':1, 'b':2}
>>> D2 = {'a':1, 'b':3}
>>> D1 == D2
False
>>> D1 < D2
TypeError: unorderable types: dict() < dict()

The alternative in 3.X is to either write loops to compare values by key, or
compare the sorted key/value lists manually—the items dictionary methods and
sorted built-in suffice:

>>> list(D1.items())
[('b', 2), ('a', 1)]
>>> sorted(D1.items())
[('a', 1), ('b', 2)]
>>>
>>> sorted(D1.items()) < sorted(D2.items()) # Magnitude test in 3.X
True
>>> sorted(D1.items()) > sorted(D2.items())
False

This takes more code, but in practice, most programs requiring this behavior
will develop more efficient ways to compare data in dictionaries than either
this workaround or the original behavior in Python 2.X.


<py-refcount>
Really, is simply compares the pointers that implement references, and it
serves as a way to detect shared references in your code if needed.

>>> import sys
>>> sys.getrefcount(1) # 647 pointers to this shared piece of memory
647


<variable-object>
Names have no types; as stated earlier, types live with objects, not names.

Objects that can be changed in place (that is, mutable objects) are lists,
dictionaries, sets, and some objects defined with class statements.


<garbage-collection>
Internally, Python accomplishes this feat by keeping a counter in every object
that keeps track of the number of references currently pointing to that
object. As soon as (and exactly when) this counter drops to zero, the object’s
memory space is automatically reclaimed.

For more details on Python's cycle detector, see the documentation for the
`gc` module in Python's library manual.


={============================================================================
|kt_dev_py_0001| py-base-type

Table A-2. Standard Python Scalar Types

`None` 
The Python "null" value (only one instance of the None object exists)

`str` 
String type. ASCII-valued only in Python 2.x and Unicode in Python 3

unicode 
Unicode string type

float 
Double-precision (64-bit) floating point number. Note there is no separate
double type.

<py-true>
True and False are just customized 1 and 0

bool 
A `True` or `False` value

`int` 
Signed integer with maximum value determined by the platform.

long 
Arbitrary precision signed integer. Large int values are automatically
converted to long.


<numeric-types> <py-print>
The primary Python types for numbers are `int` and `float`. 

The size of the integer which can be stored as an int is dependent on your
platform (whether 32 or 64-bit), but Python will transparently convert a very
large integer to `long`, which can store arbitrarily large integers.

Floating point numbers are represented with the Python float type. Under the
hood each one is a double-precision (64 bits) value.

In Python 3, integer division not resulting in a whole number will always
yield a floating point number:

In [284]: 3 / 2
Out[284]: 1.5

In Python 2.7 and below (which some readers will likely be using), you can
enable this behavior by default by putting the following cryptic-looking
statement at the top of your module:

from __future__ import division

Without this in place, you can always explicitly convert the denominator into
a floating point number:

In [285]: 3 / float(2)
Out[285]: 1.5

To get C-style integer division (which drops the fractional part if the result
is not a whole number), use the floor division operator //:

In [286]: 3 // 2
Out[286]: 1


>>> from __future__ import division
>>> 21600/90000
0.24
>>> "{:.2f}".format(21600/90000)
'0.24'


={============================================================================
|kt_dev_py_0001| py-base-none-type py-none

Learning Python 5E, 9, Tuples, Files, and Everything Else

The Meaning of True and False in Python

In Python, as in most programming languages, an integer 0 represents false,
and an integer 1 represents true. In addition, though, Python recognizes any
  empty data structure as false and any nonempty data structure as true.

More generally, the notions of true and false are intrinsic properties of
every object in Python—each `object is either true or false`, as follows:

  Numbers are false if zero, and true otherwise.

  Other objects are false if empty, and true otherwise.

As one application, because objects are true or false themselves, it’s common
to see Python programmers code tests like if X:, which, assuming X is a
string, is the same as if X != '':. In other words, you can test the object
itself to see if it contains anything, instead of comparing it to an empty,
and therefore false, object of the same type


The None object

Python also provides a special object called None, which is always considered
to be `false`. it is the only value of a special data type in Python and
typically serves as an empty placeholder (much like a NULL pointer in C).


#!/usr/bin/env python

# branches=''
branches=None

if branches:
    print ('branches is not null..')
else:
    print ('branches is null..')

the both prints:
branches is null..


For lists you cannot assign to an offset unless that offset already exists. To
preallocate a 100-item list such that you can add to any of the 100 offsets:

>>> L = [None] * 100
>>>
>>> L
[None, None, None, None, None, None, None, ... ]

This doesn't limit the size of the list (it can still grow and shrink later),
but simply presets an initial size to allow for future index assignments.  You
  could initialize a list with zeros the same way, of course, but best
  practice dictates using None if the type of the list’s contents is variable
  or not yet known.

Keep in mind that None does not mean “undefined.” That is, None is something,
not nothing (despite its name!)—it is a real object and a real piece of memory
  that is created and given a built-in name by Python itself. Watch for other
  uses of this special object later in the book; it is also the default return
  value of functions that don’t exit by running into a return statement with a
  result value.

`None` 
The Python "null" value (only one instance of the None object exists)


<string-null>

B = '1101'

while B != '':
    I = I*2 + (ord(B[0]) - ord('0'))


={============================================================================
|kt_dev_py_0001| py-base-control

{py-pass} {py-noop}
`pass` is the "no-op" statement in Python. It can be used in blocks where no
action is to be taken; it is only required because Python uses whitespace to
delimit blocks.

It's common to use pass as a place-holder in code while working on a new piece
of functionality:

# ok
if x < 0:
  print 'negative!'
elif x == 0:
  # TODO: put something smart here
  pass
else:
  print 'positive!'


# nk

if x < 0:
    print 'negative!'
elif x == 0:
    # TODO: put something here
else:
    print 'posivie!'

$ ./py-01.py 
  File "./py-01.py", line 17
    else:
       ^
IndentationError: expected an indented block


={============================================================================
|kt_dev_py_0001| py-base-loop

{range}
`range` produces integers up to but not including the endpoint. 

>>> range(10)
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

A common use of range is for iterating through sequences `by index`:

>>> range(4)
[0, 1, 2, 3]

>>> seq = [5,6,7,8]
>>> for i in range(len(seq)):
...     print seq[i]
...
5
6
7
8


For very long ranges, it's recommended to use `xrange`, which takes the same
arguments as range but `returns an iterator` that generates integers one by one
rather than generating all of them up-front and storing them in a (potentially
    very large) list. This snippet sums all numbers from 0 to 9999 that are
multiples of 3 or 5:

sum = 0
for i in xrange(10000):
  # % is the modulo operator
  if x % 3 == 0 or x % 5 == 0:
    sum += i


<py-for>
remove an element of a list in the loop

  for script_name, priority, test_id, process in running_list:
    do something
    running_list.remove((script_name, priority, test_id, process))


={============================================================================
|kt_dev_py_0001| py-base-what-evaluate

def total():
    return (elapsed, ret)

if __name__ == "main":
    ret = total( 1000, pow, 2, 1000)
    print 'time taken to pow is %d' % (ret.elapsed)

Why do not flag up error on ret.elapsed since there is no ret.elapsed
attribute?

Since python only cares code which runs. Even when have a totally nonsence def
in the code, there is no error since it's not used.

if __name__ == "__main__":
    ret = total( 1000, pow, 2, 1000)
    print 'time taken to pow is %d' % (ret.elapsed)


={============================================================================
|kt_dev_py_0001| py-base-comment


<ex> from dropbox

    def files_copy(self,
                   from_path,
                   to_path,
                   allow_shared_folder=False,
                   autorename=False,
                   allow_ownership_transfer=False):
        """
        Copy a file or folder to a different location in the user's Dropbox. If
        the source path is a folder all its contents will be copied.

        :param bool allow_shared_folder: If true, :meth:`files_copy` will copy
            contents in shared folder, otherwise
            ``RelocationError.cant_copy_shared_folder`` will be returned if
            ``from_path`` contains shared folder. This field is always true for
            :meth:`files_move`.
        :param bool autorename: If there's a conflict, have the Dropbox server
            try to autorename the file to avoid the conflict.
        :param bool allow_ownership_transfer: Allow moves by owner even if it
            would result in an ownership transfer for the content being moved.
            This does not apply to copies.
        :rtype: :class:`dropbox.files.Metadata`
        :raises: :class:`dropbox.exceptions.ApiError`

        If this raises, ApiError.reason is of type:
            :class:`dropbox.files.RelocationError`
        """

={============================================================================
|kt_dev_py_0001| py-gotchas

<default-mutable-objects>

Defaults and Mutable Objects, Chapter 21: The Benchmarking Interlude

>>> def saver(x=[]):    # Saves away a list object
x.append(1)             # Changes same object each time!
print(x)

>>> saver([2])          # Default not used
[2, 1]

>>> saver()             # Default used
[1]
>>> saver()             # Grows on each call!
[1, 1]
>>> saver()
[1, 1, 1]


<return-from-function-that-change-objects>

Expression Statements, Chapter 11: Assignments, Expressions, and Prints

Calling an in-place change operation such as append, sort, or reverse on a
list always changes the list in place, but these methods do not return the
list they have changed; instead, they return the None object.

>>> L = [1, 2]
>>> L.append(3)         # Append is an in-place change
>>> L
[1, 2, 3]

However, it’s not unusual for Python newcomers to code such an operation as an
assignment statement instead, intending to assign L to the larger list:

>>> L = L.append(4)     # But append returns None, not L
>>> print(L)            # So we lose our list!
None


Chapter 15: The Documentation Interlude

A more devious example of this pops up in Python 2.X code when trying to step
through dictionary items in a sorted fashion. It’s fairly common to see code
like for k in D.keys().sort():. 
  
This almost works—the keys method builds a keys list, and the sort method
  orders it—but because the sort method returns None, the loop fails because
  it is ultimately a loop over None (a nonsequence). 
                           
This fails even sooner in Python 3.X, because dictionary keys are views, not
lists! 

To code this correctly, either use the newer sorted built-in function, which
returns the sorted list, or split the method calls out to statements: Ks =
list(D.keys()), then Ks.sort(), and finally, for k in Ks:.  

This, by the way, is one case where you may still want to call the keys method
explicitly for looping, instead of relying on the dictionary
iterators—iterators do not sort.


={============================================================================
|kt_dev_py_0001| py-types 
More formally, there are three major type (and operation) categories in Python
that have this generic nature:

`Numbers` (integer, floating-point, decimal, fraction, others)
Support addition, multiplication, etc.

`Sequences` (strings, lists, tuples)
Support indexing, slicing, concatenation, etc.

`Mappings` (dictionaries)
Support indexing by key, etc.

`sets` are something of a category unto themselves because sets are unordered
and do not map keys to values, they are neither sequence nor mapping types


The major core types in Python break down as follows:

Immutables (numbers, strings, `tuples`, frozensets)
None of the object types in the immutable category support in-place changes,
though we can always run expressions to make new objects and assign their
  results to variables as needed.

Mutables (lists, dictionaries, sets, bytearray)
Conversely, the mutable types can always be changed in place with operations
that do not create new objects. Although such objects can be copied, in-place
changes support direct modification.


={============================================================================
|kt_dev_py_0001| py-variable

Learning Python 5E, 6, Dynamic Typing

Variable creation

A variable (i.e., name), like a, is created when your code first `assigns` it a
value.

Variable types

A variable never has any type information or constraints associated with it.
The notion of type `lives with objects, not names.` Variables refer,
reference, to any type of objects and are `never declared` ahead of time.

Variable use

When a variable appears in an expression, it is immediately replaced with the
object that it currently refers to, whatever that may be. Further, all
variables must be explicitly assigned(created) before they can be used;
referencing unassigned variables results in errors.

>>> c * 2
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
NameError: name 'c' is not defined


<py-object>
Technically speaking, objects have more structure than just enough space to
  represent their values. Each object also has two standard header fields: a
  `type designator` used to mark the type of the object, and a `reference counter`


Objects Are Garbage-Collected

The most immediately tangible benefit of garbage collection is that it means
you can use objects liberally without ever needing to allocate or free up
space in your script.  Python will clean up unused space for you as your
program runs. In practice, this eliminates a substantial amount of bookkeeping
code required in lower-level languages such as C and C++.

For more details on Python’s cycle detector, see the documentation for the gc
module in Python’s library manual.

Also note that this chapter’s description of Python’s garbage collector
applies to the standard Python (a.k.a. CPython) only;


<shared-reference> <mutalbe-object>

>>> a = 3
>>> b = a

>>> a = 3
>>> b = a
>>> a = a + 2

the last assignment then sets a to a completely different object (in this
    case, the integer 5, which is the result of the + expression). 

It does not change b as a side effect. In fact, there is no way to ever
overwrite the value of the object 3 - as introduced in Chapter 4, integers are
immutable and thus can never be changed in place.


# L1 and L2 reference the same object, shared reference

>>> L1 = [2, 3, 4]
>>> L2 = L1

# L1 refernece to different object

>>> L1 = 24

# However, in-place change has different effect.

>>> L1 = [2, 3, 4] 
>>> L2 = L1 

>>> L1[0] = 24

>>> L1
[24, 3, 4]

>>> L2 
[24, 3, 4]

This behavior only occurs for mutable objects that support in-place changes,
     and is usually what you want, but you should be aware of how it works, so
       that it’s expected.

It’s also just the default: if you don’t want such behavior, you can request
that Python copy objects instead of making references.

>>> L1 = [2, 3, 4]
>>> L2 = L1[:] # Make a copy of L1 (or list(L1), copy.copy(L1), etc.)
>>> L1[0] = 24


={============================================================================
|kt_dev_py_0001| py-number

Learning Python 5E, 5, Numeric Types

integers to have unlimited precisionthey can grow to have as many digits as
your memory space allows.

int(str, base) converts a runtime string to an integer per a given base.


{py-division}
In 3.X, the / now always performs true division, returning a float result that
includes any remainder, regardless of operand types. The // performs floor
division, which truncates the remainder and returns an integer for integer
operands or a float if any operand is a float.

In 2.X, the / does classic division, performing truncating integer division if
both operands are integers and float division (keeping remainders) otherwise.
The // does floor division and works as it does in 3.X, performing truncating
division for integers and floor division for floats.

the first operation in each set is the crucial difference between the lines
that may impact code:

C:\code> C:\Python33\python
>>>
>>> 10 / 4 # Differs in 3.X: keeps remainder
2.5

C:\code> C:\Python27\python
>>>
>>> 10 / 4 # This might break on porting to 3.X!
2

The floor division is to round `down`, not strictly truncate, and this matters
for negatives.

>>> import math
>>> math.floor(2.5) # Closest number below value
2
>>> math.floor(-2.5)
-3
>>> math.trunc(2.5) # Truncate fractional part (toward zero)
2
>>> math.trunc(-2.5)
-2


{chained-comparison} *python-way*

>>> X < Y < Z           # Chained comparisons: range tests
True

>>> X < Y and Y < Z
True


{py-random}
The standard library random module must be imported as well. This module
provides an array of tools, for tasks such as picking a random floating-point
number between 0 and 1, and selecting a random integer between two numbers:

>>> import random
  >>>
>>> random.random()
0.5566014960423105
>>> random.random()       # Random floats, integers, choices, shuffles
0.051308506597373515

>>> random.randint(1, 10)
5
>>> random.randint(1, 10)
9

>>> random.choice(['Life of Brian', 'Holy Grail', 'Meaning of Life'])
'Holy Grail'
>>> random.choice(['Life of Brian', 'Holy Grail', 'Meaning of Life'])
'Life of Brian'
>>> suits = ['hearts', 'clubs', 'diamonds', 'spades']
 
>>> random.shuffle(suits)
>>> suits
['spades', 'hearts', 'diamonds', 'clubs']
>>> random.shuffle(suits)
>>> suits
['clubs', 'diamonds', 'hearts', 'spades']


={============================================================================
|kt_dev_py_0001| py-string
  
Learning Python 5E, 7, String Fundamentals

Every string operation is defined to produce a new string as its result,
because strings are `immutable` 


String Conversion Tools

# Python 3.X
>>> "42" + 1
TypeError: Can't convert 'int' object to str implicitly

# Python 2.X
>>> "42" + 1
TypeError: cannot concatenate 'str' and 'int' objects

>>> int("42"), str(42)  # Convert from/to string
(42, '42')

>>> repr(42)            # Convert to as-code string
'42'

note: 
The `str` type name.

The repr function (and the older backquotes expression, removed in Python 3.X)
also converts an object to its string representation


Character code conversions

The built-in `ord` returns the actual binary value used to represent the
corresponding character in memory. The chr function performs the inverse
operation, taking an integer code and converting it to the corresponding
character:

>>> ord('s')
115
>>> chr(115)
's'

<ex> atoi
#!/usr/bin/python

B = '1101'
I = 0

while B != '':
    I = I*2 + (ord(B[0]) - ord('0'))
    B = B[1:]

print(I)

<ex>
int atoi(char s[])
{
  int n, i;

  for(n = 0, i = 0; s[i] >= '0' && s[i] <= '9'; i++)
    n = n*10 + (s[i]-'0');

  return n;
}


>>> int(0b1101)
13
>>> int('1101', 2)
13
>>> bin(13)
'0b1101'


<string-quote>
Can write string literal using either single quotes ' or double quotes ":
Most programmers prefer single quotes

For multiline strings with line breaks, you can use `triple quotes`, either '''
or """:

c = """
This is a longer string that
spans multiple lines
"""


https://docs.python.org/2/library/string.html

7.1.6. Deprecated string functions

The following list of functions are also defined as methods of string and
Unicode objects; see section String Methods for more information on those. 

You should consider these functions `as deprecated`, although they will not be
removed until Python 3. The functions defined in this module are:

<string-methods>
https://docs.python.org/2/library/stdtypes.html?highlight=endswith#str.endswith

5.6.1. String Methods

str.endswith(suffix[, start[, end]])

    Return True if the string ends with the specified suffix, otherwise return
    False. suffix can also be a tuple of suffixes to look for. With optional
    start, test beginning at that position. With optional end, stop comparing
    at that position.

    Changed in version 2.5: Accept tuples as suffix.


str.title()

    Return a titlecased version of the string where words start with an
    uppercase character and the remaining characters are lowercase.


str.split([sep[, maxsplit]])

    Return a list of the words in the string, using sep as the delimiter
    string. If maxsplit is given, at most maxsplit splits are done (thus, the
        list will have at most maxsplit+1 elements). If maxsplit is not
    specified or -1, then there is no limit on the number of splits (all
        possible splits are made).

    If sep is given, consecutive delimiters are not grouped together and are
    deemed to delimit empty strings (for example, '1,,2'.split(',') returns
        ['1', '', '2']). The sep argument may consist of multiple characters
    (for example, '1<>2<>3'.split('<>') returns ['1', '2', '3']). Splitting an
      empty string with a specified separator returns [''].

    If sep is not specified or is None, a different splitting algorithm is
    applied: runs of consecutive whitespace are regarded as a single
    separator, and the result will contain no empty strings at the start or
    end if the string has leading or trailing whitespace. Consequently,
                        splitting an empty string or a string consisting of
                          just whitespace with a None separator returns [].

    For example, ' 1  2   3  '.split() returns ['1', '2', '3'], and '  1  2
    3  '.split(None, 1) returns ['1', '2   3  '].

    p = re.compile (pattern)
    lines = stdOut.split ("\n")
    for line in lines:
        parts = line.split ("\t")
        if len (parts) == 2:
            tag = parts[1].strip ()
            if p.match (tag):
                if not tag.endswith ("{}"):
                    tagsAux.append (tag)

    Delimiters can be longer than a single character, too:

    >>> line = "i'mSPAMaSPAMlumberjack"
    >>> line.split("SPAM")
    ["i'm", 'a', 'lumberjack']


str.rstrip([chars])

    Return a `copy` of the string with trailing characters removed. 
    
    The chars argument is a string specifying the `set of characters` to be
    removed. If omitted or None, the chars argument defaults to removing
    whitespace. The chars argument is not a suffix; rather, all combinations
    of its values are stripped:

    >>> '   spacious   '.rstrip()
    '   spacious'
    >>> 'mississippi'.rstrip('ipz')
    'mississ'

    note: Compared with [:-1]

    Chapter 7: String Fundamentals
    A line[:−1] slice would work, too. Having said that, calling the
    line.rstrip method is often preferred for stripping newline characters
    because this call leaves the line intact if it has no newline character at
    the end - a common case for files created with some text-editing tools.
    Slicing works if you're sure the line is properly terminated.


str.join(iterable)

    Return a string which is the concatenation of the strings in the iterable.
    The separator between elements is the string providing this method.

    >>> mlist
    ['pay', 'job', 'name']
    >>> ','.join(mlist)
    'pay,job,name'
    >>> '_'.join(mlist)
    'pay_job_name'
    >>> '*'.join(mlist)
    'pay*job*name'


str.find()

The find method returns the `offset` where the substring appears (by default,
    searching from the front), or `−1 if it is not found.`
As we saw earlier, it's a substring search operation just like the in
expression, but find returns the position of a located substring.


str.replace()

>>> S = 'xxxxSPAMxxxxSPAMxxxx'
>>> S.replace('SPAM', 'EGGS')     # Replace all
'xxxxEGGSxxxxEGGSxxxx'
>>> S.replace('SPAM', 'EGGS', 1)  # Replace one
'xxxxEGGSxxxxSPAMxxxx'


<perfoemance>
If you have to apply many changes to a very large string, you might be able to
improve your script's performance by converting the string to an object that
does support in-place changes:

>>> S = 'spammy'
>>> L = list(S)
>>> L
['s', 'p', 'a', 'm', 'm', 'y']

>>> L[3] = 'x' # Works for lists, not strings
>>> L[4] = 'x'

>>> L
['s', 'p', 'a', 'x', 'x', 'y']

If, after your changes, you need to convert back to a string, use the string
  join method to "implode" the list back into a string:

>>> S = ''.join(L)
>>> S
'spaxxy'

str.join()

`join` puts the strings in a list (or other iterable) together, with the
delimiter between list items; in this case, it uses an empty string delimiter
to convert from a list back to a string. More generally, any string delimiter
and iterable of strings will do:

>>> 'SPAM'.join(['eggs', 'sausage', 'ham', 'toast'])
'eggsSPAMsausageSPAMhamSPAMtoast'

>>> '_'.join("one")
'o_n_e'
>>> '|'.join("one")     # o| n|e
'o|n|e'
>>> '|'.join(['one'])
'one'


<string-module>
The Original string Module's Functions (Gone in 3.X)

The history of Python's string methods is somewhat convoluted. For roughly the
first decade of its existence, Python provided a standard library module
called `string` that contained functions that largely mirrored the current set
of string object methods. 

By popular demand, in Python 2.0 these functions were made available as
methods of string objects. Because so many people had written so much code
that relied on the original string module, however, it was retained for
backward compatibility.

Today, you should use only string methods, not the original string module. In
fact, the original module call forms of today's string methods have been
removed completely from Python 3.X, and you should not use them in new code in
either 2.X or 3.X.


{string-formatting} *py-print*

print(*objects, sep=' ', end='\n', file=sys.stdout, flush=False)

    Print objects to the text stream file, separated by sep and followed by
    end. sep, end and file, if present, must be given as keyword arguments.
    All non-keyword arguments are converted to strings like str() does and
    written to the stream, separated by sep and followed by end. Both sep and
    end must be strings; they can also be None, which means to use the default
    values. If no objects are given, print() will just write end.  The file
    argument must be an object with a write(string) method; if it is not
    present or None, sys.stdout will be used. Since printed arguments are
    converted to text strings, print() cannot be used with binary mode file
    objects. For these, use file.write(...) instead.  Whether output is
    buffered is usually determined by file, but if the flush keyword argument
    is true, the stream is forcibly flushed.  Changed in version 3.3: Added
    the flush keyword argument.

<ex>
print "item in the list: %s" % item


String Formatting Expressions

string formatting allows us to perform multiple type-specific substitutions on
a string in a single step.

string formatting is available in two flavors

String formatting expressions: '...%s...' % (values)

The original technique available since Python's inception, this form is based
upon the C language's "printf" model, and sees widespread use in much existing
code.

String formatting method calls: '...{}...'.format(values)

A newer technique added in Python 2.6 and 3.0, this form is derived in part
from a same-named tool in C#/.NET, and overlaps with string formatting
expression functionality.

# Formatting expression (all)
>>> '%s, eggs, and %s' % ('spam', 'SPAM!') 
'spam, eggs, and SPAM!'

# Formatting method (2.6+, 3.0+)
>>> '{0}, eggs, and {1}'.format('spam', 'SPAM!') 
'spam, eggs, and SPAM!'

# Numbers optional (2.7+, 3.1+)
>>> '{}, eggs, and {}'.format('spam', 'SPAM!') 
'spam, eggs, and SPAM!'

>>> '{0:o}, {1:x}, {2:b}'.format(64, 64, 64) # Numbers=>digits, 2.6+
'100, 40, 1000000'

>>> '%s -- %s -- %s' % (42, 3.14159, [1, 2, 3]) # All types match a %s target
'42 -- 3.14159 -- [1, 2, 3]'

As every type of object can be converted to a string, every object type works
with the %s conversion code. Because of this, unless you will be doing some
special formatting, %s is often the only code you need to remember for the
formatting expression.

Again, keep in mind that formatting always makes a new string, rather than
changing the string on the left.

<ex>
>>> L = [1,2,3]
>>> print 'print list: %s' % (L)
print list: [1, 2, 3]


{raw-string}
raw string literal that turns off the backslash escape mechanism. Such
literals start with the letter r and are useful for strings like directory
paths on Windows (e.g., r'C:\text\new').


={============================================================================
|kt_dev_py_0001| py-tuple

Learning Python 5E, 9, Tuples, Files, and Everything Else

A tuple is a one-dimensional, `fixed-length`, `immutable` sequence of Python
objects. 


Tuple syntax peculiarities: Commas and parentheses

Because parentheses can also enclose expressions (see Chapter 5), you need to
do something special to tell Python when a single object in parentheses is a
  tuple object and not a simple expression.

The syntax (...) is used for tuples and expression grouping, as well as
generator expressionsa form of list comprehension that produces results on
demand, instead of building a result list. 

T = (0,)          # A one-item tuple (not an expression)

>>> x = (40)      # An integer!
>>> x
40
>>> y = (40,)     # A tuple containing an integer
>>> y
(40,)


# The easiest way to create one is with a comma-separated sequence of values

>>> tup = 4,5,6   # >>> tup = (4,5,6)
>>> tup
(4, 5, 6)


# necessary to enclose the values in `parentheses` to create a tuple of tuples:

>>> nested_tup=(4,5,6),(7,8)
>>> nested_tup
((4, 5, 6), (7, 8))


# Any sequence or iterator can be converted to a tuple by invoking tuple:

>>> tuple([4,5,6])
(4, 5, 6)

>>> string_tup=tuple('string')
>>> string_tup
('s', 't', 'r', 'i', 'n', 'g')


# Accessed with square brackets [] as with most other sequence types.
# Sequences are 0-indexed in Python:

>>> string_tup[0]
's'


# Once created it's not possible to modify which object is stored in each slot:

>>> tup
(4, 5, 6)
>>> tup[1]=7
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'tuple' object does not support item assignment


# Can be concatenated using the + operator to produce longer tuples:

>>> long_tup = tup + nested_tup + string_tup
>>> long_tup
(4, 5, 6, (4, 5, 6), (7, 8), 's', 't', 'r', 'i', 'n', 'g')

note:
`tuple of tuple` so sequence can have different types.


{py-unpacking}
If you try to assign to a tuple-like expression of variables, Python will
attempt to unpack the value on the right-hand side of the equals sign:

>>> a,b,c = tup
>>> a
4
>>> b
5
>>> c
6


Using this functionality it's easy to swap `variable names`, a task which in
many languages might look like:

  tmp = a
  a = b
  b = tmp

>>> a,b = b,a
>>> a
5
>>> b
4
>>> c
6


One of the most common uses of variable unpacking when `iterating` over
sequences of tuples or lists:

<ex>
seq = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]
for a, b, c in seq:
  print a

1
4
7

<ex>
seq = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]
for a, b, c, d in seq:
  print a

Traceback (most recent call last):
  File "./py-01.py", line 15, in <module>
    for a, b, c, d in seq:
ValueError: need more than 3 values to unpack


<method>
Since the size and contents of a tuple cannot be modified, it is very light on
instance methods. `count`, which counts the number of `occurrences` of a
value:

>>> a=1,2,2,2,3,4,2
>>> a
(1, 2, 2, 2, 3, 4, 2)
>>> a.count(2)
4


<membership>
>>> tup
('foo', 'bar', 'baz')
>>> 'bar' in tup
True


={============================================================================
|kt_dev_py_0001| py-list

In contrast with tuples, lists are `variable-length` and `mutable`. They can
be defined using `square brackets []` or using the `list` type function:

Lists and tuples are semantically similar as one-dimensional sequences of
objects and thus can be used interchangeably in many functions.

>>> b_list=list(tup)
>>> b_list
['foo', 'bar', 'baz']


<indexing>

Learning Python 5E, 8 Lists and Dictionaries

Indexing, Slicing, and Matrixes

Because lists are sequences, indexing and slicing work the same way for lists
as they do for strings. However, the result of indexing a list is whatever
type of object lives at the offset you specify, while slicing a list always
`returns a new list`:

>>> L = ['spam', 'Spam', 'SPAM!']
>>> L[2]                  # Offsets start at zero
'SPAM!'
>>> L[−2]                 # Negative: count from the right
'Spam'
>>> L[1:]                 # Slicing fetches sections
['Spam', 'SPAM!']


Index and slice assignments

When using a list, you can change its contents by assigning to either a
particular item (offset) or an entire section (slice):

>>> L = ['spam', 'Spam', 'SPAM!']
>>> L[1] = 'eggs'               # Index assignment
>>> L
['spam', 'eggs', 'SPAM!']
>>> L[0:2] = ['eat', 'more']    # Slice assignment: delete+insert
>>> L                           # Replaces items 0,1
['eat', 'more', 'SPAM!']

>>> L = [1, 2, 3]
>>> L[1:2] = [4, 5]       # Replacement/insertion
>>> L
[1, 4, 5, 3]

>>> L[1:1] = [6, 7]       # Insertion (replace nothing)
>>> L
[1, 6, 7, 4, 5, 3]

>>> L[1:2] = []           # Deletion (insert nothing)
>>> L
[1, 7, 4, 5, 3]

Because the length of the sequence being assigned does not have to match the
  length of the slice being assigned to, slice assignment can be used to
  replace (by overwriting), expand (by inserting), or shrink (by deleting) the
  subject list. 

It's a powerful operation, but frankly, one that you may not see very often in
practice. There are often `more straightforward and mnemonic ways` to replace,
  insert, and delete (concatenation, and the insert, pop, and remove list
      methods, for example), which Python programmers tend to prefer in
    practice.


Other common list operations

Because lists are mutable, you can use the `del` statement to delete an item or
section in place:

>>> L = ['spam', 'eggs', 'ham', 'toast']
>>> del L[0]          # Delete one item
>>> L
['eggs', 'ham', 'toast']
>>> del L[1:]         # Delete an entire section
>>> L # Same as L[1:] = []
['eggs']


<adding-removing>
Append to the end of the list with the `append` method and `insert` an element
at a specific location in the list:

>>> b_list.append('dwarf')
>>> b_list
['foo', 'bar', 'baz', 'dwarf']
>>> b_list.insert(1,'red')
>>> b_list
['foo', 'red', 'bar', 'baz', 'dwarf']

note: 
insert is computationally `expensive` compared with append as references to
subsequent elements have to be shifted internally to make room for the new
element.

<append-the-same>
>>> a = [s]
>>> a
['1']
>>> len(a)            # py-len
1
>>> a = [s]
>>> a = [s]
>>> len(a)
1
>>> a
['1']


`pop` removes and returns an element at a particular index:

>>> b_list.pop(2)
'bar'
>>> b_list
['foo', 'red', 'baz', 'dwarf']

>>> L
[1, 2]
>>> L.pop() # Pop off stack
2


`remove` locates `the first` such value and removes it from the last:

>>> b_list
['foo', 'red', 'red', 'red', 'baz', 'dwarf']
>>> b_list.remove('red')
>>> b_list
['foo', 'red', 'red', 'baz', 'dwarf']


<membership>
As with tuple.

note:
checking whether a list contains a value is a lot `slower` than dicts and sets
as Python makes a linear scan across the values of the list, whereas the
others (based on hash tables) can make the check in constant time.


<concatenation>
Append multiple elements to it using the `extend` method:

>>> [4, None, 'foo'] + [7,8,(2,3)]
[4, None, 'foo', 7, 8, (2, 3)]

>>> x=[4, None, 'foo']
>>> x.extend([7,8,(2,3)])
>>> x
[4, None, 'foo', 7, 8, (2, 3)]

list concatenation is a compartively `expensive` operation since a `new list`
  must be created and the objects copied over. Using extend is usually
  preferable.

everything = []
for chunk in list_of_lists:
  everything.extend(chunk)

is faster than than the concatenative alternative

everything = []
for chunk in list_of_lists:
  everything = everything + chunk

note:
Know that `extend` adds many items, and `append` adds one.


<list-sort> 
A list can be sorted `in-place` (without creating a new object) by calling its
`sort` function:

>>> tup = 2,4,3,5,7,6
>>> tup
(2, 4, 3, 5, 7, 6)

>>> lst = [2,4,3,5,7,8]
>>> lst
[2, 4, 3, 5, 7, 8]

>>> tup.sort()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'tuple' object has no attribute 'sort'

>>> lst.sort()
>>> lst
[2, 3, 4, 5, 7, 8]


Can modify sort behavior by passing in `keyword arguments` - a special
  "name=value" syntax in function calls that specifies passing by name and is
  often used for giving configuration options.

One is the ability to pass a secondary `sort key`, i.e. a `function` that
produces a value to use to sort the objects.

>>> lst = ['saw', 'small', 'He', 'foxes', 'six']
>>> lst
['saw', 'small', 'He', 'foxes', 'six']
>>> lst.sort(key=len)
>>> lst
['He', 'saw', 'six', 'small', 'foxes']


>>> L = ['abc', 'ABD', 'aBe']
>>> L.sort()                              # Sort with mixed case
>>> L
['ABD', 'aBe', 'abc']
>>> L = ['abc', 'ABD', 'aBe']
>>> L.sort(key=str.lower)                 # Normalize to lowercase
>>> L
['abc', 'ABD', 'aBe']
>>>
>>> L = ['abc', 'ABD', 'aBe']
>>> L.sort(key=str.lower, reverse=True)   # Change sort order
>>> L
['aBe', 'ABD', 'abc']


<sorted-function>
Partly because of such constraints, sorting is also available in recent
Pythons as a builtin function, which sorts any collection (not just lists) and
`returns a new list` for the result (instead of in-place changes):

>>> L = ['abc', 'ABD', 'aBe']
>>> sorted(L, key=str.lower, reverse=True) # Sorting built-in
['aBe', 'ABD', 'abc']

>>> L = ['abc', 'ABD', 'aBe']
>>> sorted([x.lower() for x in L], reverse=True) # Pretransform items: differs!
['abe', 'abd', 'abc']

Notice the last example here - we can convert to lowercase prior to the sort
  with a list comprehension, but the result `does not contain` the original
  list's values as it does with the key argument. 

The latter is applied temporarily during the sort, instead of changing the
values to be sorted altogether.


*py-bisect*

The built-in bisect module implements binary-search and insertion into a
sorted list. bisect.bisect finds the `location` where an element should be
inserted to keep it sorted, while bisect.insort actually `inserts` the element
into that location:

>>> import bisect

>>> c = [1,2,2,2,3,4,7]
>>> bisect.bisect(c,2)
4

>>> bisect.insort(c,6)
>>> c
[1, 2, 2, 2, 3, 4, 6, 7]

The bisect module functions do not check whether the list is sorted. Thus,
  using them with an unsorted list will succeed without error but may lead to
    incorrect results.


<nested-list>
# S = """ [
#     [["AMS"], ["AMS"]],
#     [["PROX"], ["darwin"]]
#     ...
#     # can have comment in the middle.
#     [["PPCM_CF"], ["ppcm", "ppcm_core"]],
# ] """

# list or string? where `eval` makes differnece *py-eval*

# this is a string
>>> rlist = '[[["AMS"], ["AMS"]],[["PROX"], ["darwin"]]]'
>>> rlist
'[[["AMS"], ["AMS"]],[["PROX"], ["darwin"]]]'
>>> rlist[0]
'['
>>> rlist[1]
'['
>>> rlist[2]
'['
>>> rlist[3]
'"'
>>> type(rlist)
<type 'str'>

# this is a list
>>> xlist = [[["AMS"], ["AMS"]],[["PROX"], ["darwin"]]]
>>> xlist
[[['AMS'], ['AMS']], [['PROX'], ['darwin']]]
>>> xlist[0]
[['AMS'], ['AMS']]
>>> type(xlist)
<type 'list'>

# `eval` used to make a list
>>> expstr = '[[["AMS"], ["AMS"]],[["PROX"], ["darwin"]]]'
>>> rlist = eval(expstr)
>>> rlist
[[['AMS'], ['AMS']], [['PROX'], ['darwin']]]
>>> rlist[0][0]
['AMS']
>>> rlist[1]
[['PROX'], ['darwin']]
>>> rlist[0]
[['AMS'], ['AMS']]
>>> rlist[1]
[['PROX'], ['darwin']]
>>> rlist[1][0]
['PROX']
>>> rlist[1][1]
['darwin']


# convert a nested list to a dict

  readList = eval (content)
  self._segments = {}
  for element in readList:

      # here element is [['AMS'], ['AMS']]
      # element[0] is ['AMS'] which is list
      # element[0][0] is 'AMS' which is string
      self._segments [element[0][0]] = element [1]


<list-string>

>>> A=["spam"]
>>> A
['spam']
>>> len(A)
1

# makes a string to a list
>>> L="spam"
>>> L
'spam'
>>> list(L)
['s', 'p', 'a', 'm']
>>> len(list(L))
4


={============================================================================
|kt_dev_py_0001| py-slice

You can select sections of list-like types (arrays, tuples, NumPy arrays) by
using slice notation, which in its basic form consists of `start:stop` passed
to the `indexing operator []`:

>>> seq=[7,2,3,7,5,6,0,1]
>>> seq
[7, 2, 3, 7, 5, 6, 0, 1]

Their general form, X[I:J], means "give me everything in X from offset I up to
  but not including offset J." The result is returned in a `new object`

# (start, end] in C++ iterator notation.

>>> seq[1:1]
[]

>>> seq[1:2]
[2]

>>> seq[1:5]
[2, 3, 7, 5]

# inserted actually

>>> seq[3:4] = [6,3]
>>> seq
[7, 2, 3, 6, 3, 5, 6, 0, 1]


<default-to>
Either the start or stop can be omitted in which case they `default to` the
start of the sequence and the end of the sequence, respectively:

>>> S[1:]   # Everything past the first (1:len(S))
'pam'
>>> S       # S itself `hasn't changed`
'Spam'
>>> S[0:3]  # Everything but the last
'Spa'
>>> S[:3]   # Same as S[0:3]
'Spa'
>>> S[:-1]  # Everything but the last again, but simpler (0:-1)
'Spa'
>>> S[:]    # All of S as a top-level `copy` (0:len(S))
'Spam'

note: see how the last operation effectively copies the entire string.


<ex> how to get last four characters of a string?
>>> s
'0123456789'
>>> s[-4:]
'6789'


<py-slice-step>
A `step` can also be used after a second colon

[7, 2, 3, 6, 3, 5, 6, 0, 1]
>>> seq[::2]
[7, 3, 3, 6, 1]

Negative indices slice the sequence relative to the end:

   H  E  L  L  O  !
   0  1  2  3  4  5  6
  -6 -5 -4 -3 -2 -1

A clever use of this is to pass -1 which has the useful effect of reversing a
list or tuple:

[7, 2, 3, 6, 3, 5, 6, 0, 1]
>>> seq[::-1]
[1, 0, 6, 5, 3, 6, 3, 2, 7]


<copy>
>>> L1 = [2, 3, 4]
>>> L2 = L1[:]      # Make a copy of L1 (or list(L1), copy.copy(L1), etc.)
>>> L1[0] = 24

>>> L1
[24, 3, 4]
>>> L2              # L2 is not changed
[2, 3, 4]


={============================================================================
|kt_dev_py_0001| py-reverse

<1> from py-list
>>> L = ['abc', 'ABD', 'aBe']
>>> L.sort(key=str.lower, reverse=True)   # Change sort order
>>> L
['aBe', 'ABD', 'abc']

<2> from py-slice-step
A clever use of this is to pass -1 which has the useful effect of reversing a
list or tuple:

[7, 2, 3, 6, 3, 5, 6, 0, 1]
>>> seq[::-1]
[1, 0, 6, 5, 3, 6, 3, 2, 7]

<3> from py-sequence-function
`reversed` iterates over the elements of a sequence in reverse order:

>>> range(10)
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> reversed(range(10))
<listreverseiterator object at 0xb753a8ec>
>>> list(reversed(range(10)))
[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]


={============================================================================
|kt_dev_py_0001| py-dict

Learning Python 5E, 8 Lists and Dictionaries

Dictionaries are sometimes called associative arrays or hashes. Internally,
dictionaries are implemented as hash tables.

Dictionaries, the only mapping type in core objects set, are also `mutable`:
like lists, they may be changed in place and can grow and shrink on demand.

Unlike out-of-bounds assignments in lists, which are forbidden, assignments to
new dictionary keys create those keys:

>>> D = {}
>>> D['name'] = 'Bob'             # Create keys by assignment
>>> D['age'] = 40


A more common name for it is hash map or associative array. It is a
flexibly-sized collection of key-value pairs, where key and value are Python
objects. 

One way to create one is by using curly `braces` {} and using `colons` to
separate keys and values:

{'name': 'Bob', 'age': 40}        # Traditional literal expression

dict(name='Bob', age=40)          # dict keyword argument form


note: set vs dict
  return {'mac': result}          # returns a dict
  return {'mac', result}          # returns a set


<dict-sort>
Notice how the left-to-right order of dictionary keys is scrambled. Mappings
are not positionally ordered, they'll come back in a different order than you
typed them.

What do we do, though, if we do need to impose an ordering on a dictionary's
items?  One common solution is to grab a list of keys with the dictionary keys
method, sort that with the list sort method, and then step through the result
with a Python for loop

>>> Ks = list(D.keys())         # Unordered keys list
>>> Ks                          # A list in 2.X, "view" in 3.X: use list()
['a', 'c', 'b']

>>> Ks.sort()                   # Sorted keys list
>>> Ks
['a', 'b', 'c']

*iteration-protocol*
>>> for key in Ks:              # Iterate though sorted keys
      print(key, '=>', D[key])  # <== press Enter twice here (3.X print)
a => 1
b => 2
c => 3

This is a three-step process, and in recent versions of Python it can be done
in one step with the newer `sorted()` built-in function.

>>> D
{'a': 1, 'c': 3, 'b': 2}
>>> for key in sorted(D):
      print(key, '=>', D[key])
a => 1
b => 2
c => 3


<membership>
although we can assign to a new key to expand a dictionary, fetching a
nonexistent key is still a mistake.

The dictionary `in membership expression` allows us to query the existence of
a key and branch on the result with a Python if statement.

Can check `if a dict contains a key` using the same syntax as with checking
whether a list or tuple contains a value:

>>> 'b' in d1
True

>>> if not 'f' in D:          # Python's sole selection statement
  print('missing')

# see comprehension:
#
# >>> strings=['a', 'as', 'bat', 'car', 'dove', 'python']
# >>> [x.upper() for x in strings if len(x) > 2]
# ['BAT', 'CAR', 'DOVE', 'PYTHON']


<dict-looping>
  # segments is a dict
  segments = D.getSegments ()
  if segments != None:

      # makes a list from keys of a dict
      keys = sorted (list (segments.keys ()))     # sorted
      for key in keys:
          if args.detail:
              print key + ":",

              # looks up the value of a dict
              for build in segments [key]:
                  print build,
              print " "

In fact, Python also lets you step through a dictionary's keys list without
actually calling the keys method in most for loops. 
For any dictionary D, saying `for key in D` works the same as saying the
complete `for key in D.keys()`

>>> table = {'1975': 'Holy Grail',    # Key: Value
... '1979': 'Life of Brian',
... '1983': 'The Meaning of Life'}

>>> for year in table:                # Same as: for year in table.keys()
... print(year + '\t' + table[year])
...
1979 Life of Brian
1975 Holy Grail
1983 The Meaning of Life


Values can be deleted either using the `del` keyword or the `pop` method
(which simultaneously returns the value and deletes the key):

>>> d1
{'a': 'some value', 'dummy': 'another value', 'b': [1, 2, 3, 4], 5: 'some value', 7: 'an integer'}
>>> del d1[5]
>>> d1
{'a': 'some value', 'dummy': 'another value', 'b': [1, 2, 3, 4], 7: 'an integer'}
>>> d1.pop('dummy')
'another value'
>>> d1
{'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}


The `keys() and values()` give you lists of the keys and values, respectively.
While the key-value pairs are not in any particular order, these functions
output the keys and values in the same order:

>>> d1.keys()
['a', 'b', 7]
>>> d1.values()
['some value', [1, 2, 3, 4], 'an integer']


<dict-update>
One dict can be merged into another using the `update` method:

>>> d1
{'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}
>>> d1.update({'b':'foo','c':12})
>>> d1
{'a': 'some value', 'c': 12, 'b': 'foo', 7: 'an integer'}

note: 'b' is overwritten.


Preview: Mapping values to keys

Notice how the prior table maps year to titles, but not vice versa. If you
want to map the other way-titles to years-you can either code the dictionary
differently, or use methods like items() that give searchable sequences,

>>> table = {'Holy Grail': '1975',  # Key=>Value (title=>year)
... 'Life of Brian': '1979',
... 'The Meaning of Life': '1983'}
>>>
>>> table['Holy Grail']
'1975'

>>> list(table.items())             # Value=>Key (year=>title)
[('The Meaning of Life', '1983'), ('Holy Grail', '1975'), ('Life of Brian', '1979')]

>>> [title for (title, year) in table.items() if year == '1975']
['Holy Grail']

searching through sequences like this is generally much slower than a direct
key index


<dict-from-sequence> <py-zip>
Common to occasionally end up with two sequences that you want to pair up
element-wise in a dict.

mapping = {}
for key, value in zip(key_list, value_list):
  mapping[key] = value

Since a dict is essentially a collection of 2-tuples, key and value tuple, the
dict type function accepts a list of 2-tuples:

mapping = dict(zip(range(5), reversed(range(5))))
mapping
Out[454]: {0: 4, 1: 3, 2: 2, 3: 1, 4: 0}

>>> bob2 = dict(zip(['name', 'job', 'age'], ['Bob', 'dev', 40])) # Zipping
>>> bob2
{'job': 'dev', 'name': 'Bob', 'age': 40}

>>> l
['one', 'two', 'three']
>>> mapping = dict((v,i) for i,v in enumerate(l))
>>> mapping
{'three': 2, 'two': 1, 'one': 0}


<dict-nonexistent-key-exception>
Elements can be accessed and inserted using the same syntax as accessing
elements of a list or tuple:

>>> d1={'a':'some value', 'b':[1,2,3,4]}
>>> d1
{'a': 'some value', 'b': [1, 2, 3, 4]}
>>> d1[7]='an integer'
>>> d1
{'a': 'some value', 'b': [1, 2, 3, 4], 7: 'an integer'}
>>> d1['b']
[1, 2, 3, 4]

>>> d1[b]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
KeyError: 8


<dict-default-values>
Very common to have logic like:

if key in some_dict:
  value = some_dict[key]
else:
  value = default_value

Thus, the dict methods `get()` and pop() can take a default value to be
returned, so that the above if-else block can be written simply as:

value = some_dict.get(key, default_value)

`get` by default will return `None` if the key is not present, while `pop`
will raise an exception.

Fetching a nonexistent key is normally an error, but the get method returns a
default value-None, or a passed-in default-if the key doesn’t exist.

>>> D.get('spam') # A key that is there
2
>>> print(D.get('toast')) # A key that is missing
None
>>> D.get('toast', 88)
88


<other-collection-as-value>

>>> di={}
>>> di['b'] = ['apple', 'atom']
>>> di
{'b': ['apple', 'atom']}

With setting values, a common case is for the values in a dict to be other
collections, like lists. For example, you could imagine categorizing a list of
words by their first letters as a dict of lists:

>>> words=['apple', 'bat', 'bar', 'atom', 'book']
>>> by_letter={}
>>> for word in words:
...     letter = word[0]
...     if letter not in by_letter:
...             by_letter[letter] = [word]
...     else:
...             by_letter[letter].append(word)
... 
>>> by_letter
{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}


The `setdefault` dict method is for precisely this purpose.

note: HOW does all work?

>>> by_letter={}
>>> words=['apple', 'bat', 'bar', 'atom', 'book']
>>> for word in words:
...     letter = word[0]
...     by_letter.setdefault(letter,[]).append(word)
... 
>>> by_letter
{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}


The built-in `collections` module has a useful class, `defaultdict`, which makes
  this even easier. One is created by passing a type or function for
  generating the default value for each slot in the dict: 
  
from collections import defaultdict 
by_letter = defaultdict(list) 
for word in words:
  by_letter[word[0]].append(word) 
  

The initializer to defaultdict only needs to be a callable object (e.g. any
    function), not necessarily a type. Thus, if you wanted the default value
to be 4 you could pass a function returning 4

counts = defaultdict(lambda: 4)


<key-types>
While the values of a dict can be `any` Python object, the keys have to be
`immutable` objects like scalar types (int, float, string) or tuples (all the
    objects in the tuple need to be immutable, too). The technical term here
is `hashability`. You can `check whether an object is hashable` (can be used as
    a key in a dict) with the hash function:

>>> hash('string')
-1542666171
>>> hash((1,2,(2,3)))
1387206534
>>> hash((1,2,[2,3]))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: unhashable type: 'list'


in dictionaries, there's just one value per key, but there may be many keys
per value.


Using dictionaries to simulate flexible lists: using integer keys

>>> L = []
>>> L[99] = 'spam'
Traceback (most recent call last):
File "<stdin>", line 1, in ?
IndexError: list assignment index out of range

Although you can use repetition to preallocate as big a list as you’ll need
(e.g., [0]*100), you can also do something that looks similar with
dictionaries that does not require such space allocations. By using integer
keys, dictionaries can emulate lists that seem to grow on offset assignment:

>>> D = {}
>>> D[99] = 'spam'
>>> D[99]
'spam'
>>> D
{99: 'spam'}

Here, it looks as if D is a 100-item list, but it's really a dictionary with a
single entry; the value of the key 99 is the string 'spam'. You can access
this structure with offsets much like a list, catching nonexistent keys with
get or in tests if required, but you don’t have to allocate space for all the
positions you might ever need to assign values to in the future. When used
like this, dictionaries are `like more flexible equivalents of lists.`


To use a list as a key, an easy fix is to convert it to a tuple:

>>> d = {}
>>> d[tuple([1,2,3])]=5
>>> d
{(1, 2, 3): 5}


Turple as a key?

Using dictionaries for sparse data structures: Tuple keys

In a similar way, dictionary keys are also commonly leveraged to implement
`sparse` data structures-for example, multidimensional arrays where only a few
positions have values stored in them:

>>> Matrix = {}
>>> Matrix[(2, 3, 4)] = 88
>>> Matrix[(7, 8, 9)] = 99
>>>
>>> X = 2; Y = 3; Z = 4       # separates statements: see Chapter 10
>>> Matrix[(X, Y, Z)]
88
>>> Matrix
{(2, 3, 4): 88, (7, 8, 9): 99}

Here, we’ve used a dictionary to represent a three-dimensional array that is
empty except for the two positions (2,3,4) and (7,8,9). The keys are tuples
that record the coordinates of nonempty slots. 

Rather than allocating a large and mostly empty threedimensional matrix to
hold these values, we can use a simple two-item dictionary. 

Accessing an empty slot triggers a `nonexistent key exception` slots are not
physically stored:

>>> Matrix[(2,3,6)]
Traceback (most recent call last):
File "<stdin>", line 1, in ?
KeyError: (2, 3, 6)


<ex>
783E53043FBA, SYSF40.73.00, Rack 44 Shelf 1
783E53043FA6, SYSF40.73.00, Rack 44 Shelf 2

#!/usr/bin/python
maps = {}

f = open('maclist.txt');
for line in f:
    tokens = line.rstrip().split(',')
    # maps[ tokens[0] ] = { 'version' : tokens[1], 'name' : tokens[2] }
    maps[ tokens[0] ] = tokens[1:]

print maps

if '783E53043FBA' in maps:
    print '783E53043FBA is in maps'

if '783E53043FBB' in maps:
    print '783E53043FBB is in maps'

if '783E53043FA6' in maps:
    print '783E53043FA6 is in maps'

print 'ends'


={============================================================================
|kt_dev_py_0001| py-set

Learning Python 5E, 5, Numeric Types

A set is `iterable` and an `unordered` collection of unique and `immutable`
objects. You can think of them like dicts, but keys only, no values.

A set can be created in two ways:

# use built-in set function
>>> set([2,2,2,1,3,3])
set([1, 2, 3])

a new set literal form, using the curly braces formerly reserved for
dictionaries. In 3.X and 2.7, the following are equivalent

>>> {2,2,2,1,3,3}
set([1, 2, 3])

This syntax makes sense, given that sets are essentially like valueless
dictionaries because a set’s items are unordered, unique, and immutable, the
items behave much like a dictionary’s keys.


sets support mathematical set operations like union, intersection, difference,
     and symmetric difference that we can’t perform on plain sequences like
       strings, lists, and tuples.

>>> a = {1,2,3,4,5}
>>> b = {3,4,5,6,7,8}

# union(or)
>>> a | b
set([1, 2, 3, 4, 5, 6, 7, 8])

# intersection(and)
>>> a & b
set([3, 4, 5])

# difference
>>> a - b
set([1, 2])

# symmetric difference
>>> a ^ b
set([1, 2, 6, 7, 8])


>>> {1,2,3}.issubset(a)
True
>>> a.issuperset({1,2,3})
True
>>> {1,2,3} == {3,2,1}
True

>>> S1 = {1, 2, 3, 4}
>>> S1 > {1, 3} # Superset
True

>>> S = set() # Initialize an empty set
>>> S.add(1.23)
>>> S
{1.23}


<set-immutable-constraints>

>>> S.add((1, 2, 3))
>>> S # No list or dict, but tuple OK
{1.23, (1, 2, 3)}


<set-membership>
sets can only contain immutable (a.k.a. “hashable”) object types as keys in
dict does.

>>> x
set(['a', 'c', 'b', 'e', 'd'])

>>> 'e' in x        # Membership (sets)
True

if you need to store a set inside another set, the frozenset built-in call
works just like set but creates an immutable set that cannot change and thus
can be embedded in other sets.


<set-method>
the set add method inserts one item, update is an in-place union, and remove
deletes an item by value

>>> z
set(['b', 'd'])
>>> z.add('SPAM') # Insert one item
>>> z
set(['b', 'd', 'SPAM'])
>>> z.update(set(['X', 'Y'])) # Merge: in-place union
>>> z
set(['Y', 'X', 'b', 'd', 'SPAM'])
>>> z.remove('b') # Delete one item
>>> z
set(['Y', 'X', 'd', 'SPAM'])


<ser-comprehension> <set-iterable>
As iterable containers, sets can also be used in operations such as len, for
loops, and list comprehensions. Because they are unordered, though, they 
`don’t support sequence operations like indexing and slicing`

>>> for item in set('abc'): print(item * 3)
aaa
ccc
bbb

>>> {x ** 2 for x in [1, 2, 3, 4]} # 3.X/2.7 set comprehension
{16, 1, 4, 9}


<why-set>
# filter duplicates
>>> set([1, 3, 5, 7]) - set([1, 2, 4, 5, 6]) # Find list differences
{3, 7}
>>> set('abcdefg') - set('abdghij') # Find string differences
{'c', 'e', 'f'}
>>> set('spam') - set(['h', 'a', 'm']) # Find differences, mixed
{'p', 's'}
>>> set(dir(bytes)) - set(dir(bytearray)) # In bytes but not bytearray
{'__getnewargs__'}
>>> set(dir(bytearray)) - set(dir(bytes))
{'append', 'copy', '__alloc__', '__imul__', 'remove', 'pop', 'insert', ...more...]

# order-neutral equality
>>> L1, L2 = [1, 3, 5, 2, 4], [2, 5, 3, 4, 1]
>>> L1 == L2 # Order matters in sequences
False
>>> set(L1) == set(L2) # Order-neutral equality
True
>>> sorted(L1) == sorted(L2) # Similar but results ordered
True


={============================================================================
|kt_dev_py_0001| py-reference

Learning Python 5E, 9, Tuples, Files, and Everything Else

Core Types Review and Summary, References Versus Copies

Assignments always store references to objects, not copies of those objects.
In practice, this is usually what you want. Because assignments can generate
multiple references to the same object, though, it's important to be aware
that changing a mutable object in place may affect other references to the
same object elsewhere in your program. If you don't want such behavior, you'll
need to tell Python to copy the object explicitly.

If you really do want copies, however, you can request them:

  Slice expressions with empty limits (L[:]) copy sequences.

  The dictionary, set, and list copy method (X.copy()) copies a dictionary,
  set, or list (the list’s copy is new as of 3.3).

  Some built-in functions, such as list and dict make copies (list(L),
  dict(D), set(S)).

  The copy standard library module makes full copies when needed.

py-top-level-copy

One final note on copies: empty-limit slices and the dictionary copy method
only make `top-level copies`; that is, they do not copy nested data structures,
if any are present. If you need a complete, fully independent copy of a deeply
  nested data structure (like the various record structures we've coded in
      recent chapters), use the standard copy module, introduced in Chapter 6:

import copy
X = copy.copy(Y)          # top-level shallow copy
X = copy.deepcopy(Y)      # Fully copy an arbitrarily nested object Y

This call recursively traverses objects to copy all their parts. This is a
much more rare case, though, which is why you have to say more to use this
scheme. References are usually what you will want; when they are not, slices
and copy methods are usually as much copying as you'll need to do.


={============================================================================
|kt_dev_py_0001| py-if

Learning Python 5E, 12, if Tests and Syntax Rules

Like all `compound statements`, the if statement may contain other statements,
including other ifs.

<python-true>
Remember that 1 is Boolean true (the word `True` is its equivalent), so this
statement’s test always succeeds. To handle a false result, code the else:

>>> if not 1:
... print('true')
... else:
... print('false')
...
false

* All objects have an inherent Boolean true or false value.
* Any nonzero number or `nonempty object` is true.
* Zero numbers, empty objects, and the special object `None` are considered false.
* Comparisons and equality tests return `True` or `False` (custom versions of 1 and 0).
* Boolean and and or operators return a true or false operand object.
* Boolean operators stop evaluating (“short circuit”) as soon as a result is known.


On the other hand, the `and` and `or` operators always `return an object` - either
the object on the left side of the operator or the object on the right. If we
test their results in if or other statements, they will be as expected
(remember, every object is inherently true or false), but we won't get back a
simple True or False.

>>> 2 or 3, 3 or 2    # Return left operand if true
(2, 3)                # Else, return right operand (true or false)
>>> [] or 3
3
>>> [] or {}
{}


Python `and` operations also stop as soon as the result is known; however, in
this case Python evaluates the operands from left to right and stops if the
left operand is a false object because it determines the result—false and
anything is always false:

>>> 2 and 3, 3 and 2  # Return left operand if false
(3, 2)                # Else, return right operand (true or false)
>>> [] and {}
[]
>>> 3 and []
[]


<dictionary-based-switch> dictionary-based multiway branch.
that there is no switch or case statement in Python that selects an action
based on a variable’s value. Instead, you usually code multiway branching as a
series of if/elif tests, as in the prior example, and occasionally by indexing
dictionaries or searching lists. 

Because dictionaries and lists `can be built at runtime dynamically`, they are
sometimes more flexible than hardcoded if logic in your script:

>>> choice = 'ham'
>>> print({'spam': 1.25,  # A dictionary-based 'switch'
...        'ham': 1.99,   # Use has_key or get for default
...        'eggs': 0.99,
...        'bacon': 1.10}[choice])
1.99


Because any expression can be enclosed in parentheses, you can usually use the
`open pairs technique` instead if you need your code to span multiple
lines—simply wrap a part of your statement in parentheses:

if (a == b and c == d and
  d == e and e == f):
  print('new')            # But parentheses usually do too, and are obvious


{ternary-expression}
The if/else Ternary Expression

if X:
  A = Y
else:
  A = Z

At other times, we may want to nest such a construct in a larger statement
instead of assigning its result to a variable. For these reasons, Python 2.5
introduced a new expression format that allows us to say the same thing in one
expression:

A = Y if X else Z

>>> A = 't' if 'spam' else 'f'  # For strings, `nonempty means true`
>>> A
't'
>>> A = 't' if '' else 'f'
>>> A
'f'

<ex>
    print 'Found %d images, ignoring those matching \'%s\'%s' % (
        len(images),
        settings.dropBoxIgnoreRegex,
        '' if stb_regex == '.*' else ' filtering on \'' + stb_regex + '\'')


={============================================================================
|kt_dev_py_0001| py-iter py-comprehension

Learning Python 5E, 14, Iterations and Comprehensions

Python’s two looping statements, while and for can handle most repetitive
tasks programs need to perform, the need to iterate over sequences is so
common and pervasive that Python provides additional tools to make it simpler
and more efficient.

<iteration-protocol>
files also have a method named __next__ in 3.X (and next in 2.X) that has a
nearly identical effect as readline(); it returns the next line from a file
each time it is called.

>>> f = open('script2.py')  # __next__ loads one line on each call too
>>> f.__next__()            # But raises an exception at end-of-file
'import sys\n'

...

>>> f.__next__()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
`StopIteration`


This interface is most of what we call the iteration protocol in Python. Any
object with a __next__ method to advance to a next result, which raises
StopIteration at the end of the series of results, is considered an iterator
in Python.

For example, the best way to read a text file line by line today is to not
read it at allinstead, allow the for loop to automatically call __next__ to
advance to the next line on each iteration. The file object’s iterator will do
the work of automatically loading lines as you go.

>>> for line in open('script2.py'): # Use file iterators to read by lines
...   print(line.upper(), end='')     # Calls __next__, catches StopIteration

Notice that the print uses end='' here to suppress adding a \n, because line
strings already have one (without this, our output would be double-spaced;
in 2.X, a trailing comma works the same as the end).

>>> for line in f:
...   print line,

fast. iterators run at C language speed inside Python, whereas the while loop
  version runs Python byte code through the Python virtual machine.


<manual-iteration>
To simplify manual iteration code, Python 3.X also provides a built-in
function, next, that automatically calls an object’s __next__ method. Per the
preceding note, this call also is `supported on Python 2.X` for portability.
Given an iterator object X, the call next(X) is the same as X.__next__() on
3.X (and X.next() on 2.X), but is noticeably simpler and more version-neutral.

>>> f = open('script2.py')
>>> f.__next__()      # Call iteration method directly
'import sys\n'
>>> f.__next__()
'print(sys.path)\n'


>>> f = open('script2.py')
>>> next(f)           # The next(f) built-in calls f.__next__() in 3.X
'import sys\n'
>>> next(f)           # => [3.X: f.__next__()], [2.X: f.next()]
'print(sys.path)\n'


<full-iteration-protocol>
Use two objects:

  * The iterable object you request iteration for, whose __iter__ is run by
    iter

  * The `iterator object` returned by `the iterable` that actually produces values
    during the iteration, whose __next__ is run by next and raises
    StopIteration when finished producing results

>>> L = [1, 2, 3]
>>> I = iter(L)     # Obtain an `iterator object` from an `iterable`
>>> I.__next__()    # Call iterator's next to advance to next item
1


This initial step is not required for files, because a file object has its own
iterator. Because they support just one iteration (they can’t seek backward to
    support multiple active scans)

Lists and many other built-in objects, though, are not their own iterators
because they do support multiple open iterationsfor example, there may be
multiple iterations in nested loops all at different positions. For such
objects, we must call iter to start iterating:


<list-comprehension>
Now that we’ve seen how the iteration protocol works, let’s turn to one of its
most common use cases. Together with for loops, list comprehensions are one of
the most prominent contexts in which the iteration protocol is applied.

List comprehensions derive from set notation; they are a way to build a new
  list by `running an expression on each item in a sequence`, one at a time,
       from left to right. 

coded in square brackets (to tip you off to the fact that they make a list)
and are composed of an `expression` and a `looping construct` that share a
variable name (row, here).

`List comprehensions` are one of the most-loved Python language features. They
allow you to concisely form a new list `by filtering` the elements of a
collection and transforming the elements passing the filter in one conscise
expression. They take the basic form:

[`expr` for val in collection if `condition`]

This is equivalent to the following for loop:

result = []
for val in collection:
  if condition:
    result.append(expr)

note:
The list comprehension isn’t exactly the same as the for loop statement
version because it makes a new list object

note: as with iter above
Moreover, depending on your Python and code, list comprehensions might run
much faster than manual for loop statements (often roughly twice as fast)
because their iterations are performed at C language speed inside the
interpreter, rather than with manual Python code.


The `filter condition` can be `omitted`, leaving only the expression. For
example, we could filter out strings with length 2 or less and also convert
them to uppercase like this:

>>> strings=['a', 'as', 'bat', 'car', 'dove', 'python']
>>> [x.upper() for x in strings if len(x) > 2]
['BAT', 'CAR', 'DOVE', 'PYTHON']


<nested-comprehension>
>>> [x + y for x in 'abc' for y in 'lmn']
['al', 'am', 'an', 'bl', 'bm', 'bn', 'cl', 'cm', 'cn']

>>> res = []
>>> for x in 'abc':
...   for y in 'lmn':
...     res.append(x + y)
...


<ex>
>>> M
[[1, 2, 3], [4, 5, 6], [7, 8, 9]]

>>> col2 = [row[1] for row in M]      # Collect the items in column 2
>>> col2
[2, 5, 8]
>>> M                                 # The matrix is unchanged
[[1, 2, 3], [4, 5, 6], [7, 8, 9]]


<ex>
>>> s = '783E53043FBA, SYSF40.73.00, Rack 44 Shelf 1'
>>> s.split(',')
['783E53043FBA', ' SYSF40.73.00', ' Rack 44 Shelf 1']

f = open('maclist.txt');
for line in f:
    tokens = line.rstrip().split(',')
    maps[ tokens[0] ] = [ tokens[1].strip(), tokens[2].strip() ]

    To:
    maps[ tokens[0] ] = [ x.strip() for x in tokens[1:] ]

note:
can use dict comprehension?


<any-iterable-object>
Can be used to iterate over any iterable object

>>> diag = [M[i][i] for i in [0, 1, 2]]   # Collect a diagonal from matrix
>>> diag
[1, 5, 9]

>>> doubles = [c * 2 for c in 'spam']     # Repeat characters in a string
>>> doubles
['ss', 'pp', 'aa', 'mm']

>>> [[x ** 2, x ** 3] for x in range(4)]  # Multiple values, "if" filters
[[0, 0], [1, 1], [4, 8], [9, 27]]

>>> [[x, x / 2, x * 2] for x in range(−6, 7, 2) if x > 0]
[[2, 1, 4], [4, 2, 8], [6, 3, 12]]


<other-iteration-context>
any tool that employs the iteration protocol will automatically work on any
built-in type or user-defined class that provides it.

  `sorted` sorts items in an iterable; `zip` combines items from iterables;
`enumerate` pairs items in an iterable with relative positions; `filter`
  selects items for which a function is true; and `reduce` runs pairs of items
  in an iterable through a function.


# <py-map>
Use functions as arguments to other functions like the built-in `map()`
function, which applies a function to a collection of some kind:

>>> uppers = [line.upper() for line in open('script2.py')]
>>> uppers
['IMPORT SYS\n', 'PRINT(SYS.PATH)\n', 'X = 2\n', 'PRINT(X ** 32)\n']

>>> map(str.upper, open('script2.py')) # map is itself an iterable in 3.X
<map object at 0x00000000029476D8>
>>> list(map(str.upper, open('script2.py')))
['IMPORT SYS\n', 'PRINT(SYS.PATH)\n', 'X = 2\n', 'PRINT(X ** 32)\n']

# <py-enumerate>
When iterating over a sequence to want to keep track of the index of the
current item. Since this is so common, Python has a built-in function
`enumerate` which returns a sequence of (i, value) tuples:

for i, value in enumerate(collection):
  # do something with value


Useful `pattern` that uses enumerate is computing a dict mapping the values of
a sequence (which are assumed to be unique) to their locations in the
sequence:

>>> l
['one', 'two', 'three']
>>> mapping = dict((v,i) for i,v in enumerate(l))
>>> mapping
{'three': 2, 'two': 1, 'one': 0}


# <py-sorted>
The sorted function returns a new sorted list from the elements of any
sequence. A common `pattern` for getting a sorted list of the `unique
elements` in a sequence is to combine sorted with set.

>>> [7,1,2,6,0,3,2,3,2]
[7, 1, 2, 6, 0, 3, 2, 3, 2]
>>> sorted([7,1,2,6,0,3,2,3,2])
[0, 1, 2, 2, 2, 3, 3, 6, 7]
>>> sorted(set([7,1,2,6,0,3,2,3,2]))
[0, 1, 2, 3, 6, 7]

note: see *list-sort* which is in-place sort


# <py-zip>
zip `pairs up` the elements of a number of lists, tuples, or other sequences,
    to create a list of tuples:

>>> seq1=['foo','bar','baz']
>>> seq2=['one','two','three']
>>> zip(seq1,seq2)
[('foo', 'one'), ('bar', 'two'), ('baz', 'three')]


A very common use of zip is for simultaneously `iterating over multiple`
sequences, possibly also combined with enumerate:

>>> for i, (a,b) in enumerate(zip(seq1,seq2)):
...     print('%d: %s, %s' % (i,a,b))
... 
0: foo, one
1: bar, two
2: baz, three


Given a zipped sequence, zip can be applied in a clever way to `unzip` the
sequence.  Another way to think about this is converting a list of rows into a
list of columns. The syntax, which looks a bit magical, is:

>>> zipped=zip(seq1, seq2)
>>> zipped
[('foo', 'one'), ('bar', 'two'), ('baz', 'three')]
>>> names, numbers = zip(*zipped)
>>> names
('foo', 'bar', 'baz')
>>> numbers
('one', 'two', 'three')


# <pyreversed>
`reversed` iterates over the elements of a sequence in reverse order:

>>> range(10)
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> reversed(range(10))
<listreverseiterator object at 0xb753a8ec>
>>> list(reversed(range(10)))
[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]


<more-and-more>
the iteration protocol is even more pervasive in Python today

>>> list(open('script2.py'))
['import sys\n', 'print(sys.path)\n', 'x = 2\n', 'print(x ** 32)\n']

>>> tuple(open('script2.py'))
('import sys\n', 'print(sys.path)\n', 'x = 2\n', 'print(x ** 32)\n')

>>> '&&'.join(open('script2.py'))
'import sys\n&&print(sys.path)\n&&x = 2\n&&print(x ** 32)\n'


Even some tools you might not expect fall into this category: sequence
assignment, the in membership test, slice assignment, and the list’s extend
method also leverage the iteration protocol to scan, and thus read a file by
lines automatically:

>>> a, b, c, d = open('script2.py') # Sequence assignment
>>> a, d
('import sys\n', 'print(x ** 32)\n')

>>> a, *b = open('script2.py')      # 3.X extended form
>>> a, b
('import sys\n', ['print(sys.path)\n', 'x = 2\n', 'print(x ** 32)\n'])

>>> 'y = 2\n' in open('script2.py') # Membership test
False
>>> 'x = 2\n' in open('script2.py')
True

>>> L = [11, 22, 33, 44]            # Slice assignment
>>> L[1:3] = open('script2.py')
>>> L
[11, 'import sys\n', 'print(sys.path)\n', 'x = 2\n', 'print(x ** 32)\n', 44]

>>> L = [11]
>>> L.extend(open('script2.py'))    # list.extend method


<dict-comporehension>
In Python 2.7 and 3.X, comprehension syntax can also be used to create `sets`
  and `dict`:

A `dict comprehension` looks like this:

dict_comp = { `key-expr:value-expr` for value in collection if `condition`}


A `set comprehension` looks like list comprehension except with curly braces
  instead of square brackets:

set_comp = {expr for value in collection if condition}


# scan the file line by line and pick out lines that begin with the letter p

>>> {line for line in open('script2.py') if line[0] == 'p'}
{'print(x ** 32)\n', 'print(sys.path)\n'}


>>> unique_lengths = {len(x) for x in strings}
>>> unique_lengths
set([1, 2, 3, 4, 6])

Could create a lookup map of these strings to their locations in the list:

>>> loc_mapping = {val:index for index, val in enumerate(strings)}
>>> loc_mapping
{'a': 0, 'bat': 2, 'python': 5, 'car': 3, 'as': 1, 'dove': 4}

Note that this dict could be equivalently constructed by:

>>> loc_mapping2 = dict((val,index) for index, val in enumerate(strings))
>>> loc_mapping2
{'a': 0, 'bat': 2, 'python': 5, 'car': 3, 'as': 1, 'dove': 4}


={============================================================================
|kt_dev_py_0001| py-module

Learning Python 5E, 22, Modules: The Big Picture

Python module—the highest-level program organization unit, which packages
program code and data for reuse, and provides self contained `namespaces` that
minimize variable name clashes across your programs.

All the names defined at the top level of a module file become `attributes` of
the imported module object. As we saw in the last part of this book, imports
give access to names `in a module's global scope`

The `top-level` (a.k.a. script) file contains the main flow of control of your
program—this is the file you run to launch your application. The module files
are libraries of tools used to collect components used by the top-level file.


How Imports Work

The `imports` are really runtime operations that perform three distinct steps
the first time a program imports a given file:

1. Find the module's file.

Python uses a standard `module search path` and `known file types` to locate the
module file corresponding to an import statement.

2. Compile it to byte code (if needed).

During an import operation Python checks both file modification times and the
byte code's Python version number to decide how to proceed.

In Python 3.2 and later, byte code files are segregated in a __pycache__
subdirectory and named with their Python version to avoid contention and
recompiles when multiple Pythons are installed.

<py-byte-code> ship byte code only
In addition, if Python finds only a byte code file on the search path and no
source, it simply loads the byte code directly; this means you can ship a
program as just byte code files and avoid sending source. 

In other words, the compile step is bypassed if possible to speed program
startup.

Notice that `compilation happens when a file is being imported.`

note: after all, to speed up.
If Python cannot write a file to save this on your computer for any reason,
your program still runs fine—Python simply creates and uses the byte code in
  memory and discards it on exit. To speed startups, though, it will try to
  save byte code in a file in order to skip the compile step next time around.

3. Run the module's code to build the objects it defines.

This last import step actually runs the file's code.


<py-search-path>
Python look:

1. The home directory of the program (automatic)
2. PYTHONPATH directories (if set)
3. Standard library directories (automatic)
4. The contents of any .pth files (if present)
5. The site-packages home of third-party extensions

<py-sys-path>
The concatenation of these four components `becomes sys.path`, a mutable list
of directory name strings. You can always inspect the path as Python knows it
by printing the built-in sys.path. The empty string at the front means current
directory

>>> import sys
>>> sys.path
['', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-linux2', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/pymodules/python2.7']

<ex>

/home/pi/snugupdate-v2-snugberrypi/run.sh

  cd /home/pi/snugupdate-v2-snugberrypi/snugupdate-v2-python
  ../update_log.py

    # note: 
    # sys.path.append('/snugupdate-v2-python/') do not work
    # sys.path.append('snugupdate-v2-python') no different and shows the same problem
    sys.path.append('snugupdate-v2-python/')
    from libraries.settings import dropboxToken

    # >>> sys.path.append('snugupdate-v2-python/')
    # >>> sys.path
    # ['', 'snugupdate-v2-python/']

cause *import-error* 

pi@raspberrypi ~/snugupdate-v2-snugberrypi/snugupdate-v2-python $ ../upload_log.py ../www/log
pi@raspberrypi ~/snugupdate-v2-snugberrypi/snugupdate-v2-python $ python ../upload_log.py ../www/log
cause *import-error* 

pi@raspberrypi ~/snugupdate-v2-snugberrypi $ ./upload_log.py www/log
okay

these fix the problem:
export PYTHONPATH=/home/pi/snugupdate-v2-snugberrypi/snugupdate-v2-python:$PYTHONPATH
sys.path.append('/home/pi/snugupdate-v2-snugberrypi/snugupdate-v2-python')

WHY?

https://docs.python.org/3/tutorial/modules.html
6.1.2. The Module Search Path

When a module named spam is imported, the interpreter first searches for a
built-in module with that name. If not found, it then searches for a file
named spam.py in a list of directories given by the variable sys.path.
sys.path is initialized from these locations:

1. The directory containing the input script (or the current directory when no
    file is specified).

2. PYTHONPATH (a list of directory names, with the same syntax as the shell
    variable PATH).

3. The installation-dependent default.

https://docs.python.org/3/library/sys.html#sys.path

So use relative path in the list from the current directory where a script
gets run? However, libraries.settings is under snugupdate-v2-python and when
run interactive python, this works:

    from libraries.settings import dropboxToken

https://stackoverflow.com/questions/7505988/importing-from-a-relative-path-in-python
From PEP8:
Relative imports for intra-package imports are highly discouraged.

https://www.python.org/dev/peps/pep-0008/
Absolute imports are recommended, as they are usually more readable and tend
to be better behaved (or at least give better error messages) if the import
system is incorrectly configured (such as when a directory inside a package
    ends up on sys.path):


Learning Python 5E, 23, Module Coding Basics

Module Filenames

Because module names become variable names inside a Python program, they
should also follow the `normal variable name rules`

The chief difference is that import fetches the module as a whole, so you must
qualify to fetch its names; in contrast, from fetches (or copies) specific
names out of the module.

The import Statement identifies an `external file to be loaded`, and it becomes
a variable in the script. Because it gives a name that refers to the whole
module object, we must go through the module name to fetch its attributes
(e.g., module1.printer).


Imports Happen Only Once

Modules are loaded and run on the first import or from, and only the first.


`import` and from Are Assignments

Just like def, import and from are executable statements, not compile-time
declarations. They may be nested in if tests, to select among options; appear
in function defs, to be loaded only on calls (subject to the preceding note);
be used in try statements, to provide defaults; and so on. They are not
  resolved or run until Python reaches them while executing your program. In
  other words, imported modules and names are not available until their
  associated import or from statements run.


<import-assignment>
Also, like `def`, the `import` and `from` are `implicit assignments`:

  * `import` assigns an entire module object to a single name.
  * `from` assigns one or more names to objects of the same names in another
    module.

As with function arguments, reassigning a copied name has no effect on the
module from which it was copied, but changing a `shared mutable object`
through a copied name can also change it in the module from which it was
imported. see *function-argument* *shared-reference*

<ex>
# small.py
x = 1
y = [1, 2]

% python
>>> from small import x, y      # Copy two names out
>>> x = 42                      # Changes local x only
>>> y[0] = 42                   # Changes shared mutable in place

Here, x is not a shared mutable object, but y is, so changing it from one
place changes it in the other:

>>> import small                # Get module name (from doesn't)
>>> small.x                     # Small's x is not my x
1
>>> small.y                     # But we share a changed mutable
[42, 2]


There is no link from a name copied with `from` back to the file it came from.
  To really change a global name in another file, you must use import:

% python
>>> from small import x, y      # Copy two names out
>>> x = 42                      # Changes my x only
>>> import small                # Get module name
>>> small.x = 42                # Changes x in other module

Note that the change to y[0] in the prior session is different; it changes an
object, not a name, and the name in both modules references the same, changed
object.


import and from Equivalence

At least conceptually, a from statement like this one:

from module import name1, name2 # Copy these two names out (only)

is equivalent to this statement sequence:

import module                   # Fetch the module object
name1 = module.name1            # Copy names out by assignment
name2 = module.name2
del module                      # Get rid of the module name

Like all assignments, the from statement creates new variables in the
importer, which initially refer to objects of the same names in the imported
file. Only the names are copied out, though, `not the objects they reference`,
  and not the name of the module itself.


Potential Pitfalls of the from Statement

Some Python users recommend using import instead of from most of the time. 

It is true that the from statement has the potential to `corrupt namespaces`,
at least in principle:

if you use it to import variables that happen to have the same names as
  existing variables in your scope, your variables will be silently
    overwritten. This problem doesn’t occur with the simple import statement
    because you must always go through a module’s name to get to its contents


={============================================================================
|kt_dev_py_0001| py-module-namespace

Technically, modules usually correspond to files, and Python creates a module
object to contain all the names assigned in a module file. But in simple
terms, modules are just `namespaces` (places where names are created), and the
names that live in a module are called its `attributes`.

Files Generate Namespaces

The short answer is that every name that is assigned a value at the top level
of a module file (i.e., not nested in a function or class body) becomes an
attribute of that module.

* Module statements run on the first import. The first time a module is
  imported anywhere in a system, Python creates an empty module object and
  executes the statements in the module file one after another, from the top
  of the file to the bottom.

* Top-level assignments create module attributes. During an import, statements
  at the top level of the file not nested in a def or class that assign names
  (e.g., =, def) create attributes of the module object; assigned names are
  stored in the module’s namespace.

* Module namespaces can be accessed via the attribute __dict__ or dir(M).
  Module namespaces created by imports are dictionaries; they may be accessed
  through the built-in __dict__ attribute associated with module objects and
  may be inspected with the dir function. The dir function is roughly
  equivalent to the sorted keys list of an object’s __dict__ attribute, but it
  includes inherited names for classes, may not be complete, and is prone to
  changing from release to release.


<ex>
# module2.py
print('starting to load...')

import sys
name = 42

def func(): pass
class klass: pass

print('done loading.')

>>> import module2
starting to load...
done loading...

>>> module2.__dict__.keys()
['name', '__builtins__', '__file__', '__package__', 'sys', 'klass', 'func', '__name__', '__doc__']

>>> module2.__dict__['__file__']
'module2.py'

Namespace Dictionaries: __dict__

module namespaces are stored as dictionary objects.

>>> list(module2.__dict__.keys())
['__loader__', 'func', 'klass', '__builtins__', '__doc__', '__file__', '__name__',
'name', '__package__', 'sys', '__initializing__', '__cached__']

Python also adds some names in the module’s namespace for us; for instance,
__file__ gives the name of the file the module was loaded from, and __name__
  gives its name as known to importers (without the .py extension and
      directory path).

>>> list(name for name in module2.__dict__.keys() if not name.startswith('__'))
['func', 'klass', 'name', 'sys']
>>> list(name for name in module2.__dict__ if not name.startswith('__'))
['func', 'sys', 'name', 'klass']

>>> module2.name, module2.__dict__['name']
(42, 42)


Imports Versus Scopes

Scopes are never influenced by function calls or module imports.

# moda.py

X = 88      # My X: global to this file only
def f():
  global X  # Change this file's X
  X = 99    # Cannot see names in other modules

# modb.py

X = 11      # My X: global to this file only
import moda # Gain access to names in moda
moda.f()    # Sets moda.X, not this file's X
print(X, moda.X)

% python modb.py
11 99


Namespace Nesting

imports do nest downward and it is possible to descend into arbitrarily nested
modules and access their attributes.

# mod3.py

X = 3

# mod2.py

X = 2
import mod3
print(X, end=' ')   # My global X
print(mod3.X)       # mod3's X

# mod1.py

X = 1
import mod2
print(X, end=' ')       # My global X
print(mod2.X, end=' ')  # mod2's X
print(mod2.mod3.X)      # note: Nested mod3's X

% python mod1.py
2 3
1 2 3

The reverse, however, is not true: mod3 cannot see names in mod2, and mod2
cannot see names in mod1.


={============================================================================
|kt_dev_py_0001| py-module-package

Learning Python 5E, 24, Module Packages

In addition to a module name, an import can name a directory path. A directory
of Python code is said to be a `package`, so such imports are known as package
imports. In effect, a package import turns a directory on your computer into
another Python namespace, with attributes corresponding to the subdirectories
and module files that the directory contains.

Instead list a path of names separated by periods:

import dir1.dir2.mod

The same goes for from statements:

from dir1.dir2.mod import x

Furthermore, these imports imply that dir1 resides within some container
directory dir0, which is a component of the normal Python module search path.

More formally, the leftmost component in a package import path is still
relative to a directory included in the sys.path module search path list
see *py-search-path*

In effect, entries on the module search path provide platform-specific
directory path prefixes, which lead to the leftmost names in import and from
statements. These import statements themselves provide the remainder of the
directory path in a platform-neutral fashion.


Package __init__.py Files

Each directory named within the path of a package import statement `must`
contain a file named __init__.py, or your package imports will fail.

<py-import-error> this is a general import error when cannot find it. 

Traceback (most recent call last):
  File "update.py", line 2, in <module>
    from libraries.stb import Devices              # Keeps track of STBs on the network
ImportError: No module named libraries.stb


for a directory structure such as this:
dir0\dir1\dir2\mod.py

and an import statement of the form:
import dir1.dir2.mod

the following rules apply:

* dir1 and dir2 both must contain an __init__.py file.
 
* dir0, the container, does not require an __init__.py file; this file will
  simply be ignored if present.

* dir0, not dir0\dir1, must be listed on the module search path sys.path.
 
The __init__.py files `can contain` Python code, just like normal module files.
Their names are special because their code is run automatically the first time
a Python program imports a directory, and thus serves primarily as a hook for
performing initialization steps required by the package. These files can also
be completely empty.

dir0\                       # Container on module search path
  dir1\
      __init__.py
      dir2\
          __init__.py
          mod.py

Once imported, the path in your import statement becomes a nested object path
in your script. Here, mod is an object nested in the object dir2, which in
turn is nested in the object dir1:

>>> dir1
<module 'dir1' from '.\\dir1\\__init__.py'>
>>> dir1.dir2
<module 'dir1.dir2' from '.\\dir1\\dir2\\__init__.py'>
>>> dir1.dir2.mod
<module 'dir1.dir2.mod' from '.\\dir1\\dir2\\mod.py'>



Why Use Package Imports?

They do serve useful roles, though, especially in larger programs: they make
imports more informative, serve as an organizational tool, simplify your
module search path, and can resolve ambiguities.


Package Relative Imports

note: TODO for Python 3.X

<ex>
from snugupdate-v2-python.libraries.settings import dropboxToken

  File "upload_log.py", line 9
    from snugupdate-v2-python.libraries.settings import dropboxToken
                   ^
SyntaxError: invalid syntax

WHY? 

https://docs.python.org/2/reference/lexical_analysis.html
2.3. Identifiers and keywords
Identifiers (also referred to as names) are described by the following lexical definitions:

identifier ::=  (letter|”_”) (letter | digit | “_”)*
letter     ::=  lowercase | uppercase
lowercase  ::=  “a”…”z”
uppercase  ::=  “A”…”Z”
digit      ::=  “0”…”9”

Identifiers are unlimited in length. Case is significant.

https://docs.python.org/2/reference/simple_stmts.html#import
6.12. The import statement

import_stmt     ::=  “import” module [“as” name] ( “,” module [“as” name] )*
                     | “from” relative_module “import” identifier [“as” name]
                     ( “,” identifier [“as” name] )*
                     | “from” relative_module “import” “(” identifier [“as” name]
                     ( “,” identifier [“as” name] )* [“,”] “)”
                     | “from” module “import” “*”

module          ::=  (identifier “.”)* identifier

So module cannot have '-' and to get it around:

sys.path.append('snugupdate-v2-python/')
from libraries import settings


={============================================================================
|kt_dev_py_0001| py-module-advanced

Learning Python 5E, 25, Advanced Module Topics

Data Hiding in Modules

There is no notion of declaring which names should and shouldn't be visible
outside the module. In fact, there's no way to prevent a client from changing
names inside a module if it wants to.  

In Python, data hiding in modules is a convention, not a syntactical
constraint.


Minimizing from * Damage: _X and __all__

a single underscore (e.g., _X) to prevent them from being copied out when a
client imports a module's names with a `from *` statement to minimise
namespace pollution.

Underscores aren't "private" declarations: you can still see and change such
names with other import forms, such as the `import` statement:

# unders.py
a, _b, c, _d = 1, 2, 3, 4
>>> from unders import * # Load non _X names only
>>> a, c
(1, 3)
>>> _b
NameError: name '_b' is not defined
>>> import unders # But other importers get every name
>>> unders._b
2


Alternatively, you can achieve a hiding effect similar to the _X naming
convention by assigning a list of variable name strings to the variable
__all__ at the top level of the module.

When this feature is used, the from * statement will copy out only those names
listed in the __all__ list. In effect, this is the converse of the _X
convention: __all__ identifies names to be copied, while _X identifies names
not to be copied.

Python looks for an __all__ list in the module first and copies its names
irrespective of any underscores; if __all__ is not defined, from * copies all
names without a single leading underscore:

# alls.py
__all__ = ['a', '_c']                 # __all__ has precedence over _X
a, b, _c, _d = 1, 2, 3, 4

>>> from alls import *                # Load __all__ names only
>>> a, _c
(1, 3)
>>> b
NameError: name 'b' is not defined

>>> from alls import a, b, _c, _d     # But other importers get every name
>>> a, b, _c, _d
(1, 2, 3, 4)

>>> import alls
>>> alls.a, alls.b, alls._c, alls._d
(1, 2, 3, 4)


<py-future>
Enabling Future Language Features: __future__

Changes to the language that may potentially break existing code are usually
introduced gradually in Python. They often initially appear as optional
extensions, which are disabled by default. To turn on such extensions, use a
special import statement of this form:

from __future__ import featurename

When used in a script, this statement must appear as the first executable
statement in the file (possibly following a docstring or comment), because it
enables special compilation of code on a per-module basis. It’s also possible
to submit this statement at the interactive prompt to experiment with upcoming
language changes; the feature will then be available for the remainder of the
interactive session.


The as Extension for import and from

allow an imported name to be given a different name in your script.

import modulename as name                 # And use name, not modulename

This works in a from statement, too

from modulename import attrname as name   # And use name, not attrname


Example: Modules Are Objects <introspection>

the module's attribute dictionary, exposed in the built-in __dict__ attribute
we met in Chapter 23. Python also exports the list of all loaded modules as
the sys.modules dictionary and provides a built-in called getattr that lets us
fetch attributes from their string names

all the following expressions reach the same attribute and object:

M.name                  # Qualify object by attribute
M.__dict__['name']      # Index namespace dictionary manually
sys.modules['M'].name   # Index loaded-modules table manually
getattr(M, 'name')      # Call built-in fetch function


Statement Order Matters in Top-Level Code

Python executes its statements one by one, from the top of the file to the
bottom.

* Code at the top level of a module file (not nested in a function) runs as
soon as Python reaches it during an import; because of that, it cannot
reference names assigned lower in the file.

* Code inside a function body `doesn’t run until the function is called`;
because names in a function aren’t resolved until the function actually runs,
        they can usually reference names anywhere in the file.

As a rule of thumb, if you need to mix immediate code with defs, put your defs
at the top of the file and your top-level code at the bottom. That way, your
functions are guaranteed to be defined and assigned by the time Python runs
the code that uses them.


from Copies Names but Doesn’t Link

the from statement is really an assignment to names in the importer's scope—a
`name-copy operation`, not a name aliasing. The implications of this are the
same as for all assignments in Python see *shared-reference*

# nested1.py
X = 99
def printer(): print(X)

# nested2.py
from nested1 import X, printer    # Copy names out
X = 88                            # Changes my "X" only which is local version
printer()                         # nested1's X is still 99

% python nested2.py
99

If we use import to get the whole module and then assign to a qualified name,
however, we change the name in nested1.py.

# nested3.py
import nested1                  # Get module as a whole
nested1.X = 88                  # OK: change nested1's X
nested1.printer()

% python nested3.py
88


={============================================================================
|kt_dev_py_0001| py-module-main-name

Mixed Usage Modes: __name__ and __main__

each module has a built-in attribute called __name__, which Python creates and
assigns automatically as follows:

* If the file is being run as a top-level program file, __name__ is set to the
  string "__main__" when it starts.

* If the file is being imported instead, __name__ is set to the module’s name
  as known by its clients.

The upshot is that a module can test its own __name__ to determine 
`whether it's being run or imported.`

In effect, a module’s __name__ variable serves as a `usage mode flag`,
allowing its code to be leveraged as both an importable library and a
  top-level script.

Coding self-test code at the bottom of a file under the __name__ test is
probably the most common and simplest unit-testing protocol in Python.

In addition, the __name__ trick is also commonly used when you’re writing
files that can be used both as command-line utilities and as tool libraries.


<reference>
29.4. __main__ — Top-level script environment

'__main__' is the name of the `scope` in which top-level code executes. A
module's __name__ is set equal to '__main__' when read from standard input, a
script, or from an interactive prompt.

A module can discover whether or not it is running in the `main-scope` by
checking its own __name__, which allows a common idiom for conditionally
executing code in a module when it is run `as-a-script` or with python -m but
not when it is imported:

if __name__ == "__main__":
    # execute only if run as a script
    main()

For a package, the same effect can be achieved by including a __main__.py
module, the contents of which will be executed when the module is run with -m.

<reference>
Python Scripts as a Replacement for Bash Utility Scripts

http://www.linuxjournal.com/content/python-scripts-replacement-bash-utility-scripts?page=0,0

Pros:

Python is a fully featured programming language. Code reuse is simple, because
Python modules easily can be imported and used in any Python script. Scripts
easily can be extended or built upon.

Python has access to an excellent standard library and thousands of third-party
libraries for all sorts of advanced utilities, such as parsers and request
libraries. For instance, Python's standard library includes datetime libraries
that allow you to parse dates into any format that you specify and compare it to
other dates easily. 

There are a lot of aspects to Python in the shell that go beyond the scope of
  this article, such as the os module and the subprocess module. The os module
  is a standard library function that holds a lot of key operating system-level
  operations, such as listing directories and stating files, along with an
  excellent submodule os.path that deals with normalizing directories paths. The
  subprocess module allows Python programs to run system commands and other
  advanced operations, such as handling piping as described above within Python
  code between spawned processes. Both of these libraries are worth checking out
  if you intend to do any Python shell scripting. 


<ex>
#!/usr/bin/env python
import sys

if __name__ == "__main__":

    # initialize a names dictionary as empty to start with.
    # each key in this dictionary will be a name and the value will be
    # the number of times that names appears. name-value-pair
    names = {}

    # sys.stdin is a file object. all the same functions that can be
    # applied to a file object can be applied to sys.stdin. 
    for name in sys.stdin.readlines():

        # each line will have a newline on the end that should be
        # removed.
        name = name.strip()

        if name in names:
            names[name] += 1
        else:
            names[name] = 1

    # iterating over the dictionary. print name followed by a space and 
    # the number of times it appeared.
    for name, count in names.iteritems():
        sys.stdout.write("%d\t%s\n" % (count, name))


$ cat names.log | python namescount.py


={============================================================================
|kt_dev_py_0001| py-function

Learning Python 5E, 16, Function Basics

Functions are declared using the `def` keyword and returned from using the
`return` keyword:

def my_function(x, y, z=1.5):
  if z > 1:
    return z * (x + y)
  else:
    return z / (x + y)

If the end of a function is reached without encountering a return statement,
    `None` is returned.


{functions-are-objects}
The `def` header line specifies a function name that is assigned the function
object, along with a list of zero or more arguments in parentheses.

Since Python functions are objects, many constructs can be easily expressed
that are difficult to do in other languages.

Suppose we were doing some data cleaning and needed to apply a bunch of
transformations to the following list of strings:

>>> states = [' Alabama ', 'Georgia!', 'Georgia', 'georgia', 'FlOrIda', \
  'south carolina##', 'West virginia?']

>>> states
[' Alabama ', 'Georgia!', 'Georgia', 'georgia', 'FlOrIda', 'south carolina##',\
    'West virginia?']

>>> import re
>>> def clean_strings(strings):
...     result=[]
...     for value in strings:
                # these are `str` methods
...             value = value.strip()
...             value = re.sub('[!#?]', '', value)
...             value = value.title()
...             result.append(value)
...     return result
... 

# see address
>>> clean_strings
<function clean_strings at 0xb7509b1c>

>>> clean_strings(states)
['Alabama', 'Georgia', 'Georgia', 'Georgia', 'Florida', 'South Carolina', 'West Virginia']


An alternate approach that you may find useful is to make a list of the
operations you want to apply to a particular set of strings:

>>> def remove_punctuation(value):
...     return re.sub('!?#', '', value)     # error in regex
... 
>>> clean_ops=[str.strip, `remove_punctuation`, str.title]

# see addresses
>>> clean_ops
[<method 'strip' of 'str' objects>, <function remove_punctuation at 0xb750979c>, 
  <method 'title' of 'str' objects>]

>>> def clean_strings_ops(strings, ops):
...     result=[]
...     for value in strings:
...             for function in ops:
...                     value = function(value)
...             result.append(value)
...     return result
... 

>>> states
[' Alabama ', 'Georgia!', 'Georgia', 'georgia', 'FlOrIda', 'south carolina##', 
  'West virginia?']

# WTF? why see punctuations? since have regex error in remove_punctuation function.
>>> clean_strings_ops(states, clean_ops)
['Alabama', 'Georgia!', 'Georgia', 'Georgia', 'Florida', 'South Carolina', 'West Virginia?']

# ok. fix that error and going to work if define function again?
>>> def remove_punctuation(value):
...     return re.sub('[!?#]', '', value)
... 

# still see !? and not working?
>>> clean_strings_ops(states, clean_ops)
['Alabama', 'Georgia!', 'Georgia', 'Georgia', 'Florida', 'South Carolina', 'West Virginia?']

# define clean_ops again and see different address for remove_punctuation
>>> clean_ops
[<method 'strip' of 'str' objects>, <function remove_punctuation at `0xb750979c`>, 
  <method 'title' of 'str' objects>]

>>> clean_ops=[str.strip, remove_punctuation, str.title]
>>> clean_ops
[<method 'strip' of 'str' objects>, <function remove_punctuation at `0xb7509e9c`>, 
  <method 'title' of 'str' objects>]

>>> clean_strings_ops(states, clean_ops)
['Alabama', 'Georgia', 'Georgia', 'Georgia', 'Florida', 'South Carolina', 'West Virginia']


note: same name but different function object?

>>> def clean_strings(strings):
...     print strings
...     return strings
...
>>> clean_strings
<function clean_strings at 0x7f70bba555f0>
>>>
>>> def clean_strings(strings):
...     print strings
...     return strings
...
>>> clean_strings
<function clean_strings at 0x7f70bba55668>
>>>
>>> clean_strings
<function clean_strings at 0x7f70bba55668>


So defining a function with the same name creates a new function object? Yes
but there is only one function exist. In the above example, clean_ops
reference old function and it makes two function objects are active. May be
the old will be gc'ed when clean_ops is assigned again.


A more `functional pattern` like this enables you to easily modify how the
strings are transformed at a very high level. The clean_strings function is
also now more reusable!


={============================================================================
|kt_dev_py_0001| py-function-polymorphism

<dynamic-typing>

>>> def times(x, y):    # Create and assign function
...   return x * y      # Body executed when called

>>> times(2, 4)         # Arguments in parentheses
8

>>> x = times(3.14, 4)  # Save the result object
>>> x
12.56

>>> times('Ni', 4)      # Functions are `typeless`
'NiNiNiNi'

The very meaning of the expression x * y in our simple times function depends
completely upon the kinds of `objects` that x and y are - thus, the same
function can perform multiplication in one instance and repetition in another.
Python leaves it up to the objects to do something reasonable for the syntax.

def intersect(seq1, seq2):
  res = []            # Start empty
  for x in seq1:      # Scan seq1
    if x in seq2:     # Common item?
    res.append(x)     # Add to end
  return res

it works on arbitrary types, `as long as` they support the expected object
interface:

>>> x = intersect([1, 2, 3], (1, 4))  # Mixed types
>>> x                                 # Saved result object
[1]

Means that the first argument has to support the for loop, and the second has
to support the in membership test.

<not-care-about-type>
If the objects passed in do not support this expected interface, Python will
detect the error when the * expression is run and raise an exception
automatically. It's therefore usually pointless to code error checking
ourselves. (error checking on type) In fact, doing so would limit our
function's utility, as it would be restricted to work only on objects whose
types we test for.

This turns out to be a crucial philosophical difference between Python and
statically typed languages like C++ and Java: in Python, your code is not
supposed to care about specific data types. If it does, it will be limited to
working on just the types you anticipated when you wrote it, and it will not
support other compatible object types that may be coded in the future.
Although it is possible to test for types with tools like the type built-in
function, doing so breaks your code's flexibility. 

By and large, we code to object `interfaces` in Python, not data types.

This polymorphic model of programming means we have to test our code to detect
errors, rather than providing type declarations a compiler can use to detect
some types of errors for us ahead of time. 

In exchange for an initial bit of testing, though, we radically reduce the
`amount of code` we have to write and radically increase our code's `flexibility`.
As you'll learn, it's a net win in practice.


={============================================================================
|kt_dev_py_0001| py-function-scope

Learning Python 5E, 17, Scopes

The term `scope` refers to a namespace: that is, the location of a name's
`assignment` in your source code determines the scope of the name's visibility
to your code. 

<namespace>
Functions can access variables in two different scopes: global and local. An
alternate and more descriptive name describing a variable scope in Python is a
namespace.

As names in Python spring into existence when they are first assigned values,
and Python uses the location of the assignment of a name to associate it with
  a particular namespace. In other words, the place where you assign a name in
  your source code determines the namespace it will live in, and hence its
  scope of visibility.

all names assigned inside a function are associated with that function's
namespace


<local-nonlocal-global>
  * If a variable is assigned inside a def, it is `local` to that function.

  * If a variable is assigned in an enclosing def, it is `nonlocal` to nested
    functions.

  * If a variable is assigned outside all defs, it is `global` to the entire
    file.

<global-scope>
The global scope spans a single file only. 

Don't be fooled by the word "global" herenames at the top level of a file are
global to code within that single file only. 

There is really no notion of a single, all-encompassing global file-based
scope in Python. 

Instead, names are partitioned into modules, and you must always import a
module explicitly if you want to be able to use the names its file defines.
When you hear "global" in Python, think "module."


<py-name-resolution>
Name Resolution: The LEGB Rule

Name references search at most four scopes: (L)local, then (E)enclosing
functions (if any), then (G)global, then (B)built-in.

E - the scopes of enclosing defs or lambdas - 
can technically correspond to more than one lookup level. This case only comes
into play when you nest functions within functions, and is enhanced by the
nonlocal statement in 3.X.

note:
so E matters for lambda if not use 3.X.


<local-name-detected>

Local Names Are Detected Statically, Chapter 21: The Benchmarking Interlude

What you may not realize is that Python detects locals statically, when it
compiles the def’s code, rather than by noticing assignments as they happen at
runtime. This leads to one of the most common oddities posted on the Python
newsgroup by beginners. 

note: compared with dynamically or runtime.

>>> X=99
>>> def selector():
...     print(X)
... 
>>> selector()
99
>>> def selector():
...     print(X)
...     X=88
... 
>>> selector()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 2, in selector
UnboundLocalError: local variable 'X' referenced before assignment

While compiling, Python sees the assignment to X and decides that X will be a
  local name everywhere in the function. But when the function is actually
  run, because the assignment hasn’t yet happened when the print executes,
  Python says you’re using an undefined name.


<closures>
closure is any `dynamically-generated function` returned by another function.
The key property is that the returned function has access to the variables in
the local namespace where it was created. Here is a very simple example:

>>> def make_closure(a):
...     def closure():
...             print('I know the secret: %d' % a)
...     return closure
... 

>>> closure = make_closure(5)
>>> closure
<function closure at 0xb75125dc>
>>> closure()
I know the secret: 5
>>> closure()
I know the secret: 5


The difference between a closure and a regular function is that the closure
continues to have access to the namespace (the function) where it was created,
    even though that function is done executing. 

So in the above case, the returned closure will always print "I know the
secret: 5" whenever you call it. 

While it's common to create closures whose internal state (in this example,
    only the `value` of a) is `static`, you can just as easily have a
`mutable` object like a dict, set, or list that can be modified. For example,
    here's a function that returns a function that keeps track of arguments it
      has been called with:


one technical limitation to keep in mind is that while you can mutate any
internal state objects (like adding key-value pairs to a dict), you cannot
bind `variables` in the enclosing function scope. One way to work around this is
to modify a dict or list rather than binding variables:

>>> def make_counter():
...     count = [0]
...     def counter():
...             count[0] +=1
...             return count[0]
...     return counter
... 
>>> cnt = make_counter()
>>> cnt
<function counter at 0xb7512684>
>>> cnt()
1
>>> cnt()
2

>>> def make_counter_two():
...     count = 0
...     def counter():
...             count +=1
...             return count
...     return counter
... 
>>> cnt_two = make_counter_two()
>>> cnt_two
<function counter at 0xb75126f4>
>>> cnt_two()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 4, in counter
UnboundLocalError: local variable 'count' referenced before assignment


Why this is useful? In practice, you can write very `general functions` with
lots of options, then fabricate simpler, more specialized functions. Here's an
example of creating a string formatting function:


def format_and_pad(template, space):
  def formatter(x):
    return (template % x).rjust(space)
  return formatter

You could then create a floating point formatter that always returns a
length-15 string like so:

In [500]: fmt = format_and_pad('%.4f', 15)
In [501]: fmt(1.756)
Out[501]: ' 1.7560'

If you learn more about object-oriented programming in Python, you might
observe that these patterns also could be implemented (albeit more verbosely)
  using classes.


={============================================================================
|kt_dev_py_0001| py-function-scope

{lambda}
Anonymous or lambda functions, which are really just simple functions
consisting of a single statement, the result of which is the return value.
They are defined using the `lambda keyword`

They are especially convenient in data analysis because, as you'll see, there
are many cases where data transformation functions will take functions as
arguments. It's often less typing (and clearer) to pass a lambda function as
opposed to writing a full-out function declaration or even assigning the
lambda function to a local variable. For example, consider this silly example:

>>> def apply_to_list(some_list, f):
...     return [f(x) for x in some_list]
... 

>>> ints = [4,0,1,5,6]
>>> apply_to_list(ints, lambda x: x*2)
[8, 0, 2, 10, 12]

You could also have written [x * 2 for x in ints], but here we were able to
succintly pass a custom operator to the apply_to_list function.



={============================================================================
|kt_dev_py_0001| py-function-arguments

Learning Python 5E, 18, Arguments

<function-argument>
arguments are passed by assignment. if explians in C way:

  * Immutable arguments are effectively passed “by value.” Objects such as
    integers and strings are passed by object reference instead of by copying,
    but because you can’t change immutable objects in place anyhow, the effect
    is much like making a copy.

  * Mutable arguments are effectively passed “by pointer.” Objects such as
    lists and dictionaries are also passed by object reference, which is
    similar to the way C passes arrays as pointersmutable objects can be
    changed in place in the function, much like C arrays.

Of course, if you’ve never used C, Python’s argument-passing mode will seem
simpler still - it involves just the assignment of objects to names, and it
works the same:

  "whether the objects are mutable or not."

see *shared-reference*


{return-multiple-values}
What's happening here is that the function is actually just returning `one`
object, namely a tuple, which is then being unpacked into the result
variables.

>>> def f():
...     a=5
...     b=6
...     c=7
...     return a,b,c
... 

>>> a,b,c = f()
>>> a
5
>>> b
6
>>> c
7


{argument-matching}
Argument Matching Basics

By default, arguments are matched by position, from left to right, and you
must pass exactly as many arguments as there are argument names in the
function header. However, you can also specify matching by name, provide
default values, and use collectors for extra arguments.


Positionals: matched from left to right

The normal case, which we’ve mostly been using so far, is to match passed
argument values to argument names in a function header by position, from left
to right.

Keywords: matched by argument name

Alternatively, callers can specify which argument in the function is to
receive a value by using the argument’s name in the call, with the name=value
syntax.

Defaults: specify values for optional arguments that aren’t passed

Functions themselves can specify default values for arguments to receive if
the call passes too few values, again using the name=value syntax.

Syntax                Location  Interpretation

func(value)           Caller    Normal argument: matched by position
func(name=value)      Caller    Keyword argument: matched by name

def func(name)        Function  Normal argument: matches any passed value 
                                by position or name
def func(name=value)  Function  Default argument value, if not passed in the call

In a function call, simple values are matched by position, but using the
name=value form to match by name to arguments instead; these are called
`keyword arguments`

In a function header, a simple name is matched by position or name depending
on how the caller passes it, but the name=value specifies a `default value`

<restriction>
In a function call, any positional arguments (value); followed by a
combination of any keyword arguments (name=value)

In a function header, arguments must appear in this order: any normal
arguments (name); followed by any default arguments (name=value)


<varargs> <py-packing>
`Varargs collecting`: collect arbitrarily many positional or keyword arguments

Functions can use special arguments preceded with `one or two * characters` to
collect an arbitrary number of possibly extra arguments. This feature is often
referred to as varargs, after a variable-length argument list tool in the C
language; in Python, the arguments are collected in a normal object.

Syntax            Location  Interpretation

def func(*name)   Function  Matches and collects remaining positional arguments 
                            in a `tuple`
def func(**name)  Function  Matches and collects remaining keyword arguments 
                            in a `dictionary`


>>> def f(*args): print(args)

>>> f()
()
>>> f(1)
(1,)
>>> f(1,2,3)
(1, 2, 3)

>>> def f(**args): print(args)

>>> f()
{}
>>> f(a=1, b=2)
{'a': 1, 'b': 2}


`Varargs unpacking`: pass arbitrarily many positional or keyword arguments

Callers can also use the * syntax to unpack argument collections into separate
arguments. This is the inverse of a * in a function header-in the header it
means collect arbitrarily many arguments, while in the call it means unpack
arbitrarily many arguments, and pass them individually as discrete values.

func(*iterable)   Caller    Pass all objects in iterable as individual 
                            positional arguments
func(**dict)      Caller    Pass all `key/value pairs in dict` as individual 
                            keyword arguments


>>> def func(a, b, c, d): print(a, b, c, d)

>>> args = (1, 2)
>>> args += (3,4)
>>> args
(1, 2, 3, 4)
>>> func(*args)
(1, 2, 3, 4)
>>> func(1,2,3,4)
(1, 2, 3, 4)


>>> args = {'a':1, 'b':2, 'c':3, 'd':4}
>>> func(**args)                  # same as func(a=1, b=2, c=3, d=4)
(1, 2, 3, 4)


<argument-order>
If you choose to use and combine the special argument-matching modes, Python
will ask you to follow these ordering rules among the modes’ optional
components:

  * In a function call, arguments must appear in this order: any positional
    arguments (value); followed by a combination of any keyword arguments
    (name=value) and the *iterable form; followed by the **dict form.

  * In a function header, arguments must appear in this order: any normal
    arguments (name); followed by any default arguments (name=value); followed
    by the *name (or * in 3.X) form; followed by any name or name=value
    keyword-only arguments (in 3.X); followed by the **name form.


={============================================================================
|kt_dev_py_0001| py-class

Learning Python 5E, 27, Class Coding Basics

  * `assignments inside class` statements make `class attributes.` Just like in
    module files, top-level assignments within a class statement (not nested
    in a def) generate attributes in a class object. Technically, the
    class statement defines a local scope that morphs into the attribute
    namespace of the class object, just like a module’s global scope. After
    running a class statement, class attributes are accessed by name
    qualification: object.name.

  * Class attributes provide object state and behavior. Attributes of a class
    object record state information and behavior to be shared by all
    instances created from the class; function def statements nested inside
    a class generate methods, which process instances.

  * Each instance object inherits class attributes and gets its own namespace.
    Instance objects created from classes are new namespaces; they start out
    empty but inherit attributes that live in the class objects from which
    they were generated.


>>> class FirstClass:             # Define a class object
      def setdata(self, value):   # Define class's methods
        self.data = value         # self is the instance
      def display(self):
        print(self.data)          # self.data: per instance

>>> x = FirstClass() # Make two instances
>>> y = FirstClass() # Each is a new namespace

>>> x.setdata("King Arthur") # Call methods: self is x
>>> y.setdata(3.14159) # Runs: FirstClass.setdata(y, 3.14159)

>>> x.display() # self.data differs in each instance
King Arthur
>>> y.display() # Runs: FirstClass.display(y)
3.14159

consider that we can change instance attributes outside the class, by
assigning to an explicit instance object:

>>> x.data = "New value" # Can get/set attributes
>>> x.display() # Outside the class too
New value


<py-inheritance>
Superclasses are listed in parentheses in a class header. To make a class
inherit attributes from another class, just list the other class in
parentheses in the new class statement’s header line. The class that inherits
is usually called a subclass, and the class that is inherited from is its
superclass.

>>> class SecondClass(FirstClass):        # Inherits setdata
      def display(self):                  # Changes display
        print('Current value = "%s"' % self.data)


<module-and-class>
class name’s just a variable assigned to an object when the `class` statement
runs and become module attributes. Each module may arbitrarily mix any number
of variables, functions, and classes, and all names in a module behave the
same way.

from modulename import FirstClass     # Copy name into my scope
  class SecondClass(FirstClass): 
    def display(self): ...

import modulename                     # Access the whole module
  class SecondClass(modulename.FirstClass):
    def display(self): ...


<py-convention>
common convention in Python dictates that class names should begin with an
uppercase letter

import person         # Lowercase for modules
x = person.Person()   # Uppercase for classes


<py-overload>

  * Methods named with double underscores (__X__) are special hooks. In Python
    classes we implement operator overloading by providing specially named
    methods to intercept operations. The Python language defines a `fixed` and
    unchangeable mapping from each of these operations to a specially named
    method.

  * Such methods are called automatically when instances appear in built-in
    operations. For instance, if an instance object inherits an __add__
    method, that method is called whenever the object appears in a +
    expression. The method’s return value becomes the result of the
    corresponding expression.

Still, there is one operator overloading method you are likely to see in
almost every realistic Python class: 
  
the `__init__` method, which is known as the constructor method and is used to
  initialize objects’ state. You should pay special attention to this method,
  because __init__, along with the self argument, turns out to be a key
    requirement to reading and understanding most OOP code in Python.

>>> class ThirdClass(SecondClass):  # Inherit from SecondClass
      def __init__(self, value):    # On "ThirdClass(value)"
        self.data = value

      def __add__(self, other):     # On "self + other"
        return ThirdClass(self.data + other)

      def __str__(self):            # On "print(self)", "str()"
        return '[ThirdClass: %s]' % self.data

      def mul(self, other):         # In-place change: named
        self.data *= other

>>> a.mul(3) # mul: changes instance in place
>>> print(a)
[ThirdClass: abcabcabc]

note: another reason to use overloading
On the other hand, you might decide to use operator overloading if you need to
  pass a user-defined object to a function that was coded to expect the
  operators available on a built-in type like a list or a dictionary.
  Implementing the same operator set in your class will ensure that your
  objects support the same expected object interface and so are compatible
  with the function.


<class-with-no-instance>
Notice that this works even though there are no instances of the class yet;
classes are objects in their own right, even without instances. In fact, they
  are just self-contained namespaces; as long as we have a reference to a
  class, we can set or change its attributes anytime we wish.


<class-print-display>
To print object, overload __repr__ or __str__.

__str__ is preferred by print and str, and __repr__ is used as a fallback for
these roles and in all other contexts.  so coding just __repr__ alone suffices
to give a single display in all casesprints, nested appearances, and
interactive echoes.

note: <py-repr-str>
Technically, the difference between default interactive echoes and print
corresponds to the difference between the built-in repr and str functions:

>>> repr('spam') # Used by echoes: as-code form
"'spam'"
>>> str('spam') # Used by print: user-friendly form
'spam'

Both of these convert arbitrary objects to their string representations: repr
(and the default interactive prompt) produces results that look as though they
were code; str (and the print operation) converts to a typically more
user-friendly format if available. Some objects have botha str for general
use, and a repr with extra details.

class Person:
    def __init__(self, name, job=None, pay=0):
        self.name = name
        self.job = job
        self.pay = pay
    def __repr__(self):
        return '[Person: %s, %s]' % (self.name, self.pay)

print sue

[Person: Sue Jones, 10000]

"as-code" means: Notice that we’re doing string % formatting to build the
  display string in __repr__ here


<class-override>
see the way that explains name resolution

Recall that inheritance searches proceed upward from instances to subclasses
  to superclasses, stopping at the first appearance of the attribute name that
  it finds. In this case, since the display name in SecondClass will be found
  before the one in First Class, we say that SecondClass overrides
  FirstClass’s display.

>>> z = SecondClass()
>>> z.setdata(42)       # Finds setdata in FirstClass
>>> z.display()         # Finds overridden method in SecondClass
Current value = "42"

the inheritance search rule applied to the method’s `name`.

class Person:
    def __init__(self, name, job=None, pay=0):
        self.name = name
        self.job = job
        self.pay = pay

    def giveRaise(self, percent):
        self.pay = int(self.pay * (1+percent))

    def __repr__(self):
        return '[Person: %s, %s]' % (self.name, self.pay)

class Manager(Person):
    def giveRaise(self, percent, bonus=.10):
        Person.giveRaise(self, percent+bonus)

    # bad way using cut and paste since need to change super and subclass if
    # logic changes
    #
    # def giveRaise(self, percent, bonus=.10):
    #   self.pay = int(self.pay * (1 + percent + bonus))

tom = Manager('Tom Jones', 'mgr', 50000)
tom.giveRaise(.10)      # custom version
print tom               # inherited


<py-polymorphism>
In this code, object is either a Person or a Manager, and Python runs the
appropriate giveRaise automatically

for obj in (bob, sue, tom):         # Process objects generically
  obj.giveRaise(.10)                # Run this object's giveRaise
  print(obj)                        # Run the common __repr__

[Person: Bob Smith, 0]
[Person: Sue Jones, 121000]
[Person: Tom Jones, 72000]


<py-ctor>

class Person:
    def __init__(self, name, job=None, pay=0):
        self.name = name
        self.job = job
        self.pay = pay

class Manager(Person):
    def __init__(self, name, pay): # Redefine constructor
        Person.__init__(self, name, 'mgr', pay) # Run original with 'mgr'


<py-inspection>

>>> rec.__bases__         # Class to superclasses link, () in 2.X
(<class 'object'>,)

>>> bob = Person('Bob Sminth')
>>> bob
[Person: Bob Sminth, 0]

>>> bob.__class__
<class person.Person at 0x7fdf1ecbd530>

>>> bob.__class__.__name__
'Person'


>>> bob.__dict__.keys()   # Instance attrs only
['pay', 'job', 'name']

>>> dir(bob)              # Plus inherited attrs in classes
['__doc__', '__init__', '__module__', '__repr__', 'giveRaise', 'job', 'lastName',
'name', 'pay']


<convention-to-avoid-name-colision>
To minimize the chances of name collisions like this, Python programmers often
prefix methods not meant for external use with a single underscore:
_gatherAttrs in our case. This isn’t foolproof (what if another class defines
    _gatherAttrs, too?), but it’s usually sufficient, and it’s a common Python
naming convention for methods internal to a class. not for override.

A better and less commonly used solution would be to use two underscores at
the front of the method name only: __gatherAttrs for us. Python automatically
expands such names to include the enclosing class’s name, which makes them
truly unique when looked up by the inheritance search. This is a feature
usually called pseudoprivate class attributes,


<class-instance-attribute>
class name(superclass,...): # Assign to name
  attr = value              # `shared` class data

  def method(self,...):     # Methods
    self.attr = value       # Per-instance data


>>> class MixedNames:
...     data = 'spam'
...     def __init__(self, value):
...             self.data = value
...     def display(self):
...             print(self.data, MixedNames.data)     # use of class attribute
...
>>> x = MixedNames(1)
>>> y = MixedNames(2)

>>> x.display(); y.display()
(1, 'spam')
(2, 'spam')


# File manynames.py
# All five different Xs

X = 11              # Global (module) name/attribute (X, or manynames.X)

def f():
  print(X)          # Access global X (11)

def g():
  X = 22            # Local (function) variable (X, hides module X)
  print(X)

class C:
  X = 33            # Class attribute (C.X)

  def m(self):
    X = 44          # Local variable in method (X)
    self.X = 55     # Instance attribute (instance.X)

if __name__ == '__main__':

  print(X)      # 11: module (a.k.a. manynames.X outside file)
  f()           # 11: global
  g()           # 22: local
  print(X)      # 11: module name unchanged

  obj = C()     # Make instance
  print(`obj.X`)  # 33: class name inherited by instance

  obj.m()       # Attach attribute name X to instance now

  # that the instance’s own X is not created until we call I.m()attributes,
  # like all variables, spring into existence when assigned, and not before.

  print(`obj.X`)  # 55: instance
  print(C.X)    # 33: class (a.k.a. obj.X if no X in instance)

  #print(C.m.X) # FAILS: only visible in method
  #print(g.X)   # FAILS: only visible in function


# example of global module name
#!/usr/bin/env python

CL_STATUS_OK = 0
CL_STATUS_ERROR = 1

if __name__ == "__main__":
    pass

Use

import scripts.common.cmdLine as cmdLine

def func (branch, tag, dir):
    retVal = cmdLine.CL_STATUS_ERROR


<abstract-superclass>
To have method to be defined in subclass. If an expected method is not defined
in a subclass, Python raises an undefined name exception when the inheritance
search fails.

class Super:
    def method(self):
        print('in Super.method')
    def delegate(self):
        self.action()         # expected to be defined

class Provider(Super):
    def action(self):
        print('in Provider.action')

if __name__ == '__main__':
    x = Provider()
    x.delegate()


Class coders sometimes make such subclass requirements more obvious with
assert statements, or by raising the built-in NotImplementedError exception

class Super:
    def method(self):
        print('in Super.method')
    def delegate(self):
        self.action()
    def action(self):
        raise NotImplementedError('action must be defined!')
        # assert False, 'action must be defined!'

class Provider(Super):
    pass

if __name__ == '__main__':
    x = Provider()
    x.delegate()

Traceback (most recent call last):
  File "./specialize.py", line 20, in <module>
    x.delegate()
  File "./specialize.py", line 9, in delegate
    self.action()
  File "./specialize.py", line 11, in action
    assert False, 'action must be defined!'
AssertionError: action must be defined!


Traceback (most recent call last):
  File "./specialize.py", line 21, in <module>
    x.delegate()
  File "./specialize.py", line 9, in delegate
    self.action()
  File "./specialize.py", line 11, in action
    raise NotImplementedError('action must be defined!')
NotImplementedError: action must be defined!


As of Python 2.6 and 3.0, the prior section’s abstract superclasses (a.k.a.
    “abstract base classes”), which require methods to be filled in by
  subclasses, may also be implemented `with special class syntax.`

Since the effect is the samewe `can’t make an instance` unless the method is
defined lower in the class tree.

the potential advantage of this approach is that errors for missing methods
are issued when we attempt to make an instance of the class, not later when we
try to call a missing method.


={============================================================================
|kt_dev_py_0001| py-class-static py-decorator

Learning Python 5E, 32, Advanced Class Topics

Static Methods in 2.X and 3.X

The concept of static methods is the same in both Python 2.X and 3.X, but its
implementation requirements have evolved somewhat in Python 3.X.

// means there are differences in coding to use static methods and skip the
// details.

However, to allow self-less methods to be called through classes in 2.X and
through instances in both 2.X and 3.X, you need to either adopt other designs
or be able to somehow mark such methods as special. Let’s look at both options
in turn.

// so use staticmethod() and classmethod() to make code work for both version
// lines.

Both mark a function object as special that is, as requiring no instance if
static and requiring a class argument if a class method.

# File bothmethods.py

class Methods:

  def imeth(self, x):     # Normal instance method: passed a self
    print([self, x])

  def smeth(x):           # Static: no instance passed
    print([x])

  def cmeth(cls, x):      # Class: gets class, not instance
    print([cls, x])

  smeth = staticmethod(smeth)   # Make smeth a static method
  cmeth = classmethod(cmeth)    # Make cmeth a class method

this assignment form simply `rebind` the method names. 

>>> from bothmethods import Methods # Normal instance methods
>>> obj = Methods()                 # Callable through instance or class

>>> obj.imeth(1)
[<bothmethods.Methods object at 0x0000000002A15710>, 1]
>>> Methods.imeth(obj, 2)
[<bothmethods.Methods object at 0x0000000002A15710>, 2]


>>> Methods.smeth(3)    # Static method: call through class
[3]                     # No instance passed or expected
>>> obj.smeth(4)        # Static method: call through instance
[4]                     # Instance not passed

>>> Methods.cmeth(5)                # Class method: call through class
[<class 'bothmethods.Methods'>, 5]  # Becomes cmeth(Methods, 5)
>>> obj.cmeth(6)                    # Class method: call through instance
[<class 'bothmethods.Methods'>, 6]  # Becomes cmeth(Methods, 6)


<ex-staticmethod>
// note: see how to use class attribute

class Spam:
  numInstances = 0 # Use static method for class data

  def __init__(self):
    Spam.numInstances += 1

  def printNumInstances():
    print("Number of instances: %s" % Spam.numInstances)

  printNumInstances = staticmethod(printNumInstances)

>>> from spam_static import Spam
>>> a = Spam()
>>> b = Spam()
>>> c = Spam()

>>> Spam.printNumInstances() # Call as simple function
Number of instances: 3

>>> a.printNumInstances() # Instance argument not passed
Number of instances: 3

advantages over using simple global function:

  * it also localizes the function name in the class scope (so it won’t clash
    with other names in the module)

  * moves the function code closer to where it is used (inside the class
    statement) 

  * allows subclasses to customize the static method with inheritance

// note: overriding static method is possible in C++?

class Sub(Spam):

  def printNumInstances():    # Override a static method
    print("Extra stuff...")   # But call back to original
    Spam.printNumInstances()

  printNumInstances = staticmethod(printNumInstances)


<ex-classmethod>
Rather than hardcoding the class name, the class method uses the automatically
passed class object generically:

// skip 

# https://docs.python.org/2/library/functions.html#staticmethod
# 
# staticmethod(function)
# 
#     Return a static method for function.
# 
#     A static method does not receive an `implicit first argument` 
#     
#     To declare a static method, use this idiom:
# 
#     class C(object):
#         @staticmethod
#         def f(arg1, arg2, ...):
#             ...
# 
#     The @staticmethod form is a `function decorator` see the description of
#       function definitions in Function definitions for details.
# 
#     It can be called either on the class (such as C.f()) or on an instance
#     (such as C().f()). The instance is ignored except for its class.
# 
#     Static methods in Python are similar to those found in Java or C++. Also
#     see classmethod() for a variant that is useful for creating alternate
#     class constructors.
# 
#     For more information on static methods, consult the documentation on the
#     standard type hierarchy in The standard type hierarchy.
# 
#     New in version 2.2.
# 
#     Changed in version 2.4: Function decorator syntax added.


<py-decorator>
Python decorators addressed this specific need and provided a general tool for
adding `logic` that manages both functions and classes, or later `calls` to them.

`Function decorators` turn out to be very general tools: they are useful for
adding many types of logic to functions besides the static and class method
use cases. For instance, they may be used to augment functions with code that
logs calls made to them, checks the types of passed arguments during
debugging, and so on. Function decorators can be used to manage either
functions themselves or later calls to them.

  * Function decorators - the initial entry in this set, added in Python 2.4 -
    augment function definitions. They specify special operation modes for
    both simple functions and classes’ methods by `wrapping` them in an extra
    layer of logic implemented as another function, usually called a
    metafunction.

built-in decorator or user-defined function decorators.

# like callables in C++

class C:
  @staticmethod # Function decoration syntax
  def meth():
  ...

Internally, this syntax has the same effect as the followingpassing the
  function through the decorator and assigning the result back to the original
  name:

class C:
  def meth():
  ...
  meth = staticmethod(meth) # Name rebinding equivalent


class Spam:
  numInstances = 0 # Use static method for class data

  def __init__(self):
    Spam.numInstances += 1

  def printNumInstances():
    print("Number of instances: %s" % Spam.numInstances)

  printNumInstances = staticmethod(printNumInstances)

to:

class Spam:
  numInstances = 0 # Use static method for class data

  def __init__(self):
    Spam.numInstances += 1

  @staticmethod
  def printNumInstances():
    print("Number of instances: %s" % Spam.numInstances)


<user-defined-function-decorators>

A First Look at User-Defined Function Decorators

Recall from Chapter 30 that the __call__ operator overloading method
implements a function-call interface for class instances. The following code
uses this to define a call proxy class that saves the decorated function in
the instance and catches calls to the original name.

#!/usr/bin/env python

class tracer:
    def __init__(self, func):
        self.calls = 0
        self.func = func

    def __call__(self, *args):
        self.calls += 1
        print('calls %s to %s' % (self.calls, self.func.__name__))
        # calls embedded func with vararg unpacking
        return self.func(*args)   

@tracer
def spam(a, b, c):
    return a + b + c

print( spam(1, 2, 3))
print( spam('a', 'b', 'c'))


$ ./decoraor.py 
calls 1 to spam
6
calls 2 to spam
abc

TODO: more on decorators

Although they are a general mechanism whose usage may be required by some
packages, coding new user-defined decorators and metaclasses is an advanced
topic of interest primarily to tool writers, not application programmers.

 Chapter 38 shows how to code properties using function decorator syntax in
 more depth.

 Chapter 39 has much more on decorators, including more comprehensive
 examples.

 Chapter 40 covers metaclasses, and more on the class and instance management
 story.


={============================================================================
|kt_dev_py_0001| py-class-super

Example from 28, see how to call super class's version:

class Person:
    def __init__(self, name, job=None, pay=0):
        self.name = name
        self.job = job
        self.pay = pay

    def giveRaise(self, percent):
        self.pay = int(self.pay * (1+percent))

    def __repr__(self):
        return '[Person: %s, %s]' % (self.name, self.pay)

class Manager(Person):
    def giveRaise(self, percent, bonus=.10):
        Person.giveRaise(self, percent+bonus)

What About super?

To extend inherited methods, the examples in this chapter simply call the
original through the superclass name: Person.giveRaise(...). 
This is the traditional and simplest scheme in Python, and the one used in
most of this book.

Python also has a super built-in function that allows calling back to a
  superclass’s methods more generically but it’s cumbersome to use in 2.X;
differs in form between 2.X and 3.X; relies on unusual semantics in 3.X; works
  unevenly with Python’s operator overloading; and does not always mesh well
  with traditionally coded multiple inheritance, where a single superclass
  call won’t suffice.

Because of these downsides, this book prefers to call superclasses by explicit
  name instead of super, recommends the same policy for newcomers, and defers
  presenting super until Chapter 32. It’s usually best judged after you learn
  the simpler, and generally more traditional and “Pythonic” ways of achieving
  the same goals, especially if you’re new to OOP. Topics like MROs and
  cooperative multiple inheritance dispatch seem a lot to ask of beginnersand
  others.


32, The super Built-in Function: For Better or Worse?

TODO: more?


={============================================================================
|kt_dev_py_0001| py-exception

Learning Python 5E, 33, Exception Basics

If an error is ignored, Python’s default exception-handling behavior kicks in:
it stops the program and prints an error message.


>>> fetcher(x, 4) # Default handler - shell interface
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "<stdin>", line 2, in fetcher
IndexError: string index out of range

which simply prints the standard error message. They include the exception
  that was raised, along with a stack trace - a list of all the lines and
  functions that were active when the exception occurred.

To trigger an exception manually, simply run a `raise` statement.

>>> try:
... raise IndexError # Trigger exception manually
... except IndexError:
... print('got exception')
...
got exception


<user-defined-exception>
User-defined exceptions are coded with classes, which inherit from a built-in
exception class: usually the class named Exception:

# note on pass
>>> class AlreadyGotOne(Exception): pass # User-defined exception

>>> def grail():
raise AlreadyGotOne() # Raise an instance
>>> try:
... grail()
... except AlreadyGotOne: # Catch class name
... print('got exception')
...
got exception

>>> class Career(Exception):
def __str__(self): return 'So I became a waiter...'

>>> raise Career()
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
__main__.Career: So I became a waiter...


<propagate>
why not seeing "after try?" on output? since 'propagates' when not caught.

>>> def after():
  try:
    fetcher(x, 4)
  finally:
    print('after fetch')
    print('after try?')
>>> after()
after fetch
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 3, in after
  File "<stdin>", line 2, in fetcher
IndexError: string index out of range
>>>


<why-exception>
One way to see how exceptions are useful is to compare coding styles in Python and
languages without exceptions.

in C, generally have to test return values or status codes after every
operation that could possibly go astray, and propagate the results of the
tests as your programs run:

doStuff()
{ 
  if (doFirstThing() == ERROR)  // Detect errors everywhere
    return ERROR;               // even if not handled here
  if (doNextThing() == ERROR)
    return ERROR;
  ...
    return doLastThing();
}

main()
{
  if (doStuff() == ERROR)
    badEnding();
  else
    goodEnding();
}

You can instead wrap arbitrarily vast pieces of a program in exception
handlers and simply write the parts that do the actual work, assuming all is
normally well:

def doStuff(): # Python code
  doFirstThing() # We don't care about exceptions here,
  doNextThing() # so we don't need to detect them
  ...
  doLastThing()

if __name__ == '__main__':
  try:
    doStuff() # This is where we care about results,
  except: # so it's the only place we must check
    badEnding()
  else:
    goodEnding()


<ex>
Suppose we wanted a version of float that fails gracefully, returning the
input argument.

# catchs all

def attempt_float(x):
  try:
    return float(x)
  except:
    return x

Want to only `suppress(catch)` ValueError, since a TypeError (the input was
    not a string or numeric value) might indicate a legitimate bug in your
program.  To do that, write the `exception type` after except:

def attempt_float(x):
  try:
    return float(x)
  except ValueError:
    return x

>>> float((1,2))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: float() argument must be a string or a number
>>> def attempt_float(x):
...     try:
...             return float(x)
...     except ValueError:
...             return x
... 
>>> attempt_float((1,2))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 3, in attempt_float
TypeError: float() argument must be a string or a number
>>> 


Can catch `multiple exception types` by writing a `tuple` of exception types
instead (the parentheses are required):

def attempt_float(x):
  try:
    return float(x)
  except (TypeError, ValueError):
    return x

>>> def attempt_float(x):
...     try:
...             return float(x)
...     except (TypeError, ValueError):
...             return x
...
>>> attempt_float((1,2))
(1, 2)


As with C++, if exception gets raised and not catched, then returns
immediately.

In some cases, you may not want to suppress an exception, but you want some
code to be executed `regardless of` whether the code in the try block
`succeeds or not` To do this, use finally:

f = open(path, 'w')

try:
  write_to_file(f)
finally:
  f.close()

Here, the file handle f will always get closed. 

Similarly, you can have code that executes `only if` the try: block `succeeds`
using else:

f = open(path, 'w')

try:
  write_to_file(f)
except:
  print 'Failed'
else:
  print 'Succeeded'
finally:
  f.close()


Learning Python 5E, 34, Exception Coding Details

<try-statement-clause>

try:
  statements    # Run this main action first
except name1:
  statements    # Run if name1 is raised during try block
except (name2, name3):
  statements    # Run if any of these exceptions occur
except name4 as var:
  statements    # Run if name4 is raised, assign instance raised to var
except:
  statements    # Run for all other exceptions raised
else:
  statements    # Run if no exception was raised during try block
finally:
  statements    # Always perform this block on exit

unlike an except, the finally does not terminate the exceptionit continues
being raised after the finally block runs


<raise-statement>

raise IndexError    # Class (instance created)
raise IndexError()  # Instance (created in statement)
raise               # Reraise the most recent exception

The `as` is optional in a try handler (if it’s omitted, the instance is simply
    not assigned to a name), but including it allows the handler to access
both data in the instance and methods in the exception class.

class MyExc(Exception): pass
...
raise MyExc('spam')     # Exception class with constructor args
...
try:
  ...
except MyExc as X:      # Instance attributes available in handler
  print(X.args)


<assert-statement>
Assertions are typically used to verify program `conditions` or user-defined
constraints during development.

When displayed, their error message text automatically includes source code
line information and the value listed in the assert statement.

def f(x):
  assert x < 0, 'x must be negative'
  return x ** 2

% python
>>> import asserter
>>> asserter.f(1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File ".\asserter.py", line 2, in f
    assert x < 0, 'x must be negative'
AssertionError: x must be negative


Use a command line like python O main.py to run in optimized mode and disable
(and hence skip) asserts.


{py-with} {with-as-context}

# https://docs.python.org/2/reference/compound_stmts.html#the-with-statement
#
# 7.5. The with statement
#
# New in version 2.5.
# 
# The with statement is used to wrap the execution of a block with methods
# defined by a context manager (see section With Statement Context Managers).
# This allows common try...except...finally usage patterns to be encapsulated
# for convenient reuse.
#
# with_stmt ::=  "with" with_item ("," with_item)* ":" suite
# with_item ::=  expression ["as" target]
#
# The execution of the with statement with one “item” proceeds as follows:
# 
# 1. The context expression (the expression given in the with_item) is
# evaluated to obtain a context manager.
# 
# 2. The context manager’s __exit__() is loaded for later use.
# 
# 3. The context manager’s __enter__() method is invoked.
# 
# 4. If a target was included in the with statement, the return value from
# __enter__() is assigned to it.
#
# Note The with statement guarantees that if the __enter__() method returns
# without an error, then __exit__() will always be called. Thus, if an error
# occurs during the assignment to the target list, it will be treated the same
# as an error occurring within the suite would be. See step 6 below.  The
#
# 5. suite is executed.
# 
# 6. The context manager’s __exit__() method is invoked. If an exception caused
# the suite to be exited, its type, value, and traceback are passed as
# arguments to __exit__(). Otherwise, three None arguments are supplied.
# 
# If the suite was exited due to an exception, and the return value from the
# __exit__() method was false, the exception is reraised. If the return value
# was true, the exception is suppressed, and execution continues with the
# statement following the with statement.
# 
# If the suite was exited for any reason other than an exception, the return
# value from __exit__() is ignored, and execution proceeds at the normal
# location for the kind of exit that was taken.
# 
#
# With more than one item, the context managers are processed as if multiple
# with statements were nested:
#
# with A() as a, B() as b:
#     suite
# is equivalent to
# 
# with A() as a:
#     with B() as b:
#         suite
#
# Note In Python 2.5, the with statement is only allowed when the
# with_statement feature has been enabled. It is always enabled in Python 2.6.

Python 2.6 and 3.0 introduced a new exception-related statementthe with, and
its optional as clause. This statement is designed to work with context
manager objects, which support a new method-based protocol,

In short, the with/as statement is designed to be an `alternative` to a common
`try/ finally` usage idiom; like that statement, with is in large part
intended for specifying termination-time or "cleanup" activities that must run
`regardless of whether an exception occurs` during a processing step.

Unlike try/finally, the with statement is based upon an object `protocol` for
specifying actions to be run around a block of code. This makes `with` less
general, qualifies it as redundant in termination roles, and requires coding
classes for objects that do not support its protocol. 

On the other hand, `with` also handles entry actions, can reduce code size,
   and allows code contexts to be managed with full OOP.

Although this option requires fewer lines of code, it’s applicable only when
processing certain object types, so try/finally is a more general termination
structure, and is often simpler than coding a class in cases where with is not
already supported. On the other hand, with/as may also run startup actions
too, and supports user-defined context management code with access to Python’s
full OOP toolset.

Python enhances some built-in tools with context managers, such as files that
automatically close themselves and thread locks that automatically lock and
unlock, but programmers can code context managers of their own with classes,
  too.


with expression [as variable]:
  with-block

The `expression` here is assumed to return an object that supports the context
management protocol (more on this protocol in a moment). This object may also
return a value that will be assigned to the name `variable` if the optional as
clause is present.

Note that the variable is not necessarily assigned the result of the
expression; the result of the expression is the object that supports the
context protocol, and the variable may be assigned something else intended to
be used inside the statement. The object returned by the expression may then
run `startup code` before the with-block is started, as well as `termination code`
after the block is done, regardless of whether the block raised an exception
or not.

file objects have a context manager that automatically closes the file after
the `with block` regardless of whether an exception is raised, and regardless of
if or when the version of Python running the code may close automatically:

with open(r'C:\misc\data') as myfile:
  for line in myfile:
    print(line)
      ...more code here...

Here, the call to open returns a simple file object that is assigned to the
name myfile. We can use myfile with the usual file tools - in this case, the
file iterator reads line by line in the for loop.

Guarantees that the file object referenced by myfile is automatically closed,
           even if the for loop raised an exception while processing the file.


<know-when-occur>
Although file objects may be automatically closed on garbage collection, it’s
not always straightforward to know when that will occur, especially when using
alternative Python implementations. The with statement in this role is an
alternative that allows us to be sure that the `close will occur` after
execution of a specific block of code.

As we saw earlier, we can achieve a similar effect with the more general and
explicit try/finally statement, but it requires three more lines of
administrative code in this case (four instead of just one):

myfile = open(r'C:\misc\data')
try:
  for line in myfile:
    print(line)
    ...more code here...
finally:
  myfile.close()


<context-management-protocol>
TODO


={============================================================================
|kt_dev_py_0001| py-sys

https://docs.python.org/2/library/sys.html?highlight=sys#module-sys

sys.exit([arg])

    Exit from Python. This is implemented by raising the SystemExit exception,
so cleanup actions specified by finally clauses of try statements are honored,
and it is possible to intercept the exit attempt at an outer level.

    The optional argument arg can be an integer giving the exit status
    (defaulting to zero), or another type of object. If it is an integer, zero
    is considered “successful termination” and any nonzero value is considered
    “abnormal termination” by shells and the like. Most systems require it to
    be in the range 0-127, and produce undefined results otherwise. Some
    systems have a convention for assigning specific meanings to specific exit
    codes, but these are generally underdeveloped; Unix programs generally use
    2 for command line syntax errors and 1 for all other kind of errors. If
    another type of object is passed, None is equivalent to passing zero, and
    any other object is printed to stderr and results in an exit code of 1. In
    particular, sys.exit("some error message") is a quick way to exit a
    program when an error occurs.

    Since exit() ultimately “only” raises an exception, it will only exit the
    process when called from the main thread, and the exception is not
    intercepted.


sys.argv

    The list of command line arguments passed to a Python script. argv[0] is
    the script name (it is operating system dependent whether this is a full
        pathname or not). If the command was executed using the -c command
    line option to the interpreter, argv[0] is set to the string '-c'. If no
    script name was passed to the Python interpreter, argv[0] is the empty
    string.

    To loop over the standard input, or the list of files given on the command
    line, see the fileinput module.

    note:
    argv[0] - len(argv[0]) is 1


sys.stdin
sys.stdout
sys.stderr

File objects corresponding to the interpreter’s standard input, output and
error streams. stdin is used for all interpreter input except for scripts but
including calls to input() and raw_input(). stdout is used for the output of
print and expression statements and for the prompts of input() and
raw_input(). The interpreter’s own prompts and (almost all of) its error
messages go to stderr. 

stdout and stderr needn’t be built-in file objects: any object is acceptable
as long as it has a write() method that takes a string argument. (Changing
    these objects doesn’t affect the standard I/O streams of processes
    executed by os.popen(), os.system() or the exec*() family of functions in
    the os module.)

<faq>
https://stackoverflow.com/questions/3263672/python-the-difference-between-sys-stdout-write-and-print


={============================================================================
|kt_dev_py_0001| py-subprocess

https://docs.python.org/2/library/subprocess.html

17.1. subprocess  Subprocess management

The subprocess module allows you to spawn new processes, connect to their
input/output/error pipes, and obtain their return codes. This module intends
to replace several older modules and functions:


The recommended way to launch subprocesses is to use the following
`convenience` functions. For more advanced use cases when these do not meet
your needs, use the underlying Popen interface.


subprocess.call(args, *, stdin=None, stdout=None, stderr=None, shell=False)

    Run the command described by args. Wait for command to complete, then
    return the `returncode` attribute.

    The arguments shown above are merely the most common ones, described below
    in Frequently Used Arguments (hence the slightly odd notation in the
        abbreviated signature). The full function signature is the same as
    that of the Popen constructor - this functions passes all supplied
    arguments directly through to that interface.

    Examples:

    >>> subprocess.call(["ls", "-l"])
    0

    >>> subprocess.call("exit 1", shell=True)
    1

    Warning
    Using shell=True can be a security hazard. See the warning under
    Frequently Used Arguments for details.

    Note
    Do not use stdout=PIPE or stderr=PIPE with this function as that can
    deadlock based on the child process output volume. Use Popen with the
    communicate() method when you need pipes.


17.1.1.1. Frequently Used Arguments

The arguments that are most commonly needed are:

`args` 
is required for all calls and should be a string, or a `sequence` of program
arguments. Providing a sequence of arguments is generally preferred, as it
allows the module to take care of any required escaping and quoting of
arguments (e.g. to permit spaces in file names). If passing a single string,
either shell must be True (see below) or else the string must simply name the
  program to be executed without specifying any arguments.

`stdin, stdout and stderr` 
specify the executed program’s standard input, standard output and standard
error file handles, respectively. 

Valid values are PIPE, an existing file descriptor (a positive integer), an
existing file object, and None. 

PIPE indicates that a new pipe to the child should be created. With the
default settings of None, no redirection will occur; the child’s file handles
will be inherited from the parent. Additionally, stderr can be STDOUT, which
indicates that the stderr data from the child process should be captured into
the same file handle as for stdout.

When stdout or stderr are pipes and universal_newlines is True then all line
endings will be converted to '\n' as described for the universal newlines 'U'
mode argument to open().

`shell` 
If shell is True, the specified command will be executed through the shell.
This can be useful if you are using Python primarily for the enhanced control
flow it offers over most system shells and still want convenient access to
other shell features such as shell pipes, filename wildcards, environment
variable expansion, and expansion of ~ to a user’s home directory. However,
note that Python itself offers implementations of many shell-like features (in
    particular, glob, fnmatch, os.walk(), os.path.expandvars(),
    os.path.expanduser(), and shutil).


<ex>
# often times, run grep on logs but forget the pattern used later. so want to
# keep the pattern used to make greped output.
# usually, run "egrep -an PATTERN LOGlastrun_realtime_1c75 > 1c75.nds"

#!/usr/bin/env python
import sys
import subprocess

if __name__ == "__main__":
    # execute only if run as a script

    if len(sys.argv) > 3:
        # print error and exit
        # e.g., file is LOGlastrun_realtime_1c75
        sys.stdout.write("mgrep {pattern} {file} \n")
        sys.exit(1)

    # note: on pattern string      
    # when pattern is $ cmd "SS|DD", pattern var gets SS|DD and subproces call
    # works okay.
    pattern = sys.argv[1]
    log_filename = sys.argv[2]
    out_filename = log_filename.split('_')[-1] + '.nds'

    sys.stdout.write('grep -an "%s" %s > %s\n' % (pattern, log_filename, out_filename))

    o = open(out_filename, 'w+')
    o.write('%s\n' % ('='*60))
    o.write('grep -an "%s" %s > %s\n' % (pattern, log_filename, out_filename))
    o.write('%s\n' % ('='*60))

    # note: without this, output file don't have the header above irrespective
    # of mode used.
    # https://stackoverflow.com/questions/10722752/python-subprocess-is-overwriting-file-used-for-stdout-i-need-it-to-append-to-t
    # os.SEEK_END or 2 (seek relative to the file’s end).
    o.seek(0, 2)

    # note: see how link to stdout of a command
    # https://stackoverflow.com/questions/3679974/run-shell-command-with-input-redirections-from-python-2-4
    # egrep -an pattern logfile > out_filename
    subprocess.call(["egrep", "-an", pattern, log_filename], stdout=o)

    o.close()


17.1.1.2. Popen Constructor

The underlying process creation and management in this module is handled by
the Popen class. It offers a lot of flexibility so that developers are able to
handle the less common cases not covered by the convenience functions.

class subprocess.Popen(args, bufsize=0, executable=None, 
    stdin=None, stdout=None, stderr=None, preexec_fn=None, 
    close_fds=False, shell=False, cwd=None, env=None, 
    universal_newlines=False, startupinfo=None, creationflags=0)

`Execute a child program in a new process` On Unix, the class uses
os.execvp()-like behavior to execute the child program. 

`args` should be a `sequence` of program arguments or else a `single string.` By
default, the program to execute is the first item in args if args is a
sequence. If args is a string, the interpretation is platform-dependent and
described below. See the `shell` and executable arguments for additional
differences from the default behavior. Unless otherwise stated, it is
recommended to pass args as a sequence.

On Unix, if args is a string, the string is interpreted as the name or path of
the program to execute. However, this can only be done if not passing
arguments to the program.


<ex>
>>> print args
['/bin/vikings', '-input', 'eggs.txt', '-output', 'spam spam.txt', '-cmd', "echo '$MONEY'"]
>>> p = subprocess.Popen(args) # Success!


On Unix with `shell=True`, the shell defaults to /bin/sh. If args is a string,
the string specifies the command to execute `through the shell`  This means that
  the string must be formatted exactly as it would be when typed at the shell
  prompt. This includes, for example, quoting or backslash escaping filenames
  with spaces in them. If args is a sequence, the first item specifies the
  command string, and any additional items will be treated as additional
  arguments to the shell itself. That is to say, Popen does the equivalent of:

Popen(['/bin/sh', '-c', args[0], args[1], ...])


subprocess.PIPE

    Special value that can be used as the stdin, stdout or stderr argument to
    Popen and indicates that a pipe to the standard stream `should be opened`

stdin, stdout and stderr specify the executed program's standard input,
standard output and standard error file handles, respectively. 
  
Valid values are `PIPE`, an existing file descriptor (a positive integer), an
existing file object, and None. PIPE indicates that a new pipe to the child
should be created. With the default settings of None, no redirection will
occur; the child's file handles will be inherited from the parent.
Additionally, stderr can be STDOUT, which indicates that the stderr data from
the child process should be captured into the same file handle as for stdout.

If close_fds is true, all file descriptors except 0, 1 and 2 will be closed
before the child process is executed. (Unix only).


17.1.2. Popen Objects

Instances of the Popen class have the following methods:

Popen.wait()

    Wait for child process to terminate. Set and return returncode attribute.

    Warning

    This will deadlock when using stdout=PIPE and/or stderr=PIPE and the child
    process generates enough output to a pipe such that it blocks waiting for
    the OS pipe buffer to accept more data. `Use communicate() to avoid that.`

Popen.communicate(input=None)

    Interact with process: Send data to stdin. Read data from stdout and
    stderr, until end-of-file is reached. `Wait for process to terminate` The
    optional input argument should be a string to be sent to the child
    process, or None, if no data should be sent to the child.

    communicate() `returns` a tuple (stdoutdata, stderrdata).

    Note 
    that if you want to send data to the process's stdin, you need to create
    the Popen object with `stdin=PIPE`. Similarly, to get anything other than
    None in the result tuple, you need to give stdout=PIPE and/or stderr=PIPE
    too.

    Note
    The data read is buffered in memory, so do not use this method if the data
    size is large or unlimited.


Popen.poll()

    Check if child process has terminated. Set and return returncode
    attribute.

<ex>
        P = subprocess.Popen (cmd, stdout = subprocess.PIPE,
                stderr = subprocess.PIPE, close_fds=True, bufsize=50000000)

        stdOut, stdErr = P.communicate ()
        retVal = P.wait ()

<ex>
pi@raspberrypi ~ $ /sbin/ifconfig
eth0      Link encap:Ethernet  HWaddr b8:27:eb:11:5f:d6
          inet addr:10.209.60.87  Bcast:10.209.60.255  Mask:255.255.255.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:294459 errors:0 dropped:12617 overruns:0 frame:0
          TX packets:71643 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:191387746 (182.5 MiB)  TX bytes:41340485 (39.4 MiB)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:1755 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1755 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:138052 (134.8 KiB)  TX bytes:138052 (134.8 KiB)


>>> p = subprocess.Popen(["/sbin/ifconfig"], stdout=subprocess.PIPE)
>>> p.communicate()
('eth0      Link encap:Ethernet  HWaddr b8:27:eb:11:5f:d6  \n          inet addr:10.209.60.87  Bcast:10.209.60.255  Mask:255.255.255.0\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:294333 errors:0 dropped:12611 overruns:0 frame:0\n          TX packets:71573 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:191380039 (182.5 MiB)  TX bytes:41333421 (39.4 MiB)\n\nlo        Link encap:Local Loopback  \n          inet addr:127.0.0.1  Mask:255.0.0.0\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:1755 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:1755 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:138052 (134.8 KiB)  TX bytes:138052 (134.8 KiB)\n\n', None)
>>>


={============================================================================
|kt_dev_py_0001| py-shlex

https://docs.python.org/2/library/shlex.html


={============================================================================
|kt_dev_py_0001| py-builtin-function

https://docs.python.org/3/library/functions.html

2. Built-in Functions
The Python interpreter has a number of functions and types built into it that
are always available. They are listed here in alphabetical order.

*eval-keyword-eval*
eval(expression, globals=None, locals=None)

    The arguments are a string and optional globals and locals. If provided,
    globals must be a dictionary. If provided, locals can be any mapping object.

    The expression argument is parsed and evaluated `as a Python expression`
    (technically speaking, a condition list) using the globals and locals
    dictionaries as global and local namespace. If the globals dictionary is
    present and lacks ‘__builtins__’, the current globals are copied into
    globals before expression is parsed. This means that expression normally
    has full access to the standard builtins module and restricted
    environments are propagated. If the locals dictionary is omitted it
    defaults to the globals dictionary. If both dictionaries are omitted, the
    expression is executed in the environment where eval() is called. The
    return value is the result of the evaluated expression. Syntax errors are
    reported as exceptions. Example:

    >>> x = 1
    >>> eval('x+1')
    2

    This function can also be used to execute arbitrary code objects (such as
        those created by compile()). In this case pass a code object instead
    of a string. If the code object has been compiled with 'exec' as the mode
    argument, eval()‘s return value will be None.

    Hints: dynamic execution of statements is supported by the exec()
    function. The globals() and locals() functions returns the current global
    and local dictionary, respectively, which may be useful to pass around for
    use by eval() or exec().

    See ast.literal_eval() for a function that can safely evaluate strings
    with expressions containing only literals.

<py-raw-input>
raw_input([prompt])

If the prompt argument is present, it is written to standard output without a
trailing newline. The function then reads a line from input, converts it to a
string (stripping a trailing newline), and returns that. When EOF is read,
EOFError is raised. Example:

>>> s = raw_input('--> ')
--> Monty Python's Flying Circus
>>> s
"Monty Python's Flying Circus"

If the readline module was loaded, then raw_input() will use it to provide
elaborate line editing and history features.

Version skew note: If you are working in Python 2.X, use raw_input() instead
of input() in this code. The former was renamed to the latter in Python 3.X.
Technically, 2.X has an input function too, but it also evaluates strings as
though they are program code typed into a script, and so will not work in this
context (an empty string is an error). Python 3.X’s input (and 2.X’s
    raw_input) simply returns the entered text as a character string,
unevaluated. To simulate 2.X’s input in 3.X, use eval(input()).

<ex>

// Do the actions look OK? (yes/no/confirm): xx
// Invalid answer!
// Do the actions look OK? (yes/no/confirm): yes
// yes

answer = getAnswer('Do the actions look OK?', ['yes', 'no', 'confirm'])

def getAnser(question, answers):
  repr_answers = ' (%s): ' % '/'.join(answers)
  answer = raw_input(question + repr_answers)
  while answer not in answers:
    print 'Invalid answer!'
    answer = raw_input(question + repr_answers)
  return answer


={============================================================================
|kt_dev_py_0001| py-os

https://docs.python.org/3.4/library/os.path.html

11.2. os.path  Common pathname manipulations


os.path.abspath(path)

    Return a normalized absolutized version of the pathname path. On most
    platforms, this is equivalent to calling the function normpath() as
    follows: normpath(join(os.getcwd(), path)).


os.path.dirname(path)

    Return the directory name of pathname path. This is the first element of
    the pair returned by passing path to the function split().


https://docs.python.org/3.4/library/os.html

16.1.5. Files and Directories


os.listdir(path='.')

    Return a list containing the names of the entries in the directory given
    by path. The list is in arbitrary order, and does not include the special
    entries '.' and '..' even if they are present in the directory.

    path may be either of type str or of type bytes. If path is of type bytes,
    the filenames returned will also be of type bytes; in all other circumstances,
    they will be of type str.

    This function can also support specifying a file descriptor; the file
    descriptor must refer to a directory.

    Note
    To encode str filenames to bytes, use fsencode().

    Availability: Unix, Windows.

    Changed in version 3.2: The path parameter became optional.

    New in version 3.3: Added support for specifying an open file descriptor
    for path.

<ex>
    # Check the folder exists
    if os.path.isdir (dir):
        # Check if the folder is empty
        if os.listdir (dir) == []:


={============================================================================
|kt_dev_py_0001| py-urllib

https://docs.python.org/2/library/urllib2.html

The urllib2 module defines functions and classes which help in opening URLs
(mostly HTTP) in a complex world  basic and digest authentication,
redirections, cookies and more.


<ex>
>>> import urllib2
>>> res = urllib2.urlopen("http://theyard.cisco.com/diagconf.txt")
>>> res
<addinfourl at 3067798252L whose fp = <socket._fileobject object at 0xb6d8266c>>
>>> content = res.read()

# the fils is:
#
# [
#       [["AMS"], ["AMS"]],
#       [["AFLPROXY"],["darwin_aflproxy_bindings_AS3"]],
#       [["DIAG_TIMESTAMP"], ["diag_svr"]],
#       [["CAPTRANS"], ["ms_captrans_src"]],
#       [["CAPTRANS_INPUT"], ["ms_captrans_src"]],
#       [["CAPTRANS_OUTPUT"], ["ms_captrans_src"]],
#       [["CAPTRANS_TTML"], ["ms_captrans_src"]],
#       [["PPCM_CF"], ["ppcm", "ppcm_core"]],
#       [["PPCM_CL"], ["ppcm", "ppcm_core"]],
#       [["PPCM_CORE"], ["ppcm", "ppcm_core"]],
#       ...
#
# returns a single long string
>>> content
'[\n      [["AMS"], ["AMS"]],\n      [["AFLPROXY"],["darwin_aflproxy_bindings_AS3"]],\n      [["DIAG_TIMESTAMP"], ["diag_svr"]],\n      [["CAPTRANS"], ["ms_captrans_src"]],\n      [["CAPTRANS_INPUT"], ["ms_captrans_src"]],\n      [["CAPTRANS_OUTPUT"], ["ms_captrans_src"]],\n      [["CAPTRANS_TTML"], ["ms_captrans_src"]],\n      [["PPCM_CF"], ["ppcm", "ppcm_core"]],\n      [["PPCM_CL"], ["ppcm", "p..'


={============================================================================
|kt_dev_py_0001| py-file

Chapter 9: Tuples, Files, and Everything Else

the built-in `open` function creates a Python `file object`, which serves as a
link to a file residing on your machine. After calling open, you can transfer
strings of data to and from the associated external file by calling the
returned file object's methods.

afile = open(filename, mode)
afile.method()

Without a directory path, the file is assumed to exist in the current working
directory. Use either a relative or absolute file path

path = 'ch13/segismundo.txt'
f = open(path)

By default, the file is opened in read-only mode 'r'. 

<py-open>

https://docs.python.org/2/library/functions.html#open

2. Built-in Functions

open(name[, mode[, buffering]])

Open a file, returning an object of the file type described in section File
Objects. If the file cannot be opened, IOError is raised. When opening a file,
  it’s preferable to use open() instead of invoking the file constructor
    directly.

The first two arguments are the same as for stdio‘s fopen(): 
name is the file name to be opened, and mode is a string indicating how the
  file is to be opened.

The most commonly-used values of mode are 'r' for reading, 'w' for writing
  (truncating the file if it already exists), and 'a' for appending (which on
      some Unix systems means that all writes append to the end of the file
      regardless of the current seek position). If mode is omitted, it
  defaults to 'r'. The default is to use text mode, which may convert '\n'
  characters to a platform-specific representation on writing and back on
  reading. Thus, when opening a binary file, you should append 'b' to the mode
  value to open the file in binary mode, which will improve portability.
  (Appending 'b' is useful even on systems that don’t treat binary and text
   files differently, where it serves as documentation.) See below for more
  possible values of mode.

The optional buffering argument specifies the file’s desired buffer size: 0
means unbuffered, 1 means line buffered, any other positive value means use a
buffer of (approximately) that size (in bytes). A negative buffering means to
use the system default, which is usually line buffered for tty devices and
fully buffered for other files. If omitted, the system default is used. [2]

Modes 'r+', 'w+' and 'a+' open the file for updating (reading and writing);
note that 'w+' truncates the file. Append 'b' to the mode to open the file in
  binary mode, on systems that differentiate between binary and text files; on
  systems that don’t have this distinction, adding the 'b' has no effect.

# from fopen
DESCRIPTION
       The fopen() function opens the file whose name is the string pointed to
       by path and associates a stream with it.

       The argument mode points to a string beginning with one of the
       following sequences (possibly followed by additional characters, as
           described below):

       r      Open text file for reading.  The stream is positioned at the
         beginning of the file.

       r+     Open for reading and writing.  The stream is positioned at the
       beginning of the file.

       w      Truncate file to zero length or create text file for writing.
       The stream is positioned at the beginning of the file.

       w+     Open for reading and writing.  The file is created if it does
       not exist, otherwise it is truncated.  The stream is positioned at the
       beginning of the file.

       a      Open for appending (writing at end of file).  The file is
       created if it does not exist.  The stream is positioned at the end of
       the file.

       a+     Open for reading and appending (writing at end of file).  The
       file is created if it does not exist.  The initial file position for
       reading is at the beginning of the file, but output is always appended
       to the end of the file.


<for-vs-readline>
https://docs.python.org/2/tutorial/inputoutput.html

7.2.1. Methods of File Objects

f.readline() reads a single line from the file; a newline character (\n) is
left at the end of the string, and is only omitted on the last line of the
file if the file doesn't end in a newline. This makes the return value
unambiguous; if f.readline() returns an empty string, the end of the file has
been reached, while a blank line is represented by '\n', a string containing
only a single newline.

>>> f.readline()
'This is the first line of the file.\n'
>>> f.readline()
'Second line of the file\n'
>>> f.readline()
''

*iteration-protocol*
For reading lines from a file, you can loop over the file object. This is
memory efficient, fast, and leads to simple code:

>>> for line in f:
        print line,

This is the first line of the file.
Second line of the file


# Or this way. File iterators read line by line
for line in open('data'): 
  use line 

The lines come out of the file with the end-of-line (EOL) markers intact, so
you'll often see code to get an EOL-free list of lines

lines = [x.rstrip() for x in open(path)]

If you want to read `all the lines` of a file in a list you can also use list(f)
or f.readlines().


To write text to a file, you can use either the file's `write` or `writelines`
methods. With no blank lines like so:

# https://docs.python.org/2/library/stdtypes.html
# 5. Built-in Types
#
# 5.9. File Objects
#
# file.write(str)
# Write a string to the file. There is no return value. Due to buffering, the
# string may not actually show up in the file until the flush() or close()
# method is called.
# 
# file.writelines(sequence) 
# Write a sequence of strings to the file. The sequence can be any iterable
# object producing strings, typically a list of strings. There is no return
# value. (The name is intended to match readlines(); writelines() `does not add`
# line separators.)
#
# file.read([size])
# Read at most size bytes from the file (less if the read hits EOF before
# obtaining size bytes). If the size argument is negative or omitted, read all
# data `until EOF is reached.` The bytes are returned as a string object. An
# empty string is returned when EOF is encountered immediately


# see *with-as-context*
with open('tmp.txt', 'w') as handle:
  handle.writelines(x for x in open(path) if len(x) > 1)

open('tmp.txt').readlines()


As discussed, in Python an object's memory space is automatically reclaimed as
soon as the object is no longer referenced anywhere in the program. When file
objects are reclaimed, Python also automatically closes the files if they are
still open (this also happens when a program shuts down).

`write` methods don't add the end-of-line character for us, so we must include
it to properly terminate our lines

>>> myfile.write('hello text file\n') # Write a line of text: string
>>>
>>> myfile.readline()                 # Read the lines back
'hello text file\n'


Storing Python Objects in Files: Conversions

Have to use other conversion tools to translate from the strings in the text
file to real Python objects.

>>> X, Y, Z = 43, 44, 45 # Native Python objects
>>> S = 'Spam' # Must be strings to store in file
>>> D = {'a': 1, 'b': 2}
>>> L = [1, 2, 3]
>>>
>>> F = open('datafile.txt', 'w') # Create output text file
>>> F.write(S + '\n') # Terminate lines with \n
>>> F.write('%s,%s,%s\n' % (X, Y, Z)) # Convert numbers to strings
>>> F.write(str(L) + '$' + str(D) + '\n') # Convert and separate with $
>>> F.close()

>>> line = F.readline() # Next line from file
>>> line # It's a string here
'43,44,45\n'
>>> parts = line.split(',') # Split (parse) on commas
>>> parts
['43', '44', '45\n']

>>> int(parts[1]) # Convert from string to int
44
>>> numbers = [int(P) for P in parts] # Convert all in list at once
>>> numbers
[43, 44, 45]

*eval-keyword-eval*

>>> line = F.readline()
>>> line
"[1, 2, 3]${'a': 1, 'b': 2}\n"
>>> parts = line.split('$') # Split (parse) on $
>>> parts
['[1, 2, 3]', "{'a': 1, 'b': 2}\n"]
>>> eval(parts[0]) # Convert to any object type
[1, 2, 3]
>>> objects = [eval(P) for P in parts] # Do same for all in list
>>> objects
[[1, 2, 3], {'a': 1, 'b': 2}]


={============================================================================
|kt_dev_py_0001| py-itertools

https://docs.python.org/2/library/itertools.html#itertools.islice

 itertools.islice(iterable, stop)

<ex>
  # reads a file by section_size unit and makes it a list
  log_list = list(islice(log_file, section_size))


={============================================================================
|kt_dev_py_0001| py-cvs

https://docs.python.org/2/library/csv.html

<ex>
import csv

    ret_list = list(csv.reader(open(path), delimiter="\t"))    


={============================================================================
|kt_dev_py_0001| py-re

https://docs.python.org/2/library/re.html

7.2. re  Regular expression operations

This module provides regular expression matching operations similar to those
found in Perl


7.2.1. Regular Expression Syntax

\d
    When the UNICODE flag is not specified, matches any decimal digit; this is
    equivalent to the set [0-9]. With UNICODE, it will match whatever is
    classified as a decimal digit in the Unicode character properties
    database.


7.2.2. Module Contents

The module defines several functions, constants, and an exception. 

Some of the functions are simplified versions of the `full featured methods`
for `compiled` regular expressions. Most non-trivial applications always use
  the compiled form.

re.compile(pattern, flags=0)

    Compile a regular expression `pattern into a regular expression object`,
            which can be used for matching using its match() and search()
              methods, described below.

    The expression's behaviour can be modified by specifying a flags value.
    Values can be any of the following variables, combined using bitwise OR
    (the | operator).

    The sequence

    prog = re.compile(pattern)
    result = prog.match(string)

    is equivalent to

    result = re.match(pattern, string)

    but using re.compile() and saving the resulting regular expression object
    for reuse is `more efficient` when the expression will be used several times
      in a single program.

    Note
    The compiled versions of the most recent patterns passed to re.match(),
        re.search() or re.compile() are cached, so programs that use only a
          few regular expressions at a time needn't worry about compiling
          regular expressions.


re.search(pattern, string, flags=0)

    Scan through string looking for `the first location` where the regular
    expression pattern produces a match, and return a corresponding
    MatchObject instance. 
    
    Return None if no position in the string matches the pattern; note that
    this is different from finding a zero-length match at some point in the
    string.


re.match(pattern, string, flags=0)

    If zero or more characters at the beginning of string match the regular
    expression pattern, return a corresponding MatchObject instance. 
    
    Return None if the string does not match the pattern; note that this is
    different from a zero-length match.

    Note that even in MULTILINE mode, re.match() will only match at the
    beginning of the string and not at the beginning of each line.

    If you want to locate a match anywhere in string, use search() instead
    (see also search() vs. match()).


re.findall(pattern, string, flags=0)

Return all non-overlapping matches of pattern in string, as a list of strings.
The string is scanned left-to-right, and matches are returned in the order
found. If one or more groups are present in the pattern, return a list of
groups; this will be a list of tuples if the pattern has more than one group.
Empty matches are included in the result unless they touch the beginning of
another match.


7.2.5.3. search() vs. match()

Python offers two different primitive operations based on regular expressions:
re.match() checks for a match only at the beginning of the string, while
re.search() checks for a match anywhere in the string (this is what Perl does
    by default).

For example:

>>> re.match("c", "abcdef")    # No match
>>> re.search("c", "abcdef")   # Match
<_sre.SRE_Match object at ...>

Regular expressions beginning with '^' can be used with search() to restrict
the match at the beginning of the string:

>>> re.match("c", "abcdef")    # No match
>>> re.search("^c", "abcdef")  # No match
>>> re.search("^a", "abcdef")  # Match
<_sre.SRE_Match object at ...>

Note however that in MULTILINE mode match() only matches at the beginning of
the string, whereas using search() with a regular expression beginning with
'^' will match at the beginning of each line.

>>> re.match('X', 'A\nB\nX', re.MULTILINE)    # No match
>>> re.search('^X', 'A\nB\nX', re.MULTILINE)  # Match
<_sre.SRE_Match object at ...>


re.split(pattern, string, maxsplit=0, flags=0)

    Split string by the occurrences of pattern. 

    If capturing parentheses are used in pattern, then the text of all groups
    in the pattern are also returned as part of the resulting list. 
    
    If maxsplit is nonzero, at most maxsplit splits occur, and the remainder
    of the string is returned as the final element of the list.
    (Incompatibility note: in the original Python 1.5 release, maxsplit was
     ignored. This has been fixed in later releases.)

>>> re.split('[/:]', '/usr/home/lumberjack')
['', 'usr', 'home', 'lumberjack']

>>> re.split('\W+', 'Words, words, words.')
['Words', 'words', 'words', '']



7.2.4. Match Objects

group([group1, ...])

    Returns one or more subgroups of the match.

    Without arguments, group1 defaults to zero (the whole match is returned).
    If a groupN argument is zero, the corresponding return value is the entire
    matching string; if it is in the inclusive range [1..99], it is the string
    matching the corresponding parenthesized group.

    If the regular expression uses the (?P<name>...) syntax, the groupN
    arguments may `also be strings identifying groups by their group name` If a
    string argument is not used as a group name in the pattern, an IndexError
    exception is raised.

groups([default])

    Return a tuple containing all the subgroups of the match, from 1 up to
    however many groups are in the pattern.

>>> match = re.match('Hello[ \t]*(.*)world', 'Hello Python world')
>>> match.group(0)
'Hello Python world'
>>> match.group(1)
'Python '

>>> match = re.match('[/:](.*)[/:](.*)[/:](.*)', '/usr/home:lumberjack')
>>> match.groups()
('usr', 'home', 'lumberjack')

>>> m = re.match(r"(?P<first_name>\w+) (?P<last_name>\w+)", "Malcolm Reynolds")
>>> m.group('first_name')
'Malcolm'
>>> m.group('last_name')
'Reynolds'

import re
f='drx890.SYSF40.73.00.enc.snugupdate'
m=re.match(r'^(?P<type>.+)\.SYSF(?P<version>[\d.]+)', f)
print 'DEBUG: f: %s, match.group(type): %s, m.group(version): %s' % (f, m.group('type'), m.group('version'))
DEBUG: f: drx890.SYSF40.73.00.enc.snugupdate, match.group(type): drx890, m.group(version): 40.73.00.


groupdict([default])

    Return a dictionary containing all the named subgroups of the match, keyed
    by the subgroup name. The default argument is used for groups that did not
    participate in the match; it defaults to None. For example:

    >>> m = re.match(r"(?P<first_name>\w+) (?P<last_name>\w+)", "Malcolm Reynolds")
    >>> m.groupdict()
    {'first_name': 'Malcolm', 'last_name': 'Reynolds'}


<ex>
  # see multiple regex
  def _getVersionNumber(self, value):
      regexes = (r'(?P<model_release>R\d+)\.(?P<version>\d+\.\d+\.\d+)',
                 r'(?P<version>\d+\.\d+\.\d+)_\d+_(?P<model_release>WALRUS-ENG-\d+)' )
      for r in regexes:
        result = re.match(r, value)
        if result is not None:
          return result.groupdict()
      raise Exception('Could not parse model number %s' % value)

<ex>
# Sample line:
# NDS: ^0946684966.710878 !ERROR -SRM          < p:000000c1 t:016121b0 T:SRM_LP_THREAD M:srm_utils.c F:SRM_SystemStringToCString L:00287 > **** SRM ERR in srm_utils.c:287
#
# currently supports match = {1, 2, 3, 4, 5}
#       error type such as FATAL, ERROR, WARN, MIL and which is GROUP(1)
#       component type such as SRM and which is GROUP(2)
#       filename which is GROUP(3)
#       func which is GROUP(4)
#       free text which is GROUP(5)

# \s : any whitespace char
# \S : any not-whilespace char
# '?': causes the resulting RE to match 0 or 1 repetitions of the preceding RE. ab? will match either 'a' or 'ab'.

# NOTE. there's no support for a leading numbers of the dict line.
# NOTE. added MIL just for usefulness.
# NOTE. possibly add 'T:' group?

# extract interested groups from a dict entry. grab the whole free text.
matchDic = re.search(r'^NDS:.*!(FATAL|ERROR|WARN|MIL)\s+-(\S+).*(M:\S+)\s+(F:\S+).*>(.*)', lineDic )


={============================================================================
|kt_dev_py_0001| py-pip

https://pip.pypa.io/en/stable/

Quickstart

Install a package from PyPI:

$ pip install SomePackage
  [...]
  Successfully installed SomePackage

https://packaging.python.org/glossary/#term-python-package-index-pypi
Python Package Index (PyPI)
    PyPI is the default Package Index for the Python community. It is open to
    all Python developers to consume and distribute their distributions.

To search package:
https://pypi.python.org/pypi/

<to-list>
https://pip.pypa.io/en/stable/reference/pip_freeze/

pi@raspberrypi ~/snugupdate-v2-snugberrypi $ pip freeze
Warning: cannot find svn location for distribute==0.6.24dev-r0
PAM==0.4.2
RPi.GPIO==0.5.11
Twisted==12.0.0
Twisted-Conch==12.0.0
Twisted-Core==12.0.0
Twisted-Lore==12.0.0
Twisted-Mail==12.0.0
Twisted-Names==12.0.0
Twisted-News==12.0.0
Twisted-Runner==12.0.0
Twisted-Web==12.0.0
Twisted-Words==12.0.0
argparse==1.2.1
cffi==1.1.2
cryptography==0.9.1
## FIXME: could not find svn URL in dependency_links for this package:
distribute==0.6.24dev-r0
dropbox==2.2.0

<to-upgrade>
https://pip.pypa.io/en/stable/reference/pip_install/

-U, --upgrade
Upgrade all specified packages to the newest available version. The handling
of dependencies depends on the upgrade-strategy used.

3. Upgrade an already installed SomePackage to the latest from PyPI.

$ pip install --upgrade SomePackage


={============================================================================
|kt_dev_py_0001| py-dropbox

https://www.dropbox.com/developers/documentation/python#install
Install Dropbox for Python

To get started with Dropbox for Python, we recommend you add the SDK to your
project using pip.

Download and install the SDK.

pip install dropbox

Now you can do "import dropbox" in your Python app, or in a Python interpreter.

import dropbox

That's it! Now you're ready to get started with the tutorial.


https://www.dropbox.com/developers/documentation/python#tutorial
Link an account

In order to make calls to the API, you'll need an instance of the Dropbox
object. To instantiate, pass in the access token for the account you want to
link. (Tip: You can generate an access token for your own account through the
    App Console).

dbx = dropbox.Dropbox('YOUR_ACCESS_TOKEN')

Test it out to make sure you've linked the right account:

dbx.users_get_current_account()


<core-api>
https://www.dropbox.com/developers-v1/core/start/python
Warning: API v1 has been deprecated. Learn more.

Using the Core API in Python

The Core API is based on HTTP and OAuth and provides low-level calls to access
and manipulate a user's Dropbox account.

If you want to follow along, first register a new app on the App Console.
You'll need the app key to access the Core API. Then install the Python SDK
and you'll be ready to go.


Downloading files

Some time has passed and you're ready to start editing that magnum opus of
yours again. We'll need the get_file_and_metadata method to download the file.

f, metadata = client.get_file_and_metadata('/magnum-opus.txt')
out = open('magnum-opus.txt', 'wb')
out.write(f.read())
out.close()
print metadata

get_file_and_metadata, like other calls that return file data, returns an
httplib.HTTPResponse that you should .read() from to get the full response.
https://www.dropbox.com/developers-v1/core/docs/python#DropboxClient.get_file_and_metadata

https://github.com/andreafabrizi/Dropbox-Uploader
# creates si-pi-prepare-deploy-roll under db:snugupdate-v2-snugberrypi-roll/
/home/kyoupark/si/si-pi-prepare-deploy-roll$ /home/kyoupark/Dropbox-Uploader/dropbox_uploader.sh upload . /snugberrypi/snugupdate-v2-snugberrypi-roll

# works
kyoupark@kit-debian64:~/si/snugupdate-v2-snugberrypi-roll$ /home/kyoupark/Dropbox-Uploader/dropbox_uploader.sh upload . /snugberrypi/


={============================================================================
|kt_dev_py_0001| py-imaplib


={============================================================================
|kt_dev_py_0001| py-time

https://docs.python.org/2/library/time.html

This module provides various time-related functions. For related
functionality, see also the datetime and calendar modules.

Although this module is always available, not all functions are available on
all platforms. Most of the functions defined in this module call platform C
library functions with the same name. It may sometimes be helpful to consult
the platform documentation, because the semantics of these functions varies
among platforms.

An explanation of some terminology and conventions is in order.

time.time()

    Return the time in seconds since the epoch as a floating point number.
    Note that even though the time is always returned as a floating point
    number, not all systems provide time with a better precision than 1
    second. While this function normally returns non-decreasing values, it can
    return a lower value than a previous call if the system clock has been set
    back between the two calls.


={============================================================================
|kt_dev_py_0001| py-time-benchmark

Learning Python 5E, 21, The Benchmarking Interlude

The range call is hoisted out of the timing loop in the total function, so its
construction cost is not charged to the timed function in Python 2.X. In 3.X
range is an iterable, so this step is neither required nor harmful, but we
still run the result through list so its traversal cost is the same in both
2.X and 3.X. This doesn’t apply to the bestof function, since no range factors
are charged to the test’s time.


<ex>
#!/usr/bin/env python
"""
homegrown timing tools for function calls.
does total time, best-of time, and best-of-totals time
"""

import time, sys

# >>> sys.platform
# 'win32'

timer = time.clock if sys.platform[:3] == 'win' else time.time

def total( reps, func, *pargs, **kargs):
    """
    total time to run func() reps times.
    returns( total time, last result )
    """
    repslist = list(range(reps))
    start = timer()
    for i in repslist:
        ret = func(*pargs, **kargs)

    elapsed = timer() - start
    return (elapsed, ret)


def bestof( reps, func, *pargs, **kargs):
    """
    quickest func() among reps runs.
    returns( best time, last result )
    """
    best = 2**32

    # range usuage not timed here
    for i in range(reps):
        start = timer()
        ret = func(*pargs, **kargs)
        elapsed = timer()-start
        if elapsed < best: best = elapsed
    return (best, ret)


def bestoftotal( rep1, rep2, func, *pargs, **kargs):
    """
    best of totals:
    (best of rep1 runs of (total of rep2 runs of func))
    """
    return bestof( rep1, total, rep2, func, *pargs, **kargs)


if __name__ == "__main__":
    ret = total( 1000, pow, 2, 1000)
    print 'time taken to pow is %f' % (ret[0])
    ret = bestof( 1000, pow, 2, 1000)
    print 'time taken to best of pow is %f' % (ret[0])
    ret = bestoftotal( 50, 1000, pow, 2, 1000)
    print 'time taken to bestoftotal of pow is %f' % (ret[0])


See the way we use these and the both do the same and return the best-of
tuple, which embeds the last total call’s result tuple.

>>> timer.bestof(50, timer.total, 1000, str.upper, 'spam')
(0.0005468751145372153, (0.0005004469323637295, 'SPAM'))

>>> timer.bestoftotal(50, 1000, str.upper, 'spam')
(0.000566912540591602, (0.0005195069228989269, 'SPAM'))

note: that is, bestoftotal limits itself to use total. 


#!/usr/bin/env python
"Test the relative speed of iteration tool alternatives."

import sys, timer

reps = 10000

replist = list(range(reps))

# for loop
def forLoop():
    res = []
    for x in replist:
        res.append(abs(x))
    return res

# list comprehension
def listComp():
    return [abs(x) for x in replist]

# map call
def mapCall():
    return map(abs, replist)

# generator expression
def genExpr():
    return list(abs(x) for x in replist)

# generator function
def genFunc():
    def gen():
        for x in replist:
            yield abs(x)
    return list(gen())


print(sys.version)
for test in (forLoop, listComp, mapCall, genExpr, genFunc):
    (bestof, (total, result)) = timer.bestoftotal(5, 1000, test)
    print('%-9s: %.5f => [%s...%s]' %
            (test.__name__, bestof, result[0], result[-1]))


kit-debian64:~/works$ ./timeseqs.py 
2.7.9 (default, Jun 29 2016, 13:08:31) 
[GCC 4.9.2]
forLoop  : 0.85904 => [0...9999]
listComp : 0.53057 => [0...9999]
mapCall  : 0.32763 => [0...9999]
genExpr  : 0.71238 => [0...9999]
genFunc  : 0.72132 => [0...9999]

note: see that list comprehension and map is better than for loop.


<py-timeit>
the standard library also ships with a module named timeit that can be used in
similar ways, but offers added flexibility and may better insulate clients
from some platform differences.

With timeit, tests are specified by either callable objects or statement
strings; the latter can hold multiple statements if they use ; separators or
\n characters for line breaks, and spaces or tabs to indent statements in
nested blocks (e.g., \n\t).

# timeit.repeat(stmt=’pass’, setup=’pass’, timer=<default timer>, repeat=3, number=1000000)
# 
# Create a Timer instance with the given statement, setup code and timer
# function and run its repeat() method with the given repeat count and number
# executions.
# 
# New in version 2.6.

# use karg version
>>> timer.bestoftotal(pow, 2, 1000, _reps1=50, _reps=1000)[0]
0.0015938282012939453


>>> import timeit
>>> min(timeit.repeat(stmt="pow(2, 1000)", number=1000, repeat=50))
0.0016210079193115234


={============================================================================
|kt_dev_py_0001| py-cvs

The standard library of Python provides a CSV reader. The Python script below
completes this goal:

#!/usr/bin/env python
# CSV module that comes with the Python standard library
import csv
import sys

if __name__ == "__main__":
    # The CSV module exposes a reader object that takes
    # a file object to read. In this example, sys.stdin.
    csvfile = csv.reader(sys.stdin)

    # The script should take one argument that is a column number.
    # Command-line arguments are accessed via sys.argv list.
    column_number = 0
    if len(sys.argv) > 1:
            column_number = int(sys.argv[1])

    # Each row in the CSV file is a list with each 
    # comma-separated value for that line.
    for row in csvfile:
            print row[column_number]


={============================================================================
|kt_dev_py_0001| py-stmplib

#!/usr/bin/env python
import smtplib
import sys


GMAIL_SMTP_SERVER = "smtp.gmail.com"
GMAIL_SMTP_PORT = 587

GMAIL_EMAIL = "Your Gmail Email Goes Here"
GMAIL_PASSWORD = "Your Gmail Password Goes Here"


def initialize_smtp_server():
    '''
    This function initializes and greets the smtp server.
    It logs in using the provided credentials and returns 
    the smtp server object as a result.
    '''
    smtpserver = smtplib.SMTP(GMAIL_SMTP_SERVER, GMAIL_SMTP_PORT)
    smtpserver.ehlo()
    smtpserver.starttls()
    smtpserver.ehlo()
    smtpserver.login(GMAIL_EMAIL, GMAIL_PASSWORD)
    return smtpserver


def send_thank_you_mail(email):
    to_email = email
    from_email = GMAIL_EMAIL
    subj = "Thanks for being an active commenter"
    # The header consists of the To and From and Subject lines
    # separated using a newline character
    header = "To:%s\nFrom:%s\nSubject:%s \n" % (to_email,
            from_email, subj)
    # Hard-coded templates are not best practice.
    msg_body = """
    Hi %s,

    Thank you very much for your repeated comments on our service.
    The interaction is much appreciated.

    Thank You.""" % email
    content = header + "\n" + msg_body

    smtpserver = initialize_smtp_server()
    smtpserver.sendmail(from_email, to_email, content)
    smtpserver.close()


if __name__ == "__main__":
    # for every line of input.
    for email in sys.stdin.readlines():
            send_thank_you_mail(email)


={============================================================================
|kt_dev_py_0001| py-parser-option

Thankfully, Python has a number of modules to deal with command-line arguments.
My personal favorite is OptionParser. OptionParser is part of the optparse
module that is provided by the standard library.

if __name__ == "__main__":
    usage = "usage: %prog [options]"
    parser = OptionParser(usage=usage)
    parser.add_option("--email", dest="email",
            help="email to login to smtp server")
    parser.add_option("--pwd", dest="pwd",
            help="password to login to smtp server")
    parser.add_option("--smtp-server", dest="smtpserver",
            help="smtp server url", default="smtp.gmail.com")
    parser.add_option("--smtp-port", dest="smtpserverport",
            help="smtp server port", default=587)
    options, args = parser.parse_args()

    if not (options.email or options.pwd):
            parser.error("Must provide both an email and a password")

    smtpserver = initialize_smtp_server(options.stmpserver,
            options.smtpserverport, options.email, options.pwd)

    # for every line of input.
    for email in sys.stdin.readlines():
            send_thank_you_mail(email, smtpserver)
    smtpserver.close()


={============================================================================
|kt_dev_py_0001| py-xml

import xml.dom.minidom


={============================================================================
|kt_dev_py_0001| py-logging

https://docs.python.org/2/library/logging.html?highlight=logging

logging.getLogger([name])

Return a logger with the specified name or, if no name is specified, return a
logger which is the `root logger` of the hierarchy. If specified, the name is
typically a dot-separated hierarchical name like “a”, “a.b” or “a.b.c.d”.
Choice of these names is entirely up to the developer who is using logging.

All calls to this function with a given name return the same logger instance.
This means that logger instances never need to be passed between different
parts of an application.

note: if set settings such as handler or formatter to root logger then all
children logger will use the same setting. However, settings in a chind take
priority over root logger.


<basic-config>

logging.basicConfig([**kwargs])

Does basic configuration for the logging system by creating a StreamHandler
with a default Formatter and adding it to the root logger. The functions
debug(), info(), warning(), error() and critical() will call basicConfig()
automatically if no handlers are defined for the root logger.

This function does nothing if the root logger already has handlers configured
for it.

Note This function should be called from the main thread before other threads
  are started. In versions of Python prior to 2.7.1 and 3.2, if this function
  is called from multiple threads, it is possible (in rare circumstances) that
  a handler will be added to the root logger more than once, leading to
  unexpected results such as messages being duplicated in the log.  The
  following keyword arguments are supported.

Format	Description

filename	
Specifies that a FileHandler be created, using the specified filename, rather
than a StreamHandler.

filemode	
Specifies the mode to open the file, if filename is specified (if filemode is
    unspecified, it defaults to ‘a’).

format	
Use the specified format string for the handler.

datefmt	
Use the specified date/time format.

level	
Set the root logger level to the specified level.

stream	
Use the specified stream to initialize the StreamHandler. Note that this
argument is incompatible with ‘filename’ - if both are present, ‘stream’ is
ignored.

    logging.basicConfig (filename="test.log", level = logging.DEBUG, \
            format = '%(asctime)s-%(levelname)s-%(message)s')

logging.disable(lvl)

Provides an overriding level lvl for all loggers which takes precedence over
the logger’s own level. When the need arises to temporarily throttle logging
output down across the whole application, this function can be useful. Its
effect is to disable all logging calls of severity lvl `and below,` so that if
you call it with a value of INFO, then all INFO and DEBUG events would be
discarded, whereas those of severity WARNING and above would be processed
according to the logger’s effective level. If logging.disable(logging.NOTSET)
is called, it effectively removes this overriding level, so that logging
output again depends on the effective levels of individual loggers.


<ex>
    logging.basicConfig(level=logging.DEBUG, format='[%(levelname)-7s] (%(threadName)-10s) %(message)s')
    # Disable this level and all above, i.e. logging.INFO disables INFO & DEBUG
    # Use logging.NOTSET to get all as per level set during initialisation.
    # logging.disable(logging.INFO)
    logging.info('logging info')
    logging.debug('logging debug')
    logging.warn('logging warn)')

# when use logging.disable(logging.INFO)
kyoupark@kit-debian64:~/works$ ./py-log.py
[WARNING] (MainThread) logging warn)

kyoupark@kit-debian64:~/works$ ./py-log.py
[INFO   ] (MainThread) logging info
[DEBUG  ] (MainThread) logging debug
[WARNING] (MainThread) logging warn)


<ex>
#!/usr/bin/env python

import logging

if __name__ == '__main__':

    # create own logger
    ml = logging.getLogger("ml")
    ml.setLevel(logging.INFO)

    # create formatter
    # 2017-08-07 20:41:44,288 - ml - INFO - server started.
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # add handler to console
    stream_handler = logging.StreamHandler()
    # set formatter
    stream_handler.setFormatter(formatter)

    ml.addHandler(stream_handler)

    # add handler to file
    file_handler = logging.FileHandler('ml.log')
    ml.addHandler(file_handler)

    ml.info("server started.")


<set-logger-from-file>

# logging.json
{
  "version": 1,
  "formatters": {
    "simple": {
      "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    }
  },

  "handlers": {
    "console": {
      "class": "logging.StreamHandler",
      "level": "INFO",
      "formatter": "simple",
      "stream": "ext://sys.stdout"
    },

    "info_file_handler": {
      "class": "logging.FileHandler",
      "level": "DEBUG",
      "formatter": "simple",
      "filename": "info.log"
    }
  },

  # for specific logger
  "loggers": {
    "my_module": {
      "level": "ERROR",
      "handlers": ["console"],
      "propagate": "no"
    }
  },

  "root": {
    "level": "DEBUG",
    "handlers": ["console", "info_file_handler"]
  }
}


#!/usr/bin/env python

import logging
import logging.config
import json

if __name__ == '__main__':

    with open('logging.json', 'rt') as f:
        config = json.load(f)

    logging.config.dictConfig(config)

    # get root logger
    ml = logging.getLogger()

    ml.info("info, server started.")
    ml.debug("debug, server started.")


$ ./py-log.py 
2017-08-07 21:01:52,990 - root - INFO - info, server started.

$ cat info.log 
2017-08-07 21:01:52,990 - root - INFO - info, server started.
2017-08-07 21:01:52,998 - root - DEBUG - debug, server started.


<set-adapter>
To add context in the logging.

#!/usr/bin/env python

import logging
import logging.config
import json

class LoggerAdapter(logging.LoggerAdapter):
    def __init__(self, prefix, logger):
        super(LoggerAdapter, self).__init__(logger, {})
        self.prefix = prefix

    def process(self, msg, kwargs):
        return '[%s] %s' % (self.prefix, msg), kwargs

if __name__ == '__main__':

    with open('logging.json', 'rt') as f:
        config = json.load(f)

    logging.config.dictConfig(config)

    # get root logger
    ml = logging.getLogger()

    # set adapter
    ml = LoggerAdapter("SAS", ml)

    ml.info("info, server started.")
    ml.debug("debug, server started.")


$ ./py-log.py 
2017-08-07 21:11:25,549 - root - INFO - [SAS] info, server started.


<custom-logging-level>

15.7.2. Logging Levels

The numeric values of logging levels are given in the following table. These
are primarily of interest if you want to define your own levels, and need them
to have specific values relative to the predefined levels. If you define a
level with the same numeric value, it overwrites the predefined value; the
predefined name is lost.

CRITICAL  50
ERROR     40
WARNING   30
INFO      20
DEBUG     10
NOTSET    0

logging.addLevelName(15, "DATA")
logging.DATA = 15

logger.log(logging.DATA, "message")


<use-q-with-logging>

http://hamait.tistory.com/880


={============================================================================
|kt_dev_py_0001| py-error-encoding

  File "./mgrep-latin.py", line 27
SyntaxError: Non-ASCII character '\xbf' in file ./mgrep-latin.py on line 27, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details

what's the problem since I cannot see that char in the file? In vim, cannot
see that char since vim encoding mode is utf-8. see odd char when change
encoding to latin1.

The new python syntax introduced in the above link works as said or can use
encoding.


={============================================================================
|kt_dev_py_0001| py-six

https://pythonhosted.org/six/#

Six: Python 2 and 3 Compatibility Library

Six provides simple utilities for wrapping over differences between Python 2
and Python 3. It is intended to support codebases that work on both Python 2
and 3 without modification. six consists of only one Python file, so it is
painless to copy into a project.

Six can be downloaded on PyPi. Its bug tracker and code hosting is on
BitBucket.

The name, “six”, comes from the fact that 2*3 equals 6. Why not addition?
Multiplication is more powerful, and, anyway, “five” has already been snatched
away by the (admittedly now moribund) Zope Five project.

Renamed modules and attributes compatibility

Python 3 reorganized the standard library and moved several functions to
different modules. Six provides a consistent interface to them through the
fake six.moves module. For example, to load the module for parsing HTML on
Python 2 or 3, write:

The urllib, urllib2, and urlparse modules have been combined in the urllib
package in Python 3. The six.moves.urllib package is a version-independent
location for this functionality; its structure mimics the structure of the
Python 3 urllib package.


// before, 2.7 code
def getAnswer(question, answers):
  repr_answers = ' (%s): ' % '/'.join(answers)
  answer = raw_input(question + repr_answers)
  while answer not in answers:
    print 'Invalid answer!'
    answer = raw_input(question + repr_answers)
  return answer
    

// after, 2.7 code
import six

def get_answer(question, answers):
    repr_answers = ' (%s): ' % '/'.join(answers)
    answer = six.moves.input(question + repr_answers)
    while answer not in answers:
        print('Invalid answer!')
        answer = six.moves.input(question + repr_answers)
    return answer


={============================================================================
|kt_dev_py_0001| py-telnet

https://docs.python.org/2/library/telnetlib.html

Telnet.read_until(expected[, timeout])

Read until a given string, expected, is encountered or until timeout seconds
have passed.

When no match is found, return whatever is available instead, possibly the
empty string. Raise EOFError if the connection is closed and no cooked data is
available.

note: `expected` can be a substring to search.

<ex>
    def openTelnetConnection(self):
        tn = telnetlib.Telnet(self.ip, 23, 30)
        regexs = [r'login: ', r'sh-\d\.\d# ']
        result = tn.expect(regexs, 30)

        if result[0] == -1:
            raise Exception('Could not log into the STB %s', self.ip)
        elif result[0] == 0:
            tn.write('root\n')
            tn.read_until('# ')
            return tn
        else:
            return tn

tn = telnetlib.Telnet('10.209.60.98', 23, 30)
regexs = [r'login: ', r'sh-\d\.\d# ']
result = tn.expect(regexs, 30)
tn.write('root\n')
tn.read_until('# ')

note:
1. if there is no '\n' in write() such as tn.write('ls -al'), it blocks
forever.

2. read_until() returns all up to the match.


tn.write('ls -al\n')
output = tn.read_until('# ')
print output

// -sh-3.2# ls -al
// drwxrwxrwx    2 root     root            0 Jan  1 02:22 .
// drwxr-xr-x   19 root     root            0 Jan  1 00:00 ..
// -rw-------    1 root     root           11 Jan  1 02:22 .bash_history
// -sh-3.2#
// 
// >>> print output
// ls -al
// drwxrwxrwx    2 root     root            0 Jan  1 02:22 .
// drwxr-xr-x   19 root     root            0 Jan  1 00:00 ..
// -rw-------    1 root     root           11 Jan  1 02:22 .bash_history
// -sh-3.2#
// >>>

tn.write('pwd\n')
output = tn.read_until('# ')
print output

// -sh-3.2# pwd
// /root
// -sh-3.2#
// 
// >>> print output
// pwd
// /root
// -sh-3.2#
// >>>


={============================================================================
|kt_dev_py_0001| py-flask-doc

http://flask-docs-kr.readthedocs.io/ko/latest/index.html
http://flask.pocoo.org/docs/0.12/
https://www.fullstackpython.com/flask.html


={============================================================================
|kt_dev_py_0001| py-flask-apache

https://code.google.com/archive/p/modwsgi/

What Is mod_wsgi?

The aim of mod_wsgi is to implement a simple to use Apache module which can
host any Python application which supports the Python WSGI interface. The
module would be suitable for use in hosting high performance production web
sites, as well as your average self managed personal sites running on web
hosting services.

http://wsgi.readthedocs.io/en/latest/what.html
What is WSGI?

WSGI is the Web Server Gateway Interface. It is a specification that describes
how a web server communicates with web applications, and how web applications
can be chained together to process one request.

WSGI is a Python standard described in detail in PEP 3333.

For more, see Learn about WSGI.



Configuring apache to use with Flask[edit]

This procedure is described in the following page: mod_wsgi(Apache).

# http://flask.pocoo.org/docs/0.11/deploying/mod_wsgi/
# 
# mod_wsgi (Apache)
# 
# If you are using the Apache webserver, consider using mod_wsgi.
# 
# Watch Out
# 
# Please make sure in advance that any app.run() calls you might have in your
# application file are inside an if __name__ == '__main__': block or moved to a
# separate file. Just make sure it’s not called because this will always start a
# local WSGI server which we do not want if we deploy that application to
# mod_wsgi.

After we have installed apache2 we also need to install mod_wsgi:

apt-get install libapache2-mod-wsgi

Create a wsgi file. If we wanted to install our application in
/var/www/html/manatee the file will look something like (file saved in
    repository):

/ETHAN_SI_SCRIPTS/manatee/manatee.wsgi

import sys
sys.path.insert(0, '/var/www/html/ESS/manatee')

print sys.path

from manatee import app as application


Create an Apache configuration file. I added one sample in the repository at
manatee/conf/manatee.conf.

/ETHAN_SI_SCRIPTS/manatee/conf/manatee.conf

$ more manatee.conf
<VirtualHost *:80>

    LoadModule wsgi_module /usr/lib/apache2/modules/mod_wsgi.so

#ServerName manatee.com

    DocumentRoot /var/www/html/
    WSGIDaemonProcess manatee user=www-data group=www-data threads=5
    WSGIScriptAlias / /var/www/html/manatee/manatee.wsgi
    Alias /static /var/www/html/manatee/static

    <Directory /var/www/html/manatee>
        WSGIProcessGroup manatee
        WSGIApplicationGroup %{GLOBAL}
        # Allow bigger files to upload
        LimitRequestBody 41943040
        Order deny,allow
        Allow from all
    </Directory>
</VirtualHost>

Copy the configuration file to /etc/apache2/sites-available. Then go to
/etc/apache2/sites-enables and replace the configuration:

 ln -s ../sites-available/manatee.conf
 rm 000-default.conf


/etc/apache2/sites-enabled$ ls -al
lrwxrwxrwx 1 root root   31 Aug  9 09:57 manatee.conf -> ../sites-available/manatee.conf

Finally restart the apache server:

service apache2 restart


kyoupark@kit-debian64:~/si/ETHAN_SI_SCRIPTS$ WORKSPACE=/home/kyoupark/si/ETHAN_SI_SCRIPTS/WS python manatee/manatee.py
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)


={============================================================================
|kt_dev_py_0001| py-flask

Part I: Hello, World!
https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world

<create-virtualenv>
create a virtual environment where everything gets installed so that your main
Python installation is not affected. As an added benefit, you won't need root
access to do the installation in this way.

https://pypi.python.org/pypi/virtualenv

sudo pip install virtualenv

the command that creates a virtual environment is the following:

$ virtualenv flask

# kyoupark@kit-debian64:~$ virtualenv flask
# New python executable in /home/kyoupark/flask/bin/python
# Installing setuptools, pip, wheel...done.

Virtual environments can be activated and deactivated, if desired. An
activated environment adds the location of its bin folder to the system path,
so that for example, when you type python you get the environment's version
  and not the system's one. But activating a virtual environment is not
  necessary, `it is equally effective to invoke the interpreter by` specifying
  its pathname.

install flask and extensions by entering the following commands, one after
another:

flask/bin/pip install flask
flask/bin/pip install flask-login
flask/bin/pip install flask-openid
flask/bin/pip install flask-mail
flask/bin/pip install flask-sqlalchemy
flask/bin/pip install sqlalchemy-migrate
flask/bin/pip install flask-whooshalchemy
flask/bin/pip install flask-wtf
flask/bin/pip install flask-babel
flask/bin/pip install guess_language
flask/bin/pip install flipflop
flask/bin/pip install coverage


"Hello, World" in Flask

You now have a flask sub-folder inside your microblog folder that is populated
with a Python interpreter and the Flask framework and extensions that we will
use for this application. Now it's time to write our first web application!

After you cd to the microblog folder, let's create the basic folder structure
for our application:

mkdir app
mkdir app/static
mkdir app/templates
mkdir tmp

The app folder will be where we will put our application package. The static
sub-folder is where we will store static files like images, javascripts, and
cascading style sheets. The templates sub-folder is obviously where our
templates will go.

Let's start by creating a simple init script for our app package (file
    app/__init__.py):

from flask import Flask

# app variable
app = Flask(__name__)

# app module
from app import views

The script above simply creates the application object (of class Flask) and
then imports the views module, which we haven't written yet. Do not confuse
app the variable (which gets assigned the Flask instance) with app the package
(from which we import the views module).

If you are wondering why the import statement is at the end and not at the
beginning of the script as it is always done, the reason is to avoid circular
references, because you are going to see that the views module needs to import
the `app variable` defined in this script. Putting the import at the end avoids
the circular import error.

<handlers>
The views are the handlers that respond to requests from web browsers or other
clients. In Flask handlers are written as Python functions. Each view function
is mapped to one or more request URLs.

Let's write our first view function (file app/views.py):

from app import app

@app.route('/')
@app.route('/index')
def index():
    return "Hello, World!"

This view is actually pretty simple, it just returns a string, to be displayed
  on the client's web browser. The two route decorators above the function
  create the mappings from URLs / and /index to this function.


The final step to have a fully working web application is to create a script
that starts up the development web server with our application. Let's call
this script run.py, and put it in the root folder:

#!flask/bin/python
from app import app
app.run(debug=True)

The script simply imports the app variable from our app package and invokes
its run method to start the server. Remember that the app variable holds the
Flask instance that we created it above.

To start the app you just run this script. On OS X, Linux and Cygwin you have
to indicate that this is an executable file before you can run it:

$ chmod a+x run.py

Then the script can simply be executed as follows:

./run.py

kyoupark@kit-debian64:~/mblog$ ./run.py 
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 204-100-668

127.0.0.1 - - [08/Aug/2017 20:36:55] "GET / HTTP/1.1" 200 -
127.0.0.1 - - [08/Aug/2017 20:36:55] "GET /favicon.ico HTTP/1.1" 404 -
127.0.0.1 - - [08/Aug/2017 20:36:55] "GET /favicon.ico HTTP/1.1" 404 -


After the server initializes it will listen on port 5000 waiting for
connections. Now open up your web browser and enter the following URL in the
address field:

http://localhost:5000

Alternatively you can use the following URL:

http://localhost:5000/index

Do you see the route mappings in action? The first URL maps to /, while the
second maps to /index. Both routes are associated with our view function, so
they produce the same result. If you enter any other URL you will get an
error, since only these two have been defined.

When you are done playing with the server you can just hit Ctrl-C to stop it.


={============================================================================
|kt_dev_py_0001| py-flask-template

Part II: Templates (this article)
https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-ii-templates

yet very simple web application that has the following file structure:

    microblog\
      flask\
        <virtual environment files>
      app\
        static\
        templates\
        __init__.py
        views.py
      tmp\
      run.py


If not use template:

from app import app

@app.route('/')
@app.route('/index')
def index():
    user = {'nickname': 'Miguel'}  # fake user
    return '''
<html>
  <head>
    <title>Home Page</title>
  </head>
  <body>
    <h1>Hello, ''' + user['nickname'] + '''</h1>
  </body>
</html>
'''

Templates to the rescue

If you could keep the logic of your application separate from the layout or
presentation of your web pages things would be much better organized, don't
you think? You could even hire a web designer to create a killer web site
while you code the site's behaviors in Python. Templates help implement this
  separation.

Let's write our first template 

(file app/templates/index.html):

<html>
  <head>
    <title>{{ title }} - microblog</title>
  </head>
  <body>
      <h1>Hello, {{ user.nickname }}!</h1>
  </body>
</html>

As you see above, we just wrote a mostly standard HTML page, with the only
difference that there are some placeholders for the `dynamic content` enclosed
in {{ ... }} sections.

Now let's see how we use this template from our view function 

(file app/views.py):

from flask import render_template
from app import app

@app.route('/')
@app.route('/index')
def index():
    user = {'nickname': 'Miguel'}  # fake user
    return render_template('index.html',
                           title='Home',
                           user=user)


note: so view has data and stitch up with template. `renders` the template.


Try the application at this point to see how the template works. Once you have
the rendered page in your browser you may want to `view the source HTML` and
compare it against the original template.

This function takes a template filename and a variable list of template
arguments and returns the rendered template, with all the arguments replaced.


<jinja2-templating-engine>
Under the covers, the render_template function invokes the Jinja2 templating
engine that is part of the Flask framework. Jinja2 substitutes {{...}} blocks
with the corresponding values provided as template arguments.


Loops in templates

The logged in user in our microblog application will probably want to see
recent posts from followed users in the home page, so let's see how we can do
that.

To begin, we use our handy fake object trick to create some users and some
posts to show 

(file app/views.py):

def index():
    user = {'nickname': 'Miguel'}  # fake user
    posts = [  # fake array of posts
        { 
            'author': {'nickname': 'John'}, 
            'body': 'Beautiful day in Portland!' 
        },
        { 
            'author': {'nickname': 'Susan'}, 
            'body': 'The Avengers movie was so cool!' 
        }
    ]
    return render_template("index.html",
                           title='Home',
                           user=user,
                           posts=posts)

To represent user posts we are using a list, where each element has author and
body fields. When we get to implement a real database we will preserve these
field names, so we can design and test our template using the fake objects
without having to worry about updating it when we move to a database.

On the template side we have to solve a new problem. The list can have any
number of elements, it will be up to the view function to decide how many
posts need to be presented. The template cannot make any assumptions about the
number of posts, so it needs to be prepared to render as many posts as the
view sends.

So let's see how we do this using a for control structure (file
    app/templates/index.html):


Template inheritance

Our microblog web application will need to have a navigation bar at the top of
the page with a few links. Here you will get the link to edit your profile, to
login, logout, etc.

We can add a navigation bar to our index.html template, but as our application
grows we will be needing to implement more pages, and this navigation bar will
have to be copied to all of them. Then you will have to keep all these
identical copies of the navigation bar in sync, and that could become a lot of
work if you have a lot of pages and templates.

Instead, we can use Jinja2's template inheritance feature, which allows us to
move the parts of the page layout that are common to all templates and put
them in a base template from which all other templates are derived.

So let's define a base template that includes the navigation bar and also the
bit of title logic we implemented earlier 

(file app/templates/base.html):

<html>
  <head>
    {% if title %}
    <title>{{ title }} - microblog</title>
    {% else %}
    <title>Welcome to microblog</title>
    {% endif %}
  </head>
  <body>
    <div>Microblog: <a href="/index">Home</a></div>
    <hr>
    {% block content %}{% endblock %}
  </body>
</html>

In this template we use the `block` control statement to define the place where
the derived templates can insert themselves. Blocks are given a unique name,
    and their content can be replaced or enhanced in derived templates.

And now what's left is to modify our index.html template to inherit from
base.html (file app/templates/index.html):

{% extends "base.html" %}
{% block content %}
    <h1>Hi, {{ user.nickname }}!</h1>
    {% for post in posts %}
    <div><p>{{ post.author.nickname }} says: <b>{{ post.body }}</b></p></div>
    {% endfor %}
{% endblock %}

Since the base.html template will now take care of the general page structure
we have removed those elements from this one and left only the content part.
The extends block establishes the inheritance link between the two templates,
    so that Jinja2 knows that when it needs to render index.html it needs to
    include it inside base.html. The two templates have matching block
    statements with name content, and this is how Jinja2 knows how to combine
    the two into one. When we get to write new templates we will also create
    them as extensions to base.html.


={============================================================================
|kt_dev_py_0001| py-flask-form

Part III: Web Forms (this article)
https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-iii-web-forms

Configuration

To handle our web forms we are going to use the Flask-WTF extension, which in
turn wraps the WTForms project in a way that integrates nicely with Flask
apps.

Many Flask extensions require some amount of configuration, so we are going to
setup a configuration file inside our root microblog folder so that it is
easily accessible if it needs to be edited. Here is what we will start with
(file config.py):

WTF_CSRF_ENABLED = True
SECRET_KEY = 'you-will-never-guess'

Pretty simple, it's just two settings that our Flask-WTF extension needs. The
WTF_CSRF_ENABLED setting activates the cross-site request forgery prevention
(note that this setting is enabled by default in current versions of
 Flask-WTF). In most cases you want to have this option enabled as it makes
your app more secure.

The SECRET_KEY setting is only needed when CSRF is enabled, and is used to
create a cryptographic token that is used to validate a form. When you write
your own apps make sure to set the secret key to something that is difficult
to guess.

Now that we have our config file we need to tell Flask to read it and use it.
We can do this right after the Flask app object is created, as follows 
(file app/__init__.py):

from flask import Flask

app = Flask(__name__)
app.config.from_object('config')      # added

from app import views


The user login form

Web forms are represented in Flask-WTF as classes, subclassed from base class
Form. A form subclass simply defines the fields of the form as class
variables.

Now we will create a login form that users will use to identify with the
system. The login mechanism that we will support in our app is not the
standard username/password type, we will have our users login using their
OpenID. OpenIDs have the benefit that the authentication is done by the
provider of the OpenID, so we don't have to validate passwords, which makes
our site more secure to our users.

The OpenID login only requires one string, the so called OpenID. We will also
throw a 'remember me' checkbox in the form, so that users can choose to have a
cookie installed in their browsers that remembers their login when they come
back.

Let's write our first form (file app/forms.py):

from flask_wtf import Form
from wtforms import StringField, BooleanField
from wtforms.validators import DataRequired

class LoginForm(Form):
    openid = StringField('openid', validators=[DataRequired()])
    remember_me = BooleanField('remember_me', default=False)

I believe the class is pretty much self-explanatory. We imported the Form
  class, and the two form field classes that we need, StringField and
  BooleanField.

The DataRequired import is a validator, a function that can be attached to a
field to perform validation on the data submitted by the user. The
DataRequired validator simply checks that the field is not submitted empty.
There are many more validators included with Flask-WTF, we will use some more
in the future.


Form templates

We will also need a template that contains the HTML that produces the form.

The good news is that the LoginForm class that we just created knows how to
render form fields as HTML, so we just need to concentrate on the layout. Here
is our login template 

(file app/templates/login.html):

<!-- extend from base layout -->
{% extends "base.html" %}

{% block content %}
  <h1>Sign In</h1>
  <form action="" method="post" name="login">
      {{ form.hidden_tag() }}
      <p>
          Please enter your OpenID:<br>
          {{ form.openid(size=80) }}<br>
      </p>
      <p>{{ form.remember_me }} Remember Me</p>
      <p><input type="submit" value="Sign In"></p>
  </form>
{% endblock %}

Note that in this template we are reusing the base.html template through the
extends template inheritance statement. We will actually do this with all our
templates, to ensure a consistent layout across all pages.


There are a few interesting differences between a regular HTML form and our
template. This template expects a form object instantiated from the form class
we just defined stored in a template argument named form. We will take care of
sending this template argument to the template next, when we write the view
function that renders this template.

The form.hidden_tag() template argument will get replaced with a hidden field
that implements the CSRF prevention that we enabled in the configuration. This
field needs to be in all your forms if you have CSRF enabled. The good news is
that Flask-WTF handles it for us, we just need to make sure it is included in
the form.

The actual fields of our form are rendered by the field objects, we just need
to refer to a {{form.field_name}} template argument in the place where each
field should be inserted. Some fields can take arguments. In our case, we are
asking the text field to generate our openid field with a width of 80
characters.

Since we have not defined the submit button in the form class we have to
define it as a regular field. The submit field does not carry any data so it
doesn't need to be defined in the form class.


Form views

The final step before we can see our form is to code a view function that
renders the template.

This is actually quite simple since we just need to pass a form object to the
template. Here is our new view function 

(file app/views.py):

from flask import render_template, flash, redirect
from app import app
from .forms import LoginForm

# index view function suppressed for brevity

@app.route('/login', methods=['GET', 'POST'])
def login():
    form = LoginForm()
    return render_template('login.html', 
                           title='Sign In',
                           form=form)


So basically, we have imported our LoginForm class, instantiated an object
from it, and sent it down to the template. This is all that is required to get
form fields rendered.

Let's ignore for now the flash and redirect imports. We'll use them a bit
later.

The only other thing that is new here is the methods argument in the route
decorator. This tells Flask that this view function accepts GET and POST
requests. Without this the view will only accept GET requests. We will want to
receive the POST requests, these are the ones that will bring in the form data
entered by the user.

At this point you can try the app and see the form in your web browser. After
you start the application you will want to open http://localhost:5000/login in
your web browser, as this is the route we have associated with the login view
function.

We have not coded the part that accepts data yet, so pressing the submit
button will not have any effect at this time.


http://localhost:5000/login

/home/kyoupark/mblog/app/views.py:7: FlaskWTFDeprecationWarning: "flask_wtf.Form" has been renamed to "FlaskForm" and will be removed in 1.0.
  form = LoginForm()

// rendered source

<!-- extend from base layout -->
<html>
  <head>
    
    <title> Sign In - microblog </title>
    
  </head>
  <body>
    <div> microblog: <a href="/index"> home </a></div>
    <hr>
    
  <h1> Sign In </h1>
  <form action="" method="post" name="login">
    <input id="csrf_token" name="csrf_token" type="hidden" value="IjE0NmFlZTNhZDQyMzBlMzliNDdjYjk2NWNlZGZmYzNiNzUyYmU4ZTgi.DG95ww.Wx8mq0hmKtPqh1EsmCfXABOy-8w">
    <p>
      Please enter your OpenID: <br>
      <input id="openid" name="openid" size="80" type="text" value=""> <br>
    </p>
    <p> <input id="remember_me" name="remember_me" type="checkbox" value="y"> Remember Me </p>
    <p><input type="submit" value="Sign In"></p>
  </form>

  </body>
</html>


Receiving form data


==============================================================================
Copyrightobjdump see |ktkb|                        vim:tw=100:ts=3:ft=help:norl:
