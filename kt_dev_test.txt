*kt_dev_test*                                                           tw=100, utf-8


/^[#=]{
Use #{ for a group and ={ for a item

|kt_dev_test_100| cppunit
|kt_dev_test_101| cppunit: cookbook
|kt_dev_test_102| cppunit: simple
|kt_dev_test_103| cppunit: macros
|kt_dev_test_104| cppunit: example
|kt_dev_test_105| cppunit: with gmock example
|kt_dev_test_106| cppunit: display test name when run
|kt_dev_test_107| cppunit: inject dependency to object under test
*kt_dev_test_108* cppunit: test runner command line options

|kt_dev_test_001| gtest-install
|kt_dev_test_002| gtest-doc
*kt_dev_test_002* gtest-tdd
*kt_dev_test_002* gtest-exception

|kt_dev_test_200| gmock-install
*kt_dev_test_200* gmock-doc
|kt_dev_test_201| gmock-for-dummies
*kt_dev_test_201* gmock-sequence
*kt_dev_test_201* gmock-for-multiple-expectations
*kt_dev_test_203* gmock-side-effect
*kt_dev_test_202* gmock-and-cppunit
*kt_dev_test_203* gmock-action
*kt_dev_test_203* gmock-matcher
*kt_dev_test_207* gmock-nice gmock-strict
*kt_dev_test_208* gmock-errors when misses expectation
*kt_dev_test_208* gmock-ex
*kt_dev_test_200* gmock-value-parameterized
*kt_dev_test_200* gmock-problem


={============================================================================
*kt_dev_test_001* test-doc

TDD: Modern C++ Programming with Test-Driven Development by Jeff Langr


# ============================================================================
#{ test framework
={============================================================================
*kt_dev_test_100* cppunit

<1>
http://cppunit.sourceforge.net/doc/1.8.0/index.html

This has the cookbook as below and source. However, this source is cvs
repository and cannot use it directly.

<2>
http://freedesktop.org/wiki/Software/cppunit/#index1h1

CppUnit is the C++ port of the famous JUnit framework for unit testing. Test
output is in XML for automatic testing and GUI based for supervised tests.

CppUnit is the unit test framework used by LibreOffice and now maintained at
FreeDesktop. This is a continuation of the original cppunit project. note: this
is <1>

Getting the sources

cppunit sources are stored in git. To get them, you can use:

git clone git://anongit.freedesktop.org/git/libreoffice/cppunit/

Release Versions

Cppunit 1.13.2 MD5: d1c6bdd5a76c66d2c38331e2d287bc01

Building it

Once the source has been checked out, cppunit can be built in usual manner:

cd cppunit
./autogen.sh
./configure
make
make check # optional
make install


={============================================================================
*kt_dev_test_101* cppunit: cookbook

http://cppunit.sourceforge.net/doc/1.8.0/cppunit_cookbook.html#cppunit_cookbook

<simple-test-case>
You want to know whether your code is working.

How do you do it?

There are many ways. Stepping through a debugger or littering your code with
stream output calls are two of the simpler ways, but they both have drawbacks. 

Stepping through your code is a good idea, but it is 'not' automatic. You have
to do it every time you make changes. Streaming out text is also fine, but it
makes code 'ugly' and it generates far more information than you need most of
the time.

Tests in CppUnit can be run automatically. They are easy to set up and once you
have written them, they are always there to help you keep confidence in the
quality of your code.

To make a simple test, here is what you do:

Subclass the TestCase class. Override the method runTest(). When you want to
check a value, call CPPUNIT_ASSERT(bool) and pass in an expression that is true
if the test succeeds.

For example, to test the equality comparison for a Complex number class, write:

class ComplexNumberTest : public CppUnit::TestCase { 
  public: 
    ComplexNumberTest( std::string name ) : CppUnit::TestCase( name ) {}

    // note: Complex is a class to test

    void runTest() {
      CPPUNIT_ASSERT( Complex (10, 1) == Complex (10, 1) );
      CPPUNIT_ASSERT( !(Complex (1, 1) == Complex (2, 2)) );
    }
};


{fixture}
That was a very simple test. Ordinarily, you'll have many little test cases that
you'll want to run on the same set of objects. To do this, use a fixture.

A fixture is a 'known' 'set' of objects that serves as a base for a set of test
cases. Fixtures come in very handy when you are testing as you develop.

Let's try out this style of development and learn about fixtures along the away.
Suppose that we are really developing a complex number class. Let's start by
defining a empty class named Complex.

class Complex {};

Now create an instance of ComplexNumberTest above, compile the code and see what
happens. The first thing we notice is a few compiler errors. The test uses
operator==, but it is not defined. Let's fix that.

bool operator==( const Complex &a, const Complex &b) 
{ 
  return true; 
}

Now compile the test, and run it. This time it compiles but the test fails.
note: since it always returns true and fails on the second assert. We need a bit
more to get an operator==() working correctly, so we revisit the code.

class Complex { 
  friend bool operator ==(const Complex& a, const Complex& b);
  double real, imaginary;
  public:
  Complex( double r, double i = 0 ) 
    : real(r), imaginary(i) 
  {
  }
};

bool operator ==( const Complex &a, const Complex &b )
{ 
  return eq( a.real, b.real )  &&  eq( a.imaginary, b.imaginary ); 
}

If we compile now and run our test it will pass.

Now we are ready to add new operations and new tests. At this point a fixture
would be handy. We would probably be better off when doing our tests if we
decided to instantiate three or four complex numbers and reuse them across our
tests.

<how-to-use-fixture>
Here is how we do it:

o Add member variables for each part of the fixture
o 'override' setUp() to initialize the variables
o 'override' tearDown() to release any permanent resources you allocated in
setUp()

note: requires to inherit from "TestFixture"

#include <cppunit/TestFixture.h>

class ComplexNumberTest : public CppUnit::TestFixture 
{
  private:
    Complex *m_10_1, *m_1_1, *m_11_2;

  protected:
    void setUp()
    {
      m_10_1 = new Complex( 10, 1 );
      m_1_1 = new Complex( 1, 1 );
      m_11_2 = new Complex( 11, 2 );  
    }

    void tearDown() 
    {
      delete m_10_1;
      delete m_1_1;
      delete m_11_2;
    }
};

Once we have this fixture, we can add the complex addition test case that we
need over the course of our development.


{fixture-add-test}
How do you write and invoke individual tests using a fixture?

There are two steps to this process:

o Write the test case 'as' a 'method' in the fixture class
o Create a TestCaller which runs that particular method

Here is our test case class with a few extra case methods:

class ComplexNumberTest : public CppUnit::TestFixture  
{
  private:
    Complex *m_10_1, *m_1_1, *m_11_2;
  protected:
    void setUp()
    {
      m_10_1 = new Complex( 10, 1 );
      m_1_1 = new Complex( 1, 1 );
      m_11_2 = new Complex( 11, 2 );  
    }

    void tearDown() 
    {
      delete m_10_1;
      delete m_1_1;
      delete m_11_2;
    }

    void testEquality()
    {
      CPPUNIT_ASSERT( *m_10_1 == *m_10_1 );
      CPPUNIT_ASSERT( !(*m_10_1 == *m_11_2) );
    }

    void testAddition()
    {
      CPPUNIT_ASSERT( *m_10_1 + *m_1_1 == *m_11_2 );
    }
};


{fixture-run-test}
<test-caller> 
One may create and run instances for 'each' test case like this:

CppUnit::TestCaller<ComplexNumberTest> 
   test( "testEquality", &ComplexNumberTest::testEquality );
CppUnit::TestResult result;
test.run( &result );

The second argument to the test caller constructor is the address of a method on
ComplexNumberTest. When the test caller is run, that specific method will be
run. This is 'not' a 'useful' thing to do since:

o as no diagnostics will be displayed. 
o run a single test case.


<test-suite>
How do you set up your tests so that you can run them 'all' at once? CppUnit
provides a TestSuite class that runs any number of TestCases together.

To create a suite of two or more tests, you do the following:

CppUnit::TestSuite suite;
CppUnit::TestResult result;

suite.addTest( new CppUnit::TestCaller<ComplexNumberTest>( "testEquality", 
                       &ComplexNumberTest::testEquality ) );

suite.addTest( new CppUnit::TestCaller<ComplexNumberTest>( "testAddition", 
                       &ComplexNumberTest::testAddition ) );

suite.run( &result );

TestSuites don't only have to contain callers for TestCases. They can contain
any object that implements the Test interface. For example, you can create a
TestSuite in your code and I can create one in mine, and we can run them
together by creating a TestSuite that contains both:

CppUnit::TestSuite suite;
CppUnit::TestResult result;
suite.addTest( ComplexNumberTest::suite() );
suite.addTest( SurrealNumberTest::suite() );
suite.run( &result );

note: this uses testRunner below and suite can be nested. That is to run
multiple 'suite' together.


{test-runner} note: this is test 'case' file
How do you run your tests and collect their results?

Once you have a test suite, you'll want to run it. CppUnit provides tools to
define the suite to be run and to display its results. You 'make' your suite
'accessible' to a "TestRunner" program with a static method suite that returns a
test suite.

For example, to make a ComplexNumberTest suite available to a TestRunner, add
the following code to ComplexNumberTest:

note: 'static' function.

{
  public: 
    static CppUnit::Test *suite()
    {
      CppUnit::TestSuite *suiteOfTests = 
          new CppUnit::TestSuite( "ComplexNumberTest" );

      suiteOfTests->addTest( 
              new CppUnit::TestCaller<ComplexNumberTest>( "testEquality", 
            &ComplexNumberTest::testEquality ) );

      suiteOfTests->addTest( 
              new CppUnit::TestCaller<ComplexNumberTest>( "testAddition",
            &ComplexNumberTest::testAddition ) );

      return suiteOfTests;
    }
}


<text-version> note: this is main file to 'run' test files.
To use the text version, include the header files for the tests in Main.cpp:

#include <cppunit/ui/text/TestRunner.h>
#include "ExampleTestCase.h"
#include "ComplexNumberTest.h"

And add a call to addTest(CppUnit::Test *) in the main() function:

int main( int argc, char **argv)
{
  CppUnit::TextUi::TestRunner runner;
  runner.addTest( ExampleTestCase::suite() );
  runner.addTest( ComplexNumberTest::suite() );
  runner.run();
  return 0;
}

The TestRunner will run the tests. If all the tests pass, you'll get an
informative message. If any fail, you'll get the following information:

o The name of the test case that failed
o The name of the source file that contains the test
o The line number where the failure occurred
o All of the text inside the call to CPPUNIT_ASSERT() which detected the failure


<failure-and-error>
CppUnit distinguishes between failures and errors. A failure is anticipated and
checked for with assertions. Errors are 'unanticipated' problems like division
by zero and other exceptions thrown by the C++ runtime or your code.

note: nested or hiarachical structure and this can be even simpler when use
factory below.

main                          Test file(feature/group) 01            Test file(feature/group) 02 
                  
{                             class A : public CppUnit::TestFixture  class B : ...
   runner.addTest(A.suite);   {
   runner.addTest(B.suite);      suite:
   runner.run();                    caller(test01);
}                                   caller(test02);
                                    ...
                              };


{helper-macros}
As you might have noticed, implementing the fixture static suite() method is a
repetitive and error prone task. A set of macros have been created to
automatically implements the static suite() method.

note: see Writing test fixture 
http://cppunit.sourceforge.net/doc/1.8.0/group___writing_test_fixture.html

The following code is a rewrite of ComplexNumberTest using those macros:

{{ ComplexNumberTest.cpp  

#include <cppunit/extensions/HelperMacros.h>

class ComplexNumberTest : public CppUnit::TestFixture  
{
  protected:
    // First, we declare the suite, passing the class name to the macro:
    CPPUNIT_TEST_SUITE( ComplexNumberTest );

    // The suite created by the static suite() method is named after the class
    // name. Then, we declare each test case of the fixture:
    CPPUNIT_TEST( testEquality ); 
    CPPUNIT_TEST( testAddition );

    // Finally, we end the suite declaration:
    CPPUNIT_TEST_SUITE_END();

    // At this point, a method with the following signature has been implemented:
    static CppUnit::TestSuite *suite();

    // note: So the above three macros is to implement "suite()" and the rest of
    // the fixture is left as it was. So 'unchanged'.

  private:
    Complex *m_10_1, *m_1_1, *m_11_2;
  protected:
    void setUp()
    {
      m_10_1 = new Complex( 10, 1 );
      m_1_1 = new Complex( 1, 1 );
      m_11_2 = new Complex( 11, 2 );  
    }

    void tearDown() 
    {
      delete m_10_1;
      delete m_1_1;
      delete m_11_2;
    }

    void testEquality()
    {
      CPPUNIT_ASSERT( *m_10_1 == *m_10_1 );
      CPPUNIT_ASSERT( !(*m_10_1 == *m_11_2) );
    }

    void testAddition()
    {
      CPPUNIT_ASSERT( *m_10_1 + *m_1_1 == *m_11_2 );
    }
};

CPPUNIT_TEST_SUITE_REGISTRATION(ComplexNumberTest);

}} ComplexNumberTest.cpp  

The name of the TestCaller added to the suite are a composition of the fixture
name and the method name.

In the present case, the names would be: "ComplexNumberTest.testEquality" and
"ComplexNumberTest.testAddition".

<handle-exception>
The helper macros help you write comon assertion. For example, to check that
ComplexNumber throws a MathException when dividing a number by 0:

o add the test to the suite using CPPUNIT_TEST_EXCEPTION, specifying the
expected exception type.

o write the test case method

CPPUNIT_TEST_SUITE( ComplexNumberTest );
// [...]
CPPUNIT_TEST_EXCEPTION( testDivideByZeroThrows, MathException );
CPPUNIT_TEST_SUITE_END();

// [...]

  void testDivideByZeroThrows()
  {
    // The following line should throw a MathException.
    *m_10_1 / ComplexNumber(0);
  }

If the expected exception is not thrown, then a assertion failure is reported.


{factory-registry}
The TestFactoryRegistry was created to solve two pitfalls:

o forgetting to add your fixture suite to the test runner since it is in another
file, it is easy to forget. note: In main file, must call "addTest()" for each
test case.

o compilation bottleneck caused by the inclusion of all test case headers (see
        previous example) note: Assume that each test case has own header file
and implementation file.

The TestFactoryRegistry is a place where suites can be registered at
initialization time.

To register the ComplexNumber suite, in the .cpp file, you add: see above.

#include <cppunit/extensions/HelperMacros.h>

CPPUNIT_TEST_SUITE_REGISTRATION( ComplexNumber );

Behind the scene, a static variable type of AutoRegisterSuite is declared. On
construction, it will register a TestSuiteFactory into the TestFactoryRegistry.
The TestSuiteFactory returns the TestSuite returned by ComplexNumber::suite().

To run the tests, using the text test runner, we don't need to include the
fixture anymore:

#include <cppunit/extensions/TestFactoryRegistry.h>
#include <cppunit/ui/text/TestRunner.h>

int main( int argc, char **argv)
{
  CppUnit::TextUi::TestRunner runner;

  // First, we retreive the instance of the TestFactoryRegistry:

  CppUnit::TestFactoryRegistry &registry = CppUnit::TestFactoryRegistry::getRegistry();

  // Then, we obtain and add a new TestSuite created by the TestFactoryRegistry
  // that contains 'all' the test suite registered using
  // CPPUNIT_TEST_SUITE_REGISTRATION().

  runner.addTest( registry.makeTest() );
  runner.run();
  return 0;
}

note: make it possible to add all tests by calling a single addTest.


<Q>
Post-build check

Well, now that we have our unit tests running, how about integrating unit
testing to our build process?

To do that, the application must returns a value different than 0 to indicate
that there was an error.

TestRunner::run() returns a boolean indicating if the run was successful.

Updating our main programm, we obtains:

#include <cppunit/extensions/TestFactoryRegistry.h>
#include <cppunit/ui/text/TestRunner.h>

int main( int argc, char **argv)
{
  CppUnit::TextUi::TestRunner runner;
  CppUnit::TestFactoryRegistry &registry = CppUnit::TestFactoryRegistry::getRegistry();
  runner.addTest( registry.makeTest() );
  bool wasSucessful = runner.run( "", false );
  return wasSucessful;
}

Now, you need to 'run' your application after compilation.


={============================================================================
*kt_dev_test_102* cppunit: simple

http://freedesktop.org/wiki/Software/cppunit/#index1h1

From cppunit/examples/simple

<test-case-header>
#ifndef CPP_UNIT_EXAMPLETESTCASE_H
#define CPP_UNIT_EXAMPLETESTCASE_H

#include <cppunit/extensions/HelperMacros.h>

/* 
 * A test case that is designed to produce
 * example errors and failures
 *
 */

class ExampleTestCase : public CPPUNIT_NS::TestFixture
{
  CPPUNIT_TEST_SUITE( ExampleTestCase );
  CPPUNIT_TEST( example );
  CPPUNIT_TEST( anotherExample );
  CPPUNIT_TEST( testAdd );
  CPPUNIT_TEST( testEquals );
  CPPUNIT_TEST_SUITE_END();

protected:
  double m_value1;
  double m_value2;

public:
  void setUp();

protected:
  void example();
  void anotherExample();
  void testAdd();
  void testEquals();
};

#endif

<test-case-source>
#include <cppunit/config/SourcePrefix.h>
#include "ExampleTestCase.h"

CPPUNIT_TEST_SUITE_REGISTRATION( ExampleTestCase );

void ExampleTestCase::example()
{
  CPPUNIT_ASSERT_DOUBLES_EQUAL( 1.0, 1.1, 0.05 );
  CPPUNIT_ASSERT( 1 == 0 );
  CPPUNIT_ASSERT( 1 == 1 );
}


void ExampleTestCase::anotherExample()
{
  CPPUNIT_ASSERT (1 == 2);
}

void ExampleTestCase::setUp()
{
  m_value1 = 2.0;
  m_value2 = 3.0;
}

void ExampleTestCase::testAdd()
{
  double result = m_value1 + m_value2;
  CPPUNIT_ASSERT( result == 6.0 );
}


void ExampleTestCase::testEquals()
{
  long* l1 = new long(12);
  long* l2 = new long(12);

  CPPUNIT_ASSERT_EQUAL( 12, 12 );
  CPPUNIT_ASSERT_EQUAL( 12L, 12L );
  CPPUNIT_ASSERT_EQUAL( *l1, *l2 );

  delete l1;
  delete l2;

  CPPUNIT_ASSERT( 12L == 12L );
  CPPUNIT_ASSERT_EQUAL( 12, 13 );
  CPPUNIT_ASSERT_DOUBLES_EQUAL( 12.0, 11.99, 0.5 );
}


<main-source>
#include <cppunit/BriefTestProgressListener.h>
#include <cppunit/CompilerOutputter.h>
#include <cppunit/extensions/TestFactoryRegistry.h>
#include <cppunit/TestResult.h>
#include <cppunit/TestResultCollector.h>
#include <cppunit/TestRunner.h>


int
main()
{
  printf("--- { main\n");

  // Create the event manager and test controller
  CPPUNIT_NS::TestResult controller;

  // Add a listener that colllects test result
  CPPUNIT_NS::TestResultCollector result;
  controller.addListener( &result );        

  // Add a listener that print dots as test run.
  CPPUNIT_NS::BriefTestProgressListener progress;
  controller.addListener( &progress );      

  // Add the top suite to the test runner
  CPPUNIT_NS::TestRunner runner;
  runner.addTest( CPPUNIT_NS::TestFactoryRegistry::getRegistry().makeTest() );
  runner.run( controller );

  // Print test in a compiler compatible format.
  CPPUNIT_NS::CompilerOutputter outputter( &result, CPPUNIT_NS::stdCOut() );
  outputter.write(); 

  printf("--- } main\n");

  return result.wasSuccessful() ? 0 : 1;
}

<result-output>
--- { main
ExampleTestCase::example : assertion                  { summary
ExampleTestCase::anotherExample : assertion
ExampleTestCase::testAdd : assertion
ExampleTestCase::testEquals : assertion               }
ExampleTestCase.cpp:8:Assertion                       { details of each failure
Test name: ExampleTestCase::example
double equality assertion failed
- Expected: 1
- Actual  : 1.1
- Delta   : 0.05
                                                      }
ExampleTestCase.cpp:16:Assertion
Test name: ExampleTestCase::anotherExample
assertion failed
- Expression: 1 == 2

ExampleTestCase.cpp:28:Assertion
Test name: ExampleTestCase::testAdd
assertion failed
- Expression: result == 6.0

ExampleTestCase.cpp:45:Assertion
Test name: ExampleTestCase::testEquals
equality assertion failed
- Expected: 12
- Actual  : 13

Failures !!!
Run: 4   Failure total: 4   Failures: 4   Errors: 0
--- } main

note: failure is different from errors.


={============================================================================
*kt_dev_test_103* cppunit-macros

* Making assertions
http://cppunit.sourceforge.net/doc/1.8.0/group___assertions.html

Assers that "..." means that expect "..." true. That is assert("...").

include/cppunit/TestAssert.h

<1> #define CPPUNIT_ASSERT_EQUAL(expected, actual)

/** Asserts that two values are equals.
 * \ingroup Assertions
 *
 * Equality and string representation can be defined with
 * an appropriate CppUnit::assertion_traits class.
 *
 * A diagnostic is printed if actual and expected values disagree.
 *
 * Requirement for \a expected and \a actual parameters:
 * - They are exactly of the same type
 * - They are serializable into a std::strstream using operator <<.
 * - They can be compared using operator ==. 
 *
 * The last two requirements (serialization and comparison) can be
 * removed by specializing the CppUnit::assertion_traits.
 */

Asserts that two values are equals. That is fail if actual and expected values
disagree. 


{2} not in the 1.8.0 doc and but in the latest.

/** Asserts that the given expression throws an exception of the specified type. 
 * \ingroup Assertions
 * Example of usage:
 * \code
 *   std::vector<int> v;
 *  CPPUNIT_ASSERT_THROW( v.at( 50 ), std::out_of_range );
 * \endcode
 */
# define CPPUNIT_ASSERT_THROW( expression, ExceptionType )              \
   CPPUNIT_ASSERT_THROW_MESSAGE( CPPUNIT_NS::AdditionalMessage(),       \
                                 expression,                            \
                                 ExceptionType )

<ex> if the expression does not raise exception, then fail

    CPPUNIT_ASSERT_THROW(mr->setSink("invalid://0").get(),
                         InvalidLocatorException);


/** Asserts that the given expression throws an exception of the specified type, 
 * setting a user supplied message in case of failure. 
 * \ingroup Assertions
 * Example of usage:
 * \code
 *   std::vector<int> v;
 *  CPPUNIT_ASSERT_THROW_MESSAGE( "- std::vector<int> v;", v.at( 50 ), std::out_of_range );
 * \endcode
 */

<3>
/** Asserts that the given expression does not throw any exceptions.
 * \ingroup Assertions
 * Example of usage:
 * \code
 *   std::vector<int> v;
 *   v.push_back( 10 );
 *  CPPUNIT_ASSERT_NO_THROW( v.at( 0 ) );
 * \endcode
 */

<4>
/** Assertion with a user specified message.
 * \ingroup Assertions
 * \param message Message reported in diagnostic if \a condition evaluates
 *                to \c false.
 * \param condition If this condition evaluates to \c false then the
 *                  test failed.
 */
#define CPPUNIT_ASSERT_MESSAGE(message,condition)                          \


={============================================================================
*kt_dev_test_104* cppunit: example

void testGetEventFromSummary()
{
    // 1
    SystemClientSummary summary = make_sample_summary();

    // 2
    CPPUNIT_ASSERT_EQUAL(ENTITY_SCHEDULE_EVENT, summary.getSummaryType());
    CPPUNIT_ASSERT_NO_THROW(summary.getEvent());
    CPPUNIT_ASSERT(summary.getEvent());
    CPPUNIT_ASSERT_EQUAL(string("Eastenders"), summary.getEvent()->getTitle());
    CPPUNIT_ASSERT_EQUAL(string("mediumSynopsis"), summary.getEvent()->getSynopsis());
    CPPUNIT_ASSERT(summary.getEvent()->getService());
    CPPUNIT_ASSERT_EQUAL(string("dvb://233a..1044"), 
        summary.getEvent()->getService()->getIdentifier("locator"));
    CPPUNIT_ASSERT_EQUAL(string("dvb://233a..1044;1"), 
        summary.getEvent()->getIdentifier("locator"));
    CPPUNIT_ASSERT_EQUAL(false, summary.getEvent()->isPresent());
    CPPUNIT_ASSERT_EQUAL(false, summary.getEvent()->getReminderBookingStatus());
    CPPUNIT_ASSERT_EQUAL(METADATA_CLIENT::COMPLETED, summary.getEvent()->getRecordingStatus());
    CPPUNIT_ASSERT_EQUAL(0U, summary.getEvent()->getContainingServiceListIndex());
}

#1: Create an object to test. In this case, makes up summary object made up with
data structure.

#2: Run tests on this object.

Essentially, this mocks up an object, and excersize tests to see if returns
matches data from the mock and to see if has expected initialize value and
state.


={============================================================================
*kt_dev_test_105* cppunit: with gmock example

#1: See how to create mock object and use it for the rest using pointer.
#2: Use setUp overloads

// called by cppunit
void setUp()
{
    // this is a mock object
    mockEventRepo = boost::make_shared<NS_IRON_SYSTEM::MockEventRepositoryAsync>();
}

// each test calls this
void setUp(MetadataSource source, bool useDTTHistory = true, bool skipPFCallsSetup = false, 
        bool enableEventCache = false, bool useBrokerMock = false)
{
    ...
}

void testGetPresentEvent()
{
    // setup the mocks
    EXPECT_CALL(*mockEventRepo, getPresentFollowing(getDefaultUnifiedServices()[0].serviceLocator)).
            WillOnce(returnNewCompletedFuture(vector<NS_IRON_SYSTEM::Event>(1, getBBC1Events()[0])));

    EXPECT_CALL(*mockEventRepo, getPresentFollowing(getDefaultUnifiedServices()[1].serviceLocator)).
            WillOnce(returnNewCompletedFuture(vector<NS_IRON_SYSTEM::Event>(1, getCafeTVEvents()[0])));

    // 
    setUp(DTT_ONLY, true, true);

    // object under test
    boost::shared_ptr<NS_CLIENT::Event> e = eventRepo->getPresentEvent(0);
    
    CPPUNIT_ASSERT_MESSAGE("There should be an event returned", e);
    
    CPPUNIT_ASSERT_EQUAL(getBBC1Events()[0].eventLocator, e->getIdentifier("locator"));
    CPPUNIT_ASSERT_EQUAL(getBBC1Events()[0].shortTitle["eng"], e->getTitle());
    CPPUNIT_ASSERT_EQUAL(getBBC1Events()[0].start, (uint32_t)to_time_t(e->getStart()));
    CPPUNIT_ASSERT_EQUAL(getBBC1Events()[0].publishedDuration, (uint32_t)e->getDuration());
}


={============================================================================
*kt_dev_test_106* cppunit: display test name when run

void testEventAccessors_When_ConstructedFromAnIronEvent()
{
    cout << "\033[01;31m { " << __PRETTY_FUNCTION__ << "\033[m" << endl;

    CPPUNIT_ASSERT_EQUAL(string("The latest national and international"
        "news stories from the BBC News team, followed by weather."), 
        event.getSynopsis());

    cout << "} " << endl;
}


={============================================================================
*kt_dev_test_107* cppunit: inject dependency to object under test

This shows the way of how injecting real resources(dependencies) directly
through the constructor by using more args than using mock.

createGstMediaRouter(dispatcher);
  boost::bind(gst_pipeline_new, "pipeline"))) :

createGstMediaRouter(dispatcher,
  boost::bind(gst_pipeline_new, "pipeline")); 


={============================================================================
*kt_dev_test_108* cppunit: test runner command line options

test$ ./gstmediaroutertest --help
Allowed options:

Test runner options:
  --cases arg            Specify which test cases to run.  This is a whitespace
                         seperated list of test cases
  --list-cases           List the cases that would be run rather than 
                         runningthem
  --help                 Print this help message


CPPUNIT_TEST_SUITE(GstMediaRouterTest);
This leads to the test binary name

CPPUNIT_TEST(test_thatCallingSetSinkTwiceThrowsIllegalConfigurationException);
This leads to the each test case name

note:
When run a single test which do not have any calls to setUp() and tearDown() to
run a test, it's done by cppunit automatically.


={============================================================================
*kt_dev_test_001* gtest-install

<gtest-build>
https://github.com/google/googletest/blob/master/googletest/README.md

The thing is that is to inlcude 'gtest-all.o' to use it in your project.
That's it.

git clone https://github.com/google/googletest.git

export GTEST_DIR=~/works/googletest/googletest

g++ -isystem ${GTEST_DIR}/include -I${GTEST_DIR} \
    -pthread -c ${GTEST_DIR}/src/gtest-all.cc

ar -rv libgtest.a gtest-all.o

kyoupark@kit-debian64:~/works/googletest/googletest$ ls -al gtest-all.o 
-rw-r--r-- 1 kyoupark kyoupark 1479576 Feb 27 16:54 gtest-all.o


<gtest-sample>
From README in the download:
  cd ${GTEST_DIR}/make
  make
  ./sample1_unittest


kyoupark@kit-debian64:~/works/googletest/googletest$ cd make/
kyoupark@kit-debian64:~/works/googletest/googletest/make$ make

kyoupark@kit-debian64:~/works/googletest/googletest/make$ ./sample1_unittest 
Running main() from gtest_main.cc
[==========] Running 6 tests from 2 test cases.
[----------] Global test environment set-up.
[----------] 3 tests from FactorialTest
[ RUN      ] FactorialTest.Negative
[       OK ] FactorialTest.Negative (0 ms)
[ RUN      ] FactorialTest.Zero
[       OK ] FactorialTest.Zero (0 ms)
[ RUN      ] FactorialTest.Positive
[       OK ] FactorialTest.Positive (0 ms)
[----------] 3 tests from FactorialTest (0 ms total)

[----------] 3 tests from IsPrimeTest
[ RUN      ] IsPrimeTest.Negative
[       OK ] IsPrimeTest.Negative (0 ms)
[ RUN      ] IsPrimeTest.Trivial
[       OK ] IsPrimeTest.Trivial (0 ms)
[ RUN      ] IsPrimeTest.Positive
[       OK ] IsPrimeTest.Positive (0 ms)
[----------] 3 tests from IsPrimeTest (0 ms total)

[----------] Global test environment tear-down
[==========] 6 tests from 2 test cases ran. (0 ms total)
[  PASSED  ] 6 tests.
kyoupark@kit-debian64:~/works/googletest/googletest/make$ 

// the result has ok(green) and failed(red) colors on terminal.


={============================================================================
*kt_dev_test_002* gtest-doc

https://github.com/google/googletest/blob/master/googletest/docs/Documentation.md

Primer -- start here if you are new to Google Test.
Samples -- learn from examples.
AdvancedGuide -- learn more about Google Test.
XcodeGuide -- how to use Google Test in Xcode on Mac.
Frequently-Asked Questions -- check here before asking a question on the mailing list.


https://github.com/google/googletest/blob/master/googletest/docs/Primer.md

Introduction: Why Google C++ Testing Framework?

Google C++ Testing Framework helps you write better C++ tests.

No matter whether you work on Linux, Windows, or a Mac, if you write C++ code,
Google Test can help you.

So what makes a good test, and how does Google C++ Testing Framework fit in?
We believe:

  Tests should be `independent and repeatable.` It's a pain to debug a test
  that succeeds or fails as a result of other tests. Google C++ Testing
  Framework isolates the tests by running each of them on a different object.
  When a test fails, Google C++ Testing Framework allows you to run it in
  isolation for quick debugging.

  Tests should be well organized and reflect the structure of the tested code.
  Google C++ Testing Framework groups related tests into test cases that can
  share data and subroutines. This common pattern is easy to recognize and
  makes tests easy to maintain. Such consistency is especially helpful when
  people switch projects and start to work on a new code base.

  Tests should be portable and reusable. The open-source community has a lot
  of code that is platform-neutral, its tests should also be platform-neutral.
  Google C++ Testing Framework works on different OSes, with different
  compilers (gcc, MSVC, and others), with or without exceptions, so Google C++
  Testing Framework tests can easily work with a variety of configurations.
  (Note that the current release only contains build scripts for Linux - we
   are actively working on scripts for other platforms.)

  When tests fail, they should provide as much information about the problem
  as possible. Google C++ Testing Framework doesn't stop at the first test
  failure.  Instead, it only stops the current test and continues with the
  next. You can also set up tests that report non-fatal failures after which
  the current test continues. Thus, you can detect and fix multiple bugs in a
  single run-edit-compile cycle.

  The testing framework should liberate test writers from housekeeping chores
  and let them focus on the test content. Google C++ Testing Framework
  automatically keeps track of all tests defined, and doesn't require the user
  to enumerate them in order to run them.

  Tests should be fast. With Google C++ Testing Framework, you can reuse
  shared resources across tests and pay for the set-up/tear-down only once,
  without making tests depend on each other.

Since Google C++ Testing Framework is based on the popular xUnit architecture,
you'll feel right at home if you've used JUnit or PyUnit before. If not, it
will take you about 10 minutes to learn the basics and get started. So let's
go!

Note: We sometimes refer to Google C++ Testing Framework informally as Google
Test.  


Meaning	                                 Google Test Term      ISTQB Term

Exercise a particular program path        TEST()	     Test Case
with specific input values and 
verify the results	

A set of several tests related            Test Case	  Test Suite
to one component


Setting up a New Test Project

To write a test program using Google Test, you need to compile Google Test
into a library and link your test with it. 

Once you are able to compile the Google Test library, you should create a
project or build target for your test program. Make sure you have
GTEST_ROOT/include in the header search path so that the compiler can find
"gtest/gtest.h" when compiling your test. Set up your test project to link
with the Google Test library

If you still have questions, take a look at how Google Test's own tests are
built and use them as examples.  


Basic Concepts

When using Google Test, you start by writing assertions, which are statements
that check whether a condition is true. An assertion's result can be success,
nonfatal failure, or fatal failure. If a fatal failure occurs, it aborts the
  current function; otherwise the program continues normally.

  `Tests` use assertions to verify the tested code's behavior. If a test crashes
  or has a failed assertion, then it fails; otherwise it succeeds.

  A `test case` contains one or many tests. You should group your tests into
  test cases that reflect the structure of the tested code. When multiple
  tests in a test case need to share common objects and subroutines, you can
  put them into a test fixture class.

  A `test program` can contain multiple test cases.

We'll now explain how to write a test program, starting at the individual
assertion level and building up to tests and test cases.  


Assertions

Google Test assertions are macros that resemble function calls. You test a
class or function by making assertions about its behavior. When an assertion
fails, Google Test prints the assertion's source file and line number
location, along with a failure message. You may also supply a custom failure
message which will be appended to Google Test's message.

The assertions come in pairs that test the same thing but have different
effects on the current function. 

  ASSERT_* versions generate fatal failures when they fail, and abort the
  current function. 

  EXPECT_* versions generate nonfatal failures, `which don't abort` the current
  function. Usually EXPECT_* are preferred, as they allow more than one
  failures to be reported in a test.

<gtest-assert-difference>
However, you should use ASSERT_* if it doesn't make sense to continue when the
assertion in question fails.

Since a failed ASSERT_* returns from the current function immediately,
possibly skipping clean-up code that comes after it, it may cause a space
  leak. Depending on the nature of the leak, it may or may not be worth fixing
  - so keep this in mind if you get a heap checker error in addition to
  assertion errors.

`To provide a custom failure message`, simply stream it into the macro using the
<< operator, or a sequence of such operators. An example:

ASSERT_EQ(x.size(), y.size()) << "Vectors x and y are of unequal length";

for (int i = 0; i < x.size(); ++i) {
  EXPECT_EQ(x[i], y[i]) << "Vectors x and y differ at index " << i;
}

Anything that can be streamed to an ostream can be streamed to an assertion
macro--in particular, C strings and string objects. If a wide string
(wchar_t*, TCHAR* in UNICODE mode on Windows, or std::wstring) is streamed to
an assertion, it will be translated to UTF-8 when printed.  


Basic Assertions

These assertions do basic true/false condition testing.

Fatal assertion           Nonfatal assertion          Verifies
ASSERT_TRUE(condition);   EXPECT_TRUE(condition);     condition is true
ASSERT_FALSE(condition);  EXPECT_FALSE(condition);    condition is false

Remember, when they fail, ASSERT_* yields a fatal failure and returns from the
current function, while EXPECT_* yields a nonfatal failure, allowing the
function to continue running. In either case, an assertion failure means its
containing test fails.


Binary Comparison

This section describes assertions that compare two values.

Fatal assertion               Nonfatal assertion            Verifies
ASSERT_EQ(`expected`, `actual`);  EXPECT_EQ(expected, actual);  expected == actual
ASSERT_NE(val1, val2);        EXPECT_NE(val1, val2);        val1 != val2
ASSERT_LT(val1, val2);        EXPECT_LT(val1, val2);        val1 < val2
ASSERT_LE(val1, val2);        EXPECT_LE(val1, val2);        val1 <= val2
ASSERT_GT(val1, val2);        EXPECT_GT(val1, val2);        val1 > val2
ASSERT_GE(val1, val2);        EXPECT_GE(val1, val2);        val1 >= val2

In the event of a failure, Google Test prints both val1 and val2. 

Value arguments must be comparable by the assertion's comparison operator or
you'll get a compiler error. We used to require the arguments to support the
<< operator for streaming to an ostream, but it's no longer necessary since
v1.6.0 

(if << is supported, it will be called to print the arguments when the
 assertion fails; otherwise Google Test will attempt to print them in the best
 way it can. For more details and how to customize the printing of the
 arguments, see this Google Mock recipe.).


These assertions can work with a user-defined type, but only if you define the
corresponding `comparison operator` (e.g. ==, <, etc). If the corresponding
operator is defined, prefer using the ASSERT_*() macros because they will
print out not only the result of the comparison, but the two operands as well.

Arguments are always evaluated exactly once. Therefore, it's OK for the
arguments to have side effects. However, as with any ordinary C/C++ function,
          the arguments' evaluation order is undefined (i.e. the compiler is
              free to choose any order) and your code should not depend on any
          particular argument evaluation order.

ASSERT_EQ() does pointer equality on pointers. If used on two C strings, it
tests if they are in the same memory location, not if they have the same
value. Therefore, if you want to compare C strings (e.g. const char*) by
value, use ASSERT_STREQ() , which will be described later on. In particular,
  to assert that a C string is NULL, use ASSERT_STREQ(NULL, c_string) .
  However, to compare two string objects, you should use ASSERT_EQ.

Macros in this section work with both narrow and wide string objects (string
    and wstring).

Availability: Linux, Windows, Mac.

// In ASSERT_EQ* and EXPECT_EQ* (and all other equality assertions we'll
//     introduce later), you should put the expression you want to test in the
// position of actual, and put its expected value in expected, as Google Test's
// failure messages are optimized for this convention.

Historical note: Before February 2016 *_EQ had a convention of calling it as
ASSERT_EQ(expected, actual), so lots of existing code uses this order. 

`Now *_EQ treats both parameters in the same way.`


<gtest-order-assert-arg>

TEST(AssertTest, CheckTheOrderOfAssertArg)
{
    int value{100};

    // t_ex_mock.cpp:262: Failure
    // `Value of: value`
    // Expected: is equal to 101
    //   Actual: 100 (of type int)
    //
    // t_ex_mock.cpp:263: Failure
    // Value of: 101
    // Expected: is equal to 100
    //   Actual: 101 (of type int)
    EXPECT_THAT(value, 101);
    EXPECT_THAT(101, value);
}


String Comparison

The assertions in this group compare two C strings. If you want to compare two
string objects, use EXPECT_EQ, EXPECT_NE, and etc instead.

Fatal assertion                               Nonfatal assertion                            Verifies
ASSERT_STREQ(expected_str, actual_str);       EXPECT_STREQ(expected_str, actual_str);       the two C strings have the same content
ASSERT_STRNE(str1, str2);                     EXPECT_STRNE(str1, str2);                     the two C strings have different content
ASSERT_STRCASEEQ(expected_str, actual_str);   EXPECT_STRCASEEQ(expected_str, actual_str);   the two C strings have the same content, ignoring case
ASSERT_STRCASENE(str1, str2);                 EXPECT_STRCASENE(str1, str2);                 the two C strings have different content, ignoring case

Note that "CASE" in an assertion name means that case is ignored.

*STREQ* and *STRNE* also accept wide C strings (wchar_t*). If a comparison of
two wide strings fails, their values will be printed as UTF-8 narrow strings.

A NULL pointer and an empty string are considered different.

Availability: Linux, Windows, Mac.

See also: For more string comparison tricks (substring, prefix, suffix, and
regular expression matching, for example), see the Advanced Google Test
Guide.


Simple Tests

To create a test:

  Use the TEST() macro to define and name a test function, These are ordinary
  C++ functions `that don't return a value.`

  In this function, along with any valid C++ statements you want to include,
  use the various Google Test assertions to check values.

  The test's result is determined by the assertions; if any assertion in the
  test fails (either fatally or non-fatally), or if the test crashes, the
  entire test fails. Otherwise, it succeeds. 

TEST(testCaseName, testName) {
 ... test body ...
}

TEST() arguments go from general to specific. 

  The `first` argument is `the name of the test case`, and the second argument is
  the `test's name` within the test case. 

Both names must be valid C++ identifiers, and they should not contain
underscore (_). A test's full name consists of its containing test case and
its individual name. Tests from different test cases can have the same
individual name.

For example, let's take a simple integer function:

int Factorial(int n); // Returns the factorial of n

A test case for this function might look like:

// Tests factorial of 0.
TEST(Factorial`Test`, HandlesZeroInput) {
  EXPECT_EQ(1, Factorial(0));
}

// Tests factorial of positive numbers.
TEST(FactorialTest, HandlesPositiveInput) {
  EXPECT_EQ(1, Factorial(1));
  EXPECT_EQ(2, Factorial(2));
  EXPECT_EQ(6, Factorial(3));
  EXPECT_EQ(40320, Factorial(8));
}

Google Test groups the test results by test cases, so logically-related tests
should be in the same test case; in other words, the first argument to their
TEST() should be the same. 

In the above example, we have two tests, HandlesZeroInput and
HandlesPositiveInput, that belong to the same test case FactorialTest.


<gmock-fixture>
Test Fixtures: `Using the Same Data Configuration` for Multiple Tests

If you find yourself writing two or more tests that operate on similar data,
   you can use a test fixture. It allows you to reuse the same configuration
   of objects for several different tests.

To create a fixture, just:

    Derive a class from ::testing::Test. Start its body with protected: or
    public: as we'll want to access fixture members from sub-classes.

    Inside the class, declare any objects you plan to use.

    If necessary, write `a default constructor or SetUp()` function to prepare
    the objects for each test. A common mistake is to spell SetUp() as Setup()
    with a small u - don't let that happen to you.

    If necessary, write `a destructor or TearDown()` function to release any
    resources you allocated in SetUp() . To learn when you should use the
    constructor/destructor and when you should use SetUp()/TearDown(), read
    this FAQ entry.

    If needed, define subroutines for your tests to share. 

When using a fixture, use TEST_F() instead of TEST() as it allows you to
access objects and subroutines in the test fixture:

TEST_F(test_case_name, test_name) {
 ... test body ...
}

Like TEST(), the first argument is the test case name, but for TEST_F() this
must be `the name of the test fixture class.` You've probably guessed: _F is for
fixture.

Unfortunately, the C++ macro system does not allow us to create a single macro
that can handle both types of tests. Using the wrong macro causes a compiler
error.

Also, you must first define a test fixture class before using it in a
TEST_F(), or you'll get the compiler error "`virtual outside class
declaration`".

`For each test` defined with TEST_F(), Google Test will:

    Create a fresh test fixture at runtime

    Immediately initialize it via SetUp() ,

    Run the test

    Clean up by calling TearDown()

    Delete the test fixture. 
    
    Note that different tests in the same test case have different test
    fixture objects, and Google Test always deletes a test fixture before it
    creates the next one. Google Test does not reuse the same test fixture for
    multiple tests. Any changes one test makes to the fixture do not affect
    other tests. 


As an example, let's write tests for a FIFO queue class named Queue, which has
the following interface:

template <typename E> // E is the element type.
class Queue {
 public:
  Queue();
  void Enqueue(const E& element);
  E* Dequeue(); // Returns NULL if the queue is empty.
  size_t size() const;
  ...
};


<gtest-name-convention>
First, define a fixture class. `By convention,` you should give it the name
FooTest where Foo is the class being tested.

class Queue`Test` : public ::testing::Test {
 protected:
  virtual void SetUp() {
    q1_.Enqueue(1);           // q1_.size() == 1
    q2_.Enqueue(2);
    q2_.Enqueue(3);           // q2_.size() == 2
  }

  // virtual void TearDown() {}

  Queue<int> q0_;
  Queue<int> q1_;
  Queue<int> q2_;
};

In this case, TearDown() is not needed since we don't have to clean up after
each test, other than what's already done by the destructor.

Now we'll write tests using TEST_F() and this fixture.

TEST_F(Queue`Test`, IsEmptyInitially) {
  EXPECT_EQ(0, q0_.size());
}

TEST_F(QueueTest, DequeueWorks) {
  int* n = q0_.Dequeue();
  EXPECT_EQ(NULL, n);

  n = q1_.Dequeue();
  ASSERT_TRUE(n != NULL);
  EXPECT_EQ(1, *n);
  EXPECT_EQ(0, q1_.size());
  delete n;

  n = q2_.Dequeue();
  ASSERT_TRUE(n != NULL);
  EXPECT_EQ(2, *n);
  EXPECT_EQ(1, q2_.size());
  delete n;
}

The above uses both ASSERT_* and EXPECT_* assertions. The rule of thumb is to
use EXPECT_* when you want the test to continue to reveal more errors after
the assertion failure, and use ASSERT_* when continuing after failure doesn't
make sense. For example, the second assertion in the Dequeue test is
ASSERT_TRUE(n != NULL), as we need to dereference the pointer n later, which
would lead to a segfault when n is NULL.

When these tests run, the following happens:

    Google Test constructs a QueueTest object (let's call it t1 ).

    t1.SetUp() initializes t1 .

    The first test ( IsEmptyInitially ) runs on t1 .

    t1.TearDown() cleans up after the test finishes.

    t1 is destructed.

    The above steps are repeated on another QueueTest object, this time
    running the DequeueWorks test. 

Note: Google Test automatically saves all Google Test flags when a test object
is constructed, and restores them when it is destructed.


Invoking the Tests

TEST() and TEST_F() implicitly register their tests with Google Test. So,
  unlike with many other C++ testing frameworks, you don't have to re-list all
  your defined tests in order to run them.

After defining your tests, you can run them with RUN_ALL_TESTS(), which
returns 0 if all the tests are successful, or 1 otherwise. Note that
RUN_ALL_TESTS() runs all tests in your link unit -- they can be from different
test cases, or even different source files.

When invoked, the RUN_ALL_TESTS() macro:

    Saves the state of all Google Test flags.

    Creates a test fixture object for the first test.

    Initializes it via SetUp().

    Runs the test on the fixture object.

    Cleans up the fixture via TearDown().

    Deletes the fixture.

    Restores the state of all Google Test flags.

    Repeats the above steps for the next test, until all tests have run. 

In addition, if the text fixture's constructor generates a fatal failure in
step 2, there is no point for step 3 - 5 and they are thus skipped. Similarly,
     if step 3 generates a fatal failure, step 4 will be skipped.

Important: You must not ignore the return value of RUN_ALL_TESTS(), or gcc
will give you a compiler error. The rationale for this design is that the
automated testing service determines whether a test has passed based on its
exit code, not on its stdout/stderr output; thus your main() function must
return the value of RUN_ALL_TESTS().

Also, you should call RUN_ALL_TESTS() only once. Calling it more than once
conflicts with some advanced Google Test features (e.g. thread-safe death
    tests) and thus is not supported.


Writing the main() Function

You can start from this boilerplate:

#include "this/package/foo.h"
#include "gtest/gtest.h"

namespace {

// The fixture for testing class Foo.
class FooTest : public ::testing::Test {
 protected:
  // You can remove any or all of the following functions if its body
  // is empty.

  FooTest() {
    // You can do set-up work for each test here.
  }

  virtual ~FooTest() {
    // You can do clean-up work that doesn't throw exceptions here.
  }

  // If the constructor and destructor are not enough for setting up
  // and cleaning up each test, you can define the following methods:

  virtual void SetUp() {
    // Code here will be called `immediately after the constructor` (right
    // before each test).
  }

  virtual void TearDown() {
    // Code here will be called `immediately after each test` (right
    // before the destructor).
  }

  // Objects declared here can be used by all tests in the test case for Foo.
};

// Tests that the Foo::Bar() method does Abc.
TEST_F(FooTest, MethodBarDoesAbc) {
  const string input_filepath = "this/package/testdata/myinputfile.dat";
  const string output_filepath = "this/package/testdata/myoutputfile.dat";
  Foo f;
  EXPECT_EQ(0, f.Bar(input_filepath, output_filepath));
}

// Tests that Foo does Xyz.
TEST_F(FooTest, DoesXyz) {
  // Exercises the Xyz feature of Foo.
}

}  // namespace

int main(int argc, char **argv) {
  ::testing::InitGoogleTest(&argc, argv);
  return RUN_ALL_TESTS();
}

The ::testing::InitGoogleTest() function parses the command line for Google
Test flags, and removes all recognized flags. This allows the user to control
a test program's behavior via various flags, which we'll cover in
AdvancedGuide. You must call this function before calling RUN_ALL_TESTS(), or
the flags won't be properly initialized.

On Windows, InitGoogleTest() also works with wide strings, so it can be used
in programs compiled in UNICODE mode as well.

But maybe you think that writing all those main() functions is too much work?
We agree with you completely and that's why Google Test provides a basic
implementation of main(). If it fits your needs, then just link your test with
gtest_main library and you are good to go.  


Important note for Visual C++ users

If you put your tests into a library and your main() function is in a
different library or in your .exe file, those tests will not run. The reason
is a bug in Visual C++. When you define your tests, Google Test creates
certain static objects to register them. These objects are not referenced from
elsewhere but their constructors are still supposed to run. When Visual C++
linker sees that nothing in the library is referenced from other places it
throws the library out. You have to reference your library with tests from
your main program to keep the linker from discarding it. Here is how to do it.
Somewhere in your library code declare a function:

__declspec(dllexport) int PullInMyLibrary() { return 0; }

If you put your tests in a static library (not DLL) then __declspec(dllexport)
  is not required. Now, in your main program, write a code that invokes that
  function:

int PullInMyLibrary();
static int dummy = PullInMyLibrary();

This will keep your tests referenced and will make them register themselves at
startup.

In addition, if you define your tests in a static library, add /OPT:NOREF to
your main program linker options. If you use MSVC++ IDE, go to your .exe
project properties/Configuration Properties/Linker/Optimization and set
References setting to Keep Unreferenced Data (/OPT:NOREF). This will keep
Visual C++ linker from discarding individual symbols generated by your tests
from the final executable.

There is one more pitfall, though. If you use Google Test as a static library
(that's how it is defined in gtest.vcproj) your tests must also reside in a
static library. If you have to have them in a DLL, you must change Google Test
to build into a DLL as well. Otherwise your tests will not run correctly or
will not run at all. The general conclusion here is: make your life easier -
do not write your tests in libraries!


Where to Go from Here

Congratulations! You've learned the Google Test basics. You can start writing
and running Google Test tests, read some samples, or continue with
AdvancedGuide, which describes many more useful Google Test features. 


Known Limitations

Google Test is designed to be thread-safe. The implementation is thread-safe
on systems where the pthreads library is available. It is currently unsafe to
use Google Test assertions from two threads concurrently on other systems
(e.g. Windows). In most tests this is not an issue as usually the assertions
are done in the main thread. If you want to help, you can volunteer to
implement the necessary synchronization primitives in gtest-port.h for your
platform. 


<gtest-doc-advanced>
https://github.com/google/googletest/blob/master/googletest/docs/AdvancedGuide.md


<gtest-exception>
Exception Assertions

These are for verifying that a piece of code throws (or does not throw) an
exception of the given type:

ASSERT_THROW(statement, exception_type);  
EXPECT_THROW(statement, exception_type);  

statement throws an exception of the given type

<ex>
ASSERT_THROW(portfolio_.Purchase(IBM, 0), InvalidPurchaseException);


    EXPECT_CALL(*eilgibility_service, checkEligible(_))
        .Times(AtLeast(1))
        .WillRepeatedly(Throw(std::runtime_error("HALTED")));


={============================================================================
*kt_dev_test_002* gtest-tdd

<ex>
TEST(ProcessMovies_PassEmptyMovieList_ReturnEmptyResult)
{}


Reading the test case name and test name together, left to right, reveals a
sentence that describes what we want to verify: Soundex encoding retains
[the] sole letter of [a] one-letter word. 

TEST(SoundexEncoding, RetainsSoleLetterOfOneLetterWord)
{}


<gtest-disabled> gtest-skip gtest-ignore
The both works but prefer #2 since it's for a test

TEST( DISABLED_WeatherStationUserInterface, snow_exception )
TEST( WeatherStationUserInterface, DISABLED_snow_exception )
{
}


[----------] Global test environment tear-down
[==========] 3 tests from 1 test case ran. (6 ms total)
[  PASSED  ] 3 tests.

  YOU HAVE 1 DISABLED TEST

kyoupark@kit-debian64:~/git/kb/code/t_ex_gmock$ 

*TN* Although it is disabled to run, it is still complied and see compiler
errors.


<gtest-run-single-test>
TEST(CxxStringTest, MakeTokensFromString)
{
    const std::string input{"Name|Address|Phone"};

    MakeToken mt;
    EXPECT_EQ(true, mt.makeToken(input));
}

kyoupark@kit-debian64:~/git/kb/code/t_ex_string$ ./test_t_string --gtest_filter=MakeTokensFromString
Note: Google Test filter = MakeTokensFromString
[==========] Running 0 tests from 0 test cases.
[==========] 0 tests from 0 test cases ran. (1 ms total)
[  PASSED  ] 0 tests.
kyoupark@kit-debian64:~/git/kb/code/t_ex_string$ ./test_t_string --gtest_filter=*MakeTokensFromString
Note: Google Test filter = *MakeTokensFromString
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from CxxStringTest
[ RUN      ] CxxStringTest.MakeTokensFromString
[       OK ] CxxStringTest.MakeTokensFromString (0 ms)
[----------] 1 test from CxxStringTest (0 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (4 ms total)
[  PASSED  ] 1 test.
kyoupark@kit-debian64:~/git/kb/code/t_ex_string$


<tdd-test-driven>

TDD: Test-Driving vs. Testing

Whats the right answer? Maybe the most important consideration is that we are
test-driving, not testing. Is there a difference? you ask. Yes. Using a
testing technique, you would seek to exhaustively analyze the specification in
question (and possibly the code) and devise tests that exhaustively cover the
behavior. TDD is instead a technique for driving the design of the code. Your
tests primarily serve the purpose of specifying the behavior of what you will
build. The tests in TDD are almost a by-product of the process. They provide
you with the necessary confidence to make subsequent changes to the code.

note: Again, TDD is not about testing so do not need many assertions.

What about the tests? Do we need three assertions in
ReplacesConsonantsWithAppropriateDigits? To answer that question, we ask
ourselves whether  having the additional assertions provides increased
understanding of how the feature works. We answer ourselves: probably not. We
eliminate two assertions, change the remaining one to use ASSERT_THAT, and
choose a different encoding just to bolster our confidence a little.


In contrast, test-driving leaves behind a clear document. We waste no analysis
time uncovering a choice made moons ago.


={============================================================================
*kt_dev_test_002* gtest-exception

By default, Google Mock swallows the problem and keeps running the rest of
your tests. If you prefer to crash the tests on an uncaught exception, you can
run Google Mock with the following command-line
option: 

--gtest_catch_exceptions=0


<gtest-gdb>

TEST(CxxStringTest, MakeTokensFromString)
{
}

Breakpoint 1, CxxStringTest_MakeTokensFromString_Test::TestBody (this=0x686f00) at t_string.cpp:275
275         MakeToken mt2("Name|Address");

TEST_F(SoundexEncoding, RetainsSoleLetterOfOneLetterWord)

void SoundexEncoding_RetainsSoleLetterOfOneLetterWord_Test::SoundexEncoding_RetainsSoleLetterOfOneLetterWord_Test();
void SoundexEncoding_RetainsSoleLetterOfOneLetterWord_Test::TestBody();
void SoundexEncoding_RetainsSoleLetterOfOneLetterWord_Test::~SoundexEncoding_RetainsSoleLetterOfOneLetterWord_Test();

(gdb) info functions _Test::*


={============================================================================
*kt_dev_test_200* gmock-install

googlemock
This project has been absorbed into the GoogleTest project. All open
googlemock issues have been moved there. https://github.com/google/googletest


Requirements for Contributors

We welcome patches. If you plan to contribute a patch, you need to build
Google Mock and its tests, which has further requirements:

Automake version 1.9 or newer
Autoconf version 2.59 or newer
Libtool / Libtoolize
Python version 2.3 or newer (for running some of the tests and re-generating
certain source files from templates)


To prepare the Autotools build system:

cd googlemock
autoreconf -fvi

export GTEST_DIR=~/works/googletest/googletest
export GMOCK_DIR=~/works/googletest/googlemock

g++ -isystem ${GTEST_DIR}/include -I${GTEST_DIR} \
    -isystem ${GMOCK_DIR}/include -I${GMOCK_DIR} \
    -pthread -c ${GTEST_DIR}/src/gtest-all.cc

g++ -isystem ${GTEST_DIR}/include -I${GTEST_DIR} \
    -isystem ${GMOCK_DIR}/include -I${GMOCK_DIR} \
    -pthread -c ${GMOCK_DIR}/src/gmock-all.cc

ar -rv libgmock.a gtest-all.o gmock-all.o

ar: creating libgmock.a
a - gtest-all.o
a - gmock-all.o

Next, you should compile your test source file with ${GTEST_DIR}/include and
${GMOCK_DIR}/include in the header search path, and link it with gmock and any
other necessary libraries:

g++ -isystem ${GTEST_DIR}/include -isystem ${GMOCK_DIR}/include \
    -pthread path/to/your_test.cc libgmock.a -o your_test


As an example, the make/ directory contains a Makefile that you can use to
build Google Mock on systems where GNU make is available (e.g. Linux, Mac OS
    X, and Cygwin). It doesn't try to build Google Mock's own tests. Instead,
it just builds the Google Mock library and a sample test. You can use it as a
  starting point for your own build script.

If the default settings are correct for your environment, the following
commands should succeed:

cd ${GMOCK_DIR}/make
make
./gmock_test

g++ -isystem ../../googletest/include -isystem ../include -g -Wall -Wextra -pthread -c ../test/gmock_test.cc
g++ -isystem ../../googletest/include -isystem ../include -I../../googletest -I.. -g -Wall -Wextra -pthread \
            -c ../src/gmock-all.cc
g++ -isystem ../../googletest/include -isystem ../include -I../../googletest -I.. -g -Wall -Wextra -pthread \
            -c ../../googletest/src/gtest-all.cc
g++ -isystem ../../googletest/include -isystem ../include -I../../googletest -I.. -g -Wall -Wextra -pthread \
            -c ../src/gmock_main.cc


={============================================================================
*kt_dev_test_200* gmock-doc

ForDummies -- start here if you are new to Google Mock.

CheatSheet -- a quick reference.
https://code.google.com/p/googlemock/wiki/CheatSheet

CookBook -- recipes for doing various tasks using Google Mock.
FrequentlyAskedQuestions -- check here before asking a question on the mailing list.


={============================================================================
*kt_dev_test_201* gmock-for-dummies

https://github.com/google/googletest/blob/master/googlemock/docs/ForDummies.md

* What Is Google C++ Mocking Framework?

When you write a prototype or test, often it's not feasible or wise to rely on
real objects 'entirely'. A mock object implements the same interface as a real
object so it can be used as one, but lets you 'specify' at *run-time* how it
will be used and what it should do; which methods will be called? in which
order? how many times? with what arguments? what will they return? etc.

Note: It is easy to confuse the term fake objects with mock objects. Fakes and
mocks actually mean very different things in the Test-Driven Development (TDD)
community:

<fake-and-mock>
Fake objects have working implementations, but usually take some shortcut;
perhaps to make the operations less expensive, which makes them not suitable
  for production. An in-memory file system would be an example of a fake.

Mocks are objects pre-programmed with expectations, which form a specification
  of the calls they are expected to receive. 

note: not sure about difference

If all this seems too abstract for you, don't worry - the most important thing
to remember is that a mock allows you to check the 'interaction' between
'itself' and 'code' that uses it. The difference between fakes and mocks will
become much clearer once you start to use mocks.

Google C++ Mocking Framework (or Google Mock for short) is a library
(sometimes we also call it a "framework" to make it sound cool) for creating
mock classes and using them. It does to C++ what jMock and EasyMock do to
Java.


Using Google Mock involves three basic steps:

1. Use some simple macros to describe the interface you want to mock, and they
   will expand to the implementation of your mock class.

2. Create some mock objects and specify its expectations and behavior using an
   intuitive syntax.

3. Exercise code that uses the mock objects. Google Mock will catch any
   violation of the expectations as soon as it arises. 


* Why Google Mock?

While mock objects help you 'remove' unnecessary 'dependencies' in tests and
make them fast and reliable, using mocks manually in C++ is hard:

* Someone has to implement the mocks. The job is usually tedious and
  error-prone. No wonder people go great distance to avoid it.

* The quality of those manually written mocks is a bit, uh, unpredictable. You
  may see some really polished ones, but you may also see some that were
  hacked up in a hurry and have all sorts of ad hoc restrictions.

* The knowledge you gained from using one mock doesn't transfer to the next. 

In contrast, Java and Python programmers have some fine mock frameworks, which
automate the creation of mocks. As a result, mocking is a proven effective
technique and widely adopted practice in those communities. Having the right
tool absolutely makes the difference.


Google Mock was built to help C++ programmers. It was inspired by jMock and
EasyMock, but designed with C++'s specifics in mind. It is your friend if any
of the following problems is bothering you:

<gmock-prototying>
* You are stuck with a sub-optimal design and wish you had done more
  prototyping before it was too late, but prototyping in C++ is by no means
  "rapid".

* Your tests are slow as they depend on too many libraries or use expensive
  resources e.g. a database.

* Your tests are brittle as some resources they use are unreliable e.g. the
  network.

* You want to test how your code handles a failure (e.g. a file checksum
  error), but it's not easy to cause one.

* You need to make sure that your module interacts with other modules in the
  right way, but it's hard to observe the interaction; therefore you resort to
  observing the side effects at the end of the action, which is awkward at
  best.

* You want to `mock out` your dependencies, except that they don't have mock
  implementations yet; and, frankly, you aren't thrilled by some of those
  hand-written mocks. 


We encourage you to use Google Mock as:

* a design tool, for it lets you experiment with your interface design early
  and often. More iterations lead to better designs!

* a testing tool to cut your tests' outbound dependencies and probe the
  interaction between your module and its collaborators. 


Getting Started

Using Google Mock is easy! Inside your C++ source file, just #include
"gtest/gtest.h" and "gmock/gmock.h", and you are ready to go. 


* A Case for Mock Turtles

Let's look at an example. Suppose you are developing a graphics program that
relies on a LOGO-like API for drawing. How would you test that it does the
right thing? Well, you can run it and compare the screen with a golden screen
snapshot, but let's admit it: tests like this are expensive to run and
fragile. 

What if you just upgraded to a shiny new graphics card that has better
anti-aliasing? Suddenly you have to update all your golden images. It would be
too painful if all your tests are like this. Fortunately, you learned about
Dependency Injection and know the right thing to do: instead of having your
application talk to the drawing API directly, wrap the API in an interface
(say, Turtle) and code to that interface:

// dependency injection?

class Turtle {
  ...
  virtual ~Turtle() {}
  virtual void PenUp() = 0;
  virtual void PenDown() = 0;
  virtual void Forward(int distance) = 0;
  virtual void Turn(int degrees) = 0;
  virtual void GoTo(int x, int y) = 0;
  virtual int GetX() const = 0;
  virtual int GetY() const = 0;
};

Note that the destructor of Turtle `must be virtual,` as is the case for all
classes you intend to inherit from - otherwise the destructor of the derived
class will not be called when you delete an object through a base pointer, and
you'll get corrupted program states like memory leaks.

You can control whether the turtle's movement will leave a trace using PenUp()
and PenDown(), and control its movement using Forward(), Turn(), and GoTo().
Finally, GetX() and GetY() tell you the current position of the turtle.

Your program will normally use a real implementation of this interface. In
tests, you can use a *mock-implementation* instead. This allows you to easily
check what drawing primitives your program is calling, with what arguments,
      and in which order. 

Tests written this way are much more robust, they won't break because your new
machine does anti-aliasing differently, easier to read and maintain; the
intent of a test is expressed in the code, not in some binary images, and run
much, much faster. 


* Writing the Mock Class

If you are lucky, the mocks you need to use have already been implemented by
some nice people. If, however, you find yourself in the position to write a
mock class, relax - Google Mock turns this task into a fun game! Well, almost. 
  

** How to Define It

Using the Turtle interface as example, here are the simple steps you need to
follow:

1. 'derive' a class MockTurtle from Turtle.

2. Take a virtual function of Turtle while it's possible to mock non-virtual
methods using templates, it's much more involved. Count how many arguments it
has.

3. In the 'public' section of the child class, write MOCK_METHODn(); or
MOCK_CONST_METHODn(); if you are mocking a const method, where n is the
`number of the arguments`; if you counted wrong, shame on you, and a compiler
error will tell you so.

4. Now comes the fun part: you take the function signature, cut-and-paste the
`function name` as the first argument to the macro, and leave what's left as the
second argument. in case you're curious, this is `the type of the function.`

5. Repeat until `all virtual functions you want to mock` are done. 

After the process, you should have something like:

// class Turtle {
//   ...
//   virtual ~Turtle() {}
//   virtual void PenUp() = 0;
//   virtual void PenDown() = 0;
//   virtual void Forward(int distance) = 0;
//   virtual void Turn(int degrees) = 0;
//   virtual void GoTo(int x, int y) = 0;
//   virtual int GetX() const = 0;
//   virtual int GetY() const = 0;
// };

#include "gmock/gmock.h"  // Brings in Google Mock.

class MockTurtle : public Turtle {
 public:
  ...
  MOCK_METHOD0(PenUp, void());
  MOCK_METHOD0(PenDown, void());
  MOCK_METHOD1(Forward, void(int distance));
  MOCK_METHOD1(Turn, void(int degrees));
  MOCK_METHOD2(GoTo, void(int x, int y));
  MOCK_CONST_METHOD0(GetX, int());
  MOCK_CONST_METHOD0(GetY, int());
};

// dtor has null implementation.
// do not need to define.

You don't need to 'define' these mock methods somewhere else - the
MOCK_METHOD* macros will 'generate' the 'definitions' for you. It's that
simple! Once you get the hang of it, you can pump out mock classes faster than
your source-control system can handle your check-ins.

Tip: If even this is too much work for you, you'll find the gmock_gen.py tool
in Google Mock's scripts/generator/ directory (courtesy of the cppclean
    project) useful. This command-line tool requires that you have Python 2.4
installed. You give it a C++ file and the name of an abstract class defined in
it, and it will print the definition of the mock class for you. Due to the
complexity of the C++ language, this script may not always work, but it can be
quite handy when it does. For more details, read the user documentation. 

http://code.google.com/p/googlemock/source/browse/trunk/scripts/generator/README


** Where to Put It

When you define a mock class, you need to decide where to put its definition.
Some people put it in a *_test.cc. This is fine when the interface being
mocked (say, Foo) is owned by the same person or team. Otherwise, when the
owner of Foo changes it, your test could break. You can't really expect Foo's
maintainer to fix every test that uses Foo, can you?

The rule of thumb is: if you need to mock Foo and it's owned by others, define
the mock class in Foo's package. Better, in a testing sub-package such that
you can clearly separate production code and testing utilities, and put it in
a mock_foo.h. Then everyone can reference mock_foo.h from their tests. If Foo
ever changes, there is only one copy of MockFoo to change, and only tests that
depend on the changed methods need to be fixed.

// use a header and put it in testing sub package.

Another way to do it: you can introduce a thin layer FooAdaptor on top of Foo
and code to this new interface. Since you own FooAdaptor, you can absorb
changes in Foo much more easily. While this is more work initially, carefully
choosing the adaptor interface can make your code easier to write and more
readable (a net win in the long run), as you can choose FooAdaptor to fit your
specific domain much better than Foo does. 


* Using Mocks in Tests

Once you have a mock class, using it is easy. The typical work flow is:

1. Import the Google Mock names from the "testing" namespace such that you can
use them unqualified (You only have to do it once per file. Remember that
    namespaces are a good idea and good for your health.).

2. `create some mock objects.`

3. `specify your expectations on them.` How many times will a method be
called?  With what arguments?  What should it do? etc.

4. `exercise some code that uses the mocks;` optionally, check the result
using Google Test assertions. If a mock method is called more than expected or
with wrong arguments, you'll get an error immediately.

5. When a mock is destructed, Google Mock will automatically check whether all
expectations on it have been satisfied. note: this causes a test failure. 

Here's an example: 

#include "path/to/mock-turtle.h"              // 0
#include "gmock/gmock.h"
#include "gtest/gtest.h"

using ::testing::AtLeast;                     // 1

TEST(PainterTest, CanDrawSomething) {
  MockTurtle turtle;                          // 2
  EXPECT_CALL(turtle, PenDown())              // 3
      .Times(AtLeast(1));

  Painter painter(&turtle);                   // 4

  EXPECT_TRUE(painter.DrawCircle(0, 0, 10));
}                                             // 5

int main(int argc, char** argv) 
{
  // The following line must be executed to initialize Google Mock
  // (and Google Test) before running the tests.
  //
  // note: does it mean that no need to call InitGoogleTest()?

  ::testing::InitGoogleMock(&argc, argv);
  return RUN_ALL_TESTS();
}

As you might have guessed, this test checks that PenDown() is called at least
once. If the painter object didn't call this method, your test will 'fail'
with a message like this:

path/to/my_test.cc:119: Failure
Actual function call count doesn't match this expectation:
Actually: never called;
Expected: called at least once.

// the painter object is one under test that uses a turtle. so here is mocking
// up "turtle" to test painter object.

// Tip 1: If you run the test from an Emacs buffer, you can hit <Enter> on the
// line number displayed in the error message to jump right to the failed
// expectation.

Tip 2: If your mock objects are never 'deleted', the final verification won't
happen. Therefore it's a good idea to use a heap leak checker in your tests
when you allocate mocks on the heap.


Important note: 
Google Mock requires expectations to be set 'before' the mock functions are
called, otherwise the behavior is 'undefined'. In particular, you mustn't
interleave EXPECT_CALL()s and calls to the mock functions.

This means EXPECT_CALL() should be read as expecting that a call will occur in
the future, not that a call has occurred. Why does Google Mock work like that?
Well, specifying the expectation beforehand allows Google Mock to report a
violation as soon as it arises, when the context such as stack trace, etc is
still available. This makes debugging much easier.

Admittedly, this test is contrived and doesn't do much. You can easily achieve
the same effect without using Google Mock. However, as we shall reveal soon,
    Google Mock allows you to do much more with the mocks. 


* Using Google Mock with Any Testing Framework

If you want to use something other than Google Test (e.g. CppUnit or CxxTest)
  as your testing framework, just change the main() function in the previous
  section to:

int main(int argc, char** argv) {
  // The following line causes Google Mock to throw an exception on failure,
  // which will be interpreted by your testing framework as a test failure.
  ::testing::GTEST_FLAG(throw_on_failure) = true;
  ::testing::InitGoogleMock(&argc, argv);
  ... whatever your testing framework requires ...
}

This approach has a catch: it makes Google Mock throw an exception from a mock
object's destructor sometimes. With some compilers, this sometimes causes the
test program to crash. You'll still be able to notice that the test has
failed, but it's not a graceful failure.

A better solution is to use Google Test's event listener API to report a test
failure to your testing framework properly. You'll need to implement the
OnTestPartResult() method of the event listener interface, but it should be
straightforward.

If this turns out to be too much work, we suggest that you stick with Google
Test, which works with Google Mock seamlessly (in fact, it is technically part
    of Google Mock.). If there is a reason that you cannot use Google Test,
  please let us know. 


* Setting Expectations

<gmock-expectation>
The key to using a mock object successfully is to set the right expectations
on it. If you set the expectations too 'strict', your test will fail as the
result of unrelated changes. If you set them too 'loose', bugs can slip
through. You want to do it just right such that your test can catch exactly
the kind of bugs you intend it to catch. Google Mock provides the necessary
means for you to do it "just right." 

** General Syntax

In Google Mock we use the EXPECT_CALL() macro to set an expectation on a mock
method. The general 'syntax' is:

EXPECT_CALL(mock_object, method(`matchers`))
    .Times(cardinality)
    .WillOnce(action)
    .WillRepeatedly(action);

The macro has two arguments: first the mock object, and then the method and
its arguments. Note that the two are separated by a comma (,), not a period
(.). Why using a comma? The answer is that it was necessary for technical
reasons.

The macro can be followed by some optional clauses that provide more
information about the expectation. We'll discuss how each clause works in the
coming sections.

This syntax is designed to make an expectation read like English. For example,
     you can probably guess that

using ::testing::Return;...

EXPECT_CALL(turtle, GetX())
    .Times(5)
    .WillOnce(Return(100))
    .WillOnce(Return(150))
    .WillRepeatedly(Return(200));

says that the turtle object's GetX() method will be called five times, it will
return 100 the first time, 150 the second time, and then 200 every time. Some
people like to call this style of syntax a Domain-Specific Language (DSL).

Note: Why do we use a macro to do this? It serves two purposes: first it makes
expectations easily identifiable (either by grep or by a human reader), and
second it allows Google Mock to include the source file location of a failed
expectation in messages, making debugging easier. 


Knowing When to Expect

ON_CALL is likely the single most under-utilized construct in Google Mock.

There are basically two constructs for defining the behavior of a mock object:
ON_CALL and EXPECT_CALL. The difference? ON_CALL defines what happens when a
mock method is called, but doesn't imply any expectation on the method being
called. EXPECT_CALL not only defines the behavior, but also sets an
expectation that the method will be called with the given arguments, for the
given number of times (and in the given order when you specify the order too).

Since EXPECT_CALL does more, isn't it better than ON_CALL? Not really. Every
EXPECT_CALL adds a constraint on the behavior of the code under test. Having
more constraints than necessary is baaad - even worse than not having enough
constraints.

This may be counter-intuitive. How could tests that verify more be worse than
tests that verify less? Isn't verification the whole point of tests?

The answer, lies in what a test should verify. A good test verifies the
contract of the code. If a test over-specifies, it doesn't leave enough
freedom to the implementation. As a result, changing the implementation
without breaking the contract (e.g. refactoring and optimization), which
should be perfectly fine to do, can break such tests. Then you have to spend
time fixing them, only to see them broken again the next time the
implementation is changed.

Keep in mind that one doesn't have to verify more than one property in one
test. `In fact, it's a good style to verify only one thing in one test.` If
you do that, a bug will likely break only one or two tests instead of dozens
(which case would you rather debug?). If you are also in the habit of giving
tests descriptive names that tell what they verify, you can often easily guess
what's wrong just from the test log itself.

So use ON_CALL by default, and only use EXPECT_CALL when you actually intend
to verify that the call is made. 

// ? share?

For example, you may have a bunch of ON_CALLs in your test fixture to set the
common mock behavior shared by all tests in the same group, and write
(scarcely) different EXPECT_CALLs in different TEST_Fs to verify different
aspects of the code's behavior. 

Compared with the style where each TEST has many EXPECT_CALLs, this leads to
tests that are more resilient to implementational changes (and thus less
    likely to require maintenance) and makes the intent of the tests more
obvious (so they are easier to maintain when you do need to maintain them).

If you are bothered by the "Uninteresting mock function call" message printed
when a mock method without an EXPECT_CALL is called, you may use a NiceMock
instead to suppress all such messages for the mock object, or suppress the
message for specific methods by adding EXPECT_CALL(...).Times(AnyNumber()). DO
NOT suppress it by blindly adding an EXPECT_CALL(...), or you'll have a test
that's a pain to maintain.


** Matchers: What Arguments Do We Expect?

When a mock function takes `arguments`, we 'must' specify what arguments we are
expecting; for example:

// Expects the turtle to move forward by 100 units.
EXPECT_CALL(turtle, Forward(100));

<gmock-any-args>
Sometimes you may not want to be too specific. Remember that talk about tests
being too rigid? Over specification leads to brittle tests and obscures the
intent of tests. Therefore we encourage you to specify only what's necessary -
no more, no less. 

<_>
If you care to check that Forward() will be called but aren't interested in
its actual argument, write _ as the argument, which means "anything goes":

using ::testing::_;
...
// Expects the turtle to move forward.
EXPECT_CALL(turtle, Forward(_));

_ is an instance of what we call 'matchers'. A matcher is like a predicate and
can test whether an argument is what we'd expect. You can use a matcher inside
EXPECT_CALL() wherever a function argument is expected.

A list of built-in matchers can be found in the CheatSheet. 

For example, here's the Ge; greater than or equal matcher:
https://code.google.com/p/googlemock/wiki/CheatSheet

using ::testing::Ge;...

EXPECT_CALL(turtle, Forward(Ge(100)));

This checks that the turtle will be told to go forward by at least 100 units. 

<ex>
should be exact match to argument

// when mock function argument is not a value

TEST( WeatherStationUserInterface, test_sample)
{
    auto weather_station = std::make_shared<MockWeatherStation>();

    EXPECT_CALL( *weather_station, sample(std::string str) )    // *see*
        .WillRepeatedly(Invoke([](std::string str) -> std::string
                    {
                        return str + "!!";
                    }));
 
    UserInterface ui( weather_station );
    EXPECT_EQ( "this is sample!!", ui.test_sample() );
}

In file included from /home/kyoupark/works/googletest/googlemock/include/gmock/gmock-generated-function-mockers.h:43:0,
                 from /home/kyoupark/works/googletest/googlemock/include/gmock/gmock.h:61,
                 from t_ex_mock.cpp:7:
t_ex_mock.cpp: In member function virtual void WeatherStationUserInterface_test_sample_Test::TestBody():
t_ex_mock.cpp:189:54: error: expected primary-expression before ) token
     EXPECT_CALL( *weather_station, sample(std::string) )


// when mock function not gets called with exact value match

class UserInterface
{
public:
    std::string test_sample()
    {
        return weather_station_->sample("this is not sample");  // *see*
    }
};

TEST( WeatherStationUserInterface, test_sample)
{
    auto weather_station = std::make_shared<MockWeatherStation>();

    EXPECT_CALL( *weather_station, sample("this is sample") )   // *see*
        .WillRepeatedly(Invoke([](std::string str) -> std::string
                    {
                        return str + "!!";
                    }));
 
    UserInterface ui( weather_station );
    EXPECT_EQ( "this is sample!!", ui.test_sample() );
}

unknown file: Failure

Unexpected mock function call - returning default value.
    Function call: sample("this is not sample")
          Returns: ""
Google Mock tried the following 1 expectation, but it didn't match:

t_ex_mock.cpp:189: EXPECT_CALL(*weather_station, sample("this is sample"))...
  Expected arg #0: is equal to "this is sample"
           Actual: "this is not sample"
         Expected: to be called any number of times
           Actual: never called - satisfied and active
t_ex_mock.cpp:196: Failure
Expected equality of these values:
  "this is sample!!"
  ui.test_sample()
    Which is: ""
[  FAILED  ] WeatherStationUserInterface.test_sample (3 ms)


// when expectation fails

TEST( WeatherStationUserInterface, test_sample)
{
    auto weather_station = std::make_shared<MockWeatherStation>();

    // GMock: inject more complex logic using C++11 lambdas,
    // and pattern match on the input value
    EXPECT_CALL( *weather_station, sample("this is sample") )
        .WillRepeatedly(Invoke([](std::string str) -> std::string
                    {
                        return str + "!!";
                    }));
 
    UserInterface ui( weather_station );
    EXPECT_EQ( "this is sample", ui.test_sample() );    // *see*
}

t_ex_mock.cpp:196: Failure
Expected equality of these values:
  "this is sample"
  ui.test_sample()
    Which is: "this is sample!!"
[  FAILED  ] WeatherStationUserInterface.test_sample (1 ms)


<gmock-cardinality>
** Cardinalities: How Many Times Will It Be Called? 

Cardinalities

These are used in Times() to specify how many times a mock function will be
called:

AnyNumber()	    The function can be called any number of times.
AtLeast(n)	    The call is expected at least n times.
AtMost(n)	    The call is expected at most n times.
Between(m, n)	 The call is expected between m and n (inclusive) times.

Exactly(n) or n 
The call is expected exactly n times. In particular, the call should never
happen when n is 0.


The first clause we can specify following an EXPECT_CALL() is Times(). We call
its argument a cardinality as it tells how many times the call should occur.
It allows us to repeat an expectation many times without actually writing it
as many times. More importantly, a cardinality can be "fuzzy", just like a
matcher can be. This allows a user to express the intent of a test exactly.

An interesting special case is when we say Times(0). You may have guessed - it
means that the function shouldn't be called with the 'given' arguments at all,
      and Google Mock will report a Google Test failure whenever the function
      is (wrongfully) called.

We've seen AtLeast(n) as an example of fuzzy cardinalities earlier. For the
list of built-in cardinalities you can use, see the CheatSheet.
https://code.google.com/p/googlemock/wiki/CheatSheet

*TN* 
Prefer Times() to be explicit than making a reader to think.

// The Times() clause can be omitted. If you omit Times(), Google Mock will
// 'infer' the cardinality for you. The rules are easy to remember:
// 
// * If neither WillOnce() nor WillRepeatedly() is in the EXPECT_CALL(), the
//   inferred cardinality is Times(1).
// 
// * If there are n WillOnce()'s but no WillRepeatedly(), where n >= 1, the
//   cardinality is Times(n).
// 
// * If there are n WillOnce()'s and one WillRepeatedly(), where n >= 0, the
//   cardinality is Times(AtLeast(n)). 
// 
// If only WillRepeatedly() means that 0 or more is okay. 

Quick quiz: what do you think will happen if a function is expected to be
called twice but actually called four times? 


<gmock-action>
** Actions: What Should It Do?

Remember that a mock object doesn't really have a working implementation? We
as users have to 'tell' it "what to do when a method is invoked." This is easy
in Google Mock.

First, if the return type of a mock function is a built-in type or a pointer,
  the function has `a default action`; a void function will just return, a bool
  function will return false, and other functions will return 0. 

In addition, in C++ 11 and above, a mock function whose return type is
default-constructible (i.e. has a default constructor) has a default action of
returning a default-constructed value. If you don't say anything, this
behavior will be used.

Second, if a mock function doesn't have a default action, or the default
action doesn't suit you, you `can specify the action to be taken each time` the
expectation matches using a series of WillOnce() clauses followed by an
optional WillRepeatedly(). For example,

using ::testing::Return;...
EXPECT_CALL(turtle, GetX())
    .WillOnce(Return(100))
    .WillOnce(Return(200))
    .WillOnce(Return(300));

This says that turtle.GetX() will be called exactly three times. Google Mock
inferred this from how many WillOnce() clauses we've written, since we didn't
explicitly write Times(), and will 'return' 100, 200, and 300 respectively.

using ::testing::Return;...
EXPECT_CALL(turtle, GetY())
    .WillOnce(Return(100))
    .WillOnce(Return(200))
    .WillRepeatedly(Return(300));

says that turtle.GetY() will be called at least twice (Google Mock knows this
    as we've written two WillOnce() clauses and a WillRepeatedly() while
    having no explicit Times()), will return 100 the first time, 200 the
second time, and 300 from the third time on.

Of course, if you explicitly write a Times(), Google Mock will not try to
infer the cardinality itself. What if the number you specified is larger than
there are WillOnce() clauses? Well, after all WillOnce()s are used up, Google
Mock will do the default action for the function every time (unless, of
    course, you have a WillRepeatedly().).


<can-call-function>
What can we do inside WillOnce() besides Return()? You can return a reference
using ReturnRef(variable), or invoke a pre-defined function, among others.

*important* The EXPECT_CALL() statement evaluates the action clause only
'once', even though the action may be performed many times. Therefore you must
be careful about side effects. The following may not do what you want:

int n = 100;
EXPECT_CALL(turtle, GetX())
.Times(4)
.WillRepeatedly(Return(n++));

Instead of returning 100, 101, 102, ..., consecutively, this mock function
will 'always' return 100 as n++ is only evaluated once. Similarly, Return(new
    Foo) will create a new Foo object when the EXPECT_CALL() is executed, and
will return the same pointer every time. 


Time for another quiz! What do you think the following means?

using ::testing::Return;...
EXPECT_CALL(turtle, GetY())
.Times(4)
.WillOnce(Return(100));

Obviously turtle.GetY() is expected to be called four times. But if you think
it will return 100 every time, think twice! Remember that one WillOnce()
  clause will be consumed each time the function is invoked and the 'default'
  action will be taken afterwards. So the right answer is that turtle.GetY()
  will return 100 the first time, but return 0 from the second time on, as
  returning 0 is the default action for int functions. 

// MOCK_CONST_METHOD0(GetX, int());
// MOCK_CONST_METHOD0(GetY, int());


={============================================================================
*kt_dev_test_201* gmock-sequence

** Ordered vs Unordered Calls

By default, an expectation can match a call even though an earlier expectation
hasn't been satisfied. In other words, the calls don't have to occur in the
order the expectations are specified.

Sometimes, you may want all the expected calls to occur in a 'strict' order.
To say this in Google Mock is easy:

using ::testing::InSequence;...
TEST(FooTest, DrawsLineSegment) {
  ...
  {
    InSequence dummy;

    EXPECT_CALL(turtle, PenDown());
    EXPECT_CALL(turtle, Forward(100));
    EXPECT_CALL(turtle, PenUp());
  }
  Foo();
}

By creating an object of type InSequence, `all expectations in its scope` are
put into a sequence and have to occur sequentially. Since we are just relying
on the constructor and destructor of this object to do the actual work, its
name is really irrelevant.

In this example, we test that Foo() calls the three expected functions in the
order as written. If a call is made out-of-order, it will be an error.

What if you care about the relative order of some of the calls, but not all of
them? Can you specify an arbitrary partial order? The answer is ... yes! If
you are impatient, the details can be found in the CookBook. 


https://github.com/google/googletest/blob/master/googlemock/docs/CookBook.md
Expecting Partially Ordered Calls

Sometimes requiring everything to occur in a predetermined order can lead to
brittle tests. For example, we may care about A occurring before both B and C,
        but aren't interested in the relative order of B and C. In this case,
        the test should reflect our real intent, instead of being overly
        constraining.

Google Mock allows you to impose an arbitrary DAG (directed acyclic graph) on
the calls. One way to express the DAG is to use the After clause of
EXPECT_CALL.

Another way is via the InSequence() clause (not the same as the InSequence
    class), which we borrowed from jMock 2. It's less flexible than After(),
        but more convenient when you have long chains of sequential calls, as
        it doesn't require you to come up with different names for the
        expectations in the chains. Here's how it works:

If we view EXPECT_CALL() statements as nodes in a graph, and add an edge from
node A to node B wherever A must occur before B, we can get a DAG. We use the
term "sequence" to mean a directed path in this DAG. Now, if we decompose the
DAG into sequences, we just need to know which sequences each EXPECT_CALL()
  belongs to in order to be able to reconstruct the orginal DAG.

So, to specify the partial order on the expectations we need to do two things:
first to define some Sequence objects, and then for each EXPECT_CALL() say
which Sequence objects it is part of. Expectations in the same sequence must
occur in the order they are written. For example,

  using ::testing::Sequence;

  Sequence s1, s2;

  EXPECT_CALL(foo, A())
      .InSequence(s1, s2);
  EXPECT_CALL(bar, B())
      .InSequence(s1);
  EXPECT_CALL(bar, C())
      .InSequence(s2);
  EXPECT_CALL(foo, D())
      .InSequence(s2);

specifies the following DAG (where s1 is A -> B, and s2 is A -> C -> D):

       +---> B
       |
  A ---|
       |
       +---> C ---> D

This means that A must occur before B and C, and C must occur before D.
There's no restriction about the order other than these.


={============================================================================
*kt_dev_test_201* gmock-for-multiple-expectations

** Using Multiple Expectations

So far we've only shown examples where you have a single expectation. More
realistically, you're going to specify expectations on multiple mock methods,
  which may be from 'multiple' mock objects.

By default, when a mock method is invoked, Google Mock will 'search' the
expectations in the `reverse order they are defined,` and stop when an active
expectation that `matches the arguments is found.` You can think of it as
"newer rules override older ones.". If the matching expectation cannot take
any more calls, you will get an `upper-bound-violated failure.` Here's an
example:

using ::testing::_;...
EXPECT_CALL(turtle, Forward(_));  // #1
EXPECT_CALL(turtle, Forward(10))  // #2
    .Times(2);

If Forward(10) is called three times in a row, the third time it will be an
error, as the last matching expectation (#2) has been 'saturated'. 

If, however, the third Forward(10) call is replaced by Forward(20), then it
would be 'okay', as now #1 will be the matching expectation.

Side note: Why does Google Mock search for a match in the reverse order of the
expectations? The reason is that this allows a user to set up the 'default'
expectations in a mock object's constructor or the test fixture's set-up phase
and then customize the mock by writing more specific expectations in the test
body. So, if you have two expectations on the same method, you want to put the
one with more 'specific' matchers after the other, or the more specific rule
would be shadowed by the more general one that comes after it. 


** All Expectations Are Sticky (Unless Said Otherwise)

Now let's do a quick quiz to see how well you can use this mock stuff already.
How would you test that the turtle is asked to go to the origin exactly twice.
you want to ignore any other instructions it receives?

After you've come up with your answer, take a look at ours and compare notes.
solve it yourself first - don't cheat!:

using ::testing::_;...
EXPECT_CALL(turtle, GoTo(_, _))  // #1
    .Times(AnyNumber());
EXPECT_CALL(turtle, GoTo(0, 0))  // #2
    .Times(2);

Suppose turtle.GoTo(0, 0) is called three times. In the third time, Google
  Mock will see that the arguments match expectation #2. Remember that we
  always pick the last matching expectation. 

Now, since we said that there should be only two such calls, Google Mock will
report an error immediately. This is basically what we've told you in the
"Using Multiple Expectations" section above.

This shows that expectations in Google Mock are *sticky* by default, in the
sense that they `remain active even after` we have reached their invocation
upper bounds. This is an important rule to remember, as it affects the meaning
of the spec, and is different to how it's done in many other mocking
frameworks (Why'd we do that? Because we think our rule makes the common cases
    easier to express and understand.).

Simple? Let's see if you've really understood it: what does the following code
say?

using ::testing::Return;
...
for (int i = n; i > 0; i--) {
  EXPECT_CALL(turtle, GetX())
      .WillOnce(Return(10*i));
}

If you think it says that turtle.GetX() will be called n times and will return
10, 20, 30, ..., consecutively, think twice! The problem is that, as we said,
  expectations are sticky. So, the second time turtle.GetX() is called, the
  last (latest) EXPECT_CALL() statement will match, and will immediately lead
  to an "upper bound exceeded" error - this piece of code is not very useful!

One correct way of saying that turtle.GetX() will return 10, 20, 30, ..., is
to explicitly say that the expectations are not sticky. In other words, they
should retire as soon as they are saturated:

using ::testing::Return;
...
for (int i = n; i > 0; i--) {
  EXPECT_CALL(turtle, GetX())
    .WillOnce(Return(10*i))
    .RetiresOnSaturation();
}

And, there's a better way to do it: in this case, we expect the calls to occur
in a specific order, and we line up the actions to match the order. Since the
order is important here, we should make it explicit using a sequence:

using ::testing::InSequence;
using ::testing::Return;
...
{
  InSequence s;

  for (int i = 1; i <= n; i++) {
    EXPECT_CALL(turtle, GetX())
        .WillOnce(Return(10*i))
        .RetiresOnSaturation();
  }
}

By the way, the other situation where an expectation may not be sticky is when
it's in a sequence - as soon as another expectation that comes after it in the
sequence has been used, it automatically retires (and will never be used to
    match any call). 


** Uninteresting Calls

A mock object may have many methods, and not all of them are that interesting.
For example, in some tests we may not care about how many times GetX() and
GetY() get called.

In Google Mock, if you are not interested in a method, just do 'not' say
anything about it. If a call to this method occurs, you'll see a warning in the
test output, but it won't be a failure.  

* What Now?

Congratulations! You've learned enough about Google Mock to start using it. Now,
    you might want to join the googlemock discussion group and actually write
    some tests using Google Mock - it will be fun. Hey, it may even be addictive
    - you've been warned.

Then, if you feel like increasing your mock quotient, you should move on to the
CookBook. You can learn many advanced features of Google Mock there -- and
advance your level of enjoyment and testing bliss. 


={============================================================================
*kt_dev_test_203* gmock-side-effect

Mocking Side Effects

Sometimes a method exhibits its effect not via returning a value but via side
effects. For example, it may change some global state or modify an output
argument. To mock side effects, in general you can define your own action by
implementing ::testing::ActionInterface.

If all you need to do is to change an output argument, the built-in
SetArgPointee() action is convenient:


SetArgPointee<N>(value)	
Assign value to the variable pointed by the N-th (0-based) argument.


using ::testing::SetArgPointee;

class MockMutator : public Mutator {
 public:
  MOCK_METHOD2(Mutate, void(bool mutate, int* value));
  ...
};
...

  MockMutator mutator;
  EXPECT_CALL(mutator, Mutate(true, _))
      .WillOnce(SetArgPointee<1>(5));

In this example, when mutator.Mutate() is called, we will assign 5 to the int
variable pointed to by argument #1 (0-based).

SetArgPointee() conveniently makes an internal copy of the value you pass to
it, removing the need to keep the value in scope and alive. The implication
however is that the value must have a copy constructor and assignment
operator.

If the mock method also needs to return a value as well, you can chain
SetArgPointee() with Return() using DoAll():

using ::testing::_;
using ::testing::Return;
using ::testing::SetArgPointee;

class MockMutator : public Mutator {
 public:
  ...
  MOCK_METHOD1(MutateInt, bool(int* value));
};
...

  MockMutator mutator;
  EXPECT_CALL(mutator, MutateInt(_))
      .WillOnce(DoAll(SetArgPointee<0>(5),
                      Return(true)));

If the output argument is an array, use the SetArrayArgument<N>(first, last)
  action instead. It copies the elements in source range [first, last) to the
  array pointed to by the N-th (0-based) argument:

using ::testing::NotNull;
using ::testing::SetArrayArgument;

class MockArrayMutator : public ArrayMutator {
 public:
  MOCK_METHOD2(Mutate, void(int* values, int num_values));
  ...
};
...

  MockArrayMutator mutator;
  int values[5] = { 1, 2, 3, 4, 5 };
  EXPECT_CALL(mutator, Mutate(NotNull(), 5))
      .WillOnce(SetArrayArgument<0>(values, values + 5));

This also works when the argument is an output iterator:

using ::testing::_;
using ::testing::SetArrayArgument;

class MockRolodex : public Rolodex {
 public:
  MOCK_METHOD1(GetNames, void(std::back_insert_iterator<vector<string> >));
  ...
};
...

  MockRolodex rolodex;
  vector<string> names;
  names.push_back("George");
  names.push_back("John");
  names.push_back("Thomas");
  EXPECT_CALL(rolodex, GetNames(_))
      .WillOnce(SetArrayArgument<0>(names.begin(), names.end()));


={============================================================================
*kt_dev_test_202* gmock-and-cppunit

<1> use gmock and cppunit

class X {

class RepositoryTest : public CppUnit::TestFixture 
{

    void setUp()
    {
        // note: 
        // Create a mock object once and use it throughout tests
        mockMB = boost::make_shared<NS_IRON_SYSTEM::MockEventRepositoryAsync>();
    }

    void testGetScheduleEventFull()
    {
        NS_X_SYSTEM::Result result;
        result.recordIdentifier = "id12";

        // note: <custom-action>
        EXPECT_CALL(*mockMB, getEvent("abc"))
            .WillOnce(returnNewCompletedFuture(result));

        // note: Broker is from setUp()
        NS_Z::Future<FullEventPtr> f = Broker->getFullEvent("abc");
        boost::shared_ptr<FullEvent> fe(f.get());

        CPPUNIT_ASSERT(fe);
        CPPUNIT_ASSERT_EQUAL(result.recordIdentifier, fe->getRecordIdentifier());
    }
};


={============================================================================
*kt_dev_test_203* gmock-action

<gmock-action-invoke>
Using Functions/Methods/Functors as Actions

If the built-in actions don't suit you, you can easily use an existing
function, method, or functor as an action:

using ::testing::_;
using ::testing::Invoke;

class MockFoo : public Foo {
 public:
  MOCK_METHOD2(Sum, int(int x, int y));
  MOCK_METHOD1(ComplexJob, bool(int x));
};

int CalculateSum(int x, int y) { return x + y; }

class Helper {
 public:
  bool ComplexJob(int x);
};
...

  MockFoo foo;
  Helper helper;
  EXPECT_CALL(foo, Sum(_, _))
      .WillOnce(Invoke(CalculateSum));
  EXPECT_CALL(foo, ComplexJob(_))
      .WillOnce(Invoke(&helper, &Helper::ComplexJob));

  foo.Sum(5, 6);       // Invokes CalculateSum(5, 6).
  foo.ComplexJob(10);  // Invokes helper.ComplexJob(10);

The only requirement is that the type of the function, etc must be compatible
with the signature of the mock function, meaning that 

the latter's, mock, arguments can be implicitly converted to the corresponding
arguments of the former, f, and the former's return type can be implicitly
converted to that of the latter. So, you can invoke something whose type is
not exactly the same as the mock function, as long as it's safe to do so -
nice, huh?


Invoke(f)
Invoke f with the arguments passed to the mock function, where f can be a
global/static function or a functor.

Invoke(object_pointer, &class::method)
Invoke the method on the object with the arguments passed to the mock
function.

InvokeWithoutArgs(f)
Invoke f, which can be a global/static function or a functor. f must take no
arguments.

InvokeWithoutArgs(object_pointer, &class::method)
Invoke the method on the object, which takes no arguments.

InvokeArgument<N>(arg1, arg2, ..., argk)
Invoke the mock function's N-th (0-based) argument, which must be a function
or a functor, with the k arguments.

The 'return' value of the invoked function is used as the return value of the
action.

When defining a function or functor to be used with Invoke*(), you can declare
any unused parameters as Unused:

double Distance(Unused, double x, double y) { return sqrt(x*x + y*y); }
...
EXPECT_CALL(mock, Foo("Hi", _, _)).WillOnce(Invoke(Distance));

In InvokeArgument<N>(...), if an argument needs to be passed by reference, wrap
  it inside ByRef(). For example,

InvokeArgument<2>(5, string("Hi"), ByRef(foo))

calls the mock function's #2 argument, passing to it 5 and string("Hi") by
value, and foo by reference. 


<gmock-action-custom>
If you want the side effect to happen every time, you need to define a custom
action, which we'll teach in the CookBook.

https://code.google.com/p/googlemock/wiki/CookBook#Writing_New_Actions_Quickly

* Writing New Actions Quickly

If the built-in actions don't work for you, and you find it inconvenient to use
Invoke(), you can use a macro from the ACTION* family to quickly define a new
action that can be used in your code as if it's a built-in action.

By writing

ACTION(name) { statements; }

In a namespace scope (i.e. not inside a class or function), you will define an
    action with the given name that executes the statements. The value returned
    by statements will be used as the return value of the action. Inside the
    statements, you can refer to the K-th (0-based) argument of the mock
    function as argK. For example:

ACTION(IncrementArg1) { return ++(*arg1); }

allows you to write

... WillOnce(IncrementArg1());

Note that you don't need to specify the types of the mock function arguments.
    Rest assured that your code is type-safe though: you'll get a compiler error
    if *arg1 doesn't support the ++ operator, or if the type of ++(*arg1) isn't
        compatible with the mock function's return type.

Another example:

ACTION(Foo) {
  (*arg2)(5);
  Blah();
  *arg1 = 0;
  return arg0;
}

defines an action Foo() that invokes argument #2 (a function pointer) with 5,
        calls function Blah(), sets the value pointed to by argument #1 to 0,
        and returns argument #0.

For more convenience and flexibility, you can also use the following pre-defined
symbols in the body of ACTION: 


* Writing New Parameterized Actions Quickly

Sometimes you'll want to 'parameterize' an action you define. For that we have
another macro

ACTION_P(name, param) { statements; }

For example,

ACTION_P(Add, n) { return arg0 + n; }

will allow you to write

// Returns argument #0 + 5.
... WillOnce(Add(5));

For convenience, we use the term 'arguments' for the values used to invoke the
mock function, and the term 'parameters' for the values used to instantiate an
action.

Note that you don't need to provide the type of the parameter either. Suppose
the parameter is named param, you can also use the Google-Mock-defined symbol
param_type to refer to the type of the parameter as inferred by the compiler.
For example, in the body of ACTION_P(Add, n) above, you can write n_type for the
type of n.

Google Mock also provides ACTION_P2, ACTION_P3, and etc to support
multi-parameter actions. For example,

ACTION_P2(ReturnDistanceTo, x, y) {
  double dx = arg0 - x;
  double dy = arg1 - y;
  return sqrt(dx*dx + dy*dy);
}

lets you write

... WillOnce(ReturnDistanceTo(5.0, 26.5));

You can view ACTION as a degenerated parameterized action where the number of
parameters is 0.

You can also easily define actions overloaded on the number of parameters:

ACTION_P(Plus, a) { ... }
ACTION_P2(Plus, a, b) { ... }


<ex>
/**
 * These helper actions are for use with Google Mock objects.
 *
 * When returning a future from a mock object expectation, care is needed to
 * make sure a new future is created each time the expectation is matched.
 * This shows up as a problem when setCallback is called on the returned future.
 *
 * The problem is that when using something like:
 *
 * EXPECT_CALL( ... ).WillRepeatedly(Return(FunctionThatReturnsAFuture()));
 *
 * Google Mock will copy the future returned from `FunctionThatReturnsAFuture()`
 * and always return the same future. This doesn't work for instance if you ever
 * call setCallback() on that future, which will throw a
 * DuplicateFutureCallback error on subsequent calls.
 *
 * Even this doesn't work, surprisingly:
 *
 * EXPECT_CALL( ... ).WillOnce(Return(FunctionThatReturnsAFuture()))
 *                   .WillOnce(Return(FunctionThatReturnsAFuture()));
 *
 * Again, the same promise is returned. I'm not quite sure how that happens
 * though.
 *
 * This can be used like this:
 *
 * EXPECT_CALL( ... ).WillOnce(returnNewCompletedFuture(someValue))
 *
 */

// copied again:
// Important note: The EXPECT_CALL() statement evaluates the action clause only
// 'once', even though the action may be performed many times. Therefore you
// must be careful about side effects. The following may not do what you want:

ACTION(returnNewCompletedFuture) {
    return completedFuture();
}

ACTION_P(returnNewCompletedFuture, value) {
    return completedFuture<typename return_type::value_type>(value);
}

ACTION_P(returnNewExceptionalFuture, exception) {
    return exceptionalFuture<typename return_type::value_type>(exception);
}


<gmock-action-combine>

Combining Actions

Want to do more than one thing when a function is called? That's fine. DoAll()
allow you to do 'sequence' of actions every time. Only the return value of the
'last' action in the sequence will be used.

DoAll(a1, a2, ..., an)

Do all actions a1 to an and return the result of an in each invocation. The
first n - 1 sub-actions must return void.


using ::testing::DoAll;

class MockFoo : public Foo {
 public:
  MOCK_METHOD1(Bar, bool(int n));
};
...

  EXPECT_CALL(foo, Bar(_))
      .WillOnce(DoAll(action_1,
                      action_2,
                      ...
                      action_n));


<gmock-action-exception>

Throw(exception)	
Throws the given exception, which can be any copyable value. Available since
v1.1.0.

class UserInterface
{
    std::string snow()
    {
        try {
            return weather_station_->snow(WeatherStation::Pessimistic);
            // return weather_station_->snow(WeatherStation::Optimistic);
        } catch(exception &e)
        {
            cout << "snow: exception: " << e.what() << endl;
            return "snow exception";
        }
    }
};


TEST( WeatherStationUserInterface, snow_exception )
{
    auto weather_station = std::make_shared<MockWeatherStation>();

    // GMock: inject more complex logic using C++11 lambdas,
    // and pattern match on the input value
    EXPECT_CALL( *weather_station, snow(_) )
        .WillOnce(Throw(WeatherException()));
 
    UserInterface ui( weather_station );
    EXPECT_EQ( "snow exception", ui.snow() );
}


={============================================================================
*kt_dev_test_203* gmock-matcher

https://github.com/google/googletest/blob/master/googlemock/docs/CheatSheet.md

Matchers

A matcher matches a single argument. You can use it inside ON_CALL() or
EXPECT_CALL(), or use it to validate a value directly:

EXPECT_THAT(value, matcher)	
Asserts that value matches matcher.

ASSERT_THAT(value, matcher)	
The same as EXPECT_THAT(value, matcher), except that it generates a fatal
failure.

// *TN* value is a single argument to matcher

Built-in matchers (where argument is the function argument) are divided into
several categories:


<ex>
// result is std::vector<> return

const auto result = reward_service.checkRewards("111", account_valid);
ASSERT_THAT(result, ElementsAre("CHAMPIONS_LEAGUE_FINAL_TICKET", "KARAOKE_PRO_MICROPHONE"));


Container Matchers

Most STL-style containers support ==, so you can use Eq(expected_container) or
simply expected_container to match a container exactly. If you want to write
the elements in-line, match them more flexibly, or get more informative
messages, you can use:

ElementsAre(e0, e1, ..., en)

argument has n + 1 elements, where the i-th element matches ei, which can be a
value or a matcher. `0 to 10 arguments are allowed.`

ElementsAreArray({ e0, e1, ..., en }), ElementsAreArray(array), or ElementsAreArray(array, count)

The same as ElementsAre() except that the expected element values/matchers
come from an initializer list, STL-style container, or C-style array.

<ex>
Use a matcher and note that have to define operator==() since it's a user
type:

bool operator==(const PurchaseRecord &lhs, const PurchaseRecord &rhs)
{
    return lhs.count_ == rhs.count_ && lhs.date_ == rhs.date_;
}

TEST_F(APortfolio, SeparatesPurchaseRecordsBySymbol) 
{
    // returns vector<PurchaseRecord> which has a single element
    auto sales = portfolio_.Purchases(SAMSUNG);

    ASSERT_THAT(sales, ElementsAre(PurchaseRecord(5, ARBITRARY_DATE)));
}


Or have direct comparison defined as a fixture member function:

class Fixture : class Test
{
        void ASSERT_PURCHASE(
                PurchaseRecord &purchase,
                int shareCount,
                const date &transactionDate)
        {
            ASSERT_THAT(purchase.count_, Eq(shareCount));
            ASSERT_THAT(purchase.date_, Eq(transactionDate));
        }
};

TEST_F(APortfolio, SeparatesPurchaseRecordsBySymbol) 
{
    // returns vector<PurchaseRecord> which has a single element
    auto sales = portfolio_.Purchases(SAMSUNG);

    ASSERT_PURCHASE( sales[0], 5, ARBITRARY_DATE );
}


<gmock-matcher-float> gmock-float
Floating-Point Matchers

DoubleEq(a_double)	
argument is a double value approximately equal to a_double, treating two NaNs
as unequal.

FloatEq(a_float)	
argument is a float value approximately equal to a_float, treating two NaNs as
unequal.


Defining Matchers

MATCHER(IsEven, "") { return (arg % 2) == 0; }	
Defines a matcher IsEven() to match an even number.

MATCHER_P(IsDivisibleBy, n, "") 
{ *result_listener << "where the remainder is " << (arg % n); return (arg % n) == 0; }	

Defines a macher IsDivisibleBy(n) to match a number divisible by n.

MATCHER_P2(IsBetween, a, b, std::string(negation ? "isn't" : "is") + " between " + PrintToString(a) + " and " + PrintToString(b)) 
{ return a <= arg && arg <= b; }	

Defines a matcher IsBetween(a, b) to match a value in the range [a, b].

Notes:
The MATCHER* macros cannot be used inside a function or class.

The matcher body must be purely functional (i.e. it cannot have any side
effect, and the result must not depend on anything other than the value
being matched and the matcher parameters).

You can use PrintToString(x) to convert a value x of any type to a string.

<ex>
As we add new functionality to RetweetCollection, such as combining similar
tweets, well want to verify that the new behavior properly impacts the
collections size and emptiness. We have several ways to address verifying
both attributes.

// since size() and isEmpty() are different interface to the same
// functionality.

The first option is for every size-related assertion, add a second assertion
around emptiness. This choice produces unnecessarily repetition and cluttered
tests.

TEST_F(ARetweetCollection, DecreasesSizeAfterRemovingTweet) 
{
  collection.add(Tweet());
  collection.remove(Tweet());

  ASSERT_THAT(collection.size(), Eq(0u));
  ASSERT_TRUE(collection.isEmpty());  // DON'T DO THIS
}
    

The second option is for every test around size, write a second test for
emptiness. While keeping in line with the notion of one assert per test, this
choice would also produce too much duplication.
    
TEST_F(ARetweetCollection, DecreasesSizeAfterRemovingTweet) 
{
  collection.add(Tweet()); 
  collection.remove(Tweet());
  ASSERT_THAT(collection.size(), Eq(0u));
}
        
TEST_F(ARetweetCollection, IsEmptyAfterRemovingTweet) 
{
  collection.add(Tweet());
  collection.remove(Tweet());
  ASSERT_TRUE(collection.isEmpty());
}
            

The third option is to create a helper method or custom assertion. Since the
code contains a conceptual link between size and emptiness, perhaps we should
link the concepts in the assertions.

class RetweetCollection
{
    public:
        RetweetCollection() {}

        bool isEmpty() const
        {
            return 0 == size();
        }

        unsigned int size() const 
        {
            return 0;
        }
};

class ARetweetCollection : public Test 
{
    public:
        RetweetCollection collection;
};

MATCHER_P(HasSize, expected, "")
{
    return arg.size() == expected 
        && arg.isEmpty() == (0 == expected); 
}

TEST_F(ARetweetCollection, MatcherWithSingleArgument)
{
    EXPECT_THAT(collection, HasSize(0u));
}


<ex> matcher to check gmock-pair

MATCHER_P(EqPair, expected, "")
{
    return arg.first == expected.first && arg.second == expected.second;
}

TEST(AssertTest, AssertOnPair)
{
    auto p1 = make_pair(1, "PAIR");
    auto p2 = make_pair(1, "PAIR");
    auto p3 = make_pair(3, "PAIR");

    EXPECT_THAT(p1, EqPair(p2));
    // ASSERT_THAT(p1.first, Eq(p2.first));
    // ASSERT_THAT(p1.second, Eq(p2.second));
    
    EXPECT_THAT(p1, EqPair(p3));
}

// can be this depending on where it's used

MATCHER_P(EqPair, expected, "")
{
    return arg->first == expected.first && arg->second == expected.second;
}

    auto posKey = coll.find(3.0);
    if( posKey != coll.end() )
    {
        ASSERT_THAT(posKey, EqPair(make_pair(3,2)));
        // ASSERT_THAT(posKey->first, Eq(3));
        // ASSERT_THAT(posKey->second, Eq(2));
    }


={============================================================================
*kt_dev_test_207* gmock-nice gmock-strict

Some of the mocks are called quite often and are expected to have some default
reaction which is expressed with WillByDefault() statement in setUp(). This
`still results is "Uninteresting mock call" warning` generated pointing at the
default value defined in setUp().

To make output less verbose and avoid warning for mocks which are know to
return some default, NiceMock template can be used. This commit wraps to of
the mock objects known to have default value returned with NiceMock. This
reduces the noise in the output while running the test. 

https://code.google.com/p/googlemock/wiki/CookBook

The Nice, the Strict, and the Naggy

If a mock method has no EXPECT_CALL spec but is called, Google Mock will print
a warning about the "uninteresting call". The 'rationale' is:

New methods may be added to an interface after a test is written. We shouldn't
fail a test just because a method it doesn't know about is called. However,
this may also mean there's a bug in the test, so Google Mock shouldn't be
  silent either. If the user believes these calls are harmless, he can add an
  EXPECT_CALL() to suppress the warning. 

However, sometimes you may want to suppress all "uninteresting call" warnings,
while sometimes you may want the opposite, i.e. to treat all of them as
  errors. Google Mock lets you make the decision on a per-mock-object basis.

Suppose your test uses a mock class MockFoo:

TEST(...) {
  MockFoo mock_foo;
  EXPECT_CALL(mock_foo, DoThis());
  ... code that uses mock_foo ...
}

<gmock-nice>
If a method of mock_foo other than DoThis() is called, it will be reported by
Google Mock as a warning. However, if you rewrite your test to use
NiceMock<MockFoo> instead, the warning will be 'gone', resulting in a cleaner
test output:

using ::testing::NiceMock;

TEST(...) {
  NiceMock<MockFoo> mock_foo;
  EXPECT_CALL(mock_foo, DoThis());
  ... code that uses mock_foo ...
}

NiceMock<MockFoo> is a subclass of MockFoo, so it can be used wherever MockFoo
is accepted.

It also works if MockFoo's constructor takes some arguments, as
NiceMock<MockFoo> "inherits" MockFoo's constructors:

using ::testing::NiceMock;

TEST(...) {
  NiceMock<MockFoo> mock_foo(5, "hi");  // Calls MockFoo(5, "hi").
  EXPECT_CALL(mock_foo, DoThis());
  ... code that uses mock_foo ...
}


<strict-mock>
The usage of StrictMock is similar, 'except' that it makes all uninteresting
calls failures:

using ::testing::StrictMock;

TEST(...) {
  StrictMock<MockFoo> mock_foo;
  EXPECT_CALL(mock_foo, DoThis());
  ... code that uses mock_foo ...

  // The test will fail if a method of mock_foo other than DoThis()
  // is called.
}

There are some caveats though (I don't like them just as much as the next guy,
    but sadly they are side effects of C++'s limitations):

NiceMock<MockFoo> and StrictMock<MockFoo> only work for mock methods defined
using the MOCK_METHOD* family of macros directly in the MockFoo class. If a
mock method is defined in a base class of MockFoo, the "nice" or "strict"
modifier may not affect it, depending on the compiler. In particular, nesting
NiceMock and StrictMock (e.g. NiceMock<StrictMock<MockFoo> >) is not
supported.

The constructors of the base mock (MockFoo) cannot have arguments passed by
non-const reference, which happens to be banned by the Google C++ style guide.

During the constructor or destructor of MockFoo, the mock object is not nice
or strict. This may cause surprises if the constructor or destructor calls a
mock method on this object. 

This behavior, however, is consistent with C++'s general rule: if a
constructor or destructor calls a virtual method of this object, that method
is treated as non-virtual. In other words, to the base class's constructor or
destructor, this object behaves like an instance of the base class, not the
derived class. This rule is required for safety. Otherwise a base constructor
may use members of a derived class before they are initialized, or a base
destructor may use members of a derived class after they have been destroyed. 

Finally, you should be very cautious about when to use naggy or strict mocks,
  as they tend to make tests more brittle and harder to maintain. When you
  refactor your code without changing its externally visible behavior, ideally
  you should't need to update any tests. If your code interacts with a naggy
  mock, however, you may start to get spammed with warnings as the result of
  your change. Worse, if your code interacts with a strict mock, your tests
  may start to fail and you'll be forced to fix them. 
  
Our general recommendation is to use nice mocks (not yet the default) most of
the time, use naggy mocks (the current default) when developing or debugging
tests, and use strict mocks only as the last resort. 


={============================================================================
*kt_dev_test_208* gmock-errors when misses expectation

This fails the test.

<ex>
Do not have expectaion of a call but there is.

terminate called after throwing an instance of 'std::runtime_error'
  what():  Uninteresting mock function call - returning default value.
    Function call: setSource(@0x7ffd3f3eda50 "http://bbc.co.uk/xx.mpd", 1)
    The mock function has no default action set, and its return type 
    has no default value set.
Aborted
FAIL: test/js/testplay


<ex>
Do have expectation but uses return value. Expected ErrorValue::locator but
returned ErrorValue::data and regarded as 'not' called.

Actual function call count doesn't match 
  EXPECT_CALL(*listener, ErrorEvent(ErrorValue::locator,
  ErrorEventContext::source,_))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
F..unknown file: Failure


<ex>
This is when EXPECT_CALL( *listener, ErrorEvent( 6, 0, _)) but object returns
ErrorEvent(6, 1). See arg# starts 0.

Unexpected mock function call - returning directly.
    Function call: ErrorEvent(6, 1, @0x18da938 "\nGStreamer error domain: gst-resource-error-quark\nAdditional debug:\n(no debug)")
Google Mock tried the following 1 expectation, but it didn't match:

/home/kpark/builds/_virtual_/pc/DEVARCH/Nickel/Nickel.System.GStreamer/test/GstMediaRouterTest.cpp:1341: EXPECT_CALL(*listener, ErrorEvent(errorVal, context, _))...
  Expected arg #1: is equal to 0
           Actual: 1
         Expected: to be called once
           Actual: never called - unsatisfied and active

Actual function call count doesn't match EXPECT_CALL(*listener, ErrorEvent(errorVal, context, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
F

GstMediaRouterTest.cpp:1348:Assertion
Test name: nickel::system::GstMediaRouterTest::test_that404OnResourceError
assertion failed
- Expression: success
- done.get() did not happen within DEFAULT_TIMEOUTms

VerifyAndClearMock.h:36:Assertion
Test name: nickel::system::GstMediaRouterTest::test_that404OnResourceError
tearDown() failed
- forced failure
- Mock::VerifyAndClear( listener ) failed at line: 479

Failures !!!
Run: 1   Failure total: 2   Failures: 2   Errors: 0


={============================================================================
*kt_dev_test_208* gmock-ex

<ex>
#include <iostream>
#include <cstdlib>

#include "gtest/gtest.h"

typedef int EntryType;

typedef struct node
{
  EntryType entry;
  node*     pnext;
} Node;

typedef struct {
  int    count;
  Node*  header;
} List;

Node* MakeNode( EntryType entry )
{
  Node* pnode = NULL;

  if( (pnode = (Node*) malloc( sizeof(Node))) == NULL )
  {
    std::cout << "no more memory" << std::endl;
    return NULL;
  }

  pnode->entry = entry;
  pnode->pnext = NULL;

  return pnode;
}

void CreatList( List* list )
{ 
  list->count = 0;
  list->header = NULL; 
}

bool ListEmpty( List* list )
{ return ( list->header == NULL ); }

// add only at the end. see there are approaches to find end; one is to use pnext and the other is
// to use count. 
bool AddList( List* list, EntryType entry )
{
  Node* pnode, *pend;

  if( (pnode = MakeNode(entry)) == NULL )
  {
    std::cout << "add: mem is full" << std::endl;
    return false;
  }

  if( ListEmpty( list ) )
  {
    list->header = pnode;
  }
  else
  {
#ifdef USE_PNEXT
    // search the end using pnext
    for( pend = list->header; pend->pnext; pend = pend->pnext )
      ;
#else
    // search the end using count
    pend = list->header;
    for( int current = 1; current < list->count; current++) // note that less than
      pend = pend->pnext;
#endif

    pend->pnext = pnode;
  }

  list->count++;

  return true;
}

typedef void(*TRAVERSEFUNC)(EntryType);

void TraverseList( List* list, TRAVERSEFUNC func)
{
  Node* pnode;

  if( ListEmpty(list) )
  {
    std::cout << "list is empty" << std::endl;
    return;
  }

  pnode = list->header;

  while(pnode)
  {
    func(pnode->entry);
    pnode = pnode->pnext;
  }
}

void EntryPrint(EntryType item)
{
  std::cout << "item is: " << item << std::endl;
}

TEST(Sample, EmptyCheck) {
  List list;
  CreatList(&list);

  EXPECT_EQ( 1, ListEmpty(&list) );
  EXPECT_EQ( 0, ListEmpty(&list) );   // line #112 
}

int main(int argc, char **argv)
{
  int item = 0;

  testing::InitGoogleTest(&argc, argv);

//  List list;
//  CreatList(&list);
//
//  std::cout << "type in 5 numbers." << std::endl;
//
//  for(int i=0; i < 5; i++)
//  {
//    std::cin >> item;
//    AddList(&list, item );
//  }
//
//  TraverseList(&list, EntryPrint);
  return RUN_ALL_TESTS();
}


// makefile

# SYNOPSIS:
#
#   make [all]  - makes everything.
#   make clean  - removes all files generated by make.

GTEST_DIR = /home/kyoupark/works/googletest/googletest

# Where to find user code.
USER_DIR = .

CPPFLAGS += -isystem $(GTEST_DIR)/include

CXXFLAGS += -g -Wall -Wextra -pthread

TESTS = list_unittest

GTEST_HEADERS = $(GTEST_DIR)/include/gtest/*.h \
                $(GTEST_DIR)/include/gtest/internal/*.h

# House-keeping build targets.
all : $(TESTS)

clean :
	rm -f $(TESTS) gtest.a gtest_main.a *.o

# Builds gtest.a and gtest_main.a.
GTEST_SRCS_ = $(GTEST_DIR)/src/*.cc $(GTEST_DIR)/src/*.h $(GTEST_HEADERS)

# For simplicity and to avoid depending on Google Test's
gtest-all.o : $(GTEST_SRCS_)
	$(CXX) $(CPPFLAGS) -I$(GTEST_DIR) $(CXXFLAGS) -c \
            $(GTEST_DIR)/src/gtest-all.cc

gtest.a : gtest-all.o
	$(AR) $(ARFLAGS) $@ $^

t_list.o : t_list.cpp $(GTEST_HEADERS)
	$(CXX) $(CPPFLAGS) $(CXXFLAGS) -c t_list.cpp -o t_list.o

list_unittest : t_list.o gtest.a
	$(CXX) $(CPPFLAGS) $(CXXFLAGS) -lpthread $^ -o $@


// output

kyoupark@kit-debian64:~/works/t_list$ ./list_unittest 
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from ListTest
[ RUN      ] ListTest.checkEmpty
t_list.cpp:112: Failure
Expected equality of these values:
  0
  ListEmpty(&list)
    Which is: true
[  FAILED  ] ListTest.checkEmpty (0 ms)
[----------] 1 test from ListTest (0 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (0 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ListTest.checkEmpty

 1 FAILED TEST
kyoupark@kit-debian64:~/works/t_list$ 


<ex> *cxx-ex-quote-example*
// To test Quote classes without user, Basket, class

#include <iostream>
#include <set>
#include <memory>

#include "gtest/gtest.h"

using namespace std;

class Quote {
    private:
        string s_book_no;

    protected:
        double price;

    public:
        Quote() : s_book_no(""), price(0.0) {}
        Quote(const string book, const double price) : s_book_no(book), price(price) {}

        // have to since it's intended to be inherited
        virtual ~Quote() {}

        string isbn() const { return s_book_no; }

        // get net price without discount
        virtual double net_price(size_t sales_count) const
        { return sales_count*price; }

        // virtual copy, copy version
        virtual Quote *clone() const &
        { 
            cout << "quote::clone::copy" << endl;
            return new Quote(*this); 
        }

        // virtual copy, move version
        virtual Quote *clone() const &&
        { 
            cout << "quote::clone::move" << endl;
            return new Quote(std::move(*this)); 
        }
};

class Disc_quote : public Quote
{
    protected:
        double d_quantity;
        double d_discount;

    public:
        Disc_quote(const string book, const double price, const double quantity, const double discount) :
            Quote(book, price), d_quantity(quantity), d_discount(discount) {}

        double net_price(size_t sales_count) const = 0;
};

class Bulk_quote : public Disc_quote
{
    public:
        Bulk_quote(const string book, const double price, const double quantity, const double discount) :
            Disc_quote(book, price, quantity, discount) {}

        // double net_price(size_t sales_count) const override
        double net_price(size_t sales_count) const 
        {
            if( sales_count >= d_quantity )
                return sales_count*(1-d_discount)*price;
            else
                return sales_count*price;
        };

        // virtual copy, copy version
        //  Unlike Quote::clone(), when there is no const, no compile error but
        //  gets always Quote version since no override. see
        //  *cpp-override-condition*
        virtual Bulk_quote *clone() const &
        { 
            cout << "bulk::clone::copy" << endl;
            return new Bulk_quote(*this); 
        }

        // virtual copy, move version
        virtual Bulk_quote *clone() const &&
        { 
            cout << "bulk::clone::move" << endl;
            return new Bulk_quote(std::move(*this)); 
        }
};

// // user, client, driver class
// class Basket
// {
//     public:
//         Basket() : items(compare) {}
// 
//         void add_item(const shared_ptr<Quote> &item);
// 
//         // virtual copy, copy version
//         // which signals to use lvalue object, lvalue reference qualifier
//         void add_item(const Quote &item)
//         {
//             cout << "basket::add_item::copy version" << endl;
//             items.insert(shared_ptr<Quote>(item.clone()));
//         }
// 
//         // virtual copy, move version
//         // which signals to use rvalue object, rvalue reference qualifier
//         void add_item(Quote &&item)
//         {
//             cout << "basket::add_item::move version" << endl;
//             items.insert(shared_ptr<Quote>(std::move(item).clone()));
//         }
// 
// 
//         double total_receipt(ostream &os) const;
// 
//     private:
//         static bool compare(const shared_ptr<Quote> lhs, const shared_ptr<Quote> rhs)
//         { return lhs->isbn() < rhs->isbn(); }
// 
//         // using comp = bool (*)(const shared_ptr<Quote> lhs, const shared_ptr<Quote> rhs);
//         // multiset<shared_ptr<Quote>, comp> items;
// 
//         using comp = bool (const shared_ptr<Quote> lhs, const shared_ptr<Quote> rhs);
//         multiset<shared_ptr<Quote>, comp*> items;
// };
// 
// void Basket::add_item(const shared_ptr<Quote> &item)
// {
//     cout << "basket::add_item::copy version" << endl;
//     items.insert(item);
// }
// 
// double Basket::total_receipt(ostream &os) const
// {
//     for (auto iter = items.cbegin(); iter != items.cend();
//             iter = items.upper_bound(*iter))
//     {
//         os << "isbn : " << (*iter)->isbn() 
//             << ", sold : " << items.count(*iter)
//             << ", total sales: " << (*iter)->net_price( items.count(*iter)) 
//             << endl;
//     }
// }

// namespace {

class BulkQuote1Test : public ::testing::Test
{
    protected:
        BulkQuote1Test() : items{compare} {}

        virtual ~BulkQuote1Test() {}

        virtual void SetUp() 
        {
            // *TN*
            // In this approach, have to crate each class for each input set
            // condition.
            //
            // // Quote sales which has no discount. 45*3 = 135
            // items.insert(shared_ptr<Quote>(new Quote("123", 45)));
            // items.insert(shared_ptr<Quote>(new Quote("123", 45)));
            // items.insert(shared_ptr<Quote>(new Quote("123", 45)));
        }

        virtual void TearDown()
        {
        }

        double total_receipt() const
        {
            double total{};

            // *TN*
            // can use cout
            for (auto it = items.cbegin(); it != items.cend();
                    it = items.upper_bound(*it))
            {
                total = (*it)->net_price(items.count(*it));

                cout << "isbn: " << (*it)->isbn()
                    << ", sold: " << items.count(*it)
                    << ", total sales: " << total;
                cout << endl;
            }

            return total;
        }

    // private:
    // *TN* TEST_F seems to create derived class since emits errors to access
    // private members.
    protected:
        static bool compare(const shared_ptr<Quote> lhs, const shared_ptr<Quote> rhs)
        {
            return lhs->isbn() < rhs->isbn();
        }

        using comp = bool(const shared_ptr<Quote> lhs, const shared_ptr<Quote> rhs);
        multiset<shared_ptr<Quote>, comp*> items;
};

// *TN*
// In this approach, have to crate each class for each input set
// condition.
//
// TEST_F(BulkQuote1Test, checkTotal)
// {
//     EXPECT_EQ(135, total_receipt());
// }


// Quote sales which has no discount. 45*3 = 135
TEST_F(BulkQuote1Test, checkTotal1)
{
    items.insert(shared_ptr<Quote>(new Quote("123", 45)));
    items.insert(shared_ptr<Quote>(new Quote("123", 45)));
    items.insert(shared_ptr<Quote>(new Quote("123", 45)));

    EXPECT_EQ(135, total_receipt());

    items.clear();
}

// minimum 3 and 15% discount. no discount 45*2 = 90
TEST_F(BulkQuote1Test, checkTotal2)
{
    items.insert(shared_ptr<Quote>(new Bulk_quote("345", 45, 3, .15)));
    items.insert(shared_ptr<Quote>(new Bulk_quote("345", 45, 3, .15)));

    // ERROR
    EXPECT_EQ(135, total_receipt());

    items.clear();
}

// Bulk_quote sales which has discount: minimum 3 and 15% discount
// 35*4*(1-.15) = 119
TEST_F(BulkQuote1Test, checkTotal3)
{
    items.insert(shared_ptr<Quote>(new Bulk_quote("678", 35, 3, .15)));
    items.insert(shared_ptr<Quote>(new Bulk_quote("678", 35, 3, .15)));
    items.insert(shared_ptr<Quote>(new Bulk_quote("678", 35, 3, .15)));
    items.insert(shared_ptr<Quote>(new Bulk_quote("678", 35, 3, .15)));

    EXPECT_EQ(119, total_receipt());

    items.clear();
}

// Bulk_quote sales which has discount: minimum 5 and 25% discount
// 35*6*(1-.25) = 157.5
TEST_F(BulkQuote1Test, checkTotal4)
{
    items.insert(shared_ptr<Quote>(new Bulk_quote("912", 35, 5, .25)));
    items.insert(shared_ptr<Quote>(new Bulk_quote("912", 35, 5, .25)));
    items.insert(shared_ptr<Quote>(new Bulk_quote("912", 35, 5, .25)));
    items.insert(shared_ptr<Quote>(new Bulk_quote("912", 35, 5, .25)));
    items.insert(shared_ptr<Quote>(new Bulk_quote("912", 35, 5, .25)));
    items.insert(shared_ptr<Quote>(new Bulk_quote("912", 35, 5, .25)));

    EXPECT_EQ(157.5, total_receipt());

    items.clear();
}


// *TN*
// In this approach, have to crate each class for each input set
// condition.
// //     // minimum 3 and 15% discount. no discount 45*2 = 90
// //     sale.add_item(Bulk_quote("345", 45, 3, .15));
// //     sale.add_item(Bulk_quote("345", 45, 3, .15));
// 
// class BulkQuote2Test : public ::testing::Test
// {
//     protected:
//         BulkQuote2Test() : items{compare} {}
// 
//         virtual ~BulkQuote2Test() {}
// 
//         virtual void SetUp() 
//         {
//             // Quote sales which has no discount. 45*3 = 135
//             items.insert(shared_ptr<Quote>(new Bulk_quote("345", 45, 3, .15)));
//             items.insert(shared_ptr<Quote>(new Bulk_quote("345", 45, 3, .15)));
//         }
// 
//         virtual void TearDown()
//         {
//         }
// 
//         double total_receipt() const
//         {
//             double total{};
// 
//             for (auto it = items.cbegin(); it != items.cend();
//                     it = items.upper_bound(*it))
//             {
//                 total = (*it)->net_price(items.count(*it));
// 
//                 cout << "isbn: " << (*it)->isbn()
//                     << ", sold: " << items.count(*it)
//                     << ", total sales: " << total;
//                 cout << endl;
//             }
// 
//             return total;
//         }
// 
//     private:
//         static bool compare(const shared_ptr<Quote> lhs, const shared_ptr<Quote> rhs)
//         {
//             return lhs->isbn() < rhs->isbn();
//         }
// 
//         using comp = bool(const shared_ptr<Quote> lhs, const shared_ptr<Quote> rhs);
//         multiset<shared_ptr<Quote>, comp*> items;
// };
// 
// TEST_F(BulkQuote2Test, checkTotal)
// {
//     EXPECT_EQ(135, total_receipt());
// }


// }   // namespace

int main(int argc, char** argv)
{
    testing::InitGoogleTest(&argc, argv);
    return RUN_ALL_TESTS();
}

// kyoupark@kit-debian64:~/works/t_ex_quote$ ./unittest 
// [==========] Running 4 tests from 1 test case.
// [----------] Global test environment set-up.
// [----------] 4 tests from BulkQuote1Test
// [ RUN      ] BulkQuote1Test.checkTotal1
// isbn: 123, sold: 3, total sales: 135
// [       OK ] BulkQuote1Test.checkTotal1 (1 ms)
// [ RUN      ] BulkQuote1Test.checkTotal2
// isbn: 345, sold: 2, total sales: 90
// t_ex_quote.cpp:231: Failure
// Expected equality of these values:
//   135
//   total_receipt()
//     Which is: 90
// [  FAILED  ] BulkQuote1Test.checkTotal2 (0 ms)
// [ RUN      ] BulkQuote1Test.checkTotal3
// isbn: 678, sold: 4, total sales: 119
// [       OK ] BulkQuote1Test.checkTotal3 (0 ms)
// [ RUN      ] BulkQuote1Test.checkTotal4
// isbn: 912, sold: 6, total sales: 157.5
// [       OK ] BulkQuote1Test.checkTotal4 (0 ms)
// [----------] 4 tests from BulkQuote1Test (1 ms total)
// 
// [----------] Global test environment tear-down
// [==========] 4 tests from 1 test case ran. (1 ms total)
// [  PASSED  ] 3 tests.
// [  FAILED  ] 1 test, listed below:
// [  FAILED  ] BulkQuote1Test.checkTotal2
// 
//  1 FAILED TEST
// kyoupark@kit-debian64:~/works/t_ex_quote$ 


<ex> *cxx-ex-quote-example*
// To user, Basket, class with Quote mock

#include <iostream>
#include <set>
#include <memory>

#include "gtest/gtest.h"
#include "gmock/gmock.h"

using namespace std;

// class Quote {
//     private:
//         string s_book_no;
// 
//     protected:
//         double price;
// 
//     public:
//         Quote() : s_book_no(""), price(0.0) {}
//         Quote(const string book, const double price) : s_book_no(book), price(price) {}
// 
//         // have to since it's intended to be inherited
//         virtual ~Quote() {}
// 
//         string isbn() const { return s_book_no; }
// 
//         // get net price without discount
//         virtual double net_price(size_t sales_count) const
//         { return sales_count*price; }
// };

class Quote {
    public:
        Quote() {}
        Quote(const string book, const double price) 
            : book_(book), price_(price) {}

        // have to since it's intended to be inherited
        virtual ~Quote() {}

        string isbn() const { return book_; }

        // get net price without discount
        virtual double net_price(size_t sales_count) const = 0;

    private:
        string book_;
        double price_;
};

class MockQuote : public Quote {
    public:
        MockQuote(const string book, const double price)
            : Quote(book, price) {}

        // MOCK_METHOD0(~Quote, ());
        MOCK_CONST_METHOD1(net_price, double (size_t sales_count));
};


class Disc_quote : public Quote
{
    protected:
        double d_quantity;
        double d_discount;

    public:
        Disc_quote(const string book, const double price, const double quantity, const double discount) :
            Quote(book, price), d_quantity(quantity), d_discount(discount) {}

        // double net_price(size_t sales_count) const = 0;
};

class Bulk_quote : public Disc_quote
{
    public:
        Bulk_quote(const string book, const double price, const double quantity, const double discount) :
            Disc_quote(book, price, quantity, discount) {}

        // double net_price(size_t sales_count) const override
        // double net_price(size_t sales_count) const 
        // {
        //     if( sales_count >= d_quantity )
        //         return sales_count*(1-d_discount)*price;
        //     else
        //         return sales_count*price;
        // };
};

// class Bulk_quote {
//     public:
//         Bulk_quote(const string book, const double price, 
//                 const double quantity, const double discout) {}
//         virtual ~Bulk_quote() {}
//         string isbn() const {}
//         virtual double net_price(size_t sales_count) const = 0;
// };

class MockBulk : public Bulk_quote {
    public:

        MockBulk(const string book, const double price, const double quantity, const double discount) :
            Bulk_quote(book, price, quantity, discount) {}

        MOCK_CONST_METHOD1(net_price, double (size_t sales_count));
};


// user class
class Basket
{
    public:
        Basket() : items(compare) {}

        void add_item(const shared_ptr<Quote> &item);
        double total_receipt(ostream &os) const;

    private:
        static bool compare(const shared_ptr<Quote> lhs, const shared_ptr<Quote> rhs)
        { return lhs->isbn() < rhs->isbn(); }

        // using comp = bool (*)(const shared_ptr<Quote> lhs, const shared_ptr<Quote> rhs);
        // multiset<shared_ptr<Quote>, comp> items;

        using comp = bool (const shared_ptr<Quote> lhs, const shared_ptr<Quote> rhs);
        multiset<shared_ptr<Quote>, comp*> items;
};

void Basket::add_item(const shared_ptr<Quote> &item)
{
    cout << "basket::add_item::copy version" << endl;
    items.insert(item);
}

double Basket::total_receipt(ostream &os) const
{
    double total{};

    for (auto iter = items.cbegin(); iter != items.cend();
            iter = items.upper_bound(*iter))
    {
        total = (*iter)->net_price(items.count(*iter));
        os << "isbn : " << (*iter)->isbn() 
            << ", sold : " << items.count(*iter)
            << ", total sales: " << total
            << endl;
    }

    return total;
}


TEST(BasketTest, checkTotal1) 
{
    using ::testing::Return;
    using ::testing::_;
    using ::testing::AnyNumber;

    // Quote sales which has no discount. 45*3 = 135
    shared_ptr<MockQuote> q1(new MockQuote("123", 45));
    shared_ptr<MockQuote> q2(new MockQuote("123", 45));
    shared_ptr<MockQuote> q3(new MockQuote("123", 45));

    EXPECT_CALL(*q1, net_price(_))
        .WillOnce(Return(135));

    // t_ex_basket.cpp:169: Failure
    // Actual function call count doesn't match EXPECT_CALL(*q3, net_price(_))...
    //          Expected: to be called once
    //            Actual: never called - unsatisfied and active
    // t_ex_basket.cpp:166: Failure
    // Actual function call count doesn't match EXPECT_CALL(*q2, net_price(_))...
    //          Expected: to be called once
    //            Actual: never called - unsatisfied and active
    // [  FAILED  ] BasketTest.checkTotal1 (2 ms)
    //
    // EXPECT_CALL(*q2, net_price(_))
    //     .WillOnce(Return(135));

    // EXPECT_CALL(*q3, net_price(_))
    //     .WillOnce(Return(135));

    EXPECT_CALL(*q2, net_price(_))
        .Times(AnyNumber())
        .WillOnce(Return(135));

    EXPECT_CALL(*q3, net_price(_))
        .Times(AnyNumber())
        .WillOnce(Return(135));

    Basket sale;

    sale.add_item(q1);
    sale.add_item(q2);
    sale.add_item(q3);

    EXPECT_EQ(135, sale.total_receipt(cout));
}

TEST(BasketTest, checkTotal2) 
{
    using ::testing::Return;
    using ::testing::_;
    using ::testing::AnyNumber;

    // Bulk_quote sales which has discount: minimum 5 and 25% discount
    // 35*6*(1-.25) = 157.5
    shared_ptr<MockBulk> q1(new MockBulk("912", 35, 5, .25));
    shared_ptr<MockBulk> q2(new MockBulk("912", 35, 5, .25));
    shared_ptr<MockBulk> q3(new MockBulk("912", 35, 5, .25));
    shared_ptr<MockBulk> q4(new MockBulk("912", 35, 5, .25));
    shared_ptr<MockBulk> q5(new MockBulk("912", 35, 5, .25));

    EXPECT_CALL(*q1, net_price(_))
        .WillOnce(Return(157.5));

    EXPECT_CALL(*q2, net_price(_))
        .Times(AnyNumber())
        .WillOnce(Return(157.5));

    EXPECT_CALL(*q3, net_price(_))
        .Times(AnyNumber())
        .WillOnce(Return(157.5));

    EXPECT_CALL(*q4, net_price(_))
        .Times(AnyNumber())
        .WillOnce(Return(157.5));

    EXPECT_CALL(*q5, net_price(_))
        .Times(AnyNumber())
        .WillOnce(Return(157.5));

    Basket sale;

    sale.add_item(q1);
    sale.add_item(q2);
    sale.add_item(q3);
    sale.add_item(q4);
    sale.add_item(q5);

    // EXPECT_EQ(135, sale.total_receipt(cout));
    EXPECT_EQ(157.5, sale.total_receipt(cout));
}

int main(int argc, char **argv)
{
    std::cout << "Running main() from gmock_main.cc\n";

    // Since Google Mock depends on Google Test, InitGoogleMock() is
    // also responsible for initializing Google Test.  Therefore there's
    // no need for calling testing::InitGoogleTest() separately.
    testing::InitGoogleMock(&argc, argv);
    return RUN_ALL_TESTS();
}


kyoupark@kit-debian64:~/git/kb/code/t_ex_quote$ ./test_basket 
Running main() from gmock_main.cc
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from BasketTest
[ RUN      ] BasketTest.checkTotal1
basket::add_item::copy version
basket::add_item::copy version
basket::add_item::copy version
isbn : 123, sold : 3, total sales: 135
[       OK ] BasketTest.checkTotal1 (1 ms)
[ RUN      ] BasketTest.checkTotal2
basket::add_item::copy version
basket::add_item::copy version
basket::add_item::copy version
basket::add_item::copy version
basket::add_item::copy version
isbn : 912, sold : 5, total sales: 157.5
[       OK ] BasketTest.checkTotal2 (2 ms)
[----------] 2 tests from BasketTest (3 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (5 ms total)
[  PASSED  ] 2 tests.


<ex>
https://musingstudio.com/2013/09/17/unit-testing-with-gtest-and-gmock/

#include "gmock\gmock.h"
 
#include <iostream>
#include <list>
#include <memory>
 
class WeatherStation
{
public:
    virtual ~WeatherStation(){};
 
    // see *cxx-enum*
    typedef enum
    {
        North, South, East, West
    } Direction;
 
    typedef enum
    {
        Optimistic, Pessimistic
    } Outlook;
 
    // NB Semantics on wind deliberately ugly to show a neat feature in gmock
    virtual void wind( Direction* pDirection, double* strength ) const = 0;
    virtual double rainfall() const = 0;
    virtual std::string prediction( Outlook outlook ) const = 0;
};
 
class UserInterface
{
public:
    UserInterface( const std::shared_ptr<WeatherStation>& weather_station ) :
        weather_station_( weather_station )
    {
    }
 
    typedef enum
    {
        Heavy, Medium, Light
    } Range;
 
    Range rain()
    {
        auto rainfall = weather_station_->rainfall();
        if ( 0.0 <= rainfall && rainfall < 2.0 ) return Light;
        else if ( 2.0 <= rainfall && rainfall < 4.0 ) return Medium;
        else return Heavy;
    }
 
    Range wind()
    {
        WeatherStation::Direction direction;
        double strength;
        weather_station_->wind( &direction, &strength );
 
        if ( 0.0 <= strength && strength < 5.0 ) return Light;
        else if ( 5.0 <= strength && strength < 10.0 ) return Medium;
        else return Heavy;
    }
 
    std::pair<std::string, std::string> predict_range()
    {
        return std::make_pair( 
            weather_station_->prediction( WeatherStation::Optimistic ),
            weather_station_->prediction( WeatherStation::Pessimistic ) );
    }
 
private:
    std::shared_ptr<WeatherStation> weather_station_;
};
 
using namespace testing;
 
class MockWeatherStation : public WeatherStation
{
public:
    MOCK_CONST_METHOD0( rainfall, double() );
    MOCK_CONST_METHOD2( wind, void(WeatherStation::Direction*, double*) );
    MOCK_CONST_METHOD1( prediction, std::string( WeatherStation::Outlook ) );
};
 
TEST( WeatherStationUserInterface, rain_should_be_heavy )
{
    auto weather_station = std::make_shared<MockWeatherStation>();
    // GMock: specify a simple return value using Return(x)
    EXPECT_CALL( *weather_station, rainfall() )
        .WillOnce( Return(5.0) );
    UserInterface ui( weather_station );
    EXPECT_EQ( UserInterface::Heavy, ui.rain() );
}
 
TEST( WeatherStationUserInterface, wind_should_be_light )
{
    auto weather_station = std::make_shared<MockWeatherStation>();

    // GMock: specify out parameter values using SetArgPointee
    //
    // *TN* wind() returns void and use agrument as output
 
    EXPECT_CALL( *weather_station, wind(_,_) )
        .WillOnce( DoAll( SetArgPointee<0>( WeatherStation::North ),
                          SetArgPointee<1>( 0.5 )) );
    UserInterface ui( weather_station );
    EXPECT_EQ( UserInterface::Light, ui.wind() );
}
 
TEST( WeatherStationUserInterface, predictions_are_displayed )
{
    auto weather_station = std::make_shared<MockWeatherStation>();
    
    // GMock: inject more complex logic using C++11 lambdas,
    // and pattern match on the input value
    //
    // *TN* (WeatherStation::Outlook _) is unused argument but not
    // *gmock-any-args*
 
    EXPECT_CALL( *weather_station, prediction(WeatherStation::Optimistic) )
        .WillOnce( Invoke( []( WeatherStation::Outlook _ ) -> std::string
            {
                return "Sunny";
            }) );
    EXPECT_CALL( *weather_station, prediction(WeatherStation::Pessimistic) )
        .WillOnce( Invoke( []( WeatherStation::Outlook _ ) -> std::string
            {
                return "Overcast";
            }) );
 
    UserInterface ui( weather_station );
    auto predicted_range = ui.predict_range();
    EXPECT_EQ( "Sunny", predicted_range.first );
    EXPECT_EQ( "Overcast", predicted_range.second );
}


<ex>

void GstMediaRouterTest::test_thatGetPositionReturnsStableValues()
{
    NS_ZINC::AtomicBool done;

    EXPECT_CALL(
            *listener, SourceEvent(_, _))
        .Times(AnyNumber());

    EXPECT_CALL(
            *listener, BufferStatusEvent(_))
        .Times(AnyNumber());

    // note:
    // Each time when it gets called, that is gets an event, invoke the function
    // which set done to true.

    EXPECT_CALL(*listener, StatusEvent(StatusEventValue::started))
        .Times(3)
        .WillRepeatedly(InvokeWithoutArgs(boost::function<void(void)>(
                        boost::bind(&NS_ZINC::AtomicBool::set,
                            boost::ref(done), true))));

    EXPECT_CALL(*listener, StatusEvent(StatusEventValue::seek_started))
        .Times(2)
        .WillRepeatedly(InvokeWithoutArgs(boost::function<void(void)>(
                        boost::bind(&NS_ZINC::AtomicBool::set,
                            boost::ref(done), false))));

    mr->setSource(server.getManifestUrl(), SetSourceReason::unspecified).get();
    mr->setSink("decoder://headless").get();
    mr->start().get();

    // note:
    // This waits for some time and do assert on if there is no the first event
    // fired. Not all three events. 

    ZINC_ASSERT_WITHIN_TIMEOUT(done.get(), DEFAULT_TIMEOUT);

    mr->setPlaySpeed(0.0).get();
    mr->seekPosition(SeekReference::end, 0, SeekMode::prioritise_speed).get();

    ZINC_ASSERT_WITHIN_TIMEOUT(done.get(), DEFAULT_TIMEOUT);

    const int32_t seek_amount = 10000;
    const int32_t position_slack = 200;

    mr->seekPosition(SeekReference::end, seek_amount, SeekMode::prioritise_speed).get();

    while (!done.get()) {
        // The assertion below needs to take into account the fact that when MR is
        // paused current position will be changing because seekable range as seen
        // by DashDemux will be tied to wall clock hence it will keep moving.
        Position p = mr->getPosition().get();
        CPPUNIT_ASSERT(p.current > -seek_amount - position_slack &&
                p.current < -seek_amount + position_slack);
    }

    mr->stop().get();
}


={============================================================================
*kt_dev_test_200* gmock-value-parameterized

https://chromium.googlesource.com/external/github.com/google/googletest/+/HEAD/googletest/docs/AdvancedGuide.md#Value_Parameterized_Tests


={============================================================================
*kt_dev_test_200* gmock-problem

Sky Q Unattended coding test

Scenario

A ground-breaking broadcaster has decided to reward its most loyal customers.
A software engineering team, developing the customer account website, is
working on the story below.  Display customer's available rewards As a
customer, if I am eligible for rewards, then I want to see which rewards are
available based on my channel subscriptions.  The Account Management team has
partnered with the Customer Status team that provides a service that checks
the customer eligibility based on loyalty and billing status.

Instructions

You are required to provide an implementation of a RewardsService. Please do
this in C++/QT. Please use the latest GCC compiler or QTCreator for Linux.
Please agree on an alternative with Sky Recruitment if this isnt possible.
The service accepts as input a customer account number and a portfolio
containing channels subscriptions. If the customer is eligible for rewards the
RewardsService should return a list of all the rewards available according to
the subscriptions on the portfolio.


Acceptance Criteria

The following table describes the codes for the channel subscriptions and the
associated rewards.

Channel Reward    SPORTS CHAMPIONS_LEAGUE_FINAL_TICKET
KIDS              N/A
MUSIC             KARAOKE_PRO_MICROPHONE
NEWS              N/A
MOVIES            PIRATES_OF_THE_CARIBBEAN_COLLECTION

The Customer Status team is currently developing the EligibilityService which
accepts the account number as an input. You are required to provide a mock or
stub of the EligibilityService interface.  This is a simple diagram of the
interaction between the services:

                        RewardService           EligibilityService
account number,
channel subscriptions ->  | 
                          | account number ->       |
                          | <- eligibility          |
                          |
<- rewards                |


The following table describes the EligibiityService output and the expected
result:

EligibilityService    Description                 RewardsService result
output

CUSTOMER_ELIGIBLE     Customer is eligible        Return relevant rewards according to
                                                  the customer's portfolio

CUSTOMER_INELIGIBLE   Customer is not eligible    Return no rewards

Technical failure     Service technical failure   Return no rewards
exception

Invalid account       The supplied account        Return no rewards and notify the
number exception      number is invalid           client that the account number is


<code>
// For brevity, uses a single file and class declaration which has
// implementations.

// output
//
// [==========] Running 9 tests from 1 test case.
// [----------] Global test environment set-up.
// [----------] 9 tests from RewardServiceTest
// [ RUN      ] RewardServiceTest.checkRewards_EligibleCustomerVariation1_ReturnRewards
// [       OK ] RewardServiceTest.checkRewards_EligibleCustomerVariation1_ReturnRewards (0 ms)
// [ RUN      ] RewardServiceTest.checkRewards_EligibleCustomerVariation2_ReturnRewards
// [       OK ] RewardServiceTest.checkRewards_EligibleCustomerVariation2_ReturnRewards (0 ms)
// [ RUN      ] RewardServiceTest.checkRewards_EligibleCustomerVariation3_ReturnRewards
// [       OK ] RewardServiceTest.checkRewards_EligibleCustomerVariation3_ReturnRewards (0 ms)
// [ RUN      ] RewardServiceTest.checkRewards_EligibleCustomerVariation4_ReturnRewards
// [       OK ] RewardServiceTest.checkRewards_EligibleCustomerVariation4_ReturnRewards (1 ms)
// [ RUN      ] RewardServiceTest.checkRewards_EligibleCustomer_ReturnNoRewards
// [       OK ] RewardServiceTest.checkRewards_EligibleCustomer_ReturnNoRewards (0 ms)
// [ RUN      ] RewardServiceTest.checkRewards_InEligibleCustomer_ReturnNoRewards
// [       OK ] RewardServiceTest.checkRewards_InEligibleCustomer_ReturnNoRewards (0 ms)
// [ RUN      ] RewardServiceTest.checkRewards_TechnicalFailure_ReturnNoRewards
// [       OK ] RewardServiceTest.checkRewards_TechnicalFailure_ReturnNoRewards (0 ms)
// [ RUN      ] RewardServiceTest.checkRewards_TechnicalFailureFromOthers_ReturnNoRewards
// [       OK ] RewardServiceTest.checkRewards_TechnicalFailureFromOthers_ReturnNoRewards (0 ms)
// [ RUN      ] RewardServiceTest.checkRewards_InvalidAccountFailure_ReturnNoRewards
// [       OK ] RewardServiceTest.checkRewards_InvalidAccountFailure_ReturnNoRewards (0 ms)
// [----------] 9 tests from RewardServiceTest (2 ms total)
// 
// [----------] Global test environment tear-down
// [==========] 9 tests from 1 test case ran. (3 ms total)
// [  PASSED  ] 9 tests.


#include <iostream>
#include <map>
#include <memory>
#include <set>
#include <stdexcept>

#include "gtest/gtest.h"
#include "gmock/gmock.h"
#include "gmock/gmock-matchers.h"
#include "gmock/gmock-generated-matchers.h"

using testing::_;
using testing::Return;
using testing::AtLeast;
using testing::ElementsAre;
using testing::IsEmpty;
using testing::Throw;


// Subscription and Portfolio
enum class Subscription {SPORTS, KIDS, MUSIC, NEWS, MOVIES};

typedef std::set<Subscription> Portfolio;

// Exceptions
class TechnicalFailure : public std::exception
{
    public:
        const char *what() const noexcept
        {
            return "Technical Failure";
        }
};

class InvalidAccount : public std::exception
{
    public:
        const char *what() const noexcept
        {
            return "Invalid Account Number";
        }
};

// Eligibility
enum class Eligibility {CUSTOMER_ELIGIBLE, CUSTOMER_INELIGIBLE};

// EligibilityService
class EligibilityService
{
    public:
        explicit EligibilityService() {}
        virtual ~EligibilityService() {}

        virtual Eligibility checkEligible(const std::string &accountNo) const = 0;
};

// RewardService
class RewardService {
    public:
        RewardService(const std::shared_ptr<EligibilityService> &eilgibility_service)
            : eilgibility_service_(eilgibility_service) {}

        virtual ~RewardService() {}

        // NB. To support differnt reward scheme, can override or can cosider
        // injecting a reward table from outside.

        virtual std::vector<std::string> 
            checkRewards(const std::string &account_number, bool &account_valid,
                    const Portfolio &subscriptions)
        {
            const std::map<Subscription, std::string> rewardsTable{
                {Subscription::SPORTS, "CHAMPIONS_LEAGUE_FINAL_TICKET"},
                {Subscription::MUSIC, "KARAOKE_PRO_MICROPHONE"},
                {Subscription::MOVIES, "PIRATES_OF_THE_CARIBBEAN_COLLECTION"}
            };

            Eligibility found_eligibility{Eligibility::CUSTOMER_INELIGIBLE};
            std::vector<std::string> found_rewards{};

            // Handle exceptions
            try {
                found_eligibility = eilgibility_service_->checkEligible(account_number);
            } catch (InvalidAccount &except)
            {
                account_valid = false;
                return found_rewards;
            } catch (std::exception &except)
            {
                account_valid = true;
                return found_rewards;
            }
            
            if (found_eligibility == Eligibility::CUSTOMER_ELIGIBLE)
            {
                for (auto e : subscriptions)
                {
                    const auto &pos = rewardsTable.find(e);
                    const auto &end = rewardsTable.end();

                    if (pos != end)
                        found_rewards.push_back(pos->second);
                }
            }

            account_valid = true;
            return found_rewards;
        }

    private:
        std::shared_ptr<EligibilityService> eilgibility_service_;
};


// Mock
class MockEligibilityService : public EligibilityService
{
    public:
        MOCK_CONST_METHOD1(checkEligible, Eligibility(const std::string &accountNo));
};


// TESTS

// Eligible and rewards
// NB. If there are more subscriptions to support, might consider usning
// value-parameterized tests for more combinations.

TEST(RewardServiceTest, checkRewards_EligibleCustomerVariation1_ReturnRewards)
{
    auto eilgibility_service = std::make_shared<MockEligibilityService>();
    bool account_valid{false};

    EXPECT_CALL(*eilgibility_service, checkEligible(_))
        .Times(AtLeast(1))
        .WillRepeatedly(Return(Eligibility::CUSTOMER_ELIGIBLE));

    RewardService reward_service(eilgibility_service);

    const auto result = reward_service.checkRewards("111", account_valid,
            Portfolio({Subscription::SPORTS, Subscription::MUSIC}));

    ASSERT_THAT(result, ElementsAre("CHAMPIONS_LEAGUE_FINAL_TICKET", "KARAOKE_PRO_MICROPHONE"));
    ASSERT_EQ(account_valid, true);
}

TEST(RewardServiceTest, checkRewards_EligibleCustomerVariation2_ReturnRewards)
{
    auto eilgibility_service = std::make_shared<MockEligibilityService>();
    bool account_valid{false};

    EXPECT_CALL(*eilgibility_service, checkEligible(_))
        .Times(AtLeast(1))
        .WillRepeatedly(Return(Eligibility::CUSTOMER_ELIGIBLE));

    RewardService reward_service(eilgibility_service);

    const auto result = reward_service.checkRewards("111", account_valid, 
            Portfolio({Subscription::MUSIC, Subscription::SPORTS}));

    ASSERT_THAT(result, ElementsAre("CHAMPIONS_LEAGUE_FINAL_TICKET", "KARAOKE_PRO_MICROPHONE"));
    ASSERT_EQ(account_valid, true);
}

TEST(RewardServiceTest, checkRewards_EligibleCustomerVariation3_ReturnRewards)
{
    auto eilgibility_service = std::make_shared<MockEligibilityService>();
    bool account_valid{false};

    EXPECT_CALL(*eilgibility_service, checkEligible(_))
        .Times(AtLeast(1))
        .WillRepeatedly(Return(Eligibility::CUSTOMER_ELIGIBLE));

    RewardService reward_service(eilgibility_service);

    const auto result = reward_service.checkRewards("111", account_valid,
            Portfolio({Subscription::SPORTS, Subscription::MUSIC, Subscription::KIDS}));

    ASSERT_THAT(result, ElementsAre("CHAMPIONS_LEAGUE_FINAL_TICKET", "KARAOKE_PRO_MICROPHONE"));
    ASSERT_EQ(account_valid, true);
}

TEST(RewardServiceTest, checkRewards_EligibleCustomerVariation4_ReturnRewards)
{
    auto eilgibility_service = std::make_shared<MockEligibilityService>();
    bool account_valid{false};

    EXPECT_CALL(*eilgibility_service, checkEligible(_))
        .Times(AtLeast(1))
        .WillRepeatedly(Return(Eligibility::CUSTOMER_ELIGIBLE));

    RewardService reward_service(eilgibility_service);

    const auto result = reward_service.checkRewards("111", account_valid,
            Portfolio({Subscription::SPORTS, Subscription::KIDS, Subscription::MUSIC}));

    ASSERT_THAT(result, ElementsAre("CHAMPIONS_LEAGUE_FINAL_TICKET", "KARAOKE_PRO_MICROPHONE"));
    ASSERT_EQ(account_valid, true);
}


// Eligible and no rewards
TEST(RewardServiceTest, checkRewards_EligibleCustomer_ReturnNoRewards)
{
    auto eilgibility_service = std::make_shared<MockEligibilityService>();
    bool account_valid{false};

    EXPECT_CALL(*eilgibility_service, checkEligible(_))
        .Times(AtLeast(1))
        .WillRepeatedly(Return(Eligibility::CUSTOMER_ELIGIBLE));

    RewardService reward_service(eilgibility_service);

    const auto result = reward_service.checkRewards("111", account_valid,
            Portfolio({Subscription::KIDS, Subscription::NEWS}));

    ASSERT_THAT(result, IsEmpty());
    ASSERT_EQ(account_valid, true);
}

// Ineligible and no rewards
TEST(RewardServiceTest, checkRewards_InEligibleCustomer_ReturnNoRewards)
{
    auto eilgibility_service = std::make_shared<MockEligibilityService>();
    bool account_valid{false};

    EXPECT_CALL(*eilgibility_service, checkEligible(_))
        .Times(AtLeast(1))
        .WillRepeatedly(Return(Eligibility::CUSTOMER_INELIGIBLE));

    RewardService reward_service(eilgibility_service);

    const auto result = reward_service.checkRewards("111", account_valid,
            Portfolio({Subscription::SPORTS, Subscription::MUSIC}));

    ASSERT_THAT(result, IsEmpty());
    ASSERT_EQ(account_valid, true);
}

// technical failure and no rewards
TEST(RewardServiceTest, checkRewards_TechnicalFailure_ReturnNoRewards)
{
    auto eilgibility_service = std::make_shared<MockEligibilityService>();
    bool account_valid{false};

    EXPECT_CALL(*eilgibility_service, checkEligible(_))
        .Times(AtLeast(1))
        .WillRepeatedly(Throw(TechnicalFailure()));

    RewardService reward_service(eilgibility_service);

    const auto result = reward_service.checkRewards("111", account_valid,
            Portfolio({Subscription::SPORTS, Subscription::MUSIC}));

    ASSERT_THAT(result, IsEmpty());
    ASSERT_EQ(account_valid, true);
}

// technical failure from other exception and no rewards
TEST(RewardServiceTest, checkRewards_TechnicalFailureFromOthers_ReturnNoRewards)
{
    auto eilgibility_service = std::make_shared<MockEligibilityService>();
    bool account_valid{false};

    EXPECT_CALL(*eilgibility_service, checkEligible(_))
        .Times(AtLeast(1))
        .WillRepeatedly(Throw(std::runtime_error("HALTED")));

    RewardService reward_service(eilgibility_service);

    const auto result = reward_service.checkRewards("111", account_valid,
            Portfolio({Subscription::SPORTS, Subscription::MUSIC}));

    ASSERT_THAT(result, IsEmpty());
    ASSERT_EQ(account_valid, true);
}

// invalid account failure and no rewards
TEST(RewardServiceTest, checkRewards_InvalidAccountFailure_ReturnNoRewards)
{
    auto eilgibility_service = std::make_shared<MockEligibilityService>();
    bool account_valid{false};

    EXPECT_CALL(*eilgibility_service, checkEligible(_))
        .Times(AtLeast(1))
        .WillRepeatedly(Throw(InvalidAccount()));

    RewardService reward_service(eilgibility_service);

    const auto result = reward_service.checkRewards("111", account_valid,
            Portfolio({Subscription::SPORTS, Subscription::MUSIC}));

    ASSERT_THAT(result, IsEmpty());
    ASSERT_EQ(account_valid, false);
}


int main(int argc, char** argv)
{
    testing::InitGoogleMock(&argc, argv);
    return RUN_ALL_TESTS();
}


==============================================================================
Copyright: see |ktkb|  vim:tw=100:ts=3:ft=help:norl:
