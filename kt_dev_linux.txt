*kt_dev_lnx*                                                           tw=100, utf-8

/^[#=]{ 

#{ tool
#{ gdb
#{ linux-core
#{ syscall
#{ sysadmin
#{ syssetup

*kt_linux_dist_000* linux-disto

|kt_linux_bash_000| sh-reference
*kt_linux_bash_005* sh-shebang tool-env
*kt_linux_bash_005* sh-quote
*kt_linux_bash_019* sh-pipeline sh-subshell sh-environment
*kt_linux_bash_019* sh-pipeline-case-problem
*kt_linux_bash_019* sh-pipeline-async
*kt_linux_bash_019* sh-list-construct
*kt_linux_bash_021* sh-script
*kt_linux_bash_021* sh-exit-status
*kt_linux_bash_021* sh-return-value-from-script
*kt_linux_bash_001* sh-looping-construct sh-compound-command
*kt_linux_bash_001* sh-looping-construct-getopt sh-parse-arg
*kt_linux_bash_001* sh-conditional-construct sh-regex
*kt_linux_bash_001* sh-conditional-expression
*kt_linux_bash_001* sh-pattern-match
*kt_linux_bash_001* sh-function
*kt_linux_bash_001* sh-variable sh-variable-no-space
*kt_linux_bash_001* sh-variable-positional-parameters
*kt_linux_bash_001* sh-variable-array
*kt_linux_bash_001* sh-expansion
*kt_linux_bash_001* sh-expansion-brace
*kt_linux_bash_001* sh-expansion-tilde
*kt_linux_bash_001* sh-expansion-parameter sh-variable-reference
*kt_linux_bash_001* sh-expansion-parameter-substitude
*kt_linux_bash_001* sh-expansion-command sh-output-from-command
*kt_linux_bash_001* sh-expansion-word sh-ifs
*kt_linux_bash_001* sh-expansion-arithmetic sh-expr
*kt_linux_bash_001* sh-expansion-process
*kt_linux_bash_001* sh-expansion-filename
*kt_linux_bash_001* sh-redirection
*kt_linux_bash_001* sh-redirection-here
*kt_linux_bash_001* sh-executing
*kt_linux_bash_024* sh-builtin
*kt_linux_bash_024* sh-builtin-base sh-trap
*kt_linux_bash_024* sh-builtin-bash sh-read
*kt_linux_bash_024* sh-builtin-set
*kt_linux_bash_038* sh-builtin-declare
*kt_linux_bash_024* sh-builtin-job
*kt_linux_bash_024* sh-builtin-help
*kt_linux_bash_024* sh-builtin-source
*kt_linux_bash_024* sh-builtin-hash which-command
*kt_linux_bash_025* sh-special-variable
*kt_linux_bash_023* sh-invoking
|kt_linux_bash_030| sh-interactive-or-not
*kt_linux_bash_023* sh-command-history
*kt_linux_bash_032* sh-command-edit
*kt_linux_bash_032* sh-command-search
*kt_linux_bash_032* sh-command-readline-init

*kt_linux_bash_100* sh-error-pushd
*kt_linux_bash_100* sh-alias
*kt_linux_bash_100* sh-tip-colour-prompt
*kt_linux_bash_101* sh-tip-nested-level
|kt_linux_bash_202| sh-tip-recursion-test
*kt_linux_bash_202* sh-tip-get-filenames-from-comment
*kt_linux_bash_202* sh-tip-search-number-range
*kt_linux_bash_202* sh-code-case
*kt_linux_bash_202* sh-code-case-use-config
*kt_linux_bash_202* sh-code-case-use-function-command
*kt_linux_bash_202* sh-code-check-file-size
*kt_linux_bash_202* sh-code-write-script


#{ tool
|kt_linux_tool_001| tool-md5sum tool-shasum
|kt_linux_tool_004| dmesg
*kt_linux_tool_005* tool-uname linux-check-distribution
|kt_linux_tool_006| tool-cp
|kt_linux_tool_007| mkdir
*kt_linux_tool_008* tool-strings
*kt_linux_tool_009* tool-head
|kt_linux_tool_002| tool-cut
|kt_linux_tool_009| tool-sort
*kt_linux_tool_009* tool-uniq
|kt_linux_tool_011| tool-find
*kt_linux_tool_011* tool-which
|kt_linux_tool_012| make a empty file without touch
*kt_linux_tool_013* tool-xargs
|kt_linux_tool_014| tool-ssh
*kt_linux_tool_014* tool-putty
*kt_linux_tool_014* tool-putty-xwin
*kt_linux_tool_015* tool-pgrep, tool-pidof
|kt_linux_tool_018| tool-ls
*kt_linux_tool_019* tool-trace tool-ftrace tool-uftrace
|kt_linux_tool_019| tool-trace tool-strace
*kt_linux_tool_020* tool-trace-case
|kt_linux_tool_020| tool-date
|kt_linux_tool_021| tool-chmod
|kt_linux_tool_022| mknod
|kt_linux_tool_023| tool-wc word count
|kt_linux_tool_025| tool-ln
*kt_linux_tool_026* tool-rsync
|kt_linux_tool_027| tool-awk
|kt_linux_tool_028| tool-sed, print a range
|kt_linux_tool_029| pyserial and grabserial
|kt_linux_tool_030| tool-diff tool-patch
|kt_linux_tool_031| tool-zip tool-tar
|kt_linux_tool_031| tool-tar-sparce
|kt_linux_tool_032| tool-split tool-merge
*kt_linux_tool_032* tool-tr
*kt_linux_tool_032* tool-dd
|kt_linux_tool_033| getconf
|kt_linux_tool_034| tool-ps tool-pstree
*kt_linux_tool_300* tool-kill
|kt_linux_tool_034| tool-mem
|kt_linux_tool_034| tool-chrt
*kt_linux_tool_034* tool-nproc
*kt_linux_tool_035* tool-wget tool-curl
|kt_linux_tool_036| nc
|kt_linux_tool_037| port checks
|kt_linux_tool_016| tool-screen
|kt_linux_tool_038| tool-minicom
|kt_linux_tool_024| tool-du tool-df tool-stat
*kt_linux_tool_024* tool-quota
*kt_linux_tool_051* tool-fdisk
*kt_linux_tool_051* tool-fdisk-resize-boot-partition
*kt_linux_tool_051* tool-fs-lvm
*kt_linux_tool_051* tool-ip
*kt_linux_tool_039* tool-udhcpc tool-dns
*kt_linux_tool_039* tool-route
*kt_linux_tool_039* tool-nfs
|kt_linux_tool_040| install
|kt_linux_tool_041| notify-send
*kt_linux_tool_042* tool-less
*kt_linux_tool_042* tool-tail: print multiple files with filename
|kt_linux_tool_044| mc
|kt_linux_tool_045| graphbiz
|kt_linux_tool_046| tool-mktemp
*kt_linux_tool_048* tool-dircolors
*kt_linux_tool_048* tool-tput
|kt_linux_tool_048| tool-gnome-terminal
*kt_linux_tool_049* tool-terminator
|kt_linux_tool_050| tool-watch
*kt_linux_tool_051* tool-github kb-github
*kt_linux_tool_051* tool-font set-font
*kt_linux_tool_051* tool-echo
*kt_linux_tool_051* tool-echo-compile
*kt_linux_tool_051* tool-tcpdump
*kt_linux_tool_051* tool-swapon
*kt_linux_tool_051* tool-htop
*kt_linux_tool_051* tool-xterm: unknown terminal type
*kt_linux_tool_051* tool-lpstat: view and cancel print jobs
*kt_linux_tool_051* tool-i3wm:
*kt_linux_tool_051* tool-hexdump
*kt_linux_tool_051* tool-hexedit tool-ghex
*kt_linux_tool_051* tool-uptime
*kt_linux_tool_051* tool-lsof
*kt_linux_tool_051* tool-dirname tool-basename tool-readlink
*kt_linux_tool_051* tool-last
*kt_linux_tool_051* tool-add-user tool-sudo
*kt_linux_tool_051* tool-virtualbox tool-samba tool-vm
*kt_linux_tool_051* tool-cron tool-incron
*kt_linux_tool_100* tool-gprof
*kt_linux_tool_100* tool-gpg
*kt_linux_tool_100* tool-chrome

*kt_linux_tool_100* tool-efence-issues
*kt_linux_tool_100* tool-efence-benchmark
*kt_linux_tool_100* tool-efence
*kt_linux_tool_100* tool-efence-duma
*kt_linux_tool_100* tool-efence-dmalloc
*kt_linux_tool_100* tool-efence-fortify
*kt_linux_tool_100* tool-asan tool-efence cpp-memory-issue
*kt_linux_tool_100* tool-asan-flag asan-code
*kt_linux_tool_100* tool-asan-code-global-buffer-overflow
*kt_linux_tool_100* tool-asan-case
*kt_linux_tool_100* tool-asan tool-kasan
*kt_linux_tool_000* tool-asan-llvm
*kt_linux_tool_000* tool-asan-gcc
*kt_linux_tool_000* tool-asan-vstb
*kt_linux_tool_100* tool-efence-others
*kt_linux_tool_100* tool-valgrind
*kt_linux_tool_100* tool-valgrind-memcheck
*kt_linux_tool_100* tool-valgrind-memcheck-doc
*kt_linux_tool_100* tool-valgrind-memcheck-case
*kt_linux_tool_100* tool-valgrind-massif
*kt_linux_tool_001* tool-bugzilla
*kt_linux_tool_001* tool-ftp
*kt_linux_tool_001* tool-tftp
*kt_linux_tool_001* tool-rpm
*kt_linux_tool_001* tool-mail


|kt_linux_tool_100| tool-package-apt
|kt_linux_tool_101| tool-pkg-config

#{ make and build
*kt_linux_tool_140* make-cmake
*kt_linux_tool_140* make-cmake-basic
*kt_linux_tool_140* make-cmake-add-subdir
*kt_linux_tool_140* make-cmake-mix-c-and-cpp
*kt_linux_tool_140* make-cmake-compile-flag
*kt_linux_tool_140* make-cmake-include
*kt_linux_tool_140* make-cmake-link-flag

*kt_linux_tool_140* make-kernel-config
*kt_linux_tool_140* make-gmake-debug
*kt_linux_tool_140* make-gmake-sample
*kt_linux_tool_140* make-gmake-rule
*kt_linux_tool_140* make-gmake-rule-implicit
*kt_linux_tool_140* make-gmake-rule-multiple
*kt_linux_tool_140* make-gmake-rule-automatic-variable
*kt_linux_tool_140* make-gmake-include
*kt_linux_tool_140* make-gmake-vpath make-o-variable
*kt_linux_tool_140* make-gmake-phony
*kt_linux_tool_140* make-gmake-builtin-target
*kt_linux_tool_140* make-gmake-recipe gmake-just-print
*kt_linux_tool_140* make-gmake-recipe-parallel make-curdir
*kt_linux_tool_140* make-gmake-two-variable make-gmake-read-makefile
*kt_linux_tool_140* make-gmake-variable
*kt_linux_tool_140* make-gmake-variable-override
*kt_linux_tool_140* make-gmake-variable-split
*kt_linux_tool_140* make-gmake-variable-define
*kt_linux_tool_140* make-gmake-variable-pattern
*kt_linux_tool_140* make-gmake-variable-empty-string
*kt_linux_tool_140* make-gmake-conditional
*kt_linux_tool_140* make-gmake-text-function
*kt_linux_tool_140* make-gmake-run

*kt_linux_tool_150* make-autoconf
*kt_linux_tool_150* make-autoconf-build-system
*kt_linux_tool_150* make-autoconf-model
*kt_linux_tool_150* make-autoconf-write
*kt_linux_tool_150* make-autoconf-macro
*kt_linux_tool_150* make-autoconf-test
*kt_linux_tool_150* make-autoconf-test-write
*kt_linux_tool_150* make-autoconf-test-result
*kt_linux_tool_150* make-autoconf-package
*kt_linux_tool_150* make-autoconf-run-configure
*kt_linux_tool_150* make-autoconf-configure

|kt_linux_tool_150| make-automake
*kt_linux_tool_150* make-automake-standard targets
*kt_linux_tool_150* make-automake-directory variable
*kt_linux_tool_150* make-automake-distcheck
*kt_linux_tool_150* make-automake-ex
*kt_linux_tool_150* make-automake-ex-explained
*kt_linux_tool_150* make-automake-variables
*kt_linux_tool_150* make-automake-ex-packages
*kt_linux_tool_150* make-automake-macros
*kt_linux_tool_150* make-automake-building programs and libraries
*kt_linux_tool_150* make-automake-build-variables
*kt_linux_tool_150* automake: 08: program variables: case issue
*kt_linux_tool_150* automake: 09: scripts
*kt_linux_tool_150* automake: 15: test and make check
|kt_linux_tool_160| ccache

|kt_linux_tool_200| bin-:
*kt_linux_tool_201* tool-size
*kt_linux_tool_202* tool-nm
*kt_linux_tool_203* tool-objdump
*kt_linux_tool_202* tool-strip
|kt_linux_tool_204| tool-addr2line
|kt_linux_tool_205| binutil: ld
*kt_linux_tool_300* tool-apache-ivy

#{ gdb
*kt_linux_gdb_000* gdb-how-works
*kt_linux_gdb_300* gdb-debugging-info
*kt_linux_gdb_001* gdb-error gdb-target
|kt_linux_gdb_300| gdb-term and links
*kt_linux_tool_300* gdb-sample session
*kt_linux_tool_300* gdb-start
*kt_linux_gdb_000* gdb-start-options gdb-start-order
*kt_linux_gdb_000* gdb-command-file gdb-init-file
*kt_linux_tool_300* gdb-start: repeatedly run a test in GDB, until it fails
*kt_linux_tool_300* gdb-start: what do during startup
*kt_linux_gdb_300* gdb-start-specify-files
*kt_linux_gdb_300* gdb-start gdb-run gdb-set-env
*kt_linux_gdb_300* gdb-start-tty gdb-tty
*kt_linux_gdb_000* gdb-attach-debug already running process
*kt_linux_tool_300* gdb-thread-multiple threads
*kt_linux_tool_300* gdb-thread: tid on ps and gdb
*kt_linux_gdb_300* gdb-log
*kt_linux_tool_300* gdb-shell
*kt_linux_tool_300* gdb-general and help
*kt_linux_gdb_300* gdb-info

*kt_linux_tool_300* gdb-sources, source path, and substitute
*kt_linux_gdb_000* gdb-print-memory
*kt_linux_gdb_300* gdb-print gdb-disp
*kt_linux_gdb_300* gdb-exam-address-and-code gdb-disassemble
*kt_linux_gdb_000* gdb-cxx
*kt_linux_gdb_300* gdb-symbol
*kt_linux_gdb_300* gdb-exam-symbol-loading
*kt_linux_gdb_300* gdb-exam-debug-searate-file
*kt_linux_gdb_300* gdb-exam-debug-with-and-without-g gdb-debug
*kt_linux_gdb_300* gdb-slib-search
*kt_linux_gdb_300* gdb-solib-break
*kt_linux_gdb_300* gdb-solib-case
*kt_linux_tool_300* gdb-case: debugging with strace and gdb in sandbox
|kt_linux_gdb_300| gdb-remote
*kt_linux_gdb_300* gdb-remote-gdbserver
|kt_linux_gdb_300| gdb-remote-args
*kt_linux_gdb_300* gdb-remote-case
*kt_linux_gdb_300* gdb-remote: frontend: cgdb
*kt_linux_gdb_300* gdb-remote: frontend: eclipse

*kt_linux_tool_300* gdb-core
|kt_linux_tool_300| gdb-core-setting
*kt_linux_tool_300* gdb-core-settting-run-commands when makes core
*kt_linux_gdb_300* gdb-core-signal tool-kill
*kt_linux_tool_300* gdb-core-force-core-command
*kt_linux_gdb_300* gdb-signal
*kt_linux_gdb_000* gdb-frame
*kt_linux_gdb_300* gdb-core-backtrace gdb-bt
|kt_linux_gdb_300| gdb-core-analysis-on-mips
*kt_linux_gdb_000* gdb-break
*kt_linux_gdb_001* gdb-break-continue gdb-step
*kt_linux_gdb_000* gdb-break-condition gdb-break-temp gdb-condition
*kt_linux_gdb_000* gdb-break-convenience-variable
*kt_linux_gdb_000* gdb-break-info
*kt_linux_tool_309* gdb-break: ignore
*kt_linux_gdb_000* gdb-break-watch gdb-watch
*kt_linux_gdb_000* gdb-break-catch
*kt_linux_gdb_000* gdb-break-control
*kt_linux_tool_310* gdb-break: save
*kt_linux_tool_300* gdb-break: dprintf
*kt_linux_tool_310* gdb-dprintf

*kt_linux_tool_400* gdb-stl-step-into-code
*kt_linux_gdb_400* gdb-debug-stl gcc-libstdc++
*kt_linux_tool_400* gdb-stl-pretty-printer-gdb-stl-views
*kt_linux_gdb_400* gdb-registers gdb-init

|kt_linux_rege_001| regex-bre-ere
|kt_linux_rege_001| regex-operators
*kt_linux_rege_001* regex-group regex-reference
*kt_linux_rege_001* regex-named-capture-group-reference
*kt_linux_rege_001* regex-pcre
|kt_linux_rege_001| regex-usage
*kt_linux_rege_001* regex-grep tool-grep

#{ busybox
*kt_linux_busy_001* busy-links
*kt_linux_busy_001* busy-build
*kt_linux_busy_001* busy-build-mount-issue

#{ gcc
*kt_linux_gcc_400* gcc-doc
*kt_linux_gcc_400* gcc-ld gcc-linker
*kt_linux_gcc_400* gcc-ld-linker-script
*kt_linux_gcc_400* gcc-asm
*kt_linux_gcc_400* gcc-libgcc gcc-ucmpdi2-error
*kt_linux_gcc_400* gcc-option-output gcc-option-verbose
*kt_linux_gcc_400* gcc-option-developers
*kt_linux_gcc_400* gcc-option-directory-search
*kt_linux_gcc_400* gcc-option-link
*kt_linux_gcc_400* gcc-option-link-order gcc-link-order
*kt_linux_gcc_400* gcc-option-warning
*kt_linux_gcc_400* gcc-option-cpp gcc-cpp
*kt_linux_gcc_203* gcc-option-cpp-search gcc-cpp-isystem
*kt_linux_gcc_400* gcc-option-cpp-dependancies
*kt_linux_gcc_400* gcc-option-cpp-check-defines
*kt_linux_gcc_400* gcc-option-cpp-where-comes-from
*kt_linux_gcc_400* gcc-option-code-gen
*kt_linux_gcc_400* gcc-option-optimization
*kt_linux_gcc_400* gcc-option-inline gcc-gnu-extension
*kt_linux_gcc_400* gcc-option-debug
*kt_linux_gcc_400* gcc-option-instrument
*kt_linux_gcc_400* gcc-pragma
*kt_linux_gcc_400* gcc-attribute
*kt_linux_gcc_400* gcc-build-execution-env
*kt_linux_gcc_400* gcc-toolchain-target-triplet
*kt_linux_gcc_400* gcc-build-option
*kt_linux_gcc_400* gcc-int-internal
*kt_linux_gcc_400* gcc-int-spec-file
*kt_linux_gcc_400* gcc-int-ucmpdi2-error
*kt_linux_gcc_400* gcc-build-sysroot
*kt_linux_gcc_400* gcc-build-tc-failed-instructions
*kt_linux_gcc_400* gcc-build-tc-buildroot-build
*kt_linux_gcc_400* gcc-build-tc-buildroot-internal
*kt_linux_gcc_400* gcc-build-buildroot-external-toolchain
*kt_linux_gcc_400* gcc-build-spk-build-vmlinux
*kt_linux_gcc_400* gcc-build-spk-toolchain
*kt_linux_gcc_400* gcc-toolchain-spec gcc-spec
*kt_linux_gcc_400* gcc-build-spk-using-own-toolchain current-working
*kt_linux_gcc_400* gcc-build-spk-gdb gcc-build-spk-bash
*kt_linux_gcc_400* gcc-toolchain-reference gcc-build
*kt_linux_gcc_400* gcc-build-c-library
*kt_linux_gcc_400* gcc-toolchain-glibc-mips buildroot-glic-mips-internal-toolchain
*kt_linux_gcc_400* gcc-toolchain-glibc-mips-own
*kt_linux_gcc_400* gcc-toolchain-tool-asan
*kt_linux_gcc_400* gcc-build-mips-uclibc
*kt_linux_gcc_400* gcc-build-mips-uclibc-asan
*kt_linux_gcc_400* gcc-build-mips-uclibc-asan-combinations
*kt_linux_gcc_400* gcc-build-tc-mips-target-tree
*kt_linux_gcc_400* gcc-build-uclibc-target-tree
*kt_linux_gcc_400* gcc-toolchain-cross-ng-mips-uclibc

*kt_linux_arch_001* arch-endian
*kt_linux_arch_001* arch-mips-registers mips-instruction
*kt_linux_arch_001* arch-mips-stack
*kt_linux_arch_001* arch-mips-backtrace mips-membuster
*kt_linux_arch_001* arch-backtrace
*kt_linux_arch_001* arch-mips-for-and-while
*kt_linux_arch_001* arch-mips-power-on-reset
*kt_linux_arch_001* arch-mips-qemu tool-qemu
*kt_linux_arch_001* arch-x86-stack
*kt_linux_arch_001* arch-x86-asm

*kt_linux_core_400* linux-elf-readelf
*kt_linux_core_400* linux-elf
*kt_linux_core_400* linux-elf-relocation
*kt_linux_core_400* linux-elf-pic linux-elf-plt
*kt_linux_core_400* linux-elf-loading
*kt_linux_core_400* linux-elf-start
*kt_linux_core_400* linux-abi
*kt_linux_core_400* linux-lib-libc-glibc check-libc-version
*kt_linux_core_400* linux-lib-libc-glibc-port
*kt_linux_core_400* linux-lib-libc-glibc-testing
*kt_linux_core_400* linux-lib-libc-uclibc
*kt_linux_core_400* linux-lib-shared
*kt_linux_core_400* linux-lib-static-how-works
*kt_linux_core_400* linux-lib-dependancy
*kt_linux_core_400* linux-lib-ld linux-dynamic-linker
*kt_linux_core_400* linux-lib-ld-glibc ld-gdb
*kt_linux_core_400* linux-lib-ld-uclibc spk-toolchain-uclibc
*kt_linux_core_400* linux-lib-ld-uclibc-fail-to-find-symbol
*kt_linux_core_412* linux-lib-ld-search
*kt_linux_core_402* linux-lib-ld-resolve-order lib-preload
*kt_linux_core_400* linux-lib-ld-debug
*kt_linux_core_405* linux-lib-ld-api
*kt_linux_core_400* linux-lib-shared-soname
*kt_linux_core_400* linux-lib-shared-visibility
*kt_linux_core_406* lib-shared-elf-edit 
*kt_linux_core_410* lib-shared: check libraries that process uses
*kt_linux_core_411* lib-shared: case problem in open failure
*kt_linux_core_401* slib: --as-needed flag and link error
*kt_linux_core_403* slib-case-problem in name resolve. crash
*kt_linux_core_404* slib-case-problem in name resolve. pthread stub 


#{ linux-performance
|kt_linux_perf_001| bootchart

#{ linux-db
*kt_linux_rmdb_001* sql-rollback

*kt_linux_core_002* linux-source
*kt_linux_core_001* linux-boot init-process
*kt_linux_core_001* linux-terminal
*kt_linux_core_002* linux-syscall
*kt_linux_core_001* linux-syscall-list
*kt_linux_core_001* linux-syscall-syscall
*kt_linux_core_001* linux-syscall-reboot
*kt_linux_core_001* linux-syscall-pause
*kt_linux_core_001* linux-syscall-alarm
*kt_linux_core_003* linux-syscall-getenv setenv
*kt_linux_core_004* linux-errno
|kt_linux_core_005| linux-error-handling-code thread-errno
*kt_linux_core_006* linux-portability
|kt_linux_core_007| /dev/null

*kt_linux_core_050* linux-signal
*kt_linux_core_050* linux-signal-handler
*kt_linux_core_051* linux-signal-names
*kt_linux_core_051* linux-signal-ex: use signal as synchronization

|kt_linux_core_100| linux-process
*kt_linux_core_107* linux-process-argv
*kt_linux_core_107* linux-process-environment-list
|kt_linux_core_101| linux-process-creation
|kt_linux_core_102| linux-process-termination
|kt_linux_core_103| linux-process-monitor-child-process
*kt_linux_core_104* linux-process-zombie *ex-interview*
*kt_linux_core_104* linux-process-sigchld
|kt_linux_core_110| linux-process-exec exec-call *exec-wrapper-example*
|kt_linux_core_110| linux-process-exec-system-call 
*kt_linux_core_110* linux-process-exec-clone-call linux-thread-id
*kt_linux_core_103* linux-process-priority-and-schedule 
*kt_linux_core_110* linux-process-resource
|kt_linux_core_108| linux-process-group todo
|kt_linux_core_109| linux-process-deamon todo

*kt_linux_core_105* linux-mem-process
*kt_linux_core_106* linux-mem-virtual
*kt_linux_core_050* linux-mem-alloc
*kt_linux_core_050* linux-mem-mapping
*kt_linux_core_050* linux-mem-mapping linux-lib-shared
*kt_linux_core_050* linux-mem-mapping oom-killer
*kt_linux_core_151* linux-thread-source-and-ex
|kt_linux_core_153| linux-thread-compile
*kt_linux_core_154* linux-thread-api
*kt_linux_core_157* linux-thread-attribute
*kt_linux_core_157* linux-thread-safe linux-thread-specific-data
*kt_linux_core_157* linux-thread-stack
*kt_linux_core_157* linux-thread-nptl
|kt_linux_core_102| linux-thread-q how to run three threads sequencially

*kt_linux_core_200* linux-io-model
*kt_linux_core_201* linux-io-ioctl
|kt_linux_core_201| linux-io-extended open-file-table
|kt_linux_core_202| linux-io-non-blocking
*kt_linux_core_203* linux-io-fs driver-api
*kt_linux_core_203* linux-io-fs-vfs
*kt_linux_core_202* linux-io-buffering
*kt_linux_core_203* linux-io-cache
*kt_linux_core_203* linux-io-ex
*kt_linux_core_203* linux-io-dev-fd
*kt_linux_core_203* linux-io-temp-file
*kt_linux_core_203* linux-io-file-attribute
*kt_linux_core_203* linux-io-directories
*kt_linux_core_203* linux-io-permission-check linux-process-credentials
*kt_linux_core_203* linux-io-fs-mount
*kt_linux_core_203* linux-io-fs-mount-tool-mount
*kt_linux_core_203* linux-io-fs-mount-bind tool-chroot
*kt_linux_core_203* linux-io-monitor-events 
*kt_linux_core_204* linux-limit-and-options limit-h
|kt_linux_core_800| linux-proc
*kt_linux_core_814* linux-proc-system-info

|kt_linux_core_200| ipc
|kt_linux_core_201| ipc: system v <ipcs-command>
|kt_linux_core_202| ipc: system v: shm
|kt_linux_core_203| ipc: server consideration

|kt_linux_core_250| ipc: posix

|kt_linux_core_300| ipc: socket: LPI 56

*kt_linux_core_101* linux-ipc-pipe
*kt_linux_core_102* linux-ipc-pipe-check-setup
|kt_linux_core_103| linux-ipc-fifo
|kt_linux_core_104| ipc: semantics of read() and write() on pipes and fifo
|kt_linux_core_105| ipc: which one to use {semaphores-versus-pthreads-mutexes}

*kt_linux_core_200* linux-sync-semaphore
*kt_linux_core_201* linux-sync-mutex
*kt_linux_core_202* linux-sync-deadlock
*kt_linux_core_202* linux-sync-cond
*kt_linux_core_202* linux-sync-cond-lost-wake-up-issue
*kt_linux_core_202* linux-sync-cond-lpi-example
*kt_linux_core_202* linux-sync-cond-unp-example
*kt_linux_core_202* linux-sync-cond-order linux-sync-cond-spurious-wakeup
*kt_linux_core_202* linux-sync-cond-advanced
*kt_linux_core_202* linux-sync-cond-examples
*kt_linux_core_202* linux-sync-between-processes
|kt_linux_core_220| sync: read-write lock
*kt_linux_core_230* linux-sync-file-lock
*kt_linux_core_260* linux-sync-atomic
|kt_linux_core_263|  conc: ref: lock-free code: a false sense of security
|kt_linux_core_264|  conc: ref: the free lunch is over 
*kt_linux_core_265* sync: case: subtle race
*kt_linux_core_266* sync: case: sync with no lock

|kt_linux_core_300|  case: own semaphore and mutex class using pthread cond-var {cqueue}
|kt_linux_core_301|  case: use of mutex and thread class
|kt_linux_core_302|  case: analysis of 200 and 201 case
|kt_linux_core_303|  case: msg q between threads


|kt_linux_core_407| shared library: further information
|kt_linux_core_408| shared library: md5sum
*kt_linux_core_411* slib: points to enhance performance
*kt_linux_core_412* slib: as-needed and _GLOBAL_OFFSET_TABLE_

|kt_linux_core_500| sandbox

|kt_linux_core_600| linux-time
|kt_linux_core_601| linux-time-conversion
|kt_linux_core_602| linux-time-resolution jiffies
*kt_linux_core_602* linux-time-process-time
*kt_linux_core_603* linux-time-timer
|kt_linux_core_603| linux-time-realtime
*kt_linux_core_604* linux-time-ns-stamp
|kt_linux_core_604| linux-time-ms-stamp
*kt_linux_core_604* linux-time-us-stamp
|kt_linux_core_606| linux-time-usleep

|kt_linux_core_700| dbus
*kt_linux_core_700* dbus-libdbus
*kt_linux_core_700* dbus-bindings
*kt_linux_core_700* dbus-case
*kt_linux_core_700* dbus-case-service
|kt_linux_core_701| dbus-introspection
|kt_linux_core_702| dbus: dbus-send tool
|kt_linux_core_703| dbus: lsdbus tool
|kt_linux_core_704| dbus: dbus-monitor tool
|kt_linux_core_710| dbus: kdbus

#{ syssetup
*kt_linux_sete_005* admin-check running services
*kt_linux_sete_005* admin: check nvidia version
*kt_linux_sete_005* admin-gnome: check gnome version
*kt_linux_sete_005* admin-gnome: shortcuts
|kt_linux_sete_005| admin: firefox shortcuts
|kt_linux_sete_005| admin: tbird shortcuts
|kt_linux_sete_006| ubuntu: connect from windows remote desktop
|kt_linux_sete_007| ubuntu: cpuinfo
|kt_linux_sete_008| ubuntu: change default application setting

|kt_linux_sete_101| gnome: workspace
*kt_linux_sete_101* gnome: change wallpapers
|kt_linux_sete_105| set: local web server
*kt_linux_sete_105* set: update adobe flash plugin

|kt_linux_sete_200| which to install?

|kt_linux_refe_001|  references


# ============================================================================
#{
={============================================================================
*kt_linux_bash_000* sh-reference

http://www.gnu.org/software/bash/
http://www.gnu.org/software/bash/manual/
http://www.gnu.org/software/bash/manual/bash.html

http://mywiki.wooledge.org/BashGuide

Advanced Bash-Scripting Guide
https://wiki.kldp.org/HOWTO/html/Adv-Bash-Scr-HOWTO/
http://www.tldp.org/LDP/abs/html/

Bash Hackers Wiki Frontpage
http://wiki.bash-hackers.org/start


={============================================================================
*kt_linux_bash_005* sh-shebang tool-env

difference between two: 

  #!/usr/bin/python
  #!/usr/bin/env python since using

env is a portable way to use?
  
NAME
       env - run a program in a modified environment

SYNOPSIS
       env [OPTION]... [-] [NAME=VALUE]... [COMMAND [ARG]...]

DESCRIPTION
       Set each NAME to VALUE in the environment and run COMMAND.

the fist use fixed /path/to and if some has different python installation, may
have a problem to run this script.

the second uses PATH variable instead but may run different python version
depending on PATH value set for a user. Also, this do not allow to pass
arguments to the command such as python -f.

<tool-env>
env - run a program in a modified environment

LD_PRELOAD=/usr/local/lib/libdirectfb.so:/usr/local/lib/libdirect.so:/usr/local/lib/libinit.so

env ${LD_PRELOAD} nickelmediad \
    --no-mediasettings --no-localmedialibrary \
    --no-outputmanager --no-servicelistbuilder \
    -b $BUS_NAME -f $cfg &


={============================================================================
*kt_linux_bash_005* sh-quote

3.1.2 Quoting

Quoting is used to remove the special meaning of certain characters or words to
the shell. Quoting can be used to disable special treatment for special
characters, to prevent reserved words from being recognized as such, and to
prevent parameter expansion.

// metacharacter
//    A character that, when unquoted, separates words. A metacharacter is a
//    blank or one of the following characters: ‘|’, ‘&’, ‘;’, ‘(’, ‘)’, ‘<’, or
//    ‘>’.

There are three quoting mechanisms: the escape character, single quotes, and
double quotes. 

* Escape Character: to remove the special meaning from a single character.
 
A non-quoted backslash ‘\’ is the Bash `escape-character`. It preserves the
literal value of the next `character` that follows

<ex>
$ echo You owe \$1250
You owe $1250


*sh-single-double-quote*
* Single Quotes: to inhibit `all` interpretation of a sequence of characters.

Enclosing `characters` in single quotes (‘'’) preserves the literal value of each
character within the quotes. A single quote `may not occur between single quotes`,
even when preceded by a backslash. 

<ex>
Quote all inside the single quote.

$ echo '<-$1250.**>; (update?) [y|n]'
<-$1250.**>; (update?) [y|n]


* Double Quotes: to suppress `most` of the interpretation of a sequence of
  characters.

Enclosing characters in double quotes (‘"’) preserves the literal value of all
characters within the quotes, with the exception of ‘$’, ‘`’, ‘\’, and, when
history expansion is enabled, ‘!’. 

The characters ‘$’ and ‘`’ retain their special meaning within double quotes
(see Shell Expansions). 

The backslash retains its special meaning only when followed by one of the
following characters: ‘$’, ‘`’, ‘"’, ‘\’, or newline. Within double quotes,
backslashes that are followed by one of these characters are removed.
  Backslashes preceding characters without a special meaning are left
  unmodified. A double quote may be quoted within double quotes by preceding it
  with a backslash. If enabled, history expansion will be performed unless an
  ‘!’ appearing in double quotes is escaped using a backslash. The backslash
  preceding the ‘!’ is not removed.

The special parameters ‘*’ and ‘@’ have special meaning when in double quotes
(see Shell Parameter Expansion). 

<ex>
Do not quote all (see below) so that `can use variables` inside the quoted.

$ echo '$USER owes <-$1250.**>; [ as of (``date +%m/%d``) ]'
$USER owes <-$1250.**>; [ as of (`date +%m/%d`) ]

$ echo "$USER owes <-$1250.**>; [ as of (``date +%m/%d``) ]"
parkkt owes <-250.**>; [ as of (01/26) ]

<ex>
To prevent globbing by shell before passing params to grep command.

$ grep '[0-9][0-9]*$' report2 report7

<ex>
ssh theyard "mysql -e "select upload_time from upload where parser_result != 'Not Yet Parsed' $1""      # not work
ssh theyard "mysql -e \"select upload_time from upload where parser_result != 'Not Yet Parsed' $1\""    # work


={============================================================================
*kt_linux_bash_019* sh-pipeline sh-subshell sh-environment

*sh-pipe*
3.2.2 Pipelines

A pipeline is a sequence of simple commands separated by one of the control
`operators | or |&.`

The format for a pipeline is

[time [-p]] [!] command1 [ | or |& command2 ] ...

The output of each command in the pipeline is connected via a pipe to the input
of the next command.  That is, each command reads the previous command's output.
This connection is performed before any redirections specified by the command.

If |& is used, command1's standard error, in addition to its standard output, is
connected to command2's standard input through the pipe; it is shorthand for
2>&1 |. This implicit redirection of the standard error to the standard output
is performed after any redirections specified by the command.

If the pipeline is 'not' executed asynchronously (see `list-construct`), the
shell 'waits' for all commands in the pipeline to complete.

*sh-pipeline-subshell*
`each-command` in a pipeline is executed in its own `subshell`. The exit status
of a pipeline is the exit status of the `last-command` in the pipeline, unless
the `pipefail-option` is enabled (see The Set Builtin). 


3.7.3 Command Execution Environment

note: shell's execution-environment

The shell has an `execution-environment`, which consists of the following:

* open files inherited by the shell at invocation, as modified by redirections
  supplied to the exec builtin

* the `current working directory` as set by cd, pushd, or popd, or inherited by
  the shell at invocation

* the file creation mode mask as set by umask or inherited from the shell's
  parent

* current traps set by trap

* `shell parameters` that are set by variable assignment or with set or inherited
  from the shell's parent in the environment

* shell functions defined during execution or inherited from the shell's parent
  in the environment 

* options enabled at invocation (either by default or with command-line arguments)
  or by set 

* options enabled by shopt (see The Shopt Builtin)

* shell aliases defined with alias (see Aliases)

* various process IDs, including those of background jobs (see Lists), the value
  of $$, and the value of $PPID 


note: subshell's execution-environment

When a simple command other than a builtin or shell function is to be
executed, it is invoked in a `separate execution-environment` that consists of
the following. Unless otherwise noted, the values are `inherited from` the
`shell`.

* the shell's open files, plus any modifications and additions specified by
  redirections to the command
* the current working directory
* the file creation mode mask

* shell variables and functions `marked-for-export`, along with variables
  exported for the command, passed in the `environment` (see Environment)

A command invoked in this separate environment cannot affect the shell's
execution environment.

*sh-parentheses-subshell*
`command-substitution`, commands `grouped-with-parentheses`, and
`asynchronous-commands` are invoked in a `subshell-environment` that is a
duplicate of the shell environment, except that traps caught by the shell are
reset to the values that the shell inherited from its parent at invocation. 

Builtin commands that are invoked as part of a pipeline are also executed in a
`subshell-environment`. Changes made to the subshell environment cannot affect
the shell's execution environment.

Subshells spawned to execute command substitutions inherit the value of the -e
option from the parent shell. When not in POSIX mode, Bash clears the -e option
in such subshells.


3.7.4 Environment *sh-export*

When a program is invoked it is given an array of strings called the
`environment`. This is a list of `name-value` pairs, of the form name=value.

Bash provides several ways to manipulate the environment. 

On invocation, the shell scans its own environment and creates a parameter for
each name found, automatically marking it for `export to child processes.`
Executed commands inherit the environment. 

The export and ‘declare -x’ commands allow parameters and functions to be
`added to` and deleted from the environment. If the value of a parameter in
the environment is modified, the new value becomes part of the environment,
replacing the old.
  
`The environment inherited by any executed command` consists of the shell's
initial environment, whose values may be modified in the shell, less any pairs
removed by the unset and ‘export -n’ commands, plus any additions via the
`export` and ‘declare -x’ commands.

The environment for any simple command or function may be augmented temporarily
by prefixing it with parameter assignments, as described in Shell Parameters.
These assignment statements affect only the environment seen by that command.

If the -k option is set (see The Set Builtin), then all parameter assignments
are placed in the environment for a command, not just those that precede the
command name.

When Bash invokes an external command, the variable ‘$_’ is set to the full
pathname of the command and passed to that command in its environment. 

<ex>
When works on some issue, have to set some environment variables before running
some commands which are affected by these settings. This is repetative and
thought it's better to have a script to set these.

#!/bin/sh
export LD_LIBRARY_PATH="/opt/zinc-trunk/lib:${LD_LIBRARY_PATH}"
export LD_PRELOAD=/usr/local/lib/libdirectfb.so:/lib/libpthread.so.0

Great. Now runs commands like this:

$ ./run-settings.sh
$ commands..

However, start to see that the expected shared library are not loaded. Why?
Suspects gdb settings or configuration for not running as expected since it was
an investigation under gdb. 

After all, the reason is that by setting env vars in the shell script and run
commands outside of script then command don't see these settings done in the
script. Use env vars with default settings.

note:
The shell script uses subshell.


3.2.4.3 Grouping Commands

Bash provides two ways to group a list of commands to be executed `as-a-unit`.
When commands are grouped, redirections may be applied to the entire command
list. For example, the output of all the commands in the list may be redirected
to a single stream.

()
( list )

Placing a list of commands between `parentheses` causes a
`subshell-environment` to be created, and each of the commands in list to be
executed `in that subshell` Since the list is executed in a subshell, variable
assignments do not remain in effect after the subshell completes.

<ex>
$ global strlen                 # strlen() is not found
$ (cd /usr/src/lib; gtags)      # library source
$ (cd /usr/src/sys; gtags)      # kernel source


{}
{ list; }

Placing a list of commands between curly braces causes the list to be executed
in the `current-shell` context. No subshell is created. The semicolon (or
    newline) following list is required. 

There is a subtle difference between these two constructs due to historical
reasons. The braces are reserved words, so they `must-be-separated` from the
list by blanks or other shell metacharacters. The parentheses are operators, and
are recognized as separate tokens by the shell even if they are not separated
from the list by whitespace.

The exit status of both of these constructs is the exit status of list. 

<ex>
Must be spaces between {}.

$ { date;time; }
$ { date;time; } > mylog


={============================================================================
*kt_linux_bash_019* sh-pipeline-case-problem

The task is that if see two log lines in order in a log and then it is to flag
it up as error.

Line xx: ... > VRM_JOB_START: jtVRM=3, jhVRM=0x3000385
Line xx: > XTVFS_WriteEx failed,returned value: E_FSSERVER_STATUS_INVALID_ALIGNMENT 290:

Use function and pipe?

scan_for()
{
  while read line
  do
      echo "$line"
  done
}

A=``mycat $1 | egrep -n "(VRM_JOB_START:|XTVFS_WriteEx)" | scan_for``


The $A will have all lines which are echoed from scan_for(). Okay, expand
this.

scan_for()
{
  STARTED=0
  MATCHED=0

  while read line
  do
    case $line in
      *jtVRM=3*)
          if [ $STARTED -ne 1 ]
          then
             STARTED=1;
             STARTLINE=``echo $line``;
          fi
          ;;
       *XTVFS_WriteEx*E_FSSERVER_STATUS_INVALID_ALIGNMENT*)
          if [ $STARTED -eq 1 ]
          then
             MATCHED=1;
             LASTLINE=``echo $line``;

             # note: can see variables as expected
             echo "started = $STARTED, MATCHED=$MATCHED";

          break;
          fi
          ;;
    esac
  done

  # note: however, see variables are '0' WHY?
  echo "started = $STARTED";
  echo "matched = $MATCHED";

  if [[ $STARTED -eq 1 && $MATCHED -eq 1 ]]
  then
      echo "$LASTLINE";
  fi
}


The reason:

From *sh-pipeline-subshell*

`each-command` in a pipeline is executed in its own `subshell`. The exit status
of a pipeline is the exit status of the `last-command` in the pipeline, unless
the `pipefail-option` is enabled (see The Set Builtin). 

See for more details:
http://mywiki.wooledge.org/BashFAQ/024
I set variables in a loop that's in a pipeline. Why do they disappear after the
loop terminates? Or, why can't I pipe data to read?

Workarounds

Use ProcessSubstitution (Bash/Zsh/Ksh93 only). note: OK 
http://mywiki.wooledge.org/ProcessSubstitution


The problem is changes in subshell cannot be seen in parent. How to solve? Use
redirection from subshell.


<xtvfs_write_error2>
##P2
## PicassoShutdownFail: Park, Kyoung-Taek, 09/01/2012
## Check for XTVFS error on a recording

mycat()
(
    F=cat
    echo "$1" | grep -q '.gz$' && F=zcat
    $F $1
)

scan_for()
{
    while read line	
    do
		case $line in
			*jtVRM=*)
				LINEJOB=`echo $line | cut -d':' -f1`;
				LASTJOB=`echo $line | cut -d'=' -f2 | cut -d',' -f1`;
				;;
			*XTVFS_WriteEx*E_FSSERVER_STATUS_INVALID_ALIGNMENT*)
				if [ $LASTJOB -eq 7 ]
				then
					LASTLINE=`echo $line`;
					echo "JOB $LINEJOB, $LASTJOB, $LASTLINE";
					break;
				fi
				;;
		esac
    done
}

if [ "$1" = "-v" ]
then
    echo "XTVFS Write error causing failed recordings in Soak tests - Jira SI-4063" 
    exit 0
fi
A=`mycat $1 | egrep -n "(VRM_JOB_START:|XTVFS_WriteEx)" | scan_for`

if [ "$A" = "" ]
then
echo 0
exit 0
fi

L=`echo $A | cut -d':' -f1`
X=`echo $A | cut -d'.' -f1 | cut -d'^' -f2`
Y=`echo $A | cut -d'>' -f2 | cut -d',' -f1-2`
echo "$L Time:$X, $Y"
echo "XTVFS Write error causing failed recordings in Soak tests - Jira SI-4063"
exit 1


The same in python:

#!/usr/bin/python
import sys, gzip, os
#
# This script is trying to find a situation where player prints a milestone
# like:
# M:player_api_session.c F:PLAYER_API_Session_Play L:3672 > PLAY 
# but no:
# M:player_api_session.c F:PLAYER_PRV_Session_Play L:5652 > PLAY on target MAIN_TARGET 
#
# This two milestones always come in sequence if the second one is missing 
# the thread is probably blocked.
#

def processFile (fileName):
    WAIT_FOR_PLAY = 0
    WAIT_FOR_PLAY_ON_TARGET = 1
    if fileName.endswith('gz'):
        fp = gzip.open(fileName, 'r')
    else:
        fp = open (fileName, 'r')
    status = WAIT_FOR_PLAY
    lineNo = 1
    errorLineNo = 0
    for line in fp:
        if status == WAIT_FOR_PLAY:
            if -1 != line.find ("PLAYER_API_Session_Play"):
                line = line.strip ()
                if line.endswith ("PLAY"):
                    pos1 = line.find ("^")
                    pos2 = line.find ("!")
                    timePlay = float (line [pos1+1: pos2])
                    status = WAIT_FOR_PLAY_ON_TARGET
                    errorLineNo = lineNo
        else:
            if -1 != line.find ("PLAYER_PRV_Session_Play"):
                if -1 != line.find ("PLAY on target"):
                    pos1 = line.find ("^")
                    pos2 = line.find ("!")
                    timePlayOnTarget = float (line [pos1+1: pos2])
                    if (timePlayOnTarget - timePlay > 30.0):
                        break
                    status = WAIT_FOR_PLAY
        lineNo += 1

    if status == WAIT_FOR_PLAY_ON_TARGET:                   
        return errorLineNo 
    else:
        return -1

def processFolder (folder, fileName):
    lineNo = -1
    fileName = os.path.join (folder, fileName)

    if os.path.isfile (fileName):
        lineNo = processFile (fileName)

    return lineNo

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == '-v':
        verbose = True
        fileName=sys.argv[2]
        print 'Player thread blocked NGDEV-34065.'
        sys.exit(0)
    else:
        verbose = False
        fileName=sys.argv[1]

    lineNo = processFolder (".", fileName)
    if lineNo == -1:
        print "0 0"
        sys.exit (0)
    else:
        print str(lineNo) + " Player blocked"
        sys.exit (1)


={============================================================================
*kt_linux_bash_019* sh-pipeline-async

From 3.2.2 Pipelines

If the pipeline is 'not' executed asynchronously (see `list-construct`), the
shell 'waits' for all commands in the pipeline to complete.

This means that each command in the pipeline runs `asynchronously`. 

http://blog.dataart.com/linux-pipes-tips-tricks/

<ex>
# telnet to a box and run commands

set -x

ip=10.209.56.13

ping -c 2 $ip &&
(
sleep 5
echo "root"
sleep 3
echo "mkdir /mnt/tmp"
echo "mount -o rw -t nfs 10.209.62.106:/home/kyoupark/ /mnt/tmp"
echo "ls /mnt/nds/dev_6/part_0 > /mnt/tmp/_LIST_OUT_"
echo "ls /mnt/tmp/_LIST_OUT_"
echo "chmod 777 /mnt/tmp/_LIST_OUT_"
echo "umount /mnt/tmp"
echo "rmdir /mnt/tmp"
echo exit
) | telnet $ip

However, the above do not work and need `wait` like:

set -x

ip=10.209.56.13

ping -c 2 $ip &&
(
sleep 5
echo "root"
sleep 3
echo "mkdir /mnt/tmp"
echo "mount -o rw -t nfs 10.209.62.106:/home/kyoupark/ /mnt/tmp"
echo "ls /mnt/nds/dev_6/part_0 > /mnt/tmp/_LIST_OUT_"
echo "ls /mnt/tmp/_LIST_OUT_"
echo "chmod 777 /mnt/tmp/_LIST_OUT_"
echo "umount /mnt/tmp"
echo "rmdir /mnt/tmp"
sleep 5                   # needs to wait
echo exit
) | telnet $ip

the wait which uses count and check on file that can be made as flag and that
file can be made when commend ends.

(
echo "commands..."
echo "touch $DIR/done"
AA=0
AALIMIT=50
if [ $SB -eq 1 ]
then
AALIMIT=1000000
fi
while [ ! -f $DIR/done -a $AA -lt $AALIMIT ]
do
let AA+=1
sleep 5
echo "Waiting $AA" >&2
done
)


kyoupark@st-castor-03:~$ sh pipe-run.sh
+ ip=10.209.56.13
+ ping -c 2 10.209.56.13
PING 10.209.56.13 (10.209.56.13) 56(84) bytes of data.
64 bytes from 10.209.56.13: icmp_seq=1 ttl=63 time=0.532 ms
64 bytes from 10.209.56.13: icmp_seq=2 ttl=63 time=0.450 ms

--- 10.209.56.13 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.450/0.491/0.532/0.041 ms
+ sleep 5
+ telnet 10.209.56.13
Trying 10.209.56.13...
Connected to 10.209.56.13.
Escape character is '^]'.
drx890 login: + echo root
+ sleep 3
root
-sh-3.2# + echo 'mkdir /mnt/tmp'
+ echo 'mount -o rw -t nfs 10.209.62.106:/home/kyoupark/ /mnt/tmp'
+ echo 'ls /mnt/nds/dev_6/part_0 > /mnt/tmp/_LIST_OUT_'
+ echo 'ls /mnt/tmp/_LIST_OUT_'
+ echo 'chmod 777 /mnt/tmp/_LIST_OUT_'
+ echo 'umount /mnt/tmp'
+ echo 'rmdir /mnt/tmp'
+ sleep 5
mkdir /mnt/tmp
chmod 777 /mnt/tmp/_LIST_OUT_
umount /mnt/tmp
rmdir /mnt/tmp
-sh-3.2# mount -o rw -t nfs 10.209.62.106:/home/kyoupark/ /mnt/tmp
-sh-3.2# ls /mnt/nds/dev_6/part_0 > /mnt/tmp/_LIST_OUT_
-sh-3.2# ls /mnt/tmp/_LIST_OUT_
/mnt/tmp/_LIST_OUT_
-sh-3.2# chmod 777 /mnt/tmp/_LIST_OUT_
-sh-3.2# umount /mnt/tmp
-sh-3.2# rmdir /mnt/tmp
-sh-3.2# + echo exit
Connection closed by foreign host.
kyoupark@st-castor-03:~$


Also, the following do not work.

ping -c 2 $ip && telnet $ip |
(
)

So, subshell from () echos commands which piped into telnet and it's timing
dependant.

ping -c 2 $ip && 
(
echo "commands.."
) | telnet $ip


={============================================================================
*kt_linux_bash_019* sh-list-construct

3.2.3 Lists of Commands

A list is a sequence of one or more `pipelines` separated by one of the
operators ;, &, &&, or ||, and optionally terminated by one of ;, &, or a
`newline`. 

note: means do on multiple pipelines

<semicolon>

   A sequence of one or more newlines may appear in a 'list' to delimit
   commands, equivalent to a semicolon.

Commands separated by a ; are executed 'sequentially'; the shell waits for each
command to terminate in turn. The return status is the exit status of the last
command executed. 

If a command is terminated by the control operator ‘&’, the shell executes the
command `asynchronously` in a subshell. This is known as executing the command
in the `background`. The shell does not wait for the command to finish, and the
return status is 0 (true). 

// When job control is not active (see Job Control), the standard input for
// asynchronous commands, in the absence of any explicit redirections, is
// redirected from /dev/null. 


<and-and-or>
AND and OR lists are sequences of one or more pipelines separated by the control
operators ‘&&’ and ‘||’, respectively. AND and OR lists are executed with left
associativity.

An AND list has the form

command1 && command2

command2 is executed if, and only if, command1 returns an exit status of `zero`.

An OR list has the form

command1 || command2

command2 is executed if, and only if, command1 returns a non-zero exit status.

The return status of AND and OR lists is the exit status of the last command
executed in the list. 

<ex>
if [ "$(whoami)" != 'root' ]; then
   echo "You have no permission to run $0 as non-root user."
   exit 1;
fi

With Bash, you can shorten this type of construct. The compact equivalent of the
above test is as follows:

[ "$(whoami)" != 'root' ] && ( echo you are using a non-privileged account; exit 1 )

Similar to the "&&" expression which indicates what to do if the test proves
true, "||" specifies what to do if the test is false.

<ex>
* Check if loginscript is not set. If it's set(defined) then run -f. 
  * if file exist then test return true so no fail() call.
  * if file not exist then test return false so run fail() call.
* If loginscript is not set, no run -f and test return true(0) so no fail() call.

[[ -z "$loginscript" || -f "$loginscript" ]] ||
    fail "Loginscript '$loginscript' doesn't exist."

Progress to next cond when only cond1 is true. [[ cond1 || cond2 ]] || exec

<ex>
To see if meet at least one of options.

[[ -n "$release" || -n "$zincdir" || -n "$jenkinshost" || -n "$jenkinsurl" ||
   -n "$galliumurl" || -n "$localgallium" || -n "$platformconfigurl" ||
   "$virtualinputdriver" -ne 0 || -n "$irdriver" || "$enabledbustcp" -ne 0 ||
   "$directfbsrc" -ne 0 || "$killzincdaemons" -ne 0 || -n "$toolnames" ||
   -n "$networkmanager" || -n "$loginscript" || -n "$startupscript" ||
   "$forcefirsttime" -ne 0 || "$needsreboot" -ne 0 || "$vidmemcapture" -ne 0 ]] ||
{
    fail "You must specify at least one of -[ZzjugcdeEtsknlbTfrv]."
}

<ex>
function x {
  ...

  [[ 
    "$cacheOptionCorrectAtExpectedPosition" = 1 &&
    "$cacheValueCorrectAtExpectedPosition" = 1 &&
    "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
    "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
    "$jarOptionCorrectAtExpectedPosition" = "1" && 
    "$jarValueCorrectAtExpectedPosition" = "1" && 
    "$urlOptionCorrectAtExpectedPosition" = "1" && 
    "$urlValueCorrectAtExpectedPosition" = "1" 
  ]] 
}

From the debug output, shows only 4 since stops as soon as see flase. This may
be confusing when debugging.

+ [[ 1 = 1 ]]
+ [[ 1 = 1 ]]
+ [[ 1 = 1 ]]
+ [[ 0 = 1 ]]

<ex>
The lsr-config returns "0/1" or "true/false" when use --bool and the problem is
when a key is boolean and is not set in a database meaning 0, it still returns
"0" which is not NULL string. So "if [ -z ]" check always becomes false.

The solution is that use variable's value depending on key value to set actual
variable.

#!/bin/sh

# when key is not set or returns "false" then run "echo false" and ends here.
# enable_yv_media="".
#
# when key set and returns "true" then enable_yv_media="true" since lsr-config
# returns zero exit status.

enable_yv_media=""
[ "false" = "$(lsr-config --bool platform.settings.enable-yv-media || echo false)" ] || {
    enable_yv_media="true"
}

USE_YV_MEDIA_ROUTER="$enable_yv_media"

if [ -z "$USE_YV_MEDIA_ROUTER" ]; then
	echo "NOT SET. $USE_YV_MEDIA_ROUTER. WILL USE OEM MR"
else
	echo "SET. $USE_YV_MEDIA_ROUTER. WILL USE YV MR"
fi

However, if use this then goes wrong.

if [ "false" = "$(lsr-config --bool platform.settings.webkit-use-media-router || echo false)" ]; then
    // things to do when true case


={============================================================================
*kt_linux_bash_021* sh-script

3.8 Shell Scripts

A shell script is a text file containing shell commands. When such a file is
used as the first non-option argument when invoking Bash, and neither the -c nor
-s option is supplied (see Invoking Bash), Bash reads and executes commands from
the file, then exits. This mode of operation creates a `non-interactive` shell.

The shell first searches for the file in the current directory, and looks in the
directories in $PATH if not found there.

When Bash runs a shell script, it sets the special parameter 0 to the name of
the file, rather than the name of the shell, and the positional parameters are
set to the remaining arguments, if any are given. If no additional arguments are
supplied, the positional parameters are unset.

A shell script may be `made-executable` by using the chmod command to turn on
the execute bit. When Bash finds such a file while searching the $PATH for a
command, it spawns a `subshell` to execute it. In other words, executing

filename arguments

is equivalent to executing

bash filename arguments

if filename is an executable shell script. This subshell reinitializes itself,
   so that the effect is as if a new shell had been invoked to interpret the
   script, with the exception that the locations of commands remembered by the
   parent (see the description of hash in Bourne Shell Builtins) are retained by
   the child.

Most versions of Unix make this a part of the operating system’s command
execution mechanism. If the first line of a script begins with the two
characters ‘#!’, the remainder of the line specifies an interpreter for the
program. Thus, you can specify Bash, awk, Perl, or some other interpreter and
write the rest of the script file in that language.

The arguments to the interpreter consist of a single optional argument following
the interpreter name on the first line of the script file, followed by the name
of the script file, followed by the rest of the arguments. Bash will perform
this action on operating systems that do not handle it themselves. Note that
some older versions of Unix limit the interpreter name and argument to a maximum
of 32 characters.

Bash scripts often begin with #! /bin/bash (assuming that Bash has been
    installed in /bin), since this ensures that Bash will be used to interpret
the script, even if it is executed under another shell. 


={============================================================================
*kt_linux_bash_021* sh-exit-status

3.2.1 Simple Commands

The return status (see Exit Status) of a simple command is its exit status as
provided by the POSIX 1003.1 waitpid function, or 128+n if the command was
terminated by signal n. 

*sh-exit-status*
The $? expands to the exit status of the 'most' recently executed foreground
pipeline. Can be used in test construct.


3.7.5 Exit Status

The exit status of an executed command is the value returned by the waitpid
system call or equivalent function. Exit statuses fall between 0 and 255,
though, as explained below, the shell may use values above 125 specially. 
  
For the shell's purposes, a command which exits with a `zero-exit-status` has
`succeeded`. A non-zero exit status indicates failure. This seemingly
counter-intuitive scheme is used so there is one well-defined way to indicate
success and a variety of ways to indicate various failure modes. When a command
terminates on a fatal signal whose number is N, Bash uses the value 128+N as the
exit status.

If a command is not found, the child process created to execute it returns a
status of 127. If a command is found but is not executable, the return status is
126.

If a command fails because of an error during expansion or redirection, the exit
status is greater than zero.

All of the Bash builtins return an exit status of zero if they succeed and a
non-zero status on failure, so they may be used by the conditional and list
constructs. All builtins return an exit status of 2 to indicate incorrect usage. 

<ex>
if which ssh-copy-id >/dev/null;
then  
  # ssh-copy-id always returns failure
  ssh-copy-id -i $privatekey root@$stbip || true
fi

<ex> tool-grep
The grep returns 0(success) when found and 1(fail) when not found.

$ if grep KB readme; then echo "found"; fi
readme:1:this is KB for keitee.
found
$ 

$ if ! grep KB readme; then echo "found"; fi
readme:1:this is KB for keitee.
$ 

$ if true; then echo "found"; fi
found

// when not found the string
if ! ( grep -qi "NDS_BUILD_TYPE_$1 = debug" ${CWD}/projects/$PROJECT/build_options.mk); then


<ex>
while kill -0 "${parent_pid}" &>/dev/null;
do
   usleep 500
done

<ex>
#include <stdio.h>
#include <stdlib.h>

int main(int argc, char** argv)
{
  printf(" this is a main function.\n");

  if( 2 == argc )
    exit(EXIT_FAILURE);

  exit(EXIT_SUCCESS);
}

$ ./a.out && echo "return success"
 this is a main function.
return success
$ 

$ echo $?
0

$ ./a.out xx && echo "return success"
 this is a main function.
$ 

$ echo $?
1


<ex>

$ rm xx.out
rm: cannot remove ‘xx.out’: No such file or directory
$ rm xx.out && echo "return success"
rm: cannot remove ‘xx.out’: No such file or directory
$ echo $?
1
$ rm -f xx.out
$ echo $?
0
$ rm -f xx.out && echo "return success"                                                                                                                                                                                                                                          
return success

*tool-rm-f*
       -f, --force
              ignore nonexistent files and arguments, never prompt


<ex>
Stash away the output of program and only use its exit status to see if it has
found a element in the system.

$ gst-inspect-1.0 nexussink &>/dev/null && echo "true" || echo "false"
true
$ gst-inspect-1.0 xx &>/dev/null && echo "true" || echo "false"
false
$   


<ref>
http://bencane.com/2014/09/02/understanding-exit-codes-and-how-to-use-them-in-bash-scripts/

What are exit codes?

On Unix and Linux systems, programs can pass a value to their parent process
while terminating. This value is referred to as an exit code or exit status. On
POSIX systems the standard convention is for the program to pass 0 for
successful executions and 1 or higher for failed executions.

Why is this important? 

If you look at exit codes in the context of scripts written to be used for the
command line the answer is very simple. Any script that is useful in some
fashion will inevitably be either used in another script, or wrapped with a bash
one liner. 
     
This becomes especially true to check the status code to determine whether that
script was successful or not.

On top of those reasons, exit codes exist within your scripts even if you don't
define them. By not defining proper exit codes you could be falsely reporting
successful executions which can cause issues depending on what the script does.

What happens if I don't specify an exit code

In Linux any script run from the command line has an exit code. With Bash
scripts, if the exit code is not specified in the script itself the exit code
used will be the exit code of the 'last' command run. To help explain exit codes
a little better we are going to use a quick sample script.

Sample Script:

#!/bin/bash
touch /root/test
echo created file

The above sample script will execute both the touch command and the echo
command. When we execute this script (as a non-root user) the touch command will
fail, ideally since the touch command failed we would want the exit code of the
script to indicate failure with an appropriate exit code. To check the exit code
we can simply print the $? special variable in bash. This variable will print
the exit code of the last run command.

Execution:

$ ./tmp.sh 
touch: cannot touch '/root/test': Permission denied
created file
$ echo $?
0

As you can see after running the ./tmp.sh command the exit code was 0 which
indicates success, even though the touch command failed. The sample script runs
two commands touch and echo, since we did not specify an exit code the script
exits with the exit code of the last run command. In this case, the last run
command is the echo command, which did execute successfully.

Script:

#!/bin/bash
touch /root/test

If we remove the echo command from the script we should see the exit code of the
touch command.

Execution:

$ ./tmp.sh 
touch: cannot touch '/root/test': Permission denied
$ echo $?
1

As you can see, since the last command run was touch the exit code reflects the
true status of the script; failed.  Using exit codes in your bash scripts

While removing the echo command from our sample script worked to provide an exit
code, what happens when we want to perform one action if the touch was
successful and another if it was not. Actions such as printing to stdout on
success and stderr on failure.

Testing for exit codes

Earlier we used the $? special variable to print the exit code of the script. We
can also use this variable within our script to test if the touch command was
successful or not.

Script:

#!/bin/bash

touch /root/test 2> /dev/null

if [ $? -eq 0 ]
then
  echo "Successfully created file"
else
  echo "Could not create file" >&2
fi

In the above revision of our sample script; if the exit code for touch is 0 the
script will echo a successful message. If the exit code is anything other than 0
this indicates failure and the script will echo a failure message to stderr.

Execution:

$ ./tmp.sh
Could not create file

Providing your own exit code

While the above revision will provide an error message if the touch command
fails, it still provides a 0 exit code indicating success.

$ ./tmp.sh
Could not create file
$ echo $?
0

Since the script failed, it would not be a good idea to pass a successful exit
code to any other program executing this script. To add our own exit code to
this script, we can simply use the exit command.

Script:

#!/bin/bash

touch /root/test 2> /dev/null

if [ $? -eq 0 ]
then
  echo "Successfully created file"
  exit 0
else
  echo "Could not create file" >&2
  exit 1
fi

With the exit command in this script, we will exit with a successful message and
0 exit code if the touch command is successful. If the touch command fails
however, we will print a failure message to stderr and exit with a 1 value which
indicates failure.

Execution:

$ ./tmp.sh
Could not create file
$ echo $?
1

Using exit codes on the command line

Now that our script is able to tell both users and programs whether it finished
successfully or unsuccessfully we can use this script with other administration
tools or simply use it with bash one liners.

Bash One Liner:

$ ./tmp.sh && echo "bam" || (sudo ./tmp.sh && echo "bam" || echo "fail")
Could not create file
Successfully created file
bam

The above grouping of commands use what is called list constructs in bash. List
constructs allow you to chain commands together with simple && for and and ||
for or conditions. The above command will execute the ./tmp.sh script, and if
the exit code is 0 the command echo "bam" will be executed. If the exit code of
./tmp.sh is 1 however, the commands within the parenthesis will be executed
next. Within the parenthesis the commands are chained together using the && and
|| constructs again.

The list constructs use exit codes to understand whether a command has
successfully executed or not. If scripts do not properly use exit codes, any
user of those scripts who use more advanced commands such as list constructs
will get unexpected results on failures.

More exit codes

The exit command in bash accepts integers from 0 - 255, in most cases 0 and 1
will suffice however there are other reserved exit codes that can be used for
more specific errors. The Linux Documentation Project has a pretty good table of
reserved exit codes and what they are used for.


<case>
After migrates to the new linux distribution, the interactive shell script fails
to run. The thing was that it reads .bashrc but fails to contiune.

# from ~/.bashc

# enable programmable completion features (you don't need to enable
# this, if it's already enabled in /etc/bash.bashrc and /etc/profile
# sources /etc/bash.bashrc).
if ! shopt -oq posix; then
  if [ -f /usr/share/bash-completion/bash_completion ]; then
    . /usr/share/bash-completion/bash_completion
  elif [ -f /etc/bash_completion ]; then
    . /etc/bash_completion
  fi
fi

# /usr/share/bash-completion/bash_completion

# This function sets correct SysV init directories
#
_sysvdirs()
{
    sysvdirs=( )
    [[ -d /etc/rc.d/init.d ]] && sysvdirs+=( /etc/rc.d/init.d )
    [[ -d /etc/init.d ]] && sysvdirs+=( /etc/init.d )
    # Slackware uses /etc/rc.d
    [[ -f /etc/slackware-version ]] && sysvdirs=( /etc/rc.d )
}

However, /etc/slackware-version do not exist and this scripts quit since the
parent script uses "-ie" option.

To see how this happen:

#!/bin/bash -i

echo "call file test"

# this file does not exist and so exit code is 1.
# [[ -f /etc/slackware-version ]]

# this file does exist and so exit code is 0.
[[ -f /etc/fstab ]]

echo "exit code after call file test: $?"


={============================================================================
*kt_linux_bash_021* sh-return-value-from-script

# a.sh
  echo $LOCATION
  exit 0

# b.sh
    DIR=``a.sh -dir ${OTHER}.tgz``

DIR will have a return from a.sh which uses echo.


={============================================================================
*kt_linux_bash_001* sh-looping-construct sh-compound-command

3.2.4 Compound Commands

* Looping Constructs:     Shell commands for iterative action.
* Conditional Constructs: Shell commands for conditional execution.
* Command Grouping:       Ways to group commands.

`compound-commands` are the shell programming constructs. Each construct begins
with a reserved word or control operator and is terminated by a corresponding
reserved word or operator. 

*sh-redirection-on-compound-commands*
Any redirections associated with a compound command apply to all commands within
that compound command unless explicitly overridden.

*sh-semicolon-newline*
In most cases a list of commands in a compound command's description may be
separated from the rest of the command by one or more `newlines`, and may be
followed by a newline `in-place-of` a semicolon.

<ex>
for item in ${list[@]}; do echo ${item}; pushd ${item}; \
  egrep -anH "Leak de|Could not unravel" LOGlastrun_realtime;popd; done


Bash provides looping constructs, conditional commands, and mechanisms to group
commands and execute them as a unit. 


{sh-for} tool-seq
for

    The syntax of the for command is:

    for `name` [ [in [`words` ...] ] ; ] do `commands`; done

    Expand `words`, and execute commands once for each member in the resultant
    list, with name bound to the current member. 
    
    `If in words is not present`, the for command executes the commands once
    for each positional parameter that is set, as if ‘in "$@"’ had been
    specified (see Special Parameters).  The return status is the exit status
    of the last command that executes. 
    
    If there are no items in the expansion of `words`, no commands are
    executed, and the return status is zero.

    An alternate form of the for command is also supported:

    for (( expr1 ; expr2 ; expr3 )) ; do commands ; done

    First, the arithmetic expression expr1 is evaluated according to the rules
    described below (see Shell Arithmetic). The arithmetic expression expr2 is
    then evaluated repeatedly until it evaluates to zero. Each time expr2
    evaluates to a non-zero value, commands are executed and the arithmetic
    expression expr3 is evaluated. If any expression is omitted, it behaves as
    if it evaluates to 1. The return value is the exit status of the last
    command in commands that is executed, or false if any of the expressions is
    invalid. 

<ex>
echo "==================="
echo "use list "

for count in 0 1 2 3
do
  echo "looping.. $count"
  filename=S$count
  echo "filename: $filename.."
done

echo "==================="
echo "use seq "

for count in $(seq 4)
do
  echo "looping.. $count"
  filename=S$count
  echo "filename: $filename.."
done

echo "==================="
echo "use arithmetic "

COUNT=0

while [ $COUNT -lt 4 ]
do
  echo "looping.. $COUNT"
  filename=S$COUNT
  echo "filename: $filename.."
  let COUNT=COUNT+1
done

<ex>
note:
'no' need to specify index and to shift.

# use `newline` as a seperator
for name in word1 word2 ... wordN
do
   list
done

# use `semicolon` as a seperator 
for f in $( ls /var/ ); do
   echo $f
done

<ex-oneline>
# use `semicolon` as a seperator and in `command-line` 
for f in $(ls); do echo "var is: $f"; done

<ex>
declare -ar list=(
    darwin_487032cd821312aea3b11274c2ff7384
    darwin_c4130c208206b80b3963ee05c861b5c3
    darwin_d23afbe329e75a1d9bcabc233373d398
)

<ex>
for FILE in $HOME/.bash*
do
   cp $FILE ${HOME}/public_html
   chmod a+r ${HOME}/public_html/${FILE}
done

<ex>
for ac_option
do

+ for ac_option in '"$@"'


{sh-while}
while

    The syntax of the while command is:

    while test-commands; do consequent-commands; done

    Execute consequent-commands as long as test-commands has an
    `zero-exit-status`. The return status is the exit status of the last command
    executed in consequent-commands, or zero if none was executed.

<ex>
while command
do
   list
done

<ex>
while :     # this cause infinite loop. see *sh-colon*
do
   read CMD
   case $CMD in
      [qQ]|[qQ][uU][iI][tT]) break ;;
   *) process $CMD ;;
   esac
done

<ex>
#/bim/bash

COUNT=0

while :
do
  echo ""
  echo "Looping $COUNT..."
  echo ""
  ./ccon_ex_out
  let COUNT=COUNT+1
done


{break-continue}
The `break` and `continue` builtins (see Bourne Shell Builtins) may be used to
control loop execution. 


={============================================================================
*kt_linux_bash_001* sh-looping-construct-getopt sh-parse-arg

<ex>
note: can use true only in the script but not command line.

read case;
case $case in
    1) echo "You selected bash";;
    2) echo "You selected perl";;
    3) echo "You selected phyton";;
    4) echo "You selected c++";;
    5) exit
esac


while [ true ] ; do ls -l; echo Wait 1 sec ; sleep 1; done

while [ "$1" != "" ]; do
  case $1 in
    -h | -help | --help)
      usage
      exit
      ;;
    -p | --platform | -b | --box)
      shift
      PLATFORM=$1
      ;;
    --project)
      shift
      PROJECT=$1
      ;;
    release)
      RELEASE="true"
      ;;
    *)
      echo "USAGE ERROR"
      usage
      exit
      ;;
  esac
  shift
done

<ex>
This code breaks up when use "--h" because (*) let it contine to run after
while. So should use exit 0 in case (*).

while [ $# -gt 0 ]; do
    case "$1" in
        (-h | -help | --help)
            grep '^#/' "$0" | cut -c 4-
            exit 0;;

        (-d | --debug)
            debug="1"
            shift;;

        (-l | --list)
            list_assets
            exit 0;;

        (*)
            break;;
    esac
done

<ex>
# Parse command line arguments.
parse_args "$@"

# Parse command line arguments.
function parse_args
{
    while [ $# -gt 0 ]
    do
        case "$1" in
            -m | --maximum)
                shift
                MAXIMUM_NUMBER_OF_UPLOADS_TO_FIND=$1
                if [ $MAXIMUM_NUMBER_OF_UPLOADS_TO_FIND -lt 1 ]
                then
                    echo "'$MAXIMUM_NUMBER_OF_UPLOADS_TO_FIND' isn't large enough.  Please enter a value greater than 0."
                    echo ""
                    exit 1
                fi
                ;;
            -l | --list)
                shift
                MAC_ADDRESS_LIST_FILE=$1
                if [ ! -e $MAC_ADDRESS_LIST_FILE ]
                then
                    echo "The MAC address list file '$MAC_ADDRESS_LIST_FILE' cannot be found."
                    echo ""
                    exit 1
                fi
                MAC_ADDRESS_LIST=$(cat $MAC_ADDRESS_LIST_FILE)
                NUMBER_OF_MAC_ADDRESSES=$(cat $MAC_ADDRESS_LIST_FILE | wc -l)
                ;;
            -v | --version)
                shift
                VERSION=$1
                ;;
            -t | --test)
                shift
                TEST_IDS=$1
                ;;
            -h | --help)
                usage
                exit 0
                ;;
            *)
                # Bail out if we hit something unexpected.
                echo "'$1' is not a valid parameter.  Please use the -h or --help parameter for more information."
                echo ""
                exit 1
                ;;
        esac
        shift
    done

    # Check all mandatory parameters are set.
    if [ -z "$MAC_ADDRESS_LIST_FILE" ]
    then
        usage
        echo "The mandatory -l/--list parameter was not provided."
        echo ""
        exit 1
    fi
}


{getopts} shell built-in

getopts optstring name [args]

getopts is used by shell scripts to parse positional parameters. 'optstring'
contains the option characters to be recognized; if a character is followed by a
colon, the option is expected to have an argument, which should be separated
from it by whitespace. 

<option>
Each time it is invoked, getopts 'places' the next option in the shell variable
'name', initializing name if it does not exist, and the 'index' of the next
argument to be processed into the variable 'OPTIND'. OPTIND is initialized to 1
each time the shell or a shell script is invoked. 

<arg>
When an option requires an argument, getopts places that 'argument' into the
variable OPTARG. 

<return>
When the end of options is encountered, getopts exits with a return value
greater than zero. note: false. OPTIND is set to the index of the first
'non'-option argument, and name is set to '?'.


={============================================================================
*kt_linux_bash_001* sh-conditional-construct

3.2.4.2 Conditional Constructs

{sh-if}
if

    The syntax of the if command is:

    if test-commands; then
      consequent-commands;
    [elif more-test-commands; then
      more-consequents;]
    [else alternate-consequents;]
    fi

    The `test-commands` list is executed, and if its return status is zero,
    the consequent-commands list is executed. If test-commands returns a
    non-zero status, each elif list is executed in turn, and if its exit
    status is zero, the corresponding more-consequents is executed and the
    command completes.


{sh-case}
case

    The syntax of the case command is:

    case word in [ [(] pattern [| pattern]...) command-list ;;]... esac

    case will selectively execute the command-list corresponding to the first
    `pattern` that matches word. If the shell option nocasematch (see the
        description of shopt in The Shopt Builtin) is enabled, the match is
    performed without regard to the case of alphabetic characters. The ‘|’ is
    used to separate multiple patterns, and `the ‘)’ operator terminates` a
    pattern list. A list of patterns and an associated command-list is known as
    a `clause`.

    Each clause `must-be-terminated` with ‘;;’, ‘;&’, or ‘;;&’. The word
    undergoes tilde expansion, parameter expansion, command substitution,
    arithmetic expansion, and quote removal before matching is attempted. Each
    pattern undergoes tilde expansion, parameter expansion, command
    substitution, and arithmetic expansion.

    There may be an arbitrary number of case clauses, each terminated by a ‘;;’,
    ‘;&’, or ‘;;&’. The first pattern that matches determines the command-list
    that is executed. It’s a common idiom to use ‘*’ as the `final-pattern` to
    define the default case, since that pattern will always match.

    Here is an example using case in a script that could be used to describe one
    interesting feature of an animal:

    echo -n "Enter the name of an animal: "
    read ANIMAL
    echo -n "The $ANIMAL has "
    case $ANIMAL in
      horse | dog | cat) echo -n "four";;
      man | kangaroo ) echo -n "two";;
      *) echo -n "an unknown number of";;
    esac
    echo " legs."

    If the ‘;;’ operator is used, no subsequent matches are attempted after the
    first pattern match. Using ‘;&’ in place of ‘;;’ causes execution to
    continue with the command-list associated with the next clause, if any.
    Using ‘;;&’ in place of ‘;;’ causes the shell to test the patterns in the
    next clause, if any, and execute any associated command-list on a successful
    match.

    The return status is zero if no pattern is matched. Otherwise, the return
    status is the exit status of the command-list executed.


{sh-select}

select

    The select construct allows the easy generation of `menus`. It has almost the
    same syntax as the for command:

    select name [in words ...]; do commands; done

    The list of words following in is expanded, generating a list of items. The
    set of expanded words is printed on the standard error output stream, each
    preceded by a number. If the ‘in words’ is omitted, the positional
    parameters are printed, as if ‘in "$@"’ had been specified. The PS3 prompt
    is then displayed and a line is read from the standard input. If the line
    consists of a number corresponding to one of the displayed words, then the
    value of name is set to that word. If the line is empty, the words and
    prompt are displayed again. If EOF is read, the select command completes.
    Any other value read causes name to be set to null. The line read is saved
    in the variable REPLY.

    The commands are executed after each selection until a break command is
    executed, at which point the select command completes.

    Here is an example that allows the user to pick a filename from the current
    directory, and displays the name and index of the file selected.

    select fname in *;
    do
    	echo you picked $fname \($REPLY\)
    	break;
    done

<ex>
select COM in comp1 comp2 comp3 all none
do
   echo "is in do"
done

$ ./sample.sh  
1) comp1
2) comp2
3) comp3
4) all
5) none
#?

Automatically make numbers and prompts. COM var will have whatever value
entered. Also this example do infinite loop until press C-c since no break
statement in do.

<ex>
select COM in comp1 comp2 comp3 all none
do
  case $COM in
      comp1 | comp2 | comp3) echo "sel is $COM" ;;
      all) echo "sel is $COM" ;;
      none)
          echo "sel is $COM"
          break
           ;;
       *) echo "ERROR. sel is $REPLY" ;;
   esac
done

You can change the prompt displayed by the select loop by altering the variable
PS3. If PS3 is not set, the default prompt, #?, is displayed. Otherwise the
value of PS3 is used as the prompt to display. For example, the commands

$ PS3="Please make a selection => " ; export PS3

note: All loops has 'do .. done' block and careful the place of 'do'.


{sh-[[} *sh-[[*
Use [] whenever you want your script to be 'portable' across shells. Use [[]] if
you want `conditional-expressions` not supported by [] and don't need to be
portable.

[[ is bash's improvement to the [ command. It has several enhancements that make
it a better choice if you write scripts that target bash. My favorites are:

1. It is a syntactical feature of the shell, so it has some special behavior
that [ doesn't have. You `no longer have to quote` variables because [[ handles
empty strings and strings with whitespace more intuitively. 
For example, with [ you have to write

    if [ -f "$FILE" ]

to correctly handle empty strings or file names with spaces in them. With [[ the
quotes are unnecessary:

    if [[ -f $FILE ]]

2. Because it is a syntactical feature, it lets you use && and || operators for
boolean tests and < and > for string comparisons. [ cannot do this because it is
a regular command and > &&, ||, <, and > are not passed to regular commands as
command-line arguments.

3. It has a wonderful =~ operator for doing regular expression matches. With [
  you might write

    if [ "$ANSWER" = y -o "$ANSWER" = yes ]

    With [[ you can write this as

    if [[ $ANSWER =~ ^y(es)?$ ]]

It even lets you access the captured groups which it stores in BASH_REMATCH. For
instance, ${BASH_REMATCH[1]} would be "es" if you typed a full "yes" above.

4. You get pattern matching aka globbing for free. Maybe you're less strict
about how to type yes.  Maybe you're okay if the user types y-anything. Got you
covered:

    if [[ $ANSWER = y* ]]

Keep in mind that it is a `bash-extension`, so if you are writing sh-compatible
scripts then you need to stick with [. Make sure you have the #!/bin/bash
`shebang` line for your script if you use double brackets.


[]

4.1 Bourne Shell Builtins

note: NO pattern and expansion support

<sh-test>
test
[

    test expr

    Evaluate a `conditional-expression` expr and return a status of 0 (true) or
    1 (false). Each operator and operand must be a separate argument.
    Expressions are composed of the `primaries`. test does not accept any
    options, nor does it accept and ignore an argument of -- as signifying the
    end of options.

    When the [ form is used, the last argument to the command must be a ].

    Expressions may be combined using the following operators, listed in
    decreasing order of precedence. The evaluation depends on the number of
    arguments; see below. Operator precedence is used when there are five or
    more arguments. 


[[...]]

    [[ expression ]]

    Return a status of 0 or 1 depending on the evaluation of the
    `conditional-expression` expression. Expressions are composed of the
    `primaries`. 
    
    note: this is enhancement?

    Word splitting and filename expansion are not performed on the words between
    the [[ and ]]; tilde expansion, parameter and variable expansion, arithmetic
    expansion, command substitution, process substitution, and quote removal are
    performed.  Conditional operators such as ‘-f’ must be unquoted to be
    recognized as primaries.

    When used with [[, the ‘<’ and ‘>’ operators sort lexicographically using
    the current locale.

    When the ‘==’ and ‘!=’ operators are used, the string to the right of the
    operator is considered a `pattern` and matched according to the rules
    described below in `pattern-matching`, as if the extglob shell option were
    enabled. 
    
    note: The ‘=’ operator is identical to ‘==’. 
    
    If the shell option `nocasematch` is enabled, the match is performed without
    regard to the case of alphabetic characters. 
    
    The return value is 0 if the string matches (‘==’) or does not match
    (‘!=’)the pattern, and 1 otherwise.

    Any part of the pattern may be quoted to force the quoted portion to be
    matched as a string.

    *sh-regex*
    An additional binary operator, ‘=~’, is available, with the same precedence
    as ‘==’ and ‘!=’. When it is used, the string to the right of the operator
    is considered an `extended-regular-expression` and matched accordingly (as
        in regex3)). 

    The return value is 0 if the string matches the pattern, and 1 otherwise. 
    
    If the regular expression is syntactically incorrect, the conditional
    expression’s return value is 2. If the shell option `nocasematch` (see the
        description of shopt in The Shopt Builtin) is enabled, the match is
    performed without regard to the case of alphabetic characters. Any part of
    the pattern may be quoted to force the quoted portion to be matched as a
    string. 
    
    `bracket-expressions` in regular expressions must be treated carefully,
    since normal quoting characters lose their meanings between brackets. If the
    pattern is stored in a shell variable, quoting the variable expansion forces
    the entire pattern to be matched as a string. Substrings matched by
    parenthesized subexpressions within the regular expression are saved in the
    array variable BASH_REMATCH. The element of BASH_REMATCH with index 0 is the
    portion of the string matching the entire regular expression.  The element
    of BASH_REMATCH with index n is the portion of the string matching the nth
    parenthesized subexpression.

    For example, the following will match a line (stored in the shell variable
        `line`) if there is a sequence of characters in the value consisting of
    any number, including zero, of space characters, zero or one instances of
    ‘a’, then a ‘b’:

    [[ $line =~ [[:space:]]*(a)?b ]]
    
    // [[:space:]]*, (a)? and b

    That means values like ‘aab’ and ‘ aaaaaab’ will match, as will a line
    containing a ‘b’ anywhere in its value. 

note: can use sh-expr

note: These are specific to [[ operator.

Expressions may be combined using the following operators, listed in decreasing
order of precedence:

( expression )

    Returns the value of expression. This may be used to override the normal
    precedence of operators.

! expression

    True if expression is false.

expression1 && expression2

    True if both expression1 and expression2 are true.

expression1 || expression2

    True if either expression1 or expression2 is true. 

The && and || operators do not evaluate expression2 if the value of expression1
is sufficient to determine the return value of the entire conditional
expression. 


={============================================================================
*kt_linux_bash_001* sh-conditional-expression

https://www.gnu.org/software/bash/manual/html_node/Bash-Conditional-Expressions.html#Bash-Conditional-Expressions
6.4 Bash Conditional Expressions

`conditional-expressions` are used by the [[ compound command and the test and [
builtin commands.

Expressions may be unary or binary. Unary expressions are often used to examine
the status of a file. There are string operators and numeric comparison
operators as well. If the file argument to one of the primaries is of the form
/dev/fd/N, then file descriptor N is checked. If the file argument to one of the
primaries is one of /dev/stdin, /dev/stdout, or /dev/stderr, file descriptor 0,
1, or 2, respectively, is checked.

When used with [[, the ‘<’ and ‘>’ operators sort lexicographically using the
current locale. The test command uses ASCII ordering.

Unless otherwise specified, `primaries` that operate on files follow symbolic
links and operate on the target of the link, rather than the link itself.

<man-test> <sh-test>
'-e FILE'
     True if FILE exists.

'-f FILE'
     True if FILE exists and is a regular file.

'-h FILE'
     True if FILE exists and is a symbolic link.

'-s FILE'
     True if FILE exists and has a size greater than zero.

'-x FILE'
     True if FILE exists and is executable.

'-z STRING'
     True if the length of STRING is zero.

'-n STRING'
'STRING'
     True if the length of STRING is non-zero.

<ex>
if [ -z "${ZB_CFG}" ]; then
   # when not set
else
   # when set
fi


<sh-equal-comparison>
-n string
    True if the length of string is non-zero.

string1 == string2
string1 = string2
    True if the strings are equal. When used with the [[ command, this performs
    `pattern` matching as described above (see `conditional-constructs`.)

    ‘=’ should be used with the test command for POSIX conformance.


<ex>
if [ ! -f $file ]; then
  echo "not exist"
fi


<ex>
gender="female"
if [[ "$gender" == f* ]]


<ex>
Used to check if there are files matched. Since *sh-expansion-filename*
happens it expands to filename if there is a file and if not, left unchanged.

    if [ "`echo [SP]*.CUR`" != '[SP]*.CUR' -o "`echo [SP]*.LOG`" != '[SP]*.LOG' ]

$ echo [SP]*.CUR
P397.CUR
$ echo [SP]*.LOG
[SP]*.LOG


'ARG1 OP ARG2'
     'OP' is one of '-eq', '-ne', '-lt', '-le', '-gt', or '-ge'.  These

      These arithmetic binary operators return true if arg1 is equal to, not
      equal to, less than, less than or equal to, greater than, or greater
      than or equal to arg2, respectively. Arg1 and arg2 may be positive or
      negative `integers`.  
      

={============================================================================
*kt_linux_bash_001* sh-pattern-match

<ex>
Suppose that there are shell scipts files to be run at boot up and which are
numbered like, S01xx.sh, S02xx.sh, and so on.

steps=$(echo /etc/init.d/S??*)

How to make sure that scripts runs in the numbered order? The answer is that the
globbing returns the `sorted-list`.


3.5.8.1 Pattern Matching

Any character that appears in a pattern, other than the special pattern
characters described below, matches itself. The NUL character may not occur in a
pattern. A backslash escapes the following character; the escaping backslash is
discarded when matching. The special pattern characters must be quoted if they
are to be matched literally.

The `special-pattern-characters` have the following meanings:

*
    Matches any string, including the null string. When the globstar shell
    option is enabled, and ‘*’ is used in a filename expansion context, two
    adjacent ‘*’s used as a single pattern will match all files and
    `zero-or-more` directories and subdirectories. If followed by a ‘/’, two
    adjacent ‘*’s will match only directories and subdirectories.  

?
    Matches any single character. 

[...]
    Matches any one of the enclosed characters. A pair of characters separated
    by a hyphen denotes a range expression; any character that falls between
    those two characters, inclusive, using the current locale’s collating
    sequence and character set, is matched. If the first character following the
    ‘[’ is a ‘!’ or a ‘^’ then any character not enclosed is matched. A ‘-’ may
    be matched by including it as the first or last character in the set. A ‘]’
    may be matched by including it as the first character in the set. The
    sorting order of characters in range expressions is determined by the
    current locale and the values of the LC_COLLATE and LC_ALL shell variables,
if set.

    For example, in the default C locale, ‘[a-dx-z]’ is equivalent to
      ‘[abcdxyz]’. Many locales sort characters in dictionary order, and in
      these locales ‘[a-dx-z]’ is typically not equivalent to ‘[abcdxyz]’; it
      might be equivalent to ‘[aBbCcDdxXyYz]’, for example. To obtain the
      traditional interpretation of ranges in bracket expressions, you can force
      the use of the C locale by setting the LC_COLLATE or LC_ALL environment
      variable to the value ‘C’, or enable the globasciiranges shell option.

    Within ‘[’ and ‘]’, `character-classes` can be specified using the syntax
    [:class:], where class is one of the following classes defined in the POSIX
               standard:

    alnum   alpha   ascii   blank   cntrl   digit   graph   lower
    print   punct   space   upper   word    xdigit

    A character class matches any character belonging to that class. The `word`
    character class matches letters, digits, and the character ‘_’.

    // http://www.gnu.org/software/grep/manual/html_node/Character-Classes-and-Bracket-Expressions.html
    // ‘[:space:]’
    // 
    // Space characters: in the ‘C’ locale, this is tab, newline, vertical tab,
    // form feed, carriage return, and space. See Usage, for more discussion of
    // matching newlines.

    Within ‘[’ and ‘]’, an equivalence class can be specified using the syntax
    [=c=], which matches all characters with the same collation weight (as
        defined by the current locale) as the character c.

    Within ‘[’ and ‘]’, the syntax [.symbol.] matches the collating symbol
    symbol. 


If the `extglob` shell option is enabled using the shopt builtin, several
extended pattern matching operators are recognized. In the following
description, a pattern-list is a list of one or more patterns separated by a
‘|’. Composite patterns may be formed using one or more of the following
sub-patterns:

?(pattern-list)
    Matches zero or one occurrence of the given patterns.

*(pattern-list)
    Matches zero or more occurrences of the given patterns.

+(pattern-list)
    Matches one or more occurrences of the given patterns.

@(pattern-list)
    Matches one of the given patterns.

!(pattern-list)
    Matches anything except one of the given patterns. 

note:
Why you shouldn't parse the output of ls(1)
http://mywiki.wooledge.org/ParsingLs


={============================================================================
*kt_linux_bash_001* sh-function

3.3 Shell Functions

Shell functions are a way to `group commands` for later execution using a single
name for the group. They are executed just like a "regular" command. When the
name of a shell function is used as a simple command name, the list of commands
associated with that function name is executed. Shell functions are executed in
the `current-shell` context; no new process is created to interpret them.

Functions are declared using this syntax:

name () compound-command [ redirections ]

or

function name [()] compound-command [ redirections ]

This defines a shell function named `name`. The reserved word `function` is
`optional`. If the function reserved word is supplied, the parentheses are
`optional`. The body of the function is the compound command `compound-command`.

That command is usually a `list` enclosed `between` { and }, but may be any
compound command listed above. compound-command is executed whenever name is
specified as the name of a command. 

Any redirections (see Redirections) associated with the shell function are
performed when the function is executed.

A function definition may be deleted using the -f option to the unset builtin
(see Bourne Shell Builtins).

The exit status of a function definition is zero unless a syntax error occurs or
a readonly function with the same name already exists. When executed, the exit
status of a function is the exit status of the `last-command` executed in the
body.

<ex>
#!/bin/bash
# BASH FUNCTIONS CAN BE DECLARED IN ANY ORDER

function function_B {
  echo "{ fb"
  echo Function B.
  echo "} fb"
}

function function_A {
  echo "{ fa"
  echo $1
  echo "} fa"
}

function function_D {
  echo "{ fd"
  echo Function D.
  echo "} fd"
}

# function function_C {
function_C () {
  echo "{ fc"
  echo $1
  echo "} fc"
}

# FUNCTION CALLS

# Pass parameter to function A
function_A "Function A via pass"
function_B

# Pass parameter to function C
function_C "Function C via pass"
function_D 

$ ./sbash.sh list 
{ fa
Function A via pass
} fa
{ fb
Function B.
} fb
{ fc
Function C via pass
} fc
{ fd
Function D.
} fd


note: this causes an error as:

function_C {
  echo "{ fc"
  echo $1
  echo "} fc"
}

./sample.sh: line 23: function_C: command not found
{ fc

} fc
./sample.sh: line 27: syntax error near unexpected token `}'
./sample.sh: line 27: `}'


<ex>
get_available_space() {
    declare -a fs_info=( $(stat -f -c "%d %s" "$core_output_path") )
    echo $(( $(IFS="*"; echo "${fs_info[*]}") ))
}

# break the loop if the available space is above the threshold
[ "$(get_available_space)" -lt "$threshold" ] || break


Note that for historical reasons, in the most common usage the curly braces that
surround the body of the function must be separated from the body by blanks or
newlines. This is because the braces are reserved words and are only recognized
as such when they are separated from the command list by whitespace or another
shell metacharacter. Also, when using the braces, the list must be terminated by
a semicolon, a ‘&’, or a newline.

*sh-function-parameters*
When a function is executed, the arguments to the function `become` the
`positional-parameters` during its execution. The special parameter ‘#’ that
expands to the number of positional parameters is updated to reflect the change.
Special parameter 0 is unchanged. The first element of the FUNCNAME variable is
set to the name of the function while the function is executing.

All other aspects of the shell execution environment are identical between a
function and its caller with these exceptions: the DEBUG and RETURN traps are
not inherited unless the function has been given the trace attribute using the
declare builtin or the -o functrace option has been enabled with the set
builtin, (in which case all functions inherit the DEBUG and RETURN traps), and
the ERR trap is not inherited unless the -o errtrace shell option has been
enabled. See Bourne Shell Builtins, for the description of the trap builtin.

If the builtin command `return` is executed in a function, the function
completes and execution resumes with the next command after the function call.
Any command associated with the RETURN trap is executed before execution
resumes. When a function completes, the values of the positional parameters and
the special parameter ‘#’ are `restored` to the values they had prior to the
function’s execution. If a numeric argument is given to `return`, that is the
function’s return status; otherwise the function’s return status is the exit
status of the `last-command` executed before the return.

*sh-variable-local*
Variables local to the function may be declared with the `local` builtin. These
variables are visible only to the function and the commands it invokes.

Function names and definitions may be listed with the -f option to the `declare`
(typeset) builtin command (see Bash Builtins). The -F option to `declare` or
`typeset` will list the function names only (and optionally the source file and
    line number, if the `extdebug` shell option is enabled). Functions may be
exported so that subshells automatically have them defined with the -f option to
the export builtin (see Bourne Shell Builtins). Note that shell functions and
variables with the same name may result in multiple identically-named entries in
the environment passed to the shell’s children. Care should be taken in cases
where this may cause a problem.

Functions may be `recursive`. The FUNCNEST variable may be used to limit the
depth of the function call stack and restrict the number of function
invocations. By default, no limit is placed on the number of recursive calls. 


{exit-and-return}

4.1 Bourne Shell Builtins

exit

    exit [n]

    `exit-the-shell`, returning a status of n to the shell’s parent. If n is
    omitted, the exit status is that of the last command executed. Any trap on
    EXIT is executed before the shell terminates.


return

    return [n]

    Cause a `shell-function` to stop executing and return the value n to its
    caller. If n is not supplied, the return value is the exit status of the
    last command executed in the function. return may also be used to terminate
    execution of a script being executed with the . (source) builtin, returning
    either n or the exit status of the last command executed within the script
    as the exit status of the script. If n is supplied, the return value is its
    least significant 8 bits. Any command associated with the RETURN trap is
    executed before execution resumes after the function or script. The return
    status is non-zero if return is supplied a non-numeric argument or is used
    outside a function and not during the execution of a script by . or source.

<ex>
Unlike a shell convention which use 0 for success, return 1 (okay) when found a
string in a array at the expected position. otherwise, return 0 (fail).

function paramsContainStringAtPosition () {
    local stringToSearchFor="$1"
    local positionToExpectStringAt="$2"
    local stringToSearch="${@:3}"
    local stringToSearchAsArray=($stringToSearch)
    local currentParamPos=0;
    local currentSearchString
        
    for currentSearchString in "${stringToSearchAsArray[@]}"
        do
            if [ "$currentSearchString" == "$stringToSearchFor" ]; then
                if [ "$positionToExpectStringAt" == "$currentParamPos" ]; then
                    return 1;
                fi
            fi
            currentParamPos=$(( $currentParamPos+1 ))
        done
    return 0
}

function x {

   ... 
   paramsContainStringAtPosition "$expected_url" 7 "$@"
   urlValueCorrectAtExpectedPosition=$?   # *sh-exit-status*

   // note: here convert c style return to shell return by comararing "return
   // from function = 1"

   [[
     "$cacheOptionCorrectAtExpectedPosition" = 1 &&
     "$cacheValueCorrectAtExpectedPosition" = 1 &&
     "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
     "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
     "$jarOptionCorrectAtExpectedPosition" = "1" && 
     "$jarValueCorrectAtExpectedPosition" = "1" && 
     "$urlOptionCorrectAtExpectedPosition" = "1" && 
     "$urlValueCorrectAtExpectedPosition" = "1" 
   ]] 
}


={============================================================================
*kt_linux_bash_001* sh-variable sh-variable-no-space

3.4 Shell Parameters

A `parameter` is an entity that stores values. It can be a name, a number, or
one of the `special-characters` listed below. A `variable` is a parameter
denoted by a `name`. 

A variable has a value and zero or more attributes. Attributes are assigned
using the `declare` builtin command.

A parameter is set if it has been assigned a value. Once a variable is set, it
may be unset only by using the `unset command`

A variable may be assigned to by a statement of the form

name=[value]

note:
`no space` between "name", "=", "[value]". Otherwise, treat it as command.


If value is not given, the variable is assigned the `null-string` and the null
string is a `valid value.` 

note:
What's the difference between that value is not given and that variable is not
set? Same since both has the same `null-string`.

<ex>
# No difference if or not it has the following
# VAR_DEFINED_BUT_NOT_SET=
#
# -n string
#    True if the length of string is non-zero.

# 1 non-zero and null-string

if [ -n ${VAR_DEFINED_BUT_NOT_SET} ]; then
  echo "1 non-zero and null-string"
else
  echo "1 zero"
fi

# 1 non-zero and null-string

if [ -n $VAR_DEFINED_BUT_NOT_SET ]; then
  echo "1 non-zero and null-string"
else
  echo "1 zero"
fi

# 1 zero

if [ -n "$VAR_DEFINED_BUT_NOT_SET" ]; then
  echo "1 non-zero and null-string"
else
  echo "1 zero"
fi

# 1 zero

if [ -n "${VAR_DEFINED_BUT_NOT_SET}" ]; then
  echo "1 non-zero and null-string"
else
  echo "1 zero"
fi

note: 
${VAR} and $VAR has no effect when referring variable since it's not array.
*sh-quote*  does has effect since it stringfies it and make it null-string.
See "-n `sting`" and  *sh-single-double-quote* *sh-expansion-parameter*
*sh-[[* When on "set -x", `null-string`  shows as ''. 


# 2 zero

if [[ -n ${VAR_DEFINED_BUT_NOT_SET} ]]; then
  echo "2 non-zero and null-string"
else
  echo "2 zero"
fi

# 2 zero

if [[ -n $VAR_DEFINED_BUT_NOT_SET ]]; then
  echo "2 non-zero and null-string"
else
  echo "2 zero"
fi

# 2 zero

if [[ -n "$VAR_DEFINED_BUT_NOT_SET" ]]; then
  echo "2 non-zero and null-string"
else
  echo "2 zero"
fi

# 2 zero

if [[ -n "${VAR_DEFINED_BUT_NOT_SET}" ]]; then
  echo "2 non-zero and null-string"
else
  echo "2 zero"
fi

# debug on
+ '[' -n ']'                          
+ echo '1 non-zero and null-string'
1 non-zero and null-string

+ '[' -n ']'
+ echo '1 non-zero and null-string'
1 non-zero and null-string

+ '[' -n '' ']'
+ echo '1 zero'
1 zero

+ '[' -n '' ']'
+ echo '1 zero'
1 zero


All values undergo tilde 'expansion', parameter and variable expansion, command
substitution, arithmetic expansion, and quote removal. 

If the variable has its `integer` attribute set, then value is evaluated as an
arithmetic expression even if the $((...)) expansion is not used (see Arithmetic
    Expansion). 

Word splitting is not performed, with the exception of "$@" as explained below.
Filename expansion is not performed.  

Assignment statements may also appear as arguments to the alias, declare,
typeset, export, readonly, and local builtin commands. 


{"+="-operator}
In the context where an assignment statement is assigning a value to a shell
variable or array index (see Arrays), the ‘+=’ operator can be used to append to
or add to the variable's previous value. 

When ‘+=’ is applied to a variable for which the integer attribute has been set,
     value is evaluated as an arithmetic expression and added to the variable's
     current value, which is also evaluated.

When ‘+=’ is applied to an array variable using compound assignment (see
    Arrays), the variable's value is not unset (as it is when using ‘=’), and
new values are appended to the array beginning at one greater than the array's
maximum index (for indexed arrays), or added as additional key-value pairs in an
associative array. When applied to a string-valued variable, value is expanded
and appended to the variable’s value.


={============================================================================
*kt_linux_bash_001* sh-variable-positional-parameters

3.4.1 Positional Parameters

A positional parameter is a parameter denoted by one or more digits, other than
the single digit 0. note: not use '0'.

*sh-variable-reference*
`positional-parameters` are assigned `from the shell's arguments` when it is
invoked, and may be reassigned using the set builtin command. Positional
parameter N may be referenced `as ${N}, or as $N` when N consists of a
`single-digit`. 

Positional parameters may not be assigned to with assignment statements. The
`set` and `shift` builtins are used to set and unset them (see Shell Builtin
    Commands).

note: uses in sh-function
The positional parameters are temporarily replaced when a shell function is
executed (see Shell Functions).

note: use brace to use more parameters than 10 *sh-brace*
When a positional parameter consisting of more than a single digit is expanded,
it must be enclosed in `braces`. 


3.4.2 Special Parameters *sh-variable-special*

The shell treats several parameters specially. These parameters may only be
referenced; assignment to them is not allowed. 

*
($*) Expands to the `positional-parameters`, starting from one. 

When the expansion is `not-within-double-quotes`, each positional parameter
expands to `a separate word` In contexts where it is performed, those words are
subject to further word splitting and pathname expansion. 

When the expansion occurs `within-double-quotes`, it expands to a `single-word`
with the value of each parameter separated by the first character of the `IFS`
special variable. That is, "$*" is equivalent to "$1c$2c..." where c is the
first character of the value of the IFS variable. If IFS is unset, the
parameters are separated by spaces. If IFS is null, the parameters are joined
without intervening separators.

note:
After all, not in double-quote, gets each word for each positional parameter and
in double-quote, gets a single-word which has all parametes.

Both do the same. Why two? So the $* provides more options than $@ since can
make single word.


<ex>
args2=($*)  # `compound-assignments` in array
echo $args2 ${args2[0]} ${args2[1]} ${args2[2]}

args3="$*"
echo $args3

$ ./t_var.sh 1 2 3 4 5
1 1 2 3
1 2 3 4 5

<ex>
This function is to make a `single-word` from a array(list) separated by ":". 

note:
Q: How to redirect output from function call? Why use () in the function? To
crate a subshell?

function list_join() {
    local sep="$1"
    shift

    (
        IFS="$sep"
        echo "$*"
    )
}

declare -ar preload_libs=(
    /usr/local/lib/libdirectfb.so
    /usr/local/lib/libdirect.so
    /usr/local/lib/libinit.so
    /lib/libpthread.so.0
)

vars+=(LD_PRELOAD=$(list_join ":" ${preload_libs[@]}))


@ *sh-args*
($@) Expands to the `positional-parameters`, starting from one. 

When the expansion occurs "within double quotes", each parameter expands to a
separate word. That is, "$@" is equivalent to "$1" "$2"... 

If the double-quoted expansion occurs within a word, the expansion of the first
parameter is joined with the beginning part of the original word, and the
expansion of the last parameter is joined with the last part of the original
word. When there are no positional parameters, "$@" and $@ expand to nothing
(i.e., they are removed).

<ex> arrayfy
args=("$@");   # to get 'all' args and set it to array.
echo ${args[0]} ${args[1]} ${args[2]}

echo $@        # to print all args in a one go


0
($0) Expands to the name of the shell or shell script. This is set at shell
initialization. If Bash is invoked with a file of commands (see Shell Scripts),
  $0 is set to the name of that file. If Bash is started with the -c option (see
      Invoking Bash), then $0 is set to the first argument after the string to
  be executed, if one is present. Otherwise, it is set to the filename used to
  invoke Bash, as given by argument zero.

<ex>
This variable is commonly used to determine the behavior of scripts that can be
invoked with more than one name.

$ ln -s mytar listtar
$ ln -s mytar maketar

$ listtar or maketar

#!/bin/sh
case $0 in
   *listtar) TARGS="-tvf $1" ;;
   *maketar) TARGS="-cvf $1.tar $1" ;;
esac
tar $TARGS


#
($#) Expands to `the-number-of` positional parameters in decimal. the number of
args passed in the command line. 

note:
exclued 0th.
$ xxx.sh      // $# is 0
$ xxx.sh 1    // $# is 1


?       *sh-exit-status*
($?) Expands to the `exit-status` of the most recently executed foreground
pipeline.


$
($$) Expands to the `process ID` of the shell. In a () subshell, it expands to
the process ID of the invoking shell, not the subshell.

'!'
($!)  Expands to the process ID of the job most recently placed into the
background, whether executed as an asynchronous command or using the 'bg'
builtin (*note Job Control Builtins::).


{global-and-local}
Global variables or 'environment' variables are available in all shells. The env
or printenv commands can be used to display environment variables.

Local variables are only available in the current shell. Using the set built-in
command without any options will display a list of all variables (including
    environment variables) and functions.


{set-variable}
Putting spaces around the equal sign will cause errors.

VARNAME="value"

A variable created like the ones in the example above is only available to the
current shell. It is a local variable: child processes of the current shell will
not be aware of this variable. In order to pass variables to a subshell, we need
to export them using the export built-in command.

export VARNAME="value"

note: parent can export variables to child but not vice versa.

<set-variable-for-child>
In the Bourne shell and its descendants (e.g., bash and the Korn shell), the
following syntax can be used to add values to the environment used to execute a
single program, without affecting the parent shell (and subsequent commands):

$ NAME=value program

This adds a definition to the environment of 'just' the child process executing
the named program. If desired, multiple assignments (delimited by white space)
  can precede the program name.


{constant}
The readonly built-in marks each specified variable as unchangeable. When tried
to set, displays error but execution continues.

readonly CONST=100
echo "my const var is $CONST.."
CONST=200
echo "my const var is $CONST.."

bash: CONST: readonly variable


={============================================================================
*kt_linux_bash_001* sh-variable-array

6.7 Arrays

Bash provides one-dimensional `indexed` and `associative-array` variables. Any
variable may be used as an indexed array; the `declare` builtin will explicitly
declare an array. There is no maximum limit on the size of an array, nor any
requirement that members be indexed or assigned contiguously. Indexed arrays are
referenced using integers (including arithmetic expressions (see Shell
      Arithmetic)) and are zero-based; associative arrays use arbitrary strings.
Unless otherwise noted, indexed array indices must be non-negative integers.

An `indexed-array` is created automatically if any variable is assigned to using
the syntax

name[subscript]=value

The subscript is treated as an arithmetic expression that must evaluate to a
number. To explicitly declare an array, use

declare -a name
declare -a name[subscript]

is also accepted; the subscript is ignored.

`associative-arrays` are created using

declare -A name.

Attributes may be specified for an array variable using the declare and readonly
builtins. Each attribute applies to all members of an array.

Arrays are assigned to using `compound-assignments` of the form

name=(value1 value2 ... )

where each value is of the form [subscript]=string. Indexed array assignments do
not require anything but string. When assigning to indexed arrays, if the
optional subscript is supplied, that index is assigned to; otherwise the index
of the element assigned is the last index assigned to by the statement plus one.
Indexing starts at `zero`.

<ex>
FRUIT[0]=apple
FRUIT[1]=banana
FRUIT[2]=orange

FRUIT=(apple plum blackberry)


When assigning to an associative array, the subscript is required.

This syntax is also accepted by the declare builtin. Individual array elements
may be assigned to using the name[subscript]=value syntax introduced above.

When assigning to an indexed array, if name is subscripted by a negative number,
that number is interpreted as relative to one greater than the maximum index of
  name, so negative indices count back from the end of the array, and an index
  of -1 references the last element.


<to-reference> *sh-brace*
Any element of an array may be referenced using ${name[subscript]}. The `braces`
are required to avoid conflicts with the shell’s `filename-expansion` operators.

If the subscript is ‘@’ or ‘*’, the word expands to `all-members` of the array
name. 

These subscripts differ only when the word appears within double quotes. If the
word is double-quoted, ${name[*]} expands to a `single-word` with the value of
each array member separated by the first character of the IFS variable, and
${name[@]} expands each element of name to a separate word. When there are no
array members, ${name[@]} expands to nothing. If the double-quoted expansion
occurs within a word, the expansion of the first parameter is joined with the
beginning part of the original word, and the expansion of the last parameter is
joined with the last part of the original word. This is `analogous` to the
expansion of the special parameters ‘@’ and ‘*’. 

<ex>
The both are the same.

declare -ar list=(
    'darwin_0019fbd18c9a_487032cd821312aea3b11274c2ff7384'
    'darwin_0019fbb71c47_c4130c208206b80b3963ee05c861b5c3'
    'darwin_0019fbb71bb1_d23afbe329e75a1d9bcabc233373d398'
)

declare -ar list2=(
    darwin_0019fbd18c9a_487032cd821312aea3b11274c2ff7384
    darwin_0019fbb71c47_c4130c208206b80b3963ee05c861b5c3
    darwin_0019fbb71bb1_d23afbe329e75a1d9bcabc233373d398
)


<ex>
echo "all = ${FRUIT[*]}"      # single-word which has all
echo "all = ${FRUIT[@]}"      # same as above


<ex>
farm_hosts=(web03 web04 web05 web06 web07)

for i in ${farm_hosts[@]}; do   # same
for i in ${farm_hosts[*]}; do   # same
  su $login -c "scp $httpd_conf_new ${i}:${httpd_conf_path}"
  su $login -c "ssh $i sudo /usr/local/apache/bin/apachectl graceful"
done


<ex> 
A cron job that fills an array with the possible candidates, uses date +%W to
find the week of the year, and does a modulo operation to find the correct
index. The lucky person gets notified by e-mail.


#!/bin/bash
# This is get-tester-address.sh
#
# First, we test whether bash supports arrays. (Support for arrays was only
# added recently.)
whotest[0]='test' || (echo 'Failure: arrays not supported in this version of bash.' && exit 2)

#
# Our list of candidates. (Feel free to add or remove candidates.)
#
wholist=(
  'Bob Smith <bob@example.com>'
  'Jane L. Williams <jane@example.com>'
  'Eric S. Raymond <esr@example.com>'
  'Larry Wall <wall@example.com>'
  'Linus Torvalds <linus@example.com>'
)

# Count the number of possible testers. (Loop until we find an empty string.)

count=0
while [ "x${wholist[count]}" != "x" ]
do
   count=$(( $count + 1 ))
done

# Now we calculate whose turn it is.

week=``date '+%W'``             # The week of the year (0..53).
week=${week#0}                # Remove possible leading zero.
let "index = $week % $count"  # week modulo count = the lucky person
email=${wholist[index]}       # Get the lucky person's e-mail address.
echo $email                   # Output the person's e-mail address.

This script is then used in other scripts, such as this one, which uses a here
document:

email=`get-tester-address.sh` # Find who to e-mail.
hostname=`hostname` # This machine's name.

# Send e-mail to the right person.

mail $email -s '[Demo Testing]' <<EOF
  The lucky tester this week is: $email
  Reminder: the list of demos is here:
  http://web.example.com:8080/DemoSites
  (This e-mail was generated by $0 on ${hostname}.)
EOF


<ex>
uri="$1"
[[ $uri =~ [1-9][0-9]* ]] && uri="${urls[$uri]}"


${#name[subscript]} expands to the length of ${name[subscript]}. If subscript is
‘@’ or ‘*’, the expansion is the number of elements in the array. Referencing an
array variable without a subscript is equivalent to referencing with a
`subscript-of-0`. 

If the subscript used to reference an element of an indexed array evaluates to a
number less than zero, it is interpreted as relative to one greater than the
maximum index of the array, so negative indices count back from the end of the
array, and an index of -1 refers to the last element.

An array variable is considered set if a subscript has been assigned a value.
The null string is a valid value.

It is possible to obtain the keys (indices) of an array as well as the values.
${!name[@]} and ${!name[*]} expand to the indices assigned in array variable
name. The treatment when in double quotes is similar to the expansion of the
special parameters ‘@’ and ‘*’ within double quotes.

The `unset` builtin is used to destroy arrays. unset name[subscript] destroys the
array element at index subscript. Negative subscripts to indexed arrays are
interpreted as described above. Care must be taken to avoid unwanted side
effects caused by filename expansion. unset name, where name is an array,
removes the entire array. A subscript of ‘*’ or ‘@’ also removes the entire
  array.

The declare, local, and readonly builtins each accept a -a option to specify an
indexed array and a -A option to specify an associative array. If both options
are supplied, -A takes precedence. The read builtin accepts a -a option to
assign a list of words read from the standard input to an array, and can read
values from the standard input into individual array elements. The set and
declare builtins display array values in a way that allows them to be reused as
input. 


={============================================================================
*kt_linux_bash_001* sh-expansion

3.5 Shell Expansions

Expansion is performed on the `command-line` after it has been split into
tokens. There are seven kinds of expansion performed:

note:
Not in the script?

* Brace Expansion: Expansion of expressions within braces.
* Tilde Expansion: Expansion of the ~ character.
* Shell Parameter Expansion: How Bash expands variables to their values.
* Command Substitution: Using the output of a command as an argument.
* Arithmetic Expansion: How to use arithmetic in shell expansions.
* Process Substitution: A way to write and read to and from a command.
* Word Splitting: How the results of expansion are split into separate arguments.
* Filename Expansion: A shorthand for specifying filenames matching patterns.
* Quote Removal: How and when quote characters are removed from words.

The `order-of-expansions` is: 

brace expansion; tilde expansion, parameter and variable expansion, arithmetic
expansion, and command substitution (done in a left-to-right fashion); word
splitting; and filename expansion.

On systems that can support it, there is an additional expansion available:
process substitution. This is performed at the same time as tilde, parameter,
variable, and arithmetic expansion and command substitution.

Only brace expansion, word splitting, and filename expansion can change the
number of words of the expansion; other expansions expand a single word to a
single word. The only exceptions to this are the expansions of "$@" (see Special
    Parameters) and "${name[@]}" (see Arrays).

After all expansions, quote removal (see Quote Removal) is performed. 


={============================================================================
*kt_linux_bash_001* sh-expansion-brace

3.5.1 Brace Expansion

`brace-expansion` is a mechanism by which arbitrary strings may be generated.
This mechanism is similar to filename expansion, but the filenames generated
need not exist. Patterns to be brace expanded take the form of an optional
preamble, followed by either a series of comma-separated strings or a sequence
expression between a pair of braces, followed by an optional postscript. The
preamble is prefixed to each string contained within the braces, and the
postscript is then appended to each resulting string, expanding left to right.

Brace expansions may be nested. The results of each expanded string are not
sorted; left to right order is preserved. For example,

bash$ echo a{d,c,b}e
ade ace abe

A sequence expression takes the form {x..y[..incr]}, where x and y are either
integers or single characters, and incr, an optional increment, is an integer.
When integers are supplied, the expression expands to each number between x and
y, inclusive. Supplied integers may be prefixed with ‘0’ to force each term to
have the same width. When either x or y begins with a zero, the shell attempts
to force all generated terms to contain the same number of digits, zero-padding
where necessary. 

When characters are supplied, the expression expands to each character
lexicographically between x and y, inclusive, using the default C locale. Note
that both x and y must be of the same type. When the increment is supplied, it
is used as the difference between each term. The default increment is 1 or -1 as
appropriate.

Brace expansion is performed before any other expansions, and any characters
special to other expansions are preserved in the result. It is strictly textual.
Bash does not apply any syntactic interpretation to the context of the expansion
or the text between the braces. To avoid conflicts with parameter expansion, the
string ‘${’ is not considered eligible for brace expansion.

A correctly-formed brace expansion must contain unquoted opening and closing
  braces, and at least one unquoted comma or a valid sequence expression. Any
  incorrectly formed brace expansion is left unchanged.

A { or ‘,’ may be quoted with a backslash to prevent its being considered part
  of a brace expression. To avoid conflicts with parameter expansion, the string
    ‘${’ is not considered eligible for brace expansion.

This construct is typically used as shorthand when the common prefix of the
  strings to be generated is longer than in the above example:

mkdir /usr/local/src/bash/{old,new,dist,bugs}

# $ echo /usr/bash/{old,new,dist,bugs}
# /usr/bash/old /usr/bash/new /usr/bash/dist /usr/bash/bugs

or

chown root /usr/{ucb/{ex,edit},lib/{ex?.?*,how_ex}}

# $ echo /usr/{ucb/{ex,edit},lib/{ex?.?*,how_ex}}
# /usr/ucb/ex /usr/ucb/edit /usr/lib/ex?.?* /usr/lib/how_ex


={============================================================================
*kt_linux_bash_001* sh-expansion-tilde

3.5.2 Tilde Expansion

If a word begins with an unquoted tilde character (‘~’), all of the characters
up to the first unquoted slash (or all characters, if there is no unquoted
    slash) are considered a tilde-prefix. 

If none of the characters in the tilde-prefix are quoted, the characters in the
tilde-prefix following the tilde are treated as a possible `login-name`. 

If this login name is the null string, the tilde is replaced with the value of
the HOME shell variable. If HOME is unset, the home directory of the user
executing the shell is substituted instead.  Otherwise, the tilde-prefix is
replaced with the home directory associated with the specified login name.


If the characters following the tilde in the tilde-prefix consist of a number N,
optionally prefixed by a ‘+’ or a ‘-’, the tilde-prefix is replaced with the
  corresponding element from the directory stack, as it would be displayed by
  the `dirs` builtin invoked with the characters following tilde in the
  tilde-prefix as an argument. If the tilde-prefix, sans the tilde, consists of
  a number without a leading ‘+’ or ‘-’, ‘+’ is assumed.


Each `variable-assignment` is checked for unquoted tilde-prefixes immediately
following a ‘:’ or the first ‘=’. In these cases, tilde expansion is also
performed. Consequently, one may use filenames with tildes in assignments to
PATH, MAILPATH, and CDPATH, and the shell assigns the expanded value.

The following table shows how Bash treats unquoted tilde-prefixes:

~ 
    The value of $HOME 

~/foo
    $HOME/foo

~fred/foo
    The subdirectory foo of the home directory of the user fred

~N
    The string that would be displayed by ‘dirs +N’
~+N
    The string that would be displayed by ‘dirs +N’
~-N
    The string that would be displayed by ‘dirs -N’ 


6.8.1 Directory Stack Builtins

pushd

    pushd [-n] [+N | -N | dir]

    Save the current directory on the top of the directory stack and then `cd`
    to dir. With no arguments, pushd exchanges the top two directories.

    -n
        Suppresses the normal change of directory when adding directories to the
        stack, so that only the stack is manipulated. 

        note: means no `cd`


={============================================================================
*kt_linux_bash_001* sh-expansion-parameter sh-variable-reference

3.5.3 Shell Parameter Expansion 

The `$` character introduces `parameter-expansion`, command substitution, or
arithmetic expansion. The parameter name or symbol to be expanded may be
enclosed in `braces`, which are `optional` but serve to `protect` the variable
to be expanded from characters immediately following it which could be
interpreted as part of the name.

When braces are used, the matching ending brace is the first ‘}’ not escaped by
a backslash or within a quoted string, and not within an embedded arithmetic
expansion, command substitution, or parameter expansion.

The basic form of parameter expansion is ${parameter}. The value of parameter is
substituted. 

*sh-brace*
The parameter is a shell parameter as described above (see *sh-variable*) or an
array reference (see *sh-varaibale-arrays*). The `braces` are required when
parameter is a positional parameter with more than one digit, or when parameter
is followed by a character that is not to be interpreted as part of its name.

In each of the cases below, `word` is subject to `tilde-expansion`,
`parameter-expansion`, `command-substitution`, and arithmetic expansion.

When not performing substring expansion, using the form described below (e.g.,
    ‘:-’), Bash tests for a parameter that is `unset-or-null`. Omitting the
colon results in a test only for a parameter that is unset. Put another way, if
the colon is included, the operator tests for both parameter’s `existence` and
that its value is not null; if the colon is omitted, the operator tests only for
existence. 

${parameter:-word}
    If parameter is `unset-or-null`, the expansion of word is substituted.
    Otherwise, the value of parameter is substituted.

    note:
    Use value of parameter or word

<ex>
The value of parameter does not change. Here TEST is not defined.

$ echo $TEST
$ echo ${TEST:-test}
test
$ echo $TEST
$

<ex>
Use 'default' to make sure that the prompt is always set correctly.

PS1=${HOST:-localhost}"$ " ; export PS1 ;

<ex>
To set default 80 when it is not defined before. Why not use {parameter:=word}
form? 

# [ -z STRING ] True of the length of "STRING" is zero.

[ -z "${COLUMNS:-}" ] && COLUMNS=80

It is a shorter notation for

if [ -z "${COLUMNS:-}" ]; then
   COLUMNS=80
fi


<ex>
[ -z "${COLUMNS:-}" ] && COLUMNS=80
echo "${COLUMNS}"

unset COLUMNS

# error when use "${COLUMNS:=80}" or ${COLUMNS:=80} since 80 command not found.
# Here [] `evaluate` and return status which are not used.

[ ${COLUMNS:=80} ]
echo "${COLUMNS}"

80
80


${parameter:=word}
    If parameter is `unset-or-null`, the expansion of word is `assigned-to`
      parameter. The value of parameter is then substituted. Positional
      parameters and special parameters may not be assigned to in this way.


${parameter:offset}
${parameter:offset:length}
    This is referred to as `substring-expansion`. It expands to up to `length`
      characters of the value of parameter starting at the character specified
      by `offset`. 
      
<ex>
$ STRING="thisisaverylongname"
$ echo ${STRING:4}
isaverylongname
$ echo ${STRING:6:5}
avery


<expansion-on-arg>
If parameter is @, the result is length positional parameters 'beginning' at
offset. A negative offset is taken relative to one greater than the greatest
positional parameter, so an offset of -1 evaluates to the last positional
parameter. It is an expansion error if length evaluates to a number less than
zero.

The following examples illustrate substring expansion using positional
parameters:

$ set -- 1 2 3 4 5 6 7 8 9 0 a b c d e f g h
$ echo ${@:7}
7 8 9 0 a b c d e f g h
$ echo ${@:7:0}

$ echo ${@:7:2}
7 8

$ echo ${@:7:-2}
bash: -2: substring expression < 0

$ echo ${@: -7:2}
b c

note: this shows all args
$ echo ${@}
1 2 3 4 5 6 7 8 9 0 a b c d e f g h

$ echo ${@:0}
./bash 1 2 3 4 5 6 7 8 9 0 a b c d e f g h

$ echo ${@:0:2}
./bash 1


note: Q? this works as it has default value when parameter is not set, null.
what is it?

box_ip=${2-localhost}
box_port=${3-2033}


={============================================================================
*kt_linux_bash_001* sh-expansion-parameter-substitude

${parameter/pattern/string}

    The pattern is expanded to produce a pattern just as in filename expansion.
    `parameter` is expanded and the longest match of pattern against its value
    is replaced with string. If pattern begins with ‘/’, all matches of pattern
    are replaced with string. Normally only the first match is replaced. If
    pattern begins with ‘#’, it must match at the beginning of the expanded
    value of parameter. If pattern begins with ‘%’, it must match at the end of
    the expanded value of parameter. 
    
    If string is null, matches of pattern are deleted and the / following
    pattern may be omitted. 
    
    If parameter is ‘@’ or ‘*’, the substitution operation is applied to each
    positional parameter in turn, and the expansion is the resultant list. If
    parameter is an array variable subscripted with ‘@’ or ‘*’, the substitution
    operation is applied to each member of the array in turn, and the expansion
    is the resultant list.

<ex>
The both has the same result.

LDFLAGS='-L/home/builds-src-dev/zinc-trunk/lib -Wl,--as-needed -Wl,-rpath-link'

-LDFLAGS=``echo $LDFLAGS | sed -e 's/\s-Wl,--as-needed//g'``

+LDFLAGS="${LDFLAGS/[[:space:]]-Wl,--as-needed}"


${parameter#word}
${parameter##word}

    The `word` is expanded to produce a pattern just as in filename expansion
      (see Filename Expansion). If the pattern matches the `beginning` of the
      expanded value of parameter, then the result of the expansion is the
      expanded value of parameter with the `shortest` matching pattern (the
          ‘#’ case) or the longest matching pattern (the ‘##’ case) `deleted`. 
      
    If parameter is ‘@’ or ‘*’, the pattern removal operation is applied to
    each positional parameter in turn, and the expansion is the resultant
    list. If parameter is an array variable subscripted with ‘@’ or ‘*’, the
    pattern removal operation is applied to each member of the array in turn,
            and the expansion is the resultant list.


<ex>
test_fun=test_that_generates_url
test_handler="${test_fun#test_*}_handler"

echo $test_handler
_that_generates_url


'${PARAMETER%WORD}'
'${PARAMETER%%WORD}'
     The WORD is expanded to produce a pattern just as in filename expansion.
     If the pattern matches a `trailing` portion of the expanded value of
     PARAMETER, then the result of the expansion is the value of PARAMETER
     with the shortest matching pattern (the '%' case) or the longest matching
     pattern (the '%%' case) deleted.  
     
     If PARAMETER is '@' or '*', the pattern removal operation is applied to
     each positional parameter in turn, and the expansion is the resultant
     list.  If PARAMETER is an array variable subscripted with '@' or '*', the
     pattern removal operation is applied to each member of the array in turn,
     and the expansion is the resultant list.

<ex>
+ HDR_VER=2.6.0.0
+ HDR_M=2
+ HDR_V=6.0.0
+ HDR_m=6


={============================================================================
*kt_linux_bash_001* sh-expansion-command sh-output-from-command

3.5.4 Command Substitution

`command-substitution` allows `the output of a command` to replace the command
itself. Command substitution occurs when a command is enclosed as follows:

$(command)

or

``command`` 

Bash performs the expansion by executing command and replacing the command
substitution with the standard output of the command, with any trailing newlines
deleted. 

Embedded newlines are not deleted, but may be removed during `word splitting` 

note:
The command substitution $(cat file) can be replaced by the equivalent but
faster $(< file).

When the `old-style backquote` form of substitution is used, backslash retains its
literal meaning except when followed by ‘$’, ‘`’, or ‘\’. The first backquote
not preceded by a backslash terminates the command substitution. When using the
$(command) form, all characters between the parentheses make up the command;
none are treated specially.

Command substitutions may be nested. To nest when using the backquoted form,
        escape the inner backquotes with backslashes.

If the substitution appears within `double-quotes`, word splitting and filename
expansion are not performed on the results. 


={============================================================================
*kt_linux_bash_001* sh-expansion-word sh-ifs

3.5.7 Word Splitting

The shell scans the results of parameter expansion, command substitution, and
arithmetic expansion that did not occur within double quotes for word splitting.

The shell treats each character of $IFS as a delimiter, and splits the `results`
of the other expansions into words using these characters as `field terminators`

If IFS is unset, or its value is exactly <space><tab><newline>, the default,
then sequences of <space>, <tab>, and <newline> at the beginning and end of the
  results of the previous expansions are ignored, and any sequence of IFS
  characters not at the beginning or end serves to delimit words. 
  
If IFS has a value other than the default, then sequences of the whitespace
characters space and tab are ignored at the beginning and end of the word, as
long as the whitespace character is in the value of IFS (an IFS whitespace
    character). Any character in IFS that is not IFS whitespace, along with any
adjacent IFS whitespace characters, delimits a field. A sequence of IFS
whitespace characters is also treated as a delimiter. If the value of IFS is
null, no word splitting occurs.

Explicit null arguments ("" or '') are retained. Unquoted implicit null
arguments, resulting from the expansion of parameters that have no values, are
removed. If a parameter with no value is expanded within double quotes, a null
argument results and is retained.

Note that if no expansion occurs, no splitting is performed. 

<ex>
The ls shows many files and directories but $FILES shows one long line due to
command expansion and word splitting.

$ls
$FILES=``ls``
$echo $FILES | wc -l


note: $FILES do not change but echo changes depending on IFS Why? Due to
*sh-expansion-parameter*

$ IFS=$'\n'
$ echo $FILES
brentwood_macs_and_names_phase1.txt brentwood_macs_and_names_r36.txt brentwood_macs_and_names.txt find_matching_tests_by_version_and_mac.sh find_uploads_by_viewing_card.sh
$ echo $FILES | wc -l
1

$ IFS=$' '
$ echo $FILES
brentwood_macs_and_names_phase1.txt
brentwood_macs_and_names_r36.txt
brentwood_macs_and_names.txt
find_matching_tests_by_version_and_mac.sh
find_uploads_by_viewing_card.sh
$ echo $FILES | wc -l
5


<ex>
OLD_IFS=IFS
IFS=$'\n'

# LIST file has a format and read a line by line
# xxxx xxxx
# xxxx xxxx
# ....
MAC_ADDRESS_LIST=$(cat $MAC_ADDRESS_LIST_FILE)

IFS=OLD_IFS


={============================================================================
*kt_linux_bash_001* sh-expansion-arithmetic sh-expr

3.5.5 Arithmetic Expansion

Arithmetic expansion allows the evaluation of an arithmetic expression and the
substitution of the result. The format for arithmetic expansion is:

$(( expression ))

The expression is treated as if it were within double quotes, but a double quote
inside the parentheses is not treated specially. All tokens in the expression
undergo parameter and variable expansion, command substitution, and quote
removal. The result is treated as the arithmetic expression to be evaluated.
Arithmetic expansions may be nested.

The evaluation is performed according to the rules listed below (see
    `shell-arithmetic`). If the expression is invalid, Bash prints a message
indicating failure to the standard error and no substitution occurs. 

<ex>
The both makes the same result.

COUNT=10

while [ $COUNT -gt 0 ]; do
   echo Value of count is: $COUNT
   let COUNT=COUNT-1
done

COUNT=10

while [ $COUNT -gt 0 ]; do
   echo Value of count is: $COUNT
   COUNT=$(( $COUNT-1 ))
done


6.5 Shell Arithmetic

The shell allows `arithmetic-expressions` to be evaluated, as one of the shell
expansions or by the `let` and the -i option to the `declare` builtins.

Evaluation is done in fixed-width integers with no check for overflow, though
division by 0 is trapped and flagged as an error. The operators and their
precedence, associativity, and values are the same as in the C language. The
following list of operators is grouped into levels of equal-precedence
operators. The levels are listed in order of decreasing precedence.

id++ id--
    variable post-increment and post-decrement

++id --id
    variable pre-increment and pre-decrement


{sh-expr}
`expr` is external command but not shell builtin. 

https://www.gnu.org/software/coreutils/manual/html_node/expr-invocation.html#expr-invocation

16.4 expr: Evaluate expressions

expr evaluates an expression and writes the result on standard output. Each
token of the expression must be a separate argument. 

$ which expr
/usr/bin/expr

<ex>
Shows the same result.

#!/bin/bash

SIZE=1265403

ONE_TWO_THIRD=$(($SIZE * 2 / 3))
TWO_TWO_THIRD=``expr $SIZE \* 2 / 3``

echo "ONE=$ONE_TWO_THIRD"
echo "TWO=$TWO_TWO_THIRD"


<ex>
16.4.1 String expressions

expr supports pattern matching and other string operators. These have higher
precedence than both the numeric and relational operators (in the next
    sections).

‘string : regex’
Perform pattern matching. The arguments are converted to strings and the
second is considered to be a (basic, a la GNU grep) regular expression, with a
^ implicitly prepended. The first argument is then matched against this
regular expression.

I mostly used expr for its regular expression matching. It is sometimes more
descriptive than sed or grep:

if expr "$value" : '.*bar$' >/dev/null; then ...

Compared to:

if echo "$value" | grep '.*bar$' >/dev/null; then ...

Or:

name=`expr "$filename" : '.*/\(.*\)\.[^.]*$'`

Compared to:

name=`echo "$filename" | sed 's!.*/\(.*\)\.[^.]*$/\1/'`

They are functionally equivalent, but expr was slightly faster. Especially in
the older days when some shells had expr as a builtin (DEC Ultrix, for
    example).

Also, I program in strict Bourne shell syntax, so I still use expr for basic
arithmatic, for example:

count=0
while [ $count -lt 10 ]; do
    # something
    count=`expr $count + 1`
done

As the question was tagged with bash, I would prefer 

if [[ "$value" =~ .*bar$ ]]; then instead your two if examples, and 
with [[ "$filename" =~ .*/(.*)\.[^.]*$ ]]; name="${BASH_REMATCH[1]}" the other two examples. –
manatwork Dec 1 '11 at 14:55 


={============================================================================
*kt_linux_bash_001* sh-expansion-process

3.5.6 Process Substitution

`process-substitution` is supported on systems that support named pipes (FIFOs)
or the /dev/fd method of naming open files. It takes the form of

<(list)

or

>(list)

The process `list` is run with its input or output connected to a FIFO or some
file in /dev/fd. The name of this file is passed as an argument to the current
command as the result of the expansion. If the >(list) form is used, writing to
the file will provide input for list. If the <(list) form is used, the file
passed as an argument should be read to obtain the output of list. Note that no
space may appear between the < or > and the left parenthesis, otherwise the
construct would be interpreted as a redirection.

When available, process substitution is performed simultaneously with parameter
and variable expansion, command substitution, and arithmetic expansion. 


={============================================================================
*kt_linux_bash_001* sh-expansion-filename sh-pattern

3.5.8 Filename Expansion

After word splitting, unless the -f option has been set, Bash scans each word
for the characters ‘*’, ‘?’, and ‘[’. If one of these characters appears, then
  the word is regarded as a `pattern`, and replaced with an
    `alphabetically-sorted` list of filenames matching the pattern. 
    
If no matching filenames are found, and the shell option `nullglob` is
disabled, `the word is left unchanged.` 

If the nullglob option is set, and no matches are found, the word is removed.
If the failglob shell option is set, and no matches are found, an error
message is printed and the command is not executed. If the shell option
nocaseglob is enabled, the match is performed without regard to the case of
alphabetic characters.

When a pattern is used for filename expansion, the character ‘.’ at the start of
a filename or immediately following a slash must be matched explicitly, unless
the shell option dotglob is set. When matching a filename, the slash character
must always be matched explicitly. In other cases, the ‘.’ character is not
treated specially.

See the description of shopt in The Shopt Builtin, for a description of the
nocaseglob, nullglob, failglob, and dotglob options.

The GLOBIGNORE shell variable may be used to restrict the set of filenames
matching a pattern. If GLOBIGNORE is set, each matching filename that also
matches one of the patterns in GLOBIGNORE is removed from the list of matches.
The filenames . and .. are always ignored when GLOBIGNORE is set and not null.
However, setting GLOBIGNORE to a non-null value has the effect of enabling the
dotglob shell option, so all other filenames beginning with a ‘.’ will match. To
get the old behavior of ignoring filenames beginning with a ‘.’, make ‘.*’ one
of the patterns in GLOBIGNORE. The dotglob option is disabled when GLOBIGNORE is
unset. 


={============================================================================
*kt_linux_bash_001* sh-redirection

3.6 Redirections

Before a command is executed, its input and output may be redirected using a
special notation interpreted by the shell. Redirection allows commands’ file
handles to be duplicated, opened, closed, made to refer to different files, and
can change the files the command reads from and writes to. Redirection may also
be used to modify file handles in the current shell execution environment. The
following redirection operators may precede or appear anywhere within a simple
command or may follow a command. Redirections are processed in the order they
appear, from left to right. 

In the following descriptions, if the file descriptor number is omitted, and the
first character of the redirection operator is ‘<’, the redirection refers to
the `standard-input` (file descriptor 0). If the first character of the
redirection operator is ‘>’, the redirection refers to the `standard-output`
(file descriptor 1).

The `word` following the redirection operator in the following descriptions,
unless otherwise noted, is subjected to brace expansion, tilde expansion,
parameter expansion, command substitution, arithmetic expansion, quote removal,
filename expansion, and word splitting. If it expands to more than one word,
Bash reports an error.

Note that the order of redirections is significant. For example, the command

ls > dirlist 2>&1

directs both standard output (file descriptor 1) and standard error (file
    descriptor 2) to the file dirlist, while the command

ls 2>&1 > dirlist

`directs only` the standard output to file dirlist, because the standard error was
made a copy of the standard output before the standard output was redirected to
dirlist.

Bash handles several filenames specially when they are used in redirections, as
described in the following table:

/dev/fd/fd
    If fd is a valid integer, file descriptor fd is duplicated.

/dev/stdin
    File descriptor 0 is duplicated.
/dev/stdout
    File descriptor 1 is duplicated.
/dev/stderr
    File descriptor 2 is duplicated.

/dev/tcp/host/port
    If host is a valid hostname or Internet address, and port is an integer port
    number or service name, Bash attempts to open the corresponding TCP socket.

/dev/udp/host/port
    If host is a valid hostname or Internet address, and port is an integer port
    number or service name, Bash attempts to open the corresponding UDP socket. 

A failure to open or create a file causes the redirection to fail.

Redirections using file descriptors greater than 9 should be used with care, as
they may conflict with file descriptors the shell uses internally. 


3.6.1 Redirecting Input

Redirection of input causes the file whose name results from the expansion of
word to be opened for reading on file descriptor n, or the standard input (file
    descriptor 0) if n is not specified.

The general format for redirecting input is:

[n]<word

<ex>
$ <file `command`


3.6.2 Redirecting Output

Redirection of output causes the file whose name results from the expansion of
word to be opened for writing on file descriptor n, or the standard output (file
    descriptor 1) if n is not specified. If the file does not exist it is
`created`; if it does exist it is truncated to `zero-size`.

The general format for redirecting output is:

[n]>[|]word

If the redirection operator is ‘>’, and the `noclobber` option to the `set`
builtin has been enabled, the redirection will fail if the file whose name
results from the expansion of word exists and is a regular file. If the
redirection operator is ‘>|’, or the redirection operator is ‘>’ and the
noclobber option is not enabled, the redirection is attempted even if the file
named by word exists. 


3.6.4 Redirecting Standard Output and Standard Error

This construct allows both the standard output (file descriptor 1) and the
standard error output (file descriptor 2) to be redirected to the file whose
name is the expansion of word.

There are two formats for redirecting standard output and standard error:

&>word

and

>&word

Of the two forms, `the first is preferred` This is semantically equivalent to

>word 2>&1

When using the second form, word may not expand to a number or ‘-’. If it does,
other redirection operators apply (see Duplicating File Descriptors below) for
  compatibility reasons. 


<ex>
head &>/dev/null


<ex>
Wants to this cover two cases: 
one when a file has null first bytes, read fails and $? is 1. The other when a
file does not exist, read fails and $? is 1. But there is a error message from
head command when there is no file. So seems fine? 

NO

head -c 10 PCAT.DBJ &>/dev/null | tr -d '\0' | read -n 1

This makes read command always fails since redirect stdin and stdout to
/dev/null and there will be no input to read command! 

So have to redirect ONLY stderr.

head -c 10 PCAT.DBJ 2>/dev/null | tr -d '\0' | read -n 1


<ex>
One of the most common uses of file descriptors is to redirect STDOUT and STDERR
to separate files.  The basic syntax is

command 1> file1 2> file2

Often the STDOUT file descriptor, 1, is not written, so a shorter form of the
basic syntax is

command > file1 2> file2

*tool-tee*
<ex> the all are the same
prog1 > /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log 2>&1
prog1 2>&1 | tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log

prog1 |& tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log

$ (ZB_CFG=humax.2100 zb-make-g listprojects) > 2100.log 2>&1


<ex>
How does "done < $1" work? This is `redirection` since:

*sh-redirection-on-compound-commands*
Any redirections associated with a compound command apply to all commands within
that compound command unless explicitly overridden.


#!/bin/sh

if [ -f "$1" ] ; then
  i=0
  while read LINE
  do
    i=`echo "$i + 1" | bc`    # essentially, inc i
    echo "inner $i..."
  done < "$1"
  echo $i
fi

echo $i

$ ./sbash.sh list 
inner 1...
inner 2...
inner 3...
inner 4...
4
4


={============================================================================
*kt_linux_bash_001* sh-redirection-here

3.6.6 Here Documents

This type of redirection instructs the shell to read input from the current
source until a line containing only `word` (with no trailing blanks) is seen.
All of the lines read up to that point are then used `as-the-standard-input` for
a `command`.

The format of here-documents is:

<<[-]word
        here-document
delimiter

No parameter and variable expansion, command substitution, arithmetic expansion,
or filename expansion is performed on `word`. 
  
If any characters in `word` are quoted, the `delimiter` is the result of quote
removal on word, and the lines in the here-document are not expanded. If word is
unquoted, all lines of the here-document are subjected to parameter expansion,
command substitution, and arithmetic expansion, the character sequence \newline
  is ignored, and ‘\’ must be used to quote the characters ‘\’, ‘$’, and ‘`’.

If the redirection operator is ‘<<-’, then all leading tab characters are
stripped from input lines and the line containing delimiter. This allows
here-documents within shell scripts to be indented in a natural fashion.


3.6.7 Here Strings

A variant of here documents, the format is:

<<< word

The word undergoes brace expansion, tilde expansion, parameter and variable
expansion, command substitution, arithmetic expansion, and quote removal.

Pathname expansion and word splitting are not performed. The result is supplied
as a single string to the command on its standard input. 

<ex>

command << delimiter
document
delimiter

The delimiter must be a single word that does not contain spaces or tabs. For
example, to print a quick list of URLs, you could use the following here
document:

lpr << MYURLS
http://www.csua.berkeley.edu/~ranga/
http://www.cisco.com/
http://www.marathon.org/story/
http://www.gnu.org/
MYURLS

For example, you can use the following command to create a file with the short
list of URLs given previously:

cat > urls << MYURLS
http://www.csua.berkeley.edu/~ranga/
http://www.cisco.com/
http://www.marathon.org/story/
http://www.gnu.org/
MYURLS

note:
Output as seen between delimeters so will have spaces.

lpr << MYURLS
    http://www.csua.berkeley.edu/~ranga/
    http://www.cisco.com/
    http://www.marathon.org/story/
    http://www.gnu.org/
MYURLS

note:
Syntax error. Seems that here shall starts from beginning of a line. 

function() {
    lpr << MYURLS
    http://www.csua.berkeley.edu/~ranga/
    http://www.cisco.com/
    http://www.marathon.org/story/
    http://www.gnu.org/
    MYURLS
}

<ex>
delete_unused_partitions()
{

    echo "delete_unused_partitions"

    cat<<EOM | fdisk /dev/sda






d
3










d
4
w
EOM

}


={============================================================================
*kt_linux_bash_001* sh-executing

3.7 Executing Commands

Q: Which one when there are script and exe binary of the same name in the same
dir?

A: The executable binary gets executed.

Since

3.7.2 Command Search and Execution

3. If the name is neither a shell function nor a builtin, and contains no
slashes, Bash searches each element of $PATH for a directory containing an
executable file by that name.

4. If the search is successful, or if the command name contains one or more
slashes, the shell executes the named program in a separate execution
environment. Argument 0 is set to the name given, and the remaining arguments to
the command are set to the arguments supplied, if any.

5. If this execution fails because the file is not in executable format, and the
file is not a directory, it is assumed to be a shell script and the shell
executes it as described in Shell Scripts. 


={============================================================================
*kt_linux_bash_024* sh-builtin

4 Shell Builtin Commands

`builtin-commands` are contained within the shell itself. When the name of a
builtin command is used as the first word of a simple command (see Simple
    Commands), the shell executes the command directly, without invoking another
program. Builtin commands are necessary to implement functionality impossible or
inconvenient to obtain with separate utilities.

Unless otherwise noted, each builtin command documented as accepting `options`
preceded by ‘-’ accepts ‘--’ to signify the end of the options. 

The :, true, false, and test builtins do not accept options and do not treat
‘--’ specially.  The exit, logout, break, continue, let, and shift builtins
accept and process arguments beginning with ‘-’ without requiring ‘--’. Other
builtins that accept arguments but are not specified as accepting options
interpret arguments beginning with ‘-’ as invalid options and require ‘--’ to
prevent this interpretation. 


={============================================================================
*kt_linux_bash_024* sh-builtin-base

4.1 Bourne Shell Builtins

The following shell builtin commands are inherited from the Bourne Shell. These
commands are implemented as specified by the POSIX standard. 

{sh-source} {sh-dot}
'. (a period)'
          . FILENAME [ARGUMENTS]

     Read and execute commands from the FILENAME argument in the `current`
     shell context.  If FILENAME does not contain a slash, the 'PATH'
     variable is used to find FILENAME.  When Bash is not in POSIX mode,
     the current directory is searched if FILENAME is not found in
     '$PATH'.  If any ARGUMENTS are supplied, they become the positional
     parameters when FILENAME is executed.  Otherwise the positional
     parameters are unchanged.  If the '-T' option is enabled, 'source'
     inherits any trap on 'DEBUG'; if it is not, any 'DEBUG' trap string
     is saved and restored around the call to 'source', and 'source'
     unsets the 'DEBUG' trap while it executes.  If '-T' is not set, and
     the sourced file changes the 'DEBUG' trap, the new value is
     retained when 'source' completes.  The return status is the exit
     status of the last command executed, or zero if no commands are
     executed.  If FILENAME is not found, or cannot be read, the return
     status is non-zero.  This builtin is equivalent to 'source'.

'source'
          source FILENAME

     A synonym for '.' (*note Bourne Shell Builtins::).


{sh-exec}
exec

    exec [-cl] [-a name] [command [arguments]]

    If command is supplied, it `replaces` the shell without creating a new
    process. 
    
    If the -l option is supplied, the shell places a dash at the beginning of
    the zeroth argument passed to command. This is what the login program does.
    The -c option causes command to be executed with an empty environment. If -a
    is supplied, the shell passes name as the zeroth argument to command. 
    
    If command cannot be executed for some reason, a non-interactive shell
    exits, unless the execfail shell option is enabled. In that case, it returns
    failure. An interactive shell returns failure if the file cannot be
    executed. If no command is specified, redirections may be used to affect the
    current shell environment. 
    
    If there are no redirection errors, the return status is zero; otherwise the
    return status is non-zero.

    // This is the second use? Usecase?
    // Set redirections for the program to execute or for the current shell. If
    // only redirections are given, the redirections affect the current shell
    // without executing any program. 

<ex>
As you can see, the subshell is replaced by echo.

user@host:~$ PS1="supershell$ "
supershell$ bash
user@host:~$ PS1="subshell$ "
subshell$ exec echo hello
hello
supershell$ 


{sh-shift}
shift

    shift [n]

    Shift the positional parameters `to-the-left` by n. The positional
    parameters from n+1 ... $# are renamed to $1 ... $#-n. Parameters
    represented by the numbers $# to $#-n+1 are unset. n must be a non-negative
    number less than or equal to $#. If n is zero or greater than $#, the
    positional parameters are not changed. If n is not supplied, it is assumed
    to be 1. The return status is zero unless n is greater than $# or less than
    zero, non-zero otherwise.

<ex>
See that use S1 all the way and $# decreases

#!/bin/bash
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"

$ ./sbash.sh 1 2 3 4 5 6 
this is 1 1, num 6
this is 1 2, num 5
this is 1 3, num 4
this is 1 4, num 3
this is 1 5, num 2

So can use 'loop' until there is no more args to process:

while [ "$1" != "" ]; do
   case $1 in
      -h | -help | --help)
         echo "help.."
         exit
         ;;
      release)
         echo "release..."
         exit
         ;;
      *)
         echo "else..."
         exit
         ;;
   esac
   shift
done


{sh-trap}
trap

    trap [-lp] [arg] [sigspec ...]

    `The commands in arg` are to be read and executed when the shell receives
    `signal sigspec.` 
    
    If arg is absent (and there is a single sigspec) or equal to ‘-’, each
    specified signal's disposition is reset to the value it had when the shell
    was started. 
    
    If arg is the null string, then the signal specified by each sigspec is
    `ignored` by the shell and commands it invokes. 
    
    If arg is not present and -p has been supplied, the shell displays the trap
    commands associated with each sigspec. If no arguments are supplied, or only
    -p is given, trap prints the list of commands associated with each signal
    number in a form that may be reused as shell input. 
    
    The -l option causes the shell to print a list of signal names and their
    corresponding numbers. 
    
    Each `sigspec` is either a signal name or a signal number. Signal names are
    case insensitive and the SIG prefix is optional.

    If a sigspec is 0 or `EXIT`, arg is executed when the `shell-exits`. 
    
    If a sigspec is DEBUG, the command arg is executed before every simple
    command, for command, case command, select command, every arithmetic for
    command, and before the first command executes in a shell function. Refer to
    the description of the extdebug option to the shopt builtin (see The Shopt
        Builtin) for details of its effect on the DEBUG trap. If a sigspec is
    RETURN, the command arg is executed each time a shell function or a script
    executed with the . or source builtins finishes executing.

    If a sigspec is ERR, the command arg is executed whenever a pipeline (which
        may consist of a single simple command), a list, or a compound command
    returns a non-zero exit status, subject to the following conditions. The ERR
    trap is not executed if the failed command is part of the command list
    immediately following an until or while keyword, part of the test following
    the if or elif reserved words, part of a command executed in a && or || list
    except the command following the final && or ||, any command in a pipeline
    but the last, or if the command’s return status is being inverted using !.
    These are the same conditions obeyed by the errexit (-e) option.

    Signals ignored upon entry to the shell cannot be trapped or reset. Trapped
    signals that are not being ignored are reset to their original values in a
    subshell or subshell environment when one is created.

    The return status is zero unless a sigspec does not specify a valid signal.

<ex>
To execute function clean_up when shell exits and reset sigspec in that
function.

trap clean_up EXIT

clean_up() {
    trap - {0..15}

    echo
    echo "Cleaning up ..."

    # NOTE: Since this is usually run in an interactive shell, Ctrl-C will send
    #       SIGINT to child processes as well so they might be terminated
    #       already.
    echo "Terminating dbussenddaemon ..."
    kill %?dbussenddaemon &>/dev/null
}

<ex>
HANGUP_RECEIVED=false
trap 'HANGUP_RECEIVED=true' SIGHUP

# The process killing sequence can also be triggered manually by logging onto
# the STB and sending signal SIGHUP to the script, like this:
#
# -sh-3.2# kill -s 1 $(pidof logfile-monitor.sh)


{sh-colon}
: (a colon)

    : [arguments]

    Do nothing beyond expanding `arguments` and performing redirections. The
    return status is zero.

<ex> can use as 'no-op'
Used as a no-op, which is a command that does nothing and thus can be safely
inserted anywhere a command is needed for purely syntactical reasons:

if [ -x $CMD ]
then :
else
   echo Error: $CMD is not executable >&2
fi

In this example, assume you are not quite ready to write the code to follow the
then statement. The shell flags a syntax error if you leave that code out
completely, so you insert the : command as a temporary noop command that can be
replaced by the desired code later.

<ex>
Because the : always returns a successful result, it is sometimes used to create
an infinite loop:

while :
do
  echo "Enter some input: \c"
  read INPUT
  [ "$INPUT" = stop ] && break
Done

Because the : always returns a successful or true result, the while loop will
continue forever or until a break is executed within the loop. Sometimes you
might find that "while true" used in place of while : but using the : is more
efficient because it is a shell built-in command, whereas true is a command that
must be read from a disk file, if you are in the Bourne shell. *sh-true*


{sh-export}
export

    export [-fn] [-p] [name[=value]]

    Mark each name to be passed to child processes in the environment. 
    
    If the `-f option` is supplied, the names refer to shell functions;
    otherwise the names refer to shell variables. 
    
    The -n option means to no longer mark each name for export. If no names
    are supplied, or if the -p option is given, a list of names of all
    exported variables is displayed. The -p option displays output in a form
    that may be reused as input. If a variable name is followed by =value, the
    value of the variable is set to value.

    The return status is zero unless an invalid option is supplied, one of the
    names is not a valid shell variable name, or -f is supplied with a name
    that is not a shell function.


{sh-let}
let

    let expression [expression ...]

    The let builtin allows arithmetic to be performed on shell variables. Each
    expression is evaluated according to the rules given below in Shell
    Arithmetic. If the last expression evaluates to 0, let returns 1;
    otherwise 0 is returned.


{sh-mask}
umask

    umask [-p] [-S] [mode]

    Set the shell process’s file creation mask to mode. If mode begins with a
    digit, it is interpreted as an octal number; if not, it is interpreted as
    a symbolic mode mask similar to that accepted by the chmod command. 
    
    If mode is omitted, the current value of the mask is printed. 
    
    If the -S option is supplied without a mode argument, the mask is printed
    in a symbolic format. If the -p option is supplied, and mode is omitted,
    the output is in a form that may be reused as input. The return status is
    zero if the mode is successfully changed or if no mode argument is
    supplied, and non-zero otherwise.

    Note that when the mode is interpreted as an octal number, each number of
    the umask is subtracted from 7. Thus, a umask of 022 results in
    permissions of 755.

<ex>
$ umask
0022
$ ls -al a.out 
-rwxr-xr-x 1 kyoupark ccusers 4948 Jan 16 19:30 a.out


{sh-type}
type [-aftpP] name [name ...]

    With no options, indicate how each name would be interpreted if used as a
    command name.  
    
    If the -t option is used, type prints a string which is one of alias,
    keyword, function, builtin, or file if name is an alias, shell reserved
    word, function, builtin, or disk file, respectively.  If the name is not
    found, then nothing is  printed,  and  an  exit  status  of  false is
    returned.   If  the  -p option is used, type either returns the name of
    the disk file that would be executed if name were specified as a command
    name, or nothing if ``type -t name'' would not return file.  The -P option
    forces a PATH search for each name, even if ``type -t name'' would not
    return file.  If a command is hashed, -p and -P print the hashed value,
    not  neces‐ sarily  the  file  that appears first in PATH.  If the -a
    option is used, type prints all of the places that contain an executable
    named name.  This includes aliases and functions, if and only if the -p
    option is not also used.  The table of hashed commands is not consulted
    when using -a.  The -f option suppresses shell function lookup, as with
    the command  builtin.   type returns true if all of the arguments are
    found, false if any are not found.

kyoupark@st-castor-03:~/STB_SW_o$ type sl
sl is aliased to `echo $SHLVL'


={============================================================================
*kt_linux_bash_024* sh-builtin-bash-eval bash-expr

<sh-eval>
'eval'
          eval [ARGUMENTS]

     The arguments are concatenated together into a single command,
     which is then read and executed, and its exit status returned as
     the exit status of 'eval'.  If there are no arguments or only empty
     arguments, the return status is zero.

<ex>
eval ac_val=$``echo $ac_var``
as_tr_sh="eval sed 'y%*+%pp%;s%[^_$as_cr_alnum]%_%g'"


<sh-expr>
  ac_optarg=``expr "x$ac_option" : 'x[^=]*=\(.*\)'``

note: expr is a tool.

/usr/bin/expr

NAME
       expr - evaluate expressions

SYNOPSIS
       expr EXPRESSION
       expr OPTION

DESCRIPTION
       STRING : REGEXP
              anchored pattern match of REGEXP in STRING

ac_option=--cache-file=/dev/null
ac_optarg=``expr "x$ac_option" : 'x[^=]*=\(.*\)'``

$echo $ac_optarg
/dev/null


={============================================================================
*kt_linux_bash_024* sh-builtin-bash

4.2 Bash Builtin Commands

This section describes builtin commands which are unique to or have been
extended in Bash. Some of these commands are specified in the POSIX standard. 

{sh-echo}
echo

    echo [-neE] [arg ...]

    Output the args, separated by spaces, terminated with a `newline`. The
    return status is 0 unless a write error occurs. If -n is specified, the
    trailing newline is suppressed. 
    
    If the -e option is given, interpretation of the following backslash-escaped
    characters is enabled. 
    
    The -E option disables the interpretation of these escape characters, even
    on systems where they are interpreted by default. The xpg_echo shell option
    may be used to dynamically determine whether or not echo expands these
    escape characters by default. echo does not interpret -- to mean the end of
    options.

note:
The `echo` prints out `newline` regardless of value of variable. The following
prints out value of variable and newline. When variable is not set or do not
have value, it prints out newline only and it the same as using echo only.

echo $variable

    echo interprets the following `escape-sequences`: 


{sh-command}
command

    command [-pVv] command [arguments …]

    Runs command with arguments ignoring any `shell-function` named command.
    Only shell builtin commands or commands found by searching the PATH are
    executed. If there is a shell function named ls, running ‘command ls’ within
    the function will execute the external command ls instead of calling the
    function recursively. The -p option means to use a default value for PATH
    that is guaranteed to find all of the standard utilities. The return status
    in this case is 127 if command cannot be found or an error occurred, and the
    exit status of command otherwise.

    If either the -V or -v option is supplied, a description of command is
    printed. The -v option causes a single word indicating the command or file
    name used to invoke command to be displayed; the -V option produces a more
    verbose description. In this case, the return status is zero if command is
    found, and non-zero if not.

<ex>
if [ -x "$(command -v nexus-inspect)" ]; then
    echo "Running on a Nexus enabled system"

note: 
-v returns a full path. On host, -v returns filename so shows the same when use
which command. That is filename when there it is and null when not.

++ command -v nexus-inspect
+ '[' -x /opt/zinc/bin/nexus-inspect ']'
+ echo 'Running on a Nexus enabled system'
Running on a Nexus enabled system


{sh-read}
read

    read [-ers] [-a aname] [-d delim] [-i text] [-n nchars]
        [-N nchars] [-p prompt] [-t timeout] [-u fd] [`name` ...]

    One line is `read from` the `standard-input`, or from the file descriptor fd
    supplied as an argument to the -u option, and the first word is assigned to
    the first `name`, the second word to the second `name`, and so on, with leftover
    words and their intervening separators assigned to the last name. If there
    are fewer words read from the input stream than names, the remaining names
    are assigned empty values. 
    
    *sh-ifs*
    The characters in the value of the `IFS` variable are used to split the
    line into words using the same rules the shell uses for expansion. The
    backslash character ‘\’ may be used to remove any special meaning for the
    next character read and for line continuation. If no names are supplied,
    the line read is assigned to the variable REPLY. 
    
    The return code is `zero-exit-status`, unless end-of-file is encountered,
    read times out (in which case the return code is greater than 128), a
    variable assignment error (such as assigning to a readonly variable)
  occurs, or an invalid file descriptor is supplied as the argument to -u. 


<ex>
A common use of input redirection in conjunction with the read command is the
reading of a file one line at a time using the while loop.

#!/bin/bash

cat list | while read f; do
  echo "line $f"
done


<ex> 
Get the `second word` from input line

$ grep 'sda' DF.LOG | grep 'FAT'
/dev/sda1              63   976751999   488375968   1 FAT12

$ echo "/dev/sda1              63   976751999   488375968   1 FAT12" | \
    while read a b c; do echo $b; done
63

In the script:

A=``grep 'sda' $F | grep 'FAT' | while read a b c;do echo $b;done``


<ex>
To search through the same errors in logs which was uploaded in Feburary and was
under translation. For example, translation/Zone9-Box5_Feb_18_09_38_42b3e5c91

ls translation/ \
    | grep Feb | while read x; do pushd -n translation/$x;
    pwd; egrep --color -ano 'btreePageFromDbPage' .detailed_output; popd -n; 
    done 

WHY? Since "pushd -n" do not `cd` and means runs those command on the parent?   


<ex>
when FILE is not in the current directory, it emits error message but not stop
running a script and cause to have empty variable. need to check return?
*sh-error-check*

FILE="version.txt"

while read line; do
cur_ver=`echo $line|awk '{print $1}'`
done <$FILE


{sh-printf}

printf

    printf [-v var] format [arguments]

    Write the formatted `arguments` to the standard output under the control of
    the `format`. The -v option causes the output to be assigned to the variable
    var rather than being printed to the standard output.

    The format is a character string which contains three types of objects:
    plain characters, which are simply copied to standard output, character
    escape sequences, which are converted and copied to the standard output, and
    format specifications, each of which causes printing of the next successive
    argument. In addition to the standard `printf(1)` formats, printf interprets
    the following extensions:

    %b
        Causes printf to expand backslash escape sequences in the corresponding
        argument, except that ‘\c’ terminates output, backslashes in ‘\'’, ‘\"’,
        and ‘\?’ are not removed, and octal escapes beginning with ‘\0’ may
        contain up to four digits. 

    %q
        Causes printf to output the corresponding argument in a format that can
        be reused as shell input. 

    %(datefmt)T
        Causes printf to output the date-time string resulting from using
        datefmt as a format string for strftime(3). The corresponding argument
        is an integer representing the number of seconds since the epoch. Two
        special argument values may be used: -1 represents the current time, and
        -2 represents the time the shell was invoked. If no argument is
        specified, conversion behaves as if -1 had been given. This is an
        exception to the usual printf behavior. 

    Arguments to non-string format specifiers are treated as C language
    constants, except that a leading plus or minus sign is allowed, and if the
    leading character is a single or double quote, the value is the ASCII value
    of the following character.

    The format is reused as necessary to consume all of the arguments. If the
    format requires more arguments than are supplied, the extra format
    specifications behave as if a zero value or null string, as appropriate, had
    been supplied. The return value is zero on success, non-zero on failure.

<ex>

debug() {
    # Note: echo -e doesn't work on OS X's default bash (3.2).
    printf '\n\033[0;32m%s\n' "$*"
    tput sgr0
}


={============================================================================
*kt_linux_bash_024* sh-builtin-set

*sh-set*

4.3.1 The Set Builtin

This builtin is so complicated that it deserves its own section. `set-command`
allows you to change the values of shell options and set the positional
parameters, or to display the names and values of shell variables. 

set

    set [--abefhkmnptuvxBCEHPT] [-o option-name] [argument ...]
    set [+abefhkmnptuvxBCEHPT] [+o option-name] [argument ...]

    If no options or arguments are supplied, set displays the names and values
    of all shell variables and functions, sorted according to the current
    locale, in a format that may be reused as input for setting or resetting
    the currently-set variables. Read-only variables cannot be reset. In POSIX
    mode, only shell variables are listed. 

*sh-debug*
-x
    Print a trace of simple commands, for commands, case commands, select
    commands, and arithmetic for commands and their arguments or associated
    word lists after they are expanded and before they are executed. The value
    of the PS4 variable is expanded and the resultant value is printed before
    the command and its expanded arguments.

<ex>
$ set -o
allexport      	off
braceexpand    	on
emacs          	on
errexit        	off
errtrace       	off
functrace      	off
hashall        	on
histexpand     	on
history        	on
ignoreeof      	off
interactive-comments	on
keyword        	off
monitor        	on
noclobber      	off
noexec         	off
noglob         	off
nolog          	off
notify         	off
nounset        	off
onecmd         	off
physical       	off
pipefail       	off
posix          	off
privileged     	off
verbose        	off
vi             	off
xtrace         	off


     '-e'
          Exit immediately if a pipeline (*note Pipelines::), which may
          consist of a single simple command (*note Simple Commands::),
          a list (*note Lists::), or a compound command (*note Compound
          Commands::) returns a non-zero status.  The shell does not
          exit if the command that fails is part of the command list
          immediately following a 'while' or 'until' keyword, part of
          the test in an 'if' statement, part of any command executed in
          a '&&' or '||' list except the command following the final
          '&&' or '||', any command in a pipeline but the last, or if
          the command's return status is being inverted with '!'.  If a
          compound command other than a subshell returns a non-zero
          status because a command failed while '-e' was being ignored,
          the shell does not exit.  A trap on 'ERR', if set, is executed
          before the shell exits.

          This option applies to the shell environment and each subshell
          environment separately (*note Command Execution
          Environment::), and may cause subshells to exit before
          executing all the commands in the subshell.

          If a compound command or shell function executes in a context
          where '-e' is being ignored, none of the commands executed
          within the compound command or function body will be affected
          by the '-e' setting, even if '-e' is set and a command returns
          a failure status.  If a compound command or shell function
          sets '-e' while executing in a context where '-e' is ignored,
          that setting will not have any effect until the compound
          command or the command containing the function call completes.


4.3.2 The Shopt Builtin

This builtin allows you to change additional shell `optional` behavior.

shopt

    shopt [-pqsu] [-o] [optname ...]


direxpand
    If set, Bash replaces directory names with the results of word expansion
    when performing filename completion. This changes the contents of the
    readline editing buffer. If not set, Bash attempts to preserve what the user
    typed.


With no options, or with the -p option, a list of all settable options is
displayed, with an indication of whether or not each is set.

$ shopt
autocd         	off
cdable_vars    	off
cdspell        	off
checkhash      	off
checkjobs      	off
checkwinsize   	on
cmdhist        	on
compat31       	off
compat32       	off
compat40       	off
compat41       	off
direxpand      	on
dirspell       	off
dotglob        	off
execfail       	off
expand_aliases 	on
extdebug       	off
extglob        	on
extquote       	on
failglob       	off
force_fignore  	on
globstar       	off
gnu_errfmt     	off
histappend     	on
histreedit     	off
histverify     	off
hostcomplete   	off
huponexit      	off
interactive_comments	on
lastpipe       	off
lithist        	off
login_shell    	off
mailwarn       	off
no_empty_cmd_completion	off
nocaseglob     	off
nocasematch    	off
nullglob       	off
progcomp       	on
promptvars     	on
restricted_shell	off
shift_verbose  	off
sourcepath     	on
xpg_echo       	off


={============================================================================
*kt_linux_bash_038* sh-builtin-declare

declare

    declare [-aAfFgilnrtux] [-p] [name[=value] ...]

    Declare variables and give them attributes. If no names are given, then
    display the values of variables instead. 

    The -p option will display the attributes and values of each name. When -p
    is used with name arguments, additional options, other than -f and -F, are
    ignored.

    When -p is supplied without name arguments, declare will display the
    attributes and values of all variables having the attributes specified by
    the additional options. If no other options are supplied with -p, declare
    will display the attributes and values of all shell variables. The -f option
    will restrict the display to shell functions.

    The -F option inhibits the display of function definitions; only the
    function name and attributes are printed. If the extdebug shell option is
    enabled using shopt (see The Shopt Builtin), the source file name and line
    number where the function is defined are displayed as well. -F implies -f.

    The -g option forces variables to be created or modified at the global
    scope, even when declare is executed in a shell function. It is ignored in
    all other cases.

    The following options can be used to restrict output to variables with the
    specified attributes or to give variables attributes: 

    -a
    Each name is an indexed array variable (see Arrays).

    -A
    Each name is an associative array variable (see Arrays).

    -r
    Make names readonly. These names cannot then be assigned values by
    subsequent assignment statements or unset.

    -x     
    Mark  names  for  export  to  subsequent commands via the environment.


={============================================================================
*kt_linux_bash_024* sh-builtin-job

7.2 Job Control Builtins

kill

    kill [-s sigspec] [-n signum] [-sigspec] jobspec or pid
    kill -l [exit_status]

    Send a signal specified by `sigspec` or `signum` to the process named by job
    specification jobspec or process ID pid. sigspec is either a
    case-insensitive signal name such as SIGINT (with or `without` the SIG
        prefix) or a signal number; signum is a signal number. If sigspec and
    signum are not present, SIGTERM is used. The -l option lists the signal
    names. If any arguments are supplied when -l is given, the names of the
    signals corresponding to the arguments are listed, and the return status is
    zero. exit_status is a number specifying a signal number or the exit status
    of a process terminated by a signal. The return status is zero if at least
    one signal was successfully sent, or non-zero if an error occurs or an
    invalid option is encountered.  
    
wait

    wait [-n] [jobspec or pid ...]

    Wait until the child process specified by each process ID pid or job
    specification jobspec exits and return the exit status of the last command
    waited for. If a job spec is given, all processes in the job are waited for.
    If no arguments are given, all currently active child processes are waited
    for, and the return status is zero. If the -n option is supplied, wait waits
      for any job to terminate and returns its exit status. If neither jobspec
        nor pid specifies an active child process of the shell, the return
          status is 127.


={============================================================================
*kt_linux_bash_024* sh-builtin-help

4.2 Bash Builtin Commands

help

    help [-dms] [pattern]

    Display helpful information about builtin commands. If pattern is specified,
    help gives detailed help on all commands matching pattern, otherwise a list
    of the builtins is printed.

    Options, if supplied, have the following meanings:

    -d
        Display a short description of each pattern 
    -m
        Display the description of each pattern in a manpage-like format 
    -s
        Display only a short usage synopsis for each pattern 

    The return status is zero unless no command matches pattern.


={============================================================================
*kt_linux_bash_024* sh-builtin-source

4.2 Bash Builtin Commands

source

    source filename

    A synonym for . (see Bourne Shell Builtins).

4.1 Bourne Shell Builtins

. (a period)

    . filename [arguments]

    Read and execute commands from the filename argument in the `current` shell
    context. 
    
    If filename does not contain a slash, the PATH variable is used to find
    filename. When Bash is not in POSIX mode, the current directory is
    searched if filename is not found in $PATH. If any arguments are supplied,
they become the positional parameters when filename is executed. Otherwise the
  positional parameters are unchanged. If the -T option is enabled, source
  inherits any trap on DEBUG; if it is not, any DEBUG trap string is saved and
  restored around the call to source, and source unsets the DEBUG trap while
  it executes. If -T is not set, and the sourced file changes the DEBUG trap,
the new value is retained when source completes. The return status is the exit
  status of the last command executed, or zero if no commands are executed. If
  filename is not found, or cannot be read, the return status is non-zero.
  This builtin is equivalent to source.

<ex>
# Grab a few variables from the .config. We can't just source the
# .config file as a shell script because some variable values are
# makefile variables.
grep BR2_PACKAGE_BUSYBOX_CONFIG .config > /tmp/spkconf$$
. /tmp/spkconf$$
rm /tmp/spkconf$$


={============================================================================
*kt_linux_bash_024* sh-builtin-hash which-command sh-hash sh-type

https://unix.stackexchange.com/questions/86012/what-is-the-purpose-of-the-hash-command

pi@raspberrypi ~/snugupdate-v2-snugberrypi $ sudo pip install -U pip
// upgrading is done.

pi@raspberrypi ~/snugupdate-v2-snugberrypi $ pip --version
-bash: /usr/bin/pip: No such file or directory

pi@raspberrypi ~/snugupdate-v2-snugberrypi $ which pip
/usr/local/bin/pip

pi@raspberrypi ~/snugupdate-v2-snugberrypi $ echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games

WHY? both hash and type are builtin commands

pi@raspberrypi ~/snugupdate-v2-snugberrypi $ hash -r

pi@raspberrypi ~/snugupdate-v2-snugberrypi $ type pip
pip is /usr/local/bin/pip

pi@raspberrypi ~/snugupdate-v2-snugberrypi $ pip --version
pip 9.0.1 from /usr/local/lib/python2.7/dist-packages (python 2.7)

pi@raspberrypi ~/snugupdate-v2-snugberrypi $ hash
hits    command
   1    /usr/bin/which
   1    /usr/local/bin/pip
   1    /usr/bin/man


={============================================================================
*kt_linux_bash_025* sh-special-variable

5.1 Bourne Shell Variables

Bash uses certain shell variables in the same way as the Bourne shell. In some
cases, Bash assigns a default value to the variable. 

PATH
    A `colon-separated` list of directories in which the shell looks for
    commands. A zero-length (null) directory name in the value of PATH indicates
    the current directory. A null directory name may appear as two adjacent
    colons, or as an initial or trailing colon.

PS1
    The primary prompt string. The default value is ‘\s-\v\$ ’. See 6.9
    Controlling the Prompt, for the complete list of escape sequences that are
    expanded before PS1 is displayed.

6.9 Controlling the Prompt

\t
    The time, in 24-hour HH:MM:SS format. 
\T
    The time, in 12-hour HH:MM:SS format. 


PS2
    The secondary prompt string. The default value is ‘> ’.


5.2 Bash Variables

These variables are set or used by Bash, but other shells do not normally treat
them specially. 


={============================================================================
*kt_linux_bash_023* sh-invoking

<ex>
# in the script
#!/bin/bash -eix


6.1 Invoking Bash

bash [long-opt] 
    [-ir] [-abefhkmnptuvxdBCDHP] [-o option] [-O shopt_option] [argument ...]
bash [long-opt] 
    [-abefhkmnptuvxdBCDHP] [-o option] [-O shopt_option] -c string [argument
    ...]
bash [long-opt] 
    -s [-abefhkmnptuvxdBCDHP] [-o option] [-O shopt_option] [argument ...]

All of the `single-character` options used with the `set` builtin can be used as
options when the shell is invoked. 

In addition, there are several multi-character options that you can use. These
options must appear on the 'command' line before the single-character options to
be recognized. 

--debugger
    Arrange for the debugger profile to be executed before the shell starts.
    Turns on extended debugging mode (see The Shopt Builtin for a description of
        the extdebug option to the shopt builtin).

    note: http://bashdb.sourceforge.net/bashdb.html

There are several single-character options that may be supplied at invocation
which are not available with the `set` builtin. 

-i
    Force the shell to run interactively. Interactive shells are described in
    Interactive Shells.


{end-of-option}
--
    A -- signals the `end-of-options` and disables further option processing.
    Any arguments after the -- are treated as filenames and arguments. 

<ex>
From the below command line, what is "--"? The "--" signify the `end-of-options`

xxx.sh -o humax.1000 -- -r

xxx.sh:
   ...
	script-two "$@" "$host"

So the "-r" is passed to script-two as `argument` which is from xxx.sh

<ex>
This syntax ensures that you can run commands on the remote server without ssh
parsing them:

ssh nixcraft@server1.cyberciti.biz -- command1 --arg1 --arg2

The above syntax tell ssh not try to parse --arg1 and --arg2 after -- command
line options. This ensures that command1 will accept --arg1 and --arg2 as
arguments.

# safe examples
ssh nixcraft@server1.cyberciti.biz -- --commandName --arg1 --arg2

This kind of behavior is mostly defined and handled by the ssh command and not
by your bash/ksh/csh shell. This is also true for many other commands. 

For example you can not create or view a file named --file or -f using cat
command

# fail
cat --file    # cat: unrecognized option '--f'
cat -f        # cat: invalid option -- 'f'

Instead try passing double dash "--" to instruct cat command not to try to parse
what comes after command line options:

# works
cat -- --file
cat -- -f


4.3.1 The Set Builtin

This builtin is so complicated that it deserves its own section. `set` allows
you to change the values of shell options and set the positional parameters, or
to display the names and values of shell variables.

set

    set [--abefhkmnptuvxBCEHPT] [-o option-name] [argument ...]
    set [+abefhkmnptuvxBCEHPT] [+o option-name] [argument ...]

If no options or arguments are supplied, `set` displays the names and values of
all shell variables and functions, sorted according to the current locale, in a
format that may be reused as input for setting or resetting the currently-set
variables. Read-only variables cannot be reset. In POSIX mode, only shell
variables are listed.

When options are supplied, they set or unset shell attributes. Options, if
specified, have the following meanings: 

-e
    Exit immediately if a pipeline (see Pipelines), which may consist of a
    single simple command (see Simple Commands), a list (see Lists), or a
    compound command (see Compound Commands) returns a non-zero status. The
    shell does not exit if the command that fails is part of the command list
    immediately following a while or until keyword, part of the test in an if
    statement, part of any command executed in a && or || list except the
    command following the final && or ||, any command in a pipeline but the
    last, or if the command’s return status is being inverted with !. If a
    compound command other than a subshell returns a non-zero status because a
    command failed while -e was being ignored, the shell does not exit. A trap
    on ERR, if set, is executed before the shell exits.

    This option applies to the shell environment and each subshell environment
    separately (see Command Execution Environment), and may cause subshells to
    exit before executing all the commands in the subshell.

    If a compound command or shell function executes in a context where -e is
    being ignored, none of the commands executed within the compound command or
    function body will be affected by the -e setting, even if -e is set and a
    command returns a failure status. If a compound command or shell function
    sets -e while executing in a context where -e is ignored, that setting will
    not have any effect until the compound command or the command containing the
    function call completes.

-x
    Print a trace of simple commands, for commands, case commands, select
    commands, and arithmetic for commands and their arguments or associated word
    lists after they are expanded and before they are executed. The value of the
    PS4 variable is expanded and the resultant value is printed before the
    command and its expanded arguments.


={============================================================================
*kt_linux_bash_030* sh-interactive-or-not

http://www.gnu.org/software/bash/manual/html_node/Interactive-Shells.html#Interactive-Shells

6.3.1 What is an Interactive Shell?

An `interactive-shell` is one started without non-option arguments, unless -s is
specified, without specifying the -c option, and whose input and error output
are both connected to terminals (as determined by isatty(3)), or one started
with the -i option.

An interactive shell generally reads from and writes to a user's 'terminal'.

The -s invocation option may be used to set the positional parameters when an
interactive shell is started. 


6.3.3 Interactive Shell Behavior

When the shell is running interactively, it changes its behavior in several
ways.

    Startup files are read and executed as described in Bash Startup Files.

    // ...


6.2 Bash Startup Files

This section describes how Bash executes its startup files. If any of the files
exist but cannot be read, Bash reports an error. Tildes are expanded in
filenames as described above under Tilde Expansion (see Tilde Expansion).

Invoked as an interactive login shell, or with --login

When Bash is invoked as an interactive login shell, or as a non-interactive
shell with the --login option, it first reads and executes commands from the
file /etc/profile, if that file exists. After reading that file, it looks for
~/.bash_profile, ~/.bash_login, and ~/.profile, in that order, and reads and
executes commands from the first one that exists and is readable. The
--noprofile option may be used when the shell is started to inhibit this
behavior.

When a login shell exits, Bash reads and executes commands from the file
~/.bash_logout, if it exists.


Invoked as an interactive non-login shell

When an interactive shell that is not a login shell is started, Bash reads and
executes commands from note: ~/.bashrc, if that file exists. This may be
inhibited by using the --norc option. The --rcfile file option will force Bash
to read and execute commands from file instead of ~/.bashrc.

So, typically, your ~/.bash_profile contains the line

if [ -f ~/.bashrc ]; then . ~/.bashrc; fi

after (or before) any login-specific initializations.

note:
When use -i option in the script, it does exit then reaches the end and there
should be other way to keep that shell running.


Invoked non-interactively

When Bash is started non-interactively, to run a `shell-script`, for example, it
looks for the variable BASH_ENV in the environment, expands its value if it
appears there, and uses the expanded value as the name of a file to read and
execute. Bash behaves as if the following command were executed:

if [ -n "$BASH_ENV" ]; then . "$BASH_ENV"; fi

but the value of the PATH variable is not used to search for the filename.

As noted above, if a non-interactive shell is invoked with the --login option,
   Bash attempts to read and execute commands from the login shell startup
     files.


Invoked with name sh

If Bash is invoked with the name sh, it tries to mimic the startup behavior of
historical versions of sh as closely as possible, while conforming to the POSIX
standard as well.

When invoked as an interactive login shell, or as a non-interactive shell with
the --login option, it first attempts to read and execute commands from
/etc/profile and ~/.profile, in that order. The --noprofile option may be used
to inhibit this behavior. When invoked as an interactive shell with the name sh,
Bash looks for the variable ENV, expands its value if it is defined, and uses
  the expanded value as the name of a file to read and execute. Since a shell
  invoked as sh does not attempt to read and execute commands from any other
  startup files, the --rcfile option has no effect. A non-interactive shell
  invoked with the name sh does not attempt to read any other startup files.

When invoked as sh, Bash enters POSIX mode after the startup files are read.


Invoked by remote shell daemon

Bash attempts to determine when it is being run with its standard input
connected to a network connection, as when executed by the remote shell daemon,
usually rshd, or the secure shell daemon sshd. If Bash determines it is being
  run in this fashion, it reads and executes commands from ~/.bashrc, if that
  file exists and is readable. It will not do this if invoked as sh. The --norc
  option may be used to inhibit this behavior, and the --rcfile option may be
  used to force another file to be read, but neither rshd nor sshd generally
  invoke the shell with those options or allow them to be specified. 


http://tldp.org/LDP/abs/html/intandnonint.html

36.1. Interactive and non-interactive shells and scripts

An interactive shell reads commands from user input on a tty. Among other
things, such a shell reads startup files on activation, displays a prompt, and
enables job control by default. The user can interact with the shell.

A shell running a script is always a non-interactive shell.

Let us consider an interactive script to be one that requires input from the
user, usually with read statements (see Example 15-3). "Real life" is actually a
bit messier than that. For now, assume an interactive script is bound to a tty,
a script that a user has invoked from the console or an xterm.

Init and startup scripts are necessarily non-interactive, since they must run
without human intervention. Many administrative and system maintenance scripts
are likewise non-interactive. Unvarying repetitive tasks cry out for automation
by non-interactive scripts.

Non-interactive scripts can run in the background, but interactive ones hang,
waiting for input that never comes.


={============================================================================
*kt_linux_bash_023* sh-command-history

9.1 Bash History Facilities

The value of the HISTSIZE shell variable is used as the number of commands to
save in a history list.

When the shell starts up, the history is initialized from the file named by the
HISTFILE variable (default ~/.bash_history). The file named by the value of
HISTFILE is truncated, if necessary, to contain no more than the number of lines
specified by the value of the HISTFILESIZE variable. When a shell with history
enabled exits, the last $HISTSIZE lines are copied from the history list to the
file named by $HISTFILE. If the histappend shell option is set (see Bash
    Builtins), the lines are appended to the history file, otherwise the history
file is overwritten. If HISTFILE is unset, or if the history file is unwritable,
the history is not saved. After saving the history, the history file is
  truncated to contain no more than $HISTFILESIZE lines. If HISTFILESIZE is
  unset, or set to null, a non-numeric value, or a numeric value less than zero,
the history file is not truncated.


{erase-duplicates}
export HISTCONTROL=erasedups

5.2 Bash Variables

HISTCONTROL

    A colon-separated list of values controlling how commands are saved on the
    history list. If the list of values includes ‘ignorespace’, lines which
    begin with a space character are not saved in the history list. A value of
    ‘ignoredups’ causes lines which match the previous history entry to not be
    saved. A value of ‘ignoreboth’ is shorthand for ‘ignorespace’ and
    ‘ignoredups’. A value of `erasedups` causes all previous lines matching the
    current line to be removed from the history list before that line is saved.
    Any value not in the above list is ignored. If HISTCONTROL is unset, or does
    not include a valid value, all lines read by the shell parser are saved on
    the history list, subject to the value of HISTIGNORE. The second and
    subsequent lines of a multi-line compound command are not tested, and are
    added to the history regardless of the value of HISTCONTROL.


<history-per-terminal>
# in .bash_profile
echo $SSH_TTY
export HISTFILE=/home/NDS-UK/parkkt/.bash_history_$(echo $SSH_TTY | cut -f 4 -d'/')
export HISTFILESIZE=10000
export HISTSIZE=10000 

note: better to have TTY number in the prompt as well.


9.2 Bash History Builtins

Bash provides two builtin commands which manipulate the history list and history
file.

fc

    fc [-e ename] [-lnr] [first] [last]

This is not quite what you want as it will launch an editor first, but that is
probably a good thing since it gives you a chance to double check that you have
the correct commands and even edit them using all the capabilities of your
favorite editor. Once you save you changes and exit the editor, the commands
will be run.

The first form selects a range of commands from first to last from the history
list and displays or edits and re-executes them.

If the -l flag is given, the commands are listed on standard output. The -n
flag suppresses the command numbers when listing. The -r flag reverses the
order of the listing. When editing is complete, the edited commands are echoed
and executed. 

$ fc 100 120


    fc -s [pat=rep] [command]

In the second form, command is re-executed after each instance of
`pat`(pattern) in the selected command is replaced by rep. command is
intepreted the same as first above.

A useful alias to use with the fc command is r='fc -s', so that typing ‘r cc’
runs the last command `beginning` with cc and typing ‘r’ re-executes the last
command (see Aliases). 


={============================================================================
*kt_linux_bash_032* sh-command-edit

https://www.gnu.org/software/bash/manual/bash.html#Command-Line-Editing

8 Command Line Editing

This chapter describes the basic features of the GNU command line editing
interface. Command line editing is provided by the Readline library, which is
used by several different programs, 'including' Bash. Command line editing is
enabled `by-default` when using an interactive shell, unless the --noediting
option is supplied at shell invocation. Line editing is also used when using the
-e option to the read builtin command (see Bash Builtins). 

By default, the line editing commands are similar to those of Emacs. A vi-style
line editing interface is also available. Line editing can be enabled at any
time using the `-o emacs` or `-o vi` options to the set builtin command (see The
    Set Builtin), or disabled using the +o emacs or +o vi options to set. 


8.1 Introduction to Line Editing

The following paragraphs describe the notation used to represent keystrokes.

The text C-k is read as ‘Control-K’ and describes the character produced when
the k key is pressed while the Control key is depressed.

The text M-k is read as ‘Meta-K’ and describes the character produced when the
Meta key (if you have one) is depressed, and the k key is pressed. The Meta key
is labeled `ALT` on many keyboards. 

If you do not have a Meta or ALT key, or another key working as a Meta key, the
identical keystroke can be generated by typing `ESC` first, and then typing k.
Either process is known as metafying the k key. 


8.2 Readline Interaction

Often during an interactive session you type in a long line of text, only to
notice that the first word on the line is misspelled. The Readline library gives
you a set of commands for manipulating the text as you type it in, allowing you
to just fix your typo, and not forcing you to retype the majority of the line.

Using these editing commands, you move the cursor to the place that needs
correction, and delete or insert the text of the corrections. Then, when you are
satisfied with the line, you simply press RET. You do not have to be at the end
of the line to press RET; the entire line is accepted regardless of the location
of the cursor within the line. 


*sh-shortcut*

C-b
    Move back one character. 
C-f
    Move forward one character. 

*C-a*
    Move to the start of the line. 
*C-e*
    Move to the end of the line. 

*M-f* note: M is ALT
    Move forward a `word`, where a word is composed of letters and digits. 
*M-b*
    Move backward a word. 

DEL or Backspace
    Delete the character to the left of the cursor. 

C-d
    Delete the character underneath the cursor. 

C-l
    Clear the screen, reprinting the current line at the top. 

*C-k*
    Kill the text from the current cursor position to the `end-of-the-line`.

*M-d*
    Kill from the cursor to the `end` of the current word, or, if between words,
to the end of the next word. Word boundaries are the same as those used by M-f.

// note:
// Do NOT work and instead use M-BACK
// M-DEL
//     Kill from the cursor the `start` of the current word, or, if between
//     words, to the start of the previous word. Word boundaries are the same as
//     those used by M-b.

*C-w*
    Kill from the cursor to the previous whitespace. This is different than
    M-DEL because the word boundaries differ.

ESC Backspace
    Kill from the cursor to the previous word boundaries.

// <ex>
// use cut and paste
//
// git init --bare /path/to/repo.git
// git remote add origin /path/to/repo.git
// 
// Notice that the second command uses the same path at the end. Instead of
// typing that path twice, you could copy and paste it from the first command,
// using this sequence of keystrokes:
// 
// Press the up arrow to bring back the previous command.
// 
// Press Ctrl-w to cut the path part: "/path/to/repo.git".
// 
// Press Ctrl-c to cancel the current command.
// 
// Type git remote add origin, and press Ctrl-y to paste the path. 


8.5 Readline vi Mode

While the Readline library does not have a full set of vi editing functions, it
does contain enough to allow simple editing of the line. The Readline vi mode
behaves as specified in the POSIX standard.

In order to switch interactively between emacs and vi editing modes, use the
‘set -o emacs’ and ‘set -o vi’ commands (see The Set Builtin). The Readline
default is emacs mode.

When you enter a line in vi mode, you are already placed in 'insertion' mode, as
if you had typed an "i". Pressing ESC switches you into 'command' mode, where
  you can edit the text of the line with the standard vi movement keys, move to
    previous history lines with "k" and subsequent lines with "j", and so forth. 


={============================================================================
*kt_linux_bash_032* sh-command-search

8.2.5 Searching for Commands in the History

Readline provides commands for searching through the command history
*sh-history* for lines containing a specified string. There are two search
modes: incremental and non-incremental.

To `incremental-search` backward in the history for a particular string, type
`C-r`. Typing C-s searches forward through the history. The characters present
in the value of the isearch-terminators variable are used to terminate an
incremental search. If that variable has not been assigned a value, the `ESC`
and C-J characters will terminate an incremental search. `C-g` will abort an
incremental search and restore the original line. When the search is terminated,
the history entry containing the search string becomes the current line.

To find other matching entries in the history list, type C-r or C-s as
appropriate. This will search backward or forward in the history for the next
entry matching the search string typed so far. 

Any other key sequence bound to a Readline command will terminate the search and
execute that command. For instance, a `RET` will terminate the search and accept
the line, thereby executing the command from the history list. 

A movement command will terminate the search, make the last line found the
current line, and begin editing.

Readline remembers the last incremental search string. If two C-rs are typed
without any intervening characters defining a new search string, any remembered
search string is used.

<ex>
Once you've found the command you have several options:

1. Press RET to run it
2. <C-r> to cycle through other commands that are filterd out with the letters
you've typed.
3. <C-g> to quit the search and back to the command line empty-handed
4. Press ESC to take one and edit the command.


={============================================================================
*kt_linux_bash_032* sh-command-readline-init

8.3.1 Readline Init File Syntax

There are only a few basic constructs allowed in the Readline init file. Blank
lines are ignored.  Lines beginning with a `#' are comments. Lines beginning
with a `$' indicate conditional constructs (see section 8.3.2 Conditional Init
    Constructs). Other lines denote variable settings and key bindings.

Variable Settings
You can modify the run-time behavior of Readline by altering the values of
variables in Readline using the set command within the init file. The syntax is
simple:

    set variable value

Here, for example, is how to change from the default Emacs-like key binding to
use vi line editing commands:

    set editing-mode vi

To set binding to up/down key to history search:

# ~/.inputrc
"\e[A": history-search-backward
"\e[B": history-search-forward

or equivalently,

# ~/.bashrc
bind '"\e[A": history-search-backward'
bind '"\e[B": history-search-forward'

Normally, Up and Down are bound to the Readline functions previous-history and
next-history respectively. I prefer to bind PgUp/PgDn to these functions,
instead of displacing the normal operation of Up/Down.

# ~/.inputrc
"\e[5~": history-search-backward
"\e[6~": history-search-forward

After you modify ~/.inputrc, restart your shell or use Ctrl+X, Ctrl+R to tell it
to re-read ~/.inputrc. By the way, if you're looking for relevant documentation:
Bash uses The GNU Readline Library for the shell prompt and history.


={============================================================================
*kt_linux_bash_100* sh-error-pushd

// errors during build using rpm

+ export PATH=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/bin:/home/kit/viminst/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+ pushd /root/rpmbuild/BUILD
/var/tmp/rpm-tmp.ug26TJ: 34: /var/tmp/rpm-tmp.ug26TJ: pushd: not found
error: Bad exit status from /var/tmp/rpm-tmp.ug26TJ (%build)

WHY?

pushd is bash builtin command and found that script spells:

#!/bin/sh

So means that sh but not bash?

// centos which works
[sky@localhost ~]$ which sh
/bin/sh
[sky@localhost ~]$ sh --version
GNU bash, version 3.2.25(1)-release (i386-redhat-linux-gnu)
Copyright (C) 2005 Free Software Foundation, Inc.

// debian which not works
root@kit-debian:~/rpmbuild/BUILD/sstrip# sh --version
sh: 0: Illegal option --
root@kit-debian:~/rpmbuild/BUILD/sstrip# ls /bin/sh
/bin/sh
root@kit-debian:~/rpmbuild/BUILD/sstrip# ls -al /bin/sh
lrwxrwxrwx 1 root root 4 Nov  8  2014 /bin/sh -> dash


={============================================================================
*kt_linux_bash_100* sh-alias

alias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'


={============================================================================
*kt_linux_bash_100* sh-tip-colour-prompt

Uncomment the line that says force_color_prompt=yes then logout and back in in
the .bashrc.


={============================================================================
*kt_linux_bash_101* sh-tip-nested-level

5.2 Bash Variables

SHLVL
    Incremented by one each time a new instance of Bash is started. This is
    intended to be a count of how deeply your Bash shells are nested.

$ echo $SHLVL

<ex>
keitee@debian-keitee:~/github/kb$ echo $PS1
\[\e]0;\u@\h: \w\a\]${debian_chroot:+($debian_chroot)}\u@\h:\w\$

PS1="\[\e]0;\u@\h: \w\a\]${debian_chroot:+($debian_chroot)}\u@\h:\w\:$SHLVL$"

keitee@debian-keitee:~/github/kb\:2$


={============================================================================
*kt_linux_bash_202* sh-tip-recursion-test

The same script file is used to define tests and routines to check result using
exec trick and communication between parent process that runs the the script and
the subshell that runs each test case.

// test.sh

test_xxx () {
  set local vars

  run.sh args...        
  # run.sh to exec the subprocess with the given cmd so the subprocess will run
  # the cmd.
}

test_xxx_handler() {

}

# when test_handler is zero
if [ -z "$test_handler" ]; then
  for each test case

  # create a subshell(subprocess)
  # run a test case that runs the other script and exec call eventually.
  (
    set env vars to pass to subprocess.

    " note: use env var to pass data to a subprocess
    " note: use a single line to run

    ENVS \
    test_handler="${test_xxx}_handler" \
    test_xxx
   )
   done

else
  xxx_handler     " that checks the results
fi

So the parent process runs the test.sh and drives the whole test. For each test,
   set env vars and create a subprocess that exec and run the given cmd. By
   having cmd as the same test.sh and the test_handler env var set, subprocess
   runs the function in the same script and env var set. So run the different
   path and run check call in this case. So looks like a recursion or callback.

<code>
test_that_generates_url() {

    local app_dir="$scratch_dir/app"
    createApp "$app_dir"

    runBrowser.sh --app "$app_dir" --data "$default_app_data_dir" \
        --app-launch-parameters \
        "launch_context.ui.youview.com" "portal" \
        "some.test.param" "some.test.param.value" \
        "test.param.spaces" "param value with spaces"
}


that_generates_url_handler() {

    echo "that_generates_url_handler() - Got args: '$@'"

    local expected_url="http://youview.tv/test-player?\
    launch_context.ui.youview.com=portal&some.test.param=some.test.param.value&\
    test.param.spaces=param%20value%20with%20spaces"

    paramsContainStringAtPosition "-cache" 0 "$@"
    cacheOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "/tmp/client-cache" 1 "$@"
    cacheValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-cache-size" 2 "$@"
    cacheSizeOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "0" 3 "$@"
    cacheSizeValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-jar" 4 "$@"
    jarOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "$default_cookie_jar_location" 5 "$@"
    jarValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-url" 6 "$@"
    urlOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "$expected_url" 7 "$@"
    urlValueCorrectAtExpectedPosition=$?

    [[ 
      "$cacheOptionCorrectAtExpectedPosition" = 1 &&
      "$cacheValueCorrectAtExpectedPosition" = 1 &&
      "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
      "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
      "$jarOptionCorrectAtExpectedPosition" = "1" && 
      "$jarValueCorrectAtExpectedPosition" = "1" && 
      "$urlOptionCorrectAtExpectedPosition" = "1" && 
      "$urlValueCorrectAtExpectedPosition" = "1" 
    ]] 
}


# In test configuration runBrowser.sh will not actually launch the browser but
# will instead call back into this test script recursively.  We then check the
# environment in the test to see if the browser would have been launched with
# the appropriate environment and parameters.  This is done with pairs of
# functions, one called test_xxx and the other xxx_handler.  test_x functions
# are tests and the xxx_handler functions are what will be run when we
# runBrowser.sh calls us back.
#
# The recursion and the callback are communicated through the environment
# variable $test_handler.
if [ -z "$test_handler" ]; then

        failures=0
        successes=0

        ## foreach test
        for test_fun in $(compgen -A function test_); do

            echo -e -n "\n$test_fun ... "

            setup
            (

                RUN_BROWSER_CMD="$this_script" \
                RUN_BROWSER_DATA_DIR="${datadir:-}" \
                GST_INSPECT_CMD="touch $scratch_dir/gst-inspect" \
                test_handler="${test_fun#test_*}_handler" \
                $test_fun

            ) >> $logfile 2>&1

            status=$?
            if [ $status -eq 0 ]; then
                echo "OK"
                ((++successes))
            else
                echo "FAILURE"
                ((++failures))
            fi

            [[ "$1" == '-v' || $status -ne 0 ]] && { cat $logfile; echo; }
            teardown &> /dev/null

        done

        printf "$invoked_as complete. %i PASSES %i FAILURES\n" \
            $successes $failures

        exit $failures

else

        $test_handler "$@"

fi


={============================================================================
*kt_linux_bash_202* sh-tip-get-filenames-from-comment

#!/bin/bash -x

# Exit immediately if any unexpected error occurs
set -e

#/ DASH: unencrypted, pseudo-live
#/ http://dash.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd
#/
#/ DASH: encrypted, non-live
#/ https://ms3.co.uk/s/Big+Buck+Bunny+DASH+2#http://bbb/avc3/1/2drm_manifest.mpd
#/
#/ HLS: unencrypted, non-live
#/ http://184.72.239.149/vod/smil:bigbuckbunnyiphone.smil/playlist.m3u8
#/
declare -ar urls=($(grep '^#/ http' "$0" | cut -c 4-))

echo "urls 0 : ${urls[0]}"


={============================================================================
*kt_linux_bash_202* sh-tip-search-number-range

Suppose to want to flag up the line from the log only when time difference in
the line exceeds certain limit.

"time diff = 136, " "warn value 300

A=`cat $FILE | strings | grep -n "\-MS_WATCHDOG-.*E_MS_PORT_THREAD_TYPE_CDI_WT.*time diff = " 2>/dev/null |
grep -v 'at time 0' 2>/dev/null |
grep -vE "time diff = [0-9]," 2>/dev/null | 
grep -vE "time diff = [0-9][0-9]," 2>/dev/null | 
grep -vE "time diff = [0-9][0-9][0-9]," 2>/dev/null | 
grep -vE "time diff = [0-9][0-9][0-9][0-9]," 2>/dev/null |
grep -vE "time diff = [0-9][0-9][0-9][0-9][0-9]," 2>/dev/null | head -1`

// when not found
if [ "$A" = "" ]

So filters out time difference in beteen 1 and 5 digits. If there is no line
  which is bigger than 5 digits then no result.

Q: this works: egrep -v "time diff = [0-9]{1,6},"


={============================================================================
*kt_linux_bash_202* sh-code-case

#!/bin/bash

mycat()
(
    F=cat
    echo "$1" | grep -q '.gz$' && F=zcat
    $F $1
)

if [ "$1" = "-v" ]
then
    VERBOSE=1
    FNAME=$2
else
    VERBOSE=0
    FNAME=$1
fi

SIZE=`mycat $FNAME | wc -l`
if [ $SIZE -lt 100000 ]
then
    echo 0 0
    exit 0
fi


TWO_THIRDS=`expr $SIZE \* 2 / 3`
ONE_TENTH=`expr $SIZE / 10`
ONE_HUNDREDTH=`expr $SIZE / 100`

// 3% and 0.3%
THREE_PERCENT=`expr $SIZE / 33`
POINTTHREE_PERCENT=`expr $SIZE / 333`

// take 1/10 from 2/3 of file to find out the line seem most from that
// section.
//
// NDS: ^0946711584.182091 !WARN   -PCATC                   < p:0x0000020e P:APP t:0x2dfa4520 T:no name M:value_list_man_clnt.c F:PCAT_VIEW_VL_LogAllocVL L:2816 > \
//  VL hdl=0x11ac1 allocated but not freed for view PLANNER29 hdl=0x7000020, " "flags=0, ObType=0, num_rows=1, total_size=1242, curpos=0
//
// $ cat LOGlastrun_realtime | head -1130954 | tail -169643 | grep "^NDS" | cut -d'!' -f2- | grep -vE "^ENTRY|^EXIT|^INFO" | head -2
// WARN   -PCATC                   < p:0x0000020e P:APP t:0x010a5520 T:PLANNER_MAIN M:value_list_man_clnt.c F:PCAT_VIEW_VL_LogAllocVL L:2816 > \
//  VL hdl=0x17794 allocated but not freed for view PLANNER29 hdl=0x7000020, " "flags=0, ObType=0, num_rows=1, total_size=1193, curpos=0
//
// remove matches to sort only for lines from mw and excludes entry/exit/info.
//
// $ cat LOGlastrun_realtime | head -1130954 | tail -169643 | grep "^NDS" | cut -d'!' -f2- | grep -vE "^ENTRY|^EXIT|^INFO" | sed -e's/[0-9a-fA-F]//g' | head -2
// WRN   -PT                       < p:x P:PP t:x T:PLNNR_MIN M:vlu_list_mn_lnt. :PT_VIW_VL_LoglloVL L: > \
//  VL hl=x llot ut not r or viw PLNNR hl=x, " "lgs=, OTyp=, num_rows=, totl_siz=, urpos=
//
// gets one line which seen most in the section
//
// $ cat LOGlastrun_realtime | head -1130954 | tail -169643 | grep "^NDS" | cut -d'!' -f2- | grep -vE "^ENTRY|^EXIT|^INFO" | sed -e's/[0-9a-fA-F]//g' | sort | uniq -c | sort -n | tail -1
// + POSSIBLE='  43546 WRN   -PT                   < p:x P:PP t:x T:no nm M:vlu_list_mn_lnt. :PT_VIW_VL_LoglloVL L: > \
//  VL hl=x llot ut not r or viw PLNNR hl=x, " "lgs=, OTyp=, num_rows=, totl_siz=, urpos= '
//
// note: "sed -e's/[0-9a-fA-F]//g'"
// 
// The trick is to remove numbers and some chars to make log lines "netural"
// and to be sorted as the same.
// 
// !WARN   -PCATC < p:0x0000020e P:APP t:0x2dfa4520 T:no name M:value_list_man_clnt.c F:PCAT_VIEW_VL_LogAllocVL L:2816 > VL hdl=0x11ac1 allocated but not freed for view PLANNER29 hdl=0x7000020, " "flags=0, ObType=0, num_rows=1, total_size=1242, curpos=0
// !WARN   -PCATC < p:0x0000020e P:APP t:0x2dfa4520 T:no name M:value_list_man_clnt.c F:PCAT_VIEW_VL_LogAllocVL L:2816 > VL hdl=0x76d4 allocated but not freed for view PLANNER17 hdl=0x7000014, " "flags=0, ObType=0, num_rows=1, total_size=712, curpos=0
// !WARN   -PCATC < p:0x0000020e P:APP t:0x2dfa4520 T:no name M:value_list_man_clnt.c F:PCAT_VIEW_VL_LogAllocVL L:2816 > VL hdl=0x115a6 allocated but not freed for view PLANNER17 hdl=0x7000014, " "flags=0, ObType=0, num_rows=1, total_size=703, curpos=0
//
// WRN   -PT                   < p:x P:PP t:x T:no nm M:vlu_list_mn_lnt. :PT_VIW_VL_LoglloVL L: > VL hl=x llot ut not r or viw PLNNR hl=x, " "lgs=, OTyp=, num_rows=, totl_siz=, urpos= '
//
// note: "sort | uniq -c | sort -n | tail -1"
//
// The trick is to sort out lines by `occurrences` and take a last line which
// is the line seem `most`

POSSIBLE=`mycat $FNAME 2>/dev/null | head -$TWO_THIRDS | tail -$ONE_TENTH | grep "^NDS" | cut -d'!' -f2- | grep -vE "^ENTRY|^EXIT|^INFO" | sed -e's/[0-9a-fA-F]//g' | sort | uniq -c | sort -n | tail -1`
POSS_COUNT=`echo $POSSIBLE | cut -d' ' -f1`
TIME_AT_START=`mycat $FNAME 2>/dev/null | head -$TWO_THIRDS | grep "^NDS" | tail -$ONE_TENTH | head -1 | cut -d'.' -f1 | cut -d'^' -f2`
TIME_AT_END=`mycat $FNAME 2>/dev/null | head -$TWO_THIRDS | grep "^NDS" | tail -1 | cut -d'.' -f1 | cut -d'^' -f2`
TOTAL_TIME=`expr $TIME_AT_END - $TIME_AT_START`
SECONDS_BETWEEN_MESSAGES=`expr $TOTAL_TIME / $POSS_COUNT`

if [ $SECONDS_BETWEEN_MESSAGES -gt 3 ]
then
    POSS_COUNT=0
fi

if [ $POSS_COUNT -gt $POINTTHREE_PERCENT ]
then
    POSS_HIPPO=`echo "$POSSIBLE" | sed -e's/^[ ]*//' | cut -d' ' -f2- | sed -e's/\[/\\\[/g' | sed -e's/\]/\\\]/g'`

    // `the line to match`
    // + POSSIBLE='  43546 WRN   -PT < p:x P:PP t:x T:no nm M:vlu_list_mn_lnt. :PT_VIW_VL_LoglloVL L: > VL hl=x llot ut not r or viw PLNNR hl=x, " "lgs=, OTyp=, num_rows=, totl_siz=, urpos= '
    // + POSS_HIPPO=      'WRN   -PT < p:x P:PP t:x T:no nm M:vlu_list_mn_lnt. :PT_VIW_VL_LoglloVL L: > VL hl=x llot ut not r or viw PLNNR hl=x, " "lgs=, OTyp=, num_rows=, totl_siz=, urpos= '

    // `how often seen?` Q: possible that the most seen line is different from
    // `the line to match`?
    HIPPO_COUNT=`mycat $FNAME 2>/dev/null | grep "^NDS" | cut -d'!' -f2- | sed -e's/[0-9a-fA-F]//g' | sort | uniq -c | sort -n | tail -1 | while read a b;do echo $a;done`
    if [ $HIPPO_COUNT -lt $THREE_PERCENT ]
    then
        echo 0 0
        exit 0
    fi
    if [ $VERBOSE -eq 1 ]
    then
        // gets `the first line` from a whole file which matches to `the line to match`
        //
        // =`mycat $FNAME 2>/dev/null | cut -d'!' -f2- | sed -e's/[0-9a-fA-F]//g' | grep -n "$POSS_HIPPO"`
        // shows all lines which matches to POS_HIPPO from a whole file.
        POS_OF_MESS=`mycat $FNAME 2>/dev/null | cut -d'!' -f2- | sed -e's/[0-9a-fA-F]//g' | grep -n "$POSS_HIPPO" | cut -d':' -f1 | head -1`
        MESSAGE=`mycat $FNAME 2>/dev/null | head -$POS_OF_MESS | tail -1`

        echo "$POS_OF_MESS Message appearing too often ($HIPPO_COUNT times/$SIZE lines): ($MESSAGE)"

        // + echo '705549 Message appearing too often (207618 times/1696431 lines): 
        // (NDS: ^0946711584.182091 !WARN   -PCATC                        < p:0x0000020e P:APP t:0x2dfa4520 T:no name M:value_list_man_clnt.c F:P
        // CAT_VIEW_VL_LogAllocVL L:2816 > VL hdl=0x11ac1 allocated but not freed for view PLANNER29 hdl=0x7000020, " "flags=0, ObType=0, num_rows=1, total_size=1242, curpos=0 )'

    else
        echo $TWO_THIRDS "Too many instances of" $POSSIBLE
    fi
    exit 1
fi
echo 0 0
exit 0


={============================================================================
*kt_linux_bash_202* sh-code-case-use-config

gets a setting from a config file

// .cfg
MIRRORMACHINE=theyard
MIRRORDIR=/home/si_logs/mirrored/upload
LOCALUSER=si_logs
MIRRORUSER=si_logs_mirror
TXDIR=/home/si_logs/tx
TRANSLATIONDIR=/home/si_logs/translation
SORTFILE=/home/si_logs/mac_list
RAWUPLOADLIST=/home/si_logs/raw_upload_list


// xx.sh

CFG=$D/.cfg

get_val()
{
    if [ -f $CFG ]
    then
        grep "^$1=" $CFG | head -1 | cut -d'=' -f2-
    fi
}

# get defaults
LOGS_DIR=``get_val TRANSLATIONDIR``


={============================================================================
*kt_linux_bash_202* sh-code-case-use-function-command

// .bashrc

t()
{
    cd $HOME/translation
    L=`echo *$1 | grep -v '*' | wc -w`
    echo "Match count $L"
    if [ $L -eq 0 ]
    then
        echo "No such directory found: $1"
    else
        if [ $L -gt 1 ]
        then
            echo "Multiple directories found - specify further:"
            ls -ld *$1
        else
            echo "Changing to \"$1\""
            cd *$1
        fi
    fi
}


// command line

when no args are given, "echo *$1" yields all files in the current directory
and runs "ls -ld <list>" Q: *$1?
$ t

$ t 123213


={============================================================================
*kt_linux_bash_202* sh-code-check-file-size

$ stat --print="%s\n" run.sh

$ wc -c www/log
808737 www/log

$ wc -c < www/log
808737

#flimit=900000
flimit=800000

fsize=$(wc -c < www/log)
if [ $fsize -ge $flimit ]; then
        echo "$fsize is over $flimit"
else
        echo "$fsize is not over $flimit..."
fi


={============================================================================
*kt_linux_bash_202* sh-code-write-script

      mkdir -p "output/nds-drv/`dirname "$SPK_SCRIPT_NDS_DRIVERS"`"
      {
        echo '#!/bin/sh'
        echo 'insmod /lib/modules/2.6.18.8/mhxnvramfs.ko'
        echo 'insmod /lib/modules/2.6.18.8/xtvfs.ko'
        echo 'rm -f /lib/modules/2.6.18.8/mhxnvramfs.ko'
        echo 'rm -f /lib/modules/2.6.18.8/xtvfs.ko'
      } >"output/nds-drv/$SPK_SCRIPT_NDS_DRIVERS"
      chmod +x "output/nds-drv/$SPK_SCRIPT_NDS_DRIVERS"
      tar -cjvf "output/nds-drv-${spk_platform}-${variant}.tar.bz2" -C output/nds-drv .


# ============================================================================
#{
={============================================================================
*kt_linux_tool_001* tool-help

info coreutils 'cat invocation'


={============================================================================
*kt_linux_tool_001* tool-md5sum tool-shasum

md5sum - compute and check MD5 message digest

Another way let say you have more files to verify, you can create a text file,
such as md5sum.txt

283158c7da8c0ada74502794fa8745eb  ubuntu-6.10-alternate-amd64.iso
549ef19097b10ac9237c08f6dc6084c6  ubuntu-6.10-alternate-i386.iso
5717dd795bfd74edc2e9e81d37394349  ubuntu-6.10-alternate-powerpc.iso
99c3a849f6e9a0d143f057433c7f4d84  ubuntu-6.10-desktop-amd64.iso
b950a4d7cf3151e5f213843e2ad77fe3  ubuntu-6.10-desktop-i386.iso
a3494ff33a3e5db83669df5268850a01  ubuntu-6.10-desktop-powerpc.iso
2f44a48a9f5b4f1dff36b63fc2115f40  ubuntu-6.10-server-amd64.iso
cd6c09ff8f9c72a19d0c3dced4b31b3a  ubuntu-6.10-server-i386.iso
6f165f915c356264ecf56232c2abb7b5  ubuntu-6.10-server-powerpc.iso
4971edddbfc667e0effbc0f6b4f7e7e0  ubuntu-6.10-server-sparc.iso

First column is the md5 string and second column is the location of the file.
To check all them from file, do this:

md5sum -c md5sum.txt

SHA1SUM(1)

NAME
       sha1sum - compute and check SHA1 message digest

DESCRIPTION
       Print or check SHA1 (160-bit) checksums.  With no FILE, or when FILE is
       -, read standard input.


={============================================================================
*kt_linux_tool_004* dmesg

To show the boot log and can see the kernel version.


={============================================================================
*kt_linux_tool_005* tool-uname linux-check-linux

       -n, --nodename
              print the network node hostname

       -a, --all
              print all information, in the following order, except omit -p
              and -i if unknown:

$ uname -a
Linux debian-keitee 3.2.0-4-486 #1 Debian 3.2.63-2+deb7u2 i686 GNU/Linux

// shows the summary
lscpu

// shows all processors
cat /proc/cpuinfo


{linux-check-distribution} *32bit-64bit*
The machine hardware name lists whether your system is 32-bit (“i686” or
“i386”) or 64-bit (“x86_64”).

cat /proc/version

lsb_release -a

// ubuntu
$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=12.04
DISTRIB_CODENAME=precise
DISTRIB_DESCRIPTION="Ubuntu 12.04.1 LTS"

// debian
$ cat /etc/os-release 
PRETTY_NAME="Debian GNU/Linux 7 (wheezy)"
NAME="Debian GNU/Linux"
VERSION_ID="7"
VERSION="7 (wheezy)"
ID=debian
ANSI_COLOR="1;31"
HOME_URL="http://www.debian.org/"
SUPPORT_URL="http://www.debian.org/support/"
BUG_REPORT_URL="http://bugs.debian.org/"


<check-kernel>
kyoupark@kt-office-debian:~/git/kb/asan$ hostnamectl
   Static hostname: kt-office-debian
         Icon name: computer-desktop
           Chassis: desktop
        Machine ID: 20f41c42c89c4de89e5080359ae65a69
           Boot ID: a4986fe9bbae4e47b3a14744b3186e6d
  Operating System: Debian GNU/Linux 9 (stretch)
            Kernel: Linux 4.9.0-7-686-pae
      Architecture: x86


<check-kernel-code>

from buildroot

# Check the specified kernel headers version actually matches the
# version in the toolchain.
#
# $1: sysroot directory
# $2: kernel version string, in the form: X.Y
#
check_kernel_headers_version = \
	if ! support/scripts/check-kernel-headers.sh $(1) $(2); then \
		exit 1; \
	fi

# $1: /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/ 
# $2: 2.6


# support/scripts/check-kernel-headers.sh
#!/bin/sh

SYSROOT="${1}"
# Make sure we have enough version components
HDR_VER="${2}.0.0"

HDR_M="${HDR_VER%%.*}"
HDR_V="${HDR_VER#*.}"
HDR_m="${HDR_V%%.*}"

EXEC="$(mktemp -t check-headers.XXXXXX)"

note:
Use the fact that mktemp creates a file with permission "rw" and gcc output a
file with default permission. If fails to compile, then cannot run a file from
mktemp and set a error code other than true, 0. This is the same when a binary
returns a false.


-rw------- 1 kyoupark ccusers 0 Jan 16 19:29 /tmp/check-headers.ZDoaQ
-rwxr-xr-x 1 kyoupark ccusers 4948 Jan 16 19:30 a.out

# We do not want to account for the patch-level, since headers are
# not supposed to change for different patchlevels, so we mask it out.
# This only applies to kernels >= 3.0, but those are the only one
# we actually care about; we treat all 2.6.x kernels equally.
${HOSTCC} -imacros "${SYSROOT}/usr/include/linux/version.h" \
          -x c -o "${EXEC}" - <<_EOF_
#include <stdio.h>
#include <stdlib.h>

int main(int argc __attribute__((unused)),
         char** argv __attribute__((unused)))
{
    if((LINUX_VERSION_CODE & ~0xFF)
        != KERNEL_VERSION(${HDR_M},${HDR_m},0))
    {
        printf("Incorrect selection of kernel headers: ");
        printf("expected %d.%d.x, got %d.%d.x\n", ${HDR_M}, ${HDR_m},
               ((LINUX_VERSION_CODE>>16) & 0xFF),
               ((LINUX_VERSION_CODE>>8) & 0xFF));
        return 1;
    }
    return 0;
}
_EOF_

"${EXEC}"
ret=${?}
rm -f "${EXEC}"
exit ${ret}


gcc -imacros "/usr/include/linux/version.h" -x c sample.c

#include <stdio.h>
#include <stdlib.h>

int HDR_M=2;
int HDR_m=6;

int main(int argc __attribute__((unused)),
         char** argv __attribute__((unused)))
{
    if((LINUX_VERSION_CODE & ~0xFF)
        != KERNEL_VERSION(HDR_M,HDR_m,0))
    {
        printf("Incorrect selection of kernel headers: ");
        printf("expected %d.%d.x, got %d.%d.x\n", HDR_M, HDR_m,
               ((LINUX_VERSION_CODE>>16) & 0xFF),
               ((LINUX_VERSION_CODE>>8) & 0xFF));
        printf("return 1.\n");
        return 1;
    }
    printf("return 0.\n");
    return 0;
}


={============================================================================
*kt_linux_tool_006* tool-cp

       -p     same as --preserve=mode,ownership,timestamps

{copy-symbolic}
To copy symbolic links as well, cp -r don't work and use

       -L, --dereference
              always follow symbolic links in SOURCE

       -d     same as --no-dereference --preserve=links

       -P, --no-dereference
              never follow symbolic links in SOURCE

       --preserve[=ATTR_LIST]
              preserve the specified attributes (default:
                  mode,ownership,timestamps), if possible additional
              attributes: context, links, xattr, all


{careful}
cp -r /xx/dir/ /dst/ 

This cause that files copied under /dst/dir/. If want to copy only files under
dir then use

cp -r /xx/dir/* /dst/

<error>
cp: cannot create regular file

this happens when file has all read permission and okay to copy it to
somewhere first time. shows this error when do it again. Since it has only
read permission cannot copy in the second.

-r--r--r-- 1 kyoupark ccusers 1976 Jun 19  2014 /home/

$ cp -afv 
`/shmem.o' -> `/shmem.o'
removed `/home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/nds/src/shmem/shmem.o'

see `removed` message when use -f


={============================================================================
*kt_linux_tool_007* mkdir

{p-option}
Use to make parent directories as well.


={============================================================================
*kt_linux_tool_008* tool-strings

To find string in the library.

keitee@linux:~/share/temp> strings sec_getba.a | grep GCC
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6


={============================================================================
*kt_linux_tool_009* tool-head

       Print  the  first 10 lines of each FILE to standard output.  With more
       than one FILE, precede each with a header giving the file name.  With
       no FILE, or when FILE is -, read stan- dard input.

       -n, --lines=[-]K
              print the first K lines instead of the first 10; with the
              leading '-', print all but the last K lines of each file

       -c, --bytes=[-]K
              print the first K bytes of each file; with the leading ‘-’,
              print all but the last K bytes of each file

<get-matched-line>
When POS_OF_MESS is a line no to see.

MESSAGE=`mycat $FNAME 2>/dev/null | head -$POS_OF_MESS | tail -1`


<to-check-if-first-n-bytes-are-null>
PCAT.DB has data but PCAT.DBJ has null bytes in first few bytes as can see:

$ hexdump -C -n 10 PCAT.DB
00000000  53 51 4c 69 74 65 20 66  6f 72                    |SQLite for|
0000000a
$ hexdump -C -n 10 PCAT.DBJ.org
00000000  00 00 00 00 00 00 00 00  00 00                    |..........|
0000000a

$ head -c 10 PCAT.DB
SQLite fork$ head -c 10 PCAT.DBJ
$

Both `head command` returns($?) 0 so cannot use this to say if it has first
few null bytes. Then how?

$ head -c 10 PCAT.DB | read -n 1      # expect $? is 0 since read is okay
$ head -c 10 PCAT.DBJ | read -n 1     # expect $? is 1 since read is fail (no input)

However, both `read command` retuns 0. WTF? The reason is that head outputs
"10" long even if these are null. 


The fix:

# when there are first null bytes, tr deletes null bytes, so no input to read
# command. Hence read fails and return 1

$ head -c 10 PCAT.DBJ | tr -d '\0' | read -n 1
$ echo $?
1

# when there are no first null bytes and read returns okay.

$ head -c 10 PCAT.DB | tr -d '\0' | read -n 1
$ echo $?
0


={============================================================================
*kt_linux_tool_002* tool-cut

       -b, --bytes=LIST
              select only these bytes

       -c, --characters=LIST
              select only these characters

       -d, --delimiter=DELIM
              use DELIM instead of TAB for field delimiter
              note: `instead of TAB` so TAB by default

       -f, --fields=LIST
              select only these fields;  also print any line that contains no
              delimiter character, unless the -s option is specified

       Use one, and only one of -b, -c or -f.  Each LIST is made up of one
       `range`, or many ranges separated by commas.  Selected input is written in
       the same order that it is read, and is written exactly once.  Each range
       is one of:

       N      N'th byte, character or field, counted from 1

       N-     from N'th byte, character or field, to end of line

       N-M    from N'th to M'th (included) byte, character or field

       -M     from first to M'th (included) byte, character or field

# input file
WIFI_SSID="SKY0F227"

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d=`
# this lead to WIFI_SSID='"SKY0F227"' and cannot use it as a var

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d \"`
# this fixes the problem.

<ex>
~/source/DEVARCH$ git symbolic-ref HEAD
refs/heads/topic
123456789012
~/source/DEVARCH$ git symbolic-ref HEAD | cut -b 12-
topic

<ex>
$ gl gst_clock_id_wait_async | cut -d' ' -f 3-4 | xargs echo

<ex>
Select first 3 chars

FIRST3=``echo $2 | cut -c-3``


={============================================================================
*kt_linux_tool_009* tool-sort

See the unix power tool 22.6 for more. -u remove duplicates and sort against
field [4,7].

sort -u -k 4,7

http://www.gnu.org/software/coreutils/manual/html_node/sort-invocation.html

--key=pos1[,pos2]

Specify a sort field that consists of the part of the line between pos1 and pos2
(or the end of the line, if pos2 is omitted), inclusive.  Each `pos` has the form
'f[.c][opts]', where f is the number of the field to use, and c is the number of
the first character from the beginning of the field. 

Fields and character positions are numbered `starting with 1`; a character
position of zero in pos2 indicates the field's last character. 

If '.c' is omitted from pos1, it defaults to 1 (the beginning of the field);
if omitted from pos2, it defaults to 0 (the end of the field). opts are
ordering options, allowing individual keys to be sorted according to different
rules; see below for details. Keys can span multiple fields.  Example: To sort
on the second field, use --key=2,2 (-k 2,2).

See below for more notes on keys and more examples. See also the --debug option
to help determine the part of the line being used in the sort.

To sort
NDS: ^0946684946.752246 !ERROR -aem          < M:aem_list.c F:AEM_ListGetApplication L:01808 > Can't find
-1-- ----------2------- ---3-- -4--         -5 ------6----- ---------------7-------- ---8--- 9 -10--

sort -u -k 4,10 ndsfusion.test > ndsfusion.dic 


-t separator --field-separator=separator

Use character separator as the field separator when finding the sort keys in
each line. By default, fields are separated by the `empty string` between a
non-blank character and a blank character. By default a blank is a space or a
tab, but the LC_CTYPE locale can change this.

That is, given the input line ' foo bar', sort breaks it into fields ' foo' and
' bar'. The field separator is not considered to be part of either the field
preceding or the field following, so with 'sort -t " "' the same input line has
three fields: an empty field, 'foo', and 'bar'. However, fields that extend to
the end of the line, as -k 2, or fields consisting of a range, as -k 2,3, retain
the field separators present between the endpoints of the range.  To specify
ASCII NUL as the field separator, use the two-character string '\0', e.g., 'sort
-t '\0''. 


mh5a_variable.c
mh5b_variable.c

sort -t _ -k 2,2

Use this to list out member functions from sources

egrep -r 'g_pAppWindow->' . | sort -t - -k 2,3 >> in.txt

<ex>
$ df -h | sort -rnk 5


<options>
       -r, --reverse
              reverse the result of comparisons

       -n, --numeric-sort
              compare according to string numerical value

       -t, --field-separator=SEP
              use SEP instead of non-blank to blank transition


={============================================================================
*kt_linux_tool_009* tool-uniq

NAME
       uniq - report or omit repeated lines

SYNOPSIS
       uniq [OPTION]... [INPUT [OUTPUT]]

DESCRIPTION
       Filter `adjacent` matching lines from INPUT (or standard input), writing to
       OUTPUT (or standard output).

       With no options, matching lines are merged to the first occurrence.

       Mandatory arguments to long options are mandatory for short options too.

       -c, --count
              prefix lines by `the number of occurrences`


={============================================================================
*kt_linux_tool_011* tool-find

SYNOPSIS
       find [-H] [-L] [-P] [-D debugopts] [-Olevel] [path...] [expression]

{find-mtime}
24(one day) hours based so -mtime 3 means that 72 hours(three days) from the
current time and -mtime -3 means 72 before. 

        72              96              120
    ----*---------------*---------------*--------------
        (               ](              ]

To specify between 72 and 96, -mtime 3 and after 96, -mtime +3. If want to find
file or dirs changed most recently, use -mtime 0 or 1.

$ find . -mtime -3 -name


<print0>
-print0

print the full file name on the standard output, followed by a null character
(instead of the newline character that -print uses). This allows file names that
contain newlines or other types of white space to be correctly interpreted by
programs that process the find output. This option corresponds to the -0 option
of xargs. note: see xargs

$ find . -name CMakeLists.txt -print0
./CMakeLists.txt./Source/CMakeLists.txt./Source/cmake/gtest/CMakeLists.txt./Sou\
rce/WebKit2/CMakeLists.txt./Source/WebKit2/UIProcess/efl/po_tizen/CMakeLists.tx\
t./Source/WebKit/CMakeLists.txt./Source/WebKit/efl/DefaultTheme/CMakeLists.txt.\
/Source/JavaScriptCore/CMakeLists.txt./Source/JavaScriptCore/shell/CMakeLists.t\
xt./Source/WebCore/CMakeLists.txt./Source/ThirdParty/gtest/CMakeLists.txt./Sour\
ce/CMakeLists.txtkeitee.park@rockford /home/tbernard/Git/vdTizen/webkit


$ find folder1 folder2 -name "*.txt" -print0 | xargs -0 myCommand


<print-fomrmat>
       -printf format
              True; print format on the standard output, interpreting '\'
              escapes and '%' directives.  Field widths and precisions can be
              specified as with the 'printf' C function.  Please note  that
              many  of  the fields  are  printed  as  %s  rather  than  %d, and
              this may mean that flags don't work as you might expect.  This
              also means that the '-' flag does work (it forces fields to be
                      left-aligned).  Unlike -print, -printf does not add a
              newline at the end of the string.  The escapes and directives are:


       %Ak    File's last access time in the format specified by k, which
              is either '@' or a directive for the C 'strftime' function.  The
              possible values for k are listed below; some of them  might  not
              be available on all systems, due to differences in 'strftime'
              between systems.


            @ seconds since Jan. 1, 1970, 00:00 GMT, with fractional part.

            Time fields:

            c locale's date and time (Sat Nov 04 12:02:33 EST 1989). The format
              is the same as for ctime(3) and so to preserve compatibility with
              that format, there is no fractional part in the seconds field.


        %p     File's name.

        %Tk    File's last modification time in the format specified by k, which
               is the same as for %A.


<case>
Want to see test failures made 20 days before. Use find since there will be a
file if there were failures. However, find prints out ascending order, from old
to newer. Want to see from the latest failure. Two problems: find output don't
have times and don't have options to change output order.

$ find . -mtime -20 -name Nickel.System.GStreamer.log -printf '%T@ %Tc %p\n' | sort -r


{find-dir}
find . -type d -name [dir]


{find-to-get-the-number-of-files}
find -type f | wc -l

find . -maxdepth 1 -type l


OPTIONS

       -L     Follow  symbolic  links.   When find examines or prints
       information about files, the information used shall be taken from the
       properties of the file to which the link points, not from the link
       itself (unless it is a broken symbolic link or find is unable to
           examine the file to which the link points).  Use of this option
       implies -noleaf.  If you later  use  the  -P  option, -noleaf  will
       still  be  in  effect.   If -L is in effect and find discovers a
       symbolic link to a subdirectory during its search, the subdirectory
       pointed to by the symbolic link will be searched.

              When the -L option is in effect, the -type predicate will always
              match against the type of the file that a symbolic link points
              to rather than the link itself (unless the symbolic link is
                  broken).  Using -L causes the -lname and -ilname predicates
              always to return false.


EXPRESSIONS
       The  expression  is  made  up  of  `options` (which affect overall
           operation rather than the processing of a specific file, and always
           return true), `tests` (which return a true or false value), and
       `actions` (which have side effects and return a true or false value),
       all `separated by operators.` `-and is assumed` where the operator is
       omitted.

       If the expression contains no actions other than -prune, -print is
       performed on all files for which the expression is true.

   OPTIONS
       All options always return true.  Except for -daystart, -follow and
       -regextype, the options affect all tests, including tests specified
       before the option.  This is because the  options  are  pro‐ cessed
       when  the  command  line  is parsed, while the tests don't do anything
       until files are examined.  The -daystart, -follow and -regextype
       options are different in this respect, and have an effect only on tests
       which appear later in the command line.  Therefore, for clarity, it is
       best to place them at the beginning of the expression.  A warning is
       issued if you don't do this.

   OPERATORS
       Listed in order of decreasing precedence:

       ( expr )
              `Force precedence`  Since parentheses are special to the shell,
              you will normally `need to quote them.`  Many of the examples in
              this manual page use backslashes for this purpose: `\(...\)'
              instead of `(...)'.

       ! expr True if expr is false.  This character will also usually need
       protection from interpretation by the shell.

       -not expr
              Same as ! expr, but not POSIX compliant.

       expr1 expr2
              Two expressions in a row are taken to be joined with an implied
              "and"; expr2 is not evaluated if expr1 is false.

       expr1 -a expr2
              Same as expr1 expr2.

       expr1 -and expr2
              Same as expr1 expr2, but not POSIX compliant.

       expr1 -o expr2
              Or; `expr2 is not evaluated if expr1 is true.`

       expr1 -or expr2
              Same as expr1 -o expr2, but not POSIX compliant.

       expr1 , expr2
              List; both expr1 and expr2 are always evaluated.  The value of
              expr1 is discarded; the value of the list is the value of expr2.
              The comma operator can be useful for searching for  several
              different types of thing, but traversing the filesystem
              hierarchy only once.  The -fprintf action can be used to list
              the various matched items into several different output files.


   ACTIONS
       *tool-find-delete*
       -delete
              Delete files; true if removal succeeded.  If the removal failed,
              an error message is issued.  If -delete fails, find's exit
              status will be nonzero (when  it  eventually  exits).   Use  of
              -delete automatically turns on the `-depth' option.

              Warnings:  Don't forget that the find command line is evaluated
              as an expression, so putting -delete first will make find try to
              delete everything below the starting points you specified.  When
              testing a find command line that you later intend to use with
              -delete, you should explicitly specify -depth in order to avoid
              later surprises.  Because -delete  implies  -depth,  you cannot
              usefully use -prune and -delete together.

       -exec command ;
              Execute  command;  true  if 0 status is returned.  All following
              arguments to find are taken to be arguments to the command until
              an argument consisting of `;' is encountered.  The string `{}'
              is replaced by the current file name being processed everywhere
              it occurs in the arguments to the command, not just in arguments
              where it is alone, as in some versions of find.  Both of  these
              constructions might need to be escaped (with a `\') or quoted to
              protect them from expansion by the shell.  See the EXAMPLES
              section for examples of the use of the -exec option.  The
              specified command is run once for each matched file.  The
              command is executed in the starting directory.   There are
              unavoidable security problems surrounding use of the -exec
              action; you should use the -execdir option instead.

        <ex>
        {} where the filename will be inserted. Add \; at the end of the
        command to complete the required syntax. note: there must be a space
        after {}
        
        $ find . -name CMakeLists.txt -exec egrep PROJECT {} \;
        
        To run dirtags script for each directory:
        
        $ find * -type d -exec dirtags {} \;

        rm all except UPLOAD.TGZ

        find ./* ! -name "UPLOAD.TGZ" -exec rm -rf {} \;


   TESTS
       Some  tests, for example -newerXY and -samefile, allow comparison
       between the file currently being examined and some reference file
       specified on the command line.  When these tests are used, the
       interpretation of the reference file is determined by the options -H,
       -L and -P and any previous -follow, but the reference file is only
       examined once, at the time the command  line  is  parsed.  If the
       reference file cannot be examined (for example, the stat(2) system call
       fails for it), an error message is issued, and find exits with a
       nonzero status.

       -executable
              Matches  files  which  are executable and directories which are
              searchable (in a file name resolution sense).  This takes into
              account access control lists and other permissions artefacts
              which the -perm test ignores.  This test makes use of the
              access(2) system call, and so can be fooled by NFS servers which
              do UID mapping (or root-squashing), since many systems implement
              access(2)  in  the  client's  kernel and so cannot make use of
              the UID mapping information held on the server.  Because this
              test is based only on the result of the access(2) system call,
              there is no guarantee that a file for which this test succeeds
              can actually be executed.

       -type c
              File is of type c:

              b      block (buffered) special

              c      character (unbuffered) special

              d      directory

              p      named pipe (FIFO)

              f      regular file

              l      symbolic link; this is never true if the -L option or the
              -follow option is in effect, unless the symbolic link is broken.
              If you want to search for symbolic links when -L  is  in effect,
              use -xtype.

              s      socket

              D      door (Solaris)


      <find-ignore>
       -path pattern
              File name matches shell pattern `pattern`.  The metacharacters do
              not treat `/' or `.' specially; so, for example,

                        find . -path "./sr*sc"

              will print an entry for a directory called `./src/misc' (if one
              exists).  
              
              To ignore a whole directory tree, use -prune rather than
              checking every file in the tree.  For example,  to  skip the
              directory `src/emacs' and all files and directories under it,
              and print the names of the other files found, do something like
              this:

                        find . -path ./src/emacs -prune -o -print

              Note  that  the  pattern match test applies to the whole file
              name, starting from one of the start points named on the command
              line.  It would only make sense to use an absolute path name
              here if the relevant start point is also an absolute path.  This
              means that this command will never match anything:

                        find bar -path /foo/bar/myfile -print

              Find compares the -path argument with the concatenation of a
              directory name and the base name of the file it's examining.
              Since the concatenation will never end with a slash, -path argu‐
              ments  ending in a slash will match nothing (except perhaps a
              start point specified on the command line).  The predicate -path
              is also supported by HP-UX find and will be in a forthcoming
              version of the POSIX standard.

              <ex>
              find / \( -size +50 -o -mtime -3 \) -print
              find -name *.[ch]

              // find c file and h file
              find /my/project/dir -name '*.c' -o -name '*.h'

              // returns nothing
              find /my/project/dir -name '*.c' -and -name '*.h'

              // exclude test, build, mock subdirectories
              find ./CMS_PLATFORM_SERVICES -type d \( -path "*/test" -o -path "*/build" -o -path "*/mock" \) -prune -o -print

find . -mtime -100000000 -type d -name SI-\* -exec ls -dl {} \;


={============================================================================
*kt_linux_tool_011* tool-which

DESCRIPTION
       which  returns  the  pathnames of the files (or links) which would be
       executed in the current environment, had its arguments been given as
       commands in a strictly POSIX-conformant shell.  It does this by
       searching the PATH for executable files matching the names of the
       arguments. It does not follow symbolic links.

EXIT STATUS
       0      if all specified commands are found and executable
       1      if one or more specified commands is nonexistent or not executable
       2      if an invalid option is specified

<ex>
      which 7za &> /dev/null
      if [ $? -eq 0 ]; then   # when 7za exists
          7za e ${SPK_TARBALL} -so | tar -C ${SPK_TARBALL_DIR} -vxf -
      else                    # when 7za do not exists
          tar -C ${SPK_TARBALL_DIR} -jxvf ${SPK_TARBALL}
      fi


={============================================================================
*kt_linux_tool_012* make a empty file without touch

can use 'touch' but when busybox do not support touch, can use following to
make a empty file or to reset a file.

cat /dev/null > file


={============================================================================
*kt_linux_tool_013* tool-xargs

--null
-0     

Input items are terminated by a null character instead of by whitespace, and the
quotes and backslash are not special (every character is taken literally).
Disables the end of file string, which is treated like any other argument.
Useful when input items might contain white space, quote marks, or  backslashes.

The GNU find -print0 option produces input suitable for this mode. note: see
find-print0

-I replace-str

Replace occurrences of replace-str in the initial-arguments with names read from
standard input.  Also, unquoted  blanks  do not terminate input items; instead
the separator is the newline character. Implies -x and -L 1.

$ ls | grep Nov_ | xargs -I{} find {} -name LOGlastrun_realtime -print \
| xargs egrep -an \ 
"(NCM_ADDRESSING_TYPE_DHCP address still in status list)"


<xargs-vs-find>
The xargs command builds and executes command lines from standard input. This
has the advantage that the command line is filled until the system limit is
reached. Only then will the command to execute be called, in the above example
this would be rm. If there are more arguments, a new command line will be used,
until that one is full or until there are no more arguments. The same thing
    using find -exec calls on the command to execute on the found files every
    time a file is found. Thus, using xargs greatly 'speeds' up your scripts and
    the performance of your machine.


<without-xarg>
$ ls -al $(find . -name "*.log")
$ tar -cjf daemon-logs.tar.bz2 $(find /opt/zinc*/var/daemons/ -name "*.log")


<ex>
$ find . -type d -name \*stb-4 | xargs -I{} find {} -name \*nickel\* | xargs ls -alF


<busybox>
xargs

    xargs [OPTIONS] [PROG [ARGS]]

    Run PROG on every item given by standard input

    Options:

            -p      Ask user whether to run each command
            -r      Do not run command if input is empty  note:
            -0      Input is separated by NUL characters
            -t      Print the command on stderr before execution
            -e[STR] STR stops input processing
            -n N    Pass no more than N args to PROG
            -s N    Pass command line of no more than N bytes
            -x      Exit if size is exceeded


={============================================================================
*kt_linux_tool_014* tool-ssh

To switch hosts using key base instead of using password.

<ssh-server-install>
apt-get install openssh-server
systemctl start ssh.service
systemctl enable ssh.service
systemctl status ssh.service


<ssh-keygen>
$ ssh-keygen

$ ls -al .ssh/
total 20
drwx------  2 parkkt ccusers 4096 Dec  9 13:24 .
drwxr-xr-x 15 parkkt ccusers 4096 Dec  9 12:47 ..
-rw-------  1 parkkt ccusers 1675 Dec  9 13:24 id_rsa
-rw-r--r--  1 parkkt ccusers  411 Dec  9 13:24 id_rsa.pub

     -p      Requests changing the passphrase of a private key file instead of
     creating a new private key.  The program will prompt for the file
     containing the private key, for the old passphrase, and twice for the new
     passphrase.

     -o      Causes ssh-keygen to save SSH protocol 2 private keys using the
     new OpenSSH format rather than the more compatible PEM format.  The new
     format has increased resistance to brute-force pass‐ word cracking but is
     not supported by versions of OpenSSH prior to 6.5.  Ed25519 keys always
     use the new private key format.


<ssh-copy-id>
NAME
       ssh-copy-id - install your public key in a remote machine's
       authorized_keys

SYNOPSIS
       ssh-copy-id [-i [identity_file]] [user@]machine

DESCRIPTION
       ssh-copy-id is a script that uses ssh to log into a remote machine and
       append the indicated identity file to that machine's
       ~/.ssh/authorized_keys file.

       If the -i option is given then the identity file (defaults to
           ~/.ssh/id_rsa.pub) is used, regardless of whether there are any keys
       in your ssh-agent.  Otherwise, if this:

             ssh-add -L

       provides any output, it uses that in preference to the identity file.


local$ ssh-copy-id root@linuxconfig.org
root@linuxconfig.org's password:

// Now try logging into the machine, with "ssh 'root@linuxconfig.org'", and 
// check in: .ssh/authorized_keys
// to make sure we haven't added extra keys that you weren't expecting.

This make one-way ssh connection which means the machine you are on is added the
authorized_keys of the server so can run scp from the machine to the server:


<ssh-utilities>
scp remote-server:{path}/filename .
scp filename remote-server:{path}/filename

note: 
If there is working ssh connection, can use tab key to get file completion when
use scp.


{to-check-match-pair}
$ ssh-keygen /?
  -y          Read private key file and print public key.

$ ssh-keygen -y -f id_rsa


{caution}
note:
If the file permissions are too open then ssh will not trust them, and will
still prompt you for your password. 

chmod 700 ~/.ssh
chmod 644 ~/.ssh/authorized_keys  // check this as caused big grief when different
chmod 644 ~/.ssh/id_dsa_pub
chmod 644 ~/.ssh/known_hosts
chmod 600 ~/.ssh/id_dsa


{ssh-config}
man ssh_config

<different-user-name>
When user name is different between servers, must have an entry in this file for
servers to connect.

note:
The username that can be different from real user.

$ cat ~/.ssh/config
Host tizen
        Hostname 168.219.241.167
        IdentityFile ~/.ssh/id_rsa
        User keitee.park
        Port 29418

To debug ssh. note: shall use name on the command line

-v  # -vv
Verbose mode. Causes ssh to print debugging messages about its progress. This is
helpful in debugging connection, authentication, and configuration problems.
Multiple -v options increase the verbosity. The maximum is 3. 

$ ssh -vT tizen


{github-when-ssh-do-not-work}
Using SSH over the HTTPS port

Sometimes the administrator of a firewall will refuse to allow SSH connections
entirely. If using HTTPS cloning with credential caching is not an option, you
can attempt to clone using an SSH connection made over the HTTPS port. Most
firewall rules should allow this, but proxy servers may interfere. 

Testing

To test if SSH over the HTTPS port is possible, run this ssh command:

ssh -T -p 443 git@ssh.github.com
# Hi username! You've successfully authenticated, but GitHub does not provide
# shell access.

Make it so

If you are able to ssh to git@ssh.github.com over port 443, you can override
your ssh settings to force any connection to github.com to run though that
server and port. To set this in your ssh config, edit the file at ~/.ssh/config
and add this section:

Host github.com
  Hostname ssh.github.com
  Port 443

You can test that this works by connecting to github.com:

ssh -T git@github.com
# Hi username! You've successfully authenticated, but GitHub does not provide
# shell access.


{ssh-error}
https://help.github.com/articles/error-agent-admitted-failure-to-sign/
Error: Agent admitted failure to sign

In rare circumstances, connecting to GitHub via SSH on Linux produces the
error "Agent admitted failure to sign using the key". Follow these steps to
resolve the problem.

Resolution

You should be able to fix this error by loading your keys into your SSH agent
with ssh-add:

# start the ssh-agent in the background
eval "$(ssh-agent -s)"
Agent pid 59566
ssh-add
Enter passphrase for /home/you/.ssh/id_rsa: [tippy tap]
Identity added: /home/you/.ssh/id_rsa (/home/you/.ssh/id_rsa)


{run-command}
$ ssh root@172.20.33.192 ls -al


     -s      May be used to request invocation of a subsystem on the remote
     system.  Subsystems are a feature of the SSH2 protocol which facilitate
     the use of SSH as a secure transport for other applications (eg.
         sftp(1)).  The subsystem is specified as the remote command.


<options>
     Compression
             Specifies whether to use compression.  The argument must be “yes”
             or “no”.  The default is “no”.


     note:
     How to prevent "Write Failed: broken pipe" on SSH connection?

     I have tried this in /etc/ssh/ssh_config for Linux and ~/.ssh/config for
     Mac:

     Host *
     ServerAliveInterval 120

     This is how often, in seconds, it should send a keepalive message to the
     server. If that doesn't work then train a monkey to press enter every
     two minutes while you work.

     You could set either ServerAliveInterval in /etc/ssh/ssh_config of the
     client machine or ClientAliveInterval in /etc/ssh/sshd_config of the
     server machine.  Try reducing the interval if you are still getting the
     error.

     ServerAliveInterval
             Sets a timeout interval in seconds after which if no data has
             been received from the server, ssh(1) will send a message through
             the encrypted channel to request a response from the server.  The
             default is 0, indicating that these messages will not be sent to
             the server, or 300 if the BatchMode option is set.  
             
             This option applies to protocol version 2 only.
             ProtocolKeepAlives and SetupTimeOut are Debian-specific
             compatibility aliases for this option.

     note:
     How to check protocol version?

     ssh -vv xxx
     ...
     debug1: Enabling compatibility mode for protocol 2.0
     debug1: Local version string SSH-2.0-OpenSSH_6.7p1 Debian-5+deb8u2
     ...

     Host    Restricts the following declarations (up to the next Host or
             Match keyword) to be only for those hosts that match one of the
             `patterns` given after the keyword.  

             If more than one pattern is provided, they should be separated by
             whitespace.  

             A single ‘*’ as a pattern can be used to provide global defaults
             for all hosts.
             
             A pattern entry may be negated by prefixing it with an
             exclamation mark (‘!’).  If a negated entry is matched, then the
             Host entry is ignored, regardless of whether any other patterns
             on the line match.  Negated matches are therefore useful to
             provide exceptions for wildcard matches.

             See PATTERNS for more information on patterns.


<ssh-x>
     ForwardX11
             Specifies whether X11 connections will be automatically
             redirected over the secure channel and DISPLAY set.  The argument
             must be “yes” or “no”.  The default is “no”.

             X11 forwarding should be enabled with caution.  Users with the
             ability to bypass file permissions on the remote host (for the
                 user's X11 authorization database) can access the local X11
             display through the forwarded connection.  An attacker may then
             be able to perform activities such as keystroke monitoring if the
             ForwardX11Trusted option is also enabled.

             For this reason, X11 forwarding is subjected to X11 SECURITY
             extension restrictions by default.  Please refer to the ssh -Y
             option and the ForwardX11Trusted directive in ssh_config(5) for
             more information.

can use `ForwardX11` or `ssh -X` which are the same. when use one of these, it
will set DISPLAY automatically like:

DISPLAY=localhost:14.0


ssh -X <remote-username>@<remote-machine>

-X      
Enables X11 forwarding.  This can also be specified on a `per-host` basis in a
configuration file.

// X server on laptop and X apps on server which is X client.
The software needed on your own computer is called an X server, it allows the
connections from the X clients to display application on your screen.

// X client use DISPLAY to find out X server to connect so that it can send
// data to display
When the X clients start the look at the DISPLAY EV to see where to connect to
by default. One way to do remote displaying would be to set this EV to a
remote X server.

hostname:D.S

hostname is the name of the computer where the X server runs. An omitted
hostname means the localhost.

D is a sequence number (usually 0). It can be varied if there are multiple
displays connected to one computer.

S is the screen number. A display can actually have multiple screens. Usually
there's only one screen though where 0 is the default


https://docstore.mik.ua/orelly/networking_2ndEd/ssh/ch09_03.htm

9.3.2. How X Forwarding Works

Although X clients can communicate with remote X servers, this communication
isn't secure. All interactions between the X client and server, such as
keystrokes and displayed text, can be easily monitored by network snooping
because the connection isn't encrypted. In addition, most X environments use
primitive authentication methods for connecting to a remote display. A
knowledgeable attacker can get a connection to your display, monitor your
keystrokes, and control other programs you're running. 

Once again, SSH comes to the rescue. An X protocol connection can be routed
through an SSH connection to provide security and stronger authentication.
This feature is called X forwarding. 

X forwarding works in the following way. (As illustration, please refer to
    Figure 9-10.) An SSH client requests X forwarding when it connects to an
SSH server (assuming X forwarding is enabled in the client). If the server
allows X forwarding for this connection, your login proceeds normally, but the
server takes some special steps behind the scenes. In addition to handling
your terminal session, it sets itself up as a proxy X server running on the
remote machine and sets the DISPLAY environment variable in your remote shell
to point to the proxy X display:

syrinx$ ssh sys1
Last login: Sat Nov 13 01:10:37 1999 from blackberry
Sun Microsystems Inc.   SunOS 5.6       Generic August 1997
You have new mail.
sys1$ echo $DISPLAY 
sys1:10.0
sys1$ xeyes
The "xeyes" X client appears on the screen

laptop with display

X server                            X app (X client)

SSH client              <--->       SSH server
(Proxy X client)                    (Proxy X server)

Figure 9-10. X forwarding

The DISPLAY value appears to refer to X display #10 on sys1, but there's no
such display. Instead, the DISPLAY value points to the X proxy established by
the SSH server, i.e., the SSH server is masquerading as an X server. If you
now run an X client program, it connects to the proxy. The proxy behaves just
like a "real" X server, and in turn instructs the SSH client to behave as a
proxy X client, connecting to the X server on your local machine. The SSH
client and server then cooperate to pass X protocol information back and forth
over the SSH pipe between the two X sessions, and the X client program appears
on your screen just as if it had connected directly to your display. That's
the general idea of X forwarding.  


={============================================================================
*kt_linux_tool_014* tool-putty

{putty-ssh-setup}
When use keys generated from putty.

o run puttygen to make key pairs. rsa or dsa
o get a pub key and save a pri key(ppk)
o run putty and set ssh key to use
  menu: connection: ssh: auth: private key file for auth: specify the path to a pri key.
o login to the host and add a pub key in the auth key list


{convert-rsa-key-to-putty-ppk}
To convert keys from linux machine to putty ppk and from ppk to lunux(opsnssh
    keys)

o run puttygen and menu: conversion: import key:
o save it as a pri key(ppk)


={============================================================================
*kt_linux_tool_014* tool-xwin tool-xserver

https://my.cqu.edu.au/web/eresearch/windows-x-server

Compared with VNC method, some of the benefits of using this method to display
GUI applications includes:

It natively uses your windows environment, so you can easily resize windows
and move windows to a separate screens (if using more than one computer
monitor). 

Interacting with the application may be faster and smoother, as there is no
delay in moving the mouse, when compared running the application within a VNC
session.


<on-putty-side>
* Click on the "Enable X11 Forwarding" check box.

* Advised to enable "compression", has it will decrease the amount for
  bandwidth required.  To do this, select the Putty Category Connection -> SSH
  and tick the "Enable compression" check box


{xserver}
I stumbled across " VcXsrv Windows X Server" - that seems to be the well
maintained and if you want you can compiled yourself (not that you need to).

http://sourceforge.net/projects/vcxsrv/?source=directory

// If you chose to use it, you need to run VcXsrc without OpenGL support but it
// works - see highlighted text below ...
// 
// "C:\Program Files\VcXsrv\vcxsrv.exe" :0 -ac -terminate -lesspointer -multiwindow -clipboard -nowgl

note: 
this server is better since it seems to have more fonts and shows gvim
properly than xming which is old xserver. It replace xming and seems to use
the rest since use xlanuch. 'One window' do not works but 'multiple windows'
works.

note:
Only multiple window option works

<run-config>
"C:\Program Files\VcXsrv\xlaunch.exe" -run C:\kit\xconfig

<?xml version="1.0" encoding="UTF-8"?>
<XLaunch WindowMode="MultiWindow" ClientMode="NoClient" LocalClient="False" Display="-1" LocalProgram="xcalc" RemoteProgram="xterm" RemotePassword="" PrivateKey="" RemoteHost="" RemoteUser="" XDMCPHost="" XDMCPBroadcast="False" XDMCPIndirect="False" Clipboard="True" ClipboardPrimary="True" ExtraParams="" Wgl="True" DisableAC="False" XDMCPTerminate="False"/>


={============================================================================
*kt_linux_tool_015* tool-pgrep, tool-pidof

pidof -- find the process ID of a running program.

Pidof finds the process id's (pids) of the named  programs. It prints those id's
on the standard output. 

$ pidof getty
2974 2973 2972 2971 2970 2969


{pgrep}
       pgrep, pkill - look up or signal processes based on name and other
       attributes

SYNOPSIS
       pgrep [options] pattern
       pkill [options] pattern

DESCRIPTION
       pgrep looks through the currently running processes and lists the process
       IDs which match the selection criteria to stdout.  All the criteria have
       to match.  For example,

              $ pgrep -u root sshd

       will only list the processes called sshd AND owned by root.  On the other
       hand,

              $ pgrep -u root,daemon

       will list the processes owned by root OR daemon.

       pkill will send the specified signal (by default SIGTERM) to each process
       instead of listing them on stdout.

$ pgrep -h    
pgrep: invalid option -- h
BusyBox v1.21.0 (2015-06-02 11:02:10 BST) multi-call binary.

Usage: pgrep [-flnovx] [-s SID|-P PPID|PATTERN]

Display process(es) selected by regex PATTERN

	-l	Show command name too
	-f	Match against entire command line
	-n	Show the newest process only
	-o	Show the oldest process only
	-v	Negate the match
	-x	Match whole name (not substring)
	-s	Match session ID (0 for current)
	-P	Match parent process ID


$ pgrep linearsourced
1079

$ pgrep -l linearsourced
1079 /opt/zinc-trunk/bin/linearsourced


={============================================================================
*kt_linux_tool_018* tool-ls

{get-filename-only}
ls -1

<sort-by-time>

       -l     use a long listing format

       -r, --reverse
              reverse order while sorting

       -t     sort by modification time, newest first

       -c     with -lt: sort by, and show, ctime (time of last modification of
           file status information) 

              with -l: show ctime and sort by name otherwise: sort by ctime,
              newest first

       -S     sort by file size


={============================================================================
*kt_linux_tool_019* tool-trace tool-ftrace

<tool-trace-chrome>
https://www.gamasutra.com/view/news/176420/Indepth_Using_Chrometracing_to_view_your_inline_profiling_data.php

Viewing external profile data files

chrome://tracing/


<tool-trace-tool-ftrace>
Ftrace is a Linux kernel internals tracing tool that was first included in the
2.6.27 kernel in 2008. 

The main ftrace documentation is available in the Linux kernel source at
…/root/Documentation/trace/ftrace.txt. It was last updated for the 3.10
kernel. In the same directory, there are a number of other interesting
documents including ftrace-design.txt by Mike Frysinger which focuses on
architecture implementation details. The actual tracemoint implementation
mechanism is detailed by Mathieu Desnoyers in tracepoints.txt.

One of the annoying things about ftrace for most first time users of the tool
is that it does not provide an easy mechanism to invoke an executable and just
trace that executable. Here is a simple shell script which provides that
functionality:

https://lwn.net/Articles/365835/

// should be root to do that:

    [~]# cd /sys/kernel/debug/tracing
    [tracing]#

// sample output
tail -f trace
            ...
            bash-6210  [000] .... 13588.205993: kmem_cache_alloc_trace <-seq_open
            bash-6210  [000] .... 13588.205993: _cond_resched <-kmem_cache_alloc_trace
            bash-6210  [000] .... 13588.205993: __mutex_init <-seq_open
            bash-6210  [000] .... 13588.205993: __kmalloc <-__tracing_open
            bash-6210  [000] .... 13588.205993: kmalloc_slab <-__kmalloc
            bash-6210  [000] .... 13588.205994: _cond_resched <-__kmalloc
            bash-6210  [000] .... 13588.205994: mutex_lock <-__tracing_open
            bash-6210  [000] .... 13588.205994: _cond_resched <-mutex_lock
            bash-6210  [000] .... 13588.205994: kmem_cache_alloc_trace <-__tracing_open
            bash-6210  [000] .... 13588.205994: _cond_resched <-kmem_cache_alloc_trace
            bash-6210  [000] .... 13588.205995: __mutex_init <-__tracing_open
            bash-6210  [000] .... 13588.205995: _raw_spin_lock_irqsave <-tracing_stop


#!/bin/bash
# Read more: https://blog.fpmurphy.com/2014/05/kernel-tracing-using-ftrace.html#ixzz5KIDBCBAz
 
DEBUGFS=`grep debugfs /proc/mounts | awk '{ print $2; }'`
 
echo $$ > $DEBUGFS/tracing/set_ftrace_pid
echo function > $DEBUGFS/tracing/current_tracer
echo 1 > $DEBUGFS/tracing/tracing_on
exec $*
echo 0 > $DEBUGFS/tracing/tracing_on


echo function > current_tracer
echo 1 > tracing_on
cat trace
echo 0 > tracing_on
echo > trace

To simplify the use of ftrace, Rostedt has written two tools  trace-cmd and
kernelshark. These are available as standard repo RPMs in Fedora 20. Trace-cmd
is a binary command line tool which acts as a user interface to ftrace whereas
Kernelshark is a graphical consumer of trace-cmd output.

note:
should run trace-cmd under a directory that you have write permission.

sudo apt-get install trace-cmd


<tool-trace-tool-ftrace-add-tag>

  trace_marker		- Writes into this file writes into the kernel buffer

A trace marker enables some coordination between what is happening in user
space and inside the kernel by providing a way to write into the ftrace kernel
ring buffer from user space. This marker will then appear in the trace output
to give a location where a specific event occurred. Here is an example of
setting a trace marker:

# echo "FINN" > /sys/kernel/debug/tracing/trace_marker


A practical example

This is how trace-cmd was used to investigate an unexpectedly long delay
between FRAME_PRESENTED and "VIDEO IS SHOWN" traces in the MW log files. The
use of the system() function is not optimal for speed, but it makes the
modification quick and simple!

Changes to /usr/bin/trace-cmd

These two chmod entries were added to /usr/bin/trace-cmd. This change of
permission was necessary to allow the MW processes to write to the trace
marker buffer. This allows you to print messages directly into your ftrace
capture.

/usr/bin/trace-cmd stop

# set ftrace buffer size default is probably to low
echo 3500 > /sys/kernel/debug/tracing/buffer_size_kb 

/usr/bin/trace-cmd start -e sched -e irq
chmod 777 /sys/kernel/debug
chmod 222 /sys/kernel/debug/tracing/trace_marker

Changes to mpm_component_video.c

This causes the text FRAME_PRESENTED to be written into the ftrace log file

DIAG_LOG_MIL(g_MPM_Logging_ID, ("FRAME_PRESENTED: hMC=0x%x", h_media_connection));
system("echo FRAME_PRESENTED > /sys/kernel/debug/tracing/trace_marker");
DIAG_PMON_DECODER_1ST_FRAME(g_MPM_Logging_ID, h_media_connection);
Changes to player_api_avcontrol.c

This causes the text VIDEO_IS_SHOWN to be written into the ftrace log, then
the trace buffer is stopped, then the trigger file is created to allow
trace_cmd.sh to collect and write the logs to the FSN_DATA directory.

if ( true == visibility )
{
    PLAYER_DIAG_LOG_MIL(("VIDEO IS SHOWN"));
    system("echo VIDEO_IS_SHOWN > /sys/kernel/debug/tracing/trace_marker ; /usr/bin/trace-cmd stop ; touch /tmp/trace_cmd_triggered");
}
else


Debugging the kernel using Ftrace - part 2
https://lwn.net/Articles/366796/


<tool-ftrace-others>
https://github.com/brendangregg/perf-tools


<tool-trace-tool-uftrace>
The uftrace tool is to trace and analyze execution of a program written in
C/C++. It was heavily inspired by the `ftrace` framework of the Linux kernel
(especially function graph tracer) and supports userspace programs. It
supports various kind of commands and filters to help analysis of the program
execution and performance.

note:
what's difference from ftrace? supports userspace programs.

The -pg debug option generates profiling information used by gprof

https://github.com/namhyung/uftrace
https://github.com/namhyung/uftrace/wiki/Tutorial

To analyze this program with uftrace, you need to enable the compiler
instrumentation. The gcc provides -pg and -finstrument-functions options for
this. We prefer to use -pg option as it's more light-weight in terms of
compiler optimization, but there were some cases it didn't work well.
Currently function arguments are only accessible if it's compiled with the -pg
option.


  -R, --retval=FUNC@retval   Show function return value
      --sample-time=TIME     Show flame graph with this sampling time
      --sort-column=INDEX    Sort diff report on column INDEX (default: 2)
      --symbols              Print symbol tables

  -A, --argument=FUNC@arg[,arg,...]
                             Show function arguments

$ uftrace record -A search@arg1,arg2 -A sum@arg1,arg2,arg3 -R search@retval -R sum@retval t_ex_test

Users can use various filter options to limit functions it records/prints. The
depth filter (-D option) is to omit functions under the given call depth. The
time filter (-t option) is to omit functions running less than the given time.
And the function filters (-F and -N options) are to show/hide functions under
the given function.

  -F, --filter=FUNC          Only trace those FUNCs
  -N, --notrace=FUNC         Don't trace those FUNCs

$ uftrace record -F search -F sum -A search@arg1,arg2 -A sum@arg1,arg2,arg3 -R search@retval -R sum@retval t_ex_test
$ uftrace record -F find_path -A find_path@arg1,arg2,arg3 -R find_sum@retval t_ex_test

// can see from the second arg
$ uftrace record -F find_path -A find_path@arg2,arg3 -R find_sum@retval t_ex_test

<ex>
            [ 10578] | find_path(1, 1) {
            [ 10578] |   find_path(1, 2) {
            [ 10578] |     find_path(1, 1) {
  12.428 us [ 10578] |     } /* find_path */
            [ 10578] |     find_path(1, 3) {
            [ 10578] |       find_path(1, 2) {
  10.787 us [ 10578] |       } /* find_path */
            [ 10578] |       find_path(1, 4) {
            [ 10578] |         find_path(1, 3) {
  13.404 us [ 10578] |         } /* find_path */
            [ 10578] |         find_path(1, 5) {
            [ 10578] |           find_path(1, 4) {
  12.767 us [ 10578] |           } /* find_path */
            [ 10578] |           find_path(2, 5) {
            [ 10578] |             find_path(1, 5) {
  85.506 us [ 10578] |             } /* find_path */
            [ 10578] |             find_path(3, 5) {
            [ 10578] |               find_path(3, 4) {
            [ 10578] |                 find_path(3, 3) {
            [ 10578] |                   find_path(3, 2) {
            [ 10578] |                     find_path(3, 1) {
            [ 10578] |                       find_path(3, 2) {
  28.249 us [ 10578] |                       } /* find_path */
            [ 10578] |                       find_path(4, 1) {
            [ 10578] |                         find_path(3, 1) {
 182.678 us [ 10578] |                         } /* find_path */
            [ 10578] |                         find_path(5, 1) {
            [ 10578] |                           find_path(5, 2) {
            [ 10578] |                             find_path(5, 1) {
  82.954 us [ 10578] |                             } /* find_path */
            [ 10578] |                             find_path(5, 3) {
            [ 10578] |                               find_path(5, 2) {
  17.163 us [ 10578] |                               } /* find_path */
            [ 10578] |                               find_path(5, 4) {
            [ 10578] |                                 find_path(5, 3) {
  19.277 us [ 10578] |                                 } /* find_path */
            [ 10578] |                                 find_path(5, 5) {
  19.241 us [ 10578] |                                 } /* find_path */
 116.223 us [ 10578] |                               } /* find_path */
 213.627 us [ 10578] |                             } /* find_path */
 391.920 us [ 10578] |                           } /* find_path */
 967.572 us [ 10578] |                         } /* find_path */
   2.857 ms [ 10578] |                       } /* find_path */
   3.996 ms [ 10578] |                     } /* find_path */
   4.060 ms [ 10578] |                   } /* find_path */
   4.126 ms [ 10578] |                 } /* find_path */
   4.220 ms [ 10578] |               } /* find_path */
   4.287 ms [ 10578] |             } /* find_path */
   5.416 ms [ 10578] |           } /* find_path */
   6.411 ms [ 10578] |         } /* find_path */
   6.485 ms [ 10578] |       } /* find_path */
   6.559 ms [ 10578] |     } /* find_path */
   6.708 ms [ 10578] |   } /* find_path */
   7.233 ms [ 10578] | } /* find_path */

$ uftrace replay > out


<tool-gprof>
http://sourceware.org/binutils/docs/gprof/

Profiling a program is an important step in analyzing program performance and
identifying bottlenecks. A C or C++ program can be easily profiled in Linux
using gprof:

Make sure your C or C++ code compiles without errors. Make sure your compiled
program starts, executes and exits without any errors.

Add the -pg option to both the object compilation and linking stages of the
program compilation. Compile the program from scratch.

Execute the program as you normally do. It might take a while longer to finish
due to the profiling. After the program exits, it writes a gmon.out file with
profiling information.

$ ./foo

Invoke gprof with your program executable as input. It prints out profiling
information in the form of both a flat profile and a call graph. You can pipe
this to a pager or redirect it to a text file for examination.

$ gprof foo

Examining the output of gprof gives you an idea of which functions in the
program are taking the most execution time. This information can be used to
optimize such functions.


={============================================================================
*kt_linux_tool_019* tool-trace tool-strace

strace - trace `system-calls` and `signals` 

-f
Trace child processes as they are created by currently traced processes as a
result of the fork(2) system call. 


-o filename
Write the trace output to the file filename rather than to stderr. Use
filename.pid if -ff is used.  If the argument begins with '|' or with '!' then
the rest of the argument is treated as a command and all output is piped to it.
This is convenient for piping the debugging output to a program without
affecting the redirections of executed programs. 


-ff         
If the -o filename option is in effect, each processes trace is written to
filename.pid where pid is the numeric process id of each process.  This is
incompatible with -c, since no per-process counts are kept.

note: 
This is important to get every thread output into a separate file (-ff option).


-p pid      
Attach to the process with the process ID pid and begin tracing. The trace may
be terminated at any time by a keyboard interrupt signal (CTRL-C).  strace will
respond by detaching itself from the traced process(es) leaving it (them) to
continue running.  Multiple -p options can be used to attach to up to 32
processes in addition to command (which is optional if at least one -p option is
    given).

-s strsize  
Specify the maximum string size to print (the default is 32).  Note that filenames are not
considered strings and are always printed in full.

-E var=val  Run command with var=val in its list of environment variables.

-e expr     

A qualifying expression which modifies which events to trace or how to trace them.  The format of
the expression is:

[qualifier=][!]value1[,value2]...

where qualifier is one of trace, abbrev, verbose, raw, signal, read, or write and value is a
qualifier-dependent symbol or number. The 'default' qualifier is trace. Using an exclamation mark
negates the set of values.  

For example, -e open means literally -e trace=open which in turn means trace only the open system
call. By contrast, -e trace=!open means to trace every system call except open. In addition, the
special values all and none have the obvious meanings.

Note that some shells use the exclamation point for history expansion even inside quoted arguments.
If so, you must escape the exclamation point with a back-slash.


<ex>
When use to run a script:

$ strace -f -o trace sh ./compile.sh

When use to run a program:

$ strace ./appname

strace -ff -s 128 -v -o dbus-trace.log -p <dbus daemon PID>


<how-to-run-app-with-strace>
# this is the original line
LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib" LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
exec "${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"

# this is the same line with strace
exec strace -ff -o /opt/adobe/stagecraft/data/trace.log -s 128 \
-E LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib" -E LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
"${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"


<ex>
LD_PRELOAD=/libyouviewrcushim.so strace /bin/true


={============================================================================
*kt_linux_tool_020* tool-trace-case

<ex>
#include <stdio.h>

int main()
{
  printf("hello world.\n");
}

// strace output
...
getrlimit(RLIMIT_STACK, {rlim_cur=8192*1024, rlim_max=RLIM64_INFINITY}) = 0
fstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 3), ...}) = 0
mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f7148b54000
write(1, "hello world.\n", 13)          = 13
exit_group(13)                          = ?
+++ exited with 13 +++

// ltrace output
__libc_start_main(0x400686, 1, 0x7ffe950fa958, 0x4006a0 <unfinished ...>
puts("hello world.")                                                                                                            = 13
+++ exited (status 13) +++

// uftrace output
kyoupark@kit-debian64:~/git/kb/code-linux/ex_gdb$ uftrace ./hello_test
hello world.
# DURATION     TID     FUNCTION
            [  3901] | main() {
   4.507 us [  3901] |   puts();
   6.538 us [  3901] | } /* main */


http://blog.hostilefork.com/where-printf-rubber-meets-road/

// <glibc>/libio/stdio.h

__BEGIN_NAMESPACE_STD
/* Write formatted output to STREAM.

   This function is a possible cancellation point and therefore not
   marked with __THROW.  */
extern int fprintf (FILE *__restrict __stream,
		    __const char *__restrict __format, ...);


// <glibc>/stdio-common/printf.c

/* Write formatted output to stdout from the format string FORMAT.  */
/* VARARGS1 */
int
__printf (const char *format, ...)
{
  va_list arg;
  int done;

  va_start (arg, format);
  done = vfprintf (stdout, format, arg);
  va_end (arg);

  return done;
}

ldbl_strong_alias (__printf, printf);

// <glibc>/stdio-common/vfprintf.c

/* The function itself.  */
int
vfprintf (FILE *s, const CHAR_T *format, va_list ap)
{

}

#define	outchar(Ch)							      \
  do									      \
    {									      \
      register const INT_T outc = (Ch);					      \
      if (PUTC (outc, s) == EOF || done == INT_MAX)			      \
	{								      \
	  done = -1;							      \
	  goto all_done;						      \
	}								      \
      ++done;								      \
    }									      \
  while (0)


={============================================================================
*kt_linux_tool_020* tool-date

{tool-date}
NAME
       date - print or set the system date and time

DESCRIPTION
       Display the current time in the given FORMAT, or set the system date.

SYNOPSIS
       date [OPTION]... [+FORMAT]

       %s     seconds since 1970-01-01 00:00:00 UTC

dbus-monitor &> /var/log/dbus-monitor/$(date +%s).log &

$ date --rfc-3339=s
2015-01-27 13:49:11+00:00

$ date
Tue Jan 27 13:49:19 UTC 2015

sudo date -s "Thu Oct 4 12:11:14 BST 2018"
sudo date -s "Tue 26 Sep 09:59:39 BST 2017"   # invalid date
sudo date -s "Tue Nov 21 13:57:22 GMT 2017"


={============================================================================
*kt_linux_tool_021* tool-chmod

{to-set-uid}
That require root access to function properly even when invoked by a nonroot
user

%chmod +s /sample_file


={============================================================================
*kt_linux_tool_022* mknod

%mknod $name c $major $minor


={============================================================================
*kt_linux_tool_023* tool-wc

       -c, --bytes
              print the byte counts

       -l, --lines
              print the newline counts


{how-to-count-lines-recursively}

find . -name '*.php' | xargs wc -l

# should be re-written using sh scripting
#!/bin/bash
echo "debug..."
find debug -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "dsm"
find dsm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "include"
find include -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "main"
find main -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

#
echo "mah"
find mah -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5dec"
find mh5dec -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5eng"
find mh5eng -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5gpi"
find mh5gpi -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mhv"
find mhv -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "pfm"
find pfm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l


={============================================================================
*kt_linux_tool_025* tool-ln

       -n, --no-dereference
              treat destination that is a symlink to a directory as if it were
              a normal file

       -f, --force
              remove existing destination files

       -s, --symbolic
              make symbolic links instead of hard links

ln -sf input output. E.g., ln -sf linux-2.4.25-2.8 linux

<ex>
$ ln -s /usr/src/linux .
lrwxrwxrwx  1 keitee keitee    14 Feb 18 23:50 linux -> /usr/src/linux/

<ex>
To create mutiple link at one go. Assume that there are two files under /bin
e.g., gdb and gdbserver then it will create two links in /usr/local/bin/.

ln -sf /opt/zinc/oss/debugtools/bin/gdb* /usr/local/bin/

<ex>
Supports multiple matches:

ln -sf ${yv_htmlengine_prefix}/tests/\
{vanadium-w3c-engine,vanadium-webkit-video-element,zinc-jscore-binding-runtime} \
${yv_prefix}/tests/


={============================================================================
*kt_linux_tool_026* tool-rsync

NAME
       rsync - a fast, versatile, remote (and local) file-copying tool

SYNOPSIS
       Local:  rsync [OPTION...] SRC... [DEST]

       Access via remote shell:
         Pull: rsync [OPTION...] [USER@]HOST:SRC... [DEST]
         Push: rsync [OPTION...] SRC... [USER@]HOST:DEST

       Access via rsync daemon:
         Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST]
               rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST]
         Push: rsync [OPTION...] SRC... [USER@]HOST::DEST
               rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST

       Usages with just one SRC arg and no DEST arg will list the source files
       instead of copying.

Rsync, which stands for "remote sync", is a remote and local file
synchronization tool. It uses an algorithm that minimizes the amount of data
copied by only moving the portions of files that have changed.


DESCRIPTION
       Rsync  is  a  fast  and extraordinarily versatile file copying tool.  It
       can copy locally, to/from another host over any remote shell, or to/from
       a remote rsync daemon.  It offers a large number of options that control
       every aspect of its behavior and permit very flexible specification of
       the set of files to be copied.  It is famous for its 'delta'-transfer
       algorithm, which reduces the amount of data sent over the network by
       sending only the  differences between the source files and the existing
       files in the destination.  Rsync is widely used for backups and mirroring
       and as an improved copy command for everyday use.

       Rsync finds files that need to be transferred using a "quick check"
       algorithm (by default) that looks for files that have changed in 'size' or
       in last-modified 'time'.  Any changes in the other preserved attributes (as
           requested by options) are made on the destination file directly when
       the quick check indicates that the file's data does not need to be
       updated.

       Some of the additional features of rsync are:

       o      support for copying links, devices, owners, groups, and
              permissions

       o      exclude and exclude-from options similar to GNU tar

       o      a CVS exclude mode for ignoring the same files that CVS would
              ignore

       o      can use any transparent remote shell, including ssh or rsh

       o      does not require super-user privileges

       o      pipelining of file transfers to minimize latency costs

       o      support for anonymous or authenticated rsync daemons (ideal for
              mirroring)


        -r, --recursive             recurse into directories

GENERAL
       Rsync copies files either to or from a remote host, or locally on the
       current host (it does 'not' support copying files between two remote
           hosts).

       There are two different ways for rsync to contact a remote system: using
       a remote-shell program as the transport (such as ssh or rsh) or
       contacting an rsync daemon directly via TCP.  
       
       The remote-shell transport is used  whenever  the  source  or destination
       path contains a single colon (:) separator after a host specification.  
       
       Contacting an rsync daemon directly happens when the source or
       destination path contains a double colon (::) separator after a host
       specification, OR when an rsync:// URL is specified (see also the "USING
       RSYNC-DAEMON FEATURES VIA A REMOTE-SHELL CONNECTION" section for an
       exception to this latter rule).

       As a special case, if a single source arg is specified without a
       destination, the files are listed in an output format similar to "ls -l".

       As expected, if neither the source or destination path specify a remote
       host, the copy occurs locally (see also the --list-only option).

       Rsync refers to the local side as the "client" and the remote side as the
       "server".  Don't confuse "server" with an rsync daemon -- a daemon is
       always a server, but a server can be either a daemon or a  remote-shell
       spawned process.

USAGE
       You use rsync in the same way you use rcp. You must specify a source and
       a destination, one of which may be remote.

       Perhaps the best way to explain the syntax is with some examples:

       -t, --times                 preserve modification times

              rsync -t *.c foo:src/

       This would transfer all files matching the pattern *.c from the current
       directory to the directory src on the machine foo. If any of the files
       already exist on the remote system then the rsync remote-update protocol
       is used to update the file by sending only the differences. See the tech
       report for details.

        -v, --verbose               increase verbosity
        -a, --archive               archive mode; 'equals' -rlptgoD (no -H,-A,-X)
            --no-OPTION             turn off an implied OPTION (e.g. --no-D)
        -z, --compress              compress file data during the transfer

              rsync -avz foo:src/bar /data/tmp

       This  would  recursively transfer all files from the directory src/bar on
       the machine foo into the /data/tmp/bar directory on the local machine.
       The files are transferred in "archive" mode, which ensures that symbolic
       links, devices, attributes, permissions, ownerships, etc. are preserved
       in the transfer.  Additionally, compression will be used to reduce the
       size of data portions of the transfer.

              rsync -avz foo:src/bar/ /data/tmp

       A 'trailing' slash on the source changes this behavior to avoid creating an
       additional directory level at the destination.  You can think of a
       trailing / on a source as meaning "copy the contents of this directory"
       as  opposed to  "copy  the directory by name", but in both cases the
       attributes of the containing directory are transferred to the containing
       directory on the destination.  In other words, each of the following
       commands copies the files in the same way, including their setting of the
       attributes of /dest/foo:

              rsync -av /src/foo /dest
              rsync -av /src/foo/ /dest/foo

       Note also that host and module references don't require a trailing slash
       to copy the contents of the default directory.  For example, both of
       these copy the remote directory's contents into "/dest":

              rsync -av host: /dest
              rsync -av host::module /dest

       You can also use rsync in local-only mode, where both the source and
       destination don't have a ':' in the name. In this case it behaves like an
       improved copy command.

       Finally, you can 'list' all the (listable) 'modules' available from a
       particular rsync daemon by leaving off the module name:

              rsync somehost.mydomain.com::

              <ex>
              ~/source/DEVARCH$ rsync zinc@humax-04535::
              Root           	Humax box

       See the following section for more details.

CONNECTING TO AN RSYNC DAEMON
       It is also possible to use rsync without a remote shell as the transport.
       In this case you will directly connect to a remote rsync daemon, 
       typically using TCP port 873.  

       (This obviously requires the daemon to be running on the remote system,
        so refer to the STARTING AN RSYNC DAEMON TO ACCEPT CONNECTIONS section
        below for information on that.)

       Using rsync in this way is the same as using it with a remote shell
       except that:

       o      you either use a double colon :: instead of a single colon to
              separate the hostname from the path, or you use an rsync:// URL.

       o      the first word of the "path" is actually a module name.

       o      the remote daemon may print a message of the day when you connect.

       o      if you specify no path name on the remote daemon then the list of
              accessible paths on the daemon will be shown.

       o      if you specify no local destination then a listing of the
              specified files on the remote daemon is provided.

       o      you must not specify the --rsh (-e) option.

       An example that copies all the files in a remote module named "src":

           rsync -av host::src /dest

       Some  modules  on the remote daemon may require authentication. 

       If so, you will receive a password prompt when you connect. You can avoid
       the password prompt by setting the environment variable RSYNC_PASSWORD to
       the password you want to use or using the --password-file option. This
       may be useful when scripting rsync.

       WARNING: On some systems environment variables are visible to all users.
       On those systems using --password-file is recommended.

STARTING AN RSYNC DAEMON TO ACCEPT CONNECTIONS
       In  order  to connect to an rsync daemon, the remote system needs to have
       a daemon already running (or it needs to have configured something like
           inetd to spawn an rsync daemon for incoming connections on a
           particular port).  
       
       For full information on how to start a daemon that will handling incoming
       socket connections, see the rsyncd.conf(5) man page -- that is the config
       file for the daemon, and it contains the full details for  how  to  run
       the daemon (including stand-alone and inetd configurations).

       If you're using one of the remote-shell transports for the transfer,
       there is no need to manually start an rsync daemon.

EXAMPLES
       Here are some examples of how I use rsync.

       To backup my wife's home directory, which consists of large MS Word files and mail folders, I use a cron job that runs

              rsync -Cavz . arvidsjaur:backup

       each night over a PPP connection to a duplicate directory on my machine "arvidsjaur".

       To synchronize my samba source trees I use the following Makefile targets:

           get:
                   rsync -avuzb --exclude '*~' samba:samba/ .
           put:
                   rsync -Cavuzb . samba:samba/
           sync: get put

       this allows me to sync with a CVS directory at the other end of the connection. I then do CVS operations on the remote machine, which saves a lot of time as the remote CVS protocol isn't very efficient.

       I mirror a directory between my "old" and "new" ftp sites with the command:

       rsync -az -e ssh --delete ~ftp/pub/samba nimbus:"~ftp/pub/tridge"

       This is launched from cron every few hours.

OPTIONS
       Rsync  accepts  both  long  (double-dash  +  word)  and  short
       (single-dash + letter) options.  The full list of the available options
       are described below.  
       
       If an option can be specified in more than one way, the choices are
       comma-separated.  Some options only have a long variant, not a short.  If
       the option takes a parameter, the parameter is only listed after the long
       variant, even though it must also be specified for the short.  When
       specify- ing  a  parameter,  you  can either use the form --option=param
       or replace the '=' with whitespace.  The parameter may need to be quoted
       in some manner for it to survive the shell's command-line parsing.  Keep
       in mind that a leading tilde (~) in a filename is substituted by your
       shell, so --option=~/foo will not change the tilde into your home
       directory (remove the '=' for that).


       -n, --dry-run
              This  makes  rsync  perform  a  trial run that doesn't make any
              changes (and produces mostly the same output as a real run).  It
              is most commonly used in combination with the -v, --verbose and/or
              -i, --itemize-changes options to see what an rsync command is
              going to do before one actually runs it.

              The output of --itemize-changes is supposed to be exactly the same
              on a dry run and a subsequent real run (barring intentional
                  trickery and system call failures); if it isn't, that's a bug.
              Other  output  should  be mostly unchanged, but may differ in some
              areas.  Notably, a dry run does not send the actual data for file
              transfers, so --progress has no effect, the "bytes sent", "bytes
              received", "literal data", and "matched data" statistics are too
              small, and the "speedup" value is equivalent to a run where no
              file transfers were needed.

       --delete
              note: delete(sync) when there are deleted files in source side.

              This tells rsync to delete extraneous files from the receiving
              side (ones that aren't on the sending side), but only for the
              directories that are being synchronized.  You must have asked
              rsync to send the whole directory (e.g. "dir" or "dir/") without
              using a wildcard for the directory's contents (e.g. "dir/*") since
              the wildcard is expanded by the shell and rsync thus gets a
              request to transfer individual files, not  the  files' parent
              directory.   Files  that  are  excluded  from  the  transfer  are
              also  excluded  from being deleted unless you use the
              --delete-excluded option or mark the rules as only matching on the
              sending side (see the include/exclude modifiers in the FILTER
                  RULES section).

              Prior to rsync 2.6.7, this option would have no effect unless
              --recursive was enabled.  Beginning with 2.6.7, deletions will
              also occur when --dirs (-d) is enabled, but only for directories
              whose  contents  are  being copied.

              This option can be dangerous if used incorrectly!  It is a very
              good idea to first try a run using the --dry-run option (-n) to
              see what files are going to be deleted.

              If  the  sending side detects any I/O errors, then the deletion of
              any files at the destination will be automatically disabled. This
              is to prevent temporary filesystem failures (such as NFS errors)
              on the sending side from causing a massive deletion of files on
              the destination.  You can override this with the --ignore-errors
              option.

              The --delete option may be 'combined' with one of the
              --delete-WHEN options without conflict, as well as
              --delete-excluded.  However, if  none  of  the  --delete-WHEN
              options  are  specified,  rsync  will  choose the --delete-during
              algorithm when talking to rsync 3.0.0 or newer, and the
              --delete-before algorithm when talking to an older rsync.  See
              also --delete-delay and --delete-after.


       --delete-excluded
              In addition to deleting the files on the receiving side that are
              'not' on the sending side, this tells rsync to also delete any
              files on the receiving side that are 'excluded' (see --exclude).
              See the FILTER  RULES  section for a way to make individual
              exclusions behave this way on the receiver, and for a way to
              protect files from --delete-excluded.  See --delete (which is
                  implied) for more details on file-deletion.


       -f, --filter=RULE
              This option allows you to add rules to selectively 'exclude'
              certain files from the list of files to be transferred. This is
              most useful in combination with a recursive transfer.

              You  may  use  as  many --filter options on the command line as
              you like to build up the list of files to exclude.  If the filter
              contains whitespace, be sure to quote it so that the shell gives
              the rule to rsync as a single argument.  The text below also
              mentions that you can use an underscore to replace the space that
              separates a rule from its arg.

              See the FILTER RULES section for detailed information on this
              option.

FILTER RULES
       The filter rules allow for flexible selection of which files to transfer
       (include) and which files to skip (exclude).  The rules either directly
       specify include/exclude  patterns  or  they  specify  a  way  to  acquire
       more include/exclude patterns (e.g. to read them from a file).

       As  the list of files/directories to transfer is built, rsync checks each
       name to be transferred against the list of include/exclude patterns in
       turn, and the first matching pattern is acted on:  if it is an exclude
       pattern, then that file is skipped; if it is an include pattern then that
       filename is not skipped; if no matching pattern is found, then the
       filename is not skipped.

       Rsync builds an ordered list of filter rules as specified on the
       command-line.  Filter rules have the following syntax:

              RULE [PATTERN_OR_FILENAME]
              RULE,MODIFIERS [PATTERN_OR_FILENAME]

       You have your choice of using either short or long RULE names, as
       described below.  If you use a short-named rule, the ',' separating the
       RULE from the MODIFIERS is optional.  The  PATTERN  or  FILENAME  that
       follows  (when present) must come after either a single space or an
       underscore (_).  
       
       Here are the available rule 'prefixes':

              exclude, - specifies an exclude pattern.
              include, + specifies an include pattern.

              merge, . specifies a merge-file to read for more rules.

              dir-merge, : specifies a per-directory merge-file.
              hide, H specifies a pattern for hiding files from the transfer.
              show, S files that match the pattern are not hidden.

              protect, P specifies a pattern for protecting files from deletion.

              risk, R files that match the pattern are not protected.
              clear, ! clears the current include/exclude list (takes no arg)

       When rules are being read from a file, empty lines are ignored, as are
       comment lines that start with a "#".

INCLUDE/EXCLUDE PATTERN RULES
       You can include and exclude files by specifying patterns using the filter
       rules.  The include/exclude rules each specify a pattern that is matched
       against  the names of the files that are going to be transferred.  These
       patterns can take several forms:

       o      if the pattern 'ends' with a / then it will only match a directory,
              not a regular file, symlink, or device.

       o      rsync chooses between doing a simple string match and wildcard
       matching by checking if the pattern contains one of these three wildcard
       characters: '*', '?', and '[' .

       o      a '*' matches any path component, but it stops at slashes.

       o      use '**' to match anything, including slashes.

       o      a '?' matches any character except a slash (/).

       o      a '[' introduces a character class, such as [a-z] or [[:alpha:]].

       o      if the pattern contains a / (not counting a trailing /) or a "**",
       then it is matched against the full pathname, including any leading
         directories. If the pattern doesn't contain a / or a "**", then it is
         matched only against the final component of the filename.  (Remember
             that the algorithm is applied recursively so "full filename" can
             actually be any portion of a path from the starting directory on
             down.)

MERGE-FILE FILTER RULES
       You can merge whole files into your filter rules by specifying either a
       merge (.) or a dir-merge (:) filter rule (as introduced in the FILTER
           RULES section above).

       There are two kinds of merged files -- single-instance ('.') and
       per-directory (':').  A single-instance merge file is read one time, and
       its rules are incorporated into the filter list in the place of  the  "."
       rule.   For per-directory  merge  files,  rsync  will scan every
       directory that it traverses for the named file, merging its contents when
       the file exists into the current list of inherited rules.  These
       per-directory rule files must be created on the sending side because it
       is the sending side that is being scanned for the available files to
       transfer.  These rule files may also need to be transferred to the
       receiving side if you want them  to  affect  what files don't get deleted
       (see PER-DIRECTORY RULES AND DELETE below).

       Some examples:

              merge /etc/rsync/default.rules
              . /etc/rsync/default.rules
              dir-merge .per-dir-filter
              dir-merge,n- .non-inherited-per-dir-excludes
              :n- .non-inherited-per-dir-excludes

       The following modifiers are accepted after a 'merge' or dir-merge rule:

       o      A - specifies that the file should consist of only exclude
       patterns, with no other rule-parsing except for in-file comments.

       <ex>
        ~/source/DEVARCH$ cat /home/kpark/source/setup-humax/rsync.filter 
        P var/applications/
        - include/
        - **.a

        # files
        - oss/bin/msggrep
        - oss/bin/envsubst
        - oss/lib/gstreamer-1.0/*.la
        - oss/lib/libmpeg*


<config>

DESCRIPTION

The rsyncd.conf file is the runtime configuration file for rsync when run with
the --daemon option. When run in this way rsync becomes a rsync server listening
on TCP port 873. Connections from rsync clients are accepted for either
anonymous or authenticated rsync sessions.

The rsyncd.conf file controls authentication, access, logging and available
modules. 

MODULE OPTIONS

After the global options you should define a number of modules, each module
exports a directory tree as a symbolic name. Modules are exported by specifying
a module name in square brackets [module] followed by the options for that
module. 

secrets file
    The "secrets file" option specifies the name of a file that contains the
    username:password pairs used for authenticating this 'module'. This file is
    only consulted if the "auth users" option is specified. The file is line
    based and contains username:password pairs separated by a single colon. Any
    line starting with a hash (#) is considered a comment and is skipped. The
    passwords can contain any characters but be warned that many operating
    systems limit the length of passwords that can be typed at the client end,
    so you may find that passwords longer than 8 characters don't work.  There
    is no default for the "secrets file" option, you must choose a name (such as
    /etc/rsyncd.secrets). The file must normally not be readable by "other"; see
    "strict modes". 


[root@HUMAX /]# cat /etc/rsyncd.conf 
log file = /var/log/rsyncd.log
pid file = /run/rsyncd.pid
lock file = /run/rsync.lock

[Root]
   path = /
   comment = Humax box
   uid = root
   gid = root
   read only = no
   list = yes
   auth users = zinc
   secrets file = /etc/rsyncd.scrt

[root@HUMAX /]# cat /etc/rsyncd.scrt 
zinc:zinc


<ex>
$ mkdir dir1 dir2
$ touch dir1/file{1..10}
$ ls dir1
file1  file10  file2  file3  file4  file5  file6  file7  file8  file9

$ rsync -r dir1/ dir2      " okay as synced
$ ls dir2
file1  file10  file2  file3  file4  file5  file6  file7  file8  file9

note: This trailing / is necessary to mean "the contents of dir1".

$ rsync -r dir1 dir2       " not okay as
$ ls dir2
dir1


The -a option is a combination flag.

It stands for "archive" and syncs recursively and preserves symbolic links, special and device
files, modification times, group, owner, and permissions. It is more commonly used than -r and is
usually what you want to use.

The -n or --dry-run options. 

The -v flag (for verbose). 

The -P flag is very helpful. 
It combines the flags --progress and --partial. The first of these gives you a progress bar for the
transfers and the second allows you to resume interrupted transfers:

The -z flag. 
If you are transferring files that have not already been compressed, like text files, you can reduce
the network transfer by adding compression with the -z option.

<update>
Update the modification time on some of the files and see that rsync intelligently re-copies only
the changed files:

<delete>
By default, rsync does not delete anything from the destination directory. We can change this
behavior with the --delete option. Before using this option, use the --dry-run option and do testing
to prevent data loss:

rsync -a --delete source destination

<ex> to see a list of files which are different

rsync -rvn --size-only /home/kyoupark/git/kb/asan/libsanitizer-from-630/ libsanitizer

<example> over SSH
rsync -a ~/dir1 username@remote_host:destination_directory
rsync -a username@remote_host:/home/username/dir1 place_to_sync_on_local_machine

<example>
Want to copy all expect .git from source to destination

# do not copy hidden files
rsync -av --progress /home/kit/mheg-remote-git/mag_shared/* . --exclude .git

# copy all
rsync -av --progress /home/kit/mheg-remote-git/mag_shared/ . --exclude .git

<example>
setupRsyncDaemon() {

    # Setup rsync daemon for faster non-encrypted transfer
    rsync ${privatekey:+ --rsh="ssh -i $privatekey"} \
        "$(thisScriptSrcDir)"/rsyncd.{conf,scrt} root@$stbip:/etc/
    $ssh "chmod o-rwx /etc/rsyncd.scrt && rsync --daemon --ipv4"
    export RSYNC_PASSWORD=zinc
}


={============================================================================
*kt_linux_tool_027* tool-awk

Another popular stream editor. The basic function of awk is to search files for lines or other text
units containing one or more patterns. When a line matches one of the patterns, special actions are
performed on that line.

<data-driven>
Programs in awk are different from programs in most other languages, because awk programs are
"data-driven": you describe the data you want to work with and then what to do when you find it.
Most other languages are "procedural."

<rule>
The program consists of a series of rules. Each rule specifies one pattern to search for and one
action to perform upon finding the pattern.

<command>
print

The print command in awk outputs selected data from the input file. $0 (zero)
holds the value of the entire line.

ls -l | awk '{ print $5 $9 }'

With formatting.

awk '{ print "Size is " $5 " bytes for " $9 }'

<regex>
awk 'EXPRESSION { PROGRAM }' file(s)

For files ending in ".conf" and starting with either "a" or "x", using extended regular expressions
note: the below do not work.

ls -l | awk '/\<(a|x).*\.conf$/ { print $9 }'

To add text before output: begin message. beginning of execution before any input has been processed

awk 'BEGIN { print "Files found:\n" } /\<[a|x].*\.conf$/ { print $9 }'

To add text after output:

awk '/\<[a|x].*\.conf$/ { print $9 } END { print \ "Can I do anything else for you, mistress?" }'

<variables>
field separator

awk 'BEGIN { FS=":" } { print $1 "\t" $5 }' /etc/passwd

output field separator

> cat test
record1 data1
record2 data2

> awk '{ print $1 $2}' test
record1data1
record2data2

> awk '{ print $1, $2}' test
record1 data1
record2 data2

<example>
To make a filename from the date:

date: Fri Jul 25 05:30:12 BST 2014 -> log-Fri-Jul-25-...

script -f /home/kit/log/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`

To make a gateway from the ip address:
gateway=`echo $ip | awk 'BEGIN { FS="." } ; { print $1"."$2"."$3"."1 }'`

<ex>
"-MS_CDI_CONTROL- Adding chunk to chunk input queue. " "chunk input fd = 332, p_address = 0x2c152000, num bytes = 385024 " "chunk_id = 336
cat  LOGlastrun.log.unix | grep -e "chunk input fd = 351" | awk '{c=$36; sum+=c; print "| " $40 "|" c "|" sum "|"}' 


={============================================================================
*kt_linux_tool_028* tool-sed

https://www.gnu.org/software/sed/manual/html_node/index.html#Top

A stream editor is used to perform basic transformations on text read from a
file or a pipe. The result is sent to standard output. The editor does not
modify the original input.

What distinguishes sed from other editors, such as vi and ed, is its ability to
filter text that it gets from a pipeline feed. You do not need to interact with
the editor while it is running. 

This feature allows use of editing commands in scripts, greatly easing
`repetitive editing tasks` When facing replacement of text in a 'large' number
of files, sed is a great help.


{command}
SYNOPSIS
       sed [OPTION]... {script-only-if-no-other-script} [input-file]...

<option>
       -e script, --expression=script
              add the script to the commands to be executed

       -i[SUFFIX], --in-place[=SUFFIX]
              edit files in place (makes backup if SUFFIX supplied)

       -n, --quiet, --silent
              suppress automatic printing of pattern space

        note: since sed print out line which are not in matches so use -n to
        output only matches.

       -u, --unbuffered
              load minimal amounts of data from the input files and flush the
              output buffers more often

       s/regexp/replacement/
              Attempt  to  match  regexp  `against the pattern space` .  If
              successful, replace that portion matched with replacement.  The
              replacement may contain the special character & to refer to that
              portion of the pattern space which matched, and 
              
              the special escapes \1 through \9 to refer to the corresponding
              matching `sub-expressions` in the regexp.


   Commands which accept address ranges

       p      Print the current pattern space.


   Zero- or One- address commands

      Q: Why this when can use "1i text"?

       // i \
       // `text`   Insert `text`, which has each embedded newline preceded by a
       // backslash.

       // \\ from online gnu sed
       // \\
       // \\ i\
       // \\ text
       // \\ As a GNU extension, this command accepts two addresses.
       // \\
       // \\ Immediately output the lines of text which follow this command (each
       // \\  but the last ending with a \, which are removed from the output). 

<ex>
Add text at the line 3. So if calls it repeatdly then it grows downwards.

sed -i '3i NDS_BUILD_TYPE_'$1' = debug' ${CWD}/projects/$PROJECT/build_options.mk

The double quote works as well:

sed -i "1i xxx" LOGlastrun_realtime_46d4


<ex>
echo "translation/darwin_783e53e0c7f8_36feaec3b67d50d62258f0d5ae9cdb40/.detailed_output \
       translation/darwin_783e53ffa5bd_8a5775d303cba39df2721bfce433cc58/.detailed_output"

$FILES has one string which has many files from commends output. If run grep
on that, will give 1 even if there are multiple matches.

NUM_FOUND=``echo $FILES | grep -c detailed_output``

To make one string multiple lines, run sed using sub expression.

note: why 2?
NUM_FOUND=``echo $FILES | sed 's/\(detailed_output\)/\2\n/g' | grep -c detailed_output``


<ex>
sed -i "s?.*\(BR2_TOOLCHAIN_EXTERNAL_PATH\).*?\1=\"$NDS_FUSIONOS_TOOLCHAIN_PATH\"?" target/device/Sky/$SPK_PLATFORM/buildroot-${SPK_VARIANT}.config
# + sed -i 's?.*\(BR2_TOOLCHAIN_EXTERNAL_PATH\).*?\1="/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2"?' \
    target/device/Sky/ams-drx890/buildroot-debug.config

# BR2_TOOLCHAIN_EXTERNAL_PATH="/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20091015"
# BR2_TOOLCHAIN_EXTERNAL_PATH="/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2"


<address>
3.2 Selecting lines with sed

Addresses in a sed `script` can be in any of the following forms:

number
    Specifying a line number will match only that line in the input. 
    (Note that sed counts lines continuously across all input files unless -i
     or -s options are specified.) 


<ex>
sed -n '2,4p' example
sed -n '3,$p' example

// before
libTitaniumDeviceAuthoritySystemDbusClient.so createDbusSystemFactory

sed -i \
  -e '1i libTitaniumDeviceAuthoritySystemOff.so createOffSystemFactory' \
  -e 's/^/# DA disabled by patch-a-tron: /' [FILE]

// after
libTitaniumDeviceAuthoritySystemOff.so createOffSystemFactory
# DA disabled by patch-a-tron: libTitaniumDeviceAuthoritySystemDbusClient.so
      createDbusSystemFactory


<ex>
Here 's/&.*//', '&' is literal but not special.

$ echo "STREAM=R015&PLATFORM=drx890&PRIORITY=1&MACLIST=&LOGS=999999" | \
    grep 'PLATFORM=' | sed -e's/.*PLATFORM=//' -e 's/&.*//'
drx890


<busybox>
sed

    sed [-efinr] SED_CMD [FILE]...

    Options:

            -e CMD  Add CMD to sed commands to be executed
            -f FILE Add FILE contents to sed commands to be executed
            -i      Edit files in-place
            -n      Suppress automatic printing of pattern space
            -r      Use extended regex syntax

    If no -e or -f is given, the first non-option argument is taken as the sed
    command to interpret. All remaining arguments are names of input files; if
    no input files are specified, then the standard input is read. Source files
    will not be modified unless -i option is given.


={============================================================================
*kt_linux_tool_029* pyserial and grabserial

http://elinux.org/Grabserial
https://github.com/tbird20d/grabserial

If no options are specified, grabserial uses serial port /dev/ttyS0, at 115200 baud with "8, None
and 1" (8N1) settings. 

alias gse="sudo grabserial -v -d "/dev/ttyUSB0" -b 115200 -w 8 -p N -s 1 \
| tee 2>&1 ~/logs/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`"

alias gse="sudo grabserial -v -d /dev/ttyUSB0 \
| tee 2>&1 ~/logs/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`"


={============================================================================
*kt_linux_tool_030* tool-diff tool-patch

{diff} <from-to-and-unified>
To compare two files:

diff [options] from-file to-file
diff -u original new

-u    Use the 'unified' output format.

This outputs a description of how to `transform original into new` to stdout
in unified format which is the easiest to read. The description is called a
patch.

You can compare a whole directory tree:

diff -u -r directory1 directory2

This recurses the directory structures. Whenever a file differs, the patch for
that file is appended to the output.

To save the patches to a file that you can store or send to someone else,
   simply redirect stdout to a file, e.g:

diff -u file1 file2 > my_changes.patch


{unified-format}
A typical patch looks like this:

--- a/vobs/DARWIN_APPLICATIONS/DARWIN_GAUDI_ORCHID/build/applications/GAUDI_Orchid/EPG/statmgr/xtv_states/QhsmPVR.c
+++ b/vobs/DARWIN_APPLICATIONS/DARWIN_GAUDI_ORCHID/build/applications/GAUDI_Orchid/EPG/statmgr/xtv_states/QhsmPVR.c
@@ -2815,7 +2815,6 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
 
         /*set the playback speed to normal*/
         DBGMSG_M("QHsmPVR_backWard_skip - PVR_NAV_SetSpeed(PVR_NAV_PLAY)\n");
-        PVR_NAV_SetSpeed(PVR_NAV_PLAY);
 
         /*check if we are in RB mode*/
         CONSELECTCO_GetCurrentPlayRBFlag( &state);
@@ -2827,6 +2826,7 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
         {
             /*In RB mode*/
             PVR_NAV_SetPosition ( PVR_NAV_RELATIVE_POSITION_START , 1000);
+           PVR_NAV_SetSpeed(PVR_NAV_PLAY);^M
         }
         else{
             /*We are in playback mode*/
@@ -2840,6 +2840,7 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
             if (rc != PVR_NAV_RC_OK) {
                 DBGMSG_M("No previous skip point; skipping to beginning and waiting for timeout;");
                 PVR_NAV_SetPosition ( PVR_NAV_RELATIVE_POSITION_START , 0);
+               PVR_NAV_SetSpeed(PVR_NAV_PLAY);^M
                 prevPoint = -1;
             }
             else {

<header>
The first two lines describe the files to be transformed. The file indicated
by "---" is `transformed into` the file indicated by "+++". Let's call them
the old file and the new file.

<hunks>
This header is followed by a series of "hunks". Each hunk describes a changes
to make to a section of the file. 

The @@ symbols indicate the 'start' of a hunk. The first set of numbers, e.g.
"-2815,7" indicates that in the old file, the section 'started' on line 2815
and 'lasted' for 7 lines. The second set of numbers, e.g. "+2815,6" indicates
that in the new file, the section starts on like 2815 and lasts for 6 lines.
(You can guess that we are going to remove a line.)

      <fuzz>
       -F num  or  --fuzz=num
          Set  the  maximum  fuzz  factor.  This option only applies to diffs
          that have context, and causes patch to ignore up to that many lines
          in looking for places to install a hunk.  Note that a larger fuzz
          factor increases the odds of a faulty patch.  The default fuzz
          factor is 2, and it may not be set to more than the number of lines
          of  context  in  the context diff, ordinarily 3.

        <ex>
        this makes it failed since used --fuzz=0 and fuzz=2 when use without
        --fuzz option.

        + /usr/bin/patch -s -p1 --fuzz=0
        1 out of 2 hunks FAILED -- saving rejects to file lib/unwind-forcedunwind.c.rej



<p-option>
The text after the @@ symbol indicates the function name that the change is
in. This is generated by the -p option of diff `so may not always be there` It
is just to make the patch easier to read.

       -p, --show-c-function
              show which C function each change is in

<context>
Next we have three lines of context. These are just the three lines before the
change. They are used to check that the change is going to be made in the
right place. For example, it is possible that you want to apply the patch to a
different version of the file it was created with. These lines help you
manually or automatically apply the change in the right place, even if the
line numbers have changed. The context is also used to detect when a patch
cannot be applied because of a conflict. More on this later.

<changes>
Next we have the change itself. Lines are either removed, indicated by a -
sign, or added, indicated by a + sign. When a line is changed, it usually
appears as a removed line followed by an added line.

Finally we have three more lines of context.

So now you can read patches and even manually apply simple ones. But with
complicated patches, you would prefer to apply them automatically.


{patch}
https://www.gnu.org/software/diffutils/manual/html_mono/diff.html
https://linuxacademy.com/blog/linux/introduction-using-diff-and-patch/

NAME
       patch - apply a diff file `to an original`

DESCRIPTION
       patch  takes  a  patch  file  patchfile  containing  a difference
       listing produced by the diff program and applies those differences to
       one or more original files, producing patched versions.  Normally the
       patched versions are put in place of the originals.  Backups can be
       made; see the -b or --backup option.  The names of the files to be
       patched are usually taken from the patch file, but if there's just one
       file to be patched it can be specified on the command line as
       originalfile.

cd directory_containing_file_to_change

patch < my_changes.patch


<p-option>
If you have a patch with lots of files in different directories, you might do
this:

cd parent_directory

patch -p1 < my_changes.patch

note: from where runs patch

The number after the p tells patch `how 'many' directory names to strip from`
the filenames before applying the patch. For example, typically the person
generating the patch did something like this:

cd my_code
cd ..
cp -R my_code my_original_code

...make changes to lots of files...

diff -u -r my_original_code my_code

This means that the patch file will contain filenames like this:

--- my_original_code/directory/file.c
+++ my_code/directory/file.c

Now you probably don't have directories called "my_code" and
"my_original_code", so you would cd to "directory" and tell patch to ignore
the first directory in each file path by using the "-p1" option.


<reverse>
You can reverse a patch if you have finished testing it, or if you want to see
whether a problem has been introduced by a particular patch. You should also
reverse a patch prior to adding a newer, updated version of the same patch. To
reverse the patch, use the patch command with the -R option:

       -R  or  --reverse
          Assume  that  this  patch  was created with the old and new files
          swapped.  (Yes, I'm afraid that does happen occasionally, human
              nature being what it is.)  patch attempts to swap each hunk
          around before applying it.  Rejects come out in the swapped format.
          The -R option does not work with ed diff scripts because there is
          too little information to reconstruct the reverse operation.

          If the first hunk of a patch fails, patch reverses the hunk to see
          if it can be applied that way.  If it can, you are asked if you want
          to have the -R option set.  If it can't, the patch continues to be
          applied normally.  (Note: this method cannot detect a reversed patch
              if it is a normal diff  and if  the  first  command is an append
              (i.e. it should have been a delete) since appends always
              succeed, due to the fact that a null context matches anywhere.
              Luckily, most patches add or change lines rather than delete
              them, so most reversed normal diffs begin with a delete, which
              fails, triggering the heuristic.)


<patching-non-identical-files>
GNU patch will try to apply the changes even if you are applying them to a
different version of the file to the one the patch was originally created
against. It uses the context lines to do this. As long as enough context lines
appear, and they have not moved to far from their original positions, the
change will be applied.

For example, if a new function was added to the top of the file, the change to
a function further down the file would still be correctly applied.  This means
it can be more useful to send someone a patch than a whole file, because it
will often still work if the person you send it to is starting with a
different version of the file.

If GNU patch decides it cannot apply a change, you will see:

HUNK FAILED

along with details of which hunk it was. You can then investigate why the hunk
failed - patches are easy enough to read to do this.


<concatenating-patches>
Patches can be concatenated, so you can combine multiple patches into one:

diff -u my_code_orig/file1.c my_code/file1.c > patch1
diff -u my_code_orig/file2.c my_code/file2.c > patch2
cat patch1 patch2 > big_patch


{example}
-a     Treat all files as text and compare them line-by-line, even if they do not seem to be text.
-N
--new-file
   In directory comparison, if a file is found in only one directory, treat it as present but empty
   in the other directory.

diff -Naur darwin-spk-1.18_orig/target/device/Sky/ams-drx890/kernel-debug.config darwin-spk-1.18/target/device/Sky/ams-drx890/kernel-debug.config
--- darwin-spk-1.18_orig/target/device/Sky/ams-drx890/kernel-debug.config	2011-10-05 12:36:12.000000000 +0100
+++ darwin-spk-1.18/target/device/Sky/ams-drx890/kernel-debug.config	2012-01-18 15:23:41.000000000 +0000
@@ -1157,7 +1157,7 @@
 CONFIG_DEBUG_FS=y
 # CONFIG_WANT_EXTRA_DEBUG_INFORMATION is not set
 CONFIG_CROSSCOMPILE=y
-CONFIG_CMDLINE="mem=160M rw console=uart,mmio,0x10400b00,115200n8 ip_conntrack_tftp.ports=4085"
+CONFIG_CMDLINE="mem=166M rw console=uart,mmio,0x10400b00,115200n8 ip_conntrack_tftp.ports=4085"
 CONFIG_SYS_SUPPORTS_KGDB=y
 # CONFIG_MIPS_BRCM_SIM is not set


<patch-options> in the script

       -d dir  or  --directory=dir
          Change to the directory dir immediately, before doing anything else.

       --dry-run
          Print  the results of applying the patches without actually changing
          any files.

       -E  or  --remove-empty-files
          Remove output files that are  empty  after  the  patches  have  been
          applied.  Normally this option is unnecessary, since patch can examâ
          ine the time stamps on the header to determine whether a file should
          exist  after  patching.  However, if the input is not a context diff
          or if patch is conforming to POSIX,  patch  does  not  remove  empty
          patched  files  unless  this  option is given.  When patch removes a
          file, it also attempts to remove any empty ancestor directories.

       -f  or  --force
          Assume that the user knows exactly what he or she is doing,  and  do
          not  ask any questions.  Skip patches whose headers do not say which
          file is to be patched; patch files even though they have  the  wrong
          version  for  the Prereq: line in the patch; and assume that patches
          are not reversed even if they look like they are.  This option  does
          not suppress commentary; use -s for that.

       -s  or  --silent  or  --quiet
          Work silently, unless an error occurs.

   patch)
      # For each patch, try to derive whether it was created as -p0 or -p1.
      # p0 is typically from svn/cvs/git, while p1 will typically come from diff.
      patchlevel=

      patch -d $SPKDIR -p1 --dry-run --quiet -f < $p > /dev/null 2>&1
      p1=$?
      if [ $p1 -eq 0 ]; then
         patchlevel=-p1
      fi

      patch -d $SPKDIR -p0 --dry-run --quiet -f < $p > /dev/null 2>&1
      p0=$?
      if [ $p0 -eq 0 ]; then
         patchlevel=-p0
      fi
      if [ $p0 -eq 0 -a $p1 -eq 0 ]; then
         echo "WARNING: patch '$p' can be applied -p0 or -p1. Using -p0."
         patchlevel=-p0
      fi
      if [ -z "$patchlevel" ]; then
         echo "Unable to apply patch '$p'"
         exit 1
      fi
      echo "Applying patch '$p'..."
      patch -d $SPKDIR $patchlevel -E < $p
      if [ $? -ne 0 ]; then
         echo "Patch $p failed. Aborting."
         exit 1
      fi
      ;;

<ex> want to see the list of files but not contents
$ diff -qr /mnt/asan/gcc/gcc-4.9.4/libsanitizer .


={============================================================================
*kt_linux_tool_031* tool-zip tool-tar

{zip}
       -# --fast --best
              Regulate  the  speed of compression using the specified digit #,
              where -1 or --fast indicates the fastest compression method
              (less compression) and -9 or --best indicates the slowest com‐
              pression method (best compression).  The default compression
              level is -6 (that is, biased towards high compression at expense
                  of speed).


-d --decompress --uncompress 
-l --list

" maintain the original file
       -c --stdout --to-stdout
              Write output on standard output; keep original files unchanged.
              If there are several input files, the output consists of a
              sequence of independently  com- pressed  members.  To  obtain
              better compression, concatenate all input files before
              compressing them.

gzip -c ramdisk_rootfs_img > ramdisk_rootfs_img.gz
/usr/bin/bzip2 -dc /usr/src/redhat/SOURCES/linux-libc-headers-2.6.18.0.tar.bz2


{7z}
https://www.ibm.com/developerworks/community/blogs/6e6f6d1b-95c3-46df-8a26-b7efd8ee4b57/entry/how_to_use_7zip_on_linux_command_line144?lang=en

1. Create an archive
$ 7z a basic.7z basic

2. Extract an archive
$ 7z e basic.7z

3. List archive details
$ 7z l basic.7z


{tool-tar}
SYNOPSIS
       tar [OPTION...] [FILE]...

DESCRIPTION
       GNU tar saves many files together into a single tape or disk `archive`,
       and can restore individual files `from the archive`


       tar -cf archive.tar foo bar
              # Create archive.tar from files foo and bar.


# maintain permissions when create a archive
% tar cvzfp xxx.tgz ./

// to extract
tar zxvf *.tgz% 

// extract single file
tar zxvf *.tgz filename

-f, --file ARCHIVE
use archive file or device ARCHIVE

# '-' is used instead of filename after -f
# to extract from stdin. wget write to stdout and tar read from stdin. '-' used differently.
wget http://xxx.tar.bz2 -O - | tar -xjf -

<tool-bz2>
     -j, --bzip2

     -p, --preserve-permissions, --same-permissions
           extract information about file permissions (default for superuser)

// to extract bz file
% tar xjf *.bz2

// to create bzip2
% tar cvjfp filename

% bzip2 -d gdb.bz2

// to list
% tar tvf 


# extract single file or folder
tar xvf WALRUS_6833.tgz deps/FUSIONOS_INPUT_DEL_2/DEL_AMS_BCM_MIPS4K_LNUX_DARWIN_01/Tarball/

<options>
       -C, --directory=DIR
              change to directory DIR
              note: can use to change dir and to extract there

        ex. 
        tar -cjvf "$SPK_TARBALL_MW" -C nds-mw .
        tar cvjfp debug.bz2 -C debug/ .

       -f, --file=ARCHIVE
              use archive file or device ARCHIVE

       -c, --create
              create a new archive

       -z, --gzip
              filter the archive through gzip

Since when run $nds-mw> tar -cjvf xx.bz . then there will be xx.bz which is
empty in the archive. To get around this, use -C to move dir and do zip but
the output will be make before moving a dir.

<symlink>
When use tar on dir which is a symlink, then tar don not work. Any options?


<ex>
    tar -c * --exclude UPLOAD.TGZ | gzip > UPLOAD.TGZ


{problem}
Have a x.gz which is actually tar file and gunzip to unzip but has one single
file. The problem is that the single file has lots of broken contents.

When use tar command to untar, see lots of files in that. So seems that when
unzip that, it puts all files into one file? 


tar --strip-components=1 -C \
/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4 \
\ --exclude='libjava/*'  --exclude='libgo/*'  --exclude='gcc/testsuite/*'
                      --exclude='libstdc++-v3/testsuite/*'   -xf -


={============================================================================
*kt_linux_tool_031* tool-tar-sparce

From The Buildroot user manual

8.7 Dealing efficiently with filesystem images

Filesystem images can get pretty big, depending on the filesystem you choose,
the number of packages, whether you provisioned free space. Yet, some
locations in the filesystems images may just be empty (e.g. a long run of
zeroes); such a file is called a sparse file.

Most tools can handle sparse files efficiently, and will only store or write
those parts of a sparse file that are not empty.  For example:

  tar accepts the -S option to tell it to only store non-zero blocks of sparse files:

  tar cf archive.tar -S [files...] will efficiently store sparse files in a tarball
  tar xf archive.tar -S will efficiently store sparse files extracted from a tarball

  cp accepts the --sparse=WHEN option (WHEN is one of auto, never or always):

  cp --sparse=always source.file dest.file will make dest.file a sparse file
  if source.file has long runs of zeroes

Other tools may have similar options. Please consult their respective man
pages.  You can use sparse files if you need to store the filesystem images
(e.g. to transfer from one machine to another), or if you need to send them
(e.g. to the Q&A team).

Note however that flashing a filesystem image to a device while using the
sparse mode of dd may result in a broken filesystem (e.g. the block bitmap of
an ext2 filesystem may be corrupted; or, if you have sparse files in your
filesystem, those parts may not be all-zeroes when read back). 

You should `only use` sparse files when handling files on the build machine, not
when transferring them to an actual device that will be used on the target.


={============================================================================
*kt_linux_tool_032* tool-split tool-merge

       split [OPTION]... [INPUT [PREFIX]]

       Output  fixed-size  pieces of INPUT to PREFIXaa, PREFIXab, ...; default
       size is 1000 lines, and `default PREFIX is 'x'`  

       -b, --bytes=SIZE
              put SIZE bytes per output file

$ split -b 200MB build-full-after-clean.log build-full-after-clean.

build-full-after-clean.aa
...
build-full-after-clean.ci

To merge:

cat xxx.* > outfile



={============================================================================
*kt_linux_tool_032* tool-tr

NAME
       tr - translate or delete characters

SYNOPSIS
       tr [OPTION]... SET1 [SET2]

DESCRIPTION
       Translate, squeeze, and/or delete characters `from standard input`,
       writing to standard output.

       -d, --delete
              delete characters in SET1, do not translate

       Translation  occurs if -d is not given and both SET1 and SET2 appear.
       -t may be used only when translating.  SET2 is extended to length of
       SET1 by repeating its last character as necessary.  Excess characters
       of SET2 are ignored.  Only [:lower:] and [:upper:] are guaranteed to
       expand in ascending order; used in SET2 while translating, they may
       only be used in pairs to specify case conversion.  -s uses SET1 if not
       translating nor deleting; else squeezing uses SET2 and occurs after
       translation or deletion.

# from net
If both the SET1 and SET2 are specified and ‘-d’ OPTION is not specified, then
tr command will replace each characters in SET1 with each character in same
position in SET2.

<ex>
    PROFILE=$(echo ${PROFILE} | tr "[:upper:]" "[:lower:]")


$ echo "SKY_trials" | tr '[:upper:]' '[:lower:]'
sky_trials


={============================================================================
*kt_linux_tool_032* tool-dd

DD(1)

NAME
       dd - convert and copy a file

SYNOPSIS
       dd [OPERAND]...
       dd OPTION

DESCRIPTION
       Copy a file, converting and formatting according to the operands.

       bs=BYTES
              read and write up to BYTES bytes `at a time`

       if=FILE
              read from FILE instead of stdin

       of=FILE
              write to FILE instead of stdout

       seek=BLOCKS
              skip BLOCKS obs-sized blocks at start of output

       count=BLOCKS
              copy only BLOCKS input blocks

       note:
       Must use `count` since not know the default and if not use, will have
       unknown size of input/output. May be bigger than you need.

       BLOCKS  and  BYTES  may  be  followed  by  the  following
       multiplicative suffixes: c =1, w =2, b =512, kB =1000, K =1024, MB
       =1000*1000, M =1024*1024, xM =M GB =1000*1000*1000, G =1024*1024*1024,
       and so on for T, P, E, Z, Y.

<ex>
dd of="${core_output_path}/core.$pid" bs=1M

<ex> create a binary file filled with zeros. must use seek.
dd of=zeros.bin if=/dev/null bs=10 seek=2

<ex>
$ hexdump -C -n 20 PCAT.DB
00000000  53 51 4c 69 74 65 20 66  6f 72 6d 61 74 20 33 00  |SQLite format 3.|
00000010  08 00 01 01                                       |....|
00000014

$ dd if=PCAT.DB bs=10c count=1
SQLite for1+0 records in
1+0 records out
10 bytes (10 B) copied, 1.0746e-05 s, 931 kB/s

$ dd if=PCAT.DB bs=20c count=1
SQLite format 1+0 records in
1+0 records out
20 bytes (20 B) copied, 1.3354e-05 s, 1.5 MB/s

$ dd if=PCAT.DB bs=30c count=1
SQLite format @  ▒1+0 records in
1+0 records out
30 bytes (30 B) copied, 1.2526e-05 s, 2.4 MB/s

$ hexdump -C -n 20 PCAT.DBJ
00000000  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|
00000010  00 00 00 00                                       |....|
00000014

$ dd if=PCAT.DBJ bs=20c count=1
1+0 records in
1+0 records out
20 bytes (20 B) copied, 1.427e-05 s, 1.4 MB/s

So dd tries to print out outputs on stdin and prints out only displayable
chars? Anyway, there will be no output for null byte stream.


={============================================================================
*kt_linux_tool_033* getconf

       getconf - Query system configuration variables

       getconf [-v specification] system_var

       -a
       Displays all configuration variables for the current system
       and their values.

       -v
       Indicate the specification and version for which to obtain
       configuration variables.

       system_var
       A system configuration variable, as defined by sysconf(3) or
       confstr(3).

$ getconf PAGESIZE
4096


={============================================================================
*kt_linux_tool_034* tool-ps tool-top tool-pstree

DESCRIPTION
       ps displays information about a selection of the active processes.  If
       you want a repetitive update of the selection and the displayed
       information, use top(1) instead.

       This version of ps accepts several kinds of options:

       1   UNIX options, which may be grouped and `must be preceded` by a dash.
       2   BSD options, which may be grouped and `must not be used` with a dash.
       3   GNU long options, which are preceded by two dashes.

       Options of different types may be freely mixed, but conflicts can appear.
       There are some synonymous options, which are functionally identical, due
       to the many standards and ps implementations that this ps is compatible
       with.

       Note that "ps -aux" is distinct from "ps aux".  The POSIX and UNIX
       standards require that "ps -aux" print all processes owned by a user
       named "x", as well as printing all processes that would be selected by
       the -a option.  If the user named "x" does not exist, this ps may
       interpret the command `as "ps aux" instead and print a warning.` This
       behavior is intended to aid in transitioning old scripts and habits.  It
       is fragile, subject to change, and thus should not be relied upon.

       By default, ps selects 'all' processes with the same effective user ID
       (euid=EUID) as the current user and associated with the same terminal as
       the invoker.  It displays the process ID (pid=PID), the terminal
       associated with the process (tname=TTY), the cumulated CPU time in
       [DD-]hh:mm:ss format (time=TIME), and the executable name (ucmd=CMD).
       Output is unsorted by default.

       The use of BSD-style options will add process state (stat=STAT) to the
       default display and show the command args (args=COMMAND) instead of the
       executable name.  You can override this with the PS_FORMAT environment
       variable.  The use of BSD-style options will also change the process
       selection to include processes on other terminals (TTYs) that are owned
       by you; alternately, this may be described as setting the selection to be
       the set of all processes filtered to exclude processes owned by other
       users or not on a terminal.  These effects are not considered when
       options are described as being "identical" below, so -M will be
       considered identical to Z and so on.

       Except as described below, process selection options are additive.  The
       default selection is discarded, and then the selected processes are added
       to the set of processes to be displayed.  A process will thus be shown if
       it meets any of the given selection criteria.


EXAMPLES

       To see every process on the system using BSD syntax:
          ps ax
          ps axu

SIMPLE PROCESS SELECTION

       a      Lift the BSD-style "only yourself" restriction, which is imposed
       upon the set of all processes when some BSD-style (without "-") options
       are used or when the ps personality setting is BSD-like.  The set of
       processes selected in this manner is in addition to the set of
       processes selected by other means.  An alternate description is that
       this option causes ps to list all processes with a terminal (tty), or
       to list all processes when used together with the x option.

       x      Lift the BSD-style "must have a tty" restriction, which is
       imposed upon the set of all processes when some BSD-style (without "-")
       options are used or when the ps personality setting is BSD-like.  The
       set of processes selected in this manner is in addition to the set of
       processes selected by other means.  An alternate description is that
       this option causes ps to list all processes owned by you (same EUID as
           ps), or to list all processes when used together with the a option.

       -A     Select all processes.  Identical to -e.
       -e     Select all processes.  Identical to -A.

PROCESS SELECTION BY LIST
       These options accept a single argument in the form of a blank-separated
       or comma-separated list.  They can be used multiple times.  For
       example: ps -p "1 2" -p 3,4

       -u userlist
              Select by effective user ID (EUID) or name.  This selects the
              processes whose effective user name or ID is in userlist.

              The effective user ID describes the user whose file access
              permissions are used by the process (see geteuid(2)).  Identical
              to U and --user.

THREAD DISPLAY
       H      Show threads as if they were processes.

       *linux-tid*
       -L     Show threads, possibly with LWP and NLWP columns.

OUTPUT FORMAT CONTROL
       These options are used to choose the information displayed by ps.  The
       output may differ by personality.

       u      Display user-oriented format.

       -f     Do full-format listing. This option can be combined with many
       other UNIX-style options to add additional columns.  It also causes the
       command arguments to be printed.  When used with -L, the NLWP (number of
               threads) and LWP (thread ID) columns will be added.  See the c
       option, the format keyword args, and the format keyword comm.

STANDARD FORMAT SPECIFIERS
       Here are the different keywords that may be used to control the output
       format (e.g. with option -o) or to sort the selected processes with the
       GNU-style --sort option.

       Some keywords may not be available for sorting.

       CODE        HEADER    DESCRIPTION

       args        COMMAND   command with all its arguments as a string.

       Modifications to the arguments may be shown. The output in this column
       may contain spaces. A process marked <defunct> is partly dead, waiting
       to be fully destroyed by its parent.  Sometimes the process args will be
       unavailable; when this happens, ps will instead print the executable name
       in brackets. (alias cmd, command).
       
       See also the comm format keyword, the -f option, and the c option.  When
       specified last, this column will extend to the edge of the display.  If
       ps can not determine display width, as when output is redirected (piped)
       into a file or another command, the output width is undefined (it may be
               80, unlimited, determined by the TERM variable, and so on).  The
       COLUMNS environment variable or --cols option may be used to exactly
       determine the width in this case.  The w or -w option may be also be used
       to adjust width.

       command     COMMAND   See args.  (alias args, command).

       wchan       WCHAN     name of the kernel function in which the process is
       sleeping, a "-" if the process is running, or a "*" if the process is
       multi-threaded and ps is not displaying threads.


       lwp         LWP       light weight process (thread) ID of the
       dispatchable entity (alias spid, tid).  See tid for additional
       information.

       tgid        TGID      a number representing the thread group to which a
       task belongs (alias pid).  It is the process ID of the thread group
       leader.

       *linux-tid*
       tid         TID       the unique number representing a dispatacable
       entity (alias lwp, spid).  This value may also appear as: a process ID
       (pid); a process group ID (pgrp); a session ID for the session leader
       (sid); a thread group ID for the thread group leader (tgid); and a tty
       process group ID for the process group leader (tpgid).


<ex>
UID        PID  PPID   LWP  C NLWP    SZ   RSS PSR STIME TTY          TIME CMD
16384     3051  1272  3089  0   32 81585 62568   0 Jan21 ?        00:00:00 /opt/stagecraft-2.0/bin/stagecraft --astrace --bgalpha 0 --outputrect 0,0,1280,720 --extensionsdir /opt/zinc-trunk/lib/stagecraft2-extensions --profile extendedTV --modulemap IGraphicsDriver:/opt/stagecraft-2.0/bin/libIGraphicsDriver2.so /app


<ex>
" to get gpid of the shell
$ ps -o pgid $$
 PGID
 7523

$ ps -p $$ -o 'pid pgid sid command'
  PID  PGID   SID COMMAND
 3697  3697  3697 bash

<ex>
ps axf

<ex>
The number of threads, per process

ps -eLf | grep -E 'zin[c]|df[b]' | awk '{print $10}' | uniq -c | sort -nr

Can also show to see what the threads are up to right now:

ps -eAL -o command,wchan | grep -E 'zin[c]|df[b]' | sort | uniq -c | sort -nr


<top>
SYNOPSIS
       top -hv|-bcHiOSs -d secs -n max -u|U user -p pid -o fld -w [cols]

       The traditional switches `-' and whitespace are optional.

Help for Interactive Commands - procps version 3.2.7
Window 1:Def: Cumulative mode Off.  System: Delay 3.0 secs; Secure mode Off.

  Z,B       Global: 'Z' change color mappings; 'B' disable/enable bold
  l,t,m     Toggle Summaries: 'l' load avg; 't' task/cpu stats; 'm' mem info
  1,I       Toggle SMP view: '1' single/separate states; 'I' Irix/Solaris mode

  f,o     . Fields/Columns: 'f' add or remove; 'o' change display order
  F or O  . Select sort field
  <,>     . Move sort field: '<' next col left; '>' next col right
  R,H     . Toggle: 'R' normal/reverse sort; 'H' show threads
  c,i,S   . Toggle: 'c' cmd name/line; 'i' idle tasks; 'S' cumulative time
  x,y     . Toggle highlights: 'x' sort field; 'y' running tasks
  z,b     . Toggle: 'z' color/mono; 'b' bold/reverse (only if 'x' or 'y')
  u       . Show specific user only
  n or #  . Set maximum tasks displayed

  k,r       Manipulate tasks: 'k' kill; 'r' renice
  d or s    Set update interval
  W         Write configuration file
  q         Quit
          ( commands shown with '.' require a visible task display window )
Press 'h' or '?' for help with Windows,
any other key to continue


={============================================================================
*kt_linux_tool_300* tool-kill

kill -s SIGSEGV 113
kill -s 11 113
kill -11 `pidof w3cEngine`
kill -s 11 `pidof w3cEngine`

note:
kill -9 do not create a core.

export P=`ps -a | grep APP_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;
export P=`ps -a | grep MW_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;

SIG_KERNEL_COREDUMP_MASK (.../kernel/signal.c) defines sianals to create a core.

SIGSEGV(segmentation fault)

#define SIG_KERNEL_COREDUMP_MASK (\
        M(SIGQUIT)   |  M(SIGILL)    |  M(SIGTRAP)   |  M(SIGABRT)   | \
        M(SIGFPE)    |  M(SIGSEGV)   |  M(SIGBUS)    |  M(SIGSYS)    | \
        M(SIGXCPU)   |  M(SIGXFSZ)   |  M_SIGEMT                     )

{kill-0}

while kill -0 "${parent_pid}" &>/dev/null;
do
   usleep 500
done

From man 2 kill

If sig is 0, then no signal is sent, but error checking is still performed; this
can be used to check for the existence of a process ID or process group ID.

<ex>
#!/bin/bash
kill -0 323232 && echo "pid 323232 is present.."
kill -0 4355 && echo "pid 4355 x sess mgr is present.."

$ ./sbsh.sh 
./sbsh.sh: line 3: kill: (323232) - No such process
pid 4355 x sess mgr is present..

<ex>
$ killall -0 `name`             // when the process is running
$
$ killall -0 `name`             // when no process is running
killall: `name`: no process killed


{kill}
       -l, --list [signal]
              List  signal  names.   This  option has optional argument, which
              will convert signal number to signal name, or other way round.


// for embedded

# kill -l
HUP INT QUIT ILL TRAP ABRT EMT FPE KILL BUS SEGV SYS PIPE ALRM TERM USR1 USR2
CHLD PWR WINCH URG IO STOP TSTP CONT TTIN TTOU VTALRM PROF XCPU XFSZ

// for pc

$ kill -l
 1) SIGHUP	 2) SIGINT	 3) SIGQUIT	 4) SIGILL	 5) SIGTRAP
 6) SIGABRT	 7) SIGBUS	 8) SIGFPE	 9) SIGKILL	10) SIGUSR1
11) SIGSEGV	12) SIGUSR2	13) SIGPIPE	14) SIGALRM	15) SIGTERM
16) SIGSTKFLT	17) SIGCHLD	18) SIGCONT	19) SIGSTOP	20) SIGTSTP
21) SIGTTIN	22) SIGTTOU	23) SIGURG	24) SIGXCPU	25) SIGXFSZ
26) SIGVTALRM	27) SIGPROF	28) SIGWINCH	29) SIGIO	30) SIGPWR
31) SIGSYS	34) SIGRTMIN	35) SIGRTMIN+1	36) SIGRTMIN+2	37) SIGRTMIN+3
...
63) SIGRTMAX-1	64) SIGRTMAX	


{default-signal}
The signal name difference cause compatibility between machines. For example,
    the below do not work on embedded.

kill -SIGINT dbussenddaemon &>/dev/null

The default signal for kill is TERM. So uses default signal to kill remaining
processes

-INT only interrupted a process rather request terminating it.

-    kill -INT %?dbussenddaemon &>/dev/null
+    kill %?dbussenddaemon &>/dev/null

       -s <signal>
       --signal <signal>
              Specify the signal to be sent.  The signal can be specified by
              using name or number.  The behavior of signals is explained in
              signal(7) manual page.


{tool-killall}
killall - kill processes by name

killall sends a signal to all processes running any of the specified commands.
If no signal name is specified, SIGTERM is sent.


<ex> from core(5) man example

           $ sleep 100
           ^\                     # type control-backslash
           Quit (core dumped)

           argc[4]=<sig=3>

^\ causes SIGQUIT and quit but not cause a core.


={============================================================================
*kt_linux_tool_034* tool-mem

<from-tool-top>
KiB Mem : 12773500+total,  3288604 free,  4055064 used, 12039134+buff/cache
KiB Swap: 10485756 total, 10188804 free,   296952 used. 12244304+avail Mem

// sort all processes by memory used
$ top -o %MEM


<from-tool-free>
$ free
              total        used        free      shared  buff/cache   available
Mem:      127735008     4053380     3290304      365184   120391324   122444748
Swap:      10485756      296952    10188804

make free a bit more user-friendly by adding the -m option, like so: free -m.
This will report the memory usage in MB

$ free -m
              total        used        free      shared  buff/cache   available
Mem:         124741        3782        3393         356      117564      119750
Swap:         10239         290        9949

$ free -mt
              total        used        free      shared  buff/cache   available
Mem:         124741        3781        3395         356      117564      119752
Swap:         10239         290        9949
Total:       134981        4071       13345


<proc-meminfo>
$ less /proc/meminfo


={============================================================================
*kt_linux_tool_034* tool-chrt

NAME
       chrt - manipulate real-time attributes of a process

SYNOPSIS
       chrt [options] prio command [arg]...
       chrt [options] -p [prio] pid

DESCRIPTION
       chrt(1) sets or retrieves the real-time scheduling attributes of an
       existing PID or runs COMMAND with the given attributes. Both policy
       (one of SCHED_OTHER, SCHED_FIFO, SCHED_RR, SCHED_BATCH, or SCHED_IDLE)
       and priority can be set and retrieved.

       The SCHED_BATCH policy is supported since Linux 2.6.16. The SCHED_IDLE
       policy is supported since Linux 2.6.23.

OPTIONS

       -r, --rr
              set scheduling policy to SCHED_RR (the default)

AVAILABILITY
       The chrt command is part of the util-linux-ng package and is available
       from ftp://ftp.kernel.org/pub/linux/utils/util-linux-ng/.

schedutils                         Apr 2003                            CHRT(1)

<ex>
chrt -r 97 /usr/bin/logfile-monitor.sh </dev/null &


={============================================================================
*kt_linux_tool_034* tool-nproc

NAME
       nproc - print the number of processing units available


={============================================================================
*kt_linux_tool_035* tool-wget tool-curl

{wget}
wget http://xxx.tar.bz2 -O - | tar -xjf -

-O file
--output-document=file
The documents will not be written to the appropriate files, but all will be
concatenated together and written to file. If - is used as file, documents will
be printed to standard output, disabling link conversion. 

--no-check-certificate

<ex> gets result from the page rather then saving that page.

pi@raspberrypi ~/snugupdate-v2-snugberrypi/www $ more time.php
<?php echo time(); ?>

id=`wget -q -O - http://$pi_addr/time.php`

kyoupark@st-castor-03:~/bootlog$ wget -q -O - http://10.209.60.87/time.php
1505796602kyoupark@st-castor-03:~/bootlog$ time


<to-get-all>
wget -e robots=off -r -l 1 -nd https://stb-tester-master/s/

       -q
       --quiet
           Turn off Wget's output.

       -e command
       --execute command
           Execute command as if it were a part of .wgetrc.  A command thus
           invoked will be executed after the commands in .wgetrc, thus taking
           precedence over them.  If you need to specify more than one wgetrc
           command, use multiple instances of -e.

   Recursive Retrieval Options
       -r
       --recursive
           Turn on recursive retrieving.    The default maximum depth is 5.

       -l depth
       --level=depth
           Specify recursion maximum depth level depth.


   Directory Options
       -nd
       --no-directories
           Do not create a hierarchy of directories when retrieving recursively.
           With this option turned on, all files will get saved to the current
           directory, without clobbering (if a name shows up more than once, the
                   filenames will get extensions .n).


{curl}
curl  is  a  tool  to  transfer data from or to a server, using one of the
supported protocols (DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS, IMAP, IMAPS,
        LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, TELNET and
        TFTP). The command is designed to work without user interaction.

curl offers a busload of useful tricks like proxy support, user authentication,
     FTP upload, HTTP post, SSL connections, cookies, file transfer resume and
     more. As  you  will  see below, the number of features will make your head
     spin!

curl is powered by libcurl for all transfer-related features. See libcurl(3) for
details.

PROGRESS METER
       curl  displays  this  data  to  the  terminal  by default, so if you
       invoke curl to do an operation and it is about to write data to the
       terminal, it disables the progress meter as otherwise it would mess up
       the output mixing progress meter and response data.

curl --silent --show-error -o "$(basename $galliumurl)" "$galliumurl"


       -o, --output <file>
              Write output to <file> instead of stdout. If you are using {} or
              [] to fetch multiple documents, you can use '#' followed by a
              number in  the  <file>  specifier.  That  variable  will  be
              replaced with the current string for the URL being fetched. Like
              in:

                curl http://{one,two}.site.com -o "file_#1.txt"

              or use several variables like:

                curl http://{site,host}.host[1-5].com -o "#1_#2"

              You may use this option as many times as the number of URLs you have.

              See also the --create-dirs option to create the local
              directories dynamically. Specifying the output as '-' (a single
                  dash) will force the output to be done to stdout.

<ex>
When wget fails, use curl 

wget https://zinc-logs.gz

ERROR: The certificate of 'stb.co.uk' is not trusted.
ERROR: The certificate of 'stb.co.uk' hasn't got a known issuer.

curl --insecure https://zinc-logs.gz > zinc-logs.gz


<curl-error>

curl: (60) SSL certificate problem: certificate is not yet valid
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a "bundle"
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
If this HTTPS server uses a certificate signed by a CA represented in
 the bundle, the certificate verification probably failed due to a
 problem with the certificate (it might be expired, or the name might
 not match the domain name in the URL).
If you'd like to turn off curl's verification of the certificate, use
 the -k (or --insecure) option.

https://serverfault.com/questions/549679/issued-certificate-not-yet-valid-with-wget
Your system clock is likely set in the past.

note: this works and see *tool-date*


{libcurl}
https://curl.haxx.se/libcurl/c/curl_easy_perform.html
https://curl.haxx.se/libcurl/c/curl_easy_cleanup.html

https://curl.haxx.se/changes.html

 Fixed in 7.33.0 - October 14 2013

Changes:
    cookies: add expiration 


={============================================================================
*kt_linux_tool_036* nc

netcat is a simple unix utility which reads and writes data across network connections, using
TCP or UDP protocol. It is designed to be a reliable "back-end" tool that can be used directly or
easily driven by other programs and scripts. At the same time, it is a feature-rich network
debugging and exploration tool, since it can create almost any  kind of connection you would need
and has several interesting built-in capabilities. 


In the simplest usage, "nc host port" creates a TCP connection to the given port on the given target
host. Your standard input is then sent to the host, and anything that  comes back across the
connection is sent to your standard output. This continues indefinitely, until the network side of
the connection shuts down.  Note that this behavior is different from most other applications which
shut everything down and exit after an end-of-file on the standard input.


note: on host but not the target, when run nc without -q then no prompt back when run so need to
specify -q to get prompt back.

printf "D\t${key}\n\0U\t${key}\n\0" | nc -q 1 $box_ip $box_port

-q seconds   after EOF on stdin, wait the specified number of seconds and then quit. If seconds is
negative, wait forever.


={============================================================================
*kt_linux_tool_037* port checks

Type the following command to see list well-known of TCP and UDP port numbers:

$ less /etc/services
$ grep -w 80 /etc/services 


{port-numbers}
Typically port number less than 1024 are used by well know network servers such as Apache. Under
UNIX and Linux like oses root (super user) privileges are required to open privileged ports. Almost
all clients uses a high port numbers for short term use.

The port numbers are divided into three ranges:

1. Well Known Ports: those from 0 through 1023.
2. Registered Ports: those from 1024 through 49151
3. Dynamic and/or Private Ports: those from 49152 through 65535

You can increase local port range by typing the following command (Linux specific example):
# echo 1024 65535 > /proc/sys/net/ipv4/ip_local_port_range
#
You can also increase or decrease socket timeout (Linux specific example):
# echo 2000 > /proc/sys/net/ipv4/tcp_keepalive_time

# for debian pc and embedded linux
#
root# cat /proc/sys/net/ipv4/ip_local_port_range 
32768	61000

<to-see-open-ports>

netstat [address_family_options] [--tcp|-t] [--udp|-u] [--raw|-w] [--listening|-l] [--all|-a]
[--numeric|-n] [--numeric-hosts] [--numeric-ports] [--numeric-users] [--symbolic|-N]
[--extend|-e[--extend|-e]] [--timers|-o] [--program|-p] [--verbose|-v] [--continuous|-c]


$ netstat -tulpn
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      -               
tcp        0      0 0.0.0.0:47541           0.0.0.0:*               LISTEN      -               
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:3350          0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      -               
tcp        0      0 0.0.0.0:3389            0.0.0.0:*               LISTEN      -               
tcp6       0      0 :::111                  :::*                    LISTEN      -               
tcp6       0      0 :::22                   :::*                    LISTEN      -               
tcp6       0      0 ::1:631                 :::*                    LISTEN      -               
tcp6       0      0 :::33815                :::*                    LISTEN      -               
tcp6       0      0 ::1:25                  :::*                    LISTEN      -               

To displays listening sockets (open ports)

$ netstat -nat | grep LISTEN


To list open IPv4 connections use the lsof command:

$ lsof -Pnl +M -i4
COMMAND     PID     USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME
icedove    3900     1024   58u  IPv4 1836476      0t0  TCP 172.20.33.215:38273->132.245.226.18:993 (ESTABLISHED)
skype      4588     1024   11u  IPv4   15436      0t0  UDP 127.0.0.1:35553 
skype      4588     1024   35u  IPv4 1859784      0t0  TCP 172.20.33.215:51348->157.56.192.193:443 (ESTABLISHED)
skype      4588     1024   36w  IPv4   17138      0t0  TCP *:28890 (LISTEN)
skype      4588     1024   45u  IPv4   17139      0t0  UDP *:28890 
skype      4588     1024   54u  IPv4   18119      0t0  TCP 172.20.33.215:54124->157.55.235.145:40030 (ESTABLISHED)
skype      4588     1024   92u  IPv4   15527      0t0  TCP 172.20.33.215:41335->91.190.218.55:12350 (ESTABLISHED)
iceweasel  9539     1024   62u  IPv4 1585820      0t0  TCP 172.20.33.215:36101->216.58.208.69:443 (ESTABLISHED)
iceweasel  9539     1024   65u  IPv4 1856332      0t0  TCP 172.20.33.215:44701->74.125.195.189:443 (ESTABLISHED)
iceweasel  9539     1024   74u  IPv4 1813863      0t0  TCP 172.20.33.215:42211->31.221.26.57:80 (ESTABLISHED)
iceweasel  9539     1024   77u  IPv4 1545135      0t0  TCP 172.20.33.215:44615->185.45.5.50:443 (ESTABLISHED)
iceweasel  9539     1024   87u  IPv4 1839953      0t0  TCP 172.20.33.215:56696->185.45.5.43:443 (ESTABLISHED)
hipchat.b 11542     1024   33u  IPv4   57796      0t0  TCP 172.20.33.215:36851->54.161.161.10:5222 (ESTABLISHED)
ssh       19536     1024    3r  IPv4 1852553      0t0  TCP 172.20.33.215:42930->172.20.33.192:22 (ESTABLISHED)
ssh       19607     1024    3r  IPv4 1744563      0t0  TCP 172.20.33.215:43054->172.20.33.192:22 (ESTABLISHED)


<firewall>
In other words, Apache port is open but it may be blocked by UNIX (pf) or Linux (iptables) firewall.
You also need to open port at firewall level. In this example, open tcp port 80 using Linux iptables
firewall tool:

$ /sbin/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT
$ /sbin/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 2033 -j ACCEPT
$ service iptables save


={============================================================================
*kt_linux_tool_016* tool-screen

Screen is a full-screen software program that can be used to multiplexes a
physical console between several processes (typically interactive shells). It
offers a user to open several separate terminal instances inside a one single
terminal window manager.

sudo screen -S name -a -D -R -fn -l /dev/ttyUSB0 115200,cs8

<help>
       C-a ?       (help)        Show key bindings.

<close>
to close screen use Ctrl-A, k, y. Do not use Ctrl-C as it can kill processes
running on the box.

       C-a k
       C-a C-k     (kill)        Destroy current window.


{logging}
Use -L for auto logging and will log on 'screenlog.0'. Use this file to see:

-L   tells screen to turn on automatic output logging for the windows.

screen -a -D -R -fn -l -L /dev/ttyUSB0 115200,cs8


       C-a h       (hardcopy)    Write a hardcopy of the current window to the
       file "hardcopy.n".

       C-a H       (log)         Begins/ends logging of the current window to
       the file "screenlog.n".

<monitoring>
       C-a M       (monitor)     Toggles monitoring of the current window.


={============================================================================
*kt_linux_tool_038* tool-minicom

SYNOPSIS
       minicom [-somMlwz8] [-c on|off] [-S script] [-d entry]
               [-a on|off] [-t term] [-p pty] [-C capturefile] [configuration]

       -c   Color usage. Some terminals (such as the Linux console) support
       color with the standard ANSI escape sequences. Because there is
       apparently no termcap support for color, these escape sequences are
       hard-coded  into  mini- com. Therefore this option is off by default.
       You can turn it on with '-c on'. This, and the '-m' option, are good
       candidates to put into the MINICOM environment variable.

1. Setup minicon with -s switch

$ minicom -s

Choose Serial port setup and specify below settings, come out of Serial port
setup by pressing Esc and then save configuration file.


<config>
Once saved, it will store the configuration in /etc/minicom/. To connect to your
device, run minicom with config file name:

$ minicom cisco or minicom -c on cisco

To Exit from minicom, Click Ctrl-A and Z then X.

alias mc='sudo minicom -C hmax.log -c on hmax'

<when-cannot-get-shell-prompt>
Without knowing either your set-top box or your cable, I would first try
disabling hardware flow control, since the set top probably doesn't implement
it. Essentially your Linux client is waiting for an "OK to send" signal that it
will never receive because there's no physical wire in the set top to send it

<log>
C-A Z L

minicom.cap


={============================================================================
*kt_linux_tool_024* tool-du tool-df tool-stat

<tool-du>
http://www.gnu.org/software/coreutils/du

14.2 du: Estimate file space usage

du reports the amount of disk space used by the set of specified files and for
each subdirectory (of directory arguments). 

Synopsis:
du [option]... [file]...

With no arguments, du reports the disk space for the `current directory.`

Normally the disk space is printed in units of 1024 bytes, but this can be
overridden (see Block size). Non-integer quantities are rounded up to the next
higher unit.

If two or more hard links point to the same file, only one of the hard links
is counted. The file argument order affects which links are counted, and
changing the argument order may change the numbers and entries that du
outputs.

‘-h’
‘--human-readable’
Append a size letter to each size, such as ‘M’ for mebibytes. Powers of 1024
are used, not 1000; ‘M’ stands for 1,048,576 bytes. This option is equivalent
to --block-size=human-readable. Use the --si option if you prefer powers of
1000.


<ex>
// to print one level
du -h --max-depth=1

// to show all and total
du -ach
du -sh


<tool-df>
http://www.gnu.org/software/coreutils/manual/html_node/df-invocation.html#df-invocation

14.1 df: Report file system disk space usage

df reports the amount of disk space `used and available` on file systems. 

Synopsis:

df [option]... [file]...

With no arguments, df reports the space used and available on all currently
mounted file systems (of all types). Otherwise, df reports on the file system
containing each argument file.

Normally the disk space is printed in units of 1024 bytes, but this can be
overridden (see Block size). Non-integer quantities are rounded up to the next
higher unit.

For bind mounts and without arguments, df only outputs the statistics for that
device with the shortest mount point name in the list of file systems (mtab),
i.e., it hides duplicate entries, unless the -a option is specified.

With the same logic, df elides a mount entry of a dummy pseudo device if there
is another mount entry of a real block device for that mount point with the
same device number, e.g. the early-boot pseudo file system ‘rootfs’ is not
shown per default when already the real root device has been mounted.

If an argument file resolves to a special file containing a mounted file
system, df shows the space available on that file system rather than on the
file system containing the device node. GNU df does not attempt to determine
the disk usage on unmounted file systems, because on most kinds of systems
doing so requires extremely nonportable intimate knowledge of file system
structures.

‘-T’
‘--print-type’
Print each file system’s type. The types printed here are the same ones you
can include or exclude with -t and -x. The particular types printed are
whatever is supported by the system. Here are some of the common names (this
    list is certainly not exhaustive):


<tool-stat>
NAME
       stat - display file or file system status

       --printf=FORMAT
              like --format, but interpret backslash escapes, and do not
              output a mandatory trailing newline.  If you want a newline,
              include \n in FORMAT.


BusyBox v1.19.4 (2015-11-13 23:53:27 KST) multi-call binary.

Usage: stat [OPTIONS] FILE...

Display file (default) or filesystem status

        -c fmt  Use the specified format
        -f      Display filesystem status
        -L      Follow links
        -t      Display info in terse form

Valid format sequences for file systems:
 %a     Free blocks available to non-superuser
 %b     Total data blocks in file system
 %c     Total file nodes in file system
 %d     Free file nodes in file system
 %f     Free blocks in file system
 %i     File System ID in hex
 %l     Maximum length of filenames
 %n     File name
 %s     Block size (for faster transfer)    
 %S     Fundamental block size (for block counts)
 %t     Type in hex
 %T     Type in human readable form


stat --printf="%s" file.any
stat -f -c "%a %s" /mnt/hd1


<ex>
2) The script uses 'ls' and 'awk' to get the size of the debug log file (in
bytes):

CUR_LOGFILE_SIZE=$(ls -l $LOGFILE | awk '{print $5}')

This method was chosen because

- The STBs haven't got 'stat', thus 'stat --printf="%s" $LOGFILE' wasn't possible.

- 'du' gives the size only in units >= kB.
note:
ture only for busybox

-sh-3.2# du --help
BusyBox v1.13.1 (2017-04-27 13:00:17 BST) multi-call binary

Usage: du [-aHLdclsxhmk] [FILE]...

Summarize disk space used for each FILE and/or directory.
Disk space is printed in units of 1024 bytes.

- Ditto for 'ls -s' (where one could have done a 'cut -d" " -f1').

- 'wc -c <$LOGFILE' would have worked but apparently 'wc' scans the whole file
  which is impractical considering the size the log file can reach.

- The output of 'ls -l' was found too unreliable to use 'cut' or 'sed' on it
  (for example, the number of spaces between fields seemed to vary, as did the
  owner of the file).


={============================================================================
*kt_linux_tool_024* tool-quota

       quota - display disk usage and limits

SYNOPSIS
       quota [ -F format-name ] [ -guqvswi ] [ -l | [ -QAm ]]
       quota [ -F format-name ] [ -qvswi ] [ -l | [ -QAm ]] -u user...
       quota [ -F format-name ] [ -qvswi ] [ -l | [ -QAm ]] -g group...
       quota [ -F format-name ] [ -qvswugQm ] -f filesystem...

DESCRIPTION
       quota displays users' disk usage and limits.  By default only the user
       quotas are printed.

       quota  reports the quotas of all the filesystems listed in /etc/mtab.
       For filesystems that are NFS-mounted a call to the rpc.rquotad on the
       server machine is performed to get the information.

       -u, --user
              flag is equivalent to the default.

       -v, --verbose
              will display quotas on filesystems where no storage is
              allocated.

       -s, --human-readable
              option will make quota(1) try to choose units for showing
              limits, used space and used inodes.


The meaning of the the columns in the output is as follows:

Value       Description

Filesystem  
The mounted file system, usually in the form <server>:/<directory>. Use df
<directory> to determine the mounted file system for a given directory

blocks      Used disk space in kilobytes

quota
Soft limit for disk usage (you can still write new files or data when exceeded
but only until the grace time is up)

limit       
Hard limit for disk usage (you cannot write new files or data anymore when
exceeded)

grace       
The grace time indicates the time left to remove data when you exceed the soft
quota (when this time is up you cannot write new files or data anymore)

file        
Number of files

quota       
Soft limit for number of files (you can still write new files when exceeded
but only until the grace time is up)

limit       
Hard limit for number of files (you cannot write new files anymore when
exceeded)

grace       
The grace time indicates the time left to delete files when you exceed the
soft quota (when this time is up you cannot write new files anymore)


={============================================================================
*kt_linux_tool_051* tool-fdisk

       -l, --list
              List the partition tables for the specified devices and then
              exit.  If no devices are given, those mentioned in
              /proc/partitions (if that file exists) are used.

       -u     When listing partition tables, give sizes in sectors instead of
              cylinders.


fdisk -lu

Disk /dev/sda: 2000.3 GB, 2000398934016 bytes
255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors
Units = sectors of 1 * 512 = 512 bytes

   Device Boot      Start         End      Blocks  Id System
/dev/sda1              64  3907007999  1953503968   1 FAT12


<set-swap>
Command (m for help): t
Partition number (1-3, default 3): 3
Hex code (type L to list all codes): L

 1  FAT12           27  Hidden NTFS Win 82  Linux swap / So c1  DRDOS/sec (FAT-

Hex code (type L to list all codes): 82

Changed type of partition 'Linux' to 'Linux swap / Solaris'.

Command (m for help): p
Disk /dev/sda: 30.5 TiB, 33554432000000 bytes, 65536000000 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x4d92b346

Device     Boot    Start      End  Sectors  Size Id Type
/dev/sda1  *        2048 62916607 62914560   30G 83 Linux
/dev/sda2       62916608 64350207  1433600  700M 82 Linux swap / Solaris
/dev/sda3       64350208 85321727 20971520   10G 82 Linux swap / Solaris


={============================================================================
*kt_linux_tool_051* tool-fdisk-resize-boot-partition
https://www.nonamehosts.com/blog/how-to-extend-ext4-root-partition-without-reboot/

note:
Works for Debian Jessy

How to extend ext4 root partition without reboot

Let’s say you want to extend existing disk on your already installed VM.
Usually to extend the existing ext4 partition where system is running you
would use some Live CD, to edit partition while it’s unmounted. However it’s
possible to extend the partition without booting from Live CD. Here are some
simple steps to do so:

Here we see that we have disk bigger than already existing partition:

root@test:~# df -h /
Filesystem Size Used Avail Use% Mounted on
/dev/vda1 24G 1008M 22G 5% /
 
root@test:~# fdisk -l /dev/vda
 
Disk /dev/vda: 53.7 GB, 53687091200 bytes
255 heads, 63 sectors/track, 6527 cylinders, total 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0009256d
 
 Device Boot Start End Blocks Id System
/dev/vda1 * 2048 50329215 25163584 83 Linux
/dev/vda2 50329216 52426367 1048576 82 Linux swap / Solaris

First of all you have to make sure to turn off swap:
root@test:~# swapoff -a
root@test:~# free -m
             total       used       free     shared    buffers     cached
Mem:           994        189        804          0         11        140
-/+ buffers/cache:         38        955
Swap:            0          0          0

#Make sure that you see zeroes in "Swap:" row

And now let’s remove swap partition and extend root partition. From previous
fdisk command we can see that our swap partition is second one (/dev/vda2).

In following steps we’re going to remove swap partition and root partition,
                           create root partition with new size (size should be
                               = [last sector]  [swap partition sector count])
                             and finally new swap partition with same size.

root@test:~# fdisk /dev/vda

Command (m for help): p

Disk /dev/vda: 53.7 GB, 53687091200 bytes
255 heads, 63 sectors/track, 6527 cylinders, total 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0009256d

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048    50329215    25163584   83  Linux
/dev/vda2        50329216    52426367     1048576   82  Linux swap / Solaris

Command (m for help): d
Partition number (1-4): 2

Command (m for help): d
Selected partition 1

Command (m for help): n
Partition type:
   p   primary (0 primary, 0 extended, 4 free)
   e   extended
Select (default p):
Using default response p
Partition number (1-4, default 1):
Using default value 1
First sector (2048-104857599, default 2048):
Using default value 2048
Last sector, +sectors or +size{K,M,G} (2048-104857599, default 104857599): +103809023

Command (m for help): n
Partition type:
   p   primary (1 primary, 0 extended, 3 free)
   e   extended
Select (default p):
Using default response p
Partition number (1-4, default 2):
Using default value 2
First sector (103811072-104857599, default 103811072):
Using default value 103811072
Last sector, +sectors or +size{K,M,G} (103811072-104857599, default 104857599):
Using default value 104857599

Command (m for help): p

Disk /dev/vda: 53.7 GB, 53687091200 bytes
255 heads, 63 sectors/track, 6527 cylinders, total 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0009256d

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1            2048   103811071    51904512   83  Linux
/dev/vda2       103811072   104857599      523264   83  Linux


Before writing changes to disk we have to set proper partition types and make
root partition bootable:

Command (m for help): t
Partition number (1-4): 1
Hex code (type L to list codes): 83

Command (m for help): t
Partition number (1-4): 2
Hex code (type L to list codes): 82
Changed system type of partition 2 to 82 (Linux swap / Solaris)

Command (m for help): a
Partition number (1-4): 1

Command (m for help): p

Disk /dev/vda: 53.7 GB, 53687091200 bytes
255 heads, 63 sectors/track, 6527 cylinders, total 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0009256d

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048   103811071    51904512   83  Linux
/dev/vda2       103811072   104857599      523264   82  Linux swap / Solaris
	

Now let’s write changes to disk and re-read partition table (to re-read
partition table you can also reboot the VM) so the system could see new
size:

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.

WARNING: Re-reading the partition table failed with error 16: Device or resource busy.
The kernel still uses the old table. The new table will be used at
the next reboot or after you run partprobe(8) or kpartx(8)
Syncing disks.
root@test:~# partprobe
root@test:~# fdisk -l

Disk /dev/vda: 53.7 GB, 53687091200 bytes
16 heads, 63 sectors/track, 104025 cylinders, total 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0009256d

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048   103811071    51904512   83  Linux
/dev/vda2       103811072   104857599      523264   82  Linux swap / Solaris
	

Now we can see that we have new partition table with resized partitions,
                           however root partition’s filesystem has to be
                             resized as well. It’s one simple step left:

root@test:~# resize2fs /dev/vda1
resize2fs 1.42.9 (4-Feb-2014)
Filesystem at /dev/vda1 is mounted on /; on-line resizing required
old_desc_blocks = 2, new_desc_blocks = 4
The filesystem on /dev/vda1 is now 12976128 blocks long.

root@test:~# df -h /
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda1        49G 1009M   46G   3% /
	
Resize2fs did the trick and you can see that we have 49G partition now.




={============================================================================
*kt_linux_tool_051* tool-badblocks tool-smartctl tool-disk

{smartctl}
https://www.smartmontools.org/
https://www.thomas-krenn.com/en/wiki/SMART_tests_with_smartctl

<man>
https://www.smartmontools.org/browser/trunk/smartmontools/smartctl.8.in

-g NAME, --get=NAME, -s NAME[,VALUE], --set=NAME[,VALUE]

wcreorder[,on|off[,p]] - [ATA only] Gets/sets Write Cache Reordering. If it is
disabled (off), disk write scheduling is executed on a first-in-first-out
(FIFO) basis. If Write Cache Reordering is enabled (on), then disk write
scheduling may be reordered by the drive. 

If write cache is disabled, the current Write Cache Reordering state is
remembered but has no effect on non-cached writes, which are always written in
the order received. 

The state of Write Cache Reordering has no effect on either NCQ or LCQ queued
commands.

If ´,p´ is specified, the setting is preserved across power cycles.

smartctl s wcreorder,off /dev/device


<ex>
-sh-3.2# smartctl -t short -d ata /dev/sda
smartctl version 5.33 [mips-unknown-linux-gnu] Copyright (C) 2002-4 Bruce Allen
Home page is http://smartmontools.sourceforge.net/

=== START OF OFFLINE IMMEDIATE AND SELF-TEST SECTION ===
Sending command: "Execute SMART Short self-test routine immediately in off-line mode".
Drive command "Execute SMART Short self-test routine immediately in off-line mode" successful.
Testing has begun.
Please wait 1 minutes for test to complete.
Test will complete after Wed Jan 12 01:39:43 2000

Use smartctl -X to abort test.

-sh-3.2# smartctl -a -d ata /dev/sda
smartctl version 5.33 [mips-unknown-linux-gnu] Copyright (C) 2002-4 Bruce Allen
Home page is http://smartmontools.sourceforge.net/

=== START OF INFORMATION SECTION ===
Device Model:     ST3500312CS
Serial Number:    5VVAVM66
Firmware Version: SC13
User Capacity:    500,107,862,016 bytes
Device is:        Not in smartctl database [for details use: -P showall]
ATA Version is:   8
ATA Standard is:  Not recognized. Minor revision code: 0x29
Local Time is:    Wed Jan 12 01:40:50 2000 UTC
SMART support is: Available - device has SMART capability.
SMART support is: Enabled

=== START OF READ SMART DATA SECTION ===
SMART overall-health self-assessment test result: PASSED

General SMART Values:
Offline data collection status:  (0x00) Offline data collection activity
                                        was never started.
                                        Auto Offline Data Collection: Disabled.
Self-test execution status:      (   0) The previous self-test routine completed
                                        without error or no self-test has ever
                                        been run.
Total time to complete Offline
data collection:                 ( 623) seconds.
Offline data collection
capabilities:                    (0x73) SMART execute Offline immediate.
                                        Auto Offline data collection on/off support.
                                        Suspend Offline collection upon new
                                        command.
                                        No Offline surface scan supported.
                                        Self-test supported.
                                        Conveyance Self-test supported.
                                        Selective Self-test supported.
SMART capabilities:            (0x0003) Saves SMART data before entering
                                        power-saving mode.
                                        Supports SMART auto save timer.
Error logging capability:        (0x01) Error logging supported.
                                        General Purpose Logging supported.
Short self-test routine
recommended polling time:        (   1) minutes.
Extended self-test routine
recommended polling time:        ( 111) minutes.
Conveyance self-test routine
recommended polling time:        (   2) minutes.

SMART Attributes Data Structure revision number: 10
Vendor Specific SMART Attributes with Thresholds:
ID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE
  1 Raw_Read_Error_Rate     0x000f   114   099   006    Pre-fail  Always       -       63067029
  3 Spin_Up_Time            0x0003   097   097   000    Pre-fail  Always       -       0
  4 Start_Stop_Count        0x0032   081   081   020    Old_age   Always       -       19484
  5 Reallocated_Sector_Ct   0x0033   100   100   036    Pre-fail  Always       -       0
  7 Seek_Error_Rate         0x000f   084   060   030    Pre-fail  Always       -       310666279
  9 Power_On_Hours          0x0032   074   074   000    Old_age   Always       -       23640
 10 Spin_Retry_Count        0x0013   100   100   097    Pre-fail  Always       -       0
 12 Power_Cycle_Count       0x0032   097   097   020    Old_age   Always       -       3397
184 Unknown_Attribute       0x0032   100   100   099    Old_age   Always       -       0
187 Unknown_Attribute       0x0032   099   099   000    Old_age   Always       -       1
188 Unknown_Attribute       0x0032   100   100   000    Old_age   Always       -       0
189 Unknown_Attribute       0x003a   100   100   000    Old_age   Always       -       0
190 Unknown_Attribute       0x0022   054   049   045    Old_age   Always       -       807796782
194 Temperature_Celsius     0x0022   046   051   000    Old_age   Always       -       46 (Lifetime Min/Max 0/17)
195 Hardware_ECC_Recovered  0x001a   049   027   000    Old_age   Always       -       63067029
197 Current_Pending_Sector  0x0012   100   100   000    Old_age   Always       -       0
198 Offline_Uncorrectable   0x0010   100   100   000    Old_age   Offline      -       0
199 UDMA_CRC_Error_Count    0x003e   200   200   000    Old_age   Always       -       0

SMART Error Log Version: 1
ATA Error Count: 1
        CR = Command Register [HEX]
        FR = Features Register [HEX]
        SC = Sector Count Register [HEX]
        SN = Sector Number Register [HEX]
        CL = Cylinder Low Register [HEX]
        CH = Cylinder High Register [HEX]
        DH = Device/Head Register [HEX]
        DC = Device Command Register [HEX]
        ER = Error register [HEX]
        ST = Status register [HEX]
Powered_Up_Time is measured from power on, and printed as
DDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,
SS=sec, and sss=millisec. It "wraps" after 49.710 days.

Error 1 occurred at disk power-on lifetime: 13129 hours (547 days + 1 hours)
  When the command that caused the error occurred, the device was active or idle.

  After command completion occurred, registers were:
  ER ST SC SN CL CH DH
  -- -- -- -- -- -- --
  40 51 00 be 08 0d 00  Error: UNC at LBA = 0x000d08be = 854206

  Commands leading to the command that caused the error were:
  CR FR SC SN CL CH DH DC   Powered_Up_Time  Command/Feature_Name
  -- -- -- -- -- -- -- --  ----------------  --------------------
  c8 00 40 89 08 0d e0 00      00:13:34.787  READ DMA
  c8 00 40 09 08 0d e0 00      00:13:34.785  READ DMA
  c8 00 40 c9 06 0d e0 00      00:13:34.784  READ DMA
  c8 00 20 69 06 0d e0 00      00:13:34.784  READ DMA
  c8 00 20 49 06 0d e0 00      00:13:34.779  READ DMA

SMART Self-test log structure revision number 1
Num  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error
# 1  Short offline       Completed without error       00%     23640         -
# 2  Short offline       Completed without error       00%     21112         -
# 3  Short offline       Interrupted (host reset)      00%     21100         -
# 4  Short offline       Completed without error       00%     21068         -
# 5  Short offline       Completed without error       00%     21007         -
# 6  Short offline       Completed without error       00%     21002         -
# 7  Short offline       Completed without error       00%     20968         -
# 8  Short offline       Completed without error       00%     20939         -
# 9  Short offline       Completed without error       00%     20936         -
#10  Short offline       Completed without error       00%     20896         -
#11  Short offline       Completed without error       00%     20847         -
#12  Short offline       Completed without error       00%     20845         -
#13  Short offline       Completed without error       00%     20796         -
#14  Short offline       Completed without error       00%     20767         -
#15  Short offline       Interrupted (host reset)      00%     20766         -
#16  Short offline       Completed without error       00%     20642         -
#17  Short offline       Completed without error       00%     20642         -
#18  Short offline       Completed without error       00%     20641         -
#19  Short offline       Completed without error       00%     20611         -
#20  Short offline       Completed without error       00%     20581         -
#21  Short offline       Completed without error       00%     20551         -

SMART Selective self-test log data structure revision number 1
 SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS
    1        0        0  Not_testing
    2        0        0  Not_testing
    3        0        0  Not_testing
    4        0        0  Not_testing
    5        0        0  Not_testing
Selective self-test flags (0x0):
  After scanning selected spans, do NOT read-scan remainder of disk.
If Selective self-test is pending on power-up, resume after 0 minute delay.


<ex-failed>
SMART Self-test log structure revision number 1
Num  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error
# 1  Short offline       Completed without error       00%     51540         -
# 2  Short offline       Completed without error       00%     51510         -
# 3  Short offline       Completed without error       00%     51510         -
# 4  Short offline       Completed: read failure       90%     51466         2877061328
# 5  Short offline       Completed: read failure       90%     51437         2877061328
# 6  Short offline       Completed: read failure       90%     51407         2877061328
# 7  Short offline       Completed: read failure       90%     51374         2877061328
# 8  Short offline       Completed: read failure       90%     51341         2877061328
# 9  Short offline       Completed: read failure       90%     51307         2877061328
#10  Short offline       Completed: read failure       90%     51278         2877061328
#11  Short offline       Completed: read failure       90%     51248         2877061328
#12  Short offline       Completed: read failure       90%     51215         2877061328
#13  Short offline       Completed: read failure       90%     51186         2877061328
#14  Short offline       Completed: read failure       90%     51142         2877061328
#15  Short offline       Completed: read failure       90%     51112         2877061328
#16  Short offline       Completed: read failure       90%     51082         2877061328
#17  Short offline       Completed: read failure       90%     51051         2877061328
#18  Short offline       Completed: read failure       90%     51021         2877061328
#19  Short offline       Completed: read failure       90%     50991         2877061328
#20  Short offline       Completed: read failure       90%     50959         2877061328
#21  Short offline       Completed: read failure       90%     50929         2877061328


<badblocks>
AVAILABILITY
       badblocks is part of the e2fsprogs package and is available from
       http://e2fsprogs.sourceforge.net.

SEE ALSO
       e2fsck(8), mke2fs(8)

E2fsprogs version 1.41.12          May 2010                       BADBLOCKS(8)


badblocks -nsv /dev/sda

-sh-3.2# badblocks -h
badblocks: option requires an argument -- h
Usage: badblocks [-b block_size] [-i input_file] [-o output_file] [-svwnf]
 [-c blocks_at_once] [-p num_passes] [-t test_pattern [-t test_pattern [...]]]
 device [last_block [start_block]]

https://wiki.archlinux.org/index.php/badblocks

NOTE: Do not select write in case of badblocks testing ... it may overwrite
the HDD contents.


={============================================================================
*kt_linux_tool_051* tool-fs-lvm

http://www.howtogeek.com/howto/40702/how-to-manage-and-use-lvm-logical-volume-management-in-ubuntu/


={============================================================================
*kt_linux_tool_051* tool-fs-btrfs

https://btrfs.wiki.kernel.org/index.php/Main_Page

Btrfs is a new copy on write (CoW) filesystem for Linux aimed at implementing
advanced features while focusing on fault tolerance, repair and easy
administration. Jointly developed at multiple companies, Btrfs is licensed under
the GPL and open for contribution from anyone. Not too many companies have said
that they are using Btrfs in production, but we welcome those who can say so on
the production users page.

The main Btrfs features available at the moment include:

Writable snapshots, read-only snapshots 
Efficient Incremental Backup

Does it support SSD optimizations?

    Yes, Debian Jessie and later automatically detect non-rotational hard disks
    and ssd is added to the btrfs mount options. For more details on using SSDs
    with Debian, refer to SSDOptimization. 
    https://wiki.debian.org/SSDOptimization

$ sudo apt-get install btrfs-tools


={============================================================================
*kt_linux_tool_039* tool-ip tool-ifconfig

Assigning IP Address on demand using IP command. ifconfig command is
deprecated and replaced by IP command in Linux. However, IFCONFIG command is
still works and available for most of the Linux distributions.


={============================================================================
*kt_linux_tool_039* tool-udhcpc tool-dns

// udhcpc gets dns from dhcp and update resolv.conf

-sh-3.2# udhcpc
udhcpc (v1.13.1) started
Sending discover...
Sending select for 10.209.60.31...
Lease of 10.209.60.31 obtained, lease time 864000
deleting routers
route: ioctl 0x890c failed: No such process
adding dns 10.209.62.1
adding dns 10.209.120.1
adding dns 10.209.120.2
chown: unknown user NDS_MW


-sh-3.2# which udhcpc
/sbin/udhcpc
-sh-3.2# ls -al /sbin/udhcpc
lrwxrwxrwx    1 root     root           14 Jan  1 00:00 /sbin/udhcpc -> ../bin/busybox


/usr/share/udhcpc/default.script

#!/bin/sh

# udhcpc script edited by Tim Riker <Tim@Rikers.org>

[ -z "$1" ] && echo "Error: should be called from udhcpc" && exit 1

RESOLV_CONF="/etc/resolv.conf"
[ -n "$broadcast" ] && BROADCAST="broadcast $broadcast"
[ -n "$subnet" ] && NETMASK="netmask $subnet"

mount | grep -q '/dev/root on / type nfs'
NFSROOT=$?

case "$1" in
	deconfig)
		if [ $NFSROOT -ne 0 ]; then
			/sbin/ifconfig $interface 0.0.0.0
		fi
		;;

	renew|bound)
		if [ $NFSROOT -ne 0 ]; then
			/sbin/ifconfig $interface $ip $BROADCAST $NETMASK

			if [ -n "$router" ] ; then
				echo "deleting routers"
				while route del default gw 0.0.0.0 dev $interface ; do
					:
				done
	
				for i in $router ; do
					route add default gw $i dev $interface
				done
			fi
		fi

		echo -n > $RESOLV_CONF
		[ -n "$domain" ] && echo search $domain >> $RESOLV_CONF
		for i in $dns ; do
			echo adding dns $i
			echo nameserver $i >> $RESOLV_CONF
		done
        	/bin/chmod g+rw $RESOLV_CONF
                /bin/chown NDS_MW $RESOLV_CONF            
		;;
esac

exit 0


-sh-3.2# udhcpc
udhcpc (v1.13.1) started
+ '[' -z deconfig ']'
+ RESOLV_CONF=/etc/resolv.conf
+ '[' -n '' ']'
+ '[' -n '' ']'
+ mount
+ grep -q '/dev/root on / type nfs'
+ NFSROOT=1
+ case "$1" in
+ '[' 1 -ne 0 ']'
+ /sbin/ifconfig eth0 0.0.0.0
+ exit 0
Sending discover...
Sending select for 10.209.60.31...
Lease of 10.209.60.31 obtained, lease time 864000
+ '[' -z bound ']'
+ RESOLV_CONF=/etc/resolv.conf
+ '[' -n '' ']'
+ '[' -n 255.255.255.0 ']'
+ NETMASK='netmask 255.255.255.0'
+ mount
+ grep -q '/dev/root on / type nfs'
+ NFSROOT=1
+ case "$1" in
+ '[' 1 -ne 0 ']'
+ /sbin/ifconfig eth0 10.209.60.31 netmask 255.255.255.0
+ '[' -n 10.209.60.254 ']'
+ echo 'deleting routers'
deleting routers
+ route del default gw 0.0.0.0 dev eth0
route: ioctl 0x890c failed: No such process
+ for i in '$router'
+ route add default gw 10.209.60.254 dev eth0
+ echo -n
+ '[' -n UK.NDS.com ']'
+ echo search UK.NDS.com
+ for i in '$dns'
+ echo adding dns 10.209.62.1
adding dns 10.209.62.1
+ echo nameserver 10.209.62.1
+ for i in '$dns'
+ echo adding dns 10.209.120.1
adding dns 10.209.120.1
+ echo nameserver 10.209.120.1
+ for i in '$dns'
+ echo adding dns 10.209.120.2
adding dns 10.209.120.2
+ echo nameserver 10.209.120.2
+ /bin/chmod g+rw /etc/resolv.conf
+ /bin/chown NDS_MW /etc/resolv.conf
chown: unknown user NDS_MW
+ exit 0

// 56 network
/sbin/ifconfig eth0 0.0.0.0
/sbin/ifconfig eth0 10.209.56.7 netmask 255.255.252.0
route del default gw 0.0.0.0 dev eth0
route add default gw 10.209.59.254 dev eth0

// 60 network
/sbin/ifconfig eth0 0.0.0.0
/sbin/ifconfig eth0 10.209.60.75 netmask 255.255.255.0
route del default gw 0.0.0.0 dev eth0
route add default gw 10.209.60.254 dev eth0


// *tool-echo*
echo -n > /etc/resolv.conf

-sh-3.2# cat /etc/resolv.conf
echo "search UK.NDS.com" >> /etc/resolv.conf
echo "nameserver 10.209.62.1" >> /etc/resolv.conf
echo "nameserver 10.209.120.1" >> /etc/resolv.conf
echo "nameserver 10.209.120.2" >> /etc/resolv.conf


// to change script to run
-sh-3.2# udhcpc -s ./default.script

udhcpc (v1.13.1) started
+ '[' -z deconfig ']'
+ RESOLV_CONF=/etc/resolv.conf
+ '[' -n '' ']'
+ '[' -n '' ']'
+ mount
+ grep -q '/dev/root on / type nfs'
+ NFSROOT=1
+ case "$1" in
+ '[' 1 -ne 0 ']'
+ /sbin/ifconfig eth0 0.0.0.0
+ exit 0
Sending discover...
Sending select for 10.209.60.53...
Lease of 10.209.60.53 obtained, lease time 25200
+ '[' -z bound ']'
+ RESOLV_CONF=/etc/resolv.conf
+ '[' -n '' ']'
+ '[' -n 255.255.255.0 ']'
+ NETMASK='netmask 255.255.255.0'
+ mount
+ grep -q '/dev/root on / type nfs'
+ NFSROOT=1
+ case "$1" in
+ '[' 1 -ne 0 ']'
+ /sbin/ifconfig eth0 10.209.60.53 netmask 255.255.255.0
+ '[' -n 10.209.60.254 ']'
+ echo 'deleting routers'
deleting routers
+ route del default gw 0.0.0.0 dev eth0
route: ioctl 0x890c failed: No such process
+ for i in '$router'
+ route add default gw 10.209.60.254 dev eth0
+ echo -n
+ '[' -n UK.NDS.com ']'
+ echo search UK.NDS.com
+ for i in '$dns'
+ echo adding dns 10.209.62.253
adding dns 10.209.62.253
+ echo nameserver 10.209.62.253
+ for i in '$dns'
+ echo adding dns 10.209.120.1
adding dns 10.209.120.1
+ echo nameserver 10.209.120.1
+ for i in '$dns'
+ echo adding dns 10.209.120.2
adding dns 10.209.120.2
+ echo nameserver 10.209.120.2
+ /bin/chmod g+rw /etc/resolv.conf
+ /bin/chown NDS_MW /etc/resolv.conf
chown: unknown user NDS_MW
+ exit 0


-sh-3.2# cat /etc/resolv.conf
nameserver 10.209.62.111

-sh-3.2# cat /etc/resolv.conf
nameserver 10.209.62.253
nameserver 10.209.120.1
-sh-3.2#


RESOLV.CONF(5)
Linux Programmer's Manual

NAME
       resolv.conf - resolver configuration file

SYNOPSIS
       /etc/resolv.conf

DESCRIPTION
       The  resolver  is  a  set  of  routines  in  the C library that provide
       access to the Internet Domain Name System (DNS).  The resolver
       configuration file contains information that is read by the resolver
       routines the first time they are invoked by a process.  The file is
       designed to be human readable and contains a list of keywords with
       values that  provide  various  types  of  resolver information.  The
       configuration file is considered a trusted source of DNS information
       (e.g., DNSSEC AD-bit information will be returned unmodified from this
        source).

       If  this file does not exist, only the name server on the local machine
       will be queried; the domain name is determined from the hostname and
       the domain search path is constructed from the domain name.

       The different configuration options are:

       nameserver Name server IP address
              Internet address of a name server that the resolver should
              query, either an IPv4 address (in dot notation), or an IPv6
              address in colon (and possibly dot) notation as per RFC 2373.
              Up to MAXNS  (currently  3, see <resolv.h>) name servers may be
              listed, one per keyword.  If there are multiple servers, the
              resolver library queries them in the order listed.  If no
              nameserver entries are present, the default is to use the name
              server on the local machine.  (The algorithm used is to try a
                  name server, and if the query times out, try the next, until
                  out of  name servers, then repeat trying all the name
                  servers until a maximum number of retries are made.)


={============================================================================
*kt_linux_tool_039* tool-route

NAME
       route - show / manipulate the IP routing table

       -n     show numerical addresses instead of trying to determine symbolic
       host names. This is useful if you are trying to determine why the route
       to your nameserver has vanished.

Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.209.59.254   0.0.0.0         UG    1024   0        0 eth0
10.209.56.0     0.0.0.0         255.255.252.0   U     0      0        0 eth0
10.209.62.253   10.209.59.254   255.255.255.255 UGH   1      0        0 eth0
169.254.0.0     0.0.0.0         255.255.0.0     U     1000   0        0 docker0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0

10.209.59.254 is gateway IP address for our computer. The flag U indicates
that route is up and G indicates that it is gateway. You can print gateway
name, enter:

0.0.0.0         `10.209.59.254`   0.0.0.0         UG    1024   0        0 eth0


kyoupark@kit-debian64:~/git/kb$ netstat -r -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
0.0.0.0         10.209.59.254   0.0.0.0         UG        0 0          0 eth0
10.209.56.0     0.0.0.0         255.255.252.0   U         0 0          0 eth0
10.209.62.253   10.209.59.254   255.255.255.255 UGH       0 0          0 eth0
169.254.0.0     0.0.0.0         255.255.0.0     U         0 0          0 docker0
172.17.0.0      0.0.0.0         255.255.0.0     U         0 0          0 docker0


={============================================================================
*kt_linux_tool_039* tool-nfs

<install> 
sudo apt-get install nfs-kernel-server
sudo apt-get install nfs-common

note:
Install nfs-common for a client.


<export>
The `two different access modes` of the nfs server. 

The directory /home/client1 is shared in standard mode, so all files written
to this directory are stored as user `nobody` and group `nogroup`. 

For the directory /var/www I use the no_root_squash option which instructs the
nfs server to preserve permissions and ownerships of the files. 

mkdir /home/client1
chown nobody:nogroup /home/client1
chmod 755 /home/client1

mkdir /var/www
chown root:root /var/www
chmod 755 /var/www

Now we must modify /etc/exports where we "export" our NFS shares. We specify
/home/client1 and /var/www as NFS shares and tell NFS to make accesses to
/home/client1 as user nobody.

note
To learn more about /etc/exports, its format and available options, take a
look at `man 5 exports`


// /etc/exports

/home/client1   192.168.0.101(rw,sync,no_subtree_check)
/var/www        192.168.0.101(rw,sync,fsid=0,crossmnt,no_subtree_check,no_root_squash)

// which solves when gets permission denied
// /home/kyoupark/xxx      *(rw,sync,no_root_squash)
/home/kyoupark/xxx      *(rw,sync,no_root_squash,no_all_squash,sync,insecure)

note: 
You can replace * with one of the hostname formats. Make the hostname
declaration as specific as possible so unwanted systems cannot access the NFS
mount.

/ubuntu  *(ro,sync,no_root_squash)
/home    *(rw,sync,no_root_squash)


<check-permission>
The no_root_squash option makes that /var/www will be accessed as root.

note:
See the user/group permission.

client:

touch /mnt/nfs/home/client1/test.txt
touch /var/www/test.txt

server:

ls -l /home/client1/
total 0
-rw-r--r-- 1 nobody nogroup 0 Feb 02 16:58 test.txt

ls -l /var/nfs
-rw-r--r-- 1 root root 0 Feb 02 16:58 test.txt


<restart>
To apply the changes in /etc/exports, we restart the kernel nfs server

/etc/init.d/nfs-kernel-server restart
sudo /etc/init.d/nfs-kernel-server stop/start
sudo service nfs-kernel-server restart
sudo service idmapd restart 


root@kit-debian:/home/kit# service nfs-kernel-server status
● nfs-kernel-server.service - LSB: Kernel NFS server support
   Loaded: loaded (/etc/init.d/nfs-kernel-server)
   Active: active (running) since Wed 2016-07-06 12:06:00 BST; 2s ago
  Process: 15454 ExecStop=/etc/init.d/nfs-kernel-server stop (code=exited, status=0/SUCCESS)
  Process: 15461 ExecStart=/etc/init.d/nfs-kernel-server start (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/nfs-kernel-server.service
           └─15489 /usr/sbin/rpc.mountd --manage-gids

Jul 06 12:06:00 kit-debian nfs-kernel-server[15461]: Exporting directories for NFS kernel daemon...exportfs: /etc/exports [1]: Neither 'subtree_check' or 'no_subtree_check' specified for expo.../kit/STB_SW".
Jul 06 12:06:00 kit-debian nfs-kernel-server[15461]: Assuming default behaviour ('no_subtree_check').
Jul 06 12:06:00 kit-debian nfs-kernel-server[15461]: NOTE: this default has changed since nfs-utils version 1.0.x
Jul 06 12:06:00 kit-debian nfs-kernel-server[15461]: .
Jul 06 12:06:00 kit-debian nfs-kernel-server[15461]: Starting NFS kernel daemon: nfsd mountd.
Jul 06 12:06:00 kit-debian rpc.mountd[15489]: Version 1.2.8 starting
Hint: Some lines were ellipsized, use -l to show in full.


<mount>
mount -o nolock -t nfs 172.18.253.143:/home/NDS-UK/parkkt/fob/logs /mnt/temp/
mount -o nolock -t nfs 172.18.200.185:/mnt ./
mount -t nfs 172.18.253.143:/home/NDS-UK/parkkt/fob /mnt/nfs


note: "-o xxx" makes a 'difference' and this was from a board, that is busybox.

http://lists.busybox.net/pipermail/busybox/2002-July/040613.html

// host side
/home/kpark/src-dev     *(rw,sync,no_root_squash,no_subtree_check)

// target side - not work
[root@HUMAX /]# mount -t nfs 172.20.33.215:/home/kpark/src-dev /mnt/tmp
mount: mounting 172.20.33.215:/home/kpark/src-dev on /mnt/tmp failed: Connection refused

// target side - work
[root@HUMAX /]# mount -t nfs -o sync,nolock 172.20.33.215:/home/kpark/src-dev /mnt/tmp
[root@HUMAX /]# ls /mnt/tmp
DEVARCH/         GPATH            GRTAGS           GTAGS            


<check-on-client> tool-df tool-mount
df -h
mount


<check-on-server>
root# showmount -e
Export list for kit-debian:
/home/kit/STB_SW *


<mount-on-fstab>
Instead of mounting the NFS shares manually on the client, you could modify
/etc/fstab so that the NFS shares get mounted automatically when the client
boots. 

// vi /etc/fstab

192.168.0.100:/home/client1  /mnt/nfs/home/client1   nfs      rw,sync,hard,intr  0     0
192.168.0.100:/var/www  /var/www   nfs      rw,sync,hard,intr  0     0

Instead of rw,sync,hard,intr you can use different mount options. To learn
more about available options, take a look at man nfs


To test if your modified /etc/fstab is working, unmount the shares and run
mount -a:

umount /mnt/nfs/home/client1
umount /var/www
mount -a

You should now see the two NFS shares in the outputs of

df -h
mount


<nfs-check-version>
This shows me that my NFS server offers versions 2, 3, and 4 of the NFS
protocol all over UDP and TCP.

$ rpcinfo -p localhost
   program vers proto   port  service
   ...
    100003    2   tcp   2049  nfs
    100003    3   tcp   2049  nfs
    100003    4   tcp   2049  nfs
    100227    2   tcp   2049  nfs_acl
    100227    3   tcp   2049  nfs_acl
    100003    2   udp   2049  nfs
    100003    3   udp   2049  nfs
    100003    4   udp   2049  nfs
    100227    2   udp   2049  nfs_acl
    100227    3   udp   2049  nfs_acl


<user-mapping>

kyoupark@ukstbuild2$ cat /etc/exports
/home/           *(rw,no_root_squash,no_all_squash,sync,insecure)
/tftpboot *(rw,no_root_squash,insecure)


From `man 5 exports`

   User ID Mapping
       `nfsd` bases its access control to files on the server machine on the
       uid and gid provided in each NFS  RPC  request.  The  normal behavior a
       user would expect is that she can access her files on the server just
       as she would on a nor‐ mal file system. This requires that the same
       uids and gids are used on the client and the server machine.  This  is
       not always true, nor is it always desirable.

       Very often, it is not desirable that the root user on a client machine
       is also treated as root when accessing files on the NFS server. 
       
       To this end, uid 0 is normally mapped to a different id: the so-called
       anonymous or `nobody`  uid.  This mode of operation (called `root
           squashing') is the `default`, and can be turned off with
       no_root_squash.

       By default, exportfs chooses a uid and gid of 65534 for squashed
       access. These values can also be overridden by the `anonuid` and
       `anongid` options.  Finally, you can map all user  requests  to  the
       anonymous  uid  by  specifying  the `all_squash` option.

       Here's the complete list of mapping options:

       root_squash
              Map  requests  from  uid/gid  0 to the anonymous uid/gid. Note
              that this does `not` apply to any other uids or gids that might be
              equally sensitive, such as user bin or group staff.

       no_root_squash
              Turn off root squashing. This option is mainly useful for
              diskless clients.

       all_squash
              Map all uids and gids to the anonymous user. Useful for
              NFS-exported  public  FTP  directories,  news  spool
              directories, etc. The opposite option is `no_all_squash`, which is
              the `default` setting.


<idmapd>
http://serverfault.com/questions/514118/mapping-uid-and-gid-of-local-user-to-the-mounted-nfs-share

// note:
// Don't needs to do for my case. That's probably because have matched uid and
// gid between two hosts.
//
// This is what idmapping is suppose to do. First of all, enable is on the
// client and server:
// 
// # echo N > /sys/module/nfs/parameters/nfs4_disable_idmapping
// 
// clean idmap cache and restart idmap daemon:
// 
// # nfsidmap -c 
// # service rpcidmapd restart


// note:
// This works.

On your nfs client, edit /etc/idmapd.conf and change

[Mapping]

Nobody-User = myappuser
Nobody-Group = myappuser


={============================================================================
*kt_linux_tool_040* install

{case}
See a case that the build output, executable, does not match with the build time. In this example,
    there are 'build-root' and 'install-root' where build-root has all temporary files from the
    build and includes the final binary and 'install-root' has only the final binary. 

However, found that when do a build, a timestamp of install-root is older than the build one. So
wondered if the build system is broken and the build is not populated to the install-root.

Found that that's due to install command but not the build system.

/usr/bin/install -C {build-root}/nexus-inspect {install-root}/nexus-inspect

-C, --compare
compare each pair of source and destination files, and in some cases, do not modify the destination
at all

That is the install-root will not copied if two are the same.


={============================================================================
*kt_linux_tool_041* notify-send

wget URL && notify-send "Done" || notify-send "Failed"

notify-send --hint=int:transient:1 -i
/usr/share/icons/gnome/32x32/status/stock_dialog-warning.png 'Building
zb/DEVARCH-8092-TEMP: failure'


={============================================================================
*kt_linux_tool_042* tool-less

$ nm MW_Process | less -N +4213

$ less +G app.log

       G or > or ESC->
              Go to line N in the file, default the end of the file.
              (Warning: this may be slow if N is large, or if N is not
               specified and standard input, rather than a file, is being
               read.)

       +cmd   Causes the specified cmd to be executed each time a new file is
       examined.  For example, +G causes less to initially display each file
       starting at the end rather than the beginning.


colordiff -r libsanitizer/ ../xxx/gcc-4.9.4/libsanitizer/ | less -R

-r or --raw-control-chars
       Causes "raw" control characters to be displayed.  The default is to
       display control characters using the caret notation; for example, a
       control-A (octal 001) is displayed as "^A".   Warn‐ ing:  when  the  -r
       option is used, less cannot keep track of the actual appearance of the
       screen (since this depends on how the screen responds to each type of
           control character).  Thus, various display problems may result,
       such as long lines being split in the wrong place.

-R or --RAW-CONTROL-CHARS
       Like -r, but only ANSI "color" escape sequences are output in "raw"
       form.  Unlike -r, the screen appearance is maintained correctly in most
       cases.   ANSI  "color"  escape  sequences  are sequences of the form:

            ESC [ ... m

       where  the "..." is zero or more color specification characters For the
       purpose of keeping track of screen appearance, ANSI color escape
       sequences are assumed to not move the cursor.  You can make less think
       that characters other than "m" can end ANSI color escape sequences by
       setting the environment variable LESSANSIENDCHARS to the list of
       characters which can end a color escape sequence.  And you can make
       less think that characters other than the standard ones may appear
       between the ESC and the m by setting the environment variable
       LESSANSIMIDCHARS to the list of characters which can appear.


={============================================================================
*kt_linux_tool_042* tool-tail: print multiple files with filename

$ grep "" *-config
default-system-factory.plugin-config:1:libNickelSystemDbusClient.so createDbusSystemFactory
http-application%2Fdash%2Bxml.plugin-config:1:libNickelSystemGstreamer.so createGstSystemFactory
https-application%2Fdash%2Bxml.plugin-config:1:libNickelSystemGstreamer.so createGstSystemFactory

$ tail -n +1 *-config
==> default-proxy-config <==

==> default-system-factory.plugin-config <==
libNickelSystemDbusClient.so createDbusSystemFactory

==> http-application%2Fdash%2Bxml.plugin-config <==
libNickelSystemGstreamer.so createGstSystemFactory

==> https-application%2Fdash%2Bxml.plugin-config <==
libNickelSystemGstreamer.so createGstSystemFactory

       -n, --lines=K
              output the last K lines, instead of the last 10; or use -n +K to
              output lines starting with the Kth

<f-option>
       -f, --follow[={name|descriptor}]
              output appended data as the file grows; -f, --follow, and
              --follow=descriptor are equivalent


={============================================================================
*kt_linux_tool_044* mc

https://www.midnight-commander.org/

apt-get install mc


={============================================================================
*kt_linux_tool_045* graphbiz

sudo apt-get install graphviz

dot -Tjpeg gst-launch.PLAYING_PAUSED.dot -o gst-launch.PLAYING_PAUSED.jpg


={============================================================================
*kt_linux_tool_046* tool-mktemp

DESCRIPTION
       Create a temporary file or directory, safely, and print its name. TEM‐
       PLATE must contain at least 3 consecutive 'X's in last  component. If
       TEMPLATE is not specified, use tmp.XXXXXXXXXX, and --tmpdir is implied.
       Files are created u+rw, and directories u+rwx, minus umask restrictions.

       -t     interpret  TEMPLATE as a single file name component, relative to
              a directory: $TMPDIR, if set; else the directory  specified  via
              -p; else /tmp [deprecated]

$ mktemp
/tmp/tmp.eezueA0X5X

$ mktemp -t check-headers.XXXXX
/tmp/check-headers.Y8A9r


={============================================================================
*kt_linux_tool_048* tool-dircolors

{get-color}
For bash, copy /etc/DIR_COLORS into home as .dir_colors and edit it to change
default values. Run man dir_colors for help.

IMPORTANT NOTE FOR TERMINAL USERS:

If you are going to use Solarized in Terminal mode (i.e. not in a GUI version
like gvim or macvim), please please please consider setting your terminal
emulator's colorscheme to used the Solarized palette. I've included palettes
for some popular terminal emulator as well as Xdefaults in the official
Solarized download available from Solarized homepage. If you use Solarized
without these colors, Solarized will need to be told to degrade its
colorscheme to a set compatible with the limited 256 terminal palette (whereas
by using the terminal's 16 ansi color values, you can set the correct,
specific values for the Solarized palette).

http://www.webupd8.org/2011/04/solarized-must-have-color-paletter-for.html

{dircolors}

#############################################################################
# Below are the color init strings for the basic file types. A color init
# string consists of one or more of the following numeric codes:
#
# Attribute codes:
#   00=none 01=bold 04=underscore 05=blink 07=reverse 08=concealed
# Text color codes:
#   30=black 31=red 32=green 33=yellow 34=blue 35=magenta 36=cyan 37=white
# Background color codes:
#   40=black 41=red 42=green 43=yellow 44=blue 45=magenta 46=cyan 47=white
#
# NOTES:
# - See http://www.oreilly.com/catalog/wdnut/excerpt/color_names.html
# - Color combinations
#   ANSI Color code       Solarized  Notes                Universal             SolDark              SolLight
#   ~~~~~~~~~~~~~~~       ~~~~~~~~~  ~~~~~                ~~~~~~~~~             ~~~~~~~              ~~~~~~~~
#   00    none                                            NORMAL, FILE          <SAME>               <SAME>
#   30    black           base02
#   01;30 bright black    base03     bg of SolDark
#   31    red             red                             docs & mm src         <SAME>               <SAME>
#   01;31 bright red      orange                          EXEC                  <SAME>               <SAME>
#   32    green           green                           editable text         <SAME>               <SAME>
#   01;32 bright green    base01                          unimportant text      <SAME>
#   33    yellow          yellow     unclear in light bg  multimedia            <SAME>               <SAME>
#   01;33 bright yellow   base00     fg of SolLight                             unimportant non-text
#   34    blue            blue       unclear in dark bg   user customized       <SAME>               <SAME>
#   01;34 bright blue     base0      fg in SolDark                                                   unimportant text
#   35    magenta         magenta                         LINK                  <SAME>               <SAME>
#   01;35 bright magenta  violet                          archive/compressed    <SAME>               <SAME>
#   36    cyan            cyan                            DIR                   <SAME>               <SAME>
#   01;36 bright cyan     base1                           unimportant non-text                       <SAME>
#   37    white           base2
#   01;37 bright white    base3      bg in SolLight
#   05;37;41                         unclear in Putty dark


// since light is better
// wget --no-check-certificate https://raw.github.com/seebi/dircolors-solarized/master/dircolors.ansi-dark
// mv dircolors.ansi-dark .dircolors
// eval `dircolors ~/.dircolors`

wget --no-check-certificate https://raw.github.com/seebi/dircolors-solarized/master/dircolors.ansi-light
mv dircolors.ansi-light .dircolors
eval `dircolors ~/.dircolors`


$ man dircolors

NAME
       dircolors - color setup for ls

       -p, --print-database
              output defaults

note:
No /etc/DIR_COLORS in debian.

As for where dircolors gets its settings from, when you don't specify a file
it just uses some builtin defaults.


{gonme}
git clone https://github.com/sigurdga/gnome-terminal-colors-solarized.git
cd gnome-terminal-colors-solarized

" can set it to light or dark using the following commands:

./set_dark.sh
./set_light.sh


{putty}
https://github.com/altercation/solarized/tree/master/putty-colors-solarized

PuTTY Tray can store sessions in text files as opposed to the Windows
registry. To modify an existing session to use Solarized colors, open the file
in a text editor and replace the lines beginning with Colour## (0-21) with the
version from solarized_dark_puttytray.txt or solarized_light_puttytray.txt.

https://puttytray.goeswhere.com/

Saves a session as a text which is made under "session" from where putty runs.

// solarized/putty-colors-solarized/solarized_light_puttytray.txt

Colour21\253,246,227\
Colour20\238,232,213\
Colour19\147,161,161\
Colour18\42,161,152\
Colour17\108,113,196\
Colour16\211,54,130\
Colour15\131,148,150\
Colour14\38,139,210\
Colour13\101,123,131\
Colour12\181,137,0\
Colour11\88,110,117\
Colour10\133,153,0\
Colour9\203,75,22\
Colour8\220,50,47\
Colour7\0,43,54\
Colour6\7,54,66\
Colour5\101,123,131\
Colour4\238,232,213\
Colour3\238,232,213\
Colour2\253,246,227\
Colour1\88,110,117\
Colour0\101,123,131\

// original value

Colour21\255,255,255\
Colour20\187,187,187\
Colour19\85,255,255\
Colour18\0,187,187\
Colour17\255,85,255\
Colour16\187,0,187\
Colour15\85,85,255\
Colour14\0,0,187\
Colour13\255,255,85\
Colour12\187,187,0\
Colour11\85,255,85\
Colour10\0,187,0\
Colour9\255,85,85\
Colour8\187,0,0\
Colour7\85,85,85\
Colour6\0,0,0\
Colour5\0,255,0\
Colour4\0,0,0\
Colour3\85,85,85\
Colour2\0,0,0\
Colour1\255,255,255\
Colour0\187,187,187\


={============================================================================
*kt_linux_tool_048* tool-tput

NAME
       tput, reset - initialize a terminal or query terminfo database

<ex>

# Definitions for bold/normal formatting.  Used to prettify the output.
bold=$(tput bold)
normal=$(tput sgr0)

    echo ""
    echo "${bold}********************************************${normal}"
    echo "${bold}** Find Latest Uploads from MAC Addresses **${normal}"
    echo "${bold}********************************************${normal}"


={============================================================================
*kt_linux_tool_048* tool-gnome-terminal

{gnome-terminal}
I have just found that you can open new terminals using 'gnome-terminal'.  You can open multiple
windows and multiple tabs like this:

gnome-terminal --window --tab --window --tab --tab

<key-shortcuts>
New Tab        Shift+Ctrl+T
Close Tab      Shift+Ctrl+W

note: this is to open new terminal
New Window     Shift+Ctrl+N
Close Window   Shift+Ctrl+Q

Copy           Ctrl+Shift+C
Paste          Ctrl+Shift+V

Switch to Previous Tab     Ctrl+Page Up
Switch to Next Tab         Ctrl+Page Down

note: this is to 'move' a tab
Move Tab to the Left       Shift+Ctrl+Page Up
Move Tab to the Right      Shift+Ctrl+Page Down

Switch to Tab 1            Alt+1
Switch to Tab N            Alt+N

<page-up>
Shift+PgUp/PgDn/Home/End will scroll in gnome-terminal and Terminal.

<find>
Using the Search menu or a keyboard short-cut Shift+Ctrl+F


={============================================================================
*kt_linux_tool_049* tool-terminator

https://launchpad.net/terminator

./setup.py install --record=install-files.txt

For more keyboard shortcuts and also the command line options, please see the
manpage "terminator". For configuration options, see the manpage
"terminator_config".

/home/keitee/.config/terminator/config

// from terminator_config

keybindings

<open-close>
new_tab
Open a new tab.  Default value: <Ctrl><Shift>T

close_term
Close the current terminal.  Default value: <Ctrl><Shift>W

close_window
Quit Terminator.  Default value: <Ctrl><Shift>Q

new_window
Open a new Terminator window as part of the existing process.  Default value: <Ctrl><Shift>I

new_terminator
Spawn a new instance of Terminator.  Default value: <Super>i


<switch-tab>
next_tab
Move to the next tab.  Default value: <Ctrl>Page_Down

prev_tab
Move to the previous tab.  Default value: <Ctrl>Page_Up


<switch-termial>
about terminals in a tab

go_up  
Move cursor focus to the terminal above.  Default value: <Alt>Up

go_down
Move cursor focus to the terminal below.  Default value: <Alt>Down

go_left
Move cursor focus to the terminal to the left.  Default value: <Alt>Left

go_right
Move cursor focus to the terminal to the right.  Default value: <Alt>Right


<split-terminal>
Q: how to change the size of split?. Use resize instead?

split_horiz
Split the current terminal horizontally.  Default value: <Ctrl><Shift>O

split_vert
Split the current terminal vertically.  Default value: <Ctrl><Shift>E


<resize-terminal>
resize_up
Move the parent dragbar upwards. value: <Ctrl>Up

resize_down
Move the parent dragbar downwards. value: <Ctrl>Down

resize_left
Move the parent dragbar left.

resize_right
Move the parent dragbar right.


<copy-paste>
copy   
Copy the currently selected text to the clipboard.  Default value: <Ctrl><Shift>C

paste  
Paste the current contents of the clipboard.  Default value: <Ctrl><Shift>V


<search>
Q: shows if there is match or not but not highlight or scroll.

search 
Search for text in the terminal scrollback history.  Default value: <Ctrl><Shift>F


<zoom>
toggle_zoom
Zoom/Unzoom the current terminal to fill the window.  Default value: <Ctrl><Shift>X


<reset>
reset_clear
Reset the terminal state and clear the terminal window.  Default value: <Ctrl><Shift>G


={============================================================================
*kt_linux_tool_050* tool-watch

$watch -n1 -d ccache -s

NAME
       watch - execute a program periodically, showing output fullscreen

SYNOPSIS
       watch [options] command

DESCRIPTION
       watch runs command repeatedly, displaying its output and errors (the
               first screenfull).  This allows you to watch the program output
       change over time.  By default, the program is run every 2 seconds.  By
       default, watch will run until interrupted.

OPTIONS
       -d, --differences [permanent] Highlight the differences between
       successive updates.  Option will read optional argument that changes
       highlight to be permanent, allowing to see what has changed at least once
       since first iteration.

       -n, --interval seconds Specify update interval.  The command will not
       allow quicker than 0.1 second interval, in which the smaller values are
       converted.


={============================================================================
*kt_linux_tool_051* tool-github kb-github

git@github.com:/keitee/kb.git

git clone git://github.com/keitee/kb.git
git clone git://github.com/keitee/vim.git


={============================================================================
*kt_linux_tool_051* tool-font set-font

Copy a font file in the directory /usr/share/fonts/truetype (for all users) or
~/.fonts (for a specific user),  ~/.local/share/fonts$ for debian.

// Remember to verify font's permissions on disk (777); if you save your fonts in
// $ chmod -R 777 /usr/local/share/fonts

fc-list       // lists fonts
fc-cache -fv  // rebuilds cached list of fonts 

http://sourcefoundry.org/hack/


={============================================================================
*kt_linux_tool_051* tool-echo

-e     enable interpretation of backslash escapes

If -e is in effect, the following sequences are recognized:

\e     escape

echo -e "\E[1;31mThe commit subject does not match \"DEVARCH-xxxx: <subject>\"";

However, "\E" also works.


<busybox>
echo

    echo [-neE] [ARG...]

    Print the specified ARGs to stdout

    Options:

            -n      Suppress trailing newline
            -e      Interpret backslash-escaped characters (i.e., \t=tab)
            -E      Disable interpretation of backslash-escaped characters

echo -e "\E[1;31mThe commit subject does not match \"DEVARCH-xxxx: <subject>\"";

<ex> *tool-touch*
create empty file

echo -n > /etc/resolv.conf


={============================================================================
*kt_linux_tool_051* tool-echo-compile

echo -e "#include <utility>\nint main() { const char* const null_ptr = 0; std::pair<bool, const char*> x(false, null_ptr); return 0; }" | g++ -Wall -std=c++0x -x c++ -o foo -


={============================================================================
*kt_linux_tool_051* tool-tcpdump

Tcpdump can capture network activity and log it to a file for analysis. If we
can compile it for MIPS, we can run it on a set top box and even capture
packets from a Verifier callback. This has been used for debugging UAM and
SSP.

You can download it from http://www.tcpdump.org/

There's a good chance these instructions will work for other software that
uses autoconf. 


For the MIPS based box (such as the AMS890), we'll make a variable called MIPS
that points to the toolchain. You can find it in a Clearcase view. Make sure
this is an absolute path. Then we set up a bunch of other variables that are
used by configure and make.

export MIPS=/home/fisherr/rf890/vobs/SYSTEM_BIN_2/mips4k_gcc_x86_linux_hound
export CC=$MIPS/bin/mips-linux-gcc
export PATH=$MIPS/bin:$PATH
export AR=$MIPS/bin/mips-linux-ar
export LDFLAGS=" -s -Xlinker -rpath /lib -Xlinker -rpath-link $MIPS/lib/gcc/mips-linux/4.2.0"
export ac_cv_linux_vers=2

*mips-endian*
For the MIPSel (little endian) boxes (such as the Fusion product box used for
    component tests, configure the environment as such:

export MIPS=/home/fisherr/rf890/vobs/SYSTEM_BIN/mips4k_gcc_x86_linux_03
export CC=$MIPS/bin/mipsel-linux-gcc
export PATH=$MIPS/bin:$PATH
export AR=$MIPS/bin/mipsel-linux-ar
export LDFLAGS=" -s -Xlinker -rpath /lib -Xlinker -rpath-link $MIPS/lib/gcc/mipsel-linux/4.2.0"
export ac_cv_linux_vers=2


libpcap

cd libpcap-1.1.1
./configure --host=mips --with-pcap=linux
make


tcpdump

The configure script will find the libpcap you just built if it is in an
adjacent directory.

cd ../tcpdump-4.1.1/
./configure --host=mips
make

Using NFS, mount the filesystem containing your MIPS tcpdump executable. Use
the "any" interface to make sure you capture even packets routed to the modem
over PPP. You can log to a file and then view the file in `wireshark`.

<ex>
/mnt/nfs/tcpdump/tcpdump-4.1.1/tcpdump -i any -w /mnt/nds/disk/FSN_DATA/TCPLOG
tcpdump -w ${FILE} -i eth0


={============================================================================
*kt_linux_tool_051* tool-swapon

http://www.tecmint.com/commands-to-monitor-swap-space-usage-in-linux/

NAME
       swapon, swapoff - enable/disable devices and files for paging and
       swapping

DESCRIPTION
       swapon is used to specify devices on which paging and swapping are to
       take place.

       The device or file used is given by the specialfile parameter. It may
       be of the form -L label or -U uuid to indicate a device by label or
       uuid.

       Calls  to  swapon  normally  occur `in the system boot scripts` making
       all swap devices available, so that the paging and swapping activity is
       interleaved across several devices and files.

       -a, --all
              All  devices  marked  as  ‘‘swap’’  in /etc/fstab are made
              available, except for those with the ‘‘noauto’’ option.  Devices
              that are already being used as swap are silently skipped.

       -s, --summary
              Display swap usage summary by device. Equivalent to "cat
              /proc/swaps".  Not available before Linux 2.1.25.


<setup-swap>
9. Setting Up Swap Space

9.1. Swap Files

Normally, there are only two steps to setting up swap space, creating the
partition and adding it to /etc/fstab. A typical fstab entry for a swap
partition at /dev/hda6 would look like this:

/dev/hda6	swap	swap	defaults	0	0


The next time you reboot, the initialization scripts will activate it
automatically and there's nothing more to be done.

However, if you want to make use of it right away, you'll need to activate it
maually. As root, type:

mkswap -f /dev/hda6
swapon /dev/hda6


={============================================================================
*kt_linux_tool_051* tool-htop

http://www.thegeekstuff.com/2011/09/linux-htop-examples | Top on Steroids  15 Practical Linux HTOP Examples


={============================================================================
*kt_linux_tool_051* tool-xterm: unknown terminal type

http://blog.timmattison.com/archives/2012/04/12/tip-fix-xterm-unknown-terminal-type-messages-in-debian/

This one has been a bit of a nuisance on newly spooled up Debian instances for
me lately. When I try to run "top" or "clear" or really anything that does
something with the terminal I get the following message:

'xterm': unknown terminal type.

This is because either you haven't installed ncurses-term (unlikely) or a
symlink from /lib/terminfo/x/xterm to /usr/share/terminfo/x/xterm is missing. To
cover all possibilities do this:

sudo apt-get install ncurses-term
sudo ln -s /lib/terminfo/x/xterm /usr/share/terminfo/x/xterm

Poof, your terminal works again!


={============================================================================
*kt_linux_tool_051* tool-lpstat: view and cancel print jobs

Command line:

lpstat -o

to view outstanding print jobs.

cancel -a {printer}

to cancel ALL jobs or ...

cancel {printerjobid}

to cancel 1 job.

$ lpstat -o
SydneyColour-233        kpark          5653504   Tue 05 Jan 2016 07:59:13 UTC
SydneyColour-234        kpark             1024   Tue 05 Jan 2016 08:09:21 UTC
$ cancel -a SydneyColour    
$ lpstat -o
$ 


={============================================================================
*kt_linux_tool_051* tool-i3wm:

https://i3wm.org/


={============================================================================
*kt_linux_tool_051* tool-hexdump tool-xxd

$ hexdump -C -n 100 DTT_Radio1_20150225_1603.ts

     -C          Canonical hex+ASCII display.  Display the input offset in
     hexadecimal, followed by sixteen space-separated, two column, hexadecimal
     bytes, followed by the same sixteen bytes in %_p format enclosed in ‘‘|’’
     characters.

     -n length   Interpret only length bytes of input.

DESCRIPTION
       xxd creates a hex dump of a given file or standard input.  It can also
       convert a hex dump back to its original binary form.  Like uuencode(1)
       and uudecode(1) it allows the transmission of binary data in a
       `mail-safe' ASCII representation, but has the advantage of decoding to
       standard output.  Moreover, it can be used to perform binary file
       patching.

       -c cols | -cols cols
              format <cols> octets per line. Default 16 (-i: 12, -ps: 30, -b:
                  6). Max 256.

       -g bytes | -groupsize bytes
              separate  the  output of every <bytes> bytes (two hex characters
                  or eight bit-digits each) by a whitespace.  Specify -g 0 to
              suppress grouping.  <Bytes> defaults to 2 in normal mode, 4 in
              little-endian mode and 1 in bits mode.  Grouping does not apply
              to postscript or include style.

       -p | -ps | -postscript | -plain
              output in postscript continuous hexdump style. Also known as
              plain hexdump style.

     -s offset
      Skip offset bytes from the beginning of the input. By default, offset is
      interpreted as a decimal number. With a leading 0x or 0X, offset is
      interpreted as a hexadecimal number, otherwise, with a leading 0, offset
      is interpreted as an octal number.  Appending the character b, k, or m
      to offset causes it to be interpreted as a multiple of 512, 1024, or
      1048576, respectively.


<ex>
xxd -ps -c 188 -g1 s6631/STREAM.STR  > s6631/hexdump_ts.txt


={============================================================================
*kt_linux_tool_051* tool-hexedit tool-ghex

the GNOME Hex Editor, ghex(1)

sudo apt-get install ghex


={============================================================================
*kt_linux_tool_051* tool-uptime

/opt/zinc/oss/bin/busybox uptime; /opt/zinc/oss/bin/busybox lsof | wc -l


={============================================================================
*kt_linux_tool_051* tool-lsof

/opt/zinc/oss/bin/busybox uptime; /opt/zinc/oss/bin/busybox lsof | wc -l


={============================================================================
*kt_linux_tool_051* tool-dirname tool-basename tool-readlink

*tool-readlink*
readlink - print value of a symbolic link or canonical file name


*tool-dirname*
dirname - strip last component from file name 

<ex>
Suppose that the script is in PATH, can run that anywhere, and $0 will be
fullpath where the script exist whereever you run.

This code is to get the real path of where the script exist if it has a
symbloc link and add it to PATH.

MYPATH=$(readlink -f ``dirname $0``)
PATH=$MYPATH:$PATH


*tool-basename*

DESCRIPTION

Print NAME with any leading directory components removed. If 'specified', also
remove a trailing SUFFIX.

$ basename /home/kt/kb
kb
$ basename cfgversion.out  
cfgversion.out

// remove traling

$ basename cfgversion.out .out
cfgversion


={============================================================================
*kt_linux_tool_051* tool-last

last command will give login history for a specific username. If we don’t give
any argument for this command, it will list login history for all users. By
default this information will read from /var/log/wtmp file. The output of this
command contains the following columns:



={============================================================================
*kt_linux_tool_051* tool-add-user tool-sudo

<sudoer>
apt-get install sudo
su
adduser <username> sudo
re-login


<groupadd>
       The groupadd command creates a new group account using the values
       specified on the command line plus the default values from the system.
       The new group will be entered into the system files as needed.


       -g, --gid GID

           The numerical value of the group's ID. This value must be unique,
unless the -o option is used. The value must be non-negative. The default is
  to use the smallest ID value greater than or equal to GID_MIN and greater
  than every other group.

$ sudo groupadd --gid 16777244 ccusers

or

$ sudo gpasswd -a ${USER} docker


Want to change user 'peter' to 'paul'.

$ usermod -d /home/paul -m -g paul -l paul peter

<usermod>
       The usermod command modifies the system account files to reflect the
       changes that are specified on the command line.

       -d, --home HOME_DIR
           The user's new login directory.

           If the -m option is given, the contents of the current home
           directory will be moved to the new home directory, which is created
           if it does not already exist.


       -g, --gid GROUP
           The group name or number of the user's new initial login group. The
           group `must exist`

           Any file from the user's home directory owned by the previous
           primary group of the user will be owned by this new group.

           The group ownership of files outside of the user's home directory
           must be fixed manually.

       -m, --move-home
           Move the content of the user's home directory to the new location.

           This option is only valid in combination with the -d (or --home)
           option.

           usermod will try to adapt the ownership of the files and to copy
           the modes, ACL and extended attributes, but manual changes might be
           needed afterwards.

       -l, --login NEW_LOGIN
           The name of the user will be changed from LOGIN to NEW_LOGIN.
           Nothing else is changed.  In particular, the user's home directory
           or mail spool should probably be renamed manually to reflect the
           new login name.

           note: this is to change username

<adduser> 
       useradd is a low level utility for adding users. On `Debian`,
administrators should usually use adduser(8) instead.

       adduser  and  addgroup  add users and groups to the system according to
       command line options and configuration information in
       /etc/adduser.conf.  They are friendlier front ends  to  the low level
       tools like useradd, groupadd and usermod programs, by default choosing
       Debian pol‐ icy conformant UID and GID values, creating a home
       directory  with  skeletal  configuration, running a custom script, and
       other features.  adduser and addgroup can be run in one of five modes:

// add user
$ sudo adduser kyoupark

// add user into a group
$ sudo adduser kyoupark ccusers


<deluser>
       deluser and delgroup remove users and groups from  the  system
       according  to  command  line options  and configuration information in
       /etc/deluser.conf and /etc/adduser.conf.  They are friendlier front
       ends to the userdel and groupdel programs, removing the home  directory
       as option  or  even  all  files on the system owned by the user to be
       removed, running a custom script, and other features.  deluser and
       delgroup can be run in one of three modes:

       deluser  [options]  [--force]  [--remove-home]  [--remove-all-files]
       [--backup] [--backup-to DIR] user

       deluser --group [options] group
       delgroup [options] [--only-if-empty] group

       deluser [options] user group

<sudo>
As an alternative to using the root account directly, users can be allowed to
temporarily give their own accounts Super User privileges using the sudo
command. Sudo provides specified users or groups of users with limited root
access. Users assume root privileges with sudo using their personal passwords,
and any activity performed is logged by the system logger (systemlogd), as are
  unsuccessful attempts to gain sudo privileges.

Access to the sudo command is policy based, with the /etc/sudoers containing
the sudo policy for a system. The visudo command opens the /etc/sudoers policy
file in a vi text editor, while this is indeed possible, it is NOT
recommended. Using visudo locks the file for edition and ensures only one
person is making changes to it at the same time.

The /etc/sudoers files contains many sane defaults, and a number of common or
useful example sudo policies. Here are some of the important concepts in the
sudoers file:

Concept: Command aliases

Cmnd_Alias SERVICES = /sbin/service, /sbin/chkconfig

In this example, an alias is declared for a group of commands, in this case, 2
service related commands. Grouping multiple commands into an alias allows the
administrator to give users or groups access to a number of commands at once.

Concept 2: Sudo policy

%wheel (ALL)=(ALL) ALL

In this example, members of the wheel group are allowed to perform any
command. The policy statement has 4 parts.

%wheel: The group or user to whom the policy applies. If the policy is
%specific to a single
user, the % is omitted.

(ALL): The first (ALL) in this example is `the host` where the policy applies.
       It could also be localhost, or other hosts that use this sudoers file
       to determine sudo policy.

(ALL): The second (ALL) in this example is `the user` with whose permissions any
       commands affected by the policy will be run. (ALL) means that any
       command affected by this policy can be run with the permissions of any
       user. If no user or group of users is specified, the root user is
       assumed.

ALL: The third ALL in this example, with no brackets, are the commands
  affected by this policy. In this case, all commands are affected, but a
  command alias or comma separated list of commands could also have been used.

<ex>
1. Create a command alias for the commands involved in creating RAID arrays.
2. Create a command alias for user administration that includes commands for
adding and removing users.

3. Create a policy that allows members of the users group to run the RAID
commands as your user on localhost.

4. Create a policy that allows members of the users group to run the user
administration commands as root on localhost.

Answer Key

1. Cmnd_Alias RAIDSTUFF = /usr/sbin/mdadm, /usr/sbin/parted
2. Cmnd_Alias USERSTUFF = /usr/sbin/useradd, /usr/sbin/userdel

3. %users localhost=USERNAME ALL RAIDSTUFF
Where USERNAME is your username.

4. %users localhost=USERSTUFF


={============================================================================
*kt_linux_tool_051* tool-docker

# restart docker
sudo service docker restart

<not-a-vm>
Use debian image as the container OS and run root shell:

kit-debian64:~$ sudo docker run -i -t debian /bin/bash
root@6ba46bf1b257:/# 
root@6ba46bf1b257:/# ls
bin  boot  dev	etc  home  lib	lib64  media  mnt  opt	proc  root  run  sbin  srv  sys  tmp  usr  var

now uname `still gives ubuntu`, because this is a container and not a true VM.

root@482791e6cc1a: uname -a
Linux ubuntu-14 3.13.0-32-generic #57-Ubuntu SMP Tue Jul 15 03:51:08 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux


docker run creates a container, configures it and runs a command in it. You
need to run to configure a container, every run creates a new container. So a
lot of containers will be generated. Observe them with `docker ps -a` You will
want to periodically clean up all the cruft with `docker rm`


A container is not a VM, it doesn't start any services or run init.d, as far
as I can tell. The command you give it is crucial. Think of it more like an
application wrapper, than a VM.

When you need to reconfigure a container, you need to "commit" it. That
creates an image out of the container, that can then be reconfigured with a
new run command. There is no other way. For a container, especially with
networking configured, expect to do a lot of commits and runs.


<attach>
Use CTRL-P CTRL-Q to step out of the container shell and keep the shell
running. Reattach with docker attach

kyoupark@kit-debian64:~$ sudo docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
3591df06b6f1        debian              "/bin/bash"         10 minutes ago      Up 10 minutes                           lonely_williams

kyoupark@kit-debian64:~$ sudo docker attach 3591df06b6f1
root@3591df06b6f1:/# 


={============================================================================
*kt_linux_tool_051* tool-virtualbox tool-samba tool-vm

{graphic-issue}
When see a problem with NVIDIA, need to install a driver for ubuntu manually:

http://askubuntu.com/questions/141606/how-to-fix-the-system-is-running-in-low-graphics-mode-error

In short, get console and do:

sudo apt-get install nvidia-current

Also, install the guest addition:

Click on Install Guest Additions from the Devices menu and all will be done
automatically.


{sharing-between-os}
http://www.virtualbox.org/manual/ch04.html#sharedfolders

# set sharing folder on the host using virtual box menu and reboot
#
select "Shared folders" from the "Devices" menu, or click on the folder icon
on the status bar in the bottom right corner.

# shared folder is
#
/media/sf_myfiles 

# add group permission when see access error when reading shared folder
#
Access to auto-mounted shared folders is only granted to the user group
vboxsf, which is created by the VirtualBox Guest Additions installer. Hence
guest users have to be member of that group to have read/write access or to
have read-only access in case the folder is not mapped writable.

sudo usermod -a -G vboxsf {username}


{sharing-using-samba}
<1> Have samba installation, setting, and user. This is simple and general and
see as an example.
http://www.sitepoint.com/ubuntu-12-04-lts-precise-pangolin-file-sharing-with-samba/

// sudo apt-get install samba samba-common system-config-samba winbind

// for debian 64
sudo apt-get install samba samba-common winbind

change winbind setting

For samba server settings: Workgroup. (this is domain) This field should be
the same value as that used by your Windows Workgroupi.e if your WIndows Users
are members of the 'Home' workgroup, type 'Home' in this field.  

<tool-samba>
sudo service smbd start
sudo service smbd stop
sudo service smbd restart
sudo systemctl restart smbd.service

<check-version>
kyoupark@kit-debian64:~/git/kb$ sudo smbstatus

Samba version 4.2.14-Debian
PID     Username      Group         Machine            Protocol Version
------------------------------------------------------------------------------

Service      pid     machine       Connected at
-------------------------------------------------------

No locked files


# when client is able to browse
kyoupark@kit-debian64:~/works$ sudo smbstatus

Samba version 4.2.14-Debian
PID     Username      Group         Machine            Protocol Version
------------------------------------------------------------------------------
18995     nobody        nogroup       10.209.56.150 (ipv4:10.209.56.150:60938) SMB3_00
19024     nobody        nogroup       10.209.56.150 (ipv4:10.209.56.150:60957) SMB3_00

Service      pid     machine       Connected at
-------------------------------------------------------
anonymous    18995   10.209.56.150  Wed Sep 20 15:48:25 2017
IPC$         19024   10.209.56.150  Wed Sep 20 15:51:06 2017

Locked files:
Pid          Uid        DenyMode   Access      R/W        Oplock           SharePath   Name   Time
--------------------------------------------------------------------------------------------------
18995        1000       DENY_NONE  0x100081    RDONLY     NONE             /home/kyoupark   .   Wed Sep 20 15:48:25 2017
18995        1000       DENY_NONE  0x100081    RDONLY     NONE             /home/kyoupark   .   Wed Sep 20 15:48:25 2017


https://www.howtoforge.com/tutorial/debian-samba-server/

3.3 Anonymous share

You like to have a share were all users in your network can write to? Be
careful, this share is open to anyone in the network, so use this only in
local networks. Add an anonymous share like this:

/etc/samba/smb.conf

[anonymous]
   path = /home/shares/anonymous
   force group = users
   create mask = 0660
   directory mask = 0771
   browsable =yes
   writable = yes
   guest ok = yes

note:
If `path` is wrong then PC host see anonymous share but cannot access through
it.


<changes>
[global]

## Browsing/Identification ###

# Change this to the workgroup/NT-domain name your Samba server will part of
   workgroup = CISCO

[anonymous]
  path=/home/kyoupark
   force group = ccusers
   create mask = 0660
   directory mask = 0771
   browsable = yes
   writable = yes
   read only = no
   guest ok = yes


<no-write>
However, the above setting has no write and tried others such as:

1. force user = username


<windows-asks-ucredential>
https://serverfault.com/questions/731038/windows-10-keeps-asking-for-authentication-for-public-samba-share

EDIT 3: When I enter \ as the username and leave the password blank it shows
me the share contents, however when I reboot the computer it asks me for
credentials again, it won't remember them.


<add-list-users>
From the man page "pdbedit - manage the SAM database (Database of Samba
Users)"

sudo pdbedit -L -v

-L to list users. -v to be verbose.

kyoupark@kit-debian64:~/pad$ sudo smbpasswd -a kyoupark
New SMB password:
Retype new SMB password:
Added user kyoupark.
kyoupark@kit-debian64:~/pad$ sudo pdbedit -L -v
---------------
Unix username:        kyoupark
NT username:
Account Flags:        [U          ]
User SID:             S-1-5-21-371559316-4287552441-583710022-1000
Primary Group SID:    S-1-5-21-371559316-4287552441-583710022-513
Full Name:            Kit Park
Home Directory:       \\kit-debian64\kyoupark
HomeDir Drive:
Logon Script:
Profile Path:         \\kit-debian64\kyoupark\profile
Domain:               KIT-DEBIAN64
Account desc:
Workstations:
Munged dial:
Logon time:           0
Logoff time:          Wed, 06 Feb 2036 15:06:39 GMT
Kickoff time:         Wed, 06 Feb 2036 15:06:39 GMT
Password last set:    Fri, 16 Jun 2017 18:29:07 BST
Password can change:  Fri, 16 Jun 2017 18:29:07 BST
Password must change: never
Last bad password   : 0
Bad password count  : 0
Logon hours         : FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
kyoupark@kit-debian64:~/pad$


{network}
2. By default, VB uses Host-only networking and this means host cannot see
guest. To enable for host to see guest, change to `bridged network` as shown
in:

6.4. Bridged networking
http://www.virtualbox.org/manual/ch06.html

So change it in setting menu of VB and restart VM. Get IP and can access guest
from host windows.

note: must have two network adaptors: NAT for internet and bridged for samba
between host and guest.  But cannot ssh to other host. seems firewall problem
and when back to host only network, ssh works but not sharing as host only net
gives private net ip.

note: must have two network adaptors: NAT for internet and bridged for samba
between host and guest.  But cannot ssh to other host. seems firewall problem
and when back to host only network, ssh works but not sharing as host only net
gives private net ip.


{resize}
C:\Program Files\Oracle\VirtualBox>VBoxManage.exe modifymedium \
    "C:\Users\kyoupark\VirtualBox VMs\debian\debian.vdi" --resize 92106
0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%
C:\Program Files\Oracle\VirtualBox>


{addition}
/tmp/vbox.0/Makefile.include.header:97: *** Error: unable to find the sources
of your current Linux kernel. Specify KERN_DIR= and run Make again.  Stop.

$ sudo apt-get install linux-headers-$(uname -r)
// needs to install gcc if there is not
$ bash VBoxLinuxAdditions.run


={============================================================================
*kt_linux_tool_051* tool-cron tool-incron

man 5 crontab

http://www.unixgeeks.org/security/newbie/unix/cron-1.html

Cron is the name of program that enables unix users to execute commands or
scripts (groups of commands) automatically at a specified time/date. It is
normally used for sys admin commands, like makewhatis, which builds a search
database for the man -k command, or for running a backup script, but can be
used for anything. A common use for it today is connecting to the internet and
downloading your email.

This file will look at Vixie Cron, a version of cron authored by Paul Vixie.


The fields are:

minute hour dom month dow user cmd

minute	
This controls what minute of the hour the command will run on, and is between
'0' and '59'

hour	
This controls what hour the command will run on, and is specified in the 24
hour clock, values must be between 0 and 23 (0 is midnight)

dom	
This is the Day of Month, that you want the command run on, e.g. to run a
command on the 19th of each month, the dom would be 19.

month	
This is the month a specified command will run on, it may be specified
numerically (0-12), or as the name of the month (e.g. May)

dow	
This is the Day of Week that you want a command to be run on, it can also be
numeric (0-7) or as the name of the day (e.g. sun).

user	
This is the user who runs the command.

cmd	
This is the command that you want run. This field may contain multiple words
or spaces.

If you don't wish to specify a value for a field, just place a * in the 
field.

e.g.
01 * * * * root echo "This command is run at one min past every hour"
17 8 * * * root echo "This command is run daily at 8:17 am"
17 20 * * * root echo "This command is run daily at 8:17 pm"
00 4 * * 0 root echo "This command is run at 4 am every Sunday"
* 4 * * Sun root echo "So is this"
42 4 1 * * root echo "This command is run 4:42 am every 1st of the month"
01 * 19 07 * root echo "This command is run hourly on the 19th of July"

Notes:

Under dow 0 and 7 are both Sunday.

If both the `dom` and `dow` are specified, the command will be executed when
either of the events happen. 

* 12 16 * Mon root cmd

Will run cmd at midday every Monday and every 16th, and will produce the same
result as both of these entries put together would:

* 12 16 * * root cmd
* 12 * * Mon root cmd


ixie Cron also accepts `lists` in the fields. Lists can be in the form, 1,2,3
(meaning 1 and 2 and 3) or 1-3 (also meaning 1 and 2 and 3).

59 11 * * 1,2,3,4,5 root backup.sh

Will run backup.sh at 11:59 Monday, Tuesday, Wednesday, Thursday and Friday,
as will:

59 11 * * 1-5 root backup.sh 


Cron also supports `step` values.

A value of */2 in the dom field would mean the command runs every two days and
likewise, */5 in the hours field would mean the command runs every 5 hours.

e.g. 
* 12 10-16/2 * * root backup.sh
is the same as:
* 12 10,12,14,16 * * root backup.sh

*/15 9-17 * * * root connection.test
Will run connection.test every 15 mins between the hours or 9am and 5pm

<ex>
[si_logs@theyard bin]$ cat /etc/crontab
SHELL=/bin/bash
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
HOME=/

# For details see man 4 crontabs

# Example of job definition:
# .---------------- minute (0 - 59)
# |  .------------- hour (0 - 23)
# |  |  .---------- day of month (1 - 31)
# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
# |  |  |  |  |
# *  *  *  *  * user-name command to be executed
17 *    * * *   root    cd / && run-parts --report /etc/cron.hourly
25 6    * * *   root    test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.daily )
47 6    * * 7   root    test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.weekly )
52 6    1 * *   root    test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.monthly )


<crontab>
man crontab

NAME
       crontab - maintain crontab files for individual users (Vixie Cron)

SYNOPSIS
       crontab [ -u user ] file
       crontab [ -u user ] [ -i ] { -e | -l | -r }

DESCRIPTION
       crontab  is  the  program used to install, deinstall or list the tables
       used to drive the cron(8) daemon in Vixie Cron.   
       
       <each-user> Each user can have their own crontab, and though these are
       files in /var/spool/cron/crontabs, they are not intended to be edited
       directly.

       pi@raspberrypi /etc/cron.daily $ sudo ls /var/spool/cron/crontabs/pi
       /var/spool/cron/crontabs/pi

       // # DO NOT EDIT THIS FILE - edit the master and reinstall.
       // # (/tmp/crontab.8nWdVI/crontab installed on Mon Oct  9 11:42:09 2017)
       // # (Cron version -- $Id: crontab.c,v 2.13 1994/01/17 03:20:37 vixie Exp $)
       // # Edit this file to introduce tasks to be run by cron.
       // #
       // # Each task to run has to be defined through a single line
       // # indicating with different fields when the task will be run
       // # and what command to run for the task
       // #
       // # To define the time you can provide concrete values for
       // # minute (m), hour (h), day of month (dom), month (mon),
       // # and day of week (dow) or use '*' in these fields (for 'any').#
       // # Notice that tasks will be started based on the cron's system
       // # daemon's notion of time and timezones.
       // #
       // # Output of the crontab jobs (including errors) is sent through
       // # email to the user the crontab file belongs to (unless redirected).
       // #
       // # For example, you can run a backup of all your user accounts
       // # at 5 a.m every week with:
       // # 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/
       // #
       // # For more information see the manual pages of crontab(5) and cron(8)
       // #
       // # m h  dom mon dow   command
       // 00 04 * * * /home/pi/snugupdate-v2-snugberrypi/run_auto.sh
       // 00 05 * * * /home/pi/snugupdate-v2-snugberrypi/run.sh

       If the /etc/cron.allow file exists, then you must be listed  (one  user
       per  line)  therein in order to be allowed to use this command.  If the
       /etc/cron.allow file does not exist but the  /etc/cron.deny  file  does
       exist,  then you must not be listed in the /etc/cron.deny file in order
       to use this command.

       `If neither of these files exists`, then depending on site-dependent
       configuration parameters, only the super user will be allowed to use
       this command, or all users will be able to use this com‐ mand.

       The first form of this command is used to install a new crontab from
       some named file or standard input if the pseudo-filename ``-'' is
       given.

       The  -l  option  causes the current crontab to be displayed on standard
       output. See the note under DEBIAN SPECIFIC below.


       The -e option is used to edit the current crontab using the editor
       specified by the VISUAL or EDITOR environment variables.  After you
       exit  from  the  editor,  the  modified  crontab  will  be installed
       automatically. If neither of the environment variables is defined, then
       the default editor /usr/bin/editor is used.

       pi@raspberrypi /etc/cron.daily $ EDITOR=vi crontab -e
       crontab: installing new crontab


<tool-incron>
incrontab(1)

NAME
       incrontab - table manipulator for inotify cron (incron)

SYNOPSIS
       incrontab [-u user] [-f config] file

       incrontab [-u user] [-f config] [-l | -r | -e | -t | -d]

DESCRIPTION
       incrontab is a table manipulator for the inotify cron (incron) system.
       It creates, removes, modifies and lists user tables (incrontab(5)).

       Each  user (including even system users without home directories) has
       an incron table which can't be manipulated directly (only root can
           effectively change these tables and is NOT recommended to do so).

       If /etc/incron.allow exists only users listed here may use incron.
       Otherwise if /etc/incron.deny exists only users NOT listed here may use
       incron. If none  of  these  files  exists  everyone  is allowed  to
       use  incron. (Important note: This behavior is insecure and will be
           probably changed to be compatible with the style used by ISC Cron.)
       Location of these files can be changed in the configuration.

       The first form of this command imports a file, validates it and stores
       to the table. "-" can be used for loading from the standard input.

       -l (or --list) option causes the current table is printed to the
       standard output.


incrontab(5)

NAME
       incrontab - tables for driving inotify cron (incron)

DESCRIPTION
       An incrontab file contains instructions to the incrond(8) daemon of the
       general form: "run this command on these `file events` ". There are two
       categories of tables: system tables (with root privi‐ leges) and user
       tables (with user privileges).

       System tables are (by default) located in /etc/incron.d and may have
       any names. Each system table exists separately inside incron and their
       watches never collide.

       Each user has their own table, and commands in any given incrontab will
       be executed as the user who owns the incrontab. System users (such as
           apache, postfix, nobody etc.)  may  have  their  own incrontab.

       incrontab files are read when the incrond(8) daemon starts and after
       any change (incrontab file are being hooked when incrond is running).

       Blank lines are ignored. The general line format is the following:

       <path> <mask> <command>

       Where `path` is an absolute filesystem path, `mask` is an event mask
       (in symbolic or numeric form) and `command` is an executable file (or a
           script) with its arguments. See bellow for event mask symbols.  The
       executable file may be noted as an absolute path or only as the name
       itself (PATH locations are examined).

       Please remember that the same path may occur only once per table
       (otherwise only the first occurrence takes effect and an error message
        is emitted to the system log).

EVENT SYMBOLS
       These basic event mask symbols are defined:

       IN_ACCESS           File was accessed (read) (*)
       IN_ATTRIB           Metadata changed (permissions, timestamps, extended attributes, etc.) (*)
       IN_CLOSE_WRITE      File opened for writing was closed (*)
       IN_CLOSE_NOWRITE    File not opened for writing was closed (*)
       IN_CREATE           File/directory created in watched directory (*)
       IN_DELETE           File/directory deleted from watched directory (*)
       IN_DELETE_SELF           Watched file/directory was itself deleted
       IN_MODIFY           File was modified (*)
       IN_MOVE_SELF        Watched file/directory was itself moved
       IN_MOVED_FROM       File moved out of watched directory (*)
       IN_MOVED_TO         File moved into watched directory (*)
       IN_OPEN             File was opened (*)

EXAMPLE
       These are some example rules which can be used in an incrontab file:

       /tmp IN_ALL_EVENTS abcd $@/$# $%

       /usr/bin IN_ACCESS,IN_NO_LOOP abcd $#

       /home IN_CREATE /usr/local/bin/abcd $#

       /var/log 12 abcd $@/$#

       The  first  line  monitors all events on the /tmp directory. When an
       event occurs it runs a application called 'abcd' with the full path of
       the file as the first arguments and the event flags as the second one.

       The second line monitors accesses (readings) on the /usr/bin directory.
       The application 'abcd' is run as a handler and the appropriate event
       watch is disabled until  the  program  finishes.  The file name
       (without the directory path) is passed in as an argument.

       The  third example is used for monitoring the /home directory for newly
       create files or directories (it practically means an event is sent when
           a new user is added). This event is processed by a program
       specified by an absolute path.

       And the final line shows how to use numeric event mask instead of
       textual one. The value 12 is exactly the same as
       IN_ATTRIB,IN_CLOSE_WRITE.

<ex> <incrontab>
pi@raspberrypi ~/snugupdate-v2-snugberrypi/snugupdate-v2-python/images $ incrontab -l
/srv/ftp/upload IN_CLOSE_WRITE /home/pi/snugupdate-v2-snugberrypi/stblog.sh $@/ $#
/home/pi/snugupdate-v2-snugberrypi/www/manual-update IN_CREATE /home/pi/snugupdate-v2-snugberrypi/run.sh

pi@raspberrypi /etc $ sudo cat /var/spool/incron/pi
/srv/ftp/upload IN_CLOSE_WRITE /home/pi/snugupdate-v2-snugberrypi/stblog.sh $@/ $#
/home/pi/snugupdate-v2-snugberrypi/www/manual-update IN_CREATE /home/pi/snugupdate-v2-snugberrypi/run.sh


<issue>
a bug of using cron

// cron runs this script

pi@raspberrypi ~ $ crontab -l
00 02 * * * /home/pi/snugupdate-v2-snugberrypi/run_auto.sh

// run_auto.sh runs autoupdate.sh

pi@raspberrypi ~/snugupdate-v2-snugberrypi $ more run_auto.sh
./autoupdate.sh

// autoupdate.sh runs

FILE="version.txt"

while read line; do
cur_ver=`echo $line|awk '{print $1}'`
done <$FILE

if test "$cur_ver" = "$new_ver";then

the problem is that autoupdate.sh assumes that version.txt is in the

/home/pi/snugupdate-v2-snugberrypi

but cron runs in /home/pi which is user pi's home and while loop fails and
cur_ver has empty which causes if check always fails.


<reading>
http://www.linux-magazine.com/Issues/2014/158/Monitoring-with-incron

Monitor file and directory activity with incron 

Controlled Files

The incron utility provides an easy way to initiate commands and scripts
triggered by filesystem events.

If you want to perform a task every time a specific event occurs, you could
employ various techniques for polling or log watching; however, Linux provides
a more general solution in the form of a tool called incron. Incron uses the
inotify subsystem [1] to listen for events that affect a filesystem, such as
opening, creating, or deleting a file; accessing a directory; or changing an
attribute.

The name incron suggests the common cron utility [2]; however, cron jobs are
triggered by a moment in time (every Friday, once a day at 3 am, in August,
    etc.), whereas incron is triggered by file or directory events. The gears
in your head are probably already spinning, thinking of all the potential uses
of incron. So, in this article, I describe how to set up incron and put it to
work in some simple examples.  Getting incron

The incron tool set is not usually preinstalled by default in most distros,
    but because it is an implementation of the inotify subsystem (see the box
        "The Origin of incron"), it is likely in your distro's repository, so
      you can install it through the package manager. However, if for some
      reason you can't install the necessary packages from your distro's
      repos, you can download the latest version [3] and build from source.

The Origin of incron

The in in incron comes from inode, the data structure that contains the
details (location on the physical disk, size, owner, etc.) of each file and
directory in the filesystem. Inotify is the kernel subsystem on which incron
is based. This subsystem monitors file and directory changes and can be
queried by applications, allowing, for example, a new folder icon to pop up in
real time in a graphical file browser window (e.g., Nautilus or Dolphin), even
if the underlying directory has been created by some other means (e.g., with
    mkdir from a shell). It can also be used to inform an application that an
  open file has been changed on disk by some other program, thus allowing the
    user to overwrite or rename the version being worked on.

<kernel-support>
Note that incron is a relatively recent technology  the inotify notification
framework was first implemented in kernel 2.6.13  so if you're running an
older kernel, incron will not work.

Incron comprises several bits and pieces, of which the main component is the
incrond daemon. This daemon installs into /etc/init.d. Depending on your
system, it can be started with one of the following commands

# /etc/init.d/incrond start
# /etc/init.d/incron start

or, if you're using systemd,

# systemctl status incron.service

or any variation thereof.


Hello incron!

Once the daemon is running, you can create your own particular "Hello World!"
In this first project, I'll create a logfile and add a line to it every time a
file is added to a monitored directory.

First, I create the directory I want to monitor:

$ mkdir my_dir

The incron system is similar to cron in that it saves a file for each user.
This file, called incrontab, contains the events you want to monitor linked to
the actions you want to run. Although you could edit this file by hand (I'll
    look at that a bit later), the done way of doing things is to use the
incrontab instruction with, in this case, the -e (for edit) argument:

incrontab -e

This command opens the file for the current user in the preconfigured default
editor  that is, whatever your $EDITOR environment variable points to (or vi
    if it isn't pointing to anything). As with cron, each event/task pair has
to be on one line and comprises:

    <a path>  the path to the directory or file to monitor.
    <a mask>  the events to monitor (see Table 1).
    <a command>  the action to run. It can be an individual command or a
    script that runs from the shell.

Table 1

Monitored Events

IN_ACCESS 
File was accessed (read)

IN_ATTRIB 
Metadata was changed (permissions, timestamps, extended attributes, etc.)

IN_CLOSE_WRITE
File opened for writing was closed

IN_CLOSE_NOWRITE
File not opened for writing was closed

IN_CLOSE
Combines IN_CLOSE_WRITE and IN_CLOSE_NOWRITE

IN_CREATE
File/directory created in watched directory

IN_DELETE
File/directory deleted from watched directory

IN_DELETE_SELF
Watched file/directory was deleted

IN_MODIFY
File was modified

IN_MOVE_SELF
Watched file/directory was moved

IN_MOVED_FROM
File moved out of watched directory

IN_MOVED_TO
File moved into watched directory

IN_MOVE
A combination of IN_MOVED_FROM and IN_MOVED_TO.

IN_OPEN
File was opened


Special Events

IN_ALL_EVENTS
Combines all of the above events

IN_DONT_FOLLOW
Don't dereference pathname if it is a symbolic link

IN_ONESHOT
Monitor pathname for only one event

IN_ONLYDIR
Only watch pathname if it is a directory

Wildcard Event
IN_NO_LOOP
Disable monitoring of events until the current event is handled completely
(until its child process exits  avoids infinite loops)

In this example, incrontab will contain a single line:

/home/<user>/my_dir IN_CREATE /home/<user>/bin/hello.sh

The hello.sh file contains the very short script,

#!/bin/bash
echo "File created." >> /home/<user>/file_log

where <user> is the username of interest. For this command to work, you must
make hello.sh executable with:

chmod a+x bin/

Once you have saved and closed the user's incrontab file, the program will
show the message table updated, indicating the changes have been registered.
Now, if you create a file in my_dir by typing, for example,

$ touch my_dir/file1.txt

the file_log file will appear in your home directory containing the line File
created.


<limitation>
Note the first limitation of incron: Compound operations, such as the echo
instruction that redirects its output to a file in hello.sh, `must be wrapped`
in shell script, or incron will choke. Because incron is not a shell command
interpreter, it does not understand such things as the redirections, pipes,
and globbings that make up a compound instruction. The only thing incron can
  use in the command section of the line is the instruction's name and its
  parameters. For example, this command

/home/paul/my_dir IN_CREATE echo "File created." >> /home/paul/file_log

would not work as expected, because it would be interpreted as follows:

    echo is the instruction to run (correct).
    "File created." is a parameter for echo (also correct).
    >> is another argument for echo (incorrect).
    /home/paul/file_log is also an argument for echo (also incorrect).

This behavior is the default for incron, so it won't generate any errors in
its logs. (You can track incron's errors or lack thereof by checking
    /var/log/cron.) Incron simply ignores what it doesn't understand and
carries on.

This example also illustrates the second limitation of incron: In the
incrontab line, you have to specify the complete absolute path to the
executable that, in this case, lives in the user's own bin/ directory.
Although this directory might be included in the user's own $PATH environment
variable, the incron daemon runs as superuser, so the executable would have to
be in the root's $PATH for the daemon to find it; otherwise, you have to
specify the absolute path. Thus, you must make sure your executable is in
/bin, /usr/bin, /usr/local/bin, or somewhere equally accessible, or point to
the program with an absolute path. 


Wild Cards

This "Hello World" incron program is as silly as any other, but with a couple
of small changes you can modify it to make it much more useful. For example,
registering the name of a created file in the logfile is easy. Incron provides
  a series of default variables (for some reason called "wildcards" in incron
      jargon) that contain this kind of information. (See Table 2 for a full
        list.) For the time being, $#, which contains the name of the file
      affected by the event, is of interest.

Table 2

Wildcards

Wildcard Meaning

$$
Dollar sign

$@
Watched filesystem path

$#
Event-related file name

$%
Event flag (textual)

$&
Event flag (numerical)

Open your incrontab file again (incrontab -e) and change your monitoring line
to:

/home/paul/my_dir IN_CREATE /home/paul/bin/hello.sh $#

Next, modify your hello.sh script to:

#!/bin/bash
echo "File $1 created." >> /home/paul/file_log

Now try it out by creating a new file in my_dir:

$ touch my_dir/file2.txt

If you look in file_log, you'll see that it contains a new line that says File
file2.txt created..


Automatic Apache

A more serious use for incron would be to monitor a server's configuration
files and order a reboot if anything changes, such as a modified Apache web
server httpd.conf or apache2.con file. Begin by finding out which user has
privileges to stop and restart Apache on your system and edit their incrontab.

For this exercise, assume that user is root, become superuser, and open root's
incrontab for editing:

$ su
# incrontab -e

If you get an error that reads user 'root' is not allowed to use incron, edit
/etc/incron.allow and add root to the list of allowed users. Then, insert the
following line into root's incrontab file:

/etc/apache2/apache2.conf IN_MODIFY /etc/init.d/apache2 restart

After modifying httpd.conf, or apache2.conf (e.g., by changing the Timeout
    value), and saving the file, look at /var/log/apache2/error.log (Figure
      1). You'll see that Apache was restarted automatically when you hit the
    Save button (or typed :wq).  Figure 1: Apache is restarted automatically
    every time you modify its main config file.

However, you probably know that it's been a long time since Apache has had
only one configuration file. Apache's configuration has been modularized and
sometimes approaches tens of files spread out over several nested directories.
To monitor the whole lot, you can add the following to your root's incrontab:

/etc/apache2/conf.d/ IN_CREATE /etc/init.d/apache2 restart

This method is not very subtle, but, again, it'll work. You've probably
figured out that this code restarts the web server every time a new file is
added to the /etc/apache2/conf.d/ directory. It doesn't check whether it is a
*.conf file or not, but because Apache's conf.d directory was created for
those kinds of files, it doesn't really matter.

Furthermore, you're going to want to monitor the files within the conf.d
directory to see whether any of the files change. To do that, you can modify
the incrontab to:

/etc/apache2/conf.d/ IN_CREATE,IN_CLOSE_WRITE /etc/init.d/apache2 restart

<limitation>
As you can see, if you want to monitor more than one event, you have to
separate the triggers with commas. In this case, you are monitoring for both
the  IN_CREATE and IN_CLOSE_WRITE events, because you also want to restart
Apache when a configuration file closes after it has been modified.

Note that your root's incrontab file is now two lines long, one for each
directory. You're probably thinking that using two lines for a directory and
one of its subdirectories is inefficient, and that it would be more elegant if
you had a way to monitor the upper directory and then drill down.

You'd be wrong.

Incron does not support recursive directory monitoring, nor are there any
plans for it to do so, and there's a good reason for this decision: infinite
recursive loops.  To Infinity and Beyond

Imagine that you want to monitor the files in a directory and write an entry
in a log each time a file is modified. You save the log for convenience in the
same directory you're monitoring.

The incrontab could look like this:

</path/to/directory/> IN_CLOSE_WRITE log_changes $#

where log_changes is the following Bash script:

#!/bin/bash
echo "`date` File $1 modified" >> </path/to/directory/>my.log

Do you start to see the problem? You've created an infinite loop.

When you finish writing the entry in my_log, the IN_CLOSE_WRITE event fires
because, you know, my_log is a file in the monitored directory, which was
closed after being written to. A new entry is written in my_log, and the
IN_CLOSE_WRITE event fires again, because my_log is a file in the monitored
directory, which was closed after being written to. IN_CLOSE_WRITE fires again
and  … . You get the picture. Bad things would happen if you let something
like that run unchecked.

To avoid this kind of situation, every sys admin worth his salt has two tools
available: (1) common sense and (2) the IN_NO_LOOP wildcard event. This
element blocks the incrontab instruction until it's completely finished,
avoiding the other event from firing in the same iteration, and thus avoiding
  loops.

Although the following is a lousy, lazy, shoddy solution (the best move would
    be to store your logfile elsewhere), you can solve the problem described
above with the following incrontab line:

</path/to/directory/> IN_CLOSE_WRITE, IN_NO_LOOP log_changes $#

So, you can see that drilling down into subdirectories of subdirectories of
subdirectories (including soft-linked directories), if it were implemented in
incron, would increase significantly the chances of infinite loops, so no
recursive monitoring is allowed in incron.  Silent Alarm

In the next example, I'm going to implement a security system against
intruders who try to access the system and directories containing sensitive
documents. I'm not talking about that folder called secret_CIA_documents that
everybody has in their home directory.

If you have learned anything from the likes of the NSA, LulzSec, and
Anonymous, it is that the files most coveted by attackers are email messages.
Email contains usernames, passwords, bank accounts, credit card numbers,
personal images, business contracts, and on and on. A frequently used email
  folder is a true treasure trove for your friendly neighborhood snooper.

However, monitoring a real email folder would be a nightmare, with so many
writes, rewrites, deletions, and changes making it impossible to filter out
false positives. Also, are you sure you want an attacker to get as far as
being able to read your messages? No, you don't. What you'll do instead is
construct a honeypot by creating a .Mail directory hanging off your home
directory and filling it with messages culled from, say, your spam folder.
That should make for fun reading. If your email client already points to that
folder to store messages, CHANGE IT! It's way too exposed.

Incron will monitor your honeypot directory as before, but with a twist: Your
action will depend on what the intruder does. If the intruder opens a file for
reading, you'll activate the silent alarm and warn the system administrator.
However, if the intruder starts writing in files, you'll disconnect the
machine from the network with, for example

ifconfig eth0 down

The problem with this solution is that, apparently, you need two lines to call
two different scripts, depending on the triggered event. Something like:

/home/<user>/.Mail IN_ACCESS warn_admin.sh
/home/<user>/.Mail IN_CLOSE_WRITE close_connection.sh

But that won't work because incron doesn't allow you to monitor the same thing
twice. If you try to run these lines to trigger one of the events, you will
get an error in /var/log/cron that says:

Jul 24 21:17:54
host
incrond[9454]:
cannot create
watch for user root:
(16) Device or resource busy

To solve this problem, you can combine both events onto one line and use the
predefined incron variable $%:

/home/paul/.Mail IN_ACCESS, IN_CLOSE_WRITE access_control.sh $%

The $% wildcard is passed to your script as an argument, telling it which
event was triggered, so you can then deal with it as shown in Listing 1.

Listing 1

Incron Event Decisions

01 ...
02 case "$1" in
03
04 "IN_ACCESS")            warn_admin.sh
05                         ;;
06
07 "IN_CLOSE_WRITE")       close_connection.sh
08                         ;;
09
10 *)                      echo "Event $1 not considered!"
11                         ;;
12 ...


Dynamic incrontab

note: skipped the rest


={============================================================================
*kt_linux_tool_100* tool-gpg

NAME
       gpg - OpenPGP encryption and signing tool

SYNOPSIS
       gpg [--homedir dir] [--options file] [options] command [args]

DESCRIPTION
       gpg is the OpenPGP only version of the GNU Privacy Guard (GnuPG). It is
       a tool to provide digital encryption and signing services using the
       OpenPGP standard. gpg features complete key management and all bells
       and whistles you can expect from a decent OpenPGP implementation.

       This is the standalone version of gpg.  For desktop use you should
       consider using gpg2 from the GnuPG-2 package ([On some platforms gpg2
           is installed under the name gpg]).

COMMANDS
       Commands are not distinguished from options except for the fact that
       `only one command is allowed`

       gpg may be run with no commands, in which case it will perform a
       reasonable action depending on the type of file it is given as input
       (an encrypted message is decrypted, a signature is verified, a file
        containing keys is listed).

       Please remember that option as well as command parsing stops as soon as
       a non-option is encountered, you can explicitly stop parsing by using
       the special option --.

   Commands to select the type of operation

       --decrypt
       -d     

       Decrypt the file given on the command line (or STDIN if no file is
           specified) and write it to STDOUT (or the file specified with
             --output). If the decrypted file is signed, the  signature is
           also  verified.  This  command  differs  from  the  default
           operation, as it never writes to the filename which is included in
           the file and it rejects files which don't begin with an encrypted
           message.

OPTIONS
       gpg features a bunch of options to control the exact behaviour and to
       change the default configuration.

       Long options can be put in an options file (default
           "~/.gnupg/gpg.conf"). Short option names will not work - for
       example, "armor" is a valid option for the options file, while "a" is
       not. Do not write the 2 dashes, but simply the name of the option and
       any required arguments. Lines with a hash ('#') as the first
       non-white-space character are ignored. Commands may be  put  in  this
       file too, but that is not generally useful as the command will execute
       automatically with every execution of gpg.

       Please remember that option parsing stops as soon as a non-option is
       encountered, you can explicitly stop parsing by using the special
       option --.

   How to change the configuration

       These options are used to change the configuration and are usually
       found in the option file.

       --batch

       --no-batch
              Use batch mode. Never ask, do not allow interactive commands.
              --no-batch disables this option. This option is commonly used
              for unattended operations.

              WARNING: Unattended operation bears a higher risk of being
                exposed to security attacks.  In particular any unattended use
                of GnuPG which involves the use of secret keys should  take
                care not to provide an decryption oracle.  There are several
                standard pre-cautions against being used as an oracle.  For
                example never return detailed error messages or any
                diagnostics printed by your software to the remote site.
                Consult with an expert in case of doubt.

              Note that even with a filename given on the command line, gpg
              might still need to read from STDIN (in particular if gpg
                  figures that the input is a detached signature and no data
                  file has been specified).  Thus if you do not want to feed
              data via STDIN, you should connect STDIN to ‘/dev/null’.

       --no-tty
              Make sure that the TTY (terminal) is never used for any output.
              This option is needed in some cases because GnuPG sometimes
              prints warnings to the TTY even if --batch is used.

   Doing things one usually doesn't want to do.

       --passphrase-fd n
              Read the passphrase from file descriptor n. Only the first line
              will be read from file descriptor n. If you use 0 for n, the
              passphrase will be read from STDIN. This can only be  used  if
              only one passphrase is supplied.


<key>

// copy it to ~/.gnupg
kyoupark@kit-debian64:~/git/kb/si/snug/gnupg$ ll
total 28
-rw-r--r-- 1 kyoupark kyoupark    0 Jun 12  2017 trustdb.gpg.lock
-rw-r--r-- 1 kyoupark kyoupark 1280 Jun 12  2017 trustdb.gpg
-rw-r--r-- 1 kyoupark kyoupark    0 Jun 12  2017 secring.gpg.lock
-rw-r--r-- 1 kyoupark kyoupark 2578 Jun 12  2017 secring.gpg
-rw-r--r-- 1 kyoupark kyoupark  600 Jun 12  2017 random_seed
-rw-r--r-- 1 kyoupark kyoupark    0 Jun 12  2017 pubring.gpg.lock
-rw-r--r-- 1 kyoupark kyoupark 1200 Jun 12  2017 pubring.gpg
-rw-r--r-- 1 kyoupark kyoupark 1200 Jun 12  2017 pubring.bak
drwxr-xr-x 3 kyoupark kyoupark 4096 Jun 12  2017 ../
drwxr-xr-x 2 kyoupark kyoupark 4096 Jun 12  2017 ./


C:\Users\kyoupark\AppData\Local\VirtualStore\Program Files (x86)\GNU\GnuPG\public.key
-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: GnuPG v1

mQENBFhKo8kBCADZzievQWECXc+eEKK5xQdsEM9dixmfODUKo7dQCFPVUeAWC/uu
g6Ayjz/ui0mantO/GqVJ/eLdvIhBtjWUd87rWhHJ9AvSz6I6LT2l5zt+fPpJvrFs
y67vKHSgxB+B0p2ffOE1aXcBHNW2ZLYG3nzvGM/BqNvNzNv+ILPuw7bNOYxzVY7O
eLB6Iu8pldNBM4JUzWs31C7IuGZLR0DguWP4mlpiNUaQsiRWgx/9QAZnsNsas2Yf
JCgC7HqU+rfY/Gq4mYp+JDiGFQ7rIn63lEf6wfg2gOUuZdA23sbJ8RmyEeGqcgOF
kOjotQg+CamMW1pP8myAY0MxIJIno3dv6T2HABEBAAG0KUtpdCBQYXJrIChncGcg
a2V5KSA8a2VpdGVlcGFya0BnbWFpbC5jb20+iQE4BBMBAgAiBQJYSqPJAhsDBgsJ
CAcDAgYVCAIJCgsEFgIDAQIeAQIXgAAKCRAtg1lxtKUMStIUCADM5J1P5kBR/dU/
cBHOgf3DwuOj0o1hPhVbL5P6Jrct8kz2FzUN2oM5DSMMWvFI/46+OgePQKJjp9E0
yHn4IzNs+j+avDPFioPfrtXuZg8xbD0iYcjQJCAyZeoyRyh6Px/jEWAoBpUE1JoI
9eCKyDxDjPYEe2Z43S5hv9NJU6D6J2fEQfExy0B7A/Hc1VFeadzezTTguWFMqW5T
zEBWHqG2X4HqJ/hvm0VhLLH8mzjZDp2iIN7EKr30LduZMKKFRtBrkUQMEjcMaF8e
w9CRmQn/xeBHGpTYo/CeOt62lw5vX+e/9hNjwCR36B4ZKJid5/dEGTscYE5Y2A1R
zVUrq6utuQENBFhKo8kBCACunUBOfIvaFXoyjSeTjWjD5aqw8pZSO7nqSrAZdAGB
7gx9JzSUfaQUDQWPPF9yzdCxGZt6N1L5duEVbuf3sd6hdceKokIyBVSjnHNvtxrG
ZZKGCv3tOVcMn4pGr4aRAZg/F32aUQ2mVLAfHXPx/FvssYZWFv3H7U7AmD4dc13f
1cOmXXsqaT9J0hvs5i9pBb7Q8Y8hKqvWybWVbx4gJ+CS5Tnm5GKULCCUPrvPYVsn
MkXDO7Yzb5CchGqtIs0NB80NHu3yUJJwdiqGJpbX5tui71CUB19ZAK31FFVSD6wM
XUyL+VJ9sWjG2lOepRWj9RDxhaCZr7MHzUQeUh2uBDb5ABEBAAGJAR8EGAECAAkF
AlhKo8kCGwwACgkQLYNZcbSlDErcUggAu9lRnrzaUqEtvtarnK+h+b9KCCFtlsVB
68lXIkNnLe88zeCbMI2PSwXEVZ0wuyyRs7dvUNJT2pMsSoloLDSysva2LU+uxgDQ
FqCEkqhdqTUz32CZdaXK9Um9ll5WmSEOqnw5GIwpGZNf+OwCZeBBPBf3F75nPCa4
MmEFLo464QNM7ZpuTfSHtvB7GT2WpS65r4oaiCpxGNCvC2eVGDlpC/el0KzS370Y
GVsqE9xeUH89c6YzT7vzNLQSdaKPVU8AlVtL8yTNRfJ7jIJWaue5y5Jf1wCwsREC
x/YHyW6aAEUx0ZpLncOBV7rdqwnr1NkxwVYAvDp/x6dcn+7BRLgxCQ==
=pJS3
-----END PGP PUBLIC KEY BLOCK-----

kyoupark@kit-debian64:~/git/kb/si/snug$ gpg --fingerprint
/home/kyoupark/.gnupg/pubring.gpg
---------------------------------
pub   2048R/B4A50C4A 2016-12-09
      Key fingerprint = C27C 1B00 C71B 9735 82F3  409E 2D83 5971 B4A5 0C4A
uid                  Kit Park (gpg key) <keiteepark@gmail.com>
sub   2048R/E0CBDE6B 2016-12-09


<key-export-import> secret-key
http://superuser.com/questions/1009623/gpg-tools-location-of-private-keys

gpg --export-secret-keys > secret-keys.gpg

gpg --import secret-keys.gpg
gpg: key B4A50C4A: public key "Kit Park (gpg key) <keiteepark@gmail.com>" imported
gpg: Total number processed: 1
gpg:               imported: 1  (RSA: 1) >


<<signature>
https://www.madboa.com/geek/gpg-quickstart/

gpg --verify crucial.tar.gz.asc crucial.tar.gz

Creating a detached signature is similarly easy. The following example will
create a signature for your-file.zip called your-file.zip.asc.

gpg --armor --detach-sign your-file.zip

People who have imported your public key into their keyrings can then verify
that their version of your file is identical to theirs.

<ex>
gpg --output foo.txt --decrypt foo.txt.gpg

# don't ask passphrase
gpg --output foo.txt --passphrase string --decrypt foo.txt.gpg
gpg --batch --no-tty --passphrase-fd 0 --decrypt "images/drx890.SYSF40.73.00.enc.snugupdate.gpg"


<ex>
pi@raspberrypi ~/snugupdate-v2-snugberrypi/snugupdate-v2-python $ gpg --list-secret-keys
/home/pi/.gnupg/secring.gpg
---------------------------
sec   2048R/E7204075 2014-09-16
uid                  Snug User (SnugberryPi) <pi@snugupdater>
ssb   2048R/2C335150 2014-09-16


<ex> to see what keys are used to encrypt
gpg2 --batch --list-packets /Path/to/your/file.gpg

kyoupark@kit-debian64:~/.gnupg$ gpg --list-secret-keys
/home/kyoupark/.gnupg/secring.gpg
---------------------------------
sec   2048R/B4A50C4A 2016-12-09
uid                  Kit Park (gpg key) <keiteepark@gmail.com>
ssb   2048R/E0CBDE6B 2016-12-09

$ gpg --batch --list-packets drx890_SYSF40.73.00.gpg
...
gpg: encrypted with 2048-bit RSA key, ID E0CBDE6B, created 2016-12-09
      "Kit Park (gpg key) <keiteepark@gmail.com>"
...

$ gpg --homedir /home/kyoupark/si/si-pi-images-to-use/gnupg --batch --list-packets drx890_SYSF89.18.00.gpg |& grep -A 2 created


<ex> to change pgp keys

       --homedir dir
              Set the name of the home directory to dir. If this option is not
              used, the home directory defaults to ‘~/.gnupg’.  It is only
              recognized when given on the command line.  It also overrides
              any home directory stated through the environment variable
              ‘GNUPGHOME’ or (on Windows systems) by means of the Registry
              entry HKCU\Software\GNU\GnuPG:HomeDir.

              On  Windows  systems  it  is  possible  to  install  GnuPG as a
              portable application.  In this case only this command line
              option is considered, all other ways to set a home directory are
              ignored.

              To install GnuPG as a portable application under Windows, create
              an empty file name ‘gpgconf.ctl’ in the same directory as the
              tool ‘gpgconf.exe’.  The root of the  installation  is  than
              that  directory; or, if ‘gpgconf.exe’ has been installed
              directly below a directory named ‘bin’, its parent directory.
              You also need to make sure that the following directories exist
              and are writable: ‘ROOT/home’ for the GnuPG home and
              ‘ROOT/var/cache/gnupg’ for internal cache files.

kyoupark@kit-debian64:~/si/xx/.gnupg$ gpg --homedir /home/kyoupark/si/xx/.gnupg --list-secret-keys
/home/kyoupark/si/xx/.gnupg/secring.gpg
---------------------------------------
sec   2048R/E7204075 2014-09-16
uid                  Snug User (SnugberryPi) <pi@snugupdater>
ssb   2048R/2C335150 2014-09-16

kyoupark@kit-debian64:~/si/xx/.gnupg$ gpg --list-secret-keys
/home/kyoupark/.gnupg/secring.gpg
---------------------------------
sec   2048R/B4A50C4A 2016-12-09
uid                  Kit Park (gpg key) <keiteepark@gmail.com>
ssb   2048R/E0CBDE6B 2016-12-09


<ex>
// to export pub key for a user
$ gpg --armor --export pubring.gpg pi@snugupdater


={============================================================================
*kt_linux_tool_100* tool-chrome

http://www.gamasutra.com/view/news/176420/Indepth_Using_Chrometracing_to_view_your_inline_profiling_data.php
chrome://tracing/


={============================================================================
*kt_linux_tool_100* tool-efence-issues
  
<double-free>
When do double free under pc linux that uses glibc, make a core as below but
run on a embedded linux that uses uclibc, shows no problem.

$ ./a.out 
pstr = 
this is..
*** glibc detected *** ./a.out: double free or corruption (fasttop): 0x000000001834b010 ***
======= Backtrace: =========
/lib64/libc.so.6[0x393c07230f]
/lib64/libc.so.6(cfree+0x4b)[0x393c07276b]
./a.out[0x40062f]
/lib64/libc.so.6(__libc_start_main+0xf4)[0x393c01d994]
./a.out[0x4004b9]
======= Memory map: ========
00400000-00401000 r-xp 00000000 fd:02 37781700                           /data/home/NDS-UK/parkkt/work/a.out
00600000-00601000 rw-p 00000000 fd:02 37781700                           /data/home/NDS-UK/parkkt/work/a.out
1834b000-1836c000 rw-p 1834b000 00:00 0                                  [heap]
393bc00000-393bc1c000 r-xp 00000000 fd:00 3538948                        /lib64/ld-2.5.so
393be1b000-393be1c000 r--p 0001b000 fd:00 3538948                        /lib64/ld-2.5.so
393be1c000-393be1d000 rw-p 0001c000 fd:00 3538948                        /lib64/ld-2.5.so
393c000000-393c14e000 r-xp 00000000 fd:00 3538955                        /lib64/libc-2.5.so
393c14e000-393c34d000 ---p 0014e000 fd:00 3538955                        /lib64/libc-2.5.so
393c34d000-393c351000 r--p 0014d000 fd:00 3538955                        /lib64/libc-2.5.so
393c351000-393c352000 rw-p 00151000 fd:00 3538955                        /lib64/libc-2.5.so
393c352000-393c357000 rw-p 393c352000 00:00 0 
394b200000-394b20d000 r-xp 00000000 fd:00 3539281                        /lib64/libgcc_s-4.1.2-20080825.so.1
394b20d000-394b40d000 ---p 0000d000 fd:00 3539281                        /lib64/libgcc_s-4.1.2-20080825.so.1
394b40d000-394b40e000 rw-p 0000d000 fd:00 3539281                        /lib64/libgcc_s-4.1.2-20080825.so.1
2ba0f68d2000-2ba0f68d4000 rw-p 2ba0f68d2000 00:00 0 
2ba0f68ec000-2ba0f68ee000 rw-p 2ba0f68ec000 00:00 0 
7fff1e6e4000-7fff1e6f9000 rw-p 7ffffffe9000 00:00 0                      [stack]
ffffffffff600000-ffffffffffe00000 ---p 00000000 00:00 0                  [vdso]
Aborted


// from man page

The free() function frees the memory space pointed to by ptr, which must have
  been returned by a previous call to malloc(), calloc() or realloc().
  Otherwise, or if free(ptr) has already been called before, 'undefined'
  behavior occurs. If ptr is NULL, no operation is performed.


={============================================================================
*kt_linux_tool_100* tool-efence-benchmark

                  | HOF       | RAF   | WAF     | DF          | SOF   | GBOF
                  -------------------------------------------------------
gcc, host         | n/d       | n/d   | n/d     | glibc trace | n/d   | n/d
gcc, target       | n/d       | n/d   | n/d     | n/d         | n/d   | n/d
dmalloc, target   | n/d       | n/d   | n/d     | detected    | n/d   | n/d
asn, host         | d         | d     | d       | d           | d     | d
duma, target      | d         | d     | d       | d           | n/d   | n/d
mudflap, host

heap overflow, read after free, write after free, double free
stack overflow, global buffer overflow


<reference>
https://fossies.org/linux/duma/comparisons/memCheckers.html


={============================================================================
*kt_linux_tool_100* tool-efence

{efence}
http://linux.die.net/man/3/efence

Synopsis

#include <stdlib.h>

void * malloc (size_t size);

void free (void *ptr);

void * realloc (void *ptr, size_t size);

void * calloc (size_t nelem, size_t elsize);

void * memalign (size_t alignment, size_t size);

void * valloc (size_t size);

extern int EF_ALIGNMENT;

extern int EF_PROTECT_BELOW;

extern int EF_PROTECT_FREE;

extern int EF_ALLOW_MALLOC_0;

extern int EF_FILL;

Author

Bruce Perens 

Description

Electric Fence helps you detect two common programming bugs: software that
overruns the boundaries of a malloc() memory allocation, and software that
touches a memory allocation that has been released by free(). Unlike other
malloc() debuggers, Electric Fence will detect read accesses as well as
writes, and it will pinpoint the exact instruction that causes an error. It
has been in use at Pixar since 1987, and at many other sites for years.

Electric Fence uses the `virtual memory hardware` of your computer to place an
inaccessible memory page immediately after (or before, at the user's option)
each memory allocation. When software reads or writes this inaccessible page,
     the hardware issues a segmentation fault, stopping the program at the
     offending instruction. It is then trivial to find the erroneous statement
     using your favorite debugger. In a similar manner, memory that has been
     released by free() is made inaccessible, and any code that touches it
     will get a segmentation fault.

Simply linking your application with libefence.a will allow you to detect
most, but not all, malloc buffer overruns and accesses of free memory. If you
want to be reasonably sure that you've found all bugs of this type, you'll
have to read and understand the rest of this man page. 


={============================================================================
*kt_linux_tool_100* tool-efence-duma

http://duma.sourceforge.net/

CHANGELOG:
==========

2.5.15
  * added alternative locking implementation in sem_inc.c
    using critical sections.
    code from Peter Harris, see
    http://code.google.com/p/electric-fence-win32/
  * added Windows Threading to testmt.c
    added win32-msvc.net project file
  * use of WIN32_SEMAPHORES on Win32-Cygwin in sem_inc.c
      - the locking mechanism for multi-threading,
      with this configuration testmt.c works either
      with pthreads and with the Win32 API
  * CreateSemaphore() now used with maximum count = initial count = 1
  (`2008`-08-03, HA)
  * removed usage of strlen() in strncpy()
  (2009-03-19, HA)
  * PATCH from Andre Landwehr <andrel@cybernoia.de>
    fixes race condition when using preprocessor macro 'delete'
  (2009-04-07, HA)
  * bugfix in strncpy()
    Roman Jokl reported the bug: error check was too rigorous
  (2009-04-11, HA)

Purpose        User dynamic Memory Corruption
Technology     Library
ARCH           ARM, Mips
OS             Linux

DUMA used for finding memory usage errors such as Overflow, Underflow, Memory
used after free, allocator/deallcator mismatch in user program. Below are the
allocator Supported in DUMA(16 type)

DUMA is an open-source library (under GNU General Public License) to detect
buffer overruns and under-runs in C and C++ programs.  This library is a fork
of `Buce Perens Electric Fence` library and adds some new features to it.

<readme>
Title: README
D.U.M.A. - Detect Unintended Memory Access - A Red-Zone memory allocator:


DESCRIPTION:

DUMA helps you detect two common programming bugs:

software that overruns the boundaries of a malloc() memory allocation, and
software that touches a memory allocation that has been released by free().

Unlike other malloc() debuggers, DUMA will `detect read accesses` as well as
writes, and it will pinpoint the exact instruction that causes an error.
It has been in use at Pixar since 1987, and at many other sites for years.

DUMA uses the virtual memory hardware of your computer to place an inaccessible
memory page immediately after (or before, at the user's option) each memory
allocation. When software reads or writes this inaccessible page, the hardware
issues a segmentation fault, stopping the program at the offending instruction.
It is then trivial to find the erroneous statement using your favorite
debugger. In a similar manner, memory that has been released by free() is made
inaccessible, and any code that touches it will get a segmentation fault.

Simply linking your application with libduma.a will allow you to detect most,
but not all, malloc buffer overruns and accesses of free memory. If you want
to be reasonably sure that you've found all bugs of this type, you'll have to
read and understand the rest of this man page.

note: how? use `segmentation fault`


<how-to-use>
CATCHING THE ERRONEOUS LINE:

note: staic

1. Compile your program (with debugging information) without DUMA.
2. Set 'ulimit -c unlimited' to get core files
3. Start your program, choose one of following options
   a) Start your program (linked statically with DUMA)
   b) Start your program with duma.sh <your_program>
4. Wait for a segmentation fault. this should have created a core[.<pid>]
   file. You can get into a debugger f.e. with 'gdb <program> -c <core file>'


<configuration>

GLOBAL AND ENVIRONMENT VARIABLES:

note: two ways to configure; env variable or macro.

You can use the gdb command 'set environment variable value' to set shell
environment variables only for the program you are going to debug. This is
useful especially if you are using the shared DUMA library.

Instead you can call macro function to set some variables.


DUMA_PROTECT_BELOW - DUMA usually places an inaccessible page immediately after
  each memory allocation, so that software that runs past the end of the
  allocation will be detected. Setting DUMA_PROTECT_BELOW to 1 causes DUMA to
  place the inaccessible page before the allocation in the address space, so
  that `under-runs` will be detected instead of over-runs.
  To change this value, set DUMA_PROTECT_BELOW in the shell environment to an
  integer value, or call the macro function DUMA_SET_PROTECT_BELOW() from your
  code.

  note: overrun is default?


// NOT IMPLEMENTED. NEED TO CONSIDER
// DUMA_SKIPCOUNT_INIT - DUMA usually does its initialization with `the first`
//   memory allocation. On some systems this may collide with initialization of
//   pthreads or other libaries and produce a hang. To get DUMA work even in these
//   situations you can control (with this environment variable) after how many
//   allocations the full internal initialization of DUMA is done. `default is 0`

//   note: shall use it? changed source directly

// #ifdef DUMA_SKIPCOUNT_INIT
//     _duma_s.SKIPCOUNT_INIT = 10000;
// #endif

void * malloc(size_t size)
{
  return _duma_malloc(size  DUMA_PARAMS_UK);
}

// _duma_g.allocList is set in _duma_init()
void * _duma_malloc(size_t size  DUMA_PARAMLIST_FL)
{
  if ( _duma_g.allocList == 0 )
    _duma_init();  /* This sets DUMA_ALIGNMENT, DUMA_PROTECT_BELOW, DUMA_FILL, ... */

  return _duma_allocate();
}


DUMA_EXPLICIT_INIT

// Function: _duma_init
//
// _duma_init sets up the memory allocation arena and the run-time
// configuration information. We will call duma_init `unless` DUMA_EXPLICIT_INIT
// is defined at compile time.
//
// See Also: duma_init
//
// _duma_init is full initialization, is called from _duma_malloc() and
// others. so set DUMA_EXPLICIT_INIT and then duma_int() will be external, and
// _duma_init will call it.


DUMA_FILL - When set to a value between 0 and 255, every byte of allocated
  memory is initialized to that value. This can help detect reads of
  uninitialized memory. When set to -1, DUMA does not initialise memory on
  allocation. But some memory is filled with zeroes (the operating system
  default on most systems) and some memory will retain the values written to
  it during its last use.
  Per `default` DUMA will initialise all allocated bytes to 255 (=0xFF).
  To change this value, set DUMA_FILL in the shell environment to an
  integer value, or call the macro function DUMA_SET_FILL() from your
  code.


DUMA_PROTECT_FREE - DUMA usually returns free memory to a pool from which it
  may be re-allocated. If you suspect that a program may be touching free
  memory, set DUMA_PROTECT_FREE shell environment to -1. `This is the default`
  and will cause DUMA not to re-allocate any memory.

  note: such as use after free?

  For programs with many allocations and deallocations this may lead to the
  consumption of the full address space and thus to the failure of malloc().
  To avoid such failures you may limit the amount of protected deallocated
  memory by setting DUMA_PROTECT_FREE to a positive value. This value in kB
  will be the limit for such protected free memory.

  `A value of 0 will disable` protection of freed memory.

  note: shall disable it?  added

#ifdef DUMA_NO_PROTECT_FREE
    _duma_s.PROTECT_FREE = 0;
#endif


DUMA_SUPPRESS_ATEXIT - Set this shell environment variable to non-zero when
  DUMA `should skip` the installation of its exit handler. The exit handler is
  called at the end of the main program and checks for memory leaks, so the
  handler's installation should usually not be suppressed. One reason for
  doing so regardless are some buggy environments, where calls to the standard
  C library's atexit()-function hangs.

  note: shall disable it? do not define DUMA_PREFER_ATEXIT to disable


DUMA_OUTPUT_STDOUT - Set this shell environment variable to non-zero to output
  all DUMA messages to STDOUT. This option is `off by default`

DUMA_OUTPUT_STDERR - Set this shell environment variable to non-zero to output
  all DUMA messages to STDERR. This option is `on by default`

DUMA_OUTPUT_FILE - Set this shell environment variable to a filename where all
  DUMA messages should be written to. This option is `off by default`


<configuration-alignment>

DUMA_ALIGNMENT - This is an integer that specifies the alignment for any memory
  allocations that will be returned by malloc(), calloc(), and realloc().
  The value is `specified in bytes`, thus a value of 4 will cause memory to be
  aligned to 32-bit boundaries unless your system doesn't have a 8-bit
  characters. DUMA_ALIGNMENT is set to the minimum required alignment specific
  to your environment by default. The minimum required alignment is detected by
  `createconf` and stored in the file duma_config.h.

  note:
  `createconf` generates duma_config.h

  no DUMA_MIN_ALIGNMENT env var but it is define used in code and defined in
  duma_config.h

  duma.c:573:  , {   DUMA_MIN_ALIGNMENT
  dumatest.c:308:  DUMA_SET_ALIGNMENT(DUMA_MIN_ALIGNMENT);

  get 1 for mips:
  duma_config_mips.h:194:#define DUMA_MIN_ALIGNMENT 1

  If your program requires that allocations be aligned to 64-bit boundaries
  you'll have to set this value to 8. This is the case when compiling with the
  '-mips2' flag on MIPS-based systems such as those from SGI. For some
  architectures the default is defined to even more - x86_64 uses alignment to
  16 bytes by default.

  DUMA internally uses a smaller value if the requested memory size is smaller
  than the alignment value: the next smaller power of 2 is used.
  Thus allocating blocks smaller than DUMA_ALIGNMENT may result into smaller
  alignments - for example when allocating 3 bytes, they would be aligned to 2
  byte boundary. `This allows better detection of overrun`
  For this reason, you will sometimes want to set `DUMA_ALIGNMENT to 1` (no
  alignment), so that you can detect overruns of less than your CPU's word
  size. Be sure to read the section 'WORD-ALIGNMENT AND OVERRUN DETECTION' in
  this manual page before you try this.

  To change this value, set DUMA_ALIGNMENT in the shell environment to an
  integer value, or call the macro function DUMA_SET_ALIGNMENT() from your
  code.
  You don't need to change this setting, if you just need bigger alignment for
  some special buffers. In this case you may use the function
  memalign(alignment, userSize).


WORD-ALIGNMENT AND OVERRUN DETECTION:

There is a conflict between the alignment restrictions that malloc() operates
under and the debugging strategy used by DUMA. When detecting overruns, DUMA
malloc() allocates two or more virtual memory pages for each allocation. The
last page is made inaccessible in such a way that any read, write, or execute
access will cause a segmentation fault. Then, DUMA malloc() will return an
address such that the first byte after the end of the allocation is on the
inaccessible page. Thus, any overrun of the allocation will cause a
segmentation fault.

It follows that the address returned by malloc() is the address of the
inaccessible page minus the size of the memory allocation. 

note:
After all,

 ===================  inaccessible page
 ===================
 ===================
 01 ----------------  accessible page. return X-size
 -------------------
 -------------------
 01 ================  inaccessible page, X
 ===================
 ===================


Unfortunately, malloc() is required to return word-aligned allocations, since
many CPUs can only access a word when its address is aligned. The conflict
happens when software makes a memory allocation using a size that is not a
multiple of the word size, and expects to do word accesses to that allocation.
The location of the inaccessible page is fixed by hardware at a word-aligned
address. If DUMA malloc() is to return an aligned address, it must increase
the size of the allocation to a multiple of the word size.

In addition, the functions memalign() and valloc() must honor explicit
specifications on the alignment of the memory allocation, and this, as well
can only be implemented by increasing the size of the allocation. 

Thus, there will be situations in which the end of a memory allocation
contains some `padding space, and accesses of that padding space will not` be
detected, even if they are overruns.

DUMA provides the variable DUMA_ALIGNMENT so that the user can control the
default alignment used by malloc(), calloc(), and realloc(). To debug overruns
as small as a single byte, you can set DUMA_ALIGNMENT to one. This will result
in DUMA malloc() returning unaligned addresses for allocations with sizes that
are not a multiple of the word size. 

This is not a problem in most cases, because `compilers must pad` the size of
objects so that alignment restrictions are honored when storing those objects
in arrays. 

The problem surfaces when software allocates odd-sized buffers for objects
that must be word-aligned. One case of this is software that allocates a
buffer to contain a structure and a string, and the string has an odd size
(this example was in a popular TIFF library). If word references are made to
un-aligned buffers, you will see a bus error (SIGBUS) instead of a
segmentation fault. The only way to fix this is to re-write the offending code
to make byte references or not make odd-sized allocations, or to set
DUMA_ALIGNMENT to the word size.

Another example of software incompatible with DUMA_ALIGNMENT < word-size
is the strcmp() function and other string functions on SunOS (and probably
Solaris), which make word-sized accesses to character strings, and may attempt
to access up to three bytes beyond the end of a string. These result in a
segmentation fault (SIGSEGV). The only way around this is to use versions of
the string functions that perform byte references instead of word references.


MEMORY USAGE AND EXECUTION SPEED:

Since DUMA `uses at least two virtual memory pages for each` of its allocations,
it's a terrible memory hog. I've sometimes found it necessary to add a swap
  file using swapon(8) so that the system would have enough virtual memory to
  debug my program. Also, the way we manipulate memory results in various
  cache and translation buffer entries being flushed with each call to malloc
  or free. 

`The end result is that program will be much slower and use more resources`
while you are debugging it with DUMA.


The Linux kernel limits the number of different page mappings `per process`

Have a look for

/proc/sys/vm/max_map_count
f.e. under
http://www.redhat.com/docs/manuals/enterprise/RHEL-4-Manual/en-US/Reference_Guide/s3-proc-sys-vm.html
You may have to increase this value to allow debugging with DUMA with a
command like:

*tool-sysctl*
sudo sysctl -w vm.max_map_count=1000000
sysctl -w vm.max_map_count=1000000

note: From mips platform
-sh-3.2# cat /proc/sys/vm/max_map_count
65536

Don't leave libduma.a linked into production software! Use it only for
debugging. See section 'COMPILATION NOTES FOR RELEASE/PRODUCTION' below.


MEMORY LEAK DETECTION:

All memory allocation is protocoled from DUMA together with the filename and
linenumber of the calling function. The atexit() function checks if each
allocated memory block was freed. To disable leak detection add the
`preprocessor definition` 'DUMA_SO_NO_LEAKDETECTION' or
'DUMA_LIB_NO_LEAKDETECTION' to DUMA_OPTIONS in Makefile.

note:
If set 'DUMA_LIB_NO_LEAKDETECTION' then `creatconf` will set
`DUMA_NO_LEAKDETECTION` in duma_config.h This means that use the same makefile
to build `creatconf`

`DUMA_NO_LEAKDETECTION` has to be set. Otherwise, will not use duma calls in
duma.h

note:
This is a tip when know the exact address

If a leak is reported without source filename and line number but is
reproducible with the same pointer, set a conditional breakpoint on the
function 'void * duma_alloc_return( void * address)' f.e. with gdb command
'break duma_alloc_return if address==0x123'


C++ MEMORY OPERATORS AND LEAK DETECTION:

Macros for "new" and "delete" are defined in dumapp.h. These macros give
filename and linenumber of the calling functions to DUMA, thus allowing the
same leak detection reports as for malloc and free. 'dumapp.h' needs to be
included from your source file(s).

For disabling the C++ new/delete/new[] and delete[] operators, add the
`preprocessor definition DUMA_NO_CPP_SUPPORT` to DUMA_OPTIONS in Makefile.

note: shall disable


<configuration-for-mips>
// duma_config.h for using in clien

// for not directing new/delete to malloc/free
#define DUMA_NO_CPP_SUPPORT
#define DUMA_LIB_PREFER_GETENV
#define DUMA_LIB_NO_HANG_MSG
#define DUMA_NO_GLOBAL_MALLOC_FREE
#define DUMA_EXPLICIT_INIT
#define DUMA_OLD_NEW_MACRO
#define DUMA_OLD_DEL_MACRO
#define DUMA_NO_STRERROR
#define DUMA_SKIPCOUNT_INIT
#define DUMA_NO_PROTECT_FREE

// GNUmakefile for building a lib
# edit following line (for createconf)
DUMA_OPTIONS =-DUMA_NO_CPP_SUPPORT
# DUMA_OPTIONS += -DUMA_PREFER_ATEXIT
DUMA_OPTIONS += -DUMA_LIB_PREFER_GETENV
DUMA_OPTIONS += -DUMA_LIB_NO_HANG_MSG
DUMA_OPTIONS += -DUMA_NO_GLOBAL_MALLOC_FREE
DUMA_OPTIONS += -DUMA_EXPLICIT_INIT
# DUMA_OPTIONS += -DUMA_NO_THREAD_SAFETY
DUMA_OPTIONS += -DUMA_OLD_NEW_MACRO
DUMA_OPTIONS += -DUMA_OLD_DEL_MACRO
DUMA_OPTIONS += -DUMA_NO_STRERROR
DUMA_OPTIONS += -DUMA_SKIPCOUNT_INIT
DUMA_OPTIONS += -DUMA_NO_PROTECT_FREE


<readme-files>

SubDirectories:
---------------

// note: removed
// win32-vide/*    project files for VIDE 1.24 (see http://www.objectcentral.com)
//                 using the Borland C++ Builder 5.5 compiler
//                 (FreeCommandLineTools, see http://www.borland.com)
// win32-devcpp/*  project files for Dev-C++ 4.9.6 (see http://www.bloodshed.net)
//                 using the gcc compiler (see http://gcc.gnu.org)
// win32-msvc/*    projects files for Microsoft Visual C++ 6.0 IDE/compiler
//                 (see http://www.microsoft.com)
// debian/*        don't know; maybe some files for the Debian Linux distribution?
// detours/        microsoft detours related
// kduma/          This is the begining of a linux kernel model duma.


Projects:
---------
dumalib         DUMA library. this library should be linked with YOUR program
`dumatest`        first small test program
`tstheap`         second small test program

Files:
------
COPYING-*       License files; reade carefully!
README          this text file
CHANGES         text file listing done CHANGES

duma.h          belongs to dumalib
                this header file should be included from within YOUR C source
// dumapp.h        belongs to dumalib
//                 this header file should be included from within YOUR C++ source
duma.c          belongs to dumalib
                contains malloc/free/.. functions
// dumapp.cpp      belongs to dumalib
//                 contains C++ new/delete/.. functions redirecting them
//                   to ANSI C malloc/free

// note: not exist in souce
// page.c          belongs to dumalib
//                 library internal source file: contains paging functions

print.c         belongs to dumalib; library internal source file: contains
                  printing/aborting functions

dumatest.c      belongs to dumatest
                small test program; checks wether dumalib's paging does its job
                `should work without any errors`

tstheap.c       belongs to tstheap
                small test program; checks wether dumalib's heap does its job
                should report many memory leaks after execution.
Makefile        Makefile for UNIX/Linux/..
// duma.3          source for UNIX man page
// duma.sh         script for UNIX/Linux to start other programs using the
//                   LD_PRELOAD mechanism


<features>

* "overloads" all standard memory allocation functions like malloc(),
  calloc(), memalign(), strdup(), operator new, operator new[] and also their
    counterpart deallocation functions like free(), operator delete and
    operator delete[]

malloc
calloc
free
memalign
posix_mem
realloc
valloc
strdup
memcpy
strcpy
strncpy
strcat
strncat
strndup
vasprintf
asprintf


* utilizes the MMU (memory management unit) of the CPU:

* allocates and protects an extra memory page to detect any illegal access
  beyond the top of the buffer (or bottom, `at the user's option`)

* stops the program at exactly that instruction, which does the erroneous
  access to the protected memory page, allowing location of the defectice
  source code in a debugger

* detects erroneous writes at the non-protected end of the memory block at
  deallocation of the memory block

* detects mismatch of allocation/deallocation functions: f.e. allocation with
  malloc() but deallocation with operator delete

* leak detection: detect memory blocks which were not deallocated until
  program exit

* runs on Linux / U*ix and MS Windows NT/2K/XP operating systems

* preloading of the library on Linux (and some U*ix) systems allowing tests
  without necessity of changing source code or recompilation


<source>
https://sourceforge.net/projects/duma/?source=typ_redirect


<ex>
$ ./out_duma_host 1
DUMA 2.5.15 (static library)
Copyright (C) 2006 Michael Eddington <meddington@gmail.com>
Copyright (C) 2002-2008 Hayati Ayguen <h_ayguen@web.de>, Procitec GmbH
Copyright (C) 1987-1999 Bruce Perens <bruce@perens.com>


=== DUMA: KIT: _duma_allocate
testmain: argv[1]
=== test read after free

=== DUMA: KIT: _duma_allocate
Segmentation fault (core dumped)


<global-malloc>
When use #define DUMA_NO_GLOBAL_MALLOC_FREE, define _duma_xxx ones and use
preprocessor scheme.

0000337c T _duma_free
00004068 D _duma_g
000013f0 T _duma_init
00000750 t _duma_init_slack
00003204 T _duma_malloc

// duma.h
      #define malloc(SIZE)                _duma_malloc(SIZE, __FILE__, __LINE__)
      #define calloc(ELEMCOUNT, ELEMSIZE) _duma_calloc(ELEMCOUNT, ELEMSIZE, __FILE__, __LINE__)

Made a binary statically linked with libduma.a, saw the module(c file) calls
the duma. However, not all the rest in the binary since other modules are not
compiled with duma preprocessor on. How to address this?

1. Find every single c files to build a binary and make sure that includes
duma header and macro on?

1. Find global header of malloc families and repalce it with duma one? But
this make all binary(process) to use duma but not only one binary.


When NOT use #define DUMA_NO_GLOBAL_MALLOC_FREE, this define malloc families
in the libary.

#include <stdio.h>
#include <stdlib.h>

int main(int argc, char **argv)
{
    int *x = malloc(10*sizeof(int));
    free(x);
    return x[5];
}

$ nm libduma.a.without.global.malloc | grep malloc
35:00003204 T _duma_malloc

$ nm out_duma | grep malloc
35:         U malloc

So not use duma. However, when use global mallocs,

$ nm libduma.a | grep malloc
35:00003268 T _duma_malloc
60:00004108 T malloc

$ nm out_duma | grep malloc
55:00403f58 T _duma_malloc
103:00404df8 T malloc


={============================================================================
*kt_linux_tool_100* tool-efence-dmalloc

note:
No longer maintained

Dmalloc Releases

5.5.2 	20070514

http://dmalloc.com/docs/latest/online/dmalloc_3.html#SEC3

* build `libdmallocth.a' which is the threaded version of the library.

2.5 How the Library Checks Your Program 

* The dmalloc library replaces the heap library calls normally found in your
  system libraries with its own versions.

In addition to per-pointer checks, you can configure the library to perform
`complete heap checks` These complete checks verify all internal heap
structures and include walking all of the known allocated pointers to verify
each one in turn. You need this level of checking to find random pointers in
your program which got corrupted but that won't be freed for a while. To turn
on these checks, you will need to enable the `check-heap` debug token. See
section Description of the `Debugging Tokens` By default this will cause the
heap to be fully checked each and every time dmalloc is called whether it is a
malloc, free, realloc, or another dmalloc overloaded function. 


Performing a full heap check can take a good bit of CPU and it may be that you
will want to run it sporadically.


3.5 Additional Non-standard Routines 

Function: void dmalloc_debug_setup ( const char * options_str )

    This routine sets the global debugging functionality as an option string.
    Normally this would be passed in in the DMALLOC_OPTIONS environmental
    variable. This is here to override the env or for circumstances where
    modifying the environment is not possible or does not apply such as
    servers or cgi-bin programs.


Function: int dmalloc_verify ( char * pnt )

    This function verifies individual memory pointers that are suspect of
    memory problems. To check the `entire heap` pass in a NULL or 0 pointer. The
    routine returns DMALLOC_VERIFY_ERROR or DMALLOC_VERIFY_NOERROR.

    NOTE: `dmalloc_verify()' can only check the heap with the functions that
    have been enabled. For example, if fence-post checking is not enabled,
`dmalloc_verify()' cannot check the fence-post areas in the heap. 


// dmalloc.h

#define malloc(size) \
  dmalloc_malloc(__FILE__, __LINE__, (size), DMALLOC_FUNC_MALLOC, 0, 0)


<dmalloc-build>

1. change app_process_main.c

/* Token 12 */
dmalloc_debug_setup("log-stats,log-non-free,check-fence,error-free-null,print-messages");

pagesize = dmalloc_page_size();
printf("APP_Process dmalloc pagesize = %d system pagesize = %d\n", pagesize, getpagesize());
printf("APP_Process size of long = %d\n", sizeof(long));


2. change makefile to link with dmalloc lib for each process

/build/processes/APP_Process/makefile

COMPONENT_SOURCE_LIBRARIES+=\
	$(NDS_ROOT)/libdmallocth.a
COMPONENT_PREBUILT_OBJECTS+=\
	$(NDS_ROOT)/libdmallocth.a


3. enable console
note: careful since it's different from release to release

/build/make_image.sh

  SPK_CONFIG+=" console"


4. copy two files

vobs/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/dmalloc.h
vobs/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/libdmallocth.a


25:08:16 15:46:17 946684826: 593: WARNING: tried to free(0) from 'unknown' 
25:08:16 15:46:17 946684826: 593:   error details: invalid 0L pointer 
25:08:16 15:46:17 946684826: 593:   from 'unknown' prev access 'unknown' 
25:08:16 15:46:17 946684826: 593: ERROR: free: pointer is null (err 20) 
25:08:16 15:46:17 946684826: 600: WARNING: tried to free(0) from 'unknown' 
25:08:16 15:46:17 946684826: 600:   error details: invalid 0L pointer 
25:08:16 15:46:17 946684826: 600:   from 'unknown' prev access 'unknown' 
25:08:16 15:46:17 946684826: 600: ERROR: free: pointer is null (err 20) 
25:08:16 15:46:17 946684826: 607: WARNING: tried to free(0) from 'unknown' 
25:08:16 15:46:17 946684826: 607:   error details: invalid 0L pointer 
25:08:16 15:46:17 946684826: 607:   from 'unknown' prev access 'unknown' 
25:08:16 15:46:17 946684826: 607: ERROR: free: pointer is null (err 20) 
25:08:16 15:46:17 946684826: 628: WARNING: tried to free(0) from 'unknown' 
25:08:16 15:46:17 946684826: 628:   error details: invalid 0L pointer 
25:08:16 15:46:17 946684826: 628:   from 'unknown' prev access 'unknown' 
25:08:16 15:46:17 946684826: 628: ERROR: free: pointer is null (err 20) 


// 

int func_double_free()
{
    int *x = malloc(10*sizeof(int));
    free(x);
    free(x);
    return 1;
}

=== test double free
946784562: 10:   error details: finding address in heap
946784562: 10:   pointer '0x483f88' from 'testmain.c:32' prev access 'unknown'
946784562: 10: ERROR: free: tried to free previously freed pointer (err 61) ~
946784562: 10: Dumping Chunk Statistics:
946784562: 10: basic-block 4096 bytes, alignment 8 bytes
946784562: 10: heap address range: 0x47d000 to 0x487000, 40960 bytes
946784562: 10:     user blocks: 2 blocks, 8000 bytes (19%)
946784562: 10:    admin blocks: 8 blocks, 32768 bytes (80%)
946784562: 10:    total blocks: 10 blocks, 40960 bytes
946784562: 10: heap checked 0
946784562: 10: alloc calls: malloc 4, calloc 0, realloc 0, free 5
946784562: 10: alloc calls: recalloc 0, memalign 0, valloc 0
946784562: 10: alloc calls: new 0, delete 0
946784562: 10:   current memory in use: 320 bytes (1 pnts)
946784562: 10:  total memory allocated: 800 bytes (4 pnts)
946784562: 10:  max in use at one time: 720 bytes (2 pnts)
946784562: 10: max alloced with 1 call: 400 bytes
946784562: 10: max unused memory space: 304 bytes (29%)
946784562: 10: top 10 allocations:
946784562: 10:  total-size  count in-use-size  count  source
946784562: 10:         400      1           0      0  testmain.c:14
946784562: 10:          40      1           0      0  testmain.c:23
946784562: 10:          40      1           0      0  testmain.c:30
946784562: 10:         480      3           0      0  Total of 3
946784562: 10: Dumping Not-Freed Pointers Changed Since Start:
946784562: 10:  not freed: '0x47de00|s1' (320 bytes) from 'unknown'
946784562: 10:  total-size  count  source
946784562: 10:           0      0  Total of 0
946784562: 10: ending time = 946784562, elapsed since start = 0:00:00


// when ends without detecting an error

946784741: 7: Dumping Chunk Statistics:
946784741: 7: basic-block 4096 bytes, alignment 8 bytes
946784741: 7: heap address range: 0x47d000 to 0x487000, 40960 bytes
946784741: 7:     user blocks: 2 blocks, 8000 bytes (19%)
946784741: 7:    admin blocks: 8 blocks, 32768 bytes (80%)
946784741: 7:    total blocks: 10 blocks, 40960 bytes
946784741: 7: heap checked 0
946784741: 7: alloc calls: malloc 3, calloc 0, realloc 0, free 3
946784741: 7: alloc calls: recalloc 0, memalign 0, valloc 0
946784741: 7: alloc calls: new 0, delete 0
946784741: 7:   current memory in use: 320 bytes (1 pnts)
946784741: 7:  total memory allocated: 760 bytes (3 pnts)
946784741: 7:  max in use at one time: 720 bytes (2 pnts)
946784741: 7: max alloced with 1 call: 400 bytes
946784741: 7: max unused memory space: 304 bytes (29%)
946784741: 7: top 10 allocations:
946784741: 7:  total-size  count in-use-size  count  source
946784741: 7:         400      1           0      0  testmain.c:14
946784741: 7:          40      1           0      0  testmain.c:23
946784741: 7:         440      2           0      0  Total of 2
946784741: 7: Dumping Not-Freed Pointers Changed Since Start:
946784741: 7:  not freed: '0x47de00|s1' (320 bytes) from 'unknown'
946784741: 7:  total-size  count  source
946784741: 7:           0      0  Total of 0
946784741: 7: ending time = 946784741, elapsed since start = 0:00:00


={============================================================================
*kt_linux_tool_100* tool-efence-dml

{DML}
Purpose        User dynamic Memory Accounting
Technology     Library
ARCH           ARM, Mips
OS             Linux
Description

DML is used for memory accounting and find out memory 'leaks' in user program.
Below are the allocator Supported in DML(17 type)

memalign
valloc
posix_memalign
asprintf
strndup
strdup
wcsdup
malloc
realloc
calloc
free
operator new[]
operator new
operator delete
operator delete[]
prctl
pthread_create


{KDEBUGD}
Purpose        User tracer, debugger, and profiler
Technology     Kernel hooking
ARCH           ARM, Mips
OS             Linux
Description

Kdebugd is kernel mode debuger, tracer and profiler for user space programs
below are the Major feature provided by Kdebugd.

1- Find User program statistics (Maps/Stack and Register)
2- Run time Kernel and User backtrace
3- Resource Monitoring
4- Lock profiler
5- User program Profiling


{FTRACE}
Purpose        Kernel tracer
Technology     Tracepoint
ARCH           ARM, Mips
OS             Linux
Description

Ftrace is kernel mode tracer for kernel threads


{MEMPS} 
note: not open source
A 'Memps' tool is developed to show memory accounting information for the
complete system and individual processes through one common interface.

This tool combines various open source proc interface output and combined them
together to show all useful information at single place.

Various proc interfaces internally called are as follows:
a./proc/meminfo
b./proc/pid/status
c./proc/pid/cmdline
d./proc/mounts
e./proc/pid/smaps
f./proc/sys/vm/drop_caches

sh-4.1# ./memps --help

Usage: ./memps [OPTION]...

Options:
  -o, --output=FILE
  -p, --pid=PID
  -v, --verbose
  -t, --tmpfs
  -g, --geminfo
  -m, --maliinfo
  -r, --dropcaches
  -d, --description
  -c, --color
  -h, --help

If run without any argument output of memps utility would show
a.
PID     CODE     DATA     PEAK      PSS DEV(PSS) COMMAND
Output for all running processes in the system

b.
MemTotal       MemFree              MemUsed*       Buffers        Cached      SwapCached
Active         Inactive             Active(anon)   Inactive(anon)
Active(file)   Inactive(file)       SwapTotal  SwapFree    AnonPages
Free*          Used*             GemTotal    MaliTotal

Output for complete system, along with information for Graphics driver and Mali drivers.

sh-4.1#> ./memps
     PID     CODE     DATA     PEAK      PSS DEV(PSS) COMMAND
       1      292       36      328      200        0 init
      73      404       80      484      356        0 -/bin/sh
     211      556       76      632      632        0 ./memps

  TOTAL:     CODE     DATA               PSS DEV(PSS)
            1,252      192             1,188        0

MemTotal:  1181 MB ( 1,209,684 kB)
MemFree:  1160 MB ( 1,188,232 kB)
MemUsed*:    20 MB (    21,452 kB)
Buffers:     2 MB (     2,472 kB)
Cached:     3 MB (     3,976 kB)
SwapCached:     0 MB (         0 kB)
Active:     1 MB (     1,936 kB)
Inactive:     4 MB (     4,668 kB)
Active(anon):     0 MB (       164 kB)
Inactive(anon):     0 MB (         4 kB)
Active(file):     1 MB (     1,772 kB)
Inactive(file):     4 MB (     4,664 kB)
SwapTotal:     0 MB (         0 kB)
SwapFree:     0 MB (         0 kB)
AnonPages:     0 MB (       156 kB)
Free*:  1166 MB ( 1,194,668 kB)
Used*:    14 MB (    15,016 kB)
GemTotal:     0 MB (         0 kB)
(Contiguous:           0 kB)
(Non-contiguous:       0 kB)
MaliTotal:     0 MB (         0 kB)


={============================================================================
*kt_linux_tool_100* tool-efence-fortify

https://android-developers.googleblog.com/2017/04/fortify-in-android.html


={============================================================================
*kt_linux_tool_100* tool-asan tool-efence cpp-memory-issue

AddressSanitizer is a tool that detects `memory corruption bugs` such as
buffer overflows or accesses to a dangling pointer (use-after-free). 


Applications are implemented in `unmanaged` programming languages (C and C++)
which do not provide any protection against invalid memory accesses. Such
accesses often result in memory corruption and eventually cause program
crashes or other abnormal behavior. AddressSanitizer (or ASan for short) is a
part of Google toolsuite for program quality assurance (the other tools being
    ThreadSanitizer, MemorySanitizer and UBSanitizer). 

https://github.com/google/sanitizers
https://github.com/google/sanitizers/wiki/AddressSanitizer

Current AddressSanitizer handles the following classes of errors (see
http://code.google.com/p/address-sanitizer/wiki/AddressSanitizer#Introduction):

- use after free
- buffer overruns and wild pointers

This is not guaranteed to always work for static and stack variables. Runtime
will only be able to detect overrun (or wild pointer access) if it falls into
one of the redzones which separate objects on stack or in global memory. If
buffer offset is too large it'll be able to overcome the redzone and fool the
checker. ASan developers report that >95% of bugs are detectable with current
redzone sizes.

- use after return / use after end-of-block


<pros>
This tool is very fast. The average slowdown of the instrumented program is
~2x (see AddressSanitizerPerformanceNumbers).

Actively evolving (in contrast to mudflap which has been recently removed from
    Gcc)


<design> AddressSanitizer Design
AddressSanitizer is supported by two major open-source toolchains: LLVM and
GCC (LLVM being the main development platform). The tool introduces a new
compiler pass which instruments the generated memory operations by inserting
instructions to perform memory checks.

Here's the bird's eye view of the performed instrumentation:

// Before
*address = ...;  // or: ... = *address;

// After
if (IsPoisoned(address)) {
  ReportError(address, kAccessSize, kIsWrite);
}
*address = ...;  // or: ... = *address;

The IsPoisoned check is implemented in such a way so as to minimize overhead
in common case (i.e.  when memory access is valid). It works by mapping the
memory address in question to a bit in a special memory region called
<shadow-memory>:

byte *shadow_address = MemToShadow(address);
byte shadow_value = *shadow_address;
if (shadow_value) {
  if (SlowPathCheck(shadow_value, address, kAccessSize)) {
    ReportError(address, kAccessSize, kIsWrite);
  }
}

// Check the cases where we access first k bytes of the qword
// and these k bytes are unpoisoned.
bool SlowPathCheck(shadow_value, address, kAccessSize) {
  last_accessed_byte = (address & 7) + kAccessSize - 1;
  return last_accessed_byte >= shadow_value);
}

inline byte *MemToShadow(byte *address) {
  return (address >> 3) | ShadowStartOffset;   // ShadowStartOffset depends on the target arch
}

The shadow memory region is invisible to the program and occupies 1/8 of
address space. It's initialized in the following way:

  bits that correspond to static variables are marked as unpoisoned

  whenever new stack frame is allocated, addresses which correspond to entries
  are marked as unpoisoned; upon frame termination bits are reset Stack and
  static objects are interspersed with redzones (short poisoned memory blocks)
  to allow for catching of buffer overruns.

To allow checking of heap-allocated memory standard memory routines are
overridden by ASan runtime library (libasan.so) to check/change poisonness of
memory regions. For example malloc changes the status of allocated memory
buffer to unpoisoned while free re-poisons it. Here is the full list of
instrumented functions:

// signal.h
sigaction
signal
 
// setjmp.h
longjmp
_longjmp
siglongjmp
__cxa_throw
 

// string.h
memcmp
memmove
memcpy
memset
strchr
strcat
strncat
strcpy
strncpy
strcmp
strncmp
strlen
strcasecmp
strncasecmp
strdup
strnlen
index

// stdlib.h
atoi
atol
strtol
atoll
strtoll
read
pread
pread64
write
pwrite
malloc
free
realloc
calloc
valloc
posix_memalign
 
// pthread.h
pthread_create

// time.h
localtime
localtime_r
gmtime
gmtime_r
ctime
ctime_r
asctime
asctime_r
vscanf
vsscanf
vfscanf
scanf
fscanf
sscanf
 
They also instrument builtin compiler intrinsic e.g. __builtin_memcpy, etc.


={============================================================================
*kt_linux_tool_100* tool-asan-flag tool-asan-code

https://github.com/google/sanitizers/wiki/AddressSanitizerFlags

<x>
Clang 3.1 and 3.2 supported __attribute__((no_address_safety_analysis)) instead.

You may also ignore certain functions using a blacklist: create a file
my_ignores.txt and pass it to AddressSanitizer at compile time using
-fsanitize-blacklist=my_ignores.txt (This flag is new and is only supported by
    Clang now): 
  

<tool-asan-recover>
Q: Can AddressSanitizer continue running after reporting first error?

A: Yes it can, AddressSanitizer has recently got continue-after-error mode.
This is somewhat experimental so may not yet be as reliable as default setting
(and not as timely supported). Also keep in mind that errors after the first
one may actually be spurious. 

To enable continue-after-error, compile with -fsanitize-recover=address and
then run your code with ASAN_OPTIONS=halt_on_error=0.

// 494
// i686-nptl-linux-gnu-gcc: error: unrecognized command line option ‘-fsanitize-recover=address’


https://gcc.gnu.org/onlinedocs/gcc-6.4.0/gcc/Instrumentation-Options.html#Instrumentation-Options

-fsanitize-recover[=opts]
-fsanitize-recover= controls error recovery mode for sanitizers mentioned in comma-separated list of opts. Enabling this option for a sanitizer component causes it to attempt to continue running the program as if no error happened. This means multiple runtime errors can be reported in a single program run, and the exit code of the program may indicate success even when errors have been reported. The -fno-sanitize-recover= option can be used to alter this behavior: only the first detected error is reported and program then exits with a non-zero exit code.

Currently this feature only works for -fsanitize=undefined (and its suboptions
    except for -fsanitize=unreachable and -fsanitize=return),
  -fsanitize=float-cast-overflow, -fsanitize=float-divide-by-zero,
  -fsanitize=kernel-address and -fsanitize=address. 

For these sanitizers error recovery is turned on by default, 
`except -fsanitize=address`, for which this feature is experimental.

-fsanitize-recover=all and -fno-sanitize-recover=all is also accepted, the
former enables recovery for all sanitizers that support it, the latter
disables recovery for all sanitizers that support it.

Syntax without explicit opts parameter is deprecated. It is equivalent to

-fsanitize-recover=undefined,float-cast-overflow,float-divide-by-zero
Similarly -fno-sanitize-recover is equivalent to

-fno-sanitize-recover=undefined,float-cast-overflow,float-divide-by-zero


<x>
To get the idea of what's supported in your version, run

// only works for GCC 6.3.0
ASAN_OPTIONS=help=1 ./a.out

ASAN_OPTIONS=verbosity=1:malloc_context_size=20 ./a.out

debug	false	
If set, prints some debugging information and does additional checks.

sleep_before_dying	0	
Number of seconds to sleep between printing an error report and terminating
the program. Useful for debugging purposes (e.g. when one needs to attach
    gdb).

halt_on_error	true	
Crash the program after printing the first error report (WARNING: USE AT YOUR
    OWN RISK!)

log_path	stderr	
Write logs to "log_path.pid". The special values are "stdout" and "stderr".
The default is "stderr".

verbosity	0	
Verbosity level (0 - silent, 1 - a bit of output, 2+ - more output).


<x>
chefmax commented on Nov 20, 2017

@keitee Do you have an ability to rebuild ASan by your own? If so, you can
backport this patch (https://reviews.llvm.org/rL309854) and use
sleep_after_init flag in ASAN_OPTIONS and attach to the process under test.

compiler-rt/trunk/lib/asan/asan_flags.inc

ASAN_FLAG(
    int, sleep_after_init, 0,
    "Number of seconds to sleep after AddressSanitizer is initialized. "
    "Useful for debugging purposes (e.g. when one needs to attach gdb).")

  if (flags()->sleep_after_init) {
    Report("Sleeping for %d second(s)\n", flags()->sleep_after_init);
    SleepForSeconds(flags()->sleep_after_init);
  }


// from runs
==981==Parsed ASAN_OPTIONS: verbosity=2

[root@vSTB flash0]# ./out_uaf_use_6
==972==AddressSanitizer: failed to intercept '__isoc99_printf'
==972==AddressSanitizer: failed to intercept '__isoc99_sprintf'
==972==AddressSanitizer: failed to intercept '__isoc99_snprintf'
==972==AddressSanitizer: failed to intercept '__isoc99_fprintf'
==972==AddressSanitizer: failed to intercept '__isoc99_vprintf'
==972==AddressSanitizer: failed to intercept '__isoc99_vsprintf'
==972==AddressSanitizer: failed to intercept '__isoc99_vsnprintf'
==972==AddressSanitizer: failed to intercept '__isoc99_vfprintf'
==972==AddressSanitizer: libc interceptors initialized
|| `[0x38000000, 0xbfffffff]` || HighMem    ||
|| `[0x27000000, 0x37ffffff]` || HighShadow ||
|| `[0x24000000, 0x26ffffff]` || ShadowGap  ||
|| `[0x20000000, 0x23ffffff]` || LowShadow  ||
|| `[0x00000000, 0x1fffffff]` || LowMem     ||
MemToShadow(shadow): 0x24000000 0x247fffff 0x24e00000 0x26ffffff
redzone=16
max_redzone=2048
quarantine_size_mb=64M
malloc_context_size=30
SHADOW_SCALE: 3
SHADOW_GRANULARITY: 8
SHADOW_OFFSET: 0x20000000
==972==Installed the sigaction for signal 11
==972==Installed the sigaction for signal 7
==972==Installed the sigaction for signal 8
==972==SetCurrentThread: 0xb725c000 for thread 0xb6f55740
==972==T0: stack [0xbf407000,0xbfc07000) size 0x800000; local=0xbfc0559c
==972==AddressSanitizer Init done
====>
====> this is uaf(use after free)..
====>
=================================================================
==972==ERROR: AddressSanitizer: heap-use-after-free on address 0xb5a00fe4 at pc 0x080486f8 bp 0xbfc05598 sp 0xbfc0558c
WRITE of size 4 at 0xb5a00fe4 thread T0
==972==Using libbacktrace symbolizer.
...


// asan_rtl.cc
void __asan_init() {

  // Initialize flags. This must be done early, because most of the
  // initialization steps look at flags().
  const char *options = GetEnv("ASAN_OPTIONS");
  InitializeFlags(flags(), options);

}

static void AsanInitInternal() {
  VReport(1, "AddressSanitizer Init done\n");
}


<asan-shadow>

Every aligned 8-byte word of memory have only 9 states:
first k (0<=k<=8) bytes are addressable, the rest are not.
State of every 8-byte word can be encoded in 1 byte
(shadow byte)

(Extreme: up to 128 application bytes per 1 shadow byte)

8 bytes access:

char *shadow = MemToShadow(a);

if (*shadow)
  ReportError(a);

*a = 


N bytes access where N=1,2,4:

char *shadow = MemToShadow(a);  
// char *shadow = a >> 3 + offset;

if (*shadow &&
    *shadow <= ((a&7)+N-1))
  ReportError(a);

*a = 

Q: As with cxx-bitset, a&7 means that the original value before scaling down.
Why need this here? Why not this code?

if (*shadow &&
    *shadow <= (N-1)) or *shadow < N:AccessSize)
  ReportError(a);

*a = 


0000  0
0001  1
0010  2
0011  3
0100  4
0101  5
0110  6
0111  7
1000  8

0x200, 10.0000.0000. 10 bytes since first byte is 0 which means 8 bytes and
second bytes is 10 which is 2. So 10 bytes in total.

*asan-shadow-magic*

0xf0, 1111.0000

// These magic values are written to shadow for better error reporting.
const int kAsanHeapLeftRedzoneMagic = 0xfa;
const int kAsanHeapRightRedzoneMagic = 0xfb;
const int kAsanHeapFreeMagic = 0xfd;
const int kAsanStackLeftRedzoneMagic = 0xf1;
const int kAsanStackMidRedzoneMagic = 0xf2;
const int kAsanStackRightRedzoneMagic = 0xf3;
const int kAsanStackPartialRedzoneMagic = 0xf4;
const int kAsanStackAfterReturnMagic = 0xf5;
const int kAsanInitializationOrderMagic = 0xf6;
const int kAsanUserPoisonedMemoryMagic = 0xf7;
const int kAsanContiguousContainerOOBMagic = 0xfc;
const int kAsanStackUseAfterScopeMagic = 0xf8;
const int kAsanGlobalRedzoneMagic = 0xf9;
const int kAsanInternalHeapMagic = 0xfe;

static const uptr kCurrentStackFrameMagic = 0x41B58AB3;
static const uptr kRetiredStackFrameMagic = 0x45E0360E;

; from gcc630
; Shadow byte legend (one shadow byte represents 8 application bytes):
;   Addressable:           00
;   Partially addressable: 01 02 03 04 05 06 07
;   Heap left redzone:       fa
;   Heap right redzone:      fb
;   Freed heap region:       fd
;   Stack left redzone:      f1
;   Stack mid redzone:       f2
;   Stack right redzone:     f3
;   Stack partial redzone:   f4
;   Stack after return:      f5
;   Stack use after scope:   f8
;   Global redzone:          f9
;   Global init order:       f6
;   Poisoned by user:        f7
;   Container overflow:      fc
;   Array cookie:            ac
;   Intra object redzone:    bb
;   ASan internal:           fe
;   Left alloca redzone:     ca
;   Right alloca redzone:    cb

Instrumenting stack

void foo() {
  char a[328];
  // <------------- CODE ------------->
}

Instrumenting stack (fast protocol)

Fast protocol
o Poison redzones at function entry
o Unpoison redzones at function exit (must happen)
o Assume the rest of the stack frame is unpoisoned
o + Fast: O(number of locals) instructions
o - Tricky when exceptions or longjmp are present
o - Small probability of finding use-after-return

void foo() {
  char rz1[32]; // 32-byte aligned
  char a[328];
  char rz2[24];
  char rz3[32];
  int *shadow = (&rz1 >> 3) + kOffset;
  shadow[0]  = 0xffffffff;  // poison rz1, represent 4*8 = 32 bytes
  // shadow[1,10], 4*8*10 = 320 bytes
  shadow[11] = 0xffffff00;  // poison rz2, 1*8 = 8 bytes
  shadow[12] = 0xffffffff;  // poison rz3
  // <------------- CODE ------------->
  shadow[0] = shadow[11] = shadow[12] = 0;
}

Instrumenting stack (slow protocol)
o Poison redzones and unpoison locals at function entry
o Poison the entire frame at function exit (optional)
o + Friendly to exceptions and longjmp
o + Better for use-after-return
o - Slower: O(size of the stack frame) instructions

void foo() {
  char rz1[32]; // 32-byte aligned
  char a[328];
  char rz2[24];
  char rz3[32];
  int *shadow = (&rz1 >> 3) + kOffset;
  shadow[0] = 0xffffffff;   // poison rz1
  shadow[1:10] = 0;         // unpoison a
  shadow[11] = 0xffffff00;  // poison rz2
  shadow[12] = 0xffffffff;  // poison rz3
  // <------------- CODE ------------->
  shadow[0:13] = 0xffffffff;
}


<tool-asan-syscall>

[ALL  ]    libtool: compile:  /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/./gcc/xgcc -shared-libgcc -B/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/./gcc -nostdinc++ -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/src -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/src/.libs -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/libsupc++/.libs -B/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/bin/ -B/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/lib/ -isystem /home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/include -isystem /home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sys-include -D_GNU_SOURCE -D_DEBUG -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -DHAVE_RPC_XDR_H=1 -DHAVE_TIRPC_RPC_XDR_H=0 -I. -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/sanitizer_common -I.. -I /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/include -isystem /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/include/system -Wall -W -Wno-unused-parameter -Wwrite-strings -pedantic -Wno-long-long -fPIC -fno-builtin -fno-exceptions -fno-rtti -fomit-frame-pointer -funwind-tables -fvisibility=hidden -Wno-variadic-macros -I../../libstdc++-v3/include -I../../libstdc++-v3/include/i686-nptl-linux-gnu -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/../libstdc++-v3/libsupc++ -std=gnu++11 -DSANITIZER_LIBBACKTRACE -DSANITIZER_CP_DEMANGLE -I /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/../libbacktrace -I ../libbacktrace -I /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/../include -include /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/libbacktrace/backtrace-rename.h -march=i686 -D_GNU_SOURCE -g -Os -MT sanitizer_linux.lo -MD -MP -MF .deps/sanitizer_linux.Tpo -c /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/sanitizer_common/sanitizer_linux.cc  -fPIC -DPIC -o .libs/sanitizer_linux.o


[ALL  ]    libtool: compile:
/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/./gcc/xgcc
-shared-libgcc
-B/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/./gcc
-nostdinc++
-L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/src
-L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/src/.libs
-L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/libsupc++/.libs
-B/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/bin/
-B/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/lib/ -isystem
/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/include
-isystem
/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sys-include
-D_GNU_SOURCE -D_DEBUG -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS
-D__STDC_LIMIT_MACROS -DHAVE_RPC_XDR_H=1 -DHAVE_TIRPC_RPC_XDR_H=0 -I.
-I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/sanitizer_common
-I.. -I
/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/include
-isystem
/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/include/system
-Wall -W -Wno-unused-parameter -Wwrite-strings -pedantic -Wno-long-long -fPIC
-fno-builtin -fno-exceptions -fno-rtti -fomit-frame-pointer -funwind-tables
-fvisibility=hidden -Wno-variadic-macros -I../../libstdc++-v3/include
-I../../libstdc++-v3/include/i686-nptl-linux-gnu
-I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/../libstdc++-v3/libsupc++
-std=gnu++11 -DSANITIZER_LIBBACKTRACE -DSANITIZER_CP_DEMANGLE -I
/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/../libbacktrace
-I ../libbacktrace -I
/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/../include
-include
/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/libbacktrace/backtrace-rename.h
-march=i686 -D_GNU_SOURCE -g -Os -MT sanitizer_linux.lo -MD -MP -MF
.deps/sanitizer_linux.Tpo -c
/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/sanitizer_common/sanitizer_linux.cc
-fPIC -DPIC -o .libs/sanitizer_linux.o

// to check cpp output
/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/./gcc/xgcc -shared-libgcc -B/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/./gcc -nostdinc++ -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/src -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/src/.libs -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/libsupc++/.libs -B/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/bin/ -B/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/lib/ -isystem /home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/include -isystem /home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sys-include -D_GNU_SOURCE -D_DEBUG -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -DHAVE_RPC_XDR_H=1 -DHAVE_TIRPC_RPC_XDR_H=0 -I. -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/sanitizer_common -I.. -I /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/include -isystem /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/include/system -Wall -W -Wno-unused-parameter -Wwrite-strings -pedantic -Wno-long-long -fPIC -fno-builtin -fno-exceptions -fno-rtti -fomit-frame-pointer -funwind-tables -fvisibility=hidden -Wno-variadic-macros -I../../libstdc++-v3/include -I../../libstdc++-v3/include/i686-nptl-linux-gnu -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/../libstdc++-v3/libsupc++ -std=gnu++11 -DSANITIZER_LIBBACKTRACE -DSANITIZER_CP_DEMANGLE -I /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/../libbacktrace -I ../libbacktrace -I /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/../include -include /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/libbacktrace/backtrace-rename.h -march=i686 -D_GNU_SOURCE -g -Os -MT sanitizer_linux.lo -MD -MP -MF sanitizer_linux.Tpo -c /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/sanitizer_common/sanitizer_linux.cc  -fPIC -DPIC -E


<tool-asan-syscall>

// sanitizer_common/sanitizer_libc.h
//
// These tools can not use some of the libc functions directly because those
// functions are intercepted. Instead, we implement a tiny subset of libc here.
// FIXME: Some of functions declared in this file are in fact POSIX, not libc.


<tool-asan-check>

Like assert(), CHECK(x) expect x is true.
  
if("Address is not in memory and not in shadow?")
becomes true
    
CHECK("Address is not in memory and not in shadow?");
However, this do not work since cannot cast to const char* to unsigned int
  
if( (0 && "Address is not in memory and not in shadow?"))
becomes false
   
CHECK("unable to unmap" && 0);

Sanitizer CHECK failed: cxx.cpp:1322 ((0 && "Address is not in memory and not in shadow?")) != (0) (0, 0)
Sanitizer CHECK failed: cxx.cpp:1325 ((100 != 100)) != (0) (0, 0)
  
The reason to use this "CHECK("unable to unmap" && 0);" is that it is always
false and used to when see errors and it calls Die() function 


<tool-asan-report-gcc494>

// asan/asan_rtl.cc
ASAN_REPORT_ERROR(store, true, 4)

// __asan_report_load8
ASAN_REPORT_ERROR(load, false, 8)

// exported functions
#define ASAN_REPORT_ERROR(type, is_write, size)                     \
extern "C" NOINLINE INTERFACE_ATTRIBUTE                        \
void __asan_report_ ## type ## size(uptr addr);                \
void __asan_report_ ## type ## size(uptr addr) {               \
  GET_CALLER_PC_BP_SP;                                              \
  __asan_report_error(pc, bp, sp, addr, is_write, size);            \
}

void __asan_report_load8(uptr addr)
{
  uptr bp = (uptr)__builtin_frame_address(0); 
  uptr pc = (uptr)__builtin_return_address(0); 
  uptr local_stack; 
  uptr sp = (uptr)&local_stack; 
  __asan_report_error(pc, bp, sp, addr, is_write, size);
}

// asan/asan_report.cc
void __asan_report_error(uptr pc, uptr bp, uptr sp, uptr addr, 
    bool is_write, uptr access_size) {

  ScopedInErrorReport in_report;

  // decide access type depending on asan-shadow-magic value
  // set bug_descr
  switch (*shadow_addr) {
  }

  Report("ERROR: AddressSanitizer: %s on address "
             "%p at pc 0x%zx bp 0x%zx sp 0x%zx\n",
             bug_descr, (void*)addr, pc, bp, sp);

  GET_STACK_TRACE_FATAL(pc, bp);
  PrintStack(&stack);

  DescribeAddress(addr, access_size);

  // bool DescribeAddressRelativeToGlobal(uptr addr, uptr size,
  //                                      const __asan_global &g) {
  //   static const uptr kMinimalDistanceFromAnotherGlobal = 64;
  //   if (addr <= g.beg - kMinimalDistanceFromAnotherGlobal) return false;
  //   if (addr >= g.beg + g.size_with_redzone) return false;
  //   Decorator d;
  //   Printf("%s", d.Location());
  //   if (addr < g.beg) {
  //     Printf("[KT:6] %p is located %zd bytes to the left", (void*)addr, g.beg - addr);
  //   } else if (addr + size > g.beg + g.size) {
  //     if (addr < g.beg + g.size)
  //       addr = g.beg + g.size;
  //     Printf("[KT:6] %p is located %zd bytes to the right", (void*)addr,
  //            addr - (g.beg + g.size));
  //   } else {
  //     // Can it happen?
  //     Printf("[KT:6] %p is located %zd bytes inside", (void*)addr, addr - g.beg);
  //   }
  //   Printf(" of global variable '%s' from '%s' (0x%zx) of size %zu\n",
  //              MaybeDemangleGlobalName(g.name), g.module_name, g.beg, g.size);
  //   Printf("%s", d.EndLocation());
  //   PrintGlobalNameIfASCII(g);
  //   return true;
  // }

  ReportErrorSummary(bug_descr, &stack);
  // void ReportErrorSummary(const char *error_message) {
  //   if (!common_flags()->print_summary)
  //     return;
  //   InternalScopedBuffer<char> buff(kMaxSummaryLength);
  //   internal_snprintf(buff.data(), buff.size(),
  //                     "[KT] SUMMARY: %s: %s", SanitizerToolName, error_message);
  //   __sanitizer_report_error_summary(buff.data());
  // }

  PrintShadowMemoryForAddress(addr);
  // static void PrintShadowMemoryForAddress(uptr addr) {
  //   if (!AddrIsInMem(addr))
  //     return;
  //   uptr shadow_addr = MemToShadow(addr);
  //   const uptr n_bytes_per_row = 16;
  //   uptr aligned_shadow = shadow_addr & ~(n_bytes_per_row - 1);
  //   Printf("[KT] Shadow bytes around the buggy address:\n");
  //   for (int i = -5; i <= 5; i++) {
  //     const char *prefix = (i == 0) ? "=>" : "  ";
  //     PrintShadowBytes(prefix,
  //                      (u8*)(aligned_shadow + i * n_bytes_per_row),
  //                      (u8*)shadow_addr, n_bytes_per_row);
  //   }
  //   if (flags()->print_legend)
  //     PrintLegend();
  // }
}


<asan-code-report-gcc630>

// Use this macro if you want to print stack trace with the caller
// of the current function in the top frame.
#define GET_CALLER_PC_BP_SP \
  uptr bp = GET_CURRENT_FRAME();              \
  uptr pc = GET_CALLER_PC();                  \
  uptr local_stack;                           \
  uptr sp = (uptr)&local_stack


// exported functions
#define ASAN_REPORT_ERROR(type, is_write, size)                     \
extern "C" NOINLINE INTERFACE_ATTRIBUTE                             \
void __asan_report_ ## type ## size(uptr addr) {                    \
  GET_CALLER_PC_BP_SP;                                              \
  ReportGenericError(pc, bp, sp, addr, is_write, size, 0, true);    \
}                                                                   \

extern "C" __attribute__((noinline)) __attribute__((visibility("default")))
void __asan_report_`load8`(uptr addr) 
{ 
  uptr bp = (uptr)__builtin_frame_address(0); 
  uptr pc = (uptr)__builtin_return_address(0); 
  uptr local_stack; 
  uptr sp = (uptr)&local_stack; 
  ReportGenericError(pc, bp, sp, addr, false, 8, `0, true`);

} 
extern "C" __attribute__((noinline)) __attribute__((visibility("default"))) 
void __asan_report_`exp_load8`(uptr addr, u32 exp) 
{ 
  uptr bp = (uptr)__builtin_frame_address(0); 
  uptr pc = (uptr)__builtin_return_address(0); 
  uptr local_stack; uptr sp = (uptr)&local_stack; 
  ReportGenericError(pc, bp, sp, addr, false, 8, `exp, true`); 
} 

*tool-asan-recover-noabort*
extern "C" __attribute__((noinline)) __attribute__((visibility("default"))) 
void __asan_report_`load8_noabort`(uptr addr) 
{ 
  uptr bp = (uptr)__builtin_frame_address(0); 
  uptr pc = (uptr)__builtin_return_address(0); 
  uptr local_stack; uptr sp = (uptr)&local_stack; 
  ReportGenericError(pc, bp, sp, addr, false, 8, `0, false`); 
}

void __asan_report_error(uptr pc, uptr bp, uptr sp, uptr addr, int is_write,
                         uptr access_size, u32 exp) {
  ENABLE_FRAME_POINTER;
  bool fatal = flags()->`halt_on_error`;
  ReportGenericError(pc, bp, sp, addr, is_write, access_size, exp, fatal);
}

void ReportGenericError(uptr pc, uptr bp, uptr sp, uptr addr, 
    bool is_write, uptr access_size, 
    u32 exp, bool fatal) 
{
  ScopedInErrorReport in_report(&report, fatal);
}

// 494
asan/asan_report.cc

  NORETURN ~ScopedInErrorReport() {
    Report("ABORTING\n");
    Die();

// 630
    if (halt_on_error_) {
      Report("ABORTING\n");
      Die();
    }


<asan-code-abort>
// to copy asan libs from 494 toolchain
kyoupark@kt-office-debian:~/asan/gcc/i686-nptl-linux-gnu-gcc/i686-nptl-linux-gnu/lib$ scp libasan* st-castor-03:/home/kyoupark/si_logs/flash0-vstb-430-asan-run/flash0

b __asan::AsanOnDeadlySignal

gdb --eval-command=run /NDS/asan_bin/MW_Process -b "__asan::ASAN_OnSIGSEGV"

void __asan_init() {
  InstallSignalHandlers();
}

void InstallSignalHandlers() {
  // Set the alternate signal stack for the main thread.
  // This will cause SetAlternateSignalStack to be called twice, but the stack
  // will be actually set only once.
  if (flags()->use_sigaltstack) SetAlternateSignalStack();
  MaybeInstallSigaction(SIGSEGV, ASAN_OnSIGSEGV);
  MaybeInstallSigaction(SIGBUS, ASAN_OnSIGSEGV);
}

static void     ASAN_OnSIGSEGV(int, siginfo_t *siginfo, void *context) {
  uptr addr = (uptr)siginfo->si_addr;
  // Write the first message using the bullet-proof write.
  if (13 != internal_write(2, "ASAN:SIGSEGV\n", 13)) Die();
  uptr pc, sp, bp;
  GetPcSpBp(context, &pc, &sp, &bp);
  ReportSIGSEGV(pc, sp, bp, addr);
}

ASAN:SIGSEGV
=================================================================
==1311==ERROR: AddressSanitizer: SEGV on unknown address 0xfffffb40 (pc 0xb71a7309 sp 0xbfaf0780 bp 0xbfaf0808 T0)
    #0 0xb71a7308 in __pthread_create_2_1 (/lib/libpthread.so.0+0x7308)
    #1 0xb71a76ea in __pthread_create_2_0 (/lib/libpthread.so.0+0x76ea)
    #2 0xb7280dd3 in pthread_create (/flash0/libasan.so+0x61dd3)
    #3 0x9cc4d5b in SYSTEMUTIL_THR_Create /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/SYSTEMUTIL/src/thread/systemutil_thread.c:465
    #4 0x8a80a34 in DIAG_IPC_CLIENT_Open /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/ipc/src/client/diag_ipc_client.c:224
    #5 0x8a8b134 in DIAGCTL_LOG_OpenCommunicationChannel /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/log/src/diagctl_log_ipc.c:675
    #6 0x8a88c98 in DIAGCTL_LOG_Init /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/log/src/diagctl_log.c:218
    #7 0x8a7a3b1 in __DIAG_LOG_Init /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/log/src/diag_log.c:1594
    #8 0x9c00ce7 in SYSINIT_ClientInit /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/SYSINIT/src/client/sysinit_api.c:333
    #9 0x822530d in main /home/kyoupark/STB_SW_o/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/processes/MW_Process/mw_process_main.c:85
    #10 0xb6c8c31f in __libc_start_main ../csu/libc-start.c:295
    #11 0x8233270 (/NDS/asan_bin/MW_Process_dynamic_unstripped+0x8233270)

AddressSanitizer can not provide additional info.
SUMMARY: AddressSanitizer: SEGV ??:0 __pthread_create_2_1
==1311==ABORTING


[KT:1]=================================================================
[1m[31m==1308==ERROR: AddressSanitizer: SEGV on unknown address 0xfffffb40 (pc 0xb7258309 sp 0xbf838c70 bp 0xbf838cf8 T0)
[1m[0m[KT:4]     #0 0xb7258308 in __pthread_create_2_1 (/lib/libpthread.so.0+0x7308)
[KT:4]     #1 0xb72586ea in __pthread_create_2_0 (/lib/libpthread.so.0+0x76ea)
[KT:4]     #2 0xb72fb943 in __interceptor_pthread_create ../../../../gcc-4.9.4/libsanitizer/asan/asan_interceptors.cc:192
[KT:4]     #3 0x9cc4d5b in SYSTEMUTIL_THR_Create (/NDS/asan_bin/MW_Process_dynamic+0x9cc4d5b)
[KT:4]     #4 0x8a80a34 in DIAG_IPC_CLIENT_Open (/NDS/asan_bin/MW_Process_dynamic+0x8a80a34)
[KT:4]     #5 0x8a8b134 in DIAGCTL_LOG_OpenCommunicationChannel (/NDS/asan_bin/MW_Process_dynamic+0x8a8b134)
[KT:4]     #6 0x8a88c98 in DIAGCTL_LOG_Init (/NDS/asan_bin/MW_Process_dynamic+0x8a88c98)
[KT:4]     #7 0x8a7a3b1 in __DIAG_LOG_Init (/NDS/asan_bin/MW_Process_dynamic+0x8a7a3b1)
[KT:4]     #8 0x9c00ce7 in SYSINIT_ClientInit (/NDS/asan_bin/MW_Process_dynamic+0x9c00ce7)
[KT:4]     #9 0x822530d in main (/NDS/asan_bin/MW_Process_dynamic+0x822530d)
[KT:4]     #10 0xb6d3d31f in __libc_start_main ../csu/libc-start.c:295
[KT:4]     #11 0x8233270 (/NDS/asan_bin/MW_Process_dynamic+0x8233270)

AddressSanitizer can not provide additional info.
[KT] [KT] SUMMARY: AddressSanitizer: SEGV ??:0 __pthread_create_2_1


<asan-code>

void NOINLINE __asan_handle_no_return() {


<asan-code-xxx>

void __asan_init() {

  // `kHighMemEnd` = 0xffffffff for 32 bits
  InitializeHighMemEnd();

  /*
  // asan/asan_interceptors.cc
  InitializeHighMemEnd()
  {
    // Intercept mem* functions.
    ASAN_INTERCEPT_FUNC(memcmp);
    ASAN_INTERCEPT_FUNC(memmove);
  }

  // asan/asan_interceptors.cc
  INTERCEPTOR(int, memcmp, const void *a1, const void *a2, uptr size) {
    ...
  }
  */

  InitializeAsanInterceptors();

  // For 32 bits:
  //
  // one shadow byte represents 8 bytes applicaion memory. 512M (2**29) shadow
  // space(1/8) represents the whole space(2G, 8/8)
  //
  // static const u64 kDefaultShadowOffset32 = 1ULL << 29;
  //
  // >>> hex(2**29)
  // '0x20000000'
  //
  // #define SHADOW_OFFSET kDefaultShadowOffset32
  // #define kLowShadowBeg   SHADOW_OFFSET
  //
  
  // #define kLowMemEnd      (SHADOW_OFFSET ? SHADOW_OFFSET - 1 : 0)
  // #define kLowShadowEnd   MEM_TO_SHADOW(kLowMemEnd)

  // pagesize is 4096
  //
  // >>> hex(4096)
  // '0x1000'
  // >>> hex(0x20000000-0x1000)
  // '0x1ffff000'

  if (kLowShadowBeg)
    shadow_start -= GetMmapGranularity();

  // MEM_TO_SHADOW()
  //
  // // If set, asan uses the values of SHADOW_SCALE and SHADOW_OFFSET
  // // provided by the instrumented objects. Otherwise constants are used.
  // # define ASAN_FLEXIBLE_MAPPING_AND_OFFSET 0
  //
  // # define SHADOW_SCALE kDefaultShadowScale
  // static const u64 kDefaultShadowScale = 3;
  // static const u64 kDefaultShadowOffset32 = 1ULL << 29;
  // 
  // #define MEM_TO_SHADOW(mem) (((mem) >> SHADOW_SCALE) + (SHADOW_OFFSET))
  //
  // `kHighMemEnd` = 0xffffffff for 32 bits
  // #define `kHighMemBeg`     (MEM_TO_SHADOW(kHighMemEnd) + 1)
  //
  // PrintAddressSpaceLayout();
  //
  // (void*)kHighMemBeg, (void*)kHighMemEnd);
  // || `[0x40000000, 0xffffffff]` || HighMem    ||   
  //
  // #define kHighShadowEnd  MEM_TO_SHADOW(kHighMemEnd)
  // #define kHighShadowBeg  MEM_TO_SHADOW(kHighMemBeg)
  //
  // (void*)kHighShadowBeg, (void*)kHighShadowEnd); 384M
  // || `[0x28000000, 0x3fffffff]` || HighShadow ||
  //
  // (void*)kShadowGapBeg, (void*)kShadowGapEnd);   64M
  // || `[0x24000000, 0x27ffffff]` || ShadowGap  ||
  //
  // (void*)kLowShadowBeg, (void*)kLowShadowEnd);   64M
  // || `[0x20000000, 0x23ffffff]` || LowShadow  ||
  //
  // (void*)kLowMemBeg, (void*)kLowMemEnd);
  // || `[0x00000000, 0x1fffffff]` || LowMem     ||
  //
  // MemToShadow(shadow): 0x24000000 0x247fffff 0x25000000 0x27ffffff
  // red_zone=16
  // quarantine_size=64M
  // malloc_context_size=30
  // SHADOW_SCALE: 3
  // SHADOW_GRANULARITY: 8
  // SHADOW_OFFSET: 20000000

  // check mem aginst /proc/maps

  bool full_shadow_is_available =
      MemoryRangeIsAvailable(shadow_start, kHighShadowEnd);
        
  if (full_shadow_is_available) {

  // mmap the low shadow plus at least one page at the left.
  ReserveShadowMemoryRange(shadow_start, kLowShadowEnd);

  // 504          Report("[KT] ReserveShadowMemoryRange(0x%x, 0x%x)\n", shadow_start, kLowShadowEnd);
  // ==1024==[KT] ReserveShadowMemoryRange(0x1ffff000, 0x23ffffff)

  // mmap the high shadow.
  ReserveShadowMemoryRange(kHighShadowBeg, kHighShadowEnd);
  // 509          Report("[KT] ReserveShadowMemoryRange(0x%x, 0x%x)\n", kHighShadowBeg, kHighShadowEnd);
  // ==1024==[KT] ReserveShadowMemoryRange(0x28000000, 0x0)

  // 512          Report("[KT] ProtectGap(0x%x, 0x%x)\n", kShadowGapBeg, kShadowGapEnd - kShadowGapBeg + 1);
  // ==1024==[KT] ProtectGap(0x24000000, 0x0)
  }
}


<asan-code-asm> *arch-x86-asm*

modified it to have function call to see the sanitized code more clearly.

     1  #include <stdio.h>
     2  #include <stdlib.h>
     3
     4  //      gcc -fsanitize=address testmain.c -o out_asn
     5
     6  int call_add(int a, int b)
     7  {
     8      int c = a*10 + b;
     9      return c;
    10  }
    11
    12  void call_uaf()
    13  {
    14      int *x = malloc(10*sizeof(int));
    15      free(x);
    16      x[5] = 10;
    17
    18      return;
    19  }
    20
    21  int main(int argc, char **argv)
    22  {
    23      printf("main: begin.\n" );
    24      printf("main: this is uaf(use after free)..\n" );
    25      printf("main: \n" );
    26
    27      call_uaf();
    28
    29      printf("main: returns from call_add() is %d\n", call_add(10, 20));
    30      printf("main: ends.\n" );
    31
    32      return 0;
    33  }


// no-sanitized

own_no_asan_uaf:     file format elf32-i386

080484fc <call_uaf>:
call_uaf():
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:13
 80484fc:	55                   	push   ebp
 80484fd:	89 e5                	mov    ebp,esp
 80484ff:	83 ec 18             	sub    esp,0x18                 ; -24
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:14
 8048502:	83 ec 0c             	sub    esp,0xc                  ; -12
 8048505:	6a 28                	push   0x28                     ; 40 bytes
 8048507:	e8 54 fe ff ff       	call   8048360 <malloc@plt>     ; call malloc
 804850c:	83 c4 10             	add    esp,0x10                 ; move sp back, +16
 804850f:	89 45 f4             	mov    DWORD PTR [ebp-0xc],eax  ; int *x = malloc()
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:15
 8048512:	83 ec 0c             	sub    esp,0xc
 8048515:	ff 75 f4             	push   DWORD PTR [ebp-0xc]      ; x
 8048518:	e8 33 fe ff ff       	call   8048350 <free@plt>       ; call free
 804851d:	83 c4 10             	add    esp,0x10                 ; move sp back,
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:16
 8048520:	8b 45 f4             	mov    eax,DWORD PTR [ebp-0xc]  ; x
 8048523:	83 c0 14             	add    eax,0x14                 ; x+20
 8048526:	c7 00 0a 00 00 00    	mov    DWORD PTR [eax],0xa      ; x[5] = 10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:18
 804852c:	90                   	nop
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:19
 804852d:	c9                   	leave  
 804852e:	c3                   	ret    

0804852f <main>:
main():
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:22
 804852f:	8d 4c 24 04          	lea    ecx,[esp+0x4]
 8048533:	83 e4 f0             	and    esp,0xfffffff0
 8048536:	ff 71 fc             	push   DWORD PTR [ecx-0x4]
 8048539:	55                   	push   ebp
 804853a:	89 e5                	mov    ebp,esp
 804853c:	51                   	push   ecx
 804853d:	83 ec 04             	sub    esp,0x4
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:23
 8048540:	83 ec 0c             	sub    esp,0xc
 8048543:	68 88 86 04 08       	push   0x8048688
 8048548:	e8 23 fe ff ff       	call   8048370 <puts@plt>
 804854d:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:24
 8048550:	83 ec 0c             	sub    esp,0xc
 8048553:	68 98 86 04 08       	push   0x8048698
 8048558:	e8 13 fe ff ff       	call   8048370 <puts@plt>
 804855d:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:25
 8048560:	83 ec 0c             	sub    esp,0xc
 8048563:	68 bc 86 04 08       	push   0x80486bc
 8048568:	e8 03 fe ff ff       	call   8048370 <puts@plt>
 804856d:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:27
 8048570:	e8 87 ff ff ff       	call   80484fc <call_uaf>


host_no_asan_uaf:     file format elf64-x86-64

00000000004005fb <call_uaf>:
call_uaf():
/home/kyoupark/git/kb/asan/uaf_mod.c:13
  4005fb:	55                   	push   rbp
  4005fc:	48 89 e5             	mov    rbp,rsp
  4005ff:	48 83 ec 10          	sub    rsp,0x10
/home/kyoupark/git/kb/asan/uaf_mod.c:14
  400603:	bf 28 00 00 00       	mov    edi,0x28                   ; 40 bytes
  400608:	e8 c3 fe ff ff       	call   4004d0 <malloc@plt>
  40060d:	48 89 45 f8          	mov    QWORD PTR [rbp-0x8],rax    ; get return
/home/kyoupark/git/kb/asan/uaf_mod.c:15
  400611:	48 8b 45 f8          	mov    rax,QWORD PTR [rbp-0x8]    ; get x
  400615:	48 89 c7             	mov    rdi,rax                    ; rdi = x
  400618:	e8 63 fe ff ff       	call   400480 <free@plt>
/home/kyoupark/git/kb/asan/uaf_mod.c:16
  40061d:	48 8b 45 f8          	mov    rax,QWORD PTR [rbp-0x8]    ; get x
  400621:	48 83 c0 14          	add    rax,0x14                   ; x+20
  400625:	c7 00 0a 00 00 00    	mov    DWORD PTR [rax],0xa        ; x[5] = 10
/home/kyoupark/git/kb/asan/uaf_mod.c:18
  40062b:	90                   	nop
/home/kyoupark/git/kb/asan/uaf_mod.c:19
  40062c:	c9                   	leave  
  40062d:	c3                   	ret    

000000000040062e <main>:
main():
/home/kyoupark/git/kb/asan/uaf_mod.c:22
  40062e:	55                   	push   rbp
  40062f:	48 89 e5             	mov    rbp,rsp
  400632:	48 83 ec 10          	sub    rsp,0x10
  400636:	89 7d fc             	mov    DWORD PTR [rbp-0x4],edi
  400639:	48 89 75 f0          	mov    QWORD PTR [rbp-0x10],rsi
/home/kyoupark/git/kb/asan/uaf_mod.c:23
  40063d:	bf 28 07 40 00       	mov    edi,0x400728
  400642:	e8 49 fe ff ff       	call   400490 <puts@plt>
/home/kyoupark/git/kb/asan/uaf_mod.c:24
  400647:	bf 38 07 40 00       	mov    edi,0x400738
  40064c:	e8 3f fe ff ff       	call   400490 <puts@plt>
/home/kyoupark/git/kb/asan/uaf_mod.c:25
  400651:	bf 5c 07 40 00       	mov    edi,0x40075c
  400656:	e8 35 fe ff ff       	call   400490 <puts@plt>
/home/kyoupark/git/kb/asan/uaf_mod.c:27
  40065b:	b8 00 00 00 00       	mov    eax,0x0
  400660:	e8 96 ff ff ff       	call   4005fb <call_uaf>


// sanitized

080486ac <call_uaf>:
call_uaf():
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:13
 80486ac:	55                   	push   ebp
 80486ad:	89 e5                	mov    ebp,esp
 80486af:	57                   	push   edi
 80486b0:	56                   	push   esi
 80486b1:	53                   	push   ebx
 80486b2:	83 ec 1c             	sub    esp,0x1c
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:14
 80486b5:	83 ec 0c             	sub    esp,0xc
 80486b8:	6a 28                	push   0x28
 80486ba:	e8 01 fe ff ff       	call   80484c0 <malloc@plt>
 80486bf:	83 c4 10             	add    esp,0x10
 80486c2:	89 45 e4             	mov    DWORD PTR [ebp-0x1c],eax
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:15
 80486c5:	83 ec 0c             	sub    esp,0xc
 80486c8:	ff 75 e4             	push   DWORD PTR [ebp-0x1c]
 80486cb:	e8 60 fe ff ff       	call   8048530 <free@plt>
 80486d0:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:16
 80486d3:	8b 45 e4             	mov    eax,DWORD PTR [ebp-0x1c]
 80486d6:	8d 48 14             	lea    ecx,[eax+0x14]
 80486d9:	89 c8                	mov    eax,ecx
 80486db:	89 c2                	mov    edx,eax
 80486dd:	c1 ea 03             	shr    edx,0x3
 80486e0:	81 c2 00 00 00 20    	add    edx,0x20000000
 80486e6:	0f b6 12             	movzx  edx,BYTE PTR [edx]
 80486e9:	84 d2                	test   dl,dl
 80486eb:	0f 95 c3             	setne  bl
 80486ee:	89 df                	mov    edi,ebx
 80486f0:	89 c6                	mov    esi,eax
 80486f2:	83 e6 07             	and    esi,0x7
 80486f5:	83 c6 03             	add    esi,0x3
 80486f8:	89 f3                	mov    ebx,esi
 80486fa:	38 d3                	cmp    bl,dl
 80486fc:	0f 9d c2             	setge  dl
 80486ff:	21 fa                	and    edx,edi
 8048701:	84 d2                	test   dl,dl
 8048703:	74 09                	je     804870e <call_uaf+0x62>
 8048705:	83 ec 0c             	sub    esp,0xc
 8048708:	50                   	push   eax
 8048709:	e8 e2 fd ff ff       	call   80484f0 <__asan_report_store4@plt>
 804870e:	c7 01 0a 00 00 00    	mov    DWORD PTR [ecx],0xa
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:18
 8048714:	90                   	nop
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:19
 8048715:	8d 65 f4             	lea    esp,[ebp-0xc]
 8048718:	5b                   	pop    ebx
 8048719:	5e                   	pop    esi
 804871a:	5f                   	pop    edi
 804871b:	5d                   	pop    ebp
 804871c:	c3                   	ret    

0804871d <main>:
main():
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:22
 804871d:	8d 4c 24 04          	lea    ecx,[esp+0x4]
 8048721:	83 e4 f0             	and    esp,0xfffffff0
 8048724:	ff 71 fc             	push   DWORD PTR [ecx-0x4]
 8048727:	55                   	push   ebp
 8048728:	89 e5                	mov    ebp,esp
 804872a:	51                   	push   ecx
 804872b:	83 ec 04             	sub    esp,0x4
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:23
 804872e:	83 ec 0c             	sub    esp,0xc
 8048731:	68 c0 88 04 08       	push   0x80488c0
 8048736:	e8 c5 fd ff ff       	call   8048500 <puts@plt>
 804873b:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:24
 804873e:	83 ec 0c             	sub    esp,0xc
 8048741:	68 00 89 04 08       	push   0x8048900
 8048746:	e8 b5 fd ff ff       	call   8048500 <puts@plt>
 804874b:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:25
 804874e:	83 ec 0c             	sub    esp,0xc
 8048751:	68 60 89 04 08       	push   0x8048960
 8048756:	e8 a5 fd ff ff       	call   8048500 <puts@plt>
 804875b:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:27
 804875e:	e8 49 ff ff ff       	call   80486ac <call_uaf>
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:29
 8048763:	83 ec 08             	sub    esp,0x8
 8048766:	6a 14                	push   0x14
 8048768:	6a 0a                	push   0xa
 804876a:	e8 1c ff ff ff       	call   804868b <call_add>
 804876f:	83 c4 10             	add    esp,0x10
 8048772:	83 ec 08             	sub    esp,0x8
 8048775:	50                   	push   eax
 8048776:	68 a0 89 04 08       	push   0x80489a0
 804877b:	e8 60 fd ff ff       	call   80484e0 <printf@plt>
 8048780:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:30
 8048783:	83 ec 0c             	sub    esp,0xc
 8048786:	68 00 8a 04 08       	push   0x8048a00
 804878b:	e8 70 fd ff ff       	call   8048500 <puts@plt>
 8048790:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:32
 8048793:	b8 00 00 00 00       	mov    eax,0x0
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:33
 8048798:	8b 4d fc             	mov    ecx,DWORD PTR [ebp-0x4]
 804879b:	c9                   	leave  
 804879c:	8d 61 fc             	lea    esp,[ecx-0x4]
 804879f:	c3                   	ret    


// static sanitized

080aceca <call_uaf>:
call_uaf():
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:13
 80aceca:	55                   	push   ebp
 80acecb:	89 e5                	mov    ebp,esp
 80acecd:	57                   	push   edi
 80acece:	56                   	push   esi
 80acecf:	53                   	push   ebx
 80aced0:	83 ec 1c             	sub    esp,0x1c
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:14
 80aced3:	83 ec 0c             	sub    esp,0xc
 80aced6:	6a 28                	push   0x28
 80aced8:	e8 83 bc fd ff       	call   8088b60 <__interceptor_malloc>
 80acedd:	83 c4 10             	add    esp,0x10
 80acee0:	89 45 e4             	mov    DWORD PTR [ebp-0x1c],eax
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:15
 80acee3:	83 ec 0c             	sub    esp,0xc
 80acee6:	ff 75 e4             	push   DWORD PTR [ebp-0x1c]
 80acee9:	e8 32 ba fd ff       	call   8088920 <__interceptor_free>
 80aceee:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:16
 80acef1:	8b 45 e4             	mov    eax,DWORD PTR [ebp-0x1c]
 80acef4:	8d 48 14             	lea    ecx,[eax+0x14]
 80acef7:	89 c8                	mov    eax,ecx
 80acef9:	89 c2                	mov    edx,eax
 80acefb:	c1 ea 03             	shr    edx,0x3
 80acefe:	81 c2 00 00 00 20    	add    edx,0x20000000
 80acf04:	0f b6 12             	movzx  edx,BYTE PTR [edx]
 80acf07:	84 d2                	test   dl,dl
 80acf09:	0f 95 c3             	setne  bl
 80acf0c:	89 df                	mov    edi,ebx
 80acf0e:	89 c6                	mov    esi,eax
 80acf10:	83 e6 07             	and    esi,0x7
 80acf13:	83 c6 03             	add    esi,0x3
 80acf16:	89 f3                	mov    ebx,esi
 80acf18:	38 d3                	cmp    bl,dl
 80acf1a:	0f 9d c2             	setge  dl
 80acf1d:	21 fa                	and    edx,edi
 80acf1f:	84 d2                	test   dl,dl
 80acf21:	74 09                	je     80acf2c <call_uaf+0x62>
 80acf23:	83 ec 0c             	sub    esp,0xc
 80acf26:	50                   	push   eax
 80acf27:	e8 14 1e fe ff       	call   808ed40 <__asan_report_store4>
 80acf2c:	c7 01 0a 00 00 00    	mov    DWORD PTR [ecx],0xa
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:18
 80acf32:	90                   	nop
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:19
 80acf33:	8d 65 f4             	lea    esp,[ebp-0xc]
 80acf36:	5b                   	pop    ebx
 80acf37:	5e                   	pop    esi
 80acf38:	5f                   	pop    edi
 80acf39:	5d                   	pop    ebp
 80acf3a:	c3                   	ret    

080acf3b <main>:
main():
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:22
 80acf3b:	8d 4c 24 04          	lea    ecx,[esp+0x4]
 80acf3f:	83 e4 f0             	and    esp,0xfffffff0
 80acf42:	ff 71 fc             	push   DWORD PTR [ecx-0x4]
 80acf45:	55                   	push   ebp
 80acf46:	89 e5                	mov    ebp,esp
 80acf48:	51                   	push   ecx
 80acf49:	83 ec 04             	sub    esp,0x4
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:23
 80acf4c:	83 ec 0c             	sub    esp,0xc
 80acf4f:	68 c0 83 0b 08       	push   0x80b83c0
 80acf54:	e8 f7 0a fa ff       	call   804da50 <puts@plt>
 80acf59:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:24
 80acf5c:	83 ec 0c             	sub    esp,0xc
 80acf5f:	68 00 84 0b 08       	push   0x80b8400
 80acf64:	e8 e7 0a fa ff       	call   804da50 <puts@plt>
 80acf69:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:25
 80acf6c:	83 ec 0c             	sub    esp,0xc
 80acf6f:	68 60 84 0b 08       	push   0x80b8460
 80acf74:	e8 d7 0a fa ff       	call   804da50 <puts@plt>
 80acf79:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:27
 80acf7c:	e8 49 ff ff ff       	call   80aceca <call_uaf>
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:29
 80acf81:	83 ec 08             	sub    esp,0x8
 80acf84:	6a 14                	push   0x14
 80acf86:	6a 0a                	push   0xa
 80acf88:	e8 1c ff ff ff       	call   80acea9 <call_add>
 80acf8d:	83 c4 10             	add    esp,0x10
 80acf90:	83 ec 08             	sub    esp,0x8
 80acf93:	50                   	push   eax
 80acf94:	68 a0 84 0b 08       	push   0x80b84a0
 80acf99:	e8 02 0a fa ff       	call   804d9a0 <printf@plt>
 80acf9e:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:30
 80acfa1:	83 ec 0c             	sub    esp,0xc
 80acfa4:	68 00 85 0b 08       	push   0x80b8500
 80acfa9:	e8 a2 0a fa ff       	call   804da50 <puts@plt>
 80acfae:	83 c4 10             	add    esp,0x10
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:32
 80acfb1:	b8 00 00 00 00       	mov    eax,0x0
/home/kyoupark/si_logs/flash0-asan/uaf_mod.c:33
 80acfb6:	8b 4d fc             	mov    ecx,DWORD PTR [ebp-0x4]
 80acfb9:	c9                   	leave  
 80acfba:	8d 61 fc             	lea    esp,[ecx-0x4]
 80acfbd:	c3                   	ret    


={============================================================================
*kt_linux_tool_100* tool-asan-code-global-buffer-overflow

<gbo_host_no_asan>

$ readelf -l gbo_host_no_asan

Elf file type is DYN (Shared object file)
Entry point 0x4e0
There are 9 program headers, starting at offset 52

Program Headers:
  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
  PHDR           0x000034 0x00000034 0x00000034 0x00120 0x00120 R E 0x4
  INTERP         0x000154 0x00000154 0x00000154 0x00013 0x00013 R   0x1
      [Requesting program interpreter: /lib/ld-linux.so.2]
  LOAD           0x000000 0x00000000 0x00000000 0x00a8c 0x00a8c R E 0x1000
  LOAD           0x000eec `0x00001eec` 0x00001eec 0x00142 0x00144 RW  0x1000
  DYNAMIC        0x000ef8 0x00001ef8 0x00001ef8 0x000f0 0x000f0 RW  0x4
  NOTE           0x000168 0x00000168 0x00000168 0x00044 0x00044 R   0x4
  GNU_EH_FRAME   0x000900 0x00000900 0x00000900 0x0004c 0x0004c R   0x4
  GNU_STACK      0x000000 0x00000000 0x00000000 0x00000 0x00000 RW  0x10
  GNU_RELRO      0x000eec 0x00001eec 0x00001eec 0x00114 0x00114 R   0x1

 Section to Segment mapping:
  Segment Sections...
   00
   01     .interp
   02     .interp .note.ABI-tag .note.gnu.build-id .gnu.hash .dynsym .dynstr \
    .gnu.version .gnu.version_r .rel.dyn .rel.plt .init .plt .plt.got .text \
    .fini .rodata .eh_frame_hdr .eh_frame
   `03`     .init_array .fini_array .jcr .dynamic .got .got.plt `.data` .bss
   ...

*cxx-static* as 'd(data)' which means local
$ nm gbo_host_no_asan
00002028 d gclientname

$ readelf -s gbo_host_no_asan
    44: 00002028     6 OBJECT  LOCAL  DEFAULT   25 gclientname

$ gdb -q gbo_host_no_asan

(gdb) b main
(gdb) run
(gdb) next
name= (9 segments) address=0x400000
                 header  0: address=  0x400034
                         type=6, flags=0x5
                 header  1: address=  0x400154
                         type=3, flags=0x4
                 header  2: address=  0x400000
                         type=1, flags=0x5
                 `header  3: address=  0x401eec`
                         type=1, flags=0x6
                 header  4: address=  0x401ef8
                         type=2, flags=0x6
                 header  5: address=  0x400168
                         type=4, flags=0x4
                 header  6: address=  0x400900
                         type=1685382480, flags=0x4
                 header  7: address=  0x400000
                         type=1685382481, flags=0x6
                 header  8: address=  0x401eec
                         type=1685382482, flags=0x4

>>> hex(0x401eec+0x142)
'0x40202e'
>>> hex(0x400000+0x2028)
'0x402028'
>>> hex(0x2e-0x28)
'0x6'

(gdb) p/x &gclientname
$4 = 0x402028

; so global gclientname is in data section which have 6 elements as code dose. 


// from objdump

00000721 <call_gbo>:
call_gbo():
/home/kyoupark/git/kb/asan/gbo.c:130
 721:   55                      push   ebp
 723:   89 e5                   mov    ebp,esp
 724:   83 ec 10                sub    esp,0x10
 727:   e8 a4 00 00 00          call   7d0 <__x86.get_pc_thunk.ax>
 72c:   05 d4 18 00 00          add    eax,0x18d4
/home/kyoupark/git/kb/asan/gbo.c:132
 731:   8b 90 2c 00 00 00       mov    edx,DWORD PTR [eax+0x2c]
 737:   8b 80 28 00 00 00       mov    eax,DWORD PTR [eax+0x28]
 73d:   89 45 f6                mov    DWORD PTR [ebp-0xa],eax
 740:   89 55 fa                mov    DWORD PTR [ebp-0x6],edx
/home/kyoupark/git/kb/asan/gbo.c:133
 743:   90                      nop
/home/kyoupark/git/kb/asan/gbo.c:134
 744:   c9                      leave
 745:   c3                      ret

000007d0 <__x86.get_pc_thunk.ax>:
__x86.get_pc_thunk.ax():
 7d0:	8b 04 24             	mov    eax,DWORD PTR [esp]
 7d3:	c3                   	ret    
 7d4:	66 90                	xchg   ax,ax
 7d6:	66 90                	xchg   ax,ax
 7d8:	66 90                	xchg   ax,ax
 7da:	66 90                	xchg   ax,ax
 7dc:	66 90                	xchg   ax,ax
 7de:	66 90                	xchg   ax,ax


; see that loaded addresss in gdb

(gdb) disas call_gbo
Dump of assembler code for function call_gbo:
   0x00400721 <+0>:     push   ebp
   0x00400722 <+1>:     mov    ebp,esp
   0x00400724 <+3>:     sub    esp,0x10
   0x00400727 <+6>:     call   `0x4007d0` <__x86.get_pc_thunk.ax>
   0x0040072c <+11>:    add    eax,0x18d4
=> 0x00400731 <+16>:    mov    edx,DWORD PTR [eax+0x2c]
   0x00400737 <+22>:    mov    eax,DWORD PTR [eax+0x28]
   0x0040073d <+28>:    mov    DWORD PTR [ebp-0xa],eax
   0x00400740 <+31>:    mov    DWORD PTR [ebp-0x6],edx
   0x00400743 <+34>:    nop
   0x00400744 <+35>:    leave
   0x00400745 <+36>:    ret
End of assembler dump.
(gdb)

(gdb) p/x $eax
$27 = 0x402000

(gdb) x/6c $eax+0x28
0x402028 <gclientname>: 95 '_'  68 'D'  76 'L'  82 'R'  72 'H'  0 '\000'
(gdb) x/4c $eax+0x2c
0x40202c <gclientname+4>:       72 'H'  0 '\000'        0 '\000'        0 '\000'

(gdb) p &gclientname
$28 = (char (*)[6]) 0x402028 <gclientname>

(gdb) p gclientname
$29 = "_DLRH"

see *x86-asm-get-ip* and it's used since it uses move to get global 

see that no memcpy function call even when use -O0 to build.


130: void call_gbo()
131: {
132:   char buffer[10];
133:   memcpy(buffer, gclientname, 8);
134:   return;
135: }


00000721 <call_gbo>:
call_gbo():
/home/kyoupark/git/kb/asan/gbo.c:131
 721:	55                   	push   ebp
 722:	89 e5                	mov    ebp,esp
 724:	83 ec 10             	sub    esp,0x10

; get global var
 727:	e8 a4 00 00 00       	call   7d0 <__x86.get_pc_thunk.ax>
 72c:	05 d4 18 00 00       	add    eax,0x18d4

/home/kyoupark/git/kb/asan/gbo.c:133
; read 4 from gclientname[4]
 731:	8b 90 2c 00 00 00    	mov    edx,DWORD PTR [eax+0x2c]
; read 4 from gclientname[0]
 737:	8b 80 28 00 00 00    	mov    eax,DWORD PTR [eax+0x28]

; *x86-asm-endian* endian-little-endian
; ebp-10 = (0,3)
 73d:	89 45 f6             	mov    DWORD PTR [ebp-0xa],eax
; ebp-6  = (4,7)
 740:	89 55 fa             	mov    DWORD PTR [ebp-0x6],edx

/home/kyoupark/git/kb/asan/gbo.c:134
 743:	90                   	nop
/home/kyoupark/git/kb/asan/gbo.c:135
 744:	c9                   	leave  
 745:	c3                   	ret    

// before call
(gdb) p $ebp
$10 = (void *) 0xbffff1a8
(gdb) p $esp
$11 = (void *) 0xbffff1a0

// enters call, since push ra
(gdb) p $ebp
$12 = (void *) 0xbffff1a8
(gdb) p $esp
$13 = (void *) 0xbffff19c

// after function prolog; push ebp; mov ebp, esp
(gdb) p $ebp
$15 = (void *) 0xbffff198
(gdb) p $esp
$16 = (void *) 0xbffff198

; sub esp, 0x10
(gdb) p $esp
$17 = (void *) 0xbffff188

; *gdb-print* prints increasing address order so print from esp(low) to ebp(high)
; local var is in [ebp, esp] and 16 is enough to hold it

(gdb) x/20xb $esp
0xbffff188:     0xa8    0xf1    0xff    0xbf    0xae    0x07   [`0x40`    0x00
0xbffff190:     0xec    0x08    0x40    0x00    0x00    0x00    0x00    0x00]
0xbffff198:     0xa8    0xf1    0xff    0xbf

(gdb) x/10xb &buffer
0xbffff18e:     `0x40`    0x00    0xec    0x08    0x40    0x00    0x00    0x00
0xbffff196:     0x00    0x00


<gbo_host_asan>

130: void call_gbo()
131: {
132:   char buffer[10];
133:   memcpy(buffer, gclientname, 8);
134:   return;
135: }


Program Headers:
  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
  PHDR           0x000034 0x00000034 0x00000034 0x00120 0x00120 R E 0x4
  INTERP         0x000154 0x00000154 0x00000154 0x00013 0x00013 R   0x1
      [Requesting program interpreter: /lib/ld-linux.so.2]
  LOAD           0x000000 0x00000000 0x00000000 0x01418 0x01418 R E 0x1000
  LOAD           0x001ec4 0x00002ec4 0x00002ec4 0x002dc 0x002e0 RW  0x1000
  DYNAMIC        0x001edc 0x00002edc 0x00002edc 0x00108 0x00108 RW  0x4
  NOTE           0x000168 0x00000168 0x00000168 0x00044 0x00044 R   0x4
  GNU_EH_FRAME   0x00121c 0x0000121c 0x0000121c 0x0005c 0x0005c R   0x4
  GNU_STACK      0x000000 0x00000000 0x00000000 0x00000 0x00000 RW  0x10
  GNU_RELRO      0x001ec4 0x00002ec4 0x00002ec4 0x0013c 0x0013c R   0x1

 name= (9 segments) address=0x400000
                  header  0: address=  0x400034
                          type=6, flags=0x5
                  header  1: address=  0x400154
                          type=3, flags=0x4
                  header  2: address=  0x400000
                          type=1, flags=0x5
                  header  3: address=  0x402ec4
                          type=1, flags=0x6
                  header  4: address=  0x402edc
                          type=2, flags=0x6
                  header  5: address=  0x400168
                          type=4, flags=0x4
                  header  6: address=  0x40121c
                          type=1685382480, flags=0x4
                  header  7: address=  0x400000
                          type=1685382481, flags=0x6
                  header  8: address=  0x402ec4
                          type=1685382482, flags=0x4


// gbo_host_asan_32.s

00000d26 <call_gbo>:
call_gbo():
/home/kyoupark/git/kb/asan/gbo.c:131
 d26:	55                   	push   ebp
 d27:	89 e5                	mov    ebp,esp
 d29:	57                   	push   edi
 d2a:	56                   	push   esi
 d2b:	53                   	push   ebx

; (gdb) p $ebp-$esp
; $6 = 136
 d2c:	83 ec 7c             	sub    esp,0x7c

 d2f:	e8 bc fb ff ff       	call   8f0 <__x86.get_pc_thunk.bx>
 d34:	81 c3 cc 22 00 00    	add    ebx,0x22cc

; (gdb) p/x $ebx
; $20 = 0x403000
; *tool-readelf-S*
;  [24] .got              PROGBITS        00002fe4 001fe4 00001c 04  WA  0   0  4
 
; *tool-readelf-r*
; 00002fe8 is mssing?

; 00002fe4  00000106 R_386_GLOB_DAT    00000000   __cxa_finalize@GLIBC_2.1.3
; 00002fec  00000b06 R_386_GLOB_DAT    00000000   _Jv_RegisterClasses
; 00002ff0  00001006 R_386_GLOB_DAT    00000000   __asan_option_detect_s
; 00002ff4  00001106 R_386_GLOB_DAT    00000000   _ITM_deregisterTMClone
; 00002ff8  00001206 R_386_GLOB_DAT    00000000   __gmon_start__
; 00002ffc  00001306 R_386_GLOB_DAT    00000000   _ITM_registerTMCloneTa

; *tool-nm*
; 00003000 d _GLOBAL_OFFSET_TABLE_

; ebx is GOT
; 
; >>> hex(0xd34+0x22cc)
; '0x3000'


; -120, use callee-saved reg. 
; do not have codes to set stack, what does ebp-0x78 is "buffer"

; *gdb-print-difference* do not use gdb-print-x on regs
; (gdb) p $esp
; $2 = (void *) 0xbffff110
; (gdb) x $esp
; 0xbffff110:     0xb6103780

; (gdb) x/34x $esp    ; 136 bytes [esp:ebp]
; 0xbffff120:     0xb6103780      0x00401160      0xb7a25d60      0xb78dcc9b
; `0xbffff130`: -0x78:0xb7a25d60      0xb6103780      0x00000007      0xb7a25000
; 0xbffff140:     0x00000000      0xb7a2ced8      0xb78dcbc9      0xb7a23960
; 0xbffff150:     0x00000006      0xb7a25000      0xbffff198      0xb78d1a2b
; 0xbffff160:     0xb7a25d60      0x0000000a      0x00000006      0x00000000
; 0xbffff170:     0xb7fea00b      0x00403000      0x000007d4      0xfbad2a84
; 0xbffff180:     0xbffff1b8      0xb7ff0710      0xb78d188b      0x00403000
; 0xbffff190:     0x00000001      0xb7a25000      0xbffff1b8      0x00403000
; 0xbffff1a0:     0x00000001      0xb7a25000
; 
; esi            0xbffff130       -1073745616

; esi is &buffer
; save address from ebp-0x78 but not contents to esi and to ebp-0x7c

 d3a:	8d 75 88             	lea    esi,[ebp-0x78]
 d3d:	89 75 84             	mov    DWORD PTR [ebp-0x7c],esi

; (gdb) x/34x $esp
; 0xbffff120:     0xb6103780      0x00401160      0xb7a25d60      `-7c:0xbffff130`
; 0xbffff130:     0xb7a25d60      0xb6103780      0x00000007      0xb7a25000
; 0xbffff140:     0x00000000      0xb7a2ced8      0xb78dcbc9      0xb7a23960


; ebx-0x10 is GOT[3]. if it is 0 then jump
; >>> hex(0xd34+0x22cc-0x10)
; '0x2ff0'
; (gdb) x/8 0x402fe4
; 0x402fe4:       0xb78a0a80      0x00400e24      0x00000000      `2ff0:0xb7b8c9ac`
; 0x402ff4:       0x00000000      0x00000000      0x00000000      0x00002edc
; 0x403004:       0xb7fff920      0xb7ff0700      0xb7aaf350      0xb7a4dce0

; check this option
 d40:	8b 83 f0 ff ff ff    	mov    eax,DWORD PTR [ebx-0x10]
 d46:	83 38 00             	cmp    DWORD PTR [eax],0x0
 d49:	74 13                	je     d5e <call_gbo+0x38>

; (gdb) i reg
; eax            0xb7b8c9ac       -1212626516
; 
; (gdb) x 0xb7b8c9ac
; 0xb7b8c9ac <__asan_option_detect_stack_use_after_return>:       0x00000000

; (gdb) info symbol 0xb7b8c9ac
; __asan_option_detect_stack_use_after_return in section .bss of /usr/lib/i386-linux-gnu/libasan.so.3

; *gdb-disassemble-issue*
; (gdb) disassemble 0xb7b8c9ac
; Dump of assembler code for function __asan_option_detect_stack_use_after_return:
;    0xb7b8c9ac <+0>:     add    BYTE PTR [eax],al
;    0xb7b8c9ae <+2>:     add    BYTE PTR [eax],al
; End of assembler dump.
;
; turns out its var, not a function.
; asan/asan_rtl.cc
; 28:int __asan_option_detect_stack_use_after_return;  // Global interface symbol.
; 458:  __asan_option_detect_stack_use_after_return =

; asan/asan_fake_stack.cc
 d4b:	83 ec 0c             	sub    esp,0xc
 d4e:	6a 60                	push   0x60
 d50:	e8 ab fa ff ff       	call   800 <__asan_stack_malloc_1@plt>
 d55:	83 c4 10             	add    esp,0x10
 d58:	85 c0                	test   eax,eax
 d5a:	74 02                	je     d5e <call_gbo+0x38>
 d5c:	89 c6                	mov    esi,eax

; edi = esi+0x60 is within the stack  and set it as dest?
; Q: by doing this, &buffer address becomes within the stack. why?
 d5e:	8d 46 60             	lea    eax,[esi+0x60]
 d61:	89 c7                	mov    edi,eax

; static const uptr kCurrentStackFrameMagic = 0x41B58AB3;
; not in mapping?
 d63:	c7 06 b3 8a b5 41    	mov    DWORD PTR [esi],0x41b58ab3

; save them in stack. what are they?
 d69:	8d 83 a0 e0 ff ff    	lea    eax,[ebx-0x1f60]
 d6f:	89 46 04             	mov    DWORD PTR [esi+0x4],eax
 d72:	8d 83 26 dd ff ff    	lea    eax,[ebx-0x22da]
 d78:	89 46 08             	mov    DWORD PTR [esi+0x8],eax

; poison around "buffer" var and esi is the start of redzone
;
; Shadow byte legend (one shadow byte represents 8 application bytes):
;   Addressable:           00
;   Partially addressable: 01 02 03 04 05 06 07
;   Heap left redzone:       fa
;   Heap right redzone:      fb
;   Freed heap region:       fd
;   Stack left redzone:      f1
;   Stack mid redzone:       f2
;   Stack right redzone:     f3
;   Stack partial redzone:   f4
;   Stack after return:      f5
;   Stack use after scope:   f8
;   Global redzone:          f9
;   Global init order:       f6
;   Poisoned by user:        f7
;   Container overflow:      fc
;   Array cookie:            ac
;   Intra object redzone:    bb
;   ASan internal:           fe
;   Left alloca redzone:     ca
;   Right alloca redzone:    cb

; 0x200, 10.0000.0000. 10 bytes since first byte is 0 which means 8 bytes and
; second bytes is 10 which is 2. So 10 bytes in total.

; char rz1[32]; // 32-byte aligned
; char a[16];
; char rz2[16];
; char rz3[32];

; (gdb) i reg
; ecx            0xbffff120       -1073745632
; esi            0xbffff120       -1073745632
; (gdb) p &buffer
; $12 = (char (*)[10]) 0xbffff140

 d7b:	89 f1                	mov    ecx,esi
 d7d:	c1 e9 03             	shr    ecx,0x3
 d80:	c7 81 00 00 00 20 f1 	mov    DWORD PTR [ecx+0x20000000],0xf1f1f1f1
 d87:	f1 f1 f1 
 d8a:	c7 81 04 00 00 20 00 	mov    DWORD PTR [ecx+0x20000004],0xf4f40200
 d91:	02 f4 f4 
 d94:	c7 81 08 00 00 20 f3 	mov    DWORD PTR [ecx+0x20000008],0xf3f3f3f3
 d9b:	f3 f3 f3 

; ebx+0x60 is in data section

; (gdb) p/x $ebx+0x60
; $16 = 0x403060

;  [26] .data             PROGBITS        00003040 002040 000160 00  WA  0   0 32
; 
; >>> hex(0x3040+0x160)
; '0x31a0'
;
;  [27] .bss              NOBITS          000031a0 0021a0 000004 00  WA  0   0  1

; (gdb) info symbol 0x403060
; gclientname in section .data of /home/kyoupark/git/kb/asan/gbo_host_asan_32

/home/kyoupark/git/kb/asan/gbo.c:133
 d9e:	8d 83 60 00 00 00    	lea    eax,[ebx+0x60]
 da4:	89 c2                	mov    edx,eax
 da6:	89 d0                	mov    eax,edx
 da8:	c1 e8 03             	shr    eax,0x3
 dab:	05 00 00 00 20       	add    eax,0x20000000

; read one shadow byte of global var
; (gdb) p/x *0x2008060c
; $20 = 0xf9f9f906
; (gdb) p/x $eax
; $19 = 0x6

 db0:	0f b6 00             	movzx  eax,BYTE PTR [eax]

; gclientname is 6 bytes and if its fitst shadow byte is zero, do memcpy. if
; not, call report which is void function.

 db3:	84 c0                	test   al,al
 db5:	74 09                	je     dc0 <call_gbo+0x9a>
 db7:	83 ec 0c             	sub    esp,0xc

; edx = gclientname
 dba:	52                   	push   edx
 dbb:	e8 b0 fa ff ff       	call   870 <__asan_report_load8@plt>

(gdb) finish
Run till exit from #0  0x004007c0 in ?? ()
=================================================================
==6199==ERROR: AddressSanitizer: 
global-buffer-overflow on address 0x00403060 at pc 0x00400dc0 bp 0xbffff0f8 sp 0xbffff0ec
READ of size 8 at 0x00403060 thread T0
    #0 0x400dbf in call_gbo /home/kyoupark/git/kb/asan/gbo.c:133
    #1 0x400e8c in main /home/kyoupark/git/kb/asan/gbo.c:145
    #2 0xb788a285 in __libc_start_main (/lib/i386-linux-gnu/libc.so.6+0x18285)
    #3 0x4008e0  (/home/kyoupark/git/kb/asan/gbo_host_asan_32+0x8e0)

0x00403066 is located 0 bytes to the right of global variable 'gclientname' defined in 'gbo.c:128:13' (0x403060) of size 6
  'gclientname' is ascii string '_DLRH'
SUMMARY: AddressSanitizer: global-buffer-overflow /home/kyoupark/git/kb/asan/gbo.c:133 in call_gbo
Shadow bytes around the buggy address:
  0x200805b0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x200805c0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x200805d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x200805e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x200805f0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
=>0x20080600: 00 00 00 00 00 00 00 00 00 00 00 00[06]f9 f9 f9
  0x20080610: f9 f9 f9 f9 00 00 00 00 00 00 00 00 00 00 00 00
  0x20080620: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20080630: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20080640: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20080650: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:       fa
  Heap right redzone:      fb
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack partial redzone:   f4
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
==6199==ABORTING


; memcpy(buffer, gclientname)
; edi, dest, endian-little, eax is low and edx is high
 dc0:	8b 83 60 00 00 00    	mov    eax,DWORD PTR [ebx+0x60]
 dc6:	8b 93 64 00 00 00    	mov    edx,DWORD PTR [ebx+0x64]
 dcc:	89 47 c0             	mov    DWORD PTR [edi-0x40],eax
 dcf:	89 57 c4             	mov    DWORD PTR [edi-0x3c],edx


; Q: why compare when esi and stack does not change?
/home/kyoupark/git/kb/asan/gbo.c:134
 dd2:	90                   	nop
/home/kyoupark/git/kb/asan/gbo.c:131
 dd3:	39 75 84             	cmp    DWORD PTR [ebp-0x7c],esi
 dd6:	74 26                	je     dfe <call_gbo+0xd8>
 dd8:	c7 06 0e 36 e0 45    	mov    DWORD PTR [esi],0x45e0360e

; poison "buffer" with stack after return
 dde:	c7 81 00 00 00 20 f5 	mov    DWORD PTR [ecx+0x20000000],0xf5f5f5f5
 de5:	f5 f5 f5 
 de8:	c7 81 04 00 00 20 f5 	mov    DWORD PTR [ecx+0x20000004],0xf5f5f5f5
 def:	f5 f5 f5 
 df2:	c7 81 08 00 00 20 f5 	mov    DWORD PTR [ecx+0x20000008],0xf5f5f5f5
 df9:	f5 f5 f5 
 dfc:	eb 1e                	jmp    e1c <call_gbo+0xf6>

; unpoison "buffer"
 dfe:	c7 81 00 00 00 20 00 	mov    DWORD PTR [ecx+0x20000000],0x0
 e05:	00 00 00 
 e08:	c7 81 04 00 00 20 00 	mov    DWORD PTR [ecx+0x20000004],0x0
 e0f:	00 00 00 
 e12:	c7 81 08 00 00 20 00 	mov    DWORD PTR [ecx+0x20000008],0x0
 e19:	00 00 00 

/home/kyoupark/git/kb/asan/gbo.c:135
 e1c:	8d 65 f4             	lea    esp,[ebp-0xc]
 e1f:	5b                   	pop    ebx
 e20:	5e                   	pop    esi
 e21:	5f                   	pop    edi
 e22:	5d                   	pop    ebp
 e23:	c3                   	ret    


<ex> from x86_32, 630

(gdb) disassemble call_gbo_1
Dump of assembler code for function call_gbo_1:
   0x08048c08 <+0>:     push   ebp
   0x08048c09 <+1>:     mov    ebp,esp
   0x08048c0b <+3>:     push   edi
   0x08048c0c <+4>:     push   esi
   0x08048c0d <+5>:     push   ebx
   0x08048c0e <+6>:     sub    esp,0x6c

; ebx, edi is &buffer
; (gdb) p/x $ebx
; $1 = 0xbffff0d0
; (gdb) p/x &buffer
; $2 = 0xbffff0f0

   0x08048c11 <+9>:     lea    ebx,[ebp-0x78]
   0x08048c14 <+12>:    mov    edi,ebx

; check this option
; (gdb) info symbol 0x804b270
; __asan_option_detect_stack_use_after_return in section 
; .bss of /home/kyoupark/git/kb/asan/gbo_target_host_recover_asan_32

   0x08048c16 <+14>:    cmp    DWORD PTR ds:0x804b270,0x0
   0x08048c1d <+21>:    je     0x8048c32 <call_gbo_1+42>
   0x08048c1f <+23>:    sub    esp,0xc
   0x08048c22 <+26>:    push   0x60
   0x08048c24 <+28>:    call   0x80486f0 <__asan_stack_malloc_1@plt>
   0x08048c29 <+33>:    add    esp,0x10
   0x08048c2c <+36>:    test   eax,eax
   0x08048c2e <+38>:    je     0x8048c32 <call_gbo_1+42>
   0x08048c30 <+40>:    mov    ebx,eax

   0x08048c32 <+42>:    lea    eax,[ebx+0x60]
   0x08048c35 <+45>:    mov    esi,eax

; static const uptr kCurrentStackFrameMagic = 0x41B58AB3;
; 0x8049200, 0x8048c08 are addresses somewhere is the executable

   0x08048c37 <+47>:    mov    DWORD PTR [ebx],0x41b58ab3
   0x08048c3d <+53>:    mov    DWORD PTR [ebx+0x4],0x8049200
   0x08048c44 <+60>:    mov    DWORD PTR [ebx+0x8],0x8048c08

; *tool-asan-recover* use `eax`
; so the trick is that "no recover" version uses $eax which is modified from
; report function and "recover" version uses $ebx which is not modified from
; report? *x86-asm-return* *x86-asm-eax* $eax gets changed even after calling
; void function.
; 
; or 
;
; https://wiki.skullsecurity.org/index.php?title=Registers
; Volatility
; 
; Some registers are typically volatile across functions, and others remain
; unchanged. This is a feature of the compiler's standards and must be looked
; after in the code, registers are not preserved automatically (although in some
;     assembly languages they are -- but not in x86). What that means is, when a
; function is called, there is no guarantee that volatile registers will retain
; their value when the function returns, and it's the function's responsibility
; to preserve non-volatile registers.
; 
; The conventions used by Microsoft's compiler are:
; 
; Volatile: ecx, edx
; Non-Volatile: ebx, esi, edi, ebp
; Special: eax, esp (discussed later)

   0x08048c4b <+67>:    mov    eax,ebx
   0x08048c4d <+69>:    shr    eax,0x3
   0x08048c50 <+72>:    mov    DWORD PTR [eax+0x20000000],0xf1f1f1f1
   0x08048c5a <+82>:    mov    DWORD PTR [eax+0x20000004],0xf4f40200
   0x08048c64 <+92>:    mov    DWORD PTR [eax+0x20000008],0xf3f3f3f3

; (gdb) info symbol 0x804b080
; gclientname in section .data of /home/kyoupark/git/kb/asan/gbo_target_host_recover_asan_32
; note: this where stops when use "b call_gbo_1"

=> 0x08048c6e <+102>:   mov    edx,0x804b080
   0x08048c73 <+107>:   mov    ecx,edx
   0x08048c75 <+109>:   shr    ecx,0x3
   0x08048c78 <+112>:   add    ecx,0x20000000
   0x08048c7e <+118>:   movzx  ecx,BYTE PTR [ecx]
   0x08048c81 <+121>:   test   cl,cl
   0x08048c83 <+123>:   je     `0x8048c8e` <call_gbo_1+134>
   0x08048c85 <+125>:   sub    esp,0xc
   0x08048c88 <+128>:   push   edx
   0x08048c89 <+129>:   call   0x8048760 <__asan_report_load8@plt>

   `0x08048c8e` <+134>:   mov    edx,DWORD PTR ds:0x804b080
   0x08048c94 <+140>:   mov    ecx,DWORD PTR ds:0x804b084
   0x08048c9a <+146>:   mov    DWORD PTR [esi-0x40],edx
   0x08048c9d <+149>:   mov    DWORD PTR [esi-0x3c],ecx
   0x08048ca0 <+152>:   nop

; is buffer changed?
; when buffer is not the same
   0x08048ca1 <+153>:   cmp    edi,ebx
   0x08048ca3 <+155>:   je     0x8048ccb <call_gbo_1+195>

; static const uptr kRetiredStackFrameMagic = 0x45E0360E;
   0x08048ca5 <+157>:   mov    DWORD PTR [ebx],0x45e0360e
   0x08048cab <+163>:   mov    DWORD PTR [eax+0x20000000],0xf5f5f5f5
   0x08048cb5 <+173>:   mov    DWORD PTR [eax+0x20000004],0xf5f5f5f5
   0x08048cbf <+183>:   mov    DWORD PTR [eax+0x20000008],0xf5f5f5f5
   0x08048cc9 <+193>:   jmp    0x8048ce9 <call_gbo_1+225>

; when buffer is the same
   0x08048ccb <+195>:   mov    DWORD PTR [eax+0x20000000],0x0
   0x08048cd5 <+205>:   mov    DWORD PTR [eax+0x20000004],0x0
   0x08048cdf <+215>:   mov    DWORD PTR [eax+0x20000008],0x0

   0x08048ce9 <+225>:   lea    esp,[ebp-0xc]
   0x08048cec <+228>:   pop    ebx
   0x08048ced <+229>:   pop    esi
   0x08048cee <+230>:   pop    edi
   0x08048cef <+231>:   pop    ebp
   0x08048cf0 <+232>:   ret


<ex> from x86_32, 630, recover option

08048b6a <call_gbo_1>:
call_gbo_1():
/home/kyoupark/git/kb/asan/gbo_recover.c:143
 8048b6a:	55                   	push   ebp
 8048b6b:	89 e5                	mov    ebp,esp
 8048b6d:	57                   	push   edi
 8048b6e:	56                   	push   esi
 8048b6f:	53                   	push   ebx
 8048b70:	83 ec 7c             	sub    esp,0x7c

 8048b73:	8d 75 88             	lea    esi,[ebp-0x78]
 8048b76:	89 75 84             	mov    DWORD PTR [ebp-0x7c],esi

 8048b79:	83 3d 50 b2 04 08 00 	cmp    DWORD PTR ds:0x804b250,0x0
 8048b80:	74 13                	je     8048b95 <call_gbo_1+0x2b>
 8048b82:	83 ec 0c             	sub    esp,0xc
 8048b85:	6a 60                	push   0x60
 8048b87:	e8 04 fb ff ff       	call   8048690 <__asan_stack_malloc_1@plt>
 8048b8c:	83 c4 10             	add    esp,0x10
 8048b8f:	85 c0                	test   eax,eax
 8048b91:	74 02                	je     8048b95 <call_gbo_1+0x2b>
 8048b93:	89 c6                	mov    esi,eax
 8048b95:	8d 46 60             	lea    eax,[esi+0x60]
 8048b98:	89 c7                	mov    edi,eax
 8048b9a:	c7 06 b3 8a b5 41    	mov    DWORD PTR [esi],0x41b58ab3
 8048ba0:	c7 46 04 80 91 04 08 	mov    DWORD PTR [esi+0x4],0x8049180
 8048ba7:	c7 46 08 6a 8b 04 08 	mov    DWORD PTR [esi+0x8],0x8048b6a

; use `$ebx`
 8048bae:	89 f3                	mov    ebx,esi
 8048bb0:	c1 eb 03             	shr    ebx,0x3
 8048bb3:	c7 83 00 00 00 20 f1 	mov    DWORD PTR [ebx+0x20000000],0xf1f1f1f1
 8048bba:	f1 f1 f1 
 8048bbd:	c7 83 04 00 00 20 00 	mov    DWORD PTR [ebx+0x20000004],0xf4f40200
 8048bc4:	02 f4 f4 
 8048bc7:	c7 83 08 00 00 20 f3 	mov    DWORD PTR [ebx+0x20000008],0xf3f3f3f3
 8048bce:	f3 f3 f3 

/home/kyoupark/git/kb/asan/gbo_recover.c:145
 8048bd1:	b8 60 b0 04 08       	mov    eax,0x804b060
 8048bd6:	89 c2                	mov    edx,eax
 8048bd8:	c1 ea 03             	shr    edx,0x3
 8048bdb:	81 c2 00 00 00 20    	add    edx,0x20000000
 8048be1:	0f b6 12             	movzx  edx,BYTE PTR [edx]
 8048be4:	84 d2                	test   dl,dl
 8048be6:	74 0c                	je     8048bf4 <call_gbo_1+0x8a>
 8048be8:	83 ec 0c             	sub    esp,0xc
 8048beb:	50                   	push   eax
 8048bec:	e8 0f fb ff ff       	call   8048700 <__asan_report_load8_`noabort`@plt>

 8048bf1:	83 c4 10             	add    esp,0x10
 8048bf4:	a1 60 b0 04 08       	mov    eax,ds:0x804b060
 8048bf9:	8b 15 64 b0 04 08    	mov    edx,DWORD PTR ds:0x804b064
 8048bff:	89 47 c0             	mov    DWORD PTR [edi-0x40],eax
 8048c02:	89 57 c4             	mov    DWORD PTR [edi-0x3c],edx
/home/kyoupark/git/kb/asan/gbo_recover.c:146
 8048c05:	90                   	nop

; is buffer changed? 
/home/kyoupark/git/kb/asan/gbo_recover.c:143
 8048c06:	39 75 84             	cmp    DWORD PTR [ebp-0x7c],esi
 8048c09:	74 26                	je     8048c31 <call_gbo_1+0xc7>

 8048c0b:	c7 06 0e 36 e0 45    	mov    DWORD PTR [esi],0x45e0360e
 8048c11:	c7 83 00 00 00 20 f5 	mov    DWORD PTR [ebx+0x20000000],0xf5f5f5f5
 8048c18:	f5 f5 f5 
 8048c1b:	c7 83 04 00 00 20 f5 	mov    DWORD PTR [ebx+0x20000004],0xf5f5f5f5
 8048c22:	f5 f5 f5 
 8048c25:	c7 83 08 00 00 20 f5 	mov    DWORD PTR [ebx+0x20000008],0xf5f5f5f5
 8048c2c:	f5 f5 f5 
 8048c2f:	eb 1e                	jmp    8048c4f <call_gbo_1+0xe5>

 8048c31:	c7 83 00 00 00 20 00 	mov    DWORD PTR [ebx+0x20000000],0x0
 8048c38:	00 00 00 
 8048c3b:	c7 83 04 00 00 20 00 	mov    DWORD PTR [ebx+0x20000004],0x0
 8048c42:	00 00 00 
 8048c45:	c7 83 08 00 00 20 00 	mov    DWORD PTR [ebx+0x20000008],0x0
 8048c4c:	00 00 00 

/home/kyoupark/git/kb/asan/gbo_recover.c:147
 8048c4f:	8d 65 f4             	lea    esp,[ebp-0xc]
 8048c52:	5b                   	pop    ebx
 8048c53:	5e                   	pop    esi
 8048c54:	5f                   	pop    edi
 8048c55:	5d                   	pop    ebp
 8048c56:	c3                   	ret    


={============================================================================
*kt_linux_tool_100* tool-asan-case

{tool-asan-build}
i686-nptl-linux-gnu-gcc -static-libasan -fsanitize=address uaf.c -o static_asan
i686-nptl-linux-gnu-gcc -fsanitize=address uaf.c -o dynamic_asan
i686-nptl-linux-gnu-gcc uaf.c -o no_asan

*asan-optimisation* uaf-write when use optimization. no asan for all optimization.

$ i686-nptl-linux-gnu-gcc -O2 -fsanitize=address 
$ i686-nptl-linux-gnu-gcc -Os -fsanitize=address
$ i686-nptl-linux-gnu-gcc -O -fsanitize=address

*TODO* clang examples use optimization but still see asan running. is it GCC
version limitation?

find the same on debian PC server:

$ gcc -v
gcc version 6.3.0 20170516 (Debian 6.3.0-18+deb9u1)

$ uname -a
Linux kt-office-debian 4.9.0-7-686-pae #1 SMP Debian 4.9.110-3+deb9u2 (2018-08-13) i686 GNU/Linux


{tool-asan-run}

* unsanitized binary meaning not use -fsanitize
  use LD_PRELOAD=libasan.so

* sanitized and dynamic
  0x00000001 (NEEDED)                     Shared library: [libasan.so.3]

* sanitized and static


{tool-asan-case-heap-usecase} `use after free` *cxx-memory-issue*

from ansic, p167, another uaf

for( p = haed; p != NULL; p = p->next )
   free(p);

shall be:

for( p = head; p != NULL; p = q )
{
   q = p->next;
   free(p);
}

for( p = head; p != NULL; )
{
  q = p;
  p = p->next;
  free(q);
}


<1> uaf-read
Here is an example usage (classical use-after-free error):

$ cat uaf.c

#include <stdlib.h>
int main() {
  int *x = malloc(10 * sizeof(int));
  free(x);
  return x[5];
}

Let's examine the generated code for return statement:

$ /opt/vd/arm-v7a15v5r1/bin/arm-v7a15v5r1-linux-gnueabi-gcc \
    -fsanitize=address -O2 -S -o - uaf.c

Below code is annotated for brevity:

// int *x = malloc(10 * sizeof(int));
  .loc 1 3 0
  mov     r0, #40
  bl      malloc
  mov     r4, r0
.LVL0:
// free(x);
  .loc 1 4 0
  bl      free
.LVL1:
// ASan `instrumentation`: check memory access
  .loc 1 5 0
  add     r0, r4, #20           // Checked memory address
  mov     r3, #536870912        // Base address of shadow region
  ldrb    r3, [r3, r0, lsr #3]
  and     r2, r0, #7
  add     r2, r2, #3
  sxtb    r3, r3
  cmp     r2, r3
  movlt   r2, #0
  movge   r2, #1
  cmp     r3, #0
  moveq   r2, #0
  cmp     r2, #0
  bne     .L4                    // Poisoned access, report error
  .loc 1 6 0
// return x[5];
  ldr     r0, [r4, #20]
  ldmfd   sp!, {r4, pc}
.L4:
// ASan instrumentation: handle poisoned access
  .loc 1 5 0
  bl      __asan_report_load4

We can see that ASan has inserted 11 additional instructions to check one
memory access. Let's now check that runtime behavior matches our expectations:


// `no optimization is used`

$ /opt/vd/arm-v7a15v5r1/bin/arm-v7a15v5r1-linux-gnueabi-gcc \
    -fsanitize=address uaf.c

$ scp a.out root@myboard:/
a.out
100%   12KB  12.1KB/s   00:00   

$ ssh root@myboard /a.out
=================================================================
==154== ERROR: AddressSanitizer: heap-use-after-free on address 0xb5500fe4
at pc 0x86b0 bp 0xbec1ecd4 sp 0xbec1eccc
`READ` of size 4 at 0xb5500fe4 thread T0
    #0 0x86af (/a.out+0x86af)
    #1 0xb57f3bcb (/lib/libc.so.6+0x17bcb)
0xb5500fe4 is located 20 bytes inside of 40-byte region [0xb5500fd0,0xb5500ff8)
freed by thread T0 here:
    #0 0xb59320f3 (/lib/libasan.so.0+0x150f3)
    #1 0x8647 (/a.out+0x8647)
    #2 0xb57f3bcb (/lib/libc.so.6+0x17bcb)
previously allocated by thread T0 here:
    #0 0xb59321bb (/lib/libasan.so.0+0x151bb)
    #1 0x8637 (/a.out+0x8637)
    #2 0xb57f3bcb (/lib/libc.so.6+0x17bcb)
Shadow bytes around the buggy address:
  0x36aa01a0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa01b0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa01c0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa01d0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa01e0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
=>0x36aa01f0: fa fa fa fa fa fa fa fa fa fa `fd fd[fd]fd fd` fa
  0x36aa0200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0210: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0220: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0230: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0240: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (`one shadow byte represents 8 application bytes`):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:     `fa`
  Heap righ redzone:     fb
  Freed Heap region:     `fd`
  Stack left redzone:    f1
  Stack mid redzone:     f2
  Stack right redzone:   f3
  Stack partial redzone: f4
  Stack after return:    f5
  Stack use after scope: f8
  Global redzone:        f9
  Global init order:     f6
  Poisoned by user:      f7
  ASan internal:         fe
==154== ABORTING

`one shadow byte represents 8 application bytes` so 40 bytes makes 5 `fd`
x[0], x[1]    to 8 bytes    1st fd
x[2], x[3]    to 16 bytes   2nd fd
x[4], x[5]    to 24 bytes   3rd fd


<2> uaf-write, `no -g and no -O`
$ ~/x-tools/i686-nptl-linux-gnu/bin/i686-nptl-linux-gnu-gcc -fsanitize=address uaf.c -o out_uaf_no_g_no_opt

   #include <stdio.h>
   #include <stdlib.h>
   
   //	gcc -fsanitize=address testmain.c -o out_asn
   int main(int argc, char **argv)
   {
       printf("====> \n" );
       printf("====> this is uaf(use after free)..\n" );
       printf("====> \n" );
       int *x = malloc(10*sizeof(int));
       free(x);
12:    x[5] = 10;
       return;
   }


[root@vSTB flash0]# ./out_uaf_no_g_no_opt
====>
====> this is uaf(use after free)..
====>
=================================================================
==1492==ERROR: AddressSanitizer: heap-use-after-free on address 0xb5b00fe4 at `pc 0x80486ba` bp 0xbfee9118 sp 0xbfee910c
`WRITE` of size 4 at 0xb5b00fe4 thread T0
    #0 0x80486b9 in main (/flash0/out_uaf_no_g_no_opt+0x80486b9)
    #1 0xb710e5ef in __libc_start_main (/lib/libc.so.6+0x185ef)
    #2 0x8048540 (/flash0/out_uaf_no_g_no_opt+0x8048540)

0xb5b00fe4 is located 20 bytes inside of `40-byte region [0xb5b00fd0,0xb5b00ff8)`
freed by thread T0 here:
    #0 0xb72e26e9 in __interceptor_free /home/kyoupark/asan/cross-build/.build/src/gcc-4.9.4/libsanitizer/asan/asan_malloc_linux.cc:79
    #1 0x804867b in main (/flash0/out_uaf_no_g_no_opt+0x804867b)
    #2 0xb710e5ef in __libc_start_main (/lib/libc.so.6+0x185ef)

previously allocated by thread T0 here:
    #0 0xb72e28d8 in __interceptor_malloc /home/kyoupark/asan/cross-build/.build/src/gcc-4.9.4/libsanitizer/asan/asan_malloc_linux.cc:96
    #1 0x804866a in main (/flash0/out_uaf_no_g_no_opt+0x804866a)
    #2 0xb710e5ef in __libc_start_main (/lib/libc.so.6+0x185ef)

SUMMARY: AddressSanitizer: heap-use-after-free ??:0 main
Shadow bytes around the buggy address:
  0x36b601a0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36b601b0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36b601c0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36b601d0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36b601e0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
=>0x36b601f0: fa fa fa fa fa fa fa fa fa fa fd fd[fd]fd fd fa
  0x36b60200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36b60210: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36b60220: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36b60230: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36b60240: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:       fa
  Heap right redzone:      fb
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack partial redzone:   f4
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Contiguous container OOB:fc
  ASan internal:           fe
==1492==ABORTING
[root@vSTB flash0]#

// addr2line
$ ~/x-tools/i686-nptl-linux-gnu/bin/i686-nptl-linux-gnu-addr2line out_uaf_no_g_no_opt 0x80486ba
??:0
/home/kyoupark/asn/uaf.c:12

// *tool-objdump*
$ ~/x-tools/i686-nptl-linux-gnu/bin/i686-nptl-linux-gnu-objdump -d -Mintel out_uaf_no_g_no_opt > out_uaf_dump_no_g_no_opt

 80486b5:	e8 06 fe ff ff       	call   80484c0 <__asan_report_store4@plt>
 80486ba:	c7 01 0a 00 00 00    	mov    DWORD PTR [ecx],0xa


<3> uaf-write, `use -g and no -O`

$ ~/x-tools/i686-nptl-linux-gnu/bin/i686-nptl-linux-gnu-gcc -g \
    -fsanitize=address uaf.c -o out_uaf_no_g_no_opt

[root@vSTB flash0]# ./out_uaf_yo_g_no_opt
====>
====> this is uaf(use after free)..
====>
=================================================================
==1493==ERROR: AddressSanitizer: heap-use-after-free on address 0xb5b00fe4 at pc 0x80486ba bp 0xbf9f8b18 sp 0xbf9f8b0c
WRITE of size 4 at 0xb5b00fe4 thread T0
    `#0 0x80486b9 in main /home/kyoupark/asn/uaf.c:12`
    #1 0xb71205ef in __libc_start_main (/lib/libc.so.6+0x185ef)
    #2 0x8048540 (/flash0/out_uaf_yo_g_no_opt+0x8048540)

0xb5b00fe4 is located 20 bytes inside of 40-byte region [0xb5b00fd0,0xb5b00ff8)


{tool-asan-case-overflow} `heap buffer overflow, out of index` *cxx-memory-issue*

<ex>
The issue was that index(dbConId) was -1(0xFFFFFFFF used as a handle) and worked
sometimes. But crashed

static MFS_STATUS MFSDBWRAP_GetDbContext(MFS_STORAGE_DB_CONTEXT_ID dbConId, 
    MFS_DB_CONTEXT **ppContext)
{
  MFS_STATUS mfsStatus = MFS_STATUS_OK;

  XDEBUG_DEFINE_FUNC_INFO("MFSDBWRAP_GetDbContext");
  XDEBUG_TRACE2_1("start: dbConId=%d", dbConId);

  // the dbConId is the index into the db context array, so we can directly
  // access it.  First check that the context is really in use. If not this is a
  // bug.

  if (db_contexts_array[dbConId].bInUse == XFALSE)
  {
    XDEBUG_ERR2_0("No free slots for contextes.  Returning the first one");
    mfsStatus = MFS_STATUS_ERROR;
  }
  else
  {
    *ppContext = &db_contexts_array[dbConId];
  }

  return mfsStatus;
}


<ex>
Let's now examine another classical memory error: buffer overflow. These are
handled differently depending on memory type (stack, static or heap) which
we'll examine separately.

$ cat heap_overflow.c

int main(int argc, char **argv) {
  int *array = new int[100];
  array[0] = 0;
  int res = array[argc + 100];  // Touch memory past end of buffer
  delete [] array;
  return res;
}

$ ssh root@myboard /a.out
=================================================================
==126== ERROR: AddressSanitizer: `heap-buffer-overflow` on address 0xb5503fb4
at pc 0x8788 bp 0xbefc7cec sp 0xbefc7ce4
`READ` of size m
    #0 0x8787 (/a.out+0x8787)
    #1 0xb5737bcb (/lib/libc.so.6+0x17bcb)
0xb5503fb4 is located 4 bytes to the right of `400-byte region` [0xb5503e20,0xb5503fb0)
allocated by thread T0 here:
    #0 0xb59a8a77 (/lib/libasan.so.0+0x10a77)
    #1 0x8697 (/a.out+0x8697)
    #2 0xb5737bcb (/lib/libc.so.6+0x17bcb)
Shadow bytes around the buggy address:
  0x36aa07a0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa07b0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa07c0: fa fa fa fa 00 00 00 00 00 00 00 00 00 00 00 00   // 8*12 = 96
  0x36aa07d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00   // 8*16 = 128
  0x36aa07e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00   // 8*16 = 128
=>0x36aa07f0: 00 00 00 00 00 00[fa]fa fa fa fa fa fa fa fa fa   // 8*6  = 48
  0x36aa0800: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0810: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0820: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0830: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0840: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:     fa
  Heap righ redzone:     fb
  Freed Heap region:     fd
  Stack left redzone:    f1
  Stack mid redzone:     f2
  Stack right redzone:   f3
  Stack partial redzone: f4
  Stack after return:    f5
  Stack use after scope: f8
  Global redzone:        f9
  Global init order:     f6
  Poisoned by user:      f7
  ASan internal:         fe
==126== ABORTING


<ex> `stack buffer overflow`
https://github.com/google/sanitizers/wiki/AddressSanitizerExampleStackOutOfBounds

Now to stack buffers:

$ cat stack_oob.c

int main(int argc, char **argv) {
  int stack_array[100];
  stack_array[1] = 0;
  return stack_array[argc + 100];  // Touch memory past end of buffer
}

$ ssh root@myboard /a.out
=================================================================
==132== ERROR: AddressSanitizer: `stack-buffer-overflow`
on address 0xbec82ccc at pc 0x875c bp 0xbec82b0c sp 0xbec82b04
READ of size 4 at 0xbec82ccc thread T0
    #0 0x875b (/a.out+0x875b)
    #1 0xb56d6bcb (/lib/libc.so.6+0x17bcb)
Address 0xbec82ccc is located at offset 436 in frame <main> of T0's stack:
  This frame has 1 object(s):
    [32, 432) 'stack_array'
HINT: this may be a false positive if your program uses some custom stack unwind mechanism or swapcontext
      (longjmp and C++ exceptions *are* supported)
Shadow bytes around the buggy address:
  0x37d90540: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d90550: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d90560: 00 00 00 f1 f1 f1 f1 00 00 00 00 00 00 00 00 00
  0x37d90570: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d90580: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
=>0x37d90590: 00 00 00 00 00 00 00 00 00[f4]f4 f3 f3 f3 f3 00
  0x37d905a0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d905b0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d905c0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d905d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d905e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:     fa
  Heap righ redzone:     fb
  Freed Heap region:     fd
  Stack left redzone:    `f1`
  Stack mid redzone:     f2
  Stack right redzone:   `f3`
  Stack partial redzone: f4
  Stack after return:    f5
  Stack use after scope: f8
  Global redzone:        f9
  Global init order:     f6
  Poisoned by user:      f7
  ASan internal:         fe
==132== ABORTING


<ex> `global buffer overflow, out of index` *cxx-memory-issue*

Accessing out of index of array is 'undefined' so sometimes work or sometimes
not.

<ex>
int g_arr[3];

int main(int argc, char* argv[])
{
  if( g_arr[5] == 0 )
    printf("idx 5 is false\n");
  else
    printf("idx 5 is true\n");
}

$ ./a.out 
idx 5 is false


<ex>
https://github.com/google/sanitizers/wiki/AddressSanitizerExampleGlobalOutOfBounds
And finally static buffers:

$ cat global_oob.c

int global_array[100] = {-1};
int main(int argc, char **argv) {
  return global_array[argc + 100];  // Touch memory past end of buffer
}

$ ssh root@myboard /a.out
=================================================================
==138== ERROR: AddressSanitizer: `global-buffer-overflow`
on address 0x00010b54 at pc 0x86f0 bp 0xbee97cf4 sp 0xbee97cec
READ of size 4 at 0x00010b54 thread T0
    #0 0x86ef (/a.out+0x86ef)
    #1 0xb56ccbcb (/lib/libc.so.6+0x17bcb)
0x00010b54 `is located 4 bytes to the right of global variable` 
  'global_array (global_oob.c)' (0x109c0) of size 400
Shadow bytes around the buggy address:
  0x20002110: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 // 16*8 = 128
  0x20002120: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20002130: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20002140: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20002150: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
=>0x20002160: 00 00 00 00 00 00 00 00 00 00[f9]f9 f9 f9 f9 f9
  0x20002170: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20002180: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20002190: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x200021a0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x200021b0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:     fa
  Heap righ redzone:     fb
  Freed Heap region:     fd
  Stack left redzone:    f1
  Stack mid redzone:     f2
  Stack right redzone:   f3
  Stack partial redzone: f4
  Stack after return:    f5
  Stack use after scope: f8
  Global redzone:        `f9`
  Global init order:     f6
  Poisoned by user:      f7
  ASan internal:         fe
==138== ABORTING


{tool-asan-case-overflow} <ex> *cxx-memcpy-issue*
The issue is that DIAG_CMN_ALIGN(6) becomes 8 bytes to make it aligned but it
has only 6 space. 

/* IPC client name */
static char gClientName[] = "_DLRH";

    if (0 != DIAG_IPC_CLIENT_Open(gClientName,)

int DIAG_IPC_CLIENT_Open(char *client_name)
{
    diag_ipc_client_P_notify_server(DIAG_IPC_MSG_TYPE_REG, 
        client_name,strlen(client_name)+1)
    {

      /* Prepare message header */
      diag_ipc_client_P_format_message_header(msg_to_server, 
          msg_type, g_ipc_ack_value, client_name, client_name_len)
      {
        /* client name */
        memcpy(msg+16,client_name,(int32_t)DIAG_CMN_ALIGN(client_name_len));
      }
    }
}

    /**
     * @def DIAG_CMN_ALIGN
     *
     * Make a value or an address multiple of __DIAG_ALIGNMENT
     */
#ifndef __DIAG_ALIGNMENT
#define __DIAG_ALIGNMENT	(4U)
#endif

#define DIAG_CMN_ALIGN(x)	(((x) + (__DIAG_ALIGNMENT-1)) &~ (__DIAG_ALIGNMENT-1))

// when use stripped binary

=================================================================
==1334==ERROR: AddressSanitizer: `global-buffer-overflow` on address 0x0818a606 at pc 0xb6f46711 bp 0xbfdf5778 sp 0xbfdf5358
READ of size 8 at 0x0818a606 thread T0
    #0 0xb6f46710 in __interceptor_memcpy /home/kyoupark/asan/cross-build/.build/src/gcc-4.9.4/libsanitizer/asan/asan_interceptors.cc:375
    #1 0x80948f1 (/NDS/bin/PWM_Process+0x80948f1)
    #2 0x8094559 (/NDS/bin/PWM_Process+0x8094559)
    #3 0x809233e (/NDS/bin/PWM_Process+0x809233e)
    #4 0x809081f (/NDS/bin/PWM_Process+0x809081f)
    #5 0x808b6a5 (/NDS/bin/PWM_Process+0x808b6a5)
    #6 0x804c460 (/NDS/bin/PWM_Process+0x804c460)
    #7 0x804ba76 (/NDS/bin/PWM_Process+0x804ba76)
    #8 0xb6d735ef in __libc_start_main (/lib/libc.so.6+0x185ef)
    #9 0x804b890 (/NDS/bin/PWM_Process+0x804b890)

0x0818a606 is located 0 bytes to the right of global variable 'gClientName' 
  from 'diagctl_log_req_handler.c' (0x818a600) of size 6
  'gClientName' is ascii string '_DLRH'
SUMMARY: AddressSanitizer: global-buffer-overflow /home/kyoupark/asan/cross-build/.build/src/gcc-4.9.4/libsanitizer/asan/asan_interceptors.cc:375 __interceptor_memcpy
Shadow bytes around the buggy address:
  0x21031470: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x21031480: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x21031490: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x210314a0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x210314b0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
=>0x210314c0:[06]f9 f9 f9 f9 f9 f9 f9 00 00 00 00 00 00 00 00
  0x210314d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x210314e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x210314f0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x21031500: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x21031510: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:       fa
  Heap right redzone:      fb
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack partial redzone:   f4
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Contiguous container OOB:fc
  ASan internal:           fe
==1334==ABORTING


// when use unstripped binary

=================================================================
==1247==ERROR: AddressSanitizer: `global-buffer-overflow` on address 0x0af71e46 at pc 0xb6e79af3 bp 0xbfd624f8 sp 0xbfd62484
READ of size 8 at 0x0af71e46 thread T0
NDS: ^0000000057.680445 !WARN  -MEMMAN       < p:000004d3 t:b6ff7ec0 T:no name M:memman_pinit.c F:PoolCreate L:00386 > Memory pools are not growable for the moment
    #0 0xb6e79af2 in memcpy (/flash0/libasan.so.1+0x62af2)
    #1 0x8a80090 in diag_ipc_client_P_format_message_header /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/ipc/src/client/diag_ipc_client.c:821
    #2 0x8a80090 in diag_ipc_client_P_notify_server /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/ipc/src/client/diag_ipc_client.c:762
    #3 0x8a80a73 in DIAG_IPC_CLIENT_Open /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/ipc/src/client/diag_ipc_client.c:238
    #4 0x8a7dcc9 in DIAGCTL_LOG_REQ_HANDLER_Init /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/log/src/diagctl_log_req_handler.c:158
    #5 0x8a7a3c9 in __DIAG_LOG_Init /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/log/src/diag_log.c:1603
    #6 0x9c00e57 in SYSINIT_ClientInit /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/SYSINIT/src/client/sysinit_api.c:333
    #7 0x82252fd in main /home/kyoupark/STB_SW_o/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/processes/MW_Process/mw_process_main.c:85
    #8 0xb6c5031f in __libc_start_main ../csu/libc-start.c:295
    #9 0x8233260 (/NDS/bin/bin-asan-compiled-without-diag/MW_Process+0x8233260)

0x0af71e46 is located 0 bytes to the right of global variable 'gClientName' from 'diagctl_log_req_handler.c' (0xaf71e40) of size 6
  'gClientName' is ascii string '_DLRH'
SUMMARY: AddressSanitizer: global-buffer-overflow ??:0 memcpy
Shadow bytes around the buggy address:
  0x215ee370: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x215ee380: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x215ee390: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x215ee3a0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x215ee3b0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
=>0x215ee3c0: 00 00 00 00 00 00 00 00[06]f9 f9 f9 f9 f9 f9 f9
  0x215ee3d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x215ee3e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x215ee3f0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x215ee400: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x215ee410: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:       fa
  Heap right redzone:      fb
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack partial redzone:   f4
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Contiguous container OOB:fc
  ASan internal:           fe
==1247==ABORTING


<ex> *cpp-memory-issue* *cpp-memcpy-issue-issue*
Seen on NDS MW issue

MW_Process crash with core dump in SIM_dbc_query_ReadProgrammeInstanceInfo
(during memcpy) 

#define SIM_DBC_MAX_SORT_STRING_LEN   40

// void *memcpy(void *dest, const void *src, size_t n);
(void)memcpy((void*)&obj_array[offset].sort_title[0], 
    (const char*)results[24].value.text, SIM_DBC_MAX_SORT_STRING_LEN);

// source when crashed
0x1d8dfec: 0x54495050 0x494e4720 0x504f494e 0x54000000
0x1d8dffc: 0xc009 0x0 0x0 0x0

// dest
x/40a obj_array[0].sort_title
0x204eb20: 0x54495050 0x494e4720 0x504f494e 0x54000000
0x204eb30: 0x0 0x0 0x0 0x0

DESCRIPTION
       The memcpy() function copies n bytes from memory area src to memory
       area dest.  The memory areas must not overlap.  Use memmove(3) if the
       memory areas do overlap.

DESCRIPTION
       The  strcpy() function copies the string pointed to by src, including
       the terminating null byte ('\0'), to the buffer pointed to by dest.
       The strings may not overlap, and the destination string dest must be
       large enough to receive the copy. Beware of buffer overruns! (See BUGS.)

       The strncpy() function is similar, except that at most n bytes of src
       are copied.  Warning: If there is no null byte among the first n bytes
       of src, the string placed in dest will not  be  null-terminated.

BUGS
       If  the  destination string of a strcpy() is not large enough, then
       anything might happen.  Overflowing fixed-length string buffers is a
       favorite cracker technique for taking complete control of the machine.
       Any time a program reads or copies data into a buffer, the program
       first needs to check that there's enough space.  This may be
       unnecessary if you can show that overflow is  impossible, but be
       careful: programs can get changed over time, in ways that may make the
       impossible possible.


// solution
strncpy(&obj_array[offset].sort_title[0], 
    (const char*)results[24].value.text, SIM_DBC_MAX_SORT_STRING_LEN);

The problem is out of index. Since when src, results[24].value.text, is less
than SIM_DBC_MAX_SORT_STRING_LEN(40) then memcpy copies blindly over from and
this cause out of index access on src.

So strncpy will ensure only the length of the value.text is copied when there
is a null in src (destination then padded out with nulls) or a maximum of 40
is copied if the source string is over 40.


<ex-real>

==136== ERROR: AddressSanitizer: `heap-buffer-overflow` on address 0xad651b10
at pc 0xb4e03054 bp 0x75c35ac4 sp 0x75c35aac
READ of size 17 at 0xad651b10 thread T457 (homesetting)
    #0 0xb4e03053 (/mtd_exe/lib/libasan.so.0+0xf053)
    #1 0x1281917 (/mtd_exe/exeAPP+0x1281917)
    #3 0x1a27f2f (/mtd_exe/exeAPP+0x1a27f2f)
    #4 0x25d78e3 (/mtd_exe/exeAPP+0x25d78e3)
    #5 0x25d7ad7 (/mtd_exe/exeAPP+0x25d7ad7)
    #6 0x12841bb (/mtd_exe/exeAPP+0x12841bb)
    #7 0x128924b (/mtd_exe/exeAPP+0x128924b)
    #8 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #9 0xae7b50f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
0xad651b1a is located 0 bytes to the right of 10-byte region [0xad651b10,0xad651b1a)
Shadow bytes around the buggy address:
  0x35aca310: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x35aca320: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x35aca330: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x35aca340: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x35aca350: fa fa 00 04 fa fa 00 02 fa fa 00 02 fa fa fa fa
=>0x35aca360: fa fa[00]02 fa fa fa fa fa fa fa fa fa fa fa fa 
  0x35aca370: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x35aca380: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x35aca390: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x35aca3a0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x35aca3b0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:     fa
  Heap righ redzone:     fb
  Freed Heap region:     fd
  Stack left redzone:    f1
  Stack mid redzone:     f2
  Stack right redzone:   f3
  Stack partial redzone: f4
  Stack after return:    f5
  Stack use after scope: f8
  Global redzone:        f9
  Global init order:     f6
  Poisoned by user:      f7
  ASan internal:         fe


Reproducing scenario: play video from usb source. Here is the backtrace
analyzed with `addr2line` tool:

`#0` 0xb4e03053 (/mtd_exe/lib/libasan.so.0+0xf053)
__interceptor_strlen /home/ygribov/build/tools/v4/sources/gcc_1/libsanitizer/asan/asan_interceptors.cc:454

note that 'strlen' in __interceptor_strlen

452   uptr length = REAL(strlen)(s);
453   if (flags()->replace_str) {
`454`     ASAN_READ_RANGE(s, length + 1);
                           ^^^^^^^^^^
455   }

#1 0x1281917 (/mtd_exe/exeAPP+0x1281917)
PCString::Length(char const*)
/Smart/CSP_Smart_2014_Prj/REL_4211/BP_CSP/CSP-4.0/CSP/Src/PCString.cpp:347

#2 0x1a286d7 (/mtd_exe/exeAPP+0x1a286d7)
CAppStateProp::operator=(CAppStateProp&)
m.guseva/TV_2014/DTV/BP_APP/BP_AppCM/Src/Common/AppStateProp.cpp:326

322    char* ntmp = prop.GetStateName();
323   if(ntmp != NULL)
324   {
325       m_pStateName = new char[PCString::Length(ntmp)+1];
`326`       PCString::Copy(m_pStateName, ntmp, PCString::Length(ntmp));
                                             ^^^^^^^^^^^^^^^^^^^^^^
327       m_pStateName[PCString::Length(ntmp)] = '\0';
328     }

So the issue happened in strlen() function called for the CAppStateProp member
during coping from one string to another.

The root cause is in CAppStateProp constructor method where m_pStateName is
initialized without null termitaning symbol. So the proposed fix is to add
null symbol to the Length position.

diff -p AppStateProp.cpp.orig  AppStateProp.cpp
*** AppStateProp.cpp.orig       2013-12-18 12:48:15.284055908 +0400
--- AppStateProp.cpp    2013-12-18 12:48:31.668057275 +0400
*************** CAppStateProp::CAppStateProp(const char*
*** 46,51 ****
--- 46,52 ----
{
  m_pStateName = new char[PCString::Length("UNDEFINED")+1];
  PCString::Copy(m_pStateName, "UNDEFINED", PCString::Length("UNDEFINED"));
+ m_pStateName[PCString::Length("UNDEFINED")] = '\0';
}

m_coexistBanner.clear();


note:
Although can see what was wrong, but not sure how it came to conclusion
that is heap overflow and why the first call of Length was not a problem.


<ex-real> `unknown crash` was detected by ASan:
==215== ERROR: AddressSanitizer: unknown-crash on address 0x8b8eb924
at pc 0xb4e03054 bp 0x789acad4 sp 0x789acabc
READ of size 65 at 0x8b8eb924 thread T247 (webserver)
    #0 0xb4e03053 (/mtd_exe/lib/libasan.so.0+0xf053)
    #1 0x8b8d8f7b (/mtd_exe/WebServerApp/bin/libwebserver.so+0x7f7b)
    #2 0x8b8d9727 (/mtd_exe/WebServerApp/bin/libwebserver.so+0x8727)
    #3 0x8b8d9ffb (/mtd_exe/WebServerApp/bin/libwebserver.so+0x8ffb)
    #4 0x8b8ddecf (/mtd_exe/WebServerApp/bin/libwebserver.so+0xcecf)
    #5 0x7d5c68b (/mtd_exe/exeAPP+0x7d5c68b)
    #6 0x7d471db (/mtd_exe/exeAPP+0x7d471db)
    #7 0x26f095f (/mtd_exe/exeAPP+0x26f095f)
    #8 0x132cca3 (/mtd_exe/exeAPP+0x132cca3)
    #9 0x132cb9b (/mtd_exe/exeAPP+0x132cb9b)
    #10 0x1331c07 (/mtd_exe/exeAPP+0x1331c07)
    #11 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #12 0xae5b70f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
0x8b8eb964 is located 0 bytes to the right of global variable 'g_lighttpd (WebServer.cpp)'
(0x8b8eb920) of size 68
==215== SetCurrent: 0xad89b000 for thread 0x7b4bf430
==215== T485: stack [0x7b480000,0x7b4c0000) size 0x40000; local=0x7b4bed74
Shadow bytes around the buggy address:
  0x3171d6d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x3171d6e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x3171d6f0: 00 00 00 00 00 00 00 00 00==2 00 00 00 f9 f9 f9
  0x3171d700: f9 f9 f9 f9 04 f9 f9 f9 f9 f9 f9 f9 04 f9 f9 f9
 ==215== T486 exited
 ==215== T486 TSDDtor
  0x3171d710: f9 f9 f9 f9 00 00 00 00 00 00 00 00 04 f9 f9
 ==215== T487: stack [0x556e8000,0x558e8000) size 0x200000; local=0x558e6d74
 ==215== SetCurrent: 0x8fa37000 for thread 0x847f3430
 ==215== T488: stack [0x845f4000,0x847f4000) size 0x200000; local=0x847f2d74
=>0x3171d720: f9 f9 f9[00]00 00 00 00 00 00 00 04 f9 f9 f9
==215== T488 TSDDtor
  0x3171d730: f9 f9 f9 f9 04 f9 f9 f9 f9 f9 f9 f9 00 f9 f9 f9
  0x3171d740: f9 f9 f9 f9 01 f9 f9 f9 f9 f9 f9 00 f9 f9 f9
==215== SetCurrent: 0x8fa36000 for thread 0x573ff430
==2 = T489: stack [0x57300000,0x57400000) size 0x100000; local=0x573fed74
==215== T489 exited
==215== T489 TSDDtor
              f9 f9 f9 f9 04 f9 f9 f9 f9 f9 f9 f9 04 f9 f9 f9
  0x3171d760: f9 f9 f9 f9 00 00 00 00 00 00 00 00 00 00 00 00
  0x3171d770: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00

Here is the backtrace analyzed with addr2line tool:

#0 0xb4e03053 (/mtd_exe/lib/libasan.so.0+0xf053)
__interceptor_strlen
/home/ygribov/build/tools/v4/sources/gcc_1/libsanitizer/asan/asan_interceptors.cc:454
453   if (flags()->replace_str) {
454     ASAN_READ_RANGE(s, length + 1);
455   }

#1 0x8b8d8f7b (/mtd_exe/WebServerApp/bin/libwebserver.so+0x7f7b)
ProcessLauncher
/home2/m.guseva/TV_2014/AP/MAIN2014/Smart/AP_WP_Smart_2014_Prj/REL/AP_WebServer/WebServer
/Swift/src/runtime/webserver/WebServer.cpp:297
294   char k_command_str[80] = {'\0'}; //"-k + secure_db_key + '\0' = 36 bytes;
295   strncat(k_command_str,"-k ", 3);
296   strncat(k_command_str,_secure_db_key, strlen(_secure_db_key));
297  //logger().debug("WEBSERVER: k_command str: %s\n",k_command_str);

...

#6 0x7d471db (/mtd_exe/exeAPP+0x7d471db)
CWebServerAppBase::t_OnUserMessage(PTEvent const*)
/home2/m.guseva/TV_2014/DTV/AP_WP/AP_WebServer/WebServer/Src/WebServerAppBase.cpp:525
521  //Retrieve key from secure storage and set for Lighttpd and Pairing plugin.
522  std::string secureKey = getSecureDBKey();  <here>
523  WebServerSetDBKey(secureKey.c_str());
524
525  WebServerStart();
526  WebServerLoadModule("libfcgicallbackplugin.so",
                                     (void*)WebConv_internal_dispatch );
527  WS_DEBUG("Pre call to WebServerLoadModule");
528  WebServerLoadModule("libpairingplugin.so",(void*)GetPINData);

The issue happened in strlen() function called for the _secure_db_key field
which was set to the string secureKey (converted to C-string char*) via
WebServerSetDBKey() call.

The root cause is in WebServer and Lighttpd classes definitions and
setLighttpdDBKey() member functions. The proposed fix is below:

$ diff WebServer.h.orig WebServer.h
96a97
> #define SECURE_DB_KEY_LEN 64
121c122
<     char _secure_db_key[64];
---
>     char _secure_db_key[SECURE_DB_KEY_LEN+1];

$ diff WebServer.cpp.orig WebServer.cpp
88a89
>
285c286
<               char _secure_db_key[64];
---
>               char _secure_db_key[SECURE_DB_KEY_LEN+1];
289c290
<                       memset(_secure_db_key,'\0',64);
---
>                       memset(_secure_db_key,'\0', SECURE_DB_KEY_LEN+1);
449c450,451
< memcpy(_secure_db_key,db_key_value,64);
---
> memcpy(_secure_db_key,db_key_value,SECURE_DB_KEY_LEN);
>                           _secure_db_key[SECURE_DB_KEY_LEN] = '\0';
506c508
<         memset(_secure_db_key,'\0',64);
---
>         memset(_secure_db_key,'\0', SECURE_DB_KEY_LEN+1);
601c603,604
<             memcpy(_secure_db_key,db_key_value,64);
---
>             memcpy(_secure_db_key,db_key_value,SECURE_DB_KEY_LEN);
>             _secure_db_key[SECURE_DB_KEY_LEN] = '\0';
 

<ex-real> `heap buffer-overflow` was detected by ASan:
==214== ERROR: AddressSanitizer: heap-buffer-overflow on address 0x75929a70
at pc 0x3b6853c bp 0x533fed54 sp 0x533fed44
READ of size 4 at 0x75929a70 thread T488 (SMPA_CPHandover)
    #0 0x3b6853b (/mtd_exe/exeAPP+0x3b6853b)
    #1 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #2 0xae5b70f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
0xad38d7d2 is located 0 bytes to the right of 2-byte region [0xad38d7d0,0xad38d7d2)
allocated by thread T399 (SMP_AsyncSelect) here:
    #0 0x3b6853b (/mtd_exe/exeAPP+0x3b6853b)
    #1 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #2 0xae5b70f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
0x75929a72 is located 0 bytes to the right of 2-byte region [0x75929a70,0x75929a72)
allocated by thread T399 (SMP_AsyncSelect) here:
Thread T487 (SMPA_CPHandover) created by T399 (SMP_AsyncSelect) here:
Thread T488 (SMPA_CPHandover) created by T399 (SMP_AsyncSelect) here:
    #0 0xb4dff22f (/mtd_exe/lib/libasan.so.0+0xb22f)
    #1 0x3b67d5b (/mtd_exe/exeAPP+0x3b67d5b)
    #2 0x3b6014f (/mtd_exe/exeAPP+0x3b6014f)
    #3 0x3bd6a4b (/mtd_exe/exeAPP+0x3bd6a4b)
    #4 0x3c1116b (/mtd_exe/exeAPP+0x3c1116b)
    #5 0x3c16beb (/mtd_exe/exeAPP+0x3c16beb)
    #6 0x3c2d493 (/mtd_exe/exeAPP+0x3c2d493)
    #7 0x3ca049f (/mtd_exe/exeAPP+0x3ca049f)
    #8 0x3ca0ac7 (/mtd_exe/exeAPP+0x3ca0ac7)
    #9 0x3c2dc73 (/mtd_exe/exeAPP+0x3c2dc73)
    #10 0x3c9c713 (/mtd_exe/exeAPP+0x3c9c713)
    #11 0x3c9d46f (/mtd_exe/exeAPP+0x3c9d46f)
    #12 0x3c2f933 (/mtd_exe/exeAPP+0x3c2f933)
    #13 0x3c785ef (/mtd_exe/exeAPP+0x3c785ef)
    #14 0x3c610e3 (/mtd_exe/exeAPP+0x3c610e3)
    #15 0x3c60487 (/mtd_exe/exeAPP+0x3c60487)
    #16 0x3c606cb (/mtd_exe/exeAPP+0x3c606cb)
    #17 0x3c6090f (/mtd_exe/exeAPP+0x3c6090f)
    #18 0x3c53c13 (/mtd_exe/exeAPP+0x3c53c13)
    #0 0xb4dff22f    #19 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0xb22f)
 (/mtd_exe/lib/libasan.so.0+0x1d26b)
 (/mtd_exe/exeAPP+0x3b67d5b)
    #2 0x3b5fb87 (/mtd_exe/exeAPP+0x3b5fb87)
    #3 0x3bd6a4b (/mtd_exe/exeAPP+0x3bd6a4b)
    #4 0x3c1116b (/mtd_exe/exeAPP+0x3c1116b)
    #5 0x3c16beb
    #6 0x3c2d493 created by T395 (dualtv) here:
    #7 0x3ca049f (/mtd_exe/exeAPP+0x3ca049f)
    #8 0x3ca0ac7 (/mtd_exe/exeAPP+0x3ca0ac7)
    #9 0x3c2dc73 (/mtd_exe/exeAPP+0x3c2dc73)
    #10 0x3c9c713 (/mtd_exe/exeAPP+0x3c9c713)
    #11 0x3c9d46f (/mtd_exe/exeAPP+0x3c9d46f)
    #12 0x3c2f933 (/mtd_exe/exeAPP+0x3c2f933)
    #13 0x3c785ef (/mtd_exe/exeAPP+0x3c785ef)
    #14 0x3c610e3 (/mtd_exe/exeAPP+0x3c610e3)
    #15 0x3c60487 (/mtd_exe/exeAPP+0x3c60487)
    #16 0x3c606cb (/mtd_exe/exeAPP+0x3c606cb)
    #17 0x3c6090f (/mtd_exe/exeAPP+0x3c6090f)
    #18 0x3c53c13 (/mtd_exe/exeAPP+0x3c53c13)
    #19 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #20 0xae5b70f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
Shadow bytes around the buggy address:
  0x2eb252f0: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x2eb25300: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x2eb25310: fa fa 00 04 fa fa 00 04 fa fa fd fd fa fa fd fd
  0x2eb25320: fa fa fd fd fa fa fd fa fa fa fd fd fa fa fd fd
=>0x2eb25340: fa fd fa fa fa fa fa[02]fa fa fa fa fd fd fa fa


Here is the backtrace analyzed with addr2line tool:

note: 
Follow T488.

#0 0x3b6853b (/mtd_exe/exeAPP+0x3b6853b)
_SMPACPHandOver(void*)
/AP_CNC/AP_DLNA/SMP/smpadaptation/src/controlpoint/SMPACPInternals.cpp:2455

2453         pstCPActionId = ( stCP_ActionIdList * ) SMPMemAllocMA( sizeof( stCP_ActionIdList ) );
2454         pstCPActionId->cpHandle = nCPHandle;
2455         pstCPActionId->actionId = ((ControlResult*)pCbInfo)->nActionId;

#1 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
`__asan::ASanThread::ThreadStart`()
/home/ygribov/build/tools/v4/sources/gcc_1/libsanitizer/asan/asan_thread.cc:99

   #1 0x3b67d5b (/mtd_exe/exeAPP+0x3b67d5b)
   `_SMPASendAppInfo`(int nCPHandle, int nMessageType, void* pCbInfo)

   //AP_DLNA/SMP/smpadaptation/src/controlpoint/SMPACPInternals.cpp:2579
   2571 pstAppCbArgs->pCbInfo = pCbInfo;
   ...
   2578 SMPADebug(SMP_INFO_LEVEL,"[_SMPASendAppInfo] Creating Thread");
   2579 nRet = pthread_create( &pHandOverTask, &pHandOverAttrTask,
                     _SMPACPHandOver, (SMPVoid *)pstAppCbArgs);
   
   #2 0x3b6014f (/mtd_exe/exeAPP+0x3b6014f)
   SMPAGenericDeviceNotifyCb(int, SMP_EVENT, SMP_ERROR_CODE, stSMPAppList*)

   //AP_DLNA/SMP/smpadaptation/src/controlpoint/SMPAControlPoint.cpp:327
   324 SMPInt8* pCopiedDeviceHandle = NULL;
   325 SMPString* pDeviceHandleString = new SMPString((SMPChar*)pDeviceHandle);
   326 pCopiedDeviceHandle = (SMPInt8*)pDeviceHandleString->GetString();                          
   327 nErrorCode = _SMPASendAppInfo( nIntendedCP, DEVICE_ADDED, pCopiedDeviceHandle );

The root cause is in SMPAGenericDeviceNotifyCb() function when
_SMPASendAppInfo() is called. The proposed fix is below:

$ diff SMPAControlPoint.cpp.orig SMPAControlPoint.cpp
227,228c227,228
<        SMPInt8* pCopiedDeviceHandle = NULL;
<
---
>        SMPInt8* pCopiedDeviceHandle = (SMPInt8*)malloc(16);
>        SMPMemSet(pCopiedDeviceHandle, 0, 16);
230c230,232
<        pCopiedDeviceHandle = (SMPInt8*)pDeviceHandleString->GetString();
---
>        SMPSize  DeviceHandleLength = pDeviceHandleString->GetStringLength();
>        if (DeviceHandleLength > 16) DeviceHandleLength = 16;
>        SMPMemCpy(pCopiedDeviceHandle, pDeviceHandleString->GetString(), DeviceHandleLength);
324,326c326,331
<  SMPInt8* pCopiedDeviceHandle = NULL;
<  SMPString* pDeviceHandleString = new SMPString((SMPChar*)pDeviceHandle);
<  pCopiedDeviceHandle = (SMPInt8*)pDeviceHandleString->GetString();
---
>  SMPInt8* pCopiedDeviceHandle = (SMPInt8*)malloc(16);
>  SMPMemSet(pCopiedDeviceHandle, 0, 16);
>  SMPString* pDeviceHandleString = new SMPString((SMPChar*)pDeviceHandle);
>  SMPSize  DeviceHandleLength = pDeviceHandleString->GetStringLength();
>  if (DeviceHandleLength > 16) DeviceHandleLength = 16;
>  SMPMemCpy(pCopiedDeviceHandle, pDeviceHandleString->GetString(), DeviceHandleLength);


<ex-real> `stack buffer-overflow` was detected by ASan:
==239== ERROR: AddressSanitizer: stack-buffer-overflow on address 0xab43da68
at pc 0xb4e023f4 bp 0xab43d644 sp 0xab43d224
READ of size 4 at 0xab43da68 thread T45 (AppInitializer)
    #0 0xb4e023f3 (/mtd_exe/lib/libasan.so.0+0xe3f3)
    #1 0x143e08f (/mtd_exe/exeAPP+0x143e08f)
    #2 0x144a547 (/mtd_exe/exeAPP+0x144a547)
    #3 0x1448cc3 (/mtd_exe/exeAPP+0x1448cc3)
    #4 0x5fb86bb (/mtd_exe/exeAPP+0x5fb86bb)
    #5 0x5fb756f (/mtd_exe/exeAPP+0x5fb756f)
    #6 0x4be0477 (/mtd_exe/exeAPP+0x4be0477)
    #7 0x4b646bf (/mtd_exe/exeAPP+0x4b646bf)
    #8 0x5b3fa7f (/mtd_exe/exeAPP+0x5b3fa7f)
    #9 0x5ad6ceb (/mtd_exe/exeAPP+0x5ad6ceb)
    #10 0x5ae930b (/mtd_exe/exeAPP+0x5ae930b)
    #11 0x4be7143 (/mtd_exe/exeAPP+0x4be7143)
    #12 0x4b64b3f (/mtd_exe/exeAPP+0x4b64b3f)
    #13 0x46eb767 (/mtd_exe/exeAPP+0x46eb767)
    #14 0x1433bbf (/mtd_exe/exeAPP+0x1433bbf)
    #15 0x1438c4f (/mtd_exe/exeAPP+0x1438c4f)
    #16 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #17 0xae2a00f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
==239== T43 exited
==239== T43 TSDDtor
Address 0xab43da68 is located at offset 416 in frame <ResetComponentsProfile> of T45's stack:
  This frame has 55 object(s):
    [32, 33) 'wlanEncryptionInfo'
    [96, 97) 'WFDSetupInfo'
    [160, 161) 'wlanSetupDone'
    [224, 225) 'moip_auto_start'
    [288, 289) 'moip_update_available'
    [352, 353) 'ucRecognitionGestureHelpbar'
    [416, 417) 'ucCamPosition'
    [480, 481) 'val'
    [544, 545) 'ucRecognitionVoice'
    [608, 609) 'ucRecognitionVoiceTVWakeOn'
    [672, 673) 'ucRecognitionVoiceHelpbar'
    [736, 737) 'ucRecognitionGesture'
    [800, 801) 'ucRecognitionFace'
    [864, 865) 'ucRecognitionFirstRecognized'
    [928, 929) 'b3DGlassPaired'
    [992, 993) 'eDTVStatus'
    [1056, 1057) 'MasterNum'
    [1120, 1121) 'TVNumForSlave'
    [1184, 1188) 'cec_enable_option'
    [1248, 1252) 'cec_auto_standby_option'
    [1312, 1316) 'cec_receiver_auto_turn_on_option'
    [1376, 1380) 'nRecognitionVoiceLanguage'
    [1440, 1444) 'nRecognitionVoiceMagicWord'
    [1504, 1508) 'nRecognitionGestureHandTheme'
    [1568, 1572) 'nRecognitionGesturePointerSpeed'
    [1632, 1636) 'nSensitivityLevel'
    [1696, 1700) 'nShopMode'
    [1760, 1764) 'nHowlingLevel'
    [1824, 1828) 'uiVFreq'
    [1888, 1892) 'nRecognitionFrequencey'
    [1952, 1956) 'nEnableTTS'
    [2016, 2020) 'nTTSSpeaker'
    [2080, 2084) 'nVolume'
    [2144, 2148) 'nSpeed'
    [2208, 2212) 'nEnableTTS_tv'
    [2272, 2276) 'nTTSSpeaker_tv'
    [2336, 2340) 'nVolume_tv'
    [2400, 2404) 'nSpeed_tv'
    [2464, 2468) 'nVoiceGuideUserLevel'
    [2528, 2532) 'nHandWlang'
    [2592, 2596) 'tValue'
    [2656, 2662) 'TVAddrForSlave'
    [2720, 2728) 'pms'
    [2784, 2812) 'NetworkDescriptor'
    [2848, 2876) 'NetworkDescriptor_2'
    [2912, 2940) 'WFDDescriptor'
    [2976, 3004) 'CEC_Descriptor'
    [3040, 3068) 'MoIP_Descriptor'
    [3104, 3132) 'MotionApp_Descriptor'
    [3168, 3196) 'VoiceApp_Descriptor'
    [3232, 3260) 'RecognitionMW_Descriptor'
    [3296, 3324) 'BluetoothDescriptor_3DDTVStatus'
    [3360, 3388) 'BluetoothDescriptor_MasterNum'
    [3424, 3452) 'BluetoothDescriptor_tvAddrForSlave'
    [3488, 3516) 'BluetoothDescriptor_MasterNumForSlave'
HINT: this may be a false positive if your program uses some custom stack unwind mechanism or swapcontext
      (longjmp and C++ exceptions *are* supported)
Shadow bytes around the buggy address:
  0x35687af0: f3 f3 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x35687b00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x35687b10: 00 00 00 00 00 00 00 00 00 f1 f1 f1 f1 01 f4 f4
  0x35687b20: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
  0x35687b30: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
=>0x35687b40: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2[01]f4 f4
  0x35687b50: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
  0x35687b60: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
  0x35687b70: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
  0x35687b80: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
  0x35687b90: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4


Here is the backtrace analyzed with addr2line tool:

#0 0xb4e023f3 (/mtd_exe/lib/libasan.so.0+0xe3f3)
`__interceptor_memcpy` 
/build/tools/v4/sources/gcc_1/libsanitizer/asan/asan_interceptors.cc:288
288     ASAN_READ_RANGE(from, size);
289     ASAN_WRITE_RANGE(to, size);

#1 0x143e08f (/mtd_exe/exeAPP+0x143e08f)
CSP0400::PCParcel::Write(void const*, int)
.../PCParcel.cpp:340

#2 0x144a547 (/mtd_exe/exeAPP+0x144a547)
CCPMSProxy::Set(int profile, unsigned char const* pBuffer, unsigned int size)
/home2/m.guseva/TV_2014/DTV/BP_MW/BP_CommSS/Src/PMS/PROXY/PMSProxy.cpp:177
176         CHECK_MARSHALLING(data_parcel.WriteInt32(size));
177         CHECK_MARSHALLING(data_parcel.Write(pBuffer, size));
178

#3 0x1448cc3 (/mtd_exe/exeAPP+0x1448cc3)
CCPMS::Set(int profile, unsigned char const* pBuffer, unsigned int size)
/home2/m.guseva/TV_2014/DTV/BP_MW/BP_CommSS/Src/PMS/COMMON/PMS.cpp:75
 73 {
 74         CC_INT_ASSERT(m_pImp);
 75         return m_pImp->Set(profile, pBuffer, size);
 76 }

#4 0x5fb86bb (/mtd_exe/exeAPP+0x5fb86bb)
CAPCNCTaskManager::ResetComponentsProfile()
.../APCNCTaskConfig.cpp:1151
1150  unsigned char ucCamPosition = 0;
1151  if (pms.Set(PROFILE_MOTIONAPP_CAMERA_POSITION, &ucCamPosition,
               PROFILE_SIZE_MOTIONAPP_CAMERA_POSITION) == false)

So the issue happened in memcpy() function. The proposed workaround:

$diff APCNCTaskConfig.cpp.orig APCNCTaskConfig.cpp
1151c1151
<       if (pms.Set(PROFILE_MOTIONAPP_CAMERA_POSITION, &ucCamPosition,
               PROFILE_SIZE_MOTIONAPP_CAMERA_POSITION) == false)
---
>       if (pms.Set(PROFILE_MOTIONAPP_CAMERA_POSITION, &ucCamPosition,
               sizeof(ucCamPosition)) == false)


<ex-real> `stack buffer-overflow` was detected by ASan:
==283== ERROR: AddressSanitizer: heap-buffer-overflow on address 0x7c25d990
at pc 0xb4e02520 bp 0x989fe85c sp 0x989fe43c
READ of size 4 at 0x7c25d990 thread T145 (CnS Server Mana)
==283== SetCurrent: 0x9c230000 for thread 0x641ff430
==283== T503: stack [0x64100000,0x64200000) size 0x100000; local=0x641fed74
0x7d519d53 is located 0 bytes to the right of 3-byte region [0x7d519d50,0x7d519d53)
allocated by thread T144 (CnS Server Mana) here:
    #0 0xb4e0251f (/mtd_exe/lib/libasan.so.0+0xe51f)
    #1 0xa8cde413 (/mtd_exe/Comp_LIB/libjson.so+0x20413)
    #2 0xa8bf81f7 (/mtd_exe/lib/libstdc++.so.6.0.17+0xa31f7)
    #3 0x7cb85c3 (/mtd_exe/exeAPP+0x7cb85c3)
    #4 0x7cb8803 (/mtd_exe/exeAPP+0x7cb8803)
    #5 0x7c73cbf (/mtd_exe/exeAPP+0x7c73cbf)
    #6 0x7c8e13f (/mtd_exe/exeAPP+0x7c8e13f)
    #7 0x7c1d60f (/mtd_exe/exeAPP+0x7c1d60f)
    #8 0x7c1e74b (/mtd_exe/exeAPP+0x7c1e74b)
    #9 0x158c3db (/mtd_exe/exeAPP+0x158c3db)
    #10 0x158c2d3 (/mtd_exe/exeAPP+0x158c2d3)
    #11 0x159133f (/mtd_exe/exeAPP+0x159133f)
    #12 0xb4e11397 (/mtd_exe/lib/libasan.so.0+0x1d397)
    #13 0xa93070f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
Shadow bytes around the buggy address:
  0x2faa3350: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fd fd
  0x2faa3360: fa fa fa fa fa fa fa fa fa fa fd fd fa fa fd fd
  0x2faa3370: fa fa fd fd fa fa fd fa fa fa fd fa fa fa fa fa
  0x2faa3380: fa fa fd fd fa fa fa fa fa fa fd fd fa fa fd fd
  0x2faa3390: fa fa fd fd fa fa fd fd fa fa fd fd fa fa fd fd
=>0x2faa33a0: fa fa fd fd fa fa 00 04 fa fa[03]fa fa fa fa fa
  0x2faa33b0: fa fa fd fa fa fa fd fd fa fa fa fa fa fa fd fd
  0x2faa33c0: fa fa fa fa fa fa fd fd fa fa fa fa fa fa fa fa
  0x2faa33d0: fa fa fa fa fa fa fd fa fa fa fd fd fa fa fd fa
  0x2faa33e0: fa fa fd fd fa fa fd fa fa fa fd fd fa fa fa fa
  0x2faa33f0: fa fa fd fa fa fa fd fd fa fa fd fa fa fa fd fd

Here is the backtrace analyzed with addr2line tool:

#0 0xb4e0251f (/mtd_exe/lib/libasan.so.0+0xe51f)
`__interceptor_memcpy`
/fox-p/vd47_a15/toolchain/build.arm.cortex-a15/sources/gcc_1/libsanitizer/
asan/asan_interceptors.cc:288
288     ASAN_READ_RANGE(from, size);
289     ASAN_WRITE_RANGE(to, size);

#1 0xa8cde413 (/mtd_exe/Comp_LIB/libjson.so+0x20413)
char* std::string::_S_construct<char const*>(char const*, char const*, std::allocator<char> const&, std::forward_iterator_tag)
??:?

#2 0xa8bf81f7 (/mtd_exe/lib/libstdc++.so.6.0.17+0xa31f7)
_S_construct_aux<const char*>
/fox-p/vd47_a15/toolchain/build/obj/gcc_final/arm-v7a15v4r3-linux-gnueabi/
libstdc++-v3/include/bits/basic_string.h:1722

#3 0x7cb85c3 (/mtd_exe/exeAPP+0x7cb85c3)
CCNSVTP::PrintVTPLocal(CCNSVTP::CCNSVTP_TYPE type, char* str)
/GOLFP_2014/AP_MM/AP_ConversationApp/ClientAgent/Src/CnSProfile.cpp:416
414         if (NULL != str)^M
415         {^M
416                 std::string str_text = (std::string)str;^M

#4 0x7cb8803 (/mtd_exe/exeAPP+0x7cb8803)
CCNSVTP::PrintVTP(CCNSVTP::CCNSVTP_TYPE type, char * str, int bool_result)
/GOLFP_2014/AP_MM/AP_ConversationApp/ClientAgent/Src/CnSProfile.cpp:241
241   PrintVTPLocal(VTP_CONVERSATION_STATE_DATA,str);^M
242   break;^M


#5 0x7c73cbf (/mtd_exe/exeAPP+0x7c73cbf)
CCnSParsePacket::t_ParseConnectionInfo(std::string&)
/GOLFP_2014/AP_MM/AP_ConversationApp/ClientAgent/Src/CnSParsePacket.cpp:639
635   char * pChar = NULL ;
636   int length = stConfServer.szStat.size();
637   pChar = new char[length+1];
638  ::strncpy(pChar,stConfServer.szStat.c_str(),length);
639  CCNSVTP::GetInstance()->PrintVTP(CCNSVTP::CCNSVTP_TYPE_STATE_DATA,pChar,0);

*cxx-strncpy-issue*
According the strncpy POSIX specification:

char *strncpy(char *restrict s1, const char *restrict s2, size_t n);

If there is no null byte in the first n bytes of the array pointed to by s2,
   the result will not be null-terminated.

So the pChar array actually is not null terminated. The proposed fix is:

$ diff CnSParsePacket.cpp.orig CnSParsePacket.cpp
638c638
<  ::strncpy(pChar,stConfServer.szStat.c_str(),length);
---
>  ::strncpy(pChar,stConfServer.szStat.c_str(),length+1);


={============================================================================
*kt_linux_tool_100* tool-asan tool-kasan

Preface

Kernel address sanitizer is a solution suitable to catch bugs known as ?use
after free?, ?buffer-overflow?, ?use of non initialized memory?, etc. This
system software helps to inspect kernel code dynamically during its
execution. Testing of VDLinux kernel with KASAN has showed that it is a
good and efficient tool that make it possible to find really serious bugs in
kernel code (including Linux kernel generic functions).
The scope of issues that can be detected is much higher than in other
memory checking solutions, the performance is also a good side of KASAN
[1].
So, use of the tool can greatly help developers to fix most of mistakes made
during their projects and prepare the solutions to be integrated in the final
product.

Now KASAN supports a number of checkers: stack checker, SLUB checker, vmalloc checker and buddy-level checker.

Preparation
To check the kernel with KASAN you must perform a number of simple
preliminary steps (KASAN patches must be applied before the first step of the
following list):
1. Enable CONFIG_SLUB_DEBUG option in the kernel ?.config? file
(KASAN can work wihout the option, but it is highly recommended to
run KASAN with slub debug functionality enabled);
2. Enable CONFIG_KASAN option to turn on basic KASAN support in the
kernel;
3. Enable CONFIG_KASAN_SLUB option if SLUB-related analysis is
necessary;
4. Enable CONFIG_KASAN_VMALLOC option if vmalloc-related analysis is
necessary;
5. Enable CONFIG_KASAN_STACK option to turn on stack issues detection
6. Enable CONFIG_KASAN_GLOBALS option for usage of global variables
checker;
7. Enable CONFIG_KASAN_UAR option to use use-after-return checker;
8. Build the kernel and modules with appropriate toolchain which
supports KASAN instrumentation options
(arm-v7a15v5r2-linux-gnueabi).
Note
If you want to check the whole kernel enable the option
CONFIG_KASAN_SANITIZE_ALL. In other cases see the section ?Usage of
KASAN with Separate Kernel Files?.
KASAN-related build options enable all necessary toolchain flags. As a result
after these steps KASAN-instrumented kernel will be ready for debugging.
Note
On Orsay platform kernel image can be larger than available mmc partition,
in this case compressed kernel image should be used. An additional option is
required to be disabled: CONFIG_AUTO_ZRELADDR.
To run user space software with instrumented kernel it is necessary to rebuild
all kernel modules to instrument them with KASAN, because stack
instrumentation changes size of thread_info structure and this change affects
all the system. If some modules are not rebuilt the system cannot be
started properly.
Separate Instrumentation
It is possible to instrument only loads or stores by usage of special toolchain
parameters. The syntax for this parameters is:
--param <parameter name>=<value>
Where <parameter name> in this case can be asan-instrument-reads and
asan-instrument-writes and <value> is ?0? or ?1?. To enable disable
instrumentation of writes, for example, it is enough to provide an additional
parameter to the compiler:
--param asan-instrument-writes=0
To disable instrumentation of reads or writes in the context of the kernel it is
necessary to provide additional CFLAGS for the whole kernel or for separate
modules.
Usage of KASAN with Separate Kernel Files
To instrument only a set of files with KASAN the option
CONFIG_KASAN_SANITIZE_ALL should be disabled. For each file that should
be instrumented corresponding Makefile should include an entry of the
following format:
KASAN_SANITIZE_<object file name>.o := y
<object file name> here is a file name of the object file to be instrumented.

Example of Analysis
When all preliminary work is done it is possible to check the kernel with
KASAN. Kernel should be flashed to MMC using SERET ordinary way,
modules should be changed to instrumented modules (see the point 7 in
Preparation part of the document).
KASAN analysis is an automatic activity, no assistance of human is required.
For every detected issue KASAN prints a report which has typical structure
regardless of type of the issue.

Typical report has the following format:
AddressSanitizer: buffer overflow in kasan_do_bo_kmalloc+0x58/0x7c at addr
e07d0012
=============================================================================
BUG kmalloc-64 (Tainted: G B ): kasan error
-----------------------------------------------------------------------------
INFO: Allocated in kasan_do_bo_kmalloc+0x34/0x7c age=0 cpu=0 pid=1
        kasan_do_bo_kmalloc+0x34/0x7c
        kasan_tests_init+0x10/0x44
        do_one_initcall+0x11c/0x2bc
        kernel_init_freeable+0x29c/0x350
        kernel_init+0x14/0xf8
        ret_from_fork+0x14/0x3c
INFO: Freed in kasan_tests_init+0xc/0x44 age=1 cpu=0 pid=1
        do_one_initcall+0x11c/0x2bc
        kernel_init_freeable+0x29c/0x350
        kernel_init+0x14/0xf8
        ret_from_fork+0x14/0x3c
INFO: Slab 0xe1719a00 objects=16 used=1 fp=0xe07d0100 flags=0x0080
INFO: Object 0xe07d0000 @offset=0 fp=0xe07d0f00
Object e07d0000: 00 0f 7d e0 6e 5f 74 65 73 74 73 5f 69 6e 69
74 ..}.n_tests_init Object e07d0010: 00 5f 63 61 63 68 65 00 00 00 00 00 00 00
00 00 ._cache.........
Object e07d0020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 ................
Object e07d0030: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 ................
Padding e07d00e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 ................
Padding e07d00f0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 ................
CPU: 0 PID: 1 Comm: swapper/0 Tainted: G B 3.16.0-rc1+ #129
[<8001a7c0>] (unwind_backtrace) from [<80015e6c>] (show_stack+0x14/0x20)
[<80015e6c>] (show_stack) from [<806bd594>] (dump_stack+0xa4/0xcc) [<806bd594>]
(dump_stack) from [<8014de0c>] (kasan_report_error+0x300/0x364) [<8014de0c>]
(kasan_report_error) from [<8014d3a0>] (check_memory_region+0x16c/0x234)
[<8014d3a0>] (check_memory_region) from [<8014df4c>]
(kasan_do_bo_kmalloc+0x58/0x7c) [<8014df4c>] (kasan_do_bo_kmalloc) from
[<808515b8>] (kasan_tests_init+0x10/0x44) [<808515b8>] (kasan_tests_init) from
[<80008d48>] (do_one_initcall+0x11c/0x2bc) [<80008d48>] (do_one_initcall) from
[<8083e0c8>] (kernel_init_freeable+0x29c/0x350)
[<8083e0c8>] (kernel_init_freeable) from [<806b6a30>] (kernel_init+0x14/0xf8)
[<806b6a30>] (kernel_init) from [<800109b8>] (ret_from_fork+0x14/0x3c) Write of
size 1 by thread T1:
Memory state around the buggy address:
e07cfd80: fd fd 00 00 00 00 00 00 00 00 00 00 00 fc fc fc
e07cfe00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
e07cfe80: fc 00 00 00 00 00 00 00 00 00 00 00 fc fc fc fc
e07cff00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
e07cff80: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
>e07d0000: 00 00 01 fc fc fc fc fc fc fc fc fc fc fc fc fc
^
e07d0080: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
e07d0100: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
e07d0180: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
e07d0200: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
e07d0280: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
==================================================================

This listing corresponds to newly detected buffer overflow issue at the
address 0xE07D0012. This log shows allocation/deallocation paths of the
memory, memory state around the address 0xE07D0012 and state of the
shadow memory.
Thus, it is possible to realize what kind of issue was found, which client
allocated the memory and which freed, what call-chain caused the issue and
why KASAN considers this behavior as a bug (shadow memory state helps to
understand the reason).
This shadow memory state shows that seven bytes correspond to redzone and
only one is allocated (because state of shadow memory entry corresponding
to the address is 0x01).
All possible states of shadow memory entries are defined in
?mm/kasan/kasan.h?.

#define KASAN_FREE_PAGE 0xFF /* page was freed */
#define KASAN_PAGE_REDZONE 0xFE /* redzone for kmalloc_large allocations
*/
#define KASAN_SLAB_REDZONE 0xFD /* Slab page redzone, does not belong to
any slub object */
#define KASAN_KMALLOC_REDZONE 0xFC /* redzone inside slub object */
#define KASAN_KMALLOC_FREE 0xFB /* object was freed
(kmem_cache_free/kfree) */
#define KASAN_SLAB_FREE 0xFA /* free slab page */
#define KASAN_SHADOW_GAP 0xF9 /* address belongs to shadow memory */
#define KASAN_VMALLOC_REDZONE 0xF8 /* address belongs to vmalloc guard page
*/
#define KASAN_VMALLOC_FREE 0xF7 /* memory was freed by vfree call */
/* Stack redzones */
#define KASAN_STACK_LEFT 0xF1
#define KASAN_STACK_MID 0xF2
#define KASAN_STACK_RIGHT 0xF3
#define KASAN_STACK_PARTIAL 0xF4

SLUB issues are represented by allocation/deallocation paths, backtrace
(sometimes it is not available), states of memory and shadow memory.
BUDDY (page) issues are represented by ?dump_page out?, backtrace and
shadow memory state.
Stack issues are represented by a backtrace and shadow memory state.
In all cases shadow memory state will be available for analysis.
Note
During development of KASAN 14_VDFuture project's team has found a
number of issues in the very core of the Linux kernel. We think that it is
usable and very effective tool appropriate for kernel engineers with any level
of expertise.
Often it is enough to know the address where the problem is happened, type
of the issue and a backtrace to fix it. Shadow memory helps to understand the
problem little bit deeper.

Examples of Real World Issues
To demonstrate an approach to find issues with KASAN a number of
real-world issues are described. All these issues were fixed and included in
KASAN patch set.
Lets look at log messages generated by KASAN stack checker:
First log:
==================================================================
AddressSanitizer: out of bounds on stack in idr_for_each+0x168/0x1d0 at addr
dfe49d3c
CPU: 0 PID: 128 Comm: fsnotify_mark Not tainted 3.16.0-rc3+ #256
[<8001a530>] (unwind_backtrace) from [<80015bf0>] (show_stack+0x14/0x20)
[<80015bf0>] (show_stack) from [<80699ad8>] (dump_stack+0x90/0xa0)
[<80699ad8>] (dump_stack) from [<8013dff8>] (kasan_report_error+0x158/0x3a4)
[<8013dff8>] (kasan_report_error) from [<8013d764>]
(check_memory_region+0x148/0x210)
[<8013d764>] (check_memory_region) from [<80336628>] (idr_for_each+0x168/0x1d0)
[<80336628>] (idr_for_each) from [<8019e688>]
(inotify_free_group_priv+0x28/0x70)
[<8019e688>] (inotify_free_group_priv) from [<8019b944>]
(fsnotify_final_destroy_group+0x38/0x50)
[<8019b944>] (fsnotify_final_destroy_group) from [<8019c6a0>]
(fsnotify_put_mark+0x50/0x7c)
[<8019c6a0>] (fsnotify_put_mark) from [<8019c894>]
(fsnotify_mark_destroy+0x1c8/0x254)
[<8019c894>] (fsnotify_mark_destroy) from [<8005b7c8>] (kthread+0x19c/0x1bc)
[<8005b7c8>] (kthread) from [<80010838>] (ret_from_fork+0x14/0x3c)
Read of size 4 by thread T128:
Memory state around the buggy address:
dfe49a80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
dfe49b00: 00 00 00 00 f1 f1 f1 f1 00 00 00 00 00 00 00 00
dfe49b80: 00 00 00 00 00 00 00 00 03 f4 f4 f4 f3 f3 f3 f3
dfe49c00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
dfe49c80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
>dfe49d00: 00 00 00 00 f1 f1 f1 f1 00 00 04 f4 f3 f3 f3 f3
^
dfe49d80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
dfe49e00: 00 00 00 00 f1 f1 f1 f1 00 f4 f4 f4 f2 f2 f2 f2
dfe49e80: 00 00 04 f4 f3 f3 f3 f3 00 00 00 00 00 00 00 00
dfe49f00: f1 f1 f1 f1 00 00 00 00 00 04 f4 f4 f3 f3 f3 f3
dfe49f80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
==================================================================

Stack checker has found an out-of-bounds access issue on the stack. The first
one was detected in the function idr_for_each and the second in idr_destroy.
Both logs point to the same address: 0xdfe49d3c. Both logs provide back
traces showing how these functions were invoked. So, the faulting trace is
fsnotify_mark_destroy ? fsnotify_put_mark ? fsnotify_final_destroy_group ?
inotify_free_group_priv ? (idr_for_each/idr_destroy) .
Checking memory state helps to understand why KASAN thinks that this is
out-of-bounds issue: 0xF1 in both cases corresponds to KASAN_STACK_LEFT
redzone, thus it was an access at the left stack boundary.

Lets examine both faulty functions.
The first one:
int idr_for_each(struct idr *idp, int (*fn)(int id, void *p, void *data), void *data)
{
    int n, id, max, error = 0;
    struct idr_layer *p;
    struct idr_layer *pa[MAX_IDR_LEVEL];
    struct idr_layer **paa = &pa[0];
   
    n = idp->layers * IDR_BITS;
   
    p = rcu_dereference_raw(idp->top);
    max = 1 << n;
   
    id = 0;
    while (id < max)
    {
        while (n > 0 && p)
        {
            n -= IDR_BITS;
            *paa++ = p;
            p = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);
        }
        if (p)
        {
            error = fn(id, (void *)p, data);
            if (error)
            break;
        }
        id += 1 << n;
       
        while (n < fls(id))
        {
            n += IDR_BITS;
            p = *--paa;
        }
    }
    return error;
}

The pointer array pa is an evident candidate for out-of-bounds access. After
some analysis of a function it is easy to make sure that both functions access
the array out of bounds. The simplest fix is growing the array on the size of
one pointer and setting paa to point initially to the second (&pa[1]) entry of
the array.

Non-supported Functionality
This time only non-initialized-read checker is not implemented.
?Use-after-return? and global data ?out-of-bounds? checkers were
implemented in KASAN in the latest release, but little bit different to their
user space analogues.


={============================================================================
*kt_linux_tool_000* tool-asan-llvm

http://llvm.org/

The compiler-rt project provides highly tuned implementations of the low-level
code generator support routines like "__fixunsdfdi" and other calls generated
when a target doesn't have a short sequence of native instructions to
implement a core IR operation. 

It also provides implementations of run-time libraries for dynamic testing
tools such as AddressSanitizer, ThreadSanitizer, MemorySanitizer, and
DataFlowSanitizer. 

Not all LLVM projects require LLVM for all use cases. For example compiler-rt
can be built without LLVM, and the compiler-rt sanitizer libraries are used
with GCC.

https://github.com/google/sanitizers/wiki/AddressSanitizerHowToBuild
http://llvm.org/docs/GettingStarted.html#getting-a-modern-host-c-toolchain

Download and install CMake (you'll need at least CMake 2.8.8).

note: 07/10/2016
when runs, errors :
CMake 3.4.3 or higher is required.  You are running version 3.0.2

Get llvm, clang and compiler-rt sources (see above).
Make sure you have a modern C++ toolchain (see above).
Set configuration and `build LLVM`


note:
To use a different GCC toolchain than one installed. When use own gcc, add
the path of latest cmake to the path before running cmake

DO NOT WORK

Once you have a GCC toolchain, configure your build of LLVM to use the new
toolchain for your host compiler and C++ standard library. Because the new
version of libstdc++ is not on the system library search path, you need to
pass extra linker flags so that it can be found at link time (-L) and at
runtime (-rpath). If you are using CMake, this invocation should produce
working binaries:

% mkdir llvm_cmake_build && cd llvm_cmake_build 
or
% mkdir build_x86 && cd build_x86

CC=$HOME/toolchains/bin/gcc CXX=$HOME/toolchains/bin/g++ \
cmake .. -DCMAKE_CXX_LINK_FLAGS="-Wl,-rpath,$HOME/toolchains/lib64 -L$HOME/toolchains/lib64"

If you fail to set rpath, most LLVM binaries will fail on startup with a
message from the loader similar to libstdc++.so.6: version `GLIBCXX_3.4.20'
not found. This means you need to tweak the -rpath linker flag.

# Choose the host compiler
# Choose CMAKE_BUILD_TYPE {Debug, Release}
# Choose LLVM_ENABLE_ASSERTIONS {ON,OFF}
# Choose LLVM_ENABLE_WERROR {ON,OFF}
# Set LLVM_TARGETS_TO_BUILD to X86 to speed up the build
[CC=clang CXX=clang++] cmake -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON [-DLLVM_ENABLE_WERROR=ON] [-DLLVM_TARGETS_TO_BUILD=X86] /path/to/llvm/checkout

CC=$HOME/toolchains/bin/gcc CXX=$HOME/toolchains/bin/g++ \
cmake .. -DCMAKE_CXX_LINK_FLAGS="-Wl,-rpath,$HOME/toolchains/lib64 -L$HOME/toolchains/lib64" -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON -DLLVM_ENABLE_WERROR=ON -DLLVM_TARGETS_TO_BUILD=X86

make check-sanitizer -j12 # build and run sanitizer_common tests
...
[100%] Built target SanitizerUnitTests
[100%] Built target SanitizerLintCheck
[100%] Running sanitizer_common tests
Testing Time: 79.34s
  Expected Passes    : 376
  Expected Failures  : 11
  Unsupported Tests  : 93


{x86}
cmake .. -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON -DLLVM_ENABLE_WERROR=ON -DLLVM_TARGETS_TO_BUILD=X86
cmake --trace ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON -DLLVM_ENABLE_WERROR=ON -DLLVM_TARGETS_TO_BUILD=X86

builds fine when increase swap space

$ more use-after-free.c 
#include <stdlib.h>
int main() {
  char *x = (char*)malloc(10 * sizeof(char*));
  free(x);
  return x[5];
}

~/si-logs/asn-llvm/build_x86$ ./bin/clang -fsanitize=address -O1 -fno-omit-frame-pointer -g use-after-free.c 
/usr/bin/ld: cannot find /home/kyoupark/si-logs/asn-llvm/build_x86/bin/../lib/clang/4.0.0/lib/linux/libclang_rt.asan-i686.a: No such file or directory
clang-4.0: error: linker command failed with exit code 1 (use -v to see invocation)

~/si-logs/asn-llvm/build_x86$ ./bin/clang -O1 -fno-omit-frame-pointer -g use-after-free.c 
kyoupark@kit-debian:~/si-logs/asn-llvm/build_x86$ ls

../lib/clang/4.0.0/lib/linux/libclang_rt.asan-i686.a
There are files which has 386 namings but why 686? Looks like that asan use
686 only.

~/si-logs/asn-llvm/llvm/projects/compiler-rt/lib/asan$
scripts/asan_device_setup
98:        _ARCH=i686

So makes a sym links to 686 files for 383 and asn works fine.


{mips}
cmake ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON -DLLVM_ENABLE_WERROR=ON -DLLVM_TARGETS_TO_BUILD=Mips -DLLVM_TARGET_ARCH=Mips

-- LLVM host triple: i686-pc-linux-gnu
-- LLVM default target triple: i686-pc-linux-gnu
-- Building with -fPIC
-- Targeting Mips

[ 98%] Built target gtest
Scanning dependencies of target Asan-i386-with-calls-Test
[ 98%] Generating ASAN_INST_TEST_OBJECTS.gtest-all.cc.i386-with-calls.o
clang (LLVM option parsing): Unknown command line argument '-asan-instrument-assembly'.  Try: 'clang (LLVM option parsing) -help'
clang (LLVM option parsing): Did you mean '-asan-instrument-atomics'?
projects/compiler-rt/lib/asan/tests/CMakeFiles/Asan-i386-with-calls-Test.dir/build.make:76: recipe for target 'projects/compiler-rt/lib/asan/tests/ASAN_INST_TEST_OBJECTS.gtest-all.cc.i386-with-calls.o' failed
make[3]: *** [projects/compiler-rt/lib/asan/tests/ASAN_INST_TEST_OBJECTS.gtest-all.cc.i386-with-calls.o] Error 1
CMakeFiles/Makefile2:8586: recipe for target 'projects/compiler-rt/lib/asan/tests/CMakeFiles/Asan-i386-with-calls-Test.dir/all' failed
make[2]: *** [projects/compiler-rt/lib/asan/tests/CMakeFiles/Asan-i386-with-calls-Test.dir/all] Error 2
CMakeFiles/Makefile2:9338: recipe for target 'projects/compiler-rt/test/asan/CMakeFiles/check-asan.dir/rule' failed
make[1]: *** [projects/compiler-rt/test/asan/CMakeFiles/check-asan.dir/rule] Error 2
Makefile:2836: recipe for target 'check-asan' failed
make: *** [check-asan] Error 2

Well, make clang is fine.

~/si-logs/asn-llvm/build_mips$ ./bin/clang -O1 -fno-omit-frame-pointer -g use-after-free.c 
error: unable to create target: 'No available targets are compatible with this triple.'
1 error generated.

However, it shows

~/si-logs/asn-llvm/build_mips$ ./bin/clang --version
clang version 4.0.0 (trunk 283488)
`Target: i686-pc-linux-gnu`
Thread model: posix
InstalledDir: /home/kyoupark/si-logs/asn-llvm/build_mips/./bin


si-logs/asn-llvm/build_mips$ ./bin/clang --target=mips -g use-after-free.c
In file included from use-after-free.c:1:
In file included from /usr/include/stdlib.h:24:
/usr/include/features.h:374:12: fatal error: 'sys/cdefs.h' file not found
#  include <sys/cdefs.h>
           ^
1 error generated.


{how-to-build-on-arms}
How To Build On ARM
http://llvm.org/docs/HowToBuildOnARM.html

note:
This is to build LLVM/clang to run ARM board. Not to cross-compile.

<back-end>
It's also a lot quicker to only build the relevant back-ends (ARM and
    AArch64), since it’s very unlikely that you'll use an ARM board to
cross-compile to other arches. If you're running Compiler-RT tests, also
include the x86 back-end, or some tests will fail.

cmake $LLVM_SRC_DIR -DCMAKE_BUILD_TYPE=Release \
                    -DLLVM_TARGETS_TO_BUILD="ARM;X86;AArch64"


{how-to-cross-compile}
How To Cross-Compile Clang/LLVM using Clang/LLVM
http://llvm.org/docs/HowToCrossCompileLLVM.html

Cross-compiling from an x86_64 host (most Intel and AMD chips nowadays) to a
hard-float ARM target (most ARM targets nowadays).

The packages you’ll need are:

        cmake
        ninja-build (from backports in Ubuntu)
        gcc-4.7-arm-linux-gnueabihf
        gcc-4.7-multilib-arm-linux-gnueabihf
        binutils-arm-linux-gnueabihf
        libgcc1-armhf-cross
        libsfgcc1-armhf-cross
        libstdc++6-armhf-cross
        libstdc++6-4.7-dev-armhf-cross


The CMake options you need to add are:

        -DCMAKE_CROSSCOMPILING=True
        -DCMAKE_INSTALL_PREFIX=<install-dir>
        -DLLVM_TABLEGEN=<path-to-host-bin>/llvm-tblgen
        -DCLANG_TABLEGEN=<path-to-host-bin>/clang-tblgen
        -DLLVM_DEFAULT_TARGET_TRIPLE=arm-linux-gnueabihf
        -DLLVM_TARGET_ARCH=ARM
        -DLLVM_TARGETS_TO_BUILD=ARM


From http://llvm.org/docs/CMake.html

LLVM_TARGETS_TO_BUILD:STRING
    Semicolon-separated list of targets to build, or all for building all
    targets. Case-sensitive. Defaults to all. Example:
    -DLLVM_TARGETS_TO_BUILD="X86;PowerPC".

LLVM_TARGET_ARCH:STRING
    LLVM target to use for native code generation. This is required for JIT
    generation. It defaults to “host”, meaning that it shall pick the
    architecture of the machine where LLVM is being built. If you are
    cross-compiling, set it to the target architecture name.

LLVM_TABLEGEN:STRING
    Full path to a native TableGen executable (usually named llvm-tblgen).
    `This is intended for cross-compiling`: if the user sets this variable, no
    native TableGen will be created.


{supported-arch}
http://llvm.org/docs/doxygen/html/Triple_8h_source.html):


For clang:
/home/NDS-UK/kyoupark/asn/llvm/projects/compiler-rt/lib/asan


={============================================================================
*kt_linux_tool_000* tool-asan-gcc

The tool works on x86, ARM, MIPS (both 32- and 64-bit versions of all
    architectures), PowerPC64. The supported operation systems are Linux,
Darwin (OS X and iOS Simulator), FreeBSD, Android:

AddressSanitizer, a fast memory error detector, has been added and can be
enabled via -fsanitize=address. Memory access instructions will be
instrumented to detect heap-, stack-, and global-buffer overflow as well as
use-after-free bugs. To get nicer stacktraces, use -fno-omit-frame-pointer.
The AddressSanitizer is available on IA-32/x86-64/x32/PowerPC/PowerPC64
GNU/Linux and on x86-64 Darwin.

AddressSanitizer is based on `compiler instrumentation` and directly-mapped
shadow memory. 

  AddressSanitizer is a part of LLVM starting with version 3.1 and a part of
  GCC starting with version `4.8`


<when-gcc-do-not-have-asan>
kyoupark@ukstbuild2:~$ ~/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin/aarch64-buildroot-linux-musl-gcc -fsanitize=address x.c
x.c:1:0: warning: -fsanitize=address not supported for this target
 #include <stdio.h>
 ^
x.c: In function ‘main’:
x.c:9:14: warning: incompatible implicit declaration of built-in function ‘malloc’
     int *x = malloc(10*sizeof(int));
              ^
x.c:10:5: warning: incompatible implicit declaration of built-in function ‘free’
     free(x);
     ^
/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2015.08.1-for-arm64/output/host/usr/bin/../lib/gcc/aarch64-buildroot-linux-musl/4.9.4/../../../../aarch64-buildroot-linux-musl/bin/ld: cannot find libasan_preinit.o: No such file or directory
/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2015.08.1-for-arm64/output/host/usr/bin/../lib/gcc/aarch64-buildroot-linux-musl/4.9.4/../../../../aarch64-buildroot-linux-musl/bin/ld: cannot find -lasan
collect2: error: ld returned 1 exit status


{for-x86}
ASAN gets built as a part of gcc build and as a shared library.

https://gcc.gnu.org/gcc-4.8/changes.html

// vm
On debian VM, and worked fine. This is gcc installed by default.

kyoupark@kit-debian:~$ gcc --version
gcc (Debian 4.9.2-10) 4.9.2

// build server
Has 4.4.7 which do not have ASAN. So have 4.8.2 built and ASAN works when
set a library path as:

kyoupark@ukstbuild2:~/toolchains/bin$ ./a.out
./a.out: error while loading shared libraries: libasan.so.0: cannot open shared object file: No such file or directory

$ ./gcc -fsanitize=address -Wl,-rpath=/home/NDS-UK/kyoupark/toolchains/lib64 uaf.c


{for-aarch64}
Enabled C++ support, used musl since there is no selection for uclibc.

kyoupark@ukstbuild2:~/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin$ ./aarch64-buildroot-linux-musl-gcc --version
aarch64-buildroot-linux-musl-gcc.br_real (Buildroot 2016.08.1) 4.9.4
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

toolchain has built but failed to compile with asan. Why?

// home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/package/gcc/gcc.mk
// since not happy with musl.

# libsanitizer requires wordexp, not in default uClibc config. Also
# doesn't build properly with musl.
ifeq ($(BR2_TOOLCHAIN_BUILDROOT_UCLIBC)$(BR2_TOOLCHAIN_BUILDROOT_MUSL),y)
HOST_GCC_COMMON_CONF_OPTS += --disable-libsanitizer
endif


Changes to use glibc, have it built but still not have asan enabled. 

note:
1. glibc gets built which is different when do on debian VM.
2. Since not use UCLIBC or MUSL, not set "--disable-libsanitizer" and will
have config checking:

// checking for libsanitizer support... no


// full command to run config. note that cd first and when use "./configure
// ...", do not work.

// no changes
PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" CXXFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null ./configure --prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr" --sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/etc" --localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/var" --enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html --disable-doc --disable-docs --disable-documentation --disable-debug --with-xmlto=no --with-fop=no --disable-dependency-tracking  --target=aarch64-buildroot-linux-gnu --with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/aarch64-buildroot-linux-gnu/sysroot --disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib --with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --with-pkgversion="Buildroot 2016.08.1" --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath --enable-tls --disable-libmudflap --enable-threads --with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --without-isl --without-cloog --disable-decimal-float --enable-languages=c --disable-shared --without-headers --disable-threads --with-newlib --disable-largefile --disable-nls 

// use --enable-libsanitizer
PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" CXXFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null ./configure --prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr" --sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/etc" --localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/var" --enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html --disable-doc --disable-docs --disable-documentation --disable-debug --with-xmlto=no --with-fop=no --disable-dependency-tracking  --target=aarch64-buildroot-linux-gnu --with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/aarch64-buildroot-linux-gnu/sysroot --disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib --with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --with-pkgversion="Buildroot 2016.08.1" --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath --enable-tls --disable-libmudflap --enable-threads --with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --without-isl --without-cloog --disable-decimal-float --enable-languages=c --disable-shared --without-headers --disable-threads --with-newlib --disable-largefile --disable-nls --enable-libsanitizer

PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin"
AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm"
CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc"
CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy"
RANLIB="/usr/bin/ranlib"
CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include"
CFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include"
CXXFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include"
LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib
-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib
-Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib"
PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1
PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin/pkg-config"
PKG_CONFIG_SYSROOT_DIR="/"
PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/share/pkgconfig"
INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include"
LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib
-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib
-Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib"
MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE
-D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE
-D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null
./configure
--prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr"
--sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/etc"
--localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/var"
--enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html
--disable-doc --disable-docs --disable-documentation --disable-debug
--with-xmlto=no --with-fop=no --disable-dependency-tracking
--target=aarch64-buildroot-linux-gnu
--with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/aarch64-buildroot-linux-gnu/sysroot
--disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib
--with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr
--with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr
--with-pkgversion="Buildroot 2016.08.1"
--with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath --enable-tls
--disable-libmudflap --enable-threads
--with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr
--without-isl --without-cloog --disable-decimal-float --enable-languages=c
--disable-shared --without-headers --disable-threads --with-newlib
--disable-largefile --disable-nls


Build fails on host-gcc-final 4.9.4


{for-mips-with-asn}
https://gcc.gnu.org/ml/gcc/2013-10/threads.html#00253

/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/libsanitizer/configure.tgt

I'm afraid GCC doesn't support ASan for MIPS even now on trunk:

$ cat libsanitizer/configure.tgt

case "${target}" in
  x86_64-*-linux* | i?86-*-linux*)
        if test x$ac_cv_sizeof_void_p = x8; then
                TSAN_SUPPORTED=yes
                LSAN_SUPPORTED=yes
                TSAN_TARGET_DEPENDENT_OBJECTS=tsan_rtl_amd64.lo
        fi
        ;;
  powerpc*-*-linux*)
        ;;
  sparc*-*-linux*)
        ;;
  arm*-*-linux*)
        ;;
  aarch64*-*-linux*)
        if test x$ac_cv_sizeof_void_p = x8; then
                TSAN_SUPPORTED=yes
                LSAN_SUPPORTED=yes
                TSAN_TARGET_DEPENDENT_OBJECTS=tsan_rtl_aarch64.lo
        fi
        ;;
  x86_64-*-darwin[1]* | i?86-*-darwin[1]*)
        TSAN_SUPPORTED=no
        ;;
  *)
        UNSUPPORTED=1
        ;;
esac


// libsanitizer/configure.tgt
  mips*-*-linux*)
	TSAN_SUPPORTED=yes
	LSAN_SUPPORTED=yes
	;;

For clang:
/home/NDS-UK/kyoupark/asn/llvm/projects/compiler-rt/lib/asan


<changes-to-buildroot>
//home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/package/gcc/gcc.mk

# libsanitizer requires wordexp, not in default uClibc config. Also
# doesn't build properly with musl.
ifeq ($(BR2_TOOLCHAIN_BUILDROOT_UCLIBC)$(BR2_TOOLCHAIN_BUILDROOT_MUSL),y)
HOST_GCC_COMMON_CONF_OPTS += --disable-libsanitizer
endif

changes to enable ASAN when use uclibc:
ifeq ($(BR2_TOOLCHAIN_BUILDROOT_UCLIBC)$(BR2_TOOLCHAIN_BUILDROOT_MUSL),y)
HOST_GCC_COMMON_CONF_OPTS += --enable-libsanitizer
endif


//home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2015.08.1-for-mips-w-asn/package/uclibc/uClibc-ng.config

add this to to enalbe wordexp:
UCLIBC_HAS_WORDEXP=y

note:
Use $make clean all for buildroot rebuild


These changes ./configure param as:

 `--enable-libsanitizer`

>>> host-gcc-initial 4.9.4 Configuring
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build
ln -sf ../configure /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build/configure

(cd /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/build/host-gcc-initial-4.9.4/build && rm -rf config.cache; 
 
PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include" CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include" CXXFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null ./configure --prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr" --sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/etc" --localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/var" --enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html --disable-doc --disable-docs --disable-documentation --disable-debug --with-xmlto=no --with-fop=no --disable-dependency-tracking  --target=mips-buildroot-linux-uclibc --with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/mips-buildroot-linux-uclibc/sysroot --disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib --with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr --with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr --with-pkgversion="Buildroot 2016.08.1" --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath --enable-libsanitizer --enable-tls --disable-libmudflap --enable-threads --with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr --without-isl --without-cloog --with-float=soft --disable-decimal-float --with-arch="mips32" --with-abi="32" --enable-languages=c --disable-shared --without-headers --disable-threads --with-newlib --disable-largefile --disable-nls --config-cache

PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin"
AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm"
CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc"
CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy"
RANLIB="/usr/bin/ranlib"
CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include"
CFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include"
CXXFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include"
LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/lib
-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib
-Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib"
PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1
PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/bin/pkg-config"
PKG_CONFIG_SYSROOT_DIR="/"
PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/share/pkgconfig"
INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include"
LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/lib
-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib
-Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib"
MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE
-D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE
-D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null
./configure
--prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr"
--sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/etc"
--localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/var"
--enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html
--disable-doc --disable-docs --disable-documentation --disable-debug
--with-xmlto=no --with-fop=no --disable-dependency-tracking
`--target=mips-buildroot-linux-uclibc`
--with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/mips-buildroot-linux-uclibc/sysroot
--disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib
--with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr
--with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr
--with-pkgversion="Buildroot 2016.08.1"
--with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath
`--enable-libsanitizer` --enable-tls --disable-libmudflap --enable-threads
--with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr
--without-isl --without-cloog --with-float=soft --disable-decimal-float
--with-arch="mips32" --with-abi="32" --enable-languages=c --disable-shared
--without-headers --disable-threads --with-newlib --disable-largefile
--disable-nls 


Therefore, this will not use checking:

// configure:2416: checking target system type
// configure:2429: result: aarch64-buildroot-linux-gnu
// checking for libsanitizer support... no
// home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/build/host-gcc-initial-4.9.4/configure.ac

# Disable libsanitizer on unsupported systems.
if test -d ${srcdir}/libsanitizer; then
  if test x$enable_libsanitizer = x; then
     AC_MSG_CHECKING([for libsanitizer support])
     if (srcdir=${srcdir}/libsanitizer; \
        . ${srcdir}/configure.tgt; \
        test -n "$UNSUPPORTED")
     then
         AC_MSG_RESULT([no])
         noconfigdirs="$noconfigdirs target-libsanitizer"
     else
         AC_MSG_RESULT([yes])
     fi
  fi
fi


// from log

checking target system type... mips-buildroot-linux-uclibc

*** This configuration is not supported in the following subdirectories:
     target-libquadmath target-libcilkrts target-libitm target-libvtv gnattools target-libada target-libstdc++-v3 target-libgfortran target-libffi target-libbacktrace target-zlib target-libobjc target-libssp target-boehm-gc 
  `target-libsanitizer`
    (Any other directories should still work fine.)


WHY???

// buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/configure.ac

# Disable libitm, libsanitizer, libvtv if we're not building C++
case ,${enable_languages}, in
  *,c++,*) ;;
  *)
    noconfigdirs="$noconfigdirs target-libcilkrts target-libitm target-libsanitizer target-libvtv"
    ;;
esac

# Remove the entries in $skipdirs and $noconfigdirs from $configdirs,
# $build_configdirs and $target_configdirs.
# If we have the source for $noconfigdirs entries, add them to $notsupp.

notsupp=""
for dir in . $skipdirs $noconfigdirs ; do
  dirname=`echo $dir | sed -e s/target-//g -e s/build-//g`
  if test $dir != .  && echo " ${configdirs} " | grep " ${dir} " >/dev/null 2>&1; then
    configdirs=`echo " ${configdirs} " | sed -e "s/ ${dir} / /"`
    if test -r $srcdir/$dirname/configure ; then
      if echo " ${skipdirs} " | grep " ${dir} " >/dev/null 2>&1; then
	true
      else
	notsupp="$notsupp $dir"
      fi
    fi
  fi
  if test $dir != .  && echo " ${build_configdirs} " | grep " ${dir} " >/dev/null 2>&1; then
    build_configdirs=`echo " ${build_configdirs} " | sed -e "s/ ${dir} / /"`
    if test -r $srcdir/$dirname/configure ; then
      if echo " ${skipdirs} " | grep " ${dir} " >/dev/null 2>&1; then
	true
      else
	notsupp="$notsupp $dir"
      fi
    fi
  fi
  if test $dir != . && echo " ${target_configdirs} " | grep " ${dir} " >/dev/null 2>&1; then
    target_configdirs=`echo " ${target_configdirs} " | sed -e "s/ ${dir} / /"`
    if test -r $srcdir/$dirname/configure ; then
      if echo " ${skipdirs} " | grep " ${dir} " >/dev/null 2>&1; then
	true
      else
	notsupp="$notsupp $dir"
      fi
    fi
  fi
done


# Produce a warning message for the subdirs we can't configure.
# This isn't especially interesting in the Cygnus tree, but in the individual
# FSF releases, it's important to let people know when their machine isn't
# supported by the one or two programs in a package.

if test -n "${notsupp}" && test -z "${norecursion}" ; then
  # If $appdirs is non-empty, at least one of those directories must still
  # be configured, or we error out.  (E.g., if the gas release supports a
  # specified target in some subdirs but not the gas subdir, we shouldn't
  # pretend that all is well.)
  if test -n "$appdirs" ; then
    for dir in $appdirs ; do
      if test -r $dir/Makefile.in ; then
	if echo " ${configdirs} " | grep " ${dir} " >/dev/null 2>&1; then
	  appdirs=""
	  break
	fi
	if echo " ${target_configdirs} " | grep " target-${dir} " >/dev/null 2>&1; then
	  appdirs=""
	  break
	fi
      fi
    done
    if test -n "$appdirs" ; then
      echo "*** This configuration is not supported by this package." 1>&2
      exit 1
    fi
  fi
  # Okay, some application will build, or we don't care to check.  Still
  # notify of subdirs not getting built.
  echo "*** This configuration is not supported in the following subdirectories:" 1>&2
  echo "    ${notsupp}" 1>&2
  echo "    (Any other directories should still work fine.)" 1>&2
fi


So add C++ support:

+BR2_TOOLCHAIN_BUILDROOT_CXX=y

BR builds libsanitizer. Great but build error:

note:
host-gcc-initial-4.9.4 build still shows the above warning but not for
host-gcc-final-4.9.4 and build fails on that.

make[3]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/mips-buildroot-linux-uclibc/libsanitizer'
/usr/bin/make "AR_FLAGS=rc" "CC_FOR_BUILD=/usr/lib64/ccache/gcc" "CFLAGS=-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os  -minterlink-mips16" "CXXFLAGS=-D_LARGEFILE_SOURCE -D_LARGEFILE6
4_SOURCE -D_FILE_OFFSET_BITS=64  -Os  -D_GNU_SOURCE -minterlink-mips16" "CFLAGS_FOR_BUILD=-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" "CFLAGS_FOR_TARGET=-D_LARGE
FILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os  -minterlink-mips16" "INSTALL=/usr/bin/install -c" "INSTALL_DATA=/usr/bin/install -c -m 644" "INSTALL_PROGRAM=/usr/bin/install -c" "INSTALL_SCRIP
T=/usr/bin/install -c" "JC1FLAGS=" "LDFLAGS=" "LIBCFLAGS=-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os  -minterlink-mips16" "LIBCFLAGS_FOR_TARGET=-D_LARGEFILE_SOURCE -D_LARGEFILE64_SO
URCE -D_FILE_OFFSET_BITS=64  -Os  -minterlink-mips16" "MAKE=/usr/bin/make" "MAKEINFO=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/missing makeinfo --split-size
=5000000 --split-size=5000000 " "PICFLAG=" "PICFLAG_FOR_TARGET=" "SHELL=/bin/sh" "RUNTESTFLAGS=" "exec_prefix=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr" "infodir=/home/NDS-UK
/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/share/info" "libdir=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" "prefix=/home/NDS-UK/kyoupark/asn/broot-late
st/buildroot-2016.08.1/output/host/usr" "includedir=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" "AR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/h
ost/usr/mips-buildroot-linux-uclibc/bin/ar" "AS=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/./gcc/as" "LD=/home/NDS-UK/kyoupark/asn/broot-latest/buildro
ot-2016.08.1/output/build/host-gcc-final-4.9.4/build/./gcc/collect-ld" "LIBCFLAGS=-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os  -minterlink-mips16" "NM=/home/NDS-UK/kyoupark/asn/broo
t-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/./gcc/nm" "PICFLAG=" "RANLIB=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/bin/ra
nlib" "DESTDIR=" all-recursive
make[4]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/mips-buildroot-linux-uclibc/libsanitizer'
Making all in sanitizer_common


libtool: compile:  /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/./gcc/xgcc -shared-libgcc -B/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/ou
tput/build/host-gcc-final-4.9.4/build/./gcc -nostdinc++ -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/mips-buildroot-linux-uclibc/libstdc++-v3/src -L/ho
me/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/mips-buildroot-linux-uclibc/libstdc++-v3/src/.libs -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1
/output/build/host-gcc-final-4.9.4/build/mips-buildroot-linux-uclibc/libstdc++-v3/libsupc++/.libs -B/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/bin/
 -B/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/lib/ -isystem /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildro
ot-linux-uclibc/include -isystem /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/sys-include -D_GNU_SOURCE -D_DEBUG -D__STDC_CONSTANT_MACROS -D__STDC_FO
RMAT_MACROS -D__STDC_LIMIT_MACROS -I. -I../../../../libsanitizer/sanitizer_common -I.. -I ../../../../libsanitizer/include -isystem ../../../../libsanitizer/include/system -Wall -W -Wno-unused-parameter -Wwr
ite-strings -pedantic -Wno-long-long -fPIC -fno-builtin -fno-exceptions -fno-rtti -fomit-frame-pointer -funwind-tables -fvisibility=hidden -Wno-variadic-macros -I../../libstdc++-v3/include -I../../libstdc++-
v3/include/mips-buildroot-linux-uclibc -I../../../../libsanitizer/../libstdc++-v3/libsupc++ -DSANITIZER_LIBBACKTRACE -DSANITIZER_CP_DEMANGLE -I ../../../../libsanitizer/../libbacktrace -I ../libbacktrace -I
../../../../libsanitizer/../include -include ../../../../libsanitizer/libbacktrace/backtrace-rename.h -D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -Os -D_GNU_SOURCE -minterlink-mips16 -MT
 sanitizer_mac.lo -MD -MP -MF .deps/sanitizer_mac.Tpo -c ../../../../libsanitizer/sanitizer_common/sanitizer_mac.cc -o sanitizer_mac.o >/dev/null 2>&1
In file included from ../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:20:0:
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:67:16: error: ‘struct___old_kernel_stat_sz’ was not declared in this scope
 COMPILER_CHECK(struct___old_kernel_stat_sz == sizeof(struct __old_kernel_stat));
                ^
../../../../libsanitizer/sanitizer_common/sanitizer_internal_defs.h:257:65: note: in definition of macro ‘IMPL_COMPILER_ASSERT’
     typedef char IMPL_PASTE(assertion_failed_##_, line)[2*(int)(pred)-1]
                                                                 ^
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:67:1: note: in expansion of macro ‘COMPILER_CHECK’
 COMPILER_CHECK(struct___old_kernel_stat_sz == sizeof(struct __old_kernel_stat));
 ^
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:67:78: error: invalid application of ‘sizeof’ to incomplete type ‘__old_kernel_stat’
 COMPILER_CHECK(struct___old_kernel_stat_sz == sizeof(struct __old_kernel_stat));
                                                                              ^
../../../../libsanitizer/sanitizer_common/sanitizer_internal_defs.h:257:65: note: in definition of macro ‘IMPL_COMPILER_ASSERT’
     typedef char IMPL_PASTE(assertion_failed_##_, line)[2*(int)(pred)-1]
                                                                 ^
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:67:1: note: in expansion of macro ‘COMPILER_CHECK’
 COMPILER_CHECK(struct___old_kernel_stat_sz == sizeof(struct __old_kernel_stat));
 ^
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:70:16: error: ‘struct_kernel_stat_sz’ was not declared in this scope
 COMPILER_CHECK(struct_kernel_stat_sz == sizeof(struct stat));
                ^
../../../../libsanitizer/sanitizer_common/sanitizer_internal_defs.h:257:65: note: in definition of macro ‘IMPL_COMPILER_ASSERT’
     typedef char IMPL_PASTE(assertion_failed_##_, line)[2*(int)(pred)-1]
                                                                 ^
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:70:1: note: in expansion of macro ‘COMPILER_CHECK’
 COMPILER_CHECK(struct_kernel_stat_sz == sizeof(struct stat));
 ^

// sanitizer_common/sanitizer_platform_limits_linux.cc

// This header seems to contain the definitions of _kernel_ stat* structs.
#include <asm/stat.h>


#if !defined(__powerpc64__) && !defined(__x86_64__) && !defined(__sparc__)
COMPILER_CHECK(struct___old_kernel_stat_sz == sizeof(struct __old_kernel_stat));
#endif


#define COMPILER_CHECK(pred) IMPL_COMPILER_ASSERT(pred, __LINE__)

#define IMPL_PASTE(a, b) a##b
#define IMPL_COMPILER_ASSERT(pred, line) \
    typedef char IMPL_PASTE(assertion_failed_##_, line)[2*(int)(pred)-1]


// buildroot-2016.08.1/output/build/linux-headers-3.2.81/arch/mips/include/asm/stat.h

struct stat {};

// buildroot-2016.08.1/output/build/linux-headers-3.2.81/arch/mips/include/asm/x86/include/asm/stat.h

/* for 32bit emulation and 32 bit kernels */
struct __old_kernel_stat {};


// useful?
https://gcc.gnu.org/ml/gcc-patches/2013-05/msg00338.html


={============================================================================
*kt_linux_tool_000* tool-asan-vstb

<old-vstb>
32bits

CC=/home/kyoupark/STB_SW/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-gnu/bin/i686-nptl-linux-gnu-gcc
*i686-nptl-linux-gnu-gcc*

kyoupark@st-castor-03:~/STB_SW/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-gnu$ ll
total 1816
-r--r--r-- 1 kyoupark ccisco     574 May 19  2016 ctng.defconfig
drwxr-xr-x 3 kyoupark ccisco    4096 Nov 13  2017 libexec/
drwxr-xr-x 4 kyoupark ccisco    4096 Nov 13  2017 lib/
drwxr-xr-x 3 kyoupark ccisco    4096 Nov 13  2017 include/
drwxr-xr-x 5 kyoupark ccisco    4096 Nov 13  2017 share/
-rw-r--r-- 1 kyoupark ccisco 1806108 Nov 13  2017 build.log.bz2
drwxr-xr-x 2 kyoupark ccisco    4096 Nov 13  2017 bin/
-rwxr-xr-x 1 kyoupark ccisco     735 Nov 13  2017 README*
drwxr-xr-x 6 kyoupark ccisco    4096 Nov 13  2017 i686-nptl-linux-gnu/
-rw-r--r-- 1 kyoupark ccisco   10949 Nov 13  2017 crosstool-ng.config

<build-mw-with-asan>

// setenv.mk

ifeq ($(COMPILER_ROOT),)
    COMPILER_ROOT := $(PLATFORM_ROOT)/compiler/i686-nptl-linux-gnu
endif


ifeq ($(COMPILER_ROOT),)
    # COMPILER_ROOT := $(PLATFORM_ROOT)/compiler/i686-nptl-linux-gnu
		COMPILER_ROOT := /home/kyoupark/asan/i686-nptl-linux-gnu-vstb-630
endif

COMPILER_ROOT := /home/kyoupark/asan/i686-nptl-linux-gnu-vstb-630


// changed to make
cd NDS_INTEL_X86_LNUX_MRFUSION_01/

kyoupark@st-castor-03:~/STB_SW_o/NDS_INTEL_X86_LNUX_MRFUSION_01$ git diff BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/platform.mk
diff --git a/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/platform.mk b/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/platform.mk
index 07c8118..4553ac0 100755
--- a/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/platform.mk
+++ b/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/platform.mk
@@ -275,6 +275,7 @@ ifeq (shared,$(NDS_LINK_TYPE_$(COMPONENT_NAME)))
 endif

 # Global compiler flags
+LOCAL_PLATFORM_CC_FLAGS += -fsanitize=address
 LOCAL_PLATFORM_CC_FLAGS += -Wall
 LOCAL_PLATFORM_CC_FLAGS += -fno-builtin
 LOCAL_PLATFORM_CC_FLAGS += -Wno-unused-but-set-variable
@@ -408,6 +409,7 @@ ifeq (shared, $(NDS_LINK_TYPE_$(COMPONENT_NAME)))
 endif

 # Global compiler flags
+LOCAL_PLATFORM_CPP_FLAGS += -fsanitize=address
 LOCAL_PLATFORM_CPP_FLAGS += -Wall
 LOCAL_PLATFORM_CPP_FLAGS += -Wno-unused-but-set-variable
 LOCAL_PLATFORM_CPP_FLAGS += -Wno-maybe-uninitialized
@@ -581,7 +583,7 @@ endif

 # Set libraries required by tools.
 # For example: LOCAL_PLATFORM_TOOLSET_LIBRARIES := -lm -lpthread -lrt
-LOCAL_PLATFORM_TOOLSET_LIBRARIES := -lm -lpthread -lrt -ldl -ltirpc -lsky_airplay_helper -lminizip -ljsoncpp -lglib -lgio -lgobject -ldbus -lz
+LOCAL_PLATFORM_TOOLSET_LIBRARIES := -lm -lpthread -lrt -ldl -ltirpc -lsky_airplay_helper -lminizip -ljsoncpp -lglib -lgio -lgobject -ldbus -lz -lasan


// to build static
+LOCAL_PLATFORM_CC_FLAGS += -static-libasan -fsanitize=address
+LOCAL_PLATFORM_CPP_FLAGS += -static-libasan -fsanitize=address

+    $(LOCAL_PLATFORM_LD) -Xlinker -Map -Xlinker $@.map -Xlinker --start-group `echo $^ | sed -e 's/[^ ]*libvg[^ ]*//g; s/[^ ]*libdsam[^ ]*//g'` $(COMPILER_ROOT)/$(COMPILER_PREFIX)/lib/libasan.a $(LOCAL_PLATFORM_PROCESS_LD_FLAGS) $(PROCESS_PROCESS_OPTIONS_ADD) -Xlinker --end-group -o $@



--- a/CMS_MEDIA_SERVICES/Media_Streamer/generic/src/ms_utils.cpp
+++ b/CMS_MEDIA_SERVICES/Media_Streamer/generic/src/ms_utils.cpp
@@ -104,7 +104,8 @@ extern "C"
        MS_ASSERT_ZERO("Pure virtual function call");
     }

-#if !defined(STBS)
+// #if !defined(STBS)
+#if 0
     void operator delete(void *)
     {
        XDEBUG_DEFINE_FUNC_INFO("operator delete");


./make.sh --debug_level release_dbg --os_debug_level release --project picasso --platform vstb --output $PWD --baseline ENG_vSTB --epg_bld_type release-trials
./release.sh --platform vstb

// more to use 630 toolchain

diff --git a/CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/mem_allocator.c b/CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/mem_allocator.c
index c602286789..4ac3ce2ec4 100644
--- a/CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/mem_allocator.c
+++ b/CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/mem_allocator.c
@@ -443,7 +443,7 @@ SYSTEM_STATUS MHWMemFreeElement(MEMMAN_API_MemoryPoolHandle *pool,
     return error;
 }

-MHWMEMORYINLINE SYSTEM_STATUS MHWMemGetElement(MEMMAN_API_MemoryPoolHandle *pool,
+SYSTEM_STATUS MHWMemGetElement(MEMMAN_API_MemoryPoolHandle *pool,
                                                MEMMAN_API_Allocator *theAllocator,
                                                uint32_t theIndex, void **theElement)
 {
@@ -555,7 +555,7 @@ SYSTEM_STATUS MHWMemIndexIsFree(MEMMAN_API_MemoryPoolHandle *pool,
     return MEMMAN_API_STATUS_OK;
 }

-MHWMEMORYINLINE SYSTEM_STATUS MHWMemGetIndex(MEMMAN_API_MemoryPoolHandle *pool,
+SYSTEM_STATUS MHWMemGetIndex(MEMMAN_API_MemoryPoolHandle *pool,
                                              MEMMAN_API_Allocator *theAllocator, void *theElement,
                                              uint32_t *theIndex)
 {


LOCAL_PLATFORM_CPP_FLAGS += -fsanitize=address -fsanitize-recover=address -Wno-narrowing



cd FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/
./make.sh --debug_level release_dbg --os_debug_level release --project picasso --platform vstb --output $PWD --baseline ENG_vSTB --epg_bld_type release-trials
./release.sh --platform vstb


<tool-asan-build-mw-with-asan-for-clang>

// to use clang
vstb.mk

# KT
CC=/home/kyoupark/asan/llvm/cmake_build/bin/clang
CXX=/home/kyoupark/asan/llvm/cmake_build/bin/clang++
AR=/home/kyoupark/asan/llvm/cmake_build/bin/llvm-ar
STRIP=/home/kyoupark/asan/llvm/cmake_build/bin/llvm-strip

PLATFORM_CFLAGS = -DPLATFORM_VSTB -DIS_LITTLE_ENDIAN -D__LITTLE_ENDIAN -fdata-sections -ffunction-sections `-D__i386__`

// remove warning options
// /home/kyoupark/STB_SW_clang_build/THIRD_PARTY_LIBRARIES/BSKYB_JTH/build/applications/Picasso/picasso/jpa/Makefile
# CFLAGS = -Wall -Wmissing-prototypes -Wshadow -Wpointer-arith -Wstrict-prototypes -Wmissing-declarations -Werror 
CFLAGS = -Wmissing-prototypes -Wshadow -Wpointer-arith -Wstrict-prototypes -Wmissing-declarations

// /CMS_SYSTEM_INFRASTRUCTURE/CISCOSAFEC/src/makefile
# KT
# CPP=$(BIN_DIR)/$(COMPILER_PREFIX)-cpp 
CPP=$(BIN_DIR)/clang-cpp 

# KT
# CONFIGURE_FLAGS+= --host=$(TARGET)
CONFIGURE_FLAGS+= --host=i686-nptl-linux-gnu

# clang don't have stropts.h so make a empty file.
# /home/kyoupark/asan/llvm/cmake_build/lib/clang/8.0.0/include
touch /usr/include/stropts.h


<new-vstb>
more README

Mr Fusion Toolchain
-------------------

This toolchain has been automatically generated by Crosstool-ng v1.23.0
(http://crosstool-ng.org/), and the configuration is available as
crosstool-ng.config.

CT_BINUTILS_VERSION="2.28"
CT_KERNEL_VERSION="3.14.43"
CT_LIBC_VERSION="2.25"
CT_CC_GCC_VERSION="4.9.4"

// BSKYB_GM_Integration/BSKYB_INTEGRATION_BLD_42.25.00_BSKYB_GM_Integration

kyoupark@st-castor-03:~/STB_SW/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-gnu/bin$ ./i686-nptl-linux-gnu-gcc --version
i686-nptl-linux-gnu-gcc (crosstool-NG crosstool-ng-1.23.0 - vstb) `4.9.4`
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

// already has asan

kyoupark@st-castor-03:~/STB_SW/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-gnu/i686-nptl-linux-gnu/lib$ ll libasan*
-rwxr-xr-x 1 kyoupark ccisco  976540 Nov 13  2017 libasan.so.1.0.0*
lrwxrwxrwx 1 kyoupark ccisco      16 Nov 13  2017 libasan.so.1 -> libasan.so.1.0.0*
lrwxrwxrwx 1 kyoupark ccisco      16 Nov 13  2017 libasan.so -> libasan.so.1.0.0*
-rw-r--r-- 1 kyoupark ccisco    2492 Nov 13  2017 libasan_preinit.o
-rw-r--r-- 1 kyoupark ccisco 1568138 Nov 13  2017 libasan.a


// However, on vstb, there is no libasan* under lib, so have to do:
// copy libasan* into /flash0

cp ~/STB_SW/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-gnu/i686-nptl-linux-gnu/lib/libasan* .


// comment out run.sh from startstack.sh in flash0/scripts since /root/run.sh
// is in read-only fs.
/
// this is last line in startstack.sh
/root/run.sh

// this is run.sh
echo "######################"
echo "Running own run.sh"
echo "######################"
ulimit -c unlimited
LD_PRELOAD=/flash0/libasan.so:/usr/lib/libforcesched.so:/lib/libresolv-refresh.so:/lib/libifnameindex.so:/usr/lib/libmutex-desched.so  /NDS/bin/PWM_Process
# LD_PRELOAD=/usr/lib/libforcesched.so:/lib/libresolv-refresh.so:/lib/libifnameindex.so:/usr/lib/libmutex-desched.so  /NDS/bin/PWM_Process


<run-sample>
~/STB_SW/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-gnu/bin/i686-nptl-linux-gnu-gcc -fsanitize=address uaf.c -o asan_uaf
~/STB_SW/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-gnu/bin/i686-nptl-linux-gnu-gcc uaf.c -o asan_uaf_wo

export ASAN_OPTIONS="verbosity=2:halt_on_error=0:debug=1"
export LD_LIBRARY_PATH="/flash0:${LD_LIBRARY_PATH}"
mkdir /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-xxx /flash0
ulimit -c unlimited
./asan_uaf


<tool-vstb-run-process>
// export PROFILE=FUSION-MRFUSION-01
// export STREAM_NETWORK=BSKYB_DARWIN
// export ASAN_OPTIONS="verbosity=2:halt_on_error=0:debug=1"
// export LD_LIBRARY_PATH="/flash0:${LD_LIBRARY_PATH}"

mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-r16/flash0 /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-gm/flash0 /flash0

// vstb 630
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-630/flash0 /flash0

// pete's build
mkdir /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-pete/ /flash0
ulimit -c unlimited

export VSTB_CORE_FORCE_BLENDER_ID=1
export VSTB_CORE_DISPMAN_FPS=25
export PROFILE=BSKYB-VSTBPVR-01
export STREAM_NETWORK=BSKYB_DARWIN

// ok
touch /flash0/scripts/this_is_first_boot
/flash0/scripts/startstack.sh $PROFILE

// not ok
touch /flash0/scripts/this_is_first_boot
/flash0/scripts/startstack.sh

// touch /flash0/scripts/this_is_first_boot
/flash0/scripts/startstack.sh $PROFILE

// touch /flash0/scripts/this_is_first_boot
/flash0/scripts/startstack.sh


// clang
mkdir /flash0
mount -o nfsvers=4 10.209.60.101:/home/kyoupark/git/kb/asan /flash0


// 430 clean and no asan
export VSTB_CORE_FORCE_BLENDER_ID=1
export VSTB_CORE_DISPMAN_FPS=25
export PROFILE=BSKYB-VSTBPVR-01
export STREAM_NETWORK=BSKYB_DARWIN
export LD_LIBRARY_PATH="/flash0:${LD_LIBRARY_PATH}"

mkdir /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-430-clean-no-asan/flash0 /flash0
ulimit -c unlimited
touch /flash0/scripts/this_is_first_boot
/flash0/scripts/startstack.sh $PROFILE


// 430 run
// 430 clean and asan (no recovery) mw binary
// clean(no asan) output and only changes mw which is asaned.

export VSTB_CORE_FORCE_BLENDER_ID=1
export VSTB_CORE_DISPMAN_FPS=25
export PROFILE=BSKYB-VSTBPVR-01
export STREAM_NETWORK=BSKYB_DARWIN
export LD_LIBRARY_PATH="/flash0:${LD_LIBRARY_PATH}"

mkdir /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-430-asan-run/flash0 /flash0
ulimit -c unlimited
touch /flash0/scripts/this_is_first_boot
/flash0/scripts/startstack.sh $PROFILE


// 430 clean and pwm diag enabled
// clean(no asan) output and only changes mw which is asaned.

export VSTB_CORE_FORCE_BLENDER_ID=1
export VSTB_CORE_DISPMAN_FPS=25
export PROFILE=BSKYB-VSTBPVR-01
export STREAM_NETWORK=BSKYB_DARWIN
export LD_LIBRARY_PATH="/flash0:${LD_LIBRARY_PATH}"

mkdir /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-430-clean-diag-on/flash0 /flash0
ulimit -c unlimited
touch /flash0/scripts/this_is_first_boot
/flash0/scripts/startstack.sh $PROFILE


// 430 run diag
export VSTB_CORE_FORCE_BLENDER_ID=1
export VSTB_CORE_DISPMAN_FPS=25
export PROFILE=BSKYB-VSTBPVR-01
export STREAM_NETWORK=BSKYB_DARWIN
export LD_LIBRARY_PATH="/flash0:${LD_LIBRARY_PATH}"

mkdir /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-430-diag-run/flash0 /flash0
ulimit -c unlimited
touch /flash0/scripts/this_is_first_boot
/flash0/scripts/startstack.sh $PROFILE


// 630 clean and no asan 

export VSTB_CORE_FORCE_BLENDER_ID=1
export VSTB_CORE_DISPMAN_FPS=25
export PROFILE=BSKYB-VSTBPVR-01
export STREAM_NETWORK=BSKYB_DARWIN

mkdir /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-630-clean-no-asan/flash0 /flash0
ulimit -c unlimited
touch /flash0/scripts/this_is_first_boot
/flash0/scripts/startstack.sh $PROFILE


// 630 clean and asan recover mw binary
// clean(no asan) output and only changes mw which is asaned.

export VSTB_CORE_FORCE_BLENDER_ID=1
export VSTB_CORE_DISPMAN_FPS=25
export PROFILE=BSKYB-VSTBPVR-01
export STREAM_NETWORK=BSKYB_DARWIN
export LD_LIBRARY_PATH="/flash0:${LD_LIBRARY_PATH}"

mkdir /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-630-clean-and-recover/flash0 /flash0
ulimit -c unlimited
touch /flash0/scripts/this_is_first_boot
/flash0/scripts/startstack.sh $PROFILE


// 630-run clean and asan mw binary
// clean(no asan) output and only changes mw which is asaned.

export VSTB_CORE_FORCE_BLENDER_ID=1
export VSTB_CORE_DISPMAN_FPS=25
export PROFILE=BSKYB-VSTBPVR-01
export STREAM_NETWORK=BSKYB_DARWIN
export LD_LIBRARY_PATH="/flash0:${LD_LIBRARY_PATH}"

mkdir /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-630-run/flash0 /flash0
ulimit -c unlimited
touch /flash0/scripts/this_is_first_boot
/flash0/scripts/startstack.sh $PROFILE


// clean vstb build
mkdir /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-clean-build/flash0 /flash0

// compiled with asan
mkdir /flash0
mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-vstb-asan-build/flash0 /flash0

ulimit -c unlimited
/flash0/scripts/startstack.sh


// copy to build server
scp libasan* 10.209.62.106:/home/kyoupark/si_logs/flash0-asan

kyoupark@kt-office-debian:~/asan/gcc/i686-nptl-linux-gnu-gcc/i686-nptl-linux-gnu/lib$ ll libasan*
-rw-r--r-- 1 kyoupark kyoupark   14708 Sep 27 15:58 libasan_preinit.o
-rwxr-xr-x 1 kyoupark kyoupark 3510988 Sep 27 15:58 libasan.so.1.0.0*
lrwxrwxrwx 1 kyoupark kyoupark      16 Sep 27 15:58 libasan.so.1 -> libasan.so.1.0.0*
lrwxrwxrwx 1 kyoupark kyoupark      16 Sep 27 15:58 libasan.so -> libasan.so.1.0.0*
-rwxr-xr-x 1 kyoupark kyoupark    1099 Sep 27 15:58 libasan.la*
-rw-r--r-- 1 kyoupark kyoupark 5319534 Sep 27 15:58 libasan.a

// mkdir /flash0
// mount -o nfsvers=4 10.209.62.106:/home/kyoupark/si_logs/flash0-asan/ /flash0


// debian office
mkdir /flash0
mount -o nfsvers=4 10.209.60.101:/home/kyoupark/ /flash0

add-auto-load-safe-path /flash0/
set breakpoint pending on
set environment LD_LIBRARY_PATH=.
set substitute-path /home/kyoupark/asan/gcc/build-gcc/i686-nptl-linux-gnu/libsanitizer/asan/../../../../gcc-4.9.4/libsanitizer/ /flash0/asan/gcc/gcc-4.9.4/libsanitizer
b __asan_init_v3
b main
b call_uaf
b __asan_report_store4
b __asan_report_error

cd /flash0/asan/flash0
gdb -command=cmd ./own_asan_uaf


// things need to do

1. to run mw process only, do not change run.sh but instead make mw process
script which runs actuall mw process

2. copy asan lib files 


/flash0/fs/NDS/bin$ more MW_Process
#!/bin/sh
echo ""
echo "*** running MW ASAN ***"
echo "*** running MW ASAN ***"
echo "*** running MW ASAN ***"
echo "*** running MW ASAN ***"
echo ""
LD_PRELOAD=/flash0/libasan.so /NDS/bin/MW_Process.org


// history
// own toolchain (do not have vstb in name)
// 4.9.4 builds is okay and confirm asan's working.
// build mw with this toolchain but no output from asan. check two things: 
//
// 1. -Dxxx to use malloc in mw. OK since it's used.  
// 2. no asan assembly code in output binary. found out that having "-O2"
// makes no asan code in the output. *asan-optimisation*
//
// mw build and asan runs but asan, mw process exits soon. around diag. 
//
// (asan error on diag)
// 0x08140086 is located 0 bytes to the right of global variable 'gClientName' from 'diagctl_log_req_handler.c' (0x8140080) of size 6
//   'gClientName' is ascii string '_DLRH'
// SUMMARY: AddressSanitizer: global-buffer-overflow ??:0 memcpy
//
// 0x0819b606 is located 0 bytes to the right of global variable 'gClientName' from 'diagctl_log_req_handler.c' (0x819b600) of size 6
//   'gClientName' is ascii string '_DLRH'
// 
// /home/kyoupark/STB_SW/CMS_SYSTEM_INFRASTRUCTURE/
// DIAG/src/log/src/diagctl_log_req_handler.c
//
// /* IPC client name */
//-static char gClientName[] = "_DLRH";
//+static char gClientName[6] = "_DLRH";
//
// so to use "after-error" of asan, tried "-fsanitizer_recover=address" but
// it's not supported in the current version.
//
// so tried 6.3.0 (v6)
// with recover and without recover shows that mw get stuck in boot up.
//
// tried "halt-on-error", "debug", "verbosity". no progress.
//
// build mw only with asan (for both v4 and v6) and see diag issue and see pwm
// abort. changed diag and goes further and stuck. 
//
// //
// since /root/run.sh is read-only, made this file in /flash0 and set
// LD_PRELOAD there.
//
// rather than LD_PRELOAD=/flash0/libasan.so /flash0/scripts/startstack.sh
//
// note: this also pick ups asan
// /flash0/fs/NDS/bin/FusionConfigImport: 
// error while loading shared libraries: 
// libasan.so.3: cannot open shared object file: No such file or directory
//
// can be fixed by using:
// export LD_LIBRARY_PATH="/flash0:${LD_LIBRARY_PATH}"
//
// note:
// ==1779==ASan runtime does not come first in initial library list; 
// you should either link runtime to your application or 
// manually preload it with LD_PRELOAD.
//
// /flash0/run.sh
// ulimit -c unlimited
// LD_PRELOAD=/usr/lib/libforcesched.so:/lib/libresolv-refresh.so:/lib/libifnameindex.so:/usr/lib/libmutex-desched.so \
// /NDS/bin/PWM_Process
//
// note:
// with/without this, both still show the above error. With this, can see
// verbose message from asan but asan stops on:
//
// ==1350==AddressSanitizer Init done
//
// LD_PRELOAD=/flash0/libasan.so:
//
// //
// How can delay asan's running so that can run it after system boots up.

kyoupark@st-castor-03:~/si_logs/flash0-asan-v6/flash0$ ./i686-nptl-linux-gnu-gdb
GNU gdb (crosstool-NG crosstool-ng-1.23.0) 7.12.1
Copyright (C) 2017 Free Software Foundation, Inc.


<ex> unsanitized binary
ASAN:SIGSEGV
=================================================================
==1186==ERROR: AddressSanitizer: SEGV on unknown address 0xfffffb40 (pc 0xb7186309 sp 0xbffbf080 bp 0xbffbf108 T0)
    #0 0xb7186308 in __pthread_create_2_1 (/lib/libpthread.so.0+0x7308)
    #1 0xb71866ea in __pthread_create_2_0 (/lib/libpthread.so.0+0x76ea)
    #2 0xb725fdd3 in pthread_create (/flash0/libasan.so+0x61dd3)
    #3 0x807e08a in SYSTEMUTIL_THR_Create (/NDS/bin/PWM_Process+0x807e08a)
    #4 0x806c49d in DIAGCTL_SVR_StartRequestHandler (/NDS/bin/PWM_Process+0x806c49d)
    #5 0x806ab4d in DIAGCTL_SVR_InitCommunicationChannel (/NDS/bin/PWM_Process+0x806ab4d)
    #6 0x806a334 in DIAGCTL_SVR_Init (/NDS/bin/PWM_Process+0x806a334)
    #7 0x8069fe8 in __DIAG_Init (/NDS/bin/PWM_Process+0x8069fe8)
    #8 0x805440d in PWM_Init (/NDS/bin/PWM_Process+0x805440d)
    #9 0x80539eb in main (/NDS/bin/PWM_Process+0x80539eb)
    #10 0xb6c8a31f in __libc_start_main ../csu/libc-start.c:295
    #11 0x8053c0a (/NDS/bin/PWM_Process+0x8053c0a)

AddressSanitizer can not provide additional info.
SUMMARY: AddressSanitizer: SEGV ??:0 __pthread_create_2_1
==1186==ABORTING


ASAN:SIGSEGV
=================================================================
==1310==ERROR: AddressSanitizer: SEGV on unknown address 0xfffffb40 (pc 0xb71e5309 sp 0xbf8155d0 bp 0xbf815658 T0)
    #0 0xb71e5308 in __pthread_create_2_1 (/lib/libpthread.so.0+0x7308)
    #1 0xb71e56ea in __pthread_create_2_0 (/lib/libpthread.so.0+0x76ea)
    #2 0xb72bedd3 in pthread_create (/flash0/libasan.so+0x61dd3)
    #3 0x8d1123a in SYSTEMUTIL_THR_Create (/NDS/bin/bins/MW_Process+0x8d1123a)
    #4 0x8566c4a in DIAG_IPC_CLIENT_Open (/NDS/bin/bins/MW_Process+0x8566c4a)
    #5 0x856adc7 in DIAGCTL_LOG_OpenCommunicationChannel (/NDS/bin/bins/MW_Process+0x856adc7)
    #6 0x85699da in DIAGCTL_LOG_Init (/NDS/bin/bins/MW_Process+0x85699da)
    #7 0x8562c44 in __DIAG_LOG_Init (/NDS/bin/bins/MW_Process+0x8562c44)
    #8 0x8cc93aa in SYSINIT_ClientInit (/NDS/bin/bins/MW_Process+0x8cc93aa)
    #9 0x8202dde in main (/NDS/bin/bins/MW_Process+0x8202dde)
    #10 0xb6cca31f in __libc_start_main ../csu/libc-start.c:295
    #11 0x8202f93 (/NDS/bin/bins/MW_Process+0x8202f93)

AddressSanitizer can not provide additional info.
SUMMARY: AddressSanitizer: SEGV ??:0 __pthread_create_2_1
==1310==ABORTING


<ex>
fix diag issue. use sanitized mw process binary and non-sanitized binaries for
others. 

=================================================================
==1221==ERROR: AddressSanitizer: `heap-use-after-free` on address 0xb46021d8 at pc 0x8d41268 bp 0xb38bd2c8 sp 0xb38bd2b0
READ of size 4 at 0xb46021d8 thread T16777215
==1221==WARNING: readlink("/proc/self/exe") failed with errno 2, some stack frames may not be symbolized
    #0 0x8d41267 (/proc/self/exe+0x8d41267)
    #1 0x8ca5408 (/proc/self/exe+0x8ca5408)
    #2 0x8ca89f0 (/proc/self/exe+0x8ca89f0)
    #3 0x9cc3f0c (/proc/self/exe+0x9cc3f0c)
    #4 0xb770e5bc (/lib/libpthread.so.0+0x65bc)
    #5 0xb6dbe8e5 (/lib/libc.so.6+0xf68e5)

0xb46021d8 is located 8 bytes inside of 44-byte region [0xb46021d0,0xb46021fc)
==1221==AddressSanitizer CHECK failed: /home/pete/vstb_toolchain/32/.build/src/gcc-4.9.4/
  libsanitizer/asan/asan_allocator2.cc:234 "((id)) != (0)" (0x0, 0x0)
    #0 0xb6f1959f (/flash0/libasan.so.1+0x7259f)
    #1 0xb6f1f5d1 (/flash0/libasan.so.1+0x785d1)
    #2 0xb6ebbfe1 (/flash0/libasan.so.1+0x14fe1)
    #3 0xb6ebc0b3 (/flash0/libasan.so.1+0x150b3)
    #4 0xb6f17346 (/flash0/libasan.so.1+0x70346)
    #5 0xb6f17641 (/flash0/libasan.so.1+0x70641)
    #6 0xb6f18d02 (/flash0/libasan.so.1+0x71d02)
    #7 0xb6f1a105 (/flash0/libasan.so.1+0x73105)
    #8 0x8d41267 (/proc/self/exe+0x8d41267)
    #9 0x8ca5408 (/proc/self/exe+0x8ca5408)
    #10 0x8ca89f0 (/proc/self/exe+0x8ca89f0)
    #11 0x9cc3f0c (/proc/self/exe+0x9cc3f0c)
    #12 0xb770e5bc (/lib/libpthread.so.0+0x65bc)
    #13 0xb6dbe8e5 (/lib/libc.so.6+0xf68e5)

~/STB_SW_o/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-gnu/bin/i686-nptl-linux-gnu-addr2line -e MW_Process 0x8d41268
/home/kyoupark/STB_SW_o/CMS_MEDIA_SERVICES/BEM/src/server/bem_link_discovery.c:957


<ex>
fix diag issue and static asan.

ASAN:SIGSEGV
=================================================================
==1221==ERROR: AddressSanitizer: SEGV on unknown address 0xfffffb40 (pc 0xb76c0309 sp 0xbfd59eb0 bp 0xbfd59f38 T0)
    #0 0xb76c0308 in __pthread_create_2_1 (/lib/libpthread.so.0+0x7308)
    #1 0xb76c06ea in __pthread_create_2_0 (/lib/libpthread.so.0+0x76ea)
    #2 0xa14c591 in pthread_create (/NDS/bin/bin-static-asan-with-diag-fix/MW_Process+0xa14c591)
    #3 0x9cd413b in SYSTEMUTIL_THR_Create /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/SYSTEMUTIL/src/thread/systemutil_thread.c:465
    #4 0x8a8fe14 in DIAG_IPC_CLIENT_Open /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/ipc/src/client/diag_ipc_client.c:224
    #5 0x8a9a514 in DIAGCTL_LOG_OpenCommunicationChannel /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/log/src/diagctl_log_ipc.c:675
    #6 0x8a98078 in DIAGCTL_LOG_Init /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/log/src/diagctl_log.c:218
    #7 0x8a89791 in __DIAG_LOG_Init /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/DIAG/src/log/src/diag_log.c:1594
    #8 0x9c100c7 in SYSINIT_ClientInit /home/kyoupark/STB_SW_o/CMS_SYSTEM_INFRASTRUCTURE/SYSINIT/src/client/sysinit_api.c:333
    #9 0x82346ed in main /home/kyoupark/STB_SW_o/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/processes/MW_Process/mw_process_main.c:85
    #10 0xb71a531f in __libc_start_main ../csu/libc-start.c:295
    #11 0x8242650 (/NDS/bin/bin-static-asan-with-diag-fix/MW_Process+0x8242650)

AddressSanitizer can not provide additional info.
SUMMARY: AddressSanitizer: SEGV ??:0 __pthread_create_2_1
==1221==ABORTING


<vstb>
[root@vSTB lib]# ./ld-2.25.so
Usage: ld.so [OPTION]... EXECUTABLE-FILE [ARGS-FOR-PROGRAM...]


// okay
[root@vSTB flash0]# LD_LIBRARY_PATH=. ./vstb_asan_uaf
[root@vSTB flash0]# ./vstb_static_asan_uaf

// okay
[root@vSTB flash0]# LD_LIBRARY_PATH=. ./own_asan_uaf
[root@vSTB flash0]# ./own_static_asan_uaf


={============================================================================
*kt_linux_tool_100* tool-valgrind

1.1. An Overview of Valgrind

Valgrind is an instrumentation framework for building dynamic analysis tools. It
comes with a set of tools each of which performs some kind of debugging,
profiling, or similar task that helps you improve your programs. Valgrind's
  architecture is modular, so new tools can be created easily and without
  disturbing the existing structure.

A number of useful tools are supplied as standard.

    Memcheck is a memory error detector. It helps you make your programs,
    particularly those written in C and C++, more correct.

    Cachegrind is a cache and branch-prediction profiler. It helps you make your
    programs run faster.

    Callgrind is a call-graph generating cache profiler. It has some overlap
    with Cachegrind, but also gathers some information that Cachegrind does not.

    Helgrind is a thread error detector. It helps you make your multi-threaded
    programs more correct.

    DRD is also a thread error detector. It is similar to Helgrind but uses
    different analysis techniques and so may find different problems.

    Massif is a heap profiler. It helps you make your programs use less memory.

    DHAT is a different kind of heap profiler. It helps you understand issues of
    block lifetimes, block utilisation, and layout inefficiencies.

    SGcheck is an experimental tool that can detect overruns of stack and global
    arrays. Its functionality is complementary to that of Memcheck: SGcheck
    finds problems that Memcheck can't, and vice versa..

    BBV is an experimental SimPoint basic block vector generator. It is useful
    to people doing computer architecture research and development.


1.2. How to navigate this manual

This manual's structure reflects the structure of Valgrind itself. First, we
describe the Valgrind core, how to use it, and the options it supports. Then,
each tool has its own chapter in this manual. You only need to read the
  documentation for the core and for the tool(s) you actually use, although you
  may find it helpful to be at least a little bit familiar with what all tools
  do. If you're new to all this, you probably want to run the Memcheck tool and
    you might find the The Valgrind Quick Start Guide useful.

Be aware that the core understands some command line options, and the tools have
their own options which they know about. This means there is no central place
describing all the options that are accepted -- you have to read the options
documentation both for Valgrind's core and for the tool you want to use.


2.3. The Commentary

Valgrind tools write a commentary, a stream of text, detailing error reports and
other significant events. All lines in the commentary have following form:

==12345== some-message-from-Valgrind

The 12345 is the process ID. This scheme makes it easy to distinguish program
output from Valgrind commentary, and also easy to differentiate commentaries
from different processes which have become merged together, for whatever reason.

By default, Valgrind tools write only essential messages to the commentary, so
as to avoid flooding you with information of secondary importance. If you want
more information about what is happening, re-run, passing the `-v` option to
Valgrind. A second -v gives yet more detail.

You can direct the commentary to three different places:

    The default: send it to a file descriptor, which is by default 2 (stderr).
    So, if you give the core no options, it will write commentary to the
    standard error stream. If you want to send it to some other file descriptor,
             for example number 9, you can specify --log-fd=9.

    This is the simplest and most common arrangement, but can cause problems
      when Valgrinding entire trees of processes which expect specific file
      descriptors, particularly stdin/stdout/stderr, to be available for their
      own use.

    A less intrusive option is to write the commentary to a file, which you
    specify by `--log-file=filename`. There are special format specifiers that can
    be used to use a process ID or an environment variable name in the log file
    name. These are useful/necessary if your program invokes multiple processes
    (especially for MPI programs). See the basic options section for more
    details.

    The least intrusive option is to send the commentary to a network socket.
    The socket is specified as an IP address and port number pair, like this:
    --log-socket=192.168.0.1:12345 if you want to send the output to host IP
    192.168.0.1 port 12345 (note: we have no idea if 12345 is a port of
        pre-existing significance). You can also omit the port number:
    --log-socket=192.168.0.1, in which case a default port of 1500 is used. This
    default is defined by the constant VG_CLO_DEFAULT_LOGPORT in the sources.

    // skipped


2.5. Suppressing errors

The error-checking tools detect numerous problems in the system libraries, such
as the C library, which come pre-installed with your OS. You can't easily fix
these, but you don't want to see these errors (and yes, there are many!) So
Valgrind reads a list of errors to suppress at startup. A default suppression
file is created by the ./configure script when the system is built.

You can modify and add to the suppressions file at your leisure, or, better,
    write your own. Multiple suppression files are allowed. This is useful if
      part of your project contains errors you can't or don't want to fix, yet
      you don't want to continuously be reminded of them.

Note: By far the easiest way to add suppressions is to use the
`--gen-suppressions=yes` option described in Core Command-line Options. This
generates suppressions automatically. For best results, though, you may want to
edit the output of --gen-suppressions=yes by hand, in which case it would be
advisable to read through this section. 

 --gen-suppressions=<yes|no|all> [default: no]

    When set to yes, Valgrind will pause after every error shown and print the
    line:

        ---- Print suppression ? --- [Return/N/n/Y/y/C/c] ----

    Pressing Ret, or N Ret or n Ret, causes Valgrind continue execution without
    printing a suppression for this error.

    Pressing Y Ret or y Ret causes Valgrind to write a suppression for this
    error. You can then cut and paste it into a suppression file if you don't
    want to hear about the error in the future.

    When set to all, Valgrind will print a suppression for every reported error,
    without querying the user.

    This option is particularly useful with C++ programs, as it prints out the
    suppressions with mangled names, as required.

    Note that the suppressions printed are as specific as possible. You may want
    to common up similar ones, by adding wildcards to function names, and by
    using frame-level wildcards. The wildcarding facilities are powerful yet
    flexible, and with a bit of careful editing, you may be able to suppress a
    whole family of related errors with only a few suppressions.

    Sometimes two different errors are suppressed by the same suppression, in
    which case Valgrind will output the suppression more than once, but you only
    need to have one copy in your suppression file (but having more than one
        won't cause problems). Also, the suppression name is given as <insert a
    suppression name here>; the name doesn't really matter, it's only used with
    the -v option which prints out all used suppression records.


={============================================================================
*kt_linux_tool_100* tool-valgrind-memcheck

http://valgrind.org/docs/manual/quick-start.html

The Valgrind Quick Start Guide

1. Introduction

The Valgrind tool suite provides a number of debugging and profiling tools that
help you make your programs faster and more correct. The most popular of these
tools is called Memcheck. It can detect many memory-related errors that are
common in C and C++ programs and that can lead to crashes and unpredictable
behaviour.

The rest of this guide gives the minimum information you need to start detecting
memory errors in your program with Memcheck. For full documentation of Memcheck
and the other tools, please read the User Manual.

2. Preparing your program

Compile your program with -g to include debugging information so that Memcheck's
error messages include exact line numbers. Using -O0 is also a good idea, if you
can tolerate the slowdown. With -O1 line numbers in error messages can be
inaccurate, although generally speaking running Memcheck on code compiled at -O1
works fairly well, and the speed improvement compared to running -O0 is quite
significant. Use of -O2 and above is not recommended as Memcheck occasionally
reports uninitialised-value errors which don't really exist.

3. Running your program under Memcheck

If you normally run your program like this:

  myprog arg1 arg2

Use this command line:

  valgrind --leak-check=yes myprog arg1 arg2

Memcheck is the default tool. The `--leak-check` option turns on the detailed
memory leak detector.

Your program will run much slower (eg. 20 to 30 times) than normal, and use a
lot more memory. Memcheck will issue messages about memory errors and leaks that
it detects.

4. Interpreting Memcheck's output

Here's an example C program, in a file called a.c, with a memory error and a
memory leak.

  #include <stdlib.h>

  void f(void)
  {
     int* x = malloc(10 * sizeof(int));
     x[10] = 0;        // problem 1: heap block overrun
  }                    // problem 2: memory leak -- x not freed

  int main(void)
  {
     f();
     return 0;
  }

Most error messages look like the following, which describes problem 1, the heap
  block overrun:

  ==19182== Invalid write of size 4
  ==19182==    at 0x804838F: f (example.c:6)
  ==19182==    by 0x80483AB: main (example.c:11)
  ==19182==  Address 0x1BA45050 is 0 bytes after a block of size 40 alloc'd
  ==19182==    at 0x1B8FF5CD: malloc (vg_replace_malloc.c:130)
  ==19182==    by 0x8048385: f (example.c:5)
  ==19182==    by 0x80483AB: main (example.c:11)

Things to notice:

    There is a lot of information in each error message; read it carefully.

    The 19182 is the process ID; it's usually unimportant.

    The first line ("Invalid write...") tells you what kind of error it is.
    Here, the program wrote to some memory it should not have due to a heap
    block overrun.

    Below the first line is a stack trace telling you where the problem
    occurred. Stack traces can get quite large, and be confusing, especially if
    you are using the C++ STL. Reading them from the bottom up can help. If the
    stack trace is not big enough, use the `--num-callers` option to make it
    bigger.

    The code addresses (eg. 0x804838F) are usually unimportant, but occasionally
    crucial for tracking down weirder bugs.

    Some error messages have a second component which describes the memory
    address involved. This one shows that the written memory is just past the
    end of a block allocated with malloc() on line 5 of example.c.

It's worth fixing errors in the order they are reported, as later errors can be
caused by earlier errors. Failing to do this is a common cause of difficulty
with Memcheck.

Memory leak messages look like this:

  ==19182== 40 bytes in 1 blocks are definitely lost in loss record 1 of 1
  ==19182==    at 0x1B8FF5CD: malloc (vg_replace_malloc.c:130)
  ==19182==    by 0x8048385: f (a.c:5)
  ==19182==    by 0x80483AB: main (a.c:11)

The stack trace tells you where the leaked memory was allocated. Memcheck cannot
tell you why the memory leaked, unfortunately. (Ignore the
    "vg_replace_malloc.c", that's an implementation detail.)

There are several kinds of leaks; the two most important categories are:

    "definitely lost": your program is leaking memory -- fix it!

    "probably lost": your program is leaking memory, unless you're doing funny
    things with pointers (such as moving them to point to the middle of a heap
        block).

Memcheck also reports uses of uninitialised values, most commonly with the
message "Conditional jump or move depends on uninitialised value(s)". It can be
difficult to determine the root cause of these errors. Try using the
`--track-origins`=yes to get extra information. This makes Memcheck run slower,
  but the extra information you get often saves a lot of time figuring out where
    the uninitialised values are coming from.

If you don't understand an error message, please consult Explanation of error
messages from Memcheck in the Valgrind User Manual which has examples of all the
error messages Memcheck produces.

5. Caveats

Memcheck is not perfect; it occasionally produces false positives, and there are
mechanisms for suppressing these (see Suppressing errors in the Valgrind User
    Manual). 

However, it is typically right 99% of the time, so you should be wary of
ignoring its error messages. After all, you wouldn't ignore warning messages
produced by a compiler, right? The suppression mechanism is also useful if
Memcheck is reporting errors in library code that you cannot change. The default
suppression set hides a lot of these, but you may come across more.

Memcheck cannot detect every memory error your program has. For example, it
can't detect out-of-range reads or writes to arrays that are allocated
statically or on the stack. But it should detect many errors that could crash
your program (eg. cause a segmentation fault).

Try to make your program so clean that Memcheck reports no errors. Once you
achieve this state, it is much easier to see when changes to the program cause
Memcheck to report new errors. Experience from several years of Memcheck use
shows that it is possible to make even huge programs run Memcheck-clean. For
example, large parts of KDE, OpenOffice.org and Firefox are Memcheck-clean, or
very close to it.


={============================================================================
*kt_linux_tool_100* tool-valgrind-memcheck-doc

http://valgrind.org/docs/manual/mc-manual.html#mc-manual.leaks

To use this tool, you may specify --tool=memcheck on the Valgrind command line.
You don't have to, though, since Memcheck is the 'default' tool.

4.1. Overview

Memcheck is a memory error detector. It can detect the following problems that
are common in C and C++ programs.

    Accessing memory you shouldn't, e.g. overrunning and underrunning heap
    blocks, overrunning the top of the stack, and accessing memory after it has
    been freed.

    Using undefined values, i.e. values that have not been initialised, or that
    have been derived from other undefined values.

    Incorrect freeing of heap memory, such as double-freeing heap blocks, or
    mismatched use of malloc/new/new[] versus free/delete/delete[]

    Overlapping src and dst pointers in memcpy and related functions.

    Passing a fishy (presumably negative) value to the size parameter of a
    memory allocation function.

    Memory leaks.

Problems like these can be difficult to find by other means, often remaining
undetected for long periods, then causing occasional, difficult-to-diagnose
crashes.


4.2.8. Memory leak detection

Memcheck keeps track of all heap blocks issued in response to calls to
malloc/new et al. So when the program 'exits', it knows which blocks have not
been freed.

If --leak-check is set appropriately, for each remaining block, Memcheck
determines if the block is 'reachable' from pointers within the root-set. The
root-set consists of (a) general purpose registers of all threads, and (b)
initialised, aligned, pointer-sized data words in accessible client memory,
including stacks.

// skppied

The following is an example leak summary.

LEAK SUMMARY:
   definitely lost: 48 bytes in 3 blocks.
   indirectly lost: 32 bytes in 2 blocks.
     possibly lost: 96 bytes in 6 blocks.
   still reachable: 64 bytes in 4 blocks.
        suppressed: 0 bytes in 0 blocks.

If heuristics have been used to consider some blocks as reachable, the leak
summary details the heuristically reachable subset of 'still reachable:' per
heuristic. In the below example, of the 95 bytes still reachable, 87 bytes
(56+7+8+16) have been considered heuristically reachable.

LEAK SUMMARY:
   definitely lost: 4 bytes in 1 blocks
   indirectly lost: 0 bytes in 0 blocks
     possibly lost: 0 bytes in 0 blocks
   still reachable: 95 bytes in 6 blocks
                      of which reachable via heuristic:
                        stdstring          : 56 bytes in 2 blocks
                        length64           : 16 bytes in 1 blocks
                        newarray           : 7 bytes in 1 blocks
                        multipleinheritance: 8 bytes in 1 blocks
        suppressed: 0 bytes in 0 blocks

If `--leak-check=full` is specified, Memcheck will give details for each
definitely lost or possibly lost block, including where it was 'allocated'.
(Actually, it merges results for all blocks that have the same leak kind and
 sufficiently similar stack traces into a single "loss record". The
 --leak-resolution lets you control the meaning of "sufficiently similar".) 

It cannot tell you when or how or why the pointer to a leaked block was lost;
you have to work that out for yourself. In general, you should attempt to ensure
  your programs do not have any definitely lost or possibly lost blocks at exit.

For example:

8 bytes in 1 blocks are definitely lost in loss record 1 of 14
   at 0x........: malloc (vg_replace_malloc.c:...)
   by 0x........: mk (leak-tree.c:11)
   by 0x........: main (leak-tree.c:39)

88 (8 direct, 80 indirect) bytes in 1 blocks are definitely lost in loss record
13 of 14
   at 0x........: malloc (vg_replace_malloc.c:...)
   by 0x........: mk (leak-tree.c:11)
   by 0x........: main (leak-tree.c:25)

The first message describes a simple case of a single 8 byte block that has been
definitely lost. The second case mentions another 8 byte block that has been
definitely lost; the difference is that a further 80 bytes in other blocks are
indirectly lost because of this lost block. The loss records are not presented
in any notable order, so the loss record numbers aren't particularly meaningful.
The loss record numbers can be used in the Valgrind gdbserver to list the
addresses of the leaked blocks and/or give more details about how a block is
still reachable.


The option `--show-leak-kinds=<set>` controls the set of leak kinds to show when
--leak-check=full is specified.

The <set> of leak kinds is specified in one of the following ways:

    a comma separated list of one or more of definite indirect possible
    reachable.

    all to specify the complete set (all leak kinds).

    none for the empty set.

The 'default' value for the leak kinds to show is
--show-leak-kinds=definite,possible.

To also show the reachable and indirectly lost blocks in addition to the
definitely and possibly lost blocks, you can use --show-leak-kinds=all. 

To only show the reachable and indirectly lost blocks, use
--show-leak-kinds=indirect,reachable. The reachable and indirectly lost blocks
will then be presented as shown in the following two examples.

64 bytes in 4 blocks are still reachable in loss record 2 of 4
   at 0x........: malloc (vg_replace_malloc.c:177)
   by 0x........: mk (leak-cases.c:52)
   by 0x........: main (leak-cases.c:74)

32 bytes in 2 blocks are indirectly lost in loss record 1 of 4
   at 0x........: malloc (vg_replace_malloc.c:177)
   by 0x........: mk (leak-cases.c:52)
   by 0x........: main (leak-cases.c:80)

Because there are different kinds of leaks with different severities, an
interesting question is: which leaks should be counted as true "errors" and
which should not?

The answer to this question affects the numbers printed in the ERROR SUMMARY
line, and also the effect of the --error-exitcode option. First, a leak is
'only' counted as a true "error" if --leak-check=full is specified. Then, the
option --errors-for-leak-kinds=<set> controls the set of leak kinds to consider
as errors. The default value is --errors-for-leak-kinds=definite,possible 


={============================================================================
*kt_linux_tool_100* tool-valgrind-memcheck-case

{double-free}
When building on x86_64, it was found that linearsourcetest would fail with a
segmentation fault. Running with:

  valgrind --tool=memcheck ./linearsourcetest

reported several errors which looked like double-free. For example:

  ==27003==  Address 0xdab2c18 is 8 bytes inside a block of size 40 free'd
  ==27003==    at 0x4A077E6: free (vg_replace_malloc.c:446)
  ==27003==    by 0x54B3311: g_mutex_impl_free (in libglib-2.0.so.0.3200.4)
  ==27003==    by 0x54B3411: g_mutex_clear (in libglib-2.0.so.0.3200.4)
  ==27003==    by 0x4EA90A8: gst_object_finalize (gstobject.c:409)
  ==27003==    by 0x4EDF344: gst_element_finalize (gstelement.c:2955)
  ==27003==    by 0xB5360DA: gst_base_src_finalize (gstbasesrc.c:487)
  ==27003==    by 0xF7E587C: gst_vqesrc_finalize (gstvqesrc.c:557)
  ==27003==    by 0x51D8184: g_object_unref (in libgobject-2.0.so.0.3200.4)
  ==27003==    by 0x4EA8B77: gst_object_unref (gstobject.c:275)
  ==27003==    by 0x4EB0A18: gst_bin_remove_func (gstbin.c:1564)
  ==27003==    by 0x4EB0C7B: gst_bin_remove (gstbin.c:1617)
  ==27003==    by 0x4EADFBD: gst_bin_dispose (gstbin.c:528)

`gst_vqesrc_finalize()` takes calls `finalize()` whilst holding a lock obtained
with `GST_OBJECT_LOCK()`.

This is a bad idea because finalize() will cause `g_mutex_clear()` to be called
on the mutex on which the lock is held, and according to the glib documentation
[1]:

"Calling g_mutex_clear() on a locked mutex leads to undefined behaviour."

This patch moves the call to `finalize()` such that it is called once the mutex
is released. This makes the valgrind errors go away, `linearsourcetest` now
completes successfully.

[1] https://developer.gnome.org/glib/2.32/glib-Threads.html#g-mutex-clear                   


<mw>
There’s just one minor downside…

The CI run doesn’t have access to the unstripped binaries, which means that
the summary comes out with addresses for all the stack traces, rather than the
function names. The thread names are correct, which is handy, and the symbols
can be recovered using addr2line, but it’s a bit of a faff. I hereby pass this
to someone who knows CI better than me to make it do the post-processing, or
to let it run with the unstripped binaries J

The results will be available via the web interface while the test is running
(at the TASKENGINE_URL thing, with the summary in the results folder for the
 run), and in the _FILES.zip output when the test is complete.

Known Limitations

Other than the symbol information thing above, there’s one slightly annoying
thing  Our tests generally don’t terminate their processes. 

This means that valgrind won’t produce it’s memory leak summary at the end of
the run, which is kind of annoying. There’s 2 answers to this: 

1  Make the test terminate properly.

The obvious (if slightly unpleasant) answer is to ensure that the process you
care about exits cleanly. Some DMS tests do this already, although most don’t.
I’d like them all to, if possible, but that might be a stretch ;)

2  Use memcheck.h macros to make things happen.

There’s a header from valgrind (memcheck.h, which also requires valgrind.h)
  that allows you to send messages to the host valgrind process, and one of
  those is “please dump memory stats now”. An example of this can be found in
  this Gerrit review. The headers in that review come from the following
  release of valgrind: svn://svn.valgrind.org/valgrind/tags/VALGRIND_3_13_0.

There’s a second annoying consequence of this  We can only use the text output
format at the moment. The XML format is preferable, as Jenkins understands it
and gives you a nice clickable report, and draw pretty graphs of how much
you’ve improved your middleware. However, because valgrind won’t write the XML
closing tags until the end of the process, we can’t use this if the tests
don’t exit. Which is annoying. I guess someone could post-process the text
version into some basic Jenkins stats? Or we could pick a selection of tests
that do shut down cleanly and run them regularly in Jenkins? Who knows…


={============================================================================
*kt_linux_tool_100* tool-valgrind-massif

valgrind massif and massif-visualizer are powerful tools to examine memory use.

valgrind --tool=massif --pages-as-heap=no --show-below-main=yes uimanagerd -A -N
cp massif.out.* /mnt/hgfs/desktop_share/logs/uimanagerd/massif/

Massif outputs a data file that is best viewed with a reporting tool. It comes
with a basic plain text output tool (ms_print), but I find that hard to use. The
best graphical viewer I've found is massif-visualizer.

Building massif-visualizer

Install the deps:
sudo yum install graphviz-devel

You may have other unfulfilled build deps. Use:
sudo yum provides */<foo>

to discover which package they are in.

download, build, install kgraphviewer-2.1.1

ditto massif-visualizer-0.2


={============================================================================
*kt_linux_tool_001* tool-bugzilla

On debian:

apt-get install apache2 mysql-server libappconfig-perl libdate-calc-perl\
libtemplate-perl libmime-perl build-essential libdatetime-timezone-perl\
libdatetime-perl libemail-sender-perl libemail-mime-perl\
libemail-mime-modifier-perl libdbi-perl libdbd-mysql-perl libcgi-pm-perl\
libmath-random-isaac-perl libmath-random-isaac-xs-perl apache2-mpm-prefork\
libapache2-mod-perl2 libapache2-mod-perl2-dev libchart-perl libxml-perl\
libxml-twig-perl perlmagick libgd-graph-perl libtemplate-plugin-gd-perl\
libsoap-lite-perl libhtml-scrubber-perl libjson-rpc-perl libtheschwartz-perl\
libtest-taint-perl libauthen-radius-perl libfile-slurp-perl\
libencode-detect-perl libmodule-build-perl libnet-ldap-perl libfile-which-perl\
libauthen-sasl-perl libtemplate-perl-doc libfile-mimeinfo-perl\
libhtml-formattext-withlinks-perl libgd-dev libmysqlclient-dev lynx-cur\
graphviz python-sphinx rst2pdf

note:
removed libdaemon-generic-perl since cannot find it for debian.


={============================================================================
*kt_linux_tool_001* tool-ftp

https://security.appspot.com/vsftpd.html#features

About vsftpd

vsftpd is a GPL licensed FTP server for UNIX systems, including Linux. It is
secure and extremely fast. It is stable. Don't take my word for it, though.
Below, we will see evidence supporting all three assertions. We will also see
a list of a few important sites which are happily using vsftpd. This
demonstrates vsftpd is a mature and trusted solution. 

Online source / docs

Browse vsftpd's online source tree - including documentation. In particular,
note the content of the EXAMPLE subdirectory. Also, here is an HTML version of
  the manual page which lists all vsftpd config options. 

https://security.appspot.com/vsftpd/vsftpd_conf.html


<default-upload-folder>

change the ftp user directory to...

to change the default login directory for vsftpd, change the ftp user home
directory in /etc/passwd:

ftp:x:116:116:vsftpd daemon:/var/vsftpd:/bin/false

The ftp user (userID=116) home directory changed to /var/vsftpd This will
allow the default/anonymous/unknown user to land into a specific
place(/var/vsftpd).

I use this feature to lock down the ftp daemon to a read-only area with a set
of general files available for upload. I also have a special directory
(/var/vsftpd/upload) which is writeable but not delete-capable. That way,
someone can send up a file for share but only I, the site administrator can
  move/change/delete it.

Hope this helps.


pi@raspberrypi /srv/ftp $ cat /etc/passwd

ftp:x:109:65534::/srv/ftp:/bin/false

<conf>
$ cat /etc/vsftpd.conf


<ftp>
DESCRIPTION
     Ftp is the user interface to the Internet standard File Transfer
     Protocol.  The program allows a user to transfer files to and from a
     remote network site.

     -n    Restrains ftp from attempting “auto-login” upon initial connection.
     If auto-login is enabled, ftp will check the .netrc (see netrc(5)) file
     in the user's home directory for an entry describing an account on the
     remote machine.  If no entry exists, ftp will prompt for the remote
     machine login name (default is the user identity on the local machine),
     and, if necessary, prompt for a password and an account with which to login.


={============================================================================
*kt_linux_tool_001* tool-tftp

$ sudo apt-cache search tftp

tftp - Trivial file transfer protocol client
tftpd - Trivial file transfer protocol server


This setup of tftpd is being managed by the super server xinetd. So the
configuration files you're listing are the ones for the setting up of tftpd as
a service provided by xinetd.


kyoupark@st-castor-03:/tftpboot$ cat /etc/xinetd.d/tftp
# TFTP configuration
#
# Ansible managed
service tftp
{
  socket_type  = dgram
  protocol     = udp
  wait         = yes
  user         = root
  server       = /usr/sbin/in.tftpd
  server_args  = -c --secure -vvv /tftpboot
  disable      = no
  per_source   = 11
  cps          = 100 2
  flags        = IPv4
}

How do I check if TFTP is running in Linux?

run the command “netstat -an | grep 69” and see if anything’s listening on udp
port 69.

kyoupark@st-castor-03:/tftpboot$ netstat -an | grep 69
34:udp        0      0 0.0.0.0:69              0.0.0.0:*


kyoupark@kit-debian64:~$ tftp st-castor-03.cisco.com
tftp> ?
Commands may be abbreviated.  Commands are:

connect         connect to remote tftp
mode            set file transfer mode
put             send file
get             receive file
quit            exit tftp
verbose         toggle verbose mode
trace           toggle packet tracing
status          show current status
binary          set mode to octet
ascii           set mode to netascii
rexmt           set per-packet retransmission timeout
timeout         set total retransmission timeout
?               print help information

tftp> get kyoupark/readme.in
Received 26 bytes in 0.1 seconds


={============================================================================
*kt_linux_tool_001* tool-rpm

   GENERAL OPTIONS
       These options can be used in all the different modes.

       -v     Print verbose information - normally routine progress messages
       will be displayed.


   INSTALL AND UPGRADE OPTIONS
       In these options, PACKAGE_FILE can be either rpm binary file or ASCII
       package manifest (see PACKAGE SELECTION OPTIONS), and may be specified
       as an ftp or http URL, in  which  case the package will be downloaded
       before being installed. See FTP/HTTP OPTIONS for information on rpm’s
       internal ftp and http client support.

       The general form of an rpm install command is

       rpm {-i|--install} [install-options] PACKAGE_FILE ...

       This installs a new package.

       The general form of an rpm upgrade command is

       rpm {-U|--upgrade} [install-options] PACKAGE_FILE ...

       This  upgrades  or  installs the package currently installed to a newer
       version.  This is the same as install, except all other version(s) of
       the package are removed after the new package is installed.

       --force
              Same as using --replacepkgs, --replacefiles, and --oldpackage.

       -h, --hash
              Print 50 hash marks as the package archive is unpacked.  Use
              with -v|--verbose for a nicer display.

       --prefix NEWPATH
              For relocatable binary packages, translate all file paths that
              start with the installation prefix in the package relocation
              hint(s) to NEWPATH.

       --relocate OLDPATH=NEWPATH
              For relocatable binary packages, translate all file paths that
              start with OLDPATH in the package relocation hint(s) to NEWPATH.
              This option can be used repeatedly if  sev- eral OLDPATH’s in
              the package are to be relocated.


<RPM_BUILD_DIR>
$RPM_BUILD_ROOT (or the equivalent %{buildroot} SPEC file macro) always holds
the directory under which RPM will look for any files to package.


<ex>
// rpm info
rpm -qi package name
rpm -qip file.rpm

// rpm list 
rpm -ql package name
rpm -qlp file.rpm


<ex>
rpm -Uvh --force --nodeps \
crosstools_hf-uclibc-<uclibc version>-common-<toolchain version>.i386.rpm \
crosstools_hf-uclibc-<uclibc version>-mips-<toolchain version>.i386.rpm \
crosstools_hf-uclibc-<uclibc version>-mipsel-<toolchain version>.i386.rpm

<ex> when rpmbuild fails and made it to -1 for Debian build
http://rpm.org/user_doc/macros.html

// from rpm 4.8.0 from Debian
/usr/lib/rpmmacros
#       Default fuzz level for %patch in spec file.
%_default_patch_fuzz    0

// from rpm 4.4.2.3 on Centos. means no fizz check?
#      Default fuzz level for %patch in spec file. -1 means no fuzz explicitely.
%_default_patch_fuzz   -1


={============================================================================
*kt_linux_tool_001* tool-mail

$ sudo apt-get install mailutils

$ echo "Message Body Here" | mail -s "Subject Here" -a backup.zip user@example.com


={============================================================================
*kt_linux_tool_100* tool-package-apt

<list>
apt-cache search <program name>

// dpkg - package manager for Debian
// List all packages `installed`
$ dpkg-query -l

// List packages using a search pattern: It is possible to add a search pattern
// to list packages: 

$ dpkg-query -l 'ibus*'

dpkg --info skype-debian_4.2.0.11-1_i386.deb 
rmp -qa

There's an easy way to see the locations of all the files installed as part of
the package, using the dpkg utility.

dpkg -L `packagename`
rpm -ql `packagename`


<install>
To install a package:
sudo apt-get install tk8.5 

# -i, --install
$ sudo dpkg --install skype-debian_4.2.0.11-1_i386.deb

To remove a package:
sudo apt-get purge tk8.5 


<aptitude>
aptitude search ^wine
aptitude install "name"
aptitude remove "name"


{update-and-upgrade}
The apt-get install command is 'recommended' because it upgrades one or more
already installed packages without upgrading every package installed, whereas
the apt-get upgrade command installs the newest version of all currently
installed packages. In additon, `apt-get update` command must be executed before
an upgrade to resynchronize the package index files.


{update-error}
When see:
Update Error: Require Installation Of Untrusted Packages

Run manually on console

sudo apt-get upgrade xxx

The simplest way to get the required packages is using apt-get build-dep wine
respectively aptitude build-dep wine. 


{source-list}
/etc/apt/sources.list

As part of its operation, Apt uses a file that lists the 'sources' from which
packages can be obtained. This file is /etc/apt/sources.list.

The entries in this file normally follow this format (the entries below are
    fictitious and should not be used):

deb http://site.example.com/debian distribution component1 component2 component3
deb-src http://site.example.com/debian distribution component1 component2 component3

Archive type: deb or deb-src

The first word on each line, deb or deb-src, indicates the type of archive. Deb
indicates that the archive contains binary packages (deb), the pre-compiled
packages that we normally use. Deb-src indicates source packages, which are the
original program sources plus the Debian control file (.dsc) and the diff.gz
containing the changes needed for packaging the program.

Repository URL

The next entry on the line is a URL to the repository that you want to download
the packages from. The main list of Debian repository mirrors is located here.

Distribution

The 'distribution' can be either the release 'code' name / alias (wheezy,
    jessie, stretch, sid) or the release 'class' (oldstable, stable, testing,
      unstable) respectively. If you mean to be tracking a release class then
    use the class name, if you want to track a Debian point release, use the
    code name.

For example, if you have a system running Debian 8.2 "jessie" and don't want to
upgrade when Debian stretch releases, use 'jessie' instead of 'stable' for the
distribution. If you always want to help test the testing release, use
'testing'. If you are tracking stretch and want to stay with it from testing to
end of life, use 'stretch'.

Component: main, contrib, ...

main consists of DFSG-compliant packages, which do not rely on software outside
this area to operate. These are the only packages considered part of the Debian
distribution.

contrib packages contain DFSG-compliant software, but have dependencies not in
main (possibly packaged for Debian in non-free).

non-free contains software that does not comply with the DFSG.

Example sources.list for Debian 8 "Jessie"

deb http://httpredir.debian.org/debian jessie main
deb-src http://httpredir.debian.org/debian jessie main

deb http://httpredir.debian.org/debian jessie-updates main
deb-src http://httpredir.debian.org/debian jessie-updates main

deb http://security.debian.org/ jessie/updates main
deb-src http://security.debian.org/ jessie/updates main

If you also want the contrib and non-free components, add contrib non-free after
main. 

<package-manager-database>
$ cat /etc/apt/sources.list
# 
# deb cdrom:[Debian GNU/Linux 7.7.0 _Wheezy_ - Official amd64 CD Binary-1 20141018-13:06]/ wheezy main

deb http://mirrors.kernel.org/debian wheezy main contrib non-free
deb-src http://mirrors.kernel.org/debian wheezy main contrib non-free

deb http://security.debian.org/ wheezy/updates main
deb-src http://security.debian.org/ wheezy/updates main

# wheezy-updates, previously known as 'volatile'
# deb http://mirrors.kernel.org/debian wheezy-updates main
# deb-src http://mirrors.kernel.org/debian wheezy-updates main
# Devarch Packages
deb http://devarch-deb.dev.youview.co.uk:8080/job/DEBs/ws wheezy main contrib non-free
deb http://http.debian.net/debian/ wheezy-backports main contrib non-free
# deb http://ftp.uk.debian.org/debian wheezy main
deb http://mozilla.debian.net/ wheezy-backports iceweasel-release


={============================================================================
*kt_linux_tool_101* tool-pkg-config

The pkg-config program is used to retrieve information about installed libraries
in the system.  It is typically used to compile and link against one or more
libraries.  Here is a typical usage scenario in a Makefile:

program: program.c
   cc program.c $(pkg-config --cflags --libs gnomeui)

pkg-config retrieves information about packages from special metadata  files.
These files  are named after the package, and has a .pc extension.

It will additionally look in the colon-separated (on Windows,
    semicolon-separated) list of directories  specified  by the PKG_CONFIG_PATH
environment variable.

       --cflags
              This  prints pre-processor and compile flags required to compile
              the packages on the command line, including flags for all their
              dependencies.  Flags  are "compressed"  so that each identical
              flag appears only once. pkg-config exits with a nonzero code if it
              can't find metadata for one or more of the packages on the command
              line.

       --cflags-only-I
              This  prints the -I part of "--cflags". That is, it defines the
              header search path but doesn't specify anything else.

       --libs 
              This option is identical to "--cflags", only it prints  the  link
              flags.  As with  "--cflags",  duplicate  flags are merged
              (maintaining proper ordering), and flags for dependencies are
              included in the output.

       --list-all
              List all modules found in the pkg-config path.


       PKG_CONFIG_PATH
              A  colon-separated  (on  Windows,  semicolon-separated)  list  of
              directories  to  search  for  .pc  files.   The  default
              directory  will  always  be   searched   after   searching   the
              path;   the   default   is libdir/pkgconfig:datadir/pkgconfig
              where libdir is the libdir for pkg-config and datadir is the
              datadir for pkg-config when it was installed.

note:
Use to check the version found in .pc file in PKG_CONFIG_PATH:

pkg-config --modversion `name`

  --modversion                            output version for package

$ pkg-config --modversion gstreamer-1.0
1.6.2


={============================================================================
*kt_linux_tool_140* make-error build-error

17:09:01 make: *** [all] Error 2

egrep -an "Error [0-9]" build.log or make.log


={============================================================================
*kt_linux_tool_140* make-cmake

https://cmake.org/

tar xvf cmake-3.13.2.tar.gz
cd cmake-3.13.2/
./configure --prefix=/home/kyoupark/
make 
make install

CC=gcc CXX=g++ cmake -DCMAKE_BUILD_TYPE=Release -DLLVM_USE_LINKER=gold -DLLVM_ENABLE_ASSERTIONS=ON [-DLLVM_ENABLE_WERROR=ON] [-DLLVM_TARGETS_TO_BUILD=X86] /home/kyoupark/asan/llvm/llvm

// initially got error saying CMAKE_ROOT is not found and works when did:
// not sure that's because CMAKE_ROOT or ~/bin/cmake that's installed locally.

export CMAKE_ROOT=/home/kyoupark/share/cmake-3.13
CC=gcc CXX=g++ ~/bin/cmake -DCMAKE_BUILD_TYPE=Release -DLLVM_USE_LINKER=gold -DLLVM_ENABLE_ASSERTIONS=ON [-DLLVM_ENABLE_WERROR=ON] [-DLLVM_TARGETS_TO_BUILD=X86] /home/kyoupark/asan/llvm/llvm


={============================================================================
*kt_linux_tool_140* make-cmake-basic

https://cmake.org/cmake-tutorial/

Note that this example uses lower case commands in the CMakeLists.txt file.
Upper, lower, and mixed case commands are supported by CMake. 


http://llvm.org/docs/CMakePrimer.html

<list>
Lists of Lists

One of the more complicated patterns in CMake is lists of lists. Because a
list cannot contain an element with a semi-colon to construct a list of lists
you make a list of variable names that refer to other lists. For example:

set(list_of_lists a b c)
set(a 1 2 3)
set(b 4 5 6)
set(c 7 8 9)

With this layout you can iterate through the list of lists printing each value
with the following code:

foreach(list_name IN LISTS list_of_lists)
  foreach(value IN LISTS ${list_name})
    message(${value})
  endforeach()
endforeach()

You'll notice that the inner foreach loop’s list is doubly dereferenced. This
is because the first dereference turns list_name into the name of the sub-list
(a, b, or c in the example), then the second dereference is to get the value
of the list.

This pattern is used throughout CMake, the most common example is the compiler
flags options, which CMake refers to using the following variable expansions:
CMAKE_${LANGUAGE}_FLAGS and CMAKE_${LANGUAGE}_FLAGS_${CMAKE_BUILD_TYPE}.


<scope>
Scope

CMake inherently has a `directory-based scoping` Setting a variable in a
CMakeLists file, will set the variable for that file, and all subdirectories.
Variables set in a CMake module that is included in a CMakeLists file will be
set in the scope they are included from, and all subdirectories.

When a variable that is already set is set again in a subdirectory it
overrides the value in that scope and any deeper subdirectories.

The CMake set command provides two scope-related options. PARENT_SCOPE sets a
variable into the parent scope, and not the current scope. The CACHE option
sets the variable in the CMakeCache, which results in it being set in all
scopes. The CACHE option will not set a variable that already exists in the
CACHE unless the FORCE option is specified.

In addition to directory-based scope, CMake functions also have their own
scope. This means variables set inside functions do not bleed into the parent
scope. This is not true of macros, and it is for this reason LLVM prefers
functions over macros whenever reasonable.


Note
Unlike C-based languages, CMake’s loop and control flow blocks do not have
their own scopes.

The single most important thing to know about CMake’s if blocks coming from a
C background is that they do not have their own scope. Variables set inside
conditional blocks persist after the endif().


<module>
Modules, Functions and Macros

Modules

Modules are CMake's vehicle for enabling code reuse. CMake modules are just
CMake script files. They can contain code to execute on include as well as
definitions for commands.

In CMake macros and functions are universally referred to as commands, and
they are the primary method of defining code that can be called multiple
times.

In LLVM we have several CMake modules that are included as part of our
distribution for developers who don’t build our project from source. Those
modules are the fundamental pieces needed to build LLVM-based projects with
CMake. We also rely on modules as a way of organizing the build system’s
functionality for maintainability and re-use within LLVM projects.


Argument Handling

When defining a CMake command handling arguments is very useful. The examples
in this section will all use the CMake function block, but this all applies to
the macro block as well.

CMake commands can have named arguments, but all commands are implicitly
`variable argument` (note, variable number of argument). If the command has
named arguments they are required and must be specified at every call site.

Below is a trivial example of providing a wrapper function for CMake’s built
in function add_dependencies.

function(add_deps target)
  add_dependencies(${target} ${ARGV})
endfunction()

This example defines a new macro named add_deps which takes a required first
argument, and just calls another function passing through the first argument
and `all trailing arguments` When variable arguments are present CMake defines
them in a list named ARGV, and the count of the arguments is defined in ARGN.

CMake provides a module CMakeParseArguments which provides an implementation
of advanced argument parsing. We use this all over LLVM, and it is recommended
for any function that has complex argument-based behaviors or optional
  arguments. 

CMake's official documentation for the module is in the cmake-modules manpage,
  and is also available at the cmake-modules online documentation.

Note
As of CMake 3.5 the cmake_parse_arguments command has become a native command
and the CMakeParseArguments module is empty and only left around for
compatibility.


={============================================================================
*kt_linux_tool_140* make-cmake-add-subdir

./mheg
+-- CMakeLists.txt
+-- include
  +-- dbg.h
  +-- def.h
+-- main
  +-- main.c

To:

./mheg
+-- CMakeLists.txt
+-- include
  +-- dbg.h
  +-- def.h
+-- main
  +-- main.c
+-- mh5eng
  +-- CMakeLists.txt
  +-- sample.c
  +-- sample.h


The changes made to build:

../mheg/CMakeLists.txt

	 INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/mh5eng)

	 ADD_EXECUTABLE(${PROJECT_NAME}
		 main/main.c # uses sample.h
	 )

	 ADD_SUBDIRECTORY(mh5eng)

	 TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})

../mheg/mh5eng/CMakeLists.txt

	 SET(MH5ENG_HEADERS
		 sample.h
	 )

	 SET(MH5ENG_SOURCES
		 sample.c
	 )

	 ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES})

This create libmh5eng.a as a output


={============================================================================
*kt_linux_tool_140* make-cmake-mix-c-and-cpp

To compile the mix of c and cpp file, when try followings:

PROJECT(mhegproto C)

SET(MH5ENG_SOURCES
	xx.c
	mh5i_residentprogram_db.cpp
)

ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES} )

cmake do not compile cpp file although it is defined in the set variable. To
make it built, must add c++ build as below:

PROJECT(mhegproto C CXX)

From http://cmake.org/cmake/help/v2.8.8/cmake.html

project: Set a name for the entire project.

project(<projectname> [languageName1 languageName2 ... ] )

Sets the name of the project. Additionally this sets the variables
<projectName>_BINARY_DIR and <projectName>_SOURCE_DIR to the respective
values.

Optionally you can specify which languages your project supports. Example
languages are CXX (i.e.  C++), C, Fortran, etc. By default C and CXX are
enabled. E.g. if you do not have a C++ compiler, you can disable the check for
it by explicitly listing the languages you want to support, e.g. C. By using
the special language "NONE" all checks for any language can be disabled. If a
variable exists called CMAKE_PROJECT_<projectName>_INCLUDE_FILE, the file
pointed to by that variable will be included as the last step of the project
command.

note: by default? seems not.


={============================================================================
*kt_linux_tool_140* make-cmake-compile-flag

To enable preprocessor, set -E as below.

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "-E ${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")


={============================================================================
*kt_linux_tool_140* make-cmake-include

Found that building a library in the subdir cannot find necessary includes.

root CMakeList.txt:

INCLUDE(FindPkgConfig)
pkg_check_modules(APPS_PKGS REQUIRED
	capi-appfw-application
	dlog
	edje
	elementary
	ecore-x
	evas
	utilX
	x11
	aul
	ail
)

ADD_EXECUTABLE(${PROJECT_NAME}
	main/main.c
	main/viewmgr.c
	main/view_main.c
)

ADD_SUBDIRECTORY(mh5eng)
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)

Here there is no problem to use headers from packages such as elementary but
when add the same to the one in /mh5eng then cannot find headers. When looked
at flags used to build mh5eng, there is no necessary -I. This is the same when
add pkg_check_moudles in the mh5eng/CMakeList.txt. Thing is build faild to
update flags as expected.

The finding is that when move mh5eng then it builds without adding
pkg_check_moudles in the mh5eng/CMakeList.txt

...
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)
ADD_SUBDIRECTORY(mh5eng) [KT] moved to here

That suggests that where to put is important and not sure it is a GBS(git
    build system) or cmake itself problem. 


Found that this error still happens when build cpp file and the solution is:

From:
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

To:
MESSAGE("KT >>>>>PKGS_LDFLAGS>>>>>" : ${APPS_PKGS_CFLAGS})
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")
SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${EXTRA_CFLAGS}")

Turns out that PKGS_CFLAGS will have necessary includes depending on pkg
selection.


={============================================================================
*kt_linux_tool_140* make-cmake-link-flag

To solve {cyclic-dependencies} in link, can use this:

From defining each: 
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
...

To use group:
TARGET_LINK_LIBRARIES(${PROJECT_NAME} -Wl,--start-group mhdebug pfm mh5eng mh5dec mhv mah
-Wl,--end-group ${APPS_PKGS_LDFLAGS})

 On 05/19/2011 11:11 AM, Anton Sibilev wrote:
> Hello!
> I'm wondering how I can use "--start-group archives --end-group"
> linker flags with "Unix Makefiles".
> May be somebody know the right way?

You might specify these flags immediately in TARGET_LINK_LIBRARIES():

CMAKE_MINIMUM_REQUIRED(VERSION 2.8 FATAL_ERROR)
PROJECT(LINKERGROUPS C)
SET(CMAKE_VERBOSE_MAKEFILE ON)
FILE(WRITE ${CMAKE_BINARY_DIR}/f.c "void f(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g1.c "void g1(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g2.c "void g2(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/h.c "void h(void){}\n")
ADD_LIBRARY(f STATIC f.c)
ADD_LIBRARY(g1 STATIC g1.c)
ADD_LIBRARY(g2 STATIC g2.c)
ADD_LIBRARY(h STATIC h.c)
FILE(WRITE ${CMAKE_BINARY_DIR}/main.c "int main(void){return 0;}\n")
ADD_EXECUTABLE(main main.c)
TARGET_LINK_LIBRARIES(main f -Wl,--start-group g1 g2 -Wl,--end-group h)

However, do you really need these flags? Refer to the target properties
[IMPORTED_]LINK_INTERFACE_MULTIPLICITY[_<CONFIG>] and the documentation
of TARGET_LINK_LIBRARIES().

Regards,

Michael

LINK_INTERFACE_MULTIPLICITY

Repetition count for STATIC libraries with cyclic dependencies.

When linking to a STATIC library target with cyclic dependencies the linker
may need to scan more than once through the archives in the strongly connected
component of the dependency graph. CMake by default constructs the link line
so that the linker will scan through the component at least twice.  This
property specifies the minimum number of scans if it is larger than the
default. CMake uses the largest value specified by any target in a component.


={============================================================================
*kt_linux_tool_140* make-kernel-config

http://www.linuxjournal.com/article/6568?page=0,0

In versions before the 2.5 kernel, configuration was driven by a `Config.in`
file within every subdirectory and a main help file,
Documentation/Configure.help. The language used to describe the build process
  was based loosely on a shell-style language that would control which
  configuration options were presented to the user, depending on which options
  were currently presented. 

This article describes the format of the makefile and configuration files in
the 2.5 kernel and shows how to add a new driver to the build process.

Configuring the Kernel

To configure different kernel options, a user runs either a text-mode or a
graphical kernel configurator. The text-mode configurator can be run with make
config and prompts the user to select configuration options in order. 

The ncurses text version is more popular and is run with the `make menuconfig`
option. The graphical configurator is run with make xconfig and uses Qt as the
widget set.

When the kernel configurator is run, it reads the main kernel configuration
file, located in arch/i386/Kconfig for the i386 platform. Other architectures
have the main configuration files located in their main directories. This main
configuration file then includes other configuration files from the different
subdirectories in the kernel directory tree. Those configuration files also
can include other configuration files as needed. For example, the
arch/i386/Kconfig file contains the line:

source "sound/Kconfig"

which will read information from that file. This sound/Kconfig file then
includes a lot of other files: 

The sound/usb/Kconfig file describes all of the ALSA USB driver options, like
this:

# ALSA USB drivers
menu "ALSA USB devices"
    depends on SND!=n && USB!=n

config SND_USB_AUDIO
    tristate "USB Audio/MIDI driver"
    depends on SND && USB
    help
      Say 'Y' or 'M' to include support for
      USB audio and USB MIDI devices.
endmenu

The # character can be used to comment Kconfig files. Anything written after
it on the same line is not read by the configurator, but it is useful for
documenting what the file is for and what it should do.

The `menu` and `endmenu` commands tell the configurator to declare a new menu
level or new screen in some of the configuration programs. On the menu line,
the name of the menu should be specified within “ characters. For this file,
the menu is called "ALSA USB devices".

Menus and configuration options can be controlled to display or not. In this
example, the USB option menu is `only displayed if` the CONFIG_SND and
CONFIG_USB options are selected, which is controlled by the line depends on
SND!=n && USB!=n. To help decrease the amount of typing involved, all
configuration options automatically start with CONFIG, which is not used
within the configuration language. The valid states for a configuration option
are:

    y—the option is enabled.

    n—the option is not enabled.

    m—the option is set to be built as a module.

If both the CONFIG_SND and CONFIG_USB options are not set to n (meaning they
    are set either to be built in to the kernel or to build as a module), the
CONFIG_SND_USB_AUDIO option is presented to the user. This option can be set
to one of the three values, and it is described as a “tristate” value. The
text that should be shown to the user is "USB Audio/MIDI driver":

tristate "USB Audio/MIDI driver"

The valid values for describing a configuration variable are:

    bool—the variable can be set only to y or n.

    tristate—the variable can be set to y, n or m.

    int—the variable can be set to any numeric value.

This configuration option is controlled by a depends logic line, which follows
the same logic as a menu option. The CONFIG_SND_USB_AUDIO option depends on
`both the CONFIG_SND and CONFIG_USB options`, meaning that 

if one of these options is set to a module, then the CONFIG_SND_USB_AUDIO
  option also should be set to a module. 

If both of the controlling options are not enabled (meaning both are set to
    n), this option will not be displayed. 

If both of these options are set to y, this option can be selected as n, y or
m. All of this is defined with the simple line:

depends on SND && USB

Within the kernel code, the `configuration variable will be seen` (the
    `CONFIG_SND_USB_AUDIO` in the above example), so the code can test for it or
any other kernel configuration option's existence. However, using #ifdef
within a .c file to test for different configuration options is against the
kernel-style programming guidelines, which I covered in my article “Proper
Linux Kernel Coding Style” [LJ, July 2002, www.linuxjournal.com/article/5780].

Instead, limit the use of #ifdef to .h files, keeping the .c files cleaner and
easier to read.

Previously, the help text for a configuration option was placed in one big
Configuration.help file. Now the help text is placed right after the depends
line within the Kconfig file. It begins with a line containing either help or
---help---, followed by a number of lines of help text that are indented two
spaces from the help line.


Building the Kernel

The kernel is built with a system of individual makefiles that are all linked
together when the kernel is built, forming a large makefile. The individual
makefiles do not look like any standard makefile, but instead follow a special
format that is unique to the kernel build process. The makefile needs to build
only the necessary files, depending on the configuration options enabled, in
the proper format (as modules or built in to the kernel). 

As an example, drivers/usb/misc/Makefile in the 2.5.59 kernel release looks
like:

#
# Makefile for the rest of the USB drivers
# (the ones that don't fit into any other
# categories)
#
obj-$(CONFIG_USB_AUERSWALD)  += auerswald.o
obj-$(CONFIG_USB_BRLVGER)    += brlvger.o
obj-$(CONFIG_USB_EMI26)      += emi26.o
obj-$(CONFIG_USB_LCD)        += usblcd.o
obj-$(CONFIG_USB_RIO500)     += rio500.o
obj-$(CONFIG_USB_SPEEDTOUCH) += speedtch.o
obj-$(CONFIG_USB_TEST)       += usbtest.o
obj-$(CONFIG_USB_TIGL)       += tiglusb.o
obj-$(CONFIG_USB_USS720)     += uss720.o
speedtch-objs := speedtouch.o atmsar.o

The line:

obj-$(CONFIG_USB_LCD)        += usblcd.o

builds the usblcd.c file into a module if the CONFIG_USB_LCD configuration
option is set to m. Otherwise, it is built into the kernel directly if that
configuration option is set to y. This step is all that is necessary to add to
a kernel makefile if the module is made from only a single .c file.

If the driver consists of multiple .c files, the name of the files needs to be
listed on separate lines, along with the name of the module that this driver
is called. In the previous example file, this listing of file and driver names
looks like:

obj-$(CONFIG_USB_SPEEDTOUCH) += speedtch.o

and

speedtch-objs := speedtouch.o atmsar.o

The first line controls whether the speedtch module is built. If it is, the
line indicates whether it is compiled into the kernel or stands as a module.
The second line explains that the speedtouch.c and atmsar.c files will be
built into .o files and then linked together into the speedtch.o module.

In older kernels, if a file exported symbols, it needed to be explicitly
mentioned in the kernel makefiles. In 2.5 and later kernels, that mention is
no longer necessary.


Adding a New Driver to the Build Process

To add a new driver to the kernel build process, a single line needs to be
added if the driver is contained within a single file. Based on the previous
example of the FooBar USB speaker device, the line:

obj-$(CONFIG_SND_USB_FOOBAR) += usbfoobar.o

is added to sound/usb/Makefile.

If the driver is contained in two files, such as foobar1.c and foobar2.c, an
additional line needs to be added:

usbfoobar-objs := foobar1.o foobar2.o

Conclusion

The kernel configuration and build process in the 2.5 kernel is much simpler
and more flexible than in the previous kernel versions. Thanks go to Roman
Zippel and Kai Germaschewski for doing the work to make it easier for kernel
developers to focus on writing code and not have to worry about the
intricacies of the kernel build process.

http://www.linuxjournal.com/content/kbuild-linux-kernel-build-system?page=0,0
http://www.linuxfromscratch.org/hints/downloads/files/kernel-configuration.txt


={============================================================================
*kt_linux_tool_140* make-kernel-oldconfig

make oldconfig

Review changes between kernel versions and update to create a new .config for
the kernel. 

// builds `conf`
// kyoupark@ukstbuild2:~/si_logs/spk/darwin-spk-1.19$ ls package/config/conf
// conf        conf.c      confdata.c  conf.o

$(CONFIG)/conf:
	@mkdir -p $(CONFIG)/buildroot-config
	$(MAKE) CC="$(HOSTCC)" -C $(CONFIG) conf
	-@if [ ! -f .config ]; then \
		cp $(CONFIG_DEFCONFIG) .config; \
	fi

oldconfig: $(CONFIG)/conf
	@mkdir -p $(CONFIG)/buildroot-config
	@KCONFIG_AUTOCONFIG=$(CONFIG)/buildroot-config/auto.conf \
		KCONFIG_AUTOHEADER=$(CONFIG)/buildroot-config/autoconf.h \
		$(CONFIG)/conf -o $(CONFIG_CONFIG_IN)

// runs conf
KCONFIG_AUTOCONFIG=package/config/buildroot-config/auto.conf \
KCONFIG_AUTOHEADER=package/config/buildroot-config/autoconf.h \
package/config/conf -o Config.in

It reads the existing .config file and prompts the user for options in the
current kernel source that are not found in the file. This is useful when
taking an existing configuration and moving it to a new kernel.

*
* Buildroot Configuration
*
Target Architecture
  1. alpha (BR2_alpha)
  2. arm (BR2_arm)
  3. armeb (BR2_armeb)
  4. avr32 (BR2_avr32)
  5. cris (BR2_cris)
  6. ia64 (BR2_ia64)
  7. i386 (BR2_i386)
  8. m68k (BR2_m68k)
> 9. mips (BR2_mips)
  10. mipsel (BR2_mipsel)
  11. nios2 (BR2_nios2)
  12. powerpc (BR2_powerpc)
  13. s390 (BR2_s390)
  14. superh (BR2_sh)
  15. superh64 (BR2_sh64)
  16. sparc (BR2_sparc)
  17. sparc64 (BR2_sparc64)
  18. x86_64 (BR2_x86_64)
choice[1-18?]: 9


={============================================================================
*kt_linux_tool_140* make-gmake-debug

The make --debug is identical to make -d but you can also specify:

make --debug=FLAGS
where flags can be:

a for all debugging (same as make -d and make --debug).
b for basic debugging.
v for slightly more verbose basic debugging.
i for implicit rules.
j for invocation information.
m for information during makefile remakes.


={============================================================================
*kt_linux_tool_140* make-gmake-sample

# simple makefile from gmake

FILE = use-main.cpp
CC = g++ -std=c++0x -I.

main : main.o fsm.o
	echo '>> bulid main'
	$(CC) -o out main.o fsm.o

fsm.o : Fsm.c
	echo '>> build fsm.c'
	$(CC) -o fsm.o -c Fsm.c

main.o : main.cpp
	echo '>> build main.cpp'
	$(CC) -o main.o -c main.cpp

clean :
	rm main.o fsm.o


={============================================================================
*kt_linux_tool_140* make-gmake-rule

2.1 What a Rule Looks Like

target ... : prerequisites ...
  recipe
  ...

A `target` is usually `the name of a file` that is generated by a program;
examples of targets are executable or object files. Can also be the `name of
  an action` to carry out, such as ‘clean’.

A `prerequisite` is a file that is used as input to create the target. A target
often depends on several files.

A `recipe` is an action that make carries out. A recipe may have more than one
command, either on the same line or each on its own line. If the line begins
with a tab, it will be considered a recipe line.
  
Please note: you need to put a `tab character` at the beginning of every
recipe line! This is an obscurity that catches the unwary. 

Usually a recipe is in a rule with prerequisites and serves to create a target
file if any of the prerequisites change. However, the rule that specifies a
recipe for the target `need not have prerequisites` For example, the rule
containing the delete command associated with the target ‘clean’ does not have
prerequisites.

A rule, then, explains `how and when` to remake certain files which are the
targets of the particular rule. make carries out the recipe on the
prerequisites to create or update the target. A rule can also explain how and
when to carry out an action.


When a target is a file, it needs to be recompiled or relinked if any of its
prerequisites change. In addition, any prerequisites that are themselves
automatically generated should be updated first.


<default-target>
By default, make starts with the `first target` (not targets whose names start
with ‘.’). This is called the `default goal`

`edit` target must be done if the file edit `does not exist`, or if any of the
object files are newer than it.


<implicit-rule>
2.5 Letting make Deduce the Recipes

It is not necessary to spell out the recipes for compiling the individual C
source files, because make can figure them out: it has an `implicit rule` for
updating a ‘.o’ file from a correspondingly named ‘.c’ file using a ‘cc -c’
command.

# example

objects = main.o kbd.o command.o display.o \
  insert.o search.o files.o utils.o

edit : $(objects)
  cc -o edit $(objects)

main.o : main.c defs.h
  cc -c main.c

kbd.o : kbd.c defs.h command.h
  cc -c kbd.c

command.o : command.c defs.h command.h
  cc -c command.c

display.o : display.c defs.h buffer.h
  cc -c display.c

insert.o : insert.c defs.h buffer.h
  cc -c insert.c

search.o : search.c defs.h buffer.h
  cc -c search.c

files.o : files.c defs.h buffer.h command.h
  cc -c files.c

utils.o : utils.c defs.h
  cc -c utils.c

clean :
  rm edit $(objects)


# When use `implicit rule`

objects = main.o kbd.o command.o display.o \
  insert.o search.o files.o utils.o

edit : $(objects)
  cc -o edit $(objects)

main.o : defs.h
kbd.o : defs.h command.h
command.o : defs.h command.h
display.o : defs.h buffer.h
insert.o : defs.h buffer.h
search.o : defs.h buffer.h
files.o : defs.h buffer.h command.h
utils.o : defs.h


4.2 Types of Prerequisites

you have a situation where you want to impose a specific ordering on the rules
to be invoked without forcing the target to be updated if one of those rules
is executed.

Order-only prerequisites can be specified by placing a pipe symbol (|) in the
prerequisites list: any prerequisites to the left of the pipe symbol are
normal; any prerequisites to the right are order-only:

targets : normal-prerequisites | order-only-prerequisites

<ex>
Consider an example where your targets are to be placed in a separate
directory, and that directory might not exist before make is run. In this
situation, you want the directory to be created before any targets are placed
into it but, because the timestamps on directories change whenever a file is
added, removed, or renamed, we certainly don’t want to rebuild all the targets
whenever the directory’s timestamp changes. One way to manage this is with
order-only prerequisites: make the directory an order-only prerequisite on all
the targets:

OBJDIR := objdir
OBJS := $(addprefix $(OBJDIR)/,foo.o bar.o baz.o)

$(OBJDIR)/%.o : %.c
  $(COMPILE.c) $(OUTPUT_OPTION) $<

all: $(OBJS)

$(OBJS): | $(OBJDIR)

$(OBJDIR):
  mkdir $(OBJDIR)

Now the rule to create the objdir directory will be run, if needed, before any
  ‘.o’ is built, but no ‘.o’ will be built because the objdir directory
  timestamp changed.


<wildcard>
4.3 Using Wildcard Characters in File Names

Wildcard expansion is performed by make automatically in `targets` and in
`prerequisites`. In recipes, the shell is responsible for wildcard expansion.
In other contexts, wildcard expansion happens only if you request it
explicitly with the `wildcard function`

Wildcard expansion does not happen when you define a variable. Thus, if you
write this: objects = *.o then the value of the variable objects is the actual
string ‘*.o’. To set objects to the expansion, instead use:

objects := $(wildcard *.o)


4.3.3 The Function wildcard

Wildcard expansion happens automatically in rules. But wildcard expansion does
not normally take place `when a variable is set`, or inside the arguments of a
function. If you want to do wildcard expansion in such places, you need to use
the wildcard function, like this:

$(wildcard pattern...)

This string, used anywhere in a makefile, is replaced by a space-separated
list of names of `existing files` that match one of the given file name
patterns. If no existing file name matches a pattern, then that pattern is
omitted from the output of the wildcard function.

Note that this is different from how unmatched wildcards behave in rules,
where they are used verbatim rather than ignored as when the below ex.

<ex>
// ex from 4.3.2 Pitfalls of Using Wildcards

An example of a naive way of using wildcard expansion, that does not do what
you would intend. Suppose you would like to say that the executable file foo
is made from all the object files in the directory, and you write this:

objects = *.o

foo : $(objects)
  cc -o foo $(CFLAGS) $(objects)

The value of objects is the actual string ‘*.o’. Wildcard expansion happens in
the rule for foo, so that each existing ‘.o’ file becomes a prerequisite of
foo and will be recompiled if necessary.

But what if you delete all the ‘.o’ files?

When a wildcard matches no files, it is left as it is, so then foo will depend
on the oddly-named file *.o. Since no such file is likely to exist, make will
give you an error saying it "cannot figure out how to make *.o." This is not
what you want!

note:
If there are no .o file, no matches and is left as it is, string, so:

foo : *.o 
  cc -o foo $(CFLAGS) *.o 

One use of the wildcard function is to get a list of all the C source files in
a directory, like this:

$(wildcard *.c)

We can change the list of C source files into a list of object files by
replacing the ‘.c’ suffix with ‘.o’ in the result, like this:

$(patsubst %.c,%.o,$(wildcard *.c))

Thus, a makefile to compile all C source files in the directory and then link
them together could be written as follows:

objects := $(patsubst %.c,%.o,$(wildcard *.c))

foo : $(objects)
  cc -o foo $(objects)

This takes advantage of the implicit rule for compiling C programs, so there
is no need to write explicit rules for compiling the files.


<ex>
# use to check if a file exist
  ifeq (,$(wildcard $(PLATFORMS_TOOLS_ROOT)/$(NDS_FUSIONOS)/BLD_$(NDS_PLATFORM)/platform_cfg))
    $(error BLD_$(NDS_PLATFORM) is not the $(NDS_FUSIONOS) vob. Please check \
        NDS_PLATFORM value or set NDS_FUSIONOS to the vob the platform resides in)
  else


={============================================================================
*kt_linux_tool_140* make-gmake-rule-multiple

4.10 Multiple Rules for One Target

One file can be the target of several rules. All the prerequisites mentioned
in all the rules are `merged into one list` of prerequisites for the target.
If the target is older than any prerequisite from any rule, the recipe is
executed.

There can only be one recipe to be executed for a file. If more than one rule
gives a recipe for the same file, make uses the last one given and prints an
error message.

An extra rule with just prerequisites can be used to give a few extra
prerequisites to many files at once. For example, makefiles often have a
variable, such as objects, containing a list of all the compiler output files
in the system being made. An easy way to say that all of them must be
recompiled `if config.h changes` is to write the following:

objects = foo.o bar.o
foo.o : defs.h
bar.o : defs.h test.h
$(objects) : config.h

This could be inserted or taken out without changing the rules that really
  specify how to make the object files, making it a convenient form to use if
  you wish to add the additional prerequisite intermittently.


={============================================================================
*kt_linux_tool_140* make-gmake-rule-implicit

10 Using Implicit Rules

"Implicit rules" tell 'make' how to use customary techniques so that you do
not have to specify them in detail when you want to use them. For example,
there is an implicit rule for C compilation.  

File names determine which implicit rules are run.  For example, C compilation
typically takes a '.c' file and makes a '.o' file.  So 'make' applies the
implicit rule for C compilation when it sees this combination of file name
endings.

The built-in implicit rules use several variables in their recipes so that, by
changing the values of the variables, you can change the way the implicit rule
works. For example, the variable 'CFLAGS' controls the flags given to the C
compiler by the implicit rule for C compilation.


10.1 Using Implicit Rules

To allow 'make' to find a customary method for updating a target file, all you
have to do is refrain from specifying recipes yourself. Either write a rule
with no recipe, or don't write a rule at all.  

Then 'make' will figure out which implicit rule to use based on which kind of
source file exists or can be made.

For example, suppose the makefile looks like this:

     foo : foo.o bar.o
             cc -o foo foo.o bar.o $(CFLAGS) $(LDFLAGS)

Because you mention 'foo.o' but do not give a rule for it, 'make' will
automatically look for an implicit rule that tells how to update it.

If an implicit rule is found, it can supply both a recipe and one or more
prerequisites (the source files). You would want to write a rule for 'foo.o'
with no recipe if you need to specify additional prerequisites, such as header
files, that the implicit rule cannot supply.

note: really? different from the previous example?

<which-rule-to-apply>
Each implicit rule has a target pattern and prerequisite patterns. There may
be many implicit rules with the same target pattern. For example, numerous
rules make '.o' files: one, from a '.c' file with the C compiler; another,
from a '.p' file with the Pascal compiler; and so on. 

The rule that actually applies is the one whose prerequisites exist or can be
made.  So, if you have a file 'foo.c', 'make' will run the C compiler;
otherwise, if you have a file 'foo.p', 'make' will run the Pascal compiler;
and so on.

Note that `explicit prerequisites` do not influence implicit rule search. For
example, consider this explicit rule:

     foo.o: foo.p

The prerequisite on 'foo.p' does not necessarily mean that 'make' will remake
'foo.o' according to the implicit rule to make an object file, a '.o' file,
  from a Pascal source file, a '.p' file. 

For example, `if 'foo.c' also exists`, the implicit rule to make an object
file from a C source file is used instead, because it `appears before` the
Pascal rule in the list of predefined implicit rules


10.2 Catalogue of Built-In Rules

Here is a catalogue of predefined implicit rules which are always available
unless the makefile explicitly overrides or cancels them.

This manual only documents the default rules available on POSIX-based
operating systems. Other operating systems may have different sets of default
rules. To see the full list of default rules and variables available in your
version of GNU 'make', run 'make -p' in a directory with no makefile.

Many of the predefined implicit rules are implemented in 'make' as suffix
rules, so which ones will be defined depends on the "suffix list".  

note: simplified

The default suffix list is: '.out', '.a', '.ln', '.o', '.c', '.cc', '.C',
'.cpp', '.s', '.S', '.h', '.sh'.  


Compiling C programs
     'N.o' is made automatically from 'N.c' with a recipe of the form 
     '$(CC) $(CPPFLAGS) $(CFLAGS) -c'.

Compiling C++ programs
     'N.o' is made automatically from 'N.cc', 'N.cpp', or 'N.C' with a recipe
     of the form 
     '$(CXX) $(CPPFLAGS) $(CXXFLAGS) -c'.  

     We encourage you to use the suffix '.cc' for C++ source files instead of
     '.C'.


10.3 Variables Used by Implicit Rules

You can alter the values of these variables in the makefile, with arguments to
'make', or in the environment to alter how the implicit rules work without
redefining the rules themselves.

By redefining 'CFLAGS' to be '-g', you could pass the '-g' option to each
compilation.

The variables used in implicit rules fall into two classes: those that are
names of programs (like 'CC') and those that contain arguments for the
programs (like 'CFLAGS'). (The "name of a program" may also contain some
    command arguments, but it must start with an actual executable program
    name.) 

If a variable value contains more than one argument, separate them with
spaces.

The following tables describe of some of the more commonly-used predefined
variables. This list is `not exhaustive`, and the default values shown here
may not be what 'make' selects for your environment. To see the complete list
of predefined variables for your instance of GNU 'make' you can run 'make -p'
in a directory with no makefiles.


Here is a table of some of the more common variables used as names of programs
in built-in rules:

'AR'
     Archive-maintaining program; default 'ar'.

'AS'
     Program for compiling assembly files; default 'as'.

'CC'
     Program for compiling C programs; default 'cc'.

'CXX'
     Program for compiling C++ programs; default 'g++'.

'CPP'
     Program for running the C preprocessor, with results to standard
     output; default '$(CC) -E'.

'RM'
     Command to remove a file; default 'rm -f'.

Here is a table of variables whose values are additional arguments for the
programs above. The default values for all of these is the empty string,
unless otherwise noted.

'CFLAGS'
     Extra flags to give to the C compiler.

'CXXFLAGS'
     Extra flags to give to the C++ compiler.

'CPPFLAGS'
     Extra flags to give to the C preprocessor and programs that use it
     (the C and Fortran compilers).

'LDFLAGS'
     Extra flags to give to compilers when they are supposed to invoke
     the linker, 'ld', such as '-L'.  Libraries ('-lfoo') should be
     added to the 'LDLIBS' variable instead.


={============================================================================
*kt_linux_tool_140* make-gmake-rule-automatic-variable

10.5 Defining and Redefining Pattern Rules

You define an implicit rule by writing a "pattern rule". A pattern rule looks
like an ordinary rule, except that its target contains the character '%'
(exactly one of them). The target `is considered a pattern` for matching file
names; the '%' can match any nonempty substring, while other characters match
only themselves. The prerequisites likewise use '%' to show how their names
relate to the target name.

Thus, a pattern rule '%.o : %.c' says how to make any file 'STEM.o' from
another file 'STEM.c'.

Note that expansion using '%' in pattern rules occurs *after* any variable or
function expansions, which take place when the makefile is read.


10.5.1 Introduction to Pattern Rules

<gmake-stem>
For example, '%.c' as a pattern matches any file name that ends in '.c'.
's.%.c' as a pattern matches any file name that starts with 's.', ends in '.c'
and is at least five characters long. (There must be at least one character to
    match the '%'.) The substring that the '%' matches is called the `"stem"`.

Thus, a rule of the form

     %.o : %.c ; RECIPE...

specifies how to make a file 'N.o', with another file 'N.c' as its
prerequisite, provided that 'N.c' `exists or can be made`


10.5.2 Pattern Rule Examples

Here are some examples of pattern rules actually predefined in 'make'. First,
the rule that compiles '.c' files into '.o' files:

     %.o : %.c
             $(CC) -c $(CFLAGS) $(CPPFLAGS) $< -o $@

defines a rule that can make any file 'X.o' from 'X.c'. The recipe uses the
automatic variables '$@' and '$<' to substitute the names of the target file
and the source file in each case where the rule applies.


10.5.3 Automatic Variables

<why-automatic-variable>
Suppose you are writing a pattern rule to compile a '.c' file into a '.o'
file: how do you write the 'cc' command so that it operates on the right
`source file name`?  You cannot write the name in the recipe, because the name
is different each time the implicit rule is applied.

What you do is use a special feature of 'make', the "automatic variables".
These variables have values computed afresh for each rule that is executed,
based on the target and prerequisites of the rule. In this example, you would
  use '$@' for the object file name and '$<' for the source file name.

It's very important that you recognize the limited scope in which automatic
variable values are available: they only have values within the recipe.


Here is a table of automatic variables:

'$@'
     `The file name of the target` of the rule. If the target is an archive
     member, then '$@' is the name of the archive file. In a pattern rule that
     has multiple targets, '$@' is the name of whichever target caused the
     rule's recipe to be run.

// '$%'
//      The target member name, when the target is an archive member.  For
//      example, if the target is 'foo.a(bar.o)' then '$%' is 'bar.o' and '$@' is
//      'foo.a'. '$%' is empty when the target is not an archive member.

'$<'
     The name of the first prerequisite. If the target got its recipe from an
     implicit rule, this will be the first prerequisite added by the implicit
     rule.

'$?'
     The names of all the prerequisites `that are newer than` the target, with
     spaces between them.  

'$^'
     `The names of all the prerequisites, with spaces between them`  A target
     has only one prerequisite on each other file it depends on, no matter how
     many times each file is listed as a prerequisite.  So if you list a
     prerequisite more than once for a target, the value of '$^' contains just
     one copy of the name.  This list does *not* contain any of the order-only
     prerequisites; for those see the '$|' variable, below.

'$+'
     This is like '$^', but prerequisites listed more than once are duplicated
     in the order they were listed in the makefile. This is primarily useful
     for use in linking commands where it is meaningful to repeat library file
       names in a particular order.

'$|'
     The names of all the order-only prerequisites, with spaces between
     them.

'$*'
     The stem with which an implicit rule matches. If the target is
     'dir/a.foo.b' and the target pattern is 'a.%.b' then the stem is
     'dir/foo'. The stem is useful for constructing names of related files.

four have values that are single file names, and three have values that are
lists of file names. These seven have `variants` that get just the file's
directory name or just the file name within the directory.  The variant
variables' names are formed by appending 'D' or 'F', respectively.  These
variants are semi-obsolete in GNU 'make' since the functions 'dir' and
'notdir' can be used to get a similar effect.


Here is a table of the variants:

'$(@D)'
     The directory part of the file name of the target, with the trailing
     slash removed. If the value of '$@' is 'dir/foo.o' then '$(@D)' is 'dir'.
     This value is '.' if '$@' does not contain a slash.

'$(@F)'
     The file-within-directory part of the file name of the target. If the
     value of '$@' is 'dir/foo.o' then '$(@F)' is 'foo.o'. '$(@F)' is
     equivalent to '$(notdir $@)'.


10.7 Old-Fashioned Suffix Rules

"Suffix rules" are the old-fashioned way of defining implicit rules for
'make'. Suffix rules `are obsolete` because pattern rules are more general and
clearer. They are supported in GNU 'make' for compatibility with old
makefiles. They come in two kinds: "double-suffix" and "single-suffix".


={============================================================================
*kt_linux_tool_140* make-gmake-include

3.1 What Makefiles Contain

`Comments within a recipe` are passed to the shell, just as with any other
recipe text.  The shell decides how to interpret it: whether or not this is a
comment is up to the shell.


3.2 What Name to Give Your Makefile

Normally you should call your makefile either makefile or Makefile. We
recommend Makefile because it appears prominently near the beginning of a
directory listing,


3.3 Including Other Makefiles

The directive is a line in the makefile that looks like this:

include filenames...

filenames can contain shell file name patterns.

When make processes an include directive, it suspends reading of the
containing makefile and reads from each listed file in turn. When that is
finished, make resumes reading the makefile in which the directive appears.

<use-cases>
One occasion for using include directives is when several programs, handled by
individual makefiles in various directories, need to use a common set of
variable definitions (see Section 6.5 [Setting Variables], page 65) or pattern
rules (see Section 10.5 [Defining and Redefining Pattern Rules], page 118).

Another such occasion is when you want to generate prerequisites from source
files automatically; the prerequisites can be put in a file that is included
by the main makefile.  This practice is generally cleaner than that of somehow
appending the prerequisites to the end of the main makefile as has been
traditionally done with other versions of make. See Section 4.13 [Automatic
Prerequisites], page 38.

<search>
If an included makefile cannot be found in any of these directories, a warning
message is generated, but it is not an immediately fatal error; processing of
the makefile containing the include continues.

Only after it has tried to find a way to remake a makefile and failed, will
make diagnose the missing makefile as a fatal error.

If you want make to simply ignore a makefile which does not exist or cannot be
remade, `with no error message`, use the -include directive instead of include,
like this:

-include filenames...


={============================================================================
*kt_linux_tool_140* make-gmake-vpath make-o-variable

4.4 Searching Directories for Prerequisites

For large systems, it is often desirable to put sources in a separate
directory from the binaries. The directory search features of make facilitate
this by searching several directories automatically to find a prerequisite.

4.4.1 VPATH: Search Path for All Prerequisites

The value of the make variable VPATH specifies a list of directories that make
should search.  Most often, the directories are expected to contain
prerequisite files that are not in the current directory; however, make uses
VPATH as a search list for both prerequisites and targets of rules.

note:

8.5 Building out-of-tree

As default, everything built by Buildroot is stored in the directory output in
the Buildroot tree. Buildroot also supports building out of tree with a syntax
similar to the Linux kernel. To use it, add O=<directory> to the make command
line:

$ make O=/tmp/build

Or:

$ cd /tmp/build; make O=$PWD -C path/to/buildroot

All the output files will be located under /tmp/build. If the O path does not
exist, Buildroot will create it.

When using out-of-tree builds, the Buildroot .config and temporary files are
also stored in the output directory. This means that you can safely run
multiple builds in parallel using the same source tree as long as they use
unique output directories.  For ease of use, Buildroot generates a Makefile
wrapper in the output directory - so after the first run, you no longer need
to pass O=<...> and -C <...>, simply run (in the output directory):

$ make <target>


={============================================================================
*kt_linux_tool_140* make-gmake-phony

Targets that do not refer to files but are just actions are `phony targets`

4.5 Phony Targets

A phony target is one that is not really the name of a file; rather it is just
a name for a recipe to be executed when you make an explicit request. There
are two reasons to use a phony target: to avoid a conflict with a file of the
same name, and to improve performance.

<ex>
2.7 Rules for Cleaning the Directory

clean:
rm edit $(objects)

.PHONY : clean
clean :
  rm edit $(objects)

This prevents make from getting confused by an `actual file` called clean and
causes it to continue in spite of errors from rm. 

A rule such as this should not be placed at the beginning of the makefile,
because we do not want it to run by default!


Phony targets are also useful in conjunction with recursive invocations of make

<ex>
The makefile will often contain a variable which lists a number of
sub-directories to be built.

A simplistic way to handle this is to define one rule with a recipe that loops
over the sub-directories, like this:

SUBDIRS = foo bar baz

subdirs:
  for dir in $(SUBDIRS); do \
    $(MAKE) -C $$dir; \
  done

There are problems with this method. 

First, any error detected in a sub-make is ignored by this rule, so it will
continue to build the rest of the directories even when one fails. 

// note: not get it
// This can be overcome by adding shell commands to note the error and exit, but
// then it will do so even if make is invoked with the -k option, which is
// unfortunate. 

Second, and perhaps more importantly, you cannot take advantage of make’s
  ability to build targets in parallel, since there is only one rule.


By declaring the sub-directories as .PHONY targets you can remove these
problems:

SUBDIRS = foo bar baz

.PHONY: subdirs $(SUBDIRS)

subdirs: $(SUBDIRS)

$(SUBDIRS):
  $(MAKE) -C $@

foo: baz


  * the foo sub-directory cannot be built until after the baz sub-directory is
    complete

    this kind of relationship declaration is particularly important when
    attempting parallel builds.

  * The implicit rule search is skipped for .PHONY targets. This is why
    declaring a target as .PHONY is good for performance

  * A phony target should not be a prerequisite of a real target file; if it
    is, its recipe will be run every time make goes to update that file.

  * Phony targets can have prerequisites. When one directory contains multiple
    programs, it is most convenient to describe all of the programs in one
    makefile ./Makefile. 
  
    Since the target remade by default will be the first one in the makefile,
    it is common to make this a phony target named ‘all’ and give it, as
    prerequisites, all the individual programs. For example:

    all : prog1 prog2 prog3

    .PHONY : all

    prog1 : prog1.o utils.o
      cc -o prog1 prog1.o utils.o

    prog2 : prog2.o
      cc -o prog2 prog2.o

    prog3 : prog3.o sort.o utils.o
      cc -o prog3 prog3.o sort.o utils.o

    Now you can say just ‘make’ to remake all three programs, or specify as
    arguments the ones to remake (as in ‘make prog1 prog3’).


When one phony target is a prerequisite of another, it serves as a subroutine
of the other. For example, here ‘make cleanall’ will delete the object files,
    the difference files, and the file program:

.PHONY: cleanall cleanobj cleandiff

cleanall : cleanobj cleandiff
  rm program

cleanobj :
  rm *.o

cleandiff :
  rm *.diff


4.7 Empty Target Files to Record Events

The empty target is a variant of the phony target; it is used to hold recipes
for an action that you request explicitly from time to time. Unlike a phony
  target, this target file can really exist; but the file’s contents do not
    matter, and usually are empty.

The purpose of the empty target file is to record, with its last-modification
time, when the rule’s recipe was last executed. It does so because one of the
commands in the recipe is a touch command to update the target file.

`The empty target file` should have some prerequisites. When you ask to remake
the empty target, the recipe is executed if any prerequisite is more recent
than the target; in other words, if a prerequisite has changed since the last
time you remade the target. Here is an example:

print: foo.c bar.c
  lpr -p $?
  touch print

With this rule, ‘make print’ will execute the lpr command if either source
file has changed since the last ‘make print’. The automatic variable ‘$?’ is
used to print only those files that have changed.


={============================================================================
*kt_linux_tool_140* make-gmake-builtin-target

4.8 Special Built-in Target Names

Certain names have special meanings if they appear as targets.

.SUFFIXES
The prerequisites of the special target .SUFFIXES are the list of suffixes to
be used in checking for suffix rules. See Section 10.7 [Old-Fashioned Suffix
Rules], page 125.


.NOTPARALLEL
If .NOTPARALLEL is mentioned as a target, then this invocation of make will be
run serially, even if the ‘-j’ option is given. Any recursively invoked make
command will still run recipes in parallel (unless its makefile also contains
    this target). Any prerequisites on this target are ignored.

<ex>
.NOTPARALLEL: install-exec-local install-data-local uninstall-local


={============================================================================
*kt_linux_tool_140* make-gmake-recipe gmake-just-print gmake-double-dollar

5 Writing Recipes in Rules

Users use many different shell programs, but recipes in makefiles are always
interpreted by /bin/sh unless the makefile specifies otherwise.


5.1 Recipe Syntax

Makefiles have the unusual property that there are really two distinct
syntaxes in one file. Most of the makefile uses make syntax.

However, recipes are meant to be interpreted by the shell and so they are
written using shell syntax. The make program does not try to understand shell
syntax: it performs only a very few specific translations on the content of
the recipe before handing it to the shell.

Each line in the recipe `must start with a tab`

<gmake-double-dollar>
5.1.2 Using Variables in Recipes

The other way in which make processes recipes is by expanding any variable
references in them.

This occurs after make has finished reading all the makefiles and the target
`is determined` to be out of date; so, the recipes for targets which are not
rebuilt are never expanded.

If you want a dollar sign to appear in your recipe, you must double it (‘$$’).
For shells like the default shell, that use dollar signs to introduce
variables, it’s important to keep clear in your mind whether the variable you
want to reference is a make variable (use a single dollar sign) or a shell
variable (use two dollar signs). For example:

LIST = one two three

all:
for i in $(LIST); do \
  echo $$i; \
done

results in the following command being passed to the shell:

for i in one two three; do \
  echo $i; \
done


<gmake-echo>
5.2 Recipe Echoing

Normally make prints each line of the recipe before it is executed. We call
this `echoing` because it gives the appearance that you are typing the lines
yourself.

When a line starts with ‘@’, the echoing of that line is suppressed.

<gmake-option>
When make is given the flag ‘-n’ or ‘--just-print’ it only echoes most
recipes, without executing them. In this case even the recipe lines starting
with ‘@’ are printed. This flag is useful for finding out which recipes make
thinks are necessary without actually doing them.

The ‘-s’ or ‘--silent’ flag to make prevents all echoing, as if all recipes
started with ‘@’.


5.3 Recipe Execution

When it is time to execute recipes to update a target, they are executed by
invoking `a new sub-shell for each line of the recipe`, unless the .ONESHELL
special target is in effect

Please note: this implies that setting shell variables and invoking shell
commands such as cd that set a context local to each process will not affect
the following lines in the recipe.1 If you want to use cd to affect the next
statement, put both statements in a single recipe line. Then make will invoke
one shell to run the entire line, and the shell will execute the statements in
sequence. For example:

foo : bar/lose
  cd $(@D) && gobble $(@F) > ../$@

Here we use the shell AND operator (&&) so that if the cd command fails, the
script will fail without trying to invoke the gobble command in the wrong
directory, which could cause problems


5.5 Errors in Recipes

After each shell invocation returns, make looks at its exit status. If the
shell completed successfully (the exit status is zero), the next line in the
recipe is executed in a new shell; after the last line is finished, the rule
is finished.  

If there is an error (the exit status is nonzero), make gives up on the
current rule, and perhaps on all rules.

Sometimes the failure of a certain recipe line does not indicate a problem.
For example, you may use the mkdir command to ensure that a directory exists.
If the directory already exists, mkdir will report an error, but you probably
want make to continue regardless. 

To ignore errors in a recipe line, write a ‘-’ at the beginning of the line’s
text

clean:
  -rm -f *.o

This causes make to continue even if rm is unable to remove a file.

<gmake-option>
Normally make gives up immediately in this circumstance, returning a nonzero
status.  However, if the ‘-k’ or ‘--keep-going’ flag is specified, make
continues to consider the other prerequisites of the pending targets, remaking
them if necessary, before it gives up and returns nonzero status. 

For example, after an error in compiling one object file, ‘make -k’ will
continue compiling other object files even though it already knows that
linking them will be impossible.

note:
To find several independent problems so that you can correct them all before
the next attempt to compile. This is why Emacs’ compile command passes the
‘-k’ flag by default.

When a recipe line fails, if it has changed the target file at all, the file
is corrupted and cannot be used—or at least it is not completely updated. Yet
the file’s time stamp says that it is now up to date, so the next time make
runs, it will not try to update that file. 

So generally the right thing to do is to delete the target file if the recipe
fails after beginning to change the file. make will do this if
.DELETE_ON_ERROR appears as a target.  This is almost always what you want
make to do, but it is not historical practice; so for compatibility, you must
`explicitly request` it.


5.9 Using Empty Recipes


={============================================================================
*kt_linux_tool_140* make-gmake-recipe-parallel make-curdir

5.4 Parallel Execution

GNU make knows how to execute several recipes at once. Normally, make will
execute only one recipe at a time, waiting for it to finish before executing
the next. However, the ‘-j’ or ‘--jobs’ option tells make to execute many
recipes simultaneously.

If the ‘-j’ option is followed by an integer, this is the number of recipes to
execute at once; this is called the number of job slots. If there is nothing
looking like an integer after the ‘-j’ option, there is no limit on the number
of job slots. The default number of job slots is one, which means serial
execution (one thing at a time).

note: recipes means a line in a recipe?

If a recipe fails (is killed by a signal or exits with a nonzero status), and
errors are not ignored for that recipe, the remaining recipe lines to remake
the same target `will not be run` If a recipe fails and the ‘-k’ or
‘--keep-going’ option was not given, make `aborts execution`


<no-parallel>
You can inhibit parallelism in a particular makefile with the .NOTPARALLEL
pseudo-target

  4.8 Special Built-in Target Names

  .NOTPARALLEL
  If .NOTPARALLEL is mentioned as a target, then this invocation of make will
  be run serially, even if the ‘-j’ option is given. Any recursively invoked
  make command will still run recipes in parallel (unless its makefile also
  contains this target). Any prerequisites on this target are ignored.


5.4.1 Output During Parallel Execution

When running several recipes in parallel the output from each recipe appears
as soon as it is generated, with the result that messages from different
recipes may be interspersed, sometimes even appearing on the same line. This
can make reading the output very difficult.

<gmake-option> gmake-print-directory
If you use several levels of recursive make invocations, the ‘-w’ or
‘--print-directory’ option can make the output a lot easier to understand by
showing each directory as make starts processing it and as make finishes
processing it.

For example, if ‘make -w’ is run in the directory /u/gnu/make, make will print
a line of the form:

make: Entering directory ‘/u/gnu/make’.

before doing anything else, and a line of the form:

make: Leaving directory ‘/u/gnu/make’.

when processing is completed.

you do not need to specify this option because ‘make’ does it for you: ‘-w’ is
turned on automatically when you use the ‘-C’ option, and in sub-makes.


<gmake-option> gmake-output-sync
To avoid this you can use the ‘--output-sync’ (‘-O’) option. This option
instructs make to save the output from the commands it invokes and print it
all once the commands are completed. Additionally, if there are multiple
recursive make invocations running in parallel, they will communicate so that
only one of them is generating output at a time.

There are four levels of granularity when synchronizing output, specified by
giving an argument to the option (e.g., ‘-Oline’ or ‘--output-sync=recurse’).

none
`This is the default`: all output is sent directly as it is generated and no
synchronization is performed.

line 
Output from each individual line of the recipe is grouped and printed as soon
as that line is complete. If a recipe consists of multiple lines, they may be
interspersed with lines from other recipes.

target 
Output from the entire recipe for each target is grouped and printed once the
target is complete. This is the default if the --output-sync or -O option is
given with no argument.

recurse 
Output from each recursive invocation of make is grouped and printed once the
recursive invocation is complete.  

Regardless of the mode chosen, the total build time will be the same. The only
difference is in how the output appears.

The ‘target’ and ‘recurse’ modes both collect the output of the entire recipe
of a target and display it uninterrupted when the recipe completes. The
difference between them is in how recipes that contain recursive invocations
of make are treated

If the ‘recurse’ mode is chosen, recipes that contain recursive make
invocations are treated the same as other targets: the output from the recipe,
including the output from the recursive make, is saved and printed after the
  entire recipe is complete. This ensures output from all the targets built by
  a given recursive make instance are grouped together, which may make the
  output easier to understand. However it also leads to long periods of time
  during the build where no output is seen, followed by large bursts of
  output. `If you are not watching the build as it proceeds`, but instead
  viewing a log of the build after the fact, this may be the best option for
  you.


note:
Some programs invoked by make may behave differently if they determine they’re
writing output to a terminal versus a file (often described as “interactive”
    vs. “non-interactive” modes). 

For example, many programs that can display colorized output will not do so if
they determine they are not writing to a terminal. If your makefile invokes a
program like this then using the output synchronization options will cause the
program to believe it’s running in “non-interactive” mode even though the
output will ultimately go to the terminal.


5.7 Recursive Use of make

Recursive use of make means using make as a command in a makefile. This
technique is useful when you want separate makefiles for various subsystems
that compose a larger system.

<make-curdir>
For your convenience, when GNU make starts (after it has processed any -C
    options) it sets the variable CURDIR to the pathname of the current
working directory. This value is never touched by make again: in particular
note that if you include files from other directories the value of CURDIR does
not change.


5.7.1 How the MAKE Variable Works

Recursive make commands should always use the variable MAKE, not the explicit
command name ‘make’, as shown here:

subsystem:
  cd subdir && $(MAKE)

The special feature makes this do what you want: whenever a recipe line of a
rule contains the variable MAKE, the flags ‘-t’, ‘-n’ and ‘-q’ do not apply to
that line. Recipe lines containing MAKE are executed normally despite the
presence of a flag that causes most recipes not to be run.

5.7.2 Communicating Variables to a Sub-make

Variable values of the top-level make can `be passed to` the sub-make through
the environment by explicit request. These variables are defined in the
sub-make as defaults, but they `do not override` variables defined in the
makefile used by the sub-make unless you use the ‘-e’ switch

<gmake-option>
‘-e’
‘--environment-overrides’
Give variables taken from the environment precedence over variables from
makefiles.

To pass down, or `export`, a variable, make adds the variable and its value to
the environment for running each line of the recipe. The sub-make, in turn,
uses the environment to initialize its table of variable values.

make exports:

Except by explicit request, make exports a variable only if it is either
defined in the environment initially or set `on the command line`, and if its
name consists only of letters, numbers, and underscores.

The special variable MAKEFLAGS is always exported. make automatically passes
down variable values that were defined on the command line, by putting them in
the MAKEFLAGS variable.

If you want to export specific variables to a sub-make, use the export
directive, like this:

<make-export>
export variable ...

If you want to prevent a variable from being exported, use the unexport
directive, like this:

unexport variable ...

In both of these forms, the arguments to export and unexport are expanded, and
so could be variables or functions which expand to a (list of) variable names
to be (un)exported.

As a convenience, you can define a variable and export it at the same time by
doing:

export variable = value

If you want all variables to be exported by default, you can use export by
itself:

export

5.7.3 Communicating Options to a Sub-make

Flags such as ‘-s’ and ‘-k’ are passed automatically to the sub-make through
the variable MAKEFLAGS. This variable is set up automatically by make to
contain the flag letters that make received. Thus, if you do ‘make -ks’ then
MAKEFLAGS gets the value ‘ks’.  

As a consequence, every sub-make gets a value for MAKEFLAGS in its
environment. In response, it takes the flags from that value and processes
them as if they had been given as arguments.

Likewise variables defined on the command line are passed to the sub-make
through MAKEFLAGS. Words in the value of MAKEFLAGS that contain ‘=’, make
treats as variable definitions just as if they appeared on the command line.

The options ‘-C’, ‘-f’, ‘-o’, and ‘-W’ are not put into MAKEFLAGS; these
options are not passed down.

If you `do not want to pass` the other flags down, you must change the value of
MAKEFLAGS, like this:

subsystem:
  cd subdir && $(MAKE) MAKEFLAGS=

The command line variable definitions really appear in the variable
MAKEOVERRIDES, and MAKEFLAGS contains a reference to this variable.


The MAKEFLAGS variable can also be useful if you want to have certain options,
set each time you run make. Simply put a value for MAKEFLAGS in your environment.

If you do put MAKEFLAGS in your environment, you should be sure not to include
any options that will drastically affect the actions of make and undermine the
purpose of makefiles and of make itself. For instance, the ‘-t’, ‘-n’, and
‘-q’ options, if put in one of these variables, could have disastrous
consequences and would certainly have at least surprising and probably
annoying effects.


={============================================================================
*kt_linux_tool_140* make-gmake-two-variable make-gmake-read-makefile

2.3 How make Processes a Makefile

make reads the makefile in the current directory and begins by processing the
first rule. In the example, this rule is for relinking edit; but before make
can fully process this rule, it must process the rules for the files that edit
depends on, which in this case are the object files.

The other rules are processed because their targets appear as prerequisites of
the goal.  If some other rule `is not depended` on by the goal (or anything it
    depends on, etc.), that rule is not processed, unless you tell make to do
so. (with a command such as make clean).

Before recompiling an object file, make considers updating its prerequisites,
the source file and header files. This makefile does not specify anything to
  be done for them-the ‘.c’ and ‘.h’ files are not the targets of any rules-so
  make does nothing for these files.


3.7 How make Reads a Makefile

a read-in phase and a target-update phase

GNU make does its work in `two distinct phases` 

During the first phase it reads all the makefiles, included makefiles, etc.
and internalizes `all the variables and their values`, implicit and explicit
rules, and constructs a dependency graph of all the targets and their
prerequisites. 

  6 How to Use Variables
  
  Variables and functions in all parts of a makefile `are expanded when read`,
  except for in recipes.

  7 Conditional Parts of Makefiles

  A conditional directive causes part of a makefile to be obeyed or ignored
  depending on the values of variables. Conditionals can compare the value of
  one variable to another, or the value of a variable to a constant string. 

During the second phase, make uses these internal structures to determine what
targets will need to be rebuilt and to invoke the rules necessary to do so.

We say that expansion is `immediate` if it happens during the first phase: in
this case make will expand any variables or functions in that section of a
construct as the makefile is parsed. 

We say that expansion is `deferred` if expansion is not performed immediately.
Expansion of a deferred construct is not performed until either the construct
appears later in an immediate context, or until the second phase.

Variable Assignment

immediate = deferred
immediate ?= deferred
immediate := immediate
immediate ::= immediate
immediate += deferred or immediate
immediate != immediate

For the `append operator`, ‘+=’, the right-hand side is considered immediate
if the variable was previously set as a `simple variable` (‘:=’ or ‘::=’), and
  deferred otherwise.

For the shell assignment operator, ‘!=’, the right-hand side is evaluated
immediately and handed to the shell. The result is stored in the variable
named on `the left`, and that variable becomes a simple variable (and will
    thus be re-evaluated on each reference).


Rule Definition

A rule is always expanded the same way, regardless of the form:

immediate : immediate 
  deferred


={============================================================================
*kt_linux_tool_140* make-gmake-variable

6 How to Use Variables

A `variable` is a name defined in a makefile to represent a string of text,
called the variable's `value`. 
  
These values are substituted by explicit request into targets, prerequisites,
recipes, and other parts of the makefile. 

Variables and functions in all parts of a makefile `are expanded when read`,
`except for in recipes`, the right-hand sides of variable definitions using ‘=’,
and the bodies of variable definitions using the define directive.

Variable names are `case-sensitive`.

It is traditional to use upper case letters in variable names, but we
recommend using lower case letters for variable names that serve internal
purposes in the makefile, and reserving upper case for parameters that control
implicit rules or for parameters that the user should override with command
options.


6.1 Basics of Variable References

To substitute a variable's value, write a dollar sign followed by the name of
the variable in parentheses or braces: either `$(foo)` or `${foo}` is a valid
reference to the variable foo.

This special significance of ‘$’ is why you must write ‘$$’ to have the effect
of a single dollar sign in a file name or recipe.

note:
sh uese $foo or ${foo}

Variable references can be used in any context: targets, prerequisites,
         recipes, most directives, and new variable values.

A dollar sign followed by a character other than a dollar sign,
open-parenthesis or open-brace treats that single character as the variable
  name. Thus, you could reference the variable x with `$x`. However, this
  practice is strongly discouraged, except in the case of the automatic
  variables


6.2 The Two Flavors of Variables

There are two ways that a variable in GNU make can have a value; we call them
the two flavors of variables.

<recursively-expanded-variable>
The first flavor of variable is a `recursively expanded` variable. Variables
of this sort are defined by lines using '=' or by the 'define' directive.  

This flavor of variable is the only sort supported by most other
versions of `make`. It has its advantages and its disadvantages.

An advantage:

     CFLAGS = $(include_dirs) -O
     include_dirs = -Ifoo -Ibar

will do what was intended: when 'CFLAGS' is expanded in a recipe, it will
expand to '-Ifoo -Ibar -O'.

A major disadvantage:

cannot append something on the end of a variable, as in

     CFLAGS = $(CFLAGS) -O

because it will cause an `infinite loop` in the variable expansion. Actually
make detects the infinite loop and reports an error.

Another disadvantage is that any functions referenced in the definition will
be executed every time the variable is expanded. This makes 'make' run slower;
worse, it causes the 'wildcard' and 'shell' functions to give unpredictable
  results because you cannot easily control when they are called, or even how
  many times.


<simply-expanded-variable>
To avoid all the problems and inconveniences of recursively expanded
variables, there is another flavor: simply expanded variables.

"Simply expanded variables" are defined by lines using ':=' or '::='. Both
forms are equivalent in GNU 'make'

The value of a simply expanded variable is scanned once and for all, expanding
any references to other variables and functions, `when the variable is defined`  
it contains their values _as of the time this variable was defined_.

     x := foo
     y := $(x) bar
     x := later

is equivalent to

     y := foo bar
     x := later

Simply expanded variables generally make complicated makefile programming more
predictable because they work like variables in most programming languages.


<trailing-space>

     nullstring :=
     space := $(nullstring) # end of the line

Here the value of the variable 'space' is precisely one space. The comment '#
end of the line' is included here just for clarity. Since trailing space
characters `are not stripped` from variable values, just a space at the end of
the line would have the same effect (but be rather hard to read). If you put
whitespace at the end of a variable value, it is a good idea to put a comment
like that at the end of the line to make your intent clear. 

Conversely, if you do not want any whitespace characters at the end of your
variable value, you must remember `not to put a random comment` on the end of
the line after some whitespace, such as this:

     dir := /foo/bar    # directory to put the frobs in

Here the value of the variable 'dir' is '/foo/bar    ' (with four trailing
    spaces), which was probably not the intention. 


<conditional-assignment>
There is another assignment operator for variables, '?='. This is called a
conditional variable assignment operator, because it only has an effect if the
variable `is not yet defined` This statement:

     FOO ?= bar

is exactly equivalent to this:

     ifeq ($(origin FOO), undefined)
       FOO = bar
     endif

Note that a variable set to an empty value is still defined, so '?=' will not
set that variable.


6.3.1 Substitution References

A "substitution reference" substitutes the value of a variable with
alterations that you specify.  It has the form '$(VAR:A=B)' (or '${VAR:A=B}')
and its meaning is to take the value of the variable VAR, replace every A at
the end of a word with B in that value, and substitute the resulting string.

When we say "at the end of a word", we mean that A must appear either followed
by whitespace or at the end of the value in order to be replaced; other
occurrences of A in the value are unaltered.  For example:

     foo := a.o b.o c.o
     bar := $(foo:.o=.c)

sets 'bar' to 'a.c b.c c.c'.

Another type of substitution reference lets you use the full power of the
'patsubst' function. It has the same form '$(VAR:A=B)' described above, except
that now A must contain a single '%' character.  This case is equivalent to
'$(patsubst A,B,$(VAR))'.  

For example:

     foo := a.o b.o c.o
     bar := $(foo:%.o=%.c)

sets 'bar' to 'a.c b.c c.c'.


6.5 Setting Variables

Most variable names are considered to have the empty string as a value if you
have never set them.

Several special variables are set automatically to a new value for each rule;
these are called the "automatic" variables

<shell-assignment>
The shell assignment operator '!=' can be used to execute a shell script and
set a variable to its output. This operator first evaluates the right-hand
side, then passes that result to the shell for execution.

If the result of the execution ends in a newline, that one newline is removed;
all other newlines are replaced by spaces. The resulting string is then placed
  into the named `recursively-expanded variable` For example:

     hash != printf '\043'
     file_list != find . -name '*.c'

Alternatively, you can set a `simply expanded variable` to the result of
running a program using the 'shell' function call.

     hash := $(shell printf '\043')
     var := $(shell find . -name "*.c")

As with the 'shell' function, the exit status of the just-invoked shell script
is stored in the '.SHELLSTATUS' variable.


6.6 Appending More Text to Variables

immediate += deferred or immediate

When the variable in question has not been defined before, '+=' acts
just like normal '=': it defines a recursively-expanded variable.

However, when there _is_ a previous definition, exactly what '+=' does
depends on what flavor of variable you defined originally.  


For both cases:
  When you add to a variable's value with '+=', 'make' acts essentially as if
  you had included the extra text in the initial definition of the variable.  


If you defined it first with ':=' or '::=', making it a
simply-expanded variable, '+=' adds to that simply-expanded definition,
and expands the new text before appending it to the old value just as
':=' does .  In fact,

     variable := value
     variable += more

is exactly equivalent to:

     variable := value
     variable := $(variable) more

When you use '+=' with a variable that you defined first to be
recursively-expanded using plain '=', 'make' does something a bit different
and it is this unexpanded text to which 'make' appends the new text you
specify.

     variable = value
     variable += more

is roughly equivalent to:

     temp = value
     variable = $(temp) more

<ex>
The importance of this comes when the variable's old value contains variable
references. Take this common example:

     CFLAGS = $(includes) -O
     ...
     CFLAGS += -pg # enable profiling

The first line defines the 'CFLAGS' variable with a reference to another
variable, 'includes'. Using '=' for the definition makes 'CFLAGS' a
recursively-expanded variable, meaning '$(includes) -O' is _not_ expanded when
'make' processes the definition of 'CFLAGS'. Thus, 'includes' need not be
defined yet for its value to take effect. It only has to be defined before any
reference to 'CFLAGS'. 

If we tried to append to the value of 'CFLAGS' without using '+=', we might do
it like this:

     CFLAGS := $(CFLAGS) -pg # enable profiling

This is pretty close, but not quite what we want. Using ':=' redefines
'CFLAGS' as a simply-expanded variable; this means 'make' expands the text
'$(CFLAGS) -pg' before setting the variable. If 'includes' is not yet defined,
we get ' -O -pg', and a later definition of 'includes' will have no effect. 

Conversely, by using '+=' we set 'CFLAGS' to the _unexpanded_ value
'$(includes) -O -pg'. Thus we preserve the reference to 'includes', so if that
variable gets defined at any later point, a reference like '$(CFLAGS)' still
uses its value.


6.9 Undefining Variables

If you want to clear a variable, setting its value to empty is usually
sufficient. Expanding such a variable will yield the same result (empty
    string) regardless of whether it was set or not. 

However, if you are using the 'flavor' and 'origin' functions, there is a
difference between a variable that was never set and a variable with an empty
value.  

In such situations you may want to use the 'undefine' directive to make a
variable appear as if it was never set. For example:

     foo := foo
     bar = bar

     undefine foo
     undefine bar

     $(info $(origin foo))
     $(info $(flavor bar))

This example will print "undefined" for both variables.


={============================================================================
*kt_linux_tool_140* make-gmake-variable-override

6.10 Variables from the Environment

Variables in make can come from the environment in which make is run. Every
environment variable that make sees when it starts up `is transformed into` a
make variable with the same name and value. 

However, an explicit assignment in the makefile, or `with a command argument`,
overrides the environment.

Thus, by setting the variable CFLAGS in your environment, you can cause all C
compilations in most makefiles to use the compiler switches you prefer.

When make runs a recipe, variables defined in the makefile are placed into the
`environment of each shell` This allows you to pass values to sub-make
invocations


<only-passed-sub-make>
By default, only variables that came from the environment or the command line
are passed to recursive invocations.


<ex>
// from buildroot-2016.08.1 makefile

ifneq ("$(origin O)", "command line")
O := output
CONFIG_DIR := $(TOPDIR)
NEED_WRAPPER =
else
# other packages might also support Linux-style out of tree builds
# with the O=<dir> syntax (E.G. BusyBox does). As make automatically
# forwards command line variable definitions those packages get very
# confused. Fix this by telling make to not do so
MAKEOVERRIDES =
# strangely enough O is still passed to submakes with MAKEOVERRIDES
# (with make 3.81 atleast), the only thing that changes is the output
# of the origin function (command line -> environment).
# Unfortunately some packages don't look at origin (E.G. uClibc 0.9.31+)
# To really make O go away, we have to override it.
override O := $(O)
CONFIG_DIR := $(O)
# we need to pass O= everywhere we call back into the toplevel makefile
EXTRAMAKEARGS = O=$(O)
NEED_WRAPPER = y
endif


9.5 Overriding Variables

An argument that contains ‘=’ specifies the value of a variable: ‘v=x’ sets
the value of the variable v to x. If you specify a value in this way, all
ordinary assignments of the same variable in the makefile are ignored; we say
they have been overridden `by the command line argument.`

The most common way to use this facility is to pass extra flags to compilers.
For example, in a properly written makefile, the variable CFLAGS is included
in each recipe that runs the C compiler, so a file foo.c would be compiled
something like this:

cc -c $(CFLAGS) foo.c

Thus, whatever value you set for CFLAGS affects each compilation that occurs.
The makefile probably specifies the usual value for CFLAGS, like this:

CFLAGS=-g

Each time you run make, you can override this value if you wish. For example,
if you say 
       
make CFLAGS=’-g -O’
  
each C compilation will be done with ‘cc -c -g -O’. This also illustrates how
you can use quoting in the shell to enclose spaces and other special
characters in the value of a variable when you override it.

<ex>

// okay
make \
NDS_ROOT=/home/nds-uk/kyoupark/STB_SW/DARWIN_PLATFORM/dms/build \
NDS_BUILD_ROOT=/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso

// error
make \
NDS_ROOT=/home/nds-uk/kyoupark/STB_SW/DARWIN_PLATFORM/dms/build \
NDS_BUILD_ROOT= /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso

$ make \
> NDS_ROOT=/home/nds-uk/kyoupark/STB_SW/DARWIN_PLATFORM/dms/build \
> NDS_BUILD_ROOT= /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso
makefile:140: *** Error, you must specify NDS_BUILD_ROOT:=<unix style filename>.  Stop.
  
ifndef NDS_BUILD_ROOT
  $(error Error, you must specify NDS_BUILD_ROOT:=<unix style filename>)
endif

see *sh-variable-no-space*


={============================================================================
*kt_linux_tool_140* make-gmake-variable-split

6.5 Setting Variables

There is no limit on the length of the value of a variable except the amount
of memory on the computer. You can split the value of a variable into multiple
physical lines for readability.


3.1.1 Splitting Long Lines

Makefiles use a “line-based” syntax in which the newline character is special
and marks the end of a statement.

However, it is difficult to read lines which are too long to display without
wrapping or scrolling. So, you can format your makefiles for readability by
adding newlines into the middle of a statement: you do this `by escaping` the
internal newlines with a backslash (\) character.

note: physical lines <= logical lines

Where we need to make a distinction we will refer to “physical lines” as a
single line ending with a newline (regardless of whether it is escaped) and a
“logical line” being a complete statement including all escaped newlines up to
the first non-escaped newline.

Outside of recipe lines, backslash/newlines are converted into a single space
character.

The way in which backslash/newline combinations are handled depends on whether
the statement is a recipe line or a non-recipe line. Handling of
backslash/newline in a recipe line is discussed later.


5.1.1 Splitting Recipe Lines

As in normal makefile syntax, a single logical recipe line can be split into
multiple physical lines in the makefile by placing a backslash before each
newline. A sequence of lines like this `is considered a single recipe line`, and
one instance of the shell will be invoked to run it.

However, in contrast to how they are treated in other places in a makefile
(3.1.1 [Splitting Long Lines]), backslash/newline pairs are `not removed from
the recipe` Both the backslash and the newline characters are preserved and
passed to the shell. How the backslash/newline is interpreted depends on your
shell.


={============================================================================
*kt_linux_tool_140* make-gmake-variable-define

5.8 Defining Canned Recipes

When the same sequence of commands is useful in making `various targets`, you
can define it as a canned sequence with the define directive, and refer to the
canned sequence from the recipes for those targets. The canned sequence is
actually a variable, so the name must not conflict with other variable names.

Here is an example of defining a canned recipe:

define run-yacc =
yacc $(firstword $^)
mv y.tab.c $@
endef

Here run-yacc is the name of the variable being defined; endef marks the end
of the definition; the lines in between are the commands. 

The define directive does not expand variable references and function calls in
the canned sequence; the ‘$’ characters, parentheses, variable names, and so
on, all become part of the value of the variable you are defining.

To use the canned sequence, substitute the variable into the recipe of a rule.
You can substitute it like any other variable. Because variables defined by
define are recursively expanded variables, all the variable references you
wrote inside the define are expanded now. For example:

foo.c : foo.y
  $(run-yacc)

‘foo.y’ will be substituted for the variable ‘$^’ when it occurs in run-yacc’s
value, and ‘foo.c’ for ‘$@’.

In recipe execution, each line of a canned sequence is treated just as if the
line appeared on its own in the rule, preceded by a tab. For example, using
this canned sequence:

define frobnicate =
@echo "frobnicating target $@"
frob-step-1 $< -o $@-step-1
frob-step-2 $@-step-1 -o $@
endef

On the other hand, prefix characters on the recipe line that refers to a
canned sequence apply to every line in the sequence. So the rule:

frob.out: frob.in
@$(frobnicate)

`does not echo any recipe lines`


6.8 Defining Multi-Line Variables

Another way to set the value of a variable is to use the `define directive`
This directive has an unusual syntax which allows newline characters to be
included in the value, which is convenient for defining both canned sequences
of commands.

The variable name may contain function and variable references, which are
expanded when the directive is read to find the actual variable name to use.

The value in an ordinary assignment cannot contain a newline; but the newlines
that separate the lines of the value in a define become part of the variable’s
value

define two-lines =
echo foo
echo $(bar)
endef

When used in a recipe, the previous example is functionally equivalent to this:

two-lines = echo foo; echo $(bar)

since two commands separated by semicolon behave much like two separate shell
commands.

However, note that using two separate lines means make will invoke the shell
twice, running an independent sub-shell for each line.


={============================================================================
*kt_linux_tool_140* make-gmake-variable-pattern

<variable-dereferencing>
One big "Gotcha" with variable dereferencing is that if commands `implicitly`
dereference values. This has some unexpected results. For example:

if("${SOME_VAR}" STREQUAL "MSVC")

In this code sample MSVC will be implicitly dereferenced, which will result in
the if command comparing the value of the dereferenced variables SOME_VAR
and MSVC. A common workaround to this solution is to prepend strings being
compared with an x.

if("x${SOME_VAR}" STREQUAL "xMSVC")

This works because while MSVC is a defined variable, xMSVC is not. This
  pattern is uncommon, but it does occur in LLVM’s CMake scripts.

note:
To prevent name colision by making it string.

note:
GNU autotools also use this


={============================================================================
*kt_linux_tool_140* make-gmake-variable-empty-string

// From 6.5 Setting Variables
Most variable names are considered to have the empty string as a value if you
have never set them.

note: defined but not set

// From <conditional-assignment>
Note that a variable set to an empty value is still defined, so '?=' will not
set that variable.

Either "not defined" or "defined but not set" has `empty string`


={============================================================================
*kt_linux_tool_140* make-gmake-conditional

7 Conditional Parts of Makefiles

A conditional directive causes part of a makefile to be obeyed or ignored
depending on the values of variables. Conditionals can compare the value of
one variable to another, or the value of a variable to a constant string. 

Conditionals control what `'make' actually "sees" in the makefile`, so they
`cannot` be used to control recipes at the time of execution.


7.1 Example of a Conditional

The result is that 'CC=gcc' as an argument to 'make' changes not only which
compiler is used but also which libraries are linked.

     libs_for_gcc = -lgnu
     normal_libs =

     foo: $(objects)
     ifeq ($(CC),gcc)
             $(CC) -o foo $(objects) $(libs_for_gcc)
     else
             $(CC) -o foo $(objects) $(normal_libs)
     endif

There are four different directives that test different conditions. Here is a
table of them:

'ifeq (ARG1, ARG2)'
'ifeq 'ARG1' 'ARG2''
'ifeq "ARG1" "ARG2"'
'ifeq "ARG1" 'ARG2''
'ifeq 'ARG1' "ARG2"'
     Expand all variable references in ARG1 and ARG2 and compare them.


<gmake-non-empty-check>
Often you want to test if a variable has a non-empty value. You can use the
'strip' function to avoid interpreting whitespace as a non-empty value. For
example:

     ifeq ($(strip $(foo)),)
     TEXT-IF-EMPTY
     endif

will evaluate TEXT-IF-EMPTY even if the expansion of '$(foo)' contains
whitespace characters.


'ifdef VARIABLE-NAME'
     The 'ifdef' form takes the `_name_ of a variable` as its argument, not a
     reference to a variable. If `the value of that variable` has a non-empty
     value, the TEXT-IF-TRUE is effective. 
     
     Variables that have never been defined have an empty value.  
     
     note: After all, "defined and set" variable.

          bar = true
          foo = bar
          ifdef $(foo)
          frobozz = yes
          endif

     The variable reference '$(foo)' is expanded, yielding 'bar', which is
     considered to be the name of a variable. The variable 'bar' is not
     expanded, but its value is examined to determine if it is non-empty.

     Note that 'ifdef' only tests whether a variable has a value.  It
     does `not expand the variable to see` if that value is nonempty.
     Consequently, tests using 'ifdef' return true for all definitions
     `except those like 'foo ='`  

     To test for an empty value, use 'ifeq ($(foo),)'. For example,

          bar =
          foo = $(bar)
          ifdef foo
          frobozz = yes
          else
          frobozz = no
          endif

     sets 'frobozz' to 'yes', while:

          foo =
          ifdef foo
          frobozz = yes
          else
          frobozz = no
          endif

     sets 'frobozz' to 'no'.

     note:
     Okay to think that expand one level only?


Conditionals affect which lines of the makefile 'make' uses.  If the condition
is true, 'make' reads the lines of the TEXT-IF-TRUE as part of the makefile;
          if the condition is false, 'make' ignores those lines completely.


={============================================================================
*kt_linux_tool_140* make-gmake-text-function

https://www.gnu.org/software/make/manual/html_node/Text-Functions.html

8 Functions for Transforming Text

8.2 Functions for String Substitution and Analysis

<make-subst>
$(subst from,to,text)

Performs a textual replacement on the text text: each occurrence of from is
replaced by to. The result is substituted for the function call. 

For example,

$(subst ee,EE,feet on the street)
substitutes the string ‘fEEt on the strEEt’.

<ex>
To remove '"' from a variable.

config BR2_TOOLCHAIN_EXTERNAL_LIB_C
	string "The core C library from the external toolchain"
	default "libc.so.0"

$(subst ",, $(BR2_TOOLCHAIN_EXTERNAL_LIB_C))


$(patsubst pattern,replacement,text)

Finds whitespace-separated words in text that match pattern and replaces them
with replacement. Here pattern may contain a ‘%’ which acts as a wildcard,
matching any number of any characters within a word. If replacement also
  contains a ‘%’, the ‘%’ is replaced by the text that matched the ‘%’ in
  pattern. Only the first ‘%’ in the pattern and replacement is treated this
  way; any subsequent ‘%’ is unchanged.


$(strip string)

    Removes leading and trailing whitespace from string and replaces each
    internal sequence of one or more whitespace characters with a single
    space. Thus, ‘$(strip a b c )’ results in ‘a b c’.

    The function `strip` can be very useful when used in conjunction with
    conditionals. When comparing something with the empty string ‘’ using ifeq
    or ifneq, you usually want a string of just whitespace to match the empty
    string (see Conditionals).

    Thus, the following may fail to have the desired results:

    .PHONY: all
    ifneq   "$(needs_made)" ""
    all: $(needs_made)
    else
    all:;@echo 'Nothing to make!'
    endif

    Replacing the variable reference ‘$(needs_made)’ with the function call
    ‘$(strip $(needs_made))’ in the ifneq directive would make it more robust.


8.5 The foreach Function

$(foreach var,list,text)

This simple example sets the variable ‘files’ to the list of all files in the
directories in the list ‘dirs’:

dirs := a b c d
files := $(foreach dir,$(dirs),$(wildcard $(dir)/*))

The first two arguments, var and list, are expanded before anything else is
done; note that the last argument, text, is not expanded at the same time.
Then for each word of the expanded value of list, the variable named by the
expanded value of var is set to that word, and text is expanded.

The result is that text is expanded as many times as there are
whitespace-separated words in list. The multiple expansions of text are
concatenated, with spaces between them, to make the result of foreach.

This example has the same result (except for setting ‘dirs’) as the following
example:

files := $(wildcard a/* b/* c/* d/*)

When text is complicated, you can improve readability by giving it a name,
with an additional variable:

find_files = $(wildcard $(dir)/*)
dirs := a b c d
files := $(foreach dir,$(dirs),$(find_files))


<make-call>
8.7 The call Function

The call function is unique in that it can be used to `create` new parameterized
functions. You can write a complex expression `as the value of a variable`, then
use call to expand it with different values.

The syntax of the call function is:

$(call variable,param,param,...)

When make expands this function, it assigns each param to temporary variables
$(1), $(2), etc. The variable $(0) will contain variable.

Then variable is expanded as a make variable in the context of these temporary
assignments. Thus, any reference to $(1) in the value of variable will resolve
to the first param in the invocation of call.

Note that variable is the name of a variable, not a reference to that
variable. Therefore you would not normally use a ‘$’ or parentheses when
writing it.

If variable is the name of a built-in function, the built-in function is
always invoked (even if a make variable by that name also exists).

This macro simply reverses its arguments:

reverse = $(2) $(1)
foo = $(call reverse,a,b)

Here foo will contain ‘b a’.

The call function can be nested. Each recursive invocation gets its own local
values for $(1), etc. that mask the values of higher-level call. For example,
here is an implementation of a map function:

map = $(foreach a,$(2),$(call $(1),$(a)))

Now you can map a function that normally takes only one argument, such as origin,
to multiple values in one step:

o = $(call map,origin,o map MAKE)

and end up with o containing something like ‘file file default’.

// map = $(foreach a,'o map MAKE',$(call origin,$(a)))

<ex> note: see $$ on variable reference in `receipe`
#
# copy_toolchain_lib_root
#
# $1: source
# $2: destination
# $3: strip (y|n)       default is to strip
#
copy_toolchain_lib_root = \
	LIB="$(strip $1)"; \
	DST="$(strip $2)"; \
	STRIP="$(strip $3)"; \
 \
	SYSROOT="" ;\
	if test -z $(BR2_SKY_TOOLCHAIN_WITHOUT_SYSROOT); then \
		PREFIX=`$(TARGET_CC) -v 2>&1 | grep ^Configured | tr " " "\n" | grep -- "--prefix" | head -1 | cut -f2 -d=`; \
		SYSROOT_DIR=`echo "$(BR2_TOOLCHAIN_EXTERNAL_PATH)" | sed -e "s|\(.*\)$${PREFIX}|\1|"` ;\
		SYSROOT="--sysroot $${SYSROOT_DIR}" ;\
	fi ;\
 \
	LIB_DIR=`$(TARGET_CC) $${SYSROOT} -print-file-name=$${LIB} | sed -e "s,$${LIB}\$$,,"`; \
 \
	if test -z "$${LIB_DIR}"; then \
		echo "copy_toolchain_lib_root: lib=$${LIB} not found"; \
		exit -1; \
	fi; \
 \

	@$(call copy_toolchain_lib_root, $(strip $(subst ",, $(BR2_TOOLCHAIN_EXTERNAL_LIB_C))), /lib, $(BR2_TOOLCHAIN_EXTERNAL_STRIP))
#")))
	for libs in $(strip $(subst ",, $(BR2_TOOLCHAIN_EXTERNAL_LIBS))); do \
		$(call copy_toolchain_lib_root, $$libs, /lib, $(BR2_TOOLCHAIN_EXTERNAL_STRIP)); \
	done


8.10 The origin Function

The origin function is unlike most other functions in that it does not operate
on the values of variables; it tells you something about a variable.
Specifically, it tells you `where it came from`

The syntax of the origin function is:

$(origin variable)

The result of this function is a string telling you how the variable variable
was defined:

‘command line’
if variable was defined on the command line.


={============================================================================
*kt_linux_tool_140* make-gmake-run

9.2 Arguments to Specify the Goals

If you specify several goals, make processes each of them in turn, in the
order you name them.

Make will set the special variable MAKECMDGOALS to the list of goals you
specified on the command line. If no goals were given on the command line,
this variable is empty. Note that this variable should be used only in special
  circumstances.


={============================================================================
*kt_linux_tool_150* make-autoconf

http://autotoolset.sourceforge.net/tutorial.html
http://www.gnu.org/software/autoconf/autoconf.html
http://www.gnu.org/software/autoconf/manual/index.html

Autoconf is a tool for `producing shell scripts` that automatically configure
software source code packages to adapt to many kinds of Posix-like systems. The
configuration scripts produced by Autoconf are independent of Autoconf when they
are run, so their users do not need to have Autoconf.


The configuration scripts produced by Autoconf require no manual user
intervention when run; they do not normally even need an argument specifying the
system type. Instead, they individually `test for the presence of each feature`
that the software package they are for might need. (Before each check, they
    print a one-line message stating what they are checking for, so the user
    doesn't get too bored while waiting for the script to finish.) As a result,
they deal well with systems that are hybrids or customized from the more common
  Posix variants. There is no need to maintain files that list the features
  supported by each release of each variant of Posix.

For each software package that Autoconf is used with, it 
`creates a configuration script from a template file` that lists the system
features that the package needs or can use. 


After the shell code to recognize and respond to a system
feature has been written, Autoconf allows it to be shared by many software
packages that can use (or need) that feature. If it later turns out that the
shell code needs adjustment for some reason, it needs to be changed in only one
place; all of the configuration scripts can be regenerated automatically to take
advantage of the updated code.

Those who do not understand Autoconf are condemned to reinvent it, poorly. The
primary goal of Autoconf is making the user's life easier; making the
maintainer's life easier is only a secondary goal. Put another way, the primary
goal is not to make the generation of configure automatic for package
maintainers (although patches along that front are welcome, since package
    maintainers form the user base of Autoconf); rather, the goal is to make
configure painless, portable, and predictable for the end user of each
autoconfiscated package. And to this degree, Autoconf is highly successful at
its goal — most complaints to the Autoconf list are about difficulties in
writing Autoconf input, and not in the behavior of the resulting configure. Even
packages that don't use Autoconf will generally provide a configure script, and
the most common complaint about these alternative home-grown scripts is that
they fail to meet one or more of the GNU Coding Standards (see Configuration)
that users have come to expect from Autoconf-generated configure scripts.

Autoconf requires GNU M4 version 1.4.6 or later in order to generate the
scripts. It uses features that some versions of M4, including GNU M4 1.3, do not
have. Autoconf works better with GNU M4 version 1.4.14 or later, though this is
not required. 


={============================================================================
*kt_linux_tool_150* make-autoconf-build-system

2 The GNU Build System

the GNU build system, whose most important components are Autoconf, Automake,
and Libtool.

2.1 Automake

The ubiquity of make means that a makefile is almost the only viable way to
distribute automatic build rules for software, but one quickly runs into its
numerous limitations.  Its lack of support for automatic dependency tracking,
recursive builds in subdirectories, reliable timestamps (e.g., for network
    file systems), and so on, mean that developers must painfully (and often
      incorrectly) reinvent the wheel for each project. 
    
Portability is nontrivial, thanks to the quirks of make on many systems. On
top of all this is the manual labor required to implement the many standard
targets that users have come to expect (make install, make distclean, make
    uninstall, etc.). Since you are, of course, using Autoconf, you also have
to insert repetitive code in your ‘Makefile.in’ to recognize @CC@, @CFLAGS@,
and other substitutions provided by configure.


2.3 Libtool

Producing `shared libraries portably`, however, is the stuff of
nightmares-each system has its own incompatible tools, compiler flags, and
magic incantations.  Fortunately, GNU provides a solution: Libtool.


={============================================================================
*kt_linux_tool_150* make-autoconf-model

3 Making configure Scripts

The configuration scripts that `Autoconf produces` are by convention called
`configure`.  When run, `configure creates several files`, replacing
configuration parameters in them with appropriate values. The files that
configure creates are:

  * one or more ‘Makefile’ files, usually one in each subdirectory of the
    package (see Section 4.8 [Makefile Substitutions], page 23);

  * optionally, a C header file, the name of which is configurable, containing
    #define directives (see Section 4.9 [Configuration Headers], page 33);

  * a shell script called `config.status` that, when run, recreates the files
    listed above (see Chapter 17 [config.status Invocation], page 301);

  * an optional shell script normally called `config.cache` (created when
    using `configure --config-cache`) that saves the results of running many
    of the tests (see Section 7.4.2 [Cache Files], page 119);

  * a file called `config.log` containing any messages produced by compilers,
    to help debugging if configure makes a mistake.

To create a configure script with Autoconf, you need to write an Autoconf
input file `configure.ac` (or ‘configure.in’) and `run autoconf` on it. 

If you write `your own feature tests` to supplement those that come with
Autoconf, you might also write files called `aclocal.m4` and ‘acsite.m4’. 

If you use a C header file to contain #define directives, you might also run
autoheader, and you can distribute the generated file `config.h.in` with the
package.

Here is a diagram showing how the files that can be used in configuration are
produced.  Programs that are executed are suffixed by ‘*’. Optional files are
enclosed in square brackets (‘[]’). autoconf and autoheader also read the
installed Autoconf macro files (by reading ‘autoconf.m4’).

Files used in preparing a software package for distribution, when using just
Autoconf:

your source files --> [autoscan*] --> [configure.scan] --> configure.ac

configure.ac --.
               |   .------> `autoconf*` -----> configure
[aclocal.m4] --+---+
               |   ‘-----> [autoheader*] --> [config.h.in]
[acsite.m4] ---’

Makefile.in

<when-use-automake>
Additionally, if you use Automake, the following additional productions come
into play:

[acinclude.m4] --.
                 |
[local macros] --+--> aclocal* --> aclocal.m4
                 |
configure.ac ----’

configure.ac --.
               +--> `automake*` --> Makefile.in
Makefile.am ---’


Files used in configuring a software package:

                       .-------------> [config.cache]
configure* ------------+-------------> config.log
                       |
[`config.h.in`] -.       v            .-> [config.h] -.
               +--> config.status* -+               +--> make*
`Makefile.in` ---’                    ‘-> `Makefile` ---’


={============================================================================
*kt_linux_tool_150* make-autoconf-write

3.1 Writing configure.ac

To produce a configure script for a software package, create a file called
configure.ac that contains invocations of the Autoconf macros that test the
system features your package needs or can use. Autoconf macros already exist to
check for many features; see Existing Tests, for their descriptions. For most
other features, you can use Autoconf template macros to produce custom checks;
see Writing Tests, for information about them. 
  
Previous versions of Autoconf promoted the name configure.in. Using
`configure.ac is now preferred`

  *what-autoconf-do*
  So, what is really needed is some kind of compiler, autoconf, that takes an
  Autoconf program, ‘configure.ac’, and transforms it into a portable shell
  script, configure.


3.1.2 The Autoconf Language

Therefore, we need a means to distinguish `literal strings` from text to be
expanded: quotation. `When calling macros that take arguments`, there must not
be any white space between the macro name and the open parenthesis.

AC_INIT ([oops], [1.0]) # incorrect
AC_INIT([hello], [1.0]) # good

Arguments should be enclosed within the `quote characters [ and ]`, and be
separated by commas.


Some macros take optional arguments, which this documentation represents as
[arg] (not to be confused with the quote characters). You may just leave them
empty, or use ‘[]’ to make the emptiness of the argument explicit, or you may
  simply omit the trailing commas.

The three lines below are equivalent:

AC_CHECK_HEADERS([stdio.h], [], [], [])
AC_CHECK_HEADERS([stdio.h],,,)
AC_CHECK_HEADERS([stdio.h])


You can include `comments` in ‘configure.ac’ files by starting them with the
‘#’. For example, it is helpful to begin ‘configure.ac’ files with a line like
this:


3.1.3 Standard ‘configure.ac’ Layout

The order in which ‘configure.ac’ calls the Autoconf macros is not important,
    with a few exceptions. 

Every configure.ac `must contain` a call to AC_INIT before the checks, and a
call to AC_OUTPUT at the end. 

Additionally, some macros rely on other macros having been called first,
because they check previously set values of some variables to decide what to
do. 

These macros are noted in the individual descriptions (see Chapter 5 [Existing
    Tests], page 41), and they also warn you when configure is created if they
  are called out of order.

Generally speaking, the things near the end of this list are those that could
depend on things earlier in it.


3.3 Using ifnames to List Conditionals

*tool-ifnames*
`ifnames` scans all of the C source files named on the command line (or the
    standard input, if none are given) and writes to the standard output a
sorted list of all the identifiers that appear in those files in
#if, #elif, #ifdef, or #ifndef directives. 

It prints each identifier on a line, followed by a space-separated list of the
files in which that identifier occurs.

<ex>
$ ifnames planner_common_pdl.c 
BSKYB_SUPPORT_HEP_WITH_OIG planner_common_pdl.c
HEADEND_METADATA_CMDC_CMC planner_common_pdl.c


3.4 Using autoconf to Create configure

`To create 'configure' from configure.ac`, run the autoconf program with no
arguments. autoconf processes configure.ac with the M4 macro processor, using
the Autoconf macros. If you give autoconf an argument, it reads that file
instead of configure.ac and writes the configuration script to the standard
output instead of to configure. If you give autoconf the argument -, it reads
from the standard input instead of configure.ac and writes the configuration
script to the standard output.

The Autoconf macros are defined in several files. If a macro is defined in
more than one of the files that autoconf reads, the last definition it reads
overrides the earlier ones.


<autoconf-options>
autoconf accepts the following options: 

‘--verbose’
‘-v’ Report processing steps.

‘--debug’
‘-d’ Don’t remove the temporary files.

‘--force’
‘-f’ Remake ‘configure’ even if newer than its input files.


‘--warnings=category’
‘-W category’
Report the warnings related to category (which can actually be a comma
separated list). See Section 10.3 [Reporting Messages], page 179, macro AC_
DIAGNOSE, for a comprehensive list of categories. Special values include:

‘all’ report all the warnings
‘none’ report none
‘error’ treats warnings as errors
‘no-category’ disable warnings falling into category

Warnings about ‘syntax’ are enabled by default, and the environment
variable WARNINGS, a comma separated list of categories, is honored as well.
Passing ‘-W category’ actually behaves as if you had passed ‘--warnings
syntax,$WARNINGS,category’. To disable the defaults and WARNINGS, and
then enable warnings about obsolete constructs, use ‘-W none,obsolete’.

<backtrace>
Because autoconf uses autom4te behind the scenes, it `displays a back trace`
for errors, but not for warnings; if you want them, just pass ‘-W error’. See
Section 8.2.1 [autom4te Invocation], page 132, for some examples.

<trace>
‘--trace=macro[:format]’
‘-t macro[:format]’

Do not create the configure script, but `list the calls to macro` according to
the format. Multiple ‘--trace’ arguments can be used to list several macros.
Multiple ‘--trace’ arguments for a single macro are not cumulative; instead,
         you should just make format as long as needed.

The format is a regular string, with newlines if desired, and several special
escape codes. It defaults to ‘$f:$l:$n:$%’; see Section 8.2.1 [autom4te
Invocation], page 132, for details on the format.


It is often necessary `to check the content of a ‘configure.ac’ file`, but
parsing it yourself is extremely fragile and error-prone. It is suggested that
you rely upon ‘--trace’ to scan ‘configure.ac’. For instance, to find the list
of variables that are substituted, use:

$ autoconf -t AC_SUBST
configure.ac:2:AC_SUBST:ECHO_C
configure.ac:2:AC_SUBST:ECHO_N
configure.ac:2:AC_SUBST:ECHO_T
More traces deleted


3.5 Using autoreconf to Update configure Scripts

`autoreconf runs` autoconf, autoheader, aclocal, automake, libtoolize, and
autopoint (when appropriate) repeatedly to update `the GNU Build System` in
the specified directories and their subdirectories


={============================================================================
*kt_linux_tool_150* make-autoconf-macro

4.1 Initializing configure

Every configure.ac must call AC_INIT before doing anything else that produces
output.

AC_INIT (package, version, [bug-report], [tarname], [url]) [Macro]

Process any command-line arguments and perform initialization and
verification.

The following M4 macros (e.g., AC_PACKAGE_NAME), output variables (e.g.,
    PACKAGE_ NAME), and preprocessor symbols (e.g., PACKAGE_NAME), are
`defined by AC_INIT`:


4.2 Dealing with Autoconf versions

The following optional macros can be used to help choose the minimum version
of Autoconf that can successfully compile a given ‘configure.ac’.

AC_PREREQ (version) [Macro]

Ensure that a recent enough version of Autoconf is being used. If the version
of Autoconf being used to create configure is earlier than version, print an
error message to the standard error output and exit with failure (exit status
is 63). For example: AC_PREREQ([2.69])

This macro may be used before AC_INIT.


4.4 Finding configure Input

AC_CONFIG_SRCDIR (unique-file-in-source-dir) [Macro]

unique-file-in-source-dir is some file that is in the package’s source
directory; configure checks for this `file's existence` to make sure that the
directory that it is told contains the source code in fact does. Occasionally
people accidentally specify the wrong directory with ‘--srcdir’; this is a
safety check. See Section 16.10 [configure Invocation], page 299, for more
information.

Packages that do manual configuration or use the install program might need to
  tell configure where to find some other shell scripts by calling
  AC_CONFIG_AUX_DIR, though the default places it looks are correct for most
  cases.


4.5 Outputting Files

Every Autoconf script, e.g., ‘configure.ac’, should finish by calling
AC_OUTPUT. (When runs `configure` scrupt) That is the macro that generates and
runs ‘config.status’, which in turn `creates the makefiles` and any other files
resulting from configuration.  This is the only required macro besides AC_INIT

AC_OUTPUT [Macro]
Generate ‘config.status’ and launch it. Call this macro once, at the end of
‘configure.ac’.

‘config.status’ performs all the configuration actions: all the output files,
header files, commands , links, subdirectories to configure are honored.

The location of your AC_OUTPUT invocation is the exact point where
configuration actions are taken: any code afterwards is executed by configure
once config.status was run.


<config-status>
4.6 Performing Configuration Actions

‘configure’ is designed so that it appears to do everything itself, but there
is actually a hidden slave: ‘config.status’. 

‘configure’ is in charge of examining your system, but it is ‘config.status’
that actually takes the proper actions based on the results of ‘configure’.
The most typical task of ‘config.status’ is `to instantiate files` 

This section describes the common behavior of the four standard instantiating
macros: AC_CONFIG_FILES, AC_CONFIG_HEADERS, AC_CONFIG_COMMANDS and
AC_CONFIG_LINKS.

They all have this prototype:

AC_CONFIG_ITEMS(tag..., [commands], [init-cmds])

where the arguments are:

`tag`

A `blank-or-newline-separated list` of tags, which are typically the names of
the files `to instantiate` You are encouraged to use `literals` as tags.

The macros AC_CONFIG_FILES and AC_CONFIG_HEADERS use special tag values: they
may have the form ‘output’ or ‘output:inputs’. The file output is instantiated
from its templates, `inputs` (`defaulting to output.in`).

‘AC_CONFIG_FILES([Makefile:boiler/top.mk:boiler/bot.mk])’, for example, asks
for the creation of the file ‘Makefile’ that contains the expansion of the
  output variables in the concatenation of ‘boiler/top.mk’ and
    ‘boiler/bot.mk’.

note:
Means that Makefile can be created from `Makefile.in`, bolier/top.mk, or
boiler/bot.mk.


4.7 Creating Configuration Files

AC_CONFIG_FILES (file . . . , [cmds], [init-cmds]) [Macro]

Make AC_OUTPUT `create each file` by copying an input file (by default
    ‘file.in’), `substituting the output variable values` This macro is one of
the instantiating macros;

This macro creates the directory that the file is in if it doesn't exist.
  Usually, makefiles are created this way, but other files


4.8 Substitutions in Makefiles

Each subdirectory in a distribution that contains something to be compiled or
installed should come with a file ‘Makefile.in’, from which configure creates
a file ‘Makefile’ in that directory. `To create Makefile`, configure performs
a simple variable substitution, replacing occurrences of `@variable@` in
`Makefile.in with the value that configure has determined for that variable`

Variables that are substituted into output files in this way are called
`output variables` They are ordinary shell variables that are set in
configure. 

To make configure substitute a particular variable into the output files, the
macro `AC_SUBST` must be called with that variable name as an argument. Any
occurrences of ‘@variable@’ for other variables are left unchanged. See
Section 7.2 [Setting Output Variables], for more information on creating
output variables with AC_SUBST.

A software package that uses a configure script should be distributed with a
file ‘Makefile.in’, but no makefile; that way, the user has to properly
configure the package for the local system before compiling it.

See Section “Makefile Conventions” in The GNU Coding Standards, for more
information on what to put in makefiles.


4.8.1 Preset Output Variables

Some output variables are preset by the Autoconf macros. Some of the Autoconf
macros set additional output variables, which are mentioned in the
descriptions for those macros.  See Section B.2 [Output Variable Index], page
364, for a complete list of output variables.  See Section 4.8.2 [Installation
Directory Variables], page 27, for the list of the preset ones related to
installation directories. 

Below are listed the other preset ones, many of which are precious variables


CFLAGS [Variable]

Debugging and optimization options for the C compiler. If it is not set in the
environment when configure runs, the default value is set when you call
AC_PROG_CC (or empty if you don’t). configure uses this variable when
compiling or linking programs to test for C features.

If a compiler option affects only the behavior of the preprocessor (e.g.,
    ‘-Dname’), it should be put into CPPFLAGS instead. If it affects only the
linker (e.g., ‘-Ldirectory’), it should be put into LDFLAGS instead. If it
affects only the compiler proper, CFLAGS is the natural home for it. If an
option affects multiple phases of the compiler, though, matters get tricky.
One approach to put such options directly into CC, e.g., CC=’gcc -m64’.
Another is to put them into both CPPFLAGS and LDFLAGS, but not into CFLAGS.

However, remember that some ‘Makefile’ variables are reserved by the GNU
Coding Standards for the use of the "user"—the person building the package.
For instance, CFLAGS is one such variable.

Sometimes package developers are tempted to set user variables such as CFLAGS
because it appears to make their job easier. However, the package itself
should never set a user variable, particularly not to include switches that
are required for proper compilation of the package. Since these variables are
documented as being for the package builder, that person rightfully expects to
be able to override any of these variables at build time. If the package
developer needs to add switches without interfering with the user, the proper
way to do that is to introduce an additional variable. Automake makes this
easy by introducing AM_CFLAGS (see Section “Flag Variables Ordering” in GNU
    Automake), but the concept is the same even if Automake is not used.


CPPFLAGS [Variable]

Preprocessor options for the C, C++, Objective C, and Objective C++
preprocessors and compilers. If it is not set in the environment when
configure runs, the default value is empty.

This variable’s contents should contain options like ‘-I’, ‘-D’, and ‘-U’ that
affect only the behavior of the preprocessor. Please see the explanation of
CFLAGS for what you can do if an option affects other phases of the compiler
as well.

Currently, configure always links as part of a single invocation of the
compiler that also preprocesses and compiles, so it uses this variable also
when linking programs.  However, `it is unwise to depend on this behavior`
because the GNU Coding Standards do not require it and many packages do not
use CPPFLAGS when linking programs.

See Section 7.3 [Special Chars in Variables], page 116, for limitations that
CPPFLAGS might run into.


CXXFLAGS [Variable]

Debugging and optimization options for the C++ compiler. It acts like CFLAGS,
but for C++ instead of C.


DEFS [Variable]

‘-D’ options to pass to the C compiler. If AC_CONFIG_HEADERS is called,
configure replaces ‘@DEFS@’ with ‘-DHAVE_CONFIG_H’ instead (see Section 4.9
    [Configuration Headers], page 33). This variable is not defined while
  configure is performing its tests, only when creating the output files.


LDFLAGS [Variable]
Options for the linker. If it is not set in the environment when configure
runs, the default value is empty. configure uses this variable when linking
programs to test for C, C++, Objective C, Objective C++, Fortran, and Go
features.  This variable’s contents should contain options like ‘-s’ and ‘-L’
that affect only the behavior of the linker. 

Don’t use this variable `to pass library names` (‘-l’) to the linker; use LIBS
instead.

<ex>
configure: using LDFLAGS:
-L/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/lib
-L/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/oss/lib
-Wl,--as-needed
-Wl,-rpath-link,/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/lib
-Wl,-rpath-link,/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/oss/lib
-L/data/ws/zinc-install-root/release/humax-dtr_t4000/lib64
-L/opt/oem-staging/humax-dtr_t4000/usr/local/lib  -mcpu=cortex-a15
-mtune=cortex-a15 -mfloat-abi=softfp -mfpu=neon-vfpv4 -D__arm__
-L/opt/oem-staging/humax-dtr_t4000/usr/local/lib
-Wl,-rpath-link,/opt/oem-staging/humax-dtr_t4000/usr/local/lib

configure: LDFLAGS error: LDFLAGS may only be used to specify linker flags, not
macro definitions. Use CPPFLAGS for: -D__arm__


LIBS [Variable]

‘-l’ options to pass to the linker. The default value is empty, but some
Autoconf macros may prepend extra libraries to this variable if those
libraries are found and provide necessary functions, see Section 5.4
[Libraries], page 49. configure uses this variable when linking programs to
test for C, C++, Objective C, Objective C++, Fortran, and Go features.


srcdir [Variable]
The name of the directory that contains the source code for that makefile.


4.8.2 Installation Directory Variables

The following variables specify the directories for package installation, see
Section “Variables for Installation Directories” in The GNU Coding Standards,
for more information.

exec_prefix [Variable]
The installation prefix for `architecture-dependent files` By default it’s the
same as `prefix`. You should avoid installing anything directly to exec_prefix.
However, the default value for directories containing architecture-dependent
files should be relative to exec_prefix.

prefix [Variable]
The common installation prefix for all files. If exec_prefix is defined to a
different value, prefix is used only for `architecture-independent files`

$ ./configure --help

Installation directories:
  --prefix=PREFIX         install architecture-independent files in PREFIX
                          [/usr/local]
  --exec-prefix=EPREFIX   install architecture-dependent files in EPREFIX
                          [PREFIX]

By default, `make install' will install all the files in `/usr/local/bin',
`/usr/local/lib' etc.  You can specify an installation prefix other than
    `/usr/local' using `--prefix', for instance `--prefix=$HOME'.

datadir [Variable]
The directory for installing idiosyncratic read-only architecture-independent
data.


Most of these variables have values that rely on prefix or exec_prefix. It is
deliberate that the directory output variables keep them unexpanded:

For instance, it is essential that datarootdir remains defined as
‘${prefix}/share’, so that its value can be expanded based on the current
value of prefix.

note:
Should be used only in makefiles.

A corollary is that you should not use these variables except in makefiles.
For instance, instead of trying to evaluate datadir in ‘configure’ and
hard-coding it in makefiles using e.g., ‘AC_DEFINE_UNQUOTED([DATADIR],
    ["$datadir"], [Data directory.])’, you should add ‘-DDATADIR=’$(datadir)’’
to your makefile’s definition of CPPFLAGS (AM_CPPFLAGS if you are also using
    Automake).

Similarly, you should not rely on AC_CONFIG_FILES to replace bindir and
friends in your shell scripts and other files; instead, let make manage their
replacement. For instance Autoconf ships templates of its shell scripts ending
with ‘.in’, and uses a makefile snippet similar to the following to build
scripts like autoheader and autom4te:


4.9 Configuration Header Files

When a package contains more than a few tests that define C preprocessor
symbols, the command lines to pass ‘-D’ options to the compiler can get quite
long. This causes two problems. One is that the make output is hard to
visually scan for errors. More seriously, the command lines can exceed the
length limits of some operating systems. 

As an alternative to passing ‘-D’ options to the compiler, configure scripts
can create a C header file containing ‘#define’ directives. The
`AC_CONFIG_HEADERS` macro selects this kind of output. Though it can be called
anywhere between AC_INIT and AC_OUTPUT, it is customary to call it right after
AC_INIT.

The package should ‘#include’ the configuration header file before any other
header files, to prevent inconsistencies in declarations (for example, if it
redefines const).

To provide for VPATH builds, remember to pass the C compiler a ‘-I.’ option
(or ‘-I..’; whichever directory contains ‘config.h’). Even if you use
‘#include "config.h"’, the preprocessor searches only the directory of the
currently read file, i.e., the source directory, not the build directory.

With the appropriate ‘-I’ option, you can use ‘#include <config.h>’. Actually,
it’s a good habit to use it, because in the rare case when the source
  directory contains another ‘config.h’, the build directory should be
  searched first.

AC_CONFIG_HEADERS (header . . . , [cmds], [init-cmds]) [Macro]

This macro is one of the instantiating macros; see Section 4.6 [Configuration
Actions], page 21. Make AC_OUTPUT create the file(s) in the
blank-or-newline-separated list header containing C preprocessor #define
statements, and replace ‘@DEFS@’ in generated files with ‘-DHAVE_CONFIG_H’
instead of the value of DEFS. The usual name for header is ‘config.h’.

configure
3527:DEFS=-DHAVE_CONFIG_H


4.9.1 Configuration Header Templates

Your distribution should contain a template file that looks as you want the
final header file to look, including comments, with #undef statements which
are used as hooks. For example, suppose your ‘configure.ac’ makes these calls:

AC_CONFIG_HEADERS([conf.h])
AC_CHECK_HEADERS([unistd.h])

Then you could have code like the following in ‘conf.h.in’. The ‘conf.h’
created by configure defines ‘HAVE_UNISTD_H’ to 1, if and only if the system
has ‘unistd.h’.
/* Define as 1 if you have unistd.h. */
#undef HAVE_UNISTD_H

note:
Can use autoreconof to create config.h.in


={============================================================================
*kt_linux_tool_150* make-autoconf-test

5 Existing Tests

These macros test for particular system features that packages might need or
want to use. If you need to test for a kind of feature that none of these
macros check for, you can probably do it by calling primitive test macros with
appropriate arguments (see Chapter 6 [Writing Tests], page 101).

These tests print messages telling the user which feature they're checking
for, and what they find. They cache their results for future configure runs
  (see Section 7.4 [Caching Results], page 117).

`Some of these macros set output variables` See Section 4.8 [Makefile
Substitutions], page 23, for how to get their values. 
  
The phrase “define name” is used below as a shorthand to mean “define the C
preprocessor symbol name to the value 1”. See Section 7.1 [Defining Symbols],
page 113, for how to get those symbol definitions into your program.  
  

5.1 Common Behavior 

Much effort has been expended to make Autoconf easy to learn. The most obvious
way to reach this goal is simply to enforce standard interfaces and behaviors,
avoiding exceptions as much as possible. Because of history and inertia,
unfortunately, there are still too many exceptions in Autoconf; nevertheless,
this section describes some of the common rules.


5.2.2 Generic Program and File Checks

These macros are used to find programs not covered by the "particular"
test macros.  If you need to check the behavior of a program as well as
find out whether it is present, you have to write your own test for it
(*note Writing Tests::).  By default, these macros use the environment
variable `PATH'.  If you need to check for a program that might not be
in the user's `PATH', you can pass a modified path to use instead, like
this:

     AC_PATH_PROG([INETD], [inetd], [/usr/libexec/inetd],
                  [$PATH$PATH_SEPARATOR/usr/libexec$PATH_SEPARATOR]dnl
     [/usr/sbin$PATH_SEPARATOR/usr/etc$PATH_SEPARATOR/etc])

   You are strongly encouraged to declare the VARIABLE passed to
`AC_CHECK_PROG' etc. as precious.  *Note Setting Output Variables::,
`AC_ARG_VAR', for more details.

 -- Macro: AC_CHECK_PROG (VARIABLE, PROG-TO-CHECK-FOR, VALUE-IF-FOUND,
          [VALUE-IF-NOT-FOUND], [PATH = `$PATH'], [REJECT])
     Check whether program PROG-TO-CHECK-FOR exists in PATH.  If it is
     found, set VARIABLE to VALUE-IF-FOUND, otherwise to
     VALUE-IF-NOT-FOUND, if given.  Always pass over REJECT (an
     absolute file name) even if it is the first found in the search
     path; in that case, set VARIABLE using the absolute file name of
     the PROG-TO-CHECK-FOR found that is not REJECT.  If VARIABLE was
     already set, do nothing.  Calls `AC_SUBST' for VARIABLE.  The
     result of this test can be overridden by setting the VARIABLE
     variable or the cache variable `ac_cv_prog_VARIABLE'.

<ex>
# Accept binutils 2.13 or newer.
AC_CHECK_PROG_VER(AS, $AS, --version,
		  [GNU assembler.* \([0-9]*\.[0-9.]*\)],
		  [2.1[3-9]*], AS=: critic_missing="$critic_missing as")
AC_CHECK_PROG_VER(LD, $LD, --version,
		  [GNU ld.* \([0-9][0-9]*\.[0-9.]*\)],
		  [2.1[3-9]*], LD=: critic_missing="$critic_missing ld")

AC_CHECK_PROG_VER(MAKE, gnumake gmake make, --version,
  [GNU Make[^0-9]*\([0-9][0-9.]*\)],
  [3.79* | 3.[89]*], critic_missing="$critic_missing make")

if test -z "$MAKE"; then
  ac_verc_fail=yes
else
  # Found it, now check the version.
  { $as_echo "$as_me:${as_lineno-$LINENO}: checking version of $MAKE" >&5
$as_echo_n "checking version of $MAKE... " >&6; }
  ac_prog_version=`$MAKE --version 2>&1 | sed -n 's/^.*GNU Make[^0-9]*\([0-9][0-9.]*\).*$/\1/p'`
  case $ac_prog_version in
    '') ac_prog_version="v. ?.??, bad"; ac_verc_fail=yes;;
    3.79* | 3.[89]*)
       ac_prog_version="$ac_prog_version, ok"; ac_verc_fail=no;;
    *) ac_prog_version="$ac_prog_version, bad"; ac_verc_fail=yes;;

  esac
  { $as_echo "$as_me:${as_lineno-$LINENO}: result: $ac_prog_version" >&5
$as_echo "$ac_prog_version" >&6; }
fi
if test $ac_verc_fail = yes; then
  critic_missing="$critic_missing make"
fi


 -- Macro: AC_CHECK_PROGS (VARIABLE, PROGS-TO-CHECK-FOR,
          [VALUE-IF-NOT-FOUND], [PATH = `$PATH'])
     Check for each program in the blank-separated list
     PROGS-TO-CHECK-FOR existing in the PATH.  If one is found, set
     VARIABLE to the name of that program.  Otherwise, continue
     checking the next program in the list.  If none of the programs in
     the list are found, set VARIABLE to VALUE-IF-NOT-FOUND; if
     VALUE-IF-NOT-FOUND is not specified, the value of VARIABLE is not
     changed.  Calls `AC_SUBST' for VARIABLE.  The result of this test
     can be overridden by setting the VARIABLE variable or the cache
     variable `ac_cv_prog_VARIABLE'.


5.5.3 Generic Function Checks

These macros are used to find functions not covered by the "particular"
test macros. If the functions might be in libraries other than the
default C library, first call `AC_CHECK_LIB' for those libraries. 

If you need to check the behavior of a function as well as find out whether it
is present, you have to write your own test for it (*note Writing Tests::).

 -- Macro: AC_CHECK_FUNC (FUNCTION, [ACTION-IF-FOUND],
          [ACTION-IF-NOT-FOUND])

     If C function FUNCTION is available, run shell commands ACTION-IF-FOUND,
otherwise ACTION-IF-NOT-FOUND.  If you just want to define a symbol if the
  function is available, consider using `AC_CHECK_FUNCS' instead.  This macro
  checks for functions with C linkage even when `AC_LANG(C++)' has been
  called, since C is more standardized than C++.  

     This macro `caches its result in the ac_cv_func_FUNCTION variable`


5.10 Compilers and Preprocessors

All the tests for compilers (AC_PROG_CC, AC_PROG_CXX, AC_PROG_F77) define the
output variable EXEEXT based on the output of the compiler, typically to the
empty string if Posix and ‘.exe’ if a DOS variant.

They also define the output variable `OBJEXT` based on the output of the
compiler, after ‘.c’ files have been excluded, typically to ‘o’ if Posix,
‘obj’ if a DOS variant.

If the compiler being used does not produce executables, the tests fail. If
the executables can’t be run, and cross-compilation is not enabled, they fail
too. See Chapter 14 [Manual Configuration], page 281, for more on support for
cross compiling.


5.10.2 Generic Compiler Characteristics

 -- Macro: AC_CHECK_SIZEOF (TYPE-OR-EXPR, [UNUSED], [INCLUDES =
          `AC_INCLUDES_DEFAULT'])

     Define `SIZEOF_TYPE-OR-EXPR' (*note Standard Symbols::) to be the
     size in bytes of TYPE-OR-EXPR, which may be either a type or an
     expression returning a value that has a size.  If the expression
     `sizeof (TYPE-OR-EXPR)' is invalid, the result is 0.  INCLUDES is
     a series of include directives, defaulting to
     `AC_INCLUDES_DEFAULT' (*note Default Includes::), which are used
     prior to the expression under test.

     This macro now works even when cross-compiling.  The UNUSED
     argument was used when cross-compiling.

     For example, the call

          AC_CHECK_SIZEOF([int *])

     defines `SIZEOF_INT_P' to be 8 on DEC Alpha AXP systems.

     This macro caches its result in the `ac_cv_sizeof_TYPE-OR-EXPR`
     variable, with `*' mapped to `p' and other characters not suitable
     for a variable name mapped to underscores.


={============================================================================
*kt_linux_tool_150* make-autoconf-test-write

6 Writing Tests
***************

If the existing feature tests don't do something you need, you have to
write new ones.  These macros are the building blocks.  They provide
ways for other macros to check whether various kinds of features are
available and report the results.


6.2.3 Generating Sources

Autoconf provides a set of macros that can be used `to generate test source`
files.  They are written to be language generic, i.e., they actually depend on
the current language (*note Language Choice::) to "format" the output
properly.

 -- Macro: AC_LANG_PROGRAM (PROLOGUE, BODY)
     Expands into a source file which consists of the PROLOGUE, and
     then BODY as body of the main function (e.g., `main' in C).  Since
     it uses `AC_LANG_SOURCE', the features of the latter are available.

   For instance:

     AC_INIT([Hello], [1.0], [bug-hello@example.org], [],
             [http://www.example.org/])
     AC_DEFINE([HELLO_WORLD], ["Hello, World\n"],
       [Greetings string.])
     AC_LANG_CONFTEST(
     [AC_LANG_PROGRAM([[const char hw[] = "Hello, World\n";]],
                      [[fputs (hw, stdout);]])])
     gcc -E -dD conftest.c

on a system with `gcc' installed, results in:

     ...
     # 1 "conftest.c"

     #define PACKAGE_NAME "Hello"
     #define PACKAGE_TARNAME "hello"
     #define PACKAGE_VERSION "1.0"
     #define PACKAGE_STRING "Hello 1.0"
     #define PACKAGE_BUGREPORT "bug-hello@example.org"
     #define PACKAGE_URL "http://www.example.org/"
     #define HELLO_WORLD "Hello, World\n"

     const char hw[] = "Hello, World\n";
     int
     main ()
     {
     fputs (hw, stdout);
       ;
       return 0;
     }


6.4 Running the Compiler

To check for a syntax feature of the current language's (*note Language
Choice::) compiler, such as whether it recognizes a certain keyword, or
simply to try some library feature, use `AC_COMPILE_IFELSE' to try to
compile a small program that uses that feature.

 -- Macro: AC_COMPILE_IFELSE (INPUT, [ACTION-IF-TRUE],
          [ACTION-IF-FALSE])
     Run the compiler and compilation flags of the current language
     (*note Language Choice::) on the INPUT, run the shell commands
     ACTION-IF-TRUE on success, ACTION-IF-FALSE otherwise.  The INPUT
     can be made by `AC_LANG_PROGRAM' and friends.


={============================================================================
*kt_linux_tool_150* make-autoconf-test-result

7 Results of Tests

Once configure has determined whether a feature exists, what can it do to
record that information? There are four sorts of things it can do: define a C
preprocessor symbol, set a variable in the output files, save the result in a
cache file for future configure runs, and print a message letting the user
know the result of the test.

7.1 Defining C Preprocessor Symbols

A common action to take in response to a feature test is to define a C
preprocessor symbol indicating the results of the test. That is done by
calling AC_DEFINE or AC_DEFINE_UNQUOTED.

By default, `AC_OUTPUT' places the symbols defined by these macros into the
output variable DEFS, which contains an option -DSYMBOL=VALUE for each
symbol defined.  Unlike in Autoconf version 1, there is no variable `DEFS'
defined while `configure' is running.

// not use DEFS so

To check whether Autoconf macros have already defined a certain C preprocessor
symbol, test the value of the appropriate `cache variable`, as in this example:

     AC_CHECK_FUNC([vprintf], [AC_DEFINE([HAVE_VPRINTF], [1],
                               [Define if vprintf exists.])])
     if test "x$ac_cv_func_vprintf" != xyes; then
       AC_CHECK_FUNC([_doprnt], [AC_DEFINE([HAVE_DOPRNT], [1],
                                 [Define if _doprnt exists.])])
     fi

If AC_CONFIG_HEADERS has been called, then instead of creating DEFS, AC_OUTPUT
`creates a header file` by substituting the correct values into #define
statements in a template file.  *Note Configuration Headers::, for more
information about this kind of output.

 -- Macro: AC_DEFINE (VARIABLE, VALUE, [DESCRIPTION])
 -- Macro: AC_DEFINE (VARIABLE)
     Define VARIABLE to VALUE (verbatim), by defining a C preprocessor
     macro for VARIABLE.  VARIABLE should be a `C identifier`, optionally
     suffixed by a parenthesized argument list to define a C
     preprocessor macro with arguments.  The macro argument list, if
     present, should be a comma-separated list of C identifiers,
     possibly terminated by an ellipsis `...' if C99 syntax is employed.
     VARIABLE should not contain comments, white space, trigraphs,
     backslash-newlines, universal character names, or non-ASCII
     characters.

     VALUE may contain backslash-escaped newlines, which will be
     preserved if you use 'AC_CONFIG_HEADERS' but flattened if passed
     via `@DEFS@' (with no effect on the compilation, since the
     preprocessor sees only one line in the first place).  VALUE should
     not contain raw newlines.  If you are not using
     'AC_CONFIG_HEADERS', VALUE should not contain any '#' characters,
     as `make' tends to eat them.  To use a shell variable, use
     'AC_DEFINE_UNQUOTED' instead.

     DESCRIPTION is only useful if you are using 'AC_CONFIG_HEADERS'.
     In this case, DESCRIPTION is put into the generated 'config.h.in'
     as the comment before the macro define.  The following example
     `defines the C preprocessor variable` 'EQUATION' to be the string
     constant `"$a > $b"':

          AC_DEFINE([EQUATION], ["$a > $b"],
            [Equation string.])

     If neither VALUE nor DESCRIPTION are given, then VALUE defaults to
     1 instead of to the empty string.  This is for backwards
     compatibility with older versions of Autoconf, but this usage is
     obsolescent and may be withdrawn in future versions of Autoconf.

     If the VARIABLE is a literal string, it is passed to
     `m4_pattern_allow' (*note Forbidden Patterns::).

     If multiple `AC_DEFINE' statements are executed for the same
     VARIABLE name (not counting any parenthesized argument list), the
     last one wins.


7.2 Setting Output Variables

Another way to record the results of tests is to set `output variables`, which
are `shell variables` whose values are substituted into files that `configure'
outputs.  The two macros below create new output variables. *Note Preset
Output Variables::, for a list of output variables that are always available.

 -- Macro: AC_SUBST (VARIABLE, [VALUE])
     Create an output variable from a shell variable.  Make `AC_OUTPUT'
     substitute the variable VARIABLE into output files (typically one
     or more makefiles). 
     
     `This means` that AC_OUTPUT replaces instances of `@VARIABLE@` in input
     files with the value that the shell variable VARIABLE has when
     `AC_OUTPUT' is called.  
     
     The value can contain any non-`NUL' character, including newline.  If you
     are using Automake 1.11 or newer, for newlines in values you might want
     to consider using `AM_SUBST_NOTMAKE' to prevent `automake' from adding a
     line VARIABLE = @VARIABLE@' to the Makefile.in files (*note Automake:
         (automake)Optional.).

     Variable occurrences should not overlap: e.g., an input file should
     not contain `@VAR1@VAR2@' if VAR1 and VAR2 are variable names.
     The substituted value is not rescanned for more output variables;
     occurrences of `@VARIABLE@' in the value are inserted literally
     into the output file.  (The algorithm uses the special marker
     `|#_!!_#|' internally, so neither the substituted value nor the
     output file may contain `|#_!!_#|'.)

     If VALUE is given, in addition assign it to VARIABLE.

     The string VARIABLE is passed to `m4_pattern_allow' (*note
     Forbidden Patterns::).


7.4 Caching Results
===================

To avoid checking for the same features repeatedly in various `configure'
scripts (or in repeated runs of one script), `configure' can `optionally` save
the results of many checks in a "cache file" (*note Cache Files::).  If a
`configure' script runs with caching enabled and finds a cache file, it reads
the results of previous runs from the cache and avoids rerunning those checks.
As a result, `configure' can then run much faster than if it had to perform
all of the checks every time.

 -- Macro: AC_CACHE_VAL (CACHE-ID, COMMANDS-TO-SET-IT)
     Ensure that the results of the check identified by CACHE-ID are
     available.  If the results of the check `were in the cache file` that was
     read, and configure was not given the --quiet or --silent option, print a
     message saying that the result was cached; otherwise, run the shell
     commands COMMANDS-TO-SET-IT.  If the shell commands are run to determine
     the value, the value is saved in the cache file just before `configure'
     creates its output files.  *Note Cache Variable Names::, for how to
     choose the name of the CACHE-ID variable.

     The COMMANDS-TO-SET-IT _must have no side effects_ except for setting the
     variable CACHE-ID, see below.

 -- Macro: AC_CACHE_CHECK (MESSAGE, CACHE-ID, COMMANDS-TO-SET-IT)
     A wrapper for `AC_CACHE_VAL' that takes care of printing the
     messages.  This macro provides a convenient shorthand for the most
     common way to use these macros.  It calls `AC_MSG_CHECKING' for
     MESSAGE, then `AC_CACHE_VAL' with the CACHE-ID and COMMANDS
     arguments, and `AC_MSG_RESULT' with CACHE-ID.

     The COMMANDS-TO-SET-IT _must have no side effects_ except for
     setting the variable CACHE-ID, see below.

   It is common to find buggy macros using `AC_CACHE_VAL' or AC_CACHE_CHECK,
      because people are tempted to call `AC_DEFINE' in the
        COMMANDS-TO-SET-IT.  Instead, the code that _follows_ the call to
        AC_CACHE_VAL' should call `AC_DEFINE', by examining the value of the
        cache variable.  For instance, the following macro is broken:

     AC_DEFUN([AC_SHELL_TRUE],
     [AC_CACHE_CHECK([whether true(1) works], [my_cv_shell_true_works],
                     [my_cv_shell_true_works=no
                      (true) 2>/dev/null && my_cv_shell_true_works=yes
                      if test "x$my_cv_shell_true_works" = xyes; then
                        AC_DEFINE([TRUE_WORKS], [1],
                                  [Define if `true(1)' works properly.])
                      fi])
     ])

   This fails if the cache is enabled: the second time this macro is run,
   `TRUE_WORKS' _will not be defined_.  The proper implementation is:

     AC_DEFUN([AC_SHELL_TRUE],
     [AC_CACHE_CHECK([whether true(1) works], [my_cv_shell_true_works],
                     [my_cv_shell_true_works=no
                      (true) 2>/dev/null && my_cv_shell_true_works=yes])
      if test "x$my_cv_shell_true_works" = xyes; then
        AC_DEFINE([TRUE_WORKS], [1],
                  [Define if `true(1)' works properly.])
      fi
     ])

   Also, COMMANDS-TO-SET-IT should not print any messages, for example with
   `AC_MSG_CHECKING'; do that before calling `AC_CACHE_VAL', so the messages
   are printed regardless of whether the results of the check are retrieved
   from the cache or determined by running the shell commands.


7.4.1 Cache Variable Names
--------------------------

The names of cache variables should have the following format:

     PACKAGE-PREFIX_cv_VALUE-TYPE_SPECIFIC-VALUE_[ADDITIONAL-OPTIONS]

for example, `ac_cv_header_stat_broken' or
`ac_cv_prog_gcc_traditional'.  The parts of the variable name are:

PACKAGE-PREFIX
     An abbreviation for your package or organization; the same prefix
     you begin local Autoconf macros with, except lowercase by
     convention.  For cache values used by the distributed Autoconf
     macros, this value is `ac'.

`_cv_'
     Indicates that this shell variable is a `cache value`  This string
     _must_ be present in the variable name, including the leading
     underscore.

VALUE-TYPE
     A convention for classifying cache values, to produce a rational
     naming system.  The values used in Autoconf are listed in *note
     Macro Names::.

SPECIFIC-VALUE
     Which member of the class of cache values this test applies to.
     For example, which function (`alloca'), program (`gcc'), or output
     variable (`INSTALL').

ADDITIONAL-OPTIONS
     Any particular behavior of the specific member that this test
     applies to.  For example, `broken' or `set'.  This part of the
     name may be omitted if it does not apply.

   The values assigned to cache variables may not contain newlines.
Usually, their values are Boolean (`yes' or `no') or the names of files
or functions; so this is not an important restriction.  *note Cache
Variable Index:: for an index of cache variables with documented
semantics.


7.4.2 Cache Files
-----------------

A cache file is a shell script that caches the results of configure
tests run on one system so they can be shared between configure scripts
and configure runs.  It is not useful on other systems.  If its contents
are invalid for some reason, the user may delete or edit it, or override
documented cache variables on the `configure' command line.

   By default, `configure' uses no cache file, to avoid problems caused
by accidental use of stale cache files.

   To enable caching, `configure' accepts `--config-cache' (or `-C') to
cache results in the file `config.cache'.  Alternatively,
`--cache-file=FILE' specifies that FILE be the cache file.  The cache
file is created if it does not exist already.  When `configure' calls
`configure' scripts in subdirectories, it uses the `--cache-file'
argument so that they share the same cache.  *Note Subdirectories::,
for information on configuring subdirectories with the
`AC_CONFIG_SUBDIRS' macro.

   `config.status' only pays attention to the cache file if it is given
the `--recheck' option, which makes it rerun `configure'.

   It is wrong to try to distribute cache files for particular system
types.  There is too much room for error in doing that, and too much
administrative overhead in maintaining them.  For any features that
can't be guessed automatically, use the standard method of the canonical
system type and linking files (*note Manual Configuration::).

   The site initialization script can specify a site-wide cache file to
use, instead of the usual per-program cache.  In this case, the cache
file gradually accumulates information whenever someone runs a new
`configure' script.  (Running `configure' merges the new cache results
with the existing cache file.)  This may cause problems, however, if
the system configuration (e.g., the installed libraries or compilers)
changes and the stale cache file is not deleted.

   If `configure' is interrupted at the right time when it updates a
cache file outside of the build directory where the `configure' script
is run, it may leave behind a temporary file named after the cache file
with digits following it.  You may safely delete such a file.


={============================================================================
*kt_linux_tool_150* make-autoconf-package

15 Site Configuration
*********************

`configure' scripts support several kinds of local configuration
decisions.  There are ways for users to specify where external software
packages are, include or exclude optional features, install programs
under modified names, and set default values for `configure' options.


15.2 Working With External Software
===================================

Some packages require, or can optionally use, other software packages
that are already installed.  The user can give `configure' command line
options to specify which such external software to use.  The options
have one of these forms:

     --with-PACKAGE[=ARG]
     --without-PACKAGE

   For example, `--with-gnu-ld' means work with the GNU linker instead
of some other linker.  `--with-x' means work with The X Window System.

   The user can give an argument by following the package name with `='
and the argument.  Giving an argument of `no' is for packages that are
used by default; it says to _not_ use the package.  An argument that is
neither `yes' nor `no' could include a name or number of a version of
the other package, to specify more precisely which other package this
program `is supposed to work with`  If no argument is given, it defaults
to `yes`.  `--without-PACKAGE' is equivalent to `--with-PACKAGE=no'.

   Normally configure scripts complain about `--with-PACKAGE' options
that they do not support.  *Note Option Checking::, for details, and
for how to override the defaults.

   For each external software package that may be used, `configure.ac'
should call `AC_ARG_WITH' `to detect` whether the `configure' user asked
to use it.  Whether each package is used or not by default, and which
arguments are valid, is up to you.

 -- Macro: AC_ARG_WITH (PACKAGE, HELP-STRING, [ACTION-IF-GIVEN],
          [ACTION-IF-NOT-GIVEN])
     If the user gave configure the option `--with-PACKAGE` or
     `--without-PACKAGE', `run shell commands ACTION-IF-GIVEN`  
     
     If neither option was given, run shell commands ACTION-IF-NOT-GIVEN.

     note:
     This seeems to be different from the above:
     "If no argument is given, it defaults to `yes`"

     The name PACKAGE indicates another software package that this
     program should work with.  It should consist only of alphanumeric
     characters, dashes, plus signs, and dots.

     The option's argument is available to the shell commands
     ACTION-IF-GIVEN in the shell variable `withval', which is actually
     just the value of the shell variable named `with_PACKAGE', with
     any non-alphanumeric characters in PACKAGE changed into `_'.  You
     may use that variable instead, if you wish.

     The argument HELP-STRING is a description of the option that looks
     like this:
            --with-readline         support fancy command line editing

     HELP-STRING may be more than one line long, if more detail is
     needed.  Just make sure the columns line up in `configure --help'.
     Avoid tabs in the help string.  The easiest way to provide the
     proper leading whitespace is to format your HELP-STRING with the
     macro `AS_HELP_STRING' (*note Pretty Help Strings::).

     The following example shows how to use the `AC_ARG_WITH' macro in
     a common situation.  You want to let the user decide whether to
     enable support for an external library (e.g., the readline
     library); if the user specified neither `--with-readline' nor
     `--without-readline', you want to enable support for readline 
     `only if the library is available on the system.`

          AC_ARG_WITH([readline],
            [AS_HELP_STRING([--with-readline],
              [support fancy command line editing @<:@default=check@:>@])],
            [],
            [with_readline=check])

          LIBREADLINE=
          AS_IF([test "x$with_readline" != xno],
            [AC_CHECK_LIB([readline], [main],
              [AC_SUBST([LIBREADLINE], ["-lreadline -lncurses"])
               AC_DEFINE([HAVE_LIBREADLINE], [1],
                         [Define if you have libreadline])
              ],
              [if test "x$with_readline" != xcheck; then
                 AC_MSG_FAILURE(
                   [--with-readline was given, but test for readline failed])
               fi
              ], -lncurses)])

     The next example shows how to use `AC_ARG_WITH' to give the user
     the possibility to enable support for the readline library, in
     case it is still experimental and not well tested, and is
     therefore disabled by default.

          AC_ARG_WITH([readline],
            [AS_HELP_STRING([--with-readline],
              [enable experimental support for readline])],
            [],
            [with_readline=no])

          LIBREADLINE=
          AS_IF([test "x$with_readline" != xno],
            [AC_CHECK_LIB([readline], [main],
              [AC_SUBST([LIBREADLINE], ["-lreadline -lncurses"])
               AC_DEFINE([HAVE_LIBREADLINE], [1],
                         [Define if you have libreadline])
              ],
              [AC_MSG_FAILURE(
                 [--with-readline was given, but test for readline failed])],
              [-lncurses])])

     The last example shows how to use `AC_ARG_WITH' to give the user
     the possibility to disable support for the readline library, given
     that it is an important feature and that it should be enabled by
     default.

          AC_ARG_WITH([readline],
            [AS_HELP_STRING([--without-readline],
              [disable support for readline])],
            [],
            [with_readline=yes])

          LIBREADLINE=
          AS_IF([test "x$with_readline" != xno],
            [AC_CHECK_LIB([readline], [main],
              [AC_SUBST([LIBREADLINE], ["-lreadline -lncurses"])
               AC_DEFINE([HAVE_LIBREADLINE], [1],
                         [Define if you have libreadline])
              ],
              [AC_MSG_FAILURE(
                 [readline test failed (--without-readline to disable)])],
              [-lncurses])])

     These three examples can be easily adapted to the case where
     AC_ARG_ENABLE should be preferred to `AC_ARG_WITH' (see *note
     Package Options::).


={============================================================================
*kt_linux_tool_150* make-autoconf-run-configure

<ex>
Appears that "--xxx" populates to "$xxx" in configure.ac since when runs
with "--enable-libsanitizer", $enable-libsanitizer and  the following checks
is not in effect. This is the same as ${prefix}

// configure.ac
# Disable libsanitizer on unsupported systems.
if test -d ${srcdir}/libsanitizer; then
  if test x$enable_libsanitizer = x; then
     AC_MSG_CHECKING([for libsanitizer support])
     if (srcdir=${srcdir}/libsanitizer; \
        . ${srcdir}/configure.tgt; \
        test -n "$UNSUPPORTED")
     then
         AC_MSG_RESULT([no])
         noconfigdirs="$noconfigdirs target-libsanitizer"
     else
         AC_MSG_RESULT([yes])
     fi
  fi
fi


16.4 Installation Names
=======================

`By default`, make install' installs the package's commands under
`/usr/local/bin`, include files under `/usr/local/include', etc.  You can
specify an installation prefix other than `/usr/local' by giving configure'
the option `--prefix=PREFIX', where PREFIX must be an absolute file name.


16.5 Optional Features
======================

If the package supports it, you can cause programs to be installed with
an extra prefix or suffix on their names by giving `configure' the
option --program-prefix=PREFIX or --program-suffix=SUFFIX.

   Some packages pay attention to `--enable-FEATURE' options to configure,
where FEATURE indicates `an optional part of the package`  They may also pay
  attention to `--with-PACKAGE' options, where PACKAGE is something like
  gnu-as or x (for the X Window System).  The `README' should mention any
  `--enable-' and `--with-' options that the package recognizes.

   For packages that use the X Window System, `configure' can usually
find the X include and library files automatically, but if it doesn't,
you can use the `configure' options `--x-includes=DIR' and
`--x-libraries=DIR' to specify their locations.

   Some packages offer the ability to configure how verbose the
execution of `make' will be.  For these packages, running `./configure
--enable-silent-rules' sets the default to minimal output, which can be
overridden with `make V=1'; while running `./configure
--disable-silent-rules' sets the default to verbose, which can be
overridden with `make V=0'.


16.9 Defining Variables
=======================

Variables not defined in a site shell script can be set in the environment
passed to `configure'.  However, some packages may run configure again during
the build, and the customized values of these variables may be lost.  In order
to avoid this problem, you should set them in the configure command line,
using VAR=value.  For example:

     ./configure CC=/usr/local2/bin/gcc

causes the specified gcc to be used as the C compiler (unless it is overridden
    in the site shell script).


16.10 `configure' Invocation
============================

configure recognizes the following options to control how it operates.

`--help'
`-h'
     Print a summary of all of the options to `configure', and exit.

`--help=short'
`--help=recursive'
     Print a summary of the options `unique to this package's`
     `configure', and exit.  The `short' variant lists options used
     only in the top level, while the `recursive' variant lists options
     also present in any nested packages.


`--version'
`-V'
     Print the version of Autoconf `used to generate` the `configure'
     script, and exit.

`--cache-file=FILE'
     Enable the cache: use and save the results of the tests in FILE,
     traditionally `config.cache'.  FILE defaults to `/dev/null' to
     disable caching.

`--config-cache'
`-C'
     Alias for `--cache-file=config.cache'.

`configure' also accepts some other, not widely useful, options.  Run
`configure --help' for more details.

note:
configure script has `here` document for a help.

#
# Report the --help message.
#
if test "$ac_init_help" = "long"; then
  # Omit some internal or obsolete options to make the list less imposing.
  # This message is too long to be a string in the A/UX 3.1 sh.
  cat <<_ACEOF
\`configure' configures this package to adapt to many kinds of systems.

Usage: $0 [OPTION]... [VAR=VALUE]...

To assign environment variables (e.g., CC, CFLAGS...), specify them as
VAR=VALUE.  See below for descriptions of some of the useful variables.

(skipped)

Optional Features:
  --disable-option-checking  ignore unrecognized --enable/--with options
  --disable-FEATURE       do not include FEATURE (same as --enable-FEATURE=no)
  --enable-FEATURE[=ARG]  include FEATURE [ARG=yes]


={============================================================================
*kt_linux_tool_150* make-autoconf-configure

for ac_option
do

  -disable-* | --disable-*)
    ac_feature=``expr "x$ac_option" : 'x-*disable-\(.*\)'``
    # Reject names that are not valid shell variable names.
    expr "x$ac_feature" : ".*[^-_$as_cr_alnum]" >/dev/null &&
      { echo "$as_me: error: invalid feature name: $ac_feature" >&2
   { (exit 1); exit 1; }; }
    ac_feature=``echo $ac_feature | sed 's/-/_/g'``
    eval "enable_$ac_feature=no" ;;

// changes "--disable-feature" to enable_feature=no
//
+ case $ac_option in
++ expr x--disable-nls : 'x-*disable-\(.*\)'
+ ac_feature=nls
+ expr xnls : '.*[^-_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789]'
++ echo nls
++ sed s/-/_/g
+ ac_feature=nls
+ eval enable_nls=no
++ enable_nls=no


  -enable-* | --enable-*)
    ac_feature=`expr "x$ac_option" : 'x-*enable-\([^=]*\)'`
    # Reject names that are not valid shell variable names.
    expr "x$ac_feature" : ".*[^-_$as_cr_alnum]" >/dev/null &&
      { echo "$as_me: error: invalid feature name: $ac_feature" >&2
   { (exit 1); exit 1; }; }
    ac_feature=`echo $ac_feature | sed 's/-/_/g'`
    case $ac_option in
      *=*) ac_optarg=`echo "$ac_optarg" | sed "s/'/'\\\\\\\\''/g"`;;
      *) ac_optarg=yes ;;
    esac
    eval "enable_$ac_feature='$ac_optarg'" ;;

+ ac_optarg=
+ case $ac_option in
++ expr x--enable-gdbserver : 'x-*enable-\([^=]*\)'
+ ac_feature=gdbserver
+ expr xgdbserver : '.*[^-_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789]'
++ echo gdbserver
++ sed s/-/_/g
+ ac_feature=gdbserver
+ case $ac_option in
+ ac_optarg=yes
+ eval 'enable_gdbserver='\''yes'\'''
++ enable_gdbserver=yes


={============================================================================
*kt_linux_make_150* make-automake

https://www.gnu.org/software/automake/

https://www.gnu.org/software/automake/manual/automake.html
https://www.gnu.org/software/automake/manual/automake.txt


1 Introduction

Automake is a tool for automatically generating `Makefile.ins` 'from'
`Makefile.am`. Each Makefile.am is basically a series of make variable
definitions, with rules being thrown in occasionally. The generated
Makefile.ins are compliant with the GNU Makefile standards

Automake does constrain a project in 'certain' ways; for instance, it
'assumes' that the project uses `autoconf`, and enforces certain restrictions
on the configure.ac contents.

2.2.1 Basic Installation

make check 

causes the package's tests to be run. This step is not mandatory, but it is
often good to make sure the programs that have been built behave as they should,
before you decide to install them. Our example does not contain any tests, so
  running make check is a no-op.

make install

After everything has been built, and maybe tested, it is time to install it on
the system. That means copying the programs, libraries, header files, scripts,
and other data files from the source directory to their final destination on the
  system. The command make install will do that. 

make installcheck

A last and optional step is to run make installcheck. This command may run tests
on the installed files. make check tests the files in the source tree, while
make installcheck tests their installed copies. The tests run by the latter can
be different from those run by the former. For instance, there are tests that
cannot be run in the source tree. Conversely, some packages are set up so that
make installcheck will run the very same tests as make check, only on different
files (non-installed vs. installed). It can make a difference, for instance when
the source tree's layout is different from that of the installation. Furthermore
it may help to diagnose an incomplete installation.

Presently most packages do not have any installcheck tests because the existence
of installcheck is little known, and its usefulness is neglected. Our little toy
package is no better: make installcheck does nothing. 


={============================================================================
*kt_linux_make_150* make-automake-standard targets

2.2.2 Standard Makefile Targets

So far we have come across four ways to run make in the GNU Build System: make,
make check, make install, and make installcheck. The words check, install, and
  installcheck, passed as arguments to make, are called `targets`. 
  
*make-all*
make is a shorthand for make all, all being the default target in the GNU
Build System.


Here is a list of the most useful targets that the GNU Coding Standards specify.

make all
    Build programs, libraries, documentation, etc. (same as make). 

make install
    Install what needs to be installed, copying the files from the package’s
    tree to system-wide directories.  make install-strip

    Same as make install, then strip debugging symbols. Some users like to trade
    space for useful bug reports... 

make uninstall
    The opposite of make install: erase the installed files. (This needs to be
        run from the same build tree that was installed.) 

make clean
    Erase from the build tree the files built by make all. 

make `distclean`
    Additionally erase anything ./configure created. 

make check
    Run the test suite, if any. 

make installcheck
    Check the installed programs or libraries, if supported. 

make dist
    Recreate package-version.tar.gz from all the source files. 


From GNU Coding Standards

http://www.gnu.org/prep/standards/html_node/Standard-Targets.html#Standard-Targets

7.2.6 Standard Targets for Users

All GNU programs should have the following targets in their Makefiles:

‘all’

    Compile the entire program. This should be the default target. This target
    need not rebuild any documentation files; Info files should normally be
    included in the distribution, and DVI (and other documentation format) files
    should be made only when explicitly asked for.

    By default, the Make rules should compile and link with ‘-g’, so that
    executable programs have debugging symbols. Otherwise, you are essentially
    helpless in the face of a crash, and it is often far from easy to reproduce
    with a fresh build.  
    
‘install’

    Compile the program and copy the executables, libraries, and so on to the
    file names where they should reside for actual use. If there is a simple
    test to verify that a program is properly installed, this target should run
    that test.

    Do not strip executables when installing them. This helps eventual debugging
    that may be needed later, and nowadays disk space is cheap and dynamic
    loaders typically ensure debug sections are not loaded during normal
    execution. Users that need stripped binaries may invoke the install-strip
    target to do that.

    If possible, write the install target rule so that it does not modify
    anything in the directory where the program was built, provided ‘make all’
    has just been done. This is convenient for building the program under one
    user name and installing it under another.

    The commands should create all the directories in which files are to be
    installed, if they don’t already exist. This includes the directories
    specified as the values of the variables prefix and exec_prefix, as well as
    all subdirectories that are needed. One way to do this is by means of an
    installdirs target as described below.

    Use ‘-’ before any command for installing a man page, so that make will
    ignore any errors. This is in case there are systems that don’t have the
    Unix man page documentation system installed.

    The way to install Info files is to copy them into $(infodir) with
    $(INSTALL_DATA) (see Command Variables), and then run the install-info
    program if it is present. install-info is a program that edits the Info dir
    file to add or update the menu entry for the given Info file; it is part of
    the Texinfo package.

    Here is a sample rule to install an Info file that also tries to handle some
    additional situations, such as install-info not being present.

    do-install-info: foo.info installdirs
            $(NORMAL_INSTALL)
    # Prefer an info file in . to one in srcdir.
            if test -f foo.info; then d=.; \
             else d="$(srcdir)"; fi; \
            $(INSTALL_DATA) $$d/foo.info \
              "$(DESTDIR)$(infodir)/foo.info"
    # Run install-info only if it exists.
    # Use 'if' instead of just prepending '-' to the
    # line so we notice real errors from install-info.
    # Use '$(SHELL) -c' because some shells do not
    # fail gracefully when there is an unknown command.
            $(POST_INSTALL)
            if $(SHELL) -c 'install-info --version' \
               >/dev/null 2>&1; then \
              install-info --dir-file="$(DESTDIR)$(infodir)/dir" \
                           "$(DESTDIR)$(infodir)/foo.info"; \
            else true; fi

    When writing the install target, you must classify all the commands into
      three categories: normal ones, pre-installation commands and
      post-installation commands. See Install Command Categories.

‘install-html’
‘install-dvi’
‘install-pdf’
‘install-ps’

    These targets install documentation in formats other than Info; they’re
    intended to be called explicitly by the person installing the package, if
    that format is desired. GNU prefers Info files, so these must be installed
    by the install target.

    When you have many documentation files to install, we recommend that you
    avoid collisions and clutter by arranging for these targets to install in
    subdirectories of the appropriate installation directory, such as htmldir.
    As one example, if your package has multiple manuals, and you wish to
    install HTML documentation with many files (such as the “split” mode output
        by makeinfo --html), you’ll certainly want to use subdirectories, or two
    nodes with the same name in different manuals will overwrite each other.

    Please make these install-format targets invoke the commands for the format
    target, for example, by making format a dependency.

‘uninstall’

    Delete all the installed files—the copies that the ‘install’ and ‘install-*’
    targets create.

    This rule should not modify the directories where compilation is done, only
    the directories where files are installed.

    The uninstallation commands are divided into three categories, just like the
    installation commands. See Install Command Categories.  
    
‘install-strip’

    Like install, but strip the executable files while installing them. In
    simple cases, this target can use the install target in a simple way:

    install-strip:
            $(MAKE) INSTALL_PROGRAM='$(INSTALL_PROGRAM) -s' \
                    install

    But if the package installs scripts as well as real executables, the
    install-strip target can’t just refer to the install target; it has to strip
    the executables but not the scripts.

    install-strip should not strip the executables in the build directory which
    are being copied for installation. It should only strip the copies that are
    installed.

    Normally we do not recommend stripping an executable unless you are sure the
    program has no bugs. However, it can be reasonable to install a stripped
    executable for actual execution while saving the unstripped executable
    elsewhere in case there is a bug.

‘clean’

    Delete all files in the current directory that are normally created by
    building the program. Also delete files in other directories if they are
    created by this makefile. However, don’t delete the files that record the
    configuration. Also preserve files that could be made by building, but
    normally aren’t because the distribution comes with them. There is no need
    to delete parent directories that were created with ‘mkdir -p’, since they
    could have existed anyway.

    Delete .dvi files here if they are not part of the distribution.

‘distclean’

    Delete all files in the current directory (or created by this makefile) that
    are created by configuring or building the program. If you have unpacked the
    source and built the program without creating any other files, ‘make
    distclean’ should leave only the files that were in the distribution.
    However, there is no need to delete parent directories that were created
    with ‘mkdir -p’, since they could have existed anyway.

‘mostlyclean’

    Like ‘clean’, but may refrain from deleting a few files that people normally
    don’t want to recompile. For example, the ‘mostlyclean’ target for GCC does
    not delete libgcc.a, because recompiling it is rarely necessary and takes a
    lot of time.  

‘maintainer-clean’

    Delete almost everything that can be reconstructed with this Makefile. This
    typically includes everything deleted by distclean, plus more: C source
    files produced by Bison, tags tables, Info files, and so on.

    The reason we say “almost everything” is that running the command ‘make
    maintainer-clean’ should not delete configure even if configure can be
    remade using a rule in the Makefile. More generally, ‘make maintainer-clean’
    should not delete anything that needs to exist in order to run configure and
    then begin to build the program. Also, there is no need to delete parent
    directories that were created with ‘mkdir -p’, since they could have existed
    anyway. These are the only exceptions; maintainer-clean should delete
    everything else that can be rebuilt.

    The ‘maintainer-clean’ target is intended to be used by a maintainer of the
    package, not by ordinary users. You may need special tools to reconstruct
    some of the files that ‘make maintainer-clean’ deletes. Since these files
    are normally included in the distribution, we don’t take care to make them
    easy to reconstruct. If you find you need to unpack the full distribution
    again, don’t blame us.

    To help make users aware of this, the commands for the special
    maintainer-clean target should start with these two:

    @echo 'This command is intended for maintainers to use; it'
    @echo 'deletes files that may need special tools to rebuild.'

‘TAGS’

    Update a tags table for this program.

‘info’

    Generate any Info files needed. The best way to write the rules is as
    follows:

    info: foo.info

    foo.info: foo.texi chap1.texi chap2.texi
            $(MAKEINFO) $(srcdir)/foo.texi

    You must define the variable MAKEINFO in the Makefile. It should run the
    makeinfo program, which is part of the Texinfo distribution.

    Normally a GNU distribution comes with Info files, and that means the Info
    files are present in the source directory. Therefore, the Make rule for an
    info file should update it in the source directory. When users build the
    package, ordinarily Make will not update the Info files because they will
    already be up to date.

‘dvi’
‘html’
‘pdf’
‘ps’

    Generate documentation files in the given format. These targets should
    always exist, but any or all can be a no-op if the given output format
    cannot be generated. These targets should not be dependencies of the all
    target; the user must manually invoke them.

    Here’s an example rule for generating DVI files from Texinfo:

    dvi: foo.dvi

    foo.dvi: foo.texi chap1.texi chap2.texi
            $(TEXI2DVI) $(srcdir)/foo.texi

    You must define the variable TEXI2DVI in the Makefile. It should run the
    program texi2dvi, which is part of the Texinfo distribution. (texi2dvi uses
        TeX to do the real work of formatting. TeX is not distributed with
        Texinfo.) Alternatively, write only the dependencies, and allow GNU make
    to provide the command.

    Here’s another example, this one for generating HTML from Texinfo:

    html: foo.html

    foo.html: foo.texi chap1.texi chap2.texi
            $(TEXI2HTML) $(srcdir)/foo.texi

    Again, you would define the variable TEXI2HTML in the Makefile; for example,
  it might run makeinfo --no-split --html (makeinfo is part of the Texinfo
      distribution).

‘dist’

    Create a distribution tar file for this program. The tar file should be set
    up so that the file names in the tar file start with a subdirectory name
    which is the name of the package it is a distribution for. This name can
    include the version number.

    For example, the distribution tar file of GCC version 1.40 unpacks into a
    subdirectory named gcc-1.40.

    The easiest way to do this is to create a subdirectory appropriately named,
    use ln or cp to install the proper files in it, and then tar that
      subdirectory.

    Compress the tar file with gzip. For example, the actual distribution file
    for GCC version 1.40 is called gcc-1.40.tar.gz. It is ok to support other
      free compression formats as well.

    The dist target should explicitly depend on all non-source files that are in
    the distribution, to make sure they are up to date in the distribution. See
    Making Releases.

‘check’

    Perform self-tests (if any). The user must build the program before running
    the tests, but need not install the program; you should write the self-tests
    so that they work when the program is built but not installed. 

The following targets are suggested as conventional names, for programs in which
they are useful.

installcheck

    Perform installation tests (if any). The user must build and install the
    program before running the tests. You should not assume that $(bindir) is in
    the search path.  
    
installdirs

    It’s useful to add a target named ‘installdirs’ to create the directories
    where files are installed, and their parent directories. There is a script
    called mkinstalldirs which is convenient for this; you can find it in the
    Gnulib package. You can use a rule like this:

    # Make sure all installation directories (e.g. $(bindir))
    # actually exist by making them if necessary.
    installdirs: mkinstalldirs
            $(srcdir)/mkinstalldirs $(bindir) $(datadir) \
                                    $(libdir) $(infodir) \
                                    $(mandir)

    or, if you wish to support DESTDIR (strongly encouraged),

    # Make sure all installation directories (e.g. $(bindir))
    # actually exist by making them if necessary.
    installdirs: mkinstalldirs
            $(srcdir)/mkinstalldirs \
                $(DESTDIR)$(bindir) $(DESTDIR)$(datadir) \
                $(DESTDIR)$(libdir) $(DESTDIR)$(infodir) \
                $(DESTDIR)$(mandir)

    This rule should not modify the directories where compilation is done. It
    should do nothing but create installation directories. 


={============================================================================
*kt_linux_make_150* make-automake-directory variable

2.2.3 Standard Directory Variables

The GNU Coding Standards also specify a hierarchy of variables to denote
installation directories. Some of these are:

Directory variable	Default value
prefix	/usr/local
  exec_prefix	${prefix}
    bindir	${exec_prefix}/bin
    libdir	${exec_prefix}/lib
    ...
  includedir	${prefix}/include
  datarootdir	${prefix}/share
    datadir	${datarootdir}
    mandir	${datarootdir}/man
    infodir	${datarootdir}/info
    docdir	${datarootdir}/doc/${PACKAGE}


2.2.6 Parallel Build Trees (a.k.a. VPATH Builds)

The GNU Build System distinguishes `two trees`: the 'source' tree, and the
'build' tree.

The source tree is rooted in the directory containing configure. It contains all
the sources files (those that are distributed), and may be arranged using
several subdirectories.

The build tree is rooted in the directory in which 'configure' was 'run', and is
populated with all object files, programs, libraries, and other derived files
built from the sources (and hence not distributed). The build tree usually has
the same subdirectory layout as the source tree; its subdirectories are created
automatically by the build system.

If configure is executed in its own directory, the source and build trees are
combined: derived files are constructed in the same directories as their
sources. This was the case in our first installation example (see Basic
    Installation).

A common request from users is that they want to confine all derived files to a
single directory, to keep their source directories uncluttered. Here is how we
could run configure to build everything in a subdirectory called build/.

~ % tar zxf ~/amhello-1.0.tar.gz
~ % cd amhello-1.0
~/amhello-1.0 % mkdir build && cd build

~/amhello-1.0/build % ../configure    note: see run configure in different dir
...
~/amhello-1.0/build % make
...

These setups, where source and build trees are different, are often called
parallel builds or VPATH builds. The expression parallel build is misleading:
the word parallel is a reference to the way the build tree shadows the source
tree, it is not about some concurrency in the way build commands are run. For
this reason we refer to such setups using the name VPATH builds in the
following. VPATH is the name of the make feature used by the Makefiles to allow
these builds (see VPATH Search Path for All Prerequisites in The GNU Make
    Manual).

VPATH builds have other interesting uses. One is to build the same sources with
'multiple' configurations. For instance:

~ % tar zxf ~/amhello-1.0.tar.gz
~ % cd amhello-1.0
~/amhello-1.0 % mkdir debug optim && cd debug
~/amhello-1.0/debug % ../configure CFLAGS='-g -O0'
...
~/amhello-1.0/debug % make
...
~/amhello-1.0/debug % cd ../optim
~/amhello-1.0/optim % ../configure CFLAGS='-O3 -fomit-frame-pointer'
...
~/amhello-1.0/optim % make
...


2.2.8 Cross-Compilation

To cross-compile is to build on one platform a binary that will run on another
platform. When speaking of cross-compilation, it is important to distinguish
between the build platform on which the compilation is performed, and the host
platform on which the resulting executable is expected to run. The following
configure options are used to specify each of them:

--build=build

    The system on which the package is built. 

--host=host

    The system where built programs and libraries will run. 

When the --host is used, configure will search for the cross-compiling suite for
this platform. Cross-compilation tools commonly have their target architecture
as prefix of their name. For instance my cross-compiler for MinGW32 has its
binaries called i586-mingw32msvc-gcc, i586-mingw32msvc-ld, i586-mingw32msvc-as,
         etc.

Here is how we could build amhello-1.0 for i586-mingw32msvc on a GNU/Linux PC.

~/amhello-1.0 % ./configure --build i686-pc-linux-gnu --host i586-mingw32msvc
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking for i586-mingw32msvc-strip... i586-mingw32msvc-strip
checking for i586-mingw32msvc-gcc... i586-mingw32msvc-gcc
checking for C compiler default output file name... a.exe
checking whether the C compiler works... yes
checking whether we are cross compiling... yes
checking for suffix of executables... .exe
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether i586-mingw32msvc-gcc accepts -g... yes
checking for i586-mingw32msvc-gcc option to accept ANSI C...
...
~/amhello-1.0 % make
...
~/amhello-1.0 % cd src; file hello.exe
hello.exe: MS Windows PE 32-bit Intel 80386 console executable not relocatable


The --host and --build options are usually all we need for cross-compiling. The
only exception is if the package being built is itself a cross-compiler: we need
a third option to specify its target architecture.

--target=target

    When building compiler tools: the system for which the tools will create
    output. 

For instance when installing GCC, the GNU Compiler Collection, we can use
--target=target to specify that we want to build GCC as a cross-compiler for
target. Mixing --build and --target, we can actually cross-compile a
cross-compiler; such a three-way cross-compilation is known as a Canadian cross. 


2.2.10 Building Binary Packages Using DESTDIR

The GNU Build System's make install and make uninstall interface does not
exactly fit the needs of a system administrator who has to deploy and upgrade
packages on lots of hosts. In other words, the GNU Build System does not replace
a package manager.

Such package managers usually need to know which files have been installed by a
package, so a mere make install is inappropriate.

The DESTDIR variable can be used to perform a staged installation. The package
should be configured as if it was going to be installed in its final location
(e.g., --prefix /usr), but when running make install, the DESTDIR should be set
to the absolute name of a directory into which the installation will be
diverted. From this directory it is easy to review which files are being
installed where, and finally copy them to their final location by some means. 


2.2.12 Automatic Dependency Tracking

Dependency tracking is performed as a side-effect of compilation. Each time the
build system compiles a source file, it computes its list of dependencies (in C
    these are the header files included by the source being compiled). Later,
any time make is run and a dependency appears to have changed, the dependent
  files will be rebuilt.

Automake generates code for automatic dependency tracking by default, unless the
developer chooses to override it; for more information, see Dependencies.

When configure is executed, you can see it probing each compiler for the
dependency mechanism it supports (several mechanisms can be used):

~/amhello-1.0 % ./configure --prefix /usr
...
checking dependency style of gcc... gcc3
...

Because dependencies are only computed as a side-effect of the compilation, no
dependency information exists the first time a package is built. This is OK
because all the files need to be built anyway: make does not have to decide
which files need to be rebuilt. 


={============================================================================
*kt_linux_tool_150* make-automake-distcheck

2.2.11 Preparing Distributions

We have already mentioned make dist. This target collects all your source files
and the necessary parts of the build system to create a tarball named
package-version.tar.gz.

Another, more useful command is make distcheck. The distcheck target constructs
package-version.tar.gz just as well as dist, but it additionally ensures 'most'
of the use cases presented so far work:

-. It attempts a full compilation of the package (see Basic Installation),
unpacking the newly constructed tarball, running make, make check, make install,
as well as make installcheck, and even make dist,

-. it tests VPATH builds with read-only source tree (see VPATH Builds),

-. it makes sure make clean, make distclean, and make uninstall do not omit any
  file (see Standard Targets),

-. and it checks that DESTDIR installations work (see DESTDIR). 

All of these actions are performed in a 'temporary' directory, so that no root
privileges are required. 

Please note that the exact location and the exact structure of such a
subdirectory (where the extracted sources are placed, how the temporary build
    and install directories are named and how deeply they are nested, etc.) is
to be considered an implementation detail, which can change at any time; so do
not rely on it.

Releasing a package that fails make distcheck means that one of the scenarios we
presented will not work and some users will be disappointed. 

Therefore it is a good 'practice' to release a package only after a successful
make distcheck. This of course does not imply that the package will be flawless,
but at least it will prevent some of the embarrassing errors you may find in
  packages released by people who have never heard about distcheck (like DESTDIR
      not working because of a typo, or a distributed file being erased by make
      clean, or even VPATH builds not working).

See Creating amhello, to recreate amhello-1.0.tar.gz using make distcheck. See
Checking the Distribution, for more information about distcheck. 


14.4 Checking the Distribution

Automake also 'generates' a distcheck rule that can be of help to ensure that a
given distribution will actually work. Simplifying a bit, we can say this rule
first makes a distribution, and then, operating from it, takes the following
steps:

-. tries to do a VPATH build (see VPATH Builds), with the srcdir and all its
content made read-only;

-. runs the test suite (with make check) on this fresh build;

-. installs the package in a temporary directory (with make install), and tries
  runs the test suite on the resulting installation (with make installcheck);

-. checks that the package can be correctly uninstalled (by make uninstall) and
  cleaned (by make distclean);

-. finally, makes another tarball to ensure the distribution is self-contained. 


distcheck-hook

If the distcheck-hook rule is defined in your top-level Makefile.am, then it
will be invoked by distcheck after the new distribution has been unpacked, but
before the unpacked copy is configured and built. Your distcheck-hook can do
almost anything, though as always caution is advised. Generally this hook is
used to check for potential distribution errors not caught by the standard
mechanism. Note that distcheck-hook as well as AM_DISTCHECK_CONFIGURE_FLAGS and
DISTCHECK_CONFIGURE_FLAGS are not honored in a subpackage Makefile.am, but the
flags from AM_DISTCHECK_CONFIGURE_FLAGS and DISTCHECK_CONFIGURE_FLAGS are passed
down to the configure script of the subpackage. 


={============================================================================
*kt_linux_tool_150* make-automake-example

2.4 A Small Hello World

In this section we recreate the amhello-1.0 package from scratch. The first
subsection shows how to call the Autotools to instantiate the GNU Build System,
while the second explains the meaning of the configure.ac and Makefile.am files
  read by the Autotools. 


2.4.1 Creating amhello-1.0.tar.gz

Create the following files in an empty directory.

src/main.c is the source file for the hello program. We store it in the src/
subdirectory, because later, when the package evolves, it will ease the
addition of a man/ directory for man pages, a data/ directory for data files,
etc.

<source>
~/amhello % cat src/main.c
#include <config.h>             // note:
#include <stdio.h>

int
main (void)
{
  puts ("Hello World!");
  puts ("This is " PACKAGE_STRING ".");
  return 0;
}

<readme>
README contains some very limited documentation for our little package.

~/amhello % cat README
This is a demonstration package for GNU Automake.
Type 'info Automake' to read the Automake manual.

<am-file>
Makefile.am and src/Makefile.am contain Automake 'instructions' for these
two directories.

~/amhello % cat src/Makefile.am
bin_PROGRAMS = hello
hello_SOURCES = main.c

~/amhello % cat Makefile.am
SUBDIRS = src
dist_doc_DATA = README


<ac-file> *make-autoconf*
Finally, `configure.ac` contains Autoconf instructions to create the configure
script.

~/amhello % cat configure.ac
AC_INIT([amhello], [1.0], [bug-automake@gnu.org])
AM_INIT_AUTOMAKE([-Wall -Werror foreign])
AC_PROG_CC
AC_CONFIG_HEADERS([config.h])
AC_CONFIG_FILES([
 Makefile
 src/Makefile
])
AC_OUTPUT


Once you have these five files, it is time to run the Autotools to instantiate
the build system. Do this using the autoreconf command as follows:

~/amhello % autoreconf --install
configure.ac: installing './install-sh'
configure.ac: installing './missing'
configure.ac: installing './compile'
src/Makefile.am: installing './depcomp'

At this point `the build system is complete` 

In addition to the three scripts mentioned in its output, you can see that
autoreconf 'created' four other files: 

configure, config.h.in, Makefile.in, and src/Makefile.in. 

The `latter three files` are `templates` that will be adapted to the system by
configure under the names config.h, Makefile, and src/Makefile.


~/amhello % autoreconf --install


<makefile-in>
~/amhello % cat Makefile.am
SUBDIRS = src
dist_doc_DATA = README

becomes `Makefile.in`

DIST_SUBDIRS = $(SUBDIRS)

	@list='$(DIST_SUBDIRS)'; for subdir in $$list; do \
	  if test "$$subdir" = .; then :; else \
	    $(am__make_dryrun) \
	      || test -d "$(distdir)/$$subdir" \
	      || $(MKDIR_P) "$(distdir)/$$subdir" \
	      || exit 1; \
	    dir1=$$subdir; dir2="$(distdir)/$$subdir"; \
	    $(am__relativize); \
	    new_distdir=$$reldir; \
	    dir1=$$subdir; dir2="$(top_distdir)"; \
	    $(am__relativize); \
	    new_top_distdir=$$reldir; \
	    echo " (cd $$subdir && $(MAKE) $(AM_MAKEFLAGS) top_distdir="$$new_top_distdir" distdir="$$new_distdir" \\"; \
	    echo "     am__remove_distdir=: am__skip_length_check=: am__skip_mode_fix=: distdir)"; \
	    ($(am__cd) $$subdir && \
	      $(MAKE) $(AM_MAKEFLAGS) \
	        top_distdir="$$new_top_distdir" \
	        distdir="$$new_distdir" \
		am__remove_distdir=: \
		am__skip_length_check=: \
		am__skip_mode_fix=: \
	        distdir) \
	      || exit 1; \
	  fi; \
	done


~/amhello % cat src/Makefile.am
bin_PROGRAMS = hello
hello_SOURCES = main.c

becomes `src/Makefile.in`


<run-configure>
~/amhello % ./configure
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for gawk... no
checking for mawk... mawk
checking whether make sets $(MAKE)... yes
...

configure: creating ./config.status
`config.status: creating Makefile`
`config.status: creating src/Makefile`
`config.status: creating config.h`
config.status: executing depfiles commands


$ ls -alR
.:
total 364
drwxr-xr-x 4 kpark kpark   4096 Sep 24 13:57 .
drwxr-xr-x 8 kpark kpark   4096 Sep 23 17:14 ..
-rw-r--r-- 1 kpark kpark  34939 Sep 24 13:49 aclocal.m4
drwxr-xr-x 2 kpark kpark   4096 Sep 24 13:49 autom4te.cache
-rw-r--r-- 1 kpark kpark    774 Sep 24 13:57 config.h ~
-rw-r--r-- 1 kpark kpark    625 Sep 24 13:49 config.h.in
-rw-r--r-- 1 kpark kpark   8255 Sep 24 13:57 config.log
-rwxr-xr-x 1 kpark kpark  32599 Sep 24 13:57 config.status
-rwxr-xr-x 1 kpark kpark 139386 Sep 24 13:49 configure
-rw-r--r-- 1 kpark kpark    188 Sep 24 13:47 configure.ac
-rwxr-xr-x 1 kpark kpark  20899 Sep 24 13:49 depcomp
-rwxr-xr-x 1 kpark kpark  13998 Sep 24 13:49 install-sh
-rw-r--r-- 1 kpark kpark  17972 Sep 24 13:57 Makefile ~
-rw-r--r-- 1 kpark kpark     36 Sep 23 15:39 Makefile.am
-rw-r--r-- 1 kpark kpark  17822 Sep 24 13:49 Makefile.in
-rwxr-xr-x 1 kpark kpark  10346 Sep 24 13:49 missing
-rw-r--r-- 1 kpark kpark     56 Sep 23 15:36 README
drwxr-xr-x 3 kpark kpark   4096 Sep 24 13:57 src
-rw-r--r-- 1 kpark kpark     23 Sep 24 13:57 stamp-h1

./autom4te.cache:
total 352
drwxr-xr-x 2 kpark kpark   4096 Sep 24 13:49 .
drwxr-xr-x 4 kpark kpark   4096 Sep 24 13:57 ..
-rw-r--r-- 1 kpark kpark 139847 Sep 24 13:49 output.0
-rw-r--r-- 1 kpark kpark 139847 Sep 24 13:49 output.1
-rw-r--r-- 1 kpark kpark   6687 Sep 24 13:49 requests
-rw-r--r-- 1 kpark kpark  34070 Sep 24 13:49 traces.0
-rw-r--r-- 1 kpark kpark  20296 Sep 24 13:49 traces.1

./src:
total 52
drwxr-xr-x 3 kpark kpark  4096 Sep 24 13:57 .
drwxr-xr-x 4 kpark kpark  4096 Sep 24 13:57 ..
drwxr-xr-x 2 kpark kpark  4096 Sep 24 13:57 .deps
-rw-r--r-- 1 kpark kpark   133 Sep 23 15:35 main.c
-rw-r--r-- 1 kpark kpark 14541 Sep 24 13:57 Makefile
-rw-r--r-- 1 kpark kpark    45 Sep 23 15:39 Makefile.am
-rw-r--r-- 1 kpark kpark 14732 Sep 24 13:49 Makefile.in

./src/.deps:
total 12
drwxr-xr-x 2 kpark kpark 4096 Sep 24 13:57 .
drwxr-xr-x 3 kpark kpark 4096 Sep 24 13:57 ..
-rw-r--r-- 1 kpark kpark    8 Sep 24 13:57 main.Po

You can see Makefile, src/Makefile, and config.h being created at the end after
configure has probed the system. It is now possible to run all the targets we
wish 

<run-make>
~/amhello % make
…
~/amhello % src/hello
Hello World!
This is amhello 1.0.
~/amhello % make distcheck
...
=============================================
amhello-1.0 archives ready for distribution:
amhello-1.0.tar.gz
=============================================


<files-created>
Note that running `autoreconf` is only needed initially when the GNU Build
System does not exist. When you later change some instructions in a
Makefile.am or configure.ac, the relevant part of the build system will be
`regenerated automatically when you execute make`

However, because Autoconf and Automake have separate manuals, the important
point to understand is that autoconf is in charge of 'creating' configure
'from' configure.ac, while automake is in charge of 'creating' Makefile.ins
'from' Makefile.am and configure.ac. This should at least direct you to the
right manual when seeking answers. 


={============================================================================
*kt_linux_tool_150* make-automake-example-explained

2.4.2 amhello’s configure.ac Setup Explained

Let us begin with the contents of `configure.ac`.

     AC_INIT([amhello], [1.0], [bug-automake@gnu.org])
     AM_INIT_AUTOMAKE([-Wall -Werror foreign])
     AC_PROG_CC
     AC_CONFIG_HEADERS([config.h])
     AC_CONFIG_FILES([
      Makefile
      src/Makefile
     ])
     AC_OUTPUT

This file is read `by both ‘autoconf’ (to create ‘configure’) and automake`
(to create the various ‘Makefile.in’s). It contains a series of M4 macros that
will be expanded as `shell code` to finally form the ‘configure’ script.  


<autoconf-automake-macros>
The macros prefixed with ‘AC_’ are `autoconf macros`, documented in the Autoconf
manual. The macros that start with ‘AM_’ are `automake macros`


The first two lines of ‘configure.ac’ initialize Autoconf and Automake.
‘AC_INIT’ takes in as parameters the name of the package, its version number,
and a contact address for bug-reports about the package.

The argument to AM_INIT_AUTOMAKE is a list of options for automake (see
    Chapter 17 [Options], page 117). -Wall and -Werror ask automake to turn on
all warnings and report them as errors. `We are speaking of Automake warnings`
here, such as dubious instructions in Makefile.am. This has absolutely nothing
to do with how the compiler will be called, even though it may support options
with similar names. Using -Wall -Werror is a safe setting when starting to
work on a package: you do not want to miss any issues. Later you may decide to
relax things a bit. 

The `foreign option` tells Automake that this package will not follow the GNU
Standards. GNU packages should always distribute additional files such as
ChangeLog, AUTHORS, etc. We do not want automake to complain about these
missing files in our small example.
  
The `AC_PROG_CC` line causes the ‘configure’ script to search for a C compiler
and define the variable ‘CC’ with its name. 

The ‘src/Makefile.in’ file generated by Automake uses the variable ‘CC’ to
build ‘hello’, so when ‘configure’ creates ‘src/Makefile’ from
‘src/Makefile.in’, it will define ‘CC’ with the value it has found. If
Automake is asked to create a ‘Makefile.in’ that uses ‘CC’ but ‘configure.ac’
does not define it, it will suggest you add a call to ‘AC_PROG_CC’.


The ‘AC_CONFIG_HEADERS([config.h])’ invocation causes the ‘configure’ script
to create a ‘config.h’ file gathering ‘#define’s defined by other macros in
‘configure.ac’. In our case, the ‘AC_INIT’ macro already defined a few of
them.  Here is an excerpt of ‘config.h’ after ‘configure’ has run:

     ...
     /* Define to the address where bug reports for this package should be sent. */
     #define PACKAGE_BUGREPORT "bug-automake@gnu.org"

     /* Define to the full name and version of this package. */
     #define PACKAGE_STRING "amhello 1.0"
     ...

As you probably noticed, ‘src/main.c’ includes ‘config.h’ so it can use
‘PACKAGE_STRING’. In a real-world project, ‘config.h’ can grow really big,
with one ‘#define’ per feature probed on the system.


The `AC_CONFIG_FILES` macro declares the list of files that `configure` should
create `from their *.in templates` Automake also scans this list to find the
‘Makefile.am’ files it must process. (This is important to remember: when
    adding a new directory to your project, you should add its ‘Makefile’ to
    this list, otherwise Automake will never process the new ‘Makefile.am’ you
    wrote in that directory.)

Finally, the ‘AC_OUTPUT’ line is a closing command that actually produces the
part of the script in charge of creating the files registered with
‘AC_CONFIG_HEADERS’ and ‘AC_CONFIG_FILES’.


2.4.3 ‘amhello’’s ‘Makefile.am’ Setup Explained
-----------------------------------------------

We now turn to ‘src/Makefile.am’. This file contains Automake instructions to
build and install ‘hello’.

     bin_PROGRAMS = hello
     hello_SOURCES = main.c

<create-makefilein>
A ‘Makefile.am’ has the `same syntax` as an ordinary ‘Makefile’. When automake
processes a ‘Makefile.am’ it copies the entire file into the output
‘Makefile.in’ (that will be later `turned into 'Makefile' by configure`) but
will react to certain variable definitions by generating some build rules and
other variables. Often ‘Makefile.am’s contain only a list of variable
definitions as above, but they can also contain other variable and rule
definitions that automake will pass along without interpretation.


//     bin_PROGRAMS = hello

Variables that 'end' with `_PROGRAMS` are special variables that list 'programs'
that the resulting `Makefile should build` In Automake speak, this ‘_PROGRAMS’
suffix is called a `primary`; Automake recognizes other primaries such as
‘_SCRIPTS’, ‘_DATA’, ‘_LIBRARIES’, etc. corresponding to different 'types' of
files.


The ‘bin’ part of the ‘bin_PROGRAMS’ tells ‘automake’ that the resulting
programs `should be installed` in BINDIR. Recall that the GNU Build System
uses a set of variables to denote destination directories and allow users to
customize these locations (*note Standard Directory Variables::). Any such
directory variable can be put in front of a primary (omitting the ‘dir’
    suffix) to tell ‘automake’ where to install the listed files.


//     hello_SOURCES = main.c

Programs need to be built from source files, so for each program ‘PROG’ listed
in a ‘_PROGRAMS’ variable, ‘automake’ will look for another variable named
`PROG_SOURCES` listing its source files. There may be more than one source
file: they will all be compiled and linked together.

Automake also knows that source files need to be distributed when creating a
tarball (unlike built programs).  So a side-effect of this ‘hello_SOURCES’
declaration is that ‘main.c’ will be part of the tarball created by ‘make
dist’.


Finally here are some explanations regarding the top-level ‘Makefile.am’.

     SUBDIRS = src
     dist_doc_DATA = README

`SUBDIRS` is a special variable listing all directories that ‘make’ should
`recurse into before` processing the current directory.  So this line is
responsible for ‘make’ building ‘src/hello’ even though we run it from the
top-level.  This line also causes ‘make install’ to install ‘src/hello’ before
installing ‘README’ (not that this order matters).

The line ‘dist_doc_DATA = README’ causes ‘README’ to be distributed and
installed in DOCDIR.  Files listed with the ‘_DATA’ primary are not
automatically part of the tarball built with ‘make dist’, so we add the ‘dist_’
prefix so they get distributed.  However, for ‘README’ it would not have been
necessary: ‘automake’ automatically distributes any ‘README’ file it encounters
(the list of other files automatically distributed is presented by ‘automake
 --help’).  The only important effect of this second line is therefore to
install ‘README’ during ‘make install’.

One thing not covered in this example is accessing the installation directory
values (*note Standard Directory Variables::) from your program code, that is,
converting them into defined macros.  For this, *note (autoconf)Defining
  Directories::.


={============================================================================
*kt_linux_tool_150* make-automake-variables

3.1 General Operation

<create-makefilein>
Automake works by reading a ‘Makefile.am’ and generating a ‘Makefile.in’.

Certain variables and rules defined in the ‘Makefile.am’ instruct Automake to
generate more specialized code; for instance, a ‘bin_PROGRAMS’ variable
definition will cause 'rules' for 'compiling' and 'linking' programs to be
generated.

Note that most GNU make extensions are not recognized by Automake.  Using such
extensions in a ‘Makefile.am’ will lead to errors or confusing behavior.

A special exception is that the GNU make append operator, ‘+=’, is supported.
This operator appends its right hand argument to the variable specified on the
left.  Automake will translate the operator into an ordinary ‘=’ operator; ‘+=’
will thus work with any make program.


Generally, Automake is not particularly smart in the parsing of unusual Makefile
constructs, so you’re advised to avoid fancy constructs or “creative” use of
whitespace.  For example, <TAB> characters cannot be used between a target name
and the following “‘:’” character, and variable assignments shouldn’t be
indented with <TAB> characters.  Also, using more complex macro in target names
can cause trouble:

     % cat Makefile.am
     $(FOO:=x): bar
     % automake
     Makefile.am:1: bad characters in variable name '$(FOO'
     Makefile.am:1: ':='-style assignments are not portable


A rule defined in ‘Makefile.am’ generally 'overrides' any such rule of a similar
name that would be automatically generated by ‘automake’.  Although this is a
supported feature, it is generally best to avoid making use of it, as sometimes
the generated rules are very particular.


Similarly, a variable defined in ‘Makefile.am’ or ‘AC_SUBST’ed from
‘configure.ac’ will 'override' any definition of the variable that ‘automake’
would ordinarily create.  This feature is more often 'useful' than the ability
to override a rule.  Be warned that many of the variables generated by
‘automake’ are considered to be for internal use only, and their names might
change in future releases.

<variable-expansion>
When examining a variable definition, Automake will recursively examine
variables referenced in the definition.  For example, if Automake is looking at
the content of ‘foo_SOURCES’ in this snippet

     xs = a.c b.c
     foo_SOURCES = c.c $(xs)

it would use the files ‘a.c’, ‘b.c’, and ‘c.c’ as the contents of ‘foo_SOURCES’.


<comments>
Automake also allows a form of comment that is _not_ copied into the output; all
lines beginning with ‘##’ (leading spaces allowed) are completely ignored by
Automake.


3.3 The Uniform Naming Scheme

Automake variables generally follow a "uniform naming scheme" that makes it easy
to decide how programs (and other derived objects) are built, and how they are
installed.  
  
At ‘make’ time, certain variables are used to determine which objects are to be
built.  The variable names are made of several pieces that are concatenated
together.

<primary>
The piece that tells ‘automake’ what is being built is commonly called the
"primary".  For instance, the primary ‘PROGRAMS’ holds a list of programs that
are to be compiled and linked.


<where-prefix>
A different set of names is used to decide where the built objects should be
'installed'.  These names are 'prefixes' to the primary, and they indicate which
standard directory should be used as the installation directory.  The standard
directory names are given in the GNU standards (*note (standards)Directory
    Variables::).  

Automake extends this list with ‘pkgdatadir’, ‘pkgincludedir’, ‘pkglibdir’, and
‘pkglibexecdir’; these are the same as the non-‘pkg’ versions, but with
‘$(PACKAGE)’ appended.  For instance, ‘pkglibdir’ is defined as
‘$(libdir)/$(PACKAGE)’.

For each primary, there is one additional variable named by prepending ‘EXTRA_’
to the primary name.  This variable is used to list objects that may or may not
be built, depending on what ‘configure’ decides.  This variable is required
because Automake must statically know the entire list of objects that may be
built in order to generate a ‘Makefile.in’ that will work in all cases.

For instance, ‘cpio’ decides at configure time which programs should be built.
Some of the programs are installed in ‘bindir’, and some are installed in
‘sbindir’:

     EXTRA_PROGRAMS = mt rmt
     bin_PROGRAMS = cpio pax
     sbin_PROGRAMS = $(MORE_PROGRAMS)

Defining a primary without a prefix as a variable, e.g., ‘PROGRAMS’, is an
error.

<bindir>
Note that the common ‘dir’ suffix is 'left' off when constructing the variable
names; thus one writes ‘bin_PROGRAMS’ and not ‘bindir_PROGRAMS’.

Not every sort of object can be installed in every directory.
Automake will flag those attempts it finds in error (but see below how
to override the check if you really need to).  Automake will also
diagnose obvious misspellings in directory names.

<non-standard-directory>
Sometimes the standard directories are not enough.  In particular it is
sometimes useful, for clarity, to install objects in a subdirectory of some
predefined directory.  To this end, Automake allows you to extend the list of
possible installation directories.  A given prefix (e.g., ‘zar’) is valid if a
variable of the same name with ‘dir’ appended is defined (e.g., ‘zardir’).

For instance, the following snippet will install ‘file.xml’ into
‘$(datadir)/xml’.

     xmldir = $(datadir)/xml
     xml_DATA = file.xml

This feature can also be used to override the sanity checks Automake performs to
diagnose suspicious directory/primary couples (in the unlikely case these checks
    are undesirable, and you really know what you’re doing).  For example,
               Automake would error out on this input:

     # Forbidden directory combinations, automake will error out on this.
     pkglib_PROGRAMS = foo
     doc_LIBRARIES = libquux.a

but it will succeed with this:

     # Work around forbidden directory combinations.  Do not use this
     # without a very good reason!
     my_execbindir = $(pkglibdir)
     my_doclibdir = $(docdir)
     my_execbin_PROGRAMS = foo
     my_doclib_LIBRARIES = libquux.a

The ‘exec’ substring of the ‘my_execbindir’ variable lets the files be installed
at the right time (*note The Two Parts of Install::).

<noinst-prefix>
The special prefix ‘noinst_’ indicates that the objects in question should be
built but not installed at all.  This is usually used for objects required to
build the rest of your package, for instance static libraries (*note A
    Library::), or helper scripts.

The special prefix ‘check_’ indicates that the objects in question should not be
built 'until' the ‘make check’ command is run.  Those objects are not installed
either.

<supported-primary>
The current primary names are:

‘PROGRAMS’, ‘LIBRARIES’, ‘LTLIBRARIES’, ‘LISP’, ‘PYTHON’, ‘JAVA’, ‘SCRIPTS’,
                ‘DATA’, ‘HEADERS’, ‘MANS’, and ‘TEXINFOS’.

Some primaries also allow additional prefixes that control other aspects of
‘automake’’s behavior.  The currently defined prefixes are ‘dist_’, ‘nodist_’,
                ‘nobase_’, and ‘notrans_’. 

These prefixes are explained later (*note Program and Library Variables::)
(*note Man Pages::).


3.5 How derived variables are named

// bin_PROGRAMS = hello
// hello_SOURCES = main.c

Sometimes a Makefile variable name is derived from some text the maintainer
supplies.  For instance, a program name listed in ‘_PROGRAMS’ is rewritten into
the name of a ‘_SOURCES’ variable.  In cases like this, Automake canonicalizes
the text, so that program names and the like do not have to follow Makefile
variable naming rules.  

<to-underscores>
All characters in the name except for letters, numbers, the strudel (@), and the
underscore are 'turned' into 'underscores' when making variable references.

For example, if your program is named ‘sniff-glue’, the derived variable name
would be ‘sniff_glue_SOURCES’, not ‘sniff-glue_SOURCES’.  Similarly the sources
for a library named ‘libmumble++.a’ should be listed in the
  ‘libmumble___a_SOURCES’ variable.

The strudel is an addition, to make the use of Autoconf substitutions in
variable names less obfuscating.


3.6 Variables reserved for the user *user-variable*

Some ‘Makefile’ variables are 'reserved' by the GNU Coding Standards for the use
of the 'user' - the person 'building' the package. For instance, ‘CFLAGS’ is one
such variable.

Sometimes package developers are tempted to set user variables such as ‘CFLAGS’
because it appears to make their job easier.  However, the package itself should
never set a user variable, particularly not to include switches that are
required for proper compilation of the package.  Since these variables are
documented as being for the package builder, that person rightfully expects to
be able to override any of these variables at build time.

<shadow-variable>
To get around this problem, Automake introduces an automake-specific shadow
variable for 'each' user flag variable. Shadow variables are not introduced for
variables like ‘CC’, where they would make no sense. 

The shadow variable is named by prepending ‘AM_’ to the user variable’s name.
For instance, the shadow variable for ‘YFLAGS’ is ‘AM_YFLAGS’.  The package
maintainer—that is, the author(s) of the ‘Makefile.am’ and ‘configure.ac’
files—may adjust these shadow variables however necessary.


={============================================================================
*kt_linux_tool_150* make-automake-ex-packages

4 Some example packages

The second example shows how two programs can be built from the same file, using
different compilation parameters.  It contains some technical digressions that
are probably best skipped on first read.

Here is another, trickier example.  It shows how to generate two programs
(‘true’ and ‘false’) from the same source file (‘true.c’).  The difficult part
is that each compilation of ‘true.c’ requires different ‘cpp’ flags.

     bin_PROGRAMS = true false
     false_SOURCES =
     false_LDADD = false.o

     true.o: true.c
             $(COMPILE) -DEXIT_CODE=0 -c true.c

     false.o: true.c
             $(COMPILE) -DEXIT_CODE=1 -o false.o -c true.c

However if you were to build ‘true’ and ‘false’ in real life, you would probably
use per-program compilation flags, like so:

     bin_PROGRAMS = false true

     false_SOURCES = true.c
     false_CPPFLAGS = -DEXIT_CODE=1

     true_SOURCES = true.c
     true_CPPFLAGS = -DEXIT_CODE=0

In this case Automake will cause ‘true.c’ to be compiled twice, with different
flags.  In this instance, the names of the object files would be chosen by
automake; they would be ‘false-true.o’ and ‘true-true.o’. The name of the object
files rarely matters.


={============================================================================
*kt_linux_tool_150* make-automake-marco

6 Scanning configure.ac, using aclocal

`Automake scans the package’s configure.ac` to determine certain information
about the package. `Some autoconf macros are required` and `some variables` must
be defined in configure.ac. Automake will also use information from
configure.ac to further tailor its output.

Automake also `supplies some Autoconf macros` to make the maintenance easier.
These macros can automatically be put into your aclocal.m4 using the aclocal
program.


6.1 Configuration requirements

Here are the other macros that Automake requires but which are not run by
AM_INIT_AUTOMAKE:

AC_CONFIG_FILES
AC_OUTPUT

AC_CONFIG_FILES([
Makefile
doc/Makefile
src/Makefile
src/lib/Makefile
...
])
AC_OUTPUT

Automake uses these `to determine which files to create` (see Section
    “Creating Output Files” in The Autoconf Manual).
‘AC_CONFIG_FILES([foo/Makefile])’ will cause Automake to generate
foo/Makefile.in `if foo/Makefile.am exists`


6.2 Other things Automake recognizes

Every time Automake is run it `calls Autoconf to trace configure.ac` This way
it can recognize the use of certain macros and tailor the generated
Makefile.in appropriately.

Currently recognized macros and their effects are:

AC_CONFIG_HEADERS

Automake will generate rules to rebuild these headers from the corresponding
templates (usually, the template for a foo.h header being foo.h.in).


AM_CONDITIONAL

This introduces an Automake conditional (see Chapter 20 [Conditionals],
page 124).


6.4 Autoconf macros supplied with Automake

Automake ships with several Autoconf macros that you can use from your
configure.ac.  When you use one of them it will be included by aclocal in
aclocal.m4.

6.4.1 Public Macros

AM_INIT_AUTOMAKE([OPTIONS])

Runs many macros required for proper operation of the generated Makefiles.
Today, AM_INIT_AUTOMAKE is called with a single argument: a space-separated
list of Automake options that should be applied to every Makefile.am in the
tree. The effect is as if each option were listed in AUTOMAKE_OPTIONS (see
Chapter 17 [Options], page 117).


17.1 Options generalities

Various features of Automake can be controlled by options. Except where noted
otherwise, options can be specified in one of several ways. Most options can
be applied on a per-Makefile basis when listed in a special Makefile variable
named AUTOMAKE_OPTIONS. Some of these options only make sense when specified
in the toplevel Makefile.am file. 

Options are applied globally to all processed Makefile files when listed in
the first argument of AM_INIT_AUTOMAKE in configure.ac, and some options
which require changes to the configure script can only be specified there.
These are annotated below.

As a general rule, options specified in AUTOMAKE_OPTIONS take precedence over
those specified in AM_INIT_AUTOMAKE, which in turn take precedence over those
specified on the command line.


17.2 List of Automake options

foreign

Set the strictness as appropriate.

no-dist 

Don’t emit any code related to dist target. This is useful when a package has
its own method for making distributions.

-Wcategory or --warnings=category

These options behave exactly like their command-line counterpart (see Chapter
    5 [automake Invocation], page 26). This allows you to enable or disable
some warning categories on a per-file basis. You can also setup some warnings
for your entire project; for instance, try ‘AM_INIT_AUTOMAKE([-Wall])’ in your
  configure.ac.


note:
From For version 1.4, 10 January 1999 but not from the latest.

AM_ENABLE_MULTILIB
    This is used when a "multilib" library is being built. A multilib library
    is one that is built multiple times, once per target flag combination.
    This is only useful when the library is intended to be cross-compiled. 
    
    The first optional argument is the name of the `Makefile' being generated;
it defaults to `Makefile'. The second option argument is used to find the top
  source directory; it defaults to the empty string (generally this should not
      be used unless you are familiar with the internals). 


={============================================================================
*kt_linux_tool_150* make-automake-building programs and libraries

8 Building Programs and Libraries

A large part of Automake’s functionality is dedicated to making it easy to build
programs and libraries.

In order to build a program, you need to tell Automake which sources are
part of it, and which libraries it should be linked with.

This section also covers conditional compilation of sources or programs. Most
of the comments about these `also apply to libraries and libtool libraries`


8.1.1 Defining program sources

In a directory containing source that gets built into a program (`as opposed
    to` a library or a script), the `PROGRAMS primary` is used.  Programs can
be installed in ‘bindir’, ‘sbindir’, ‘libexecdir’, ‘pkglibexecdir’, or not at
all (‘noinst_’).  They can also be built only for ‘make check’, in which case
the prefix is ‘check_’.

     bin_PROGRAMS = hello

In this simple case, the resulting ‘Makefile.in’ will contain code to generate
a program named ‘hello’.

The variable ‘hello_SOURCES’ is used to specify which source files get built
into an executable:

     hello_SOURCES = hello.c version.c getopt.c getopt1.c getopt.h system.h

This causes each mentioned ‘.c’ file to be compiled into the corresponding
‘.o’.  Then all are linked to produce ‘hello’.

Header files listed in a ‘_SOURCES’ `will be included in the distribution` but
otherwise ignored.  In case it isn’t obvious, you should not include the
header file generated by ‘configure’ in a ‘_SOURCES’ variable; this file
should not be distributed.


8.1.2 Linking the program

If you need to link against libraries `that are 'not' found by ‘configure’`,
you can use `LDADD` to do so. This variable is used to specify 'additional'
  objects or libraries to link with; it is inappropriate for specifying
  specific linker 'flags', you should use `AM_LDFLAGS` for this purpose.


8.2 Building a library

Building a library is much like building a program.  In this case, the name of
the primary is `LIBRARIES`.  Libraries can be installed in ‘libdir’ or
‘pkglibdir’.

*Note A Shared Library::, for information on how to build shared libraries
using libtool and the ‘LTLIBRARIES’ primary.


For instance, to create a library named ‘libcpio.a’, but not install it, you
would write:

     noinst_LIBRARIES = libcpio.a
     libcpio_a_SOURCES = ...
     
The sources that go into a library are determined exactly as they are for
programs, via the ‘_SOURCES’ variables.  Note that the library name is
canonicalized, so the ‘_SOURCES’ variable corresponding to ‘libcpio.a’ is
`libcpio_a_SOURCES`, not ‘libcpio.a_SOURCES’.

Extra objects can be added to a library using the ‘LIBRARY_LIBADD’ variable.
This should be used for objects determined by ‘configure’.  Again from ‘cpio’:

     libcpio_a_LIBADD = $(LIBOBJS) $(ALLOCA)


Building a static library is done by compiling all object files, then by
invoking ‘$(AR) $(ARFLAGS)’ followed by the name of the library and the list
of objects, and finally by calling ‘$(RANLIB)’ on that library.  You should
call ‘AC_PROG_RANLIB’ from your ‘configure.ac’ to define ‘RANLIB’ (Automake
    will complain otherwise).

You `can override` the ‘AR’ variable by defining a per-library ‘maude_AR’
variable (*note Program and Library Variables::).

<use-static-library>
To use a static library when building a program, add it to ‘LDADD’ for this
program.  In the following example, the program ‘cpio’ is statically linked
with the library ‘libcpio.a’.

     noinst_LIBRARIES = libcpio.a
     libcpio_a_SOURCES = ...

     bin_PROGRAMS = cpio
     cpio_SOURCES = cpio.c ...
     cpio_LDADD = libcpio.a     // note: see


8.3 Building a Shared Library

<libtool>
Building shared libraries portably is a relatively complex matter.  For this
reason, GNU Libtool was created to help build shared libraries in a
platform-independent way.

8.3.1 The Libtool Concept
-------------------------

<la-lo-suffix>

Libtool abstracts shared and static libraries into a unified concept henceforth
called "libtool libraries".  Libtool libraries are files using the ‘.la’
'suffix', and can designate a static library, a shared library, or maybe both.
Their exact nature cannot be determined until ‘./configure’ is run: not all
platforms support all kinds of libraries, and users can explicitly select which
libraries should be built.  (However the package’s maintainers can tune the
    default, *note The ‘AC_PROG_LIBTOOL’ macro: (libtool)AC_PROG_LIBTOOL.)

Because object files for shared and static libraries must be compiled
'differently', libtool is also used during compilation.  Object files built by
libtool are called "libtool objects": these are files using the ‘.lo’ suffix.
Libtool libraries are built from these libtool objects.

You should not assume anything about the structure of ‘.la’ or ‘.lo’ files and
how libtool constructs them: this is libtool’s concern, and the last thing one
wants is to learn about libtool’s guts.  However the existence of these files
matters, because they are used as targets and dependencies in ‘Makefile’s rules
when building libtool libraries.  There are situations where you may have to
refer to these, for instance when expressing dependencies for building source
files conditionally


8.3.2 Building Libtool Libraries
--------------------------------

Automake uses libtool to build libraries declared with the ‘LTLIBRARIES’
primary.  Each ‘_LTLIBRARIES’ variable is a list of libtool libraries to build.
For instance, to create a libtool library named ‘libgettext.la’, and install it
in ‘libdir’, write:

     lib_LTLIBRARIES = libgettext.la
     libgettext_la_SOURCES = gettext.c gettext.h ...
     include_HEADERS = gettext.h ...

Automake predefines the variable ‘pkglibdir’, so you can use
‘pkglib_LTLIBRARIES’ to install libraries in ‘$(libdir)/@PACKAGE@/’.

If ‘gettext.h’ is a public header file that needs to be installed in order for
people to use the library, it should be declared using a ‘_HEADERS’ variable,
       not in ‘libgettext_la_SOURCES’.  Headers listed in the latter should be
         internal headers that are not part of the public interface.


The following example builds a program named ‘hello’ that is linked with
‘libgettext.la’.

     lib_LTLIBRARIES = libgettext.la
     libgettext_la_SOURCES = gettext.c …

     bin_PROGRAMS = hello
     hello_SOURCES = hello.c …
     hello_LDADD = libgettext.la

Whether ‘hello’ is statically or dynamically linked with ‘libgettext.la’ is not
yet known: this will depend on the configuration of libtool and the capabilities
of the host.


8.3.5 Libtool Convenience Libraries
-----------------------------------

Sometimes you want to build libtool libraries that should 'not' be installed.
These are called "libtool convenience libraries" and are typically used to
encapsulate many sublibraries, later gathered into one 'big' installed library.

Unlike installed libtool libraries they do not need an ‘-rpath’ flag at link
time (actually this is the only difference).

Convenience libraries listed in ‘noinst_LTLIBRARIES’ are always built.  Those
listed in ‘check_LTLIBRARIES’ are built only upon ‘make check’.  Finally,
libraries listed in ‘EXTRA_LTLIBRARIES’ are never built explicitly: Automake
  outputs rules to build them, but if the library does not appear as a Makefile
  dependency anywhere it won’t be built (this is why ‘EXTRA_LTLIBRARIES’ is used
      for conditional compilation).

Here is a sample setup merging libtool convenience libraries from subdirectories
into one main ‘libtop.la’ library.

     # -- Top-level Makefile.am --
     SUBDIRS = sub1 sub2 ...
     lib_LTLIBRARIES = libtop.la
     libtop_la_SOURCES =
     libtop_la_LIBADD = \
       sub1/libsub1.la \
       sub2/libsub2.la \
       ...

     # -- sub1/Makefile.am --
     noinst_LTLIBRARIES = libsub1.la
     libsub1_la_SOURCES = ...

     # -- sub2/Makefile.am --
     # showing nested convenience libraries
     SUBDIRS = sub2.1 sub2.2 ...
     noinst_LTLIBRARIES = libsub2.la
     libsub2_la_SOURCES =
     libsub2_la_LIBADD = \
       sub21/libsub21.la \
       sub22/libsub22.la \
       ...

When using such setup, beware that ‘automake’ will assume ‘libtop.la’ is to be
linked with the C linker. This is because ‘libtop_la_SOURCES’ is empty, so
‘automake’ picks C as 'default' language. If ‘libtop_la_SOURCES’ was not empty,
‘automake’ would select the linker as explained in *note How the Linker is
  Chosen::.

If one of the sublibraries contains non-C source, it is important that the
appropriate linker be chosen. One way to achieve this is to pretend that there
is such a non-C file among the sources of the library, thus 'forcing' ‘automake’
to select the appropriate linker.  Here is the top-level ‘Makefile’ of our
example updated to force C++ linking.

     SUBDIRS = sub1 sub2 ...
     lib_LTLIBRARIES = libtop.la
     libtop_la_SOURCES =
     # Dummy C++ source to cause C++ linking.
     nodist_EXTRA_libtop_la_SOURCES = dummy.cxx
     libtop_la_LIBADD = \
       sub1/libsub1.la \
       sub2/libsub2.la \
       ...

‘EXTRA_*_SOURCES’ variables are used to keep track of source files that might be
compiled (this is mostly useful when doing conditional compilation using
    ‘AC_SUBST’, *note Conditional Libtool Sources::), and the ‘nodist_’ prefix
means the listed sources are not to be distributed (*note Program and Library
    Variables::).  

In effect the file ‘dummy.cxx’ does not need to exist in the source tree.  Of
course if you have some real source file to list in ‘libtop_la_SOURCES’ there is
no point in cheating with ‘nodist_EXTRA_libtop_la_SOURCES’.


<am-dummy-file>
// This file is necessary because automake assumes that a shared library with
// no source files (i.e. a shared library built from an archive library) is a C
// library, so it incorrectly links it with the C compiler, omitting libstdc++
// and causing undefined symbols
//
// TL;DR: This file is necessary because automake is rubbish.

libVanadiumWebKitVideoElement_la_SOURCES = src/Dummy.cpp

libVanadiumWebKitVideoElement_la_LIBADD = \
  libVanadiumWebKitVideoElementStatic.la \
  $(LIBADDS)


8.3.7 ‘_LIBADD’, ‘_LDFLAGS’, and ‘_LIBTOOLFLAGS’
------------------------------------------------

note: LIBRARY_LIBADD means xxx_LIBADD

As shown in previous sections, the ‘LIBRARY_LIBADD’ variable should be used to
list extra libtool objects (‘.lo’ files) or libtool libraries (‘.la’) to add to
LIBRARY.

The ‘LIBRARY_LDFLAGS’ variable is the place to list 'additional' libtool linking
'flags', such as ‘-version-info’, ‘-static’, and a lot more.  *Note Link mode:
(libtool)Link mode.

The ‘libtool’ command has 'two' kinds of options: mode-specific options and
generic options.  Mode-specific options such as the aforementioned linking flags
should be lumped with the other flags passed to the tool invoked by ‘libtool’
(hence the use of ‘LIBRARY_LDFLAGS’ for libtool linking flags).  


Generic options include ‘--tag=TAG’ and ‘--silent’ (*note Invoking ‘libtool’:
    (libtool)Invoking libtool. for more options) should appear before the mode
selection on the command line; in ‘Makefile.am’s they should be listed in the
‘LIBRARY_LIBTOOLFLAGS’ variable.


If ‘LIBRARY_LIBTOOLFLAGS’ is not defined, then the variable ‘AM_LIBTOOLFLAGS’ is
used instead.

These flags are passed to libtool after the ‘--tag=TAG’ option computed by
Automake (if any), so ‘LIBRARY_LIBTOOLFLAGS’ (or ‘AM_LIBTOOLFLAGS’) is a good
place to override or supplement the ‘--tag=TAG’ setting.

The libtool rules also use a ‘LIBTOOLFLAGS’ variable that should not be set in
‘Makefile.am’: this is a user variable (*note Flag Variables Ordering::.  It
    allows users to run ‘make LIBTOOLFLAGS=--silent’, for instance.  Note that
    the verbosity of ‘libtool’ can also be influenced by the Automake support
    for silent rules (*note Automake Silent Rules::).


={============================================================================
*kt_linux_tool_150* make-automake-build-variables

8.4 Program and Library Variables

Associated with each program is a collection of variables that can be used to
modify how that program is built.  There is a similar list of such variables
for each library.  The canonical name of the program (or library) is used as a
base for naming these variables.

In the list below, we use the name “maude” to refer to the 'program' or
'library'.  In your ‘Makefile.am’ you would replace this with the canonical
name of your program.  This list also refers to “maude” as a program, but in
general the same rules apply for both static and dynamic libraries; the
documentation below notes situations `where programs and libraries differ`

‘maude_SOURCES’
     This variable, if it exists, lists all the source files that are compiled
     to build the program.  These files are added to the distribution by
     default.  When building the program, Automake will cause each source file
     to be compiled to a single ‘.o’ file (or ‘.lo’ when using libtool).

     Normally these object files are named after the source file, but other
     factors can change this.  If a file in the ‘_SOURCES’ variable has an
     unrecognized extension, Automake will do one of two things with it.  If a
     suffix rule exists for turning files with the unrecognized extension into
     ‘.o’ files, then ‘automake’ will treat this file as it will any other
     source file.  Otherwise, the file will be ignored as though it were a
     header file.

     The prefixes ‘dist_’ and ‘nodist_’ can be used to control whether files
     listed in a ‘_SOURCES’ variable are distributed. ‘dist_’ is 'redundant',
            as sources are distributed by default, but it can be specified for
              clarity if desired.

     It is possible to have both ‘dist_’ and ‘nodist_’ variants of a given
     ‘_SOURCES’ variable at once; this lets you easily distribute some files and
     not others, for instance:

          nodist_maude_SOURCES = nodist.c
          dist_maude_SOURCES = dist-me.c

     By default the output file (on Unix systems, the ‘.o’ file) will be put
     into the current 'build' directory.  However, if the option
     ‘subdir-objects’ is in effect in the current directory then the ‘.o’ file
     will be put into the subdirectory named after the source file.  For
     instance, with ‘subdir-objects’ enabled, ‘sub/dir/file.c’ will be compiled
     to ‘sub/dir/file.o’.  Some people prefer this mode of operation.  You can
     specify ‘subdir-objects’ in ‘AUTOMAKE_OPTIONS’ (*note Options::).

‘EXTRA_maude_SOURCES’
     Automake needs to know the list of files you intend to compile
     _statically_.  For one thing, this is the only way Automake has of knowing
     what sort of language support a given ‘Makefile.in’ requires.  (1) This
     means that, for example, you can’t put a configure substitution like
     ‘@my_sources@’ into a ‘_SOURCES’ variable.  If you intend to conditionally
     compile source files and use ‘configure’ to substitute the appropriate
     object names into, e.g., ‘_LDADD’ (see below), then you should list the
     corresponding source files in the ‘EXTRA_’ variable.

     This variable also supports ‘dist_’ and ‘nodist_’ prefixes.

‘maude_AR’
     A static library is created by default by invoking ‘$(AR) $(ARFLAGS)’
     followed by the name of the library and then the objects being put into the
     library.  You can override this by setting the ‘_AR’ variable.  This is
     usually used with C++; some C++ compilers require a special invocation in
     order to instantiate all the templates that should go into a library.  For
     instance, the SGI C++ compiler likes this variable set like so:

     libmaude_a_AR = $(CXX) -ar -o

‘maude_LIBADD’
     Extra objects can be added to a _library_ using the ‘_LIBADD’ variable.
     For instance, this `should be used for objects determined by ‘configure’`
     (*note A Library::).

     In the case of libtool libraries, ‘maude_LIBADD’ can also refer to other
     libtool libraries.


‘maude_LDADD’
     Extra objects (‘*.$(OBJEXT)’) and libraries (‘*.a’, ‘*.la’) can be added to
     a _program_ by listing them in the ‘_LDADD’ variable.  For instance, this
     should be used for objects determined by ‘configure’ (*note Linking::).

     ‘_LDADD’ and ‘_LIBADD’ are `inappropriate for passing program-specific`
     linker flags (except for ‘-l’, ‘-L’, ‘-dlopen’ and ‘-dlpreopen’).  Use
     the ‘_LDFLAGS’ variable for this purpose.

     For instance, if your ‘configure.ac’ uses ‘AC_PATH_XTRA’, you could link
     your program against the X libraries like so:

          maude_LDADD = $(X_PRE_LIBS) $(X_LIBS) $(X_EXTRA_LIBS)

     We recommend that you use ‘-l’ and ‘-L’ only when referring to
     third-party libraries, and give the explicit file names of any library
     built by your package.  Doing so will ensure that ‘maude_DEPENDENCIES’
     (see below) is correctly defined by default.


‘maude_LDFLAGS’
     This variable is used to pass 'extra''flags' to the link step of a program
     or a shared library.  It overrides the ‘AM_LDFLAGS’ variable.

‘maude_LIBTOOLFLAGS’
     This variable is used to pass extra options to ‘libtool’.  It overrides the
     ‘AM_LIBTOOLFLAGS’ variable.  These options are output before ‘libtool’’s
     ‘--mode=MODE’ option, so they should not be mode-specific options (those
         belong to the compiler or linker flags).  *Note Libtool Flags::.

‘maude_DEPENDENCIES’
‘EXTRA_maude_DEPENDENCIES’
     It is also occasionally useful to have a target (program or library) depend
     on some other file that is not actually part of that target.  This can be
     done using the ‘_DEPENDENCIES’ variable.  Each target depends on the
     contents of such a variable, but no further interpretation is done.

     Since these dependencies are associated to the link rule used to
     create the programs they should normally list files used by the
     link command.  That is ‘*.$(OBJEXT)’, ‘*.a’, or ‘*.la’ files for
     programs; ‘*.lo’ and ‘*.la’ files for Libtool libraries; and
     ‘*.$(OBJEXT)’ files for static libraries.  In rare cases you may
     need to add other kinds of files such as linker scripts, but
     _listing a source file in ‘_DEPENDENCIES’ is wrong_.  If some
     source file needs to be built before all the components of a
     program are built, consider using the ‘BUILT_SOURCES’ variable
     (*note Sources::).

     If ‘_DEPENDENCIES’ is not supplied, it is computed by Automake.
     The automatically-assigned value is the contents of ‘_LDADD’ or
     ‘_LIBADD’, with most configure substitutions, ‘-l’, ‘-L’, ‘-dlopen’
     and ‘-dlpreopen’ options removed.  The configure substitutions that
     are left in are only ‘$(LIBOBJS)’ and ‘$(ALLOCA)’; these are left
     because it is known that they will not cause an invalid value for
     ‘_DEPENDENCIES’ to be generated.

     ‘_DEPENDENCIES’ is more likely used to perform conditional
     compilation using an ‘AC_SUBST’ variable that contains a list of
     objects.  *Note Conditional Sources::, and *note Conditional
     Libtool Sources::.

     The ‘EXTRA_*_DEPENDENCIES’ variable may be useful for cases where
     you merely want to augment the ‘automake’-generated ‘_DEPENDENCIES’
     variable rather than replacing it.

‘maude_LINK’
     You can override the linker on a per-program basis.  By default the
     linker is chosen according to the languages used by the program.
     For instance, a program that includes C++ source code would use the
     C++ compiler to link.  The ‘_LINK’ variable must hold the name of a
     command that can be passed all the ‘.o’ file names and libraries to
     link against as arguments.  Note that the name of the underlying
     program is _not_ passed to ‘_LINK’; typically one uses ‘$@’:

          maude_LINK = $(CCLD) -magic -o $@

     If a ‘_LINK’ variable is not supplied, it may still be generated
     and used by Automake due to the use of per-target link flags such
     as ‘_CFLAGS’, ‘_LDFLAGS’ or ‘_LIBTOOLFLAGS’, in cases where they
     apply.


<per-target-compilation>

‘maude_CCASFLAGS’
‘maude_CFLAGS’
‘maude_CPPFLAGS’
‘maude_CXXFLAGS’
‘maude_FFLAGS’
‘maude_GCJFLAGS’
‘maude_LFLAGS’
‘maude_OBJCFLAGS’
‘maude_OBJCXXFLAGS’
‘maude_RFLAGS’
‘maude_UPCFLAGS’
‘maude_YFLAGS’
     Automake allows you to set 'compilation' flags on a per-program (or
         per-library) basis.  A single source file can be included in several
     programs, and it will potentially be compiled with different flags for each
     program.  This works for any language directly supported by Automake.

     These `per-target compilation flags` are 
     ‘_CCASFLAGS’, ‘_CFLAGS’, ‘_CPPFLAGS’, ‘_CXXFLAGS’, ‘_FFLAGS’, ‘_GCJFLAGS’, 
     ‘_LFLAGS’, ‘_OBJCFLAGS’, ‘_OBJCXXFLAGS’, ‘_RFLAGS’, ‘_UPCFLAGS’,
     and ‘_YFLAGS’.

     When using a per-target compilation flag, Automake will choose a
     `different name` for the intermediate object files.  Ordinarily a file like
     ‘sample.c’ will be compiled to produce ‘sample.o’.  However, if the
     program’s ‘_CFLAGS’ variable is set, then the object file will be named,
     for instance, ‘maude-sample.o’.  (See also *note Renamed Objects::).

     In compilations with per-target flags, the ordinary ‘AM_’ form of the flags
     variable is _not_ automatically included in the compilation (however, the
         user form of the variable _is_ included).

     So for instance, if you want the hypothetical ‘maude’ compilations
     to also use the value of ‘AM_CFLAGS’, you would need to write:

          maude_CFLAGS = … your flags ... $(AM_CFLAGS)

     *Note Flag Variables Ordering::, for more discussion about the interaction
     between 'user' variables, ‘AM_’ 'shadow' variables, and per-'target'
     variables.

‘maude_SHORTNAME’
     On some platforms the allowable file names are very short.  In
     order to support these systems and per-target compilation flags at
     the same time, Automake allows you to set a “short name” that will
     influence how intermediate object files are named.  For instance,
     in the following example,

          bin_PROGRAMS = maude
          maude_CPPFLAGS = -DSOMEFLAG
          maude_SHORTNAME = m
          maude_SOURCES = sample.c …

     the object file would be named ‘m-sample.o’ rather than
     ‘maude-sample.o’.

     This facility is rarely needed in practice, and we recommend
     avoiding it until you find it is required.

   ---------- Footnotes ----------

   (1) There are other, more obscure reasons for this limitation as well.


8.6 Special handling for LIBOBJS and ALLOCA

The ‘$(LIBOBJS)’ and ‘$(ALLOCA)’ variables `list object files` that should be
compiled into the project to provide an implementation for functions that are
missing or broken on the host system. `They are substituted by configure`

These variables are defined by Autoconf macros such as AC_LIBOBJ,
AC_REPLACE_FUNCS, or AC_FUNC_ALLOCA. Many other Autoconf macros call AC_LIBOBJ
  or AC_REPLACE_FUNCS `to populate` ‘$(LIBOBJS)’.

// more


8.7 Variables used when building a program

Occasionally it is useful to know `which ‘Makefile’ variables Automake uses` for
compilations, and in `which order`; for instance, you might need to do your own
compilation in some special cases.

   Some variables are 'inherited' from Autoconf; these are 

   ‘CC’, ‘CFLAGS’, ‘CPPFLAGS’, ‘DEFS’, ‘LDFLAGS’, and ‘LIBS’.

   There are some additional variables that Automake defines on its own:

‘AM_CPPFLAGS’
     The contents of this variable are passed to `every compilation` that invokes
     the C 'preprocessor'; it is a list of arguments to the preprocessor.  For
     instance, ‘-I’ and ‘-D’ options should be listed here.

     Automake already provides some ‘-I’ options automatically, in a separate
     variable that is also passed to every compilation that invokes the C
     preprocessor. In particular it generates ‘-I.’, ‘-I$(srcdir)’, and a ‘-I’
     pointing to the directory holding ‘config.h’ (if you’ve used
         ‘AC_CONFIG_HEADERS’).  You can disable the default ‘-I’ options using
     the `nostdinc` option.

     When a file to be included is generated during the build and not
     part of a distribution tarball, its location is under
     ‘$(builddir)’, not under ‘$(srcdir)’.  This matters especially for
     packages that use header files placed in sub-directories and want
     to allow builds outside the source tree (*note VPATH Builds::).  In
     that case we recommend to use a pair of ‘-I’ options, such as,
     e.g., ‘-Isome/subdir -I$(srcdir)/some/subdir’ or
     ‘-I$(top_builddir)/some/subdir -I$(top_srcdir)/some/subdir’.  Note
     that the reference to the build tree should come before the
     reference to the source tree, so that accidentally leftover
     generated files in the source directory are ignored.

     `AM_CPPFLAGS is ignored` in preference to a per-executable (or
         per-library) ‘_CPPFLAGS’ variable `if it is defined`

// ‘INCLUDES’
//      This does the same job as ‘AM_CPPFLAGS’ (or any per-target
//      ‘_CPPFLAGS’ variable if it is used).  It is an older name for the
//      same functionality.  `This variable is deprecated`; we suggest using
//      ‘AM_CPPFLAGS’ and per-target ‘_CPPFLAGS’ instead.

‘AM_CFLAGS’
     This is the variable the ‘Makefile.am’ author can use to pass in
     additional C compiler flags.  In some situations, this is not used, in
     preference to the per-executable (or per-library) ‘_CFLAGS’.

‘COMPILE’
     This is the command used to actually compile a C source file.  The file
     name is appended to form the complete command line.

‘AM_LDFLAGS’
     This is the variable the ‘Makefile.am’ author can use to pass in
     additional linker flags.  In some situations, this is not used, in
     preference to the per-executable (or per-library) ‘_LDFLAGS’.

‘LINK’
     This is the command used to actually link a C program.  It already
     includes ‘-o $@’ and the usual variable references (for instance,
         ‘CFLAGS’); it takes as “arguments” the names of the object files and
     libraries to link in.  This variable is not used when the linker is
     overridden with a per-target ‘_LINK’ variable or per-target flags cause
     Automake to define such a ‘_LINK’ variable.


8.9 C++ Support

Automake includes full support for C++.

Any package including C++ code 'must' define the output variable ‘CXX’ in
‘configure.ac’; the simplest way to do this is to use the ‘AC_PROG_CXX’ macro
(*note Particular Program Checks: (autoconf)Particular Programs.).

A few additional variables are defined when a C++ source file is seen:

‘CXX’
     The name of the C++ compiler.

‘CXXFLAGS’
     Any flags to pass to the C++ compiler.

‘AM_CXXFLAGS’
     The maintainer’s variant of ‘CXXFLAGS’.

‘CXXCOMPILE’
     The command used to actually compile a C++ source file.  The file name is
     appended to form the complete command line.

‘CXXLINK’
     The command used to actually link a C++ program.


={============================================================================
*kt_linux_tool_150* make-automake-building variable order

27.6 Flag Variables Ordering

What is the difference between ‘AM_CFLAGS’, ‘CFLAGS’, and ‘mumble_CFLAGS’?

Why does ‘automake’ output ‘CPPFLAGS’ after ‘AM_CPPFLAGS’ on compile lines?
Shouldn’t it be the converse?

My ‘configure’ adds some warning flags into ‘CXXFLAGS’.  In one ‘Makefile.am’ I
would like to append a new flag, however if I put the flag into ‘AM_CXXFLAGS’ it
is `prepended` to the other flags, `not appended`


Compile Flag Variables

This section attempts to answer all the above questions. We will mostly
discuss ‘CPPFLAGS’ in our examples, but actually the answer holds for 'all'
the compile flags used in Automake: ‘CCASFLAGS’, ‘CFLAGS’, ‘CPPFLAGS’,
‘CXXFLAGS’, ‘FCFLAGS’, ‘FFLAGS’, ‘GCJFLAGS’, ‘LDFLAGS’, ‘LFLAGS’,
‘LIBTOOLFLAGS’, ‘OBJCFLAGS’, ‘OBJCXXFLAGS’, ‘RFLAGS’, ‘UPCFLAGS’, and
  ‘YFLAGS’.

‘CPPFLAGS’, ‘AM_CPPFLAGS’, and ‘mumble_CPPFLAGS’ are three variables that can
be used to pass flags to the C preprocessor (actually these variables are also
    used for other languages like C++ or preprocessed Fortran). ‘CPPFLAGS’ is
the user variable *user-variable* , ‘AM_CPPFLAGS’ is the Automake variable,
and ‘mumble_CPPFLAGS’ is the variable specific to the ‘mumble’ target (we call
    this a per-target variable).


<variable-order>
Automake always uses two of these variables when compiling C sources files.
When compiling an object file for the ‘mumble’ target, the first variable will
be ‘mumble_CPPFLAGS’ if it is defined, or ‘AM_CPPFLAGS’ otherwise. The second
variable is always ‘CPPFLAGS’.

   In the following example,

     bin_PROGRAMS = foo bar
     foo_SOURCES = xyz.c
     bar_SOURCES = main.c
     foo_CPPFLAGS = -DFOO
     AM_CPPFLAGS = -DBAZ

‘xyz.o’ will be compiled with ‘$(foo_CPPFLAGS) $(CPPFLAGS)’, (because ‘xyz.o’
    is part of the ‘foo’ target), 

while ‘main.o’ will be compiled with ‘$(AM_CPPFLAGS) $(CPPFLAGS)’ (because
    there is no per-target variable for target ‘bar’).


The difference between ‘mumble_CPPFLAGS’ and ‘AM_CPPFLAGS’ being clear enough,
    let’s focus on ‘CPPFLAGS’. ‘CPPFLAGS’ is a user variable, i.e., a variable
      that users are entitled to modify in order to compile the package.  This
      variable, like many others, is documented at the end of the output of
      ‘configure --help’.

For instance, someone who needs to add ‘/home/my/usr/include’ to the C
compiler’s search path would configure a package with

     ./configure CPPFLAGS='-I /home/my/usr/include'

and this flag would be 'propagated' to the compile rules of `all Makefile’s`


<make-time-change>
It is also not uncommon to override a user variable at `make-time`.  Many
installers do this with ‘prefix’, but this can be useful with compiler flags
too. For instance, if, while debugging a C++ project, you need to 'disable'
optimization `in one specific object file`, you can run something like

     rm file.o
     make CXXFLAGS=-O0 file.o
     make

The reason ‘$(CPPFLAGS)’ appears after ‘$(AM_CPPFLAGS)’ or ‘$(mumble_CPPFLAGS)’
in the compile command is `that users should always have the last say`  

*should-not-use-user-variable-in-makefile-or-config-ac*

It probably makes more sense if you think about it while looking at the
‘CXXFLAGS=-O0’ above, which should 'supersede' any other switch from
‘AM_CXXFLAGS’ or ‘mumble_CXXFLAGS’ (and this of course replaces the previous
    value of ‘CXXFLAGS’).

You should never redefine a user variable such as ‘CPPFLAGS’ in ‘Makefile.am’.
Use ‘automake -Woverride’ to diagnose such mistakes.

Even something like

     CPPFLAGS = -DDATADIR=\"$(datadir)\" @CPPFLAGS@

is erroneous.  Although this preserves ‘configure’’s value of ‘CPPFLAGS’, the
definition of ‘DATADIR’ will disappear if a user attempts to override ‘CPPFLAGS’
from the ‘make’ command line.

     AM_CPPFLAGS = -DDATADIR=\"$(datadir)\"

is all that is needed here if no per-target flags are used.

You should not add options to these user variables within ‘configure’ either,
for the same reason.  Occasionally you need to modify these variables to perform
  a test, but you should reset their values afterwards.  In contrast, it is OK
    to modify the ‘AM_’ variables within ‘configure’ if you ‘AC_SUBST’ them, but
    it is rather rare that you need to do this, unless you really want to change
    the default definitions of the ‘AM_’ variables in all ‘Makefile’s.

What we recommend is that you define extra flags in separate variables.  For
instance, you may write an Autoconf macro that computes a set of warning options
for the C compiler, and ‘AC_SUBST’ them in ‘WARNINGCFLAGS’; you may also have an
  Autoconf macro that determines which compiler and which linker flags should be
    used to link with library ‘libfoo’, and ‘AC_SUBST’ these in ‘LIBFOOCFLAGS’
    and ‘LIBFOOLDFLAGS’.  Then, a ‘Makefile.am’ could use these variables as
    follows:

     AM_CFLAGS = $(WARNINGCFLAGS)
     bin_PROGRAMS = prog1 prog2
     prog1_SOURCES = ...
     prog2_SOURCES = ...
     prog2_CFLAGS = $(LIBFOOCFLAGS) $(AM_CFLAGS)
     prog2_LDFLAGS = $(LIBFOOLDFLAGS)

In this example both programs will be compiled with the flags substituted into
‘$(WARNINGCFLAGS)’, and ‘prog2’ will additionally be compiled with the flags
required to link with ‘libfoo’.

Note that listing ‘AM_CFLAGS’ in a per-target ‘CFLAGS’ variable is a common
idiom to ensure that ‘AM_CFLAGS’ applies to every target in a ‘Makefile.in’.

Using variables like this gives you full control over the ordering of the flags.
For instance, if there is a flag in $(WARNINGCFLAGS) that you want to negate for
a particular target, you can use something like ‘prog1_CFLAGS = $(AM_CFLAGS)
-no-flag’.  If all of these flags had been forcefully appended to ‘CFLAGS’,
  there would be no way to disable one flag.  Yet another reason to leave user
    variables to users.

Finally, we have avoided naming the variable of the example ‘LIBFOO_LDFLAGS’
(with an underscore) because that would cause Automake to think that this is
actually a per-target variable (like ‘mumble_LDFLAGS’) for some non-declared
‘LIBFOO’ target.

Other Variables
---------------

There are other variables in Automake that follow similar principles to allow
user options.  For instance, Texinfo rules (*note Texinfo::) use ‘MAKEINFOFLAGS’
and ‘AM_MAKEINFOFLAGS’.  Similarly, DejaGnu tests (*note DejaGnu Tests::) use
‘RUNTESTDEFAULTFLAGS’ and ‘AM_RUNTESTDEFAULTFLAGS’.  The tags and ctags rules
(*note Tags::) use ‘ETAGSFLAGS’, ‘AM_ETAGSFLAGS’, ‘CTAGSFLAGS’, and
‘AM_CTAGSFLAGS’.  Java rules (*note Java::) use ‘JAVACFLAGS’ and
‘AM_JAVACFLAGS’.  None of these rules support per-target flags (yet).

To some extent, even ‘AM_MAKEFLAGS’ (*note Subdirectories::) obeys this naming
scheme.  The slight difference is that ‘MAKEFLAGS’ is passed to sub-‘make’s
implicitly by ‘make’ itself.

‘ARFLAGS’ (*note A Library::) is usually defined by Automake and has neither
‘AM_’ nor per-target cousin.

Finally you should not think that the existence of a per-target variable implies
the existence of an ‘AM_’ variable or of a user variable.  For instance, the
‘mumble_LDADD’ per-target variable overrides the makefile-wide ‘LDADD’ variable
(which is not a user variable), and ‘mumble_LIBADD’ exists only as a per-target
variable.  *Note Program and Library Variables::.


={============================================================================
*kt_linux_tool_150* automake: 08: program variables: case issue

In short LDFLAGS is added before the object files on the command line and LDADD
is added afterwards.

<fail-case>
nexus_inspect_LDADD = \
   @NEXUS_LIBS@            note: this has -Wl,--as-needed

nexus_inspect_LDFLAGS = \
   -lpthread \
   -linit \
   -rdynamic

-Wl,--as-needed -L/zinc-install-root/release/huawei-bcm7409/lib
...
-lnexusMgr -ldirectfb -lfusion -ldirect -linit -lnexus -lz -lpthread

<okay-case>
nexus_inspect_LDFLAGS = @NEXUS_LIBS@

So -lxxx and then --as-needed

-lnexusMgr -ldirectfb -lfusion -ldirect -linit -lnexus -lz -lpthread
...
-Wl,--as-needed -L/zinc-install-root/release/huawei-bcm7409/lib

So not sure how automake combines all but it makes different order. This caues
the problem because shared libraries after as-needed may not linked at the time
of linking and cause undefined symbol when making executable binary.


={============================================================================
*kt_linux_tool_150* automake: 09: scripts

9 Other Derived Objects
***********************

Automake can handle derived objects that are not C programs.  Sometimes the
support for actually building such objects must be explicitly supplied, but
Automake will still automatically handle installation and distribution.

9.1 Executable Scripts
======================

<when-copied>
It is possible to define and install programs that are scripts.  Such
programs are listed using the ‘SCRIPTS’ primary name.  When the script
is distributed in its final, installable form, the ‘Makefile’ usually
looks as follows:

     # Install my_script in $(bindir) and distribute it.
     dist_bin_SCRIPTS = my_script

Scripts are not distributed by default; as we have just seen, those that should
be distributed can be specified using a ‘dist_’ prefix as with other primaries.

Scripts can be installed in ‘bindir’, ‘sbindir’, ‘libexecdir’, ‘pkglibexecdir’,
or ‘pkgdatadir’.

Scripts that need not be installed can be listed in ‘noinst_SCRIPTS’, and among
them, those which are needed only by ‘make check’ should go in ‘check_SCRIPTS’.

<when-to-be-built>
When a script needs to be built, the ‘Makefile.am’ should include the
appropriate rules.  For instance the ‘automake’ program itself is a Perl script
that is generated from ‘automake.in’.  Here is how this is handled:

     bin_SCRIPTS = automake
     CLEANFILES = $(bin_SCRIPTS)
     EXTRA_DIST = automake.in

     do_subst = sed -e 's,[@]datadir[@],$(datadir),g' \
                 -e 's,[@]PERL[@],$(PERL),g' \
                 -e 's,[@]PACKAGE[@],$(PACKAGE),g' \
                 -e 's,[@]VERSION[@],$(VERSION),g' \
                 …

     automake: automake.in Makefile
             $(do_subst) < $(srcdir)/automake.in > automake
             chmod +x automake

Such scripts for which a build rule has been supplied need to be deleted
explicitly using ‘CLEANFILES’ (*note Clean::), and their sources have to be
distributed, usually with ‘EXTRA_DIST’ (*note Basics of Distribution::).

Another common way to build scripts is to process them from ‘configure’ with
‘AC_CONFIG_FILES’.  In this situation Automake knows which files should be
cleaned and distributed, and what the rebuild rules should look like.

For instance if ‘configure.ac’ contains

     AC_CONFIG_FILES([src/my_script], [chmod +x src/my_script])

to build ‘src/my_script’ from ‘src/my_script.in’, then a ‘src/Makefile.am’ to
install this script in ‘$(bindir)’ can be as simple as

     bin_SCRIPTS = my_script
     CLEANFILES = $(bin_SCRIPTS)

There is no need for ‘EXTRA_DIST’ or any build rule: Automake infers them from
‘AC_CONFIG_FILES’ (*note Requirements::).  ‘CLEANFILES’ is still useful, because
by default Automake will clean targets of ‘AC_CONFIG_FILES’ in ‘distclean’, not
‘clean’.

Although this looks simpler, building scripts this way has one drawback:
directory variables such as ‘$(datadir)’ are not fully expanded and may refer to
other directory variables.


={============================================================================
*kt_linux_tool_150* automake: 15: test and make check

15 Support for test suites
**************************

Automake can generate code to handle two kinds of test suites.  One is based on
integration with the ‘dejagnu’ framework.  The other (and most used) form is
based on the use of generic test scripts, and its 'activation' is triggered by
the definition of the special ‘TESTS’ variable.  This second form allows for
various degrees of sophistication and customization; in particular, it allows
for concurrent execution of test scripts, use of established test protocols such
  as TAP, and definition of custom test drivers and test runners.

In either case, the testsuite is 'invoked' via ‘make check’.


15.2.1 Scripts-based Testsuites
-------------------------------

If the 'special' variable ‘TESTS’ is defined, its value is taken to be a list of
'programs' or 'scripts' to 'run' in order to do the testing.  Under the
appropriate circumstances, it’s possible for ‘TESTS’ to list also data files to
be passed to one or more test scripts defined by different means (the so-called
    “log compilers”, *note Parallel Test Harness::).

Test scripts can be executed serially or concurrently.  Automake supports both
these kinds of test execution, with the 'parallel' test harness being the
'default'.  The concurrent test harness relies on the concurrence capabilities
(if any) offered by the underlying ‘make’ implementation, and can thus only be
as good as those are.

By default, only the exit statuses of the test scripts are considered when
determining the testsuite outcome.  But Automake allows also the use of more
complex test protocols, either standard (*note Using the TAP test protocol::) or
custom (*note Custom Test Drivers::). In the rest of this section we are going
to concentrate mostly on protocol-less tests, since we cover test protocols in a
later section (again, *note Custom Test Drivers::).

<return-value>
When no test protocol is in use, an exit status of 0 from a test script will
denote a success, an exit status of 77 a skipped test, an exit status of 99 an
hard error, and any other exit status will denote a failure.

You may define the variable ‘XFAIL_TESTS’ to a list of tests (usually a subset
    of ‘TESTS’) that are expected to fail; this will effectively reverse the
result of those tests (with the provision that skips and hard errors remain
    untouched).  You may also instruct the testsuite harness to treat hard
errors like simple failures, by defining the ‘DISABLE_HARD_ERRORS’ make variable
to a nonempty value.

Note however that, for tests based on more complex test protocols, the exact
effects of ‘XFAIL_TESTS’ and ‘DISABLE_HARD_ERRORS’ might change, or they might
even have no effect at all (for example, in tests using TAP, there is not way to
    disable hard errors, and the ‘DISABLE_HARD_ERRORS’ variable has no effect on
    them).

The result of each test case run by the scripts in ‘TESTS’ will be 'printed' on
standard output, along with the test name.  For test protocols that allow more
test cases per test script (such as TAP), a number, identifier and/or brief
description specific for the single test case is expected to be printed in
addition to the name of the test script.  The possible results (whose meanings
    should be clear from the previous *note Generalities about Testing::) are
‘PASS’, ‘FAIL’, ‘SKIP’, ‘XFAIL’, ‘XPASS’ and ‘ERROR’.  Here is an example of
output from an hypothetical testsuite that uses both plain and TAP tests:

     PASS: foo.sh
     PASS: zardoz.tap 1 - Daemon started
     PASS: zardoz.tap 2 - Daemon responding
     SKIP: zardoz.tap 3 - Daemon uses /proc # SKIP /proc is not mounted
     PASS: zardoz.tap 4 - Daemon stopped
     SKIP: bar.sh
     PASS: mu.tap 1
     XFAIL: mu.tap 2 # TODO frobnication not yet implemented

A testsuite summary (expected to report at least the number of run, skipped and
    failed tests) will be printed at the end of the testsuite run.

If the standard output is connected to a capable terminal, then the test results
and the summary are colored appropriately.  The developer and the user can
disable colored output by setting the ‘make’ variable ‘AM_COLOR_TESTS=no’; the
user can in addition force colored output even without a connecting terminal
with ‘AM_COLOR_TESTS=always’.  It’s also worth noting that some ‘make’
implementations, when used in parallel mode, have slightly different semantics
(*note (autoconf)Parallel make::), which can break the automatic detection of a
connection to a capable terminal.  If this is the case, the user will have to
resort to the use of ‘AM_COLOR_TESTS=always’ in order to have the testsuite
output colorized.


Test programs that need data files should look for them in ‘srcdir’ (which is
    both a make variable and an environment variable made available to the
    tests), so that they work when building in a separate directory (*note Build
      Directories: (autoconf)Build Directories.), and in particular for the
    ‘distcheck’ rule (*note Checking the Distribution::).

The ‘AM_TESTS_ENVIRONMENT’ and ‘TESTS_ENVIRONMENT’ variables can be used to run
initialization code and set environment variables for the test scripts.  The
former variable is developer-reserved, and can be defined in the ‘Makefile.am’,
       while the latter is reserved for the user, which can employ it to extend
         or override the settings in the former; for this to work portably,
            however, the contents of a non-empty ‘AM_TESTS_ENVIRONMENT’ _must_
              be terminated by a semicolon.

The ‘AM_TESTS_FD_REDIRECT’ variable can be used to define file descriptor
redirections for the test scripts.  One might think that ‘AM_TESTS_ENVIRONMENT’
could be used for this purpose, but experience has shown that doing so portably
is practically impossible.  The main hurdle is constituted by Korn shells, which
usually set the close-on-exec flag on file descriptors opened with the ‘exec’
builtin, thus rendering an idiom like ‘AM_TESTS_ENVIRONMENT = exec 9>&2;’
ineffectual.  This issue also affects some Bourne shells, such as the HP-UX’s
‘/bin/sh’,

     AM_TESTS_ENVIRONMENT = \
     ## Some environment initializations are kept in a separate shell
     ## file 'tests-env.sh', which can make it easier to also run tests
     ## from the command line.
       . $(srcdir)/tests-env.sh; \
     ## On Solaris, prefer more POSIX-compliant versions of the standard
     ## tools by default.
       if test -d /usr/xpg4/bin; then \
         PATH=/usr/xpg4/bin:$$PATH; export PATH; \
       fi;
     ## With this, the test scripts will be able to print diagnostic
     ## messages to the original standard error stream, even if the test
     ## driver redirects the stderr of the test scripts to a log file
     ## before executing them.
     AM_TESTS_FD_REDIRECT = 9>&2

Note however that ‘AM_TESTS_ENVIRONMENT’ is, for historical and implementation
reasons, _not_ supported by the serial harness (*note Serial Test Harness::).

Automake ensures that each file listed in ‘TESTS’ is built 'before' it is run;
     you can list both source and derived programs (or scripts) in ‘TESTS’; the
       generated rule will look both in ‘srcdir’ and ‘.’.  For instance, you
       might want to run a C program as a test.  To do this you would list its
       name in ‘TESTS’ and also in ‘check_PROGRAMS’, and then specify it as you
       would any other program.

Programs listed in ‘check_PROGRAMS’ (and ‘check_LIBRARIES’,
    ‘check_LTLIBRARIES’...)  are only built 'during' ‘make check’, not during
‘make all’.  You should list there any program needed by your tests that does
'not' need to be built by ‘make all’.  Note that ‘check_PROGRAMS’ are _not_
automatically added to ‘TESTS’ because ‘check_PROGRAMS’ usually lists programs
used by the tests, not the tests themselves.  Of course you can set ‘TESTS =
$(check_PROGRAMS)’ if all your programs are test cases.


15.2.3 Parallel Test Harness
----------------------------

By default, Automake generated a parallel (concurrent) test harness.  It
features automatic collection of the test scripts output in ‘.log’ files,
         concurrent execution of tests with ‘make -j’, specification of
           inter-test dependencies, lazy reruns of tests that have not completed
           in a prior run, and hard errors for exceptional failures.

The parallel test harness operates by defining a set of ‘make’ rules that run
the test scripts listed in ‘TESTS’, and, for each such script, save its output
in a corresponding ‘.log’ file and its results (and other “metadata”, *note API
    for Custom Test Drivers::) in a corresponding ‘.trs’ (as in Test ReSults)
file.  The ‘.log’ file will contain all the output emitted by the test on its
standard output and its standard error.  The ‘.trs’ file will contain, among the
other things, the results of the test cases run by the script.

The parallel test harness will also create a summary log file, ‘TEST_SUITE_LOG’,
which defaults to ‘test-suite.log’ and requires a ‘.log’ suffix.  This file
  depends upon all the ‘.log’ and ‘.trs’ files created for the test scripts
  listed in ‘TESTS’.

As with the serial harness above, by default one status line is printed per
completed test, and a short summary after the suite has completed.  However,
standard output and standard error of the test are redirected to a per-test log
  file, so that parallel execution does not produce intermingled output.  The
  output from failed tests is collected in the ‘test-suite.log’ file.  If the
  variable ‘VERBOSE’ is set, this file is output after the summary.

Each couple of ‘.log’ and ‘.trs’ files is created when the corresponding test
has completed.  The set of log files is listed in the read-only variable
‘TEST_LOGS’, and defaults to ‘TESTS’, with the executable extension if any
(*note EXEEXT::), as well as any suffix listed in ‘TEST_EXTENSIONS’ removed, and
‘.log’ appended.  Results are undefined if a test file name ends in several
concatenated suffixes.  ‘TEST_EXTENSIONS’ defaults to ‘.test’; it can be
overridden by the user, in which case any extension listed in it must be
constituted by a dot, followed by a non-digit alphabetic character, followed by
any number of alphabetic characters.  For example, ‘.sh’, ‘.T’ and ‘.t1’ are
valid extensions, while ‘.x-y’, ‘.6c’ and ‘.t.1’ are not.

It is important to note that, due to current limitations (unlikely to be
    lifted), configure substitutions in the definition of ‘TESTS’ can only work
if they will expand to a list of tests that have a suffix listed in
  ‘TEST_EXTENSIONS’.

For tests that match an extension ‘.EXT’ listed in ‘TEST_EXTENSIONS’, you can
provide a custom “test runner” using the variable ‘EXT_LOG_COMPILER’ (note the
    upper-case extension) and pass options in ‘AM_EXT_LOG_FLAGS’ and allow the
user to pass options in ‘EXT_LOG_FLAGS’.  It will cause all tests with this
extension to be called with this runner.  For all tests without a registered
extension, the variables ‘LOG_COMPILER’, ‘AM_LOG_FLAGS’, and ‘LOG_FLAGS’ may be
used.  For example,

     TESTS = foo.pl bar.py baz
     TEST_EXTENSIONS = .pl .py
     PL_LOG_COMPILER = $(PERL)
     AM_PL_LOG_FLAGS = -w
     PY_LOG_COMPILER = $(PYTHON)
     AM_PY_LOG_FLAGS = -v
     LOG_COMPILER = ./wrapper-script
     AM_LOG_FLAGS = -d

will invoke ‘$(PERL) -w foo.pl’, ‘$(PYTHON) -v bar.py’, and ‘./wrapper-script -d
baz’ to produce ‘foo.log’, ‘bar.log’, and ‘baz.log’, respectively.  The
‘foo.trs’, ‘bar.trs’ and ‘baz.trs’ files will be automatically produced as a
side-effect.

It’s important to note that, differently from what we’ve seen for the serial
test harness (*note Serial Test Harness::), the ‘AM_TESTS_ENVIRONMENT’ and
‘TESTS_ENVIRONMENT’ variables _cannot_ be use to define a custom test runner;
     the ‘LOG_COMPILER’ and ‘LOG_FLAGS’ (or their extension-specific
         counterparts) should be used instead:

     ## This is WRONG!
     AM_TESTS_ENVIRONMENT = PERL5LIB='$(srcdir)/lib' $(PERL) -Mstrict -w

     ## Do this instead.
     AM_TESTS_ENVIRONMENT = PERL5LIB='$(srcdir)/lib'; export PERL5LIB;
     LOG_COMPILER = $(PERL)
     AM_LOG_FLAGS = -Mstrict -w

By default, the test suite harness will run all tests, but there are several
ways to 'limit' the set of tests that are run:

• You can set the ‘TESTS’ variable.  For example, you can use a command like
this to run only a subset of the tests:

          env TESTS="foo.test bar.test" make -e check

Note however that the command above will unconditionally overwrite the
‘test-suite.log’ file, thus clobbering the recorded results of any previous
testsuite run.  This might be undesirable for packages whose testsuite takes
long time to execute.  Luckily, this problem can easily be avoided by overriding
also ‘TEST_SUITE_LOG’ at runtime; for example,

          env TEST_SUITE_LOG=partial.log TESTS="..." make -e check

will write the result of the partial testsuite runs to the ‘partial.log’,
without touching ‘test-suite.log’.

• You can set the ‘TEST_LOGS’ variable.  By default, this variable is computed
at ‘make’ run time from the value of ‘TESTS’ as described above.  For example,
you can use the following:

          set x subset*.log; shift
          env TEST_LOGS="foo.log $*" make -e check

The comments made above about ‘TEST_SUITE_LOG’ overriding applies here too.

• By default, the test harness removes all old per-test ‘.log’ and ‘.trs’ files
before it starts running tests to regenerate them.  The variable ‘RECHECK_LOGS’
contains the set of ‘.log’ (and, by implication, ‘.trs’) files which are
removed.  ‘RECHECK_LOGS’ defaults to ‘TEST_LOGS’, which means all tests need to
be rechecked.  By overriding this variable, you can choose which tests need to
be reconsidered.  For example, you can lazily rerun only those tests which are
outdated, i.e., older than their prerequisite test files, by setting this
variable to the empty value:

          env RECHECK_LOGS= make -e check

• You can ensure that all tests are rerun which have failed or passed
unexpectedly, by running ‘make recheck’ in the test directory.  This convenience
target will set ‘RECHECK_LOGS’ appropriately before invoking the main test
harness.

In order to guarantee an ordering between tests even with ‘make -jN’,
dependencies between the corresponding ‘.log’ files may be specified through
  usual ‘make’ dependencies.  For example, the following snippet lets the test
  named ‘foo-execute.test’ depend upon completion of the test
  ‘foo-compile.test’:

     TESTS = foo-compile.test foo-execute.test
     foo-execute.log: foo-compile.log

Please note that this ordering ignores the _results_ of required tests, thus the
test ‘foo-execute.test’ is run even if the test ‘foo-compile.test’ failed or was
skipped beforehand.  Further, please note that specifying such dependencies
currently works only for tests that end in one of the suffixes listed in
‘TEST_EXTENSIONS’.

Tests without such specified dependencies may be run concurrently with parallel
‘make -jN’, so be sure they are prepared for concurrent execution.

The combination of lazy test execution and correct dependencies
between tests and their sources may be exploited for efficient unit
testing during development.  To further speed up the edit-compile-test
cycle, it may even be useful to specify compiled programs in
‘EXTRA_PROGRAMS’ instead of with ‘check_PROGRAMS’, as the former allows
intertwined compilation and test execution (but note that
‘EXTRA_PROGRAMS’ are not cleaned automatically, *note Uniform::).

The variables ‘TESTS’ and ‘XFAIL_TESTS’ may contain conditional parts as well as
configure substitutions.  In the latter case, however, certain restrictions
apply: substituted test names must end with a nonempty test suffix like ‘.test’,
so that one of the inference rules generated by ‘automake’ can apply.  For
  literal test names, ‘automake’ can generate per-target rules to avoid this
  limitation.

Please note that it is currently not possible to use ‘$(srcdir)/’ or
‘$(top_srcdir)/’ in the ‘TESTS’ variable.  This technical limitation is
necessary to avoid generating test logs in the source tree and has the
unfortunate consequence that it is not possible to specify distributed tests
that are themselves generated by means of explicit rules, in a way that is
portable to all ‘make’ implementations (*note (autoconf)Make Target Lookup::,
    the semantics of FreeBSD and OpenBSD ‘make’ conflict with this).  In case of
doubt you may want to require to use GNU ‘make’, or work around the issue with
inference rules to generate the tests.



={============================================================================
*kt_linux_tool_160* ccache

The ccache tool tries to speed up a build by taking object files from cache when
the preprocessed source didn't change. In addition, it can often avoid
preprocessing, as a cache lookup on the source file path and compiler options
returns a list of include files and their modification times. If those weren't
updated, that counts as a 'direct hit'. Otherwise it falls back to preprocessing
the source and trying to look-up its hash in the cache. If successful, that
counts as a 'preprocessed hit'.

<problem>
The main barrier preventing cache sharing across git branches has to do with the fact we create
separate build slaves for each git branch, which results in different source and binary directories
being seen by the compiler. More specifically:

The compiler sets the __FILE__ macro to be the absolute path to the source file. The value of that
macro leaks to preprocessed source through the use of assert().

We pass preprocessor definitions like -DMACRO__prefix="...", -DMACRO__builddir="...",
   -DMACRO__top_srcdir="...", which can also leak to preprocessed source.

note: After all, like to have speed-up when switching between branches since the most are the same
between them.

branch 01                                    branch 02                     
 source dirs, build root dirs                 source dirs, build root dirs

             -> maps                             <- maps to virtual
                        _virtual_

So it is not to save space but to make ccache see _virtual_ when switching branches so that have
more cache hit.

To address this issue, the new zb-virtual-slave tool was developed. It's meant to wrap around other
build tools, such as zb-build-with-progress, zb-deploy, zb-shell, etc. and leading them to believe
we are always building a branch called _virtual_. This way, the compiler will see identical source
and binary locations, enabling cache hits across different git branches. zb-virtual-slave uses the
map-dir-and-exec tool internally, which you will have to build and install yourself.

note: need to install map-dir-and-exec package

CCACHE_DIR

The CCACHE_DIR environment variable specifies where ccache will keep its cached
compiler output. The default is $HOME/.ccache.

You can monitor the current usage of ccache by executing

CCACHE_DIR=.. $watch -n1 -d ccache -s


={============================================================================
*kt_linux_tool_161* icecream

icecream is a better version of distcc - a tool for distributing build jobs
across multiple machines.

It has a central scheduler (running `icecc-scheduler`) which runs on some server
and one or more nodes (machines running `iceccd`). The scheduler distributes
jobs depending on the load and speed of nodes - always picking the fastest and
least-loaded (preferring localhost when load is low).

One nice thing is it lets you specify what toolchain you are using and icecream
will distribute your toolchain for you so the remote machine can build
compatible object files.

 
note: do all as a root

Building icecream
-----------------
To build icecream the following worked for me, putting all files under
/opt/icecream


darren@djg-ubuntu:/data/software/icecream/build$ cat /data/documents/recipes/icecream.sh

#!/bin/bash
 
# Get source
git clone https://github.com/icecc/icecream.git
cd icecream
 
# Get dependencies
sudo apt-get install -y libcap-ng-dev liblzo2-dev docbook2x

or

sudo apt-get install -y --force-yes libcap-ng-dev liblzo2-dev docbook2x
 
# Build it
mkdir build
cd build
../autogen.sh
../configure --with-pic --prefix=/opt/icecream
make -j8
sudo make install

note: should run build on /opt/icecream but not /opt/icecream/build since fails
to search headers when building.


Preparing the Toolchain
-----------------------
toolchainTarball=$(/opt/icecream/bin/icecc --build-native | grep creating | cut -d' ' -f2)
sudo mkdir -p /opt/icecream/etc/
sudo cp -a $toolchainTarball /opt/icecream/etc/gcc-native.tar.gz


Preparing the Scheduler Node
-----------------------
Just pick a fast server for this. The scheduler doesn't appear to use much in
the way of resources. Then run: 

/opt/icecream/sbin/icecc-scheduler -d


Preparing Client Nodes
-----------------------

Add the icecc user (`sudo useradd icecc`).
Run the iceccd daemon (`sudo /opt/icecream/sbin/iceccd -d`). Note that it drops
privileges after chrooting.

If you're on Debian, this should work for you:

sudo apt-get install liblzo2-d libcap-ng-dev
curl devnfs2/~darren.garvey/icecream-debian.tar.bz2 | sudo tar -C / -xjf - && sudo useradd icecc && sudo /opt/icecream/sbin/iceccd -d

note: For debian whiz: sudo apt-get install liblzo2-2 libcap-ng-dev


={============================================================================
*kt_linux_tool_200* bin-:

{gcc-binutil}
https://sourceware.org/binutils/
http://sourceware.org/binutils/docs-2.20/
http://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html

GCC provides a large number of built-in functions other than the ones mentioned
above. Some of these are for internal use in the processing of exceptions or
variable-length argument lists and are not documented here because they may
change from time to time; we do not recommend general use of these functions. 


={============================================================================
*kt_linux_tool_201* tool-size

NAME
       size - list section sizes and total size.

SYNOPSIS
       size [-A|-B|--format=compatibility]
            [--help]
            [-d|-o|-x|--radix=number]
            [--common]
            [-t|--totals]
            [--target=bfdname] [-V|--version]
            [objfile...]

DESCRIPTION
       The GNU size utility lists the section sizes---and the total size---for
       each of the object or archive files objfile in its argument list.  By
       default, one line of output is generated for each object file or each
       module in an archive.

       objfile... are the object files to be examined.  If none are specified,
       the file "a.out" will be used.

<ex>
$ size ~/bash-dynamic
   text    data     bss     dec     hex filename
 722859   22732   21924  767515   bb61b /home/nds-uk/kyoupark/bash-dynamic


={============================================================================
*kt_linux_tool_202* tool-nm

%nm *.a

%/opt/toolchains/bin/mipsel-linux-uclibc-nm
%nm libicammulti.a | grep vendor

00000bc0 t _GLOBAL__I_vendor_init
00000980 T vendor_cleanup
00000000 T vendor_init
000002c8 T vendor_setup
000000bc r _ZZ12vendor_setupE12__FUNCTION__
00000088 r _ZZ12vendor_setupE19__PRETTY_FUNCTION__


$ nm -A /usr/lib/lib*.so 2> /dev/null | grep ' crypt$'
/usr/lib/libcrypt.so:00007080 W crypt

<demangle>
       -C
       --demangle[=style]
           Decode (demangle) low-level symbol names into user-level names.
           Besides removing any initial underscore prepended by the system,
this makes C++ function names readable.  Different compilers have different
  mangling styles. The optional demangling style argument can be used to
  choose an appropriate demangling style for your compiler.


-A
-o
--print-file-name
Precede each symbol by the name of the input file (or archive member) in which
it was found, rather than identifying the input file once only, before all of
its symbols.

*symbol-global* *symbol-local*
           `uppercase` means global and `lowercase` means local. 

           "R"
           "r" The symbol is in a read only data section.

           "B"
           "b" The symbol is in the uninitialized data section (known as BSS).

           <ex>
           static void AsanDie() {
           
             static atomic_uint32_t num_calls;
           }
           
           00164f74 b __asan::AsanDie()::num_calls
           
           However, the global var is:
           
           001639d4 B __asan_rt_version


           "T"
           "t" The symbol is in the text (code) section.

           "D"
           "d" The symbol is in the initialized data section.

           "A" The symbol's value is absolute, and will not be changed by
           further linking.

           "U" The symbol is undefined.

           "W"
           "w" The symbol is a weak symbol that has not been specifically
           tagged as a weak object symbol. When a weak defined symbol is
           linked with a normal defined symbol, the normal defined symbol is
           used with no error. When a weak undefined symbol is linked and the
           symbol is not defined, the value of the symbol is determined in a
           system-specific manner without error. On some systems, uppercase
           indicates that a default value has been specified.


       --defined-only
           Display only defined symbols for each object file.

<on-shared-stripped>

       -D
       --dynamic
           Display the dynamic symbols rather than the normal symbols.  This is
           only meaningful for dynamic objects, such as certain types of shared
           libraries.

note: this shows only dynamic symbols and shows a smaller set than running it
without option.

$ nm libcvms.so.1.0
nm: libcvms.so.1.0: no symbols

$ nm -D libcvms.so.1.0
000add7c T APUC_Decompress
000831f8 T AP_AM_AfterFormatProcessingHDD
00081e04 T AP_AM_BeforeFormatProcessingHDD
000821f4 T AP_AM_ChangeChasePbToNormalPb
...


<ex>
$ nm libhttp_curl.a | grep NDS_HTTP_strdup
279:         U NDS_HTTP_strdup
2169:         U NDS_HTTP_strdup
2170:00000000 T NDS_HTTP_strdup_DEBUG

$ nm libhttp_upset.a | grep NDS_HTTP_strdup
12:00000000 T NDS_HTTP_strdup


<mapfile>
Have statically linked application binary, can check if there is a symbol by
using nm. However, cannot tell from where this symbol. How?

Use map file for that application and this will show:

  Archive member included because of file (symbol)

  //BigEndian/release_dbg//components/media_services/asm/src/libasm.a(asm_version.o)
  //BigEndian/release_dbg/components/../processes/mw_process_main.o (ASM_GetVersionString)

  49629  .text.Curl_cookie_add
  49630                 0x000000000076cb40      0xb5c /FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION
    /build/picasso/AMS_BCM_MIPS4K_LNUX_DARWIN_01/BigEndian/release_dbg//components/misc_libraries/http/src/libhttp.a(cookie.o)

  49631                 0x000000000076cb40                Curl_cookie_add


={============================================================================
*kt_linux_tool_203* tool-objdump

gcc -S
objdump -d ELF > out.txt

note: difference between gcc -S and objdump? objdump do on objects after
complilation.

       -d
       --disassemble
           Display the assembler mnemonics for the machine instructions from
           objfile.  This option only disassembles those sections which are
           expected to contain instructions.

       -S
       --source
           Display source code intermixed with disassembly, if possible.
           Implies -d.

       -T
       --dynamic-syms
           Print the dynamic symbol table entries of the file.  This is only
           meaningful for dynamic objects, such as certain types of shared
           libraries.  This is similar to the information provided by the nm
           program when given the -D (--dynamic) option.

       -t
       --syms
           Print the symbol table entries of the file.  This is similar to the
           information provided by the nm program, although the display format
           is different.  The format of the output depends upon the format of
           the file being dumped, but there are two main types.  One looks
           like this:

       -f
       --file-headers
           Display summary information from the overall header of each of the
           objfile files.

       -R
       --dynamic-reloc
           Print the dynamic relocation entries of the file.  This is only
           meaningful for dynamic objects, such as certain types of shared
           libraries.  As for -r, if used with -d or -D, the relocations are
           printed interspersed with the disassembly.


       -l
       --line-numbers
           Label the display (`using debugging information`) with the filename
           and source line numbers corresponding to the object code or relocs
           shown.  Only useful with -d, -D, or -r.

          $ objdump -d -Mintel -l ml_reloc.o


$ objdump -f a.out_from_mips_asan

a.out_from_mips_asan:     file format elf32-big
architecture: UNKNOWN!, flags 0x00000112:
EXEC_P, HAS_SYMS, D_PAGED
start address 0x00400790

$ readelf -h !$
readelf -h a.out_from_mips_asan
ELF Header:
  Magic:   7f 45 4c 46 01 02 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, big endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           MIPS R3000
  Version:                           0x1
  Entry point address:               0x400790
  Start of program headers:          52 (bytes into file)
  Start of section headers:          8056 (bytes into file)
  Flags:                             0x1007, noreorder, pic, cpic, o32, mips1
  Size of this header:               52 (bytes)
  Size of program headers:           32 (bytes)
  Number of program headers:         9
  Size of section headers:           40 (bytes)
  Number of section headers:         38
  Section header string table index: 35

<ex>
shows a dump adding load address:

PT_LOAD header(3): type=1 offset=0 
vaddr=0x400000 paddr=0x400000 fsize=11297060 msize=11297060 flags=0x5

APP_Process:     file format elf32-tradbigmips

Disassembly of section .init:

0040bbd4 <_init>:
  40bbd4:	3c1c00b1 	lui	gp,0xb1
  40bbd8:	279c055c 	addiu	gp,gp,1372
  40bbdc:	0399e021 	addu	gp,gp,t9
  40bbe0:	27bdffe0 	addiu	sp,sp,-32
  40bbe4:	afbc0010 	sw	gp,16(sp)
  40bbe8:	afbf001c 	sw	ra,28(sp)
  40bbec:	afbc0018 	sw	gp,24(sp)
  40bbf0:	04110001 	bal	40bbf8 <_init+0x24>
  40bbf4:	00000000 	nop
  40bbf8:	0c103260 	jal	40c980 <frame_dummy>
  40bbfc:	00000000 	nop
  40bc00:	04110001 	bal	40bc08 <_init+0x34>
  40bc04:	00000000 	nop
  40bc08:	0c318660 	jal	c61980 <__do_global_ctors_aux>
  40bc0c:	00000000 	nop
  40bc10:	8fbf001c 	lw	ra,28(sp)
  40bc14:	03e00008 	jr	ra
  40bc18:	27bd0020 	addiu	sp,sp,32
Disassembly of section .plt:

0040bc20 <pthread_cond_signal@plt-0x20>:
  40bc20:	3c0f00f1 	lui	t7,0xf1
  40bc24:	8df94148 	lw	t9,16712(t7)


={============================================================================
*kt_linux_tool_202* tool-strip

http://man7.org/linux/man-pages/man1/strip.1.html

DESCRIPTION
       GNU strip discards all symbols from object files objfile.  The list of
       object files may include archives.  At least one object file must be
       given.

       strip modifies the files named in its argument, rather than writing
       modified copies under different names.

       -g
       -S
       -d
       --strip-debug
           Remove debugging symbols only.

<ex>
-rwxr-xr-x 1 kyoupark ccusers 1520616 Mar 27 08:52 bash         // after strip
-rwxr-xr-x 1 kyoupark ccusers 5905254 Mar 27 08:52 bash-org


={============================================================================
*kt_linux_tool_204* tool-addr2line

build/src/binutils-2.28/binutils/addr2line.c

http://man7.org/linux/man-pages/man1/addr2line.1.html

Usage: mipsel-linux-uclibc-addr2line [option(s)] [addr(s)]

Convert addresses into line number/file name pairs. If no addresses are
specified on the command line, they will be read from stdin The options are:

  @<file>                Read options from <file>
  -b --target=<bfdname>  Set the binary file format
  -i --inlines           Unwind inlined functions
  -j --section=<name>    Read section-relative offsets instead of addresses
  
  -e --exe=<executable>  Set the input file name (default is a.out)

  // not not use -e
  // i686-nptl-linux-gnu-addr2line: 'a.out': No such file

  -s --basenames         Strip directory names
  -f --functions         Show function names
  -C --demangle[=style]  Demangle function names

  -h --help              Display this information
  -v --version           Display the program's version

./mipsel-linux-uclibc-addr2line: supported targets: elf32-tradlittlemips
elf32-tradbigmips ecoff-littlemips ecoff-bigmips elf32-ntradlittlemips
elf64-tradlittlemips elf32-ntradbigmips elf64-tradbigmips elf64-little
elf64-big elf32-little elf32-big srec symbolsrec tekhex binary ihex Report
bugs to URL:http://www.sourceware.org/bugzilla/

       *trace-two-question-marks*
       If the file name or function name can not be determined, addr2line
       will print two question marks in their place.  If the line number can
       not be determined, addr2line will print 0.

mips-uclibc-addr2line -f -s -e fs/NDS/bin/MW_Process 5dd134 5dbe00 5d3ee4 5d6cb4 5ddb64 5cef3c a5f298   

~/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-addr2line -e lib/ld.so.1 0x2aab4160


# ============================================================================
#{ apache-ivy
={============================================================================
*kt_linux_tool_300* tool-apache-ivy

http://ant.apache.org/ivy/history/latest-milestone/index.html

What is Ivy?

Ivy is a tool for managing (recording, tracking, resolving and reporting)
project dependencies. It is characterized by the following:

    flexibility and configurability - Ivy is essentially process agnostic and
    is not tied to any methodology or structure. Instead it provides the
    necessary flexibility and configurability to be adapted to a broad range
    of dependency management and build processes.

    
    tight integration with Apache Ant - while available as a standalone tool,
Ivy works particularly well with Apache Ant providing a number of powerful Ant
  tasks ranging from dependency resolution to dependency reporting and
  publication.

Ivy is open source and released under a very permissive Apache License.

Ivy has a lot of powerful Features, the most popular and useful being its
flexibility, integration with ant, and its strong transitive dependencies
management engine.

The transitive dependencies management is a feature which lets you get
dependencies of your dependencies, transitively. In order to address this
general problem, ivy needs to find metadata about your modules, usually in an
ivy file. To find the metadata and your dependencies' artifacts (usually
    jars), Ivy can be configured to use a lot of different repositories.


http://ant.apache.org/ivy/history/latest-milestone/tutorial/start.html

Quick Start

In this tutorial, you will see one of the simplest ways to use Ivy. With no
specific settings, Ivy uses the maven 2 repository to resolve the dependencies
you declare in an Ivy file. Let's have a look at the content of the files
involved.

You'll find this tutorial's sources in the ivy distribution in the
src/example/hello-ivy directory.


The ivy.xml file

This file is used to describe the dependencies of the project on other
libraries. Here is the sample:

<ivy-module version="2.0">
    <info organisation="org.apache" module="hello-ivy"/>
    <dependencies>
        <dependency org="commons-lang" name="commons-lang" rev="2.0"/>
        <dependency org="commons-cli" name="commons-cli" rev="1.0"/>
    </dependencies>
</ivy-module>

The format of this file should pretty easy to understand, but let's give some
details about what is declared here. First, the root element ivy-module, with
the version attribute used to tell Ivy which version of Ivy this file uses.

Then there is an info tag, which is used to give information about the module
for which we are defining dependencies. Here we define only the organization
  and module name. You are free to choose whatever you want for them, but we
    recommend avoiding spaces for both.

Finally, the dependencies section lets you define dependencies. Here this
module `depends on two libraries`: commons-lang and commons-cli. 

As you can see, we use the `org` and `name` attributes to define the organization
and module name of the dependencies we need. The `rev` attribute is used to
specify the version of the module you depend on.

`To know what to put in these attributes`, you need to know the exact
information for the libraries you depend on. Ivy uses the maven 2 repository
by default, so we recommend you use mvnrepository.com to look for the module
you want. Once you find it, you will have the details on how to declare the
dependency in a maven POM. For instance:

<dependency>
    <groupId>commons-lang</groupId>
    <artifactId>commons-lang</artifactId>
    <version>2.0</version>
</dependency>

To convert this into an Ivy dependency declaration, all you have to do is use
the groupId as organization, the artifactId as module name, and the version as
revision. That's what we did for the dependencies in this tutorial, that is
commons-lang and commons-cli. 

Note that having commons-lang and commons-cli as organization is not the best
example of what the organization should be. It would be better to use
org.apache, org.apache.commons or org.apache.commons.lang. However, this is
how these modules are identified in the maven 2 repository, so the simplest
way to get them is to use the details as is (you will see in Building a
    repository that you can use namespaces to redefine these names if you want
    something cleaner).

If you want more details on what you can do in Ivy files, you can have a look
at the Ivy files reference documentation.


The build.xml file

The corresponding build file contains a set of targets, allowing you to
resolve dependencies declared in the Ivy file, to compile and run the sample
code, produce a report of dependency resolution, and clean the cache or the
project.  You can use the standard "ant -p" to get the list of available
targets. Feel free to have a look at the whole file, but here is the part
relevant to dependency resolution:

<project xmlns:ivy="antlib:org.apache.ivy.ant" name="hello-ivy" default="run">
    
    ...
    
    <!-- ================================= 
          target: resolve              
         ================================= -->
    <target name="resolve" description="--> retrieve dependencies with ivy">
        <ivy:retrieve />
    </target>
</project>


As you can see, it's very easy to call Ivy to resolve and retrieve
dependencies: all you need if Ivy is properly installed is to define an XML
namespace in your Ant file (xmlns:ivy="antlib:org.apache.ivy.ant"). Then all
the Ivy ant tasks will be available in this namespace.

Here we use only one task: the retrieve task. With no attributes, it will use
default settings and look for a file named ivy.xml for the dependency
definitions. That's exactly what we want, so we need nothing more than that.

Note that in this case we define a resolve target and call the retrieve task.
This may sound confusing, actually the retrieve task performs a resolve (which
    resolves dependencies and downloads them to a cache) followed by a
retrieve (a copy of those file to a local project directory). Check the How
does it work ? page for details about that.  


Running the project 

OK, now that we have seen the files involved, let's run the sample to see what
happens. Open a shell (or command line) window, and enter the hello-ivy
example directory.  Then, at the command prompt, run ant:


kyoupark@ukstbuild2:~/ivy-tut/ex/ant-ivy-b8b77a2$ ant
Buildfile: build.xml

resolve:
[ivy:retrieve] impossible to define new type: class not found: org.apache.ivy.plugins.signer.bouncycastle.OpenPGPSignatureGenerator in [] nor Ivy classloader
[ivy:retrieve] :: Apache Ivy 2.3.0-local-20131108183505 - 20131108183505 :: http://ant.apache.org/ivy/ ::
[ivy:retrieve] :: loading settings :: url = jar:file:/usr/share/java/ivy-2.3.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[ivy:retrieve] :: resolving dependencies :: org.apache#hello-ivy;working@ukstbuild2.uk.nds.com
[ivy:retrieve]  confs: [default]
[ivy:retrieve]  found commons-lang#commons-lang;2.0 in public
[ivy:retrieve]  found commons-cli#commons-cli;1.0 in public
[ivy:retrieve]  found commons-logging#commons-logging;1.0 in public
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-lang/commons-lang/2.0/commons-lang-2.0-javadoc.jar ...
[ivy:retrieve] ............................................
[ivy:retrieve] ................................................................. (467kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-lang#commons-lang;2.0!commons-lang.jar(javadoc) (2552ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-lang/commons-lang/2.0/commons-lang-2.0-sources.jar ...
[ivy:retrieve] ......................... (245kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-lang#commons-lang;2.0!commons-lang.jar(source) (762ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-lang/commons-lang/2.0/commons-lang-2.0.jar ...
[ivy:retrieve] ............... (165kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-lang#commons-lang;2.0!commons-lang.jar (31756ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-cli/commons-cli/1.0/commons-cli-1.0-javadoc.jar ...
[ivy:retrieve] ......... (92kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-cli#commons-cli;1.0!commons-cli.jar(javadoc) (585ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-cli/commons-cli/1.0/commons-cli-1.0.jar ...
[ivy:retrieve] .. (29kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-cli#commons-cli;1.0!commons-cli.jar (769ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-cli/commons-cli/1.0/commons-cli-1.0-sources.jar ...
[ivy:retrieve] ..... (48kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-cli#commons-cli;1.0!commons-cli.jar(source) (318ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-logging/commons-logging/1.0/commons-logging-1.0.jar ...
[ivy:retrieve] .. (21kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-logging#commons-logging;1.0!commons-logging.jar (506ms)
[ivy:retrieve] :: resolution report :: resolve 1852ms :: artifacts dl 37272ms
[ivy:retrieve]  :: evicted modules:
[ivy:retrieve]  commons-lang#commons-lang;1.0 by [commons-lang#commons-lang;2.0] in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   4   |   3   |   3   |   1   ||   7   |   7   |
        ---------------------------------------------------------------------
[ivy:retrieve] :: retrieving :: org.apache#hello-ivy
[ivy:retrieve]  confs: [default]
[ivy:retrieve]  7 artifacts copied, 0 already retrieved (1069kB/19ms)

run:
    [mkdir] Created dir: /home/NDS-UK/kyoupark/ivy-tut/ex/ant-ivy-b8b77a2/build
    [javac] Compiling 1 source file to /home/NDS-UK/kyoupark/ivy-tut/ex/ant-ivy-b8b77a2/build
     [java] standard message : hello ivy !
     [java] capitalized by org.apache.commons.lang.WordUtils : Hello Ivy !

BUILD SUCCESSFUL
Total time: 40 seconds
kyoupark@ukstbuild2:~/ivy-tut/ex/ant-ivy-b8b77a2$


What happened ?

Without any settings, Ivy retrieves files from the maven 2 repository. That's
what happened here.  

The resolve task has found the commons-lang and commons-cli modules in the
maven 2 repository, identified that commons-cli depends on commons-logging and
so resolved it as a transitive dependency. Then Ivy has downloaded all
corresponding `artifacts` in its cache (by default in your user home, in a
    .ivy2/cache directory). Finally, the retrieve task copies the resolved
jars from the ivy cache to the default library directory of the project: the
lib dir (you can change this easily by setting the pattern attribute on the
    retrieve task).

You might say that the task took a long time just to write out a "Hello Ivy!"
message. But remember that a lot of time was spent downloading the required
files from the web. Let's try to run it again:

kyoupark@ukstbuild2:~/ivy-tut/ex/ant-ivy-b8b77a2$ ant
Buildfile: build.xml

resolve:
[ivy:retrieve] impossible to define new type: class not found: org.apache.ivy.plugins.signer.bouncycastle.OpenPGPSignatureGenerator in [] nor Ivy classloader
[ivy:retrieve] :: Apache Ivy 2.3.0-local-20131108183505 - 20131108183505 :: http://ant.apache.org/ivy/ ::
[ivy:retrieve] :: loading settings :: url = jar:file:/usr/share/java/ivy-2.3.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[ivy:retrieve] :: resolving dependencies :: org.apache#hello-ivy;working@ukstbuild2.uk.nds.com
[ivy:retrieve]  confs: [default]
[ivy:retrieve]  found commons-lang#commons-lang;2.0 in public
[ivy:retrieve]  found commons-cli#commons-cli;1.0 in public
[ivy:retrieve]  found commons-logging#commons-logging;1.0 in public
[ivy:retrieve] :: resolution report :: resolve 208ms :: artifacts dl 15ms
[ivy:retrieve]  :: evicted modules:
[ivy:retrieve]  commons-lang#commons-lang;1.0 by [commons-lang#commons-lang;2.0] in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   4   |   0   |   0   |   1   ||   7   |   0   |
        ---------------------------------------------------------------------
[ivy:retrieve] :: retrieving :: org.apache#hello-ivy
[ivy:retrieve]  confs: [default]
[ivy:retrieve]  0 artifacts copied, 7 already retrieved (0kB/8ms)

run:
     [java] standard message : hello ivy !
     [java] capitalized by org.apache.commons.lang.WordUtils : Hello Ivy !

BUILD SUCCESSFUL
Total time: 1 second


Great! The cache was used, so no download was needed and the build was
instantaneous.

And now, if you want to generate a report detailing all the dependencies of
your module, you can call the report target, and check the generated file in
the build directory. You should obtain something looking like this.

As you can see, using Ivy to resolve dependencies stored in the maven 2
repository is extremely easy. Now you can go on with the next tutorials to
learn more about how to use module configurations which is a very powerful Ivy
specific feature. Other tutorials are also available where you will learn how
to use Ivy settings to leverage a possibly complex enterprise repository. It
may also be a good time to start reading the reference documentation, and
especially the introduction material which gives a good overview of Ivy. The
best practices page is also a must read to start thinking about how to use
Ant+Ivy to build a clean and robust build system. 


<ex>
<?xml version="1.0" encoding="UTF-8"?>
<ivy-module version="2.0">
  <info organisation="platforms" module="bin_BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01"/>
       <dependencies>
<dependency org="platforms" name="BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01" rev="BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01_16.9.2-h1.19p1_SI-9085">
<artifact name="BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01" ext="gz" type="gz"/>
        </dependency>
        </dependencies>
</ivy-module>


<term>
An artifact is a single file ready for delivery with the publication of a
module revision, as a product of development.

Compressed package formats are often preferred because they are easier to
manage, transfer and store. For the same reasons, only one or a few artifacts
per module are commonly used. However, artifacts can be of any file type and
any number of them can be declared in a single module.

In the Java world, common artifacts are Java archives or JAR files. In many
cases, each revision of a module publishes only one artifact (like
    jakarta-log4j-1.2.6.tar.gz, for instance), but some of them publish many
artifacts dependending on the use of the module (like apache-ant binary and
    source distributions in zip, gz and bz2 package formats, for instance).

Examples: ant-1.7.0-bin.zip, apache-ant-1.7.0-src.tar.gz


Type of an artifact

The artifact type is a category of a particular kind of artifact specimen. It
is a classification based on the intended purpose of an artifact or why it is
provided, not a category of packaging format or how the artifact is delivered.

Although the type of an artifact may (rather accidentally) imply its file
format, they are two different concepts. The artifact file name extension is
more closely associated with its format. For example, in the case of Java
archives the artifact type "jar" indicates that it is indeed a Java archive as
per the JAR File specification. The file name extension happens to be "jar" as
well. On the other hand, with source code distributions, the artifact type may
be "source" while the file name extensions vary from "tar.gz", "zip", "java",
   "c", or "xml" to pretty much anything. So, the type of an artifact is
     basically an abstract functional category to explain its purpose, while
     the artifact file name extension is a more concrete technical indication
     of its format and, of course, naming.

Defining appropriate artifact types for a module is up to its development
organisation. Common choices may include: "jar", "binary", "bin", "rc", "exe",
"dll", "source", "src", "config", "conf", "cfg", "doc", "api", "spec",
"manual", "man", "data", "var", "resource", "res", "sql", "schema", "deploy",
"install", "setup", "distrib", "distro", "distr", "dist", "bundle", etc.

Module descriptors are not really artifacts, but they are comparable to an
artifact type, i.e. "descriptor" (an ivy file or a Maven POM).

Electronic signatures or digests are not really artifacts themselves, but can
be found with them in repositories. They also are comparable to an artifact
type, i.e. "digest" (md5 or sha1).  

Artifact file name extension 

In some cases the artifact type already implies its file name extension, but
not always. More generic types may include several different file formats,
e.g.  documentation can contain tarballs, zip packages or any common document
  formats.

Examples: zip, tar, tar.gz, rar, jar, war, ear, txt, doc, xml, html


# ============================================================================
#{ gdb
={============================================================================
*kt_linux_gdb_000* gdb-how-works

https://eli.thegreenplace.net/2011/01/23/how-debuggers-work-part-1/

This is the first part in a series of articles on how debuggers work. I'm
still not sure how many articles the series will contain and what topics it
will cover, but I'm going to start with the basics.

In this part I'm going to present the main building block of a debugger's
implementation on Linux - the ptrace system call. All the code in this article
is developed on a 32-bit Ubuntu machine. Note that the code is very much
platform specific, although porting it to other platforms shouldn't be too
difficult.

Motivation

To understand where we're going, try to imagine what it takes for a debugger
to do its work. A debugger can start some process and debug it, or attach
itself to an existing process. It can single-step through the code, set
breakpoints and run to them, examine variable values and stack traces. Many
debuggers have advanced features such as executing expressions and calling
functions in the debbugged process's address space, and even changing the
process's code on-the-fly and watching the effects.

Although modern debuggers are complex beasts [1], it's surprising how simple
is the foundation on which they are built. Debuggers start with only a few
basic services provided by the operating system and the compiler/linker, all
the rest is just a simple matter of programming.

Linux debugging - ptrace

The Swiss army knife of Linux debuggers is the ptrace system call [2]. It's a
versatile and rather complex tool that allows one process to control the
execution of another and to peek and poke at its innards [3]. ptrace can take
a mid-sized book to explain fully, which is why I'm just going to focus on
some of its practical uses in examples.

Let's dive right in.

Stepping through the code of a process

I'm now going to develop an example of running a process in "traced" mode in
which we're going to single-step through its code - the machine code (assembly
    instructions) that's executed by the CPU. I'll show the example code in
parts, explaining each, and in the end of the article you will find a link to
download a complete C file that you can compile, execute and play with.

The high-level plan is to write code that splits into a child process that
will execute a user-supplied command, and a parent process that traces the
child. First, the main function:

int main(int argc, char** argv)
{
    pid_t child_pid;

    if (argc < 2) {
        fprintf(stderr, "Expected a program name as argument\n");
        return -1;
    }

    child_pid = fork();
    if (child_pid == 0)
        run_target(argv[1]);
    else if (child_pid > 0)
        run_debugger(child_pid);
    else {
        perror("fork");
        return -1;
    }

    return 0;
}

Pretty simple: we start a new child process with fork [4]. The if branch of
the subsequent condition runs the child process (called "target" here), and
the else if branch runs the parent process (called "debugger" here).

Here's the target process:

void run_target(const char* programname)
{
    procmsg("target started. will run '%s'\n", programname);

    /* Allow tracing of this process */
    if (ptrace(PTRACE_TRACEME, 0, 0, 0) < 0) {
        perror("ptrace");
        return;
    }

    /* Replace this process's image with the given program */
    execl(programname, programname, 0);
}

The most interesting line here is the ptrace call. ptrace is declared thus (in
    sys/ptrace.h):

long ptrace(enum __ptrace_request request, pid_t pid, void *addr, void *data);

The first argument is a request, which may be one of many predefined PTRACE_*
constants. The second argument specifies a process ID for some requests. The
third and fourth arguments are address and data pointers, for memory
manipulation. The ptrace call in the code snippet above makes the
PTRACE_TRACEME request, which means that this child process asks the OS kernel
to let its parent trace it. The request description from the man-page is quite
clear:

Indicates that this process is to be traced by its parent. Any signal (except
    SIGKILL) delivered to this process will cause it to stop and its parent to
be notified via wait(). Also, all subsequent calls to exec() by this process
will cause a SIGTRAP to be sent to it, giving the parent a chance to gain
control before the new program begins execution. A process probably shouldn't
make this request if its parent isn't expecting to trace it. (pid, addr, and
    data are ignored.) I've highlighted the part that interests us in this
example. Note that the very next thing run_target does after ptrace is invoke
the program given to it as an argument with execl. This, as the highlighted
part explains, causes the OS kernel to stop the process just before it begins
executing the program in execl and send a signal to the parent.

Thus, time is ripe to see what the parent does:

void run_debugger(pid_t child_pid)
{
    int wait_status;
    unsigned icounter = 0;
    procmsg("debugger started\n");

    /* Wait for child to stop on its first instruction */
    wait(&wait_status);

    while (WIFSTOPPED(wait_status)) {
        icounter++;
        /* Make the child execute another instruction */
        if (ptrace(PTRACE_SINGLESTEP, child_pid, 0, 0) < 0) {
            perror("ptrace");
            return;
        }

        /* Wait for child to stop on its next instruction */
        wait(&wait_status);
    }

    procmsg("the child executed %u instructions\n", icounter);
}

Recall from above that once the child starts executing the exec call, it will
stop and be sent the SIGTRAP signal. The parent here waits for this to happen
with the first wait call. wait will return once something interesting happens,
     and the parent checks that it was because the child was stopped
     (WIFSTOPPED returns true if the child process was stopped by delivery of
      a signal).

What the parent does next is the most interesting part of this article. It
invokes ptrace with the PTRACE_SINGLESTEP request giving it the child process
ID. What this does is tell the OS - please restart the child process, but stop
it after it executes the next instruction. Again, the parent waits for the
child to stop and the loop continues. The loop will terminate when the signal
that came out of the wait call wasn't about the child stopping. During a
normal run of the tracer, this will be the signal that tells the parent that
the child process exited (WIFEXITED would return true on it).

Note that icounter counts the amount of instructions executed by the child
process. So our simple example actually does something useful - given a
program name on the command line, it executes the program and reports the
amount of CPU instructions it took to run from start to finish. Let's see it
in action.

A test run

I compiled the following simple program and ran it under the tracer:

#include <stdio.h>

int main()
{
    printf("Hello, world!\n");
    return 0;
}

To my surprise, the tracer took quite long to run and reported that there were
more than 100,000 instructions executed. For a simple printf call? What gives?
The answer is very interesting [5]. By default, gcc on Linux links programs to
the C runtime libraries dynamically. What this means is that one of the first
things that runs when any program is executed is the dynamic library loader
that looks for the required shared libraries. This is quite a lot of code -
and remember that our basic tracer here looks at each and every instruction,
    not of just the main function, but of the whole process.

So, when I linked the test program with the -static flag (and verified that
    the executable gained some 500KB in weight, as is logical for a static
    link of the C runtime), the tracing reported only 7,000 instructions or
so. This is still a lot, but makes perfect sense if you recall that libc
initialization still has to run before main, and cleanup has to run after
main. Besides, printf is a complex function.

Still not satisfied, I wanted to see something testable - i.e. a whole run in
which I could account for every instruction executed. This, of course, can be
done with assembly code. So I took this version of "Hello, world!" and
assembled it:

// for 32 bits

section    .text
    ; The _start symbol must be declared for the linker (ld)
    global _start

_start:

    ; Prepare arguments for the sys_write system call:
    ;   - eax: system call number (sys_write)
    ;   - ebx: file descriptor (stdout)
    ;   - ecx: pointer to string
    ;   - edx: string length
    mov    edx, len
    mov    ecx, msg
    mov    ebx, 1
    mov    eax, 4

    ; Execute the sys_write system call
    int    0x80

    ; Execute sys_exit
    mov    eax, 1
    int    0x80

section   .data
msg db    'Hello, world!', 0xa
len equ    $ - msg

Sure enough. Now the tracer reported that 7 instructions were executed, which
is something I can easily verify.

Deep into the instruction stream

The assembly-written program allows me to introduce you to another powerful
use of ptrace - closely examining the state of the traced process. Here's
another version of the run_debugger function:

void run_debugger(pid_t child_pid)
{
    int wait_status;
    unsigned icounter = 0;
    procmsg("debugger started\n");

    /* Wait for child to stop on its first instruction */
    wait(&wait_status);

    while (WIFSTOPPED(wait_status)) {
        icounter++;
        struct user_regs_struct regs;
        ptrace(PTRACE_GETREGS, child_pid, 0, &regs);
        unsigned instr = ptrace(PTRACE_PEEKTEXT, child_pid, regs.eip, 0);

        procmsg("icounter = %u.  EIP = 0x%08x.  instr = 0x%08x\n",
                    icounter, regs.eip, instr);

        /* Make the child execute another instruction */
        if (ptrace(PTRACE_SINGLESTEP, child_pid, 0, 0) < 0) {
            perror("ptrace");
            return;
        }

        /* Wait for child to stop on its next instruction */
        wait(&wait_status);
    }

    procmsg("the child executed %u instructions\n", icounter);
}

The only difference is in the first few lines of the while loop. There are two
new ptrace calls. The first one reads the value of the process's registers
into a structure. user_regs_struct is defined in sys/user.h. Now here's the
fun part - if you look at this header file, a comment close to the top says:

/* The whole purpose of this file is for GDB and GDB only.
   Don't read too much into it. Don't use it for
   anything other than GDB unless know what you are
   doing.  */

Now, I don't know about you, but it makes me feel we're on the right track :-)
Anyway, back to the example. Once we have all the registers in regs, we can
peek at the current instruction of the process by calling ptrace with
PTRACE_PEEKTEXT, passing it regs.eip (the extended instruction pointer on x86)
  as the address. What we get back is the instruction [6]. Let's see this new
  tracer run on our assembly-coded snippet:

$ simple_tracer traced_helloworld
[5700] debugger started
[5701] target started. will run 'traced_helloworld'
[5700] icounter = 1.  EIP = 0x08048080.  instr = 0x00000eba
[5700] icounter = 2.  EIP = 0x08048085.  instr = 0x0490a0b9
[5700] icounter = 3.  EIP = 0x0804808a.  instr = 0x000001bb
[5700] icounter = 4.  EIP = 0x0804808f.  instr = 0x000004b8
[5700] icounter = 5.  EIP = 0x08048094.  instr = 0x01b880cd
Hello, world!
[5700] icounter = 6.  EIP = 0x08048096.  instr = 0x000001b8
[5700] icounter = 7.  EIP = 0x0804809b.  instr = 0x000080cd
[5700] the child executed 7 instructions

OK, so now in addition to icounter we also see the instruction pointer and the
instruction it points to at each step. How to verify this is correct? By using
objdump -d on the executable:

$ objdump -d traced_helloworld

traced_helloworld:     file format elf32-i386

Disassembly of section .text:

08048080 <.text>:
 8048080:     ba 0e 00 00 00          mov    $0xe,%edx
 8048085:     b9 a0 90 04 08          mov    $0x80490a0,%ecx
 804808a:     bb 01 00 00 00          mov    $0x1,%ebx
 804808f:     b8 04 00 00 00          mov    $0x4,%eax
 8048094:     cd 80                   int    $0x80
 8048096:     b8 01 00 00 00          mov    $0x1,%eax
 804809b:     cd 80                   int    $0x80

The correspondence between this and our tracing output is easily observed.

Attaching to a running process

As you know, debuggers can also attach to an already-running process. By now
you won't be surprised to find out that this is also done with ptrace, which
can get the PTRACE_ATTACH request. I won't show a code sample here since it
should be very easy to implement given the code we've already gone through.
For educational purposes, the approach taken here is more convenient (since we
    can stop the child process right at its start).

The code

The complete C source-code of the simple tracer presented in this article (the
    more advanced, instruction-printing version) is available here. It
compiles cleanly with -Wall -pedantic --std=c99 on version 4.4 of gcc.

Conclusion and next steps

Admittedly, this part didn't cover much - we're still far from having a real
debugger in our hands. However, I hope it has already made the process of
debugging at least a little less mysterious. ptrace is truly a versatile
system call with many abilities, of which we've sampled only a few so far.

Single-stepping through the code is useful, but only to a certain degree. Take
the C "Hello, world!" sample I demonstrated above. To get to main it would
probably take a couple of thousands of instructions of C runtime
initialization code to step through. This isn't very convenient. What we'd
ideally want to have is the ability to place a breakpoint at the entry to main
and step from there. Fair enough, and in the next part of the series I intend
to show how breakpoints are implemented.


={============================================================================
*kt_linux_gdb_300* gdb-debugging-info

https://eli.thegreenplace.net/2011/02/07/how-debuggers-work-part-3-debugging-information

How debuggers work: Part 3 - Debugging information

This is the third part in a series of articles on how debuggers work. Make
sure you read the first and the second parts before this one.

In this part I'm going to explain how the debugger figures out where to find
the C functions and variables in the machine code it wades through, and the
data it uses to map between C source code lines and machine language words.

Debugging information

Modern compilers do a pretty good job converting your high-level code, with
its nicely indented and nested control structures and arbitrarily typed
variables into a big pile of bits called machine code, the sole purpose of
which is to run as fast as possible on the target CPU. Most lines of C get
converted into several machine code instructions. Variables are shoved all
over the place - into the stack, into registers, or completely optimized away.
Structures and objects don't even exist in the resulting code - they're merely
an abstraction that gets translated to hard-coded offsets into memory buffers.

So how does a debugger know where to stop when you ask it to break at the
entry to some function? How does it manage to find what to show you when you
ask it for the value of a variable? The answer is - debugging information.

Debugging information is generated by the compiler together with the machine
code. It is a representation of the relationship between the executable
program and the original source code. This information is encoded into a
pre-defined format and stored alongside the machine code. Many such formats
were invented over the years for different platforms and executable files.
Since the aim of this article isn't to survey the history of these formats,
but rather to show how they work, we'll have to settle on something. This
  something is going to be DWARF, which is almost ubiquitously used today as
  the debugging information format for ELF executables on Linux and other
  Unix-y platforms.


The DWARF in the ELF

According to its Wikipedia page, DWARF was designed alongside ELF, although it
can in theory be embedded in other object file formats as well [1].

DWARF is a complex format, building on many years of experience with previous
formats for various architectures and operating systems. It has to be complex,
since it solves a very tricky problem - presenting debugging information from
  any high-level language to debuggers, providing support for arbitrary
  platforms and ABIs. It would take much more than this humble article to
  explain it fully, and to be honest I don't understand all its dark corners
  well enough to engage in such an endeavor anyway [2]. In this article I will
  take a more hands-on approach, showing just enough of DWARF to explain how
  debugging information works in practical terms.


Debug sections in ELF files

First let's take a glimpse of where the DWARF info is placed inside ELF files.
ELF defines arbitrary sections that may exist in each object file. A section
header table defines which sections exist and their names. Different tools
treat various sections in special ways - for example the linker is looking for
some sections, the debugger for others.

We'll be using an executable built from this C source for our experiments in
this article, compiled into tracedprog2:

#include <stdio.h>

void do_stuff(int my_arg)
{
    int my_local = my_arg + 2;
    int i;

    for (i = 0; i < my_local; ++i)
        printf("i = %d\n", i);
}


int main()
{
    do_stuff(2);
    return 0;
}


*tool-readelf-S*
Dumping the section headers from the ELF executable using objdump -h we'll
notice several sections with names beginning with .debug_ - these are the
DWARF debugging sections:

26 .debug_aranges 00000020  00000000  00000000  00001037
                 CONTENTS, READONLY, DEBUGGING
27 .debug_pubnames 00000028  00000000  00000000  00001057
                 CONTENTS, READONLY, DEBUGGING
28 .debug_info   000000cc  00000000  00000000  0000107f
                 CONTENTS, READONLY, DEBUGGING
29 .debug_abbrev 0000008a  00000000  00000000  0000114b
                 CONTENTS, READONLY, DEBUGGING
30 .debug_line   0000006b  00000000  00000000  000011d5
                 CONTENTS, READONLY, DEBUGGING
31 .debug_frame  00000044  00000000  00000000  00001240
                 CONTENTS, READONLY, DEBUGGING
32 .debug_str    000000ae  00000000  00000000  00001284
                 CONTENTS, READONLY, DEBUGGING
33 .debug_loc    00000058  00000000  00000000  00001332
                 CONTENTS, READONLY, DEBUGGING

The first number seen for each section here is its size, and the last is the
offset where it begins in the ELF file. The debugger uses this information to
read the section from the executable.

Now let's see a few practical examples of finding useful debug information in
DWARF.


Finding functions

One of the most basic things we want to do when debugging is placing
breakpoints at some function, expecting the debugger to break right at its
entrance. To be able to perform this feat, the debugger must have some mapping
between a function name in the high-level code and the address in the machine
code where the instructions for this function begin.

This information can be obtained from DWARF by looking at the .debug_info
section. Before we go further, a bit of background. The basic descriptive
entity in DWARF is called the Debugging Information Entry (DIE). Each DIE has
a tag - its type, and a set of attributes. DIEs are interlinked via sibling
and child links, and values of attributes can point at other DIEs.

Let's run:

*tool-readelf-debug* 
readelf --debug-dump=info 
objdump --dwarf=info tracedprog2

The output is quite long, and for this example we'll just focus on these lines
[3]:

<1><71>: Abbrev Number: 5 (DW_TAG_subprogram)
    <72>   DW_AT_external    : 1
    <73>   DW_AT_name        : (...): do_stuff
    <77>   DW_AT_decl_file   : 1
    <78>   DW_AT_decl_line   : 4
    <79>   DW_AT_prototyped  : 1
    <7a>   DW_AT_low_pc      : `0x8048604`
    <7e>   DW_AT_high_pc     : 0x804863e
    <82>   DW_AT_frame_base  : 0x0      (location list)
    <86>   DW_AT_sibling     : <0xb3>

<1><b3>: Abbrev Number: 9 (DW_TAG_subprogram)
    <b4>   DW_AT_external    : 1
    <b5>   DW_AT_name        : (...): main
    <b9>   DW_AT_decl_file   : 1
    <ba>   DW_AT_decl_line   : 14
    <bb>   DW_AT_type        : <0x4b>
    <bf>   DW_AT_low_pc      : 0x804863e
    <c3>   DW_AT_high_pc     : 0x804865a
    <c7>   DW_AT_frame_base  : 0x2c     (location list)

There are two entries (DIEs) tagged DW_TAG_subprogram, which is a function in
DWARF's jargon. Note that there's an entry for do_stuff and an entry for main.
There are several interesting attributes, but the one that interests us here
is DW_AT_low_pc. This is the program-counter (EIP in x86) value for the
beginning of the function. Note that it's 0x8048604 for do_stuff. Now let's
see what this address is in the disassembly of the executable by running
objdump -d:

`08048604` <do_stuff>:
 8048604:       55           push   ebp
 8048605:       89 e5        mov    ebp,esp
 8048607:       83 ec 28     sub    esp,0x28
 804860a:       8b 45 08     mov    eax,DWORD PTR [ebp+0x8]
 804860d:       83 c0 02     add    eax,0x2
 8048610:       89 45 f4     mov    DWORD PTR [ebp-0xc],eax
 8048613:       c7 45 (...)  mov    DWORD PTR [ebp-0x10],0x0
 804861a:       eb 18        jmp    8048634 <do_stuff+0x30>
 804861c:       b8 20 (...)  mov    eax,0x8048720
 8048621:       8b 55 f0     mov    edx,DWORD PTR [ebp-0x10]
 8048624:       89 54 24 04  mov    DWORD PTR [esp+0x4],edx
 8048628:       89 04 24     mov    DWORD PTR [esp],eax
 804862b:       e8 04 (...)  call   8048534 <printf@plt>
 8048630:       83 45 f0 01  add    DWORD PTR [ebp-0x10],0x1
 8048634:       8b 45 f0     mov    eax,DWORD PTR [ebp-0x10]
 8048637:       3b 45 f4     cmp    eax,DWORD PTR [ebp-0xc]
 804863a:       7c e0        jl     804861c <do_stuff+0x18>
 804863c:       c9           leave
 804863d:       c3           ret

Indeed, 0x8048604 is the beginning of do_stuff, so the debugger can have a
mapping between functions and their locations in the executable.

// -Mintel and on x86_64

0000000000400506 <do_stuff>:
  400506:       55                      push   rbp
  400507:       48 89 e5                mov    rbp,rsp
  40050a:       48 83 ec 20             sub    rsp,0x20
  40050e:       89 7d ec                mov    DWORD PTR [rbp-0x14],edi
  400511:       8b 45 ec                mov    eax,DWORD PTR [rbp-0x14]
  400514:       83 c0 02                add    eax,0x2
  400517:       89 45 f8                mov    DWORD PTR [rbp-0x8],eax
  40051a:       c7 45 fc 00 00 00 00    mov    DWORD PTR [rbp-0x4],0x0
  400521:       eb 18                   jmp    40053b <do_stuff+0x35>
  400523:       8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]
  400526:       89 c6                   mov    esi,eax
  400528:       bf e4 05 40 00          mov    edi,0x4005e4
  40052d:       b8 00 00 00 00          mov    eax,0x0
  400532:       e8 a9 fe ff ff          call   4003e0 <printf@plt>
  400537:       83 45 fc 01             add    DWORD PTR [rbp-0x4],0x1
  40053b:       8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]
  40053e:       3b 45 f8                cmp    eax,DWORD PTR [rbp-0x8]
  400541:       7c e0                   jl     400523 <do_stuff+0x1d>
  400543:       c9                      leave  
  400544:       c3                      ret    


Finding variables

Suppose that we've indeed stopped at a breakpoint inside do_stuff. We want to
ask the debugger to show us the value of the my_local variable. How does it
know where to find it? Turns out this is much trickier than finding functions.
Variables can be located in global storage, on the stack, and even in
registers. Additionally, variables with the same name can have different
values in different lexical scopes. The debugging information has to be able
to reflect all these variations, and indeed DWARF does.

I won't cover all the possibilities, but as an example I'll demonstrate how
the debugger can find my_local in do_stuff. Let's start at .debug_info and
look at the entry for do_stuff again, this time also looking at a couple of
its sub-entries:

<1><71>: Abbrev Number: 5 (DW_TAG_subprogram)
    <72>   DW_AT_external    : 1
    <73>   DW_AT_name        : (...): do_stuff
    <77>   DW_AT_decl_file   : 1
    <78>   DW_AT_decl_line   : 4
    <79>   DW_AT_prototyped  : 1
    <7a>   DW_AT_low_pc      : 0x8048604
    <7e>   DW_AT_high_pc     : 0x804863e
    <82>   DW_AT_frame_base  : 0x0      (location list)
    <86>   DW_AT_sibling     : <0xb3>
 <2><8a>: Abbrev Number: 6 (DW_TAG_formal_parameter)
    <8b>   DW_AT_name        : (...): my_arg
    <8f>   DW_AT_decl_file   : 1
    <90>   DW_AT_decl_line   : 4
    <91>   DW_AT_type        : <0x4b>
    <95>   DW_AT_location    : (...)       (DW_OP_fbreg: 0)
 <2><98>: Abbrev Number: 7 (DW_TAG_variable)
    <99>   DW_AT_name        : (...): my_local
    <9d>   DW_AT_decl_file   : 1
    <9e>   DW_AT_decl_line   : 6
    <9f>   DW_AT_type        : <0x4b>
    <a3>   DW_AT_location    : (...)      `(DW_OP_fbreg: -20)`
<2><a6>: Abbrev Number: 8 (DW_TAG_variable)
    <a7>   DW_AT_name        : i
    <a9>   DW_AT_decl_file   : 1
    <aa>   DW_AT_decl_line   : 7
    <ab>   DW_AT_type        : <0x4b>
    <af>   DW_AT_location    : (...)      (DW_OP_fbreg: -24)

Note the first number inside the angle brackets in each entry. This is the
`nesting level` - in this example entries with <2> are children of the entry
with <1>. So we know that the variable my_local (marked by the DW_TAG_variable
    tag) is a child of the do_stuff function. The debugger is also interested
in a variable's type to be able to display it correctly. In the case of
my_local the type points to another DIE - <0x4b>. If we look it up in the
output of objdump we'll see it's a signed 4-byte integer.

To actually locate the variable in the memory image of the executing process,
   the debugger will look at the DW_AT_location attribute. 
   
For my_local it says DW_OP_fbreg: -20. This means that the variable is stored
at offset -20 from the DW_AT_frame_base attribute of its containing function -
which is the base of the frame for the function.

The DW_AT_frame_base attribute of do_stuff has the value 0x0 (location list),
`which means that this value actually has to be looked up` in the location
list section. Let's look at it:

$ objdump --dwarf=loc tracedprog2

tracedprog2:     file format elf32-i386

Contents of the .debug_loc section:

    Offset   Begin    End      Expression
    00000000 08048604 08048605 (DW_OP_breg4: 4 )
    00000000 08048605 08048607 (DW_OP_breg4: 8 )
    00000000 08048607 0804863e (DW_OP_breg5: 8 )
    00000000 <End of list>
    0000002c 0804863e 0804863f (DW_OP_breg4: 4 )
    0000002c 0804863f 08048641 (DW_OP_breg4: 8 )
    0000002c 08048641 0804865a (DW_OP_breg5: 8 )
    0000002c <End of list>

The location information we're interested in is the first one [4]. For each
address where the debugger may be, it specifies the current frame base from
which offsets to variables are to be computed as an offset from a register.
For x86, bpreg4 refers to esp and bpreg5 refers to ebp.

// [4] Because the DW_AT_frame_base attribute of do_stuff contains offset
// 0x0 into the location list. Note that the same attribute for main contains
// the offset 0x2c which is the offset for the second set of location
// expressions.  
//
// <82>   DW_AT_frame_base  : 0x0      (location list)

void do_stuff(int my_arg)
{
    int my_local = my_arg + 2;
    ...
}

It's educational to look at the first several instructions of do_stuff again:

08048604 <do_stuff>:
 8048604:       55          push   ebp
 8048605:       89 e5       mov    ebp,esp
 8048607:       83 ec 28    sub    esp,0x28
 804860a:       8b 45 08    mov    eax,DWORD PTR [ebp+0x8]  // my_arg
 804860d:       83 c0 02    add    eax,0x2
 8048610:       89 45 f4    mov    DWORD PTR [ebp-0xc],eax

Note that ebp becomes relevant only after the second instruction is executed,
     and indeed for the first two addresses the base is computed from esp in
     the location information listed above. Once ebp is valid, it's convenient
     to compute offsets relative to it because it stays constant while esp
     keeps moving with data being pushed and popped from the stack.

So where does it leave us with my_local? We're only really interested in its
value after the instruction at 0x8048610 (where its value is placed in memory
    after being computed in eax), so the debugger will be using the
`DW_OP_breg5: 8` frame base to find it. Now it's time to rewind a little and
recall that the DW_AT_location attribute for my_local says DW_OP_fbreg: -20.

// not sure
Let's do the math: -20 from the frame base, which is ebp + 8. We get ebp - 12.
Now look at the disassembly again and note where the data is moved from eax -
indeed, ebp - 12 is where my_local is stored.


Looking up line numbers

When we talked about finding functions in the debugging information, I was
cheating a little. When we debug C source code and put a breakpoint in a
function, we're usually not interested in the first machine code instruction
[5]. What we're really interested in is the first C code line of the function.


// [5] Where the function prologue is usually executed and the local variables
// aren't even valid yet.

This is why DWARF encodes a full mapping between lines in the C source code
and machine code addresses in the executable. This information is contained in
the .debug_line section and can be extracted in a readable form as follows:

$ objdump --dwarf=decodedline tracedprog2

tracedprog2:     file format elf32-i386

Decoded dump of debug contents of section .debug_line:

CU: /home/eliben/eli/eliben-code/debugger/tracedprog2.c:
File name           Line number    Starting address
tracedprog2.c                5           0x8048604
tracedprog2.c                6           0x804860a
tracedprog2.c                9           0x8048613
tracedprog2.c               10           0x804861c
tracedprog2.c                9           0x8048630
tracedprog2.c               11           0x804863c
tracedprog2.c               15           0x804863e
tracedprog2.c               16           0x8048647
tracedprog2.c               17           0x8048653
tracedprog2.c               18           0x8048658

It shouldn't be hard to see the correspondence between this information, the C
source code and the disassembly dump. Line number 5 points at the entry point
to do_stuff - 0x8040604. The next line, 6, is where the debugger should really
stop when asked to break in do_stuff, and it points at 0x804860a which is just
past the prologue of the function. This line information easily allows
bi-directional mapping between lines and addresses:

When asked to place a breakpoint at a certain line, the debugger will use it
to find which address it should put its trap on (remember our friend int 3
    from the previous article?)

*seg-fault*
When an instruction causes a segmentation fault, the debugger will use it to
find the source code line on which it happened.


libdwarf - Working with DWARF programmatically

Employing command-line tools to access DWARF information, while useful, isn't
fully satisfying. As programmers, we'd like to know how to write actual code
that can read the format and extract what we need from it.

Naturally, one approach is to grab the DWARF specification and start hacking
away. Now, remember how everyone keeps saying that you should never, ever
parse HTML manually but rather use a library? Well, with DWARF it's even
worse. DWARF is much more complex than HTML. What I've shown here is just the
tip of the iceberg, and to make things even harder, most of this information
is encoded in a very compact and compressed way in the actual object file [6].

So we'll take another road and use a library to work with DWARF. There are two
major libraries I'm aware of (plus a few less complete ones):

BFD (libbfd) is used by the GNU binutils, including objdump which played a
star role in this article, ld (the GNU linker) and as (the GNU assembler).

libdwarf - which together with its big brother libelf are used for the tools
on Solaris and FreeBSD operating systems.

I'm picking libdwarf over BFD because it appears less arcane to me and its
license is more liberal (LGPL vs. GPL).

Since libdwarf is itself quite complex it requires a lot of code to operate.
I'm not going to show all this code here, but you can download and run it
yourself. To compile this file you'll need to have libelf and libdwarf
installed, and pass the -lelf and -ldwarf flags to the linker.

The demonstrated program takes an executable and prints the names of functions
in it, along with their entry points. Here's what it produces for the C
program we've been playing with in this article:

$ dwarf_get_func_addr tracedprog2
DW_TAG_subprogram: 'do_stuff'
low pc  : 0x08048604
high pc : 0x0804863e
DW_TAG_subprogram: 'main'
low pc  : 0x0804863e
high pc : 0x0804865a

The documentation of libdwarf (linked in the References section of this
    article) is quite good, and with some effort you should have no problem
pulling any other information demonstrated in this article from the DWARF
sections using it.

Conclusion and next steps

Debugging information is a simple concept in principle. The implementation
details may be intricate, but in the end of the day what matters is that we
now know how the debugger finds the information it needs about the original
source code from which the executable it's tracing was compiled. With this
information in hand, the debugger bridges between the world of the user, who
thinks in terms of lines of code and data structures, and the world of the
executable, which is just a bunch of machine code instructions and data in
registers and memory.

This article, with its two predecessors, concludes an introductory series that
explains the inner workings of a debugger. Using the information presented
here and some programming effort, it should be possible to create a basic but
functional debugger for Linux.

As for the next steps, I'm not sure yet. Maybe I'll end the series here, maybe
I'll present some advanced topics such as backtraces, and perhaps debugging on
Windows. Readers can also suggest ideas for future articles in this series or
related material. Feel free to use the comments or send me an email.

References

objdump man page

Wikipedia pages for ELF and DWARF.

Dwarf Debugging Standard home page - from here you can obtain the excellent
DWARF tutorial by Michael Eager, as well as the DWARF standard itself. You'll
probably want version 2 since it's what gcc produces.

libdwarf home page - the download package includes a comprehensive reference
document for the library

BFD documentation

note:
file command do not show difference between nodebug and debug binary.

<when-build-with-g>

$ readelf -S
  ...
  [31] .debug_aranges    MIPS_DWARF      0000 206cf0 008908 00      0   0  1 { ~
  [32] .debug_pubnames   MIPS_DWARF      0000 20f5f8 0c6fea 00      0   0  1
  [33] .debug_info       MIPS_DWARF      0000 2d65e2 1a07f7 00      0   0  1
  [34] .debug_abbrev     MIPS_DWARF      0000 476dd9 006741 00      0   0  1
  [35] .debug_line       MIPS_DWARF      0000 47d51a 039d68 00      0   0  1
  [36] .debug_frame      MIPS_DWARF      0000 4b7284 03f620 00      0   0  4
  [37] .debug_str        MIPS_DWARF      0000 4f68a4 14d3fc 01  MS  0   0  1
  [38] .debug_loc        MIPS_DWARF      0000 643ca0 05326e 00      0   0  1
  [39] .debug_ranges     MIPS_DWARF      0000 696f0e 013188 00      0   0  1 } ~
  ...


{tool-readelf-dump}
note: This is better since there will be 'no' output on release version.

Also this command line shows no output on nodebug version but shows all symbols
on debug version. 

$ readelf --debug-dump=decodedline nickelmediad 

--debug-dump[=rawline,=decodedline,=info,=abbrev,=pubnames,=aranges,=macro,
  =frames,=frames-interp,=str,=loc,=Ranges,=pubtypes,=trace_info,
  =trace_abbrev,=trace_aranges,=gdb_index]

   Displays the contents of the debug sections in the file, if any are
   present.  If one of the optional letters or words follows the switch then
   only data found in those specific sections will be dumped.

   Note that there is no single letter option to display the content of trace
   sections or .gdb_index.

   Note: the =decodedline option will display the interpreted contents of a
   .debug_line section whereas the =rawline option dumps the contents in a raw
   format.

   Note: the =frames-interp option will display the interpreted contents of a
   .debug_frame section whereas the =frames option dumps the contents in a raw
   format.

   Note: the output from the =info option can also be affected by the options
   --dwarf-depth and --dwarf-start.


{when-use-gdb}

# gdb /bin/nickelmediad
...
Reading symbols from /bin/nickelmediad...(no debugging symbols found)...done. ~
(gdb) 

for debug version.

# gdb /bin/nickelmediad
...
Reading symbols from /lds/Nickel.System.DBusServer/.libs/nickelmediad...done.
(gdb)


={============================================================================
*kt_linux_gdb_001* gdb-error gdb-target

# see this when uses cross gdb which built with different configuration.

(gdb) file a.out_from_mips_asan
warning: core file may not match specified executable file.
Reading symbols from /home/NDS-UK/kyoupark/a.out_from_mips_asan...\
  Dwarf Error: wrong version in compilation unit header (is 4, should be 2) [in module /home/NDS-UK/kyoupark/a.out_from_mips_asan]


={============================================================================
*kt_linux_gdb_300* gdb-term and links

{inferior}
GDB represents the state of each program execution with an object called an
'inferior'. 


{target}
info files shows your active targets.

{reference}
http://www.gnu.org/software/gdb/
https://sourceware.org/gdb/current/onlinedocs/gdb/
http://visualgdb.com/gdbreference/commands/sharedlibrary 


={============================================================================
*kt_linux_tool_300* gdb-sample session

1 A Sample gdb Session

You can use this manual at your leisure to read all about gdb. However, a
handful of commands are enough to get started using the debugger. This chapter
illustrates those commands.

One of the preliminary versions of gnu m4 (a generic macro processor) exhibits
the following bug: sometimes, when we change its quote strings from the default,
the commands used to capture one macro definition within another stop working.
  In the following short m4 session, we define a macro foo which expands to
  0000; we then use the m4 built-in defn to define bar as the same thing.
  However, when we change the open quote string to <QUOTE> and the close quote
  string to <UNQUOTE>, the same procedure fails to define a new synonym baz:

     $ cd gnu/m4
     $ ./m4
     define(foo,0000)
     
     foo
     0000
     define(bar,defn(`foo'))
     
     bar
     0000
     changequote(<QUOTE>,<UNQUOTE>)
     
     define(baz,defn(<QUOTE>foo<UNQUOTE>))
     baz
     Ctrl-d
     m4: End of input: 0: fatal error: EOF in string

Let us use gdb to try to see what is going on.

     $ gdb m4
     
     
     (gdb)

gdb reads only enough symbol data to know where to find the rest when needed; as
a result, the first prompt comes up very quickly. We now tell gdb to use a
narrower display width than usual, so that examples fit in this manual.

     (gdb) set width 70

We need to see how the m4 built-in changequote works. Having looked at the
source, we know the relevant subroutine is m4_changequote, so we set a
breakpoint there with the gdb break command.

     (gdb) break m4_changequote
     Breakpoint 1 at 0x62f4: file builtin.c, line 879.

Using the run command, we start m4 running under gdb control; as long as control
does not reach the m4_changequote subroutine, the program runs as usual:

     (gdb) run
     Starting program: /work/Editorial/gdb/gnu/m4/m4
     define(foo,0000)
     
     foo
     0000


<gdb-call-function>

To trigger the breakpoint, we 'call' changequote. gdb suspends execution of m4,
displaying information about the context where it stops.

     changequote(<QUOTE>,<UNQUOTE>)
     
     Breakpoint 1, m4_changequote (argc=3, argv=0x33c70)
         at builtin.c:879
     879         if (bad_argc(TOKEN_DATA_TEXT(argv[0]),argc,1,3))

Now we use the command n (next) to advance execution to the next line of the
current function.

     (gdb) n
     882         set_quotes((argc >= 2) ? TOKEN_DATA_TEXT(argv[1])\
      : nil,

set_quotes looks like a promising subroutine. We can go into it by using the
command s (step) instead of next. step goes to the next line to be executed in
any subroutine, so it steps into set_quotes.

     (gdb) s
     set_quotes (lq=0x34c78 "<QUOTE>", rq=0x34c88 "<UNQUOTE>")
         at input.c:530
     530         if (lquote != def_lquote)

The display that shows the subroutine where m4 is now suspended (and its
  arguments) is called a stack frame display. It shows a summary of the stack.
We can use the backtrace command (which can also be spelled bt), to see where we
are in the stack as a whole: the backtrace command displays a stack frame for
each active subroutine.

     (gdb) bt
     #0  set_quotes (lq=0x34c78 "<QUOTE>", rq=0x34c88 "<UNQUOTE>")
         at input.c:530
     #1  0x6344 in m4_changequote (argc=3, argv=0x33c70)
         at builtin.c:882
     #2  0x8174 in expand_macro (sym=0x33320) at macro.c:242
     #3  0x7a88 in expand_token (obs=0x0, t=209696, td=0xf7fffa30)
         at macro.c:71
     #4  0x79dc in expand_input () at macro.c:40
     #5  0x2930 in main (argc=0, argv=0xf7fffb20) at m4.c:195

We step through a few more lines to see what happens. The first two times, we
can use ‘s’; the next two times we use n to avoid falling into the xstrdup
subroutine.

     (gdb) s
     0x3b5c  532         if (rquote != def_rquote)
     (gdb) s
     0x3b80  535         lquote = (lq == nil || *lq == '\0') ?  \
     def_lquote : xstrdup(lq);
     (gdb) n
     536         rquote = (rq == nil || *rq == '\0') ? def_rquote\
      : xstrdup(rq);
     (gdb) n
     538         len_lquote = strlen(rquote);

note: looks odd since "len_lquote (left) = strlen(rquote) (right)

The last line displayed looks a little odd; we can examine the variables lquote
  and rquote to see if they are in fact the new left and right quotes we
  specified. We use the command p (print) to see their values.

     (gdb) p lquote
     $1 = 0x35d40 "<QUOTE>"
     (gdb) p rquote
     $2 = 0x35d50 "<UNQUOTE>"

lquote and rquote are indeed the new left and right quotes. To look at some
context, we can display ten lines of source surrounding the current line with
the l (list) command.

     (gdb) l
     533             xfree(rquote);
     534
     535         lquote = (lq == nil || *lq == '\0') ? def_lquote\
      : xstrdup (lq);
     536         rquote = (rq == nil || *rq == '\0') ? def_rquote\
      : xstrdup (rq);
     537
     538         len_lquote = strlen(rquote);
     539         len_rquote = strlen(lquote);
     540     }
     541
     542     void

Let us step past the two lines that set len_lquote and len_rquote, and then
examine the values of those variables.

     (gdb) n
     539         len_rquote = strlen(lquote);
     (gdb) n
     540     }
     (gdb) p len_lquote
     $3 = 9
     (gdb) p len_rquote
     $4 = 7

That certainly looks 'wrong', assuming len_lquote and len_rquote are meant to be
the lengths of lquote and rquote respectively. We can 'set' them to better
values using the p command, since it can print the value of any expression—and
that expression can include subroutine calls and assignments.

<gdb-set-var>

     (gdb) p len_lquote=strlen(lquote)
     $5 = 7
     (gdb) p len_rquote=strlen(rquote)
     $6 = 9

Is that enough to fix the problem of using the new quotes with the m4 built-in
defn? We can allow m4 to continue executing with the c (continue) command, and
then try the example that caused trouble initially:

     (gdb) c
     Continuing.
     
     define(baz,defn(<QUOTE>foo<UNQUOTE>))
     
     baz
     0000

Success! The new quotes now work just as well as the default ones. The problem
seems to have been just the two typos defining the wrong lengths. We allow m4
exit by giving it an EOF as input:

     Ctrl-d
     Program exited normally.

The message ‘Program exited normally.’ is from gdb; it indicates m4 has finished
executing. We can end our gdb session with the gdb quit command.

     (gdb) quit


={============================================================================
*kt_linux_tool_300* gdb-start

2 Getting In and Out of gdb

2.1 Invoking gdb

The most usual way to start gdb is with one argument, specifying an executable
program:

     gdb program

note
Since it reads any arguments other than options as see the first argument as
equivalent to the '-se' option and the second argument as equivalent to the
'-c'/'-p' option followed by that argument:

"gdb program" is the same as "gdb -se program"

-se file
Read symbol table from file 'file' and use it as the executable file.


You can also start with both an executable program and a core file specified:

     gdb program core

You can, instead, specify a process ID as a second argument, if you want to
debug a running process:

     gdb program 1234
     gdb -p 1234 program 
     gdb -p $(pidof w3cEngine) /opt/zinc-trunk/bin/w3cEngine

would 'attach' gdb to process 1234 (unless you also have a file named 1234; gdb
        does check for a core file first).


<gdb-args-options>

Taking advantage of the second command-line argument requires a fairly
complete operating system; when you use gdb as a remote debugger attached to a
bare board, there may not be any notion of “process”, and there is often no
way to get a core dump. gdb will warn you if it is unable to attach or to read
core dumps.

You can optionally have gdb pass any arguments after the executable file 'to'
the inferior using --args. This option 'stops' option processing.

     gdb --args gcc -O2 -c foo.c      // gdb --args <app> args

This will cause gdb to debug `gcc`, and to set gcc's command-line arguments (see
    Arguments) to '-O2 -c foo.c'.


<ex>
gdb --args /opt/zinc-trunk/oss/bin/gst-launch-1.0 $@
gdb --eval-command=run --args ${env_vars[@]} $prefix/bin/nickelmediad -b "Zinc.MediaProxy"


You can run gdb without printing the front material, which describes gdb's
non-warranty, by specifying --silent (or -q/--quiet):

     gdb --silent

You can further control how gdb starts up by using command-line options. gdb
itself can remind you of the options available.

Type

     gdb -help

to display all available options and briefly describe their use (‘gdb -h’ is a
    shorter equivalent).

All options and command line arguments you give are processed in sequential
order. The order makes a difference when the ‘-x’ option is used.


={============================================================================
*kt_linux_gdb_000* gdb-start-options

https://sourceware.org/gdb/current/onlinedocs/gdb/Invoking-GDB.html#Invoking-GDB

2 Getting In and Out of gdb

2.1 Invoking gdb

The most usual way to start gdb is with one argument, specifying an executable
program:

gdb program

You can also start with both an executable program and a core file specified:

gdb program core

You can, instead, specify a process ID as a second argument, if you want to debug a
running process:

*gdb-attach*
gdb program 1234

would attach gdb to process 1234

-pid number
-p number

Connect to process ID number, as with the attach command.


To pass any arguments after the executable file to the inferior using --args.
This option stops option processing.

gdb --args gcc -O2 -c foo.c

This will cause gdb to debug gcc, and to set gcc’s command-line arguments to
‘-O2 -c foo.c’.


2.1.1 Choosing Files

Many options have both long and short forms; both are shown in the following
list. gdb also recognizes the long forms if you truncate them, so long as enough
of the option is present to be unambiguous. 

If you prefer, you can flag option arguments with "--" rather than "-", though
we illustrate the more usual convention. 

2.1.2 Choosing Modes

-quiet
-silent
-q
“Quiet”. Do not print the introductory and copyright messages. These messages
are also suppressed in batch mode.


<gdb-eval-option>
-eval-command command
-ex command

Execute a 'single' gdb command. This option may be used multiple times to call
multiple commands. It may also be interleaved with '-command' as required.

gdb -eval-command=run --args /opt/zinc-trunk/oss/bin/gst-launch-1.0 $@


<gdb-command-file>
// works for target
-command file
-x file
    Execute commands from file file. The contents of this file is evaluated
    exactly as the `source command` would. See 23.1.3 Command Files

// not works for target version.
-init-command file
-ix file
    Execute commands from file file `before` loading the inferior (but after
        loading gdbinit files). See Startup.

-init-eval-command command
-iex command
    Execute a single gdb command `before` loading the inferior (but after
        loading gdbinit files). See Startup.


<gdb-start-order>
note:
All command line options and command line arguments you give are processed in
sequential order. The order makes a difference when the ‘-x’ option is used.

2.1.3 What gdb Does During Startup

Here’s the description of what gdb does during session startup:

1. Sets up the command interpreter as specified by the command line (see
    Section 2.1.2 [Mode Options], page 13).

2. Reads the system-wide init file (if ‘--with-system-gdbinit’ was used when
    building gdb; see Section C.6 [System-wide configuration and settings],
    page 608) and executes all the commands in that file.

3. Reads the init file (if any) in your home directory and executes all the
commands in that file.

4. Executes commands and command files specified by the ‘-iex’ and ‘-ix’
options in their specified order. Usually you should use the ‘-ex’ and ‘-x’
options instead, but this way you can apply settings before gdb init files get
executed and before inferior gets loaded.

5. Processes command line options and operands.

6. *init-from-current-working-directory* Reads and executes the commands from
init file (if any) in `the current working directory` as long as "set
auto-load local-gdbinit" is set to ‘on’ (see Section 22.7.1 [Init File in the
Current Directory], page 310). 

This is only done if the current directory is different from your home
directory.  Thus, you can have more than one init file, one generic in your
home directory, and another, specific to the program you are debugging, in the
directory where you invoke gdb.

7. *run-command-line* If the command line specified a program to debug, or a
process to attach to, or a core file, gdb loads any auto-loaded scripts
provided for the program or for its loaded shared libraries. See Section 22.7
[Auto-loading], page 308.  
If you wish to disable the auto-loading during
startup, you must do something like the following: 

$ gdb -iex "set auto-load python-scripts off" myprogram 

Option ‘-ex’ does not work because the auto-loading is then turned off too
late.

8. *run-command-file* Executes commands and command files specified by the
‘-ex’ and ‘-x’ options in their specified order. 

9. Reads the command history recorded in the history file. See Section 22.3
[Command History], page 304, for more details about the command history and
the files where gdb records it.


<gdb-set-search>
-directory directory
-d directory
    Add directory to the path to search `for source and script files.` 


={============================================================================
*kt_linux_gdb_000* gdb-command-file gdb-init-file

23.1.3 Command Files

A command file for gdb is a text file made of lines that are gdb commands.
Comments (lines starting with #) may also be included. 

An empty line in a command file does nothing; it does not mean to repeat the
last command, as it would from the terminal.

The lines in a command file are generally executed sequentially, unless the
order of execution is changed by one of the flow-control commands described
below. 

The commands are not printed as they are executed. An error in any command
terminates execution of the command file and control is returned to the
console.


<gdb-command-file-or-init-file>

However, the line would be fine in your local project directory’s startup
file, because the local startup file is read after the executable (and its
    symbol table) has been loaded. Note that this feature of GDB implies that
it is best to not put programming projects in your home directory, as you
would not be able to put project-specific infor- mation in .gdbinit.  You can
specify the startup file at the time you invoke GDB. For example, 

// z is text file
$ gdb -command=z x


<gdb-init-file>
Init files use the same syntax as command files and are processed by gdb in the
same way. The init file in your home directory can set options (such as 'set
    complaints') that affect subsequent processing of command line options and
operands. Init files are not executed if you use the ‘-nx’ option (see Choosing
    Modes).

To display the list of init files loaded by gdb at startup, you can use gdb
--help.

The gdb init files are normally called .gdbinit. 

Q: use HOME varaible?


// host version

$ gdb --help
...
At startup, GDB reads the following init files and executes their commands:
   * system-wide init file: /etc/gdb/gdbinit

For more information, type "help" from within GDB, or consult the
...

// target version

$ gdb --help
...
At startup, GDB reads the following init files and executes their commands:

For more information, type "help" from within GDB, or consult the
...

// target version when have gdbinit in the root dir. Show the same when cd into
// other directory and run gdb there.

At startup, GDB reads the following init files and executes their commands:
   * user-specific init file: //.gdbinit

For more information, type "help" from within GDB, or consult the


<ex>
can define user func that have commands to run

$ more .gdbinit
set history save on
set history filename ./.gdb_history
set output-radix 16

define connect
    handle SIG32 nostop noprint pass
    handle SIG33 nostop noprint pass
#    b CTL_SimpleZapperTestStep
#    b CTL_ChannelZapping_FullStbTearDown
#               b readSectionFilterDataAndWriteToFile
    b sectionFilterTask
                b CTL_SectionFilter_Engine.c:2126
                b CTL_SectionFilter_Engine.c:2146
    directory components/FOSH/FUSIONOS_XTV_TESTS/xtv_test/src/
                i b
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define init_task
   set $t=&init_task
   printf "task name \"%s\", pid %05d \n", $t->comm, t->pid
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define find_task
  # Addresses greater than _end: kernel data...
  # ...user passed in an address
  if ((unsigned)$arg0 > (unsigned)&_end)
    set $t=(struct task_struct *)$arg0
  else
    # User entered a numeric PID
    # Walk the task list to find it
    set $t=&init_task
    if (init_task.pid != (unsigned)$arg0)
      find_next_task $t
      while (&init_task!=$t && $t->pid != (unsigned)$arg0)
        find_next_task $t
      end
      if ($t == &init_task)
        printf "Couldn't find task; using init_task\n"
      end
    end
  end
  printf "Task \"%s\":\n", $t->comm
end


Reads and executes the commands from init file (if any) in the current working
directory. This is only done if the current directory is different from your
home directory. Thus, you can have more than one init file, one generic in your
home directory, and another, specific to the program you are debugging, in the
directory where you invoke gdb.


={============================================================================
*kt_linux_tool_300* gdb-start: repeatedly run a test in GDB, until it fails

HOWTO repeatedly run a test in GDB, until it fails

Those of you grappling with intermittent failures may find this useful. I hit a
test that was intermittently aborting in x86 debug build, due to use of a
"singular iterator". It was particularly awkward to reproduce in gdb, so I
wanted to script it such that gdb would only prompt in the event of failure, and
otherwise quit and allow the next iteration.

Here's what I ended-up with:

while gdb --eval-command=start --eval-command="b abort" \
  --eval-command=continue --eval-command=quit \
    --args ./dbusoutputmanagertest; do echo "OK"; done


={============================================================================
*kt_linux_tool_300* gdb-start: what do during startup

2.1.3 What gdb Does During Startup

Here's the description of what gdb does during session startup:

    Sets up the command interpreter as specified by the command line (see
        interpreter).  
    
    Reads the system-wide init file (if --with-system-gdbinit was used when
        building gdb; see System-wide configuration and settings) and executes
    all the commands in that file.

    Reads the init file (if any) in your home directory and executes all the
    commands in that file.

    Executes commands and command files specified by the ‘-iex’ and ‘-ix’
    options in their specified order. note: Usually you should use the '-ex' and
    '-x' options instead, but this way you can apply settings before gdb init
    files get executed and before inferior gets loaded.  
    
    Processes command line options and operands.

    Reads and executes the commands from init file (if any) in the current
    working directory as long as ‘set auto-load local-gdbinit’ is set to ‘on’
    (see Init File in the Current Directory). 
    
    note: This is only done if the current directory is different from your home
    directory. Thus, you can have more than one init file, one generic in your
    home directory, and another, specific to the program you are debugging, in
    the directory where you invoke gdb.  
  
    If the command line specified a program to debug, or a process to attach to,
    or a core file, gdb loads any auto-loaded scripts provided for the program or
    for its loaded shared libraries. See Auto-loading.

    Executes commands and command files specified by the ‘-ex’ and ‘-x’ options
      in their specified order.

    note: command-history
    Reads the command history recorded in the history file. See Command History,
          for more details about the command history and the files where gdb
            records it. 


={============================================================================
*kt_linux_gdb_300* gdb-start-specify-files

18 gdb Files

You may want to specify executable and core dump file names. 

<why-needed>
Occasionally it is necessary to change to a different file during a gdb session.
Or you may run gdb and forget to specify a file you want to use. 

Or you are debugging a remote target via gdbserver. In these situations the gdb
commands to specify new files are useful.

file filename

Use filename as the program to be debugged. Use the file command to get `both`
symbol table and program to run from the same file. 

It is read for its symbols and for the contents of pure memory. It is also the
program executed when you use the run command. If you do not specify a
directory and the file is not found in the gdb working directory, gdb uses the
environment variable PATH as a list of directories to search, just as the
shell does when looking for a program to run. You can change the value of this
variable, for both gdb and your program, using the path command.

You can load unlinked object ‘.o’ files into gdb using the file command. You
will not be able to “run” an object file, but you can disassemble functions and
inspect variables. 


file 

file with no argument makes gdb discard any information it has on both
executable file and the symbol table.

note: any real case when useful to load seperately symbol file or executable?

core-file [filename]
core 

Specify the whereabouts of a core dump file to be used as the "contents of
memory". core files contain only 'some' parts of the address space of the
process that generated them; gdb can access the executable file itself for other
parts.  

core-file 

with no argument specifies that no core file is to be used.


<gdb-info-command>
info files
info target

info files and info target are synonymous; both print the current target,
     including the names of the executable and core dump files currently in use
     by gdb, and the files from which symbols were loaded. The command "help
     target" lists all possible targets rather than current ones.


symbol-file [ -readnow ] filename
file [ -readnow ] filename

You can override the gdb two-stage strategy for reading symbol tables by using
the ‘-readnow’ option with any of the commands that load symbol table
information, if you want to be sure gdb has the entire symbol table available.


add-symbol-file filename address
add-symbol-file filename address [ -readnow ]
add-symbol-file filename address -s section address ...

The add-symbol-file command reads additional symbol table information
from the file filename. You would use this command when filename has been
dynamically loaded (by some other means) into the program that is running.

The address should give the memory address at which the file has been loaded;
gdb cannot figure this out for itself. You can additionally specify an
arbitrary number of ‘-s section address’ pairs, to give an explicit section
name and base address for that section. You can specify any address as an
expression.

The symbol table of the file filename is added to the symbol table originally
read with the symbol-file command. You can use the add-symbol-file command any
number of times; the new symbol data thus read is kept in addition to the old.

Changes can be reverted using the command remove-symbol-file.

Although filename is typically a shared library file, an executable file, or
some other object file which has been fully relocated for loading into a
process, you can also load symbolic information from relocatable ‘.o’ files,
  as long as:

  the file’s symbolic information refers only to linker symbols defined in
  that file, not to symbols defined by other object files,

  every section the file’s symbolic information refers to has actually been
  loaded into the inferior, as it appears in the file, and

  you can determine the address at which every section was loaded, and provide
  these to the add-symbol-file command.

Some embedded operating systems, like Sun Chorus and VxWorks, can load
relocatable files into an already running program; such systems typically make
the requirements above easy to meet. However, it’s important to recognize that
many native systems use complex link procedures (.linkonce section factoring
    and C++ constructor table assembly, for example) that make the
requirements difficult to meet. In general, one cannot assume that using
add-symbol-file to read a relocatable object file’s symbolic information will
have the same effect as linking the relocatable object file into the program
in the normal way.


={============================================================================
*kt_linux_gdb_300* gdb-start gdb-run gdb-set-arg

4 Running Programs Under gdb

When you run a program under gdb, you must first generate debugging information
when you compile it.

If you are doing native debugging, you may redirect your program's input and
output, debug an already running process, or kill a child process. 

*gdb-cpp* *gdb-define*

4.1 Compiling for Debugging

In order to debug a program effectively, you need to generate debugging
information when you compile it. This debugging information is stored in the
object file; it describes the data type of each variable or function and the
correspondence between source line numbers and addresses in the executable code.

To request debugging information, specify the ‘-g’ option when you run the
compiler.

gcc, the gnu C/C++ compiler, supports ‘-g’ with or without ‘-O’, making it
possible to debug optimized code. We recommend that you always use ‘-g’ whenever
you compile a program. You may think your program is correct, but there is no
sense in pushing your luck. For more information, see Optimized Code.

<gdb-cpp> macro-expansion
gdb knows about preprocessor macros and can show you their expansion (see
    Macros). Most compilers do not include information about preprocessor macros
in the debugging information if you specify the -g flag alone. 

Version 3.1 and later of gcc, the gnu C compiler, provides macro information
if you are using the DWARF debugging format, and specify the option -g3.

See Options for Debugging Your Program or GCC, for more information on gcc
options affecting debug information.

You will have the best debugging experience if you use the latest version of the
DWARF debugging format that your compiler supports. DWARF is currently the most
expressive and best supported debugging format in gdb. 

<gdb-run>
4.2 Starting your Program

run
r 
run args

Use the `run` command to start your program under gdb. You must first specify
the program name with an argument to gdb (see Getting In and Out of gdb), or by
using the `file` or exec-file command (see Commands to Specify Files). 

(gdb) help run
Start debugged program.  You `may specify arguments` to give it.
Args may include "*", or "[...]"; they are expanded using "sh".
Input and output redirection with ">", "<", or ">>" are also allowed.

With no arguments, uses arguments last specified (with "run" or "set args").
To cancel previous arguments and run with no arguments,
use "set args" without arguments.


If you are running your program in an execution environment that supports
processes, run 'creates' an 'inferior' process and makes that process run your
program.

then use `continue` to run your program. You may need load first (see load).

note: the load command is for remote debugging.


{information-to-inferior}
The execution of a program is affected by certain information it receives from
its 'superior'. gdb provides ways to specify this information, which you must do
'before' starting your program. 

You can change it after starting your program, but such changes only affect your
program the 'next' time you start it.

This information may be divided into four categories: 

arguments, environment, working directory, standard input and output.


<gdb-set-args>
4.3 Your Program's Arguments

The arguments to your program can be specified by the arguments of the run
command. They are passed to a 'shell', which expands wildcard characters and
performs redirection of I/O, and thence to your program. 

note: gdb runs your program via a shell

run with no arguments uses the same arguments used by the previous run, or those
set by the set args command.

Specify the arguments to be used the 'next' time your program is run. If set
args has no arguments, run executes your program with no arguments. Once you
have run your program with arguments, using `set args` before the next run is
the only way to run it again without arguments.

show args 

Show the arguments to give your program when it is started.

note: don't need to care about \"..." as below.

(gdb) set args -b Zinc.MediaProxy 
(gdb) show args
Argument list to give program being debugged when it is started 
  is "-b Zinc.MediaProxy".


<gdb-setenv>
4.4 Your Program's Environment

Usually you set up environment variables with the shell and they are inherited
by all the other programs you run. When debugging, it can be useful to try
running your program with a modified environment 'without' having to start gdb
over again.

show paths

Display the list of search paths for executables (the PATH environment
    variable).

show environment [varname]

Print the value of environment variable varname to be given to your program when
it starts. If you do not supply varname, print the names and values of 'all'
environment variables to be given to your program. You can abbreviate
environment as env.

(gdb) show environment LD_LIBRARY_PATH
LD_LIBRARY_PATH = .

set environment varname [=value]

Set environment variable varname to value. The value changes for your program
(and the shell gdb uses to launch it), not for gdb itself. The value parameter
is optional; if it is eliminated, the variable is set to a null value.

unset environment varname

Remove variable varname from the environment to be passed to your program.

<ex>
// can set env either before running gdb or after running gdb by using above
// command.
gst-run.sh souphttpsrc location="http://www.bbc.co.uk/mediaselector/playlists/hls/hdtv/ak/bbc1.m3u8" ! hlsdemux ! tsnexusbin

# gst-run.sh
#!/bin/bash

export LD_LIBRARY_PATH=/lib:/usr/local/lib:/opt/zinc-trunk/oss/lib/gstreamer-1.0:/opt/zinc-trunk/oss/lib:/opt/zinc-trunk/oss/lib/python2.6:$LD_LIBRARY_PATH
export GST_PLUGIN_PATH=/opt/zinc-trunk/oss/lib/gstreamer-1.0
export LD_PRELOAD=/usr/local/lib/libdirectfb.so:/usr/local/lib/libdirect.so:/usr/local/lib/libinit.so

gdb --eval-command=run --args /opt/zinc-trunk/oss/bin/gst-launch-1.0 $@

<ex> gdb-run
gdb --eval-command=run --args ${env_vars[@]} /bin/nickelmediad -b "Zinc.MediaProxy"


4.5 Your Program's Working Directory

Each time you start your program with run, it inherits its working directory
from the current working directory of gdb. The gdb working directory is
initially whatever it inherited from its 'parent' process, typically the shell,
          but you can specify a new working directory in gdb with the cd
          command.

The gdb working directory also serves as a 'default' for the 'commands' that
specify files for gdb to operate on. See Commands to Specify Files.

cd [directory]
    Set the gdb working directory to directory. If not given, directory uses
    '~'.

pwd
    Print the gdb working directory. 

note:
It is generally impossible to find the current working directory of the process
being debugged (since a program can change its directory during its run). If you
work on a system where gdb is configured with the /proc support, you can use the
`info proc` command (see SVR4 Process Information) to find out the current
working directory of the debuggee. 


When you issue the `run` command, your program begins to 'execute' immediately.
See Stopping and Continuing, for discussion of how to arrange for your program
to stop. Once your program has stopped, you may call functions in your program,
   using the `print` or `call` commands. See Examining Data.

If the modification time of your symbol file has changed since the last time gdb
read its symbols, gdb discards its symbol table, and reads it again. When it
does this, gdb tries to retain your current breakpoints. 


start 

The name of the main procedure can vary from language to language. With C or
C++, the main procedure name is always main, but other languages such as Ada do
not require a specific name for their main procedure. 

The debugger provides a convenient way to start the execution of the program and
to 'stop' at the beginning of the main procedure, depending on the language
used. The `start` command does the equivalent of setting a temporary breakpoint
at the beginning of the main procedure and then invoking the run command.

Specify the arguments to give to your program as arguments to the 'start'
command.

note: Sams as with run, but stop as at the main.


={============================================================================
*kt_linux_gdb_300* gdb-start-tty gdb-tty

4.6 Your Program's Input and Output

// Can use this to get output in sandboxed debuggee?

Your program normally uses the 'same' device for standard input and standard
output as gdb is using.  You can redirect input and output in the `run` command
line, or you can use the tty command to set a different device for your program.
See Section 4.6 [Your Program's Input and Output], page 32.

note: Warning: While input and output redirection work, you cannot use pipes to
pass the output of the program you are debugging to another program; if you
attempt this, gdb is likely to wind up debugging the wrong program.

By default, the program you run under gdb does input and output to the same
terminal that gdb uses. gdb switches the terminal to its own terminal modes to
interact with you, but it records the terminal modes your program was using and
switches back to them when you continue running your program.

run > outfile

starts your program, diverting its output to the file 'outfile'.

Another way to specify where your program should do input and output is with the
tty command.

An explicit redirection in run overrides the tty command's effect on the
input/output device, but 'not' its effect on the controlling terminal.

When you use the tty command or redirect input in the run command, only the
input for your program is affected. The input for gdb still comes from your
terminal. tty is an alias for set inferior-tty.

set inferior-tty /dev/ttyb

Set the tty for the program being debugged to /dev/ttyb.

show inferior-tty

Show the current tty for the program being debugged.

<ex>

// opens up a new terminal and run tty command to find out ID and use it in
// gdb.

So start up GDB, but there is one extra thing we must do, related to that last
point. We must tell GDB to have the program execute in a different termi- nal
window than the one that GDB is running in. We can do that with GDB’s tty
command. First, we go to another window, in which the program I/O will be
done, and run the Unix tty command there to determine the ID for that window.
In this case, the output of that command tells us that the win- dow is
terminal number /dev/pts/8, so we type

(gdb) tty /dev/pts/8

in the GDB window. From now on, all keyboard input and screen output for the
program will be in the execution window.  One last thing before we start: We
must type something like

sleep 10000

in the execution window, so that our keyboard input to that window will go to
the program, rather than to the shell.

NOTE There are other ways we could handle the problem of separating GDB output
from the program’s output. For instance, we could start the program’s
execution first, then fire up GDB in another window, attaching it to the
running program.


={============================================================================
*kt_linux_gdb_000* gdb-attach-debug already running process

4.7 Debugging an Already-running Process

attach process-id

This command attaches to a running process—one that was started outside gdb.

info files 

shows your active targets.

note: You must also have permission to send the process a signal.

The first thing gdb does after arranging to debug the specified process is to
'stop' it. You can examine and modify an attached process with all the gdb
commands that are ordinarily available when you start processes with `run`. You
can insert breakpoints; you can step and continue; you can modify storage. If
you would rather the process continue running, you may use the continue
'command' after attaching gdb to the process.

detach 

When you have finished debugging the attached process, you can use the detach
command to release it from gdb control. Detaching the process continues its
execution. After the detach command, that process and gdb become completely
independent once more, and you are ready to attach another process or start one
with run.

$ gdb gst-launch-1.0 11476


={============================================================================
*kt_linux_tool_300* gdb-thread-multiple threads

4.10 Debugging Programs with Multiple Threads

note: These facilities are not yet available on every gdb configuration where
the operating system supports threads. If your gdb does not support threads,
these commands have no effect. For example, a system without thread support
  shows no output from 'info threads', and always rejects the thread command,
like this:

(gdb) info threads
(gdb) thread 1
Thread ID 1 not known. Use the "info threads" command to
see the IDs of currently known threads.


<current-thread>
One thread in particular is always the focus of debugging. This thread is called
the current thread. Debugging commands show program information from the
perspective of the current thread.


For debugging purposes, gdb associates its own thread number

info threads [id...]

Display a summary of all threads currently in your program. Optional argument
id... is one or more thread ids separated by spaces, and means to print
information only about the specified thread or threads. gdb displays for each
thread (in this order):

1. the thread number assigned by gdb
2. the target system's thread identifier (systag)
3. the thread's name, if one is known. A thread can either be named by the user
(see thread name, below), or, in some cases, by the program itself.
4. the current stack frame summary for that thread

An asterisk ‘*’ to the left of the gdb thread number indicates the current
thread.

For example,

(gdb) info threads
  Id Target Id        Frame
  3 process 35 thread 27 0x34e5 in sigpause ()
  2 process 35 thread 23 0x34e5 in sigpause ()
* 1 process 35 thread 13 main (argc=1, argv=0x7ffffff8)
at threadtest.c:68


thread threadno

Make thread number threadno the current thread.

thread apply [threadno | all] command

The thread apply command allows you to apply the named command to one or more
threads. Specify the numbers of the threads that you want affected with the
command argument threadno. It can be a single thread number, one of the numbers
shown in the first field of the 'info threads' display; or it could be a range
of thread numbers, as in 2-4. To apply a command to all threads, type thread
apply all command.

<ex>
thread apply all bt full


={============================================================================
*kt_linux_tool_300* gdb-thread: tid on ps and gdb

[root@HUMAX /]# ps -eAL -o 'tid command,wchan' | grep -E 'w3'
 7239 /opt/zinc-trunk/bin/w3cEngi fusion_core_wq_wait
 7269 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7270 /opt/zinc-trunk/bin/w3cEngi fusion_core_wq_wait
 7271 /opt/zinc-trunk/bin/w3cEngi BKNI_WaitForEvent_tagged
 7272 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7273 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7274 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7275 /opt/zinc-trunk/bin/w3cEngi poll_schedule_timeout
 7276 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7277 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7278 /opt/zinc-trunk/bin/w3cEngi poll_schedule_timeout
 7279 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7280 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7285 /opt/zinc-trunk/bin/w3cEngi poll_schedule_timeout
 7286 /opt/zinc-trunk/bin/w3cEngi poll_schedule_timeout
 7288 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7290 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7291 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7292 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7303 grep -E w3                  pipe_wait


(gdb) i th
  19 Thread 0x2fc71520 (LWP 7269)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  18 Thread 0x3067e520 (LWP 7270)  0x2df6c5bc in read () from /lib/libpthread.so.0
  17 Thread 0x4068e520 (LWP 7271)  0x2df8e11c in ioctl () from /lib/libc.so.0
  16 Thread 0x4069e520 (LWP 7272)  0x2df68560 in pthread_cond_timedwait () from /lib/libpthread.so.0
  15 Thread 0x406ae520 (LWP 7273)  0x2df68560 in pthread_cond_timedwait () from /lib/libpthread.so.0
  14 Thread 0x40f2a520 (LWP 7274)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  13 Thread 0x41a71520 (LWP 7275)  0x2df8ed04 in poll () from /lib/libc.so.0
  12 Thread 0x42341520 (LWP 7276)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  11 Thread 0x42b41520 (LWP 7277)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  10 Thread 0x43341520 (LWP 7278)  0x2df8ed04 in poll () from /lib/libc.so.0
  9 Thread 0x43b41520 (LWP 7279)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  8 Thread 0x44341520 (LWP 7280)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  7 Thread 0x453ab520 (LWP 7285)  0x2df8ed04 in poll () from /lib/libc.so.0
  6 Thread 0x45f77520 (LWP 7286)  0x2df8ed04 in poll () from /lib/libc.so.0
  5 Thread 0x47213520 (LWP 7288)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  4 Thread 0x48935520 (LWP 7290)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  3 Thread 0x49135520 (LWP 7291)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  2 Thread 0x49f17520 (LWP 7292)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
* 1 Thread 0x2aab0000 (LWP 7239)  0x2df8e11c in ioctl () from /lib/libc.so.0


={============================================================================
*kt_linux_gdb_300* gdb-log

2.4 Logging Output

You may want to save the output of gdb commands to a file. There are several
commands to control gdb's logging.

set logging on
    Enable logging.

set logging off
    Disable logging.

set logging file file
    Change the name of the current logfile. `The default logfile is gdb.txt.`

set logging overwrite [on|off]
    By default, gdb will append to the logfile. Set overwrite if you want set
    logging on to 'overwrite' the logfile instead.

set logging redirect [on|off]
    By default, gdb output will go to both the terminal and the logfile. Set
    redirect if you want output to go 'only' to the log file.

show logging
    Show the current values of the logging settings. 


={============================================================================
*kt_linux_tool_300* gdb-shell

2.3 Shell Commands

If you need to execute occasional shell commands during your debugging session,
there is no need to leave or suspend gdb; you can just use the shell command. 

shell command-string
!command-string

Invoke a standard shell to execute command-string. Note that no space is needed
between ! and command-string.


={============================================================================
*kt_linux_tool_300* gdb-general and help

3 gdb Commands

{test-abbreviation}
You can test abbreviations by using them as arguments to the help command.

(gdb) help s
Step program until it reaches a different source line.
Argument N means do this N times (or till program stops for another reason).


{return}
A blank line as input to gdb (typing just RET) means to 'repeat' the previous
command. Certain commands (for example, run) will not repeat this way.

The list and x commands, when you repeat them with RET, construct new arguments
rather than repeating exactly as typed. This permits easy scanning of source or
memory.


{comment}
Any text from a # to the end of the line is a comment; it does nothing. This is
useful mainly in command files


{interrupt}
An interrupt (often Ctrl-c) does not exit from gdb, but rather terminates the
action of any gdb command that is in progress and returns to gdb command prompt.
It is safe to type the interrupt character at any time because gdb does not
allow it to take effect until a time when it is safe.

What if the program is running but you forgot to set breakpoints? You can hit
CTRL-C and that'll stop the program wherever it happens to be and return you to
a "(gdb)" prompt. At that point, you could set up a proper breakpoint somewhere
and continue to that breakpoint.


{help}
help class

Using one of the general help classes as an argument, you can get a list of the
individual commands in that class.

help command

With a command name as help argument, gdb displays a short paragraph on how to
use that command.

apropos args

The apropos command searches through all of the gdb commands, and their
documentation, for the regular expression specified in args. It prints out all
matches found.


={============================================================================
*kt_linux_gdb_300* gdb-info

To inquire about the state of your program, or the state of gdb itself.

<gdb-info>
This command (abbreviated i) is for describing the state of your program. For
example, you can show the arguments passed to a function with "info args", list
the registers currently in use with "info registers", or list the breakpoints
you have set with "info breakpoints". You can get a complete list of the info
sub-commands with help info.

info args
info registers
info breakpoints

<gdb-info-proc> gdb-proc
(gdb) help info proc
Show /proc process information about any running process.
Specify any process id, or use the program being debugged by default.

List of info proc subcommands:

info proc all -- List all available /proc info
info proc cmdline -- List command line arguments of the process
info proc cwd -- List current working directory of the process
info proc exe -- List absolute filename for executable of the process
info proc mappings -- List of mapped memory regions
info proc stat -- List process info from /proc/PID/stat
info proc status -- List process info from /proc/PID/status


(gdb) info inferior
  Num  Description       Executable
* 1    process 25854     /home/kyoupark/git/kb/asan/gbo_target_host_recover_asan_32
(gdb) p getpid()
$1 = 25854


{show}
In contrast to info, show is for describing the state of gdb itself. You can
change most of the things you can show, by using the related command set; 

for example, you can control what number system is used for displays with set
radix, or simply inquire which is currently in use with show radix. To display
all the settable parameters and their current values, you can use show with no
arguments; you may also use info set. Both commands produce the same display.


={============================================================================
*kt_linux_tool_300* gdb-sources, source path, and substitute

9 Examining Source Files

gdb can print parts of your program's source, since the debugging information
recorded in the program tells gdb what source files were used to build it.

To print lines from a source file, use the `list` command (abbreviated l). By
default, ten lines are printed.

list linenum
Print lines centered around line number linenum in the current source file.

list function
Print lines centered around the beginning of function function.

list 
Print more lines. If the last lines printed were printed with a list command,
this prints lines following the last lines printed; however, if the last line
  printed was a solitary line printed as part of displaying a stack frame (see
      Chapter 8 [Examining the Stack], page 91), this prints lines centered
  around that line.

note:
(gdb) i sources
Source files for which symbols have been read in:


{source-path}
9.5 Specifying Source Directories

Executable programs sometimes do not record the directories of the source files
from which they were compiled, just the names. Even when they do, the
directories could be moved between the compilation and your debugging session.
gdb has a list of directories to search for source files; this is called the
source path.

Each time gdb wants a source file, it tries all the directories in the list, in
the 'order' they are present in the list, until it finds a file with the desired
name.

For example, suppose an executable references the file
/usr/src/foo-1.0/lib/foo.c, and our source path is /mnt/cross. The file is first
looked up literally; if this fails, /mnt/cross/usr/src/foo-1.0/lib/foo.c is
tried; if this fails, /mnt/cross/foo.c is opened; if this fails, an error
message is printed.

Plain file names, relative file names with leading directories, file names
containing dots, etc. are all treated as described above.

Note that the executable search path is not used to locate the source files.

When you start gdb, its source path includes only `cdir` and `cwd`, in that
order. To add other directories, use the `directory` command.

The search path is used to find 'both' program source files and gdb script files
(read using the `-command` option and `source` command).


{substitution}
A substitution rule specifies how to rewrite source directories stored in the
program's debug information in case the sources were moved to a different
directory between compilation and debugging.

gdb does a simple string replacement of from with to at the start of the
directory part of the source file name, and uses that result instead of the
original file name to look up the sources.

a rule is applied only if the from part of the directory name 'ends' at a
directory separator and only at the 'beginning' of the directory name,

‘/mnt/cross’ will be applied to ‘/usr/source/foo-1.0’ but not to
‘/usr/sourceware/foo-2.0’ and not be applied to ‘/root/usr/source/baz.c’ either.

In many cases, you can achieve the same result using the directory command.
However, set substitute-path can be more efficient in the case where the sources
are organized in a complex tree with multiple subdirectories.

set substitute-path from to

Define a source path substitution rule, and add it at the end of the current
list of existing substitution rules. If a rule with the same from was already
defined, then the old rule is also deleted.

For example, if the file ‘/foo/bar/baz.c’ was moved to ‘/mnt/cross/baz.c’, then
the command

(gdb) set substitute-path /usr/src /mnt/cross

will tell gdb to replace ‘/usr/src’ with ‘/mnt/cross’, which will allow gdb to
find the file ‘baz.c’ even though it was moved.

show substitute-path [path]

If a path is specified, then print the source path substitution rule which would
rewrite that path, if any. If no path is specified, then print all existing
source path substitution rules.


<to-reset>
If your source path is cluttered with directories that are no longer of
interest, gdb may sometimes cause confusion by finding the wrong versions of
source. You can correct the situation as follows:

1. Use directory with no argument to reset the source path to its default value.
2. Use directory with suitable arguments to reinstall the directories you want
in the source path.  You can add all the directories in one command.


<todo>
You can configure a default source path substitution rule by configuring gdb
with the ‘--with-relocated-sources=dir’ option. The dir should be the name of a
directory under gdb’s configured prefix (set with ‘--prefix’ or
    ‘--exec-prefix’), and directory names in debug information under dir will be
adjusted automatically if the installed gdb is moved to a new location. This is
useful if gdb, libraries or executables with debug information and corresponding
source code are being moved together.


={============================================================================
*kt_linux_gdb_000* gdb-print-memory

10.6 Examining Memory

You can use the `command x` (for "examine") to examine memory in any of several
formats, independently of your program's data types.

x/nfu addr
x addr
x
    Use the x command to examine memory. 

n, f, and u are all `optional parameters` that specify how much memory to
display and how to format it; addr is an expression giving the address where
you want to start displaying memory. If you use defaults for nfu, you need not
type the slash ‘/’. Several commands set convenient defaults for addr.

n, the repeat count
    The repeat count is a decimal integer; the default is 1. It specifies how
    much memory (counting by units u) to display.

f, the display format
    The display format is one of the formats used by `print` (‘x’, ‘d’, ‘u’, ‘o’,
        ‘t’, ‘a’, ‘c’, ‘f’, ‘s’), 

    and in addition 'i' (for machine instructions). 

    The 'default' is 'x' (hexadecimal) initially. The default changes each time
    you use either x or print.

u, the unit size
    The unit size is any of

    b Bytes.
    h Halfwords (two bytes).
    w Words (four bytes). This is the initial 'default'.
    g Giant words (eight bytes). 

    Each time you specify a unit size with x, that size becomes the default unit
    the next time you use x. 
    
    For the ‘i’ format, the unit size is ignored and is normally not written. 
    
    For the ‘s’ format, the unit size defaults to ‘b’, unless it is explicitly
    given. Use x /hs to display 16-bit char strings and x /ws to display 32-bit
    strings. The next use of x /s will again display 8-bit strings. Note that
    the results depend on the programming language of the current compilation
    unit. If the language is C, the ‘s’ modifier will use the UTF-16 encoding
    while ‘w’ will use UTF-32. The encoding is set by the programming language
      and cannot be altered.

addr, starting display address

    addr is the address where you want gdb to begin displaying memory. The
    expression need not have a pointer value (though it may); it is always
    interpreted as an integer address of a byte of memory. See Expressions, for
    more information on expressions. 
    
    The 'default' for addr is usually just after the last address examined—but
    several other commands also set the default address: info breakpoints (to
        the address of the last breakpoint listed), info line (to the starting
          address of a line), and print (if you use it to display a value from
            memory). 

For example, ‘x/3uh 0x54320’ is a request to display three halfwords (h) of
memory, formatted as unsigned decimal integers (‘u’), starting at address
0x54320. ‘x/4xw $sp’ prints the four words (‘w’) of memory above the stack
pointer (here, ‘$sp’; see Registers) in hexadecimal (‘x’).

Since the letters indicating unit sizes are all distinct from the letters
specifying output formats, you do not have to remember whether unit size or
format comes first; either order works. The output specifications ‘4xw’ and
‘4wx’ mean exactly the same thing. (However, the count n must come first; ‘wx4’
    does not work.)

Even though the unit size u is ignored for the formats ‘s’ and ‘i’, you might
still want to use a count n; for example, ‘3i’ specifies that you want to see
three machine instructions, including any operands. For convenience, especially
when used with the display command, the ‘i’ format also prints branch delay slot
instructions, if any, beyond the count specified, which immediately follow the
last instruction that is within the count. The command disassemble gives an
alternative way of inspecting machine instructions; see Source and Machine Code.

All the defaults for the arguments to x are designed to make it easy to continue
scanning memory with minimal specifications each time you use x. For example,
after you have inspected three machine instructions with ‘x/3i addr’, you can
  inspect the next seven with just ‘x/7’. If you use <RET> to repeat the x
  command, the repeat count n is used again; the other arguments default as for
  successive uses of x.

When examining machine instructions, the instruction at current program counter
is shown with a => marker. For example:

     (gdb) x/5i $pc-6
        0x804837f <main+11>: mov    %esp,%ebp
        0x8048381 <main+13>: push   %ecx
        0x8048382 <main+14>: sub    $0x4,%esp
     => 0x8048385 <main+17>: movl   $0x8048460,(%esp)
        0x804838c <main+24>: call   0x80482d4 <puts@plt>

The addresses and contents printed by the x command are 'not' saved in the value
'history' because there is often too much of them and they would get in the way.
Instead, gdb makes these values available for subsequent use in expressions as
values of the convenience variables $_ and $__. After an x command, the last
address examined is available for use in expressions in the convenience variable
$_. The contents of that address, as examined, are available in the convenience
variable $__.

If the x command has a repeat count, the address and contents saved are from the
last memory unit printed; this is not the same as the last address printed if
several units were printed on the last line of output.

Most targets have an addressable memory unit size of 8 bits. This means that to
each memory address are associated 8 bits of data. Some targets, however, have
other addressable memory unit sizes. Within gdb and this document, the term
addressable memory unit (or memory unit for short) is used when explicitly
referring to a chunk of data of that size. The word byte is used to refer to a
chunk of data of 8 bits, regardless of the addressable memory unit size of the
target. For most systems, addressable memory unit is a synonym of byte.

When you are debugging a program running on a remote target machine (see Remote
    Debugging), you may wish to verify the program's image in the remote
machine's memory against the executable file you downloaded to the target. Or,
on any target, you may want to check whether the program has corrupted its own
  read-only sections. The compare-sections command is provided for such
  situations.

<ex>
--eval-command 'x/100a $sp' \
--eval-command 'x/50i $pc' \


={============================================================================
*kt_linux_gdb_300* gdb-print gdb-disp

10 Examining Data

10.1 Expressions

print and many other GDB commands accept `an expression` and compute its value.
Any kind of constant, variable or operator defined by the programming language
you are using is valid in an expression in GDB. This includes conditional
expressions, function calls, casts, and string constants. 

GDB supports array constants in expressions input by the user. The syntax is
{element, element…}. For example, you can use the command print {1, 2, 3} to
create an array of three integers. If you pass an array to a function or
assign it to a program variable, GDB copies the array to memory that is
malloced in the target program.

Because C is so widespread, most of the expressions shown in examples in this
manual are in C. See Using GDB with Different Languages, for information on
how to use expressions in other languages.

<ex>
(gdb) p {begin, end}
$4 = {0, 4}

10.5 Output Formats

By default, gdb prints a value according to its data type. Sometimes this is
not what you want. For example, you might want to print a number in hex, or a
pointer in decimal. Or you might want to view data in memory at a certain
address as a character string or as an instruction. To do these things,
specify an output format when you print a value.

The simplest use of output formats is to say how to print a value already
computed. This is done by starting the arguments of the print command with a
slash and a format letter. The format letters supported are:

<gdb-print-memory>
x
    Regard the bits of the value as an integer, and print the integer in
    hexadecimal.

d
    Print as integer in signed decimal.

u
    Print as integer in unsigned decimal.

o
    Print as integer in octal.

t
    Print as integer in binary. The letter ‘t’ stands for “two”. 1

a
    Print as an address, both absolute in hexadecimal and as an offset from
    the `nearest preceding symbol` You can use this format used `to discover`
    where (in what function) an unknown address is located:

              (gdb) p/a 0x54320
              $3 = 0x54320 <_initialize_vx+396>

    The command `info symbol 0x54320` yields similar results. See info symbol.

c
    Regard as an integer and print it as a character constant. This prints
    both the numerical value and its character representation. The character
    representation is replaced with the octal escape ‘\nnn’ for characters
    outside the 7-bit ascii range.

    Without this format, gdb displays char, unsigned char, and signed char
    data as character constants. Single-byte members of vectors are displayed
    as integer data.

f
    Regard the bits of the value as a floating point number and print using
    typical floating point syntax.

s
    Regard as a string, if possible. With this format, pointers to single-byte
    data are displayed as null-terminated strings and arrays of single-byte
    data are displayed as fixed-length strings. Other values are displayed in
    their natural types.

    Without this format, gdb displays pointers to and arrays of char, unsigned
    char, and signed char as strings. Single-byte members of a vector are
    displayed as an integer array.

z
    Like ‘x’ formatting, the value is treated as an integer and printed as
    hexadecimal, but leading zeros are printed to pad the value to the size of
    the integer type.

r
    Print using the ‘raw’ formatting. By default, gdb will use a Python-based
    pretty-printer, if one is available (see Pretty Printing). This typically
    results in a higher-level display of the value's contents. The ‘r’ format
    bypasses any Python pretty-printer which might exist. 

For example, to print the program counter in hex (see Registers), type

     p/x $pc

Note that no space is required before the slash; this is because command names
in gdb cannot contain a slash.

To reprint the last value in the value history with a different format, you
can use the print command with just a format and no expression. For example,
‘p/x’ reprints the last value in hex. 


(gdb) printf "%d\n", i
40
(gdb) printf "%08X\n", i
00000028

print i
Print the value of variable i.

print *p
Print the contents of memory pointed to by p, where p is a pointer variable.

(gdb) whatis pszSection
type = uint32_t *
(gdb) print *pszSection

print x.field
Check the different members of a structure.

print x
Check all the members of a structure, assuming x is a structure.

print y-field
y is a pointer to a structure.

print array[i]
Print the i'th element of array.

print array
Print all the elements of array.

(gdb) print /x block1->magic
$5 = 0xabeaa5b3

(gdb) print /x block1 
$9 = 0x1150f4c

(gdb) print /x *block1
$8 = {magic = 0xabeaa5b3, size = 0x28, line = 0x0, owner = 0x0, header = 0x21, data = {free = {previous = 0x8, next = 0x115106c}, 
    userData = {0x0}}}

# db_contexts_array is a global var
(gdb) p db_contexts_array
(gdb) p db_contexts_array[-1]


<gdb-disp>
This command tells GDB to automatically print the specified item each time
there is a pause in execution (due to a breakpoint, the next or step commands,
and so on):

(gdb) [disp]lay var
(gdb) [undisp] disp_num
(gdb) info disp 	" to list disps


<gdb-set> <gdb-print-expression>
can 'set' them to better values using the p command, since it can print the
value of any expression that expression can include subroutine calls and
assignments. set new value and continue to see if it fixes the bug.

(gdb) p len lquote=strlen(lquote)
$5 = 7
(gdb) p len rquote=strlen(rquote)
$6 = 9
(gdb) c
Continuing.


<gdb-print-array>
What if the array had been created dynamically? Can solve this problem by
creating an artificial array.

x = (int *) malloc(25*sizeof(int));

As you can see, the general form is

*pointer@number_of_elements

GDB also allows casts to be used when appropriate, for example,

(gdb) p (int [25]) *x
$2 = {0, 0, 0, 12, 0 <repeats 21 times>}


<gdb-print-pretty>
set print pretty on

Cause gdb to print structures in an indented format with one member per line,
like this:

$1 = {
  next = 0x0,
  flags = {
    sweet = 1,
    sour = 1
  },
  meat = 0x54 "Pork"
}


={============================================================================
*kt_linux_gdb_300* gdb-exam-address-and-code

https://sourceware.org/gdb/current/onlinedocs/gdb/Machine-Code.html#Machine-Code

9.6 Source and Machine Code

You can use the command `info line` to map source lines to program addresses
(and vice versa), and the command `disassemble` to display a range of
addresses as machine instructions.

info line linespec

Print the starting and ending addresses of the compiled code for source line
linespec. You can specify source lines in any of the ways.

For example, we can use info line to discover the location of the object code
for the first line of 'function' m4_changequote:

(gdb) info line m4_changequote
Line 895 of "builtin.c" starts at pc 0x634c and ends at 0x6350.

We can also inquire (using *addr as the form for linespec) what source line
covers a particular address:

(gdb) info line *0x63ff
Line 926 of "builtin.c" starts at pc 0x63e4 and ends at 0x6404.


note: changes pc?

After info line, the default address for the x command is 'changed' to the
starting address of the line, so that 'x/i' is sufficient to begin examining the
machine code

*gdb-disassemble*
disassemble
disassemble /m
disassemble /r

This specialized command dumps a range of memory as machine instructions. It can
also print 'mixed' source+disassembly by specifying the /m modifier and 

`print the raw instructions in hex` as well as in symbolic form by specifying
the /r. 

The default memory range is the function surrounding the "program counter" of
the selected frame.  A single argument to this command is a program counter
value; gdb dumps the function surrounding this value. 
    
When two arguments are given, they should be separated by a comma, possibly
surrounded by whitespace. The arguments specify a range of addresses to dump, in
one of two forms: 

start,end 

the addresses from start (inclusive) to end (exclusive)

The argument(s) can be any expression yielding a numeric value, such as 0x32c4,
    &main+10 or $pc-8.

<address>
Addresses 'cannot' be specified as a linespec (see Section 9.2 [Specify
    Location]). So, for example, if you want to disassemble function bar in file
‘foo.c’, you must type ‘disassemble ’foo.c’::bar’ and not ‘disassemble
foo.c:bar’.


*gdb-disassemble*
set disassembly-flavor instruction-set

Select the instruction set to use when disassembling the program via the
disassemble or x/i commands.

Currently this command is `only defined for the Intel x86 family.` You can set
instruction-set to either `intel or att.` `The default is att,` the AT&T flavor
used by default by Unix assemblers for x86-based targets.

show disassembly-flavor
Show the current setting of the disassembly flavor.


*gdb-disassemble*
set disassemble-next-line
show disassemble-next-line

Control whether or not gdb will disassemble the next source line or instruction
when execution stops. If ON, gdb will display disassembly of the next source
line when execution of the program being debugged stops. This is in addition to
displaying the source line itself, which gdb always does if possible. 

If the next source line cannot be displayed for some reason (e.g., if gdb cannot
    find the source file, or there's no line info in the debug info), gdb will
display disassembly of the next instruction instead of showing the next source
line. If AUTO, gdb will display disassembly of next instruction only if the
source line cannot be displayed. This setting causes gdb to display some
feedback when you step through a function with no line info or whose source file
is unavailable.  The default is OFF, which means never display the disassembly
of the next line or instruction.


<shared-libraries>
For programs that were dynamically linked and use shared libraries, instructions
that call functions or branch to locations in the shared libraries might show a
seemingly bogus location; it's actually a location of the relocation table. On
some architectures, gdb might be able to resolve these to actual function names.

note: might? how about linux?


={============================================================================
*kt_linux_gdb_000* gdb-cxx

15.4.1.7 gdb Features for C++

Some gdb commands are particularly useful with C++, and some are designed
specifically for use with C++. Here is a summary:

breakpoint menus
    When you want a breakpoint in a function whose name is 'overloaded', gdb has
    the capability to display a menu of possible breakpoint locations to help
    you specify which function definition you want. See Ambiguous Expressions.


rbreak regex
    Setting breakpoints using regular expressions is helpful for setting
    breakpoints on overloaded functions that are not members of any special
    classes. See Setting Breakpoints.

catch throw
catch rethrow
catch catch
    Debug C++ exception handling using these commands. See Setting Catchpoints.

<gdb-ptype> gdb-print
ptype typename
    Print inheritance relationships as well as other information for type
    typename. See Examining the Symbol Table.

info vtbl expression.
    The info vtbl command can be used to display the virtual method tables of
    the object computed by expression. This shows one entry per virtual table.
    there may be multiple virtual tables when multiple inheritance is in use.

demangle name
    Demangle name. See Symbols, for a more complete description of the demangle
    command.


set print demangle
show print demangle
set print asm-demangle
show print asm-demangle
    Control whether C++ symbols display in their source form, both when
    displaying code as C++ source and when displaying disassemblies. See Print
    Settings.

set print object
show print object
    Choose whether to print derived (actual) or declared types of objects. See
    Print Settings.

set print vtbl
show print vtbl
    Control the format for printing virtual function tables. See Print Settings.
    (The vtbl commands do not work on programs compiled with the HP ANSI C++
     compiler (aCC).)


set overload-resolution on
    Enable overload resolution for C++ expression evaluation. The default is on.
    For overloaded functions, gdb evaluates the arguments and searches for a
    function whose signature matches the argument types, using the standard C++
    conversion rules (see C++ Expressions, for details). If it cannot find a
    match, it emits a message.

set overload-resolution off
    Disable overload resolution for C++ expression evaluation. For overloaded
    functions that are not class member functions, gdb chooses the first
    function of the specified name that it finds in the symbol table, whether or
    not its arguments are of the correct type. For overloaded functions that are
    class member functions, gdb searches for a function whose signature exactly
    matches the argument types.


show overload-resolution
    Show the current setting of overload resolution.

Overloaded symbol names
    You can specify a particular definition of an overloaded symbol, using the
    same notation that is used to declare such symbols in C++: type
    symbol(types) rather than just symbol. You can also use the gdb command-line
    word completion facilities to list the available choices, or to finish the
    type list for you. See Command Completion, for details on how to do this. 



={============================================================================
*kt_linux_gdb_300* gdb-symbol

16 Examining the Symbol Table

To allow gdb to recognize 'foo.c' as a single symbol, enclose it in single
quotes; for example,

p ’foo.c’::x


{symbol-and-address}

info address <symbol>

Print the address of a symbol. Describe where the data for symbol is stored. For
a register variable, this says which register it is kept in. For a non-register
local variable, this prints the stack-frame offset at which the variable is
always stored.

info symbol <addr>

Print the name of a symbol which is stored at the address addr. If no symbol is
stored exactly at addr, gdb prints the nearest symbol and an offset from it:

(gdb) info symbol 0x54320
_initialize_vx + 396 in section .text

This is the 'opposite' of the info address command. You can use it to find out
the name of a variable or a function given its address.

<shared-library>
For dynamically linked executables, the name of executable or shared library
containing the symbol is also printed:

(gdb) info symbol 0x400225
_start + 5 in section .text of /tmp/a.out
(gdb) info symbol 0x2aaaac2811cf
__read_nocancel + 6 in section .text of /usr/lib64/libc.so.6


{demangle}
demangle [-l language] [--] name

Demangle name. If language is provided it is the name of the language to
demangle name in. Otherwise name is demangled in the current language.
The ‘--’ option specifies the end of options, and is useful when name begins
with a dash.
The parameter demangle-style specifies how to interpret the kind of mangling
used. See Section 10.8 [Print Settings], page 121.


{gdb-whatis} gdb-print
whatis[/flags] [arg]

Print the data 'type' of arg, which can be either an expression or a name of a
data type. With no argument, print the data type of $, the last value in the
value history. If arg is an expression (see Section 10.1 [Expressions], page
    109), it is not actually evaluated, and any side-effecting operations (such
      as assignments or function calls) inside it do not take place. 

If arg is a variable or an expression, whatis prints its literal type as it is
used in the source code. If the type was defined using a typedef, whatis will
not print the data type underlying the typedef. If the type of the variable or
the expression is a compound data type, such as struct or class, whatis never
prints their fields or methods. It just prints the struct/class name a.k.a.  its
tag. 

If you want to see the members of such a compound data type, use ptype. If arg
is a type name that was defined using typedef, whatis unrolls only one level of
that typedef. Unrolling means that whatis will show the underlying type used in
the typedef declaration of arg. However, if that underlying type is also a
typedef, whatis will not unroll it.

(gdb) whatis ppkSectionData
type = const uint8_t **

flags can be used to modify how the type is displayed. Available flags are:

r 
Display in "raw" form. Normally, gdb substitutes template parameters and
typedefs defined in a class when printing the class’ members. The /r flag
disables this.

m 
Do not print methods defined in the class. M Print methods defined in the class.
This is the 'default', but the flag exists in case you change the default with
set print type methods.

t 
Do not print typedefs defined in the class. Note that this controls whether the
typedef definition itself is printed, not whether typedef names are substituted
when printing other types.

T 
Print typedefs defined in the class. This is the default, but the flag exists in
case you change the default with set print type typedefs.


{gdb-ptype} gdb-print
ptype[/flags] [arg]

ptype accepts the same arguments as whatis, but prints a 'detailed' description
of the type, instead of just the name of the type.

Contrary to whatis, ptype 'always' 'unrolls' any typedefs in its argument
declaration, whether the argument is a variable, expression, or a data type.


typedef double real_t;
struct complex { real_t real; double imag; };

typedef struct complex complex_t;

complex_t var;
real_t *real_pointer_var;

(gdb) whatis var
type = complex_t

(gdb) ptype var
type = struct complex {
    real_t real;
    double imag;
}

(gdb) whatis complex_t
type = struct complex

(gdb) whatis struct complex
type = struct complex

(gdb) ptype struct complex
type = struct complex {
    real_t real;
    double imag;
}

(gdb) whatis real_pointer_var
type = real_t *

(gdb) ptype real_pointer_var
type = double *


{info-scope}
info scope location

List all the variables local to a particular scope. This command accepts a
location argument; a function name, a source line, or an address preceded by a
'*', and prints all the variables local to the scope defined by that location.

For example:

(gdb) info scope command_line_handler
Scope for command_line_handler:
Symbol rl is an argument at stack/frame offset 8, length 4.
Symbol linebuffer is in static storage at address 0x150a18, length 4.
Symbol linelength is in static storage at address 0x150a1c, length 4.
Symbol p is a local variable in register $esi, length 4.
Symbol p1 is a local variable in register $ebx, length 4.
Symbol nline is a local variable in register $edx, length 4.
Symbol repeat is a local variable at frame offset -8, length 4.

This command is especially useful for determining what data to collect during a
trace experiment


{info-sources}
info source

Show information about the current source file

info sources

Print the names of 'all' source files in your program for which organized into
two lists: files whose symbols have already been read, and files whose symbols
'will' be read when needed.


{info-variables}
info variables

Print the names and data types of all variables that are defined 'outside' of
functions i.e. excluding local variables.


{gdb-symbol-find}
info functions
Print the names and data types of all defined functions.

info functions regexp
Print the names and data types of all defined functions whose names contain a
match for regular expression regexp. 

Thus, ‘info fun step’ finds all functions whose names include step; ‘info fun
^step’ finds those whose names start with step. If a function name contains
characters that conflict with the regular expression language (e.g.
‘operator*()’), they may be quoted with a backslash.


={============================================================================
*kt_linux_gdb_300* gdb-exam-symbol-loading

note:
not supported in 7.2

set print symbol-loading
set print symbol-loading full
set print symbol-loading brief
set print symbol-loading off
show print symbol-loading

The set print symbol-loading command allows you to control the printing of
messages when gdb loads symbol information. By default a message is printed for
the executable and one for each shared library, and normally this is what you
want. 
    
However, when debugging apps with large numbers of shared libraries these
messages can be annoying.  When set to 'brief' a message is printed for each
executable, and when gdb loads a collection of shared libraries at once it will
only print one message regardless of the number of shared libraries.


={============================================================================
*kt_linux_gdb_300* gdb-exam-debug-searate-file

Create build with -g, then get symbol map out of it. Save it somewhere (i
    would recommend saving binary with debugging symbols too - it's easier
    that way), then strip debugging symbols out (with strip program) and
deploy resulting binary to target system. Here is howto:
https://sourceware.org/gdb/onlinedocs/gdb/Separate-Debug-Files.html

After it crashes, either restore dump with -g-compiled binary or with release
binary and separate debug file. If you have crash address and binary with
debugging symbols and you want to map it to source code line - you could use
addr2line -e your_binary crash_address instead of gdb.

https://sourceware.org/gdb/onlinedocs/gdb/Separate-Debug-Files.html

18.3 Debugging Information in Separate Files

GDB allows you to put a program’s debugging information in a file separate
from the executable itself, in a way that allows GDB to find and load the
debugging information automatically. Since debugging information can be very
largesometimes larger than the executable code itselfsome systems distribute
debugging information for their executables in separate files, which users can
install only when they need to debug a problem.

The debugging information file itself should be an ordinary executable,
containing a full set of linker symbols, sections, and debugging information.
  The sections of the debugging information file should have the same names,
addresses, and sizes as the original file, but they need not contain any
  datamuch like a .bss section in an ordinary executable.

The GNU binary utilities (Binutils) package includes the ‘objcopy’ utility
that can produce the separated executable / debugging information file pairs
using the following commands:

objcopy --only-keep-debug foo foo.debug
strip -g foo

<tool-strip>
       --only-keep-debug

           The intention is that this option will be used in conjunction
           with --add-gnu-debuglink to create a two part executable.  One a
           stripped binary which will occupy less space in RAM and in a
           distribution and the second a debugging information file which is
           only needed if debugging abilities are required.  The suggested
           procedure to create these files is as follows:

           1.<Link the executable as normal.  Assuming that is is called>
               "foo" then...

           1.<Run "objcopy --only-keep-debug foo foo.dbg" to>
               create a file containing the debugging info.

           1.<Run "objcopy --strip-debug foo" to create a>
               stripped executable.

           1.<Run "objcopy --add-gnu-debuglink=foo.dbg foo">
               to add a link to the debugging info into the stripped
               executable.

           Note---the choice of ".dbg" as an extension for the debug info
           file is arbitrary.  Also the "--only-keep-debug" step is
           optional.  You could instead do this:

           1.<Link the executable as normal.>
           1.<Copy "foo" to "foo.full">
           1.<Run "strip --strip-debug foo">
           1.<Run "objcopy --add-gnu-debuglink=foo.full foo">

           i.e., the file pointed to by the --add-gnu-debuglink can be the
           full executable.  It does not have to be a file created by the
           --only-keep-debug switch.

           Note---this switch is only intended for use on fully linked
           files.  It does not make sense to use it on object files where
           the debugging information may be incomplete.  Besides the
           gnu_debuglink feature currently only supports the presence of one
           filename containing debugging information, not multiple filenames
           on a one-per-object-file basis.

<ex>
$ objcopy --only-keep-debug a.out a.debug

-rwxr-xr-x  1 kyoupark ccusers    4926 Apr 21 07:36 a.debug*
-rwxrwxr-x  1 kyoupark ccusers    6662 Apr 21 07:36 a.out*

$ strip -g a.out

-rwxr-xr-x  1 kyoupark ccusers    4926 Apr 21 07:36 a.debug*
-rwxrwxr-x  1 kyoupark ccusers    6570 Apr 21 07:37 a.out*


={============================================================================
*kt_linux_gdb_300* gdb-exam-debug-with-and-without-g gdb-debug

Without -g, do not have source and symbol information so:

* do not show source information.
* do not allow stepping.
* can check if it's built with/withoug g using *gdb-check-built-with-debug*


{gdb-debug-slib}

// no -g on shared

  $ gcc -fpic -shared -o foo.so foo.c
  $ gcc -o main main.c ./foo.so -g
  $ gdb main

// pending on slib

  (gdb) b foo
  Function "foo" not defined.
  Make breakpoint pending on future shared library load? (y or [n]) y
  Breakpoint 1 (foo) pending.

  (gdb) r
  Starting program: /home/bhushan/RD/Shared_Lib_Debug/main
  Reading symbols from shared object read from target memory...done.
  Loaded system supplied DSO at 0x470000
  Breakpoint 2 at 0xdb7493

// pending resolved

  Pending breakpoint "foo" resolved
  Breakpoint 2, 0x00db7493 in foo () from ./foo.so ~

  (gdb) s
  Single stepping until exit from function foo, ~
  which has no line number information.
  main () at main.c:7
  7 printf("inside main i = %d\n", i);

  (gdb) s
  inside main i = 4
  8 return 0;


// build the shared libarary using -g option

  $ gcc -fpic -shared -o foo.so foo.c -g
  $ gcc -o main main.c ./foo.so -g
  $ gdb main

  (gdb) b foo

  Function "foo" not defined.
  Make breakpoint pending on future shared library load? (y or [n]) y
  Breakpoint 1 (foo) pending.

  (gdb) r
  Starting program: /home/bhushan/RD/Shared_Lib_Debug/main
  Reading symbols from shared object read from target memory...done.
  Loaded system supplied DSO at 0x470000
  Breakpoint 2 at 0x1c5493: file foo.c, line 5. ~
  Pending breakpoint "foo" resolved

  Breakpoint 2, foo () at foo.c:5 ~
  5 return 2*2; ~

  (gdb) s ~
  7 } ~

  (gdb) s
  main () at main.c:7
  7 printf("inside main i = %d\n", i);

  (gdb) s
  inside main i = 4
  8 return 0;


{gdb-debug-core}

// Seems that core file are the same since the size is the same and show the
// same gdb result when use with both debug and release executable.

-rw-------   1 kpark kpark 397312 Sep  1 14:11 7842.core
-rw-------   1 kpark kpark 397312 Sep  1 14:15 7960.core
-rwxr-xr-x   1 kpark kpark  27974 Sep  1 13:40 a-g.out*
-rwxr-xr-x   1 kpark kpark  10350 Sep  1 14:15 a-r.out*

// with -g option

$ gdb a-g.out 7960.core
...
Reading symbols from /home/kpark/work/a-g.out...done. ~

warning: core file may not match specified executable file.
[New LWP 7960]

warning: Can't read pathname for load map: Input/output error.
Core was generated by `./a.out'.
Program terminated with signal 11, Segmentation fault.
#0  0x0000000000400a1a in main () at tdebug.cpp:16
16	    cout << "anode's next is " << anode.next->val << endl;;

(gdb) l
11	
12	int main()
13	{
14	    node anode = { NULL, 0 };
15	
16	    cout << "anode's next is " << anode.next->val << endl;;
17	
18	    cout << "anode {" << anode.next << ", " << anode.val << "}" << endl;
19	}

(gdb) bt
#0  0x0000000000400a1a in main () at tdebug.cpp:16
(gdb) 


// without -g 

$ gdb a.out 7960.core
...
Reading symbols from /home/kpark/work/a.out...(no debugging symbols found)...done. ~
Illegal process-id: 7960.core.
[New LWP 7960]

warning: Can't read pathname for load map: Input/output error.
Core was generated by `./a.out'.
Program terminated with signal 11, Segmentation fault.
#0  0x0000000000400a1a in main ()

(gdb) l
No symbol table is loaded.  Use the "file" command. ~

(gdb) bt
#0  0x0000000000400a1a in main ()

// bt when without g

(gdb) bt
#0  0x779dfd1c in __lll_lock_wait () from /lib/libpthread.so.0
#1  0x779e75cc in pthread_mutex_lock () from /lib/libpthread.so.0
#2  0x742993a8 in nickel::system::GstMediaRouter::getPosition() const () from 
  /opt/zinc-trunk/lib/libNickelSystemGStreamer.so

#3  0x766add10 in boost::_bi::bind_t<zinc::Future<Zinc::Media::Position>,
  boost::_mfi::cmf0<zinc::Future<Zinc::Media::Position>,
  Zinc::Media::MediaRouterAsync>, boost::_bi::list1<boost::arg<1> >
    >::result_type
    nickel::system::ProxyMediaRouter::deferForwardOrDefault<boost::_bi::bind_t<zinc::Future<Zinc::Media::Position>,
  boost::_mfi::cmf0<zinc::Future<Zinc::Media::Position>,
  Zinc::Media::MediaRouterAsync>, boost::_bi::list1<boost::arg<1> > >
    >(boost::_bi::bind_t<zinc::Future<Zinc::Media::Position>,
        boost::_mfi::cmf0<zinc::Future<Zinc::Media::Position>,
        Zinc::Media::MediaRouterAsync>, boost::_bi::list1<boost::arg<1> > >,
        nickel::system::returned_future_type<boost::_bi::bind_t<zinc::Future<Zinc::Media::Position>,
        boost::_mfi::cmf0<zinc::Future<Zinc::Media::Position>,
        Zinc::Media::MediaRouterAsync>, boost::_bi::list1<boost::arg<1> > >
        >::type const&) const () from
    /opt/zinc-trunk/lib/libNickelSystemProxy.so

#4  0x7667fe8c in nickel::system::ProxyMediaRouter::getPosition() const () 
    from /opt/zinc-trunk/lib/libNickelSystemProxy.so
#5  0x0044926c in Zinc::Media::(anonymous namespace)::getPosition_stub
    (Zinc::Media::MediaRouterAsync&, DBus::Connection::WeakRef&, DBus::CallMessage const&) ()
#6  0x0045d3ec in Zinc::Media::MediaRouterAsyncToDBus::call_method_async(DBus::CallMessage const&) ()
#7  0x0045d784 in Zinc::Media::MediaRouterAsyncToDBus::handle_message(DBus::Message const&) ()
#8  0x004634d0 in zinc::binding::dbus::
    RefCountedAdaptorDecorator<Zinc::Media::MediaRouterAsyncToDBus>::operator()(DBus::Connection&, DBus::Message&) ()

#9  0x00463678 in DBus::detail::AdaptorWrapper<zinc::binding::dbus::
    RefCountedAdaptorDecorator<Zinc::Media::MediaRouterAsyncToDBus> >::call(DBusConnection*, DBusMessage*, void*) ()

#10 0x777d316c in _dbus_object_tree_dispatch_and_unlock () from /opt/zinc-trunk/oss/lib/libdbus-1.so.3
#11 0x777b4944 in dbus_connection_dispatch () from /opt/zinc-trunk/oss/lib/libdbus-1.so.3
#12 0x778337f8 in DBus::drain_pending_messages(DBus::Connection::Private*) () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#13 0x777b3ea8 in _dbus_connection_update_dispatch_status_and_unlock () from /opt/zinc-trunk/oss/lib/libdbus-1.so.3
#14 0x777adef4 in _dbus_connection_handle_watch () from /opt/zinc-trunk/oss/lib/libdbus-1.so.3
#15 0x777e66dc in dbus_watch_handle () from /opt/zinc-trunk/oss/lib/libdbus-1.so.3
#16 0x77846484 in DBus::Watch::handle(int) () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#17 0x77851f6c in DBus::DefaultWatch::do_callback() () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#18 0x77852444 in DBus::DefaultMainLoop::processCommands() () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#19 0x77854030 in DBus::DefaultMainLoop::dispatch(unsigned int) () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#20 0x77855704 in DBus::BusDispatcher::run() () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#21 0x778bd9bc in zinc::binding::dbus::MainLoop::run() () from /opt/zinc-trunk/lib/libZincDbusBindingRuntime.so.0
#22 0x00409674 in main ()


{gdb-debug-stl}

<1>
// $ ../install/bin/g++4.9.2 -std=c++0x t_templ.cpp

Do not step into and continue to the end.

Reading symbols from a.out...(no debugging symbols found)...done. ~
(gdb) start
Temporary breakpoint 1 at 0x400d0a
Starting program: /home/kpark/gcc-build/work/a.out 

Temporary breakpoint 1, 0x0000000000400d0a in main ()
(gdb) s
Single stepping until exit from function main, ~
which has no line number information. ~
---
this is main
---
__libc_start_main (main=0x400d06 <main>, argc=1, argv=0x7fffffffdcf8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffdce8) at libc-start.c:321
321	libc-start.c: No such file or directory.
(gdb) 


// $ ../install/bin/g++4.9.2 -g -std=c++0x t_templ.cpp

// Step into the the executable but not into the stl since stl is not debug
// version.

Reading symbols from a.out...done.
(gdb) start
Temporary breakpoint 1 at 0x400d0f: file t_templ.cpp, line 21.
Starting program: /home/kpark/gcc-build/work/a.out 

Temporary breakpoint 1, main () at t_templ.cpp:21
21	  cout << "---" << endl;
(gdb) next
---
23	  My mime;
(gdb) 
26	  debug_rep(str);
(gdb) step
debug_rep<std::string> (t="this is string") at t_templ.cpp:14
14	    ostringstream ret;
(gdb) 
15	    ret << t;
(gdb) step                      // note do not step into
16	    return ret.str();
(gdb) 
17	}


// *gdb-debug-stl*
// $ ../install/bin/g++4.9.2 -g -std=c++0x -D_GLIBCXX_DEBUG t_templ.cpp 

Reading symbols from a.out...done.
(gdb) start
Temporary breakpoint 1 at 0x401643: file t_templ.cpp, line 21.
Starting program: /home/kpark/gcc-build/work/a.out 

Temporary breakpoint 1, main () at t_templ.cpp:21
21	  cout << "---" << endl;

(gdb) 
26	  debug_rep(str);
(gdb) s
debug_rep<std::string> (t="this is string") at t_templ.cpp:14
14	    ostringstream ret;
(gdb) 
15	    ret << t;
(gdb) 
std::operator<< <char, std::char_traits<char>, std::allocator<char> > (__os=..., __str="this is string") at /home/kpark/gcc-build/install/include/c++/4.9.2/bits/basic_string.h:2777
2777	      return __ostream_insert(__os, __str.data(), __str.size());
(gdb) 


<2>
This cause *seg-fault* when this is already gone.

~HasPtr() { cout << "::~HasPtr(), " << *ps_ << endl; delete ps_; }

$ g++ -std=c++11 t_sp.cpp

(gdb) bt
#0  0x00007f26b4131f10 in std::basic_ostream<char, std::char_traits<char> >& std::operator<< <char, std::char_traits<char>, std::allocator<char> >(std::basic_ostream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) ()
   from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#1  0x0000000000403fa6 in VALUELIKE_MOVE::HasPtr::~HasPtr() ()
#2  0x0000000000402761 in t_sp_13_2() ()
#3  0x000000000040374e in main ()

(gdb) p *this
No symbol "this" in current context.


$ g++ -std=c++11 -g t_sp.cpp

(gdb) bt
#0  0x00007f71f7873f10 in std::basic_ostream<char, std::char_traits<char> >& std::operator<< <char, std::char_traits<char>, std::allocator<char> >(std::basic_ostream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) ()
   from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#1  0x0000000000403fa6 in VALUELIKE_MOVE::HasPtr::~HasPtr (
    this=0x7ffe23049dd0, __in_chrg=<optimized out>) at t_sp.cpp:280
#2  0x0000000000402761 in t_sp_13_2 () at t_sp.cpp:341
#3  0x000000000040374e in main () at t_sp.cpp:634
(gdb) f 1
#1  0x0000000000403fa6 in VALUELIKE_MOVE::HasPtr::~HasPtr (
    this=0x7ffe23049dd0, __in_chrg=<optimized out>) at t_sp.cpp:280
280	        ~HasPtr() { cout << "::~HasPtr(), " << *ps_ << endl; delete ps_; }

(gdb) p *this
$2 = {ps_ = 0x0, count_ = 0}


$ g++ -std=c++11 -g -D_GLIBCXX_DEBUG t_sp.cpp 

(gdb) bt
#0  0x0000000000407e00 in std::string::_M_data (this=0x0)
    at /usr/include/c++/4.9/bits/basic_string.h:293
#1  0x0000000000407e1e in std::string::_M_rep (this=0x0)
    at /usr/include/c++/4.9/bits/basic_string.h:301
#2  0x00000000004085ce in std::string::size (this=0x0)
    at /usr/include/c++/4.9/bits/basic_string.h:725
#3  0x0000000000406eec in std::operator<< <char, std::char_traits<char>, std::allocator<char> > (__os=..., 
    __str=<error reading variable: Cannot access memory at address 0x0>)
    at /usr/include/c++/4.9/bits/basic_string.h:2777
#4  0x0000000000405c66 in VALUELIKE_MOVE::HasPtr::~HasPtr (
    this=0x7ffdb2824b60, __in_chrg=<optimized out>) at t_sp.cpp:280
#5  0x00000000004039da in t_sp_13_2 () at t_sp.cpp:341
#6  0x0000000000404ac0 in main () at t_sp.cpp:634


={============================================================================
*kt_linux_gdb_300* gdb-slib-search

18 gdb Files

Normally, GDB will load the shared library symbols automatically. You can
control this behavior using set auto-solib-add command.

set auto-solib-add mode

If mode is on, symbols from all shared object libraries will be loaded
'automatically' when the inferior 'begins' execution, you 'attach' to an
independently started inferior, or when the dynamic linker informs gdb that a
new library has been loaded. If mode is off, symbols must be loaded manually,
using the sharedlibrary command. The 'default' value is on.

<selective-load>
If your program uses lots of shared libraries with debug info that takes large
amounts of memory, you can 'decrease' the gdb memory footprint by preventing it
from automatically loading the symbols from shared libraries. To that end, type
set auto-solib-add off before running the inferior, then load each library whose
debug symbols you do need with sharedlibrary regexp, where regexp is a regular
expression that matches the libraries whose symbols you want to be loaded.

(gdb) show auto-solib-add
Autoloading of shared library symbols is on.


{info-share-and-share}
info share regex
info sharedlibrary regex

'print' the names of the shared libraries which are currently loaded that match
regex. If regex is omitted then print 'all' shared libraries that are loaded.

<ex>
(gdb) i sharedlibrary 
From        To          Syms Read   Shared Object Library
0x773da470  0x77429860  Yes (*)     /opt/zinc/oss/lib/libboost_program_options.so.1.57.0
0x773a99b0  0x773a9b60  Yes (*)     /opt/zinc-trunk/lib/libNickelSystemDbus.so.0
0x77380880  0x77392520  Yes (*)     /opt/zinc-trunk/lib/libNickelSystemAPI.so.0
0x77303b70  0x77350790  Yes (*)     /opt/zinc-trunk/lib/libZincDbusBindingRuntime.so.0
0x7729f170  0x772c3580  Yes (*)     /opt/zinc-trunk/lib/libdbus-c++-1.so.0
0x7720d950  0x77272f20  Yes (*)     /opt/zinc/oss/lib/libdbus-1.so.3
0x7717c940  0x771e6240  Yes (*)     /opt/zinc-trunk/lib/libZincCommon.so.0
0x77151a60  0x77152ed0  Yes (*)     /lib/libdl.so.0
0x77128740  0x7713b9f0  Yes (*)     /opt/zinc/oss/lib/libboost_thread.so.1.57.0
0x770b7950  0x770fb360  Yes (*)     /opt/zinc/oss/lib/liblog4cplus-1.0.so.4
0x770859e0  0x7708e260  Yes (*)     /opt/zinc/oss/lib/libboost_date_time.so.1.57.0
0x77055380  0x770697b0  Yes (*)     /opt/zinc/oss/lib/libboost_filesystem.so.1.57.0
0x7703dd60  0x7703ee30  Yes (*)     /opt/zinc/oss/lib/libboost_system.so.1.57.0
0x770286f0  0x7702b220  Yes (*)     /opt/zinc/oss/lib/libboost_chrono.so.1.57.0
0x76f9d780  0x77001aa0  Yes (*)     /lib/libstdc++.so.6
0x76f3f250  0x76f482f0  Yes (*)     /lib/libm.so.0
0x76f13680  0x76f2c760  Yes (*)     /lib/libgcc_s.so.1
0x76ee5f10  0x76ef2da0  Yes (*)     /lib/libpthread.so.0
0x76e33fd0  0x76e86bb0  Yes (*)     /lib/libc.so.0
0x76e15d20  0x76e174f0  Yes (*)     /lib/librt.so.0
0x76ded730  0x76dfe710  Yes (*)     /opt/zinc/lib/libNickelSystemAPI.so.0
0x76d70b40  0x76dbd760  Yes (*)     /opt/zinc/lib/libZincDbusBindingRuntime.so.0
0x76d0b130  0x76d2f540  Yes (*)     /opt/zinc/lib/libdbus-c++-1.so.0
0x76c6d950  0x76cd7420  Yes (*)     /opt/zinc/lib/libZincCommon.so.0
0x76c33560  0x76c418b0  Yes (*)     /opt/zinc/oss/lib/libz.so.1
0x76b7f6b0  0x76c157e0  Yes (*)     /opt/zinc/oss/lib/libsqlite3.so.0
0x77448eb0  0x7744e6f0  Yes (*)     /lib/ld-uClibc.so.0
0x76b01ac0  0x76b4f5e0  Yes         /opt/zinc-trunk/lib/libNickelSystemProxy.so
0x76ae3c60  0x76ae7690  Yes (*)     /opt/zinc/lib/libZincHttpServer.so.0
0x76aba0b0  0x76acdf10  Yes (*)     /opt/zinc/lib/libZincHttpClient.so.0
...


sharedlibrary regex
share regex

'load' shared object library symbols for files matching a Unix regular
expression.  As with files loaded automatically, it only loads shared libraries
required by your program for a core file or 'after' typing run. If regex is
omitted 'all' shared libraries required by your program are loaded.

note: 
Can specify library name which are required by a program but 'not' a filename
with a path to force a loading of it. That means cannot load a libaray until a
program needs it.

nosharedlibrary

'unload' all shared object library symbols. This discards all symbols that have
been loaded from all shared libraries. Symbols from shared libraries that were
loaded by explicit user requests are 'not' discarded.


{support-shared-library-remote} from 18 gdb Files
Shared libraries are also supported in many cross or remote debugging
configurations. gdb needs to have access to the target's libraries; this can be
accomplished either by providing copies of the libraries "on the host system",
             or by asking gdb to automatically retrieve the libraries from the
             target. 

If copies of the target libraries are provided, they need to be the same as the
target libraries, although the copies on the 'target' 'can' be 'stripped' as
long as the copies on the host are not.

For remote debugging, you need to tell gdb where the target libraries are, so
that it can load the correct copies. otherwise, it may try to load the host's
libraries. gdb has two variables to specify the search directories for target
libraries.


<sysroot>
set sysroot path

Use path as the system root 'for' the program being debugged. Any 'absolute'
shared library paths will be 'prefixed' with path; many runtime loaders store
the absolute paths to the shared library in the target program's memory. 

If you use set sysroot to find shared libraries, they need to be laid out in the
'same' way that they are on the target, with e.g. a '/lib' and '/usr/lib'
hierarchy under path.

The `set solib-absolute-prefix` command is an 'alias' for set sysroot.

show sysroot

Display the current shared library prefix.

note: said that this command forces to 'reload' shared libraries after changes
debuggee using file command.

(gdb) set sysroot /


<solib-search-path>
set solib-search-path path

If this variable is set, path is a colon-separated list of directories to
search for shared libraries. solib-search-path is used after sysroot fails to
locate the library, or if the path to the library is relative instead of
absolute. 

note:
GNU gdb 6.6.0.20070423-cvs do not support sysroot command

If you want to use solib-search-path instead of 'sysroot', be sure to set
sysroot to a nonexistent directory to prevent gdb from finding your host's
libraries.

sysroot is preferred; setting it to a nonexistent directory may interfere with
automatic loading of shared library symbols.

show solib-search-path
Display the current shared library search path.

(gdb) show solib-search-path
The search path for loading non-absolute shared library symbol files is .

<ex>
(gdb) set solib-search-path ./mips-libs-from-build/
Reading symbols from /home/kyoupark/asn/mips-libs-from-build/libasan.so.0...done.
Reading symbols from /home/kyoupark/asn/mips-libs-from-build/libssp.so.0...done.
Reading symbols from /home/kyoupark/asn/mips-libs-from-build/libc.so.0...done.
Reading symbols from /home/kyoupark/asn/mips-libs-from-build/libpthread.so.0...(no debugging symbols found)...done.
Reading symbols from /home/kyoupark/asn/mips-libs-from-build/libdl.so.0...(no debugging symbols found)...done.
Reading symbols from /home/kyoupark/asn/mips-libs-from-build/libstdc++.so.6...done.
Reading symbols from /home/kyoupark/asn/mips-libs-from-build/libm.so.0...done.
Reading symbols from /home/kyoupark/asn/mips-libs-from-build/ld-uClibc.so.1...(no debugging symbols found)...done.
Reading symbols from /home/kyoupark/asn/mips-libs-from-build/libgcc_s.so.1...done.
Reading symbols from /home/kyoupark/asn/mips-libs-from-build/ld-uClibc.so.0...(no debugging symbols found)...done.


{when-need-manual}
However, in some cases (e.g. when debugging with gdbserver and having
    incompatible symbols or using old Android toolchains) GDB will not load the
symbols automatically. In this case you can use the info sharedlibrary command
to list the loaded shared libraries and the sharedlibrary command to force the
symbols to be loaded. 

<check-solib-search-path>
If GDB does not automatically load debugging symbols for your library when
debugging with gdbserver, please check the search path using the set
solib-search-path command.


={============================================================================
*kt_linux_gdb_300* gdb-solib-break

5.1.1 Setting Breakpoints

<pending-breakpoint>

Shared libraries can be loaded and unloaded explicitly, and possibly repeatedly,
as the program is executed.  To support this use case, gdb updates breakpoint
  locations 'whenever' any shared library is loaded or unloaded.  Typically, you
  would set a breakpoint in a shared library at the 'beginning' of your
  debugging session, when the library is not loaded, and when the symbols from
  the library are not available. When you try to set breakpoint, gdb will ask
  you if you want to set a so called 'pending' breakpoint-breakpoint whose
  address is not yet resolved.  After the program is run, whenever a new shared
  library is loaded, gdb reevaluates all the breakpoints. When a newly loaded
  shared library contains the symbol or line referred to by some pending
  breakpoint, that breakpoint is resolved and becomes an ordinary breakpoint.
  When a library is unloaded, all breakpoints that refer to its symbols or
  source lines become pending again.

This logic works for breakpoints with multiple locations, too. For example, if
you have a breakpoint in a C++ template function, and a newly loaded shared
library has an instantiation of that template, a new location is added to the
list of locations for the breakpoint.

Except for having unresolved address, pending breakpoints do not differ from
regular breakpoints.  You can set conditions or commands, enable and disable
them and perform other breakpoint operations.

set breakpoint pending auto

This is the 'default' behavior. When gdb cannot find the breakpoint location, it
queries you whether a pending breakpoint should be created.

*gdb-break-solib*
set breakpoint pending on

This indicates that an unrecognized breakpoint location should automatically
result in a pending
breakpoint being created.


={============================================================================
*kt_linux_gdb_300* gdb-solib-case

In this example we will disable shared library loading using the set
auto-solib-add command, then run the application, list the source files and load
the symbols manually:

  (gdb) set auto-solib-add off
  (gdb) break main
  Breakpoint 1 at 0x80484ed: file main.cpp, line 7.
  (gdb) run
  Starting program: /home/testuser/libtest/testApp

  Breakpoint 1, main () at main.cpp:7
  7 printf("In main()\n");

  (gdb) info sources
  Source files for which symbols have been read in:

  /home/testuser/libtest/main.cpp

  Source files for which symbols will be read in on demand:

  (gdb) info sharedlibrary
  From To Syms Read Shared Object Library
  0xb7fde820 0xb7ff6b9f No /lib/ld-linux.so.2
  0xb7fd83a0 0xb7fd84c8 No /home/testuser/libtest/libTest.so
  0xb7e30f10 0xb7f655cc No /lib/i386-linux-gnu/libc.so.6

  (gdb) sharedlibrary libTest
  Reading symbols from /home/testuser/libtest/libTest.so...done.
  Loaded symbols for /home/testuser/libtest/libTest.so

  (gdb) info sources
  Source files for which symbols have been read in:

  /home/testuser/libtest/main.cpp

  Source files for which symbols will be read in on demand:

  /home/testuser/libtest/lib.cpp    // note. see added source file

  (gdb) break lib.cpp:5
  Breakpoint 2 at 0xb7fd846e: file lib.cpp, line 5.

  (gdb) continue
  Continuing.
  In main()

  Breakpoint 2, func () at lib.cpp:5
  5 printf("In func()\n");


={============================================================================
*kt_linux_tool_300* gdb-case: debugging with strace and gdb in sandbox

Getting a core dump from the sandboxed application

First of all, bad things happen and sometimes we need to find out what went
wrong. Because the sandbox is intended to be robust, we do not want to leave any
back-door to get into with the debugger or tracing tools. Obviously back-door
hooks are allowed for debug builds but sometimes getting a debug build is
inconvenient.

In this short article I want to focus on a rapid approach to get to the point as
quickly as possible. Please bear in mind that this is more a bunch of advise
rather than a copy-and-paste approach. A bit of warning: if you get frustrated
very early with questions like "where do I get this" or "where do I get that",
then have a break with a cup of tea, ask people around etc. If you're still
  frustrated after that then probably this article is not for you.

note:
The first thing to try is to get the core dump and then possibly use strace
and/or GDB. By GDB I mean here a 'native' build of GDB rather than gdbserver as
debugging with the latter one gets really awkward with a "heavy" Stagecraft
process.

Note that these tools take control over the application process and affect
process relations, i.e.  they get in the middle of the application process and
its parent. This has implications wherever process relations are important, e.g.
no remote control input will be directed to the application process and the
application process will not be allowed to write into the screen buffer.

Getting the core dump

This is the easiest thing to do and requires the least effort. Obviously you
need a seg-fault. So if you don't have a seg-fault handy, then this is not for
you. You need to get a grip and carry on.

You need to apply all below before starting the babysitterd (C13) (uimanagerd
    before C13).

The instructions below must be applied in the same console which the babysitterd
is started from.  Note that it won't work when starting the babysitterd over the
D-BUS as the D-BUS daemon will not be aware of the new settings until restarted.

In this situation start the babysitterd from the command line.

# tell the kernel where to store the core file (it's going to be from within the
# sandbox, so bear with me)

echo /opt/adobe/stagecraft/data/core > /proc/sys/kernel/core_pattern

# tell the kernel we don't mind ending with a large core file (use your
# imagination to get the size you like)

ulimit -c 10000000000

# in case of a crash in any *setuid* executable, additionally you need to do the
# following:

echo 1 > /proc/sys/fs/suid_dumpable
chmod +r <setuid executable>

# start the babysitterd, launch the UIME and app and make it crash

Note that currently the only location where the sandboxed application can write
to, is its private /tmp which is not persistent. The other one is its private
"copy" of /opt/adobe/stagecraft/data tree. Obviously the choice it to create the
core file in /opt/adobe/stagecraft/data.

Where to look for:

private writeable area mapped as /opt/adobe/stagecraft/data:
$PREFIX/var/applications/data/air/<app CRID sha256>/stagecraft-data/

private log files:
$PREFIX/var/applications/data/air/<app CRID sha256>/log/


* Attaching to the application process

Unless the investigation is related to the application startup, it is better and
easier to attach to the process. 

Strace

# get the stagecraft process(es) PID(s)
/opt/zinc/oss/bin/busybox pgrep stagecraft

# pick the interesting process PID and strace it
strace -ff -o /tmp/stagecraft.log -s 128 -p <PID>

Please note that some of the Stagecraft threads are quite "intensive" loops
producing a lot of useless strace output. This is important to get every thread
output into a separate file (-ff option).  

GDB

As a prerequisite, you need to get MIPSEL build of GDB and a corresponding
version of thread libraries.

# override the default thread libraries
mount --bind <MY_PATH_TO_GDB_PTHREAD>/usr/lib /usr/lib
mount --bind <MY_PATH_TO_GDB_PTHREAD>/lib/libthread_db-0.9.29.so /lib/libthread_db-0.9.29.so
mount --bind <MY_PATH_TO_GDB_PTHREAD>/lib/libpthread-0.9.29.so /lib/libpthread.so.0

# add <MY_PATH_TO_GDB_PTHREAD> to $PREFIX/lib/sandbox/application.conf somewhere
# under [directories]

# launch the app

# get the stagecraft process(es) PID(s)
/opt/zinc/oss/bin/busybox pgrep stagecraft

# pick the interesting process PID and attach the debugger to it
<MY_PATH_TO_GDB>/gdb -p <PID> /opt/stagecraft-2.0/bin/stagecraft


* Starting the application process with a diagnostic tool

Strace

The easiest way to use strace is to edit $PREFIX/bin/runStagecraft2.sh somewhere
at the end:

# this is the original line
LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib:/opt/zinc-trunk/lib:/usr/local/lib" \
LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
exec "${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"

# this is the same line with strace
exec strace -ff -o /opt/adobe/stagecraft/data/trace.log -s 128 \
-E LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib:/opt/zinc-trunk/lib:/usr/local/lib" \
-E LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
"${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"

You will find strace log files here: $PREFIX/var/applications/data/air/<app CRID
sha256>/stagecraft-data/.  

GDB

The prerequisites are the same as in "Attaching to the application process". You
can use GDB_STAGECRAFT variable in $PREFIX/bin/runStagecraft2.sh. As setting it
in the environment would require re-starting UIME, I prefer to edit
runStagecraft2.sh instead:

+GDB_STAGECRAFT=1
 if [ -z "${GDB_STAGECRAFT}" ]; then
 	LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib:/opt/zinc-trunk/lib:/usr/local/lib" \
 	LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
 	exec "${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"
 else
-	GDB="gdb"
+	GDB="<MY_PATH_TO_GDB>/gdb"
 	ORG_LD_LIBRARY_PATH="${LD_LIBRARY_PATH}"
 	LD_LIBRARY_PATH="/usr/lib:/usr/local/lib:/opt/zinc-trunk/oss/lib" exec ${GDB} \

The other important thing here is to start the babysitterd in foreground as at
some point you will want GDB interactive session. You will also need to add
<MY_PATH_TO_GDB> to $PREFIX/lib/sandbox/application.conf somewhere under
[directories].

Remember about permissions


={============================================================================
*kt_linux_gdb_300* gdb-remote

20 Debugging Remote Programs

If you are trying to debug a program running on a machine that cannot run gdb
in the usual way, it is often useful to use remote debugging. For example, you
might use remote debugging on an operating system kernel, or on a small system
which does not have a general purpose operating system powerful enough to run
a full-featured debugger.

Some configurations of gdb have special serial or TCP/IP interfaces to make
this work with particular debugging targets. In addition, gdb comes with a
generic serial protocol (specific to gdb, but not specific to any particular
    target system) which you can use if you write the `remote stubs`; the code
that runs on the remote system to communicate with gdb.

Other `remote targets` may be available in your configuration of gdb; use help
target to list them.

20.1.1 Types of Remote Connections

gdb supports two types of remote connections, `target remote` mode and `target`
`extended-remote` mode. Note that many remote targets support only target remote
mode. There are several major differences between the two types of
connections, enumerated here:

Result of detach or program exit

With target remote mode: When the debugged program exits or you detach from
it, gdb disconnects from the target. When using gdbserver, gdbserver will
exit.

The run command

With target remote mode: The run command is not supported. Once a connection
has been established, you can use all the usual gdb commands to examine and
change data. The remote program is already running, so you can use commands
like step and continue.

Attaching 

With target remote mode: The gdb command attach is not supported. To attach to
a running program using gdbserver, you must use the ‘--attach’ option (see
[Running gdbserver], page 265).


20.1.2 Host and Target Files

gdb, `running on the host`, needs access to symbol and debugging information for
your program running on the target. This requires access to an unstripped copy
of your program, and possibly any associated symbol files. Note that this
section applies equally to both target remote mode and target extended-remote
mode.

Some remote targets (see [qXfer executable filename read], page 653, and see
    Section E.7 [Host I/O Packets], page 665) allow gdb to access program
files over the same connection used to communicate with gdb. With such a
target, if the remote program is unstripped, the only command you need is
target remote (or target extended-remote).

If the `remote program` is stripped, or the target does not support remote
program file access, start up gdb using the name of the `local` unstripped copy
of your program as the first argument, or use the file command. 

Use set sysroot to specify the location (on the host) of target libraries
(unless your gdb was compiled with the correct sysroot using --with-sysroot).
Alternatively, you may use set solib-search-path to specify how gdb locates
target libraries.

The symbol file and target libraries must exactly match the executable and
libraries on the target, with one exception: the files on the host system
should not be stripped, even if the files on the target system are. Mismatched
or missing files will lead to confusing results during debugging. On gnu/Linux
targets, mismatched or missing files may also prevent gdbserver from debugging
multi-threaded programs.


20.1.3 Remote Connection Commands

gdb can communicate with the target over a serial line, or over an IP network
using TCP or UDP. In each case, gdb uses the same protocol for debugging your
program; only the medium carrying the debugging packets varies. The target
remote and target extendedremote commands establish a connection to the
target. Both commands accept the same arguments, which indicate the medium to
use:

note: run this command on host side

The `target remote` command establishes a connection `to the target.` Its
arguments indicate which medium to use:

target remote host:port
target remote tcp:host:port
target extended-remote host:port
target extended-remote tcp:host:port

Debug using a TCP connection to port `on host.` The host may be either a host
name or a numeric IP address; port must be a decimal number. The host could be
the target machine itself, if it is directly connected to the net, or it might
be a terminal server which in turn has a serial line to the target.

For example, to connect to port 2828 on a terminal server named manyfarms:
target remote manyfarms:2828

If your remote target is actually running `on the same machine` as your debugger
session (e.g. a simulator for your target running on the same host), you can
omit the hostname. For example, to connect to port 1234 on your local machine:
target remote :1234

Note that the colon is still required here.

detach 

When you have finished debugging the remote program, you can use the detach
command to release it from gdb control. Detaching from the target normally
resumes its execution, but the results will depend on your particular remote
stub. After the detach command in target remote mode, gdb is free to connect
to another target. In target extended-remote mode, gdb is still connected to
the target.

disconnect

The disconnect command closes the connection to the target, and the target is
generally not resumed. It will wait for gdb (this instance or another one) to
connect and continue debugging. After the disconnect command, gdb is again
free to connect to another target.


20.2 Sending files to a remote system


={============================================================================
*kt_linux_gdb_300* gdb-remote-gdbserver

20.3 Using the gdbserver Program

note: runs gdbserver on a target

`gdbserver` is a control program for Unix-like systems, which allows you to
connect your program with a remote gdb via target remote or target
extended-remotebut without linking in the usual debugging stub.

gdbserver is not a complete replacement for the debugging stubs, because it
requires essentially the same operating-system facilities that gdb itself
does. In fact, a system that can run gdbserver to connect to a remote gdb
could also run gdb locally! gdbserver is sometimes useful nevertheless,
because it is a much smaller program than gdb itself. It is also easier to
  port than all of gdb, so you may be able to get started more quickly on a
  new system by using gdbserver.


20.3.1 Running gdbserver

Run gdbserver on the target system. You need a copy of the program you want to
debug, including any libraries it requires. gdbserver does 'not' need your
program's symbol table, so you can strip the program if necessary to save space.
gdb on the host system does all the symbol handling.

To use the server, you must tell it how to communicate with gdb; the name of
your program; and the arguments for your program. The usual syntax is:

target> gdbserver comm program [ args ... ]

<gdb-comm>
`comm` is either a device name (to use a serial line), or a TCP hostname and
portnumber, or - or stdio to use stdin/stdout of gdbserver.

For example, to debug Emacs with the argument ‘foo.txt’ and communicate with
gdb over the serial port ‘/dev/com1’:

target> gdbserver /dev/com1 emacs foo.txt

gdbserver waits passively for the `host gdb` to communicate with it.

To use a TCP connection instead of a serial line:

target> gdbserver host:2345 emacs foo.txt

The host:2345 argument `means that` we are expecting to see a TCP connection
`from` host to local TCP port 2345. (note Currently, the host part is
    ignored.) You can choose any number you want for the port number as long
as it does not conflict with any existing TCP ports on the target system. You
must use the same port number with the host gdb target remote command.


20.3.1.1 Attaching to a Running Program

On some targets, gdbserver can also attach to running programs. This is
accomplished via the --attach argument.

target> gdbserver --attach comm pid

$ gdbserver --attach 172.20.33.215:12345 1368

You can debug processes by name instead of process ID if your target has the
pidof utility:

target> gdbserver --attach comm ‘pidof program‘


20.3.2 Connecting to gdbserver

The basic procedure for connecting to the remote target is:

  Run gdb on the host system.

  Make sure you have the necessary symbol files. Load symbols for your
  application using the file command before you connect. Use set sysroot to
  locate target libraries (unless your gdb was compiled with the correct
      sysroot using --with-sysroot).

  Connect to your target. For TCP connections, you must start up gdbserver
  prior to using the target command. Otherwise you may get an error whose
  text depends on the host system, but which usually looks something like
  "Connection refused". Don't use the load command in gdb when using target
  remote mode, since the program is already on the target.

<once-connected>
Once the connection has been established, you can use all the usual commands to
examine and change data. The remote program is already running; you can use step
and continue, and you do not need to use run.

note:
The remote targets are always running. If you get an error message like this one
below then use continue to run your program. You may need load first.

The "remote" target does not support "run".  Try "help target" or "continue".  

note:
"Cannot access memory at address 0x0" warning, This happens when run gdb client
in case use to debug a applicaiton using a shared library. Thought that gdb is
not working but gdb works as usual.


<gdb-vs-gdbserver>
~/spk-out/build_mips/gdb-6.7.1-target/gdb$ readelf -d gdb
 0x00000001 (NEEDED)                     Shared library: [libncurses.so.5]
 0x00000001 (NEEDED)                     Shared library: [libm.so.0]
 0x00000001 (NEEDED)                     Shared library: [libdl.so.0]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]

~/spk-out/build_mips/gdb-6.7.1-target/gdb/gdbserver$ readelf -d gdbserver
 0x00000001 (NEEDED)                     Shared library: [libthread_db.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]


={============================================================================
*kt_linux_tool_300* gdb-remote: args

<arguments>
20.3.1.4 Other Command-Line Arguments for gdbserver

--debug

Instruct gdbserver to display extra status information about the debugging
process. This option is intended for gdbserver development and for bug reports
to the developers.

--remote-debug

Instruct gdbserver to display remote protocol debug output. This option is
intended for gdbserver development and for bug reports to the developers.

<symbols>
First make sure you have the necessary symbol files. Load symbols for your
application using the file command before you connect. Use set sysroot to locate
target libraries (unless your GDB was compiled with the correct sysroot using
        --with-sysroot). Q: <sysroot>?

The symbol file and target libraries must exactly match the executable and
libraries on the target with one exception: the files on the host system should
not be stripped, even if the files on the target system are. Mismatched or
missing files will lead to confusing results during debugging. On GNU/Linux
targets, mismatched or missing files may also prevent gdbserver from debugging
multi-threaded programs. 


{gdb-client}
target remote host:port

monitor cmd

This command allows you to send arbitrary commands directly to the remote
monitor. Since GDB doesn't care about the commands it sends like this, this
command is the way to extend GDBâyou can add new commands that only the
external monitor will understand and implement. 
	

20.3.3 Monitor Commands for gdbserver

During a GDB session using gdbserver, you can use the monitor command to send
special requests to gdbserver. Here are the available commands.

monitor help
List the available monitor commands.

monitor set debug 0
monitor set debug 1
Disable or enable general debugging messages.

monitor set remote-debug 0
monitor set remote-debug 1
Disable or enable specific debugging messages associated with the remote
protocol (see Remote Protocol).  

monitor set debug-format option1[,option2,...]

Specify additional text to add to debugging messages. Possible options are:

none Turn off all extra information in debugging output. 

all Turn on all extra information in debugging output. 

timestamps Include a timestamp in each line of debugging output. 

Options are processed in order. Thus, for example, if none appears last then no
additional information is added to debugging output. 

monitor exit

Tell gdbserver to exit immediately. This command should be followed by
disconnect to close the debugging session. gdbserver will detach from any
attached processes and kill any processes it created. Use monitor exit to
terminate gdbserver at the end of a multi-process mode debug session.


={============================================================================
*kt_linux_gdb_300* gdb-remote-case

<case-run-remote> 
* Use remote debugging when debug it from main out of sandbox
* Not work when use release version
* 172.20.33.215 is host machine

<target>
gdbserver 172.20.33.215:12345 /opt/zinc-trunk/bin/nickelmediad -b Zinc.MediaProxy

// console logs comes here

<host>
$ ./mips-linux-uclibc-gdb 

(gdb) file /home/kpark/builds/yv-media-next/humax.1000/zinc-install-root
  /debug/humax-dtr_t1000/opt/zinc-trunk/bin/nickelmediad

// when use release version
//
// (gdb) file /home/kpark/builds-src-dev/topic-master/humax.2100/\
//    zinc-install-root/release/humax-dtr_t2100/opt/zinc-trunk/bin/nickelmediad
// Reading symbols from /home/kpark/builds-src-dev/topic-master/humax.2100/\
//  zinc-install-root/release/humax-dtr_t2100/opt/zinc-trunk/bin/nickelmediad\
//  ...(no debugging symbols found)...done.

(gdb) target remote 172.20.32.34:12345

(gdb) set substitute-path /home/kpark/builds/_virtual_/humax.1000 /home/kpark/src-dev

(gdb) set sysroot /home/kpark/builds/yv-media-next/humax.1000/zinc-install-root/debug/humax-dtr_t1000/opt/zinc-trunk/lib

(gdb) b MediaDaemon.cpp:187
Breakpoint 1 at 0x41d238: file
/home/kpark/builds/_virtual_/humax.1000/DEVARCH/Nickel/
  Nickel.System.DBusServer/src/MediaDaemon.cpp,
    line 187.

(gdb) c
Continuing.
Error while mapping shared library sections:
/usr/local/lib/libdirectfb.so: No such file or directory.
...
/lib/ld-uClibc.so.0: No such file or directory.

Breakpoint 1, parseArgs (this=0x7f839f28, argc=3, argv=0x7f83a184) at 
  /home/kpark/builds/_virtual_/humax.1000/DEVARCH/Nickel/
  Nickel.System.DBusServer/src/MediaDaemon.cpp:189
189		std::string defaultConfigFilePath = 
  NS_ZINC::PackageDataFinder().find("media-daemon.plugin-config");
(gdb) list


// {scenario-two} 
// Use remote when debug stating from main. NOT OK. However, when code has a call
// to load shared library using dl_open then gdb do not picks up the library and
// hence do not hit a break point in that shared library. Tried sysroot and
// solib-search-path. 


<case-run-local> 
To solve the above, use local approach. Copy sources from host to target and set
substitute. This works since can trace routine calling code to load shared
library.

$ gdb --args /opt/zinc-bin/bin/nickelmediad -b Zinc.MediaProxy 
(gdb) set substitute-path 
  /home/kpark/builds/_virtual_/humax.1000/DEVARCH/Nickel /root
(gdb) b ProxySystemFactory.cpp:82
(gdb) run


{on-target-attach}
Still cannot use remote since still use dl_open depending on selection via dbus.
So must use local approach.

$ dbussenddaemon &
$ sleep 1

// this is necessary to make it run in background
$ /opt/zinc-trunk/bin/nickelmediad -b Zinc.MediaProxy > /var/tmp/x.log 2>&1 &

// to check PID and if necessary, kill the previous one.
$ pgrep -l nickelmediad

$ tail /var/tmp/x.log 
$ gdb -p 1846 /opt/zinc-trunk/bin/nickelmediad 

// gdb starts to load libraries
// do not need to run file since it's attached
// 172.20.33.215:/home/kpark/src-dev/DEVARCH/ on /mnt/tmp type nfs

(gdb) set substitute-path 
  /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/ /mnt/tmp
(gdb) b ProxyMediaRouter.cpp:448


note: when use release version

(gdb) b ProxyMediaRouter.cpp:448
No source file named ProxyMediaRouter.cpp.
Make breakpoint pending on future shared library load? (y or [n]) y
Breakpoint 1 (ProxyMediaRouter.cpp:448) pending.

note: when use debug build and gdb is able to set break, shows this:

Breakpoint 1 at 0x76b0c5e4: file
/home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Nickel/\
 Nickel.System.Proxy/src/ProxyMediaRouter.cpp,
line 448.


// From another ssh session, run dbus commands:

MR=`dbus-send 2>/dev/null --session --print-reply --type=method_call \
--dest='Zinc.DBusSendDaemon' \
/Zinc/Media/MediaRouterFactory \
Zinc.Media.MediaRouterFactory.createMediaRouter \
string:Zinc.MediaProxy 2>/dev/null |grep "/Zinc/Media/MediaRouters/" |cut -d'"' -f2`

dbus-send --session --print-reply --type=method_call --dest='Zinc.MediaProxy' $MR \
Zinc.Media.MediaRouter.setSource \
string:http://dash.bidi.int.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd \
int32:0


// then gdb hits a break point.

note: hit the break but a source is not found due to "set substitute-path" error

Breakpoint 1, nickel::system::ProxyMediaRouter::setSource (this=<optimized out>
    , mediaLocator_in=<optimized out>, reason_in=<optimized out>)
    at 
    /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Nickel/
      Nickel.System.Proxy/src/ProxyMediaRouter.cpp:448

448	/home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Nickel/
  Nickel.System.Proxy/src/ProxyMediaRouter.cpp: No such file or directory. ~
	in /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Nickel/
    Nickel.System.Proxy/src/ProxyMediaRouter.cpp
(gdb)

note: hit the break and found a source

Breakpoint 1, nickel::system::ProxyMediaRouter::setSource (this=<optimized out>
    , mediaLocator_in=<optimized out>, reason_in=<optimized out>)
    at 
    /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Nickel/
      Nickel.System.Proxy/src/ProxyMediaRouter.cpp:448

448	NS_ZINC::Future< void > ProxyMediaRouter::setSource
  (const std::string& mediaLocator_in, const SetSourceReason::Enum reason_in) {
(gdb)


note:
Before setting a break, file should be visible to gdb such as nfs mounted.


{scenario-five}
This is a case which have no direct loading of shared library and hence remote
approach is used.

<target>
$ gdbserver 172.20.33.215:12345 /opt/zinc-trunk/bin/nickelmediad -b Zinc.MediaProxy

<host>

// to see the default setting
(gdb)  show auto-solib-add             
Autoloading of shared library symbols is on.

// shall load file first before setting a breakpoint
(gdb) file /home/kit/tizen/tv-viewer/tv-viewer
Reading symbols from /home/kit/tizen/tv-viewer/tv-viewer...done.

(gdb) target remote 106.1.11.219:2345
Remote debugging using 106.1.11.219:2345
warning: Unable to find dynamic linker breakpoint function.
GDB will be unable to debug shared library initialisers
and track explicitly loaded dynamic code.
0xb63da7c0 in ?? ()

// to set a breakpoint in the gdbint; otherwise, gdb set no since no input from the user
*gdb-break*
(gdb) set breakpoint pending on

// this is a warning at this moment
(gdb) b main
Cannot access memory at address 0x0
Breakpoint 1 at 0x29842716: file /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1/src/core/AppMain.cpp, line 33.

// no shared library sicne the application do not started yet.
(gdb) i sharedlibrary
No shared libraries loaded at this time.

// see warnings on shared library now.
// set solib-search-path before and seems to have only one path as set solib-search-path /home/kit/mheg-port-ug
(gdb) c
Continuing.
warning: `/usr/lib/libicui18n.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicuuc.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicudata.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libfribidi.so.0': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicule.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: Could not load shared library symbols for 184 libraries, e.g. /usr/lib/libsys-assert.so.
Use the "info sharedlibrary" command to see the complete listing.
Do you need "set solib-search-path" or "set sysroot"?

Breakpoint 1, main (argc=-1237321556, argv=0xb63f4a30)
    at /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1/src/core/AppMain.cpp:33
33           _ERR("%s","Failed to create app!");

(gdb)
Continuing.

// see that even after running, not loaded full libraries. want to debug libug-mhegUG-efl.so
(gdb) i sharedlibrary
From        To          Syms Read   Shared Object Library
                        No          /usr/lib/libsys-assert.so
                        ...
                        No          /usr/lib/libsqlite3.so.0
0xb55efb24  0xb572307c  Yes (*)     /usr/lib/libicui18n.so.48
0xb55154e4  0xb55dfe7c  Yes (*)     /usr/lib/libicuuc.so.48
0xb43e4258  0xb43e4350  Yes (*)     /usr/lib/libicudata.so.48
                        No          /usr/lib/libavoc.so
                        ...
(*): Shared library is missing debugging information.

(gdb) c
Continuing.

(gdb) CTRL-C
Program received signal SIGINT, Interrupt.
0xb3db296c in ?? ()
warning: Could not load shared library symbols for 21 libraries, e.g. /usr/lib/ecore/immodules/libisf-imf-module.so.
Use the "info sharedlibrary" command to see the complete listing.
Do you need "set solib-search-path" or "set sysroot"?

// see that the wanted library is loaded
(gdb) i sharedlibrary
From        To          Syms Read   Shared Object Library
                        ...
0xab8d96c0  0xab9a2d78  No          /home/kit/mheg-port-ug/libug-mhegUG-efl.so
                        ..
(*): Shared library is missing debugging information.

// note:
// HOWEVER, the problem is that the library is loaded but 'not' the symblos and files. Can check to
// see if that is the case by running "i sources" to see files.

// note to fource to load again and check with i sources
(gdb) sharedlibrary mheg
Reading symbols from /home/kit/mheg-port-ug/libug-mhegUG-efl.so...done.
Loaded symbols for /home/kit/mheg-port-ug/libug-mhegUG-efl.so
Cannot access memory at address 0xb

// here can see the result of this substitude command:
// set substitute-path /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1 /home/kit/tizen/tv-viewer
(gdb) i sources
Source files for which symbols have been read in:

/home/kit/tizen/tv-viewer/src/core/AppMain.cpp, /usr/include/c++/4.5.3/new,
...

Source files for which symbols will be read in on demand:
...
/home/kit/mheg-port-ug/mh5eng/mh5e_token.c, /home/kit/mheg-port-ug/mh5eng/mh5b_program.c,

// now set a breakpoint in the loaded library
(gdb) b _key_pressed(char const*)
Cannot access memory at address 0xb
Breakpoint 2 at 0xab8d9afa: file /home/abuild/rpmbuild/BUILD/ug-mheg-0.2/main/Main.cpp, line 144.

(gdb) c

# hit breakpoint

(gdb) list

// note that if do not run this command fast enough, it seems that gdb fails to run a session. means
// not hitting a breakpoint.

(gdb) c
Continuing.


={============================================================================
*kt_linux_tool_300* gdb-remote: frontend: cgdb

It is how to change cgdb to use cross tool gdb.

http://www.programdevelop.com/4527764/

The GDB code CGDB calls in the path: /VARIOUS/util/src/fork_util.c by function invoke_debugger in

int invoke_debugger( 
            const char *path,  
            int argc, char *argv[],  
            int *in, int *out,  
            int choice, char *filename)  
{ 
    pid_t pid;     
    //GDBGDB?arm-linux-gdb 
    const char * const GDB               = "arm-linux-gdb"; 
}

./configure --prefix=/usr/local/ --program-suffix=arm-linux
make
sudo make install

kit@kit-vb:~/mheg-port$ ls /usr/local/bin/cgdb*
/usr/local/bin/cgdb  /usr/local/bin/cgdbarm-linux
kit@kit-vb:~/mheg-port$

http://cgdb.github.io/


{config}
/.cgdb/cgdbrc
==
:set winspilt=top_big
:set arrowstyle=long
:map <F6> :continue<CR>
:map <F7> :finish<CR>
:map <F8> :step<CR>
:map <F9> :next<CR>
==


={============================================================================
*kt_linux_tool_300* gdb-remote: frontend: eclipse

Debugging with Eclipse and gdb

I have recently tried to debug some elements with Eclipse. It's quite
straightforward. 

Steps to start debugging:

1. Right click on a project to be debugged and choose "Debug As" -> "Debug
Configurations...".

It should create new debug configuration under C/C++ Application branch named
same as project itself. If it is not created then it is possible to create it
with "New launch configuration" (button in upper left corner).

2. Select executable

In the Main tab it is required to provide a path to executable file which will
be launched ("Browse...") so need to know which executable we're going to debug
(e. g. some test).

3. Set environment

In most cases D-BUS demon is required. In Environment tab need to add variable
DBUS_SESSION_BUS_ADDRESS set to proper value (assuming that D-BUS daemon is
    started, e. g.
    unix:path=/tmp/dbus-7a0FdfCMQ8,guid=02115674e4f1464d822c1e700000230a).

4. Start debugging

Eclipse automatically stops on the first line in main() so there's no need to
worry about first breakpoint. Depending on settings in Eclipse it might ask for
switching to Debug perspective.

It is also possible to attach to the running process. In case of test
application running under Stagecraft some hack is required: need to hold
application, attach to it and resume it. It's a matter of experience for
choosing proper point of suspension. Using Lead project as an example I added
following line at the beginning of LeadAirPlugin::registerBindings() :

for ( bool bDbg = true; bDbg; ) {}

Rebuild:

make Lead.AIR.Client.API-clean
make Lead.AIR.Client.API

Run application:

$PREFIX/bin/runStagecraft2.sh --forceembeddedvectorfonts
/home/zinc/Zinc/source/Canvas/elements.trunk/Lead/Lead.ClientApiTests/as3/getTunedChannel/bin/YoureWatching.swf

Now need to attach to stagecraft application as follows:

1. Right click on particular project and choose  Debug As" -> "Debug
Configurations...".

In C/C++ Attach to Application branch create new configuration (name it whatever
    you want).

2. Select stagecraft executable: /opt/stagecraft-2.0/bin/stagecraft

3. Start debugging

New window with a list of processes will occur. Just start typing 'stagecraft'
and select proper process (it should be only one if you're not running any other
    Stagecraft applications) and choose "OK". In debug perspective take a look
into stack and find registerBindings() in one of threads (possibly the last
    one). Double-click it and the hack-line should appear in the source code
editor. In the upper right corner of Debug perspective there's a set of tabbed
windows. Select Variables then select 'bDbg' variable by clicking on its 'Value'
column and change it to false. Now you can step through the code.

This method is very simple but requires rebuilding project whenever holding
application is required or not. A better approach might be inserting this sort
of code somewhere:

for ( bool bDbg = getenv("GDB_HOLD_STAGECRAFT"); bDbg; ) {}  

This hack works also with optimized code ('bDbg' variable is set conditionally
    so compiler cannot optimize any code afterwards). It requires looking into
  assembly and finding out where the program execution should continue after the
  hack-loop. Example below is based on a simple few-liner application I've made
  for this purpose (test.cpp, I've used "KRK_DBG" environment variable):

#include <cstdlib>
#include <iostream>
 
int main() {
    for ( bool bDbg = getenv("KRK_DBG"); bDbg; ) {}
 
    std::cout << "haha" << std::endl;
    return 0;
}
 

And the session:

> g++ -O3 -o test test.cpp
> export KRK_DBG=1
> ./test &
[1] 1258

> gdb -p 1258
... some GDB introduction stuff ...

(gdb) set disassembly-flavor intel

(gdb) x/2i $pc
0x8048815 <main+245>:	lea    esi,[esi+0x0]
0x8048818 <main+248>:	jmp    0x8048815 <main+245>

(gdb) x/20i main
0x8048720 <main>:	lea    ecx,[esp+0x4]
0x8048724 <main+4>:	and    esp,0xfffffff0
0x8048727 <main+7>:	push   DWORD PTR [ecx-0x4]
0x804872a <main+10>:	push   ebp
0x804872b <main+11>:	mov    ebp,esp
0x804872d <main+13>:	push   edi
0x804872e <main+14>:	push   esi
0x804872f <main+15>:	push   ebx
0x8048730 <main+16>:	push   ecx
0x8048731 <main+17>:	sub    esp,0x118
0x8048737 <main+23>:	mov    DWORD PTR [esp],0x80488e4
0x804873e <main+30>:	call   0x80485a8 <getenv@plt>
0x8048743 <main+35>:	test   eax,eax
0x8048745 <main+37>:	jne    0x8048815 <main+245>
0x804874b <main+43>:	mov    DWORD PTR [esp+0x8],0x4
0x8048753 <main+51>:	mov    DWORD PTR [esp+0x4],0x80488ec
0x804875b <main+59>:	mov    DWORD PTR [esp],0x8049b20
0x8048762 <main+66>:	call   0x8048618 <_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_i@plt>
0x8048767 <main+71>:	mov    eax,ds:0x8049b20
0x804876c <main+76>:	mov    eax,DWORD PTR [eax-0xc]
(gdb) set $pc=0x804874b
(gdb) cont
Continuing.
haha

Program exited normally.
(gdb)


Stepping through chain of calls.

Assume this part of code:
class A {
   void someMethod() const {
      // (3)
 
   }
 
} ;
 
const A& getA() {
   static A a;
   return a;
} // (2)
 
void f() {
   getA().someMethod(); // (1)
   // (4)
 
}

Stepping in (F5) at point (1) will follow getA() body which might be stepped
over (F6) but it is important to step in (F5) at point (2) to get into
A::someMethod() at point (3). It's confusing but stepping over (F6) at point (2)
will bring the debugger into point (4).

Some observations:

    Keeping 'Variables' tab opened causes gdb to ignore breakpoints or crashes
    gdb session in some situations.

    Sometimes breakpoints are not re-installed (i. e. after restarting debug
        session).

    Stepping through chain of calls is not intuitive (see the example above).

All of these might be caused rather by MI bugs (gdb interface).

It is also possible to debug one or more processes simultaneously in Eclipse.
There is no problem to debug full conversation of Flash application (stagecraft
    -> Flash -> C++ bindings actually) with service by attaching to stagecraft
and particular daemon.

Obviously it is also possible to do it with pure GDB (command line) and requires
starting another instance of GDB what Eclipse as a matter of fact does.


={============================================================================
*kt_linux_tool_300* gdb-core

$ man 5 core

CORE(5)

NAME
       core - core dump file

DESCRIPTION
       The  default action of certain signals is to cause a process to terminate
       and produce a core dump file, a disk file containing an image of the
       process's memory at the time of termination.  This image can be used in a
       debugger (e.g., gdb(1)) to inspect the state of the program at the time
       that it terminated.  A list of the signals which cause a process to dump
       core can be found in signal(7).

       A process can set its soft RLIMIT_CORE resource limit to place an upper
       limit on the size of the core dump file that will be produced if it
       receives a "core dump" signal; see getrlimit(2) for details.

       There are various circumstances in which a core dump file is 'not'
       produced:
       ...

   Naming of core dump files
       By  default, a core dump file is named core, but the
       /proc/sys/kernel/core_pattern file (since Linux 2.6 and 2.4.21) can be
       set to define a template that is used to name core dump files.  The
       template can contain % specifiers which are substituted by the following
       values when a core file is created:

           %h  hostname (same as nodename returned by uname(2))

           %p  PID of dumped process, as seen in the PID namespace in which the
               process resides
           
           %t  time of dump, expressed as seconds since the Epoch, 1970-01-01
               00:00:00 +0000 (UTC)

           %e  executable filename (without path prefix)

   Piping core dumps to a program
       Since kernel 2.6.19, Linux supports an alternate syntax for the
       /proc/sys/kernel/core_pattern file. If the first character of this file
       is a pipe symbol (|), then the remainder of the line is interpreted as a
       'program' to be executed. Instead of being written to a disk file, the
       core dump is given as standard input to the program. Note the following
       points:

       *  The program must be specified using an 'absolute' pathname (or a
          pathname relative to the root directory, /), and must immediately
          follow the '|' character.

       *  The process created to run the program runs as user and group 'root'.

       *  Command-line arguments can be supplied to the program (since Linux
          2.6.24), delimited by white space (up to a total line length of 128
          bytes).

       *  The command-line arguments can include any of the % specifiers listed
          above. For example, to pass the PID of the process that is being
          dumped, specify %p in an argument.

   Controlling which mappings are written to the core dump
       Since kernel 2.6.23, the Linux-specific /proc/PID/coredump_filter file
       can be used to control which memory segments are written to the core dump
       file in the event that a core dump is performed for the process with the
       corresponding process ID.

       The  value  in  the file is a bit mask of memory mapping types (see
               mmap(2)). If a bit is set in the mask, then memory mappings of
       the corresponding type are dumped; otherwise they are not dumped. The
       bits in this file have the following meanings:

           bit 0  Dump anonymous private mappings.
           bit 1  Dump anonymous shared mappings.
           bit 2  Dump file-backed private mappings.
           bit 3  Dump file-backed shared mappings.
           bit 4 (since Linux 2.6.24)
                  Dump ELF headers.
           bit 5 (since Linux 2.6.28)
                  Dump private huge pages.
           bit 6 (since Linux 2.6.28)
                  Dump shared huge pages.

       By default, the following bits are set: 0, 1, 4 (if the
               CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS kernel configuration option
               is enabled), and 5.  The value of this file is displayed in
       hexadecimal. (The default value is thus displayed as 33.)

       <ex>
        [root@HUMAX /]# cat /proc/1339/coredump_filter 
        00000023

           543210
        '0b100011'


<core-linux>
Look for the code in fs/binfmt_aout.c and fs/binfmt_elf.c (in newer kernels,
you'll have to grep around a little in older ones) that says

        memcpy(corefile,"core.",5);
#if 0
        memcpy(corefile+5,current->comm,sizeof(current->comm));
#else
        corefile[4] = '\0';
#endif

and change the 0s to 1s.


={============================================================================
*kt_linux_tool_300* gdb-core-setting

{core-pattern}
$ cat /proc/sys/kernel/core_pattern 
core

<syntax>
# set core dump location and format
echo '%p.COR' >/proc/sys/kernel/core_pattern
echo '/tmp/%p.COR' >/proc/sys/kernel/core_pattern

<system-wide>
The changes done before are only applicable until the next reboot. In order to
make the change in all future reboots, you will need to add the following in
/etc/sysctl.conf

# own core file pattern...
kernel.core_pattern=/tmp/cores/core.%e.%p.%h.%t

sysctl.conf is the file controlling every configuration under /proc/sys

Just wanted to say that there is no need to edit the file manually. simply run
the sysctl command, which does the stuff

<things-to-check>
  * write permissions in the directory 
  * ulimit -c unlimited
  * should be debug version and statically linked. If instead of the callstack
    you see only "??", it was not compiled using static linkage

mkdir -p /tmp/cores
chmod a+rwx /tmp/cores
echo "/tmp/cores/core.%e.%p.%h.%t" > /proc/sys/kernel/core_pattern
 

<ulimit>
Do not use "ulimit" bash command to check if ulimit is unlimited and think that
ulimit setting for core is done since "If no option is given, then -f is
assumed." from bash manual.

// -f
//    The maximum size of files written by the shell and its children.
// -a
//    All current limits are reported.

$ ulimit
unlimited

note: okay as shows unlimited. really?

$ ulimit -a
core file size          (blocks, -c) 0             note: not set!
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128235
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128235
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited


$ ulimit -c unlimited
$ ulimit
unlimited
$ ulimit -a 
core file size          (blocks, -c) unlimited     note: now okay
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128235
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128235
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited


={============================================================================
*kt_linux_tool_300* gdb-core-setting-run-commands when makes core

echo "|/bin/bash /path/to/core-dump /mnt/hd1/ %p \
    [ disk_space_threshold_in_bytes ]" > /proc/sys/kernel/core_pattern

note: Use one like this and otherwise get an error from the script
note: Also, should set big enough value 

root     root      175.4M Feb 18 11:09 core.6135

echo "|/bin/bash /usr/local/bin/core-dump /mnt/hd1/ %p 10000000" > /proc/sys/kernel/core_pattern


[root@HUMAX sandbox]# cat /proc/sys/kernel/core_pattern 
|/bin/bash /usr/local/bin/core-dump /mnt/hd1/ %p

10:14:38 ~/source$ cat ./setup-xxx/scripts/core-dump
#!/bin/bash

##
# This is meant to be used to:
# - save the core dump piped to STDIN,
# - capture process memory map (/proc/pid/maps)
# - process the core dump (if gdb is installed)
#   to produce a text file with the backtrace
#   already demangled and info about threads.
# See core (5) man page for more information.
#
# This script is meant to be executed by the Kernel if
# /proc/sys/kernel/core_pattern references it.
#
# Usage:
#         ulimit -c unlimited
#         echo "|/bin/bash /path/to/core-dump /mnt/hd1/ %p \
#           [ disk_space_threshold_in_bytes ]" > /proc/sys/kernel/core_pattern
##

gdb_path=/opt/zinc/oss/debugtools/bin/gdb


##
# Defining custom reporting routines (instead of using the ones from
# common-functions) to minimize the dependency chain, since this script will be
# called by the kernel.
##
blit() {
    local severity="${1:-INFO}" && shift
    echo "[$(date +"%F %T")] $severity: $*"
}

error() {
    blit "ERROR" "$*"
}

die() {
    blit "DIE" "$*"
    exit 1
}

info() {
    blit "INFO" "$*"
}

##
# Produce a text file from the core dump with the backtrace
# already demangled and info about threads...
##
process_core_dump() {
    local core_dump="$1"
    local core_maps="$2"
    local output_file="$3"

    # note: gets a exec name from a map file
    local first_maps_line=$(head -1 "$core_maps")
    # Assume the executable is under /opt
    local executable="/opt/${first_maps_line#*/opt/}"

    [[ -e "$executable" ]] || {
        error "Failed to extract executable from '$first_maps_line'." 2> "$output_file"
        return
    }

    $gdb_path --batch \
              --eval-command="info threads" \
              --eval-command="echo ------------\n" \    // note: can add this
              --eval-command "bt" \
              --eval-command "thread apply all bt" \
              --eval-command 'info sharedlibrary' \
              --eval-command 'info registers' \
              --eval-command 'info locals' \
              --eval-command 'info args' \
              --eval-command 'x/100a $sp' \
              --eval-command 'x/50i $pc' \
              "$executable" "$core_dump" &> "$output_file"
}


##
# gets the available free space in the directory designated for core files
##
get_available_space() {
    declare -a fs_info=( $(stat -f -c "%a %s" "$core_output_path") )
    echo $(( $(IFS="*"; echo "${fs_info[*]}") ))
}


##
# Find the pid (which is the extension) of the oldest log entries
##
get_pid_of_oldest_logs() {
    local oldest_file=$(find "$core_output_path" -type f -exec stat -c "%Y %n" \
            {} \; | sort | head -n 1 | cut -d ' ' -f 2)
    echo "${oldest_file##*.}"
}


##
# Removes the oldest files in the core_output_path directory
##
do_cleanup() {
    local oldest_pid=$(get_pid_of_oldest_logs)
    rm -fv "$core_output_path"/*."$oldest_pid"
}

#
# Count number of entries in the core file directory
#
count_files() {
    echo $(find "$core_output_path" -type f | wc -l)
}


# note:
# echo "|/bin/bash /path/to/core-dump /mnt/hd1/ %p \
#   [ disk_space_threshold_in_bytes ]" > /proc/sys/kernel/core_pattern

core_output_path=$1
pid=$2
threshold=${3:-0}

# redirect stdout/stderr to the log file
exec 1>"${core_output_path}/core-log.$pid" 2>&1

[[ $# -eq 3 ]] || die "Script is not meant to be run manually!"


##
# Perform cleanup to reclaim some space if required
##
while true; do
    files_count=$(count_files)

    # break the loop if the available space is above the threshold
    [[ "$(get_available_space)" -lt "$threshold" ]] || break

    # break the loop if there are no files left to remove
    [[ "$files_count" -eq 0 ]] && break

    info "Performing oldest core files cleanup to free some space ..."
    do_cleanup

    # sanity check: break the loop if the cleanup didn't remove anything
    [[ "$files_count" -eq "$(count_files)" ]] && break
done


##
# Quit if free space is still bellow the threshold
##
[[ "$(get_available_space)" -lt "$threshold" ]] &&
    die "Core dump won't be created. Space usage limit exceeded!"

cat "/proc/$pid/maps" > "${core_output_path}/core-maps.$pid"

# save core file from stdin
dd of="${core_output_path}/core.$pid" bs=1M

[[ -x "$gdb_path" ]] &&
    process_core_dump \
        "${core_output_path}/core.$pid" \
        "${core_output_path}/core-maps.$pid" \
        "${core_output_path}/core-text.$pid"

# Unfortunately, this takes too much time (at least on t1000) and can result in
# a broken archive. Needs to be disabled.
#gzip "${core_output_path}/core.$pid"
info "Core file obtained successfully."


-rw-r--r--    1 root     root         161 Feb 18 10:37 core-log.5120
-rw-r--r--    1 root     root       46.7K Feb 18 10:37 core-maps.5120
-rw-r--r--    1 root     root       23.9K Feb 18 10:37 core-text.5120
-rw-r--r--    1 root     root      176.6M Feb 18 10:37 core.5120

<ex>
[root@HUMAX /]# more core-text.5339 
[New LWP 5339]
[New LWP 5404]
[New LWP 5405]
[New LWP 5403]
[New LWP 5406]
[New LWP 5408]
[New LWP 5407]
[New LWP 5423]
[New LWP 5420]
[New LWP 5409]
[New LWP 5410]
[New LWP 5419]
[New LWP 5411]
[New LWP 5414]
[New LWP 5413]
[New LWP 5412]
[New LWP 5422]
[New LWP 5425]
[Thread debugging using libthread_db enabled]
Core was generated by `/opt/zinc-trunk/bin/w3cEngine -debug -cache /app-data/client-cache -cache-size'.
Program terminated with signal 11, Segmentation fault.
#0  0x5447955c in vanadium::VanadiumMediaPlayer::currentTime() const () from /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0
  Id   Target Id         Frame 
  18   Thread 0x4ee7b510 (LWP 5425) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  17   Thread 0x53f1e510 (LWP 5422) 0x73a59284 in ?? () from /lib/libc.so.0
  16   Thread 0x5853c510 (LWP 5412) 0x73a59284 in ?? () from /lib/libc.so.0
  15   Thread 0x57d3c510 (LWP 5413) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  14   Thread 0x5753c510 (LWP 5414) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  13   Thread 0x58d3c510 (LWP 5411) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  12   Thread 0x564e4510 (LWP 5419) 0x73a59284 in ?? () from /lib/libc.so.0
  11   Thread 0x5953c510 (LWP 5410) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  10   Thread 0x59fee510 (LWP 5409) 0x73a59284 in ?? () from /lib/libc.so.0
  9    Thread 0x558e2510 (LWP 5420) 0x73a59284 in ?? () from /lib/libc.so.0
  8    Thread 0x5358c510 (LWP 5423) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  7    Thread 0x5abb3510 (LWP 5407) 0x73b0af84 in pthread_cond_timedwait () from /lib/libpthread.so.0
  6    Thread 0x5ab44510 (LWP 5408) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  5    Thread 0x5abc3510 (LWP 5406) 0x73b0af84 in pthread_cond_timedwait () from /lib/libpthread.so.0
  4    Thread 0x720c9510 (LWP 5403) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  3    Thread 0x70ed3510 (LWP 5405) 0x73a58640 in ioctl () from /lib/libc.so.0
  2    Thread 0x716d3510 (LWP 5404) 0x73aa6820 in read () from /lib/libc.so.0
* 1    Thread 0x773f4320 (LWP 5339) 0x5447955c in vanadium::VanadiumMediaPlayer::currentTime() const () from /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0
#0  0x5447955c in vanadium::VanadiumMediaPlayer::currentTime() const () from /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0
#1  0x756e1fdc in ?? () from /opt/zinc-trunk/oss/lib/libQt5WebKit.so.5
warning: GDB can't find the start of the function at 0x756e1fda.

    GDB is unable to find the start of the function at 0x756e1fda
and thus can't determine the size of that function's stack frame.
This means that GDB may be unable to access that stack frame, or
the frames below it.
    This problem is most likely caused by an invalid program counter or
stack pointer.
    However, if you think GDB should simply search farther back
from 0x756e1fda for code which looks like the beginning of a
function, you can increase the range of the search using the `set
heuristic-fence-post' command.

Thread 18 (Thread 0x4ee7b510 (LWP 5425)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x763df9bc in zinc::run(boost::asio::io_service*) () from /opt/zinc-trunk/lib/libZincCommon.so.0
#2  0x76351a8c in thread_proxy () from /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
#3  0x73b0f738 in start_thread () from /lib/libpthread.so.0
#4  0x73b08030 in __thread_start () from /lib/libpthread.so.0
Backtrace stopped: frame did not save the PC

Thread 17 (Thread 0x53f1e510 (LWP 5422)):
#0  0x73a59284 in ?? () from /lib/libc.so.0
warning: GDB can't find the start of the function at 0x73a59284.

Thread 16 (Thread 0x5853c510 (LWP 5412)):
#0  0x73a59284 in ?? () from /lib/libc.so.0
warning: GDB can't find the start of the function at 0x73a59284.

Thread 15 (Thread 0x57d3c510 (LWP 5413)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x763df9bc in zinc::run(boost::asio::io_service*) () from /opt/zinc-trunk/lib/libZincCommon.so.0
#2  0x76351a8c in thread_proxy () from /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
#3  0x73b0f738 in start_thread () from /lib/libpthread.so.0
#4  0x73b08030 in __thread_start () from /lib/libpthread.so.0
Backtrace stopped: frame did not save the PC

Thread 14 (Thread 0x5753c510 (LWP 5414)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x75b3bd78 in ?? () from /opt/zinc-trunk/oss/lib/libQt5WebKit.so.5
warning: GDB can't find the start of the function at 0x75b3bd76.

Thread 13 (Thread 0x58d3c510 (LWP 5411)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x763df9bc in zinc::run(boost::asio::io_service*) () from /opt/zinc-trunk/lib/libZincCommon.so.0
#2  0x76351a8c in thread_proxy () from /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
#3  0x73b0f738 in start_thread () from /lib/libpthread.so.0
#4  0x73b08030 in __thread_start () from /lib/libpthread.so.0
Backtrace stopped: frame did not save the PC

Thread 12 (Thread 0x564e4510 (LWP 5419)):
#0  0x73a59284 in ?? () from /lib/libc.so.0
warning: GDB can't find the start of the function at 0x73a59284.

Thread 11 (Thread 0x5953c510 (LWP 5410)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x763df9bc in zinc::run(boost::asio::io_service*) () from /opt/zinc-trunk/lib/libZincCommon.so.0
#2  0x76351a8c in thread_proxy () from /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
#3  0x73b0f738 in start_thread () from /lib/libpthread.so.0
#4  0x73b08030 in __thread_start () from /lib/libpthread.so.0
Backtrace stopped: frame did not save the PC

Thread 10 (Thread 0x59fee510 (LWP 5409)):
#0  0x73a59284 in ?? () from /lib/libc.so.0
warning: GDB can't find the start of the function at 0x73a59284.

Thread 9 (Thread 0x558e2510 (LWP 5420)):
#0  0x73a59284 in ?? () from /lib/libc.so.0
warning: GDB can't find the start of the function at 0x73a59284.

Thread 8 (Thread 0x5358c510 (LWP 5423)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x763bdd94 in zinc::run_catching_exceptions(boost::asio::io_service*) () from /opt/zinc-trunk/lib/libZincCommon.so.0
#2  0x763c0b7c in boost::detail::thread_data<zinc::detail::WorkerFunction<boost::function<void ()> > >::run() () from /opt/zinc-trunk/lib/libZincCommon.so.0
#3  0x76351a8c in thread_proxy () from /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
#4  0x73b0f738 in start_thread () from /lib/libpthread.so.0
#5  0x73b08030 in __thread_start () from /lib/libpthread.so.0
Backtrace stopped: frame did not save the PC

Thread 7 (Thread 0x5abb3510 (LWP 5407)):
#0  0x73b0af84 in pthread_cond_timedwait () from /lib/libpthread.so.0
#1  0x72f33488 in BKNI_WaitForEvent () from /usr/local/lib/libnexus.so
#2  0x72e91a18 in ?? () from /usr/local/lib/libnexus.so
warning: GDB can't find the start of the function at 0x72e91a16.

Thread 6 (Thread 0x5ab44510 (LWP 5408)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x773386f0 in ?? () from /usr/local/lib/libdirectfb.so
warning: GDB can't find the start of the function at 0x773386ee.

Thread 5 (Thread 0x5abc3510 (LWP 5406)):
#0  0x73b0af84 in pthread_cond_timedwait () from /lib/libpthread.so.0
#1  0x72f33488 in BKNI_WaitForEvent () from /usr/local/lib/libnexus.so
#2  0x72e91a18 in ?? () from /usr/local/lib/libnexus.so
warning: GDB can't find the start of the function at 0x72e91a16.

Thread 4 (Thread 0x720c9510 (LWP 5403)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x75e7c964 in ?? () from /opt/zinc-trunk/oss/lib/libQt5WebKit.so.5
warning: GDB can't find the start of the function at 0x75e7c962.

Thread 3 (Thread 0x70ed3510 (LWP 5405)):
#0  0x73a58640 in ioctl () from /lib/libc.so.0
#1  0x72e575b0 in ?? () from /usr/local/lib/libnexus.so
warning: GDB can't find the start of the function at 0x72e575ae.

Thread 2 (Thread 0x716d3510 (LWP 5404)):
#0  0x73aa6820 in read () from /lib/libc.so.0
#1  0x73aa6804 in read () from /lib/libc.so.0
Backtrace stopped: previous frame identical to this frame (corrupt stack?)

Thread 1 (Thread 0x773f4320 (LWP 5339)):
#0  0x5447955c in vanadium::VanadiumMediaPlayer::currentTime() const () from /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0
#1  0x756e1fdc in ?? () from /opt/zinc-trunk/oss/lib/libQt5WebKit.so.5
warning: GDB can't find the start of the function at 0x756e1fda.
From        To          Syms Read   Shared Object Library
0x772cfd70  0x773b9ee0  Yes (*)     /usr/local/lib/libdirectfb.so
0x77297d60  0x772a7f70  Yes (*)     /usr/local/lib/libdirect.so
0x7727bf90  0x772836f0  Yes (*)     /usr/local/lib/libinit.so
0x771f0780  0x77254aa0  Yes (*)     /lib/libstdc++.so.6
0x7714e9e0  0x7718f3f0  Yes (*)     /opt/zinc-trunk/lib/libCobaltSystemAPI.so.0
0x77127ae0  0x771283e0  Yes (*)     /opt/zinc-trunk/lib/libHeliumSystemAPI.so.0
0x7708b810  0x770e5870  Yes (*)     /opt/zinc-trunk/oss/lib/libsoup-2.4.so.1
0x76eb6890  0x770378f0  Yes         /opt/zinc-trunk/oss/lib/libgio-2.0.so.0
0x76e389d0  0x76e7bad0  Yes         /opt/zinc-trunk/oss/lib/libgobject-2.0.so.0
0x76caef50  0x76dc8e10  Yes         /opt/zinc-trunk/oss/lib/libglib-2.0.so.0
0x76c83010  0x76c89f30  Yes (*)     /opt/zinc-trunk/oss/lib/libintl.so.8
0x76c628f0  0x76c6c4b0  Yes (*)     /opt/zinc-trunk/lib/libMercurySystemAPI.so.0
0x76c3dea0  0x76c44c60  Yes (*)     /opt/zinc-trunk/lib/libNeonSystemAPI.so.0
0x76c1cd60  0x76c206c0  Yes (*)     /opt/zinc-trunk/lib/libNickelTunerSystemAPI.so.0
0x76c04bc0  0x76c06d20  Yes (*)     /opt/zinc-trunk/lib/libTitaniumApplicationContainerAPI.so.0
0x76bce750  0x76be8aa0  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5WebKitWidgets.so.5
0x76ba22e0  0x76ba2c80  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5PrintSupport.so.5
0x766869e0  0x76ada3f0  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5Widgets.so.5
0x765b8770  0x765b8920  Yes (*)     /opt/zinc-trunk/lib/libVanadiumSystemDbus.so.0
0x765a64c0  0x765a6bf0  Yes (*)     /opt/zinc-trunk/lib/libVanadiumSystemAPI.so.0
0x76535b70  0x76582790  Yes (*)     /opt/zinc-trunk/lib/libZincDbusBindingRuntime.so.0
0x764d1170  0x764f5580  Yes (*)     /opt/zinc-trunk/lib/libdbus-c++-1.so.0
0x764678a0  0x764a5330  Yes (*)     /opt/zinc-trunk/oss/lib/libdbus-1.so.3
0x76436750  0x7644d4b0  Yes (*)     /opt/zinc-trunk/lib/libZincJsCoreBindingRuntime.so.0
0x763a1940  0x7640b240  Yes (*)     /opt/zinc-trunk/lib/libZincCommon.so.0
0x76376a60  0x76377ed0  Yes (*)     /lib/libdl.so.0
0x7634d740  0x763609f0  Yes (*)     /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
0x762ddba0  0x76321200  Yes (*)     /opt/zinc-trunk/oss/lib/liblog4cplus-1.0.so.4
0x762ab9e0  0x762b4260  Yes (*)     /opt/zinc-trunk/oss/lib/libboost_date_time.so.1.57.0
0x7627c380  0x762907b0  Yes (*)     /opt/zinc-trunk/oss/lib/libboost_filesystem.so.1.57.0
0x76264d60  0x76265e30  Yes (*)     /opt/zinc-trunk/oss/lib/libboost_system.so.1.57.0
0x7624f6f0  0x76252220  Yes (*)     /opt/zinc-trunk/oss/lib/libboost_chrono.so.1.57.0
0x74768620  0x75eb2650  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5WebKit.so.5
0x74290a70  0x74568890  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5Gui.so.5
0x740e6c40  0x741f6970  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5Network.so.5
0x73c27fb0  0x73f48050  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5Core.so.5
0x73b7e810  0x73baea20  Yes (*)     /usr/local/lib/libjpeg.so.8
0x73b61250  0x73b6a2f0  Yes (*)     /lib/libm.so.0
0x73b35680  0x73b4e760  Yes (*)     /lib/libgcc_s.so.1
0x73b07f10  0x73b14da0  Yes (*)     /lib/libpthread.so.0
0x73a55fd0  0x73aa8bb0  Yes (*)     /lib/libc.so.0
0x73a1c560  0x73a366d0  Yes (*)     /usr/local/lib/libz.so.1
0x739f5d00  0x73a084a0  Yes (*)     /usr/local/lib/libfusion-1.4.so.17
0x739d0560  0x739de8b0  Yes         /opt/zinc-trunk/oss/lib/libz.so.1
0x72e554d0  0x72f36830  Yes (*)     /usr/local/lib/libnexus.so
0x72cdab00  0x72debd20  Yes (*)     /opt/zinc-trunk/oss/lib/libxml2.so.2
0x72c0d6b0  0x72ca37e0  Yes         /opt/zinc-trunk/oss/lib/libsqlite3.so.0
0x72bf30a0  0x72bf79d0  Yes         /opt/zinc-trunk/oss/lib/libffi.so.5
0x72bdfb70  0x72be1ad0  Yes (*)     /opt/zinc-trunk/oss/lib/libgmodule-2.0.so.0
0x72b9dd30  0x72bc6bc0  Yes (*)     /opt/zinc-trunk/oss/lib/libxslt.so.1
0x72b7ceb0  0x72b85070  Yes         /opt/zinc-trunk/oss/lib/libgstapp-1.0.so.0
0x72b49a20  0x72b61500  Yes         /opt/zinc-trunk/oss/lib/libgstpbutils-1.0.so.0
0x72ad4060  0x72b1fb90  Yes         /opt/zinc-trunk/oss/lib/libgstvideo-1.0.so.0
0x72a721e0  0x72aac8e0  Yes         /opt/zinc-trunk/oss/lib/libgstaudio-1.0.so.0
0x729fc8f0  0x72a4c760  Yes         /opt/zinc-trunk/oss/lib/libgstbase-1.0.so.0
0x728e0f20  0x729b2e90  Yes         /opt/zinc-trunk/oss/lib/libgstreamer-1.0.so.0
0x7287fc70  0x728b0f80  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5Sql.so.5
0x72860d20  0x728624f0  Yes (*)     /lib/librt.so.0
0x7282b1c0  0x7284b500  Yes (*)     /usr/local/lib/libpng15.so.15
0x72663a70  0x727d0d10  Yes (*)     /opt/zinc-trunk/oss/lib/libicui18n.so.51
0x72495040  0x72578e20  Yes (*)     /opt/zinc-trunk/oss/lib/libicuuc.so.51
                        Yes (*)     /opt/zinc-trunk/oss/lib/libicudata.so.51
0x72111630  0x72111900  Yes (*)     /opt/zinc-trunk/oss/lib/libgthread-2.0.so.0
0x720cf190  0x720f34a0  Yes         /opt/zinc-trunk/oss/lib/libgsttag-1.0.so.0
0x773e0eb0  0x773e66f0  Yes (*)     /lib/ld-uClibc.so.0
0x716ffa60  0x7176c3c0  Yes (*)     /opt/zinc-trunk/oss/plugins/platforms/libqdirectfb.so
0x716d6800  0x716e2470  Yes (*)     /usr/local/lib/directfb-1.4-17-pure/systems/libdirectfb_bcmnexus_sys.so
0x5ab8a2a0  0x5ab92c60  Yes (*)     /usr/local/lib/directfb-1.4-17-pure/gfxdrivers/libdirectfb_bcmnexus_gfx.so
0x5ab6f740  0x5ab77a30  Yes (*)     /usr/local/lib/directfb-1.4-17-pure/wm/libdirectfbwm_sawman.so
0x5ab495c0  0x5ab5ab70  Yes (*)     /usr/local/lib/libsawman-1.5.so.0
0x5a1d22d0  0x5a2f1180  Yes         /opt/zinc-trunk/oss/lib/libcrypto.so.1.0.0
0x5a13edc0  0x5a17b970  Yes (*)     /opt/zinc-trunk/oss/lib/libssl.so.1.0.0
0x5a1171f0  0x5a11f870  Yes (*)     /opt/zinc-trunk/lib/engines/libyvSSLClientAuth.so
0x5a0f9d00  0x5a102450  Yes (*)     /opt/zinc-trunk/lib/libTitaniumTlsSelectorAPI.so.0
0x5a0b6090  0x5a0db500  Yes (*)     /opt/zinc-trunk/lib/libTitaniumUtils.so.0
0x5a07be60  0x5a096a00  Yes (*)     /opt/zinc-trunk/lib/libTitaniumTlsSelectorProduction.so
0x5a041140  0x5a05bcd0  Yes (*)     /opt/zinc-trunk/lib/libCopperSystemAPI.so.0
0x5a016ea0  0x5a023860  Yes (*)     /opt/zinc-trunk/lib/engines/libskb-yv.so
0x59ff5ab0  0x5a00c5b0  Yes (*)     /opt/zinc-trunk/lib/libTitaniumSystemKeysDbusClient.so.0
0x597b46b0  0x597d8280  Yes (*)     /opt/zinc-trunk/lib/libTitaniumApplicationContainerProduction.so
0x59798cb0  0x5979c6e0  Yes         /opt/zinc-trunk/lib/libZincHttpServer.so.0
0x5976f0f0  0x59782f50  Yes         /opt/zinc-trunk/lib/libZincHttpClient.so.0
0x59752d90  0x59758e30  Yes         /opt/zinc-trunk/lib/libZincHttpCurl.so.0
0x59724750  0x5973c860  Yes         /opt/zinc-trunk/lib/libZincHttpCommon.so.0
0x5970bbd0  0x5970f6f0  Yes         /opt/zinc-trunk/lib/libZincHttpMIMETypes.so.0
0x596a3c50  0x596ee010  Yes         /opt/zinc-trunk/oss/lib/libcurl.so.4
0x59682c30  0x5968d1a0  Yes         /opt/zinc-trunk/oss/lib/libmongoose.so.0
0x596405d0  0x59669050  Yes (*)     /opt/zinc-trunk/oss/lib/libcppunit-1.12.so.1
0x59606a30  0x5961aa50  Yes (*)     /opt/zinc-trunk/oss/lib/libgmock.so.0
0x595b2cc0  0x595e28d0  Yes (*)     /opt/zinc-trunk/oss/lib/libgtest.so.0
0x5955c010  0x59585390  Yes (*)     /opt/zinc-trunk/lib/libCopperSystemDbusClient.so
0x5953e1d0  0x5953efe0  Yes (*)     /opt/zinc-trunk/lib/libCopperSystemDbus.so.0
0x5679eb70  0x567a8ff0  Yes (*)     /opt/zinc-trunk/lib/jsplugins/libVanadiumJsCoreClientApi.so
0x56783cf0  0x56786500  Yes (*)     /opt/zinc-trunk/lib/libVanadiumClientApi.so.0
0x56759cd0  0x5676c4d0  Yes (*)     /opt/zinc-trunk/lib/libVanadiumClientSystem.so
0x5673e830  0x56743af0  Yes (*)     /opt/zinc-trunk/lib/libCopperAnnouncementSystemAPI.so.0
0x5671f900  0x56726950  Yes (*)     /opt/zinc-trunk/lib/jsplugins/libSodiumJsCoreClientApi.so
0x56708f00  0x56709df0  Yes (*)     /opt/zinc-trunk/lib/libSodiumClientAPI.so.0
0x566bed50  0x566ed9a0  Yes (*)     /opt/zinc-trunk/lib/libSodiumCommon.so.0
0x566a39e0  0x566a41d0  Yes (*)     /opt/zinc-trunk/lib/libSodiumSystemAPI.so.0
0x565e1310  0x56679990  Yes (*)     /opt/zinc-trunk/lib/libSodiumClientSystem.so
0x56572cf0  0x565b5390  Yes (*)     /opt/zinc-trunk/lib/libNickelTunerSystemDbusClient.so.0
0x5651bf20  0x56545bb0  Yes (*)     /opt/zinc-trunk/lib/libUraniumClientAPI.so.0
0x564e5d70  0x564e6300  Yes (*)     /opt/zinc-trunk/lib/libUraniumCommon.so.0
0x55cca1a0  0x55cd2190  Yes (*)     /opt/zinc-trunk/lib/jsplugins/libHeliumJsCoreClientApi.so
0x55cb33e0  0x55cb3bc0  Yes (*)     /opt/zinc-trunk/lib/libHeliumClientApi.so.0
0x55ca30e0  0x55cae550  Yes (*)     /opt/zinc-trunk/lib/libHeliumClientSystem.so
0x55c5fe90  0x55c85940  Yes (*)     /opt/zinc-trunk/lib/jsplugins/libNickelJsCoreClientApi.so
0x55c0fff0  0x55c31700  Yes (*)     /opt/zinc-trunk/lib/libNickelClientApi.so.0
0x55b78be0  0x55bd4140  Yes (*)     /opt/zinc-trunk/lib/libNickelClientSystem.so
0x55b42880  0x55b54520  Yes (*)     /opt/zinc-trunk/lib/libNickelSystemAPI.so.0
0x55b12f20  0x55b20420  Yes (*)     /opt/zinc-trunk/lib/libNickelAudioFeedbackDbusClient.so.0
0x55ae6180  0x55af8090  Yes (*)     /opt/zinc-trunk/lib/libHeliumSystemDbusClient.so
0x55acf760  0x55acf910  Yes (*)     /opt/zinc-trunk/lib/libHeliumSystemDbus.so.0
0x559d9640  0x55a788b0  Yes (*)     /opt/zinc-trunk/lib/libNickelSystemDbusClient.so
0x559aa9b0  0x559aab60  Yes (*)     /opt/zinc-trunk/lib/libNickelSystemDbus.so.0
0x5598cc80  0x559a36a0  Yes (*)     /opt/zinc-trunk/lib/libSodiumSystemDbusClient.so
0x559741a0  0x55974fb0  Yes (*)     /opt/zinc-trunk/lib/libSodiumSystemDbus.so.0
0x558fda60  0x55911bf0  Yes (*)     /opt/zinc-trunk/lib/libCopperAnnouncementSystemDbusClient.so
0x558e41b0  0x558e4fc0  Yes (*)     /opt/zinc-trunk/lib/libCopperAnnouncementSystemDbus.so.0
0x548d2320  0x548d2330  Yes (*)     /lib/libresolv.so
0x540049d0  0x54042a10  Yes (*)     /opt/zinc-trunk/lib/plugins/webkit/libVanadiumOipfPlugin.so.0.0.0
0x5451f810  0x5452c290  Yes (*)     /opt/zinc-trunk/lib/libVanadiumOipfPluginAPI.so.0
0x544ad280  0x544de660  Yes         /opt/zinc-trunk/lib/libVanadiumOipfPluginProduction.so

// This is a so in question which put some code to crash and see no debug info
// in it.
0x5446c670  0x5448c130  Yes (*)     /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0

0x53f3cc60  0x53facdb0  Yes (*)     /opt/zinc-trunk/lib/libNeonSystemDbusClient.so
0x536e12a0  0x53703cb0  Yes (*)     /opt/zinc-trunk/lib/libMercurySystemDbusClient.so
0x536c31a0  0x536c3fb0  Yes (*)     /opt/zinc-trunk/lib/libMercurySystemDbus.so.0
0x5368a560  0x536a9390  Yes (*)     /opt/zinc-trunk/lib/libMercuryCommon.so.0
0x535d9f80  0x5364a790  Yes (*)     /opt/zinc-trunk/lib/libCobaltSystemDbusClient.so
0x535b07d0  0x535b0980  Yes (*)     /opt/zinc-trunk/lib/libCobaltSystemDbus.so.0
0x535903e0  0x5359dc40  Yes (*)     /opt/zinc-trunk/lib/libCobaltCommon.so.0
(*): Shared library is missing debugging information.
          zero       at       v0       v1       a0       a1       a2       a3
 R0   00000000 00000001 00000064 00000001 77273d5c ffffffff 00000001 00000000 
            t0       t1       t2       t3       t4       t5       t6       t7
 R8   00000000 00000051 00000000 00000000 7fde3c38 00000001 73b03584 3031203e 
            s0       s1       s2       s3       s4       s5       s6       s7
 R16  004ea9b0 7fde3f6c 7fde3ef8 7642d7dc 54485000 7726f15c 7726e520 7726f124 
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00000000 771fc768 00000000 00000000 544ac170 7fde3ec0 7726f138 54479c54 
            sr       lo       hi      bad    cause       pc
      00008b13 00000000 00000000 00000000 0080000c 5447955c 
           fsr      fir
      02000074 00000000 
No symbol table info available.
No symbol table info available.
0x7fde3ec0:	0x0	0x7180f700	0x7622d5a0	0xb3dd98
0x7fde3ed0:	0x10d	0x71812a68	0x544ac170	0x7fde3eac
0x7fde3ee0:	0x1	0xffffffff	0xb33784	0xb7cea8
0x7fde3ef0:	0x4faa9000	0xb5c68c	0xb27724	0x0
0x7fde3f00:	0x7620a7e0	0xb7cea8	0xb7cea8	0x568dffe0
0x7fde3f10:	0xb7cea8	0x1	0x0	0x7558
0x7fde3f20:	0x3613c0	0xb2e800	0x0	0x7633f5c8 <_ZTVN9log4cplus6LoggerE+8>
0x7fde3f30:	0x4eb0d8	0xb33784	0x5448c8e8	0xfd
0x7fde3f40:	0x7726f15c	0x7726f2c0 <_ZTVSt15basic_streambufIcSt11char_traitsIcEE+8>	0xb0f44d	0xb0f44d
0x7fde3f50:	0xb0f44d	0xb0f44c	0xb0f47d	0xb0f64c
0x7fde3f60:	0x77273d5c	0x10	0xb0f44c	0x7726dbf8 <_ZTVSt8ios_base+8>
0x7fde3f70:	0x6	0x0	0x1002	0x0
0x7fde3f80:	0x0	0x0	0x0	0x0
0x7fde3f90:	0x0	0x0	0x0	0x0
0x7fde3fa0:	0x0	0x0	0x0	0x0
0x7fde3fb0:	0x0	0x0	0x0	0x0
0x7fde3fc0:	0x0	0x0	0x0	0x0
0x7fde3fd0:	0x8	0x7fde3f90	0x77273d5c	0x0
0x7fde3fe0:	0x0	0x7fde3f44	0x772730a4	0x7727334c
0x7fde3ff0:	0x77273344	0x4e1008c8	0x7fde3ef4	0x7fde3f44
0x7fde4000:	0x7fde3f60	0x7726f1e0 <_ZTVSt15basic_stringbufIcSt11char_traitsIcESaIcEE+8>	0x7726f2c0 <_ZTVSt15basic_streambufIcSt11char_traitsIcEE+8>	0x4ea9b0
0x7fde4010:	0x7726f170	0x73cf4384	0xb486b8	0x7605c000
0x7fde4020:	0x4acc50	0x7fde4984	0x1	0x0
0x7fde4030:	0x73ee1508	0x17fc9974	0x2	0x756e1fdc
0x7fde4040:	0x74cec000	0x74cf39c4 <_ZSt13__adjust_heapIN7WebCore17TimerHeapIteratorEiPNS0_9TimerBaseENS0_25TimerHeapLessThanFunctionEEvT_T0_S6_T1_T2_+340>	0x7622d5a0	0x12b6006a
=> 0x5447955c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+696>:	sw	v0,0(zero)
   0x54479560 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+700>:	lw	a0,0(s3)
   0x54479564 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+704>:	beqz	a0,0x54479da0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+2812>
   0x54479568 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+708>:	lw	t9,-32128(gp)
   0x5447956c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+712>:	lw	t9,-32296(gp)
   0x54479570 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+716>:	jalr	t9
   0x54479574 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+720>:	move	a1,zero
   0x54479578 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+724>:	bnez	v0,0x54479914 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+1648>
   0x5447957c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+728>:	lw	gp,24(sp)
   0x54479580 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+732>:	lw	v0,92(sp)
   0x54479584 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+736>:	mtc1	v0,$f0
   0x54479588 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+740>:	lw	v0,-32708(gp)
   0x5447958c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+744>:	cvt.s.w	$f1,$f0
   0x54479590 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+748>:	lwc1	$f0,-28112(v0)
   0x54479594 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+752>:	b	0x544794a8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+516>
   0x54479598 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+756>:	div.s	$f0,$f1,$f0
   0x5447959c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+760>:	lw	v0,116(sp)
   0x544795a0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+764>:	addiu	s0,sp,64
   0x544795a4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+768>:	lw	a1,-12(v0)
   0x544795a8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+772>:	lw	v0,-31588(gp)
   0x544795ac <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+776>:	lw	t9,-31824(gp)
   0x544795b0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+780>:	addiu	v0,v0,12
   0x544795b4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+784>:	sw	v0,64(sp)
   0x544795b8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+788>:	move	a0,s0
   0x544795bc <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+792>:	addiu	a1,a1,7
   0x544795c0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+796>:	jalr	t9
   0x544795c4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+800>:	lw	s7,104(sp)
   0x544795c8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+804>:	lw	gp,24(sp)
   0x544795cc <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+808>:	lw	a1,-32728(gp)
   0x544795d0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+812>:	lw	t9,-31972(gp)
   0x544795d4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+816>:	move	a0,s0
   0x544795d8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+820>:	addiu	a1,a1,31076
   0x544795dc <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+824>:	jalr	t9
   0x544795e0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+828>:	li	a2,7
   0x544795e4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+832>:	lw	gp,24(sp)
   0x544795e8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+836>:	lw	t9,-31940(gp)
   0x544795ec <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+840>:	move	a0,s0
   0x544795f0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+844>:	jalr	t9
   0x544795f4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+848>:	move	a1,s5
   0x544795f8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+852>:	lw	gp,24(sp)
   0x544795fc <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+856>:	lw	v0,124(sp)
   0x54479600 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+860>:	lw	a3,120(sp)
   0x54479604 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+864>:	lw	t9,-31744(gp)
   0x54479608 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+868>:	sw	v0,16(sp)
   0x5447960c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+872>:	move	a0,s6
   0x54479610 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+876>:	move	a1,s7
   0x54479614 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+880>:	jalr	t9
   0x54479618 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+884>:	move	a2,s0
   0x5447961c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+888>:	lw	gp,24(sp)
   0x54479620 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+892>:	lw	t9,-32180(gp)
[root@HUMAX /]# 


When build with -g and this is summarized output. Shows exact line which
caues null pointer access.

[Thread debugging using libthread_db enabled]
Core was generated by `/opt/zinc-trunk/bin/w3cEngine -debug -cache /app-data/client-cache -cache-size'.

Program terminated with signal 11, Segmentation fault.

#0  0x5443955c in vanadium::VanadiumMediaPlayer::currentTime (this=<optimized out>) at /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Vanadium/Vanadium.WebKit.VideoElement/src/VanadiumMediaPlayer.cpp:271

note: can show source if have sources?

271	/home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Vanadium/Vanadium.WebKit.VideoElement/src/VanadiumMediaPlayer.cpp: No such file or directory.
	in /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Vanadium/Vanadium.WebKit.VideoElement/src/VanadiumMediaPlayer.cpp
  Id   Target Id         Frame 

...

* 1    Thread 0x77273320 (LWP 5552) 0x5443955c in vanadium::VanadiumMediaPlayer::currentTime (this=<optimized out>) at /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Vanadium/Vanadium.WebKit.VideoElement/src/VanadiumMediaPlayer.cpp:271
#0  0x5443955c in vanadium::VanadiumMediaPlayer::currentTime (this=<optimized out>) at /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Vanadium/Vanadium.WebKit.VideoElement/src/VanadiumMediaPlayer.cpp:271


note: now has debugging info

0x5442c670  0x5444c130  Yes         /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0

=> 0x5443955c <vanadium::VanadiumMediaPlayer::currentTime() const+696>:	sw	v0,0(zero)


={============================================================================
*kt_linux_tool_300* gdb-core-code

#include <stdio.h>

int main()
{
    char *p = (char*)0;
    p[0] = 1;
    return 0;
}


={============================================================================
*kt_linux_tool_300* gdb-core-force-core-command

10.19 How to Produce a Core File from Your Program

A core file or core dump is a file that records the memory image of a running
process and its process status (register values etc.). Its primary use is
post-mortem debugging of a program that crashed while it ran outside a debugger.
A program that crashes automatically produces a core file, unless this feature
is disabled by the user. 

See Section 18.1 [Files], page 233, for information on invoking gdb in the
post-mortem debugging mode.

Occasionally, you may wish to produce a core file of the program you are
debugging in order to preserve a snapshot of its state. gdb has a special
command for that.

generate-core-file [file]
gcore [file]

Produce a core dump of the inferior process. The optional argument file
specifies the file name where to put the core dump. If not specified, the file
name defaults to ‘core.pid’, where pid is the inferior process ID.  Note that
this command is implemented only for some systems (as of this writing,
        gnu/Linux, FreeBSD, Solaris, and S390). 

On gnu/Linux, this command can take into account the value of the file
'/proc/pid/coredump_filter' when generating the core dump (see [set
        use-coredump-filter], page 144).


={============================================================================
*kt_linux_gdb_300* gdb-signal

5.4 Signals

A signal is an asynchronous event that can happen in a program. The operating
system defines the possible kinds of signals, and gives each kind a name and a
number. For example, in Unix SIGINT is the signal a program gets when you type
an interrupt character (often Ctrl-c); SIGSEGV is the signal a program gets
from referencing a place in memory far away from all the areas in use; SIGALRM
occurs when the alarm clock timer goes off (which happens only if your program
    has requested an alarm).

Some signals, including SIGALRM, are a normal part of the functioning of your
program. Others, such as SIGSEGV, indicate errors; these signals are fatal
(they kill your program immediately) if the program has not specified in
advance some other way to handle the signal. SIGINT does not indicate an error
in your program, but it is normally fatal so it can carry out the purpose of
the interrupt: to kill the program.

GDB has the ability to detect any occurrence of a signal in your program. You
can tell GDB in advance what to do for each kind of signal.

Normally, GDB is set up to let the non-erroneous signals like SIGALRM be
silently passed to your program (so as not to interfere with their role in the
    program’s functioning) but to stop your program immediately whenever an
error signal happens. You can change these settings with the handle command.

info signals
info handle

Print a table of all the kinds of signals and how GDB has been told to handle
each one. You can use this to see the signal numbers of all the defined types
of signals.

info signals sig

Similar, but print information only about the specified signal number.

info handle is an alias for info signals.

catch signal [signal... | ‘all’]

Set a catchpoint for the indicated signals. See Set Catchpoints, for details
about this command. *gdb-catch*

handle signal [keywords...]

Change the way GDB handles signal signal. The signal can be the number of a
signal or its name (with or without the ‘SIG’ at the beginning); a list of
signal numbers of the form ‘low-high’; or the word ‘all’, meaning all the
known signals. Optional arguments keywords, described below, say what change
to make.

The keywords allowed by the handle command can be abbreviated. Their full
names are:

nostop

GDB should not stop your program when this signal happens. It may still print
a message telling you that the signal has come in.

stop

GDB should stop your program when this signal happens. This implies the print
keyword as well.

print

GDB should print a message when this signal happens.

noprint

GDB should not mention the occurrence of the signal at all. This implies the
nostop keyword as well.

pass
noignore

GDB should allow your program to see this signal; your program can handle the
signal, or else it may terminate if the signal is fatal and not handled. pass
and noignore are synonyms.

nopass
ignore

GDB should not allow your program to see this signal. nopass and ignore are
synonyms.

When a signal stops your program, the signal is not visible to the program
until you continue. Your program sees the signal then, if pass is in effect
for the signal in question at that time. 

In other words, after GDB reports a signal, you can use the `handle command`
  with pass or nopass to control whether your program sees that signal when
  you continue.

The default is set to `nostop, noprint, pass for non-erroneous signals` such
as SIGALRM, SIGWINCH and SIGCHLD, and to `stop, print, pass` for the erroneous
signals.

<ex>
(gdb) info signals SIGSEGV
Signal        Stop      Print   Pass to program Description
SIGSEGV       Yes       Yes     Yes             Segmentation fault

You can also use the signal command to prevent your program from seeing a
signal, or cause it to see a signal it normally would not see, or to give it
any signal at any time. For example, if your program stopped due to some sort
of memory reference error, you might store correct values into the erroneous
variables and continue, hoping to see more execution; but your program would
probably terminate immediately as a result of the fatal signal once it saw the
signal. To prevent this, you can continue with ‘signal 0’. See Giving your
Program a Signal.

GDB optimizes for stepping the mainline code. If a signal that has 
`handle nostop and handle pass set` arrives while a stepping command (e.g.,
    stepi, step, next) is in progress, GDB lets the signal handler run and
then resumes stepping the mainline code once the signal handler returns. In
other words, `GDB steps over the signal handler.` This prevents signals that
you’ve specified `as not interesting (with handle nostop)` from changing the
focus of debugging unexpectedly. Note that the signal handler itself may still
hit a breakpoint, stop for another signal that has handle stop in effect, or
for any other event that normally results in stopping the stepping command
  sooner. Also note that GDB still informs you that the program received a
    signal if handle print is set.

  If you set `handle pass` for a signal, and your program sets up a handler for
  it, then issuing a stepping command, such as step or stepi, when your
  program is `stopped` due to the signal `will step into the signal handler` (if
      the target supports that).


// so can use stop and pass to setp into handler or can use nostop and pass
// and set a break on handler to step into handler

Likewise, if you use the queue-signal command to queue a signal to be
delivered to the current thread when execution of the thread resumes (see
    Giving your Program a Signal), then a stepping command will step into the
signal handler.


On some targets, GDB can inspect extra signal information associated with the
intercepted signal, before it is actually delivered to the program being
debugged. This information is exported by the convenience variable $_siginfo,
  and consists of data that is passed by the kernel to the signal handler at
    the time of the receipt of a signal. The data type of the information
    itself is target dependent. You can see the data type using the ptype
    $_siginfo command. On Unix systems, it typically corresponds to the
    standard siginfo_t type, as defined in the signal.h system header.

Here’s an example, on a GNU/Linux system, printing the stray referenced
address that raised a segmentation fault.

(gdb) continue
Program received signal SIGSEGV, Segmentation fault.
0x0000000000400766 in main ()
69        *(int *)p = 0;
(gdb) ptype $_siginfo
type = struct {
    int si_signo;
    int si_errno;
    int si_code;
    union {
        int _pad[28];
        struct {...} _kill;
        struct {...} _timer;
        struct {...} _rt;
        struct {...} _sigchld;
        struct {...} _sigfault;
        struct {...} _sigpoll;
    } _sifields;
}
(gdb) ptype $_siginfo._sifields._sigfault
type = struct {
    void *si_addr;
}
(gdb) p $_siginfo._sifields._sigfault.si_addr
$1 = (void *) 0x7ffff7ff7000


17.3 Giving your Program a Signal

signal signal

Resume execution where your program is stopped, but immediately give it the
signal signal. The signal can be the name or the number of a signal. For
example, on many systems signal 2 and signal SIGINT are both ways of sending
an interrupt signal.

Alternatively, if signal is zero, continue execution without giving a signal.
This is useful when your program stopped on account of a signal and would
ordinarily see the signal when resumed with the continue command; ‘signal 0’
causes it to resume without a signal.

Note: When resuming a multi-threaded program, signal is delivered to the
currently selected thread, not the thread that last reported a stop. This
includes the situation where a thread was stopped due to a signal. So if you
want to continue execution suppressing the signal that stopped a thread, you
should select that same thread before issuing the ‘signal 0’ command. If you
issue the ‘signal 0’ command with another thread as the selected one, GDB
detects that and asks for confirmation.

Invoking the signal command is not the same as invoking the kill utility from
the shell. Sending a signal with kill causes GDB to decide what to do with the
signal depending on the signal handling tables (see Signals). The signal
command passes the signal directly to your program.

signal does not repeat when you press RET a second time after executing the
command.


<ex>
(gdb) info signal SIGSEGV
Signal        Stop      Print   Pass to program Description
SIGSEGV       Yes       Yes     Yes             Segmentation fault

(gdb) i b
No breakpoints or watchpoints.

(gdb) run
Starting program: /home/kyoupark/git/kb/code-linux/ex_signal/sig_info_out
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
0x400ac2 ~ 0x400b5f ~ 0x400b6c

Program received signal SIGSEGV, Segmentation fault.
`0x0000000000400b66` in main () at sig_info.c:45
45          *causecrash = 0;


(gdb) stepi
handler (si=0x0, ptr=0x0) at sig_info.c:20
20      {

(gdb) next
21         ucontext_t *uc = (ucontext_t *)ptr;
(gdb) next
23         cout << "si:" << si->si_addr << '\n';

(gdb) ptype uc
type = struct ucontext {
    unsigned long uc_flags;
    ucontext *uc_link;
    stack_t uc_stack;
    mcontext_t uc_mcontext;
    __sigset_t uc_sigmask;
    _libc_fpstate __fpregs_mem;
} *

(gdb) p/x (ucontext_t)*uc
$6 = {
  uc_flags = 0x1,
  uc_link = 0x0,
  uc_stack = {
    ss_sp = 0x0,
    ss_flags = 0x2,
    ss_size = 0x0
  },
  uc_mcontext = {
    gregs = {0x0, 0x7fffffffdf30, 0x8, 0x202, 0x400930, 0x7fffffffe160, 0x0, 0x0, 0xb, 0x7fffffffde90, 0x7fffffffe080, 0x0, 0x0, 0x0, 0x7ffff719599d, 0x7fffffffdfe0, 
      `0x400b66`, 0x10206, 0x33, 0x6, 0xe, 0x0, 0x0},
    fpregs = 0x7fffffffdc00,
    __reserved1 = {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}
  },
  uc_sigmask = {
    __val = {0x0, 0xb, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x7ffff7de4e4c, 0x0, 0xd827524, 0x6, 0x8, 0x7ffff7fd6658}
  },
  __fpregs_mem = {
    cwd = 0x57be,
    swd = 0xf7de,
    ftw = 0x7fff,
    fop = 0x0,
    rip = 0x7ffff7fd6658,
    rdp = 0x7ffff7de4e4c,
    mxcsr = 0x37f,
    mxcr_mask = 0x0,
    _st = {{
        significand = {0x0, 0x0, 0x0, 0x0},
        exponent = 0x0,
        padding = {0x0, 0x0, 0x0}
      }, {
        significand = {0x1f80, 0x0, 0xffff, 0x0},
        exponent = 0x0,
        padding = {0x0, 0x0, 0x0}
      }, {
        significand = {0x0, 0x0, 0x0, 0x0},
        exponent = 0x0,
        padding = {0x0, 0x0, 0x0}
      }, {
        significand = {0x0, 0x0, 0x0, 0x0},
        exponent = 0x0,
        padding = {0x0, 0x0, 0x0}
      }, {
        significand = {0x0, 0x0, 0x0, 0x0},
        exponent = 0x0,
        padding = {0x0, 0x0, 0x0}
      }, {
        significand = {0x0, 0x0, 0x0, 0x0},
        exponent = 0x0,
        padding = {0x0, 0x0, 0x0}
      }, {
        significand = {0x0, 0x0, 0x0, 0x0},
        exponent = 0x0,
        padding = {0x0, 0x0, 0x0}
      }, {
        significand = {0x0, 0x0, 0x0, 0x0},
        exponent = 0x0,
        padding = {0x0, 0x0, 0x0}
      }},
    _xmm = {{
        element = {0x0, 0x0, 0x0, 0x0}
      }, {
        element = {0x0, 0x0, 0x0, 0x0}
      }, {
        element = {0x0, 0x0, 0xeae9e8e7, 0xeeedeceb}
      }, {
        element = {0xf2f1f0ef, 0xf6f5f4f3, 0x0, 0x0}
      }, {
        element = {0x0, 0x0, 0xff, 0x0}
      }, {
        element = {0xff0000, 0x0, 0xff0000, 0xff00}
      }, {
        element = {0x0, 0x0, 0x0, 0x0}
      }, {
        element = {0x0, 0x0, 0x0, 0x0}
      }, {
        element = {0x0, 0x0, 0x0, 0x0}
      }, {
        element = {0x0, 0x0, 0x0, 0x0}
      }, {
        element = {0x0, 0x0, 0x0, 0x0}
      }, {
        element = {0x0, 0x0, 0x0, 0x0}
      }, {
        element = {0x0, 0x0, 0x0, 0x0}
      }, {
        element = {0x0, 0x0, 0xff000000, 0x0}
      }, {
        element = {0xffff00, 0x0, 0x0, 0x0}
      }, {
        element = {0x0, 0x0, 0x0, 0x0}
      }},
    padding = {0x0 <repeats 18 times>, 0x46505853, 0x344, 0x7, 0x0, 0x340, 0x0}
  }
}

<ex>
which one is right to see siginfo? do not use $_siginfo since it represents
SIGTRAP which is used for gdb

kyoupark@kit-debian64:/mnt/git/kb/asan$ kill -l

5) SIGTRAP
11) SIGSEGV

(gdb) p $_siginfo
$7 = {
  si_signo = 5,
  si_errno = 0,
  si_code = 2,
  _sifields = {
    _pad = {4196922, 0, 0, 0, 0, 0, 7, 1, 62, 0, 16213600, 0, 16278992, 0, 5666947, 0, 4196922, 0, 0, 0, 16308384, 0, 16213600, 0, 17288048, 0, -1321253344, 32763},
    _kill = {
      si_pid = 4196922,
      si_uid = 0
    },
    _timer = {
      si_tid = 4196922,
      si_overrun = 0,
      si_sigval = {
        sival_int = 0,
        sival_ptr = 0x0
      }
    },
    _rt = {
      si_pid = 4196922,
      si_uid = 0,
      si_sigval = {
        sival_int = 0,
        sival_ptr = 0x0
      }
    },
    _sigchld = {
      si_pid = 4196922,
      si_uid = 0,
      si_status = 0,
      si_utime = 0,
      si_stime = 30064771072
    },
    _sigfault = {
      si_addr = 0x400a3a <handler(int, siginfo_t*, void*)+20>
    },
    _sigpoll = {
      si_band = 4196922,
      si_fd = 0
    }
  }
}

(gdb) p (siginfo_t) si

// shows wrong values

(gdb) p (siginfo_t) *si
$11 = {
  si_signo = 11,
  si_errno = 0,
  si_code = 1,
  _sifields = {
    _pad = {0 <repeats 14 times>, -136425908, 32767, 0, 0, 226653476, 0, 6, 0, 8, 0, -134388136, 32767, -136423490, 32767},
    _kill = {
      si_pid = 0,
      si_uid = 0
    },
    _timer = {
      si_tid = 0,
      si_overrun = 0,
      si_sigval = {
        sival_int = 0,
        sival_ptr = 0x0
      }
    },
    _rt = {
      si_pid = 0,
      si_uid = 0,
      si_sigval = {
        sival_int = 0,
        sival_ptr = 0x0
      }
    },
    _sigchld = {
      si_pid = 0,
      si_uid = 0,
      si_status = 0,
      si_utime = 0,
      si_stime = 0
    },
    _sigfault = {
      si_addr = 0x0,
      si_addr_lsb = 0
    },
    _sigpoll = {
      si_band = 0,
      si_fd = 0
    },
    _sigsys = {
      _call_addr = 0x0,
      _syscall = 0,
      _arch = 0
    }
  }
}


={============================================================================
*kt_linux_gdb_000* gdb-frame

frame n 

Select frame number n. Recall that frame zero is the innermost (currently
    executing) frame, frame one is the frame that called the innermost one, and
so on. The highest-numbered frame is the one for main.


={============================================================================
*kt_linux_gdb_300* gdb-core-backtrace gdb-bt

8.2 Backtraces

A backtrace is a summary of how your program got where it is. It shows one
line per frame, for many frames, starting with the currently executing frame
(frame zero), followed by its caller (frame one), and on up the stack.


backtrace
bt 

Print a backtrace of the entire stack: one line per frame for all frames in
the stack. You can stop the backtrace at any time by typing the system
interrupt character, normally Ctrl-c.


backtrace n
bt n 

Similar, but print only the innermost n frames.


backtrace full
bt full
bt full n
bt full -n

`Print the values of the local variables also.` As described above, n specifies
the number of frames to print.

The names `where` and `info stack` (abbreviated info s) are additional aliases
for backtrace.


When see call traces using bt, it does not show:

1. function calls which is completed along the call tree.
2. function calls which do "sleep()" call in an atempt to see if call traces
up to this point when attaching gdb.

Q: how to put some code to provide a point to use gdb?


={============================================================================
*kt_linux_tool_300* gdb-core-analysis-on-mips

export LD_LIBRARY_PATH=/home/NDS-UK/parkkt/bins
mips-gdb
set solib-absolute-prefix /junk
set solib-search-path 
  /parkkt/com.nds.darwin.debugsupport/debug_libs/uClibc-nptl-0.9.29-20070423
file APP_Process 
core 272.COR
thread apply all bt full
bt

(gdb) f n

{1}
(gdb) bt
#0 memset () at libc/string/mips/memset.S:132
...

(gdb) i r
	zero     at       v0         v1       a0         a1       a2       a3
R0	00000000 fffffffc [00000000] ffffffff [00000008] 00000000 00000004 00022000
	t0 t1 t2 t3 t4 t5 t6 t7
R8	00000004 00000000 ffffffff 000000c2 00000000 00000004 00a52630 00000000
	s0 s1 s2 s3 s4 s5 s6 s7
R16 00020000 00000000 00022004 2c57b174 00000004 2c70d4d0 2d9dcd30 00be6f70
	t8 t9 k0 k1 gp sp s8 ra
R24 00befcc8 2ab25160 00000000 00000000 00bf7e60 2d9dc858 2d9dcbd0 00993fdc
	sr       lo       hi       bad        cause    pc
	00008413 00000000 00000000 [00000000] 0080000c [2ab251b4]
	fsr fir
	00001004 00000000

// void *memset(void *s, int c, size_t n);

(gdb) disassemble $pc
Dump of assembler code for function memset:
0x2ab25160 <memset+0>: slti t1,a2,8
0x2ab25164 <memset+4>: bnez t1,0x2ab251d4 <memset+116>
0x2ab25168 <memset+8>: move v0,a0 ~
0x2ab2516c <memset+12>: beqz a1,0x2ab25184 <memset+36>
0x2ab25170 <memset+16>: andi a1,a1,0xff
0x2ab25174 <memset+20>: sll t0,a1,0x8
0x2ab25178 <memset+24>: or a1,a1,t0
0x2ab2517c <memset+28>: sll t0,a1,0x10
0x2ab25180 <memset+32>: or a1,a1,t0
0x2ab25184 <memset+36>: negu t0,a0 # negu d, s; d = -s;
0x2ab25188 <memset+40>: andi t0,t0,0x3
0x2ab2518c <memset+44>: beqz t0,0x2ab2519c <memset+60>
0x2ab25190 <memset+48>: subu a2,a2,t0
0x2ab25194 <memset+52>: swl a1,0(a0) # swl t, addr; Store word left/right
0x2ab25198 <memset+56>: addu a0,a0,t0
0x2ab2519c <memset+60>: andi t0,a2,0x7
0x2ab251a0 <memset+64>: beq t0,a2,0x2ab251c0 <memset+96>
0x2ab251a4 <memset+68>: subu a3,a2,t0
0x2ab251a8 <memset+72>: addu a3,a3,a0
0x2ab251ac <memset+76>: move a2,t0
0x2ab251b0 <memset+80>: addiu a0,a0,8 ~
0x2ab251b4 <memset+84>: sw a1,-8(a0) ~
0x2ab251b8 <memset+88>: bne a0,a3,0x2ab251b0 <memset+80>

note: 
This shows that a0 which is the first argument of memset fuction and is NULL.
Hence SIGSEGV. *sigseg*


{2}
(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc38, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x007f09c8 in MHWMemAllocStatic (pool=0x131bc38, size=64) at mem_static.c:63
#2  0x007e6854 in MEMMAN_API_AllocStaticP (pool=0x131bc38, size=41) at memman_st.c:350
#3  0x00419328 in DIAG_JAVA_GetJavaString (env=0x2d7056c0, 
        l_java_string=0x2c4b3358, l_byte_array=0x2d705698, 
        l_len=0x2d70569c) at ../src/natdiag.c:63
#4  0x004197fc in DIAG_JAVA_GetJavaString (env=0x131bc38) at ../src/natdiag.c:341
#5  0x0041a0f8 in Java_com_nds_fusion_diagimpl_DiagImpl_natLogInfo (
        THIS=<value optimized out>, jClass=0x40, jInt=1, 
        jString=0x0) at sunnatdiag.c:177

[New process 124]
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
271     mem_blockpool.c: No such file or directory.
        in mem_blockpool.c

(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x72617469 in ?? ()
Backtrace stopped: previous frame inner to this frame (corrupt stack?)

(gdb) i r
          zero       at       v0       v1       a0       a1       a2       a3
 R0   00000000 00000005 00000004 00000000 0131bc40 00000040 00000001 00000000 
            t0       t1       t2       t3       t4       t5       t6       t7
 R8   0000000c 00000001 ffffffff ffffffff 00000000 00000000 00d7f220 00000200 
            s0       s1       s2       s3       s4       s5       s6       s7
 R16  00000000 0131bc40 0131bc40 00000040 00000001 00000000 00000001 2d705b00 
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00f0c360 00b62d98 00000000 00000000 00f14570 2d7055a0 2d7057d8 007f09c8 
            sr       lo       hi      bad    cause       pc
      00008413 000efdcb 00000005 00000018 00800008 007eb73c 
           fsr      fir
      00001004 00000000 

(gdb) disassemble $pc

// uint32_t MHWMemCheckBank(MEMMAN_API_MemoryPoolHandle * mpool, 
//    uint32_t size, uint32_t mem_nb_bank)
// {
//     MemWholeMemory *bank = mpool->s_WholeMem;
// }

Dump of assembler code for function MHWMemCheckBank:
0x007eb6b8 <MHWMemCheckBank+0>: addiu   sp,sp,-48
0x007eb6bc <MHWMemCheckBank+4>: sw      s4,40(sp)
0x007eb6c0 <MHWMemCheckBank+8>: sw      s3,36(sp)
0x007eb6c4 <MHWMemCheckBank+12>:        sw      s2,32(sp)
0x007eb6c8 <MHWMemCheckBank+16>:        sw      ra,44(sp)
0x007eb6cc <MHWMemCheckBank+20>:        sw      s1,28(sp)
0x007eb6d0 <MHWMemCheckBank+24>:        sw      s0,24(sp)
0x007eb6d4 <MHWMemCheckBank+28>:        lw      s0,40(a0)
0x007eb6d8 <MHWMemCheckBank+32>:        lw      v1,36(a0)
0x007eb6dc <MHWMemCheckBank+36>:        lui     v0,0x4
0x007eb6e0 <MHWMemCheckBank+40>:        lw      a3,108(a0)
0x007eb6e4 <MHWMemCheckBank+44>:        and     v1,v1,v0
0x007eb6e8 <MHWMemCheckBank+48>:        addiu   t0,s0,12
0x007eb6ec <MHWMemCheckBank+52>:        addiu   v0,s0,4
0x007eb6f0 <MHWMemCheckBank+56>:        move    s2,a0
0x007eb6f4 <MHWMemCheckBank+60>:        movn    t0,v0,v1
0x007eb6f8 <MHWMemCheckBank+64>:        move    s3,a1
0x007eb6fc <MHWMemCheckBank+68>:        beqz    a3,0x7eb73c <MHWMemCheckBank+132>
0x007eb700 <MHWMemCheckBank+72>:        move    s4,a2
0x007eb704 <MHWMemCheckBank+76>:        lw      a1,0(t0)
0x007eb708 <MHWMemCheckBank+80>:        lw      v1,24(s0)
0x007eb70c <MHWMemCheckBank+84>:        lw      v0,104(a0)
0x007eb710 <MHWMemCheckBank+88>:        li      a2,100
0x007eb714 <MHWMemCheckBank+92>:        mul     v1,v1,a2
0x007eb718 <MHWMemCheckBank+96>:        mul     v0,a1,v0
0x007eb71c <MHWMemCheckBank+100>:       sltu    v0,v1,v0
0x007eb720 <MHWMemCheckBank+104>:       beqz    v0,0x7eb73c <MHWMemCheckBank+132>
0x007eb724 <MHWMemCheckBank+108>:       nop
0x007eb728 <MHWMemCheckBank+112>:       divu    zero,v1,a1
0x007eb72c <MHWMemCheckBank+116>:       teq     a1,zero,0x7
0x007eb730 <MHWMemCheckBank+120>:       mflo    a1
0x007eb734 <MHWMemCheckBank+124>:       jal     0x7efa14  <MEMMAN_SHL_Notify>
0x007eb738 <MHWMemCheckBank+128>:       subu    a1,a2,a1
0x007eb73c <MHWMemCheckBank+132>:       lw      v0,24(s0) ~
...
---Type <return> to continue, or q <return> to quit---

(gdb) info locals
ind_bank = <value optimized out>
bank = (MemWholeMemory *) 0x0           // note: "bank = mpool->sWholeMem;"
pool_max = (uint32_t *) 0xc

(gdb) x/16wx 0x0131bc40 	// value of a0
0x131bc40:      0x0131f408      0x2ab97ca0      0x00000000      0x00000000
0x131bc50:      0x00000000      0x00000000      0x00000000      0x00000000
0x131bc60:      0xffffffff      0x00000000      0x00000000      0x41445054
0x131bc70:      0x5f444941      0x47000000      0x00000000      0x00000000

note:
This shows that structure passed in a0 has some NULLs and means that this pool
was already destoried. When see destory func, it sets:

poolHandle->s_WholeMem=NULL;


{3}
When a crash in a kernel module happens, you should see output like the
following on the serial or in the dmesg buffer (just run the dmesg command to
    see it). 

// (gdb) i r
//           zero       at       v0       v1       a0       a1       a2       a3
//  R0   00000000 00000005 00000004 00000000 0131bc40 00000040 00000001 00000000 
//             t0       t1       t2       t3       t4       t5       t6       t7
//  R8   0000000c 00000001 ffffffff ffffffff 00000000 00000000 00d7f220 00000200 
//             s0       s1       s2       s3       s4       s5       s6       s7
//  R16  00000000 0131bc40 0131bc40 00000040 00000001 00000000 00000001 2d705b00 
//             t8       t9       k0       k1       gp       sp       s8       ra
//  R24  00f0c360 00b62d98 00000000 00000000 00f14570 2d7055a0 2d7057d8 007f09c8 
//             sr       lo       hi      bad    cause       pc
//       00008413 000efdcb 00000005 00000018 00800008 007eb73c 
//            fsr      fir
//       00001004 00000000 

<4>Unhandled kernel unaligned access[#1]:
<4>Cpu 0
<4>$ 0   : 00000000 10008400 <f7ffdfdf> 80070000                    // note: `v0`
<4>$ 4   : c06e27c0 000ee208 8123a000 898d2680
<4>$ 8   : 00000000 7edbffff ffdeff7f fffb7fff
<4>$12   : fdf7fed7 000ee247 00000001 00000001
<4>$16   : 898d2680 00000000 00000001 00000001
<4>$20   : 898d2680 c06e2800 898d2680 00000001
<4>$24   : 00000001 c0504bcc                  
<4>$28   : 89cd2000 89cd3830 000ee208 c050cbe4
<4>Hi    : 00000128
<4>Lo    : 003e5708
<4>epc   : c050cc54 XHddLowIO+0x124/0x3d8 [xtvfs]     Tainted: P    // note `pc`
<4>ra    : c050cbe4 XHddLowIO+0xb4/0x3d8 [xtvfs]
<4>Status: 10008403    KERNEL EXL IE 
<4>Cause : 00800010
<4>BadVA : f7ffe073
<4>PrId  : 0002a044
<4>Modules linked in: xtvfs mhxnvramfs callisto_periph callisto_tuner callisto
<4>Process mount (pid: 404, threadinfo=89cd2000, task=89a249e8)
<4>Stack : 00000001 000ee08d 00000000 c04e523c c05394c4 00000000 00000000 f7ffdfdf
<4>        c06e2800 003e5708 00000001 000ee208 00000000 c04e523c c05394c4 00000000
<4>        c053950c c050cf50 00808000 c050d21c c06e2800 8567bc00 00000000 003e5708
<4>        c04d7f78 c04d7f58 ff7fffdf 000edf25 00000001 00000001 c067e200 c04d804c
<4>        c05394c4 89cd3950 00000000 c04e523c 000ee208 c06e2800 00000001 00000000
<4>        ...
<4>Call Trace:
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]
<4>[<c050cf50>] XHddLowRead+0x1c/0x28 [xtvfs]
<4>[<c04d7f78>] root_dir_devio_read+0x58/0x12c [xtvfs]
<4>[<c04d809c>] root_dir_devio_lock_read+0x50/0x84 [xtvfs]
<4>[<c04d8430>] RootDirCpyClusterReadBuffer+0xec/0x180 [xtvfs]
<4>[<c04d90cc>] RootDirCpyCheckCreate+0xf0/0x1214 [xtvfs]
<4>[<c04da344>] RootDirCpyInit+0x154/0x200 [xtvfs]
<4>[<c04d2944>] pc_ppart_init+0x10bc/0x12a8 [xtvfs]
<4>[<c04fb178>] XTVFS_CheckPpartInit+0x38/0x32c [xtvfs]
<4>[<c04fc684>] InitPpart+0x238/0x540 [xtvfs]
<4>[<c04fca28>] XTVFS_Mount+0x9c/0x490 [xtvfs]
<4>[<c050c8c8>] xtvfs_read_super+0x1e0/0x370 [xtvfs]
<4>[<c050bb80>] xtvfs_fill_super+0x18/0x48 [xtvfs]
<4>[<8007a6b0>] get_sb_bdev+0x114/0x194
<4>[<c050bb5c>] xtvfs_get_sb+0x2c/0x38 [xtvfs]
<4>[<80079f2c>] vfs_kern_mount+0x68/0xc4
<4>[<80079fe4>] do_kern_mount+0x4c/0x7c
<4>[<80094f10>] do_mount+0x5a8/0x614
<4>[<80095010>] sys_mount+0x94/0xec
<4>[<8000e5f0>] stack_done+0x20/0x3c
<4>
<4>
<4>Code: 00008821  8fa2001c  3c038007 <8c440094> 24631824  0060f809  8c46000c  ae020000  27de0001


Unhandled kernel unaligned access 

An unaligned access is a type of crash. Unlike a segmentation fault, where a
process tries to read memory that is not in its memory map, and unaligned
access is an attempt to read or write an address that is not on a word
boundry. On 32 bit CPUs this means an address not divisble by 4. Often this
will generate an exception and some software will handle the access by reading
adjacent words and piecing things together. But in our case the exception is
unhandled. 

This output shows the value of the registers. We are on a MIPS CPU and many of
the registers have defined uses. 

epc c050cc54 XHddLowIO+0x124/0x3d8 [xtvfs] 

This is the `exception-program-counter`. It shows the address that the
exception occurred at, and that this is 0x124 bytes into the function
XHddLowIO in the module xtvfs.ko. 


Getting the disassembly

Given a kernel module like xtvfs.ko, it is possible to see the disassembled
code using the objdump -D command. Since we have a mips module, we use the
cross-compiler from the Clearcase Fusion view, so our command will look
something like: 

mips-linux-uclibc-objdump -D xtvfs.ko

We can then look for the function where we crashed, which is XHddLowIO from
the epc trace above. It starts like this: 

00045b30 <XHddLowIO>:
   45b30:       27bdffb8        addiu   sp,sp,-72
   45b34:       3c020002        lui     v0,0x2
   45b38:       afb50034        sw      s5,52(sp)
   45b3c:       345500d0        ori     s5,v0,0xd0
   45b40:       3c020000        lui     v0,0x0
   45b44:       afb7003c        sw      s7,60(sp)

From the call trace we can calculate the address offset in use. Recall: 

<4>epc   : c050cc54 XHddLowIO+0x124/0x3d8 [xtvfs]     Tainted: P    // note `pc`
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]

Virtual         
0xxxxxx         00045b30
    +0x124          +0x124
c050cc54        00045c54

So c050cc54 - 0x124 - 45b30 = offset = c04c7000 (the start of loadded in memory
    for this xtvfs.ko). The crash happened at c050cc54 which will appear as
c050cc54 - c04c7000 = 45c54 in the disassembly. That code looks like this: 

or 45b30+0x124 = 45c54


   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)
   45c50:       3c030000        lui     v1,0x0
 {45c54}:       8c440094        lw      a0,148(v0)
   45c58:       24630000        addiu   v1,v1,0
   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

So we have crashed executing an lw instruction. 


Using the relocation table

Given a `kernel module` like xtvfs.ko, it is possible to see the offsets of
functions (the relocation table) using the objdump -r command. Since we have a
mips module, we use the cross-compiler from the Clearcase Fusion view, so our
command will look something like: 

mips-linux-uclibc-objdump -r xtvfs.ko > xtvfs_relocations

//       -r
//       --reloc
//           Print the relocation entries of the file.  If used with -d or -D,
//           the relocations are printed interspersed with the disassembly.

The output near to our crash address of 45c54 is a table like this: 

 00045c20 R_MIPS_26         .text
 00045c2c R_MIPS_26         .text
 00045c44 R_MIPS_26         .text
 00045c50 R_MIPS_HI16       __getblk
 00045c58 R_MIPS_LO16       __getblk
 00045c78 R_MIPS_HI16       printk
 00045c80 R_MIPS_LO16       printk


Because we are using load time relocation of shared libraries, this table
tells the operating system how to replace addresses in the code. 

The first column is the address in the code, the second column is the type of
relocation to do, and the third column is the address to relocate. So the code
at address 45c50 and 45c58 will get 'overwritten' with the address of __getblk.
That makes the disassembly of the code near our crash look like this: 

   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)

   45c50:       3c030000        lui     v1,__getblk     // note: overwritten
   45c54:       8c440094        lw      a0,148(v0) ~
   45c58:       24630000        addiu   v1,v1,0

   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

Understanding __getblk

At 45c5c there is a jump instruction to v1 which has been loaded with the
address of __getblk. But we crashed immediately before that.

    "So it seems we crashed while 'preparing' to call __getblk." 

So it would help to understand this function. We can look it up in the kernel
code: http://lxr.free-electrons.com/source/fs/buffer.c?v=2.6.30;a=mips#L1363

1362 /*
1363  * __getblk will locate (and, if necessary, create) the buffer_head
1364  * which corresponds to the passed block_device, block and size. The
1365  * returned buffer has its reference count incremented.
1366  *
1367  * __getblk() cannot fail - it just keeps trying.  If you pass it an
1368  * illegal block number, __getblk() will happily return a buffer_head
1369  * which represents the non-existent block.  Very weird.
1370  *
1371  * __getblk() will lock up the machine if grow_dev_page's try_to_free_buffers()
1372  * attempt is failing.  FIXME, perhaps?
1373  */
1374 struct buffer_head *
1375 __getblk(struct block_device *bdev, sector_t block, unsigned size)
1376 {
1377         struct buffer_head *bh = __find_get_block(bdev, block, size);
1378 
1379         might_sleep();
1380         if (bh == NULL)
1381                 bh = __getblk_slow(bdev, block, size);
1382         return bh;
1383 }
1384 EXPORT_SYMBOL(__getblk);

We should also notice that it can be called via inline function sb_blk : 

287 static inline struct buffer_head *
288 sb_getblk(struct super_block *sb, sector_t block)
289 {
290         return __getblk(sb->s_bdev, block, sb->s_blocksize);
291 }


Understanding where in our C code the crash happened

Now we can tell exactly where in our C code the crash happened. We know we were
in the function XHddLowIO from the call trace and now we know we were calling
__getblk or sb_getblk. In XHddLowIO in the XTVFS code we can see: 


/* allocate sector buffers */
for(i = 0; i < cnt; i++){
    bh_array[i] = sb_getblk(sb,  block++);                          // note
    if(!bh_array[i]){ /* no sufficient buffers */
        printk("\n BH_ArrayXHddLowIO: bh = 0, i = %d !!!!!", i);
        if(0 == i){ /* no at all */
            return X_ERROR;
        }
        /* use what we have */
        cnt = i;
    }

So it is likely that the crash happened very close to this sb_getblk call. 


Understanding the lw instruction

Recall the instruction that crashed: 

   45c54:       8c440094        lw      a0,148(v0)

What does that notation mean? We can look up information about the MIPS
instructions: Description: A word is loaded into a register from the specified
address. 

Operation: $t = MEM[$s + offset]; advance_pc (4); Syntax: lw $t, offset($s)

The whole instruction means: load a0 with the address in v0 + 148. 


`a0`, the register we are writing to, is the first of the "function argument
registers that hold the first four words of integer type arguments." So a0 is
the first argument to the function we are calling.  

`v0` is a "function result register" and is also called $2. So we know its value
from the original trace: 

<4>$ 0   : 00000000 10008400 f7ffdfdf 80070000

v0 is f7ffdfdf. Which is an *odd number*. Since we are trying to read from
this address and do arithmetic (add 148) with it, this would explain why we
get an unaligned access. 


Putting it all together

We are executing this line of C: 

bh_array[i] = sb_getblk(sb,  block++);

<conclusion>
Because sb_getblk is an inline function, it has been expanded by the compiler
into: __getblk(sb->s_bdev, block, sb->s_blocksize); So our crashing
instruction is adding 148 because 148 is the offset of s_bdev withing the sb
struct. We hav verify this by looking at struct super_block in the code.
However, sb has somehow become and odd number, and THAT is our bug. 


{4}
(gdb) bt
#0  getPresentationTimeCached (timeFormat=GST_FORMAT_TIME, this=0x56b900) 
    at /r.cpp:1433

(gdb) disassemble $pc
Dump of assembler code for function nickel::system::GstMediaRouter::getPosition_locked(bool) const:
   0x73a1dac0 <+0>:	lui	gp,0x8
   0x73a1dac4 <+4>:	addiu	gp,gp,7440
   0x73a1dac8 <+8>:	addu	gp,gp,t9
   0x73a1dacc <+12>:	addiu	sp,sp,-328
   0x73a1dad0 <+16>:	sw	s7,316(sp)
   0x73a1dad4 <+20>:	lw	s7,-29712(gp)
   0x73a1dad8 <+24>:	sw	s3,300(sp)
   0x73a1dadc <+28>:	lw	s3,0(s7)
   0x73a1dae0 <+32>:	sw	gp,24(sp)
   0x73a1dae4 <+36>:	sw	s8,320(sp)
   0x73a1dae8 <+40>:	sw	s1,292(sp)
   0x73a1daec <+44>:	sw	s0,288(sp)
   0x73a1daf0 <+48>:	sw	ra,324(sp)
   0x73a1daf4 <+52>:	sw	s6,312(sp)
   0x73a1daf8 <+56>:	sw	s5,308(sp)
   0x73a1dafc <+60>:	sw	s4,304(sp)
   0x73a1db00 <+64>:	sw	s2,296(sp)
   0x73a1db04 <+68>:	move	s1,a0
   0x73a1db08 <+72>:	move	s0,a1
   0x73a1db0c <+76>:	beqz	s3,0x73a1df18 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+1112>
   0x73a1db10 <+80>:	andi	s8,a2,0xff
   0x73a1db14 <+84>:	lw	a1,-32724(gp)
   0x73a1db18 <+88>:	lw	t9,-30044(gp)
   0x73a1db1c <+92>:	addiu	s2,sp,36
   0x73a1db20 <+96>:	move	a0,s2
   0x73a1db24 <+100>:	addiu	a1,a1,27172
   0x73a1db28 <+104>:	jalr	t9
   0x73a1db2c <+108>:	addiu	a2,sp,32
   0x73a1db30 <+112>:	lw	gp,24(sp)
   0x73a1db34 <+116>:	addiu	v0,sp,64
---Type <return> to continue, or q <return> to quit---q


Use `disassemble` and  get the same result but don't need to work out
function address.


(gdb) x/10i $pc
=> 0x73a1de64 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+932>:	lw	v0,0(a0)
   0x73a1de68 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+936>:	lw	t9,44(v0)
   0x73a1de6c <nickel::system::GstMediaRouter::getPosition_locked(bool) const+940>:	jalr	t9
   0x73a1de70 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+944>:	li	a1,3
   0x73a1de74 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+948>:	lw	gp,24(sp)
   0x73a1de78 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+952>:	sw	v0,276(sp)
   0x73a1de7c <nickel::system::GstMediaRouter::getPosition_locked(bool) const+956>:	b	0x73a1dbac <nickel::system::GstMediaRouter::getPosition_locked(bool) const+236>
   0x73a1de80 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+960>:	move	s5,v1
   0x73a1de84 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+964>:	bne	s5,s2,0x73a1dc48 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+392>
   0x73a1de88 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+968>:	move	v0,zero
(gdb) 

(gdb) p getPresentationTimeCached
$3 = {int64_t (const nickel::system::GstMediaRouter * const, GstFormat)} 0x73a1c7c4 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const>

(gdb) x/50i 0x73a1c7c4
   0x73a1c7c4 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const>:	lbu	v0,40(a0)
   0x73a1c7c8 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+4>:	beqz	v0,
   0x73a1c7dc <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+24>
   0x73a1c7cc <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+8>:	nop
   0x73a1c7d0 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+12>:	lw	v1,124(a0)
   0x73a1c7d4 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+16>:	jr	ra
   0x73a1c7d8 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+20>:	lw	v0,120(a0)
   0x73a1c7dc <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+24>:	lw	a0,144(a0)
   0x73a1c7e0 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+28>:	lw	v0,0(a0)
   0x73a1c7e4 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+32>:	lw	t9,44(v0)
   0x73a1c7e8 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+36>:	jr	t9
   0x73a1c7ec <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+40>:	nop
   0x73a1c7f0 <nickel::system::GstMediaRouter::getControlCapabilities_async() const>:	lui	gp,0x8
   0x73a1c7f4 <nickel::system::GstMediaRouter::getControlCapabilities_async() const+4>:	addiu	gp,gp,12256


(gdb) p getPosition_locked
$6 = {const Zinc::Media::Position (const nickel::system::GstMediaRouter * const, bool)} 0x73a1dac0 <nickel::system::GstMediaRouter::getPosition_locked(bool) const>

(gdb) x/500i 0x73a1dac0

   0x73a1db7c <nickel::system::GstMediaRouter::getPosition_locked(bool) const+188>:	bal	0x73a1d60c <nickel::system::(anonymous namespace)::getBufferPosition(RefObj<_GstElement> const&, bool)>

const Position GstMediaRouter::getPosition_locked(bool liveMode) const
{
    const std::pair<gint64, gint64> bufPos = getBufferPosition(pipeline,liveMode);
    int64_t presentationStreamTime = getPresentationTimeCached(GST_FORMAT_TIME);
    ...
}

Couldn't see getPresentationTimeCached in getPositon_locked disassemled code as
getBufferPosition does. Why?


<05>
(gdb) bt
#0  MHWMemElementIsFree (pool=0xf49060, theAllocator=0x0, theElement=0x1981fa8, theIndex=0x0, 
    isFree=0xfa0c98) at mem_allocator.c:648
#1  0x006672a0 in IS_HANDLE (ptr=0x1981fa8, pool=0xf49060) at mem_moveable.c:731
#2  0x006666b8 in MHWMemFreeStatic (pool=0xf49060, staticBlock=0x19a5a20) at mem_static.c:771
#3  0x0066371c in MEMMAN_API_FreeStaticP (pool=0xf49060, ptr=0x19a5a20) at memman_st.c:667
#4  0x0060be08 in FDM_SVR_OPTV_TASK_P_LoadModuleNotifierCb (msg_ctx=0x19a5a20, 
    answer=<optimized out>, user_tag=<optimized out>, status=<optimized out>)
    at fdm_svr_optv_processor.c:448
...

(gdb) i r
          zero       at       v0       v1       a0       a1       a2       a3
 R0   00000000 10008400 00fa0c98 00000000 00f49060 00000000 01981fa8 00000000 
            t0       t1       t2       t3       t4       t5       t6       t7
 R8   0199d3c8 0199d3c8 ffffffff ffffffff 00fa0978 00000001 00000000 00000000 
            s0       s1       s2       s3       s4       s5       s6       s7
 R16  019a5a20 00f2f288 019a5a18 00f49060 00fa7e54 0060bd68 00000001 0000001f 
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00c9eb58 2aada010 00000000 00000000 2aae5010 00fa0c58 0051ff93 006672a0 
            sr       lo       hi      bad    cause       pc
      00008413 00000001 00000000 00000008 00800008 00664a10 
           fsr      fir
      00000000 00000000 

(gdb) disassemble $pc
Dump of assembler code for function MHWMemElementIsFree:
   0x00664a04 <+0>:	addiu	sp,sp,-32
   0x00664a08 <+4>:	sw	s0,24(sp)
   0x00664a0c <+8>:	sw	ra,28(sp)
=> 0x00664a10 <+12>:	lw	v1,8(a1)
   0x00664a14 <+16>:	lw	t4,16(a1)
   0x00664a18 <+20>:	lw	t0,12(a1)
   0x00664a1c <+24>:	mul	t5,t4,v1
   0x00664a20 <+28>:	mul	t3,t0,v1
   0x00664a24 <+32>:	lw	s0,48(sp)
   0x00664a28 <+36>:	lw	a0,0(a1)
   0x00664a2c <+40>:	move	t1,a2
   0x00664a30 <+44>:	j	0x664a44 <MHWMemElementIsFree+64>


(gdb) f 0
#0  MHWMemElementIsFree (pool=0xf49060, theAllocator=0x0, theElement=0x1981fa8, theIndex=0x0, 
    isFree=0xfa0c98) at mem_allocator.c:648
warning: Source file is more recent than executable.
648	    frameSize = theAllocator->elemNumber * elemSize;


646:    frame = theAllocator->firstFrame;
647:    elemSize = theAllocator->elemSize;
648:    frameSize = theAllocator->elemNumber * elemSize;
649:    nextFrameSize = theAllocator->numberIncrement * elemSize;

note that the line number of source do not exactly match to gdb.


={============================================================================
*kt_linux_gdb_000* gdb-break

5.1 Breakpoints, Watchpoints, and Catchpoints

A breakpoint makes your program stop whenever a certain point in the program is
reached. For each breakpoint, you can add conditions to control in finer detail
whether your program stops. You can set breakpoints with the break command and
its variants (see Setting Breakpoints), to specify the place where your program
should stop 'by' line number, function name or exact address in the program. 

On some systems, you can set breakpoints in shared libraries before the
executable is run. 

A watchpoint is a special breakpoint that stops your program when the value of
an expression changes. The expression may be a value of a variable, or it could
involve values of one or more variables combined by operators, such as 'a + b'.
This is sometimes called 'data' breakpoints. You must use a different command to
set watchpoints, but aside from that, you can manage a watchpoint like any other
breakpoint: you enable, disable, and delete both breakpoints and watchpoints
using the same commands.

You can arrange to have values from your program displayed automatically
whenever gdb stops at a breakpoint. See Automatic Display.

A catchpoint is another special breakpoint that stops your program when a
certain kind of 'event' occurs, such as the throwing of a C++ exception or the
loading of a library. As with watchpoints, you use a different command to set a
catchpoint, but aside from that, you can manage a catchpoint like any other
breakpoint. (To stop when your program receives a signal, use the handle
    command; see Signals.)

gdb assigns a number to each breakpoint, watchpoint, or catchpoint when you
create it; these numbers are successive integers starting with one. In many of
the commands for controlling various features of breakpoints you use the
breakpoint number to say which breakpoint you want to change. Each breakpoint
may be enabled or disabled; if disabled, it has no effect on your program until
you enable it again.

Some gdb commands accept a range of breakpoints on which to operate. A
breakpoint range is either a single breakpoint number, like '5', or two such
numbers, in increasing order, separated by a hyphen, like '5-7'. When a
breakpoint range is given to a command, all breakpoints in that range are
operated on. 


5.1.1 Setting Breakpoints

Breakpoints are set with the break command (abbreviated b).

break location
    Set a breakpoint at the given location, which can specify a function name, a
    line number, or an address of an instruction. The breakpoint will stop your
    program just before it executes any of the code in the specified location.

    When using source languages that permit overloading of symbols, such as C++,
    a function name may refer to more than one possible place to break. See
      Ambiguous Expressions, for a discussion of that situation.

    It is also possible to insert a breakpoint that will stop the program only
    if a specific 'thread' (see Thread-Specific Breakpoints).

*gbd-break-location*
9.2 Specifying a Location

Several gdb commands accept arguments that specify a location of your program's
code. Since gdb is a source-level debugger, a location usually specifies some
line in the source code; for that reason, locations are also known as linespecs.

9.2.1 Linespec Locations

A linespec is a colon-separated list of source location parameters such as file
name, function name, etc. Here are all the different ways of specifying a
linespec:

linenum
    Specifies the line number linenum of the current source file.
-offset
+offset
    Specifies the line offset lines before or after the current line. For the
    list command, the current line is the last one printed; for the breakpoint
    commands, this is the line at which execution stopped in the currently
    selected stack frame (see Frames, for a description of stack frames.) When
    used as the second of the two linespecs in a list command, this specifies
    the line offset lines up or down from the first linespec.

filename:linenum
    Specifies the line linenum in the source file filename. If filename is a
    relative file name, then it will match any source file name with the same
    trailing components. For example, if filename is ‘gcc/expr.c’, then it will
    match source file name of /build/trunk/gcc/expr.c, but not
    /build/trunk/libcpp/expr.c or /build/trunk/gcc/x-expr.c.

function
    Specifies the line that begins the body of the function function. For
    example, in C, this is the line with the open brace.

function:label
    Specifies the line where label appears in function.

filename:function
    Specifies the line that begins the body of the function in the file
    filename. You only need the file name with a function name to avoid
    ambiguity when there are identically named functions in different source
    files.

label
    Specifies the line at which the label named label appears in the function
    corresponding to the currently selected stack frame. If there is no current
    selected stack frame (for instance, if the inferior is not running), then
    gdb will not search for a label. 


9.2.2 Explicit Locations

Explicit locations allow the user to directly specify the source location's
parameters using option-value pairs.

Explicit locations are useful when several functions, labels, or file names have
the 'same' name (base name for files) in the program's sources. In these cases,
    explicit locations point to the source line you meant more accurately and
      unambiguously. Also, using explicit locations might be faster in large
      programs.

For example, the linespec 'foo:bar' may refer to a function bar defined in the
file named foo or the label bar in a function named foo. gdb must search either
the file system or the symbol table to know.

The list of valid explicit location options is summarized in the following
table:

-source filename
    The value specifies the source file name. To differentiate between files
    with the same base name, prepend as many directories as is necessary to
    uniquely identify the desired file, e.g., foo/bar/baz.c. Otherwise gdb will
    use the first file it finds with the given base name. This option requires
    the use of either -function or -line.

-function function
    The value specifies the name of a function. Operations on function locations
    unmodified by other options (such as -label or -line) refer to the line that
    begins the body of the function. In C, for example, this is the line with
    the open brace.

-label label
    The value specifies the name of a label. When the function name is not
    specified, the label is searched in the function of the currently selected
    stack frame.

-line number
    The value specifies a line offset for the location. The offset may either be
    absolute (-line 3) or relative (-line +3), depending on the command. When
    specified without any other options, the line offset is relative to the
    current line. 

Explicit location options may be abbreviated by omitting any non-unique trailing
characters from the option name, e.g., break -s main.c -li 3. 


9.2.3 Address Locations

The form break `*address` can be used to set a breakpoint at a virtual memory
address. This would be necessary for sections of a program that don’t have
debugging information, like when the source code is unavailable or for shared
libraries, for instance.


<gdb-break-set-cpp> gdb-linenumber

<ex>
(gdb) break lib.cpp:5
Breakpoint 2 at 0xb7fd846e: file lib.cpp, line 5.


<namespace>
ProxyMediaRouter.cpp
448: NS_ZINC::Future< void > ProxyMediaRouter::setSource(
    const std::string& mediaLocator_in, const SetSourceReason::Enum reason_in)

(gdb) b ProxyMediaRouter.cpp:448                            // okay

(gdb) b ProxyMediaRouter.cpp:ProxyMediaRouter::setSource    // not work

note: why?

$ nm --defined-only --demangle Nickel.System.Proxy/src/.libs/
    libNickelSystemProxy.so.0.0.0 | ag setSource

000135e4 t nickel::system::ProxyMediaRouter::setSource(
    std::string const&, Zinc::Media::SetSourceReason::Enum)

(gdb) b ProxyMediaRouter.cpp:nickel::system::ProxyMediaRouter::setSource  // okay


<completion>
Press the TAB key whenever you want gdb to fill out the rest of a word; command,
function name or a field in a structure,

Sometimes the string you need, while logically a "word", may contain parentheses
  or other characters that gdb normally excludes from its notion of a word. To
  permit word completion to work in this situation, you may enclose words in ’
  (single quote marks) in gdb commands.

The most likely situation is C++ function overloading:

(gdb) b ’bubble( <tab>
bubble(double,double) bubble(int,int)
(gdb) b ’bubble(

note: "nickel::system" is namespace  

(gdb) b nickel::system::ProxyMediaRouter::setS
setSink(std::string const&)
setSink(std::string const&)::__PRETTY_FUNCTION__
setSource(std::string const&, Zinc::Media::SetSourceReason::Enum)
setSourceComplete(std::string const&, boost::shared_ptr<Zinc::Media::MediaRouterAsync>, boost::shared_ptr<Zinc::Media::MediaRouterFactoryAsync>)
setSourceFailed(zinc::ErrorCode const&)
setSubtitleTrack(int, std::string const&)
(gdb) b nickel::system::ProxyMediaRouter::setS


<template>
template <typename T> void coin(T v);
template<typename T> class Foo;

(gdb) b Foo<int>::bar(int)
Breakpoint 2 at 0x804871d: file main.cpp, line 16.
(gdb) b void coin<int>(int)
Breakpoint 1 at 0x804872a: file main.cpp, line 6.


<overloads>
Using break function will set a breakpoint at all functions with the same
name. If you want to set a breakpoint at a particu- lar instance of the
function, you need to be unambiguous, such as by giving the line number within
a source code file in your break command.


<read>
Setting Breakpoint in Templates:

Templates can be much harder to set breakpoints in because we have to specify
the exact prototype for the fully-defined template. We as programmers are used
to the compiler handling the template type stuff for us, so it can be difficult
to guess the correct type.

For example, assume we have some abstract class BarAbstract that uses the
template Foo<> to make it concrete. Now assume we’re really clever and we want
to hide this from the users of our class. We could use a typedef to hide the
true type:

typedef Foo<BarAbstract> Bar;

Now, all the user needs to do is instantiate Bar without a thought to the Foo<>
    template.

int DoSomething()
{
  Bar b;
  return b.Baz();
}

So far so good? Okay, now how the heck do you set a breakpoint in the Baz()
    method of class Bar?

The quick answer: use objdump, c++filt, and grep to find the complete definition
that GDB will need.

$ objdump -t libMyLib.so | c++filt | grep ‘BarAbstract.*Baz’
0000d2d6 w F .text 0000000a     MY_PLUGIN_A::Foo<MY_PLUGIN_A::BarAbstract>::Baz()

Now, simply copy-n-paste the full method definition to GDB when setting the breakpoint:

(gdb) b MY_PLUGIN_A::Foo<MY_PLUGIN_A::BarAbstract>::Baz()
Breakpoint 6 at 0×8048890: file Source/Bar.cpp, line 355

*tool-demangle*
$ man c++filt

NAME
       c++filt - Demangle C++ and Java symbols.


={============================================================================
*kt_linux_gdb_001* gdb-break-continue

5.2 Continuing and Stepping

until
u
    Continue running until a source line past the current line, in the current
    stack frame, is reached. This command is used to avoid single stepping
    through a loop more than once. It is like the next command, except that when
    until encounters a jump, it automatically continues execution until the
    program counter is greater than the address of the jump.

    This means that when you reach the end of a loop after single stepping
    though it, until makes your program continue execution until it exits the
    loop. In contrast, a next command at the end of a loop simply steps back to
    the beginning of the loop, which forces you to step through the next
    iteration.

    until always stops your program if it attempts to exit the current stack
    frame.

    until may produce somewhat counterintuitive results if the order of machine
    code does not match the order of the source lines. For example, in the
    following excerpt from a debugging session, the f (frame) command shows that
    execution is stopped at line 206; yet when we use until, we get to line 195:

              (gdb) f
              #0  main (argc=4, argv=0xf7fffae8) at m4.c:206
              206                 expand_input();
              (gdb) until
              195             for ( ; argc > 0; NEXTARG) {

    This happened because, for execution efficiency, the compiler had generated
        code for the loop closure test at the end, rather than the start, of the
        loop - even though the test in a C for-loop is written before the body
        of the loop. The until command appeared to step back to the beginning of
        the loop when it advanced to this expression; however, it has not really
        gone to an earlier statement - not "in terms of" the actual machine
        code.

    until with no argument works by means of single instruction stepping, and
    hence is slower than until with an argument.

*gdb-until*
until location
u location
    Continue running your program until either the specified location is
    reached, or the current stack frame returns. The location is any of the
    forms described in Specify Location. This form of the command uses temporary
    breakpoints, and hence is quicker than until without an argument. The
    specified location is actually reached 'only' if it is in the current frame.
    This implies that until can be used to skip over recursive function
    invocations. For instance in the code below, if the current location is line
    96, issuing until 99 will execute the program up to line 99 in the same
    invocation of factorial, i.e., after the inner invocations have returned.

              94	int factorial (int value)
              95	{
              96	    if (value > 1) {
              97            value *= factorial (value - 1);
              98	    }
              99	    return (value);
              100     }

  what until really does is execute until it reaches a machine instruction
    that has a higher memory address than the current one, rather than until
    it reaches a larger line number in the source code.


advance location
    Continue running the program up to the given location. An argument is
    required, which should be of one of the forms described in Specify Location.
    Execution will also stop upon exit from the current stack frame. This
    command is similar to until, but advance will not skip over recursive
    function calls, and the target location does 'not' have to be in the same
    frame as the current one. 

*gdb-step*
step, s
    Continue running your program until control reaches a different source line,
             then stop it and return control to gdb.

    Warning: If you use the step command while control is within a function that
    was compiled without debugging information, execution proceeds until control
    reaches a function that does have debugging information. Likewise, it will
    'not' step into a function which is compiled without debugging information.
    To step through functions without debugging information, use the `stepi`
    command, described below. 

    The step command only stops at the first instruction of a source line. This
    prevents the multiple stops that could otherwise occur in switch statements,
    for loops, etc. step continues to stop if a function that has debugging
        information is called within the line. In other words, step steps inside
            any functions called within the line.

    Also, the step command 'only' enters a function if there is line number
    information for the function. Otherwise it acts like the next command. This
    avoids problems when using cc -gl on MIPS machines. Previously, step entered
    subroutines if there was any debugging information about the routine. 

<gdb-finish> out of step
finish
    Continue running until just after function in the selected `stack frame`
    returns. Print the returned value (if any). This command can be
    abbreviated as fin.

    Contrast this with the return command (see Returning from a Function).

stepi
stepi arg
si
    Execute one machine instruction, then stop and return to the debugger.

    It is often useful to do "display/i $pc" when stepping by machine
    instructions. This makes gdb automatically display the next instruction to
    be executed, each time your program stops. See Automatic Display.
    *gdb-disp*

    An argument is a repeat count, as in step. 

    note:
    `stepi` shows more than when use "set step-mode on; step".

nexti
nexti arg
ni
    Execute one machine instruction, but if it is a function call, proceed until
    the function returns.

    An argument is a repeat count, as in next. 


<ex>
The "at lineno" means the line to be run next.

(gdb) n
main () at sample.c:28
28	    printf("ends main\n");
(gdb)

<ex>
This shows something puzzling.

template<typename T, typename A1>
void expose_impl(DBus::Connection conn, const char* path,
        boost::shared_ptr<T> obj, A1 a1) {
    using namespace std;

// note: the first break
46:	expose(conn, path, obj, a1);
}

template<typename T, typename A1>
void DBusService::expose(const char* path, boost::shared_ptr<T> obj, A1 a1) {
    using namespace std;
145:	detail::expose_impl(conn, path, obj, a1);
      objects.push_back(path);
}


// note: the first break
(gdb) bt
#0  expose_impl<Zinc::Media::MediaRouterFactoryAsync, boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > (
    a1=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
    obj=<error reading variable: access outside bounds of object referenced via synthetic pointer>, conn=<incomplete type>, path=<optimized out>)
    at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:46

#1  expose<Zinc::Media::MediaRouterFactoryAsync, boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > (
    a1=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
    obj=<error reading variable: access outside bounds of object referenced via synthetic pointer>, this=0x7fff6510, path=<optimized out>)
    at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:145

#2  (anonymous namespace)::MediaDaemon::start (this=0x7fff64e4, argc=<optimized out>, argv=<optimized out>)
    at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Nickel/Nickel.System.DBusServer/src/MediaDaemon.cpp:123
#3  0x77e57404 in DBus::DefaultMainLoop::processCommands() () from /opt/zinc/lib/libdbus-c++-1.so.0
#4  0x77e58ff0 in DBus::DefaultMainLoop::dispatch(unsigned int) () from /opt/zinc/lib/libdbus-c++-1.so.0
#5  0x77e5a6c4 in DBus::BusDispatcher::run() () from /opt/zinc/lib/libdbus-c++-1.so.0
#6  0x77ec24ec in zinc::binding::dbus::MainLoop::run (this=0x7fff64c8)
    at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Zinc/Zinc.DBus.BindingRuntime/src/dbus/MainLoop.cpp:70
#7  0x00408150 in main (argc=1, argv=0x7fff6864) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Nickel/Nickel.System.DBusServer/src/MediaDaemon.cpp:234


// note: when do n(next) the line 46 has no effect and goes back to where
// expose_impl gets called.

(gdb) n
expose<Zinc::Media::MediaRouterFactoryAsync, boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > (
    a1=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
    obj=<error reading variable: access outside bounds of object referenced via synthetic pointer>, this=0x7fff6510, path=<optimized out>)
    at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:145
145		detail::expose_impl(conn, path, obj, a1);


={============================================================================
*kt_linux_gdb_000* gdb-break-condition gdb-break-temp gdb-condition

5.1.1 Setting Breakpoints

tbreak args
    Set a breakpoint enabled only for 'one' stop. The args are the same as for
    the break command, and the breakpoint is set in the same way, but the
    breakpoint is automatically deleted after the first time your program stops
    there. See Disabling Breakpoints. 


break ... if cond
    Set a breakpoint with condition cond; evaluate the expression cond each time
    the breakpoint is reached, and stop only if the value is nonzero—that is, if
    cond evaluates as true. ‘...’ stands for one of the possible arguments
    described above (or no argument) specifying where to break. See Break
    Conditions, for more information on breakpoint conditions. 


Whatever you use needs to have a Boolean value, that is, true (nonzero) or
false (zero). This includes:

o Your own functions, as long as they’re linked into the program; .e.g:

break test.c:myfunc if ! check_variable_sanity(i)

o Library functions, as long as the library is linked into your code; e.g.:

break 44 if strlen(mystring) == 0

Also, if you use a library function in a GDB expression, and the library was
not compiled with debugging symbols (which is almost certainly the case), the
only return values you can use in your breakpoint conditions are those of type
int. In other words, without debugging information, GDB as- sumes the return
value of a function is an int. When this assumption isn’t correct, the
function’s return value will be misinterpreted.


It is possible to set conditions on normal breakpoints to turn them into
conditional breakpoints. For example, if you have set breakpoint 3 as uncon-
ditional but now wish to add the condition i == 3, simply type

(gdb) cond 3 i == 3

If you later want to remove the condition but keep the breakpoint, simply type

(gdb) cond 3


<ex>
(gdb) break 30
Breakpoint 1 at 0x80483fc: file ins.c, line 30.
(gdb) condition 1 num_y==1

We could have combined the break and condition commands into a single step by
using break if as follows:

(gdb) break 30 if num_y==1


<ex>
gpointer
g_object_ref (gpointer _object)
{
  ...
}

break g_object_ref if _object == 0xcafebabe
break g_object_unref if _object == 0xcafebabe


<ex>
bool find_path(Maze &maze, int row, int col);

(gdb) b find_path if row==1 && col==2


<ex>
in addition to a watchpoint nested inside a breakpoint you can also set a
single breakpoint on the 'filename:line_number' and use a condition. I find it
sometimes easier.

(gdb) break iter.c:6 if i == 5
Breakpoint 2 at 0x4004dc: file iter.c, line 6.
(gdb) c
Continuing.
0
1
2
3
4

Breakpoint 2, main () at iter.c:6
6           printf("%d\n", i);

If like me you get tired of line numbers changing, you can add a label then
set the breakpoint on the label like so:

#include <stdio.h>
main()
{ 
     int i = 0;
     for(i=0;i<7;++i) {
       looping:
        printf("%d\n", i);
     }
     return 0;
}

(gdb) break main:looping if i == 5


gdb normally implements breakpoints by replacing the program code at the
breakpoint address with a special instruction, which, when executed, given
control to the debugger.  By default, the program code is so modified only when
the program is resumed. As soon as the program stops, gdb restores the original
instructions. This behaviour guards against leaving breakpoints inserted in the
target should gdb abrubptly disconnect. However, with slow remote targets,
inserting and removing breakpoint can reduce the performance. This behavior can
  be controlled with the following commands::

set breakpoint always-inserted off
    All breakpoints, including newly added by the user, are inserted in the
    target only when the target is resumed. All breakpoints are removed from the
    target when it stops. This is the 'default' mode.

set breakpoint always-inserted on
    Causes all breakpoints to be inserted in the target at all times. If the
    user adds a new breakpoint, or changes an existing breakpoint, the
    breakpoints in the target are updated immediately. A breakpoint is removed
    from the target only when breakpoint itself is deleted. 


gdb handles conditional breakpoints by evaluating these conditions when a
breakpoint breaks. If the condition is true, then the process being debugged
stops, otherwise the process is resumed.

If the target supports evaluating conditions on its end, gdb may download the
breakpoint, together with its conditions, to it.

This feature can be controlled via the following commands:

set breakpoint condition-evaluation auto
    This is the default mode. If the target supports evaluating breakpoint
    conditions on its end, gdb will download breakpoint conditions to the target
    (limitations mentioned previously apply). If the target does not support
    breakpoint condition evaluation, then gdb will fallback to evaluating all
    these conditions on the host's side. 


<gdb-break-command-list>
5.1.7 Breakpoint Command Lists

You can give any breakpoint (or watchpoint or catchpoint) a series of commands
to 'execute' when your program stops due to that breakpoint. For example, you
might want to print the values of certain expressions, or enable other
breakpoints.

For example, here is how you could use breakpoint commands to print the value of
x at entry to foo whenever x is positive.

  break foo if x>0
  `commands`
  silent
  printf "x is %d\n",x
  cont
  `end`

One application for breakpoint commands is to compensate for one bug so you can
test for another. Put a breakpoint just after the erroneous line of code, give
it a condition to detect the case in which something erroneous has been done,
and give it commands to assign correct values to any variables that need them.

End with the continue command so that your program does not stop, and start with
the silent command so that no output is produced. Here is an example:

  break 403
  commands
  silent
  set x = y + 4           // note <gdb-set>
  cont
  end


The command set for a given breakpoint can be modified dynamically, or simply
canceled by redefining an empty set:


<ex>
$ gdb fibonacci
(gdb) break fibonacci
Breakpoint 1 at 0x80483e0: file fibonacci.c, line 13.
(gdb) commands 1
Type commands for when breakpoint 1 is hit, one per line.
End with a line saying just "end".
>printf "fibonacci was passed %d.\n", n
>end
(gdb)

Fortunately, you can make GDB more quiet about triggering breakpoints using
the silent command, which needs to be the first item in the command list.

(gdb) commands 1
Type commands for when breakpoint 1 is hit, one per line.
End with a line saying just "end".
>silent
>printf "fibonacci was passed %d.\n", n
>end
(gdb)

*gdb-macro*
You might want to do this type of thing in other programs, or at other lines
of this program, so let’s make a macro out of it, using GDB’s define command.
First, let’s define the macro, which we’ll name print_and_go:

(gdb) define print_and_go
Redefine command "print_and_go"? (y or n) y
Type commands for definition of "print_and_go".
End with a line saying just "end".
>printf $arg0, $arg1
>continue
>end
>
To use it as above, you would type:

(gdb) commands 1
Type commands for when breakpoint 1 is hit, one per line.
End with a line saying just "end".
>silent
>print_and_go "fibonacci() was passed %d\n" n
>end

the point is that now you can use it generally, anywhere in the code.
Moreover, you can put it in your .gdbinit file for use in other programs.

Command lists are very useful, but you can also combine them with conditional
breaking, and that’s powerful.


*gdb-command-file*

// z is text file
$ gdb -command=z x

// z
b 40
commands
p tmp->val
if (tmp->left !=0)
  printf "left val: %d\n", tmp->left->val
else
  printf "left val: %s\n", "none"
end
if (tmp->right !=0)
  printf "right val: %d\n", tmp->right->val
else
  printf "right val: %s\n", "none"
end
end


Breakpoint 1 at 0x4006e6: file bintree.cpp, line 40.
(gdb) i b
Num     Type           Disp Enb Address            What
1       breakpoint     keep y   0x00000000004006e6 in insert(node**, int) at bintree.cpp:40
        p tmp->val
        if (tmp->left !=0)
          printf "left val: %d\n", tmp->left->val
        else
          printf "left val: %s\n", "none"
        end
        if (tmp->right !=0)
          printf "right val: %d\n", tmp->right->val
        else
          printf "right val: %s\n", "none"
        end
(gdb) run 12 8 5 19 16


<gdb-call-command>
set args 12 8 5 19 16
b 79
commands
printf "****** current tree *******\n"
call printtree(root)
end


={============================================================================
*kt_linux_gdb_000* gdb-break-convenience-variable

Say you have a pointer variable p which at different times points to differ-
ent nodes in a linked list. During your debugging session, you may wish to
record the address of a particular node, say because you wish to recheck the
value in the node at various times during the debugging process. The first
time p gets to that node, you could do something like

(gdb) set $q = p

and from then on do things like

(gdb) p *$q

The variable $q here is called a convenience variable.  Convenience variables
can change values according to C rules. For ex- ample, consider the code

int w[4] = {12,5,8,29};

main()
{
  w[2] = 88;
}

In GDB you might do something like

Breakpoint 1, main () at cv.c:7
7 w[2] = 88;
(gdb) n
8 }
(gdb) set $i = 0
(gdb) p w[$i++]
$1 = 12
(gdb)
$2 = 5
(gdb)
$3 = 88
(gdb)
$4 = 29

To understand what happened here, recall that if we simply hit the ENTER key
in GDB without issuing a command, GDB takes this as a request to repeat the
last command. In the GDB session above, you kept hitting the ENTER key, which
meant you were asking GDB to repeat the command (gdb) p w[$i++]

That meant not only that a value would be printed, but also that the
convenience variable $i would increment.


={============================================================================
*kt_linux_gdb_000* gdb-break-info

info breakpoints [n...]
info break [n...]

Print a table of all breakpoints, watchpoints, and catchpoints set and not
deleted. Optional argument n means print information only about the specified
breakpoint(s) (or watchpoint(s) or catchpoint(s)). For each breakpoint,
following columns are printed:


note: tip

(gdb) info break 

displays a count of the number of times the breakpoint has been hit. This is
especially useful in conjunction with the `ignore` command. You can ignore a
large number of breakpoint hits, look at the breakpoint info to see how many
times the breakpoint was hit, and then run again, ignoring one less than that
number. This will get you 'quickly' to the last hit of that breakpoint.


*gdb-cpp*
It is possible that a breakpoint corresponds to several locations in your
program. Examples of this situation are:

    Multiple functions in the program may have the same name.

    For a C++ constructor, the gcc compiler generates several instances of the
    function body, used in different cases.

    For a C++ template function, a given line in the function can correspond to
    any number of instantiations.

    For an inlined function, a given source line can correspond to several
    places where that function is inlined. 


In all those cases, gdb will insert a breakpoint at all the relevant locations.

A breakpoint with multiple 'locations' is displayed in the breakpoint table
using several rows: 

one header row, followed by one row for each breakpoint location.  The header
row has ‘<MULTIPLE>’ in the address column. The rows for individual locations
contain the actual addresses for locations, and show the functions to which
those locations belong. The number column for a location is of the form
breakpoint-number.'location'-number.

For example:

     Num     Type           Disp Enb  Address    What
     1       breakpoint     keep y    <MULTIPLE>
             stop only if i==1
             breakpoint already hit 1 time
     1.1                         y    0x080486a2 in void foo<int>() at t.cc:8
     1.2                         y    0x080486ca in void foo<double>() at t.cc:8

Each location can be individually enabled or disabled by passing
breakpoint-number.location-number as argument to the enable and disable
commands. 

Note that you 'cannot' delete the individual locations from the list, you can
only delete the entire list of locations that belong to their parent breakpoint
(with the delete num command, where num is the number of the parent breakpoint,
 1 in the above example). Disabling or enabling the parent breakpoint (see
     Disabling) affects all of the locations that belong to that breakpoint. 


Disposition (Disp):

Each breakpoint has a disposition, which indicates what will happen to the
  breakpoint after the next time it causes GDB to pause the program’s
  execution. There are three possible dispositions:

keep 

The breakpoint will be unchanged after the next time it’s reached.  This is
the default disposition of newly created breakpoints.

del 

The breakpoint will be deleted after the next time it’s reached.  This
disposition is assigned to any breakpoint you create with the tbreak command
(see Section 2.4.1).

dis 

The breakpoint will be disabled the next time it’s reached. This is set using
the enable once command (see Section 2.7.2).

={============================================================================
*kt_linux_tool_309* gdb-break: ignore

5.1.6 Break Conditions

A special case of a breakpoint condition is to stop 'only' when the breakpoint
has been reached a certain number of times. This is so useful that there is a
special way to do it, using the ignore count of the breakpoint. 

Every breakpoint has an ignore count, which is an integer. Most of the time, the
ignore count is zero, and therefore has no effect. But if your program reaches a
breakpoint whose ignore count is positive, then instead of stopping, it just
decrements the ignore count by one and continues. As a result, if the ignore
count value is n, the breakpoint does not stop the next n times your program
reaches it.

ignore bnum count

Set the ignore count of breakpoint number bnum to count. The next count times
the breakpoint is reached, your program's execution does not stop; other than to
decrement the ignore count, gdb takes no action. 

To make the breakpoint stop the next time it is reached, specify a count of
zero. When you use `continue` to resume execution of your program from a
breakpoint, you can specify an ignore count directly as an argument to continue,
rather than using ignore. See Section 5.2 [Continuing and Stepping], page 68.

If a breakpoint has a positive ignore count and a condition, the condition is
not checked. Once the ignore count reaches zero, gdb resumes checking the
condition. You could achieve the effect of the ignore count with a condition
such as ‘$foo-- <= 0’ using a debugger convenience variable that is decremented
each time. See Section 10.11 [Convenience Variables], page 132.

Ignore counts apply to breakpoints, watchpoints, and catchpoints.


={============================================================================
*kt_linux_gdb_000* gdb-break-watch gdb-watch

However, watchpoints are invaluable if one of your variables changes,
especially a global variable, and you have no idea where or how it changed. If
you’re dealing with threaded code, watchpoints have limited usefulness;


5.1.2 Setting Watchpoints

You can use a watchpoint to stop execution whenever the 'value' of an
`expression changes`, 'without' having to predict a particular place where this
may happen. (This is sometimes called a data breakpoint.) The expression may be
as simple as the value of a single variable, or as complex as many variables
combined by operators. Examples include:

    A reference to the value of a single variable.

    An address cast to an appropriate data type. For example, ‘*(int
        *)0x12345678’ will watch a 4-byte region at the specified address
    (assuming an int occupies 4 bytes).

    An arbitrarily complex expression, such as ‘a*b + c/d’. The expression can
    use any operators valid in the program's native language (see Languages). 


// still true?
// Because C has rigid scoping rules, you can only watch a variable that exists
// and is in scope. Once the variable no longer exists in any frame of the call
// stack (when the function containing a local variable returns), GDB
// automatically deletes the watchpoint.

You can set a watchpoint on an expression even if the expression can not be
evaluated `yet`. For instance, you can set a watchpoint on ‘*global_ptr’
before ‘global_ptr’ is initialized. gdb will stop when your program sets
‘global_ptr’ and the expression produces a valid value. If the expression
becomes valid in some other way than changing a variable (e.g. if the memory
    pointed to by ‘*global_ptr’ becomes readable as the result of a malloc
    call), gdb may not stop until the next time the expression changes.

Depending on your system, watchpoints may be implemented in software or
hardware. gdb does 'software' watchpointing by single-stepping your program and
testing the variable's value each time, which is hundreds of times slower than
normal execution. (But this may still be worth it, to catch errors where you
    have no clue what part of your program is the culprit.)

watch [-l|-location] `expr` [thread threadnum] [mask maskvalue]

    Set a watchpoint for an expression. gdb will break when the expression expr
    is written into by the program and its value 'changes'. The simplest (and
        the most popular) use of this command is to watch the value of a
    'single' variable:

              (gdb) watch foo
              (gdb) watch (z > 28)

    If the command includes a [thread threadnum] argument, gdb breaks only when
    the thread identified by threadnum changes the value of expr. If any other
    threads change the value of expr, gdb will not break. Note that watchpoints
    restricted to a single thread in this way only work with Hardware
    Watchpoints.

    Ordinarily a watchpoint respects the scope of variables in expr (see below).
    The -location argument tells gdb to instead watch the memory referred to by
    expr. In this case, gdb will evaluate expr, take the address of the result,
and watch the memory at that address. The type of the result is used to
  determine the size of the watched memory. If the expression's result does not
  have an address, then gdb will print an error.

    The [mask maskvalue] argument allows creation of 'masked' watchpoints, if the
    current architecture supports this feature (e.g., PowerPC Embedded
        architecture, see PowerPC Embedded.) A masked watchpoint specifies a
    mask in addition to an address to watch. The mask specifies that some 'bits'
    of an address (the bits which are reset in the mask) should be ignored when
    matching the address accessed by the inferior against the watchpoint
    address. Thus, a masked watchpoint watches many addresses
    simultaneously—those addresses whose unmasked bits are identical to the
    unmasked bits in the watchpoint address. The mask argument implies
    -location. Examples:

              (gdb) watch foo mask 0xffff00ff
              (gdb) watch *0xdeadbeef mask 0xffffff00


rwatch [-l|-location] expr [thread threadnum] [mask maskvalue]
    Set a watchpoint that will break when the value of expr is read by the
    program.


awatch [-l|-location] expr [thread threadnum] [mask maskvalue]
    Set a watchpoint that will break when expr is either read from or written
    into by the program.


info watchpoints [n...]
    This command prints a list of watchpoints, using the same format as info
    break (see Set Breaks). 

*gdb-watch-address*
If you watch for a change in a numerically entered address you need to
'dereference' it, as the address itself is just a constant number which will
never change. gdb refuses to create a watchpoint that watches a never-changing
value:

     (gdb) watch 0x600850
     Cannot watch constant value 0x600850.
     (gdb) watch *(int *) 0x600850
     Watchpoint 1: *(int *) 6293584

gdb sets a hardware watchpoint if 'possible'. Hardware watchpoints execute very
quickly, and the debugger reports a change in value at the exact instruction
where the change occurs. If gdb cannot set a hardware watchpoint, it sets a
software watchpoint, which executes more slowly and reports the change in value
at the next statement, not the instruction, after the change occurs.

Currently, the awatch and rwatch commands can only set hardware watchpoints,
because accesses to data that don't change the value of the watched expression
  cannot be detected without examining every instruction as it is being
  executed, and gdb does not do that currently. If gdb finds that it is unable
  to set a hardware breakpoint with the awatch or rwatch command, it will print
  a message like this:

     Expression cannot be implemented with read/access watchpoint.

Sometimes, gdb cannot set a hardware watchpoint because the data type of the
watched expression is wider than what a hardware watchpoint on the target
machine can handle. For example, some systems can only watch regions that are up
to 4 bytes wide; on such systems you cannot set hardware watchpoints for an
expression that yields a double-precision floating-point number (which is
    typically 8 bytes wide). As a work-around, it might be possible to break the
large region into a series of smaller ones and watch them with separate
watchpoints.

If you set too many hardware watchpoints, gdb might be unable to insert all of
them when you resume the execution of your program. Since the precise number of
active watchpoints is unknown until such time as the program is about to be
resumed, gdb might not be able to warn you about this when you set the
watchpoints, and the warning will be printed only when the program is resumed:

     Hardware watchpoint num: Could not insert watchpoint

If this happens, delete or disable some of the watchpoints.

Watching complex expressions that reference many variables can also exhaust the
resources available for hardware-assisted watchpoints. That's because gdb needs
to watch every variable in the expression with separately allocated resources.

If you call a function interactively using print or call, any watchpoints you
have set will be inactive until gdb reaches another kind of breakpoint or the
call completes.

gdb automatically deletes watchpoints that watch local (automatic) variables, or
expressions that involve such variables, when they go out of scope, that is,
when the execution leaves the block in which these variables were defined. In
  particular, when the program being debugged terminates, all local variables go
  out of scope, and so only watchpoints that watch global variables remain set.
  If you rerun the program, you will need to set all such watchpoints again. One
  way of doing that would be to set a code breakpoint at the entry to the main
  function and when it breaks, set all the watchpoints.

In multi-threaded programs, watchpoints will detect changes to the watched
expression 'from' every thread.

note:

Warning: In multi-threaded programs, software watchpoints have only limited
usefulness. If gdb creates a software watchpoint, it can only watch the value of
an expression in a single thread. If you are confident that the expression can
only change due to the current thread's activity (and if you are also confident
    that no other thread can become current), then you can use software
watchpoints as usual. However, gdb may not notice when a non-current thread's
activity changes the expression. Hardware watchpoints, in contrast, watch an
expression in all threads. 


={============================================================================
*kt_linux_gdb_000* gdb-break-catch gdb-catch

*gdb-signal*
C++ programmers will want to check out the catch command, which sets
catchpoints. Catchpoints are similar to breakpoints, but can be triggered by
different things like thrown exceptions, caught exceptions, signals, calls to
fork(), loading and unloading of libraries, and many other events.


5.1.3 Setting Catchpoints

You can use catchpoints to cause the debugger to stop for certain kinds of
program events, such as C++ 'exceptions' or the loading of a shared library.
Use the catch command to set a catchpoint.

catch event
    Stop when event occurs. The event can be any of the following:

    throw [regexp]
    rethrow [regexp]
    catch [regexp]
        The throwing, re-throwing, or catching of a C++ exception.

        If regexp is given, then only exceptions whose type matches the regular
        expression will be caught.

        The convenience variable $_exception is available at an
        exception-related catchpoint, on some systems. This holds the exception
        being thrown.

        There are currently some limitations to C++ exception handling in gdb:

            note:
            The support for these commands is system-dependent. Currently, only
            systems using the ‘gnu-v3’ C++ ABI (see ABI) are supported.

            The regular expression feature and the $_exception convenience
            variable rely on the presence of some SDT probes in libstdc++. If
            these probes are not present, then these features cannot be used.
            These probes were first available in the GCC 4.8 release, but
            whether or not they are available in your GCC also depends on how it
            was built.
            
            The $_exception convenience variable is only valid at the
            instruction at which an exception-related catchpoint is set.

            When an exception-related catchpoint is hit, gdb stops at a location
            in the system library which implements runtime exception support for
            C++, usually libstdc++. You can use up (see Selection) to get to
            your code.

            If you call a function interactively, gdb normally returns control
            to you when the function has finished executing. If the call raises
            an exception, however, the call may bypass the mechanism that
            returns control to you and cause your program either to abort or to
            simply continue running until it hits a breakpoint, catches a signal
            that gdb is listening for, or exits. This is the case even if you
            set a catchpoint for the exception; catchpoints on exceptions are
            disabled within interactive calls. See Calling, for information on
            controlling this with set unwind-on-terminating-exception.

            You cannot raise an exception interactively.

            You cannot install an exception handler interactively. 

    exec
        A call to exec. This is currently only available for HP-UX and gnu/Linux.

    syscall
    syscall [name | number] ...
        A call to or return from a system call, a.k.a. syscall. A syscall is a
        mechanism for application programs to request a service from the
        operating system (OS) or one of the OS system services. gdb can catch
        some or all of the syscalls issued by the debuggee, and show the related
        information for each syscall. If no argument is specified, calls to and
        returns from all system calls will be caught.

        name can be any system call name that is valid for the underlying OS.
        you can use the gdb command-line completion facilities to list the
        available choices.

        You may also specify the system call numerically. A syscall's number is
        the value passed to the OS's syscall dispatcher to identify the
        requested service. When you specify the syscall by its name, gdb uses
        its database of syscalls to convert the name into the corresponding
        numeric code, but using the number directly may be useful if gdb's
        database does not have the complete list of syscalls on your system
        (e.g., because gdb lags behind the OS upgrades).

        The example below illustrates how this command works if you don't
        provide arguments to it:

                       (gdb) catch syscall
                       Catchpoint 1 (syscall)
                       (gdb) r
                       Starting program: /tmp/catch-syscall
                       
                       Catchpoint 1 (call to syscall 'close'), \
                       	   0xffffe424 in __kernel_vsyscall ()
                       (gdb) c
                       Continuing.
                       
                       Catchpoint 1 (returned from syscall 'close'), \
                       	0xffffe424 in __kernel_vsyscall ()
                       (gdb)

        Here is an example of catching a system call by name:

                       (gdb) catch syscall chroot
                       Catchpoint 1 (syscall 'chroot' [61])
                       (gdb) r
                       Starting program: /tmp/catch-syscall
                       
                       Catchpoint 1 (call to syscall 'chroot'), \
                       		   0xffffe424 in __kernel_vsyscall ()
                       (gdb) c
                       Continuing.
                       
                       Catchpoint 1 (returned from syscall 'chroot'), \
                       	0xffffe424 in __kernel_vsyscall ()
                       (gdb)

        An example of specifying a system call numerically. In the case below,
        the syscall number has a corresponding entry in the XML file, so gdb
          finds its name and prints it:

                       (gdb) catch syscall 252
                       Catchpoint 1 (syscall(s) 'exit_group')
                       (gdb) r
                       Starting program: /tmp/catch-syscall
                       
                       Catchpoint 1 (call to syscall 'exit_group'), \
                       		   0xffffe424 in __kernel_vsyscall ()
                       (gdb) c
                       Continuing.
                       
                       Program exited normally.
                       (gdb)

        // skipped

    fork
        A call to fork. This is currently only available for HP-UX and gnu/Linux.

    vfork
        A call to vfork. This is currently only available for HP-UX and gnu/Linux.

    load [regexp]
    unload [regexp]
        The loading or unloading of a shared library. If regexp is given, then
        the catchpoint will stop only if the regular expression matches one of
        the affected libraries.

    signal [signal... | ‘all’]
        The delivery of a signal.

        With no arguments, this catchpoint will catch any signal that is not
        used internally by gdb, specifically, all signals except ‘SIGTRAP’ and
        ‘SIGINT’.

        With the argument ‘all’, all signals, including those used by gdb, will
        be caught. This argument cannot be used with other signal names.

        Otherwise, the arguments are a list of signal names as given to handle
        (see Signals). Only signals specified in this list will be caught.

        One reason that catch signal can be more useful than handle is that you
        can attach commands and conditions to the catchpoint.

        When a signal is caught by a catchpoint, the signal's stop and print
        settings, as specified by handle, are ignored. However, whether the
        signal is still delivered to the inferior depends on the pass setting;
        this can be changed in the catchpoint's commands. 


tcatch event
    Set a catchpoint that is enabled only for 'one' stop. The catchpoint is
    automatically deleted after the first time the event is caught. 

Use the info break command to list the current catchpoints. 


={============================================================================
*kt_linux_gdb_000* gdb-break-control
  
<gdb-break-delete>

5.1.4 Deleting Breakpoints

It is often necessary to eliminate a breakpoint, watchpoint, or catchpoint once
it has done its job and you no longer want your program to stop there. This is
called deleting the breakpoint. A breakpoint that has been deleted no longer
exists; it is forgotten.

With the clear command you can delete breakpoints according to where they are in
your program. With the delete command you can delete individual breakpoints,
watchpoints, or catchpoints by specifying their breakpoint numbers.

clear
    Delete any breakpoints at the next instruction to be executed in the
    selected stack frame (see Selecting a Frame). When the innermost frame is
    selected, this is a good way to delete a breakpoint where your program just
    stopped.  clear location Delete any breakpoints set at the specified
    location. See Specify Location, for the various forms of location; the most
    useful ones are listed below:

    clear function
    clear filename:function
        Delete any breakpoints set at entry to the named function.
    clear linenum
    clear filename:linenum
        Delete any breakpoints set at or within the code of the specified
        linenum of the specified filename. 


delete [breakpoints] [range...]
    Delete the breakpoints, watchpoints, or catchpoints of the breakpoint ranges
    specified as arguments. If 'no' argument is specified, delete 'all'
    breakpoints (gdb asks confirmation, unless you have set confirm off). You
    can abbreviate this command as d. 


<gdb-break-disable>

5.1.5 Disabling breakpoints

This makes the breakpoint inoperative as if it had been deleted, but remembers
the information on the breakpoint so that you can enable it again later. 

A breakpoint, watchpoint, or catchpoint can have any of 'four' different
'states' of enablement:


    Enabled. The breakpoint stops your program. A breakpoint set with the break
    command starts out in this state.

    Disabled. The breakpoint has no effect on your program.

    Enabled 'once'. The breakpoint stops your program, but then becomes disabled.

    Enabled for a 'count'. The breakpoint stops your program for the next N
    times, then becomes disabled.

    Enabled for 'deletion'. The breakpoint stops your program, but immediately
    after it does so it is deleted permanently. A breakpoint set with the tbreak
    command starts out in this state. 


You can use the following commands to enable or disable breakpoints,
watchpoints, and catchpoints:

disable [breakpoints] [range...]
    Disable the specified breakpoints—or all breakpoints, if none are listed. A
    disabled breakpoint has no effect but is not forgotten. All options such as
    ignore-counts, conditions and commands are remembered in case the breakpoint
    is enabled again later. You may abbreviate disable as dis.


enable [breakpoints] [range...]
    Enable the specified breakpoints (or all defined breakpoints). They become
    effective once again in stopping your program.

enable [breakpoints] once range...
    Enable the specified breakpoints temporarily. gdb disables any of these
    breakpoints immediately after stopping your program.

enable [breakpoints] count count range...
    Enable the specified breakpoints temporarily. gdb records count with each of
    the specified breakpoints, and decrements a breakpoint's count when it is
    hit. When any count reaches 0, gdb disables that breakpoint. If a breakpoint
    has an ignore count (see Break Conditions), that will be decremented to 0
    before count is affected.

enable [breakpoints] delete range...
    Enable the specified breakpoints to work once, then die. gdb deletes any of
    these breakpoints as soon as your program stops there. Breakpoints set by
    the tbreak command start out in this state. 

Except for a breakpoint set with tbreak, breakpoints that you set are initially
enabled; subsequently, they become disabled or enabled only when you use one of
the commands above. (The command until can set and delete a breakpoint of its
    own, but it does not change the state of your other breakpoints; see
    Continuing and Stepping.) 


={============================================================================
*kt_linux_tool_310* gdb-break: save

5.1.9 How to save breakpoints to a file

To save breakpoint definitions to a file use the save breakpoints command. 

save breakpoints [filename]


={============================================================================
*kt_linux_tool_300* gdb-break: dprintf

The dynamic printf command dprintf 'combines' a breakpoint with formatted
printing of your program's data to give you the effect of inserting printf calls
into your program on-the-fly, 'without' having to recompile it.

<to-direct-to-programs-output>
In its most basic form, the output goes to the GDB console. However, you can set
the variable dprintf-style for alternate handling. For instance, you can ask to
format the output by calling 'your' program's printf function. This has the
advantage that the characters go to the program's 'output' device, so they can
recorded in redirects to files and so forth.

As an example, if you wanted dprintf output to go to a logfile that is a
standard I/O stream assigned to the variable mylog, you could do the following:

(gdb) set dprintf-style call
(gdb) set dprintf-function fprintf
(gdb) set dprintf-channel mylog
(gdb) dprintf 25,"at line 25, glob=%d\n",glob
Dprintf 1 at 0x123456: file main.c, line 25.
(gdb) info break
1 dprintf keep y 0x00123456 in main at main.c:25
call (void) fprintf (mylog,"at line 25, glob=%d\n",glob)
continue
(gdb)

note that the info break displays the dynamic printf commands as normal
breakpoint commands; you can thus easily see the effect of the variable
settings.

dprintf location,template,expression[,expression...]

Whenever execution reaches location, print the values of one or more expressions
under the control of the string template. To print several values, separate them
with commas.

set dprintf-style style

Set the dprintf output to be handled in one of several different styles
enumerated below. A change of style affects all existing dynamic printfs
immediately. (If you need individual control over the print commands, simply
    define normal breakpoints with explicitly-supplied command lists.) gdb
Handle the output using the gdb printf command.

'call' Handle the output by calling a function in your program (normally
    printf).  agent Have the remote debugging agent (such as gdbserver) handle
the output itself.  This style is only available for agents that support running
commands on the target.

set dprintf-function function

Set the function to call if the dprintf style is call. By default its value is
printf. You may set it to any expression. that gdb can evaluate to a function,
as per the call command.

set dprintf-channel channel

Set a channel for dprintf. If set to a non-empty value, gdb will evaluate it as
an expression and pass the result as a <first-argument> to the dprintf-function,
in the manner of fprintf and similar functions. Otherwise, the dprintf format
  string will be the first argument, in the manner of printf.


={============================================================================
*kt_linux_tool_400* gdb-stl-step-into-code

note:
This seems wrong since able to step into stl code without building gcc. Why
thought that needs to build gcc in the first place?

// To be able to step into stl code, need to have the debug version of C++ library
// and this is libstdc++6 in Debian Wheezy case.
// 
// https://packages.debian.org/wheezy/libstdc++6
// 
// (on pc vm, gcc-4.7)
// $ ldd a.out 
// 	linux-gate.so.1 =>  (0xb771f000)
// 	libstdc++.so.6 => /usr/lib/i386-linux-gnu/libstdc++.so.6 (0xb761a000)
// 	libm.so.6 => /lib/i386-linux-gnu/i686/cmov/libm.so.6 (0xb75f4000)
// 	libgcc_s.so.1 => /lib/i386-linux-gnu/libgcc_s.so.1 (0xb75d6000)
// 	libc.so.6 => /lib/i386-linux-gnu/i686/cmov/libc.so.6 (0xb7471000)
// 	/lib/ld-linux.so.2 (0xb7720000)
// 
// (on pc Debian Jessie, gcc-4.9)
// 09:26:35 ~/work$ ldd a.out 
// 	linux-vdso.so.1 (0x00007fff73fd5000)
// 	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fcba2103000)
// 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fcba1e02000)
// 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fcba1bec000)
// 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fcba1841000)
// 	/lib64/ld-linux-x86-64.so.2 (0x00007fcba240e000)
// 
// $ readelf --debug-dump=decodedline /usr/lib/i386-linux-gnu/libstdc++.so.6
// $ 
// 
// This shows that the release version.

a.out.rel : g++ -g -std=c++0x $1
a.out     : g++ -g -O0 -std=c++0x -D_GLIBCXX_DEBUG $1

$ ll a.out*
-rwxr-xr-x 1 keitee keitee  61920 Mar  7 23:27 a.out*
-rwxr-xr-x 1 keitee keitee  41069 Mar  7 23:22 a.out.rel*

$ ldd a.out
	linux-gate.so.1 =>  (0xb775d000)
	libstdc++.so.6 => /usr/lib/i386-linux-gnu/libstdc++.so.6 (0xb7658000)
	libm.so.6 => /lib/i386-linux-gnu/i686/cmov/libm.so.6 (0xb7632000)
	libgcc_s.so.1 => /lib/i386-linux-gnu/libgcc_s.so.1 (0xb7614000)
	libc.so.6 => /lib/i386-linux-gnu/i686/cmov/libc.so.6 (0xb74af000)
	/lib/ld-linux.so.2 (0xb775e000)

$ ldd a.out.rel 
	linux-gate.so.1 =>  (0xb7701000)
	libstdc++.so.6 => /usr/lib/i386-linux-gnu/libstdc++.so.6 (0xb75fc000)
	libm.so.6 => /lib/i386-linux-gnu/i686/cmov/libm.so.6 (0xb75d6000)
	libgcc_s.so.1 => /lib/i386-linux-gnu/libgcc_s.so.1 (0xb75b8000)
	libc.so.6 => /lib/i386-linux-gnu/i686/cmov/libc.so.6 (0xb7453000)
	/lib/ld-linux.so.2 (0xb7702000)

$ readelf --debug-dump=decodedline /usr/lib/i386-linux-gnu/libstdc++.so.6
$ 

Interestingly, the both case shows the same when do info sharedlibrary in gdb

(gdb) i sharedlibrary 
From                To                  Syms Read   Shared Object Library
0x00007ffff7b2c8b0  0x00007ffff7b94b1f  Yes (*)     /usr/lib/x86_64-linux-gnu/libstdc++.so.6
(*): Shared library is 'missing' debugging information.


={============================================================================
*kt_linux_gdb_400* gdb-debug-stl gcc-libstdc++

https://gcc.gnu.org/onlinedocs/libstdc++/index.html
http://gcc.gnu.org/onlinedocs/libstdc++/manual/debug_mode_using.html#debug_mode.using.mode

Chapter 17. Debug Mode

Using

Using the Debug Mode

To use the libstdc++ debug mode, compile your application with the compiler flag
-D_GLIBCXX_DEBUG. Note that this flag changes the sizes and behavior of standard
class templates such as std::vector, and therefore you can only link code
compiled with debug mode and code compiled without debug mode if no
instantiation of a container is passed between the two translation units.

By default, error messages are formatted to fit on lines of about 78 characters.
The environment variable GLIBCXX_DEBUG_MESSAGE_LENGTH can be used to request a
different length.  

Using a Specific Debug Container

When it is not feasible to recompile your entire application, or only specific
containers need checking, debugging containers are available as GNU extensions.
These debugging containers are functionally equivalent to the standard drop-in
containers used in debug mode, but they are available in a separate namespace as
GNU extensions and may be used in programs compiled with either release mode or
with debug mode. The following table provides the names and headers of the
debugging containers:

Table 17.1. Debugging Containers

Container           Header    Debug container       Debug header
std::bitset         bitset    __gnu_debug::bitset   <debug/bitset>
std::deque          deque     __gnu_debug::deque    <debug/deque>
std::list           list      __gnu_debug::list     <debug/list>
std::map            map       __gnu_debug::map      <debug/map>
std::multimap       map       __gnu_debug::multimap <debug/map>
std::multiset       set       __gnu_debug::multiset <debug/set>
std::set            set       __gnu_debug::set      <debug/set>
...

In addition, when compiling in C++11 mode, these additional containers have
additional debug capability.

Table 17.2. Debugging Containers C++11
...


{debug-version}
/toolchain/mipsel-linux-uclibc/include/c++/4.4.5/debug$ ll
-rw-r--r--  1 root root 10042 Nov 15  2010 bitset
-rw-r--r--  1 root root  5250 Nov 15  2010 debug.h
-rw-r--r--  1 root root 12814 Nov 15  2010 deque
-rw-r--r--  1 root root 11235 Nov 15  2010 formatter.h
-rw-r--r--  1 root root 13046 Nov 15  2010 functions.h
-rw-r--r--  1 root root 16527 Nov 15  2010 list
-rw-r--r--  1 root root 10924 Nov 15  2010 macros.h
-rw-r--r--  1 root root  1328 Nov 15  2010 map
-rw-r--r--  1 root root 12014 Nov 15  2010 map.h
-rw-r--r--  1 root root 11874 Nov 15  2010 multimap.h
-rw-r--r--  1 root root 11665 Nov 15  2010 multiset.h
-rw-r--r--  1 root root  7900 Nov 15  2010 safe_base.h
-rw-r--r--  1 root root 21599 Nov 15  2010 safe_iterator.h
-rw-r--r--  1 root root  4412 Nov 15  2010 safe_iterator.tcc
-rw-r--r--  1 root root  6206 Nov 15  2010 safe_sequence.h
-rw-r--r--  1 root root  1322 Nov 15  2010 set
-rw-r--r--  1 root root 11592 Nov 15  2010 set.h
-rw-r--r--  1 root root 30299 Nov 15  2010 string
-rw-r--r--  1 root root 17180 Nov 15  2010 unordered_map
-rw-r--r--  1 root root 17013 Nov 15  2010 unordered_set
-rw-r--r--  1 root root 15055 Nov 15  2010 vector


{debug-release-mix}
Mixing debug and release code is bad practice. The problem is that the different
versions can depend on different fundamental parts of the C++ runtime library,
         such as how memory is allocated, structures for things like iterators
         might be different, extra code could be generated to perform operations
         (e.g. checked iterators).

It's the same as mixing library files built with any other different settings.
Imagine a case where a header file contains a structure that is used by both
application and library. The library is built with structure packing and
alignment set to one value and the application built with another. There are no
guarantees that passing the structure from the application into the library will
work since they could vary in size and member positions.

Is it possible to build your 3rd party libraries as DLLs? Assuming the interface
to any functions is cleaner and does not try to pass any STL objects you will be
able to mix a debug application with release DLLs without problems.


Q:
When this parameter is defined I want all my std::vectors to check their
boundarys when accesing an element, the way when using vector::at().

When the parameter is omitted I want all vectors to behave as if the normal []
operator is used, meaning no performance is "wasted" for boundary checking.

You can activate runtime iterator and bounds checking by compiling with
-D_GLIBCXX_DEBUG. Also note that random-access containers provide the always
bounds-checking at() operation in addition to operator [].


<safe-stl>
The STL is not required to protect you from yourself and the STL is as error
prone as pointers are in C. Thus, it is a good idea to use a "safe" STL, at
least during software development.

Cay S. Horstmann. Safe STL
http://www.horstmann.com/safestl.html

STLport
http://www.stlport.org/


={============================================================================
*kt_linux_tool_400* gdb-stl-pretty-printer-gdb-stl-views

https://sourceware.org/gdb/wiki/STLSupport

When you try to use GDB's "print" command to display the contents of a vector, a
stack, or any other GDB abstract data structure, you will get useless results.
Instead, download and install one of following tools to properly view the
contents of STL containers from within GDB. 

<ex>
GNU gdb (Debian 7.7.1+dfsg-5) 7.7.1

// Use -D_GLIBCXX_DEBUG

(gdb) p coll
$1 = std::__debug::vector of length 6, capacity 8 = {1, 2, 3, 4, 5, 6}

// when install gdb-stl-views

(gdb) pvector coll
elem[0]: $2 = 1
elem[1]: $3 = 2
elem[2]: $4 = 3
elem[3]: $5 = 4
elem[4]: $6 = 5
elem[5]: $7 = 6
Vector size = 6
Vector capacity = 8
Element type = std::__cxx1998::_Vector_base<int, std::allocator<int> >::pointer
(gdb) 


// Not use -D_GLIBCXX_DEBUG

(gdb) p coll
$1 = std::vector of length 6, capacity 8 = {1, 2, 3, 4, 5, 6}

(gdb) pvector coll
elem[0]: $2 = 1
elem[1]: $3 = 2
elem[2]: $4 = 3
elem[3]: $5 = 4
elem[4]: $6 = 5
elem[5]: $7 = 6
Vector size = 6
Vector capacity = 8
Element type = std::_Vector_base<int, std::allocator<int> >::pointer
(gdb) 

note: WHY p coll works without configuring any? Maybe:

GDB 7.0 will include support for writing pretty-printers in Python. This
feature, combined with the pretty-printers in the libstdc++ svn repository,
    yields the best way to visualize C++ containers. Some distros (Fedora 11+)
        ship all this code in a way that requires no configuration; in other
        cases, this email message explains how to set everything up. The main
        points have been redacted here: 

// gdb-man
10.9.1 Pretty-Printer Introduction

When gdb prints a value, it first sees if there is a pretty-printer registered
for the value. If there is then gdb invokes the pretty-printer to print the
    value. Otherwise the value is printed normally.

Pretty-printers are normally named. This makes them easy to manage. The 

info pretty-printer~

will list all the installed pretty-printers with their names. If a
pretty-printer can handle multiple data types, then its subprinters are the
printers for the individual data types. Each such subprinter has its own name.
The format of the name is printer-name;subprinter-name.

Pretty-printers are installed by registering them with gdb. Typically they are
automatically loaded and registered when the corresponding debug information is
loaded, thus making them available without having to do anything special.

There are three places where a pretty-printer can be registered.

    Pretty-printers registered globally are available when debugging all
    inferiors.

    Pretty-printers registered with a program space are available only when
    debugging that program. See Progspaces In Python, for more details on
    program spaces in Python.

    Pretty-printers registered with an objfile are loaded and unloaded with the
    corresponding objfile (e.g., shared library). See Objfiles In Python, for
    more details on objfiles in Python. 

<ex>
(gdb) info pretty-printer 
global pretty-printers:
  .*
    bound
  objfile /usr/lib/x86_64-linux-gnu/libstdc++.so.6 pretty-printers:
  libstdc++-v6
    __gnu_cxx::_Slist_iterator
    __gnu_cxx::__7::_Slist_iterator
    __gnu_cxx::__7::__normal_iterator
    __gnu_cxx::__7::slist
    __gnu_cxx::__normal_iterator
    __gnu_cxx::slist
    __gnu_debug::_Safe_iterator
    std::_Deque_const_iterator
    std::_Deque_iterator
    std::_List_const_iterator
    std::_List_iterator
    std::_Rb_tree_const_iterator
    std::_Rb_tree_iterator
    std::__7::_Deque_const_iterator
    std::__7::_Deque_iterator
    std::__7::_List_const_iterator
    std::__7::_List_iterator
    std::__7::_Rb_tree_const_iterator
    std::__7::_Rb_tree_iterator
    std::__7::basic_string
    std::__7::bitset
    std::__7::deque
    std::__7::forward_list
    std::__7::list
    std::__7::map
    std::__7::multimap
 ---Type <return> to continue, or q <return> to quit---


{gdb-stl-views}

<init-file>
#
#   STL GDB evaluators/views/utilities - 1.03
#
#   The new GDB commands:
#       are entirely non instrumental
#       do not depend on any "inline"(s) - e.g. size(), [], etc
#       are extremely tolerant to debugger settings
#
#   This file should be "included" in .gdbinit as following:
#   source stl-views.gdb or just paste it into your .gdbinit file
#
#   The following STL containers are currently supported:
#
#       std::vector<T> -- via pvector command
#       std::list<T> -- via plist or plist_member command
#       std::map<T,T> -- via pmap or pmap_member command
#       std::multimap<T,T> -- via pmap or pmap_member command
#       std::set<T> -- via pset command
#       std::multiset<T> -- via pset command
#       std::deque<T> -- via pdequeue command
#       std::stack<T> -- via pstack command
#       std::queue<T> -- via pqueue command
#       std::priority_queue<T> -- via ppqueue command
#       std::bitset<n> -- via pbitset command
#       std::string -- via pstring command
#       std::widestring -- via pwstring command
#
#   The end of this file contains (optional) C++ beautifiers
#   Make sure your debugger supports $argc
#
#   Simple GDB Macros writen by Dan Marinescu (H-PhD) - License GPL
#   Inspired by intial work of Tom Malnar,
#     Tony Novac (PhD) / Cornell / Stanford,
#     Gilad Mishne (PhD) and Many Many Others.
#   Contact: dan_c_marinescu@yahoo.com (Subject: STL)
#
#   Modified to work with g++ 4.3 by Anders Elton
#   Also added _member functions, that instead of printing the entire class in map, prints a member.

# support for pending breakpoints - you can now set a breakpoint into a shared library before the it was loaded.
set breakpoint pending on

#
# std::vector<>
#

define pvector
    if $argc == 0
        help pvector
    else
        set $size = $arg0._M_impl._M_finish - $arg0._M_impl._M_start
        set $capacity = $arg0._M_impl._M_end_of_storage - $arg0._M_impl._M_start
        set $size_max = $size - 1
    end
    if $argc == 1
        set $i = 0
        while $i < $size
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
    end
    if $argc == 2
        set $idx = $arg1
        if $idx < 0 || $idx > $size_max
            printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
        else
            printf "elem[%u]: ", $idx
            p *($arg0._M_impl._M_start + $idx)
        end
    end
    if $argc == 3
      set $start_idx = $arg1
      set $stop_idx = $arg2
      if $start_idx > $stop_idx
        set $tmp_idx = $start_idx
        set $start_idx = $stop_idx
        set $stop_idx = $tmp_idx
      end
      if $start_idx < 0 || $stop_idx < 0 || $start_idx > $size_max || $stop_idx > $size_max
        printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
      else
        set $i = $start_idx
        while $i <= $stop_idx
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
      end
    end
    if $argc > 0
        printf "Vector size = %u\n", $size
        printf "Vector capacity = %u\n", $capacity
        printf "Element "
        whatis $arg0._M_impl._M_start
    end
end

document pvector
    Prints std::vector<T> information.
    Syntax: pvector <vector> <idx1> <idx2>
    Note: idx, idx1 and idx2 must be in acceptable range [0..<vector>.size()-1].
    Examples:
    pvector v - Prints vector content, size, capacity and T typedef
    pvector v 0 - Prints element[idx] from vector
    pvector v 1 2 - Prints elements in range [idx1..idx2] from vector
end

#
# std::list<>
#

define plist
    if $argc == 0
        help plist
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 2
                printf "elem[%u]: ", $size
                p *($arg1*)($current + 1)
            end
            if $argc == 3
                if $size == $arg2
                    printf "elem[%u]: ", $size
                    p *($arg1*)($current + 1)
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist <variable_name> <element_type> to see the elements in the list.\n"
        end
    end
end

document plist
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist l - prints list size and definition
    plist l int - prints all elements and list size
    plist l int 2 - prints the third element in the list (if exists) and list size
end

define plist_member
    if $argc == 0
        help plist_member
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 3
                printf "elem[%u]: ", $size
                p (*($arg1*)($current + 1)).$arg2
            end
            if $argc == 4
                if $size == $arg3
                    printf "elem[%u]: ", $size
                    p (*($arg1*)($current + 1)).$arg2
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist_member <variable_name> <element_type> <member> to see the elements in the list.\n"
        end
    end
end

document plist_member
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist_member l int member - prints all elements and list size
    plist_member l int member 2 - prints the third element in the list (if exists) and list size
end


#
# std::map and std::multimap
#

define pmap
    if $argc == 0
        help pmap
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 3
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p *($arg1*)$value
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p *($arg2*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 4
            set $idx = $arg3
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p *($arg1*)$value
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p *($arg2*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        if $argc == 5
            set $idx1 = $arg3
            set $idx2 = $arg4
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                set $valueLeft = *($arg1*)$value
                set $valueRight = *($arg2*)($value + sizeof($arg1))
                if $valueLeft == $idx1 && $valueRight == $idx2
                    printf "elem[%u].left: ", $i
                    p $valueLeft
                    printf "elem[%u].right: ", $i
                    p $valueRight
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap m - prints map size and definition
    pmap m int int - prints all elements and map size
    pmap m int int 20 - prints the element(s) with left-value = 20 (if any) and map size
    pmap m int int 20 200 - prints the element(s) with left-value = 20 and right-value = 200 (if any) and map size
end


define pmap_member
    if $argc == 0
        help pmap_member
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 5
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p (*($arg1*)$value).$arg2
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p (*($arg3*)$value).$arg4
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 6
            set $idx = $arg5
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p (*($arg1*)$value).$arg2
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p (*($arg3*)$value).$arg4
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap_member
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap_member m class1 member1 class2 member2 - prints class1.member1 : class2.member2
    pmap_member m class1 member1 class2 member2 lvalue - prints class1.member1 : class2.member2 where class1 == lvalue
end


#
# std::set and std::multiset
#

define pset
    if $argc == 0
        help pset
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Set "
            whatis $tree
            printf "Use pset <variable_name> <element_type> to see the elements in the set.\n"
        end
        if $argc == 2
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u]: ", $i
                p *($arg1*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 3
            set $idx = $arg2
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u]: ", $i
                    p *($arg1*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Set size = %u\n", $tree_size
    end
end

document pset
    Prints std::set<T> or std::multiset<T> information. Works for std::multiset as well.
    Syntax: pset <set> <T> <val>: Prints set size, if T defined all elements or just element(s) having val
    Examples:
    pset s - prints set size and definition
    pset s int - prints all elements and the size of s
    pset s int 20 - prints the element(s) with value = 20 (if any) and the size of s
end



#
# std::dequeue
#

define pdequeue
    if $argc == 0
        help pdequeue
    else
        set $size = 0
        set $start_cur = $arg0._M_impl._M_start._M_cur
        set $start_last = $arg0._M_impl._M_start._M_last
        set $start_stop = $start_last
        while $start_cur != $start_stop
            p *$start_cur
            set $start_cur++
            set $size++
        end
        set $finish_first = $arg0._M_impl._M_finish._M_first
        set $finish_cur = $arg0._M_impl._M_finish._M_cur
        set $finish_last = $arg0._M_impl._M_finish._M_last
        if $finish_cur < $finish_last
            set $finish_stop = $finish_cur
        else
            set $finish_stop = $finish_last
        end
        while $finish_first != $finish_stop
            p *$finish_first
            set $finish_first++
            set $size++
        end
        printf "Dequeue size = %u\n", $size
    end
end

document pdequeue
    Prints std::dequeue<T> information.
    Syntax: pdequeue <dequeue>: Prints dequeue size, if T defined all elements
    Deque elements are listed "left to right" (left-most stands for front and right-most stands for back)
    Example:
    pdequeue d - prints all elements and size of d
end



#
# std::stack
#

define pstack
    if $argc == 0
        help pstack
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = $size - 1
        while $i >= 0
            p *($start_cur + $i)
            set $i--
        end
        printf "Stack size = %u\n", $size
    end
end

document pstack
    Prints std::stack<T> information.
    Syntax: pstack <stack>: Prints all elements and size of the stack
    Stack elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    pstack s - prints all elements and the size of s
end



#
# std::queue
#

define pqueue
    if $argc == 0
        help pqueue
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = 0
        while $i < $size
            p *($start_cur + $i)
            set $i++
        end
        printf "Queue size = %u\n", $size
    end
end

document pqueue
    Prints std::queue<T> information.
    Syntax: pqueue <queue>: Prints all elements and the size of the queue
    Queue elements are listed "top to bottom" (top-most element is the first to come on pop)
    Example:
    pqueue q - prints all elements and the size of q
end



#
# std::priority_queue
#

define ppqueue
    if $argc == 0
        help ppqueue
    else
        set $size = $arg0.c._M_impl._M_finish - $arg0.c._M_impl._M_start
        set $capacity = $arg0.c._M_impl._M_end_of_storage - $arg0.c._M_impl._M_start
        set $i = $size - 1
        while $i >= 0
            p *($arg0.c._M_impl._M_start + $i)
            set $i--
        end
        printf "Priority queue size = %u\n", $size
        printf "Priority queue capacity = %u\n", $capacity
    end
end

document ppqueue
    Prints std::priority_queue<T> information.
    Syntax: ppqueue <priority_queue>: Prints all elements, size and capacity of the priority_queue
    Priority_queue elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    ppqueue pq - prints all elements, size and capacity of pq
end



#
# std::bitset
#

define pbitset
    if $argc == 0
        help pbitset
    else
        p /t $arg0._M_w
    end
end

document pbitset
    Prints std::bitset<n> information.
    Syntax: pbitset <bitset>: Prints all bits in bitset
    Example:
    pbitset b - prints all bits in b
end



#
# std::string
#

define pstring
    if $argc == 0
        help pstring
    else
        printf "String \t\t\t= \"%s\"\n", $arg0._M_data()
        printf "String size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "String capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "String ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pstring
    Prints std::string information.
    Syntax: pstring <string>
    Example:
    pstring s - Prints content, size/length, capacity and ref-count of string s
end

#
# std::wstring
#

define pwstring
    if $argc == 0
        help pwstring
    else
        call printf("WString \t\t= \"%ls\"\n", $arg0._M_data())
        printf "WString size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "WString capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "WString ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pwstring
    Prints std::wstring information.
    Syntax: pwstring <wstring>
    Example:
    pwstring s - Prints content, size/length, capacity and ref-count of wstring s
end

#
# C++ related beautifiers (optional)
#

set print pretty on
set print object on
set print static-members on
set print vtbl on
set print demangle on
set demangle-style gnu-v3
set print sevenbit-strings off

set history filename ~/.gdb_history
set history save

# finally stop the silly "A debugging session is active." - question ... just quit both.
set confirm off


={============================================================================
*kt_linux_gdb_400* gdb-registers gdb-init

10.13 Registers

You can refer to machine register contents, in expressions, as variables with
names starting with ‘$’. The names of registers are different for each
machine; use info registers to see the names used on your machine.

info registers

(gdb) i reg
eax            0xb7b8c9ac       -1212626516
ecx            0xfbad0084       -72548220
edx            0xb7a26870       -1214093200
ebx            0x403000 4206592
esp            0xbffff120       0xbffff120
ebp            0xbffff1a8       0xbffff1a8
esi            0xbffff130       -1073745616
edi            0xb7a25000       -1214099456
eip            0x400d49 0x400d49 <call_gbo+35>
eflags         0x246    [ PF ZF IF ]
cs             0x73     115
ss             0x7b     123
ds             0x7b     123
es             0x7b     123
fs             0x0      0
gs             0x33     51


Print the names and values of all registers except floating-point and vector
registers (in the selected stack frame).

info all-registers

Print the names and values of all registers, including floating-point and
vector registers (in the selected stack frame).


https://github.com/gdbinit/Gdbinit/blob/master/gdbinit

<ex>
gdb$ next
------------------------------------------------------------------------[regs]
  RAX: 0x0000000000000001  RBX: 0x0000000000000000  RBP: 0x00007FFFFFFFDC10  RSP: 0x00007FFFFFFFDBD0  o d I t s z a p c 
  RDI: 0x00007FFFFFFFDBE0  RSI: 0x0000000000603064  RDX: 0x0000000000000018  RCX: 0x0000000000603064  RIP: 0x0000000000400CFD
  R8 : 0x0000000000000000  R9 : 0x0000000000603010  R10: 0x00007FFFFFFFD890  R11: 0x00007FFFF728B6E0  R12: 0x0000000000400BA0
  R13: 0x00007FFFFFFFDCF0  R14: 0x0000000000000000  R15: 0x0000000000000000
  CS: 0033  DS: 0000  ES: 0000  FS: 0000  GS: 0000  SS: 002B				
------------------------------------------------------------------------[code]
=> 0x400cfd <main()+103>:	mov    eax,DWORD PTR [rbp-0x14]
   0x400d00 <main()+106>:	movsxd rdx,eax
   0x400d03 <main()+109>:	lea    rax,[rbp-0x30]
   0x400d07 <main()+113>:	mov    rsi,rdx
   0x400d0a <main()+116>:	mov    rdi,rax
   0x400d0d <main()+119>:	call   0x400f0e <std::vector<int, std::allocator<int> >::operator[](unsigned long)>
   0x400d12 <main()+124>:	mov    eax,DWORD PTR [rax]
   0x400d14 <main()+126>:	mov    esi,eax
 ----------------------------------------------------------------------------
17	        cout << coll[i] << ':';
gdb$


# ============================================================================
#{
={============================================================================
*kt_linux_rege_001* regex-bre-ere

http://regexr.com/
https://www.gnu.org/software/grep/manual/grep.html#Regular-Expressions

This manual is for version 2.25 of GNU Grep.

3 Regular Expressions

A `regular-expression` is a pattern that describes a set of strings. Regular
expressions are constructed analogously to arithmetic expressions, by using
various operators to combine smaller expressions. grep understands three
different versions of regular expression syntax: 

basic(BRE), extended(ERE) and perl(PCRE). 

In GNU grep, there is no difference in available functionality between the basic
and extended syntaxes. In other implementations, basic regular expressions are
less powerful. 

The following description applies to `extended regular expressions`; differences
for basic regular expressions are summarized afterwards. 

Perl-compatible regular expressions give additional functionality, and are
documented in the pcresyntax(3) and pcrepattern(3) manual pages, but work only
if PCRE is available in the system. 


http://www.regular-expressions.info/posix.html
A BRE supports POSIX bracket expressions, which are similar to character
classes in other regex flavors, with a few special features. Shorthands are
not supported. Other features using the usual metacharacters are the dot to
match any character except a line break, the caret and dollar to match the
start and end of the string, and the star to repeat the token zero or more
times. To match any of these characters literally, escape them with a
backslash.

The other BRE metacharacters require a backslash to give them their special
meaning. The reason is that the oldest versions of UNIX grep did not support
these. The developers of grep wanted to keep it `compatible` with existing
regular expressions, which may use these characters as literal characters. The
BRE a{1,2} matches a{1,2} literally, while a\{1,2\} matches a or aa.

BRE is similar to the one used by the traditional UNIX grep command. The
original UNIX grep, which only had bracket expressions, dot, caret, dollar and
star.

<bre-vs-ere> *regex-basic* 
One thing that sets this flavor apart is that most metacharacters require a
backslash to give the metacharacter its flavor. Most other flavors, including
POSIX ERE, use a backslash to suppress the meaning of metacharacters.

https://www.gnu.org/software/sed/manual/sed.html
Appendix A Extended regular expressions
The only difference between basic and extended regular expressions is in the
behavior of a few characters: ‘?’, ‘+’, parentheses, and braces (‘{}’). 

While `basic regular expressions require these to be escaped` if you want them
to behave as special characters, when using extended regular expressions you
must escape them if you want them to match a literal character. 


3.6 Basic vs Extended Regular Expressions

*sh-regex* *vim-regex*
In basic regular expressions the meta-characters ‘?’, `+`, ‘{’, ‘|’, ‘(’, and
    ‘)’ lose their special meaning; instead use the backslashed versions ‘\?’,
`\+`, ‘\{’, ‘\|’, ‘\(’, and ‘\)’.

Traditional egrep did not support the ‘{’ meta-character, and some egrep
  implementations support ‘\{’ instead, so portable scripts should avoid ‘{’ in
    ‘grep -E’ patterns and should use ‘[{]’ to match a literal ‘{’.

GNU grep -E attempts to support traditional usage by assuming that ‘{’ is not
  special if it would be the start of an invalid interval specification. 

For example, the command ‘grep -E '{1'’ searches for the two-character string
  ‘{1’ instead of reporting a syntax error in the regular expression. POSIX
    allows this behavior as an extension, but portable scripts should avoid it. 

<ex>
since vim-regex uses basic

/w+
/\w\+       // vim-regex


={============================================================================
*kt_linux_rege_001* regex-operators

1 Introduction

grep searches input files for lines containing a match to a given pattern
list. When it finds `a match in a line`, it copies the line to standard output
(by default), or produces whatever other sort of output you have requested
with options.

Though grep expects to do the matching on text, it has no limits on input line
length other than available memory, and it can match arbitrary characters
within a line.

note:
regex is `line-operation`


2 Invoking grep

The general synopsis of the grep command line is 

grep options pattern input_file_names


3.1 Fundamental Structure

The fundamental building blocks are the regular expressions that match a `single
character`. Most characters, including all letters and digits, are regular
expressions that match themselves. 

Any `meta-character` with special meaning may be quoted by preceding it with a
backslash.

A regular expression may be followed by one of several `repetition-operators`:

‘.’
    The period ‘.’ matches any single character.

‘?’
    The `preceding` item is `optional` and will be matched at most once.

    note: from py.re
    Causes the resulting RE to match 0 or 1 repetitions of the preceding RE.
    ab? will match either ‘a’ or ‘ab’.

‘*’
    The preceding item will be matched zero or more times.

‘+’
    The preceding item will be matched one or more times.

‘{n}’
    The preceding item is matched exactly n times.

‘{n,}’
    The preceding item is matched n or more times.

‘{,m}’
    The preceding item is matched at most m times. This is a GNU extension.

‘{n,m}’
    The preceding item is matched at least n times, but not more than m times.

The empty regular expression matches the empty string. Two regular expressions
may be concatenated; the resulting regular expression matches any string formed
by concatenating two substrings that respectively match the concatenated
expressions.

*regex-alternation* *regex-meta-or*
Two regular expressions may be joined by the `infix-operator` ‘|’; the resulting
regular expression matches any string matching either `alternate` expression.

Repetition takes `precedence` over concatenation, which in turn takes precedence
over alternation. A whole expression may be enclosed in `parentheses` to
override these precedence rules and form a subexpression. An unmatched ‘)’
matches just itself. 

<ex>
hMC=(0x47|0x48)


3.2 Character Classes and Bracket Expressions

A `bracket-expression` is a list of characters enclosed by ‘[’ and ‘]’. It
matches any `single character` in that list; 

if the first character of the list is the caret ‘^’, then it matches any
  character `not in the list.` 

For example, the regular expression ‘[0123456789]’ matches any single digit.


<ex>
s:[65]:Dig:g      " substitue 

High 65 to 70. Southeast wind around 10
->
High DigDig to 70. Southeast wind around 10

s:65:Dig:g

High 65 to 70. Southeast wind around 10
->
High Dig to 70. Southeast wind around 10


Within a bracket expression, a `range-expression` consists of two characters
separated by a hyphen. It matches any single character that sorts between the
two characters, inclusive. In the default C locale, the sorting sequence is the
native character order; for example, ‘[a-d]’ is equivalent to ‘[abcd]’. In other
locales, the sorting sequence is not specified, and ‘[a-d]’ might be equivalent
to ‘[abcd]’ or to ‘[aBbCcDd]’, or it might fail to match any character, or the
set of characters that it matches might even be erratic. To obtain the
traditional interpretation of bracket expressions, you can use the ‘C’ locale by
setting the LC_ALL environment variable to the value ‘C’.

Finally, certain named `classes of characters` are predefined within bracket
expressions, as follows. Their interpretation depends on the LC_CTYPE locale;
for example, ‘[[:alnum:]]’ means the character class of numbers and letters in
  the current locale.

‘[:alnum:]’
    Alphanumeric characters: ‘[:alpha:]’ and ‘[:digit:]’; in the ‘C’ locale and
    ASCII character encoding, this is the same as ‘[0-9A-Za-z]’.

‘[:alpha:]’
    Alphabetic characters: ‘[:lower:]’ and ‘[:upper:]’; in the ‘C’ locale and
    ASCII character encoding, this is the same as ‘[A-Za-z]’.

‘[:blank:]’
    Blank characters: space and tab.

‘[:cntrl:]’
    Control characters. In ASCII, these characters have octal codes 000 through
    037, and 177 (DEL). In other character sets, these are the equivalent
    characters, if any.

‘[:digit:]’
    Digits: 0 1 2 3 4 5 6 7 8 9.

‘[:graph:]’
    Graphical characters: ‘[:alnum:]’ and ‘[:punct:]’.

‘[:lower:]’
    Lower-case letters; in the ‘C’ locale and ASCII character encoding, this is
    a b c d e f g h i j k l m n o p q r s t u v w x y z.

‘[:print:]’
    Printable characters: ‘[:alnum:]’, ‘[:punct:]’, and space.

‘[:punct:]’
    Punctuation characters; in the ‘C’ locale and ASCII character encoding, this
    is ! " # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \ ] ^ _ ` { | } ~.

‘[:space:]’
    Space characters: in the ‘C’ locale, this is tab, newline, vertical tab,
          form feed, carriage return, and space. See Usage, for more discussion
            of matching newlines.

‘[:upper:]’
    Upper-case letters: in the ‘C’ locale and ASCII character encoding, this is
    A B C D E F G H I J K L M N O P Q R S T U V W X Y Z.

‘[:xdigit:]’
    Hexadecimal digits: 0 1 2 3 4 5 6 7 8 9 A B C D E F a b c d e f.


Note that the brackets in these class names are part of the symbolic names, and
must be included in addition to the brackets delimiting the bracket expression.

If you mistakenly omit the outer brackets, and search for say, ‘[:upper:]’, GNU
grep prints a diagnostic and exits with status 2, on the assumption that you did
not intend to search for the nominally equivalent regular expression: ‘[:epru]’.
Set the POSIXLY_CORRECT environment variable to disable this feature.

Most meta-characters lose their special meaning inside bracket expressions.

‘]’
    ends the bracket expression if it’s not the first list item. So, if you want
    to make the ‘]’ character a list item, you must put it first.

‘[.’
    represents the open collating symbol.

‘.]’
    represents the close collating symbol.

‘[=’
    represents the open equivalence class.

‘=]’
    represents the close equivalence class.

‘[:’
    represents the open character class symbol, and should be followed by a
    valid character class name.

‘:]’
    represents the close character class symbol.

‘-’
    represents the range if it’s not first or last in a list or the ending point
    of a range.

‘^’
    represents the characters not in the list. If you want to make the ‘^’
    character a list item, place it anywhere but first.

<regex-backslash>
3.3 The Backslash Character and Special Expressions

The `\` character, when followed by certain ordinary characters, takes a special
meaning:

‘\b’
    Match the `empty string` `at-the-edge` of a word.

‘\B’
    Match the empty string provided it's not at the edge of a word.

‘\<’
    Match the empty string at the beginning of word.

‘\>’
    Match the empty string at the end of word.

‘\w’
    Match `word-constituent`, it is a synonym for ‘[_[:alnum:]]’.

‘\W’
    Match non-word constituent, it is a synonym for ‘[^_[:alnum:]]’.

‘\s’
    Match whitespace, it is a synonym for ‘[[:space:]]’.

‘\S’
    Match non-whitespace, it is a synonym for ‘[^[:space:]]’.
    *regex-negative-match*


3.4 Anchoring

The caret ‘^’ and the dollar sign ‘$’ are meta-characters that respectively
match the `empty string` at the beginning and end of a line. They are termed
anchors, since they force the match to be "anchored" to beginning or end of a
line, respectively. 


<word-boundary-empty-string-zero-length>
http://www.regular-expressions.info/wordboundaries.html

Word Boundaries

‘\b’
    Match the `empty string` `at-the-edge` of a word.

The metacharacter \b is an `anchor` like the caret and the dollar sign. It
matches at a position that is called a "word boundary". This match is
`zero-length`

There are three different positions that qualify as word boundaries:

    Before the first character in the string, if the first character is a word
    character.

    After the last character in the string, if the last character is a word
    character.

    Between two characters in the string, where one is a word character and
    the other is not a word character.

Simply put: \b allows you to perform a "whole words only" search using a
regular expression in the form of \bword\b. A "word character" is a character
that can be used to form words. All characters that are not "word characters"
are "non-word characters".

Exactly which characters are word characters depends on the regex flavor
you're working with. In most flavors, characters that are matched by the
short-hand character class `\w` are the characters that are treated as word
characters by word boundaries. Java is an exception. Java supports Unicode for
\b but not for \w.

\B is the negated version of \b. \B matches at every position where \b does
not. Effectively, \B matches at any position between two word characters as
well as at any position between two non-word characters.


Looking Inside The Regex Engine

See what happens when we apply "\bis\b" to the string "This island is
beautiful". The engine starts with the first token \b at the first character
T. Since this token is `zero-length`, the position before the character is
inspected. \b matches here, because the T is a word character and the
character before it is the void before the start of the string. The engine
continues with the next token: the literal i. The engine does not advance to
the next character in the string, because the previous regex token was
zero-length. i does not match T, so the engine retries the first token at the
next character position.

$ echo "This island is beautiful" | grep "\bis\b"
1:This island is beautiful
              ^^


={============================================================================
*kt_linux_rege_001* regex-group regex-reference

$ man pcresyntax

CAPTURING

         (...)           capturing group
         (?<name>...)    named capturing group (Perl)
         (?'name'...)    named capturing group (Perl)
         (?P<name>...)   named capturing group (Python)
         (?:...)         non-capturing group
         (?|...)         non-capturing group; reset group numbers for
                          capturing groups in each alternative

http://www.regular-expressions.info/brackets.html

Use Parentheses for Grouping and Capturing

You can group that part of the regular expression together. This allows you to
`apply a quantifier to the entire group` or to restrict alternation to part of
the regex.


Parentheses Create Numbered Capturing Groups

Besides grouping part of a regular expression together, parentheses also
create a numbered capturing group. It stores the part of the string matched by
the part of the regular expression inside the parentheses.

The regex Set(Value)? matches Set or SetValue. In the first case, the first
(and only) capturing group remains empty. In the second case, the first
capturing group matches Value.


http://docs.splunk.com/Documentation/Splunk/6.5.2/Knowledge/AboutSplunkregularexpressions
Non-capturing group matching

Use the syntax (?: ... ) to create groups that are matched but which are not
captured. Note that here you do not need to include a field name in angle
brackets. The colon character after the ? character is what identifies it as a
non-capturing group.

For example, (?:Foo|Bar) matches either Foo or Bar, but neither string is
captured.


note: WHY use non-capturing group match?
https://stackoverflow.com/questions/18578714/what-is-the-purpose-of-the-passive-non-capturing-group-in-a-javascript-regex

The use case is that sometimes you must (or should) use a group not because
you are interested in what it captures but for syntactic reasons. In these
situations it makes sense to use a non-capturing group instead of a "standard"
capturing one because it is `less resource intensive` -- but if you don't care
about that, a capturing group will behave in the exact same manner.


Using Text Matched By Capturing Groups

Capturing groups make it easy to extract part of the regex match. You can
reuse the text inside the regular expression via a backreference.
Backreferences can also be used in replacement strings. Please check the
replacement text tutorial for details.


3.5 Back-references and Subexpressions

The back-reference ‘\n’, where n is a single digit, matches the substring
`previously` matched by `the nth parenthesized subexpression` of the regular
expression. For example, ‘(a)\1’ matches ‘aa’. 

When used with alternation, if the group does not participate in the match then
the back-reference makes the whole match fail. For example, ‘a(.)|b\1’ will not
match ‘ba’. 

When multiple regular expressions are given with -e or from a file (‘-f file’),
back-references are local to each expression. 

<ex>
ac_option=--cache-file=/dev/null
ac_optarg=``expr "x$ac_option" : 'x[^=]*=\(.*\)'``

$echo $ac_optarg
/dev/null


={============================================================================
*kt_linux_rege_001* regex-named-capture-group-reference

As shown, it's Python specific.

         (?P<name>...)   named capturing group (Python)

https://docs.python.org/2/howto/regex.html#regex-howto

7.2.1. Regular Expression Syntax

A brief explanation of the format of regular expressions follows. For further
information and a gentler presentation, consult the Regular Expression HOWTO.

(?...)
    This is an `extension` notation (a '?' following a '(' is not meaningful
        otherwise). The first character after the '?' determines what the
    meaning and further syntax of the construct is. 

    Extensions usually do not create a new group; (?P<name>...) is the only
    `exception to this rule` 
    
    Following are the currently supported extensions.

(?P<name>...)

    Similar to regular parentheses, but the substring matched by the group is
    accessible via `the symbolic group name` 
    
    Group names must be valid Python identifiers, and each group name must be
    defined only once `within a regular expression` A symbolic group is also a
    numbered group, just as if the group were not named.

    Named groups can be referenced in three contexts. If the pattern is
    (?P<quote>['"]).*?(?P=quote) (i.e. matching a string quoted with either
    single or double quotes):

(?P=name)

    A backreference to a named group; it matches whatever text was matched by
    the earlier group named name.


http://www.regular-expressions.info/named.html

http://docs.splunk.com/Documentation/Splunk/6.5.2/Knowledge/AboutSplunkregularexpressions

Capture groups in regular expressions

A named capture group is a regular expression grouping that extracts a field
value when regular expression matches an event. Capture groups include the
name of the field. They are notated with angle brackets as follows:

matching text (?<field_name>capture pattern) more matching text.

For example, you have this event text:

131.253.24.135 fail admin_user

Here are two regular expressions that use different syntax in their capturing
groups to pull the same set of fields from that event.

 Expression A: (?<ip>\d+\.\d+\.\d+\.\d+) (?<result>\w+) (?<user>.*)
 Expression B: (?<ip>\S+) (?<result>\S+) (?<user>\S+)

In Expression A, the pattern-matching characters used for the first capture
group (ip) are specific. \d means "digit" and + means "one or more." So \d+
means "one or more digits." \. refers to a period.

The capture group for ip wants to match one or more digits, followed by a
period, followed by one or more digits, followed by a period, followed by one
or more digits, followed by a period, followed by one or more digits. This
describes the syntax for an ip address.

The second capture group in Expression A for the result field has the pattern
\w+, which means "one or more alphanumeric characters." The third capture
group in Expression A for the user field has the pattern .*, which means
"match everything that's left."

Expression B uses a common technique called `negative matching.` With negative
matching, the regular expression does not try to define which text to match.
Instead it defines what the text is not. In this Expression B, the values that
should be extracted from the sample event are "not space" characters (\S). It
uses the + to specify "one or more" of the "not space" characters.

So Expression B says:

Pull out the first string of not-space characters for the ip field value.

Ignore the following space.

Then pull out the second string of not-space characters for the result field
value.

Ignore the second space.

Pull out the third string of not-space characters for the user field value."


={============================================================================
*kt_linux_rege_001* regex-pcre

PCRE is short for Perl Compatible Regular Expressions. It is the name of an
open source library written in C by Philip Hazel. The library is compatible
with a great number of C compilers and operating systems. Many people have
derived libraries from PCRE to make it compatible with other programming
languages. The regex features included with PHP, Delphi, and R, and Xojo
(REALbasic) are all based on PCRE. The library is also included with many
Linux distributions as a shared .so library and a .h header file.

Though PCRE claims to be Perl-compatible, there are more than enough
differences between contemporary versions of Perl and PCRE to consider them
distinct regex flavors. Recent versions of Perl have even copied features from
PCRE that PCRE had copied from other programming languages before Perl had
them, in an attempt to make Perl more PCRE-compatible. Today PCRE is used more
widely than Perl because PCRE is part of so many libraries and applications.

Philip Hazel has recently released a new library called PCRE2. The first PCRE2
release was given version number 10.00 to make a clear break with the previous
PCRE 8.36. Future PCRE releases will be limited to bug fixes. New features
will go into PCRE2 only. If you're taking on a new development project, you
should consider using PCRE2 instead of PCRE. But for existing projects that
already use PCRE, it's probably best to stick with PCRE. Moving from PCRE to
PCRE2 requires significant changes to your source code (but not to your
    regular expressions).

You can find more information about PCRE and PCRE2 at http://www.pcre.org/.

http://perldoc.perl.org/perlre.html


={============================================================================
*kt_linux_rege_001* regex-usage

4 Usage

Here is an example command that invokes GNU grep:

grep -i 'hello.*world' menu.h main.c

This lists all lines in the files menu.h and main.c that contain the string
‘hello’ followed by the string ‘world’; this is because ‘.*’ matches zero or
more characters within a line. See Regular Expressions. The -i option causes
grep to ignore case, causing it to match the line ‘Hello, world!’, which it
would not otherwise match. See Invoking, for more details about how to invoke
grep.


Q: Why quote the pattern? A: From *sh-quote* :

<ex>
To prevent globbing by shell before passing params to grep command.

$ grep '[0-9][0-9]*$' report2 report7


Here are some common questions and answers about grep usage.

    1. How can I list just the names of matching files?

    grep -l 'main' *.c

    lists the names of all C files in the `current-directory` whose contents
    mention ‘main’.

    // *grep-option-l*
    // -l, --files-with-matches
    //        Suppress normal output; instead  print  the name of each input
    //        file from which output would normally have been printed. The
    //        scanning will stop on the first  match.


    2. How do I search directories recursively?

    grep -r 'hello' /home/gigi

    searches for ‘hello’ in `all-files` under the /home/gigi directory. For more
    control over which files are searched, use find, grep, and xargs. For
    example, the following command searches only C files:

    // *grep-option-r*
    // -R, -r, --recursive
    //        Read  all  files  under  each  directory, recursively; this is
    //        equivalent to the -d recurse option.

    find /home/gigi -name '*.c' -print0 | xargs -0r grep -H 'hello'

    // *grep-option-h*
    // -H, --with-filename
    //        Print  the  file  name for each match.  This is the `default` when
    //        there is more than one file to search.

    This differs from the command:

    grep -H 'hello' *.c

    which merely looks for ‘hello’ in all files in the current directory whose
    names end in ‘.c’. The ‘find ...’ command line above is more similar to the
    command:

    grep -rH --include='*.c' 'hello' /home/gigi

    // grep -nr --include *.c __setup ./
    // grep -nr --include *.c [a-z]*_initcall ./

    // *grep-option-n*
    // -n, --line-number
    //        Prefix each line of output with the 1-based line number within its
    //        input file.  (-n is specified by POSIX.)


    3. What if a pattern has a leading ‘-’?

    grep -e '--cut here--' *

    searches for all lines matching ‘--cut here--’. Without -e, grep would
    attempt to parse ‘--cut here--’ as a list of options.

    // -e PATTERN, --regexp=PATTERN
    //        Use  PATTERN  as  the  pattern.   This  can  be  used to specify
    //        multiple search patterns, or to protect a pattern beginning with
    //        a hyphen (-).  (-e is specified by POSIX.)


    4. Suppose I want to search for a whole word, not a part of a word?

    grep -w 'hello' *

    searches only for instances of ‘hello’ that are entire words; it does not
    match ‘Othello’. For more control, use ‘\<’ and ‘\>’ to match the start and
    end of words. For example:

    grep 'hello\>' *

    searches only for words ending in ‘hello’, so it matches the word ‘Othello’.


    5. How do I output `context` around the matching lines?

    grep -C 2 'hello' *

    prints two lines of context around each matching line.

    // -C NUM, -NUM, --context=NUM
    //      Print NUM lines of output context.  Places a line containing a group
    //      separator (--) between contiguous groups of matches.  With the -o or
    //      --only-matching option, this  has no effect and a warning is given.

    6. How do I force grep to print the name of the file?

    Append /dev/null:

    grep 'eli' /etc/passwd /dev/null

    gets you:

    /etc/passwd:eli:x:2098:1000:Eli Smith:/home/eli:/bin/bash

    Alternatively, use -H, which is a `GNU extension`:

    grep -H 'eli' /etc/passwd


    7. Why do people use strange regular expressions on ps output?

    ps -ef | grep '[c]ron'

    If the pattern had been written without the square brackets, it would have
    matched not only the ps output line for cron, but also the ps output line
    for grep. 

    $ ps -ef | grep cron
    :56:root      2376     1  0 00:01 ?        00:00:00 /usr/sbin/cron
    :116:keitee   4510  3744  0 01:07 pts/0    00:00:00 grep --color=auto -anH cron

    $ ps -ef | grep [c]ron
    :56:root      2376     1  0 00:01 ?        00:00:00 /usr/sbin/cron

    note:
    Shows `default-option` HOW DOES IT WORK?


    8. Why does grep report “Binary file matches”?

    If grep listed all matching “lines” from a binary file, it would probably
    generate output that is not useful, and it might even muck up your display.
    So GNU grep suppresses output from files that appear to be binary files. 
    
    // *grep-option-a*
    To force GNU grep to output lines even from files that appear to be binary,
       use the -a or ‘--binary-files=text’ option. 

    To eliminate the “Binary file matches” messages, use the -I or
    ‘--binary-files=without-match’ option.


    9.Why doesn’t ‘grep -lv’ print non-matching file names?

    ‘grep -lv’ lists the names of all files containing one or more lines that do
    not match. 
    
    To list the names of all files that contain no matching lines, use the -L or
    --files-without-match option.

    // *grep-and*
    10. I can do “OR” with ‘|’, but what about “AND”?

    grep 'paul' /etc/motd | grep 'franc,ois'

    finds all lines that contain both ‘paul’ and ‘franc,ois’.


    11. Why does the `empty-pattern` match every input line?

    The grep command searches for lines that contain strings that match a
    pattern. Every line contains the `empty-string`, so an empty pattern causes
    grep to find a match on each line. It is not the only such pattern: ‘^’,
         ‘$’, ‘.*’, and many other patterns cause grep to match every line.

    To match empty lines, use the pattern ‘^$’. To match blank lines, use the
    pattern ‘^[[:blank:]]*$’. To match no lines at all, use the command ‘grep -f
    /dev/null’.


    12. How can I search in both standard input and in files?

    Use the special file name ‘-’:

    cat /etc/passwd | grep 'alain' - /etc/motd


    13. How to express palindromes in a regular expression?

    It can be done by using back-references; for example, a palindrome of `4`
    characters can be written with a BRE:

    grep -w -e '\(.\)\(.\).\2\1' file

    It matches the word “radar” or “civic.”

    Guglielmo Bondioni proposed a single RE that finds all palindromes up to 19
    characters long using 9 subexpressions and 9 back-references:

    grep -E -e '^(.?)(.?)(.?)(.?)(.?)(.?)(.?)(.?)(.?).?\9\8\7\6\5\4\3\2\1$' file

    Note this is done by using GNU ERE extensions; it might not be portable to
    other implementations of grep.


    14. Why is this back-reference failing?

    echo 'ba' | grep -E '(a)\1|b\1'

    This gives no output, because the first alternate ‘(a)\1’ does not match, as
    there is no ‘aa’ in the input, so the ‘\1’ in the second alternate has
    nothing to refer back to, meaning it will never match anything. (The second
        alternate in this example can only match if the first alternate has
        matched—making the second one superfluous.)


    15. How can I match across lines?

    Standard grep cannot do this, as it is fundamentally `line-based`.
    Therefore, merely using the [:space:] character class does not match
    newlines in the way you might expect.

    With the GNU grep option -z (--null-data), each input “line” is terminated
    by a null byte; see Other Options. Thus, you can match newlines in the
    input, but typically if there is a match the entire input is output, so this
    usage is often combined with output-suppressing options like -q, e.g.:

    printf 'foo\nbar\n' | grep -z -q 'foo[[:space:]]\+bar'

    If this does not suffice, you can transform the input before giving it to
    grep, or turn to awk, sed, perl, or many other utilities that are designed
    to operate across lines.


    // -z, --null-data
    //        `treat-the-input` as a set of lines, each  terminated  by  a  zero
    //        byte  (the  ASCII NUL character) instead of a newline.  Like the
    //        -Z or --null option, this option can be used with commands  like
    //        sort -z to process arbitrary file names.

    // -Z, --null
    //        Output  a  zero  byte  (the  ASCII NUL character) instead of the
    //        character that normally follows a file name.  For example,  grep
    //        -lZ  outputs  a  zero  byte  after each file name instead of the
    //        usual newline.  This option makes the output  unambiguous,  even
    //        in the presence of file names containing unusual characters like
    //        newlines.  This option can  be  used  with  commands  like  find
    //        -print0,  perl  -0,  sort  -z, and xargs -0 to process arbitrary
    //        file names, even those that contain newline characters.


    16. What do grep, fgrep, and egrep stand for?

    The name grep comes from the way line editing was done on Unix. For example,
    ed uses the following syntax to print a list of matching lines on the
      screen:

    global/regular expression/print
    g/re/p

    fgrep stands for Fixed grep; egrep stands for Extended grep.


    *grep-q-option*
    To use the status in a script.
    
    -q, --quiet, --silent
        Quiet; do not write anything to standard output. Exit immediately with zero
        status if any match is found, even if an error was detected. Also see the -s
        or --no-messages option. 
    
    echo xx | grep -q '.gz$' && echo ture
    echo xx.gz | grep -q '.gz$' && echo ture
    ture
    
    The script example:
    
    ##:zcat if this is a .gz, cat otherwise
    F=cat
    echo "$1" | grep -q '\.gz$' && F=zcat
    $F "$1"


={============================================================================
*kt_linux_rege_001* regex-grep tool-grep

$ grep "--prefix" out.txt
grep: unrecognized option '--prefix'
Usage: grep [OPTION]... PATTERN [FILE]...
Try `grep --help' for more information.

$ grep -e "--prefix" out.txt
4:--prefix=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/

<ex>
To find and replace string in multiple files.

$ egrep -lr --include *.c mhvSessionCancel .
./mh5eng/mh5a_application.c

$ egrep -lr --include *.c mhvSessionCancel . \
  | xargs sed -i 's/mhvSessionCancel/mmhv_session_cancel/'


       -R, --dereference-recursive
              Read all files under each directory, recursively.  Follow all
              symbolic links, unlike -r.


# ============================================================================
#{
={============================================================================
|kt_linux_perf_001| perf-bootchart

http://www.bootchart.org/

Bootchart 2.0

I couldn't get bootchart2 to work, (no netlink taskstat interface).  However,
bootchart-0.9 works with only small tweaks. (See attached tarball.)


={============================================================================
|kt_linux_perf_001| perf-timeline

Using Frederico's Timeline Tools

These tools can be used to dig into the reasons for the time taken.

The tools used are strace and frederico's timeline tools from
http://people.gnome.org/~federico/news-2006-03.html#timeline-tools

Download requirements:

sudo yum install python-cairo
git clone git://gitorious.org/performance-scripts/mainline.git performance-scripts


# ============================================================================
#{
={============================================================================
*kt_linux_rmdb_001* sql-rollback

<how-rollback-works>
https://www.sqlite.org/atomiccommit.html

An important feature of `transactional databases` like SQLite is `atomic commit`

SQLite has the important property that transactions appear to be atomic even
if the transaction is interrupted by an operating system crash or power
failure.

This article describes the techniques used by SQLite to create the illusion of
atomic commit.

The information in this article applies only when SQLite is operating in
"rollback mode", or in other words when SQLite is not using a write-ahead log.


2. Hardware Assumptions

SQLite assumes that the operating system will buffer writes and that a write
request will return before data has actually been stored in the mass storage
device. SQLite further assumes that write operations will be reordered by the
operating system. For this reason, SQLite does a "flush" or "fsync" operation
at key points. SQLite assumes that the flush or fsync will not return until
all pending write operations for the file that is being flushed have
completed. We are told that the flush and fsync primitives are broken on some
versions of Windows and Linux. This is unfortunate. It opens SQLite up to the
possibility of database corruption following a power loss in the middle of a
commit. However, there is nothing that SQLite can do to test for or remedy the
situation. SQLite assumes that the operating system that it is running on
works as advertised. If that is not quite the case, well then hopefully you
will not lose power too often.


3. Single File Commit

We begin with an overview of the steps SQLite takes in order to perform an
atomic commit of a transaction against a single database file.

note: see these steps in time order.
note: make a journal file

3.5. Creating A Rollback Journal File

Prior to making any changes to the database file, SQLite first creates a
separate `rollback journal file` and writes into the rollback journal the
`original content of the database pages` that are to be altered. The idea behind
the rollback journal is that it contains all information needed to restore the
database back to its original state.

The rollback journal contains a small `header`  

When a new file is created, most desktop operating systems will not actually
write anything to disk. The new file is created in the operating systems disk
cache only. The file is not created on mass storage until sometime later, when
the operating system has a spare moment. This creates the impression to users
that I/O is happening much faster than is possible when doing real disk I/O.
We illustrate this idea in the diagram to the right by showing that the new
rollback journal appears in the operating system disk cache only and not on
the disk itself.


3.6. Changing Database Pages In User Space

After the original page content has been saved in the rollback journal, the
pages can `be modified in user memory` Each database connection has its own
private copy of user space, so the changes that are made in user space are
only visible to the database connection that is making the changes. Other
database connections still see the information in operating system disk cache
buffers which have not yet been changed. And so even though one process is
busy modifying the database, other processes can continue to read their own
copies of the original database content.


3.7. Flushing The Rollback Journal File To Mass Storage

The next step is to flush the content of the rollback journal file to
nonvolatile storage. As we will see later, this is a critical step in insuring
that the database can survive an unexpected power loss. This step also takes a
lot of time, since writing to nonvolatile storage is normally a slow
operation.

This step is usually more complicated than simply flushing the rollback
journal to the disk. 

note: change (main) db

3.9. Writing Changes To The Database File

Once an exclusive lock is held, we know that no other processes are reading
from the database file and it is safe to write changes into the database file.
Usually those changes only go as far as the operating systems disk cache and
do not make it all the way to mass storage.

3.10. 0 Flushing Changes To Mass Storage

Another flush must occur to make sure that all the database changes are
written into nonvolatile storage. This is a critical step to ensure that the
database will survive a power loss without damage. However, because of the
inherent slowness of writing to disk or flash memory, this step together with
the rollback journal file flush in section 3.7 above takes up most of the time
required to complete a transaction commit in SQLite.


note: reset a journal file since pcat use "reset" optimisation.

3.11. 1 Deleting The Rollback Journal

After the database changes are all safely on the mass storage device, the
rollback journal file is deleted. This is the instant where the transaction
commits. 

If a power failure or system crash occurs prior to this point, then recovery
processes to be described later make it appear as if no changes were ever made
to the database file. 

note: means a power failure before resetting the header then will see this in
the next reboot or next transaction. so will start "rollback" recovery process.

If a power failure or system crash occurs after the rollback journal is
deleted, then it appears as if all changes have been written to disk. Thus,
  SQLite gives the appearance of having made no changes to the database file
  or having made the complete set of changes to the database file `depending
  on whether or not the rollback journal file exists`

Deleting a file is not really an atomic operation, but it appears to be from
the point of view of a user process. A process is always able to ask the
operating system "does this file exist?" and the process will get back a yes
or no answer. After a power failure that occurs during a transaction commit,
   SQLite will ask the operating system whether or not the rollback journal
   file exists. If the answer is "yes" then the transaction is incomplete and
   is rolled back. If the answer is "no" then it means the transaction did
   commit.

The existence of a transaction depends on whether or not the rollback journal
file exists and the deletion of a file appears to be an atomic operation from
the point of view of a user-space process. Therefore, a transaction appears to
be an atomic operation.

The act of deleting a file is expensive on many systems. As an optimization,
    SQLite can be configured to truncate the journal file to zero bytes in
    length or `overwrite the journal file header with zeros` In either case,
    the resulting journal file is no longer capable of rolling back and so the
    transaction still commits. Truncating a file to zero length, like deleting
    a file, is assumed to be an atomic operation from the point of view of a
    user process. 
    
Overwriting the header of the journal with zeros is not atomic, but if any
part of `the header is malformed the journal will not roll back` Hence, one
can say that the commit occurs as soon as the header is sufficiently changed
to make it invalid. Typically this happens as soon as the first byte of the
header is zeroed.


4. Rollback

An atomic commit is supposed to happen instantaneously. But the processing
described above clearly takes a finite amount of time. Suppose the power to
the computer were cut part way through the commit operation described above.
In order to maintain the illusion that the changes were instantaneous, we have
to "rollback" any partial changes and restore the database to the state it was
in prior to the beginning of the transaction.  


note: hot journal means

4.2. Hot Rollback Journals

The first time that any SQLite process attempts to access the database file,
    it obtains a shared lock as described in section 3.2 above. But then it
    `notices` that there is a rollback journal file present. SQLite then checks
    to see if the rollback journal is a "hot journal". A hot journal is a
    rollback journal that needs to be played back in order to restore the
    database to a sane state.


4.4. Rolling Back Incomplete Changes

Once a process obtains an exclusive lock, it is permitted to write to the
database file. It then proceeds to read the original content of pages out of
the rollback journal and write that content back to where it came from in the
database file.


4.5. Deleting The Hot Journal

After all information in the rollback journal has been played back into the
database file (and flushed to disk in case we encounter yet another power
    failure), the hot rollback journal can be deleted.

As in section 3.11, the journal file might be truncated to zero length or its
header might be overwritten with zeros as an optimization on systems where
deleting a file is expensive. Either way, the journal is `no longer hot` after
this step.


# ============================================================================
#{
={============================================================================
*kt_linux_core_002* linux-source

https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git
git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git

<linux-archive>
https://www.kernel.org/pub/linux/kernel/

<linux-source-navigation>
http://lxr.free-electrons.com/source/


={============================================================================
*kt_linux_core_001* linux-boot init-process

BOOTPARAM(7)               Linux Programmer’s Manual              BOOTPARAM(7)

NAME
       bootparam - Introduction to boot time parameters of the Linux kernel

DESCRIPTION
       The Linux kernel accepts certain ’command-line options’ or ’boot time
       parameters’ at the moment it is started.  In general this is used to
       supply the kernel with information about hardware parameters that the
       kernel would not be able to determine on its own, or to avoid/override
       the values that the kernel would otherwise detect.

       When the kernel is booted directly by the BIOS (say from a floppy to
           which you copied a kernel using ’cp zImage /dev/fd0’), you have no
       opportunity to specify any parameters.  So, in order to take advantage
       of this possibility you have to use `software that is able to pass`
       parameters, like LILO or loadlin.  For a few parameters one can also
       modify the kernel image itself, using rdev, see rdev(8) for further
       details.

       The LILO program (LInux LOader) written by Werner Almesberger is the
       most commonly used.  It has the ability to boot various kernels, and
       stores the configuration information in a plain text file.  (See
           lilo(8) and lilo.conf(5).)

       The  other  commonly used Linux loader is ’LoadLin’ which is a DOS
       program that has the capability to launch a Linux kernel from the DOS
       prompt (with boot-args) assuming that cer- tain resources are
       available.  This is good for people that want to launch Linux from DOS.

       It is also very useful if you have certain hardware which relies on the
       supplied DOS driver to put the hardware into a known state.  A common
       example is ’SoundBlaster  Compatible’ sound  cards  that  require  the
       DOS driver to twiddle a few mystical registers to put the card into a
       SB compatible mode.  Booting DOS with the supplied driver, and then
       loading Linux from the DOS prompt with loadlin avoids the reset of the
       card that happens if one rebooted instead.

       Any remaining arguments that were not picked up by the kernel and were
       not interpreted as environment variables are then passed onto process
       one, which is usually the init program. The most common argument that
       is passed to the init process is the word ’single’ which instructs init
       to boot the computer in single user mode, and not launch all the usual
       daemons.  Check the manual page for the version of init installed on
       your system to see what arguments it accepts.


   General Non-device Specific Boot Arguments
       ’init=...’
              This sets the initial command to be executed by the kernel. If
              this is not set, or cannot be found, the kernel will try
              /sbin/init, then /etc/init,  then  /bin/init,  then /bin/sh and
              panic if all of this fails.


init(8)                                                                init(8)

NAME
       init - Upstart process management daemon

https://github.com/mozilla-b2g/busybox/blob/master/init/init.c


<inittab>
http://www.manpages.info/linux/inittab.5.html

       Valid actions for the action field are:

       respawn
	      The process will	be  restarted  whenever	 it  terminates	 (e.g.
	      getty).

https://git.busybox.net/busybox/tree/examples/inittab

# <action>: Valid actions include: sysinit, respawn, askfirst, wait, once,
#                                  restart, ctrlaltdel, and shutdown.
#
#       Note: askfirst acts just like respawn, but before running the specified
#       process it displays the line "Please press Enter to activate this
#       console." and then waits for the user to press enter before starting
#       the specified process.
#
# Note: BusyBox init works just fine without an inittab. If no inittab is
# found, it has the following default behavior:
#         ::sysinit:/etc/init.d/rcS
#         ::askfirst:/bin/sh
#         ::ctrlaltdel:/sbin/reboot
#         ::shutdown:/sbin/swapoff -a
#         ::shutdown:/bin/umount -a -r
#         ::restart:/sbin/init
#         tty2::askfirst:/bin/sh
#         tty3::askfirst:/bin/sh
#         tty4::askfirst:/bin/sh


<mini-init>
#!/bin/sh
 
mount -t proc none /proc
mount -t sysfs none /sys
 
echo -e "\nBoot took $(cut -d' ' -f1 /proc/uptime) seconds\n"
 
exec /bin/sh


https://busybox.net/FAQ.html#init
Busybox init isn't working!

Init is the first program that runs, so it might be that no programs are
working on your new system because of a problem with your cross-compiler,
kernel, console settings, shared libraries, root filesystem... To rule all
  that out, first build a statically linked version of the following "hello
  world" program with your cross compiler toolchain:

// minit.c
#include <stdio.h>

int main(int argc, char *argv)
{
    int i;
    printf("Hello minit!\n");
    for(i = 0; i < 999999999; i++)
    {
        printf("....minit> %d\n", i);
        sleep(3);
    }
}

Now try to boot your device with an "init=" argument pointing to your hello
world program. Did you see the hello world message? Until you do, don't bother
messing with Busybox init.

Once you've got it working statically linked, try getting it to work
dynamically linked. Then read the FAQ entry How do I build a Busybox-based
system?, and the documentation for Busybox init (FIXME: dead link). 


<cmdline> <console>
sh-3.2# cat /proc/cmdline
mem=166M rw console=uart,mmio,0x10400b00,115200n8 ip_conntrack_tftp.ports=4085

[0946684811.106040]Adding console on ttyS0 at MMIO 0x10400b00 (options '115200n8')
[0946684811.114040]Freeing unused kernel memory: 4008k freed
init started: BusyBox v1.13.1 (2017-03-06 07:49:02 GMT)
starting pid 1, tty '/dev/ttyS0': '/bin/sh'
sh-3.2#


<boot>
* Appendix A: Linux startup sequence in 2.4.x

    kernel_entry() - arch/mips/kernel/head.S
       set stack;
       prepare argc/argp/envp;
       jal init_arch - arch/mips/kernel/setup.c
            cpu_probe() -
            prom_init(...) - arch/mips/ddb5074/prom.c
            loadmmu()
            start_kernel()  - init/main.c
                    setup_arch(&commaind_line); - arch/mips/kernel/setup.c
                            ddb_setup()         - arch/mips/ddb5074/setup.c
                    parse_options(command_line);
                    trap_init();
                    init_IRQ();                 - arch/mips/kernel/irq.c
                    sched_init();
                    softirq_init();
                    time_init();
                            if (board_time_init) board_time_init();
                            set xtime by calling rtc_get_time();
    			pick appropriate do_gettimeoffset()
    			board_timer_setup(&timer_irqaction);
                    console_init();
                    init_modules();
                    kmem_cache_init();
                    sti();			/* interrupt first open */
                    calibrate_delay();
                    mem_init();
                    kmem_cache_sizes_init();
    		pgtable_cache_init();
                    fork_init();
                    proc_caches_init();
                    vfs_caches_init();
                    buffer_init();
                    page_cache_init();
                    signals_init();
                    smp_init();
                    kernel_thread(init, ...)
                    cpu_idle();


    init() - init/main.c
      - lock_kernel();
        do_basic_setup();
          - [MTRR] mtrr_init();
            [SYSCTL] sysctl_init();
    	[S390] s390_init_machine_check();
            [ACPI] acpi_init();
            [PCI] pci_init();
            [SBUS] sbus_init();
            [PPC] ppc_init();
            [MCA] mca_init();
            [ARCH_ACORN] ecard_init();
            [ZORRO] zorro_init();
            [DIO] dio_init();
            [MAC] nubus_init();
            [ISAPNP] isapnp_init();
            [TC] tc_init();
            sock_init();
    	start_context_thread();
            do_initcalls();
            [IRDA] irda_proto_init();
            [IRDA] irda_device_init();
            [PCMCIA] init_pcmcia_ds();

        prepare_namespace();
        free_initmem();
        unlock_kernel();

        files = current->files;
        if(unshare_files())
                panic("unshare");
        put_files_struct(files);

        if (open("/dev/console", O_RDWR, 0) < 0)
                    printk("Warning: unable to open an initial console.\n");
        (void) dup(0);
        (void) dup(0);

        if (execute_command)
                    run_init_process(execute_command);
        run_init_process("/sbin/init");
        run_init_process("/etc/init");
        run_init_process("/bin/init");
        run_init_process("/bin/sh");

        panic("No init found.  Try passing init= option to kernel.");


={============================================================================
*kt_linux_core_001* linux-terminal

LPI 62 TERMINALS

On early UNIX systems, the terminal lines connected to the system were
represented by character devices with names of the form /dev/ttyn. 

On Linux, the /dev/ttyn devices are the virtual consoles on the system. It is
common to see the abbreviation tty (derived from teletype) as a shorthand for
terminal.

Especially during the early years of UNIX, terminal devices were not
standardized, which meant that different character sequences were required to
perform operations such as moving the cursor to the beginning of the line or
the top of the screen.

This lack of standardization meant that it was difficult to write portable
programs that used terminal features.  The vi editor was an early example of a
program with such requirements. The termcap and terminfo databases (described
    in [Strang et al., 1988]), which tabulate 1290 Chapter 62 how to perform
various screen-control operations for a wide variety of terminal types, and
the curses library ([Strang, 1986]) were developed in response to this lack of
standardization.

62.1 Overview

Both a conventional terminal and a terminal emulator have an associated
`terminal driver` that handles input and output on the device.

A terminal driver operates two queues (Figure 62-1): 
one for input characters transmitted from the terminal device to the reading
  process(es) and the other for output characters transmitted from processes
  to the terminal.

62.2 Retrieving and Modifying Terminal Attributes

The tcgetattr() and tcsetattr() functions retrieve and modify the attributes
of a terminal.

62.3 The stty Command

The stty command is the command-line analog of the tcgetattr() and tcsetattr()
functions, allowing us to view and change terminal attributes from the shell.
This is useful when trying to monitor, debug, or undo the effects of programs
that modify terminal attributes.

can view the current settings of all terminal attributes using the following
command (here carried out on a virtual console):

$ stty -a


={============================================================================
*kt_linux_core_002* linux-syscall

<system-call-is-expensive>
The *call-syscall(2)* manual page lists the Linux system calls.

LPI-3.1 System Calls

A system call is a `controlled entry point` into the kernel, allowing a
process to request that the kernel perform some action on the process's
behalf.

From a programming point of view, invoking a system call looks much like
calling a C function. However, behind the scenes, many steps occur during the
execution of a system call.

The steps in the order that they occur on a specific hardware implementation,
the x86-32.

  *call-syscall-wrapper-function*
  1. The application program makes a system call by invoking a wrapper
  function in the C library.

  2. The wrapper function must make all of the system call arguments available
  to the system call trap-handling routine (described shortly). These
  arguments are passed to the wrapper via the stack, but the kernel expects
  them in specific registers. The wrapper function copies the arguments to
  these registers.

  3. Since all system calls enter the kernel in the same way, the kernel needs
  some method of identifying the system call. To permit this, the wrapper
  function copies the `system call number` into a specific CPU register
  (%eax).

  * `The set of system calls is fixed.` Each system call is identified by a
    unique number. (This numbering scheme is not normally visible to programs,
    which identify system calls by name.)

  4. The wrapper function executes a trap machine instruction (int 0x80),
  which causes the processor to switch from user mode to kernel mode and execute
  code pointed to by location 0x80 (128 decimal) of the system's trap vector.

  * A system call changes the processor state from user mode to kernel mode,
    so that the CPU can access protected kernel memory.

  5. In response to the trap to location 0x80, the kernel invokes its
  `system_call()` routine (located in the assembler file arch/i386/entry.S) to
  handle the trap. This handler:

  a) Saves register values onto the kernel stack (Section 6.5).
  b) Checks the validity of the system call number.
  c) Invokes the appropriate system call service routine, which is found by
  using the system call number to index a table of all system call service
  routines (the kernel variable `sys_call_table`). If the system call service
  routine has any arguments, it first checks their validity; for example, it
  checks that addresses point to valid locations in user memory. Then the
  service routine performs the required task, which may involve modifying
  values at addresses specified in the given arguments and transferring data
  between user memory and kernel memory (e.g., in I/O operations). Finally,
  the service routine returns a result status to the system_call() routine.
  d) Restores register values from the kernel stack and places the system call
  return value on the stack.  
  e) Returns to the wrapper function, simultaneously returning the processor
  to user mode.

  6. If the return value of the system call service routine indicated an
  error, the wrapper function sets the `global variable errno` using this value.
  The wrapper function then returns to the caller, providing an integer return
  value indicating the success or failure of the system call.


*call-syscall-return*
On Linux, system call service routines follow a convention of returning a
nonnegative value to indicate success. In case of an error, the routine
returns a negative number, which is the negated value of one of the errno
constants. When a negative value is returned, the C library wrapper function
negates it (to make it positive), copies the result into errno, and returns -1
as the function result of the wrapper to indicate an error to the calling
program.

// note that the above is about system call service routine and system call
// retuns as:
//
// RETURN VALUE
//       On success, zero is returned.  On error, -1 is returned, and errno is set
//       appropriately.


The example of the execve() system call. On Linux/x86-32, execve() is system
call number 11 (`__NR_execve`). Thus, in the sys_call_table vector, entry 11
contains the address of sys_execve(), `the service routine` for this system
call. On Linux, system call service routines typically have names of the form
sys_xyz(), where xyz() is the system call in question.

The important point that, even for a simple system call, quite a bit of work
must be done, and thus system calls `have a small but appreciable overhead.`

As an example of the overhead of making a system call, consider the getppid()
system call, which simply returns the process ID of the parent of the calling
process. On one of the author's x86-32 systems running Linux 2.6.25, 10
million calls to getppid() required approximately 2.2 seconds to complete.
This amounts to around 0.3 microseconds per call. By comparison, on the same
system, 10 million calls to a C function that simply returns an integer
required 0.11 seconds, or around one-twentieth of the time required for calls
to getppid(). Of course, most system calls have significantly more overhead
than getppid().

note:
The Linux system call interface is a single instruction: int 0x80. All system
calls are done via this interrupt. To make a system call, eax should contain a
number that indicates which system call is being invoked, and other registers
are used to hold the arguments, if any. If the system call takes one argument,
it will be in ebx; a system call with two arguments will use ebx and ecx.

Likewise, edx, esi, and edi are used if a third, fourth, or fifth argument is
required, respectively. Upon return from a system call, eax will contain the
return value. If an error occurs, eax will contain a negative value, with the
absolute value indicating the error.


={============================================================================
*kt_linux_core_001* linux-syscall-list

What syscalls are valid depends on the OS. On GNU and Unix systems, you can find
the full list of valid syscall names on /usr/include/asm/unistd.h.

./include/asm-sparc/unistd.h
./include/asm-x86_64/unistd.h
./include/asm-ia64/unistd.h
./include/asm-frv/unistd.h
./include/asm-i386/unistd.h
./include/asm-sparc64/unistd.h
./include/asm-parisc/unistd.h
./include/asm-arm/unistd.h
./include/asm-v850/unistd.h
./include/asm-h8300/unistd.h
./include/asm-powerpc/unistd.h
./include/asm-alpha/unistd.h
./include/asm-sh64/unistd.h
./include/asm-m68k/unistd.h
./include/asm-sh/unistd.h
./include/asm-cris/unistd.h
./include/asm-cris/arch-v10/unistd.h
./include/asm-cris/arch-v32/unistd.h
./include/asm-m32r/unistd.h
./include/asm-m68knommu/unistd.h
./include/asm-s390/unistd.h
./include/asm-xtensa/unistd.h
./include/asm-um/unistd.h
./include/asm-arm26/unistd.h


./include/linux/unistd.h

#ifndef _LINUX_UNISTD_H_
#define _LINUX_UNISTD_H_


/*
 * Include machine specific syscallX macros
 */
#include <asm/unistd.h>

#endif /* _LINUX_UNISTD_H_ */


./include/asm-mips/unistd.h

#ifndef _ASM_UNISTD_H
#define _ASM_UNISTD_H

#include <asm/sgidefs.h>

#if _MIPS_SIM == _MIPS_SIM_ABI32

/*
 * Linux o32 style syscalls are in the range from 4000 to 4999.
 */
#define __NR_Linux                      4000
#define __NR_syscall                    (__NR_Linux +   0)
#define __NR_exit                       (__NR_Linux +   1)
#define __NR_fork                       (__NR_Linux +   2)
#define __NR_read                       (__NR_Linux +   3)
#define __NR_write                      (__NR_Linux +   4)
#define __NR_open                       (__NR_Linux +   5)
#define __NR_close                      (__NR_Linux +   6)
#define __NR_waitpid                    (__NR_Linux +   7)
#define __NR_creat                      (__NR_Linux +   8)
...


<call-syscall-symbolic-constant>
// x86_64//VM
$ more /usr/include/x86_64-linux-gnu/bits/syscall.h

/* Generated at libc build time from kernel syscall list.  */

#ifndef _SYSCALL_H
# error "Never use <bits/syscall.h> directly; include <sys/syscall.h> instead."
#endif


#if !defined __x86_64__
#define SYS__llseek __NR__llseek
#define SYS__newselect __NR__newselect
#define SYS__sysctl __NR__sysctl
#define SYS_access __NR_access
#define SYS_acct __NR_acct
#define SYS_add_key __NR_add_key
#define SYS_adjtimex __NR_adjtimex
#define SYS_afs_syscall __NR_afs_syscall
#define SYS_alarm __NR_alarm
#define SYS_bdflush __NR_bdflush
#define SYS_break __NR_break
#define SYS_brk __NR_brk
#define SYS_capget __NR_capget
#define SYS_capset __NR_capset
...


={============================================================================
*kt_linux_core_001* linux-syscall-syscall


[ RUN      ] Syscall.GetTid
[       OK ] Syscall.GetTid (0 ms)
[ RUN      ] Syscall.GetPidThroughGlibc
[       OK ] Syscall.GetPidThroughGlibc (182 ms)
[ RUN      ] Syscall.GetPidThroughSyscall
[       OK ] Syscall.GetPidThroughSyscall (965 ms)
[ RUN      ] Syscall.SanitizerSyscallInternal
[       OK ] Syscall.SanitizerSyscallInternal (6129 ms)
[ RUN      ] Syscall.SanitizerSyscallExternal
[       OK ] Syscall.SanitizerSyscallExternal (6001 ms)

Using system call via glibc is faster then using syscall(). Then why sanitizer
uses that approach?

/*
syscall

SYSCALL(2)                                           Linux Programmer's Manual

NAME
       syscall - indirect system call

SYNOPSIS
       #define _GNU_SOURCE          // See feature_test_macros(7)
       #include <unistd.h>
       #include <sys/syscall.h>     // For SYS_xxx definitions

       long syscall(long number, ...);

DESCRIPTION
       syscall()  is  a small library function that invokes the system call
       whose assembly language interface has the specified number with the
       specified arguments.  Employing syscall() is useful, for example, when
       invoking a system call that has no wrapper function in the C library.

       syscall() saves CPU registers before making the system call, restores the
       registers upon return from the system call, and stores any error code
       returned by the system call in errno(3) if an error occurs.

       Symbolic constants for system call numbers can be found in the header
       file <sys/syscall.h>.

RETURN VALUE
       The return value is defined by the system call being invoked.  In
       general, a 0 return value indicates success.  A -1 return value indicates
       an error, and an error code is stored in errno.

NOTES
       syscall() first appeared in 4BSD.

   Architecture-specific requirements
       Each architecture ABI has its own requirements on how system call
       arguments are passed to the kernel.  For system calls that have a glibc
       wrapper (e.g., most system calls), glibc handles the details of  copying
       arguments  to the  right  registers  in a manner suitable for the
       architecture.  However, when using syscall() to make a system call, the
       caller might need to handle architecture-dependent details; this
       requirement is most commonly encountered on certain 32-bit architectures.

       For example, on the ARM architecture Embedded ABI (EABI), a 64-bit value
       (e.g., long long) must be aligned to an even register pair.  Thus, using
       syscall() instead of the wrapper provided by glibc, the readahead()
       system call would be invoked as follows on the ARM architecture with the
       EABI:

           syscall(SYS_readahead, fd, 0,
                   (unsigned int) (offset >> 32),
                   (unsigned int) (offset & 0xFFFFFFFF),
                   count);

       Since the offset argument is 64 bits, and the first argument (fd) is
       passed in r0, the caller must manually split and align the 64-bit value
       so that it is passed in the r2/r3 register pair.  That means inserting a
       dummy value into r1 (the second argument of 0).

       Similar issues can occur on MIPS with the O32 ABI, on PowerPC with the
       32-bit ABI, and on Xtensa.

       The affected system calls are fadvise64_64(2), ftruncate64(2),
       posix_fadvise(2), pread64(2), pwrite64(2), readahead(2),
       sync_file_range(2), and truncate64(2).

*/

TEST(Syscall, GetTid)
{
  // cout << "tid : " << syscall(SYS_gettid) << endl;
  syscall(SYS_gettid);
}

TEST(Syscall, GetPidThroughGlibc)
{
  // int pid{};

  for (int i = 0; i < (1 << 24); ++i)
  {
    getpid();
  }
  // cout << "pid : " << pid << endl;
}

TEST(Syscall, GetPidThroughSyscall)
{
  // int pid{};

  for (int i = 0; i < (1 << 24); ++i)
  {
    syscall(SYS_getpid);
  }
  // cout << "pid : " << pid << endl;
}

namespace sanitizer_syscall {

// NAME
//        stat, fstat, lstat, fstatat - get file status
// 
// SYNOPSIS
//        #include <sys/types.h>
//        #include <sys/stat.h>
//        #include <unistd.h>
// 
//        int stat(const char *pathname, struct stat *buf);
//        int fstat(int fd, struct stat *buf);
//        int lstat(const char *pathname, struct stat *buf);
// 
// 
// DESCRIPTION
//        These  functions  return  information  about  a file, in the buffer
//        pointed to by stat.  No permissions are required on the file itself,
//        butin the case of stat(), fstatat(), and lstat()execute (search) 
//        permission is required on all of the directories in pathname that lead to
//        the file.
// 
//        stat() and fstatat() retrieve information about the file pointed to by
//        pathname; the differences for fstatat() are described below.
// 
//        lstat() is identical to stat(), except that if pathname is a symbolic
//        link, then it returns information about the link itself, not the file
//        that it refers to.
// 
//        fstat() is identical to stat(), except that the file about which
//        information is to be retrieved is specified by the file descriptor fd.
// 
//        All of these system calls return a stat structure, which contains the
//        following fields:
// 
//            struct stat {
//                dev_t     st_dev;         /* ID of device containing file */
//                ino_t     st_ino;         /* inode number */
//                mode_t    st_mode;        /* protection */
//                nlink_t   st_nlink;       /* number of hard links */
//                uid_t     st_uid;         /* user ID of owner */
//                gid_t     st_gid;         /* group ID of owner */
//                dev_t     st_rdev;        /* device ID (if special file) */
//                off_t     st_size;        /* total size, in bytes */
//                blksize_t st_blksize;     /* blocksize for filesystem I/O */
//                blkcnt_t  st_blocks;      /* number of 512B blocks allocated */
//
//               /* Since Linux 2.6, the kernel supports nanosecond
//                  precision for the following timestamp fields.
//                  For the details before Linux 2.6, see NOTES. */
//
//               struct timespec st_atim;  /* time of last access */
//               struct timespec st_mtim;  /* time of last modification */
//               struct timespec st_ctim;  /* time of last status change */
//
//           #define st_atime st_atim.tv_sec      /* Backward compatibility */
//           #define st_mtime st_mtim.tv_sec
//           #define st_ctime st_ctim.tv_sec
//           };
//
//       Because tests of the above form are common, additional macros are
//       defined by POSIX to allow the test of the file type in st_mode to be
//       written more concisely:
//
//           S_ISREG(m)  is it a regular file?
//
// <skipped>
//
// RETURN VALUE
//       On success, zero is returned.  On error, -1 is returned, and errno is set
//       appropriately.
//


typedef unsigned long uptr;

// #if !((SANITIZER_FREEBSD || SANITIZER_MAC) && defined(__x86_64__))
#define SYSCALL(name) __NR_##name
#define internal_syscall syscall

uptr internal_stat(const char *path, void *buf) {

  // #if SANITIZER_LINUX && (defined(__x86_64__) || SANITIZER_WORDSIZE == 64)
  // # define SANITIZER_LINUX_USES_64BIT_SYSCALLS 1

  return internal_syscall(SYSCALL(stat), (uptr)path, (uptr)buf);
}

bool FileExistsInternal(const char *filename) {
  struct stat st;

  if (internal_stat(filename, &st))
    return false;

  // Sanity check: filename is a regular file.
  return S_ISREG(st.st_mode);
}

bool FileExistsExternal(const char *filename) {
  struct stat st;

  if (stat(filename, &st))
    return false;

  // Sanity check: filename is a regular file.
  return S_ISREG(st.st_mode);
}

} // namespace

TEST(Syscall, SanitizerSyscallInternal)
{
  using namespace sanitizer_syscall;

  for (int i = 0; i < (1 << 24); ++i)
  {
    EXPECT_TRUE(FileExistsInternal("syscall.cpp"));
  }
}

TEST(Syscall, SanitizerSyscallExternal)
{
  using namespace sanitizer_syscall;

  for (int i = 0; i < (1 << 24); ++i)
  {
    EXPECT_TRUE(FileExistsExternal("syscall.cpp"));
  }
}

={============================================================================
*kt_linux_core_001* linux-syscall-reboot

http://man7.org/linux/man-pages/man2/reboot.2.html

SYNOPSIS         top

       /* Since kernel version 2.1.30 there are symbolic names
       LINUX_REBOOT_*
          for the constants and a fourth argument to the call: */

       #include <unistd.h>
       #include <linux/reboot.h>

       int reboot(int magic, int magic2, int cmd, void *arg);

       /* Under glibc and most alternative libc's (including uclibc,
       dietlibc,
          musl and a few others), some of the constants involved have gotten
          symbolic names RB_*, and the library call is a 1-argument
          wrapper around the system call: */

       #include <unistd.h>
       #include <sys/reboot.h>

       int reboot(int cmd);

       This system call will fail (with EINVAL) unless magic equals
       LINUX_REBOOT_MAGIC1 (that is, 0xfee1dead) and magic2 equals
       LINUX_REBOOT_MAGIC2 (that is, 672274793).

       LINUX_REBOOT_CMD_RESTART
              (RB_AUTOBOOT, 0x1234567).  The message "Restarting system." is
              printed, and a default restart is performed immediately.  If
              not preceded by a sync(2), data will be lost.

The precise effect of the above actions depends on the architecture.


include/sys/reboot.h
28:#define RB_AUTOBOOT  0x01234567

// ./libc/sysdeps/linux/common/reboot.c
/*
 * _reboot() for uClibc
 *
 * Copyright (C) 2000-2004 by Erik Andersen <andersen@codepoet.org>
 *
 * GNU Library General Public License (LGPL) version 2 or later.
 */

#include "syscalls.h"
#define __NR__reboot __NR_reboot
static inline _syscall3(int, _reboot, int, magic, int, magic2, int, flag);
int reboot(int flag)
{
	return (_reboot((int) 0xfee1dead, 672274793, flag));
}

sys_reboot        115 arch/xtensa/kernel/syscalls.h SYSCALL(sys_reboot, 3)
sys_reboot        390 include/asm-x86_64/unistd.h __SYSCALL(__NR_reboot, sys_reboot)
sys_reboot        161 include/linux/syscalls.h asmlinkage long sys_reboot(int magic1, int magic2, unsigned int cmd,

kernel/sys.c
/*
 * Reboot system call: for obvious reasons only root may call it,
 * and even root needs to set up some magic numbers in the registers
 * so that some mistake won't make this reboot the whole machine.
 * You can also set the meaning of the ctrl-alt-del-key here.
 *
 * reboot doesn't sync: do that yourself before calling this.
 */
asmlinkage long sys_reboot(int magic1, int magic2, unsigned int cmd, void __user * arg)
{
	char buffer[256];

	/* We only trust the superuser with rebooting the system. */
	if (!capable(CAP_SYS_BOOT))
		return -EPERM;

	/* For safety, we require "magic" arguments. */
	if (magic1 != LINUX_REBOOT_MAGIC1 ||
	    (magic2 != LINUX_REBOOT_MAGIC2 &&
	                magic2 != LINUX_REBOOT_MAGIC2A &&
			magic2 != LINUX_REBOOT_MAGIC2B &&
	                magic2 != LINUX_REBOOT_MAGIC2C))
		return -EINVAL;

	/* Instead of trying to make the power_off code look like
	 * halt when pm_power_off is not set do it the easy way.
	 */
	if ((cmd == LINUX_REBOOT_CMD_POWER_OFF) && !pm_power_off)
		cmd = LINUX_REBOOT_CMD_HALT;

	lock_kernel();
	switch (cmd) {
	case LINUX_REBOOT_CMD_RESTART:
		kernel_restart(NULL);
		break;

	unlock_kernel();
	return 0;
}

void kernel_restart(char *cmd)
{
	machine_restart(cmd);
}

this machine_restart(cmd) is `arch` specific verison for each arch.


={============================================================================
*kt_linux_core_001* linux-syscall-pause

pause() causes the calling process (or thread) to sleep until a signal is delivered that either
terminates the process or causes the invocation of a signal-catching function.


={============================================================================
*kt_linux_core_001* linux-syscall-alarm

Since the final for loop of the program loops forever, this program uses alarm() to establish a
timer to deliver SIGALRM. The arrival of an unhandled SIGALRM signal guarantees process termination,
if the process is not other- wise terminated.

unsigned int alarm(unsigned int seconds);

alarm() arranges for a SIGALRM signal to be delivered to the calling process in seconds seconds.

int main(int argc, char *argv[])
{
  /* An unhandled SIGALRM ensures this process will die if nothing else terminates it */
  alarm(60);

  for(;;) {
    /* Wait for signals */
    pause();
    printf("%ld: caught SIGHUP\n", (long) getpid());
  }
}


={============================================================================
*kt_linux_core_003* linux-syscall-getenv setenv

<getenv>
#include <stdlib.h>

char *getenv(const char *name);

RETURN VALUE
The getenv() function returns a pointer to the value in the environment, or NULL
if there is no match.

note:
"no match" means when there is no env variable set so when "SAMPLE_ENV=" and it
has null but is set, getenv returns "\0" null string but not null pointer. So
need to use strcmp to check if env variable is set or unset since check on NULL
means that env not defined but not set. 


<setenv>
int setenv(const char *name, const char *value, int overwrite);

The setenv() function adds the variable name to the environment with the value
  value, if name does not already exist. If name does exist in the environment,
  then its value is changed to value 'if' overwrite is 'nonzero'; if overwrite
    is zero, then the value of name is not changed. This function makes copies
    of the strings pointed to by name and value (by contrast with putenv(3)).


={============================================================================
*kt_linux_core_004* linux-error-errno linux-errno

LPI-3.4 Handling Errors from System Calls and Library Functions

{system-call-on-error}
A few system calls never fail. For example, getpid() always successfully
returns the ID of a process, and _exit() always terminates a process. It is
not necessary to check the return values from such system calls.

The section headed ERRORS in each manual page provides a list of possible
errno values that can be returned by each system call.

Usually, an error is indicated by a return of -1. When a system call fails, it
sets `the global integer variable errno` to a positive value that identifies
the specific error.

Including the <errno.h> header file provides a declaration of errno, as well
as a set of constants for the various error numbers. All of these symbolic
names commence with E.

% man errno

EINVAL          Invalid argument (POSIX.1)

Thus, a system call can be checked with code such as the following:

cnt = read(fd, buf, numbytes);
if (cnt == -1) {
  if (errno == EINTR)
    fprintf(stderr, "read was interrupted by a signal\n");
  else {
    /* Some other error occurred */
  }
}

First check if the function return value indicates an error, and `only then`
examine errno to determine the cause of the error.

because `successful` system calls and library functions never reset errno to
0, so this variable may have a nonzero value as a consequence of an error from
a previous call. Furthermore, SUSv3 permits a successful function call to set
errno to a nonzero value (although few functions do this).


<exceptions>
A few system calls (e.g., getpriority()) can legitimately return -1 on
success. To determine whether an error occurs in such calls, we set errno to 0
before the call, and then check it afterward. If the call returns -1 and errno
is nonzero, an error occurred.


<linux-errno-call-perror>
The perror() function prints the string pointed to by its msg argument,
`followed by` a message corresponding to the current value of errno.

The strerror() function returns the error string corresponding to the error
number given in its errnum argument.

#include <stdio.h>
void perror(const char *msg);

#include <string.h>
char *strerror(int errnum);


{library-calls-on-error}
The various library functions return different data types and different values
to indicate failure. For our purposes, library functions can be divided into
the following `categories`:

1. Some library functions return error information in exactly the same way as
system calls: a -1 return value, with errno indicating the specific error. 

  An example of such a function is remove(), which removes a file (using the
      unlink() system call) or a directory (using the rmdir() system call).
  Errors from these functions can be diagnosed in the same way as errors from
  system calls.

2. Some library functions return a value 'other' than -1 on error, but
nevertheless set errno to indicate the specific error condition. 

  For example, fopen() returns a NULL pointer on error, and the setting of
  errno depends on which underlying system call failed. The perror() and
  strerror() functions can be used to diagnose these errors.

3. Other library functions don't use errno at all. The method for determining
the existence and cause of errors depends on the particular function and is
documented in the function's manual page. For these functions, it is a mistake
to use errno, perror(), or strerror() to diagnose errors.


{linux-errno-values}

#include <errno.h>


kyoupark@kit-debian64:~$ cat /usr/include/linux/errno.h
#include <asm/errno.h>

/usr/include/asm-generic/errno.h
#ifndef _ASM_GENERIC_ERRNO_H
#define _ASM_GENERIC_ERRNO_H

#include <asm-generic/errno-base.h>

#define EDEADLK         35      /* Resource deadlock would occur */
#define ENAMETOOLONG    36      /* File name too long */
...

// <linux>/include/asm-generic/errno-base.h

#ifndef _ASM_GENERIC_ERRNO_BASE_H
#define _ASM_GENERIC_ERRNO_BASE_H

#define	EPERM		 1	/* Operation not permitted */
#define	ENOENT		 2	/* No such file or directory */
#define	ESRCH		 3	/* No such process */
#define	EINTR		 4	/* Interrupted system call */
#define	EIO		 5	/* I/O error */
#define	ENXIO		 6	/* No such device or address */
#define	E2BIG		 7	/* Argument list too long */
#define	ENOEXEC		 8	/* Exec format error */
#define	EBADF		 9	/* Bad file number */
#define	ECHILD		10	/* No child processes */
#define	EAGAIN		11	/* Try again */
#define	ENOMEM		12	/* Out of memory */
#define	EACCES		13	/* Permission denied */
#define	EFAULT		14	/* Bad address */
#define	ENOTBLK		15	/* Block device required */
#define	EBUSY		16	/* Device or resource busy */
#define	EEXIST		17	/* File exists */
#define	EXDEV		18	/* Cross-device link */
#define	ENODEV		19	/* No such device */
#define	ENOTDIR		20	/* Not a directory */
#define	EISDIR		21	/* Is a directory */
#define	EINVAL		22	/* Invalid argument */
#define	ENFILE		23	/* File table overflow */
#define	EMFILE		24	/* Too many open files */
#define	ENOTTY		25	/* Not a typewriter */
#define	ETXTBSY		26	/* Text file busy */
#define	EFBIG		27	/* File too large */
#define	ENOSPC		28	/* No space left on device */
#define	ESPIPE		29	/* Illegal seek */
#define	EROFS		30	/* Read-only file system */
#define	EMLINK		31	/* Too many links */
#define	EPIPE		32	/* Broken pipe */
#define	EDOM		33	/* Math argument out of domain of func */
#define	ERANGE		34	/* Math result not representable */

#endif

// <linux>/include/asm-mips/errno.h
// <toolchain>/mips-linux-uclibc/include/asm/errno.h

/*
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1995, 1999, 2001, 2002 by Ralf Baechle
 */
#ifndef _ASM_ERRNO_H
#define _ASM_ERRNO_H

/*
 * These error numbers are intended to be MIPS ABI compatible
 */

#include <asm-generic/errno-base.h>

#define	ENOMSG		35	/* No message of desired type */
#define	EIDRM		36	/* Identifier removed */
#define	ECHRNG		37	/* Channel number out of range */
#define	EL2NSYNC	38	/* Level 2 not synchronized */
#define	EL3HLT		39	/* Level 3 halted */
#define	EL3RST		40	/* Level 3 reset */
#define	ELNRNG		41	/* Link number out of range */
#define	EUNATCH		42	/* Protocol driver not attached */
#define	ENOCSI		43	/* No CSI structure available */
#define	EL2HLT		44	/* Level 2 halted */
#define	EDEADLK		45	/* Resource deadlock would occur */
#define	ENOLCK		46	/* No record locks available */
#define	EBADE		50	/* Invalid exchange */
#define	EBADR		51	/* Invalid request descriptor */
#define	EXFULL		52	/* Exchange full */
#define	ENOANO		53	/* No anode */
#define	EBADRQC		54	/* Invalid request code */
#define	EBADSLT		55	/* Invalid slot */
#define	EDEADLOCK	56	/* File locking deadlock error */
#define	EBFONT		59	/* Bad font file format */
#define	ENOSTR		60	/* Device not a stream */
#define	ENODATA		61	/* No data available */
#define	ETIME		62	/* Timer expired */
#define	ENOSR		63	/* Out of streams resources */
#define	ENONET		64	/* Machine is not on the network */
#define	ENOPKG		65	/* Package not installed */
#define	EREMOTE		66	/* Object is remote */
#define	ENOLINK		67	/* Link has been severed */
#define	EADV		68	/* Advertise error */
#define	ESRMNT		69	/* Srmount error */
#define	ECOMM		70	/* Communication error on send */
#define	EPROTO		71	/* Protocol error */
#define	EDOTDOT		73	/* RFS specific error */
#define	EMULTIHOP	74	/* Multihop attempted */
#define	EBADMSG		77	/* Not a data message */
#define	ENAMETOOLONG	78	/* File name too long */
#define	EOVERFLOW	79	/* Value too large for defined data type */
#define	ENOTUNIQ	80	/* Name not unique on network */
#define	EBADFD		81	/* File descriptor in bad state */
#define	EREMCHG		82	/* Remote address changed */
#define	ELIBACC		83	/* Can not access a needed shared library */
#define	ELIBBAD		84	/* Accessing a corrupted shared library */
#define	ELIBSCN		85	/* .lib section in a.out corrupted */
#define	ELIBMAX		86	/* Attempting to link in too many shared libraries */
#define	ELIBEXEC	87	/* Cannot exec a shared library directly */
#define	EILSEQ		88	/* Illegal byte sequence */
#define	ENOSYS		89	/* Function not implemented */
#define	ELOOP		90	/* Too many symbolic links encountered */
#define	ERESTART	91	/* Interrupted system call should be restarted */
#define	ESTRPIPE	92	/* Streams pipe error */
#define	ENOTEMPTY	93	/* Directory not empty */
#define	EUSERS		94	/* Too many users */
#define	ENOTSOCK	95	/* Socket operation on non-socket */
#define	EDESTADDRREQ	96	/* Destination address required */
#define	EMSGSIZE	97	/* Message too long */
#define	EPROTOTYPE	98	/* Protocol wrong type for socket */
#define	ENOPROTOOPT	99	/* Protocol not available */
#define	EPROTONOSUPPORT	120	/* Protocol not supported */
#define	ESOCKTNOSUPPORT	121	/* Socket type not supported */
#define	EOPNOTSUPP	122	/* Operation not supported on transport endpoint */
#define	EPFNOSUPPORT	123	/* Protocol family not supported */
#define	EAFNOSUPPORT	124	/* Address family not supported by protocol */
#define	EADDRINUSE	125	/* Address already in use */
#define	EADDRNOTAVAIL	126	/* Cannot assign requested address */
#define	ENETDOWN	127	/* Network is down */
#define	ENETUNREACH	128	/* Network is unreachable */
#define	ENETRESET	129	/* Network dropped connection because of reset */
#define	ECONNABORTED	130	/* Software caused connection abort */
#define	ECONNRESET	131	/* Connection reset by peer */
#define	ENOBUFS		132	/* No buffer space available */
#define	EISCONN		133	/* Transport endpoint is already connected */
#define	ENOTCONN	134	/* Transport endpoint is not connected */
#define	EUCLEAN		135	/* Structure needs cleaning */
#define	ENOTNAM		137	/* Not a XENIX named type file */
#define	ENAVAIL		138	/* No XENIX semaphores available */
#define	EISNAM		139	/* Is a named type file */
#define	EREMOTEIO	140	/* Remote I/O error */
#define EINIT		141	/* Reserved */
#define EREMDEV		142	/* Error 142 */
#define	ESHUTDOWN	143	/* Cannot send after transport endpoint shutdown */
#define	ETOOMANYREFS	144	/* Too many references: cannot splice */
#define	ETIMEDOUT	145	/* Connection timed out */
#define	ECONNREFUSED	146	/* Connection refused */
#define	EHOSTDOWN	147	/* Host is down */
#define	EHOSTUNREACH	148	/* No route to host */
#define	EWOULDBLOCK	EAGAIN	/* Operation would block */
#define	EALREADY	149	/* Operation already in progress */
#define	EINPROGRESS	150	/* Operation now in progress */
#define	ESTALE		151	/* Stale NFS file handle */
#define ECANCELED	158	/* AIO operation canceled */

/*
 * These error are Linux extensions.
 */
#define ENOMEDIUM	159	/* No medium found */
#define EMEDIUMTYPE	160	/* Wrong medium type */
#define	ENOKEY		161	/* Required key not available */
#define	EKEYEXPIRED	162	/* Key has expired */
#define	EKEYREVOKED	163	/* Key has been revoked */
#define	EKEYREJECTED	164	/* Key was rejected by service */

/* for robust mutexes */
#define	EOWNERDEAD	165	/* Owner died */
#define	ENOTRECOVERABLE	166	/* State not recoverable */

#define EDQUOT		1133	/* Quota exceeded */

#endif /* _ASM_ERRNO_H */


={============================================================================
*kt_linux_core_005* linux-error-handling-code thread-errno

LPI. 3.5.2 Common Functions and Header Files

void errMsg(const char *format, ...);

  Prints a message on standard error. Its argument list is the same as for
    printf(), except that a terminating newline character is automatically
    appended to the output string. The errMsg() function prints the error text
    corresponding to the current value of errno-this consists of the error
    name, such as EPERM, plus the error description as returned by
    strerror()-followed by the formatted output specified in the argument
    list.

void errExit(const char *format, ...);

  Operates like errMsg(), but also terminates the program, either by calling
    exit() or, if the environment variable EF_DUMPCORE is defined with a
    nonempty string value, `by calling abort() to produce a core dump file`

void err_exit(const char *format, ...);

This is similar to errExit(), but differs in two respects:

  It doesn't flush standard output before printing the error message.

  It terminates the process by calling _exit() instead of exit(). This causes
  the process to terminate without flushing stdio buffers or invoking exit
  handlers.

The details of these differences in the operation of err_exit() will become
clearer where we describe the differences between _exit() and exit(), and
consider the treatment of stdio buffers and exit handlers in a child created
by fork(). 

For now, we simply note that err_exit() is especially useful if we write a
library function that creates a child process that needs to terminate because
of an error. This termination should occur without flushing the child's copy
of the parent's (i.e., the calling process's) stdio buffers and without
invoking exit handlers established by the parent.


<thread-errno>
The errno is a global integer variable. However, this doesn't suffice for
threaded programs. If a thread made a call that returned an error in a
`global-errno` variable, then this would confuse other threads that might also
be making calls and checking errno. In other words, race conditions would
result.

<how>
Each thread has its own errno value. On Linux, a `thread-specific-errno` is
achieved in a similar manner to most other UNIX: errno is defined as a macro
that `expands into a function call` returning a modifiable lvalue that is
distinct for each thread. Since the lvalue is modifiable, it is still possible
to write assignment statements of the form `errno = value` in threaded
programs.

To summarize, the errno mechanism has been adapted for threads in a manner
that leaves error reporting unchanged from the traditional UNIX API.

note: 
After all, errno macro returns a per-thread errno. A program is required to
declare errno by including <errno.h>, which enables the implementation of a
per-thread errno.

void errExitEN(int errnum, const char *format, ...);

is the same as errExit(), except that instead of printing the error text
  corresponding to the current value of errno, it prints the text
  corresponding to the error number (thus, the EN suffix) given in the
  argument errnum.

Mainly, we use errExitEN() in programs that employ the POSIX threads API.

Since:

The traditional method of returning status from system calls and some library
functions is to `return 0 on success and -1 on error`, with errno being set to
indicate the error. The functions in the `pthreads API do things differently`
All pthreads functions return 0 on success or a positive value on failure. The
failure value is one of the same values that can be placed in errno by
traditional UNIX system calls. 

We could diagnose errors from the POSIX threads functions using code such as
the following:

errno = pthread_create(&thread, NULL, func, &arg);
if (errno != 0)
   errExit("pthread_create");

This approach is `inefficient` because errno is defined in threaded programs
  as a macro. Under the POSIX threads API, errno is redefined to be a function
  that returns a `pointer to a thread-specific storage area` 

errExitEN() function allows us to write a more efficient equivalent of the
above code because each reference to errno in a threaded program carries the
overhead of a function call, our example programs don’t directly assign the
return value of a Pthreads function to errno.

int s;
s = pthread_create(&thread, NULL, func, &arg);
if (s != 0)
   errExitEN(s, "pthread_create");


<code>
<lib/error_functions.h>

/*************************************************************************\
*                  Copyright (C) Michael Kerrisk, 2014.                   *
*                                                                         *
* This program is free software. You may use, modify, and redistribute it *
* under the terms of the GNU Lesser General Public License as published   *
* by the Free Software Foundation, either version 3 or (at your option)   *
* any later version. This program is distributed without any warranty.    *
* See the files COPYING.lgpl-v3 and COPYING.gpl-v3 for details.           *
\*************************************************************************/

/* Listing 3-2 */

/* error_functions.h

   Header file for error_functions.c.
*/
#ifndef ERROR_FUNCTIONS_H
#define ERROR_FUNCTIONS_H

/* Error diagnostic routines */

void errMsg(const char *format, ...);

#ifdef __GNUC__

    /* This macro stops 'gcc -Wall' complaining that "control reaches
       end of non-void function" if we use the following functions to
       terminate main() or some other non-void function. */

#define NORETURN __attribute__ ((__noreturn__))
#else
#define NORETURN
#endif

void errExit(const char *format, ...) NORETURN ;

void err_exit(const char *format, ...) NORETURN ;

void errExitEN(int errnum, const char *format, ...) NORETURN ;

void fatal(const char *format, ...) NORETURN ;

void usageErr(const char *format, ...) NORETURN ;

void cmdLineErr(const char *format, ...) NORETURN ;

#endif

<lib/error_functions.c>

/*************************************************************************\
*                  Copyright (C) Michael Kerrisk, 2014.                   *
*                                                                         *
* This program is free software. You may use, modify, and redistribute it *
* under the terms of the GNU Lesser General Public License as published   *
* by the Free Software Foundation, either version 3 or (at your option)   *
* any later version. This program is distributed without any warranty.    *
* See the files COPYING.lgpl-v3 and COPYING.gpl-v3 for details.           *
\*************************************************************************/

/* Listing 3-3 */

/* error_functions.c

   Some standard error handling routines used by various programs.
*/
#include <stdarg.h>
#include "error_functions.h"
#include "tlpi_hdr.h"
#include "ename.c.inc"          /* Defines ename and MAX_ENAME */

#ifdef __GNUC__                 /* Prevent 'gcc -Wall' complaining  */
__attribute__ ((__noreturn__))  /* if we call this function as last */
#endif                          /* statement in a non-void function */
static void
terminate(Boolean useExit3)
{
    char *s;

    /* Dump core if EF_DUMPCORE environment variable is defined and
       is a nonempty string; otherwise call exit(3) or _exit(2),
       depending on the value of 'useExit3'. */

    s = getenv("EF_DUMPCORE");

    if (s != NULL && *s != '\0')
        abort();
    else if (useExit3)
        exit(EXIT_FAILURE);
    else
        _exit(EXIT_FAILURE);
}

/* Diagnose 'errno' error by:

      * outputting a string containing the error name (if available
        in 'ename' array) corresponding to the value in 'err', along
        with the corresponding error message from strerror(), and

      * outputting the caller-supplied error message specified in
        'format' and 'ap'. */

static void
outputError(Boolean useErr, int err, Boolean flushStdout,
        const char *format, va_list ap)
{
#define BUF_SIZE 500
    char buf[BUF_SIZE], userMsg[BUF_SIZE], errText[BUF_SIZE];

    vsnprintf(userMsg, BUF_SIZE, format, ap);

    if (useErr)
        snprintf(errText, BUF_SIZE, " [%s %s]",
                (err > 0 && err <= MAX_ENAME) ?
                ename[err] : "?UNKNOWN?", strerror(err));
    else
        snprintf(errText, BUF_SIZE, ":");

    snprintf(buf, BUF_SIZE, "ERROR%s %s\n", errText, userMsg);

    if (flushStdout)
        fflush(stdout);       /* Flush any pending stdout */
    fputs(buf, stderr);
    fflush(stderr);           /* In case stderr is not line-buffered */
}

/* Display error message including 'errno' diagnostic, and
   return to caller */

void
errMsg(const char *format, ...)
{
    va_list argList;
    int savedErrno;

    savedErrno = errno;       /* In case we change it here */

    va_start(argList, format);
    outputError(TRUE, errno, TRUE, format, argList);
    va_end(argList);

    errno = savedErrno;
}

/* Display error message including 'errno' diagnostic, and
   terminate the process */

void
errExit(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errno, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Display error message including 'errno' diagnostic, and
   terminate the process by calling _exit().

   The relationship between this function and errExit() is analogous
   to that between _exit(2) and exit(3): unlike errExit(), this
   function does not flush stdout and calls _exit(2) to terminate the
   process (rather than exit(3), which would cause exit handlers to be
   invoked).

   These differences make this function especially useful in a library
   function that creates a child process that must then terminate
   because of an error: the child must terminate without flushing
   stdio buffers that were partially filled by the caller and without
   invoking exit handlers that were established by the caller. */

void
err_exit(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errno, FALSE, format, argList);
    va_end(argList);

    terminate(FALSE);
}

/* The following function does the same as errExit(), but expects
   the error number in 'errnum' */

void
errExitEN(int errnum, const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errnum, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Print an error message (without an 'errno' diagnostic) */

void
fatal(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(FALSE, 0, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Print a command usage error message and terminate the process */

void
usageErr(const char *format, ...)
{
    va_list argList;

    fflush(stdout);           /* Flush any pending stdout */

    fprintf(stderr, "Usage: ");
    va_start(argList, format);
    vfprintf(stderr, format, argList);
    va_end(argList);

    fflush(stderr);           /* In case stderr is not line-buffered */
    exit(EXIT_FAILURE);
}

/* Diagnose an error in command-line arguments and
   terminate the process */

void
cmdLineErr(const char *format, ...)
{
    va_list argList;

    fflush(stdout);           /* Flush any pending stdout */

    fprintf(stderr, "Command-line usage error: ");
    va_start(argList, format);
    vfprintf(stderr, format, argList);
    va_end(argList);

    fflush(stderr);           /* In case stderr is not line-buffered */
    exit(EXIT_FAILURE);
}


={============================================================================
*kt_linux_core_006* linux-portability

LPI-3.6 Portability Issues

<feature-test-macros>
Sometimes, when writing a portable application, we may want the various header
files to expose only the definitions (constants, function prototypes, and so
    on) that follow a particular standard.

To do this, we define one or more of the feature test macros listed below when
compiling a program. One way that we can do this is by defining the macro in
the program source code before including any header files:

#define _BSD_SOURCE 1

Alternatively, we can use the D option to the C compiler:

$ cc -D_BSD_SOURCE prog.c


The following feature test macros listed are glibc-specific:

_BSD_SOURCE

If defined (with any value), expose BSD definitions. Defining this macro also
defines _POSIX_C_SOURCE with the value 199506. Explicitly setting just this
macro causes BSD definitions to be favored in a few cases where standards
conflict.

_SVID_SOURCE

If defined (with any value), expose System V Interface Definition (SVID)
definitions.

_GNU_SOURCE

If defined (with any value), expose all of the definitions provided by setting
all of the preceding macros, as well as various GNU extensions.


https://www.gnu.org/software/libc/manual/html_node/Feature-Test-Macros.html
Macro: _GNU_SOURCE
If you define this macro, everything is included: ISO C89, ISO C99, POSIX.1,
POSIX.2, BSD, SVID, X/Open, LFS, and GNU extensions. In the cases where
POSIX.1 conflicts with BSD, the POSIX definitions take precedence.


If individual macros are defined, or the compiler is invoked in one of its
standard modes (e.g., cc –ansi or cc –std=c99), then only the requested
definitions are supplied.

The <features.h> header file and the `feature_test_macros(7)` manual page
provide further information on precisely what values are assigned to each of
the feature test macros.


<system-data-type>
Various implementation data types are represented using standard C types, for
example, process IDs, user IDs, and file offsets. Although it would be
possible to use the C fundamental types, `native type`, such as int and long
to declare variables storing such information, this reduces portability across
UNIX systems, for the following reasons:

  The sizes of these fundamental types vary across UNIX implementations (e.g.,
      a long may be 4 bytes on one system and 8 bytes on another), or
  sometimes even in different compilation environments on the same
  implementation.  Furthermore, different implementations may use different
  types to represent the same information. For example, a process ID might be
  an int on one system but a long on another.

  Even on a single UNIX implementation, the types used to represent
  information may differ between releases of the implementation. Notable
  examples on Linux are user and group IDs. On Linux 2.2 and earlier, these
  values were represented in 16 bits. On Linux 2.4 and later, they are 32-bit
  values.

To avoid such portability problems, SUSv3 specifies various standard system
data types, and requires an implementation to define and use these types
appropriately.

Each of these types is defined using the `C typedef feature` For example, the
pid_t data type is intended for representing process IDs, and on Linux/x86-32
this type is defined as follows:

typedef int pid_t;

Most of the standard system data types have names ending in _t. Many of them
  are declared in the header file <sys/types.h>, although a few are defined in
  other header files.

An application should employ these type definitions to portably declare the
variables it uses. For example, the following declaration would allow an
application to correctly represent process IDs on any SUSv3-conformant system:

pid_t mypid;

Table 3-1 lists some of the system data types we'll encounter in this book.

When discussing the data types in Table 3-1 in later chapters, we'll often
make statements that some type "is an integer type [specified by SUSv3]." This
means that SUSv3 requires the type to be defined as an integer, but doesn't
require that a particular `native integer type` (e.g., short, int, or long) be
used.


<printf-representation>
When printing values of one of the numeric system data types shown in Table
3-1 (e.g., pid_t and uid_t), we must be careful not to include a
representation dependency in the printf() call. A representation dependency
can occur because C’s argument promotion rules convert values of type short to
int, but leave values of type int and long unchanged. This means that,
  depending on the definition of the system data type, either an int or a long
    is passed in the printf() call. However, because printf() has no way to
    determine the types of its arguments at run time, the caller must
    explicitly provide this information using the %d or %ld format specifier.
    The problem is that simply coding one of these specifiers within the
    printf() call creates an implementation dependency. The usual solution is
    to use the %ld specifier and always cast the corresponding value to long,
  like so:

pid_t mypid;
mypid = getpid(); /* Returns process ID of calling process */
printf("My PID is %ld\n", (long) mypid);

We make one exception to the above technique. Because the off_t data type is
  the size of long long in some compilation environments, we cast off_t values
  to this type and use the %lld specifier, as described in Section 5.10.
  

<structures>
Each UNIX implementation specifies a range of standard structures that are
used in various system calls and library functions. As an example, consider
the sembuf structure, which is used to represent a semaphore operation to be
performed by the semop() system call:

struct sembuf {
  unsigned short sem_num; /* Semaphore number */
  short sem_op; /* Operation to be performed */
  short sem_flg; /* Operation flags */
};

Although SUSv3 specifies structures such as sembuf, it is important to realize
the following:

  `In general`, the order of field definitions within such structures is not
  specified.

  In some cases, extra implementation-specific fields may be included in such
  structures.

Consequently, it is not portable to use a structure initializer such as the
following:

struct sembuf s = { 3, -1, SEM_UNDO };

Although this initializer will work on Linux, it won't work on another
implementation where the fields in the sembuf structure are defined in a
different order. 

To portably initialize such structures, we must use explicit assignment
statements, as in the following:

struct sembuf s;
s.sem_num = 3;
s.sem_op = -1;
s.sem_flg = SEM_UNDO;

If we are using C99, then we can employ that language's new syntax for
structure initializers to write an equivalent initialization:

struct sembuf s = { .sem_num = 3, .sem_op = -1, .sem_flg = SEM_UNDO };

Considerations about the order of the members of standard structures also
apply if we want to write the contents of a standard structure to a file. To
do this portably, we can't simply do a binary write of the structure. Instead,
   the structure fields must be written individually (probably in text form)
  in a specified order.


={============================================================================
*kt_linux_core_007* linux-dev-null

`/dev/null` is a virtual device that always discards the data written to it.
When we want to eliminate the standard output or error of a shell command, we
can redirect it to this file. 

Reads from this device always return end-of-file.

http://stackoverflow.com/questions/19955260/what-is-dev-null-in-bash

Redirecting /dev/null to stdin will give an immediate EOF to any read call
from that process. This is typically useful to detach a process from a tty
(such a process is called a daemon). For example, when starting a background
process remotely over ssh, you must redirect stdin to prevent the process
waiting for local input.

Another reason to redirect to /dev/null is to prevent an unused file
descriptor being created for stdin. This can minimize the total open file
handles when you have many long running processes.

https://serverfault.com/questions/36419/using-ssh-to-remotely-start-a-process/36436#36436

The problem is:
Unfortunately, whenever I use SSH to start the process, the SSH command never
seems to return, causing the script to stall.

SSH connects stdin, stdout and stderr of the remote shell to your local
terminal, so you can interact with the command that's running on the remote
side.

As a side effect, it will keep running until these connections have been
closed, which happens only when the remote command and all its children (!)
have terminated (because the children, which is what "&" starts, inherit std*
    from their parent process and keep it open).

So you need to use something like

ssh user@host "/script/to/run < /dev/null > /tmp/mylogfile 2>&1 &"

The <, > and 2>&1 redirect stdin/stdout/stderr away from your terminal. The
"&" then makes your script go to the background. In production you would of
course redirect stdin/err to a suitable logfile.

See

http://osdir.com/ml/network.openssh.general/2006-05/msg00017.html

Edit:
Just found out that the < /dev/null above is not necessary (but redirecting
stdout/err is). No idea why...


={============================================================================
*kt_linux_core_050* linux-signal

LPI-20 SIGNALS: FUNDAMENTAL CONCEPTS

{signal-is-notification}
A signal is a notification to a process that an event has occurred. Sometimes
described as `software-interrupts` and are analogous to hardware interrupts in
that they interrupt the normal flow of execution of a program.

20.4 Introduction to Signal Handlers

Invocation of a signal handler may interrupt the main program flow at any
time; the kernel calls the handler on the process’s behalf, and when the
handler returns, execution of the program resumes at the point where the
handler interrupted it.

21.2 Other Methods of Terminating a Signal Handler

All of the signal handlers that we have looked at so far complete by returning
to the main program. However, simply returning from a signal handler sometimes
isn’t desirable, or in some cases, isn’t even useful.


{signal-and-kernel}
One process can (if it has suitable permissions) send a signal to another
process. Can be employed as a synchronization technique, or even as a primitive
form of interprocess communication (IPC). It is also possible for a process to
send a signal to itself. However, the usual source of many signals sent to a
process is the kernel.


{signal-symbolic-names}
Each signal is defined as a unique (small) integer, starting sequentially from
1. These integers are defined in <signal.h> with symbolic names of the form
SIGxxxx. Since the actual numbers used for each signal vary across
implementations, it is these symbolic names that are always used in programs.

*signal-list* to see symbolic names.

$kill -l


<signal-standard-and-realtime>
Signals fall into two broad categories. The first set constitutes the
traditional or standard signals, which are used by the kernel to notify
processes of events. On Linux, the standard signals are numbered from 1 to 31.

The other set of signals consists of the realtime signals


{signal-mask-per-process} {signal-block} {signal-pending}
A signal is said to be generated by some event. Once generated, a signal is
later delivered to a process, which then takes some action in response to the
signal. Between the time it is generated and the time it is delivered, a signal
is said to be pending. Why pending since do not know when it is next scheduled
to run, or immediately if the process is already running

Sometimes, however, we need to ensure that a segment of code is 'not'
interrupted by the delivery of a signal. To do this, we can add a signal to
the process's `signal-mask`; a set of signals whose delivery is currently
blocked.  If a signal is generated while it is blocked, it `remains pending`
until it is later unblocked (removed from the signal mask).

If a process receives a signal that it is currently blocking, that signal is
added to the process's `set-of-pending-signals`.

That delivery of a signal is blocked during the execution of its handler
(unless we specify the SA_NODEFER flag to sigaction()). If the signal is
(again) generated while the handler is executing, then it is marked as pending
and later delivered when the handler returns.

note:
Signals can be pending due to either blocked or its handling running.


<signal-is-queued-or-not>
`standard-signals` can't be queued; delivered only once. The set of pending
signals is only a mask; it indicates whether or not a signal has occurred, but
not how many times it has occurred. 

In other words, if the same signal is generated multiple times while it is
blocked ot the handler is executing, then it is recorded in the set of pending
signals, and later delivered, `just once.` 

One of the differences between standard and realtime signals is that
`realtime-signals` are queued.


<signal-proc> *proc-status*
The Linux-specific /proc/PID/status file contains various bit-mask fields that
can be inspected to determine a process's treatment of signals. The bit masks
are displayed as hexadecimal numbers, with the least significant bit
representing signal 1, the next bit to the left representing signal 2, and so
on. 

SigQ: 0/3941
SigPnd: 0000000000000000
ShdPnd: 0000000000000000
SigBlk: 0000000000000000
SigIgn: fffffffe57f0d8fc
SigCgt: 00000000280b2603

These fields are 

SigPnd (`per-thread` pending signals)
ShdPnd (`process-wide` pending signals since Linux 2.6)
SigBlk (blocked signals), SigIgn (ignored signals)
SigCgt (caught signals)

The difference between the SigPnd and ShdPnd fields will become clear when we
describe the handling of signals in multithreaded processes in Section 33.2.
The same information can also be obtained using various options to the ps(1)
  command.


={============================================================================
*kt_linux_core_050* linux-signal-handler

LPI-20, 21

Instead of accepting the default for a particular signal, a program can change
the action that occurs when the signal is delivered. This is known as setting
the `disposition` of the signal. A program can set one of the following
dispositions for a signal:

o The default action should occur. This is useful to undo an earlier change of
the disposition of the signal to something other than its default.

o The signal is ignored. This is useful for a signal whose default action
would be to terminate the process.

o A signal handler is executed.

To change a default is usually referred to as `installing` or establishing a
signal handler. When a signal handler is invoked in response to the delivery
of a signal, we say that the signal has been handled or, synonymously, caught.


<signal-abort> *sigabrt*
It isn't possible to set the disposition of a signal to terminate or dump core
unless one of these is the default disposition of the signal. The nearest we
can get to this is to install a handler for the signal that then calls either
exit() or abort(). 
  
*coredump* 
The abort() function generates a SIGABRT signal for the process, which causes
it to dump core and terminate.

note:
After all, can install signal handler only for signals which do exit() or
abort() by default?


{signal-reentrant} *thread-safe-thread-reentrant*

21.1.2 Reentrant and Async-Signal-Safe Functions

Because a signal handler may asynchronously interrupt the execution of a
program at any point in time, the main program and the signal handler in
effect `form two-independent` (although not concurrent) threads of execution
within the same process.


A function is said to be `reentrant` if it can safely be simultaneously
executed by multiple threads of execution in the `same-process`. In this
context, "safe" means that the function achieves its expected result,
  regardless of the state of execution of any other thread of execution.

A function may be `nonreentrant` if it updates global or static data
structures because of race-condition. A function that employs only local
variables is guaranteed to be reentrant because of race-condition. 

This book shows an example using crypt() in both main and signal handler. This
corrupts internal buffer which is statically allocated and crypt uses when
calls it with differnt parameter.

note:
`nonreentrant` means that function is not safe when gets called by multiple
threads. Using only local variables means reentrant. 


*call-malloc*
Such possibilities are in fact rife within the standard C library. For
example, we already noted in Section 7.1.3 that malloc() and free() maintain a
linked list of freed memory blocks available for reallocation from the heap.
If a call to malloc() in the main program is interrupted by a signal handler
that also calls malloc(), then this linked list can be corrupted.  For this
reason, the malloc() family of functions, and other library functions that use
them, are nonreentrant.

Even if we are not using nonreentrant library functions, reentrancy issues can
still be relevant. If a signal handler updates programmer-defined global data
structures that are also updated within the main program, then we can say that
the signal handler is nonreentrant with respect to the main program.

If a function is nonreentrant, then its manual page will normally provide an
explicit or implicit indication of this fact. In particular, watch out for
statements that the function uses or returns information in statically
allocated variables.


{async-signal-safe-function} *signal-safe*
An `async-signal-safe` function is one that the implementation guarantees to
be safe when called from a signal handler. A function is async-signal-safe
either because it is reentrant or because it is not interruptible by a signal
handler.

Table 21-1: Functions required to be async-signal-safe by POSIX.1-1990, SUSv2,
      and SUSv3

Real-world applications should avoid calling non-async-signal-safe functions
from signal handlers.


<signal-frame>

21.3 Handling a Signal on an Alternate Stack: sigaltstack()

Normally, when a signal handler is invoked, the kernel creates a frame for it
on the process stack.

// skipped


<signal-kill>

20.5 Sending Signals: kill()

One process can send a signal to another process using the kill() system call,

#include <signal.h>
int kill(pid_t pid, int sig); Returns 0 on success, or -1 on error

Four different cases determine how pid is interpreted:

*process-group*
o If pid equals 0, the signal is sent to every process in the same process
group as the calling process, `including the calling process itself.`

// skipped

If no process matches the specified pid, kill() fails and sets errno to ESRCH
("No such process"). 

A process needs appropriate permissions to be able send a signal to another
process. The permission rules are as follows:

// skipped


20.6 Checking for the Existence of a Process

The kill() system call can serve another purpose. If the sig argument is
specified as 0, the so-called null signal, then no signal is sent. 
Instead, kill() merely performs error checking to see if the process 'can' be
signaled. 

Read another way, this means we can use the null signal to test if a process
with a specific process ID exists. 

  o If sending a null signal fails with the error ESRCH, then we know the
  process doesn't exist. 
  
  o If the call fails with the error EPERM (meaning the process exists, but we
      don't have permission to send a signal to it) or 

  o or succeeds (meaning we do have permission to send a signal to the
      process), then we know that the process exists.

So this code do sleep while a process of pid is alive.

while (0 == kill(parent_pid, 0))
{
    sleep(1);
}

Verifying the existence of a particular process ID doesn't guarantee that a
'particular' program is still running. Because the kernel recycles process IDs
as processes are born and die, the same process ID may, over time, refer to a
different process. Furthermore, a particular process ID may exist, but be a
zombie (i.e., a process that has died, but whose parent has not yet performed
a wait() to obtain its termination status) *process-zombie*

Various other techniques can also be used to check whether a particular
process is running, including the following:

// skipped

All of these techniques, except the last, are unaffected by recycling of
process IDs.


<signal-raise>

20.7 Other Ways of Sending Signals: raise() and killpg()

Sometimes, it is useful for a process to `send a signal to itself.` (We see an
example of this in Section 34.7.3.) The raise() function performs this task.

RAISE(3)
Linux Programmer's Manual

#include <signal.h>
int raise(int sig);

In a single-threaded program, a call to raise() is equivalent to the following
call to kill():

kill(getpid(), sig);

On a system that supports threads, raise(sig) is implemented as:

pthread_kill(pthread_self(), sig)

The only error that can occur with raise() is EINVAL, because sig was invalid.
Therefore, where we specify one of the SIGxxxx constants, we don’t check the
return status of this function.

We describe the pthread_kill() function in Section 33.2.3, but for now it is
sufficient to say that this implementation means that the signal will be
delivered to the specific thread that called raise(). By contrast, the call
kill(getpid(), sig) sends a signal to the calling process, and that signal may
be delivered to any thread in the process.

When a process sends itself a signal using raise() (or kill()), the signal is
delivered immediately (i.e., before raise() returns to the caller).


The killpg() function sends a signal to all of the members of a process group.


<signal-and-sigaction>

20.3 Changing Signal Dispositions: signal()

UNIX systems provide two ways of changing the disposition of a signal:
signal() and sigaction(). 

  The signal() system call, which is described in this section, was the
  original API for setting the disposition of a signal, and it provides a
  simpler interface than sigaction(). On the other hand, sigaction() provides
  functionality that is not available with signal(). Furthermore, there are
  variations in the behavior of signal() across UNIX implementations 

which mean that `it should never be used` for establishing signal handlers in
portable programs. 

Because of these portability issues, sigaction() is the (strongly) preferred
API for establishing a signal handler.  After we explain the use of
sigaction() in Section 20.13, we’ll always employ that call when establishing
signal handlers in our example programs.

Although documented in section 2 of the Linux manual pages, signal() is
actually implemented in glibc as a library function layered on top of the
sigaction() system call.


20.13 Changing Signal Dispositions: sigaction()

#include <signal.h>
int sigaction(int sig, const struct sigaction *act, struct sigaction *oldact);


The sa_handler field corresponds to the handler argument given to signal(). It
specifies the address of a signal handler, or one of the constants SIG_IGN or
SIG_DFL.


The sa_flags field is a bit mask specifying various options controlling how
the signal is handled. The following bits may be ORed (|) together in this
field:

// skipped

SA_RESETHAND

When this signal is caught, reset its disposition to the default (i.e.,
    SIG_DFL) before invoking the handler. (By default, a signal handler
      remains established until it is explicitly disestablished by a further
      call to sigaction().) The name SA_ONESHOT is provided as a historical
    synonym for SA_RESETHAND, but the latter name is preferable because it is
    standardized in SUSv3.

SA_SIGINFO

Invoke the signal handler with additional arguments providing further
information about the signal. We describe this flag in Section 21.4.

SA_ONSTACK

Invoke the handler for this signal using an alternate stack installed by
sigaltstack(). Refer to Section 21.3.


<signal-info>

21.4 The SA_SIGINFO Flag

Setting the `SA_SIGINFO flag` when establishing a handler with sigaction()
  allows the handler to obtain `additional information` about a signal when it
  is delivered. In order to obtain this information, we must declare the
  handler as follows:

void handler(int sig, siginfo_t *siginfo, void *ucontext);

Instead, we must use an alternative field: sa_sigaction

struct sigaction {
  union {
    void (*sa_handler)(int);
    `void (*sa_sigaction)(int, siginfo_t *, void *);`
  } __sigaction_handler;
  sigset_t sa_mask;
  int sa_flags;
  void (*sa_restorer)(void);
};


On Linux, as on most UNIX implementations, many of the fields in the siginfo_t
structure are combined into a union, since not all of the fields are needed
for each signal. (See <bits/siginfo.h> for details.)

typedef struct {
  int si_signo; /* Signal number */
  int si_code; /* Signal code */
  int si_trapno; /* Trap number for hardware-generated signal
                    (unused on most architectures) */
  union sigval si_value; /* Accompanying data from sigqueue() */
  pid_t si_pid; /* Process ID of sending process */
  uid_t si_uid; /* Real user ID of sender */
  int si_errno; /* Error number (generally unused) */

  `void *si_addr;` /* Address that generated signal
                    (hardware-generated signals only) */

  int si_overrun; /* Overrun count (Linux 2.6, POSIX timers) */
  int si_timerid; /* (Kernel-internal) Timer ID
                     (Linux 2.6, POSIX timers) */
  long si_band; /* Band event (SIGPOLL/SIGIO) */
  int si_fd; /* File descriptor (SIGPOLL/SIGIO) */
  int si_status; /* Exit status or signal (SIGCHLD) */
  clock_t si_utime; /* User CPU time (SIGCHLD) */
  clock_t si_stime; /* System CPU time (SIGCHLD) */
} siginfo_t;

si_signo

This field is set for all signals. It contains the number of the signal
causing invocation of the handler - that is, the same value as the sig
argument to the handler.

si_code

This field is set for all signals. It contains a code providing further
information about the origin of the signal, as shown in Table 21-1.

si_addr

This field is set only for hardware-generated SIGBUS, SIGSEGV, SIGILL, and
SIGFPE signals. For the SIGBUS and SIGSEGV signals, this field contains the
address that caused the invalid memory reference. For the SIGILL and SIGFPE
signals, this field contains the address of the program instruction that
caused the signal.


The si_code field provides further information about the origin of the signal,
using the values shown in Table 21-2.

<ex>

The ucontext argument

The final argument passed to a handler established with the SA_SIGINFO flag,
    ucontext, is a pointer to a structure of type ucontext_t (defined in
        <ucontext.h>).(SUSv3 uses a void pointer for this argument because
          it doesn’t specify any of the details of the argument.) 
        
This structure provides so-called user-context information describing the
process state prior to invocation of the signal handler, including the
previous process signal mask and saved register values (e.g., program counter
    and stack pointer). This information is rarely used in signal handlers, so
we don’t go into further details.


/usr/include/signal.h

# include <bits/sigstack.h>
# if defined __USE_XOPEN || defined __USE_XOPEN2K8
/* This will define `ucontext_t' and `mcontext_t'.  */
#  include <sys/ucontext.h>
# endif


/usr/include/i386-linux-gnu/sys/ucontext.h
/usr/include/x86_64-linux-gnu/sys/ucontext.h

# else

// can see *x86-asm-reg-enums*
enum
{
  REG_GS = 0,
  REG_FS,
  REG_ES,
  REG_DS,
  REG_EDI,
  REG_ESI,
  REG_EBP,
  REG_ESP,
  REG_EBX,
  REG_EDX,
  REG_ECX,
  REG_EAX,
  REG_TRAPNO,
  REG_ERR,
  REG_EIP,
  REG_CS,
  REG_EFL,
  REG_UESP,
  REG_SS
};


/* Context to describe whole processor state.  */
typedef struct
  {
    gregset_t gregs;
    /* Due to Linux's history we have to use a pointer here.  The SysV/i386
       ABI requires a struct with the values.  */
    fpregset_t fpregs;
    unsigned long int oldmask;
    unsigned long int cr2;
  } mcontext_t;

/* Userlevel context.  */
typedef struct ucontext
  {
    unsigned long int uc_flags;
    struct ucontext *uc_link;
    stack_t uc_stack;
    mcontext_t uc_mcontext;
    __sigset_t uc_sigmask;
    struct _libc_fpstate __fpregs_mem;
  } ucontext_t;

#endif /* !__x86_64__ */


// *x86-asm-reg*
/usr/include/x86_64-linux-gnu/sys/user.h

The whole purpose of this file is for GDB and GDB only.  Don't read
   too much into it.  Don't use it for anything other than GDB unless
   you know what you are doing.

#ifdef __x86_64__

struct user_regs_struct
{
  __extension__ unsigned long long int r15;
  __extension__ unsigned long long int r14;
  __extension__ unsigned long long int r13;
  __extension__ unsigned long long int r12;
  __extension__ unsigned long long int rbp;
  __extension__ unsigned long long int rbx;
  __extension__ unsigned long long int r11;
  __extension__ unsigned long long int r10;
  __extension__ unsigned long long int r9;
  __extension__ unsigned long long int r8;
  __extension__ unsigned long long int rax;
  __extension__ unsigned long long int rcx;
  __extension__ unsigned long long int rdx;
  __extension__ unsigned long long int rsi;
  __extension__ unsigned long long int rdi;
  __extension__ unsigned long long int orig_rax;
  __extension__ unsigned long long int rip;
  __extension__ unsigned long long int cs;
  __extension__ unsigned long long int eflags;
  __extension__ unsigned long long int rsp;
  __extension__ unsigned long long int ss;
  __extension__ unsigned long long int fs_base;
  __extension__ unsigned long long int gs_base;
  __extension__ unsigned long long int ds;
  __extension__ unsigned long long int es;
  __extension__ unsigned long long int fs;
  __extension__ unsigned long long int gs;
};

#else
// These are the 32-bit x86 structures.

struct user_regs_struct
{
  long int ebx;
  long int ecx;
  long int edx;
  long int esi;
  long int edi;
  long int ebp;
  long int eax;
  long int xds;
  long int xes;
  long int xfs;
  long int xgs;
  long int orig_eax;
  long int eip;
  long int xcs;
  long int eflags;
  long int esp;
  long int xss;
};

#endif


<ex>
// this ex works. However, the gregs are empty for above asan case.
//
// from SO
// kyoupark@kt-office-debian:~/works$ ./a.out
// 0x40793a ~ 0x407a0d ~ 0x407a19
// si:0
// ip:407a13
// Segmentation fault

#define _GNU_SOURCE 1
#include <iostream>
#include <iomanip>
#include <signal.h>
#include <ucontext.h>

using std::cout;

static volatile int *causecrash;

static void handler(int, siginfo_t *si, void *ptr)
{
   ucontext_t *uc = (ucontext_t *)ptr;

   cout << "si:" << si->si_addr << '\n';
   cout << "ip:" << std::hex << uc->uc_mcontext.gregs[REG_EIP] << '\n';
}

int main()
{
begin:
    cout.setf(std::ios::unitbuf);
    cout << &&begin << " ~ " << &&before << " ~ " << &&after << '\n';

    struct sigaction s;
    s.sa_flags = SA_SIGINFO|SA_RESETHAND;
    s.sa_sigaction = handler;
    sigemptyset(&s.sa_mask);
    sigaction(SIGSEGV, &s, 0);

before:
    *causecrash = 0;
after:
    cout << "End.\n";
}


<ex>
Program received signal SIGSEGV, Segmentation fault.
0x08048ccb in call_gbo_1 () at gbo_recover.c:143
143	{
   0x08048ca1 <call_gbo_1+153>:	39 df	cmp    edi,ebx
   0x08048ca3 <call_gbo_1+155>:	74 26	je     0x8048ccb <call_gbo_1+195>
   0x08048ca5 <call_gbo_1+157>:	c7 03 0e 36 e0 45	mov    DWORD PTR [ebx],0x45e0360e
   0x08048cab <call_gbo_1+163>:	c7 80 00 00 00 20 f5 f5 f5 f5	mov    DWORD PTR [eax+0x20000000],0xf5f5f5f5
   0x08048cb5 <call_gbo_1+173>:	c7 80 04 00 00 20 f5 f5 f5 f5	mov    DWORD PTR [eax+0x20000004],0xf5f5f5f5
   0x08048cbf <call_gbo_1+183>:	c7 80 08 00 00 20 f5 f5 f5 f5	mov    DWORD PTR [eax+0x20000008],0xf5f5f5f5
   0x08048cc9 <call_gbo_1+193>:	eb 1e	jmp    0x8048ce9 <call_gbo_1+225>
=> 0x08048ccb <call_gbo_1+195>:	c7 80 00 00 00 20 00 00 00 00	mov    DWORD PTR [eax+0x20000000],0x0
   0x08048cd5 <call_gbo_1+205>:	c7 80 04 00 00 20 00 00 00 00	mov    DWORD PTR [eax+0x20000004],0x0
   0x08048cdf <call_gbo_1+215>:	c7 80 08 00 00 20 00 00 00 00	mov    DWORD PTR [eax+0x20000008],0x0

(gdb) p *((ucontext_t*)context)
$6 = {
  uc_flags = 0x1, 
  uc_link = 0x0, 
  uc_stack = {
    ss_sp = 0xb7a36000, 
    ss_flags = 0x0, 
    ss_size = 0x8000
  }, 
  uc_mcontext = {
    gregs = {0x33, 0x0, 0x7b, 0x7b, 0xbffff0b0, 0xbffff110, 0xbffff128, 0xbffff0a0, 0xbffff0b0, 0x524c445f, 0x48, 0xbfffe980, 0xe, 0x7, 0x8048ccb, 0x73, 0x10246, 0xbffff0a0, 0x7b}, 
    fpregs = 0xb7a3dd10, 
    oldmask = 0x0, 
    cr2 = 0xdfffe980
  }, 
  uc_sigmask = {
    __val = {0x0, 0x0, 0xadb8, 0x80cd00, 0x0, 0x0, 0xffff037f, 0xffff0000, 0xffffffff, 0xb776aafd, 0x5980073, 0xb784795c, 0x7b, 0x0 <repeats 19 times>}
  }, 

  //
}

(gdb) p/x *((siginfo_t*)siginfo)
$4 = {
  si_signo = 11, 
  si_errno = 0, 
  si_code = 1, 
  _sifields = {
    _pad = {-536876672, 0 <repeats 28 times>}, 
    _kill = {
      si_pid = -536876672, 
      si_uid = 0
    }, 
    _timer = {
      si_tid = -536876672, 
      si_overrun = 0, 
      si_sigval = {
        sival_int = 0, 
        sival_ptr = 0x0
      }
    }, 
    _rt = {
      si_pid = -536876672, 
      si_uid = 0, 
      si_sigval = {
        sival_int = 0, 
        sival_ptr = 0x0
      }
    }, 
    _sigchld = {
      si_pid = -536876672, 
      si_uid = 0, 
      si_status = 0, 
      si_utime = 0, 
      si_stime = 0
    }, 
    _sigfault = {
      si_addr = 0xdfffe980, 
      si_addr_lsb = 0, 
      si_addr_bnd = {
        _lower = 0x0, 
        _upper = 0x0
      }
    }, 
    _sigpoll = {
      si_band = -536876672, 
      si_fd = 0
    }, 
    _sigsys = {
      _call_addr = 0xdfffe980, 
      _syscall = 0, 
      _arch = 0
    }
  }
}


<signal-pause>

20.14 Waiting for a Signal: pause()

Calling pause() suspends execution of the process until the call is
interrupted by a signal handler (or until an unhandled signal terminates the
    process).


{standard-and-realtime-signal}
Signals fall into two broad categories; standard and realtime. On Linux, the
standard signals are numbered from 1 to 31. We describe the standard signals in
this chapter. The other set of signals consists of the realtime signals.

<why-reliable-signal>
In early implementations, signals could be lost (i.e., not delivered to the
    target process) in certain circumstances. Furthermore, although facilities
were provided to block delivery of signals while critical code was executed, in
some circumstances, blocking was not reliable. These problems were remedied in
4.2BSD, which provided so-called reliable signals.

However, the Linux signal(7) manual page lists more than 31 signal names. The
excess names can be accounted for in a variety of ways. Some of the names are
simply synonyms for other names, and are defined for source compatibility with
other UNIX implementations. Other names are defined but unused.


={============================================================================
*kt_linux_core_051* linux-signal-names

LPI-20.2 Signal Types and Default Actions

Earlier, we mentioned that the standard signals are numbered from 1 to 31 on
Linux. However, the Linux signal(7) manual page lists more than 31 signal
names.  The excess names can be accounted for in a variety of ways. Some of
the names are simply synonyms for other names, and are defined for source
compatibility with other UNIX implementations. Other names are defined but
unused. The following list describes the various signals:

Table 20-1 summarizes a range of information about signals on Linux. Note the
following points about this table:

The Default column indicates the default action of the signal: 

term means that the signal terminates the process, 
core means that the process produces a core
dump file and terminates, 
ignore means that the signal is ignored, 
stop means that the signal stops the process, 
and cont means that the signal resumes a stopped process.


<SIGHUP> see process group for more.
When a terminal disconnect (hangup) occurs, this signal is sent to the
'controlling' process of the terminal. A second use of SIGHUP is with daemons
(e.g., init, httpd, and inetd). Many daemons are designed to respond to the
receipt of SIGHUP by reinitializing themselves and rereading their configuration
files. The system administrator triggers these actions by manually sending
SIGHUP to the daemon, either by using an explicit kill command or by executing a
program or script that does the same.


<sigabrt-SIGABRT> *coredump*
A process is sent this signal when it calls the abort() function (Section
21.2.2). By default, this signal terminates the process with a core dump.
This achieves the intended purpose of the abort() call: to produce a core dump
for debugging.


SIGBUS

This signal (“bus error”) is generated to indicate certain kinds of
memoryaccess errors. One such error can occur when using memory mappings
created with mmap(), if we attempt to access an address that lies beyond the
end of the underlying memory-mapped file, as described in Section 49.4.3.


<SIGINT>
When the user types the terminal interrupt character (usually Control-C), the
terminal driver sends this signal to the foreground process group. The default
action for this signal is to terminate the process.

<sigpipe-SIGPIPE> *broken-pipe*
This signal is generated when a process tries to write to a pipe, a FIFO, or a
socket for which there is no corresponding reader process. This normally
occurs because the reading process has closed its file descriptor for the IPC
channel. See Section 44.2 for further details.


<sigsegv-SIGSEGV> *sigseg* 11
This very popular signal is generated when a program makes an
`invalid-memory-reference`. A memory reference may be invalid because:

the referenced page doesn't exist e.g., it lies in an unmapped area somewhere
between the heap and the stack, 

the process tried to update a location in read-only memory e.g., the program
  text segment or a region of mapped memory marked read-only, 

the process tried to access a part of kernel memory while running in user mode
  (Section 2.1). 

In C, these events often result from dereferencing a pointer containing a bad
address (e.g., an uninitialized pointer) or passing an invalid argument in a
function call. 

The name of this signal derives from the term segmentation violation.

<ex> from 21.3 Handling a Signal on an Alternate Stack: sigaltstack()

When a process attempts to grow its stack beyond the maximum possible size,
the kernel generates a SIGSEGV signal for the process. 
However, since the stack space is exhausted, the kernel can’t create a frame
for any SIGSEGV handler that the program may have established. Consequently,
the handler is not invoked, and the process is terminated 
(`core is the default action for SIGSEGV`).


<sigkill-SIGKILL> 9
This is the `sure-kill-signal`. It can't be blocked, ignored, or caught by a
handler, and thus always terminates a process.


<SIGTERM> 15
This is the `default-signal` used for terminating a process and is the signal
sent by the 'kill' and 'killall' commands. 


<difference-between-sigkill-and-sigterm>
Users sometimes explicitly send the SIGKILL signal to a process using kill -KILL
or kill -9. 

However, this is generally a mistake. A well-designed application will have a
handler for SIGTERM that causes the application to exit 'gracefully', cleaning
up temporary files and releasing other resources beforehand. Killing a process
with SIGKILL `bypasses` the SIGTERM handler. Thus, we should always first
attempt to terminate a process using SIGTERM, and reserve SIGKILL as a last
resort for killing runaway processes that don't respond to SIGTERM. 

note: 
This means that if use sure kill, then can cause resource locked up and leaking.

What if there is no handler of SIGTERM? will be terminated by kernel anyway?
when tried to kill a simple application which do sleep and loop, it is
terminated when use kill, SIGTERM.

<SIGURG>
This signal is sent to a process to indicate the presence of out-of-band (also
    known as urgent) data on a socket

<SIGCHLD>
This signal is sent (by the kernel) to a parent process when one of its children
terminates: either by calling exit() or as a result of being killed by a signal.
It may also be sent to a process when one of its children is stopped or resumed
by a signal. By default, SIGCHLD is ignored.

<SIGUSR1>
This signal and SIGUSR2 are available for programmer-defined purposes. The
kernel never generates these signals for a process. Processes may use these
signals to notify one another of events or to synchronize with each other. In
early UNIX implementations, these were the only two signals that could be freely
used in applications. In fact, processes can send one another any signal, but
this has the potential for confusion if the kernel also generates one of the
signals for a process.  Modern UNIX implementations provide a large set of
realtime signals that are also available for programmer-defined purposes
(Section 22.8).

<sigquit-SIGQUIT>
When the user types the quit character (usually Control-\), this signal is
sent to the foreground process group. By default, this signal terminates a
process and causes it to produce a core dump, which can then be used for
debugging. Using SIGQUIT in this manner is useful with a program that is stuck
in an infinite loop or is otherwise not responding. 

By typing Control-\ and then loading the resulting core dump with the gdb
debugger and using the backtrace command to obtain a stack trace, we can find
out which part of the program code was executing.
    

={============================================================================
*kt_linux_core_051* linux-signal-ex: use signal as synchronization

#include <signal.h>
#include "curr_time.h" /* Declaration of currTime() */
#include "tlpi_hdr.h"

#define SYNC_SIG SIGUSR1 /* Synchronization signal */

static void /* Signal handler - does nothing but return */
handler(int sig)
{
}

int
main(int argc, char *argv[])
{
  pid_t childPid;
  sigset_t blockMask, origMask, emptyMask;
  struct sigaction sa;

  setbuf(stdout, NULL); /* Disable buffering of stdout */

  // The sigemptyset() function initializes a signal set to contain no members. The sigfillset()
  // function initializes a set to contain all signals (including all realtime signals). After
  // initialization, individual signals can be added to a set using sigaddset() and removed using
  // sigdelset().

  sigemptyset(&blockMask);
  sigaddset(&blockMask, SYNC_SIG); /* Block signal */

  // We can use sigprocmask() to change the process signal mask, to retrieve the existing mask, or
  // both. The how argument determines the changes that sigprocmask() makes to the signal mask:

  if (sigprocmask(SIG_BLOCK, &blockMask, &origMask) == -1)
    errExit("sigprocmask");

  sigemptyset(&sa.sa_mask);
  sa.sa_flags = SA_RESTART;
  sa.sa_handler = handler;

  if (sigaction(SYNC_SIG, &sa, NULL) == -1)
    errExit("sigaction");

  switch (childPid = fork()) {
    case -1:
      errExit("fork");

    case 0: /* Child */

      /* Child does some required action here... */
      printf("[%s %ld] Child started - doing some work\n", currTime("%T"), (long) getpid());
      sleep(2); /* Simulate time spent doing some work */

      /* And then signals parent that it's done */
      printf("[%s %ld] Child about to signal parent\n", currTime("%T"), (long) getpid());

      if (kill(getppid(), SYNC_SIG) == -1)
        errExit("kill");

      /* Now child can do other things... */
      _exit(EXIT_SUCCESS);

    default: /* Parent */

      /* Parent may do some work here, and then waits for child to complete the required action */
      printf("[%s %ld] Parent about to wait for signal\n", currTime("%T"), (long) getpid());

      sigemptyset(&emptyMask);
      if (sigsuspend(&emptyMask) == -1 && errno != EINTR)
        errExit("sigsuspend");

      printf("[%s %ld] Parent got signal\n", currTime("%T"), (long) getpid());

      /* If required, return signal mask to its original state */
      if (sigprocmask(SIG_SETMASK, &origMask, NULL) == -1)
        errExit("sigprocmask");

      /* Parent carries on to do other things... */
      exit(EXIT_SUCCESS);
  }
}


={============================================================================
*kt_linux_core_100* linux-process

LPI-6 PROCESSES

6.1 Processes and Programs

A process is an instance of an executing program. A process is an `abstract
entity`, defined by the kernel, to which system resources are allocated in
order to execute a program.

A program is a file containing a range of information that describes how to
construct a process at run time. This information includes the following:

  Binary format identification: Each program file includes metainformation
  describing the format of the executable file. This enables `the kernel` to
  interpret the remaining information in the file.

  Symbol and relocation tables: These describe the locations and names of
  functions and variables within the program. These tables are used for a
  variety of purposes, including debugging and run-time symbol resolution
  (dynamic linking).

From the kernel's point of view, a process consists of user-space memory
containing program code and variables used by that code, and a range of kernel
data structures that maintain information about the state of the process. The
information recorded in the kernel data structures includes various identifier
numbers (IDs) associated with the process, virtual memory tables, the table of
open file descriptors, information relating to signal delivery and handling,
process resource usages and limits, the current working directory, and a host
of other information.


6.2 Process ID and Parent Process ID

#include <unistd.h>
pid_t getpid(void);

PID is integer type and the Linux limits PIDs to 32767. Once it has reached
32,767, the process ID counter is reset to 300, rather than 1. This is done
because many low-numbered process IDs are in permanent use by system processes
and daemons, and thus time would be wasted searching for an unused process ID in
this range. 

In Linux 2.4 and earlier, the process ID limit of 32,767 is defined by the
kernel constant PID_MAX. With Linux 2.6, things change. While the default
upper limit for process IDs remains 32,767, this limit is adjustable via the
value in the Linux-specific /proc/sys/kernel/pid_max file (which is one
    greater than the maximum process ID). On 32-bit platforms, the maximum
value for this file is 32,768, but on 64-bit platforms, it can be adjusted to
any value up to 222 (approximately 4 million), making it possible to
accommodate very large numbers of processes.


={============================================================================
*kt_linux_core_107* linux-process-argv

LPI-6.6 Command-Line Arguments (argc, argv)

<busybox-argv-trick>
The fact that argv[0] contains the name used to invoke the program can be
employed to perform a useful trick. We can create multiple links to (i.e.,
    names for) the same program, and then have the program look at argv[0] and
take different actions depending on the name used to invoke it. An example of
this technique is provided by the gzip(1), gunzip(1), and zcat(1) commands,
all of which are links to the same executable file. 

lrwxrwxrwx    1 root     root           14 Dec  6  2018 syslogd -> ../bin/busybox
lrwxrwxrwx    1 root     root           14 Dec  6  2018 telnetd -> ../bin/busybox
lrwxrwxrwx    1 root     root            7 Dec  6  2018 tune2fs -> e2label
lrwxrwxrwx    1 root     root           14 Dec  6  2018 udhcpc -> ../bin/busybox

If we employ this technique, we must be careful to handle the possibility that
the user might invoke the program via a link with a name other than any of
those that we expect.

One limitation of the argc/argv mechanism is that these variables are
available only as arguments to main().


<ARG_MAX>
There is an upper limit on the total number of bytes that can be stored in
this area. SUSv3 prescribes the use of the ARG_MAX constant (defined in
    <limits.h>) or the call sysconf(_SC_ARG_MAX) to determine this limit. 

SUSv3 requires ARG_MAX to be at least _POSIX_ARG_MAX (4096) bytes. Most UNIX
implementations allow a considerably higher limit than this.  SUSv3 leaves it
unspecified whether an implementation counts overhead bytes (for terminating
    null bytes, alignment bytes, and the argv and environ arrays of pointers)
against the ARG_MAX limit.

Starting with kernel 2.6.23, the limit on the total space used for argv and
environ can be controlled via the RLIMIT_STACK resource limit, and a much
larger limit is permitted for argv and environ.


{proc-cmdline} *check-boot-params*
The command-line arguments of any process can be read via the Linux-specific
/proc/PID/cmdline file, with each argument being terminated by a null byte. A
program can access its own command-line arguments via /proc/self/cmdline.

-sh-3.2# cat /proc/cmdline
mem=165M rw console=uart,mmio,0x10400b00,115200n8 ip_conntrack_tftp.ports=4085


={============================================================================
*kt_linux_core_107* linux-process-environment-list

LPI-6.7 Environment List

Each process has an associated array of strings called the environment list,
or simply the environment. Thus, the environment represents a set of
  `name-value pairs` that can be used to hold arbitrary information. The names
  in the list are referred to as `environment variables.`


<copied>
When a new process is created, it inherits a copy of its parent's environment.
This is a primitive but frequently used form of interprocess communication;
the environment provides a way to transfer information from a parent process
to its child(ren). Since the child gets a copy of its parent's environment at
the time it is created, this transfer of information is one-way and once-only.

Each process has an environment list, which is a set of environment variables
that are maintained within the `user-space memory of the process.` 

When a new process is created via fork(), it inherits a copy of its parent's
environment. Thus, the environment provides a mechanism for a parent process
to `communicate` information to a child process. When a process replaces the
program that it is running using exec(), the new program either inherits the
environment used by the old program or receives a new environment specified as
part of the exec() call.


{sh-export}
A value can be added to the environment using the export command. The above
commands permanently add a value to the shell's environment, and this
environment is then inherited by all child processes that the shell creates.
At any point, an environment variable can be removed with the unset command

$ SHELL=/bin/bash 			# create a `shell variable`
$ export SHELL=/bin/bash	# put variable into shell process's environment

In the Bourne shell and its descendants, the following syntax can be used to
add values to the environment used to execute a single program, without
affecting the parent shell (and subsequent commands):

$ NAME=value program

This adds a definition to the environment of `just the child process`
executing the named program. If desired, multiple assignments (delimited by
    white space) can precede the program name.


{proc-environ}
The environment list of any process can be examined via the Linux-specific
/proc/PID/environ file, with each NAME=value pair being terminated by a null
byte.

$ cat /proc/1155/environ | strings


{in-process}
Within a C program, the environment list can be accessed using the global
variable `char **environ.` The C run-time startup code defines this variable
and assigns the location of the environment list to it. Like argv, environ
points to a NULL-terminated list of pointers to null-terminated strings.

the argv and environ arrays 'reside' in a single contiguous area of memory
just above the process stack.

#include <stdio.h>

int main(int argc, char **argv, char **env)
{
    char **ep;

    printf("this prints envs\n");
    for(ep = env; *ep != NULL; ep++)
        puts(*ep);

    return;
}


{getenv-setenv}
The getenv() function retrieves individual values from the 'process' environment.

#include <stdlib.h>

char *getenv(const char *name);

Returns pointer to (value) string, or NULL if no such variable

The setenv() function is an alternative to putenv() for adding a variable to
the environment.

int setenv(const char *name, const char *value, int overwrite);
int unsetenv(const char *name);

Returns 0 on success, or -1 on error

int putenv(char *string);

Returns 0 on success, or nonzero on error

The string argument is a pointer to a string of the form name=value. After the
putenv() call, this string is part of the environment. In other words, rather
than duplicating the string pointed to by string, one of the elements of
environ will be set to point to the same location as string.

The setenv() function 'creates' a new environment variable `by allocating` a
memory buffer for a string of the form name=value, and copying the strings
pointed to by name and value into that buffer. note: this is why setenv() is
preferable to putenv() since putenv() do not allocate.

The setenv() function doesn't change the environment if the variable
identified by name already exists and overwrite has the value 0. If overwrite
is nonzero, the environment is always changed.

note: Q: where does setenv() allocate a memory? is there any max limit of the
number of env var to create? what if it goes over in term of process memory
layout?

From setenv() man:
       ENOMEM Insufficient memory to add a new variable to the environment.


The unsetenv() function removes the variable identified by name from the
environment. 

note: from man page, The unsetenv() function 'deletes' the variable name from
the environment. If name does not exist in the environment, then the function
succeeds, and the environment is unchanged.


{clearenv}
On occasion, it is useful to erase the entire environment, and then rebuild it
with selected values. For example, we might do this in order to execute
set-user-ID programs in a secure manner (Section 38.8). We can erase the
environment by assigning NULL to environ:

environ = NULL;

This is exactly the step performed by the clearenv() library function.

#define _BSD_SOURCE /* Or: #define _SVID_SOURCE */
#include <stdlib.h>

int clearenv(void)

Returns 0 on success, or a nonzero on error

<memory-leaks>
In some circumstances, the use of setenv() and clearenv() can lead to memory
leaks in a program. We noted above that setenv() allocates a memory buffer
that is then made part of the environment. When we call clearenv(), it doesn't
free this buffer (it can't, since it doesn't know of the buffer's existence).
A program that repeatedly employed these two functions would steadily leak
memory. 

In practice, this is unlikely to be a problem, because a program typically
calls clearenv() just once on startup, in order to remove all entries from the
environment that it inherited from its predecessor (i.e., the program that
    called exec() to start this program).


$ ./modify_env "GREET=Guten Tag" SHELL=/bin/bash BYE=Ciao
GREET=Guten Tag
SHELL=/bin/bash

$ ./modify_env SHELL=/bin/sh BYE=byebye
SHELL=/bin/sh
GREET=Hello world


#define _GNU_SOURCE /* To get various declarations from <stdlib.h> */
#include <stdlib.h>
#include "tlpi_hdr.h"

extern char **environ;

int main(int argc, char *argv[])
{
    int j;
    char **ep;

    // note: do not 'free' actually
    clearenv(); /* Erase entire environment */

    for (j = 1; j < argc; j++)
        if (putenv(argv[j]) != 0)
            errExit("putenv: %s", argv[j]);

    // note: when no GREET in arguments, this will create one but no call to free it?
    if (setenv("GREET", "Hello world", 0) == -1)
        errExit("setenv");

    // note: when BYE is from argv, how dose this free it if unsetenv() do delete it?
    unsetenv("BYE");

    for (ep = environ; *ep != NULL; ep++)
        puts(*ep);

    exit(EXIT_SUCCESS);
}


<inter-process-inter-program>
Sometimes, it is useful for a process to modify its environment. One reason is
to make a change that is visible in all child processes subsequently created
by that process.  Another possibility is that we want to set a variable that
is visible to a new program to be loaded into the memory of this process
("`exec`ed"). In this sense, the environment is not just a form of interprocess
communication, but also a method of interprogram communication.


={============================================================================
*kt_linux_core_101* linux-process-creation

LPI 24 Process Creation

{process-creation}
The wait(&status) system call has `two-purposes`. First, if a child of this
process has not yet terminated by calling exit(), then wait() suspends execution
of the process until one of its children has terminated. Second, the termination
status of the child is returned in the status argument of wait().

<fork-and-exec>
The UNIX approach is usually simpler and more elegant. Separating these two
steps makes the APIs simpler and allows a program a great degree of flexibility
in the actions it performs between the two steps. Moreover, it is often useful
to perform a fork() without a following exec().

Parent process running program A
      |
      A
      |
      child PID = fork(void);         Child process running program A. 
                                      0 = fork(void);
      |                                     |
Parent may perform other atcions here       A  
      |                                     | 
      wait(&status); // optional      Child may perform further actions here
                                            |
                                            execev( B, ... ); // optional
                                            |
                                            B
                                            |
                                      Execution of program B
                                            |
                                            exit(status);
                                            1. Child status passed to parent 
                                            and kernel restarts parent.
                                            2. Delivers SIGCHLD optionally.

<pid-of-child>
The PID of parent remains the same but child is not. Then when child pid is
assigned and what pid 0 check in the code is? 

The following idiom is sometimes employed when calling fork():

/* Used in parent after successful fork() to record PID of child */
pid_t childPid; 

switch (childPid = fork()) {
  case -1: /* fork() failed */
    /* Handle error */
  case 0: /* Child of successful fork() comes here */
    /* Perform actions specific to child */
  default: /* Parent comes here after successful fork() */
    /* Perform actions specific to parent */
}

note 
child pid is assigned `immediately after fork` and has the same name in ps. So pid
== 0 check is to distinguish execution path in the same code in parent.

$ pid (26337): before fork:  
pid (26337): in parent before sleep:  
pid (26338): in child before sleep:  

$ ps   
  PID TTY          TIME CMD
26131 pts/10   00:00:00 bash
26337 pts/10   00:00:00 a.out
26338 pts/10   00:00:00 a.out
26339 pts/10   00:00:00 ps


<sharing-between-parent-and-child>
The child's stack, data, and heap segments are initially exact duplicates of the
corresponding parts the parent's memory. The child receives duplicates of all of
the parent's file descriptors.


<execing>
The execve(pathname, argv, envp) system call loads a new program (pathname,
with argument list argv, and environment list envp) into a processs memory.
The existing program text is discarded, and the stack, data, and heap segments
are `freshly created` for the new program. This operation is often referred to
as execing a new program.


<fork-copy-on-write>
24.2.2

However, actually performing a simple copy of the parents virtual memory
pages into the new child process would be wasteful for a number of reasonsone
being that a fork() is often followed by an immediate exec(), which replaces
the processs text with a new program and reinitializes the processs data,
heap, and stack segments.

note: this is a reason to introduce vfork() and clone().


<race-condition-after-fork>
After a fork(), it is indeterminate which of the two is next scheduled to use
CPU. If we need to guarantee a particular order, we must use some kind of
synchronization technique.

<case-use-signal>
Avoiding race conditions by synchronizing with signals. In practice, such
coordination is more likely to be done using semaphores, file locks, or
message passing. 


={============================================================================
*kt_linux_core_102* linux-process-termination

LPI-25 Process Termination

<exit-function-system-call>
The exit(status) library function terminates a process, making all resources
(memory, open file descriptors, and so on) used by the process available for
subsequent reallocation by the kernel. The status argument is an integer that
determines the termination status for the process. Using the wait() system
call, the parent can retrieve this status.

The exit() library function is layered on top of the _exit() system call.


{two-ways}
A process may terminate in two general ways. One of these is abnormal
termination, caused by the delivery of a signal whose default action is to
terminate the process (with or without a core dump). Alternatively, a process
can terminate normally, using the _exit() system call.

#include <unistd.h>
void _exit(int status);

`#include <stdlib.h>`
void exit(int status); 


{linux-exit-value}
This is exception to the rule that a function with a return type 'must' return a
value. If control reaches the end of main and there is no return, then the
compiler inserts a return of 0.

By convention, a termination status of 0 indicates that a process completed
successfully, and a `nonzero-value` indicates that the process terminated
unsuccessfully. 
  
There are no fixed rules about how nonzero status values are to be interpreted;
SUSv3 specifies two constants, EXIT_SUCCESS (0) and EXIT_FAILURE (1)

To make return value `machine-independent`, use EXIT_FAILURE and EXIT_SUCCESS
from cstdlib header which are preprocessor variables.

Performing an explicit return n is generally equivalent to calling exit(n) since
the run-time function that invokes main() uses the return value from main() in a
call to exit().

Performing a return without specifying a value, or falling off the end of the
main() function, also results in the caller of main() invoking exit(), but with
results that vary depending on the version of the C standard supported and the
compilation options employed:


{linux-exit-call}
Programs generally don't call _exit() directly, but instead call the exit()
library function, which performs various actions before calling _exit().

#include <stdlib.h>
void exit(int status);

The following actions are performed by exit():

 1 Exit handlers (functions registered with atexit() and on_exit()) are called,
  in reverse order of their registration (Section 25.3).

<2> The stdio stream buffers are flushed.

 3 The _exit() system call is invoked, using the value supplied in status.

Unlike _exit(), which is UNIX-specific, exit() is defined as part of the
standard C library; that is, it is available with every C implementation.


{actions-on-termination}
25.2 Details of Process Termination

During both normal and abnormal termination of a process, the following actions
occur:

1. Open file descriptors, directory streams (Section 18.8), message catalog
descriptors (see the catopen(3) and catgets(3) manual pages), and conversion
descriptors (see the iconv_open(3) manual page) are closed.

2. As a consequence of closing file descriptors, any file locks (Chapter 55)
held by this process are released.

<3> Any attached System V shared memory segments are detached, and the
shm_nattch counter corresponding to each segment is decremented by one.

4. For each System V semaphore for which a semadj value has been set by the
process, that semadj value is added to the semaphore value. (Refer to Section
47.8.)

<5> If this is the controlling process for a controlling terminal, then the
SIGHUP signal is sent to each process in the controlling terminal's foreground
process group, and the terminal is disassociated from the session.

6. Any POSIX named semaphores that are open in the calling process are closed
as though sem_close() were called.

7. Any POSIX message queues that are open in the calling process are closed as
though mq_close() were called.

8. If, as a consequence of this process exiting, a process group becomes
orphaned and there are any stopped processes in that group, then all processes
in the group are sent a SIGHUP signal followed by a SIGCONT signal. We
consider this point further in Section 34.7.4.

9. Any memory locks established by this process using mlock() or mlockall()
(Section 50.2) are removed.

10. Any memory mappings established by this process using mmap() are unmapped.


{exit-handler}
Sometimes, an application needs to automatically perform some operations on
process termination.

The GNU C library provides two ways of registering exit handlers. The first
method, specified in SUSv3, is to use the atexit() function.


{stdio-buffers-and-exit}
25.4 Interactions Between fork(), stdio Buffers, and _exit()

#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

int main(int argc, char** argv)
{
  printf("Hello world.\n");
  write( STDOUT_FILENO, "Ciao\n", 5);

  // setbuf( stdout, NULL );

  if( -1 == fork() )
  {
    fprintf( stderr, "fork() failed\n" );
    exit(EXIT_FAILURE);
  }

  exit(EXIT_SUCCESS);
}

$ gcc sample-two.c 
$ ./a.out 
Hello world. 				# printf
Ciao 							# write

$ ./a.out > a
$ cat a
Ciao
Hello world.
Hello world.

$ ./a.out > a 				# when use setbuf()
$ cat a
Ciao
Hello world.

note: Q: Why different output order and duplicates?

1. duplicates.

Recall that the stdio buffers are maintained in a process's user-space memory
(refer to 13.2). Therefore, these buffers are duplicated in the child by
fork(). When standard output is `directed to a terminal`, it is
`line-buffered` by default, with the result that the newline-terminated string
written by printf() appears `immediately`. 

However, when standard output is directed to a file, it is `block-buffered` by
default. Thus the string written by printf() is still in the parent's stdio
buffer at the time of the fork(), and this string is duplicated in the child.
When the parent and the child later call exit(), they both flush their copies
of the stdio buffers, resulting in duplicate output.

To remedy this, can use setbuf() and fflush() to flush the stdio buffer prior to
a fork() call.  However, still have different order.

2. the output order.
The write() transfers data directly to a kernel buffer, and this buffer is not
duplicated during a fork(). note: write is a system call.

The output of write() appears before that from printf() because the output of
write() is immediately transferred to the kernel buffer cache, while the output
from printf() is transferred only when the stdio buffers are flushed by the call
to exit().

note: In general, care is required when mixing stdio functions and system calls
to perform I/O on the same file, as described in Section 13.7.


={============================================================================
*kt_linux_core_103* linux-process-monitor-child-process

LPI-26 MONITORING CHILD PROCESSES

In many applications where a parent creates child processes, it is useful for
the parent to be able to monitor the children to find out when and how they
terminate. This facility is provided by wait() and a number of related system
calls.

<syscall-wait>
The wait() system call waits for one of the children of the calling process to
terminate and returns `the termination status` of that child in the buffer
pointed to by status.

#include <sys/wait.h>

pid_t wait(int *status);

Returns process ID of terminated child, or 1 on error

  * If no (previously unwaited-for) child of the calling process has yet
    terminated, the call `blocks` until one of the children terminates. If a
    child has already terminated by the time of the call, wait() returns
    immediately.

  * If status is not NULL, information about how the child terminated is
    returned in the integer to which status points. We describe the
    information returned in status in Section 26.1.3.

  * As its function result, wait() returns the process ID of the child that
    has terminated.

On error, wait() returns -1. One possible error is that the calling process
has no (previously unwaited-for) children, which is indicated by the errno
value ECHILD. This means that we can use the following loop to wait for all
children of the calling process to terminate:

while ((childPid = wait(NULL)) != -1)
  continue;
if (errno != ECHILD) /* An unexpected error... */
  errExit("wait");

note:
If there are multiple terminated children at a particular moment, SUSv3 leaves
  unspecified the order in which these children will be reaped by a sequence
  of wait() calls; that is, the order depends on the implementation. Even
  across versions of the Linux kernel, the behavior varies.


<syscall-waitpid>
The wait() system call has a number of limitations, which waitpid() was designed
to address.

  * If a parent process has created multiple children, it is not possible to
    wait() for the completion of a specific child; we can only wait for the
    next child that terminates.

  * If no child has yet terminated, wait() always blocks. Sometimes, it would
    be preferable to perform a nonblocking wait so that if no child has yet
    terminated, we obtain an immediate indication of this fact.

  * Using wait(), we can find out only about children that have terminated. It
    is not possible to be notified when a child is stopped by a signal (such
    as SIGSTOP or SIGTTIN) or when a stopped child is resumed by delivery
    of a SIGCONT signal.  


#include <sys/wait.h>

pid_t waitpid(pid_t pid, int *status, int options);

The pid argument enables the selection of the child to be waited for:

  * If pid is greater than 0, wait for the child whose process ID equals pid.
  
  * If pid equals 0, wait for any child in the same process group as the
    caller (parent). We describe process groups in Section 34.2.

  * If pid is less than 1, wait for any child whose process group identifier
    equals the absolute value of pid.

  * If pid equals 1, wait for any child. The call wait(&status) is equivalent
    to the call waitpid(1, &status, 0).


26.1.3 The Wait Status Value

The status value returned by wait() and waitpid() allows us to distinguish the
following events for the child:

  * The child terminated by calling _exit() (or exit()), specifying an integer
    exit status.
  
  * The child was terminated by the delivery of an unhandled signal.
  
  * The child was stopped by a signal, and waitpid() was called with the
    WUNTRACED flag.
  
  * The child was resumed by a SIGCONT signal, and waitpid() was called with
    the WCONTINUED flag.

We use the term `wait status` to encompass all of the above cases. The
designation `termination status` is used to refer to the first two cases.

Although defined as an int, `only the bottom 2 bytes` of the value pointed to by
status are actually used. The way in which these 2 bytes are filled depends on
which of the above events occurred for the child.

normal termination

15 <---- bits ----> 8| 7          0
| exit status(0-255) | 0          |

note:
the layout of the wait status value for Linux/x86-32. The details vary across
implementations. SUSv3 doesn't specify any particular layout for this
information, or even require that it is contained in the bottom 2 bytes of the
value pointed to by status. Portable applications should always use `the macros`
described in this section to inspect this value, rather than directly
inspecting its bit-mask components.

The <sys/wait.h> header file defines a standard set of macros that can be used
to dissect a wait status value. When applied to a status value returned by
wait() or waitpid(), only one of the macros in the list below will return
true. Additional macros are provided to further dissect the status value, as
noted in the list.

WIFEXITED(status)

This macro returns true if the child process exited normally. In this case,
     the macro WEXITSTATUS(status) returns the exit status of the child
       process.  
(As noted in Section 25.1, only the least significant byte of the childâs exit
 status is available to the parent.)

Note that although the name status is also used for the argument of the above
macros, they expect a plain integer, rather than a pointer to an integer as
required by wait() and waitpid().
       
<ex> <call-strsignal>

#define _GNU_SOURCE             // get strsignal() declaration from <string.h>
#include <string.h>
#include <sys/wait.h>
#include "print_wait_status.h"  // declaration of printWaitStatus()
#include "tlpi_hdr.h"

// NOTE: The following function employs printf(), which is not
// async-signal-safe (see Section 21.1.2). As such, this function is also not
// async-signal-safe (i.e., beware of calling it from a SIGCHLD handler)

// examine a wait() status using the W* macros
void printWaitStatus(const char *msg, int status)
{
  if (msg != NULL)
    printf("%s", msg);

  if (WIFEXITED(status))
  {
    printf("child exited, status=%d\n", WEXITSTATUS(status));
  }
  else if (WIFSIGNALED(status))
  {
    printf("child killed by signal %d (%s)\n",
        WTERMSIG(status), strsignal(WTERMSIG(status)));

#ifdef WCOREDUMP  // not in SUSv3, may be absent on some systems
    if (WCOREDUMP(status))
      printf(" (core dumped)");
#endif

    printf("\n");
  }
  else if (WIFSTOPPED(status))
  {
    printf("child stopped by signal %d (%s)\n",
        WSTOPSIG(status), strsignal(WSTOPSIG(status)));

#ifdef WIFCONTINUED
  }
  else if (WIFCONTINUED(status))
  {
    printf("child continued\n");
#endif
  }

  // should never happen
  else {
    printf("what happened to this child? (status=%x)\n", 
        (unsigned int) status);
  }
}


={============================================================================
*kt_linux_core_104* linux-process-zombie *ex-interview*

2014.02 from google phone interview. 

LPI-26.2 Orphans and Zombies

The lifetimes of parent and child processes are usually not the same-either
the parent outlives the child or vice versa. This raises two questions: 

1. Who becomes the parent of an `orphaned-child`? 

Each process has a parent-the process that created it. If a child process
becomes orphaned because its "birth" parent terminates, then the child is
adopted by the `init-process`, and subsequent calls to getppid() in the child
return 1. See Section 26.2. The process 1, init, the ancestor of all
processes. 

2. What happens to a child that terminates before its parent has had a chance
to perform a wait()?

The point here is that, although the child has finished its work, the parent
should still be permitted to perform a wait() at some later time to determine
how the child terminated. The kernel deals with this situation by `turning`
the child into a `zombie`. This means that most of the resources held by the
child are released back to the system to be reused by other processes. 

*kernel-process-table*
The only part of the process that remains is an entry in the kernel's process
table recording; among other things the child's process ID, termination
status, and resource usage statistics. Section 36.1.

A zombie process can't be killed by a signal, not even the (silver bullet)
SIGKILL. This ensures that the parent can always eventually perform a wait().

`When the parent does perform a wait(), the kernel removes the zombie`, since
the last remaining information about the child is no longer required.

On the other hand, if the parent terminates without doing a wait(), then the
init process adopts the child and automatically performs a wait(), thus
removing the zombie process from the system.

<why-zombie-can-be-a-problem>
If a parent creates a child and `alive`, but fails to perform a wait(), then
an entry for the zombie child will be maintained indefinitely in the kernel's
process table. If a large number of such zombie children are created, they
will eventually fill the kernel process table, `preventing` the creation of
new processes. 

note:
zombie-process is different from orphaned-process since parent is still alive
but fails to wait. So `killing parent makes` zombie orphaned process and then
adopted by init-process.

Since the zombies can't be killed by a signal, the only way to remove them
from the system is to kill their parent (or wait for it to exit), at which
time the zombies are adopted and waited on by init, and consequently removed
from the system.

<ex>
$ ./make_zombie
Parent PID=1013
Child (PID=1014) exiting
  1013 pts/4 00:00:00 make_zombie       // Output from ps(1)
  1014 pts/4 00:00:00 make_zombie <defunct>
After sending SIGKILL to make_zombie (PID=1014):
  1013 pts/4 00:00:00 make_zombie       // Output from ps(1)
  1014 pts/4 00:00:00 make_zombie <defunct>

In the output, we see that ps(1) displays the string <defunct> to indicate a
  process in the zombie state.

A common way of reaping dead child processes is to establish a handler for the
SIGCHLD signal.

#include <signal.h>
#include <libgen.h>         // for basename() declaration
#include "tlpi_hdr.h"

#define CMD_SIZE 200

int main(int argc, char *argv[])
{
  char cmd[CMD_SIZE];
  pit_t childPid;

  setbuf(stdout, NULL);     // disable buffering of stdout

  printf("Parent PID=%ld\n", (long) getpid());

  switch( childPid = fork())
  {
    case -1:
      errExit("fork");

      // child: immediately exits to become zombie
    case 0:
      printf("Child PID=%ld exiting\n", (long) getpid());
      _exit(EXIT_SUCCESS);

      // parent
    default:
      sleep(3);     // give child a chance to start and exit
      snprintf(cmd, CMD_SIZE, "ps | grep %s", basename(argv[0]));
      cmd[CMD_SIZE-1] = '\0';   // ensure string is null-terminated
      system(cmd);  // view zombie child again

      // now send the "sure kill" signal to the zombie

      if( kill(childPid, SIGKILL) == -1 )
        errMsg("kill");

      sleep(3);     // give child a chance to react to signal
      printf("After sending SIGKILL to zombie (PID=%ld):\n", (long) childPid);
      system(cmd);  // view zombie child again

      exit(EXIT_SUCCESS);
  }
}


={============================================================================
*kt_linux_core_104* linux-process-sigchld

note:
An example that has a possibility of creating zombie childrens and has to
handle it.

These semantics have important implications for the design of long-lived
parent processes, such as network servers and shells, that create numerous
children.

26.3 The SIGCHLD Signal

The termination of a child process is an event that occurs asynchronously. A
parent can't predict when one of its child will terminate. Even if the parent
sends a SIGKILL signal to the child, the exact time of termination is still
dependent on when the child is next scheduled for use of a CPU. We have
already seen that the parent should use wait() (or similar) in order to
prevent the accumulation of zombie children, and have looked at two ways in
which this can be done:

  * The parent can call wait(), or waitpid() without specifying the WNOHANG
    flag, in which case the call will block if a child has not already
    terminated.


  * The parent can periodically perform a nonblocking check (a poll) for dead
    children via a call to waitpid() specifying the WNOHANG flag.

Both of these approaches can be inconvenient. On the one hand, we may not want
the parent to be blocked waiting for a child to terminate. On the other hand,
making repeated nonblocking waitpid() calls wastes CPU time and adds
  complexity to an application design. 

To get around these problems, we can employ a handler for the SIGCHLD signal.

note: skipped and see the LPI for more.


={============================================================================
*kt_linux_core_110* linux-process-exec exec-call

LPI 27 Program Execution

{execve-system-call}
Various library functions, all with names beginning with exec, are layered on
top of the execve() system call. Each of these functions provides a different
interface to the same functionality. The loading of a new program by any of
these calls is commonly referred to as an exec operation, or simply by the
notation exec().

#include <unistd.h>
int execve(const char *pathname, char *const argv[], char *const envp[]);

note: 'never' returns on success; returns -1 on error


<attribute-that-remains>
After an execve(), the process ID of the process remains the same, because the
same process continues to exist.


<never-return>
Never need to check the return value from execve(); it will always be -1 if get
since the very fact that it returned informs us that an error occurred,

<ex>
note: See how envs are passed to.

$ ./t_execve ./envargs 
argv[0] = envargs 
argv[1] = hello world 
argv[2] = goodbye 
env: GREET=salut 
env: BYE=adieu 

// t_execve.c

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

int main(int argc, char** argv)
{
  char *argVec[10];
  char *envVec[] = { "GREET=salut", "BYE=adieu", NULL };

  if( argc != 2 || strcmp( argv[1], "--help" ) == 0 )
    printf("%s pathname\n", argv[0] );

  // get basename
  argVec[0] = strrchr( argv[1], '/' );
  if( argVec[0] != NULL )
    argVec[0]++;
  else
    argVec[0] = argv[1];

  // note: ternimate with NULL and argVec[] is passed as a whole to execed
  // application.

  argVec[1] = "hello world";
  argVec[2] = "goodbye";
  argVec[3] = NULL;

  execve( argv[1], argVec, envVec );

  printf("if we get here, someting wrong.\n" );
  exit(EXIT_FAILURE);
}

// envarg.c

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

extern char **environ;

int main(int argc, char** argv)
{
  int j;
  char **ep;

  for( j = 0; j < argc; j++ )
    printf("argv[%d] = %s \n", j, argv[j] );

  for( ep = environ; *ep != NULL; ep++ )
    printf("env: %s \n", *ep );

  exit(EXIT_SUCCESS);
}


{exec-library}
The library functions perfors an exec() and all are layered on top of execve().

#include <unistd.h>

int execle(const char *pathname, const char *arg, ...  
    /* , (char *) NULL, char *const envp[] */ );
int execlp(const char *filename, const char *arg, ...  
    /* , (char *) NULL */);
int execvp(const char *filename, char *const argv[]);
int execv(const char *pathname, char *const argv[]);
int execl(const char *pathname, const char *arg, ...  /* , (char *) NULL */);

None of the above returns on success; all return -1 on error

<path-filename>
execvp() allow the program to be specified using just a filename. The filename
is sought in the list of directories specified in the PATH environment variable.
The functions contain the letter `p for PATH` to indicate this.

The PATH variable is 'not' used if the filename contains a slash (/), in which
case it is treated as a relative or absolute pathname.

The PATH value for a login shell is set by system-wide and user-specific shell
startup scripts. Since a child process inherits a copy of its parent's
environment variables, each process that the shell creates to execute a
command inherits a copy of the shell's PATH.

<env>
The execve() and execle() functions allow the programmer to 'explicitly'
specify the environment for the new program using envp, a NULL-terminated
array of pointers to character strings. The names of these functions end with
the letter `e to indicate` this fact. All of the other exec() functions use the
caller's existing environment (i.e., the contents of environ) as the
environment for the new program.


{descriptors-and-exec} *how-implement-redirection*
27.4 File Descriptors and exec()

By default, all file descriptors opened by a program that calls exec() remain
open across the exec() and are available for use by the new program. This is
frequently useful, because the calling program may open files on particular
descriptors, and these files are automatically available to the new program,
without it needing to know the names of, or open, the files.

see *open-file-table*

The shell takes advantage of this feature to handle I/O redirection for the
programs that it executes. For example, suppose the following shell command:

$ ls /tmp > dir.txt

The shell performs the following steps to execute this command:

1. A fork() is performed to create a child process that is also running a copy
of the shell (and thus has a copy of the command).

2. The child shell opens dir.txt for output using file descriptor 1 (standard
output). This can be done in either of the following ways:

a) The child shell closes descriptor 1 (STDOUT_FILENO) and then opens the file
dir.txt. Since open() always uses the lowest available file descriptor, and
standard input (descriptor 0) remains open, the file will be opened on
descriptor 1.

b) The shell opens dir.txt, obtaining a new file descriptor. Then, if that
file descriptor is not standard output, the shell uses dup2() to force
standard output to be a duplicate of the new descriptor and closes the new
descriptor, since it is no longer required.

3. The child shell execs the ls program. The ls program writes its output to
standard output, which is the file dir.txt.


<ex>
This is to two things: parent is to run actual application given and child is to
check if a parent is alive in a loop. If parent is died, do some action.

1. shell runs this line in a script. 
note: this cmd comes from outside to specify the application to run.

   exec $exec_wrapper "$cmd" args...
   
2. $exec_wrapper is an application, exec-then-cleanup-app, which do two things: 
   @ ppid from env is null and set ppid of shell. 
   @ call fork

   @ child sets env with ppid and 'exec' wrapper again. since env is set, child
   runs in a loop while checking if parent is died. If so, 'exec' cleanup
   application with ppid. 

   note: ppid comes from env and argv as well. argv's one is not used.

   @ parent 'exec' $cmd to run the given application such as a browser.

shell: exec(wrapper);

  // 'ppid' is null and set ppid with shell's pid
  wrapper: ppid = getpid();    

  wrapper: fork()
    -> child: setenv(PPID);
       child: exec(wrapper);

       // run 'wrapper' code again but with 'ppid' set this time before exec and
       // this is <inter-program> communication using envronment variable.

       // will have different pid from parent which comes from fork() but not
       // exec.

       wrapper: do watch
       wrapper: if parent is died, exec(cleanup);

    -> parent: exec(application);
       // has the same pid which is the shell's pid and run application code.


note: see how exec used to replace "program code(text)" to run. the shell,
  parent, replace itself with given application and the chain of exec in the
  child.

exec "$prefix/bin/exec-then-cleanup-app" "$prefix/bin/w3cEngine" \
$enable_webkit_remote_debugging_as_needed \
-cache "$app_data_dir/client-cache" \
-cache-size "$cacheSize" \
-jar "$app_data_dir/cookies.sqlite" \
-url "$url" \
-src "$sources"

// wrapper: exec-then-cleanup-app

#include <unistd.h>
#include <signal.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <assert.h>
#include <sys/stat.h>
#include <sys/types.h>

void usage(const char* exec_name)
{
  fprintf(stderr, "Usage: %s cmd [args]\n\n"
                  "Run `cmd` passing it `args` and run inspect"
                  " when that completes\n", basename(exec_name));
}

int main(int argc, char** argv)
{
  pid_t pid;
  int opt = 0;

  // note: getenv returns NULL if an env is not set.

  char* parent_pid_str = getenv("NEXUS_INSPECT_PARENT_PID");
  char* cleanup_exe = PKG_BIN_DIR "/nexus-inspect";

  if (NULL == parent_pid_str && argc < 2)
  {
    usage(argv[0]);
    return 1;
  }

  if (NULL == parent_pid_str)
  {
    // Even if pids were unsigned 64-bit numbers, there could be a maximum
    // of 20 characters in it. We add another for the null terminator.
    char parent_pid[21];
    int ret = snprintf(parent_pid, sizeof(parent_pid), "%i", getpid());
    if (ret >= 20)
    {
      // If you're on a system with pids that are larger than unsigned
      // 64-bit integers, you should seriously consider why you're still
      // using nexus-inspect.
      fprintf(stderr, "Pids are longer than expected (ie %d characters)."
          " What's going on?", ret);
      return 2;
    }

    pid = fork();
    assert(!(pid < 0));
    if (0 == pid)
    {
      // Child.

      // Only launch the watcher if the cleanup app exists.
      struct stat sb;
      if (-1 == stat(cleanup_exe, &sb))
      {
        fprintf(stderr, "Cleanup executable (%s) doesn't exist, so not" 
            " spawning watcher\n", cleanup_exe);
        return 0;
      }

      // Create new group and become group leader.
      if (setpgid(0, 0)) {
        perror("setpgid failed!");
        return 3;
      }

      // "$prefix/bin/exec-then-cleanup-app" "-p" "$parent_pid"
      // args: /opt/zinc-trunk/bin/exec-then-cleanup-app -p 1479 

      // note: ppid via args is not used.

      const char* args[4] = { argv[0], "-p", parent_pid, NULL };

      // We set the pid in the environment. Doing it via environment rather than
      // arguments simplifies the code because we don't have to do any argument
      // parsing - note that we do accept arguments and options, but they are
      // *all* passed on to the exec'd process (see the parent branch, below).

      setenv("NEXUS_INSPECT_PARENT_PID", parent_pid, 1);
      execv(argv[0], (char*const*)args);
    }
    else
    {
      // Parent.
      if (execvp(argv[1], (char*const*)(argv+1)) != 0) {
        perror("exec failed!");
        return 4;;
      }
    }
  }
  else
  {
    // note: even if got ppid in number, have to use ppid string to pass over
    // cleanup_exe

    int parent_pid=atoi(parent_pid_str);
    fprintf(stderr, "Going to watch pid: %d\n", parent_pid);
    while (0 == kill(parent_pid, 0))
    {
      sleep(1);
    }
    fprintf(stderr, "Pid %d has died. Going to cleanup.\n", parent_pid);

    char* args[5] = { cleanup_exe, "-r", "-p", parent_pid_str, NULL };
    return execv(cleanup_exe, args);
  }

  return 0;
}


<ex> pass-arguments
To run "strace -e open /bin/ls" to see open system calls when ls runs, but was
not straightforward. Why?

./a.out /usr/bin/strace

#include <stdio.h>
#include <stdlib.h>

int main(int argc, char* argv[] )
{
    if( argc != 2 || strcmp( argv[1], "--help" ) == 0 )
    {
        printf("%s pathname\n", argv[0] );
        exit(EXIT_FAILURE);
    }

    // const char* args[] = {"ls", "-al", ".", NULL };
    // note: works fine

    // const char* args[] = {"strace", "-e open", "/bin/ls", NULL };
    // note: error when run "strace: invalid system call ` open'"

    const char* args[] = {"strace", "-e", "open", "/bin/ls", NULL };
    // note: works fine

    if( execvp( argv[1], args ) != 0 )
    {
        // if (execvp("/bin/strace", (char*const*)(args)) != 0) {
        perror("exec failed!");
        return 4;;
    }

    exit(EXIT_FAILURE);
}


<ex>
#!/bin/sh
#
# 'logfile-monitor.sh' checks periodically (every $INTERVAL seconds) if there
# has been a change to the debug log file size. If there hasn't (which might be
# indicative of a process stuck in a tight loop) then it will kill the MW, APP,
# and CA processes to obtain their core files.
#
# The script will not kill the SCD process: PWM receives a SIGCHLD signal when
# SCD (its child process) dies which causes it to restart the STB. There is
# also a software watchdog which would eventually trigger.
#
# Before killing the processes the script will capture the output of the 'ps'
# and 'top' commands in files 'PS.TXT' and 'TOP.TXT', respectively. Those files
# will be uploaded automatically as part of the logs and will be available on
# the respective log parser page.
#
# The script also creates two empty files, FORCED.TXT and AUTOKILL.TXT, which
# will enable the log parser to mark the logs appropriately.
#
# After having been triggered and successfully done the above, the script will
# (where it is supported) flash the STB's front panel indicators in one second
# intervals.
#
# The process killing sequence can also be triggered manually by logging onto
# the STB and sending signal SIGHUP to the script, like this:
#
# -sh-3.2# kill -s 1 $(pidof logfile-monitor.sh)
#
# (Note that it might take up to $INTERVAL seconds before the script responds.)
#
# <michael.petzold@sky.uk>
# Version: 22/03/2017
###############################################################################

HANGUP_RECEIVED=false
trap 'HANGUP_RECEIVED=true' SIGHUP

# Function: Manipulate the STB's front panel lights; call with 1=on or 0=off
set_fpi() {
    for index in $(seq 0 6); do
        /usr/bin/cdi-fpi $index $1 1
    done
}

# Function: Flash the STB's front panel lights
blink_fpi() {
    if [ -x /usr/bin/cdi-fpi ]; then
        while true; do
            set_fpi 1
            sleep 1
            set_fpi 0
            sleep 1
        done
    fi
}

# Wait for all the processes to start (also, the new log file doesn't exist yet
# at this point)
sleep 330

# Get mount point (this will set FSN_DATAPATH)
. /usr/bin/bootlog-fsn_data-config.sh
[ -n "${FSN_DATAPATH}" ] || exit 1

# Work out the PID for the PWM process; we need this for the LOG file name
PID_PWM=$(pidof PWM_Process)
[ $? = 0 ] || exit 1

# Find the log file (diag bin only)
LOGFILE=${FSN_DATAPATH}/P${PID_PWM}.CUR
[ -f ${LOGFILE} ] || exit 1

PREV_LOGFILE_SIZE=0
INTERVAL=60

while true; do
    # Get the current log file size
    CUR_LOGFILE_SIZE=$(ls -l ${LOGFILE} | awk '{print $5}')

    # Calculate diff between current and previous log file sizes
    DIFF=$(expr ${CUR_LOGFILE_SIZE} - ${PREV_LOGFILE_SIZE})

    # If no size change, kill off the processes to create core files and exit
    # (Note that $DIFF might be negative if the log has just been auto-rotated)
    if [ $DIFF -eq 0 -o "${HANGUP_RECEIVED}" = "true" ]; then

        # Take a snapshot of the processes
        ps auxf >${FSN_DATAPATH}/PS.TXT
        top -b -n 1 >${FSN_DATAPATH}/TOP.TXT

        # Work out the PIDs for the processes to be killed
        PID_MW=$(pidof MW_Process)
        [ $? = 0 ] || PID_MW=0
        PID_APP=$(pidof APP_Process)
        [ $? = 0 ] || PID_APP=0
        PID_CA=$(pidof CA_Process)
        [ $? = 0 ] || PID_CA=0

        # If one of the processes was already dead (and hence probably the
        # reason that the log has stopped) we don't want to kill the remaining
        # processes in order not to change the failure symptoms
        if [ ${PID_MW} = 0 -o ${PID_APP} = 0 -o ${PID_CA} = 0 ]; then
            exit 0
        fi

        # Kill the processes to generate core files; use SIGSEGV (11)
        for PID in ${PID_MW} ${PID_APP} ${PID_CA}; do
            while (ps $PID >/dev/null) && !(ps $PID | grep -qi "<defunct>"); do
                kill -s 11 $PID
                sleep 10
            done
        done

        # These are not 'natural' crash dumps
        touch ${FSN_DATAPATH}/FORCED.TXT
        touch ${FSN_DATAPATH}/AUTOKILL.TXT

        # Flash the STB's front panel lights to indicate what has happened
        sleep 60
        blink_fpi
        exit 0
    else
        PREV_LOGFILE_SIZE=${CUR_LOGFILE_SIZE}
        # Wait for a bit & then loop again
        sleep $INTERVAL
    fi
done


={============================================================================
*kt_linux_core_110* linux-process-exec-system-call

#include <stdlib.h>
int system(const char *command);

The principal advantages of system() are simplicity and convenience:

* We dont need to handle the details of calling fork(), exec(), wait(), and exit().

* Error and signal handling are performed by system() on our behalf.

* Because system() `uses the shell` to execute command, all of the usual shell
  processing, substitutions, and redirections are performed on command before
  it is executed. This makes it easy to add an "execute a shell command"
  feature to an application. (Many interactive applications provide such a
  feature in the form of a ! command.)

The main cost of system() is inefficiency. Executing a command using system()
requires the creation of at least two processesone for the shell and one or
more for the command(s) it executeseach of which performs an exec(). If
efficiency or speed is a requirement, it is preferable to use explicit fork()
and exec() calls to execute the desired program.


Avoid using system() in set-user-ID and set-group-ID programs

TODO: security example


={============================================================================
*kt_linux_core_110* linux-process-exec-clone-call linux-thread-id

LPI-28.2 The clone() System Call

Linux-specific clone() system call creates a new process. It differs from the
other two calls in allowing finer control over the steps that occur during
process creation. The main use of clone() is in the implementation of
threading libraries.

#define _GNU_SOURCE
#include <sched.h>

int clone(int (*func) (void *), void *child_stack, int flags, void *func_arg, ...
/* pid_t *ptid, struct user_desc *tls, pid_t *ctid */ );

Returns process ID of child on success, or -1 on error

Like fork(), a new process created with clone() is an almost exact duplicate
of the parent. Unlike fork(), the cloned child doesn't continue from the point
of the call, but instead commences by calling the function specified in the
func argument; we'll refer to this as the child function. When called, the
child function is passed the value specified in func_arg. Using appropriate
casting, the child function can freely interpret this argument; for example,
  as an int or as a pointer to a structure.

The clone() flags argument serves two purposes. First, its lower byte
specifies the child's termination signal, which is the signal to be sent to
the parent when the child terminates.

The remaining bytes of the flags argument hold a bit mask that controls the
operation of clone(). We summarize these bit-mask values in Table 28-2.


28.2.1 The clone() flags Argument

<thread-group>
Thread groups: CLONE_THREAD

If the CLONE_THREAD flag is set, then the child is placed in the same thread
group as the parent. If this flag not set, the child is placed in its own new
thread group.

Threads groups were introduced in Linux 2.4 to allow threading libraries to
support the POSIX threads requirement that all of the threads in a process
`share a single process ID` (i.e., getpid() in each of the threads should
    return the `same value`). 

A thread group is a group of KSEs that share the same thread group identifier
(TGID), as shown in Figure 28-1. For the remainder of the discussion of
CLONE_THREAD, we’ll refer to these KSEs as threads.

Since Linux 2.4, getpid() returns the calling thread’s TGID. In other words, a
`TGID is the same thing as a process ID`

Each thread within a thread group is distinguished by a unique thread
identifier (TID). Linux 2.4 introduced a new system call, `gettid()`, to allow
a thread to obtain its own thread ID (this is the same value as is returned to
    the thread that calls clone()). A thread ID is represented using the same
data type that is used for a process ID, pid_t. Thread IDs are unique
system-wide, and the kernel guarantees that no thread ID will be the same as
any process ID on the system, except when a thread is the thread group leader
for a process.

The first thread in a new thread group has a thread ID that is the same as its
  thread group ID. This thread is referred to as `the thread group leader`

note:
The thread IDs that we are discussing here `are not the same` as the thread IDs
(the `pthread_t` data type) used by POSIX threads. The latter identifiers are
generated and maintained internally (in user space) by a POSIX threads
implementation.

<ex> *tool-ps* *linux-tid*
-sh-3.2# ps -eLf
UID        PID  PPID   LWP  C NLWP STIME TTY          TIME CMD
NDS_U5     581   130   581  0   15 Jan01 ?        00:00:03 /NDS/bin/PWM_Process
root       581   130   582  0   15 Jan01 ?        00:01:00 /NDS/bin/PWM_Process
root       581   130   583  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process
root       581   130   584  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process
root       581   130   585  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process
root       581   130   586  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process
root       581   130   587  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process
root       581   130   588  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process
root       581   130   589  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process
NDS_U5     581   130   590  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process
NDS_U5     581   130   591  0   15 Jan01 ?        00:00:04 /NDS/bin/PWM_Process
NDS_U5     581   130   592  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process
NDS_U5     581   130   593  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process
NDS_U5     581   130   594  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process
NDS_U5     581   130   595  0   15 Jan01 ?        00:00:00 /NDS/bin/PWM_Process


<why-equal-function-needed> *opaque-data*
Each thread within a process is uniquely identified by a thread ID. This ID is
returned to the caller of pthread_create(), and a thread can obtain its own ID
using pthread_self().

int pthread_equeal( pthread_t tid, pthread_t tid );
Returns nonzero value if t1 and t2 are equal, otherwise 0

pthread_t pthread_self(void);

All pthread data type should be treated 'opaque' data. On Linux, pthread_t
happens to be defined as an unsigned long, but on other implementations, it
could be a pointer or a structure.

In NPTL, pthread_t is actually a pointer that has been cast to unsigned long.

Two problems:

1. Therefore, we can't 'portably' use code such as the following to display a
thread ID. Though it does work on many implementations, including Linux, and
is sometimes useful for debugging purposes:

pthread_t thr;
printf("Thread ID = %ld\n", (long) thr);

2. In the Linux threading implementations, thread IDs are unique across
processes. However, this is 'not' necessarily the case on other
implementations


{posix-TID-and-kernel-tid}
POSIX thread IDs are `not the same` as the thread IDs returned by the Linux
specific gettid() system call. POSIX thread IDs are assigned and maintained by
the threading implementation. The thread ID returned by gettid() is a number
(similar to a process ID) that is assigned by the kernel. 

Although each POSIX thread has a unique kernel thread ID in the Linux NPTL
implementation, an application generally doesn't need to know about the kernel
IDs and won't be portable if it depends on knowing them.


<pthread-self>
NAME
       pthread_self - obtain ID of the calling thread

NOTES
       POSIX.1  allows  an  implementation  wide  freedom  in  choosing  the
       type used to represent a thread ID; for example, representation using
       either an arithmetic type or a structure is permitted.  Therefore,
       variables of type pthread_t can't portably be compared using the C
       equality operator (==); use pthread_equal(3) instead.

       Thread identifiers should be considered opaque: any attempt to use a
       thread ID other than in pthreads calls is nonportable and can lead to
       unspecified results.

       Thread IDs are guaranteed to be unique only within a process.  A thread
       ID may be reused after a terminated thread has been joined, or a detached
       thread has terminated.

       note:
       The thread ID returned by pthread_self() is not the same thing as the
       kernel thread ID returned by a call to gettid(2).

<call-gettid> *linux-tid*
SYNOPSIS
       #include <sys/types.h>
       #include <unistd.h>

       pid_t gettid(void);

       Note: There is no glibc wrapper for this system call; see NOTES.

NOTES
       `Glibc does not provide a wrapper for this system call`; call it using
       syscall(2).

       The thread ID returned by this call is not the same thing as a POSIX
       thread ID (i.e., the opaque value returned by pthread_self(3)).

<ex> *call-syscall*
#include <sys/syscall.h>

static void * threadFunc(void *arg)
{
    printf("sleeps for 30s, getpid=%ld, gettid=%ld\n", getpid(), syscall(SYS_gettid));
}


={============================================================================
*kt_linux_core_103* linux-process-priority-and-schedule 

35.1 Process Priorities (Nice Values)

{nice-value} {sched-other}

low	high
0     99 		100   					139
					-20 		0 				+19
					high 		default 		low nice value

0  -99  : realtime priority. static and can set when create a process

100-139 : user priority. can use nice command which uses with value from -20
to +19. The default nice value is 0. note: does it mean 120 is default? 100 is
highest?

Processes are not scheduled in a strict hierarchy by nice value; rather, the
nice value acts as weighting factor that causes the kernel scheduler to favor
processes with higher priorities.

Starting in kernel 2.6.23, a new kernel scheduling algorithm means that
relative differences in nice values have a much stronger effect than in
previous kernels.  As a result, processes with low nice values receive less
CPU than before, and processes with high nice values obtain a greater
proportion of the CPU.

Retrieving and modifying priorities

The getpriority() and setpriority() system calls allow a process to retrieve
and change its own nice value or that of another process.


{realtime-and-latency}
POSIX realtime versus hard realtime

POSIX API merely provides us with so-called soft realtime, allowing us to
control which processes are scheduled for use of the CPU.

The aim to distribute fairly CPU resource to all process on a system is not
realtime approach.

This is why most UNIX kernels-including, historically, Linux-have not natively
supported realtime applications. Nevertheless, starting from around version
2.6.18, various features have been added to the Linux kernel with the eventual
aim of allowing Linux to natively provide full support for hard realtime
applications, without imposing the aforementioned overhead for time-sharing
operation.


<latency-components>

interrupt      ISR         ISR signals       user process
event          runs        user process      runs
 |             |              |                 |
---------------------------------------------------------> time
   interrupt      interrupt      scheduling
   latency        processing     latency

<linux-scheduler>
Before Linux 2, kernel didn't support preemption which means that no other
process can run in kernel mode when one user process is already in the kernel
mode until that is bloked or finishes its work.

O(1) scheduler from Linux 2.5 and supports constant scheduling decision
regardless of the number of process.


{sched-realtime}
35.2 Overview of Realtime Process Scheduling

SUSv3 specifies a realtime process scheduling API (originally defined in
POSIX.1b) that partly addresses these requirements. This API provides two
realtime scheduling policies: SCHED_RR and SCHED_FIFO. Processes operating
under either of these policies always have priority over processes scheduled
using the standard round-robin timesharing policy described in Section 35.1,
which the realtime scheduling API identifies using the constant SCHED_OTHER.

Linux provides 99 realtime priority levels, 1 (lowest) to `99 (highest)`, and
this range applies in both realtime scheduling policies. The priorities in
each policy are equivalent. This means that, given two processes with the same
priority, one operating under the SCHED_RR policy and the other under
SCHED_FIFO, either may be the next one eligible for execution, depending on
the order in which they were scheduled.

In effect, each priority level maintains a queue of runnable processes, and
the next process to run is selected from the front of the highest-priority
nonempty queue.


<schedule-policy>
(from ~/include/linux/sched.h)
/*
 * Scheduling policies
 */

// normal user process. fairness
// SCHED_NORMAL(OTHER) is default.
#define SCHED_NORMAL 0     

// realtime and run the highest priority. On the same priority, the first runs
// until it's blocked. So it is realtime `without` time slice.
#define SCHED_FIFO   1     

// realtime and run the highest priority. On the same priority, the first runs
// but in the time sliced. So it is realtime with time slice.
#define SCHED_RR     2


<sched-rr>
35.2.1 The SCHED_RR Policy

Once scheduled, a process employing the SCHED_RR policy maintains control of
the CPU until either:

  * it reaches the end of its time slice;
  * it voluntarily relinquishes the CPU, either by performing a blocking
    system call or by calling the sched_yield() system call.
  * it terminates; or
  * it is preempted by a higher-priority process.
 
For the first two events above, when a process running under the SCHED_RR
policy loses access to the CPU, it is placed at the back of the queue for its
priority level. In the final case, when the higher-priority process has ceased
execution, the preempted process continues execution, consuming the remainder
of its time slice (i.e., the `preempted process remains` at the head of the
    queue `for its priority level`).

In both the SCHED_RR and the SCHED_FIFO policies, the currently running
process may be preempted for one of the following reasons:

  * a higher-priority process that was blocked became unblocked (e.g., an I/O
    operation on which it was waiting completed);

  * the priority of another process was raised to a higher level than the
    currently running process; or

  * the priority of the currently running process was decreased to a lower
    value than that of some other runnable process.


<different-from-sched-other>
The SCHED_RR policy is similar to the standard round-robin time-sharing
scheduling algorithm (SCHED_OTHER), in that it allows a group of processes
with the same priority to share access to the CPU. 

The most notable difference is the existence of `strictly` distinct priority
levels, with higher-priority processes always taking precedence over
lower-priority processes. By contrast, a low nice value (i.e., high priority)
  doesn’t give a process exclusive access to the CPU; it merely gives the
  process a favorable weighting in scheduling decisions. As noted in Section
  35.1, a process with a low priority (i.e., high nice value) always receives
  at least some CPU time.

The other important difference is that the SCHED_RR policy allows us to
precisely control `the order` in which processes are scheduled.


<sched-batch>
35.2.3 The SCHED_BATCH and SCHED_IDLE Policies

The Linux 2.6 kernel series added two nonstandard scheduling policies:
SCHED_BATCH and SCHED_IDLE.


<sched-api>
#include <sched.h>
int sched_get_priority_min(int policy);
int sched_get_priority_max(int policy);

Both return nonnegative integer priority on success, or -1 on error

// sched.c
/**
 * sys_sched_get_priority_max - return maximum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the maximum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_max(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = MAX_USER_RT_PRIO-1;
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
                break;
        }
        return ret;
}

/**
 * sys_sched_get_priority_min - return minimum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the minimum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_min(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = 1;
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
        }
        return ret;
}

The range of realtime priorities differs from one UNIX implementation to
another. Therefore, instead of hard-coding priority values into an
application, we should specify priorities relative to the return value from
one of these functions.


35.3.2 Modifying and Retrieving Policies and Priorities

The sched_setscheduler() system call changes both the scheduling policy and
the priority of the process whose process ID is specified in pid. If pid is
specified as 0, the attributes of the calling process are changed.

#include <sched.h>
int sched_setscheduler(pid_t pid, int policy, const struct sched_param *param);

<ex>
/* from sched.h.
#define MAX_USER_RT_PRIO   100
#define MAX_RT_PRIO        MAX_USER_RT_PRIO
*/

#define MY_RT_PRIORITY MAX_USER_RT_PRIO /* Highest possible */

int main(int argc, char **argv)
{
  ...
  int rc, old_scheduler_policy;
  struct sched_param my_params;
  ...

  /* Passing zero specifies caller's (our) policy */
  old_scheduler_policy = sched_getscheduler(0);
  my_params.sched_priority = MY_RT_PRIORITY;

  /* Passing zero specifies callers (our) pid */
  rc = sched_setscheduler(0, SCHED_RR, &my_params);
  if ( rc == -1 )
    handle_error();
  ...
}

A successful sched_setscheduler() call moves the process specified by pid to
the `back of the queue for its priority level.`

The scheduling policy and priority are inherited by a child created via
fork(), and they are preserved across an exec().

To change the policy after a boot:

If scheduling is modified before insmod-ing callisto BCM drivers, tasks
inherit changed scheduling. In that case last two task mods can be dropped.
The list is quite aggressive as changing policy of kthread affects all new
kernel threads. Fine tuning would obviously have to be done along with BCM.

(about ways to change from NORMAL to RR)
http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0182.html
http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0330.html


The sched_setparam() system call provides a subset of the functionality of
sched_setscheduler(). It modifies the scheduling priority of a process while
leaving the policy unchanged.

#include <sched.h>
int sched_setparam(pid_t pid, const struct sched_param *param);


Privileges and resource limits affecting changes to scheduling parameters

Since kernel 2.6.12, the rules about setting realtime scheduling policies and
priorities have changed with the introduction of a new, nonstandard resource
limit, RLIMIT_RTPRIO. As with older kernels, privileged (CAP_SYS_NICE)
  processes can make arbitrary changes to the scheduling policy and priority
  of any process. However, an unprivileged process can also change scheduling
  policies and priorities, according to the following rules:


={============================================================================
*kt_linux_core_110* linux-process-resource

LPI 36 Process Resources

36.1 Process Resource Usage

36.2 Process Resource Limits

Each process has a set of resource limits that can be used to restrict the
amounts of various system resources that the process may consume. 

For example, we may want to set resource limits on a process before execing an
arbitrary program, if we are concerned that it may consume excessive
resources. 

We can set the resource limits of the shell using the ulimit built-in command
(limit in the C shell). These limits are `inherited` by the processes that the
shell creates to execute user commands.

Since kernel 2.6.24, the Linux-specific `/proc/PID/limits` file can be used to
view all of the resource limits of any process. This file is owned by the real
user ID of the corresponding process and its permissions allow reading only by
that user ID (or by a privileged process).

$ cat /proc/3576/limits 
Limit                     Soft Limit           Hard Limit           Units     
Max cpu time              unlimited            unlimited            seconds   
Max file size             unlimited            unlimited            bytes     
Max data size             unlimited            unlimited            bytes     
Max stack size            8388608              unlimited            bytes     
Max core file size        0                    unlimited            bytes     
Max resident set          unlimited            unlimited            bytes     
Max processes             3941                 3941                 processes 
Max open files            1024                 4096                 files     
Max locked memory         65536                65536                bytes     
Max address space         unlimited            unlimited            bytes     
Max file locks            unlimited            unlimited            locks     
Max pending signals       3941                 3941                 signals   
Max msgqueue size         819200               819200               bytes     
Max nice priority         0                    0                    
Max realtime priority     0                    0                    
Max realtime timeout      unlimited            unlimited            us        


The getrlimit() and setrlimit() system calls allow a process to fetch and
modify its resource limits.

#include <sys/resource.h>
int getrlimit(int resource, struct rlimit *rlim);
int setrlimit(int resource, const struct rlimit *rlim);

struct rlimit {
  rlim_t rlim_cur; /* Soft limit (actual process limit) */
  rlim_t rlim_max; /* Hard limit (ceiling for rlim_cur) */
};

These fields correspond to the two associated limits for a resource: the soft
(rlim_cur) and hard (rlim_max) limits. (The rlim_t data type is an integer
    type.) 

The soft limit governs the amount of the resource that may be consumed by the
process. A process can adjust the soft limit to any value from 0 up to the hard
limit. For most resources, the sole purpose of the hard limit is to provide this
'ceiling' for the soft limit.

They are inherited by child processes created via fork() and are preserved
across an exec().


Although a resource limit is a per-process attribute, `in some cases`, the
limit is measured against not just that process’s consumption of the
corresponding resource, but also against the sum of resources consumed `by all
processes` with the same real user ID. The RLIMIT_NPROC limit, which places a
limit on the number of processes that can be created, is a good example of the
rationale for this approach.

If not otherwise specified, then a resource limit is measured 'only' against the
process's own consumption of the resource.


36.3 Details of Specific Resource Limits

In this section, we provide details on each of the resource limits available on
Linux, noting those that are Linux-specific.

RLIMIT_NOFILE

The RLIMIT_NOFILE limit specifies a number one greater than the maximum file
descriptor number that a process may allocate.

There is also a `system-wide limit` on the total number of files that may be
`opened by all processes.` This limit can be retrieved and modified via the
Linux-specific /proc/sys/fs/file-max file.


RLIMIT_AS

The RLIMIT_AS limit specifies the maximum size for the process's virtual memory
(address space), in bytes. Attempts (brk(), sbrk(), mmap(), mremap(), and
    shmat()) to exceed this limit fail with the error ENOMEM. In practice, the
most common place where a program may hit this limit is in calls to functions in
the malloc package, which make use of sbrk() and mmap(). Upon encountering this
limit, stack growth can also fail with the consequences listed below for
RLIMIT_STACK.


RLIMIT_STACK

The RLIMIT_STACK limit specifies the maximum size of the process stack, in
bytes. Attempts to grow the stack beyond this limit result in the generation of
a `SIGSEGV` signal for the process. Since the stack is exhausted, the only way
to catch this signal is by establishing an alternate `signal-stack`, as
described in Section 21.3.

Since Linux 2.6.23, the RLIMIT_STACK limit also determines the amount of space
available for holding the process's command-line arguments and environment
variables. See the execve(2) manual page for details.


<case>
The daemon crashes when it shows huge numbers in vm memory status and shows a
call trace in coredump. Believe that it has no memory to create a thread stack.
See RLIMIT_STACK and SIGSEGV. *sigseg*

VmPeak:	 1893244 kB
VmSize:	 1535856 kB

#0  0x77399418 in stack_list_add () from /lib/libpthread.so.0
#1  0x7739af1c in pthread_create () from /lib/libpthread.so.0
#2  0x773e1488 in direct_thread_create () from /usr/local/lib/libdirect.so
#3  0x76d6cb70 in fusion_enter () from /usr/local/lib/libfusion-1.4.so.17
#4  0x7742e05c in dfb_core_create () from /usr/local/lib/libdirectfb.so
#5  0x774100a8 in DirectFBCreate () from /usr/local/lib/libdirectfb.so

libpthread/nptl/pthread_create.c

int
__pthread_create_2_1 (
     pthread_t *newthread,
     const pthread_attr_t *attr,
     void *(*start_routine) (void *),
     void *arg)
{
  STACK_VARIABLES;

  const struct pthread_attr *iattr = (struct pthread_attr *) attr;
  if (iattr == NULL)
    /* Is this the best idea?  On NUMA machines this could mean
       accessing far-away memory.  */
    iattr = &default_attr;

  struct pthread *pd = NULL;
  int err = ALLOCATE_STACK (iattr, &pd);
  ...
}

static int
allocate_stack (const struct pthread_attr *attr, struct pthread **pdp,
		ALLOCATE_STACK_PARMS)
{
	  /* And add to the list of stacks in use.  */
	  stack_list_add (&pd->list, &stack_used);
}


={============================================================================
*kt_linux_core_107* process: group

LPI 34.

{why-group-and-session}
A process group is a collection of 'related' processes, and a session is a collection of 'related'
process groups. Process groups and sessions are abstractions defined to 'support' two uses:

1. shell job control, which allows interactive users to run commands in the foreground or in the
background. note: So need to know fine details when write a shell.

2. send a signal to a group.


{process-group} also known as job
A process group is a set of one or more processes sharing the same process group identifier (PGID).
A process group ID is a number of the same type (pid_t) as a process ID. A process group has a
process group 'leader', which is the process that 'creates' the group and whose process ID becomes
the process group ID of the group. A new process inherits its parent's process group ID.

note: the group leader has PID == PGID.

A process group has a lifetime, which is the period of time beginning when the leader creates the
group and ending when the last member process leaves the group.


{process-session}
A session is a collection of process groups. A process's session membership is determined by its
session identifier (SID), which, like the process group ID, is a number of type pid_t. A session
'leader' is the process that 'creates' a new session and whose process ID becomes the session ID. A
new process inherits its parent's session ID.

note: the session leader has PID == PGID == SID. That is group and session leader.


{controlling-process-and-terminal}
All of the processes in a session 'share' a single controlling terminal. The controlling terminal is
established when the 'session' leader first 'opens' a terminal device, /dev/tty. A terminal may be
the controlling terminal of at most 'one' session. Opening the controlling terminal also causes the
session leader to become the 'controlling' process for the terminal.

<forground-process-group>
At any point in time, one of the process groups in a session is the foreground process group for the
terminal, and the others are background process groups. 'only' processes in the foreground process
group can read input from the controlling terminal. note: process'es'

<signal-forground-group>
When the user types one of the signal-generating terminal characters on the controlling terminal, a
signal is sent to 'all' members of the 'foreground' process group. These characters are the
interrupt character (usually Control-C), which generates SIGINT; the quit character (usually
    Control-\), which generates SIGQUIT; and the suspend character (usually Control-Z), which
    generates SIGTSTP.


{use-other-case} 
Process groups occasionally find uses in areas other than job control, since they have two useful
properties: a parent process can wait on any of its children in a particular process group (Section
  26.1.2), and a signal can be sent to all of the members of a process group.

note: Use kill system call and see <signal-to-process-group> for detail.


{use-job-control} login-shell
Why session and group? Sessions and process groups were defined to support 'shell' job control

For an interactive login, the controlling terminal is the one on which the user logs in. The login
shell becomes the session leader and the controlling process for the terminal, and is also made the
'sole' member of its own process group. 

'each' job(a simple command or pipeline of commands) started from the shell results in the creation
of one or more processes, and the shell places all of these processes in a 'new' process group.

$ echo $$                              " Display the PID of the shell
400
$ find / 2> /dev/null | wc -l &        " Creates 2 processes in background group
[1] 659
$ sort < longlist | uniq -c            " Creates 2 processes in foreground group

<---------------------------- session 400 ----------------------------------->
bash (session/group leader)      find (group leader)        sort(group leader)
   PID  = 400                       PID  = 658                 PID  = 660
   PPID = 399                       PPID = <400>               PPID = <400>
   PGID = 400                       PGID = 658                 PGID = 660
   SID  = 400                       SID  = 400                 SID  = 400

                                 wc                         uniq
                                    PID  = 659                 PID  = 661
                                    PPID = <400>               PPID = <400>
                                    PGID = 658                 PGID = 660
                                    SID  = 400                 SID  = 400

<-------------------------->    <-------------------->    <------------------>
process group 400                process group 658          process group 660
'controlling' process
background process group         background group           forground group

<controlling-terminal>
Forground PGID    = 660
Controlling SID   = 400


{group}
<calls>
The setpgid() system call changes the process group of the process whose process ID is pid to the
value specified in pgid. Put simply, change pgid of a process with pid.

#include <unistd.h>
pid_t getpgrp(void);
Always successfully returns process group ID of calling process

int setpgid(pid_t pid, pid_t pgid);
Returns 0 on success, or -1 on error

If pid is specified as 0, the 'calling' process's process group ID is changed. If pgid is specified
as 0, then the process group ID of the process specified by pid is made the same as its process ID.
Thus, the following setpgid() calls are equivalent:

setpgid(0, 0);
setpgid(getpid(), 0);
setpgid(getpid(), getpid());

note: Put simply, this call is to change pgid of a process with the pid. However, when pid is 0, it
is for the calling process.

If the pid and pgid arguments specify the 'same' process, then a 'new' process group is created, and
the specified process is made the 'leader' of the new group. The typical callers of setpgid() and
setsid() are programs such as the shell 

If the two arguments specify different values, then setpgid() is being used to move a process
between process groups.

<group-creation>
The setpgid(0,0) is a way to create a new group and a group leader once a process is created such as
fork as shown example below.

However, from LPI 34-1 code:

childPid = fork();
switch (childPid) 
{
  case -1: /* fork() failed */
  /* Handle error */

  case 0: /* Child */
    if (setpgid(0, pipelinePgid) == -1)
    /* Handle error */
    /* Child carries on to exec the required program */

   default: /* Parent */
      if (setpgid(childPid, pipelinePgid) == -1 && errno != EACCES)
        /* Handle error */
        /* Parent carries on to do other things */
}

and 

Each job(a simple command or pipeline of commands) started from the shell results in the creation of
one or more processes, and the shell places all of these processes in a 'new' process group.

note: Q? 'not' clear when and how a group is created? The <example> seems to make more sense.

note: The above code is to show "How a job-control shell sets the process group ID of a child
process" because the scheduling of the parent and child is indeterminate after a fork(). 

Therefore, job-control shells are programmed so that the parent and the child process both call
setpgid() to change the childâs process group ID to the same value immediately after a fork(), and
the parent ignores any occurrence of the EACCES error on the setpgid() call.

So seems that the group creation is done before doing this.

<restrictions>
1. The pid argument may specify only the calling process or one of its children. Violation of this
rule results in the error ESRCH.

2. A process may 'not' change the process group ID of one of its children 'after' that child has
performed an exec(). Violation of this rule results in the error EACCES. The rationale for this
constraint is that it could confuse a program if its process group ID were changed after it had
commenced.

This restriction affects the programming of job-control shells, which have the following
requirements:

<sent-by-shell>
All of the processes in a job (i.e., a command or a pipeline) must be placed in a single process
group. This step permits the 'shell' to use killpg() (or, kill() with a negative pid argument) to
simultaneously send job-control signals to 'all' of the members of the process group. Naturally this
step must be carried out before any job-control signals are sent.

killpg - send signal to a process group

#include <signal.h>
int killpg(int pgrp, int sig);

<example>
The thing is that when a shell runs this line, creates a child process to run this application and
in this application, it make a new background group and make it itself a group leader.

note: In the first time, scripts runs "else" and set PPID with the pid that runs this script

#!/bin/bash

# to clean up A/V resources when a process dies, especially on a crash.
#
this_script=$0
prefix=@prefix@
parent_pid=${NEXUS_INSPECT_PARENT_PID:-}

if [ -n "${parent_pid}" ];
then
        echo "Going to watch pid: ${parent_pid}"
        while kill -0 "${parent_pid}" &>/dev/null;
        do
                usleep 500
        done
        echo "Pid ${parent_pid} has died. Going to cleanup."
        ${prefix}/bin/nexus-inspect -r -p "${parent_pid}"
else
        [ -x "${prefix}/bin/nexus-inspect" ] && \
            NEXUS_INSPECT_PARENT_PID=$$ \
               ${prefix}/bin/setpgid-and-exec bash "${this_script}" &
        exec "${@}"
fi


/*
 * The purpose of this simple program is to launch a program in a new process
 * group and have it become the group leader.
 *
 * This is useful in situations where the launched program fork()s and we want
 * to be sure that when we kill it, all of it's children are also killed.
 */

note: Q? does this mean that if kill group leader then all children will be killed automatically? 

#include <unistd.h>
#include <error.h>
#include <stdio.h>

int main(int argc, char* argv[]) {

  if (argc < 2) {
    fprintf(stderr, "Usage: %s <program> [ <arg> ... ]", argv[0]);
    exit(1);
  }

  /* Create new group and become group leader */
  if (setpgid(0, 0)) {
    perror("setpgid failed!");
    exit(2);
  }

  /* Execute the command */
  if (execvp(argv[1], (char*const*)(argv+1)) != 0) {
    perror("exec failed!");
    exit(3);
  }
}


{session}
<calls>
If the calling process is 'not' a process group leader, setsid() creates a 'new' session.

#include <unistd.h>
pid_t setsid(void);

Returns session ID of new session, or (pid_t) -1 on error

The setsid() system call creates a new session as follows:

1. The calling process becomes the leader of a new session, and is made the leader of a new process
group within that session. The calling processâs process group ID and session ID are set to the same
value as its process ID. note: this is a login shell example.

2. The calling process has 'no' controlling terminal. Any previously existing connection to a
controlling terminal is broken. note: Upon creation, a session has no controlling terminal

<restriction>
If the calling process is a process group leader, setsid() fails with the error EPERM. The simplest
way of ensuring that this doesn't happen is to perform a fork() and have the parent exit while the
child carries on to call setsid(). Since the child inherits its parent's process group ID and
receives its own unique process ID, it can't be a process group leader.

The restriction that group leader cannot call setsid() is necessary because, without it, the process
group leader would be able to place itself in another (new) session,

<example>
Shows the use of setsid() to create a new session. To check that it no longer has a controlling
terminal, this program attempts to open the special file /dev/ tty

$ ps -p $$ -o 'pid pgid sid command'      # $$ is PID of shell
PID PGID SID COMMAND
12243 12243 12243                         # bash PID, PGID, and SID of shell

$ ./t_setsid
$ PID=12352, PGID=12352, SID=12352
ERROR [ENXIO Device not configured] open /dev/tty

As can be seen from the output, the process successfully places itself in a new process group within
a new session. Has no controlling terminal, the open() call fails.

int
main(int argc, char *argv[])
{
  if (fork() != 0) /* Exit if parent, or on error */
    _exit(EXIT_SUCCESS);

  if (setsid() == -1)
    errExit("setsid");

  printf("PID=%ld, PGID=%ld, SID=%ld\n", (long) getpid(),
      (long) getpgrp(), (long) getsid(0));

  if (open("/dev/tty", O_RDWR) == -1)
    errExit("open /dev/tty");

  exit(EXIT_SUCCESS);
}

note: From this example, shell is not a group leader and this shows how a shell creates a new
session.


{SIGHUP-signal}
SIGHUP
When a terminal disconnect (hangup) occurs, this signal is sent to the controlling process of the
terminal. A second use of SIGHUP is with daemons (e.g., init, httpd, and inetd). Many daemons are
designed to respond to the receipt of SIGHUP by reinitializing themselves and rereading their
configuration files. The system administrator triggers these actions by manually sending SIGHUP to
the daemon, either by using an explicit kill command or by executing a program or script that does
the same.

The default action of SIGHUP is to terminate a process (by kernel). The delivery of SIGHUP to the
controlling process can set off a kind of chain reaction, resulting in the delivery of SIGHUP to
many other processes. 

This may occur in two ways:

First, in a login session, the shell is normally the controlling process for the terminal.  Most
shells are programmed so that, when run interactively, they establish a handler for SIGHUP. This
handler terminates the shell, but beforehand sends a SIGHUP signal to each of the process groups
(both foreground and background) created by the shell. 

note: sends a SIGHUP signal to only groups that the shell created. The below example shows that.

note: NOT understand this in 34.6.1 since if not handle SIGHUP then kernel will terminate all
anyway. So why install a hander and a shell send a signal? 

<because> Since only a foreground process can read input from the controlling terminal and receive
terminal generated signals. 

Second, if delivery of SIGHUP results in termination of a controlling process, then the kernel also
sends SIGHUP to all of the members of the 'foreground' process group of the controlling terminal.

note: SIGHUP is to terminate by default. So both terminates all children.

<example>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <signal.h>

static void
handler(int sig)
{
}

int main(int argc, char *argv[])
{
  pid_t childPid;
  struct sigaction sa;

  setbuf(stdout, NULL); /* Make stdout unbuffered */

  sigemptyset(&sa.sa_mask);
  sa.sa_flags = 0;
  sa.sa_handler = handler;

  if (sigaction(SIGHUP, &sa, NULL) == -1)
  {
    fprintf(stderr, "error: sigaction\n");
    exit(1);
  }

  childPid = fork();
  if (childPid == -1)
  {
    fprintf(stderr, "error: fork\n");
    exit(1);
  }

  if (childPid == 0 && argc > 1)
    if (setpgid(0, 0) == -1) /* Move to new process group */
    {
      fprintf(stderr, "error: setpgid\n");
      exit(1);
    }

  printf("PID=%ld; PPID=%ld; PGID=%ld; SID=%ld\n", (long) getpid(),
      (long) getppid(), (long) getpgrp(), (long) getsid(0));

  /* An unhandled SIGALRM ensures this process will die if nothing else terminates it */
  alarm(60);

  for(;;) {
    /* Wait for signals */
    pause();
    printf("%ld: caught SIGHUP\n", (long) getpid());
  }
}

The output on debian linux:

kpark@wll1p04345:~/work$ ps -o pgid $$
 PGID
 7523

kpark@wll1p04345:~/work$ echo $$
7523

kpark@wll1p04345:~/work$ ./a.out > samegroup.log 2>&1 &
[1] 16098
kpark@wll1p04345:~/work$ ./a.out x > diffgroup.log 2>&1  
[1]+  Alarm clock             ./a.out > samegroup.log 2>&1
Alarm clock

kpark@wll1p04345:~/work$ cat samegroup.log 
PID=16098; PPID=7523; PGID=16098; SID=7523         " shell, 7523 created a new process and group
PID=16099; PPID=16098; PGID=16098; SID=7523        " child

kpark@wll1p04345:~/work$ cat diffgroup.log         
PID=16231; PPID=7523; PGID=16231; SID=7523         " shell created a new process and group.
PID=16232; PPID=16231; PGID=16232; SID=7523        " child created a new group as well.

note: Unlike example output in 34.6.1, there are no output like:

$ cat samegroup.log
PID=5612; PPID=5611; PGID=5611; SID=5533  "child. This example shows that child runs first.
PID=5611; PPID=5533; PGID=5611; SID=5533  "parent
5611: caught SIGHUP
5612: caught SIGHUP

WHY NOT? The problem is the above application is not a shell and bash doc says:

The shell exits by default upon receipt of a SIGHUP. Before exiting, an interactive shell resends
the SIGHUP to all jobs, running or stopped. Stopped jobs are sent SIGCONT to ensure that they
receive the SIGHUP.

So when runs above commands, exit the shell by pressing C-D or close terminal. The output is:

kpark@wll1p04345:~/work$ cat samegroup.log 
PID=12104; PPID=7523; PGID=12104; SID=7523
PID=12105; PPID=12104; PGID=12104; SID=7523
12104: caught SIGHUP
12105: caught SIGHUP

kpark@wll1p04345:~/work$ cat diffgroup.log 
PID=15299; PPID=7523; PGID=15299; SID=7523
PID=15300; PPID=15299; PGID=15300; SID=7523
15299: caught SIGHUP
15299: caught SIGHUP


{session-leader-is-controlling-process}
As a consequence of establishing the connection to (i.e., opening) the controlling terminal, the
'session' 'leader' becomes the 'controlling' process for the terminal.

<disconnect>
The principal significance of being the controlling process is that the 'kernel' sends 'this'
process a SIGHUP signal if a terminal 'disconnect' occurs. note: say only SIGHUP.

<termination>
As for the termination of the controlling process, the following steps occur: The kernel sends a
SIGHUP signal (and a SIGCONT signal) to all members of the foreground process group, to inform them
of the loss of the controlling terminal.


{io-on-controlling-terminal}
The controlling terminal is inherited by the child of a fork() and preserved across an exec(). When
a session leader opens a controlling terminal, it becomes controlling process.

<terminal-driver>
To support job control, the terminal driver maintains a record of the foreground process group for
the controlling terminal. The terminal driver delivers job-control signals to the foreground job
when certain characters are typed. These signals either terminate or stop the foreground job.

<read>
The notion of the terminal's foreground job is also used to arbitrate terminal I/O requests. Only
processes in the foreground job may read from the controlling terminal. Background jobs are
prevented from reading by delivery of the SIGTTIN signal, whose default action is to stop the job. 

note: process'es'

SIGTTIN
When running under a job-control shell, the terminal driver sends this signal to a background
process 'group' when it attempts to read() from the terminal. This signal stops 'a' process by
default.

note: group but not process? when one of process in background group then get this signal.

<write>
If the terminal TOSTOP is set, then background jobs are also prevented from writing to the
controlling terminal by delivery of a SIGTTOU signal, whose default action is to stop the job.

SIGTTOU
This signal serves an analogous purpose to SIGTTIN, but for terminal output by background jobs. When
running under a job-control shell, if the TOSTOP (terminal output stop) option has been enabled for
the terminal (perhaps via the command stty tostop), the terminal driver sends SIGTTOU to a
background process group when it attempts to write() to the terminal (see Section 34.7.1).  This
signal stops a process by default.


={============================================================================
*kt_linux_core_108* linux-process-daemon

LPI 37.

{characteritics}
A daemon is a process with the following characteristics:

* It is long-lived. Often, a daemon is created at system startup and runs
  until the system is shut down.

* It runs in the background and has `no controlling terminal.` The lack of a
  controlling terminal ensures that the kernel 'never' automatically generates
  any job-control or terminal-related signals (such as SIGINT, SIGTSTP, and
  SIGHUP) for a daemon.

* Daemons are written to carry out specific tasks
 
* Many standard daemons run as privileged processes (i.e., effective user ID
  of 0),

* It is a 'convention' (not universally observed) that daemons have names
  ending with the letter d.

  On Linux, certain daemons are run as `kernel threads.` The code of such
  daemons is part of the kernel, and they are typically created during system
  startup.  When listed using ps(1), the names of these daemons are surrounded
  by square brackets ([]). One example of a kernel thread is pdflush, which
  periodically flushes dirty pages (e.g., pages from the buffer cache) to
  disk.

root      3241  0.0  0.0      0     0 ?        S    21:13   0:00 [kworker/0:0]
root      3288  0.0  0.0      0     0 ?        S    21:18   0:00 [kworker/0:2]
kyoupark  3298  0.0  0.2  19100  2508 pts/0    R+   21:21   0:00 ps aux


{to-create-daemon}
37.2 Creating a Daemon

1. Perform a fork(), after which the parent exits and the child continues. As
a consequence, the daemon becomes a child of the init process. see
*linux-process-zombie*

This is done for two reasons:

  Assuming the daemon was started from the command line, the parent's
  termination is noticed by the shell, which then displays another shell
  prompt and leaves the child to 'continue' in the background.

  To guarantee not to be a process group leader, since it inherited its
  process group ID from its parent and obtained its own unique process ID,
  which differs from the inherited process group ID.

This is required in order to be able to successfully perform the next step.

2. The child process calls setsid() to start a new session and free itself of
any association with a controlling terminal.

note: From step 1, it is guaranteed not to be a group leader and this is
necessary since only a process which is not a group leader can create a new
session and this has no controlling terminal.

See process: group: session.

3. If the daemon never opens any terminal devices thereafter, then we don't
need to worry about the daemon reacquiring a controlling terminal.

4.5. See 37.2

6. Close all open file descriptors that the daemon has inherited from its parent. A daemon may need
to keep certain inherited file descriptors open, so this step is optional, or open to variation.

This is done for a variety of reasons. Since the daemon has lost its controlling terminal and is
running in the background, it makes 'no' sense for the daemon to keep file descriptors 0, 1, and 2
open if these refer to the terminal. 

Furthermore, we can't 'unmount' any file systems on which the long-lived daemon holds files open.
And, as usual, we should close unused open file descriptors because file descriptors are a finite
resource.

7. After having closed file descriptors 0, 1, and 2, a daemon normally opens /dev/null and uses
dup2() (or similar) to make all those descriptors refer to this device.

note: this is why cannot see output when use sandbox?

This is done for two reasons:

@ It ensures that if the daemon calls library functions that perform I/O on these descriptors, those
functions won't unexpectedly fail. note: due to step 6.

@ It prevents the possibility that the daemon later opens a file using descriptor 1 or 2, which is
then written to-and thus corrupted-by a library function that expects to treat these descriptors as
standard output and standard error.

TODO: Once figure out the daemon implementation and compare it with listing 37.1.


{reinit-daemon}
The fact that many daemons should run continuously presents a couple of programming hurdles:

@ Typically, a daemon reads operational parameters from an associated configuration file on startup.
Sometimes, it is desirable to be able to change these parameters "on the fly," 'without' needing to
stop and restart the daemon.

@ Some daemons produce log files. If the daemon never closes the log file, then it may grow
endlessly, eventually clogging the file system. noted that even if we remove the last name of a
file, the file continues to exist as long as any process has it open. What we need is a way of
telling the daemon to close its log file and open a new file, so that we can rotate log files as
required.

The solution to both of these problems is to have the daemon establish a handler for SIGHUP, and
perform the required steps upon receipt of this signal.

Why is it possible? Since a daemon has no controlling terminal, the kernel never generates this
signal for a daemon. Therefore, daemons can use SIGHUP for the purpose described here.


{syslog}
note: no syslogd on a embedded system.

<why>
When writing a daemon, one problem we encounter is how to display error
messages. Since a daemon runs in the background, we can't display messages on
an associated terminal, as we would typically do with other programs. 

One possible alternative is to write messages to an application-specific log
file, as is done in the program in Listing 37-3. The main problem with this
approach is that it is difficult for a system administrator to manage multiple
application log files and monitor them all for error messages. 

The syslog facility was devised to address this problem which provides a
single, centralized logging facility that can be used to log messages by 'all'
applications on the system.


Figure 37-1: Overview of system logging 

printf uses syslog 2 and /proc/kmsg to log and klogd uses
syslog 3 to log in to /dev/log.

Kernel                            User process
  |                                 |
  | printk                          | syslog(3)
  | syslog(2), /proc/kmsg           |
  |                                 |
klogd                               |
  |                                 |
  | syslog(3)                       |
  |                                 |
  +---------------------------------+-------
                                |
                             /dev/log
                    Unix domain datagram socket
                                |
                            syslogd


<daemon-and-conf>
The syslog facility has two principal components: the syslogd daemon and the
syslog(3) library function.

The System Log daemon, `syslogd`, accepts log messages from two different
sources: a UNIX domain socket, /dev/log, which holds locally produced
messages, and (if enabled) an Internet domain socket (UDP port 514), which
holds messages sent across a TCP/IP network.

Each message processed by syslogd has a number of `attributes`, including a
`facility`, which specifies the `type of program` generating the message, and
a `level`, which specifies the `severity` (priority) of the message. The
syslogd daemon examines the facility and level of each message, and then
passes it along to any of several possible destinations according to the
dictates of an associated configuration file, /etc/syslog.conf. 

Possible destinations include a terminal or virtual console, a disk file, a
FIFO, one or more (or all) logged-in users, or a process (typically another
    syslogd daemon) on another system connected via a TCP/IP network. (Sending
      the message to a process on another system is useful for reducing
      administrative overhead by consolidating messages from multiple systems
      to a single location.) A single message may be sent to multiple
    destinations (or none at all), and messages with different combinations of
    facility and level can be targeted to different destinations or to
    different instances of destinations (i.e., different consoles, different
        disk files, and so on).


The `syslog(3)` library function can be used by any process to log a message.
This function, which we describe in detail in a moment, uses its supplied
arguments to construct a message in a standard format that is then placed on
the /dev/log socket for reading by syslogd.


<klogd>
An alternative source of the messages placed on /dev/log is the Kernel Log
daemon, `klogd`, which collects kernel log messages (produced by the kernel
    using its printk() function). 

These messages are collected using either of two equivalent Linux-specific
interfaces-the `/proc/kmsg` file and the syslog(2) 'system' call-and then
placed on /dev/log using the syslog(3) 'library' function.


Although `syslog(2)` and `syslog(3)` share the same name, they perform quite
different tasks. An interface to syslog(2) is provided in glibc under the name
klogctl(). Unless explicitly indicated otherwise, when we refer to syslog() in
this section, `we mean syslog(3)`


<syslog2>
SYSLOG(2)                  Linux Programmer’s Manual                 SYSLOG(2)

NAME
       syslog, klogctl - read and/or clear kernel message ring buffer; set console_loglevel

SYNOPSIS
       int syslog(int type, char *bufp, int len);
                       /* No wrapper provided in glibc */

       /* The glibc interface */
       #include <sys/klog.h>

       int klogctl(int type, char *bufp, int len);

DESCRIPTION
       If you need the C library function syslog() (which talks to
           syslogd(8)), then look at syslog(3).  
       
       The system call of this name is about controlling the kernel printk()
  buffer, and the glibc version is called klogctl().


   The kernel log buffer
       The kernel has a cyclic buffer of length LOG_BUF_LEN in which messages
       given as arguments to the kernel function printk() are stored
       (regardless of their loglevel).  In early ker- nels,  LOG_BUF_LEN  had
       the value 4096; from kernel 1.3.54, it was 8192; from kernel 2.1.113 it
       was 16384; since 2.4.23/2.6 the value is a kernel configuration option.
       In recent kernels the size can be queried with command type 10.

   The loglevel
       The  kernel  routine printk() will only print a message on the console,
       if it has a loglevel less than the value of the variable
       console_loglevel.  This variable initially has the value
       DEFAULT_CONSOLE_LOGLEVEL (7), but is set to 10 if the kernel command
       line contains the word "debug", and to 15 in case of a kernel fault
       (the 10 and 15 are just  silly,  and equivalent to 8).  This variable
       is set (to a value in the range 1-8) by the call syslog(8,dummy,value).
       The calls syslog(type,dummy,dummy) with type equal to 6 or 7, set it to
       1 (kernel panics only) or 7 (all except debugging messages),
       respectively.

       Every text line in a message has its own loglevel.  This level is
       DEFAULT_MESSAGE_LOGLEVEL - 1 (6) unless the line starts with <d> where
       d is a digit in the range  1-7,  in  which case the level is d.  The
       conventional meaning of the loglevel is defined in <linux/kernel.h> as
       follows:

       #define KERN_EMERG    "<0>"  /* system is unusable               */
       #define KERN_ALERT    "<1>"  /* action must be taken immediately */
       #define KERN_CRIT     "<2>"  /* critical conditions              */
       #define KERN_ERR      "<3>"  /* error conditions                 */
       #define KERN_WARNING  "<4>"  /* warning conditions               */
       #define KERN_NOTICE   "<5>"  /* normal but significant condition */
       #define KERN_INFO     "<6>"  /* informational                    */
       #define KERN_DEBUG    "<7>"  /* debug-level messages             */
       
       note:
       How to set level? Depends on words contained in a line.


<syslog3>
#include <syslog.h>

void openlog(const char *ident, int log_options, int facility);
void syslog(int priority, const char *format, ...);

<facility-and-level>
The priority argument is created by ORing together a facility value and a
level value. The `facility` indicates the general `category` of the application
logging the message, and is specified as one of the values listed in Table
37-1. If omitted, the facility defaults to the value specified in a previous
openlog() call, or to LOG_USER if that call was omitted.

       LOG_AUTH       security/authorization messages
       LOG_AUTHPRIV   security/authorization messages (private)
       ...

The `level` value indicates the `severity` of the message, and is specified as one
of the values in Table 37-2. All of the level values listed in this table
appear in SUSv3.

       LOG_EMERG      system is unusable
       LOG_ALERT      action must be taken immediately
       LOG_CRIT       critical conditions
       LOG_ERR        error conditions
       LOG_WARNING    warning conditions
       LOG_NOTICE     normal, but significant, condition
       LOG_INFO       informational message
       LOG_DEBUG      debug-level message

One difference from printf() is that the format string doesn't need to include
a terminating newline character. Also, the format string may include the
2-character sequence %m, which is replaced by the error string correspond- ing
to the current value of errno (i.e., the equivalent of strerror(errno)).




={============================================================================
*kt_linux_core_050* process-function-reentrant

From LPI signal.

Because a signal handler may asynchronously interrupt the execution of a program
at any point in time, the main program and the signal handler in effect form
`two-independent` (although not concurrent) threads of execution within the same
process.


A function is said to be `reentrant` if it can safely be simultaneously executed
by multiple threads of execution in the `same-process`. In this context, "safe"
means that the function achieves its expected result, regardless of the state of
execution of any other thread of execution.

A function may be `nonreentrant` if it updates global or static data structures. A
function that employs only local variables is guaranteed to be reentrant because
of race-condition. This book shows an example using crypt() in both main and
signal handler. This corrupts internal buffer which is statically allocated and
crypt uses when calls it with differnt parameter.

note:
`nonreentrant` means that function is not safe when gets called by multiple
threads. Using only local variables means reentrant. Why mentions race
condition?

Such possibilities are in fact rife within the standard C library. For example,
     we already noted in Section 7.1.3 that malloc() and free() maintain a
     linked list of freed memory blocks available for reallocation from the
     heap. If a call to malloc() in the main program is interrupted by a signal
     handler that also calls malloc(), then this linked list can be corrupted.
     For this reason, the malloc() family of functions, and other library
     functions that use them, are nonreentrant.

Even if we are not using nonreentrant library functions, reentrancy issues can
still be relevant. If a signal handler updates programmer-defined global data
structures that are also updated within the main program, then we can say that
the signal handler is nonreentrant with respect to the main program.

If a function is nonreentrant, then its manual page will normally provide an
explicit or implicit indication of this fact. In particular, watch out for
statements that the function uses or returns information in statically allocated
variables.


={============================================================================
*kt_linux_core_105* linux-mem-process

*LPI-6.3* Memory Layout of a Process

LPI Figure 6-1: Typical memory layout of a process on Linux/x86-32.

note: 
The ** areas represent invalid ranges in the process's virtual address space;
that is, areas for which page tables have 'not' been created.

High           +----------------+
                  ** kernel. mapped into process virtual memory but 
                             not accessible to program.
               +----------------+                  0xC0000000
                  argv, environ
               +----------------+
                  stack (grows down)
top of stack   +----------------+
                  ** unallocated
               +----------------+
                  heap (grows up)
               +----------------+ < &end
                  bss
               +----------------+ < &edata
                  inited data
               +----------------+ < &etext
                  text
               +----------------+
                  **
Low            +----------------+                  0x00000000

note: what's the kernel in this diagram?

/proc/kallsyms provides addresses of kernel symbols in this region. /proc/ksyms
in kernel 2.4 and earlier.


The memory allocated to each process is composed of a number of parts, usually
referred to as `segments`.

<text-segment> 
The text segment contains the machine-language instructions of the program run
by the process. The text segment is made `read-only` so that a process doesn't
accidentally modify its own instructions via a bad pointer value. Since many
processes may be running the same program, the text segment is made 'sharable'
so that a single copy of the program code can be mapped into the virtual address
space of all of the processes.

<initialized-data-segment> process-data
The initialized data segment contains 'global' and 'static' variables that are
explicitly initialized. The values of these variables are read from the
executable file when the program is loaded into memory.

<uninitialized-data-segment> <process-bss>
The `uninitialized data segment` contains 'global' and 'static' variables that are
'not' explicitly initialized. Before starting the program, the system
initializes all memory in this segment to 0.  For historical reasons, this is
often called the bss segment, a name derived from an old assembler mnemonic for
"block started by symbol." 

The main reason for placing global and static variables that are initialized
into a separate segment from those that are uninitialized is that, when a
program is stored on disk, it is not necessary to allocate space for the
uninitialized data. Instead, the executable merely needs to record the location
and size required for the uninitialized data segment, and this space is
allocated by the program loader at 'runtime'.

Less commonly used, but more descriptive labels for the initialized and
uninitialized data segments are user-initialized data segment and
zero-initialized data segment.

<stack>
The stack is a dynamically growing and shrinking segment containing stack
frames. One stack frame is allocated for each currently called function. A frame
stores the function's local variables (so-called automatic variables),
       arguments, and return value. Stack frames are discussed in more detail in
         Section 6.5.

<heap>
The heap is an area from which memory (for variables) can be dynamically
allocated at run time. The top end of the heap is called the program break.


Sometimes, the term section is used instead of segment, since section is more
consistent with the terminology used in the now ubiquitous ELF specification
for executable file formats.

<etext>
the C program environment on most UNIX implementations (including Linux)
provides three global symbols: etext, edata, and end.

These symbols can be used from within a program to obtain the addresses of the
next byte past, respectively, the end of the program text, the end of the
initialized data segment, and the end of the uninitialized data segment.

extern char etext, edata, end;
/* For example, &etext gives the address of the end
of the program text / start of initialized data */


<size-command>
The size(1) command displays the size of the text, initialized data, and
uninitialized data (bss) segments of a binary executable.

kyoupark@kit-debian:~$ ll libgcc_s.so.1
-rw-r--r-- 1 kyoupark ccusers 290250 Apr  7 08:38 libgcc_s.so.1

kyoupark@kit-debian:~$ size !$
size libgcc_s.so.1
   text    data     bss     dec     hex filename
  60600    2064     288   62952    f5e8 libgcc_s.so.1

>>> 60600+2064+288
62952
>>> hex(62952)
'0xf5e8'

<ex> linux-mem
#include <stdio.h>
#include <stdlib.h>

int bssvar;
int datavar = 99;

int main()
{
  int stackvar;
  char *mallocptr = malloc(100);

  printf("\n");
  printf("address of bss section    = 0x%16lx\n", (unsigned long)&bssvar);
  printf("address of data section   = 0x%16lx\n", (unsigned long)&datavar);
  printf("address of stack section  = 0x%16lx\n", (unsigned long)&stackvar);
  printf("address of malloc section = 0x%16lx\n", (unsigned long)&mallocptr);

  while(1)
    sleep(10);
}


={============================================================================
*kt_linux_core_106* linux-mem-virtual

*LPI-6.4* Virtual Memory Management

The aim of this virtual memory is to make efficient use of both the CPU and RAM
(physical memory) by exploiting a property that is typical of most programs.

// Linkers, Ian Lance Taylor

The virtual memory system would be used to map the single copy into the
address space of each process which needed it. This would require less
physical memory to run multiple programs, and thus yield better performance.


`locality-of-reference`. Most programs demonstrate two kinds of locality:

  `spatial locality` is the tendency of a program to reference memory
  addresses that are 'near' those that were recently accessed (because of
      'sequential' processing of instructions, and, sometimes, sequential
      processing of data structures).

  `temporal locality` is the tendency of a program to access the 'same' memory
  addresses in the near future that it accessed in the recent past (because of
      loops).

The upshot of locality of reference is that it is possible to execute a program
while maintaining only 'part' of its address space in RAM.


<page>
A virtual memory scheme splits the memory used by each program into small,
  fixed-size units called `pages`. Correspondingly, RAM is divided into a series
  of page frames of the same size. 

At any one time, only some of the pages of a program need to be resident in
physical memory page frames; these pages form the so-called `resident-set`.


<swap-space>
Copies of the unused pages of a program are maintained in the `swap-area` - a
reserved area of disk space used to supplement the computer's RAM - and loaded
into physical memory only as required. 


<page-fault>
When a process references a page that is `not currently resident` in physical
memory, a `page-fault` occurs, at which point the kernel suspends execution of
the process while the page is loaded from disk into memory.


<per-process-page-table>
The kernel maintains a `page-table` for `each-process` (Figure 6-2). The page
table describes the location of each page in the process's virtual address
space (the set of all virtual memory pages available to the process). 

                          access -->
process virtual address space    page table        physical memory(RAM) 
                                                      page frames 
low   page 0                           4                    0
      page 1                           2                    1
      page 2                           7                    2
high  page 3                           0                    3

Each entry in the page table either indicates the location of a virtual page
in `RAM or indicates that it currently resides on disk.`

Not all address ranges in the process's virtual address space require
page-table entries. Typically, large ranges of the potential virtual address
space are unused, so that it isn't necessary to maintain corresponding
page-table entries.

If a process tries to access an address for which there is 'no' corresponding
page-table entry, it receives a SIGSEGV signal. *SIGSEGV* *seg-fault*

Whenever that process runs, the hardware’s page table register will point to
that table.

*debugging*
pages are the smallest unit of memory manipulated by the VM system. This is an
important point to understand when debugging, because it implies that some
erroneous memory accesses by the program will not trigger seg faults, as you
will see below. In other words, during your debugging session, you cannot say
something like, “This line of source code must be okay, since it didn’t cause
a seg fault.”

During the execution of the program, the addresses it generates will be
virtual. When the program attempts to access memory at a certain virtual ad-
dress, say y, the hardware will convert that to a virtual page number v, which
equals y divided by 4,096 (where the division uses integer arithmetic, dis-
    carding the remainder). The hardware will then check entry v in the page
table to see whether the permissions for the page match the operation to be
performed. 

If they do match, the hardware will get the desired location’s actual physical
page number from this table entry and then carry out the requested memory
operation. 

But if the table entry shows that the requested operation does not have the
proper permission, the hardware will execute an internal interrupt. This will
cause a jump to the OS’s error-handling routine. The OS will normally then
announce a memory access violation and discontinue execution of the program
(i.e., remove it from the process table and from memory).


<page-fault-seg-fault>
page-fault is different from seg-fault since page-fault happens when finds an
entry in page table but not in the resident-set so raise page-fault to signal
that reads from swap-space. But seg-fault happens when no entry in page table.

*PMMU*
The implementation of virtual memory requires hardware support in the form of
a paged memory management unit (PMMU). The PMMU translates each virtual memory
address reference into the corresponding physical memory address and advises
the kernel of a page fault when a particular virtual memory address
corresponds to a page that is not resident in RAM.


{valid-virtual-addresses}
A process's range of 'valid' virtual addresses can change over its lifetime, as
the kernel allocates and deallocates pages (and page-table entries) for the
process.

* as the stack grows downward beyond limits previously reached;

* when memory is allocated or deallocated on the heap, by raising the program
  break using brk(), sbrk(), or the malloc family of functions

* when System V shared memory regions are attached using shmat() and detached
  using shmdt()

* when memory mappings are created using mmap() and unmapped using munmap()


{linux-mem-virtual-advantages}
Virtual memory management separates the virtual address space of a process
from the physical address space of RAM. This provides many advantages:

* Processes are `isolated` from one another and from the kernel, so that one
  process can't read or modify the memory of another process or the kernel. This
  is accomplished by having the page-table entries for each process point to
  distinct sets of physical pages in RAM (or in the swap area).

* Where appropriate, two or more processes can share memory. The kernel makes
  this possible by having page-table entries in different processes refer to the
  same pages of RAM. Memory sharing occurs in two common circumstances:

  - Multiple processes executing the same program can share a single (readonly)
  copy of the program code. This type of sharing is performed implicitly when
  multiple programs execute the same program file (or load the same shared
      library).

  - Processes can use the shmget() and mmap() system calls to explicitly request
  sharing of memory regions with other processes. This is done for the purpose
  of interprocess communication.

* The implementation of `memory-protection` schemes is facilitated; that is,
  pagetable entries can be marked to indicate that the contents of the
  corresponding page are readable, writable, executable, or some combination
  of these protections. Where multiple processes share pages of RAM, it is
  possible to specify that each process has different protections on the
  memory; for example, one process might have read-only access to a page,
  while another has read-write access.  
       
* Because only a part of a program needs to reside in memory, the program loads
  and runs faster.  Furthermore, the memory footprint (i.e., virtual size) of a
  process can exceed the capacity of RAM.

* One final advantage of virtual memory management is that since each process
  uses less RAM, more processes can simultaneously be held in RAM. This
  typically leads to better CPU utilization, since it increases the likelihood
  that, at any moment in time, there is at least one process that the CPU can
  execute.


LPI 6.5 The Stack and Stack Frames

<kernel-stack>
Sometimes, the term "user stack" is used to distinguish the stack we describe
here from the "kernel stack". The kernel stack is a `per-process` memory region
maintained in kernel memory that is used as the stack for execution of the
functions called internally during the execution of a `system-call`.  The kernel
can't employ the user stack for this purpose since it resides in unprotected
user memory.


<user-stack>
Each (user) stack frame contains the following information:

* Function arguments and local variables: 

In C these are referred to as automatic variables, since they are automatically
created when a function is called. These variables also automatically disappear
when the function returns (since the stack frame disappears), and this forms the
primary semantic distinction between automatic and static (and global)
  variables: the latter have a permanent existence independent of the execution
  of functions.

* `call-linkage` information: 

Each function uses certain CPU registers, such as the program counter, which
points to the next machine-language instruction to be executed. Each time one
function calls another, a copy of these registers is saved in the called
function's stack frame so that when the function returns, the appropriate
register values can be restored for the calling function.

Referring to Listing 6-1, during the execution of the function square(), the
stack will contain frames as:

+----------------+
 frames for c run-time startup functions  note:
+----------------+
 frame for main
+----------------+
 frame for doCalc()
+----------------+
 frame for square() 
+----------------+      <- sp. grows down


={============================================================================
*kt_linux_core_050* linux-mem-alloc

*LPI-7* Memory Allocation

7.1 Allocating Memory on the Heap

The current limit of the heap is referred to as the `program break.`

Resizing the heap (i.e., allocating or deallocating memory) is actually as
simple as telling the kernel to adjust its idea of where the `process's`
program break is. Initially, the program break lies just past the end of the
uninitialized data segment (i.e., the same location as &end).

After the program break is increased, the program may access any address in
the newly allocated area, `but no physical memory pages are allocated yet.`
The kernel automatically allocates new physical pages on the first attempt by
the process to access addresses in those pages.

the UNIX system has provided two system calls for manipulating the program
break, and these are both available on Linux: brk() and sbrk().

On Linux, sbrk() is a library function implemented on top of brk(). On
success, sbrk() returns the previous address of the program break. In other
words, if we have increased the program break, then the return value is a
pointer to the `start` of the newly allocated block of memory.

note:
Since heap grows upwards, the previous address of the program break is the
start of the new block.

The call sbrk(0) returns the current setting of the program break without
changing it. This can be useful if we want to track the size of the heap,
perhaps in order to monitor the behavior of a memory allocation package.


7.1.2 Allocating Memory on the Heap: malloc() and free()

The block of memory returned by malloc() is always aligned on a byte boundary
suitable for any type of C data structure. In practice, this means that it is
allocated on an 8-byte or 16-byte boundary on most architectures.

note:
SUSv3 specifies that the call malloc(0) may return either NULL or a pointer to a
small piece of memory that can (and should) be freed with free(). On Linux,
malloc(0) follows the latter behavior.

<free-list>
In general, free() doesn't lower the program break, but instead adds the block
of memory to a list of free blocks that are recycled by future calls to
malloc().

<double-free>
Making any use of ptr after the call to free()—for example, passing it to
free() a second time—is an error that can lead to unpredictable results.

It is an error to free the same piece of allocated memory more than once. With
glibc on Linux, we often get a segmentation violation (SIGSEGV signal). This
is good, because it alerts us that we’ve made a programming error. However,
more generally, freeing the same memory twice leads to unpredictable behavior.


In this case, the (glibc) free() function is able to recognize that an entire
region at the top end of the heap is free, since, when releasing blocks, it
coalesces neighboring free blocks into a single larger block. 

Such coalescing is done to avoid having a large number of small fragments on
the free list, all of which may be too small to satisfy subsequent malloc()
requests.

The glibc free() function calls sbrk() to lower the program break only when
the free block at the top end is "sufficiently" large, where sufficient is
determined by parameters controlling the operation of the malloc package (128
    kB is a typical value). This reduces the number of sbrk() calls (i.e., the
      number of brk() system calls) that must be made.


To free() or not to free()?

When a process terminates, all of its memory is returned to the system,
including heap memory allocated by functions in the malloc package. In
  programs that allocate memory and continue using it until program
  termination, it is common to omit calls to free(), relying on this behavior
  to automatically free the memory.

Although relying on process termination to automatically free memory is
acceptable for many programs, there are a couple of reasons why it can be
desirable to explicitly free all allocated memory:


7.1.3 Implementation of malloc() and free()

Looking at the implementation of free(), things start to become more
interesting.  When free() places a block of memory onto the free list, how
does it know what size that block is? This is done via a trick. When malloc()
allocates the block, it allocates extra bytes to hold an integer containing
the size of the block. This integer is located at the beginning of the block;
the address actually returned to the caller points to the location just past
  this length value, as shown in Figure 7-1.

a block returned by malloc

Length of |   Memory for use by caller
block(L)  |
            
          ^ address returned by malloc()

When a block is placed on the (doubly linked) free list, free() uses the bytes
of the block itself in order to add the block to the list, as shown in Figure
7-2.

a block on the free list

Length of | Pointer to | Pointer to | Remining bytes of free block
block(L)  | previous   | next free  |
          | free block | block      |
            

Tools and libraries for malloc debugging

Among the malloc debugging tools provided by glibc are the following:

A malloc debugging library offers the same API as the standard malloc package,
  but does extra work to catch memory allocation bugs. In order to use such a
    library, we link our application against that library instead of the
    malloc package in the standard C library. Because these libraries
    typically operate at the cost of slower run-time operation, increased
    memory consumption, or both, we should use them only for debugging
    purposes, and then return to linking with the standard malloc package for
    the production version of an application. Among such libraries are
    Electric Fence (http://www.perens.com/FreeSoftware/), dmalloc
    (http://dmalloc.com/), Valgrind (http://valgrind.org/), and Insure++
    (http://www.parasoft.com/).


Controlling and monitoring the malloc package

The glibc manual describes a range of nonstandard functions that can be used
to monitor and control the allocation of memory by functions in the malloc
package, including the following:


7.1.4 Other Methods of Allocating Memory on the Heap

#include <stdlib.h>
void *calloc(size_t numitems, size_t size);

Returns pointer to allocated memory on success, or NULL on error

The numitems argument specifies how many items to allocate, and size specifies
their size. Unlike malloc(), calloc() initializes the allocated memory to 0.

#include <stdlib.h>
void *realloc(void *ptr, size_t size);

Returns pointer to allocated memory on success, or NULL on error

Since realloc() may relocate the block of memory, we must use the returned
pointer from realloc() for future references to the memory block. We can
employ realloc() to reallocate a block pointed to by the variable ptr as
follows:

nptr = realloc(ptr, newsize);

if (nptr == NULL) {
  /* Handle error */
} else { /* realloc() succeeded */
  ptr = nptr;
}


7.2 Allocating Memory on the Stack: alloca()

Using alloca() to allocate memory has a few advantages over malloc(). One of
these is that allocating blocks of memory is faster with alloca() than with
malloc(), because alloca() is implemented by the compiler as inline code that
directly adjusts the stack pointer. Furthermore, alloca() doesn’t need to
maintain a list of free blocks.

Another advantage of alloca() is that the memory that it allocates is
automatically freed when the stack frame is removed; that is, when the
function that called alloca() returns.


={============================================================================
*kt_linux_core_050* linux-mem-mapping

*LPI-49* Memory Mappings

The mmap() system call creates a new memory mapping `in the calling process’s`
virtual address space. A mapping can be of two types:

o File mapping: A file mapping maps a region of a file directly into the
  calling process’s virtual memory. Once a file is mapped, its contents can be
  accessed by operations on the bytes in the corresponding memory region. The
  pages of the mapping are (automatically) loaded from the file as required.
  This type of mapping is also known as 
  `a file-based mapping or memory-mapped file.`

*call-map-anonymous*
o Anonymous mapping: An anonymous mapping doesn’t have a corresponding file.
  Instead, the pages of the mapping are initialized to 0.

  Another way of thinking of an anonymous mapping (and one is that is close to
  the truth) is that it is a mapping of a virtual file whose contents are
  always initialized with zeros.

  *proc-maps* 
  If the pathname field is blank, this is an anonymous mapping as obtained via
  the mmap(2) function.


Memory mappings can be used for IPC

The memory in one process’s mapping may be shared with mappings in other
processes:

    i.e., the page-table entries of each process point to the same
    pages of RAM. This can occur in two ways:

* When two processes map the same region of a file, they share the same pages
  of physical memory.

* A child process created by fork() inherits copies of its parent’s mappings,
  and these mappings refer to the same pages of physical memory as the
  corresponding mappings in the parent.

When two or more processes share the same pages, each process can potentially
see the changes to the page contents made by other processes, depending on
`whether the mapping is private or shared`:

* Private mapping (MAP_PRIVATE)
  Modifications to the contents of the mapping are not visible to other
  processes and, for a file mapping, are not carried through to the underlying
  file. Although the pages of a private mapping are initially shared in the
  circumstances described above, changes to the contents of the mapping are
  nevertheless private to each process. The kernel accomplishes this using the
  `copy-on-write` technique (Section 24.2.2). This means that whenever a process
  attempts to modify the contents of a page, the kernel first creates a new,
  separate copy of that page for the process (and adjusts the process’s page
  tables). For this reason, a MAP_PRIVATE mapping is sometimes referred to
  as a private, copy-on-write mapping.

* Shared mapping (MAP_SHARED)
  Modifications to the contents of the mapping are visible to other processes
  that share the same mapping and, for a file mapping, are carried through to
  the underlying file.


The four different types of memory mappings are created and used as follows:

* Private file mapping: 
  The contents of the mapping are initialized from a file region. Multiple
  processes mapping the same file initially share the same physical pages of
  memory, but the copy-on-write technique is employed, so that changes to the
  mapping by one process are invisible to other processes. 

  The main use of this type of mapping is to initialize a region of memory
  from the contents of a file. Some common examples are initializing a
  process’s text and initialized data segments from the corresponding parts of
  a binary executable file or a shared library file.

* Private anonymous mapping: 
  Each call to mmap() to create a private anonymous mapping yields a new
  mapping that is distinct from (i.e., does not share physical pages with)
  other anonymous mappings created by the same (or a different) process.
  Although a child process inherits its parent’s mappings, `copy-on-write`
  semantics ensure that, after the fork(), the parent and child don’t see
  changes made to the mapping by the other process. 
    
  *memory-allocation*
  The primary purpose of private anonymous mappings is to allocate new
  (zero-filled) memory for a process (e.g., malloc() employs mmap() for this
      purpose when allocating large blocks of memory).

* `Shared file mapping:`
  All processes mapping the same region of a file share the same physical
  pages of memory, which are initialized from a file region.  Modifications to
  the contents of the mapping are carried through to the file. 
 
  This type of mapping serves two purposes. First, it permits memory-mapped
  I/O. By this, we mean that a file is loaded into a region of the process’s
  virtual memory, and modifications to that memory are automatically written
  to the file. Thus, memory-mapped I/O provides an alternative to using read()
  and write() for performing file I/O. 
  
  A second purpose of this type of mapping is to allow `unrelated processes`
  to share a region of memory in order to perform (fast) IPC in a manner
  similar to System V shared memory segments (Chapter 48).

* Shared anonymous mapping: 
  As with a private anonymous mapping, each call to mmap() to create a shared
  anonymous mapping creates a new, distinct mapping that doesn’t share pages
  with any other mapping. 
  
  The difference is that the pages of the mapping are not copied-on-write.
  This means that when a child inherits the mapping after a fork(), the parent
  and child share the same pages of RAM, and changes made to the contents of
  the mapping by one process are visible to the other process. 
  
  Shared anonymous mappings allow IPC in a manner similar to System V shared
  memory segments, but only between `related processes.`


<call-map-mmap>
LPI-49.2 Creating a Mapping: mmap()

The mmap() system call creates a new mapping in the calling process’s virtual
address space.

`#include <sys/mman.h>`
void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);

Returns starting address of mapping on success, or MAP_FAILED on error


The addr argument indicates the virtual address at which the mapping is to be
located. `If we specify addr as NULL`, the kernel chooses a suitable address for
the mapping. This is the preferred way of creating a mapping.

The length argument specifies the size of the mapping in bytes. Although
length doesn’t need to be a multiple of the system page size (as returned by
sysconf(_SC_PAGESIZE)), the kernel creates mappings in units of this size,
so that length is, in effect, rounded up to the next multiple of the
page size.

The prot argument is a bit mask specifying the protection to be placed on the
mapping.

*call-map-flag*
Table 49-2: Memory protection values

`PROT_NONE`   The region may not be accessed
PROT_READ   The contents of the region can be read
PROT_WRITE  The contents of the region can be modified
PROT_EXEC   The contents of the region can be executed

The flags argument is a bit mask of options controlling various aspects of the
mapping operation. Exactly one of the following values must be included in
this mask: 

MAP_PRIVATE
MAP_SHARED

The remaining arguments, fd and offset, are used with file mappings (they are
    ignored for anonymous mappings). 

The fd argument is a file descriptor identifying the file to be mapped. The
offset argument specifies the starting point of the mapping in the file, and
must be a multiple of the system page size. 

To map the entire file, we would specify offset as 0 and length as the size of
the file. We say more about file mappings in Section 49.5.


Alignment restrictions specified in standards for offset and addr

SUSv3 specifies that the offset argument of mmap() must be page-aligned, and
that the addr argument must also be page-aligned if MAP_FIXED is specified.
Linux conforms to these requirements. However, it was later noted that the
SUSv3 requirements differed from earlier standards, which imposed looser
requirements on these arguments.  The consequence of the SUSv3 wording was to
(unnecessarily) render some formerly standards-conformant implementations
nonconforming. SUSv4 returns to the looser requirement:

o An implementation may require that offset be a multiple of the system page
  size.

o If MAP_FIXED is specified, then an implementation may require that addr be
  page-aligned.

o If MAP_FIXED is specified, and addr is nonzero, then addr and offset shall
  have the same remainder modulo the system page size.

A similar situation arose for the addr argument of mprotect(), msync(), and
munmap(). SUSv3 specified that this argument must be page-aligned. SUSv4 says
that an implementation may require this argument to be page-aligned.


<call-map-munmap>
49.3 Unmapping a Mapped Region: munmap()

The munmap() system call performs the converse of mmap(), removing a mapping
from the calling process’s virtual address space.

#include <sys/mman.h>
int munmap(void *addr, size_t length);

Returns 0 on success, or 1 on error

  "If there are no mappings in the address range specified by addr and length,
  then munmap() has no effect, and returns 0 (for success)."

During unmapping, the kernel removes any memory locks that the process holds
for the specified address range. (Memory locks are established using mlock()
    or mlockall(), as described in Section 50.2.)

  <when-unmap>
  "All of a process’s mappings are automatically unmapped when it terminates or
  performs an exec()."

To ensure that the contents of a shared file mapping are written to the
underlying file, a call to msync() (Section 49.5) should be made before
unmapping a mapping with munmap().


49.4 File Mappings

To create a file mapping, we perform the following steps:

1. Obtain a descriptor for the file, typically via a call to open().

2. Pass that file descriptor as the fd argument in a call to mmap().

As a result of these steps, mmap() maps the contents of the open file into the
address space of the calling process. 

Once mmap() has been called, we `can close the file descriptor` without
affecting the mapping. However, in some cases it may be useful to keep this
file descriptor open; see, for example, Listing 49-1 and also Chapter 54.

On Linux, the pages of a file mapping are mapped in on the first access. This
means that if changes are made to a file region after the mmap() call, but
before the corresponding part (i.e., page) of the mapping is accessed, then
the changes may be visible to the process, if the page has not otherwise
already been loaded into memory. This behavior is implementation-dependent;
portable applications should avoid relying on a particular kernel behavior in
  this scenario.


49.4.1 Private File Mappings

49.4.2 Shared File Mappings

When multiple processes create shared mappings of the same file region, they
all share the same physical pages of memory. In addition, modifications to the
contents of the mapping are carried through to the file. In effect, the file
is being treated as the paging store for this region of memory, as shown in
Figure 49-2. (We simplify things in this diagram by omitting to show that the
    mapped pages are typically not contiguous in physical memory.)

Figure 49-2: Two processes with a shared mapping of the same region of a file

Shared file mappings serve two purposes: memory-mapped I/O and IPC. We
consider each of these uses below.


Memory-mapped I/O

Since the contents of the shared file mapping are initialized from the file,
      and any modifications to the contents of the mapping are automatically
        carried through to the file, we can perform file I/O simply by
        accessing bytes of memory, relying on the kernel to ensure that the
        changes to memory are propagated to the mapped file.

The reasons that memory-mapped I/O can provide performance benefits are as
follows:

* A normal read() or write() involves two transfers: one between the file and
  the kernel buffer cache, and the other between the buffer cache and a
  user-space buffer. Using mmap() eliminates the second of these transfers.
  For input, the data is available to the user process as soon as the kernel
  has mapped the corresponding file blocks into memory. For output, the user
  process merely needs to modify the contents of the memory, and can then rely
  on the kernel memory manager to automatically update the underlying file.

* In addition to saving a transfer between kernel space and user space, mmap()
  can also improve performance by lowering memory requirements. When using
  read() or write(), the data is maintained in two buffers: one in user space
  and the other in kernel space. When using mmap(), a single buffer is shared
  between the kernel space and user space. Furthermore, if multiple processes
  are performing I/O on the same file, then, using mmap(), they can all share
  the same kernel buffer, resulting in an additional memory saving.

Performance benefits from memory-mapped I/O are most likely to be realized
when performing repeated random accesses in a large file. If we are performing
sequential access of a file, then mmap() will probably provide little or no
gain over read() and write(), assuming that we perform I/O using buffer sizes
big enough to avoid making a large number of I/O system calls. 

The reason that there is little performance benefit is that, regardless of
which technique we use, the entire contents of the file will be transferred
between disk and memory exactly once, and the efficiency gains of eliminating
a data transfer between user space and kernel space and reducing memory usage
are typically negligible compared to the time required for disk I/O.

Memory-mapped I/O can also have disadvantages. For small I/Os, the cost of
memory-mapped I/O (i.e., mapping, page faulting, unmapping, and updating the
    hardware memory management unit’s translation look-aside buffer) can
actually be higher than for a simple read() or write(). In addition, it can
sometimes be difficult for the kernel to efficiently handle write-back for
writable mappings (the use of msync() or sync_file_range() can help improve
    efficiency in this case).


<why-use-mapping-over-shm>
IPC using a shared file mapping

Since all processes with a shared mapping of the same file region share the
same physical pages of memory, the second use of a shared file mapping is as a
method of (fast) IPC. 

The feature that distinguishes this type of shared memory region from a System
V shared memory object (Chapter 48) is that modifications to the contents of
the region are carried through to the underlying mapped file. This feature is
useful in an application that requires the shared memory contents to persist
across application or system restarts.

<ex> 
show many mmaped entries of /tmp/ctm_tmp_0(deleted)

2c148000     12K r--s-  /tmp/ctm_tmp_0 (deleted)
2c354000      8K r--s-  /tmp/ctm_tmp_0 (deleted)

since repeate a sequence which has close and unlik but no unmap calls and
leaks memory of calling process.


49.7 Anonymous Mappings

//       MAP_ANON
//              Synonym for MAP_ANONYMOUS.  Deprecated.
//
//       MAP_ANONYMOUS
//              The mapping is not backed by any file; its contents are
//              initialized to zero. The fd and offset arguments are ignored;
//              however, some implementations require fd to be -1  if
//              MAP_ANONYMOUS (or MAP_ANON) is specified, and portable
//              applications should ensure this.  The use of MAP_ANONYMOUS in
//              conjunction with MAP_SHARED is supported on Linux only since
//              kernel 2.4.

MAP_ANONYMOUS

Create an anonymous mapping that is, a mapping that is not backed by a
file. We describe this flag further in Section 49.7.

*call-map-anonymous*
With both the MAP_ANONYMOUS and the /dev/zero techniques, the bytes of the
resulting mapping are initialized to 0.


MAP_PRIVATE anonymous mappings

MAP_PRIVATE anonymous mappings are used to allocate blocks of process-private
memory initialized to 0. We can use the /dev/zero technique to create a
MAP_PRIVATE anonymous mapping as follows:

fd = open("/dev/zero", O_RDWR);
if (fd == -1)
  errExit("open");

addr = mmap(NULL, length, PROT_READ | PROT_WRITE, MAP_PRIVATE, fd, 0);
if (addr == MAP_FAILED)
  errExit("mmap");


The glibc implementation of malloc() uses MAP_PRIVATE anonymous mappings to
allocate blocks of memory larger than MMAP_THRESHOLD bytes. This makes it
possible to efficiently deallocate such blocks (via munmap()) if they are
later given to free().  (It also reduces the possibility of memory
    fragmentation when repeatedly allocating and deallocating large blocks of
    memory.) MMAP_THRESHOLD is 128 kB by default, but this parameter is
adjustable via the mallopt() library function.


49.4.3 Boundary Cases *SIGBUS*

In many cases, the size of a mapping is a multiple of the system page size,
and the mapping falls entirely within the bounds of the mapped file.  
  
However, this is not necessarily so, and we now look at what happens when
these conditions don’t hold.

mmap(0, 6000, prot, MAP_SHARED, fd, 0);

                          5999 6000             8191
| requested size of mapping   | remainder of page   |
|<- accessible,                                   ->|<- references ->
      mapped to file                                    yield SIGSEGV

| actual mapped region                              | unmapped      |
  of file

Since the size of the mapping is not a multiple of the system page size, it is
rounded up to the next multiple of the system page size. Because the file is
larger than this rounded-up size, the corresponding bytes of the file are
mapped.

Attempts to access bytes beyond the end of the mapping result in the
generation of a SIGSEGV signal


When the mapping extends beyond the end of the underlying file, the situation
is more complex.

mmap(0, 8192, prot, MAP_SHARED, fd, 0);

                  2199 2200              4096           8191
|                     | remainder of    |                   |
                        page(0)   

|<- accessible,     -> <- accessible, -> <- references    -> <- references  ->
    mapped to file        not mapped        yield SIGBUG        yield SIGSEGV

| actual file         | 

If the mapping includes pages beyond the rounded-up region, then attempts to
access addresses in these pages result in the generation of a SIGBUS signal,
which warns the process that there is no region of the file corresponding to
these addresses.


={============================================================================
*kt_linux_core_050* linux-mem-mapping linux-lib-shared

*proc-maps*
Mappings are lost when a process performs an exec(), but are inherited by the
child of a fork(). The mapping type (MAP_PRIVATE or MAP_SHARED) is also
inherited. 

To find out which shared libraries a process is currently using, and
Information about all of a process’s mappings is visible in the Linux-specific
/proc/PID/maps file, which we described in Section 48.5.


48.5 Location of Shared Memory in Virtual Memory

Using the Linux-specific /proc/PID/maps file, we can see the location of the
shared memory segments and shared libraries mapped by a program, as we
demonstrate in the shell session below.

Starting with kernel 2.6.14, Linux also provides the /proc/PID/smaps file,
which exposes more information about the memory consumption of each of a
process’s mappings. For further details, see the proc(5) manual page.

<ex>
Three lines for the main program, shm_attach. These correspond to the text and
data segments of the program q. The second of these lines is for a readonly
page holding the string constants used by the program.

       /proc/[pid]/maps
              A file containing the currently mapped memory regions and their
              access permissions.  See mmap(2) for some further information
              about memory mappings.

              The format of the file is:

       address           perms offset  dev   inode       pathname
       00400000-00452000 r-xp 00000000 08:02 173521      /usr/bin/dbus-daemon


              The format of the file is:
              The address field is the address space in the process that the
              mapping occupies.  The perms field is a set of permissions:

                   r = read
                   w = write
                   x = execute
                   s = shared
                   p = private (copy on write)


kyoupark@kit-debian64:/proc/12361$ more maps
00400000-00401000 r-xp 00000000 08:01 89260104                           /home/kyoupark/git/kb/code-linux/ex_shared/maps/ex_func_out
00600000-00601000 rw-p 00000000 08:01 89260104                           /home/kyoupark/git/kb/code-linux/ex_shared/maps/ex_func_out
7f59c9448000-7f59c95e9000 r-xp 00000000 08:01 55184367                   /lib/x86_64-linux-gnu/libc-2.19.so
7f59c95e9000-7f59c97e9000 ---p 001a1000 08:01 55184367                   /lib/x86_64-linux-gnu/libc-2.19.so
7f59c97e9000-7f59c97ed000 r--p 001a1000 08:01 55184367                   /lib/x86_64-linux-gnu/libc-2.19.so
7f59c97ed000-7f59c97ef000 rw-p 001a5000 08:01 55184367                   /lib/x86_64-linux-gnu/libc-2.19.so
7f59c97ef000-7f59c97f3000 rw-p 00000000 00:00 0
7f59c97f3000-7f59c97f4000 r-xp 00000000 08:01 89260100                   /home/kyoupark/git/kb/code-linux/ex_shared/maps/libfunc.so
7f59c97f4000-7f59c99f3000 ---p 00001000 08:01 89260100                   /home/kyoupark/git/kb/code-linux/ex_shared/maps/libfunc.so
7f59c99f3000-7f59c99f4000 rw-p 00000000 08:01 89260100                   /home/kyoupark/git/kb/code-linux/ex_shared/maps/libfunc.so
7f59c99f4000-7f59c9a15000 r-xp 00000000 08:01 55184364                   /lib/x86_64-linux-gnu/ld-2.19.so
7f59c9bf2000-7f59c9bf5000 rw-p 00000000 00:00 0
7f59c9c11000-7f59c9c14000 rw-p 00000000 00:00 0
7f59c9c14000-7f59c9c15000 r--p 00020000 08:01 55184364                   /lib/x86_64-linux-gnu/ld-2.19.so
7f59c9c15000-7f59c9c16000 rw-p 00021000 08:01 55184364                   /lib/x86_64-linux-gnu/ld-2.19.so
7f59c9c16000-7f59c9c17000 rw-p 00000000 00:00 0
7ffe6352e000-7ffe6354f000 rw-p 00000000 00:00 0                          [stack]
7ffe635fb000-7ffe635fd000 r-xp 00000000 00:00 0                          [vdso]
7ffe635fd000-7ffe635ff000 r--p 00000000 00:00 0                          [vvar]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]


kyoupark@kit-debian64:~/git/kb/code-linux/ex_shared/maps$ LD_LIBRARY_PATH=. ./ex_func_out
name= (8 segments) address=(nil)
                 header  0: address=  0x400040
                         type=6, flags=0x5
                 header  1: address=  0x400200
                         type=3, flags=0x4
                 header  2: address=  0x400000
                         type=1, flags=0x5
                 header  3: address=  0x600ba8
                         type=1, flags=0x6
                 header  4: address=  0x600bc0
                         type=2, flags=0x6
                 header  5: address=  0x40021c
                         type=4, flags=0x4
                 header  6: address=  0x400a50
                         type=1685382480, flags=0x4
                 header  7: address=     (nil)
                         type=1685382481, flags=0x6

name=linux-vdso.so.1 (4 segments) address=0x7ffe635fb000
                 header  0: address=0x7ffe635fb000
                         type=1, flags=0x5
                 header  1: address=0x7ffe635fb318
                         type=2, flags=0x4
                 header  2: address=0x7ffe635fb818
                         type=4, flags=0x4
                 header  3: address=0x7ffe635fb854
                         type=1685382480, flags=0x4

name=./libfunc.so (6 segments) address=0x7f59c97f3000
                 header  0: address=0x7f59c97f3000
                         type=1, flags=0x5
                 header  1: address=0x7f59c99f3810
                         type=1, flags=0x6
                 header  2: address=0x7f59c99f3828
                         type=2, flags=0x6
                 header  3: address=0x7f59c97f3190
                         type=4, flags=0x4
                 header  4: address=0x7f59c97f3760
                         type=1685382480, flags=0x4
                 header  5: address=0x7f59c97f3000
                         type=1685382481, flags=0x6

name=/lib/x86_64-linux-gnu/libc.so.6 (10 segments) address=0x7f59c9448000
                 header  0: address=0x7f59c9448040
                         type=6, flags=0x5
                 header  1: address=0x7f59c95b6650
                         type=3, flags=0x4
                 header  2: address=0x7f59c9448000
                         type=1, flags=0x5
                 header  3: address=0x7f59c97e9748
                         type=1, flags=0x6
                 header  4: address=0x7f59c97ecba0
                         type=2, flags=0x6
                 header  5: address=0x7f59c9448270
                         type=4, flags=0x4
                 header  6: address=0x7f59c97e9748
                         type=7, flags=0x4
                 header  7: address=0x7f59c95b666c
                         type=1685382480, flags=0x4
                 header  8: address=0x7f59c9448000
                         type=1685382481, flags=0x6
                 header  9: address=0x7f59c97e9748
                         type=1685382482, flags=0x4

name=/lib64/ld-linux-x86-64.so.2 (7 segments) address=0x7f59c99f4000
                 header  0: address=0x7f59c99f4000
                         type=1, flags=0x5
                 header  1: address=0x7f59c9c14c00
                         type=1, flags=0x6
                 header  2: address=0x7f59c9c14e70
                         type=2, flags=0x6
                 header  3: address=0x7f59c99f41c8
                         type=4, flags=0x4
                 header  4: address=0x7f59c9a11680
                         type=1685382480, flags=0x4
                 header  5: address=0x7f59c99f4000
                         type=1685382481, flags=0x6
                 header  6: address=0x7f59c9c14c00
                         type=1685382482, flags=0x4

main: pid: 12361, sleep(10)...


={============================================================================
*kt_linux_core_050* linux-mem-mapping oom-killer

*LPI-49.9* MAP_NORESERVE and Swap Space Overcommitting

Some applications create large (usually private anonymous) mappings, but use
only a small part of the mapped region. For example, certain types of
scientific applications allocate a very large array, but operate on only a few
widely separated elements of the array (a so-called sparse array).  

If the kernel always allocated (or reserved) enough swap space for the whole
of such mappings, then a lot of swap space would potentially be wasted.
Instead, the kernel can reserve swap space for the pages of a mapping only as
they are actually required (i.e., when the application accesses a page). This
approach is called `lazy swap reservation`, and has the advantage that the
total virtual memory used by applications can exceed the total size of RAM
plus swap space.

To put things another way, lazy swap reservation allows `swap space` to be
`overcommitted`. This works fine, as long as all processes don’t attempt to
access the entire range of their mappings. However, if all applications do
attempt to access the full range of their mappings, RAM and swap space will be
exhausted. 

In this situation, the kernel reduces memory pressure by killing one or more
of the processes on the system. Ideally, the kernel attempts to select the
process causing the memory problems (see the discussion of the OOM killer
    below), but this isn’t guaranteed.  

For this reason, we may choose to prevent lazy swap reservation, instead
forcing the system to allocate all of the necessary swap space when the
mapping is created.  How the kernel handles reservation of swap space is
controlled by the use of the MAP_NORESERVE flag when calling mmap(), and via
/proc interfaces that affect the system-wide operation of swap space
overcommitting. These factors are summarized in Table 49-4.


*proc-overcommit*
How the kernel handles reservation of swap space is controlled by the use of
the MAP_NORESERVE flag when calling mmap(), and via /proc interfaces that
affect the `system-wide operation of swap space overcommitting`

       MAP_NORESERVE
              Do not reserve swap space for this mapping.  When swap space is
              reserved, one has the guarantee that it is possible to modify
              the mapping.  When swap space is not reserved one  might  get
              SIGSEGV  upon a write if no physical memory is available.  See
              also the discussion of the file /proc/sys/vm/overcommit_memory
              in proc(5).  In kernels before 2.6, this flag had effect only
              for private writable mappings.


The Linux-specific /proc/sys/vm/overcommit_memory file contains an integer
value that controls the kernel’s handling of swap space overcommits. Linux
versions before 2.6 differentiated only two values in this file: 

-sh-3.2# cat /proc/sys/vm/overcommit_memory
0


Table 49-4: Handling of swap space reservation during mmap()

overcommit_memory value | MAP_NORESERVE specified in mmap() call?
                        | No                        Yes
 --------------------------------------------------------------------
0                       | Deny obvious overcommits  Allow overcommits

0 meaning deny obvious overcommits 
  (subject to the use of the MAP_NORESERVE flag)

Denying obvious overcommits means that new mappings whose size doesn’t exceed
the amount of currently available free memory are permitted.

1  meaning that overcommits should be permitted in all cases.

2 (since Linux 2.6) Strict overcommitting


// means that can do overcommit by using MAP_NORESERVE in mmap call

The overcommit_ratio value is an integerexpressing a percentagecontained in
the Linux-specific /proc/sys/vm/overcommit_ratio file. The default value
contained in this file is 50, meaning that the kernel can overallocate up to
50% of the size of the system’s RAM, and this will be successful, as long as
not all processes try to use their full allocation.

-sh-3.2# cat /proc/sys/vm/overcommit_ratio
50

systemutil_thread.c:1462:  
if (MAP_FAILED == 
    (ptMem = mmap(0, stackSize + 2 * g_stack_guard_size, 
                  PROT_NONE, MAP_NORESERVE | MAP_PRIVATE | MAP_ANON, -1, 0)))


When a child process inherits a mapping across a fork(), it inherits the
  MAP_NORESERVE setting for the mapping. The MAP_NORESERVE flag is not
  specified in SUSv3, but it is supported on a few other UNIX implementations.


The OOM killer

Above, we noted that when we employ lazy swap reservation, memory may become
exhausted if applications attempt to employ the entire range of their
mappings. In this case, the kernel relieves memory exhaustion by killing
processes.

The kernel code dedicated to selecting a process to kill when memory is
exhausted is commonly known as the out-of-memory (OOM) killer. The OOM killer
tries to choose the best process to kill in order to relieve the memory
exhaustion, where “best” is determined by a range of factors. For example, the
more memory a process is consuming, the more likely it will be a candidate for
the OOM

killer. Other factors that increase a process’s likelihood of selection are
forking to create many child processes and having a low nice value (i.e., one
    that is greater than 0). 

The kernel disfavors killing the following:

* processes that are privileged, since they are probably performing important
  tasks;
 
* processes that are performing raw device access, since killing them may
  leave the device in an unusable state; and

* processes that have been running for a long time or have consumed a lot of
  CPU, since killing them would result in a lot of lost “work.”

To kill the selected process, the OOM killer delivers a SIGKILL signal.

The Linux-specific `/proc/PID/oom_score` file, available since kernel 2.6.11,
    shows the weighting that the kernel gives to a process if it is necessary
      to invoke the OOM killer. The greater the value in this file, the more
      likely the process is to be selected, if necessary, by the OOM killer. 
  
The Linux-specific /proc/PID/oom_adj file, also available since kernel 2.6.11,
can be used to influence the oom_score of a process. This file can be set to
  any value in the range 16 to +15, where negative values decrease the
  oom_score and positive values increase it. The special value 17 removes the
  process altogether as a candidate for selection by the OOM killer. For
  further details, see the proc(5) manual page.


-sh-3.2# cat /proc/{535,550,560,563,565,566}/oom_score
498
20
758077
1240
7446
831


DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING oom-killer: gfp_mask=0xd2, order=0
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING Call Trace:
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING [<8000bf08>] dump_stack+0x20/0x50
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING [<800562e8>] out_of_memory+0x54/0x1dc
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING [<80057ea0>] __alloc_pages+0x268/0x328
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING [<8006b094>] __vmalloc_area_node+0x100/0x190
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING [<c017afb4>] cdi_nvram_init+0xb00/0x15cc [extra_drivers]
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING 
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING Mem-info:
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING DMA per-cpu:
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING cpu 0 hot: high 90, batch 15 used:83
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING cpu 0 cold: high 30, batch 7 used:28
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING DMA32 per-cpu: empty
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING Normal per-cpu: empty
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING HighMem per-cpu: empty
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING Free pages:        2896kB (0kB HighMem)
DRV: ^[2017:11:03 02:00:44]0946797286.974116 KERN_WARNING Active:26828 inactive:6599 dirty:1 writeback:0 unstable:0 free:724 slab:2963 mapped:151 pagetables:120
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING DMA free:2896kB min:2048kB low:2560kB high:3072kB active:107312kB inactive:26396kB present:262144kB pages_scanned:644 all_unreclaimable? no
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING lowmem_reserve[]: 0 0 0 0
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING DMA32 free:0kB min:0kB low:0kB high:0kB active:0kB inactive:0kB present:0kB pages_scanned:0 all_unreclaimable? no
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING lowmem_reserve[]: 0 0 0 0
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING Normal free:0kB min:0kB low:0kB high:0kB active:0kB inactive:0kB present:0kB pages_scanned:0 all_unreclaimable? no
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING lowmem_reserve[]: 0 0 0 0
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING HighMem free:0kB min:128kB low:128kB high:128kB active:0kB inactive:0kB present:0kB pages_scanned:0 all_unreclaimable? no
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING lowmem_reserve[]: 0 0 0 0
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING DMA: 350*4kB 3*8kB 0*16kB 2*32kB 0*64kB 1*128kB 1*256kB 0*512kB 1*1024kB 0*2048kB 0*4096kB = 2896kB
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING DMA32: empty
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING Normal: empty
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING HighMem: empty
DRV: ^[2017:11:03 02:00:44]0946797286.975116 KERN_WARNING Free swap:            0kB
DRV: ^[2017:11:03 02:00:44]0946797286.983116 KERN_WARNING 65536 pages of RAM
DRV: ^[2017:11:03 02:00:44]0946797286.983116 KERN_WARNING 0 pages of HIGHMEM
DRV: ^[2017:11:03 02:00:44]0946797286.983116 KERN_WARNING 24541 reserved pages
DRV: ^[2017:11:03 02:00:44]0946797286.983116 KERN_WARNING 2859 pages shared
DRV: ^[2017:11:03 02:00:44]0946797286.983116 KERN_WARNING 0 pages swap cached
DRV: ^[2017:11:03 02:00:44]0946797286.984116 KERN_ERROR Out of Memory: Kill process 1568 (SCD_Process) score 773870 and children.
DRV: ^[2017:11:03 02:00:44]0946797286.984116 KERN_ERROR Out of memory: Killed process 1572 (MW_Process).


linux-2.6.18.8/mm/oom_kill.c

Documentation/sysctl/vm.txt

This file contains the documentation for the sysctl files in
/proc/sys/vm and is valid for Linux kernel version 2.2.

The files in this directory can be used to tune the operation
of the virtual memory (VM) subsystem of the Linux kernel and
the writeout of dirty data to disk.

Default values and initialization routines for most of these
files can be found in mm/swap.c.

Currently, these files are in /proc/sys/vm:
- overcommit_memory
- page-cluster
- dirty_ratio
- dirty_background_ratio
- dirty_expire_centisecs
- dirty_writeback_centisecs
- max_map_count
- min_free_kbytes
- laptop_mode
- block_dump
- drop-caches
- zone_reclaim_mode
- min_unmapped_ratio
- min_slab_ratio
- panic_on_oom


=============================================================

panic_on_oom

This enables or disables panic on out-of-memory feature.  If this is set to 1,
the kernel panics when out-of-memory happens.  If this is set to 0, the kernel
will kill some rogue process, called oom_killer.  Usually, oom_killer can kill
rogue processes and system will survive.  If you want to panic the system
rather than killing rogue processes, set this to 1.

The default value is 0.


={============================================================================
*kt_linux_core_150* linux-thread
  
LPI 29 Threads Introduction

A process is an instance of an executing program and a thread is an instance of
an executing a task. In other words, a process is `processor-abstract`, an
abstract entity defined by kernel and with allocated resources in order to
execute a program.  

UNIX programs have a single thread of execution: the CPU processes instructions
for a single logical flow of execution through the program. In a multithreaded
  program, there are multiple, independent, concurrent logical
    `flows-of-execution` within the same process.

note:
A process is an instance of `processor-abstract` and a thread is an instance of
single flow of execution.

<on-linux>
Two virtualisation in Linux: processer and memory.

Linux don't distinguish a process(task) with a thread. The thread is just a
special process: 

* Linux kernel scheduler schedules based on 'thread'.
 
* Linux kernel has a double-linked list which has <thread_info> struct element
  which has thread_struct*.

* Thread is special since it shares resource (open files, pending signals,
  internal kernel data, process state, address space, text and data section)
  with others threads.

note:
A process is a single thread that don't share resources.

This is clear when see how to create a thread:

clone( CLONE_VM| CLONE_FS| CLONE_FILES| CLONE_SIGHAND, 0);

to create a process:

clone(SIGCHLD, 0);

Two usual steps to create a process:

fork();  // copy a child from a parent and actually use clone() call
exec*()  // load a new program text.


Figure 29-1: Four threads executing in a process (Linux/x86-32)

<process-model>
Traditional UNIX approach to achieving concurrency by creating multiple
processes:

* It is difficult to share information between processes. Since the parent and
  child don't share memory other than the read-only text segment, we must use
  IPC.

* Process creation with fork() is relatively expensive although copy-on-write
  is used. The need to duplicate various process attributes such as page tables
  and file descriptor tables means that a fork() call is still time-consuming.

<thread-model>
* Sharing information between threads is easy and fast since it is just a matter
  of copying data into shared (global or heap) variables. However, needs
  syncronization.

  All of these threads are independently executing the same program, and they
  all share the same global memory, including the initialized data,
  uninitialized data, and heap segments.

* Thread creation is faster because many of the attributes that must be
  duplicated in a child created by fork() are `instead-shared` between threads
  such as page table.


<thread-model-disadvantages>
29.9 Threads Versus Processes

* Buggy thread can damage all of the threads in the process. less protection.
  A bug in one thread (e.g., modifying memory via an incorrect pointer) can
  damage all of the threads in the process, since they share the same address
  space and other attributes. By contrast, processes are more isolated from
  one another.

* More threads means more memory and `context-switching`.

* More efforts to ensure thread-safe. *When programming with threads, we need
  to ensure that the functions we call are thread-safe or are called in a
  thread-safe manner. Multiprocess applications don’t need to be concerned
  with this.

* Each thread is competing for use of the finite virtual address space of a
  host process
  Although the available virtual address space is large, this factor may be a
  significant limitation for processes employing large numbers of threads or
  threads that require large amounts of memory. By contrast, separate
  processes can each employ the full range of available virtual memory

* Desirable to avoid the use of signals in multi-threaded since requires
  careful designs.


<shared-attributes-between-threads>
The attributes that are shared; in other words, global attributes to a process:

the same global memory, process code and most data;
process ID and parent process ID;
process group ID and session ID;
controlling terminal
process credentials (user and group IDs);

open file descriptors;
signal dispositions;
file system-related information: umask, current working directory, and root directory;
resource limits;

<not-shared-attributes-between-threads> thread specific
thread ID (Section 29.5);
signal mask;
thread-specific data (Section 31.3);
alternate signal stack (sigaltstack());
the `errno` variable;
realtime scheduling policy and priority (Sections 35.2 and 35.3);
capabilities (Linux-specific, described in Chapter 39); and
stack (local variables and function call linkage information).
registers including PC and SP


{multithreaded-vs-singlethreaded}
The mutiltithreaded means that a single process has multiple threads. The
singlethreaed means that a single process has a single thread. MT has less IPC
but prone to error because shares resources; less protection. ST has more IPC
but more protection. 

note: tradeoff between IPC and protection.


={============================================================================
*kt_linux_core_152* linux-thread-source-and-ex

POSIX.1 threads approved in 1995 is a POSIX standard for threads. The standard
defines an API for creating and manipulating threads. The two main Linux
threading implementations - LinuxThreads and Native POSIX Threads Library (NPTL)
- deviate from the standard.


{source}
# from uclibc source

uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\pthread.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\rwlock.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\pthread.h
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\bits\pthreadtypes.h
(_pthread_rwlock_t)


<ex>
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <pthread.h>

static void * threadFunc(void *arg)
{
    const char *s = (char *) arg;
    printf("%s", s);
    printf("sleeps for 30s\n");
    sleep(30);
    printf("wakes up\n");
    return (void *) strlen(s);
}

int main(int argc, char *argv[])
{
    pthread_t t1;
    void *res;
    int s;
    s = pthread_create(&t1, NULL, threadFunc, "Hello world\n");
    if (s != 0)
    {
        printf("pthread_create failed");
        exit(EXIT_FAILURE);
    }

    printf("Message from main()\n");

    s = pthread_join(t1, &res);
    if (s != 0)
    {
        printf("pthread_join failed");
        exit(EXIT_FAILURE);
    }

    printf("Thread returned %ld\n", (long) res);
    exit(EXIT_SUCCESS);
}


={============================================================================
*kt_linux_core_153* linux-thread-compile

On Linux, programs that use the Pthreads API must be compiled with the cc
-pthread option. The effects of this option include the following:

1. The _REENTRANT preprocessor macro is defined. This causes the declarations
of a few reentrant functions to be exposed.

2. The program is linked with the libpthread library (the equivalent of
    -lpthread).

gcc -pthread sample.c

3.12 Options Controlling the Preprocessor

-pthread
Define additional macros required for using the POSIX threads library. You
should use this option consistently for both compilation and linking. This
option is supported on GNU/Linux targets, most other Unix derivatives, and
also on x86 Cygwin and MinGW targets.

3.14 Options for Linking

-pthread
Link with the POSIX threads library. This option is supported on GNU/Linux
targets, most other Unix derivatives, and also on x86 Cygwin and MinGW
targets. On some targets this option also sets flags for the preprocessor, so
it should be used consistently for both compilation and linking.


={============================================================================
*kt_linux_core_154* linux-thread-create-and-exit

<pthread-create>
#include <pthread.h>

// return 0 if okay, positive Exxx on error which is different from most
// system calls. 
//
// If need multiple arguments to the function, must package them into a
// structure and then pass the address of this as the single argument to the
// start function.
//
// The tid argument points to a buffer of type pthread_t into which the unique
// identifier for this thread is 'copied' before pthread_create() returns.
// This identifier can be used in later Pthreads calls to refer to the thread.
// The arg should be in global or heap.
//
// If attr is NULL, uses default values.
//
// EAGAIN : when cannot create a new thread because exceeded the limit on the
// number of threads

int pthread_create( pthread_t *tid, const pthread_attr_t *attr, void *(*func)(void*), void *arg);


<pthread-exit>
// The execution of a thread terminates in one of the following ways: 
//
// o The thread's start function performs a return specifying a return value
// for the thread. This is equivalent to pthread_exit()  
// 
// o The thread calls pthread_exit() which can be called in any func called by
// start func.
//
// o The thread is canceled using pthread_cancel()
//
// * `any-of-the-threads` calls exit(), or the main thread performs a return
// (in the main() function), which causes 'all' threads in the process to
// terminate immediately.
// 
// @retval
//
// Specifies a return value that can be obtained in 'another' thread by
// calling pthread_join(). The value pointed to by retval should 'not' be
// located on the thread's stack, since the contents of that stack become
// undefined on thread termination. The same statement applies to the value
// given to a return statement in the thread's start function.
//
// <continue-after-main-thread-exit> If the main thread calls pthread_exit()
// instead of calling exit() or performing a return, then the other threads
// continue to execute.

void pthread_exit(void *retval);


{pthread-join} 
// return 0 if okay, positive Exxx on error. 
//
// To 'wait' for a given thread to terminate. If that thread has already
// terminated, pthread_join() returns 'immediately'.
//
// If retval is a non-NULL pointer, then it receives a copy of the terminated
// thread's return value - that is, the value that was specified when the
// thread performed a return or called pthread_exit().

int pthread_join( pthread_t thread, void **retval );

<join-restriction>
Calling pthread_join() for a thread ID that has been previously joined can
lead to 'unpredictable' behavior; for example, it might instead join with a
thread created later that happened to reuse the same thread ID.


{pthread-detach}
int pthread_detach(pthread_t tid);

Thread is either 'joinable'(the 'default') or 'detached'. 

To make the specified thread detached and often used to detach itself or can
create detached thread when create it by using attr setting.

pthread_detach( pthread_self() );

By default, a thread is joinable, meaning that when it terminates, another
thread can obtain its return status using pthread_join(). 

Sometimes, we don't care about the thread's return status; we simply want the
system to automatically clean up and remove the thread when it terminates. In
this case, can mark the thread as detached, by making a call to
pthread_detach().

Detaching a thread does 'not' make it 'immune' to a call to `exit()` in another
thread or a return in the main thread. In such an event, all threads in the
process are immediately terminated, 'regardless' of whether they are joinable or
detached. 
    
To put things another way, pthread_detach() simply controls what happens after a
thread terminates, not how or when it terminates.


{zombie-thread} *zombie-process*
If a thread is not detached, then we must join with it using pthread_join().
If not, then, when the thread terminates, it produces the thread equivalent of
a `zombie-process`. Aside from wasting system resources, if enough thread
zombies accumulate, we won't be able to create additional threads.


{what-join-do}
The task that pthread_join() performs for threads is similar to that performed
by waitpid() for processes. However, there are some notable differences:

1. Threads are 'peers'. 'any' thread in a process can use pthread_join() to join
with any other thread in the process. For example, if thread A creates thread B,
     which creates thread C, then it is possible for thread A to join with
     thread C, or vice versa. This differs from the 'hierarchical' relationship
     between processes.

When a parent process creates a child using fork(), it is the only process that
can wait() on that child. There is no such relationship between the thread that
calls pthread_create() and the resulting new thread.

2. There is no way of saying "join with any thread" (for processes, we can do
    this using the call waitpid(-1, &status, options)); nor is there a way to do
a nonblocking join (analogous to the waitpid() WNOHANG flag). There are ways to
achieve similar functionality using condition variables; we show an example in
Section 30.2.4.


={============================================================================
*kt_linux_core_157* linux-thread-attribute

29.8 Thread Attributes

Attributes include information such as the location and size of the thread’s
stack, the thread’s scheduling policy and priority (akin to the process
    realtime scheduling policies and priorities), and whether the thread is
joinable or detached.


To override the default and normally take the detault using the attr arg as a
NULL. Attributes are a way to specify behavior that is different from the
default. When a thread is created with pthread_create or when a
synchronization variable is initialized, an attribute object can be specified.
However the default atributes are usually sufficient for most applications. 

Note: Attributes are specified [only] at thread creation time; they cannot be
altered while the thread is being used. {Q} really?

Thus three functions are usually called in tandem when setting attribute

Thread attibute intialisation 

pthread_attr_init() create a default pthread_attr_t tattr. The function
pthread_attr_init() is used to initialize object attributes to their default
values. The storage is allocated by the thread system during execution. note:
once the thread has been creted, the attribute object is no longer needed, and
so is destoryed.

Thread attribute value change (unless defaults appropriate) 

A variety of pthread_attr_*() functions are available to set individual
attribute values for the pthread_attr_t tattr structure. (see below).  

Thread creation 

A call to pthread_create() with approriate attribute values set in a
pthread_attr_t structure. 
 

<ex>
The following code fragment should make this point clearer: 

#include <pthread.h> 

pthread_attr_t tattr; // note: can be a local var
pthread_t tid;
void *start_routine;
void arg
int ret;

ret = pthread_attr_init(&tattr);
ret = pthread_attr_*(&tattr,SOME_ATRIBUTE_VALUE_PARAMETER);
ret = pthread_create(&tid, &tattr, start_routine, arg);
ret = pthread_attr_destroy(&tattr);

In order to save space, code examples mainly focus on the attribute setting
  functions and the intializing and creation functions are ommitted. These
  must of course be present in all actual code fragtments. 

An attribute object is opaque, and cannot be directly modified by assignments.
A set of functions is provided to initialize, configure, and destroy each
object type. Once an attribute is initialized and configured, it has
process-wide scope. The suggested method for using attributes is to configure
all required state specifications at one time in the early stages of program
execution. The appropriate attribute object can then be referred to as needed.
Using attribute objects has two primary advantages: 

First, it adds to code portability. Even though supported attributes might
vary between implementations, you need not modify function calls that create
thread entities because the attribute object is hidden from the interface. If
the target port supports attributes that are not found in the current port,
    provision must be made to manage the new attributes. This is an easy
      porting task though, because attribute objects need only be initialized
      once in a well-defined location. 

Second, state specification in an application is simplified. As an example,
consider that several sets of threads might exist within a process, each
  providing a separate service, and each with its own state requirements. At
  some point in the early stages of the application, a thread attribute object
  can be initialized for each set. All future thread creations will then refer
  to the attribute object initialized for that type of thread. The
  initialization phase is simple and localized, and any future modifications
  can be made quickly and reliably. Attribute objects require attention at
  process exit time. When the object is initialized, memory is allocated for
  it. This memory must be returned to the system. The pthreads standard
  provides function calls to destroy attribute objects. 


{pthread-example-in-FOSH}
/* Assert throughout that the POSIX calls worked. If not, HFL cannot be
 * guaranteed to work. */
resPOSIX = pthread_attr_init(&threadAttrs); HFL_DEBUGMGR_ASSERT(resPOSIX ==
    0);


/* The stacksize attribute defines the minimum stack size (in bytes) allocated for the created
 * threads stack.
 *
 * int pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize);
 */
resPOSIX = pthread_attr_setstack(&threadAttrs, ptThreadInfo->pvStack,(size_t)ptThreadInfo->szStack);
/* If this assert is triggered, it might be because of not aligning address to a boundary of 8. */
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * To set the other scheduling policy: 
 */
resPOSIX = pthread_attr_setschedpolicy(&threadAttrs, SCHED_RR);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * The example to change prio:
 *
 * sched_param param;
 * param.sched_priority = 30;
 * ret = pthread_attr_setschedparam (&tattr, &param);
 * 
 * used to set/inquire a current thread's priority of scheduling.
 * 
 * int pthread_attr_setschedparam(pthread_attr_t *attr, const struct sched_param *param);
 * int pthread_attr_getschedparam(const pthread_attr_t *attr, struct sched_param *param);
 *
 * {Q} {pthread-schedule-at-runtime} is it possible as attr is only at creation?
 */
resPOSIX = pthread_attr_setschedparam(&threadAttrs, &tSchedParam);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);

resPOSIX = pthread_attr_setscope(&threadAttrs, ContentionScope);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* The pthread_attr_setinheritsched() function sets the scheduling parameter inheritance state
 * attribute in the specified attribute object. The thread's scheduling parameter inheritance state
 * determines whether scheduling parameters are explicitly specified in this attribute object, or if
 * scheduling attributes should be inherited from the creating thread. Valid settings for
 * inheritsched include:
 * 
 * PTHREAD_INHERIT_SCHED Scheduling parameters for the newly created thread are the same as those of
 * the creating thread.
 * 
 * PTHREAD_EXPLICIT_SCHED Scheduling parameters for the newly created thread are specified in the
 * thread attribute object.
 */
resPOSIX = pthread_attr_setinheritsched(&threadAttrs, PTHREAD_EXPLICIT_SCHED);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);

resPOSIX = pthread_create(&(ptThreadInfo->idThread), &threadAttrs, pfThreadMain, pvParam);


={============================================================================
*kt_linux_core_157* linux-thread-safe linux-thread-specific-data

LPI-31.1 Thread Safety (and Reentrancy Revisited)

<thread-safety-is> *thread-safe*
A function is said to be `thread-safe` if it can safely be invoked by multiple
threads at the same time.

There are various methods of rendering a function thread-safe.


<serialisation>
One way is to associate a mutex with the function, lock that mutex when the
function is called, and unlock it when the mutex returns. We say that access
to the function is `serialized`.


<critical-section>
A more sophisticated solution is to associate the mutex with a shared
variable.  We then determine which parts of the function are critical sections
that access the shared variable, and acquire and release the mutex only during
the execution of these critical sections. This allows multiple threads to
execute the function at the same time and to operate in parallel, except when
more than one thread needs to execute a critical section.


SUSv3 are required to be implemented in a thread-safe manner, except
those listed in Table 31-1.

Table 31-1: Functions that SUSv3 does not require to be thread-safe


<reentrant-function> *thread-reentrant*
Although the use of critical sections to implement thread safety is a
significant improvement over the use of per-function mutexes, it is still
somewhat inefficient because there is a cost to locking and unlocking a mutex.

A reentrant function achieves thread safety without the use of mutexes. It
does this by avoiding the use of global and static variables. It does this by
avoiding the use of global and static variables. Any information that must be
returned to the caller, or maintained between calls to the function, is stored
in buffers allocated by the caller.

However, not all functions can be made reentrant. SUSv3 specifies reentrant
equivalents with names ending with the suffix _r.


<thread-specific-data>

31.2 One-Time Initialization

Sometimes, a threaded application needs to ensure that some initialization
action occurs just once, regardless of how many threads are created. For
example, a mutex may need to be initialized with special attributes using
pthread_mutex_init(), and that initialization must occur just once. If we are
creating the threads from the main program, then this is generally easy to
achievewe perform the initialization before creating any threads that depend
on the initialization. 

However, in a library function, this is not possible, because the calling
program may create the threads before the first call to the library function.
Therefore, the library function needs a method of performing the
initialization the first time that it is called from any thread.

#include <pthread.h>
int pthread_once(pthread_once_t *once_control, void (*init)(void));
Returns 0 on success, or a positive error number on error

The once_control argument is a pointer to a variable that must be statically
initialized with the value PTHREAD_ONCE_INIT:

pthread_once_t once_var = PTHREAD_ONCE_INIT;


31.3 Thread-Specific Data

The most efficient way of making a function thread-safe is to make it
reentrant. All new library functions should be implemented in this way.
However, for an existing nonreentrant library function (one that was perhaps
    designed before the use of threads became common), this approach usually
requires changing the function’s interface, which means modifying all of the
programs that use the function. 

Thread-specific data is a technique for making an existing function
thread-safe `without changing its interface.`

Thread-specific data allows a function to maintain a separate copy of a
variable for each thread that calls the function. This allows the function to
maintain per-thread information between calls to the function


#include <pthread.h>
int pthread_key_create(pthread_key_t *key, void (*destructor)(void *));

Returns 0 on success, or a positive error number on error
Different (i.e., independent) functions may each need thread-specific data.
Each function needs a method of identifying its thread-specific data (`a key`),
as distinct from the thread-specific data used by other functions.

The function creates a key, which is the means of differentiating the
thread-specific data item used by this function from the thread-specific data
items used by other functions. The key is created by calling the
pthread_key_create() function. Creating a key needs to be done only once, when
the first thread calls the function.  For this purpose, pthread_once() is
employed.


#include <pthread.h>

int pthread_setspecific(pthread_key_t key, const void *value);
Returns 0 on success, or a positive error number on error

void *pthread_getspecific(pthread_key_t key);
Returns pointer, or NULL if no thread-specific data isassociated with key

In order to save a pointer to the storage allocated in the previous step, the
function employs two Pthreads functions: pthread_setspecific() and
pthread_getspecific().  A call to pthread_setspecific() is a request to the
Pthreads implementation to say “save this pointer, recording the fact that it
is associated with a particular key (the one for this function) and a
particular thread (the calling thread).” Calling pthread_getspecific()
performs the complementary task, returning the pointer previously associated
with a given key for the calling thread. If no pointer was previously
associated with a particular key and thread, then pthread_getspecific()
returns NULL. This is how a function can determine that it is being called for
the first time by this thread, and thus must allocate the storage block for
the thread.


A typical implementation (NPTL is typical), involves the following arrays:

o a single global (i.e., process-wide) array of information about
thread-specific data keys; and

o a set of per-thread arrays, each containing pointers to all of the
thread-specific data blocks allocated for a particular thread (i.e., this
    array contains the pointers stored by calls to pthread_setspecific()).


Figure 31-3: Data structure used to implement thread-specific data (TSD)
pointers

// pthread implementation know mapping key(which function) and thread which
// calls function

process-wide array              thread #0

keys[0]                         tsd[0]  -> buffer allocaed for this thread 
keys[1]                         tsd[1]
keys[2]                         tsd[2]
...                             ...                                          

                                thread #1
                                                                           
                                tsd[0]  -> buffer allocaed for this thread 
                                tsd[1]
                                tsd[2]
                                ...                                         

the key is simply index of keys array which directly maps to tsd array of each
thread.


When a thread is first created, all of its thread-specific data pointers are
`initialized to NULL.` This means that when our library function is called by
a thread for the first time, it must begin by using pthread_getspecific() to
check whether the thread already has an associated value for key. If it does
not, then the function allocates a block of memory and saves a pointer to the
block using pthread_setspecific(). We show an example of this in the
thread-safe strerror() implementation presented in the next section.


31.3.4 Employing the Thread-Specific Data API

31.3.5 Thread-Specific Data Implementation Limits


<linux-thread-local-storage>

31.4 Thread-Local Storage

Like thread-specific data, thread-local storage provides persistent per-thread
storage. `This feature is non-standard`, but it is provided in the same or a
similar form on many other UNIX implementations (e.g., Solaris and FreeBSD).
The main advantage of thread-local storage is that it is `much simpler` to use
than thread-specific data. To create a thread-local variable, we simply
include the __thread specifier in the declaration of a global or static
variable:

static __thread buf[MAX_ERROR_LEN];

Each thread has its own copy of the variables declared with this specifier.
The variables in a thread’s thread-local storage persist until the thread
terminates, at which time the storage is automatically deallocated.

Thread-local storage requires support from the kernel (provided in Linux 2.6),
  the Pthreads implementation (provided in NPTL), and the C compiler (provided
      on x86-32 with gcc 3.3 and later).


={============================================================================
*kt_linux_core_157* linux-thread-stack

33.1 Thread Stacks

Each thread has its own stack whose size is fixed when the thread is created.
On Linux/x86-32, for all threads other than the main thread, the default size
of the per-thread stack is 2 MB. The main thread has a much larger space for
stack growth. Occasionally, it is useful to change the size of a thread's
stack. 

The pthread_attr_setstacksize() function sets a thread attribute (Section
    29.8) that determines the size of the stack in threads created using the
thread attributes object. The related pthread_attr_setstack() function can be
used to control both the size and the location of the stack, but setting the
location of a stack can decrease application portability.

One reason to change the size of per-thread stacks is to allow for larger
stacks for threads that allocate large automatic variables or make nested
function calls of great depth (perhaps because of recursion).

Alternatively, an application may want to reduce the size of per-thread stacks
to allow for a greater number of threads within a process.

The minimum stack that can be employed on a particular architecture can be
determined by calling sysconf(_SC_THREAD_STACK_MIN). For the NPTL
implementation on Linux/x86-32, this call returns the value 16,384.


={============================================================================
*kt_linux_core_157* linux-thread-nptl

33.4 Thread Implementation Models

The differences between these implementation models hinge on how threads are
mapped onto `kernel scheduling entities` (KSEs), which are the units to which
the kernel allocates the CPU and other system resources. In traditional UNIX
implementations that predate threads, the term kernel scheduling entity is
synonymous with the term process.

Many-to-one (M:1) implementations (user-level threads)
Many-to-many (M:N) implementations (two-level model)

One-to-one (1:1) implementations (kernel-level threads)

In a 1:1 threading implementation, each thread maps onto a separate KSE. The
kernel handles each thread’s scheduling separately. Thread synchronization
operations are implemented using system calls into the kernel.

Despite these disadvantages, a 1:1 implementation is usually preferred over an
M:1 implementation. Both of the Linux threading implementations-LinuxThreads
and NPTL-employ the 1:1 model.


At this point, it is worth emphasizing that the LinuxThreads implementation is
now `obsolete`; it is not supported in glibc 2.4 and later. All new thread
library development occurs only in NPTL.

33.5.2 NPTL

Threads are created using a clone() call that specifies the following flags:

CLONE_VM | CLONE_FILES | CLONE_FS | CLONE_SIGHAND |
CLONE_THREAD | CLONE_SETTLS | CLONE_PARENT_SETTID |
CLONE_CHILD_CLEARTID | CLONE_SYSVSEM

NPTL threads share all of the information that LinuxThreads threads share, and
more. The CLONE_THREAD flag means that a thread is placed in the same thread
group as its creator and shares the same process ID and parent process ID. The
CLONE_SYSVSEM flag means that a thread shares System V semaphore undo values
with its creator.

When we use ps(1) to list a multithreaded process running under NPTL, we see
just a single line of output. To see information about the threads within a
process, we can use the ps -L option. see *linux-tread-id*

<how-to-check-nptl-version>
On a system providing glibc version 2.3.2 or later

$ getconf GNU_LIBPTHREAD_VERSION
NPTL 2.15

On systems with older GNU C libraries, we must do a little more work. TODO


={============================================================================
*kt_linux_core_102* how to run three threads sequencially

From Cracking the coding interview, p425, 16.5:

Suppose we have following code:

public class Foo {
  public Foo() {...}
  public void first() {...}
  public void second() {...}
  public void third() {...}
}

The same instance of Foo will be passed to three different threads. ThreadA
will call first, ThreadB will call second, and ThreadC will call third. Design
a mechanism to ensure that first is called before second and second is called
before third.

Using lock?

public class FooBad {
  public FooBad() {
    lock1 = new ReentrantLock();
    lock2 = new ReentrantLock();

    // locks all in a ctor
    lock1.lock(); lock2.lock(); 
  }
}

// already got lock1
public void first()
{
  ...
  lock1.unlock(); // finished first
}

// already got lock2
public void second()
{
  lock1.lock();   // wait first to finish
  lock1.unlock();
  ...
  lock2.unlock(); // finished second
}

public void third()
{
  lock2.lock();   // wait second to finish
  lock2.unlock();
  ...
}

This WON'T work in JAVA since a lock in JAVA is owned by the same thread which
locked in.

http://docs.oracle.com/javase/6/docs/api/java/util/concurrent/locks/ReentrantLock.html

A ReentrantLock is owned by the thread last successfully locking, but not yet
unlocking it. A thread invoking lock will return, successfully acquiring the
lock, when the lock is not owned by another thread. The method will return
immediately if the current thread already owns the lock. 

note: There's no such limitation in Linux and it's possible in the same thread
group and if threads in different group use lock, can use semaphore or lock on
the shared memory.

The mutex has ownership as well and see {mutex-ownership}.


={============================================================================
*kt_linux_core_200* linux-io-model

LPI 4. FILE I/O

<std-decs>
Or, more precisely, the program inherits copies of the shell's file
descriptors, and the shell normally operates with these three file descriptors
always open. (In an interactive shell, these three file descriptors normally
    refer to the terminal under which the shell is running.) 

If I/O redirections are specified on a command line, then the shell ensures
that the file descriptors are suitably modified before starting the program.


<proc-fdinfo>
Each process has its own set of file descriptors.

Since kernel 2.6.22, the Linux-specific files in the directory
`/proc/PID/fdinfo` can be read to obtain information about the file descriptors
of any process on the system. There is one file in this directory for each of
the process's open file descriptors, with a name that matches the number of
the descriptor. The pos field in this file shows the current file offset
(Section 4.7). The flags field is an octal number that shows the file access
mode flags and open file status flags.  (To decode this number, we need to
    look at the numeric values of these flags in the C library header files.)

# cat /proc/847/fdinfo/71
pos:	0
flags:	0200


<four-key-system-calls>

fd = open(pathname, flags, mode) 

opens the file identified by pathname, returning a file descriptor used to
refer to the open file in subsequent calls. If the file doesn't exist,
`open()` may create it, depending on the settings of the flags bit-mask
  argument. The flags argument also specifies whether the file is to be opened
  for reading, writing, or both. The mode argument specifies the permissions
    to be placed on the file if it is created by this call. If the open() call
      is not being used to create a file, this argument is ignored and can be
      omitted.


numread = read(fd, buffer, count) 

reads at 'most' count bytes from the open file referred to by fd and stores
them in buffer. The read() call returns the number of bytes 'actually' read.
If no further bytes could be read (i.e., end-of-file was encountered), read()
returns 0.  


numwritten = write(fd, buffer, count)

writes up to count bytes from buffer to the open file referred to by fd. The
write() call returns the number of bytes actually written, which may be less
than count.  


status = close(fd)

is called after all I/O has been completed, in order to release the file
descriptor fd and its associated kernel resources.


<ex-cp>
/* Transfer data until we encounter end of input or an error */
while ((numRead = read(inputFd, buf, BUF_SIZE)) > 0)
   if (write(outputFd, buf, numRead) != numRead)
      fatal("couldn't write whole buffer");

if (numRead == -1)
   errExit("read");


<universal-io-model>
We focus on I/O on disk files. However, much of the material covered here is
relevant for later chapters, since the same system calls are used for
performing I/O on all types of files, such as pipes and terminals.

All system calls for performing I/O refer to open files using a file
descriptor, a (usually small) nonnegative integer. File descriptors are used
to refer to 'all' types of open files, including pipes, FIFOs, sockets,
terminals, devices, and regular files. 

The concept of universality of I/O. This means that the same four system calls
- open(), read(), write(), and close() - are used to perform I/O on all types
of files, including devices such as terminals. 

$ ./copy test test.old           Copy a regular file
$ ./copy a.txt /dev/tty          Copy a regular file to this terminal
$ ./copy /dev/tty b.txt          Copy input from this terminal to a regular file
$ ./copy /dev/pts/16 /dev/tty    Copy input from another terminal

Universality of I/O is achieved by ensuring that each file system and device
driver implements the `same set of I/O system calls` Because details specific
to the file system or device are handled within the kernel, we can generally
ignore device-specific factors when writing application programs. When access
to specific features of a file system or device is required, a program can use
the catchall ioctl() system call (Section 4.8), which provides an interface to
features that fall outside the universal I/O model.


{syscall-open}

4.3 Opening a File: open()

#include <sys/stat.h>
#include <fcntl.h>

int open(const char *pathname, int flags, ... /* mode_t mode */);

Returns file descriptor on success, or –1 on error

/* Open existing file for reading */

fd = open("startup", O_RDONLY);
if (fd == -1)
errExit("open");

/* Open new or existing file for reading and writing, truncating to zero
 * bytes; file permissions read+write for owner, nothing for all others */

fd = open("myfile", O_RDWR | O_CREAT | O_TRUNC, S_IRUSR | S_IWUSR);
if (fd == -1)
errExit("open");

/* Open new or existing file for writing; writes should always append to end
 * of file */

fd = open("w.log", O_WRONLY | O_CREAT | O_TRUNC | O_APPEND, S_IRUSR | S_IWUSR);


<open-flags>
Table 4-3 are divided into the following groups:

File `access mode flags` 

These are the O_RDONLY, O_WRONLY, and O_RDWR flags described earlier. They can
be retrieved using the fcntl() F_GETFL operation (Section 5.3).

File `creation flags` 

These are the flags shown in the second part of Table 4-3. They control
various aspects of the behavior of the open() call, as well as options for
subsequent I/O operations. These flags can't be retrieved or changed.

Open file status flags

These are the remaining flags in Table 4-3. They can be retrieved and modified
using the fcntl() F_GETFL and F_SETFL operations (Section 5.3). These flags
are sometimes simply called the file status flags.


O_CLOEXEC (since Linux 2.6.23)

Enable the close-on-exec flag (FD_CLOEXEC) for the new file descriptor. We
describe the FD_CLOEXEC flag in Section 27.4. Using the O_CLOEXEC flag allows
a program to avoid additional fcntl() F_SETFD and F_SETFD operations to set
the close-on-exec flag. It is also necessary in multithreaded programs to
avoid the race conditions that could occur using the latter technique. These
races can occur when one thread opens a file descriptor and then tries to mark
it close-on-exec at the same time as another thread does a fork() and then an
exec() of an arbitrary program. (Suppose that the second thread manages to
    both fork() and exec() between the time the first thread opens the file
    descriptor and uses fcntl() to set the close-on-exec flag.) Such races
could result in open file descriptors being unintentionally passed to unsafe
programs. (We say more about race conditions in Section 5.1.)

O_CREAT

If the file doesn't already exist, it is created as a new, empty file. This
flag is effective even if the file is being opened only for reading. If we
specify O_CREAT, then we must supply a mode argument in the open() call;
otherwise, the permissions of the new file will be set to some random value
from the stack.


O_EXCL

This flag is used in conjunction with O_CREAT to indicate that if the file
already 'exists', it should not be opened; instead, open() should fail, with
errno set to EEXIST. In other words, this flag allows the caller to 'ensure'
that it is the process creating the file. 


<open-errors>

ENOENT

The specified file doesn’t exist, and O_CREAT was not specified, or O_CREAT
was specified, and one of the directories in pathname doesn’t exist or is a
symbolic link pointing to a nonexistent pathname (a dangling link).


{syscall-read}
#include <unistd.h>

ssize_t read(int fd, void *buffer, size_t count);

Returns number of bytes read, 0 on EOF, or -1 on error

The count argument specifies the maximum number of bytes to read. The size_t
data type is an unsigned integer type. The buffer argument supplies the
address of the memory buffer into which the input data is to be placed. This
buffer must be at least count bytes long.

A successful call to read() returns the number of bytes actually read, or 0 if
EOF is encountered.  On error, the usual -1 is returned. The ssize_t data type
is a signed integer type used to hold a byte count or a -1 error indication.  

A call to read() may read 'less' than the requested number of bytes. For a
regular file, the probable reason for this is that we were close to the end of
the file.


<not-null-terminated>
The output from read() is strange because read() doesn't place a terminating
null byte at the end of the string that printf() is being asked to print. A
moment's reflection leads us to realize that this must be so, since read() can
be used to read 'any' sequence of bytes from a file. 

In some cases, this input might be text, but in other cases, the input might
be binary integers or C structures in binary form. There is no way for read()
  to tell the difference, and so it can't attend to the C convention of null
  terminating character strings. If a terminating null byte is required at the
  end of the input buffer, we must put it there explicitly:


{syscall-write}
#include <unistd.h>

ssize_t write(int fd, void *buffer, size_t count);

Returns number of bytes written, or -1 on error


={============================================================================
*kt_linux_core_201* linux-io-ioctl

The ioctl() system call is a general-purpose mechanism for performing file and
device operations that fall `outside the universal I/O model` described earlier
in this chapter.

#include <sys/ioctl.h>

int ioctl(int fd, int request, ... /* argp */);

Value returned on success depends on request, or -1 on error

The fd argument is an open file descriptor for the device or file upon which
the control operation specified by `request` is to be performed.

Device-specific header files define constants that can be passed in the
request argument.


<ioctl-list>
IOCTL_LIST(2)              Linux Programmer’s Manual             IOCTL_LIST(2)

NAME
       ioctl_list - list of ioctl calls in Linux/i386 kernel

DESCRIPTION
       This  is  Ioctl  List 1.3.27, a list of ioctl calls in Linux/i386
       kernel 1.3.27.  It contains 421 ioctls from
       </usr/include/{asm,linux}/*.h>.  For each ioctl, its numerical value,
                                  its name, and its argument type are given.

       An argument type of const struct foo * means the argument is input to
       the kernel.  struct foo * means the kernel outputs the argument.  If
       the kernel uses the  argument  for  both input and output, this is
       marked with // I-O.

       Some ioctls take more arguments or return more values than a single
       structure.  These are marked // MORE and documented further in a
       separate section.

       This list is very `incomplete.`


={============================================================================
*kt_linux_core_201* linux-io-extended

LPI 5 File I/O: Futher Details

{automicity}
All system calls are executed atomically. By this, we mean that the kernel
guarantees that all of the steps in a system call are completed as a single
operation, without being interrupted by another process or thread.

<race-condition>
Atomicity is essential to the successful completion of some operations. In
particular, it allows us to avoid race conditions (sometimes known as race
    hazards). A race condition is a situation where the result produced by two
processes (or threads) operating on shared resources depends in an unexpected
way on the relative order in which the processes gain access to the CPU(s).


{two-cases-when-needs-automicity}
Two cases that shows race condition when do not use O_EXCL and O_APPEND flag.

<creation>
Creating a file exclusively

Using a single open() call that specifies the O_CREAT and O_EXCL flags
prevents this possibility by guaranteeing that the check and creation steps
are carried out as a single atomic uninterruptible operation.

See 5.1 for the problem description.

<appending>
When we have multiple processes appending data to the same file (e.g., a
    global log file).

if (lseek(fd, 0, SEEK_END) == -1)
   errExit("lseek");

if (write(fd, buf, len) != len)
   fatal("Partial/failed write");

Again, this is a race condition because the results depend on the order of
scheduling of the two processes. Avoiding this problem requires that the seek
to the next byte past the end of the file and the write operation happen
atomically. This is what opening a file with the `O_APPEND flag guarantees`

note: man 2 open shows

       O_APPEND
              The  file  is opened in append mode.  Before each write(2), the
              file offset is positioned at the end of the file, as if with
              lseek(2).  O_APPEND may lead to corrupted files on NFS
              filesystems if more than one process appends data to a file at
              once.  This is because NFS does not support appending to a file,
              so the client kernel  has  to  simulate  it,  which can't be
              done without a race condition.


{fcntl}

5.2 File Control Operations: fcntl()

#include <fcntl.h>

int fcntl(int fd, int cmd, ...);

Return on success depends on cmd, or -1 on error

One use of fcntl() is to retrieve or modify the access mode and open file
status flags of an open file.

Using fcntl() to modify open file status flags is particularly useful in the
following cases:

  The file was not opened by the calling program, so that it had no control
  over the flags used in the open() call. e.g., the file may be one of the
  three standard descriptors that are opened before the program is started.

  The file descriptor was obtained from a system call other than open().
  Examples of such system calls are pipe(), which creates a pipe and returns
  two file descriptors referring to either end of the pipe, and socket(),
  which creates a socket and returns a file descriptor referring to the
  socket.


{file-descriptors-and-open-files} *open-file-table*

5.4 Relationship Between File Descriptors and Open Files

It is possible-and useful-to have multiple descriptors referring to the same
open file. These file descriptors may be open in the same process or in
different processes. Three data structures maintained by the 'kernel':

File descriptor   ->  Open file   ->    i-node table
    table               table
    fd x 					(system-wide) 		(system-wide)


1. the `per-process open file descriptor table`

  * a set of flags controlling the operation of the file descriptor (there is
    just one such flag, the close-on-exec flag, which we describe in Section
    27.4); and

  * a reference to the open file description.


2. the `system-wide` table of open file `descriptions`, `open file table`

An open file description stores all information relating to an open file,
including:

  * the current file offset (as updated by read() and write(), or explicitly
    modified using lseek());

  * status flags specified when opening the file (i.e., the flags argument to
    open());
 
  * the file access mode (read-only, write-only, or read-write, as specified
    in open());

  * settings relating to signal-driven I/O (Section 63.3); and

  * a reference to the i-node object for this file.


3. the file system `i-node table`

Each file system has a table of i-nodes for all files residing in the file
system. The i-node for each file includes the following information:

  * file type (e.g., regular file, socket, or FIFO) and permissions;
  * a pointer to a list of locks held on this file; and
  * various properties of the file, including its size and timestamps relating
    to different types of file operations.


Figure 5-2: Relationship between file descriptors, open file descriptions, and
i-nodes

<when-the-same-process-has-multiple-descriptors-for-the-same-file>
In process A, descriptors 1 and 20 both, fd 1 and fd 20, refer to the same
open file `description` (labeled 23). This situation may arise as a result of a
call to dup(), dup2(), or fcntl() (see Section 5.5).

<when-different-process-has-descriptor-for-the-same-file>
Descriptor 2 of process A and descriptor 2 of process B refer to a single open
file description (73). This scenario could occur after a call to fork() (i.e.,
    process A is the parent of process B, or vice versa), or if one process
passed an open descriptor to another process using a UNIX domain socket
(Section 61.13.3).

<when-different-process-has-the-same-inode>
Finally, we see that descriptor 0 of process A and descriptor 3 of process B
refer to different open file descriptions, but that these descriptions refer
to the same i-node table entry (1976)-in other words, to the same file. This
occurs because each process independently called `open()` for the same file. A
similar situation could occur if a single process opened `the same file twice`


{duplicating-descriptors}

5.5 Duplicating File Descriptors

The shell that we wish to have standard error (2) redirected to the same place
to which standard output (1) is being sent.

$ ./myscript > results.log 2>&1

The shell achieves the redirection of standard error `by duplicating` file
descriptor 2 so that it refers to the same open file description as file
descriptor 1.

Note that it is not sufficient for the shell simply to open the results.log
file twice since this makes two descriptors and descriptions: 

One reason for this is that the two file descriptors would not share a file
offset pointer, and hence could end up overwriting each other's output. (Since
    uses different description)

Another reason is that the file may not be a disk file. Consider the following
command, which sends standard error down the same pipe as standard output:
(Same problem of overwriting? not sure)

$ ./myscript 2>&1 | less


<call-dup>
The dup() call takes oldfd, an open file descriptor, and returns a new
descriptor that refers to the `same open file description` The new descriptor
is guaranteed to be `the lowest unused file descriptor`

#include <unistd.h>

int dup(int oldfd);
int dup2(int oldfd, int newfd);

Returns (new) file descriptor on success, or -1 on error

newfd = dup(1);

The dup2() system call makes a duplicate of the file descriptor given in oldfd
using the descriptor number `supplied` in newfd. If the file descriptor
specified in newfd is already open, dup2() 'closes' it first. Any error that
occurs during this close is silently ignored; safer programming practice is to
explicitly close() newfd if it is open before the call to dup2().

If we wanted the duplicate to be descriptor 2, we could use the following
technique:

close(2);         /* Frees file descriptor 2 */
newfd = dup(1);   /* Should reuse file descriptor 2 */

We could simplify the preceding calls to close() and dup() to the following:

dup2(1, 2);


<syscall-pread>
5.6 File I/O at a Specified Offset: pread() and pwrite()

The pread() and pwrite() system calls operate just like read() and write(),
    except that the file I/O is performed at the location specified by offset,
    rather than at the current file offset. The file offset is left unchanged
    by these calls.

These system calls can be particularly useful in multithreaded applications.
As we'll see in Chapter 29, all of the threads in a process share the same
file descriptor table. This means that the file offset for each open file is
global to all threads. Using pread() or pwrite(), multiple threads can
simultaneously perform I/O on the same file descriptor without being affected
by changes made to the file offset by other threads.


={============================================================================
*kt_linux_core_202* linux-io-non-blocking

LPI 5.9 Nonblocking I/O

The O_NONBLOCK flag when opening a file serves two purposes:

  If the file can't be opened immediately, then open() returns an error
  instead of blocking. One case where open() can block is with FIFOs (Section
      44.7).

  After a successful open(), `subsequent I/O operations` are also nonblocking.
  If an I/O system call can't complete immediately, then either a partial data
  transfer is performed or the system call fails with one of the errors EAGAIN
  or EWOULDBLOCK. Which error is returned depends on the system call. On
  Linux, as on many UNIX implementations, these two error constants are
  synonymous.

<nonblocking-pipe>
Nonblocking mode can be used with devices (e.g., terminals and
    pseudoterminals), pipes, FIFOs, and sockets. Because file descriptors for
pipes and sockets are not obtained using open(), we must enable this flag
using the fcntl() F_SETFL operation.

note: This is why use syscall-pipe2.

O_NONBLOCK `is generally ignored for regular files`, because `kernel buffer cache` 
ensures that `I/O on regular files does not block`, as described in Section
13.1. However, O_NONBLOCK does have an effect for regular files when mandatory
file locking is employed (Section 55.4). We say more about nonblocking I/O in
Section 44.9 and in Chapter 63.

note: non-blocking means using buffering.


={============================================================================
*kt_linux_core_203* linux-io-fs driver-api

14.1 Device Special Files (Devices)

A device special file corresponds to a device on the system. Within the
kernel, each device type has a corresponding device driver, which handles all
I/O requests for the device. A device driver is a unit of kernel code that
implements a set of operations that (normally) correspond to input and output
actions on an associated piece of hardware. 

<driver-api>
The API provided by device drivers is fixed, and includes operations
corresponding to the system calls open(), close(), read(), write(), mmap(),
and ioctl(). 

The fact that each device driver provides a consistent interface, hiding the
differences in operation of individual devices, allows for universality of I/O
(Section 4.2).

Devices can be divided into two types:

  * Character devices handle data on a character-by-character basis. Terminals
    and keyboards are examples of character devices.

  * Block devices handle data a block at a time. The size of a block depends
    on the type of device, but is typically some multiple of 512 bytes.
    Examples of block devices include disks and tape drives.

Device IDs

Each device file has a major ID number and a minor ID number. The major ID
identifies the general class of device, and is used by the kernel to look up
the appropriate driver for this type of device. The minor ID uniquely
identifies a particular device within a general class. The major and minor IDs
of a device file are displayed by the ls -l command.


14.2 Disks and Partitions

Disk drives

A hard disk drive is a mechanical device consisting of one or more `platters`
that rotate at high speed (of the order of thousands of revolutions per
    minute). Magnetically encoded information on the disk surface is retrieved
or modified by read/ write heads that move radially across the disk.
Physically, information on the disk surface is located on a set of concentric
circles called `tracks`. Tracks themselves are divided into a number of
`sectors`, each of which consists of a series of physical `blocks`.

Physical blocks are typically 512 bytes (or some multiple thereof) in size,
and represent the `smallest unit` of information that the drive can read or
  write.

Although modern disks are fast, reading and writing information on the disk
still takes significant time. The disk head must first move to the appropriate
track (seek time), then the drive must wait until the appropriate sector
rotates under the head (rotational latency), and finally the required blocks
must be transferred (transfer time). The total time required to carry out such
an operation is typically of the order of milliseconds. By comparison, modern
CPUs are capable of executing millions of instructions in this time.

Disk partitions

Each disk is divided into one or more (nonoverlapping) partitions. Each
partition is treated by the kernel as a separate device residing under the
/dev directory.


14.3 File Systems

A file system is an organized collection of regular files and directories. A
file system is created using the mkfs command. One of the strengths of Linux
is that it supports a wide variety of file systems.

<proc-filesystems>
The file-system types currently known by the kernel can be viewed in the
Linux-specific /proc/filesystems file.

File-system structure

The basic unit for allocating space in a file system is a `logical block`,
which is some multiple of `contiguous physical blocks` on the disk device on
  which the file system resides. For example, the logical block size on ext2
  is 1024, 2048, or 4096 bytes. (The logical block size is specified as an
      argument of the mkfs(8) command used to build the file system.)

note:
From here, block refers to logical block.

| partition                                            |
| boot block | super block | i-node table | data block |

Figure 14-1: Layout of disk partitions and a file system

A file system contains the following parts:

  * Boot block: 
  This is always the first block in a file system. The boot block is not used
  by the file system; rather, it contains information used to boot the
  operating system. Although only one boot block is needed by the operating
  system, all file systems have a boot block (most of which are unused).

  * Superblock: 
  This is a single block, immediately following the boot block, which contains
  parameter information about the file system, including:

  the size of the i-node table;
  the size of `logical blocks` in this file system; and
  the size of the file system in logical blocks.


  * I-node table: 
  Each file or directory in the file system has a unique entry in the i-node
  table. This entry records various information about the file. 

  * Data blocks: 
  The great majority of space in a file system is used for the blocks of data
  that form the files and directories residing in the file system.


<fs-inode>
14.4 I-nodes

A file system's i-node table contains one i-node (short for index node) for
each file residing in the file system. I-nodes are identified numerically by
their sequential location in the i-node table. The i-node number (or simply
    i-number) of a file is the first field displayed by the "ls -li" command.
The information maintained in an i-node includes the following:

  * File type (e.g., regular file, directory, symbolic link, character device).
  * Owner (also referred to as the user ID or UID) for the file.
  * Group (also referred to as the group ID or GID) for the file.
  * Access permissions for three categories of user

  * Three timestamps: <three-timestamps>
  time of `last access` to the file (shown by ls –lu), `last modification` of
  the file (the default time shown by ls –l), and time of `last status change`
  (last change to i-node information, shown by ls –lc).

  * Number of hard links to the file.
  * Size of the file in bytes.

  * Number of blocks actually allocated to the file
  measured in units of 512-byte blocks. There may not be a simple
  correspondence between this number and the size of the file in bytes, since
  a file can contain holes (Section 4.7), and thus require fewer allocated
  blocks than would be expected according to its nominal size in bytes.

  * Pointers to the data blocks of the file.

I-nodes and data block pointers in ext2

Like most UNIX file systems, the ext2 file system doesn't store the data
blocks of a file contiguously or even in sequential order. To locate the file
data blocks, the kernel maintains a set of pointers in the i-node. The system
used for doing this on the ext2 file system is shown in Figure 14-2.

note:
Removing the need to store the blocks of a file contiguously allows the file
system to use space in an efficient way. In particular, it reduces the
incidence of fragmentation of free disk space—the wastage created by the
existence of numerous pieces of noncontiguous free space, all of which are too
small to use.

Under ext2, each i-node contains 15 pointers. The first 12 of these pointers
(numbered 0 to 11 in Figure 14-2) point to the location in the file system of
the first 12 blocks of the file. 

The next pointer is a pointer to a block of pointers that give the locations
of the thirteenth and subsequent data blocks of the file. The number of
pointers in this block depends on the block size of the file system. Each
pointer requires 4 bytes, so there may be from 256 pointers (for a 1024-byte
    block size) to 1024 pointers (for a 4096-byte block size).

For even larger files, the fourteenth pointer (numbered 13 in the diagram) is
a `double` indirect pointer—it points to blocks of pointers that in turn point
to blocks of pointers that in turn point to data blocks of the file. And
should the need for a truly enormous file arise, there is a further level of
indirection: the last pointer in the i-node is a `triple`-indirect pointer.

designed to satisfy a number of requirements:

To begin with, it allows the i-node structure to be a fixed size, while at the
same time allowing for files of an arbitrary size. 

Additionally, it allows the file system to store the blocks of a file
noncontiguously, while also allowing the data to be accessed randomly via
lseek(); the kernel just needs to calculate which pointer(s) to follow. 

Finally, for small files, which form the overwhelming majority of files on
most systems, this scheme allows the file data blocks to be accessed rapidly
via the direct pointers of the i-node.


14.6 Journaling File Systems

The ext2 file system is a good example of a traditional UNIX file system, and
suffers from a classic limitation of such file systems: after a system crash,
a file-system consistency check (fsck) must be performed on reboot in order to
  ensure the integrity of the file system. 

This is necessary because, at the time of the system crash, a file update may
have been only partially completed, and the file-system metadata (directory
    entries, i-node information, and file data block pointers) may be in an
inconsistent state, so that the file system might be further damaged if these
inconsistencies are not repaired. 

A file-system consistency check ensures the consistency of the file-system
metadata. Where possible, repairs are performed; otherwise, information that
is not retrievable (possibly including file data) is discarded.

Journaling file systems eliminate the need for lengthy file-system consistency
checks after a system crash. A journaling file system logs (journals) all
metadata updates to a special on-disk journal file before they are actually
carried out. The updates are logged in groups of related metadata updates
(transactions). In the event of a system crash in the middle of a transaction,
on system reboot, the log can be used to rapidly redo any incomplete updates
  and bring the file system back to a consistent state.

note: see *sql-rollback*

<on-metadata>
Some journaling file systems ensure `only the consistency of file metadata`
Because they don't log file data, data may still be lost in the event of a
crash. The ext3, ext4, and Reiserfs file systems provide options for logging
data updates, but, depending on the workload, this may result in lower file
I/O performance. 

Two other file systems that provide journaling and a range of other advanced
features:

* The ext4 file system (http://ext4.wiki.kernel.org/) 
  is the successor to ext3. The first pieces of the implementation were added
  in kernel 2.6.19, and various features were added in later kernel versions.
  Among the planned (or already implemented) features for ext4 are extents
  (reservation of contiguous blocks of storage) and other allocation features
  that aim to reduce file fragmentation, online file-system defragmentation,
  faster file-system checking, and support for nanosecond timestamps.

* Btrfs (B-tree FS, usually pronounced “butter FS”; http://btrfs.wiki.kernel.org/) 
  is a new file system designed from the ground up to provide a range of
  modern features, including extents, writable snapshots (which provide
  functionality equivalent to metadata and data journaling), checksums on
  data and metadata, online file-system checking, online file-system
  defragmentation, space-efficient packing of small files, and space-efficient
  indexed directories. It was integrated into the kernel in version 2.6.29.


14.10 A Virtual Memory File System: tmpfs

Linux also supports the notion of virtual file systems that reside in memory.
There is, however, one important difference: file operations are much faster,
since no disk access is involved.

The tmpfs file system differs from other memory-based file systems in that it
is a virtual memory file system. This means that tmpfs uses not only RAM, but
also the swap space, if RAM is exhausted.

note:
The tmpfs file system is an optional Linux kernel component that is configured
via the CONFIG_TMPFS option.

Aside from use by user applications, tmpfs file systems also serve two special
purposes:

* An invisible tmpfs file system, mounted internally by the kernel, is used
  for implementing System V shared memory (Chapter 48) and shared anonymous
  memory mappings (Chapter 49).

* A tmpfs file system mounted at /dev/shm is used for the glibc implementation
  of POSIX shared memory and POSIX semaphores.


={============================================================================
*kt_linux_core_203* linux-io-fs-vfs

14.5 The Virtual File System (VFS)

If every program that worked with files needed to understand the specific
details of each file system, the task of writing programs that worked with all
of the different file systems would be nearly impossible. The virtual file
system (VFS, sometimes also referred to as the virtual file switch) is a
kernel feature that resolves this problem by creating an abstraction layer for
file-system operations

The ideas behind the VFS are straightforward:

  * The VFS defines a generic interface for file-system operations. All
    programs that work with files specify their operations in terms of this
    generic interface.

  * Each file system provides an implementation for the VFS interface.

Under this scheme, programs need to understand only the VFS interface and can
ignore details of individual file-system implementations.

The VFS interface includes operations corresponding to all of the usual system
calls for working with file systems and directories, such as 

<fs-api>
open(), read(), write(), lseek(), close(), truncate(), stat(), mount(),
  umount(), mmap(), mkdir(), link(), unlink(), symlink(), and rename().


Understanding The Linux Kernel, 3rd Ed 12.1.1. The Common File Model

The key idea behind the VFS consists of introducing a common file model
capable of representing all supported filesystems.

For instance, in the common file model, each directory is regarded as a file,
which contains a list of files and other directories. However, several
  non-Unix disk-based filesystems use a File Allocation Table (FAT), which
  stores the position of each file in the directory tree. In these
  filesystems, directories are not files. To stick to the VFS's common file
  model, the Linux implementations of such FAT-based filesystems must be able
  to construct on the fly, when needed, the files corresponding to the
  directories. Such files exist only as objects in kernel memory.

More essentially, the Linux kernel cannot hardcode a particular function to
handle an operation such as read( ) or ioctl( ) . Instead, it must use a
pointer for each operation; the pointer is made to point to the proper
function for the particular filesystem being accessed.

The application's call to read( ) makes the kernel invoke the corresponding
sys_read( ) service routine, like every other system call. The file is
represented by a file data structure in kernel memory, as we'll see later in
this chapter. This data structure contains a field called f_op that contains
pointers to functions

sys_read( ) finds the pointer to this function and invokes it. Thus, the
application's read( ) is turned into the rather indirect call:

file->f_op->read(...);


={============================================================================
*kt_linux_core_202* linux-io-buffering

<kernel-buffer-cache>
13.1 Kernel Buffering of File I/O: The Buffer Cache

In the interests of speed and efficiency, I/O system calls (i.e., the kernel)
and the I/O functions of the standard C library (i.e., the stdio functions)
buffer data when operating on disk files.

When working with disk files, the `read()` and `write()` system calls don't
directly initiate disk access. Instead, they simply copy data between a
user-space buffer and a buffer in the kernel `buffer cache`

write(fd, "abc", 3);

At this point, write() returns. At some later point, the kernel writes
  (flushes) its buffer to the disk. (Hence, we say that the system call is not
      synchronized with the disk operation.) If, in the interim, another
  process attempts to read these bytes of the file, then the kernel
  automatically supplies the data from the buffer cache, rather than from (the
      outdated contents of) the file.

`The aim of this design` is to allow read() and write() to be fast, since they
don't need to wait on a (slow) disk operation. This design is also efficient,
  since it reduces the number of disk transfers that the kernel must perform.

The Linux kernel imposes `no fixed upper limit on the size of the buffer cache`
The kernel will allocate as many buffer cache pages as are required, limited
only by the amount of available physical memory and the demands for physical
memory for other purposes (e.g., holding the text and data pages required by
    running processes). If available memory is scarce, then the kernel flushes
some modified buffer cache pages to disk, in order to free those pages for
reuse.

Speaking more precisely, from kernel 2.4 onward, Linux no longer maintains a
separate buffer cache. Instead, file I/O buffers are included in the page
cache, which, for example, also contains pages from memory-mapped files.
Nevertheless, in the discussion in the main text, we use the term buffer
cache, since that term is historically common on UNIX implementations.


Effect of buffer size on I/O system call performance

The kernel performs the same number of disk accesses, regardless of whether we
perform 1000 writes of a single byte or a single write of a 1000 bytes.
However, the latter is preferable, since it requires a single system call,
  while the former requires 1000.

note: example 1. uses the program to copy a file.

The BUF_SIZE constant specifies how many bytes are transferred by each call to
  read() and write().)

Since the total amount of data transferred (and hence the number of disk
    operations) is the same for the various buffer sizes, what Table 13-1
illustrates is the overhead of making read() and write() calls.

In summary, if we are transferring a large amount of data to or from a file,
   then by buffering data in large blocks, and thus performing fewer system
     calls, we can greatly improve I/O performance.

note: example 2. use write() only.

However, we already saw that write() returns immediately after transferring
data from user space to the kernel buffer cache.

Since the RAM size on the test system (4 GB) far exceeds the size of the file
being copied (100 MB), we can assume that by the time the program completes,
the output file has not actually been written to disk.

This is because `no actual disk I/O is being performed` in the latter case. In
other words, the majority of the time required for the large buffer cases in
Table 13-1 is due to the disk reads.


13.2 Buffering in the stdio Library

<stdio-role-in-io>
Buffering of data into large blocks to reduce system calls is exactly what is
done by the C library I/O functions (e.g., fprintf(), fscanf(), fgets(),
    fputs(), fputc(), fgetc()) when operating on disk files. Thus, using the
stdio library relieves us of the task of buffering data for output with
write() or input via read().

The setvbuf() function controls the form of buffering employed by the stdio
library.

#include <stdio.h>

int setvbuf(`FILE *stream`, char *buf, int mode, size_t size);

Returns 0 on success, or nonzero on error

The setvbuf() call affects the behavior of `all subsequent stdio operations` on
the specified stream.

The buf and size arguments specify the buffer to be used for stream. These
arguments may be specified in two ways:

  If buf is NULL, then the stdio library `automatically allocates a buffer` for
  use with stream (unless we select unbuffered I/O, as described below). SUSv3
  permits, but does not require, an implementation to use size to determine
  the size for this buffer. In the glibc implementation, size is ignored in
  this case.

The mode argument specifies the type of buffering and has one of the following
values:

_IONBF

Don't buffer I/O. Each stdio library call results in an immediate write() or
read() system call. The buf and size arguments are ignored, and can be
specified as NULL and 0, respectively. This is the default for stderr, so that
error output is guaranteed to appear immediately.

_IOFBF

Employ fully buffered I/O. Data is read or written (via calls to read() or
    write()) in units equal to the size of the buffer. This mode is the
default for streams referring to disk files.


The setbuf() function is layered on top of setvbuf(), and performs a similar
task.

#include <stdio.h>

void setbuf(FILE *stream, char *buf);

Other than the fact that it doesn't return a function result, the call
  setbuf(fp, buf) is equivalent to:

setvbuf(fp, buf, (buf != NULL) ? _IOFBF: _IONBF, BUFSIZ);


Flushing a stdio buffer

Regardless of the current buffering mode, at any time, we can force the data
in a stdio output stream to be written (i.e., flushed to a kernel buffer via
    write()) using the fflush() library function. This function flushes the
output buffer for the specified stream.

#include <stdio.h>

int fflush(FILE *stream);

Returns 0 on success, EOF on error

If stream is NULL, fflush() flushes all stdio buffers. A stdio buffer is
`automatically flushed` when the corresponding stream is closed.


13.3 Controlling Kernel Buffering of File I/O

It is possible to force flushing of kernel buffers for output files.
Sometimes, this is necessary if an application (e.g., a database journaling
    process) must ensure that output really has been written to the disk (or
      at least to the disk’s hardware cache) before continuing.


System calls for controlling kernel buffering of file I/O

SUSv3 defines two different types of synchronized I/O completion. The
difference between the types involves the `metadata` ("data about data")
describing the file, which the kernel stores along with the data for a file.

The other type of synchronized I/O completion defined by SUSv3 is synchronized
I/O file integrity completion, which is a superset of synchronized I/O data
integrity completion. The difference with this mode of I/O completion is that
during a file update, all updated file metadata is transferred to disk, even
if it is not necessary for the operation of a subsequent read of the file
  data.

The fsync() system call causes `the buffered data and all metadata` associated
with the open file descriptor fd to be flushed to disk. Calling fsync() forces
the file to `the synchronized I/O file integrity completion` state.

#include <unistd.h>

int fsync(int fd);

Returns 0 on success, or 1 on error

An fsync() call returns only after the transfer to `the disk device` (or at
    least its cache) has completed.

// FSYNC(2)                   Linux Programmer’s Manual                  FSYNC(2)
// 
// NAME
//        fsync, fdatasync - synchronize a file’s in-core state with storage
//        device
// 
// DESCRIPTION
//        fsync() transfers ("flushes") all modified in-core data of (i.e.,
//            modified buffer cache pages for) the file referred to by the file
//        descriptor fd to the disk device (or other permanent storage device)
//        where that file resides.  The call blocks until the device reports that
//        the transfer has completed.  It also flushes metadata  information
//        associated  with the file (see stat(2)).
// 
// NOTES
//        If the underlying hard disk has write caching enabled, then the data
//        may not really be on permanent storage when fsync() / fdatasync()
//        return.

note: here "disk device" means actual disk? seems a disk as below.

// Your problem with fsync is the interpretation of “all data for the open
// file descriptor”.
// 
// For every inode in use on the system, Linux creates a separate “page
// cache”. Fsync just writes the modified parts of this cache to the disk.
// 
// Different inodes have different page caches. So, an fsync on /dev/foo will
// not have any effect on the cache of /mnt/file, even though /dev/foo is
// mounted on /mnt .

// <from-sync-to-fs>
//
// fs/sync.c (VFS)
//
// /*
//  * sync everything.  Start out by waking pdflush, because that writes back
//  * all queues in parallel.
//  */
// static void do_sync(unsigned long wait)
// {
// 	wakeup_pdflush(0);
// 	sync_inodes(0);		/* All mappings, inodes and their blockdevs */
// 	DQUOT_SYNC(NULL);
// 	sync_supers();		/* Write the superblocks */
// 	sync_filesystems(0);	/* Start syncing the filesystems */
// 	sync_filesystems(wait);	/* Waitingly sync the filesystems */
// 	sync_inodes(wait);	/* Mappings, inodes and blockdevs, again. */
// 	if (!wait)
// 		printk("Emergency Sync complete\n");
// 	if (unlikely(laptop_mode))
// 		laptop_sync_completion();
// }
//
// long do_fsync(struct file *file, int datasync)
// {
//   struct address_space *mapping = file->f_mapping;
// 
//   ret = filemap_fdatawrite(mapping);
// 
//   /*
//    * We need to protect against concurrent writers, which could cause
//    * livelocks in fsync_buffers_list().
//    */
//   err = file->f_op->fsync(file, file->f_dentry, datasync);
// }
// 
// ext2/fsync.c
// int ext2_sync_file(struct file *file, struct dentry *dentry, int datasync)
// {
// 	struct inode *inode = dentry->d_inode;
// 
// 	ret = sync_mapping_buffers(inode->i_mapping);
// 	if (!(inode->i_state & I_DIRTY))
// 		return ret;
// 	if (datasync && !(inode->i_state & I_DIRTY_DATASYNC))
// 		return ret;
// 
// 	err = ext2_sync_inode(inode);
// }
// 
// int ext2_sync_inode(struct inode *inode)
// {
// 	struct writeback_control wbc = {
// 		.sync_mode = WB_SYNC_ALL,
// 		.nr_to_write = 0,	/* sys_fsync did this */
// 	};
// 	return sync_inode(inode, &wbc);
// }
// 
// fs-writeback.c
// 
// /**
//  * sync_inode - write an inode and its pages to disk.
//  * @inode: the inode to sync
//  * @wbc: controls the writeback mode
//  *
//  * sync_inode() will write an inode and its pages to disk.  It will also
//  * correctly update the inode on its superblock's dirty inode lists and will
//  * update inode->i_state.
//  *
//  * The caller must have a ref on the inode.
//  */
// int sync_inode(struct inode *inode, struct writeback_control *wbc)
// {
// 	int ret;
// 
// 	spin_lock(&inode_lock);
// 	ret = __writeback_single_inode(inode, wbc);
// 	spin_unlock(&inode_lock);
// 	return ret;
// }
// EXPORT_SYMBOL(sync_inode);
// 
// fs/fs-writeback.c
// __writeback_single_inode(struct inode *inode, struct writeback_control *wbc)
// {
//   wait_queue_head_t *wqh;
// 
//   __wait_on_bit(wqh, &wq, inode_wait, TASK_UNINTERRUPTIBLE);
// }
// 
// fs/inode.c
// int inode_wait(void *word)
// {
// 	schedule();
// 	return 0;
// }
// 
// fs-writeback.c
// /**
//  * sync_inodes - writes all inodes to disk
//  * @wait: wait for completion
//  *
//  * sync_inodes() goes through each super block's dirty inode list, writes the
//  * inodes out, waits on the writeout and puts the inodes back on the normal
//  * list.
//  *
//  * This is for sys_sync().  fsync_dev() uses the same algorithm.  The subtle
//  * part of the sync functions is that the blockdev "superblock" is processed
//  * last.  This is because the write_inode() function of a typical fs will
//  * perform no I/O, but will mark buffers in the blockdev mapping as dirty.
//  * What we want to do is to perform all that dirtying first, and then write
//  * back all those inode blocks via the blockdev mapping in one sweep.  So the
//  * additional (somewhat redundant) sync_blockdev() calls here are to make
//  * sure that really happens.  Because if we call sync_inodes_sb(wait=1) with
//  * outstanding dirty inodes, the writeback goes block-at-a-time within the
//  * filesystem's write_inode().  This is extremely slow.
//  */
// static void __sync_inodes(int wait)
// {
// 		down_read(&sb->s_umount);
// 		if (sb->s_root) {
// 			sync_inodes_sb(sb, wait);
// 			sync_blockdev(sb->s_bdev);
// 		}
// 
//       // note:
//       // after all, uses __writeback_single_inode(struct inode *inode, struct
//       // writeback_control *wbc)
// }


<generic_file_write>
// mm/filemap.c from linux-2.6.18.8
// ssize_t generic_file_write(struct file *file, const char __user *buf,
// 			   size_t count, loff_t *ppos)
// {
// 	struct inode *inode = mapping->host;
// 	mutex_lock(&inode->i_mutex);
// 	ret = __generic_file_write_nolock(file, &local_iov, 1, ppos);
// 	mutex_unlock(&inode->i_mutex);
// 	return ret;
// }
// EXPORT_SYMBOL(generic_file_write);
//
// this leads to __generic_file_aio_write_nolock;


<maps-to-block-dev>
// note:
//
// NOT SURE HOW THIS block_dev.c in VFS maps to real block device?
//
// void init_special_inode(struct inode *inode, umode_t mode, dev_t rdev)
// {
// 	inode->i_mode = mode;
// 	if (S_ISCHR(mode)) {
// 		inode->i_fop = &def_chr_fops;
// 		inode->i_rdev = rdev;
// 	} else if (S_ISBLK(mode)) {
// 		inode->i_fop = &def_blk_fops;
// 		inode->i_rdev = rdev;
// }
// EXPORT_SYMBOL(init_special_inode);
//
// https://github.com/torvalds/linux/commit/ab0a973
// 
// blkdev: flush disk cache on ->fsync
// 
// Currently there is no barrier support in the block device code.  That
// means we cannot guarantee any sort of data integerity when using the
// block device node with dis kwrite caches enabled.  Using the raw block
// device node is a typical use case for virtualization (and I assume
// databases, too).  This patch changes block_fsync to issue a cache flush
// and thus make fsync on block device nodes actually useful.
// 
// Note that in mainline we would also need to add such code to the
// ->aio_write method for O_SYNC handling, but assuming that Jan's patch
// series for the O_SYNC rewrite goes in it will also call into ->fsync
// for 2.6.32.
//
// fs/block_dev.c (2.6.19)
// /*
//  *	Filp is never NULL; the only case when ->fsync() is called with
//  *	NULL first argument is nfsd_sync_dir() and that's not a directory.
//  */
// // old 
// static int block_fsync(struct file *filp, struct dentry *dentry, int datasync)
// {
// 	return sync_blockdev(I_BDEV(filp->f_mapping->host));
// }
//
// // new
// int blkdev_fsync(struct file *filp, loff_t start, loff_t end, int datasync)
// {
// 	struct inode *bd_inode = bdev_file_inode(filp);
// 	struct block_device *bdev = I_BDEV(bd_inode);
// 	int error;
// 	
// 	error = filemap_write_and_wait_range(filp->f_mapping, start, end);
// 	if (error)
// 		return error;
// 
// 	/*
// 	 * There is no need to serialise calls to blkdev_issue_flush with
// 	 * i_mutex and doing so causes performance issues with concurrent
// 	 * O_SYNC writers to a block device.
// 	 */
// 	error = blkdev_issue_flush(bdev, GFP_KERNEL, NULL);
// 	if (error == -EOPNOTSUPP)
// 		error = 0;
// 
// 	return error;
// }
// 
// block/blk-flush.c
// /**
//  * blkdev_issue_flush - queue a flush
//  * @bdev:	blockdev to issue flush for
//  * @gfp_mask:	memory allocation flags (for bio_alloc)
//  * @error_sector:	error sector
//  *
//  * Description:
//  *    Issue a flush for the block device in question. Caller can supply
//  *    room for storing the error offset in case of a flush error, if they
//  *    wish to. If WAIT flag is not passed then caller may check only what
//  *    request was pushed in some internal queue for later handling.
//  */
// int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
// 		sector_t *error_sector)
// {
//	   return q->issue_flush_fn(q, bdev->bd_disk, error_sector);
// }
// 
// drivers/scsi/scsi_lib.c
// static int scsi_issue_flush_fn(request_queue_t *q, struct gendisk *disk,
// 			       sector_t *error_sector)
// {
// 	struct scsi_device *sdev = q->queuedata;
// 	struct scsi_driver *drv;
// 
// 	if (sdev->sdev_state != SDEV_RUNNING)
// 		return -ENXIO;
// 
// 	drv = *(struct scsi_driver **) disk->private_data;
// 	if (drv->issue_flush)
// 		return drv->issue_flush(&sdev->sdev_gendev, error_sector);
// 
// 	return -EOPNOTSUPP;
// }
// 
// // drivers/scsi/sd.c
// static int sd_issue_flush(struct device *dev, sector_t *error_sector)
// {
// 	int ret = 0;
// 	struct scsi_device *sdp = to_scsi_device(dev);
// 	struct scsi_disk *sdkp = scsi_disk_get_from_dev(dev);
// 
// 	if (!sdkp)
//                return -ENODEV;
// 
// 	if (sdkp->WCE)
// 		ret = sd_sync_cache(sdp);
// 	scsi_disk_put(sdkp);
// 	return ret;
// }


The sync() system call causes `all` kernel buffers containing updated file
information (i.e., data blocks, pointer blocks, metadata, and so on) to be
flushed to disk.

#include <unistd.h>

void sync(void);    // note on "void"

In the Linux implementation, sync() returns only after all data has been
  transferred to the disk device (or at least to its cache). However, SUSv3
  permits an implementation of sync() to simply schedule the I/O transfer and
  return before it has completed.


<pdflush>
A permanently running kernel thread ensures that modified kernel buffers are
flushed to disk if they are not explicitly synchronized within 30 seconds.
This is done to ensure that buffers don't remain unsynchronized with the
corresponding disk file (and thus vulnerable to loss in the event of a system
    crash) for long periods. 
  
In Linux 2.6, this task is performed by the `pdflush` kernel thread. (In Linux
    2.4, it is performed by the kupdated kernel thread.)

The file `/proc/sys/vm/dirty_expire_centisecs` specifies the age (in hundredths
    of a second) that a dirty buffer must reach before it is flushed by
pdflush. Additional files in the same directory control other aspects of the
operation of pdflush.

-sh-3.2# cat /proc/sys/vm/dirty_expire_centisecs
3000

// mm/pdflush.c
// /*
//  * Attempt to wake up a pdflush thread, and get it to do some work for you.
//  * Returns zero if it indeed managed to find a worker thread, and passed your
//  * payload to it.
//  */
// int pdflush_operation(void (*fn)(unsigned long), unsigned long arg0)
//
// fs/sync.c
// void emergency_sync(void)
// {
// 	pdflush_operation(do_sync, 0);
// }
//
// mm/page-writeback.c
//
// /*
//  * writeback at least _min_pages, and keep writing until the amount of dirty
//  * memory is less than the background threshold, or until we're all clean.
//  */
// static void background_writeout(unsigned long _min_pages);
//
// /*
//  * Periodic writeback of "old" data.
//  *
//  * Define "old": the first time one of an inode's pages is dirtied, we mark the
//  * dirtying-time in the inode's address_space.  So this periodic writeback code
//  * just walks the superblock inode list, writing back any inodes which are
//  * older than a specific point in time.
//  *
//  * Try to run once per dirty_writeback_interval.  But if a writeback event
//  * takes longer than a dirty_writeback_interval interval, then leave a
//  * one-second gap.
//  *
//  * older_than_this takes precedence over nr_to_write.  So we'll only write back
//  * all dirty pages if they are all attached to "old" mappings.
//  */
// static void wb_kupdate(unsigned long arg);
//
// 245:            pdflush_operation(background_writeout, 0);
// 371:    return pdflush_operation(background_writeout, nr_pages);
// 455:    if (pdflush_operation(wb_kupdate, 0) < 0)
// 466:    pdflush_operation(laptop_flush, 0);
//
// note: all these use the below
//
// fs/fs-writeback.c
// /*
//  * Start writeback of dirty pagecache data against all unlocked inodes.
//  *
//  * Note:
//  * We don't need to grab a reference to superblock here. If it has non-empty
//  * ->s_dirty it's hadn't been killed yet and kill_super() won't proceed
//  * past sync_inodes_sb() until both the ->s_dirty and ->s_io lists are
//  * empty. Since __sync_single_inode() regains inode_lock before it finally moves
//  * inode from superblock lists we are OK.
//  *
//  * If `older_than_this' is non-zero then only flush inodes which have a
//  * flushtime older than *older_than_this.
//  *
//  * If `bdi' is non-zero then we will scan the first inode against each
//  * superblock until we find the matching ones.  One group will be the dirty
//  * inodes against a filesystem.  Then when we hit the dummy blockdev superblock,
//  * sync_sb_inodes will seekout the blockdev which matches `bdi'.  Maybe not
//  * super-efficient but we're about to do a ton of I/O...
//  */
// void
// writeback_inodes(struct writeback_control *wbc)


<sync-io>
Specifying the O_SYNC flag when calling open() makes all subsequent output
synchronous:

fd = open(pathname, O_WRONLY | O_SYNC);

After this open() call, every write() to the file automatically flushes the
  file data and metadata to the disk (i.e., writes are performed according to
      `synchronized I/O file integrity completion`).


Performance impact of O_SYNC

As can be seen from the table, O_SYNC increases elapsed times enormouslyin the
1-byte buffer case, by a factor of more than 1000. Note also the large
differences between the elapsed and CPU times for writes with O_SYNC. This is
a consequence of the program being blocked while each buffer is actually
transferred to disk.


<disk-cache>
Modern disk drives have large internal caches, and by default, O_SYNC merely
causes data to be transferred `to the cache` If we disable caching on the disk
(using the command hdparm W0), then the performance impact of O_SYNC becomes
even more extreme. 

// hdparm
// https://sourceforge.net/projects/hdparm/
//
// For Get/Set options, a query without an optional parameter (e.g. -d) will
// query (get) the device state, and with a parameter (e.g., -d0) will set the
// device state. 
//
//      -W     Get/set the IDE/SATA drive's write-caching feature.
//
//      --fibmap
//        When used, this must be the only flag given. It requires a file path
//        as a parameter, and will print out a list of the device extents
//        (sector ranges)
//        occupied by that file on disk. Sector numbers are given as absolute LBA
//        numbers, referenced from sector 0 of the physical device (*not* the
//        partition or filesystem). This information can then be used for a
//        variety of purposes, such as examining the degree of fragmenation of
//        larger files, or determining appropriate sectors to deliberately corrupt
//        during fault-injection testing procedures. 
//
// -sh-3.2# hdparm -W /dev/sda
// 
// /dev/sda:
//  write-caching =  1 (on)
//
// -sh-3.2# hdparm -W0 /dev/sda
// 
// /dev/sda:
//  setting drive write-caching to 0 (off)
//  write-caching =  0 (off)
//
// -sh-3.2# hdparm -W /dev/sda
// 
// /dev/sda:
//  write-caching =  0 (off)
// -sh-3.2#
//
//
// From 'ATA8-ACS':
//
// 7.47.4 Enable/disable write cache Subcommand codes 02h and 82h allow the
// host to enable or disable write cache in devices that implement write
// cache. When the subcommand disable write cache is issued, the device shall
// initiate the sequence to flush cache to non-volatile memory before command
// completion (see 7.14). These subcommands may affect caching for commands in
// the Streaming feature set.
//
// So just disabling the write cache causes the flush
//
// hdparm.c
//
// note:
// this opens /dev/sda in this case.
//
// process_dev(argp);
//
// <usage>
// fprintf(desc,"Usage:  %s  [options] [device ...]\n\n", progname);
//
// <W-option>
// -W   Get/set drive write-caching flag (0/1)\n"
//
// case GET_SET_PARM('W',"write-cache",wcache,0,1);
//
// void process_dev (char *devname)
// {
//  if (set_wcache) {
//    if (get_wcache) {
//      printf(" setting drive write-caching to %d", wcache);
//      on_off(wcache);
//    }
//    if (!wcache)
//      err = flush_wcache(fd);
//
//    if (ioctl(fd, HDIO_SET_WCACHE, wcache)) {
//      __u8 setcache[4] = {ATA_OP_SETFEATURES,0,0,0};
//      setcache[2] = wcache ? 0x02 : 0x82;
//      if (do_drive_cmd(fd, setcache, 0)) {
//        err = errno;
//        perror(" HDIO_DRIVE_CMD(setcache) failed");
//      }
//    }
//
//    if (!wcache)
//      err = flush_wcache(fd);
//  }
// }
//
//
// <f-option>
// " -f   Flush buffer cache for device on exit\n"
//
// case      DO_FLAG('f',do_flush);
//
// static void flush_buffer_cache (int fd)
// {
//   sync();
//   fsync(fd);				/* flush buffers */
//   fdatasync(fd);				/* flush buffers */
//   sync();
//   if (ioctl(fd, BLKFLSBUF, NULL))		/* do it again, big time */
//     perror("BLKFLSBUF failed");
//   else
//     do_drive_cmd(fd, NULL, 0);	/* IDE: await completion */
//   sync();
// }
//
//
// <F-option>
// " -F   Flush drive write cache\n"
//
// case      DO_FLAG('F',do_flush_wcache);
//
// static int flush_wcache (int fd)
// {
//  __u8 args[4] = {ATA_OP_FLUSHCACHE,0,0,0};
//  int err = 0;
//  get_identify_data(fd);
//  if (id && (id[83] & 0xe000) == 0x6000)
//    args[0] = ATA_OP_FLUSHCACHE_EXT;
//  if (do_drive_cmd(fd, args, timeout_60secs)) {
//    err = errno;
//    perror (" HDIO_DRIVE_CMD(flushcache) failed");
//  }
//  return err;
// }
//
// // <include/linux/fs.h>
//
// 0x00001261   BLKFLSBUF    void
//
// // uapi/linux/fs.h
//
// /*
//  * This file has definitions for some important file table structures
//  * and constants and structures used by various generic file system
//  * ioctl's.  Please do not make any changes in this file before
//  * sending patches for review to linux-fsdevel@vger.kernel.org and
//  * linux-api@vger.kernel.org.
//  */
//
// #define BLKFLSBUF  _IO(0x12,97)	/* flush buffer cache */


In the 1-byte case, the elapsed time rises from 1030 seconds to around 16,000
seconds. In the 4096-byte case, the elapsed time rises from 0.34 seconds to 4
seconds.  

In summary, if we need to force flushing of kernel buffers, we should consider
whether we can design our application to use large write() buffer sizes or
make judicious use of occasional calls to fsync() or fdatasync(), instead of
using the O_SYNC flag when opening the file.


13.4 Summary of I/O Buffering

`Figure 13-1` provides an overview of the buffering employed (for output files)
by the stdio library and the kernel, along with the mechanisms for controlling
each type of buffering.


13.6 Bypassing the Buffer Cache: Direct I/O

note: The details described here are `Linux-specific` and are not standardized
by SUSv3.

Starting with kernel 2.4, Linux allows an application to bypass the buffer
cache when performing disk I/O, thus transferring data directly from user
space to a file or disk device. This is sometimes termed direct I/O or raw
I/O.

Direct I/O is sometimes misunderstood as being a means of obtaining fast I/O
performance. 

However, for most applications, using direct I/O can considerably degrade
performance. This is because the kernel applies `a number of optimizations` to
improve the performance of I/O done via the buffer cache, including performing
sequential read-ahead, performing I/O in clusters of disk blocks, and allowing
processes accessing the same file to share buffers in the cache. All of these
optimizations are lost when we use direct I/O.

Direct I/O is intended only for applications with specialized I/O
requirements. For example, database systems that perform their own caching and
I/O optimizations don’t need the kernel to consume CPU time and memory
performing the same tasks.

We can perform direct I/O either `on an individual file` or on a block device
(e.g., a disk). To do this, we specify the O_DIRECT flag when opening the file
or device with open().


<with-filesystem>
The O_DIRECT flag is effective since kernel 2.4.10. Not all Linux file systems
and kernel versions support the use of this flag. Most native file systems
support O_DIRECT, but many non-UNIX file systems (e.g., VFAT) do not. It may
be necessary to test the file system concerned (if a file system doesn’t
    support O_DIRECT, then open() fails with the error EINVAL) or read the
kernel source code to check for this support.

// skipped the rest since as it noted, not all file system supports it.


={============================================================================
*kt_linux_core_203* linux-io-cache

<write-to-fs>
// fs/read_write.c
ssize_t do_sync_write(struct file *filp, const char __user *buf, size_t len, loff_t *ppos)
{
	for (;;) {
		ret = filp->f_op->aio_write(&kiocb, &iov, 1, kiocb.ki_pos);
		if (ret != -EIOCBRETRY)
			break;
		wait_on_retry_sync_kiocb(&kiocb);
	}
}

EXPORT_SYMBOL(do_sync_write);

aio_write          47 fs/ext2/file.c    .aio_write      = generic_file_aio_write,

mm/filemap.c
ssize_t generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,
	unsigned long nr_segs, loff_t pos)
{
  ret = __generic_file_aio_write_nolock(iocb, iov, nr_segs, &iocb->ki_pos)
  {
    written = generic_file_direct_write();

    // or

    written_buffered = generic_file_buffered_write();
  }
}


={============================================================================
*kt_linux_core_203* linux-io-ex

#include <sys/stat.h>
#include <fcntl.h>
#include <sys/wait.h>
#include "tlpi_hdr.h"

int
main(int argc, char *argv[])
{
  int fd, flags;
  char template[] = "/tmp/testXXXXXX";

  setbuf(stdout, NULL);    // {KT} Disable buffering of stdout

  fd = mkstemp(template);  // opens a temporary file
  if (fd == -1)
    errExit("mkstemp");

  printf("File offset before fork(): %lld\n", (long long) lseek(fd, 0, SEEK_CUR));

  flags = fcntl(fd, F_GETFL);    // get file flags
  if (flags == -1)
    errExit("fcntl - F_GETFL");
  
  printf("O_APPEND flag before fork() is: %s\n", (flags & O_APPEND) ? "on" : "off");

  switch (fork()) {
    case -1:
      errExit("fork");

    case 0: /* Child: change file offset and status flags */
      if (lseek(fd, 1000, SEEK_SET) == -1)
        errExit("lseek");

      flags = fcntl(fd, F_GETFL); /* Fetch current flags */
      if (flags == -1)
        errExit("fcntl - F_GETFL");

      flags |= O_APPEND; /* Turn O_APPEND on */
      if (fcntl(fd, F_SETFL, flags) == -1)
        errExit("fcntl - F_SETFL");
      _exit(EXIT_SUCCESS);

    default: /* Parent: can see file changes made by child */
      if (wait(NULL) == -1)
        errExit("wait"); /* Wait for child exit */

      printf("Child has exited\n");
      printf("File offset in parent: %lld\n", (long long) lseek(fd, 0, SEEK_CUR));

      flags = fcntl(fd, F_GETFL);
      if (flags == -1)
        errExit("fcntl - F_GETFL");

      printf("O_APPEND flag in parent is: %s\n", (flags & O_APPEND) ? "on" : "off");
      exit(EXIT_SUCCESS);
  }
}

For an explanation of why we cast the return value from lseek() to long long
in Listing 24-2, see Section 5.10.


={============================================================================
*kt_linux_core_203* linux-io-dev-fd

For each process, the kernel provides the special virtual directory /dev/fd.
This directory contains filenames of the form /dev/fd/n, where n is a number
corresponding to one of the open file descriptors for the process.

note: 
current process

/dev/fd is actually a symbolic link to the `Linux-specific` /proc/self/fd
directory. The latter directory is a `special case` of the Linux-specific
/proc/PID/fd directories, each of which contains symbolic links corresponding
to all of the files held open by a process.

$ ls -al /dev/fd
lrwxrwxrwx 1 root root 13 Nov 17 11:59 /dev/fd -> /proc/self/fd

$ ll /dev/fd/
total 0
dr-x------ 2 keitee keitee  0 May 10 22:22 ./
dr-xr-xr-x 8 keitee keitee  0 May 10 22:22 ../
lrwx------ 1 keitee keitee 64 May 10 22:22 0 -> /dev/pts/1
lrwx------ 1 keitee keitee 64 May 10 22:22 1 -> /dev/pts/1
lrwx------ 1 keitee keitee 64 May 10 22:22 2 -> /dev/pts/1
lr-x------ 1 keitee keitee 64 May 10 22:22 3 -> /proc/4122/fd/
$

<use>
Their most common use is in the shell. Many user-level commands take filename
arguments, and sometimes we would like to put them in a pipeline and have one
of the arguments be standard input or output instead. For this purpose, some
programs (e.g., diff, ed, tar, and comm) have evolved the hack of using an
argument consisting of a single hyphen (-) to mean "use standard input or
output (as appropriate) for this filename argument." Thus, to compare a file
list from ls against a previously built file list, we might write the
following:

$ ls | diff - oldfilelist

This approach has various problems. First, it requires specific interpretation
of the hyphen character on the part of each program, and many programs don't
perform such interpretation; they are written to work only with filename
arguments, and they have no means of specifying standard input or output as
the files with which they are to work. Second, some programs instead interpret
a single hyphen as a delimiter marking the end of command-line options.

Using /dev/fd eliminates these difficulties, allowing the specification of
standard input, output, and error as filename arguments to any program
requiring them. Thus, we can write the previous shell command as follows:

$ ls | diff /dev/fd/0 oldfilelist

As a convenience, the names /dev/stdin, /dev/stdout, and /dev/stderr are
provided as symbolic links to, respectively, /dev/fd/0, /dev/fd/1, and
/dev/fd/2.


={============================================================================
*kt_linux_core_203* linux-io-temp-file

5.12 Creating Temporary Files

Some programs need to create temporary files that are used only while the
program is running, and these files should be removed when the program
terminates.

Here, we describe two of these functions: mkstemp() and tmpfile().

The mkstemp() function generates a unique filename based on a template
supplied by the caller and opens the file, returning a file descriptor that
can be used with I/O system calls.

#include <stdlib.h>

int mkstemp(char *template);

Returns file descriptor on success, or 1 on error

The mkstemp() function creates the file with read and write permissions for
the file owner (and no permissions for other users), and opens it with the
O_EXCL flag, guaranteeing that the caller has exclusive access to the file.
Typically, a temporary file is unlinked (deleted) soon after it is opened,
  using the unlink() system call (Section 18.3).


The tmpfile() function creates a uniquely named temporary file that is opened
for reading and writing. (The file is opened with the O_EXCL flag to guard
    against the unlikely possibility that another process has already created
    a file with the same name.)

#include <stdio.h>

FILE *tmpfile(void);

Returns file pointer on success, or NULL on error

On success, tmpfile() returns a file stream that can be used with the stdio
library functions. The temporary file is automatically deleted when it is
closed. To do this, tmpfile() makes an internal call to unlink() to remove the
filename immediately after opening the file.


={============================================================================
*kt_linux_core_203* linux-io-file-attribute

LPI 15 File Attributes
LPI 15.1 Retrieving File Information: stat()

<syscall-stat>
The stat(), lstat(), and fstat() system calls retrieve information about a
file, mostly drawn from the file i-node.

man 2 stat

#include <sys/stat.h>
int stat(const char *pathname, struct stat *statbuf);
int lstat(const char *pathname, struct stat *statbuf);
int fstat(int fd, struct stat *statbuf);

All return 0 on success, or -1 on error

stat() returns information about a named file;

lstat() is similar to stat(), except that if the named file is a symbolic
link, information about the link itself is returned, rather than the file to
which the link points; and

fstat() returns information about a file referred to by an open file
descriptor.

<struct-stat>
struct stat {
  dev_t st_dev; /* IDs of device on which file resides */
  ino_t st_ino; /* I-node number of file */
  mode_t st_mode; /* File type and permissions */
  nlink_t st_nlink; /* Number of (hard) links to file */
  uid_t st_uid; /* User ID of file owner */
  gid_t st_gid; /* Group ID of file owner */
  dev_t st_rdev; /* IDs for device special files */
  off_t st_size; /* Total file size (bytes) */
  blksize_t st_blksize; /* Optimal block size for I/O (bytes) */
  blkcnt_t st_blocks; /* Number of (512B) blocks allocated */
  time_t st_atime; /* Time of last file access */
  time_t st_mtime; /* Time of last file modification */
  time_t st_ctime; /* Time of last status change */
};


15.4.1 Permissions on Regular Files

File type and permissions

The `st_mode` field is a bit mask serving the dual purpose of identifying the file type
and specifying the file permissions. The bits of this field are laid out as shown in

<-file type-> <- premission                     ->
 |  |  |  |  | U | G | T | R W X | R W X | R W X |

Figure 15-1: Layout of st_mode bit mask

The first 3 of these bits are special bits known as the set-user-ID,
    set-group-ID, and sticky bits (labeled U, G, and T, respectively)

Table 15-4: Constants for file permission bits

S_ISUID 04000 Set-user-ID
S_ISGID 02000 Set-group-ID
S_ISVTX 01000 Sticky


<permission-mask>
Listing 15-4: Convert file permissions mask to string

#include <sys/stat.h>
#include <stdio.h>
#include "file_perms.h" /* Interface for this implementation */

#define STR_SIZE sizeof("rwxrwxrwx")

char * /* Return ls(1)-style string for file permissions mask */
filePermStr(mode_t perm, int flags)
{
  static char str[STR_SIZE];
  snprintf(str, STR_SIZE, "%c%c%c%c%c%c%c%c%c",
      (perm & S_IRUSR) ? 'r' : '-', (perm & S_IWUSR) ? 'w' : '-',
      (perm & S_IXUSR) ?
      (((perm & S_ISUID) && (flags & FP_SPECIAL)) ? 's' : 'x') :
      (((perm & S_ISUID) && (flags & FP_SPECIAL)) ? 'S' : '-'),
      (perm & S_IRGRP) ? 'r' : '-', (perm & S_IWGRP) ? 'w' : '-',
      (perm & S_IXGRP) ?
      (((perm & S_ISGID) && (flags & FP_SPECIAL)) ? 's' : 'x') :
      (((perm & S_ISGID) && (flags & FP_SPECIAL)) ? 'S' : '-'),
      (perm & S_IROTH) ? 'r' : '-', (perm & S_IWOTH) ? 'w' : '-',
      (perm & S_IXOTH) ?
      (((perm & S_ISVTX) && (flags & FP_SPECIAL)) ? 't' : 'x') :
      (((perm & S_ISVTX) && (flags & FP_SPECIAL)) ? 'T' : '-'));
  return str;
}


15.4.2 Permissions on Directories

Directories have the same permission scheme as files. However, the three permissions
are interpreted differently:

* Read: The contents (i.e., the list of filenames) of the directory may be listed
(e.g., by ls).

* Write: Files may be created in and removed from the directory. Note that it is
not necessary to have any permission on a file itself in order to be able to
delete it.

* Execute: Files within the directory may be accessed. Execute permission on a
directory is sometimes called search permission.

<exec-permission>
When accessing a file, execute permission is required on all of the
directories listed in the pathname.

We must have execute permission on the directory in order to access the
contents or the i-node information of files in the directory.

If experimenting to verify the operation of the directory read permission bit,
   be aware that some Linux distributions alias the ls command to include
   flags (e.g., F) that require access to i-node information for files in the
   directory, and this requires execute permission on the directory. To ensure
   that we are using an unadulterated ls, we can specify the full pathname of
   the command (/bin/ls).

The stat() and lstat() system calls don't require permissions on the file
itself. However, execute (search) permission is required on 'all' of the
`parent directories specified in pathname. `The fstat() system call always
succeeds, if provided with a valid file descriptor. 

<ex>
sycall-chdir follows links so need to have exec permission on paths.

when there is no excute permission on a directory, cannot 'cd' in. this causes
a failure when do build and turns out that some dirs don't have execute on
dirs for a group and make system cannot cd in.


<trick>
Conversely, if we have execute permission on a directory, but not read
permission, then we can access a file in the directory if we know its name,
  but we can’t list the contents of (i.e., the other filenames in) the
  directory. This is a simple and frequently used technique to control access
  to the contents of a public directory.


<use-to-check>
Shows that can use stat() to check if the file exist or not.

struct stat sb;
if (-1 == stat(cleanup_exe, &sb))
{
  fprintf(stderr, "executable (%s) doesn't exist\n", cleanup_exe);
  return 0;
}


={============================================================================
*kt_linux_core_203* linux-io-directories

LPI 18 Directories and Links

Each process has `two directory-related attributes`: a root directory, which
determines the point from which absolute pathnames are interpreted, and a
current working directory, which determines the point from which relative
pathnames are interpreted.


18.1 Directories and (Hard) Links

A directory is stored in the file system in a similar way to a regular file.
Two things distinguish a directory from a regular file:

* A directory is marked with a different file type in its i-node entry
  (Section 14.4).

* A directory is a file with a special organization. Essentially, it is a
  table consisting of filenames and i-node numbers.

The i-node table is numbered starting at 1, rather than 0, because 0 in the
i-node field of a directory entry indicates that the entry is unused. I-node 1
is used to record bad blocks in the file system. The root directory (/) of a
file system is always stored in i-node entry 2 (as shown in Figure 18-1), so
that the kernel knows where to start when resolving a pathname.

i-node number   i-node table
2               UID=root, GID=root
                type=directory
                perm=rw-r-xr-x
                ...
                Data block pointers       ->  / directory
                                              ... | ...
                                              tmp | 5
                                              ... | ...
                                              etc | 7 
7               UID=root, GID=root
                type=directory
                ...
                Data block pointers       ->  /etc directory
                                              ...    | ...
                                              group  | 282
                                              ...    | ...
                                              passwd | 6422 

6422            type=file
                ...
                Data block pointers       ->  /etc/passwd file
                                              File data

Figure 18-1: Relationship between i-node and directory structures for the file
/etc/passwd

<link-and-link-count>
the i-node doesn’t contain a filename; it is only the mapping within a
directory list that defines the name of a file. This has a useful consequence:
we can create multiple namesin the same or in different directorieseach of
which refers to the same i-node. These multiple names are known as `links`, or
sometimes as hard links to distinguish them from symbolic links.

In the third field by `ls li`, can see the link count for the i-node.

The i-node entry and data blocks for the file are removed (deallocated) only
when the i-node’s link count falls to 0-that is, when all of the names for the
file have been removed. To summarize: the rm command removes a filename from a
directory list, decrements the link count of the corresponding i-node by 1,
and, if the link count thereby falls to 0, deallocates the i-node and the data
  blocks to which it refers.


<fd-and-filename>
A question often asked in online forums is “How can I find the filename
associated with the file descriptor X in my program?” The short answer is that
we can’t- at least not portably and unambiguously-since a file descriptor
refers to an i-node, and multiple filenames (or even, as described in Section
    18.3, none at all) may refer to this i-node.

<hard-link-limitation>
Hard links have two limitations, both of which can be circumvented by the use
of symbolic links:

* Because directory entries (hard links) refer to files using just an i-node
  number, and i-node numbers are unique only within a file system, a hard link
  must reside on the same file system as the file to which it refers.

* A hard link can’t be made to a directory. This prevents the creation of
  circular links, which would confuse many system programs.


18.2 Symbolic (Soft) Links

Symbolic links don’t have the same status as hard links. In particular, a
symbolic link is not included in the link count of the file to which it
refers. Therefore, if the filename to which the symbolic link refers is
removed, the symbolic link itself continues to exist, even though it can no
longer be dereferenced (followed).


Interpretation of symbolic links by system calls

Many system calls dereference (follow) symbolic links and thus work on the
file to which the link refers. Some system calls don’t dereference symbolic
links, but instead operate directly on the link file itself.

One point generally applies: symbolic links in the directory part of a
pathname (i.e., all of the components preceding the final slash) are always
dereferenced.  Thus, in the pathname /somedir/somesubdir/file, somedir and
somesubdir will always be dereferenced if they are symbolic links, and file
may be dereferenced, depending on the system call to which the pathname is
passed.

<syscall-unlink>
#include <unistd.h>

int unlink(const char *pathname);
Returns 0 on success, or 1 on error

The unlink() system call removes a link (deletes a filename) and, if that is
the last link to the file, also removes the file itself. If the link specified
in pathname doesn’t exist, then unlink() fails with the error ENOENT.

We can’t use unlink() to remove a directory; that task requires rmdir() or
remove(), which we look at in Section 18.6.

The unlink() system call doesn’t dereference symbolic links. If pathname is a
symbolic link, the link itself is removed, rather than the name to which it
points.


An open file is deleted only when all file descriptors are closed

In addition to maintaining a link count for each i-node, the kernel also
counts open file descriptions for the file (see Figure 5-2, on page 95). If
the last link to a file is removed and any processes hold open descriptors
referring to the file, the file won’t actually be deleted until all of the
descriptors are closed. 

This is a useful feature, because it permits us to unlink a file without
needing to worry about whether some other process has it open. In addition, we
can perform tricks such as creating and opening a temporary file, unlinking it
immediately, and then continuing to use it within our program, relying on the
fact that the file is destroyed only when we close the file descriptoreither
explicitly, or implicitly when the program exits. (This is what the tmpfile()
    function described in Section 5.12 does.)


18.10 The Current Working Directory of a Process

A process’s current working directory defines the starting point for the
resolution of `relative pathnames` referred to by the process. A new process
inherits its current working directory from its parent.


18.12 Changing the Root Directory of a Process: chroot()

`Every process has a root directory`, which is the point from which absolute
pathnames (i.e., those beginning with /) are interpreted. By default, this is
the real root directory of the file system.

<syscall-chroot>
On occasion, it is useful for a process to change its root directory, and a
privileged (CAP_SYS_CHROOT) process can do this using the chroot() system
call.

Thereafter, all absolute pathnames are interpreted as starting from that
location in the file system. This is sometimes referred to as setting up a
`chroot jail`, since the program is then confined to a particular area of the
file system.

The chroot() system call is employed by the chroot command, which enables us
to execute shell commands in a chroot jail. The root directory of any process
can be found by reading (readlink()) the contents of the Linux-specific
/proc/PID/root symbolic link.

<chroot-case>
The classic example of the use of chroot() is in the ftp program. As a
security measure, when a user logs in anonymously under FTP, the ftp program
uses chroot() to set the root directory for the new process to the directory
specifically reserved for anonymous logins. After the chroot() call, the user
is limited to the file-system subtree under their new root directory, so they
can’t roam around the entire file system. 

This relies on the fact that the root directory is its own parent; that is,
     /.. is a link to /, so that changing directory to / and then attempting a
       cd .. leaves the user in the same directory.

<chroot-break>
The chroot() system call was not conceived as a completely secure jail
mechanism.

note: skipped.


={============================================================================
*kt_linux_core_203* linux-io-permission-check linux-process-credentials

15.4.3 Permission-Checking Algorithm

The kernel checks file permissions whenever we specify a pathname in a system
call that accesses a file or directory. When the pathname given to the system
call includes a directory prefix, then, in addition to checking for the
required permissions on the file itself, the kernel also checks for execute
permission on each of the directories in this prefix. 

// Permission checks are made using the process's effective user ID, effective
// group ID, and supplementary group IDs.

To be strictly accurate, `for file permission checks on Linux`, the
file-system user and group IDs are used instead of the corresponding effective
IDs, as described in Section 9.5.


Permission checking for privileged processes

Above, we said that if a process is privileged, all access is granted when
checking permissions. We need to add one proviso to this statement. For a file
that is not a directory, Linux grants execute permission to a privileged
process only if that permission is granted to at least one of the permission
categories for the file. 

On some other UNIX implementations, a privileged process can execute a file
even when no permission category grants execute permission. When accessing a
directory, a privileged process is always granted execute (search) permission.


{linux-process-credentials}
LPI 9.1 Real User ID and Real Group ID

The real user ID and group ID identify the user and group to which the process
belongs. As part of the login process, a login shell gets its real user and
group IDs from the third and fourth fields of the user's password record in
the /etc/passwd file 168 Chapter 9 (Section 8.1). 

When a new process is created (e.g., when the shell executes a program),
it `inherits these identifiers from its parent.`


9.2 Effective User ID and Effective Group ID

On most UNIX implementations (Linux is a little different, as explained in
    Section 9.5), the `effective user ID and group ID`, in conjunction with
the supplementary group IDs, are used to determine the permissions granted to
a process when it tries to perform various operations (i.e., system calls). 

For example, these identifiers determine the permissions granted to a process
when it accesses resources such as files and System V interprocess
communication (IPC) objects, which themselves have associated user and group
IDs determining to whom they belong. As we’ll see in Section 20.5, the
effective user ID is also used by the kernel to determine whether one process
can send a signal to another.

A process whose effective user ID is 0 has all of the privileges of the
superuser. Such a process is referred to as a `privileged process.` Certain
system calls can be executed only by privileged processes.

Normally, the effective user and group IDs have the same values as the
corresponding real IDs, but there are two ways in which the effective IDs can
assume different values. One way is through the use of system calls that we
discuss in Section 9.7. 

<set-userid>
The second way is through the execution of set-user-ID and set-group-ID
programs.


9.3 Set-User-ID and Set-Group-ID Programs

A set-user-ID program allows a process to gain privileges it would not
normally have, by setting `the process's effective user ID` to the same value
as `the user ID (owner) of the executable file.`

$ su
Password:
# ls -l prog
-rwxr-xr-x 1 root root 302585 Jun 26 15:05 prog
# chmod u+s prog      // Turn on set-user-ID permission bit
# chmod g+s prog      // Turn on set-group-ID permission bit
# ls -l prog
-rwsr-sr-x 1 root root 302585 Jun 26 15:05 prog

Examples of commonly used set-user-ID programs on Linux include: passwd(1),
which changes a user's password; mount(8) and umount(8), which mount and
  unmount file systems; and su(1), which allows a user to run a shell under a
  different user ID. An example of a set-group-ID program is wall(1), which
  writes a message to all terminals owned by the tty group

$ ls -al /bin/mount
-rwsr-xr-x 1 root root 88744 Dec  9  2012 /bin/mount


9.6 Supplementary Group IDs

The supplementary group IDs are a set of additional groups to which a process
belongs. A new process inherits these IDs from its parent. A login shell
obtains its supplementary group IDs from the system group file. As noted
above, these IDs are used in conjunction with the effective and file-system
IDs to determine permissions for accessing files, System V IPC objects, and
other system resources.


9.5 File-System User ID and File-System Group ID

On Linux, it is the file-system user and group IDs, rather than the effective
user and group IDs, that are used (in conjunction with the supplementary group
    IDs) to determine permissions when performing file-system operations such
as opening files, changing file ownership, and modifying file permissions.

note: skipped


<sticky-bit>
15.4.5 Set-User-ID, Set-Group-ID, and Sticky Bits

// On older UNIX implementations, the sticky bit was provided as a way of making
// commonly used programs run faster. If the sticky bit was set on a program
// file, then the first time the program was executed, a copy of the program text
// was saved in the swap areathus it sticks in swap, and loads faster on
// subsequent executions.  Modern UNIX implementations have more sophisticated
// memory-management systems, which have rendered this use of the sticky
// permission bit obsolete.

In modern UNIX implementations (including Linux), the sticky permission bit
serves another, quite different purpose. For directories, the sticky bit acts
as the restricted deletion flag. Setting this bit on a directory means that an
unprivileged process can unlink (unlink(), rmdir()) and rename (rename())
files in the directory `only if` it has write permission on the directory and
owns either the file or the directory.

(A process with the CAP_FOWNER capability can bypass the latter ownership check.)

This makes it possible to create a directory that is shared by many users, who
can each create and delete their own files in the directory but can’t delete
files owned by other users. The sticky permission bit is commonly set on the
/tmp directory for this reason.

<ex>
Sticky Bit is mainly used on folders in order to avoid deletion of a folder
and its content by other users though they having write permissions on the
folder contents. If Sticky bit is enabled on a folder, the folder contents are
deleted by only owner who created them and the root user. No one else can
delete other users data in this folder(Where sticky bit is set). This is a
security measure to avoid deletion of critical folders and their
content(sub-folders and files), though other users have full permissions.

The sticky bit allows only user even if permission is granted for group and
others.

drwxrwxrwt  18 root root  4096 May 10 12:12 tmp/

// from /etc/group which shows two users are in the same group
ccusers:x:16777244:keitee

-rwxrw-rw-  1 keitee   ccusers        0 May 10 12:38 this_is_not_use_sticky*

// when not in /tmp and no sticky bit on
kyoupark@kit-debian:/xxx$ rm this_is_not_use_sticky
$


// when use kyoupark user and in /tmp, cannot delete
-rwxrw-rw-  1 keitee   ccusers    0 May 10 12:34 this_is_temp_xx*
kyoupark@kit-debian:/tmp$ rm this_is_temp_xx
rm: cannot remove ‘this_is_temp_xx’: Operation not permitted


={============================================================================
*kt_linux_core_203* linux-io-fs-mount

14.7 Single Directory Hierarchy and Mount Points

With Linux 2.4.19 and later, things became more complicated. The kernel now
supports `per-process mount namespaces` This means that each process
potentially has its own set of file-system mount points, and thus may see a
different single directory hierarchy from other processes. We explain this
point further when we describe the CLONE_NEWNS flag in Section 28.2.1.


14.8 Mounting and Unmounting File Systems

Three files that contain information about the file systems that are currently
mounted or can be mounted:

<proc-mounts>
  * /proc/mounts
  
  A list of the `currently mounted` file systems can be read from the
  Linux-specific /proc/mounts virtual file. /proc/mounts is an interface to
  kernel data structures, so it always contains `accurate information` about
  mounted file systems.

  note:
  With the arrival of the per-process mount namespace feature mentioned
  earlier, each process now has a /proc/PID/mounts file that lists the mount
  points constituting its mount namespace, and /proc/mounts is just a symbolic
  link to /proc/self/mounts.

  * /etc/mtab

  The mount(8) and umount(8) commands automatically maintain the file
  /etc/mtab, which contains information that is similar to that in
  /proc/mounts, but slightly more detailed. In particular, /etc/mtab includes
  file system–specific options given to mount(8), which are not shown in
  /proc/mounts. 

  However, because the `mount(2) and umount() system calls don't update`
  /etc/mtab, this file may be inaccurate if some application that mounts or
  unmounts devices fails to update it.

  * /etc/fstab

  maintained manually by the system administrator, contains descriptions of
  all of the available file systems on a system, and is used by the mount(8),
  umount(8), and fsck(8) commands.


note: from stackoverflow
The definitive list of mounted filesystems in in /proc/mounts.

If you have any form of containers on your system, /proc/mounts only lists the
filesystems that are in your present container. For example, in a chroot,
/proc/mounts lists only the filesystems whose mount point is within the
  chroot. (There are ways to escape the chroot, mind.)

There's also a list of mounted filesystems in /etc/mtab. This list is
maintained by the mount and umount commands. That means that if you don't use
these commands (which is pretty rare), your action (mount or unmount) won't be
recorded. In practice, it's mostly in a chroot that you'll find /etc/mtab
files that differ wildly from the state of the system (also mounts performed
    in the chroot will be reflected in the chroot's /etc/mtab but not in the
    main /etc/mtab). 

Actions performed while /etc/mtab is on a read-only filesystem are also not
recorded there. The reason why you'd sometimes want to consult /etc/mtab in
preference to or in addition to /proc/mounts is that because it has access to
the mount command line, it's sometimes able to present information in a way
that's easier to understand; for example you see mount options as requested
(whereas /proc/mounts lists the mount and kernel defaults as well), and bind
mounts appear as such in /etc/mtab.


Common format:

/dev/sda9 /boot ext3 rw 0 0

1. The name of the mounted device.

2. The mount point for the device.

3. The file-system type.

4. Mount flags. In the above example, rw indicates that the file system was
mounted read-write.

5. A number used to control the operation of file-system backups by dump(8).
This field and the next are used only in the /etc/fstab file; for /proc/mounts
and /etc/mtab, these fields are always 0.

6. A number used to control the order in which fsck(8) checks file systems at
system boot time.

The mountflags argument is a bit mask constructed by ORing (|) zero or more
of the flags shown in Table 14-1, which are described in more detail below.

// from mount(8)
       -o, --options opts
              Use the specified mount options.  The opts argument is a
              comma-separated list.  For example:

                     mount LABEL=mydisk -o noatime,nodev,nosuid

              For more details, see the FILESYSTEM-INDEPENDENT MOUNT OPTIONS
              and FILESYSTEM-SPECIFIC MOUNT OPTIONS sections.

FILESYSTEM-INDEPENDENT MOUNT OPTIONS

       dirsync
              All directory updates within the filesystem should be done
              synchronously.  This affects the following system calls: creat,
              link, unlink, symlink, mkdir, rmdir, mknod and rename.

       sync   
              All I/O to the filesystem should be done synchronously.  In the
              case of media with a limited number of write cycles (e.g. some
              flash drives), sync may cause life-cycle shortening.

Table 14-1: mountflags values for mount()

MS_DIRSYNC (since Linux 2.6)

Make directory updates synchronous. This is similar to the effect of the
open() O_SYNC flag (Section 13.3), but applies only to directory updates. The
MS_SYNCHRONOUS flag described below provides a superset of the functionality
of MS_DIRSYNC, ensuring that both file and directory updates are performed
synchronously. The MS_DIRSYNC flag allows an application to ensure that
directory updates (e.g., open(pathname, O_CREAT), rename(), link(), unlink(),
    symlink(), and mkdir()) are synchronized without incurring the expense of
synchronizing all file updates. The FS_DIRSYNC_FL flag (Section 15.5) serves a
similar purpose to MS_DIRSYNC, with the difference that FS_DIRSYNC_FL can be
applied to individual directories. In addition, on Linux, calling fsync() on a
file descriptor that refers to a directory provides a means of synchronizing
directory updates on a per-directory basis. (This Linux-specific fsync()
    behavior is not specified in SUSv3.)


MS_SYNCHRONOUS

Make all file and directory updates on this file system synchronous. (In the
    case of files, this is as though files were always opened with the open()
    O_SYNC flag.)

// sys/mount.h
// /* These are the `fs-independent mount-flags`: up to 16 flags are
//    supported  */
// enum
// {
//   MS_RDONLY = 1,                /* Mount read-only.  */
// #define MS_RDONLY       MS_RDONLY
//   MS_NOSUID = 2,                /* Ignore suid and sgid bits.  */
// #define MS_NOSUID       MS_NOSUID
//   MS_NODEV = 4,                 /* Disallow access to device special files.  */
// #define MS_NODEV        MS_NODEV
//   MS_NOEXEC = 8,                /* Disallow program execution.  */
// #define MS_NOEXEC       MS_NOEXEC
//   MS_SYNCHRONOUS = 16,          /* Writes are synced at once.  */


14.9 Advanced Mount Features

14.9.1 Mounting a File System at Multiple Mount Points

From kernel 2.4 onward, a file system can be mounted at multiple locations
within the file system. Because each of the mount points shows the same
subtree, changes made via one mount point are visible through the other(s),

It is useful to mount a file system at multiple points when describe bind
  mounts

<bind-mount>
A bind mount (created using the mount() MS_BIND flag) results in the directory
or file being visible in both locations. A bind mount is somewhat like a hard
link, but differs in two respects:

* A bind mount can `cross file-system` mount points (and even chroot jails).

* It is possible to make a bind mount `for a directory`

note:bind mount a file at another location

One example of when we might use a bind mount is in the creation of a chroot
jail (Section 18.12). Rather than replicating various standard directories
(such as /lib) in the jail, we can simply create bind mounts for these
directories (possibly mounted read-only) within the jail.


14.9.5 Recursive Bind Mounts


14.9.2 Stacking Multiple Mounts on the Same Mount Point

Since kernel 2.4, Linux allows multiple mounts to be stacked on a single mount
point. Each new mount hides the directory subtree previously visible at that
mount point. When the mount at the top of the stack is unmounted, the
previously hidden mount becomes visible once more

One use of mount stacking is to stack a new mount on an existing mount point
that is busy. Processes that hold file descriptors open, that are
chroot()-jailed, or that have current working directories within the old mount
point continue to operate under that mount, but processes making new accesses
to the mount point use the new mount.

Could employ mount stacking so that we don't need to care if /tmp is already
in use.


={============================================================================
*kt_linux_core_203* linux-io-fs-mount-tool-mount

       mount [-fnrsvw] [-t vfstype] [-o options] device dir

       The standard form of the mount command, is

              mount -t type device dir

       This tells the kernel to attach the filesystem found on `device` (which
           is of type `type`) `at` the directory `dir`.  The previous contents
       (if any) and  owner  and  mode  of  dir  become invisible, and as long
       as this filesystem remains mounted, the pathname dir refers to the root
       of the filesystem on device.


{after-change-fstab}
mount -a

       -a, --all
              Mount all filesystems (of the given types) mentioned in fstab
              (except for those whose line contains the noauto keyword).  The
              filesystems are mounted following their order in fstab.


{find-rfs}
[root@HUMAX /]# mount
rootfs on / type rootfs (rw)
/dev/root on / type ext3 (rw,relatime,errors=continue,barrier=1,data=ordered)
proc on /proc type proc (rw,relatime)
...

For the / mount point, you are just told that it corresponds to /dev/root, which
is not the real device you are looking for.

Of course, you can look at the kernel command line and see on which initial root
filesystem Linux was instructed to boot (root parameter):

[root@HUMAX /]# cat /proc/cmdline 
root=/dev/sda1 rw  macaddr=28:32:C5:3F:59:4E bmem=360M@512M wakeup=REBOOT ...


<options>
       -o, --options opts
              Use the specified mount options.  The opts argument is a
              comma-separated list.  For example:

                     mount LABEL=mydisk -o noatime,nodev,nosuid

              For more details, see the FILESYSTEM-INDEPENDENT MOUNT OPTIONS and
              FILESYSTEM-SPECIFIC MOUNT OPTIONS sections.


<when-cannot-run>
./yv-free: Permission denied

is it on a mount point with noexec?

/dev/mapper/sda5 on /var type ext3 (rw,nosuid,nodev,noexec,relatime,errors=continue,barrier=0,data=ordered)

/tmp is a link to /var/tmp

Try

mount -o remount,exec /var

that looks to have done the trick


<busybox-mount>
BusyBox v1.13.1 (2016-07-19 14:09:40 BST) multi-call binary

Usage: mount [flags] DEVICE NODE [-o options,more-options]

Mount a filesystem. Filesystem autodetection requires /proc be mounted.

Options:
        -a              Mount all filesystems in fstab
        -r              Read-only mount
        -t fs-type      Filesystem type
        -w              Read-write mount (default)
-o option:
        loop            Ignored (loop devices are autodetected)
        [a]sync         Writes are asynchronous / synchronous
        [no]atime       Disable / enable updates to inode access times
        [no]diratime    Disable / enable atime updates to directories
        [no]relatime    Disable / enable atime updates relative to modification time
        [no]dev         Allow use of special device files / disallow them
        [no]exec        Allow use of executable files / disallow them
        [no]suid        Allow set-user-id-root programs / disallow them
        [r]shared       Convert [recursively] to a shared subtree
        [r]slave        Convert [recursively] to a slave subtree
        [r]private      Convert [recursively] to a private subtree
        [un]bindable    Make mount point [un]able to be bind mounted
        bind            Bind a directory to an additional location
        move            Relocate an existing mount point
        remount         Remount a mounted filesystem, changing its flags
        ro/rw           Mount for read-only / read-write

There are EVEN MORE flags that are specific to each filesystem
You'll have to see the written documentation for those filesystems


<mount-usb>
You'll need to know what the drive is called to mount it.

sudo fdisk -l

Device Boot      Start         End      Blocks  Id System
/dev/sda1              64  3907007999  1953503968   1 FAT12


<case>
When tried to run a shell script on a cdrom, got "permission denied" even if
it has excutable and run it as a root. The problem was it was mounted as a
read-only. The solution is to edit /etc/fstab to mount it with exec option as:

# /etc/fstab: static file system information.
/dev/sr0        /media/cdrom0   udf,iso9660 user,noauto,exec   0       0

the filesystem is mounted with the noexec option, so the execute permission
bits on all files are ignored, and you cannot directly execute any program
residing on this filesystem. Note that the noexec mount option is implied by
the user option in /etc/fstab. ... If you use user and want to have executable
files, use user,exec.

<ex>
kyoupark@kit-debian:~$ cat /etc/fstab 
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a
# device; this may be used with UUID= as a more robust way to name devices
# that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
# / was on /dev/sda1 during installation
UUID=3f053275-8cc0-4ce8-89c6-412cc9773f1c /               ext4    errors=remount-ro 0       1
# swap was on /dev/sda5 during installation
UUID=64bc1edf-1b58-4e86-b1a9-521c09fbf263 none            swap    sw              0       0
# /dev/sr0        /media/cdrom0   udf,iso9660 user,exec     0       0
/dev/disk/by-uuid/91479737-b91b-4271-87d1-806f084a4c57 /mnt/ext auto nosuid,nodev,nofail,x-gvfs-show 0 0
# nfs
#10.209.62.232:/home/NDS-UK/kyoupark/STB_SW 	/home/kyoupark/bld_STB_SW 	nfs 	rw,sync,hard,intr 	0 0
#10.209.62.232:/home/NDS-UK/kyoupark/si_logs 	/home/kyoupark/bld_si_logs	nfs 	rw,sync,hard,intr 	0 0


={============================================================================
*kt_linux_core_203* linux-io-fs-mount-bind tool-chroot

https://unix.stackexchange.com/questions/198590/what-is-a-bind-mount

<bind-mount>
What is a bind mount?

A bind mount is an alternate view of a directory tree. Classically, mounting
creates a view of a storage device as a directory tree. A bind mount instead
takes an existing directory tree and replicates it under a different point.
The directories and files in the bind mount are the same as the original. Any
modification on one side is immediately reflected on the other side, since the
two views show the same data.

For example, after issuing the Linux command

mount --bind /some/where /else/where

the directories /some/where and /else/where have the same content.

Unlike a hard link or symbolic link, a bind mount doesn't affect what is
stored on the filesystem. It's a property of the live system.


How do I create a bind mount?

Linux bind mount

Under Linux, bind mounts are available as a kernel feature. You can create one
with the mount command, by passing either the --bind command line option or
the bind mount option. The following two commands are equivalent:

mount --bind /some/where /else/where
mount -o bind /some/where /else/where

Here, the “device” /some/where is not a disk partition like in the case of an
on-disk filesystem, but an existing directory. The mount point /else/where
must be an existing directory as usual. 

Note that no filesystem type is specified either way: making a bind mount
doesn't involve a filesystem driver, it copies the kernel data structures from
the original mount.

A Linux bind mount is mostly indistinguishable from the original. The command
df -T /else/where shows the same device and the same filesystem type as df -T
/some/where. The files /some/where/foo and /else/where/foo are
indistinguishable, as if they were hard links. It is possible to unmount
/some/where, in which case /else/where remains mounted.

With older kernels (I don't know exactly when, I think until some 3.x), bind
mounts were truly indistinguishable from the original. Recent kernels do track
bind mounts and expose the information through PID/mountinfo, which allows
findmnt to indicate bind mount as such.

If there are mount points under /some/where, their contents are not visible
under /else/where. 

<rbind>
// from man page
// The bind mount call attaches only (part of) a single filesystem, not
// possible submounts.  The entire file hierarchy including submounts is
// attached a second place by using:
//
// mount --rbind olddir newdir

Instead of bind, you can use rbind, also replicate mount points underneath
/some/where. For example, if /some/where/mnt is a mount point then

mount --rbind /some/where /else/where
is equivalent to

mount --bind /some/where /else/where
mount --bind /some/where/mnt /else/where/mnt

In addition, Linux allows mounts to be declared as shared, slave, private or
unbindable. This affects whether that mount operation is reflected under a
bind mount that replicates the mount point. For more details, see the kernel
documentation.

Linux also provides a way to move mounts: where --bind copies, --move moves a
mount point.

It is possible to have different mount options in two bind-mounted
directories. There is a quirk, however: making the bind mount and setting the
mount options cannot be done atomically, they have to be two successive
operations. (Older kernels did not allow this.) For example, the following
commands create a read-only view, but there is a small window of time during
which /else/where is read-write:

mount --bind /some/where /else/where
mount -o remount,ro,bind /else/where

// Note that the filesystem mount options will remain the same as those on the
// original mount point, and cannot be changed by passing the -o option along
// with  --bind/--rbind.   
// The mount options can be changed by a separate remount command, for
// example:
//
// mount --bind olddir newdir
// mount -o remount,ro newdir


<tool-chroot>
Use cases

Mounting in a jail

A chroot jail runs a process in a subtree of the system's directory tree. This
can be useful to run a program with restricted access, e.g. run a network
server with access to only its own files and the files that it serves, but not
to other data stored on the same computer). A limitation of chroot is that the
program is confined to one subtree: it can't access independent subtrees. Bind
mounts allow grafting other subtrees onto that main tree.

For example, suppose that a machine runs a service /usr/sbin/somethingd which
should only have access to data under /var/lib/something. The smallest
directory tree that contains both of these files is the root. How can the
service be confined? One possibility is to make hard links to all the files
that the service needs (at least /usr/sbin/somethingd and several shared
    libraries) under /var/lib/something. But this is cumbersome (the hard
      links need to be updated whenever a file is upgraded), and doesn't work
    if /var/lib/something and /usr are on different filesystems. A better
      solution is to create an ad hoc root and populate it with using mounts:

mkdir /run/something
cd /run/something
mkdir -p etc/something lib usr/lib usr/sbin var/lib/something
mount --bind /etc/something etc/something
mount --bind /lib lib
mount --bind /usr/lib usr/lib
mount --bind /usr/sbin usr/sbin
mount --bind /var/lib/something var/lib/something
mount -o remount,ro,bind etc/something
mount -o remount,ro,bind lib
mount -o remount,ro,bind usr/lib
mount -o remount,ro,bind usr/sbin
chroot . /usr/sbin/somethingd &

Linux's mount namespaces generalize chroots. Bind mounts are how namespaces
can be populated in flexible ways. See Making a process read a different file
for the same filename for an example.


LPI 18.12 Changing the Root Directory of a Process: chroot()

Normally, we can’t execute arbitrary programs within a chroot jail. This is
because most programs are dynamically linked against shared libraries.

Therefore, we must either limit ourselves to executing statically linked
programs, or replicate a standard set of system directories containing shared
libraries (including, for example, /lib and /usr/lib) within the jail (in this
    regard, the bind mount feature described in Section 14.9.4 can be useful).


={============================================================================
*kt_linux_core_203* linux-io-monitor-events 

LPI 19 Monitoring File Events

Starting with kernel 2.6.13, Linux provides the inotify mechanism, which
allows an application to monitor file events. This chapter describes the use
of inotify.

see tool-incron


={============================================================================
*kt_linux_core_204* linux-limit-and-options

LPI 11 System Limits and Options

A portable application needs ways of determining limit values and whether
options are supported.

Two principal avenues:

  Some limits and options can be determined at compile time. For example, the
  maximum value of an int is determined by the hardware architecture and
  compiler design choices. Such limits can be recorded in header files.

  Other limits and options may vary at run time. For such cases, SUSv3 defines
  three functionssysconf(), pathconf(), and fpathconf()that an application can
  call to check these implementation limits and options.


11.1 System Limits

For each limit that it specifies, SUSv3 requires that all implementations
support a `minimum value` for the limit. In most cases, this minimum value is
defined as a constant in `<limits.h>` with a name prefixed by the string
_POSIX_, and (usually) containing the string _MAX; thus, the form of the name
is _POSIX_XXX_MAX.

If an application restricts itself to the minimum values that SUSv3 requires
for each limit, then it will be portable to all implementations of the
  standard. However, doing so prevents an application from taking advantage of
    implementations that provide higher limits. 

For this reason, it is usually preferable to determine the limit on a
particular system using <limits.h>, sysconf(), or pathconf().

note: minimum?
The use of the string _MAX in the limit names defined by SUSv3 can appear
confusing, given their description as minimum values. The rationale for the
names becomes clear when we consider that each of these constants defines an
upper limit on some resource or feature, and the standards are saying that
this upper limit must have a certain minimum value.


Each limit has a name, which corresponds to the minimum value name described
above, `but lacks the _POSIX_ prefix.` An implementation may define a constant
with this name in <limits.h> to indicate the corresponding limit for this
implementation.

If defined, this limit will always be at least the size of the minimum value
described above (i.e., XXX_MAX >= _POSIX_XXX_MAX).


SUSv3 divides the limits that it specifies into three categories: runtime
invariant values, pathname variable values, and runtime increasable values.

Table 11-1: Selected SUSv3 limits

Name of limit   Min.      sysconf() / pathconf()      Description
(<limits.h>)    value     name (<unistd.h>)     

ARG_MAX 4096 _SC_ARG_MAX 
Maximum bytes for arguments (argv) plus environment (environ) that can be
supplied to an exec() (Sections 6.7 and 27.2.3)

PATH_MAX 256 _PC_PATH_MAX 
Maximum number of bytes in a pathname, including terminating null byte


Determining limits and options from the shell: getconf

From the shell, we can use the getconf command to obtain the limits and
options implemented by a particular UNIX implementation. The general form of
this command is as follows:

$ getconf variable-name [ pathname ]

The variable-name identifies the limit of interest and is one of the SUSV3
standard limit names, such as ARG_MAX or NAME_MAX. Where the limit relates to
a pathname, we must specify a pathname as the second argument to the command,
  as in the second of the following examples.

$ getconf ARG_MAX
131072
$ getconf NAME_MAX /boot
255

$ getconf PATH_MAX /
4096
$ getconf PATH_MAX /xx
4096


<proc-sys-kernel>
Various files under the /proc/sys/kernel directory can be used to view and
modify these limits.

On Linux, the ipcs -l command can be used to list the limits on each of the
IPC mechanisms. 


={============================================================================
*kt_linux_core_800* linux-proc

LPI-12 System and Process Information

Although many UNIX implementations provide a /proc file system, SUSv3 doesn’t
specify this file system; the details described in this book are
Linux-specific.


<proc-pid>
The /proc/PID directories are volatile. Each of these directories comes into
existence when a process with the corresponding process ID is created and
disappears when that process terminates.

12.1.1 Obtaining Information About a Process: /proc/PID

Table 12-1: Selected files in each /proc/PID directory

File      Description (process attribute)

cmdline   Command-line arguments delimited by \0
cwd       Symbolic link to current working directory
environ   Environment list NAME=value pairs, delimited by \0
exe       Symbolic link to file being executed
fd        Directory containing symbolic links to files opened by this process
maps      Memory mappings
mem       Process virtual memory (must lseek() to valid offset before I/O)
mounts    Mount points for this process
root      Symbolic link to root directory
status    Various information (e.g., process IDs, credentials, memory usage, signals)
task      Contains one subdirectory for each thread in process (Linux 2.6)


-sh-3.2# ls -al /proc/521
dr-xr-xr-x    4 NDS_U2   NDS_MW          0 Jan  1 00:06 .
dr-xr-xr-x   69 root     root            0 Jan  1 00:00 ..
-r--------    1 NDS_U2   NDS_MW          0 Jan  1 03:01 auxv
-r--r--r--    1 NDS_U2   NDS_MW          0 Jan  1 00:11 cmdline
lrwxrwxrwx    1 NDS_U2   NDS_MW          0 Jan  1 03:01 cwd -> /tmp
-r--------    1 NDS_U2   NDS_MW          0 Jan  1 03:01 environ
lrwxrwxrwx    1 NDS_U2   NDS_MW          0 Jan  1 03:01 exe -> /NDS/bin/MW_Process
dr-x------    2 NDS_U2   NDS_MW          0 Jan  1 03:01 fd
-r--r--r--    1 NDS_U2   NDS_MW          0 Jan  1 03:01 maps
-rw-------    1 NDS_U2   NDS_MW          0 Jan  1 03:01 mem
-r--r--r--    1 NDS_U2   NDS_MW          0 Jan  1 03:01 mounts
-r--------    1 NDS_U2   NDS_MW          0 Jan  1 03:01 mountstats
-rw-r--r--    1 NDS_U2   NDS_MW          0 Jan  1 03:01 oom_adj
-r--r--r--    1 NDS_U2   NDS_MW          0 Jan  1 03:01 oom_score
lrwxrwxrwx    1 NDS_U2   NDS_MW          0 Jan  1 03:01 root -> /
-r--r--r--    1 NDS_U2   NDS_MW          0 Jan  1 03:01 smaps
-r--r--r--    1 NDS_U2   NDS_MW          0 Jan  1 00:11 stat
-r--r--r--    1 NDS_U2   NDS_MW          0 Jan  1 03:01 statm
-r--r--r--    1 NDS_U2   NDS_MW          0 Jan  1 00:11 status
dr-xr-xr-x  145 NDS_U2   NDS_MW          0 Jan  1 00:11 task
-r--r--r--    1 NDS_U2   NDS_MW          0 Jan  1 03:01 wchan


<ex>
The parent of any process can be found by looking at the Ppid field provided in
the Linux-specific /proc/PID/status file.

Uid:  1024  1024  1024  1024
Gid:  1025  1025  1025  1025

The credentials of any process can be found by examining the Uid, Gid, and
Groups lines provided in the Linux-specific /proc/PID/status file. The Uid and
Gid lines list the identifiers in the order real, effective, saved set, and
file system.

FDSize:	32
Groups:	
VmPeak:	    2292 kB
VmSize:	    2288 kB
VmLck:	       0 kB
VmPin:	       0 kB
VmHWM:	     752 kB
VmRSS:	     752 kB
VmData:	     164 kB
VmStk:	     136 kB
VmExe:	      32 kB
VmLib:	    1904 kB
VmPTE:	      16 kB
VmSwap:	       0 kB
Threads:	1


              * VmPeak: Peak virtual memory size.
              * VmSize: Virtual memory size.

<signals>
SigQ: 0/3941
SigPnd: 0000000000000000
ShdPnd: 0000000000000000
SigBlk: 0000000000000000
SigIgn: fffffffe57f0d8fc
SigCgt: 00000000280b2603

CapInh:	0000000000000000
CapPrm:	ffffffffffffffff
CapEff:	ffffffffffffffff
CapBnd:	ffffffffffffffff
Cpus_allowed:	1
Cpus_allowed_list:	0
Mems_allowed:	1
Mems_allowed_list:	0
voluntary_ctxt_switches:	310
nonvoluntary_ctxt_switches:	127


<proc-pid-stat>
-sh-3.2# cat /proc/521/stat
521 (MW_Process) S 518 32 32 0 -1 4202752 10035 0 440 0 66117 181791 0 0 -7 0 143 0 8911 60956672 8707 2147483647 4194304 22313252 2140515872 2140514904 716227284 0 0 4096 0 4294967295 0 0 18 0 6 2 0


<proc-exe>
note: to check the path name of the process.

The Linux-specific /proc/PID/exe file is a symbolic link containing the
absolute pathname of the executable file being run by the corresponding
process.

$ ls -1l /proc/3510/exe
lrwxrwxrwx 1 keitee keitee 0 Feb  2 22:21 /proc/3510/exe -> /home/keitee/bin/vim


<proc-cmdline>
note: get process name

{
  printf( "Nexus resources are owned by PID %d\n", sharedMem->resource_pid);
  char name[512];
  sprintf(name, "cat /proc/%d/cmdline", sharedMem->resource_pid);
  system(name);
}


[si_logs@theyard 26360]$ cat cmdline
-bash[si_logs@theyard 26360]$


<proc-maps> *tool-pmap*
Using the Linux-specific /proc/PID/maps file, we can see the location of the
shared memory segments and shared libraries mapped by a program.

From LPI 48.5.

$ cat /proc/9903/maps
08048000-0804a000 r-xp 00000000 08:05 5526989 /home/mtk/svshm_attach          <1>
0804a000-0804b000 r--p 00001000 08:05 5526989 /home/mtk/svshm_attach
0804b000-0804c000 rw-p 00002000 08:05 5526989 /home/mtk/svshm_attach
b7bed000-b7f0d000 rw-s 00000000 00:09 9666565 /SYSV00000000 (deleted)         <2>
b7f0d000-b7f26000 rw-s 00000000 00:09 9633796 /SYSV00000000 (deleted)
b7f26000-b7f27000 rw-p b7f26000 00:00 0
b7f27000-b8064000 r-xp 00000000 08:06 122031 /lib/libc-2.8.so                 <3>
b8064000-b8066000 r--p 0013d000 08:06 122031 /lib/libc-2.8.so
b8066000-b8067000 rw-p 0013f000 08:06 122031 /lib/libc-2.8.so
b8067000-b806b000 rw-p b8067000 00:00 0
b8082000-b8083000 rw-p b8082000 00:00 0
b8083000-b809e000 r-xp 00000000 08:06 122125 /lib/ld-2.8.so                   <4>
b809e000-b809f000 r--p 0001a000 08:06 122125 /lib/ld-2.8.so
b809f000-b80a0000 rw-p 0001b000 08:06 122125 /lib/ld-2.8.so
bfd8a000-bfda0000 rw-p bffea000 00:00 0 [stack]                               <5>
ffffe000-fffff000 r-xp 00000000 00:00 0 [vdso]                                <6>

<1> Three lines for the main program, shm_attach. These correspond to the text
and data segments of the program. The second of these lines is for a readonly
page holding the string constants used by the program.

<2> Two lines for the attached System V shared memory segments.

<3> Lines corresponding to the segments for two shared libraries. One of these
is the standard C library (libc-version.so). 

<4> The other is the dynamic linker (ld-version.so).

<5> A line labeled [stack]. This corresponds to the process stack.

<columns>
1. A pair of hyphen-separated numbers indicating the virtual address range at
which the memory segment is mapped. The second of these numbers is the address
of the next byte after the end of the segment.

note: From this, can get the size. b7f0d000-b7bed000=0x320000. 3,276,800

2. Protection and flags for this memory segment. The first three letters
indicate the protection of the segment: read (r), write (w), and execute (x).
A hyphen (-) in place of any of these letters indicates that the corresponding
protection is disabled. The final letter indicates the mapping flag for the
memory segment; it is either private (p) or shared (s). For an explanation of
these flags, see the description of the MAP_PRIVATE and MAP_SHARED flags in
Section 49.2. 
    
note: A System V shared memory segment is always marked shared.

3. The hexadecimal offset (in bytes) of the segment within the corresponding
mapped file. The meanings of this and the following two columns will become
clearer when we describe the mmap() system call in Chapter 49. 

note: For a System V shared memory segment, the offset is always 0.

4. The device number (major and minor IDs) of the device on which the
corresponding mapped file is located.

5. The i-node number of the mapped file, or, for System V shared memory
segments, the 'identifier' for the segment.

6. The filename or other identifying tag associated with this memory segment.
For a System V shared memory segment, this consists of the string SYSV
concatenated with the shmget() key of the segment (expressed in hexadecimal).
In this example, SYSV is followed by zeros because we created the segments
using the key IPC_PRIVATE (which has the value 0). The string (deleted) that
appears after the SYSV field for a System V shared memory segment is an
artifact of the implementation of shared memory segments. Such segments are
created as mapped files in an invisible tmpfs file system (Section 14.10), and
then later unlinked. Shared anonymous memory mappings are implemented in the
same manner. (We describe mapped files and shared anonymous memory mappings in
    Chapter 49.)

<smaps> *tool-pmap*
Starting with kernel 2.6.14, Linux also provides the /proc/PID/smaps file,
         which exposes more information about the memory consumption of each
         of a process's mappings. For further details, see the proc(5) manual
         page.

The snippet from proc man and see man page for more.

/proc/[pid]/smaps (since Linux 2.6.14)

This file shows memory consumption for each of the process's mappings. (The
    pmap(1) command displays similar information, in a form that may be easier
    for parsing.) For each mapping there is a series of lines such as the
following:

00400000-0048a000 r-xp 00000000 fd:03 960637       /bin/bash
Size:                552 kB
Rss:                 460 kB
Pss:                 100 kB
Shared_Clean:        452 kB
Shared_Dirty:          0 kB
Private_Clean:         8 kB
Private_Dirty:         0 kB
Referenced:          460 kB
Anonymous:             0 kB
AnonHugePages:         0 kB
Swap:                  0 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB
Locked:                0 kB


<ex>
# pmap `pgrep MW_Process`


<ex> note: to check shared libraries that process uses

cat /proc/NNNN/maps | awk '{print $6}' | grep '\.so' | sort | uniq

or can use ldd.


<proc-fd> <proc-files>

/proc/[pid]/fd

This is a subdirectory containing one entry for each file which the process
has open, named by its file descriptor, and which is a symbolic link to the
actual file. Thus, 0 is standard input, 1 standard output, 2 standard error,
       etc.

In a multithreaded process, the contents of this directory are not available
if the main thread has already terminated (typically by calling
    pthread_exit(3)).

root       793  0.7  1.0  25528  4176 ?        Sl   16:04   0:00 /opt/zinc/oss/sbin/NetworkManager 
   --no-daemon --log-level=INFO

# ll /proc/793/fd
fd/     fdinfo/ 

# ll /proc/793/fd/
dr-x------    2 root     root           0 Feb 16 16:06 ./
dr-xr-xr-x    6 root     root           0 Jan  1  2000 ../
lr-x------    1 root     root          64 Feb 16 16:06 0 -> /dev/null
l-wx------    1 root     root          64 Feb 16 16:06 1 -> /var/tmp/NetworkManager.log
lrwx------    1 root     root          64 Feb 16 16:06 10 -> /dev/nexus_astm
lrwx------    1 root     root          64 Feb 16 16:06 11 -> /dev/nexus_display
lrwx------    1 root     root          64 Feb 16 16:06 12 -> /dev/nexus_graphics2d
lrwx------    1 root     root          64 Feb 16 16:06 13 -> /dev/nexus_surface
lrwx------    1 root     root          64 Feb 16 16:06 14 -> /dev/nexus_audio
lrwx------    1 root     root          64 Feb 16 16:06 15 -> /dev/nexus_video_decoder
lrwx------    1 root     root          64 Feb 16 16:06 16 -> /dev/nexus_transport
lrwx------    1 root     root          64 Feb 16 16:06 17 -> /dev/nexus_dma
lrwx------    1 root     root          64 Feb 16 16:06 18 -> /dev/nexus_security
lrwx------    1 root     root          64 Feb 16 16:06 19 -> /dev/nexus_spi
l-wx------    1 root     root          64 Feb 16 16:06 2 -> /var/tmp/NetworkManager.log
lrwx------    1 root     root          64 Feb 16 16:06 20 -> /dev/nexus_frontend
lrwx------    1 root     root          64 Feb 16 16:06 21 -> /dev/nexus_keypad
lrwx------    1 root     root          64 Feb 16 16:06 22 -> /dev/nexus_rfm
lrwx------    1 root     root          64 Feb 16 16:06 23 -> /dev/nexus_uhf_input
lrwx------    1 root     root          64 Feb 16 16:06 24 -> /dev/nexus_input_capture
lrwx------    1 root     root          64 Feb 16 16:06 25 -> /dev/nexus_ir_blaster
lrwx------    1 root     root          64 Feb 16 16:06 26 -> /dev/nexus_ir_input
lrwx------    1 root     root          64 Feb 16 16:06 27 -> /dev/nexus_led
lrwx------    1 root     root          64 Feb 16 16:06 28 -> /dev/nexus_gpio
lrwx------    1 root     root          64 Feb 16 16:06 29 -> /dev/nexus_i2c
lr-x------    1 root     root          64 Feb 16 16:06 3 -> pipe:[548]|
lrwx------    1 root     root          64 Feb 16 16:06 30 -> /dev/nexus_pwm
...

As a convenience, any process can access its own /proc/PID directory using the
symbolic link /proc/self.


<proc-task>
Linux 2.4 added the notion of thread groups to properly support the POSIX
threading model.

Under each /proc/PID/task/TID subdirectory is a set of files and directories
exactly like those that are found under /proc/PID. 

Since threads share many attributes, much of the information in these files is
the same for each of the threads in the process. However, where it makes
sense, these files show distinct information for each thread. For example, in
the /proc/PID/task/TID/status files for a thread group, State, Pid, SigPnd,
    SigBlk, CapInh, CapPrm, CapEff, and CapBnd are some of the fields that may
    be distinct for each thread.


={============================================================================
*kt_linux_core_814* linux-proc-system-info

Table 12-2: Purpose of selected /proc subdirectories

Directory Information exposed by files in this directory

/proc             Various system information
/proc/net         Status information about networking and sockets
/proc/sys/fs      Settings related to file systems
/proc/sys/kernel  Various general kernel settings
/proc/sys/net     Networking and sockets settings
/proc/sys/vm      Memory-management settings
/proc/sysvipc     Information about System V IPC objects


<proc-stat>
root# cat /proc/stat 
cpu  33343 0 15876 89567 4707 788 3710 0 0
cpu0 15661 0 8726 40043 3559 788 3698 0 0
cpu1 17681 0 7149 49524 1147 0 11 0 0
intr 3547815 0 125304 0 2 128777 0 1798 0 3886 2 7 0 0 71484 0 2 42852 0 1 5264 435754 0 5346 0 0 0 0 0 0 0 0 34415 47045 0 0 0 0 0 0 0 0 0 27337 0 0 0 0 0 7031 0 0 0 0 0 0 0 0 0 16826 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2181 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 65562 107793 0 0 0 0 0 2419144 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
ctxt 4296231
btime 1429628483
processes 1602
procs_running 4
procs_blocked 0
softirq 2964653 827836 1298645 1927 47517 27361 5147 32904 1321 721995

cpu  3357 0 4313 1362393
The amount of time, measured in units of USER_HZ (1/100ths of a second on most
    architectures, use sysconf(_SC_CLK_TCK) to obtain the right value), 

1. that  the  system spent  in  user  mode, 
2. user mode with low priority (nice), 
3. system mode, and 
4. the idle task, respectively.  

The last value should be USER_HZ times the second entry in the uptime
pseudo-file.

In Linux 2.6 this line includes three additional columns: 

5. iowait - time waiting for I/O  to  complete  (since  2.5.41);  
6. irq  -  time  servicing  interrupts  (since 2.6.0-test4); 
7. softirq - time servicing softirqs (since 2.6.0-test4).

Since Linux 2.6.11, there is an eighth column, 
      
8. steal - stolen time, which is the time spent in other operating systems
  when running in a virtualized environment

Since Linux 2.6.24, there is a ninth column, 

9. guest, which is the time spent running a virtual CPU for guest operating
  systems under the control of the Linux kernel.


<ex>
/**
 * Abstraction of proc/stats file format to hold the data and to
 *   allow calculation of the CPU utilisation based on it.
 */
struct ProcStats
{
    long user;   // Time spent executing user applications (user mode).
    long nice;   // Time spent executing user applications with low priority (nice).
    long system; // Time spent executing system calls (system mode).
    long idle;   // Idle time.
    long iowait; // Time waiting for I/O operations to complete.
    long irq;    // Time spent servicing interrupts.
    long softirq;// Time spent servicing soft-interrupts.

    ProcStats operator-(const ProcStats& other)
    {
        ProcStats res;
        res.user    = user   - other.user;
        res.nice    = nice   - other.nice;
        res.system  = system - other.system;
        res.idle    = idle   - other.idle;
        res.iowait  = iowait - other.iowait;
        res.irq     = irq    - other.irq;
        res.softirq = softirq- other.softirq;
        return res;
    }

    /**
     * Returns calculated total cpu utilisation in percentage.
     */
    double cpuUtilisation() const
    {
        double cpu_total = user + nice + system + irq + softirq;
        double cpu_idle  = idle + iowait;
        return cpu_total / (cpu_total + cpu_idle) * 100; // in percentage
    }
};

inline std::istream& operator>>(std::istream& in, ProcStats& st)
{
    std::string cpu;
    if (in >> cpu)
    {
        in >> st.user >> st.nice >> st.system >> st.idle;
        in >> st.iowait >> st.irq >> st.softirq;
    }
    return in;
}


/**
 * Objects of this type can read (repeatedly) cpu stats file and compare
 * it against values read previously.
 */
class CpuProcStatsReader
{
public:
    CpuProcStatsReader() :
        stat_filename("/proc/stat"),
        now_first(false)
    {
    }

    /**
     * Reads current value of /proc/stats and returns ProcStats with differences
     * with values read during last call to get().
     */
    ProcStats get()
    {
        std::string cpu;
        std::ifstream f(stat_filename.c_str());
        ProcStats& st = curr();
        f >> st;
        now_first = !now_first;
        return now_first ? stats.second - stats.first : stats.first - stats.second;
    }

    /**
     * Helper function (useful rather for testing) to setup a location of
     *   /proc/stat file.
     */
    void setupProcStatLocation(const std::string& filename)
    {
        stat_filename = filename;
    }

private:
    /**
     * Internal method to get the least recently used stats that is to be used/updated.
     */
    ProcStats& curr()
    {
        return now_first ? stats.first : stats.second;
    }

    std::pair<ProcStats, ProcStats> stats;
    std::string stat_filename;
    bool now_first;
};


={============================================================================
*kt_linux_core_200* ipc

CH43, Fig 43-1 in {ref-LPI} which says that the 'general' term IPC is often used to describe them all;
communication, signal, and synchronization.

communication - data transfer - byte stream  - pipe
                                             - fifo
                                             - stream socket

                              - message      - sys v message q
                                             - posix message q
                                             - datagram socket

                              - pseudoterminal

               - shared memory   - sys v shm
                                 - posix shm
                                 - memory mapping  - anonymous mapping
                                                   - mapped file

signal   - standard signal
         - realtime signal

synchronization   - semaphore - sys v
                  - posix     - named
                              - unnamed
                  - file lock - record lock
                              - file lock

                  - mutex
                  - condition variable

Signals: Although signals are intended primarily for other purposes, they can be used as a
synchronization technique in certain circumstances. More rarely, signals can be used as a
communication technique: the signal number itself is a form of information, and realtime signals can
be accompanied by associated data (an integer or a pointer).


{nonnetworked-ipc}
From {ref-UNP}. nonnetworked-ipc means that ipc for local and newtworked-ipc means that for remote
such as socket.


{categories-of-ipc}
From {ref-UNP}:

three-ways-to-share-between-processes

(1) Process Process       (2) Process Process          (3) Process <- shm -> Process
      |        |                 |       |      
Kernel                          shared info in kernel
      |        |
Filesystem


<persistence> which is lifetime of an objects
Define the persistence of any type of IPC as how long an object of that type remains in existence.

Process-persistence exists until last process with IPC open closes the object and kernel one exists
until kernel reboots or IPC objects is explicitly deleted.

process-persistence: pipe, fifo, mutex, condition-var, read-write-lock, ...
kernel-presistence : shm, named-semaphore, ...

Be careful when defining the presistence of ipc because it is not always as it seems: the data
within a pipe is maintained within the kernel, but pipes have process-persistence because after the
last process that has the pipe open for reading closes the pipe, the kernel discard all data and
remove the pipe.

From {ref-LPI}:

<data-transfer> 
1. requires two data transfers between user and kenel memory
2. synchronization between the reader and writer processes is automatic by kernel. if a reader
attempts to fetch data from data-transfer facility that has no data, then read will bock until some
write data to it.  
3. available to one which done read operation since read consumes data.

<byte-stream>
read and write is independent meaning read may read an arbitrary bytes. This models "file as a
sequence of bytes".

An application can also impose a message-oriented model on a byte-stream facility, by using
delimiter characters, fixed-length messages, or message headers that encode the length of the total
message message: each read reads a whole message. not possible to read part of a message and to read
multiple messages.


<shared-memory>
1. don't require system calls or data transfer between user and kenel. Hence shared memory
provide very fast communication. 
2. However it can be offset by the need to sync and semaphore is the usual method used with shared
memory.
3. avaible to all of the processes that share that memory

<file-desc-based>
facility using file descriptors like pipe, fifo, and sockets

The primary benefit of these techniques is that they allow an application to simultaneously monitor
multiple file descriptors to see whether I/O is possible on any of them.


{namespace}
Use name or identifier so that one process can create ipc object and other processes can specify
that same ipc object.

type                       name used to identify   handle used to refer to object
------------------------------------------------------------------------------------
pipe                       no name                 file descriptor
fifo                       pathname                ditto

UNIX domain socket         pathname                ditto
Internet domain socket     IP and port             ditto

posix message q            posix ipc pathname      mqd_t
posix named semaphore      ditto                   sem_t* (sem pointer)
posix unnamed semaphore    no name                 sem_t*
posix shared memory        posix ipc pathname      file descriptor 

anonymous mapping          no name                 none
memory mapped file         pathname                file descriptor 


{accessibility-and-persistence}
type                       accessibility                 persistence
------------------------------------------------------------------------------------
pipe                       only by related processes     process
fifo                       permission mask               ditto

UNIX domain socket         permission mask               ditto
Internet domain socket     by any processe               ditto

posix message q            permission mask               kernel
posix named semaphore      permission mask               kernel
posix unnamed semaphore    permission of underlying mem  depends
posix shared memory        permission mask               kernel

anonymous mapping          only by releated              process 
memory mapped file         permission mask               file system 

unix and network domain:

Of all of the IPC methods shown in Figure 43-1, only sockets permit processes to communicate over a
network. Sockets are generally used in one of two domains: the UNIX domain, which allows
communication between processes on the same system, and the Internet domain, which allows
communication between processes on different hosts connected via a TCP/IP network. Often, only minor
changes are required to convert a program that uses UNIX domain sockets into one that uses Internet
domain sockets, so an application that is built using UNIX domain sockets can be made
network-capable with relatively little effort.

<portability>
However, the POSIX IPC facilities (message queues, semaphores, and shared memory) are not quite as
widely available as their System V IPC counterparts, especially on older UNIX systems. An
implementation of POSIX message queues and 'full' support for POSIX semaphores have appeared on Linux
only in the 2.6.x kernel series. Therefore, from a portability point of view, System V IPC may be
preferable to POSIX IPC.

As of 06 Jan 2014, the latest stable kernel release is 3.12.6

note: Here, 'related' means related via fork(). In order for two processes to access the object, one
of them must create the object and then call fork(). As a consequence of the fork(), the child
process inherits a handle referring to the object, allowing both processes to share the object.

<performance>
In some circumstances, different IPC facilities may show notable differences in performance.
However, in later chapters, we generally refrain from making performance comparisons, for the
following reasons:

1) The performance of an IPC facility may not be a significant factor in the overall performance of
an application, and it may not be the only factor in determining the choice of an IPC facility.

2) The relative performance of the various IPC facilities may vary across UNIX implementations or
between different versions of the Linux kernel.

3) Most importantly, the performance of an IPC facility will vary depending on the precise manner
and environment in which it is used. Relevant factors include the size of the data units exchanged
in each IPC operation, the amount of unread data that may be outstanding on the IPC facility,
whether or not a process context switch is required for each unit of data exchanged, and other
load on the system.

If IPC performance is crucial, there is no substitute for application-specific benchmarks run under
an environment that matches the target system. To this end, it may be worth writing an 'abstract'
software layer that hides details of the IPC facility from the application and then testing
performance when different IPC facilities are substituted underneath the abstract layer.


={============================================================================
*kt_linux_core_201* ipc: system v

{interfaces}
A more significant reason for discussing the System V IPC mechanisms together is that their
programming interfaces share a number of common characteristics, so that many of the same concepts
apply to all of these mechanisms.


{key-and-identifier}
System V IPC keys are integer values represented using the data type key_t which is analogous to a
filename and get calls which is analogous to the open() system call used for files. The get calls
'translate' a key into the corresponding integer IPC identifier which is analogous to a file
descriptor.

<identifier>
There is, however, an important semantic difference. Whereas a file descriptor is a process
attribute, an IPC identifier is a property of the object itself and is visible 'system'-wide.

All processes accessing the same object use the same identifier. This means that if we know an IPC
object already exists, we can skip the get call, provided we have some other means of knowing the
identifier of the object.

<flag>
We specify the permissions to be placed on the new object as part of the final (flags) argument to
the get call, using the 'same' bit-mask constants as are used for files.

<key>
So, how do we provide a unique key? (LPI, 45.2)

One of three methods. Specify the IPC_PRIVATE constant as the key value to the get call when
creating the IPC object, which 'always' results in the creation of a 'new' IPC object that is
guaranteed to have a 'unique' key.


{ctl-calls}
A few are generic to all IPC mechanisms. An example of a generic control operation is IPC_RMID,
  which is used to delete an object.

<when-deleted>
For message queues and semaphores, deletion of the IPC object is immediate, and any information
contained within the object is destroyed, regardless of whether any other process is still using the
object.

For files, if we remove the last link to a file, then the file is actually removed only after all
open file descriptors referring to it have been closed. As with files, Deletion of shared memory
objects occurs differently.


{persistence}
System V IPC objects have 'kernel' persistence. Once created, an object continues
to exist until it is explicitly deleted or the system is shut down. 

Two disadvantages: 
1. system-imposed limit. If we fail to remove unused objects, we may eventually encounter
application errors as a result of reaching these limits.

2. When deleting a message queue or semaphore object, a multiprocess application may not be able to
easily determine which will be the last process requiring access to the object, and thus when the
object can be safely deleted. Not for shm.


{associated-data}
The kernel maintains an associated data structure for 'each' instance of a System V IPC object. 

The associated data structure for an IPC object is initialized when the object is created via the
appropriate get system call. Once the object has been 'created', a program can obtain a copy of this
data structure using the appropriate ctl system call, by specifying an operation type of IPC_STAT.

<ipc_perm>
As well as data specific to the type of IPC object, the associated data structure for all three IPC
mechanisms includes a substructure, ipc_perm, that holds information used to determine permissions
granted on the object

EACCES.

See LPI 45.3 for more about permission checks.

<bypass-permission-check>
The second user could bypass this check by specifying 0 for the second argument of the msgget()
  call, in which case an error would occur only when the program attempted an operation requiring
  write permission on the IPC object

msgget(key, 0);


{ipcs-command}
The ipcs command lists the System V IPC objects that currently exist on the system. The ipcrm
command is used to remove System IPC objects.

ipcs - provide information on ipc facilities

       ipcs [-asmq] [-tclup]

       Resources may be specified as follows:
       -m     shared memory segments
       -q     message queues
       -s     semaphore arrays
       -a     all (this is the default)

       The output format may be specified as follows:
       -t     time
       -p     pid
       -c     creator
       -l     limits
       -u     summary

$ ipcs -m -l

The status flags indicate whether the region has been locked into RAM to prevent swapping (Section
    48.7) and whether the region has been marked to be destroyed when all processes have detached
it.

<limitation>
On Linux, ipcs(1) displays information 'only' about IPC objects for which we have read permission,
   regardless of whether we own the objects.

<non-portable-means>
Linux provides two nonstandard methods of obtaining a list of all IPC objects on the system:

1. files within the /proc/sysvipc directory that list all IPC objects

/proc/sysvipc/msg
/proc/sysvipc/sem
/proc/sysvipc/shm
key         shmid    perms    size  cpid  lpid nattch uid   gid  cuid  cgid      atime      dtime      ctime
305450176   163844   666      24    1063  1395 4      504   504   504   504 1422517874 1422517866         33

note: see cpid and size

/proc/sysvipc
Subdirectory containing the pseudo-files msg, sem and shm. These files list the System V
Interprocess Communication (IPC) objects (respectively: message queues, semaphores, and shared
    memory) that currently exist on the system, providing similar information to that available via
ipcs(1).  These files have headers and are formatted (one IPC object per line) for easy
understanding. <svipc>(7) provides further background on the information shown by these files.

2. the use of Linux-specific ctl calls.

Unlike the ipcs command, these files always show all objects of the corresponding type, regardless
of whether read permission is available on the objects.


={============================================================================
*kt_linux_core_202* ipc: system v: shm

{segment}
Shared memory allows two or more processes to share the same region (usually referred to as a
segment) of physical memory. 


{good-and-bad}
Since a shared memory segment becomes part of a process's user-space memory, no kernel intervention
is required for IPC. note: means the fastest IPC.

On the other hand, the fact that IPC using shared memory is not mediated by the kernel means that,
   typically, some method of synchronization is required so that processes don't simultaneously
   access the shared memory. e.g., two processes performing simultaneous updates, or one process
   fetching data from the shared memory while another process is in the middle of updating it.


{interfaces}
<shmget>
To create a new shared memory segment or obtain the identifier of an existing segment. note: The
contents of a newly created shared memory segment are initialized to 0.

#include <sys/shm.h>

int shmget(key_t key, size_t size, int shmflg);

Returns shared memory segment 'identifier' on success, or -1 on error

<creator-and-size>
The 'shmflg' argument performs the same task as for the other IPC get calls, specifying the
permissions to be placed on a new shared memory segment or 'checked' against an existing segment. In
addition, zero or more of the fol- lowing flags can be ORed (|) in shmflg to control the operation
of shmget():

The 'kernel' allocates shared memory in multiples of the system page size, so size is effectively
rounded up to the next multiple of the system 'page' size. If we are using shmget() to obtain the
identifier of an existing segment, then size has 'no' effect on the segment, but it 'must' be less
than or equal to the size of the segment.

to-creat:

IPC_CREAT
If no segment with the specified key exists, create a new segment.

<when-get-errors>
The real case which got EINVAL when called shmget. The below is from man page.

EINVAL 

A new segment was to be created and size < SHMMIN or size > SHMMAX, or no new segment was to be
created, a segment with given key existed, but size is 'greater' than the size of that segment.

note: The problem was to ask shm which is 'greater' than the size of the segment. Interestingly, the
real size was 24 but /proc/PID/maps shows 4K since the pagesize is 4K. So careful to see mapping
info.

<shmat>
#include <sys/shm.h>

void *shmat(int shmid, const void *shmaddr, int shmflg);

Returns address at which shared memory is attached on success, or (void *) -1 on error

The shmaddr argument and the setting of the SHM_RND bit in the shmflg bit-mask argument control
how the segment is attached: See 48.3 for full options.

If shmaddr is NULL, then the segment is attached at a suitable address selected by the kernel. This
is the 'preferred' method of attaching a segment since specifying a non-NULL value for shmaddr is
not recommended, for the following reasons:

It reduces the portability of an application and the particular address will be already in use.

As its function result, shmat() returns the address at which the shared memory segment is attached.
Typically, we assign the return value from shmat() to a pointer to some programmer-defined
structure, in order to impose that structure on the segment. 

If SHM_RDONLY is not specified, the memory can be both read and modified.

note: Seen shmat(, , 0), that is shmflg is 0. what is it? See <bypass-permission-check>

<shmdt>
When a process no longer needs to access a shared memory segment, it can call shmdt() to detach the
segment from its virtual address space.

#include <sys/shm.h>

int shmdt(const void *shmaddr);

Returns 0 on success, or -1 on error

Detaching a shared memory segment is not the same as deleting it. Deletion is performed using the
shmctl() IPC_RMID operation

<child-and-exec>
A child created by fork() inherits its parent's attached shared memory segments. Thus, shared memory
provides an easy method of IPC between parent and child. During an exec(), all attached shared
memory segments are detached. Shared memory segments are also 'automatically' detached on process
'termination'.

<shmctl> 
#include <sys/shm.h>

int shmctl(int shmid, int cmd, struct shmid_ds *buf);

Returns 0 on success, or -1 on error

IPC_RMID <deletion>
Mark the shared memory segment and its associated shmid_ds data structure for deletion. If no
processes currently have the segment attached, deletion is immediate; otherwise, the segment is
removed only 'after' all processes have detached from it (i.e., when the value of the shm_nattch
    field in the shmid_ds data structure falls to 0). This is much closer to the situation with file
deletion.

shmctl(id, IPC_RMID, NULL);   note: buf must be NULL.

Only one process needs to perform this step.

IPC_STAT
Place a copy of the shmid_ds data structure associated with this shared memory segment in the buffer
pointed to by buf.

IPC_SET
Update selected fields of the shmid_ds data structure associated with this shared memory segment
using values in the buffer pointed to by buf.


{associated-data}
See LPI 48.8 for full details.

struct shmid_ds {
  struct ipc_perm shm_perm;   /* Ownership and permissions */
  size_t shm_segsz;           /* Size of segment in bytes */
  time_t shm_atime;           /* Time of last shmat() */
  time_t shm_dtime;           /* Time of last shmdt() */
  time_t shm_ctime;           /* Time of last change */
  pid_t shm_cpid;             /* PID of creator */
  pid_t shm_lpid;             /* PID of last shmat() / shmdt() */
  shmatt_t shm_nattch;        /* Number of currently attached processes */
};

shm_cpid
This field is set to the process ID of the process that created the segment using shmget().

shm_lpid
This field is set to 0 when the shared memory segment is created, and then set to the process ID of
the calling process on each successful shmat() or shmdt().

shm_nattch
This field counts the number of processes that currently have the segment attached. It is
initialized to 0 when the segment is 'created', and then incremented by each successful shmat() and
decremented by each successful shmdt(). 


{shm-in-virtual-memory}
From LPI 48.5. To check shm details from /proc/PID/maps.

The shared memory segments are attached starting at the virtual address 0x40000000 between heap and
stack. Mapped mappings and shared libraries are also placed in this area. The address 0x40000000 is
defined as the kernel constant TASK_UNMAPPED_BASE. Than can be changed.

$ ./svshm_create -p 102400       # size
9633796                          # shm id
$ ./svshm_create -p 3276800
9666565

$ ./svshm_attach 9633796:0 9666565:0
SHMLBA = 4096 (0x1000), PID = 9903
1: 9633796:0 ==> 0xb7f0d000      # attached at a address chosen by kernel
2: 9666565:0 ==> 0xb7bed000

$ cat /proc/9903/maps
...
b7bed000-b7f0d000 rw-s 00000000 00:09 9666565 /SYSV00000000 (deleted)
b7f0d000-b7f26000 rw-s 00000000 00:09 9633796 /SYSV00000000 (deleted)
...

<2> Two lines for the attached System V shared memory segments.

<columns>
1. A pair of hyphen-separated numbers indicating the virtual address range at which the memory
segment is mapped. The second of these numbers is the address of the next byte after the end of the
segment.

note: From this, can get the size. b7f0d000-b7bed000=0x320000. 3,276,800

2. Protection and flags for this memory segment. The first three letters indicate the protection of
the segment: read (r), write (w), and execute (x). A hyphen (-) in place of any of these letters
indicates that the corresponding protection is disabled. The final letter indicates the mapping flag
for the memory segment; it is either private (p) or shared (s). For an explanation of these flags,
    see the description of the MAP_PRIVATE and MAP_SHARED flags in Section 49.2. 
    
note: A System V shared memory segment is always marked shared.

3. The hexadecimal offset (in bytes) of the segment within the corresponding mapped file. The
meanings of this and the following two columns will become clearer when we describe the mmap()
system call in Chapter 49. 

note: For a System V shared memory segment, the offset is always 0.

4. The device number (major and minor IDs) of the device on which the corresponding mapped file is
located.

5. The i-node number of the mapped file, or, for System V shared memory segments, the 'identifier'
for the segment.

6. The filename or other identifying tag associated with this memory segment. For a System V shared
memory segment, this consists of the string SYSV concatenated with the shmget() key of the segment
(expressed in hexadecimal). In this example, SYSV is followed by zeros because we created the
segments using the key IPC_PRIVATE (which has the value 0). The string (deleted) that appears after
the SYSV field for a System V shared memory segment is an artifact of the implementation of shared
memory segments. Such segments are created as mapped files in an invisible tmpfs file system
(Section 14.10), and then later unlinked. Shared anonymous memory mappings are implemented in the
same manner. (We describe mapped files and shared anonymous memory mappings in Chapter 49.)

<example> 
Linux (none) 2.6.31-3.2 #2 SMP Wed Dec 10 02:53:42 EST 2014 mips GNU/Linux

cat /proc/sysvipc/shm

key         shmid    perms    size  cpid  lpid nattch uid   gid  cuid  cgid      atime      dtime      ctime
305450176   163844   666      24    1066  1470 4      504   504   504   504 1423064150 1423064135         33

root# ls -al /proc/1066/
lrwxrwxrwx    1 root     root             0 Jan  1 00:12 exe -> /opt/cds/bin/huaweidaemon

root# cat /proc/1066/maps | grep SYS      note: why twice?
2aab3000-2aab4000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)
2aab5000-2aab6000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)

root# cat /proc/1470/maps | grep SYS
2aab2000-2aab3000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)

The diff is 0x1000. 4096.

root# cat /proc/1523/smaps | grep -A 20 2fc37
2fc37000-2fc38000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)
Size:                  4 kB
Rss:                   4 kB
Pss:                   1 kB
Shared_Clean:          4 kB
Shared_Dirty:          0 kB
Private_Clean:         0 kB
Private_Dirty:         0 kB
Referenced:            4 kB
Swap:                  0 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB            <pagesize>


{shm-limits}
On Linux, some of the limits can be viewed or changed via files in the /proc file system. See LPI
48.9 for more details. 

Below is a list of the Linux shared memory limits. The system call affected by the limit and the
error that results if the limit is reached are noted in parentheses.

SHMMNI
This is a system-wide limit on the number of shared memory 'identifiers' (in other words, shared
    memory segments) that can be created. (shmget(), ENOSPC)

SHMMIN
This is the minimum size (in bytes) of a shared memory segment. This limit is defined with the value
1 (this can't be changed). However, the effective limit is the system page size. (shmget(), <EINVAL>)

SHMMAX
This is the maximum size (in bytes) of a shared memory segment. The practical upper limit for SHMMAX
depends on available RAM and swap space. (shmget(), <EINVAL>)

SHMALL
This is a system-wide limit on the total number of pages of shared memory. Most other UNIX
implementations don't provide this limit. The practical upper limit for SHMALL depends on available
RAM and swap space. (shmget(), ENOSPC)

<debian-linux>
keitee@debian-keitee:/proc/sys/kernel$ uname -a
Linux debian-keitee 3.2.0-4-486 #1 Debian 3.2.63-2+deb7u2 i686 GNU/Linux
keitee@debian-keitee:/proc/sys/kernel$ ls -al shm*
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmall
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmmax
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmmni
-rw-r--r-- 1 root root 0 Jan 28 21:25 shm_rmid_forced
keitee@debian-keitee:/proc/sys/kernel$ cat shmall
2097152
keitee@debian-keitee:/proc/sys/kernel$ cat shmmax
33554432
keitee@debian-keitee:/proc/sys/kernel$ cat shmmni
4096
keitee@debian-keitee:/proc/sys/kernel$ cat shm_rmid_forced 
0

<source> ucLinux case
/include/linux/shm.h

SHMMAX: shm segment max size in byte,     131,072 (typical value),      33,554,432 (ucLinux)
SHMMIN: shm segment min size in byte,     1 (typical value),            1 (ucLinux)
SHMMNI: shm segment max num in system,    100 (typical value),          4,096 (ucLinux)
SHMSEG: shm segment max size in process,  6 (typical value),            4,096 (ucLinux)


{locking}
A shared memory segment can be locked into RAM, so that it is never swapped out. This provides a
performance benefit, since, once each page of the segment is memory-resident, an application is
guaranteed never to be delayed by a page fault when it accesses the page.

<not-all-support> TODO:
These operations are not specified by SUSv3, and they are not provided on all UNIX implementations.
In versions of Linux before 2.6.10, only privileged (CAP_IPC_LOCK) processes can lock a shared
memory segment into memory. Since Linux 2.6.10, an unprivileged process can lock and unlock a shared
memory segment if its effective user ID matches either the owner or the creator user ID of the
segment and (in the case of SHM_LOCK) the process has a sufficiently high RLIMIT_MEMLOCK resource
limit. See Section 50.2 for details.


={============================================================================
*kt_linux_core_203* ipc: server consideration

From LPI 45.4.

The consideration when IPC server is terminated prematurely.

Suppose a client engages in an extended dialogue with a server, with multiple IPC operations being
performed by each process (e.g., multiple messages exchanged, a sequence of semaphore operations, or
    multiple updates to shared memory). 

What happens if the server process 'crashes' or is deliberately halted and then restarted? 

At this point, it would make no sense to blindly reuse the existing IPC object created by the
previous server process, since the new server process has no knowledge of the historical information
associated with the current state of the IPC object. (For example, there may be a secondary request
    within a message queue that was sent by a client in response to an earlier message from the old
    server process.)

In such a scenario, the only option for the server may be to 'abandon' all existing
clients, 'delete' the IPC objects created by the previous server process, and create new
instances of the IPC objects. 

A newly started server handles the possibility that a previous instance of the server terminated
prematurely by first trying to create an IPC object by specifying both the IPC_CREAT and the
IPC_EXCL flags within the get call. If the get call fails because an object with the specified key
already exists, then the server assumes the object was created by an old server process; it
therefore uses the IPC_RMID ctl operation to delete the object, and once more performs a get call to
create the object. (This may be combined with other steps to ensure that another server process is
    not currently running, such as those described in Section 55.6.)

<example>
// return false if process is in slave mode(client)
// return true if process is in master mode(server)

// if no shm created already, then return true to create one.
// if shm is there, but no one use. so try to delete it and return true.
// if shm is there, but some use it, then return false.

// if no one use it then crash might happened so start it all over again.

static bool do_platform_init( size_t shMemSz )
{
  int    shmid, shmflag;
  struct shmid_ds shmds;

  shmflag = 0666;
  shmid = shmget( SHAREDMEM_KEY, shMemSz, shmflag );

  if ( shmid >= 0 )
  {
    if ( shmctl( shmid, IPC_STAT, &shmds ) < 0 )
    {
      perror( "shmctl - IPC_STAT failed" );
    }
    else
    {
      fprintf( stderr, "%s(): shmds.shm_nattch=%d\n",
               __FUNCTION__, (unsigned)shmds.shm_nattch );
      if ( 0 == shmds.shm_nattch && shmctl( shmid, IPC_RMID, (struct shmid_ds *)NULL ) < 0 )
      {
         fprintf( stderr, "shmctl - destroy failed\n" );
      }
    }
    return (0 == (unsigned)shmds.shm_nattch);
  }
  else
  {
    perror("shmget");
  }
  return true;
}


={============================================================================
*kt_linux_core_250* ipc: posix

One of the POSIX.1b developers' aims was to devise a set of IPC mechanisms that did not suffer the
deficiencies of the System V IPC facilities. These IPC mechanisms are collectively referred to as
POSIX IPC.

note: what deficiencies?


={============================================================================
*kt_linux_core_300* ipc: socket: LPI 56

Sockets are a method of IPC that allow data to be exchanged between applications, either on the same
host (computer) or on different hosts connected by a network.

A socket is created using the socket() system call, which returns a file descriptor used to refer to
the socket in subsequent system calls:

fd = socket(domain, type, protocol);

For all applications described in this book, protocol is 'always' specified as 0.


{communication-domain}
domain determines:

1. the method of identifying a socket (i.e., the format of a socket “address”)

2. the range of communication (i.e., either between applications on the same host or between
    applications on different hosts connected via a network).


OS supports at least the followings: AF stands for address family

Table 56-1: Socket domains

Domain   Communication     Communication           Address format          Address structure
         performed         between applications

AF_UNIX  within kernel     on same host            pathname                sockaddr_un

AF_INET  via IPv4          on hosts connected      32-bit IPv4 address +   sockaddr_in
                           via an IPv4 network     16-bit port number

AF_INET6 via IPv6          on hosts connected      128-bit IPv6 address +  sockaddr_in6
                           via an IPv6 network     16-bit port number


{type}
Every sockets implementation provides at least two types of sockets: stream and datagram.

Table 56-2: Socket types and their properties

Property                         Stream Socket     Datagram Socket 
Reliable delivery?               Y                 N
Message boundaries preserved?    N                 Y
Connection-oriented?             Y                 N

<stream-socket>
Stream sockets (SOCK_STREAM) provide a reliable, bidirectional, byte-stream communication channel.

These socket types are supported in both the UNIX(on same host) and the Internet domains(on
network).

1. Reliable means that we are guaranteed that either the transmitted data will arrive intact at the
receiving application, exactly as it was transmitted by the sender (assuming that neither the
    network link nor the receiver crashes), or that we'll receive notification of a probable failure
in transmission.

2. Bidirectional means that data may be transmitted in either direction between two sockets.

3. Byte-stream means that, as with pipes, there is no concept of message boundaries

Stream sockets operate in connected pairs so described as connection-'oriented'. A stream socket can
be connected to 'only' one peer.

<datagram-socket>
Datagram sockets (SOCK_DGRAM) allow data to be exchanged in the form of 'messages' called datagrams.

With datagram sockets, message boundaries are preserved, but data transmission is not reliable.
Messages may arrive out of order, be duplicated, or not arrive at all.

Datagram sockets are an example of the more generic concept of a 'connectionless' socket since
unlike a stream socket, a datagram socket doesn't need to be connected to another socket in order to
be used.

note:
In the Internet domain, datagram sockets employ the User Datagram Protocol (UDP), and stream sockets
(usually) employ the Transmission Control Protocol (TCP).  So often just use the terms UDP socket
and TCP socket, respectively.


{socket-io}
By default, these system calls 'block' if the I/O operation can't be completed immediately.
Nonblocking I/O is also possible, by using the fcntl() F_SETFL operation (Section 5.3) to enable the
O_NONBLOCK open file status flag.


{stream-example}

Passive socket (server)

  socket()

  bind()

  listen()

  accept()                                      Active socket(client)
      : blocks until client connects
   ...                                             socket()

      : resumes                  <-                connect()

+-------------------------------------------------------------+
| read()                         <-                write()    |
|                       : (possibly multiple) data            |
|                          transfers in either direction      |
| write()                        ->                read()     |
+-------------------------------------------------------------+
  close()                                          close()


<well-known-address>
Typically, we bind a server's socket to a well-known address - that is, a fixed address that is
known in advance to client applications that need to communicate with that server.


{syscalls}
#include <sys/socket.h>

<socket>
int socket(int domain, int type, int protocol);

Creates a new socket. Returns file descriptor on success, or -1 on error.
note: returns fd on success.

<bind>
int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

Binds a socket to an address. Returns 0 on success, or -1 on error

The sockfd argument is a file descriptor obtained from a previous call to socket(). 

<accept>
int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);

Returns file descriptor on success, or -1 on error

The accept() system call accepts an incoming connection on the listening stream socket referred to
by the file descriptor sockfd. If there are no pending connections when accept() is called, the call
blocks until a connection request arrives.

note: listening vs connection socket

The key point to understand about accept() is that it creates a 'new' socket, and it is this new
socket that is connected to the peer socket that performed the connect(). A file descriptor for the
connected socket is returned as the function result of the accept() call. The listening socket
(sockfd) remains open, and can be used to accept 'further' connections. A typical server application
creates one listening socket, binds it to a well-known address, and then handles all client requests
by accepting connections via that socket.

note: this is why accept() requires addr argument

The remaining arguments to accept() 'return' the address of the peer socket. The addr argument
points to a structure that is used to return the socket address. The type of this argument depends
on the socket domain

note: <value-result> argument is in-out argument which is also mentioned in man page.

The addrlen argument is a value-result argument. It points to an integer that, prior to the call,
    must be initialized to the size of the buffer pointed to by addr, 'so' that the kernel 'knows'
    how much space is available to return the socket address. Upon return from accept(), this
    integer is set to 'indicate' the number of bytes of data actually copied into the buffer.

If we are not interested in the address of the peer socket, then addr and addrlen should be
specified as NULL and 0, respectively. If desired, we can retrieve the peer’s address later using
the getpeername() system call, as described in Section 61.5.


{generic-address-structure}
The addr argument is a pointer to a structure specifying the address to which this socket is to be
bound. The type of structure passed in this argument 'depends' on the socket domain. The addrlen
argument specifies the size of the address structure.

For each socket domain, a 'different' structure type is defined to store a socket address. However,
    because system calls such as bind() are 'generic' to 'all' socket 'domains', they must be able
    to accept address structures of any type.

How?

In order to permit this, the sockets API defines a 'generic' address structure, struct sockaddr. The
only purpose for this type is to cast the various domain-specific address structures to a single
type for use as arguments in the socket system calls.

This structure serves as a template for all of the domain-specific address structures.

The value in the family field is 'sufficient' to determine the size and format of the address stored
in the remainder of the structure.

<sockaddr>
/* Structure describing a generic socket address.  */
struct sockaddr
{
  __SOCKADDR_COMMON (sa_);    /* Common data: address family and length.  */
  // note: that is sa_family_t sa_family
  char sa_data[14];           /* Address data.  */
};

#define  __SOCKADDR_COMMON(sa_prefix) \
  sa_family_t sa_prefix##family

<sockaddr_un>
/* Structure describing the address of an AF_LOCAL (aka AF_UNIX) socket.  */
struct sockaddr_un
{
  __SOCKADDR_COMMON (sun_);
  char sun_path[108];   /* Path name.  */          note: bigger size!
};

<sockaddr_in>
/* Structure describing an Internet (IP) socket address. */
#define __SOCK_SIZE__   16    /* sizeof(struct sockaddr) */
struct sockaddr_in {
  __kernel_sa_family_t  sin_family;    /* Address family */
  __be16                sin_port;      /* Port number    */    // 16 bit
  struct in_addr        sin_addr;      /* Internet address */  // 32 bit

  /* Pad to size of `struct sockaddr'. */
  unsigned char   __pad[__SOCK_SIZE__ - sizeof(short int) -
    sizeof(unsigned short int) - sizeof(struct in_addr)];
};

<sockaddr_in6>
struct sockaddr_in6 {
  unsigned short int    sin6_family;    /* AF_INET6 */
  __be16                sin6_port;      /* Transport layer port # */
  __be32                sin6_flowinfo;  /* IPv6 flow information */
  struct in6_addr       sin6_addr;      /* IPv6 address */
  __u32                 sin6_scope_id;  /* scope id (new in RFC2553) */
};


{code-example}

{
  const char *SOCKNAME = "/tmp/mysock";

  int sfd;
  struct sockaddr_un addr;

  sfd = socket(AF_UNIX, SOCK_STREAM, 0); /* Create socket */
  if (sfd == -1)
    errExit("socket");

  memset(&addr, 0, sizeof(struct sockaddr_un)); /* Clear structure */

  addr.sun_family = AF_UNIX; /* UNIX domain address */

  strncpy(addr.sun_path, SOCKNAME, sizeof(addr.sun_path) - 1);

  if (bind(sfd, (struct sockaddr *) &addr, sizeof(struct sockaddr_un)) == -1)
    errExit("bind");
}

// net/socket.c
//
/*
 * We move the socket address to kernel space before we call
 * the protocol layer (having also checked the address is ok).
 */
SYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)
{
  sock = sockfd_lookup_light(fd, &err, &fput_needed);
  if (sock) {
    err = move_addr_to_kernel(umyaddr, addrlen, &address);
    if (err >= 0) {
      err = security_socket_bind(sock,
          (struct sockaddr *)&address,
          addrlen);
      if (!err)
        err = sock->ops->bind(sock,
            (struct sockaddr *)
            &address, addrlen);
    }
    fput_light(sock->file, fput_needed);
  }
  return err;
}


// net/ipv4/af_inet.c
int inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
{
  // note: only used to check size
  if (addr_len < sizeof(struct sockaddr_in))
    goto out;
}


{
  struct sockaddr_storage claddr;

  addrlen = sizeof(struct sockaddr_storage);
  cfd = accept(lfd, (struct sockaddr *) &claddr, &addrlen);
  if (cfd == -1) {
    errMsg("accept");
    continue;
  }
}

/*
 * For accept, we attempt to create a new socket, set up the link
 * with the client, wake up the client, then return the new
 * connected fd. We collect the address of the connector in kernel
 * space and 'move' it to user at the very end. This is unclean because
 * we open the socket then return an error.
 *
 * 1003.1g adds the ability to recvmsg() to query connection pending
 * status to recvmsg. We need to add that support in a way thats
 * clean when we restucture accept also.
 */
SYSCALL_DEFINE4(accept4, int, fd, struct sockaddr __user *, upeer_sockaddr, int __user *, upeer_addrlen, int, flags)
{
  err = move_addr_to_user(&address, len, upeer_sockaddr, upeer_addrlen);
}


={============================================================================
*kt_linux_core_101* linux-ipc-pipe

{pipe}
The pipe is `unnamed-fifo` and is an early form that can be used
`related-processes` such as parent and child. In other words, created using
fork() call. Linux supports uni-directional pipe or half-duplex so need 'two'
pipes for read and write.


<connect-two-process>

$ ls | wc -l

ls                          pipe                           wc
(stdout, fd 1)    ->        unidirectional byte stream  -> (stdin, fd 0)
                  write end                             read end
                  of pipe                               of pipe


<byte-stream-and-sequential> LPI 44.1
When we say that a pipe is a byte stream, we mean that there is no concept of
'messages' or message boundaries when using a pipe. 

The process reading from a pipe can read 'blocks' of data of any size,
    regardless of the size of blocks written by the writing process.
Furthermore, the data passes through the pipe 'sequentially'
- bytes are read from a pipe in exactly the order they were written. It is not
possible to randomly access the data in a pipe using lseek().


<block-on-read> *sync-deadlock*
Attempts to read from a pipe that is currently empty 'block' until at least
one byte has been written to the pipe. 

If the write end of a pipe is closed, then a process reading from the pipe
will see end-of-file (i.e., read() returns 0) once it has read all remaining
data in the pipe.


<block-on-write> <limited-capacity> LPI 44 *sync-deadlock*
A pipe is simply a 'buffer' maintained in kernel memory. This buffer has a
maximum capacity. Once a pipe is full, further writes to the pipe 'block'
until the reader removes some data from the pipe.

<wary-of-deadlock> *sync-deadlock*
If employing two pipes for bi-directional buffer, then we need to be wary of
deadlocks that may occur if both processes block while trying to read from
empty pipes OR while trying to write to pipes that are already full.


<use-large-buffer-size>
SUSv3 makes no requirement about the capacity of a pipe. In Linux kernels
before 2.6.11, the pipe capacity is the same as the system page size (e.g.,
    4096 bytes on x86-32); since Linux 2.6.11, the pipe capacity is 65,536
bytes. Other UNIX implementations have different pipe capacities.

This means that pipe is kernel resource meaning that there is copy between
kernel and process. In theory, there is no reason why a pipe couldn't operate
with smaller capacities, even with a single-byte buffer. The reason for
employing large buffer sizes is efficiency: each time a writer fills the pipe,
          the kernel must perform a context switch to allow the reader to be
          scheduled so that it can empty some data from the pipe. Employing a
          larger buffer size means that fewer context switches are required.


<create-pipe-using-pipe-call>
#include <unistd.h>

int pipe(int filedes[2]);        Returns 0 on success, or -1 on error


<use-read-and-write>
As with any file descriptor, we can use the read() and write() system calls to
perform I/O on the pipe.


<set-up-pipe>
Parent gets two fds from a pipe() call which are fd[0] for read and fd[1] for
write. Create a child via fork() and the child process inherits copies of its
parent's file descriptors. Close unused fds to create a single channel between
parent and child. 0 for read and 1 for write which are fixed.

            parent                 ->                    parent           
                                             
fd[1] write       fd[0] read                 fd[1] write
                                             
   [         pipe       ]                       [         pipe        ]
                                             
            child                                        child
                                             
fd[0] write       fd[0] read                                fd[0] read


int filedes[2];

if (pipe(filedes) == -1) /* Create the pipe */
   errExit("pipe");

switch (fork()) { /* Create a child process */
 case -1:
   errExit("fork");

 case 0: /* Child */
   if (close(filedes[1]) == -1) // close unused 'write' end
     errExit("close");
   /* Child now reads from pipe */
   break;

 default: /* Parent */
   if (close(filedes[0]) == -1) // close unused 'read' end
     errExit("close");
   /* Parent now writes to pipe */
   break;
}


<needs-of-closing-for-reading-process>
If the reading process doesn't close the write end of the pipe, then, after
the other process closes its write descriptor, the reader won't see
end-of-file, even after it has read all data from the pipe. Instead, a read()
  would block waiting for data, because the kernel knows that there is still
  at least one write descriptor open for the pipe. That this descriptor is
  held open by the reading process itself is irrelevant; in theory, that
  process could still write to the pipe, even if it is blocked trying to read.
For example, the read() might be interrupted by a signal handler that writes
data to the pipe. (This is a realistic scenario, as we’ll see in Section
    63.5.2.)


<broken-pipe> *sigpipe*
When a process tries to write to a pipe for which no process has an open read
descriptor, the kernel sends the SIGPIPE signal to the writing process. By
default, this signal kills a process. A process can instead arrange to catch
or ignore this signal, in which case the write() on the pipe fails with the
error EPIPE (broken pipe). Receiving the SIGPIPE signal or getting the EPIPE
error is a useful indication about the status of the pipe, and this is why
unused read descriptors for the pipe should be closed.


<needs-of-closing-for-writing-process>
If the writing process doesn't close the read end of the pipe, then, even after the other process
closes the read end of the pipe, the writing process will still be able to write to the pipe.
Eventually, the writing process will fill the pipe, and a further attempt to write will block
indefinitely.


<pipe2-call>
Starting with kernel 2.6.27, Linux supports a new, nonstandard system call, pipe2(). This system
call performs the same task as pipe(), but supports an additional argument, flags, that can be used
to modify the behavior of the system call. Two flags are supported. The O_CLOEXEC flag causes the
kernel to enable the close-on-exec flag (FD_CLOEXEC) for the two new file descriptors. This flag is
useful for the same reasons as the open() O_CLOEXEC flag described in Section 4.3.1. The O_NONBLOCK
flag causes the kernel to mark both underlying open file descriptions as nonblocking, so that future
I/O operations will be nonblocking. This saves additional calls to fcntl() to achieve the same
result.


<pipe-for-unrelated-process>
There is an exception that pipes can be used to communicate only between
related processes. Passing a file descriptor over a UNIX domain socket (a
    technique that described in Section 61.13.3) makes it possible to pass a
file descriptor for a pipe to an unrelated process.


<second> to-create-pipe
popen() call which simplfies pipe creation, fork, reading/writing setting. However, need to set
problem to fork in the command line.

The C standard I/O library popen(3) makes it easy for the application programmer to open a pipe to
an external process.

#include <stdio.h>
FILE *popen(const char *command, const char *mode);
int pclose(FILE *stream);

The argument command must be a command that is acceptable to the UNIX shell. The second argument
mode must be the C string "r" for reading or "w" for writing.  No other combination, such as "w+",
is acceptable. 

(reading example)
FILE *p;
char cmd[1000];
/* argv[2] is fname */
sprintf(cmd,"grep 'Time has been updated to (Year:Month' %s | head -1",argv[2]);
p=popen(cmd,"r");
fgets(tmp,sizeof(tmp),p);
pclose(p);

After all, the reason that can use pipe between parent/child is that fds are shared.


={============================================================================
*kt_linux_core_102* linux-ipc-pipe-check-setup

[root@HUMAX 856]# ll /proc/846/fd
...
lr-x------    1 root     root          64 May  6 14:35 68 -> socket:[22102]=
lrwx------    1 root     root          64 May  6 14:45 69 -> socket:[22104]=
l-wx------    1 root     root          64 May  6 14:35 7 -> /var/opt-zinc-var/log/copper.log
lrwx------    1 root     root          64 May  6 14:35 70 -> /var/tmp/zmp-log
lrwx------    1 root     root          64 May  6 14:45 71 -> /var/tmp/zmp-log
lr-x------    1 root     root          64 May  6 14:45 72 -> pipe:[22129]| // note:
lrwx------    1 root     root          64 May  6 14:35 8 -> /dev/fusion0
lrwx------    1 root     root          64 May  6 14:35 9 -> /dev/nexus_proxy

[root@HUMAX 856]# lsof | grep 22129
856	/opt/zinc/bin/bronzemediad.oem	pipe:[22129]
1067	/run/youview/jail/daemons/linearsourced-S3kWYA/opt/zinc-trunk/bin/linearsourced	pipe:[22129]

This shows 'two' ends of pipe.


={============================================================================
*kt_linux_core_103* linux-ipc-fifo

{fifo}
To solve this, fifo was introduced and is called `named-pipe` since has path
name. Means that it is created in the filesystem as a file. Use usual read and
write call. Fifo is either read-only or write-only.

* create a fifo using mkfifo call.
* open a fifo for read or write using open call

<fifo-create>
#include <sys/stat.h>
int mkfifo(const char *pathname, mode_t mode);

<fifo-sync>
By default, opening a FIFO for reading (the open() O_RDONLY flag) blocks until
another process opens the FIFO for writing (the open() O_WRONLY flag).
Conversely, opening the FIFO for writing blocks until another process opens
the FIFO for reading.  In other words, opening a FIFO synchronizes the reading
and writing processes.

<note-from-nds-fusion-ipc>
FIFOs have certain natural limitations, they are unidirectional, and fragment
large message sizes with no built in support for reassembling the fragments.
Furthermore FIFOs are limited in number due to system resources. The key are
'fragment' and limit-in-number.

Due to the limited number of FIFOs it was determined that there would be one
control FIFO per server (to establish communication from clients) and one pair
of unidirectional FIFOs for every pair of server and client instances (note
    that there is one instance of a client in every process that uses that
    client). This is illustrated below.

server                  client a
- control pipe          -> and <-

                        client b
                        -> and <-

note: KT. do not match with this pic?

To cope with fragmentation and the fact that there may be several interfaces
being used on a single client instance (and multiple components using that
client instance in a single process) a protocol was designed as described
below. This protocol is used in both FIFO and TCP/IP IPC communication.


={============================================================================
*kt_linux_core_104* ipc: nonblocking read() and write() on pipes and fifo

LPI 44.10

Summarizes the operation of read() for pipes and FIFOs, and includes the effect of the O_NONBLOCK
flag.

<on-read>
The only difference between blocking and nonblocking reads occurs when no data is present and the
write end is open. In this case, a normal read() blocks, while a nonblocking read() fails with the
error EAGAIN.

Table 44-2: Semantics of 'read'ing n bytes from a pipe or FIFO containing p bytes

O_NONBLOCK  | Data bytes available in pipe or FIFO (p)
enabled?    | p = 0, write end open    | p = 0, write end closed  | p < n        | p >= n
----------------------------------------------------------------------------------------------- 
No          | block                    | return 0 (EOF)           | read p bytes | read n bytes
Yes         | fail (EAGAIN)            | return 0 (EOF)           | read p bytes | read n bytes

note: read(n) gets n or p


<on-write>
The impact of the O_NONBLOCK flag when writing to a pipe or FIFO is made complex by interactions
with the PIPE_BUF limit. The write() behavior is summarized in Table 44-3.

Table 44-3: Semantics of 'write'ing n bytes from a pipe or FIFO

O_NONBLOCK  | read end open
enabled?    | n <= PIFE_BUF
----------------------------------------------------------------------------------------------- 
No          | Atomically write n bytes; may block until sufficient data is read for write() to
            | be performed
Yes         | If sufficient space is available to immediately write n bytes, then write()
            | succeeds atomically; otherwise, it fails (EAGAIN)

note: if there is no space to write, then either blocks or EAGAIN.

O_NONBLOCK  | read end open
enabled?    | n > PIPE_BUF
----------------------------------------------------------------------------------------------- 
No          | Write n bytes; may block until sufficient data read for write() to complete; 
            | data may be interleaved with writes by other processes
Yes         | If there is sufficient space to immediately write some bytes, then write between 1 
            | and n bytes (which may be interleaved with data written by other processes);
            | otherwise, write() fails (EAGAIN)

            | read end closed
              SIGPIPE + EPIPE

The O_NONBLOCK flag causes a write() on a pipe or FIFO to fail (with the error EAGAIN) in any case
where data can't be transferred immediately. 

This means that if we are writing up to PIPE_BUF bytes, then the write() will fail if there is not
sufficient space in the pipe or FIFO, because the kernel can't complete the operation immediately
and can't perform a partial write, since that would break the requirement that writes of up to
PIPE_BUF bytes are 'atomic'.

When writing more than PIPE_BUF bytes at a time, a write is 'not' required to be atomic. For this
reason, write() transfers as many bytes as possible (a partial write) to fill up the pipe or FIFO.
In this case, the return value from write() is the number of bytes actually transferred, and the
'caller' 'must' retry later in order to write the remaining bytes. 

However, if the pipe or FIFO is full, so that not even one byte can be transferred, then write()
fails with the error EAGAIN.

Although blocks of data of any size can be written to a pipe, only writes that do not exceed
PIPE_BUF bytes are guaranteed to be atomic. 


={============================================================================
*kt_linux_core_105* ipc: which one to use

When performing interprocess synchronization, our choice of facility is typically determined by the
functional requirements. When coordinating access to a file, file record locking is usually the best
choice. Semaphores are often the better choice for coordinating access to other types of shared
resource.

{semaphores-versus-pthreads-mutexes}

<1>
Unlike mutex (this mean condition?), semaphore does not get lost when there is no waiting one.

<2>
POSIX semaphores and Pthreads mutexes can both be used to synchronize the actions of threads within
the same process, and their performance is [similar]. 

However, mutexes are usually preferable, because the [ownership] property of mutexes enforces good
structuring of code; only the thread that locks a mutex can unlock it. By contrast, one thread can
increment a semaphore that was decremented by another thread. This flexibility can lead to poorly
structured synchronization designs. For this reason, semaphores are sometimes referred to as the
"gotos" of concurrent programming.

There is one circumstance in which mutexes can't be used in a multithreaded application and
semaphores may therefore be preferable. Because it is async-signalsafe. See
{async-signal-safe-function}. The sem_post() function can be used from within a signal handler to
synchronize with another thread. This is not possible with mutexes, because the Pthreads functions
for operating on mutexes are not asyncsignal-safe. 

However, because it is usually preferable to deal with asynchronous signals by accepting them using
sigwaitinfo() (or similar), rather than using signal handlers (see Section 33.2.4), this advantage
of semaphores over mutexes is seldom required.

<3>
From 30.1.3. Peformance of mutex in ref-LPI. The problem with file locks and semaphores is that they
always require a system call for the lock and unlock operations, and each system call has a small
but appreciable, cost (Section 3.1). By contrast, mutexes are implemented using atomic
machine-language operations; performed on memory locations visible to all threads and require system
calls only in case of lock contention.


On Linux, mutexes are implemented using futexes (an acronym derived from fast user space mutexes),
   and lock contentions are dealt with using the futex() system call. We donât describe futexes in
   this book (they are not intended for direct use in user-space applications), but details can be
   found in [Drepper, 2004 (a)], which also describes how mutexes are implemented using futexes.
   [Franke et al., 2002] is a (now outdated) paper written by the developers of futexes, which
   describes the early futex implementation and looks at the performance gains derived from futexes.


# ============================================================================
#{
={============================================================================
*kt_linux_core_200* linux-sync-semaphore

{what-is-semaphore}
note: This can be used for process or thread but it is expensive for thread.

System V semaphores are not used to transfer data between processes. Instead,
       they allow processes to synchronize their actions. A semaphore is a
       'kernel'-maintained 'integer' whose value is never permitted to fall
       below 0. A process can decrease or increase the value of a semaphore.
       If an attempt is made to decrease the value of the semaphore below 0,
       then the kernel blocks the operation until the semaphore's value
       increases to a level that permits the operation to be performed. 

The meaning of a semaphore is determined by the application. A process
decrements a semaphore from say, 1 to 0 in order to reserve exclusive access
to some 'shared' resource, and after completing work on the resource,
   increments the semaphore so that the shared resource is released for use by
   some other process. The use of a binary semaphore-a semaphore whose value
   is limited to 0 or 1-is common.

However, an application that deals with multiple instances of a shared
resource would employ a semaphore whose maximum value equals the 'number' of
shared resources. Linux provides both System V semaphores and POSIX
semaphores, which have essentially similar functionality.

1. Setting the semaphore to an absolute value;
2. Adding a number to the current value of the semaphore;            // give
3. Subtracting a number from the current value of the semaphore; and // take
4. Waiting for the semaphore value to be equal to 0.

The last two of these operations may cause the calling process to block. A
semaphore has no meaning in and of itself. Its meaning is determined only by
the associations given to it by the processes using the semaphore.

However, System V semaphores are rendered unusually complex by the fact that
they are allocated in groups called semaphore sets. So move to POSIX
semaphore.


{posix-semaphore}
SUSv3 specifies two types of POSIX semaphores:

1. Named semaphores: This type of semaphore has a name. By calling sem_open()
with the same name 'unrelated' processes can access the same semaphore.

2. Unnamed semaphores: This type of semaphore doesn't have a name; instead, it
resides at an agreed-upon location in 'memory'. Unnamed semaphores can be
shared between processes or between a group of 'threads'. When shared between
processes, the semaphore must reside in a region of (System V, POSIX, or
    mmap()) shared memory. When shared between threads, the semaphore may
reside in an area of memory shared by the threads (e.g., on the heap or in a
    global variable).

POSIX semaphores operate in a manner similar to System V semaphores; that is,
      a POSIX semaphore is an integer whose value is not permitted to fall
      below 0. If a process attempts to decrease the value of a semaphore
      below 0(take), then, depending on the function used, the call either
      blocks or fails with an error indicating that the operation was not
      currently possible.

Some systems don't provide a full implementation of POSIX semaphores. A
typical restriction is that only unnamed thread-shared semaphores are
supported. That was the situation on Linux 2.4; 

<linux-2-6-support>
With Linux 2.6 and a glibc that provides NPTL, a full implementation of POSIX
semaphores is available.

<named-semaphore>
To work with a named semaphore, we employ the following functions:

1. The sem_open() function opens or creates a semaphore, initializes the semaphore if it is created
by the call, and returns a handle for use in later calls.

#include <fcntl.h> /* Defines O_* constants */
#include <sys/stat.h> /* Defines mode constants */
#include <semaphore.h>

sem_t *sem_open(const char *name, int oflag, ...  /* mode_t mode, unsigned int value */ );

Returns pointer to semaphore on success, or SEM_FAILED on error

2. The sem_post(sem) and sem_wait(sem) functions respectively increment and decrement a semaphore's
value. give and take

3. The sem_getvalue() function retrieves a semaphore's current value.

4. The sem_close() function removes the calling processâs association with a semaphore that it
previously opened.

5. The sem_unlink() function removes a semaphore name and marks the semaphore for deletion when all
processes have closed it.

<named-semaphore-on-linux>
SUSv3 doesn't specify how named semaphores are to be implemented. On Linux, they are created as
small POSIX shared memory objects with names of the form sem.name, in a dedicated tmpfs file system
(Section 14.10) mounted under the directory /dev/shm. This file system has 'kernel'-persistence-the
semaphore objects that it contains will persist, even if no process currently has them open, but
they will be lost if the system is shut down.

<unnamed-semaphore>
The semaphore is made available to the processes or threads that use it by placing it in an area of
memory that they share. Operations on unnamed semaphores use the same functions; sem_wait(),
sem_post(), sem_getvalue(), and so on that are used to operate on named semaphores.

In addition, two further functions are required:

The sem_init() function initializes a semaphore and informs the system of whether the semaphore will
be shared between processes or between the threads of a single process.

The sem_destroy(sem) function destroys a semaphore. These functions should not be used with named
semaphores.

<when-useful-to-use-unnamed>
1. A semaphore that is shared between 'threads' doesn't need a name. Making an
unnamed semaphore a shared (global or heap) variable automatically makes it
accessible to all threads.

2. A semaphore that is being shared between 'related' processes doesn't need a
name. If a parent process allocates an unnamed semaphore in a region of shared
memory (e.g., a shared anonymous mapping), then a child automatically inherits
the mapping and thus the semaphore as part of the operation of fork().

3. If we are building a dynamic data structure (e.g., a binary tree), each of
whose items requires an associated semaphore, then the simplest approach is to
allocate an unnamed semaphore within each item. Opening a named semaphore for
each item would require us to design a convention for generating a (unique)
  semaphore name for each item and to manage those names (e.g., unlinking them
      when they are no longer required). note: real example? useful?


={============================================================================
*kt_linux_core_201* linux-sync-mutex

LPI-30 Thread Synchronization

Mutexes allow threads to synchronize their use of a shared resource, so that,
for example, one thread doesn’t try to access a shared variable at the same
  time as another thread is modifying it. Condition variables perform a
    `complementary task`: they allow threads to inform each other that a
    shared variable (or other shared resource) has changed state.

The term critical section is used to refer to a section of code that accesses
a shared resource and whose execution should be `atomic`; that is, its
execution should not be interrupted by another thread that simultaneously
accesses the same shared resource.

In the below problem, two threads shares the same code. It is about critial
region but what is really protected is the 'data' being manipulated within the
critical region.

note:
From 2.6, NPTL(New Posix Threading Library) that supports futex(fast user
    space mutex) and is part of glibc.


<problem>
The kind of problems that can occur when shared resources are not accessed
atomically.

This program creates two threads, each of which executes the same function. The
function executes a loop that repeatedly increments a global variable, glob, by
copying glob into the local variable loc.


Since loc is an automatic variable allocated on the per-thread stack, each
thread has its 'own' copy of this variable.

The vagaries of the kernel's CPU scheduling decisions:
The scheduler time slice for thread 1 expires, and thread 2 commences execution.

In complex programs, this `nondeterministic` behavior means that such errors may
occur only rarely, be hard to reproduce, and therefore be difficult to find.


static int glob = 0;

// Loop 'arg' times incrementing 'glob'
static void * threadFunc(void *arg)
{
  int loops = *((int *) arg);
  int loc, j;
  for (j = 0; j < loops; j++) {
    loc = glob;
    loc++;
    glob = loc;
  }
  return NULL;
}

} // namespace

TEST(Async, RaceCondition)
{
  using namespace lpi_30_1;

  pthread_t t1, t2;

  int loops, s;

  loops = 10000000; // 10M

  s = pthread_create(&t1, NULL, threadFunc, &loops);
  EXPECT_EQ(s, 0);

  s = pthread_create(&t2, NULL, threadFunc, &loops);
  EXPECT_EQ(s, 0);

  s = pthread_join(t1, NULL);
  EXPECT_EQ(s, 0);

  s = pthread_join(t2, NULL);
  EXPECT_EQ(s, 0);

  // expects 20M if there's no race
  cout << "glob = " << glob << endl;
}


It might seem that we could eliminate the problem by replacing the three
statements inside the for loop with a single statement. Looks atomic?

staic void threadFunc()
{
  int loc, j;

  for(j=0; j < loops; j++)
  {
    glob++; /* or: ++glob; */
  }
}

However, on many hardware architectures (e.g., RISC architectures), the
compiler would still need to convert this single statement into machine code
whose steps are equivalent to the three statements inside the loop. In other
words, despite its simple appearance, even a C increment operator may 'not' be
atomic.


{mutex}
More generally, mutexes can be used to ensure atomic access to any shared
resource, but protecting shared variables is the most common use.

<mutex-ownership>
A mutex has two states: locked and unlocked. At any moment, *at-most* one
thread may hold the lock on a mutex. Attempting to lock a mutex that is
already locked either blocks or fails with an error depending on the method
used to place the lock.

When a thread locks a mutex, it becomes the 'owner' of that mutex. 'only' the
mutex owner can unlock the mutex. Because of this ownership property, the
terms 'acquire' and 'release' are sometimes used synonymously for lock and
unlock.


<mutex-protocol>
Each thread employs the following protocol for accessing a resource:
1. lock the mutex for the shared resource;
2. access the shared resource; and
3. unlock the mutex.


<mutex-cooperative-lock>
This means that mutex locking is advisory, rather than mandatory; nothing can
prevent one thread from manipulating the data without first obtaining the
mutex. For example, threads not participating a mutex circle can.


<mutex-allocation>
statically allocated mutexes

A mutex can either be allocated as a static variable or be created dynamically
at run time. Before it can be used, a mutex must always be initialized.

pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;

dynamically initializing mutex

The static initializer value PTHREAD_MUTEX_INITIALIZER can be used only for
initializing a statically allocated mutex with default attributes. In all
other cases, we must dynamically initialize the mutex using
pthread_mutex_init().

// note:
// can use ptherd_mutex_init() for statically allocated mutex. see the same in
// real code
// https://www.ibm.com/support/knowledgecenter/en/ssw_i5_54/apis/users_61.htm

pthread_mutex_t       mutex2;
pthread_mutex_t       mutex3;
pthread_mutexattr_t   mta;
pthread_mutexattr_init(&mta);

The following three mutex initialization mechanisms have equivalent function.

pthread_mutex_t       mutex1 = PTHREAD_MUTEX_INITIALIZER;

pthread_mutex_init(&mutex2, NULL);
pthread_mutex_init(&mutex3, &mta);

All three mutexes are created with the default mutex attributes.

// from man page
In cases where default mutex attributes are appropriate, the macro
PTHREAD_MUTEX_INITIALIZER can be used to initialize mutexes that are
statically allocated. The effect shall  be  equivalent  to dynamic
initialization by a call to pthread_mutex_init() with parameter attr specified
as NULL, except that no error checks are performed.

// uclibc@libpthread/linuxthreads/sysdeps/pthread/pthread.h

enum
{
  PTHREAD_MUTEX_TIMED_NP,
  PTHREAD_MUTEX_RECURSIVE_NP,
  PTHREAD_MUTEX_ERRORCHECK_NP,
  PTHREAD_MUTEX_ADAPTIVE_NP
#ifdef __USE_UNIX98
  ,
  `PTHREAD_MUTEX_NORMAL = PTHREAD_MUTEX_TIMED_NP,`
  PTHREAD_MUTEX_RECURSIVE = PTHREAD_MUTEX_RECURSIVE_NP,
  PTHREAD_MUTEX_ERRORCHECK = PTHREAD_MUTEX_ERRORCHECK_NP,
  PTHREAD_MUTEX_DEFAULT = PTHREAD_MUTEX_NORMAL
#endif
#ifdef __USE_GNU
  /* For compatibility.  */
  , PTHREAD_MUTEX_FAST_NP = PTHREAD_MUTEX_ADAPTIVE_NP
#endif
};

#define PTHREAD_MUTEX_INITIALIZER \
  {0, 0, 0, PTHREAD_MUTEX_TIMED_NP, __LOCK_INITIALIZER}

/* Mutexes (not abstract because of PTHREAD_MUTEX_INITIALIZER).  */
/* (The layout is unnatural to maintain binary compatibility
    with earlier releases of LinuxThreads.) */
typedef struct
{
  int __m_reserved;               /* Reserved for future use */
  int __m_count;                  /* Depth of recursive locking */
  _pthread_descr __m_owner;       /* Owner thread (if recursive or errcheck) */
  int __m_kind;                   /* Mutex kind: fast, recursive or errcheck */
  struct _pthread_fastlock __m_lock; /* Underlying fast lock */
} pthread_mutex_t;


<mutex-lockin>
30.1.2 Locking and Unlocking a Mutex

After initialization, a mutex is unlocked.

#include <pthread.h>

int pthread_mutex_lock(pthread_mutex_t *mutex);
int pthread_mutex_unlock(pthread_mutex_t *mutex);

Both return 0 on success, or a positive error number on error


<mutex-lock-block>
If the mutex is currently locked by another thread, then pthread_mutex_lock()
  `blocks until` the mutex is unlocked, at which point it locks the mutex and
  returns.

If more than one other thread is waiting to acquire the mutex unlocked by a
call to pthread_mutex_unlock(), it is `indeterminate` which thread will
succeed in acquiring it.


<mutex-type>
30.1.7 Mutex Types

  * A single thread may not lock the same mutex twice.
  
  * A thread may not unlock a mutex that it doesn’t currently own or locked

Precisely what happens in each of these cases depends on the type of the
mutex.

note: ownership matters when unlock and semaphore do not
have ownership. 


PTHREAD_MUTEX_NORMAL *mutex-self-lock* *same-thread*

(Self-)deadlock detection is not provided for this type of mutex. If a thread
tries to lock a mutex that it has already locked, then 'deadlock' results. On
Linux, the thread deadlocks by default.

Unlocking a mutex that is not locked or that is locked by another thread
produces 'undefined' results. 

note: Q: What does this mean?
On Linux, both of these operations succeed for this mutex type and a
PTHREAD_MUTEX_DEFAULT mutex behaves like a PTHREAD_MUTEX_NORMAL mutex.


PTHREAD_MUTEX_ERRORCHECK

All three of the above scenarios cause the relevant Pthreads function to
return an error than blocking or deadlock but 'slower' than a normal so
debugging purpose.


PTHREAD_MUTEX_RECURSIVE

A recursive mutex maintains the concept of a *lock-count* When a thread first
acquires the mutex, the lock count is set to 1. Each subsequent lock operation
by the same thread increments the lock count, and each unlock operation
decrements the count. The mutex is released (i.e., made available for other
    threads to acquire) `only when the lock count falls to 0` 

When is it useful?

Still have ownership notion but there is no mutex deadlock or undefined result
as _NORMAL type has.


<mutex-locking-variants>
Two variants of the pthread_mutex_lock() function which are much less
frequently used:

The pthread_mutex_trylock() function is the same as pthread_mutex_lock(),
    except that if the mutex is currently locked, pthread_mutex_trylock()
  fails, returning the error EBUSY.

The pthread_mutex_timedlock() function is the same as pthread_mutex_lock(),
    except that the caller can specify an additional argument, abstime, that
    places a limit on the time that the thread will sleep while waiting to
    acquire the mutex. If the time interval specified by its abstime argument
    expires without the caller becoming the owner of the mutex,
    pthread_mutex_timedlock() returns the error ETIMEDOUT.

note: Why less used?

In most well-designed applications, a thread should hold a mutex for only a
short time, so that other threads are not prevented from executing in
parallel. 

A thread that uses pthread_mutex_trylock() to periodically poll the mutex to
see if it can be locked risks being starved of access to the mutex while other
queued threads are successively granted to the mutex via pthread_mutex_lock().


<mutex-glib-code>
// glib-2.40.0/gthread-posix.c
//
// The glib 2.40 changes unlock-not-locked which aborts from glib 2.0 which
// don't have check.

#if !defined(USE_NATIVE_MUTEX)

/**
 * g_mutex_unlock:
 * @mutex: a #GMutex
 *
 * Unlocks @mutex. If another thread is blocked in a g_mutex_lock()
 * call for @mutex, it will become unblocked and can lock @mutex itself.
 *
 * Calling g_mutex_unlock() on a mutex that is not locked by the
 * current thread leads to undefined behaviour.
 */
void
g_mutex_unlock (GMutex *mutex)
{
  gint status;

  if G_UNLIKELY ((status = pthread_mutex_unlock (g_mutex_get_impl (mutex))) != 0)
    g_thread_abort (status, "pthread_mutex_unlock");
}

#endif /* !defined(USE_NATIVE_MUTEX) */


#if defined(USE_NATIVE_MUTEX)

/* Our strategy for the mutex is pretty simple:
 *
 *  0: not in use
 *
 *  1: acquired by one thread only, no contention
 *
 *  > 1: contended
 *
 *
 * As such, attempting to acquire the lock should involve an increment.
 * If we find that the previous value was 0 then we can return
 * immediately.
 *
 * On unlock, we always store 0 to indicate that the lock is available.
 * If the value there was 1 before then we didn't have contention and
 * can return immediately.  If the value was something other than 1 then
 * we have the contended case and need to wake a waiter.
 *
 * If it was not 0 then there is another thread holding it and we must
 * wait.  We must always ensure that we mark a value >1 while we are
 * waiting in order to instruct the holder to do a wake operation on
 * unlock.
 */

void
g_mutex_unlock (GMutex *mutex)
{
  guint prev;

  prev = exchange_release (&mutex->i[0], 0);

  /* 1-> 0 and we're done.  Anything else and we need to signal... */
  if G_UNLIKELY (prev != 1)
    g_mutex_unlock_slowpath (mutex, prev);
}

static void __attribute__((noinline))
g_mutex_unlock_slowpath (GMutex *mutex,
                         guint   prev)
{
  /* We seem to get better code for the uncontended case by splitting
   * this out...
   */
  if G_UNLIKELY (prev == 0)
    {
      fprintf (stderr, "Attempt to unlock mutex that was not locked\n");
      abort ();
    }

  syscall (__NR_futex, &mutex->i[0], (gsize) FUTEX_WAKE_PRIVATE, (gsize) 1, NULL);
}

#endif

// callstack when aborted

#0  0x00007ffff5692107 in __GI_raise (sig=sig@entry=6)
    at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff56934e8 in __GI_abort () at abort.c:89
#2  0x00007ffff739bcea in g_mutex_unlock_slowpath ()
   from /release/debian-8-x86_64/oss/lib/libglib-2.0.so.0
#3  0x00007ffff739bdb6 in g_mutex_unlock ()
   from /release/debian-8-x86_64/oss/lib/libglib-2.0.so.0
#4  0x00007fffd8a94c3f in gst_hls_demux_switch_playlist ()
   from /release/debian-8-x86_64/oss/lib/gstreamer-1.0/libgsthlsdemux.so


<mutex-performance>
This is relatively cheap and the performance impact of using a mutex is not
significant in most applications.

The problem with file locks and semaphores is that they always require a
system call for the lock and unlock operations, and each system call has a
small, but appreciable, cost (Section 3.1). By contrast, mutexes are
implemented using atomic machine-language operations (performed on memory
    locations visible to all threads) and require system calls only in case of
lock contention.

On Linux, mutexes are implemented using futexes (derived from fast user space
    mutexes), and lock contentions are dealt with using the futex() system
call.


={============================================================================
*kt_linux_core_202* linux-sync-deadlock

30.1.4 Mutex Deadlocks

A deadlock is a situation where more than one thread is locking the same set
of mutexes and will remain blocked indefinitely. Waiting for something never
happens. Unrequited love?

Thread A                            Thread B
1. pthread_mutex_lock(mutex1);      1. pthread_mutex_lock(mutex2);
2. pthread_mutex_lock(mutex2);      2. pthread_mutex_lock(mutex1);
blocks                              blocks

The simplest deadlock condition is when there are two threads and thread A
can't progress until thread B finishes, while thread B can't progress until
thread A finishes. This is usually because both need the same two resources to
progress, A has one and B has the other. Various symmetry breaking algorithms
can prevent this in the two thread or larger circle cases.


<deadlock-examples>
<1> from pipe
If employing this bidirectional communication using two pipes, then we need to
be wary of deadlocks that may occur if both processes block while trying to
read from empty pipes or while trying to write to pipes that are already full.

<2> from fifo.
use-different-fifo-for-read-and-write

Process X                              Process Y
1. Open FIFO A for reading(blocks)     1. Open FIFO A for reading(blocks)
2. Open FIFO B for writing             2. Open FIFO B for writing

When open a fifo to read which is not opend to write but there is no writing
process then reading process is blocked. Therefore, if parent and child opens
different fifos to read, deadlock happens.

use-same-fifo-for-read-and-write-but-the-same-order

Process X                              Process Y
1. Open FIFO A for reading(blocks)     1. Open FIFO B for reading(blocks)
2. Open FIFO B for writing             2. Open FIFO A for writing

The two processes shown in Figure 44-8 are deadlocked. Each process is blocked
waiting to open a FIFO for reading. This blocking would not happen if each
process could perform its second step (opening the other FIFO for writing). So
change order or use non-blocking call.

<3>
Message queues have a limited capacity. This has the potential to cause a
couple of problems. One of these is that multiple simultaneous clients could
fill the message queue, resulting in a deadlock situation, where no new client
requests can be submitted and the server is blocked from writing any
responses.

<4> from mutex-deadlock
Locking. If try to lock what is already locked by self, two may happen for
'default' type of mutex: mutex-deadlock or EDEADLK. On Linux, deadlock by
default from {ref-LPI}. Why deadlock? Because blocked trying to lock a mutex
that it already owns. This could happen when exception is raised between lock
and unlock pair.

<5> from lock-free-queue
Used the timed_wait() instead of the simpler wait() to solve a possible
deadlock when Produce() is called between line A and line B in Listing One.
Then wait() will miss the notify_one() call and have to wait for the next
produced element to wake up. 'if' this element never comes (no more produced
    elements or if the Produce() call actually waits for Consume() to return),
         there's a deadlock.

<6> mutex-deadlock

Thead A             Thread B
lock(mutex1)        lock(mutex2)
lock(mutex1)        lock(mutex2)


<avoid-deadlock>
* Mutex hierarchy or order

The simplest way to avoid such deadlocks is to define a mutex hierarchy. When
threads can lock the same set of mutexes, they should always lock them in the
same order. Less flexible.

* Try and back off

An alternative strategy that is 'less' frequently used is "try, and then back
off." See above on why try_lock is less used. If try_lock fails, release 'all'
and try again later. This is less efficient than hierarchy approach but can be
more 'flexible' since no need rigid hierarchy.


<race>
Races happen when one thread changes the state of some resource when another
thread is not expecting it (such as changing the contents of a memory location
    when another thread is part way through reading, or writing to that
    memory). Locking methods are the key here. (Some lock free methods and
      containers are also good choices for this. As are atomic operations, or
      transaction based operations.)


<starvation>
Starvation happens when a thread needs a resource to proceed, but can't get
it. The resource is constantly tied up by other threads and the one that needs
it can't get in. The scheduling algorithm is the problem when this happens.
Look at algorithms that assure access.


={============================================================================
*kt_linux_core_202* linux-sync-cond

LPI 30.2.2 Signaling and Waiting on Condition Variables

int pthread_cond_signal( pthread_cond_t *cptr );
int pthread_cond_broadcast( pthread_cond_t *cptr );
int pthread_cond_timedwait( ... );

<cond-signal-more-efficient>
The difference between pthread_cond_signal() and pthread_cond_broadcast() lies
in what happens if multiple threads are blocked in pthread_cond_wait(). With
pthread_cond_signal(), we are simply guaranteed that at least one of the
blocked threads is woken up; with pthread_cond_broadcast(), all blocked
threads are woken up.

Using pthread_cond_broadcast() always yields correct results (since all
    threads should be programmed to handle redundant and spurious wake-ups),
      but pthread_cond_signal() can be more efficient. 

However, pthread_cond_signal() should be used only if `just one` of the
waiting threads needs to be woken up to handle the change in state of the
shared variable, and it `doesn't matter` which one of the waiting threads is
woken up. This scenario typically applies when all of the waiting threads are
designed to perform the exactly `same task.` 

Given these assumptions, pthread_cond_signal() can be more efficient than
pthread_cond_broadcast(), because it `avoids` the following possibility:

* All waiting threads are awoken.

* One thread is scheduled first. This thread checks the state of the shared
variable(s) (under protection of the associated mutex) and sees that there is
work to be done. The thread performs the required work, changes the state of
the shared variable(s) to indicate that the work has been done, and unlocks
the associated mutex.

* Each of the remaining threads in turn locks the mutex and tests the state
of the shared variable. However, because of the change made by the first
thread, these threads see that there is no work to be done, and so unlock the
mutex and go back to sleep (i.e., call pthread_cond_wait() once more).

By contrast, pthread_cond_broadcast() handles the case where the waiting
threads are designed to perform `different tasks` (in which case they probably
    have different predicates associated with the condition variable).


<cond-wait>
The pthread_cond_wait() function blocks a thread until the condition variable
cond is signaled.

ALWAYS has an associated because both producer and consumer accesses to the
shared state variable which is linked to that condition so should be synced.
In other words, there is a natural association of a mutex with a condition
varaible.

int `pthread_cond_wait`( pthread_cond_t *cptr, pthread_mutex_t* mptr);
pthread_cond_wait( &nready.cond, &nready.mutex );

Do `three things`: unlock a mutex, block a calling thread until signaled, and
  relock mutex when signaled. Releasing the mutex and blocking on the
  condition variable are performed `atomically`.


<cond-timedwait>
pthread_cond_timedwait() function is the same as pthread_cond_wait(), except
that the `abstime` argument specifies `an upper limit on the time` that the
thread will sleep while waiting for the condition variable to be signaled.

int pthread_cond_timedwait(pthread_cond_t *cond, pthread_mutex_t *mutex,
  const struct timespec *abstime);

Returns 0 on success, or a positive error number on error

The abstime argument is a timespec structure specifying an absolute time
expressed as seconds and nanoseconds since the Epoch. 

If the time interval specified by abstime expires `without` the condition
variable being signaled, then pthread_cond_timedwait() returns the error
ETIMEDOUT.


={============================================================================
*kt_linux_core_202* linux-sync-cond-lost-wake-up-issue

<unp-example>
* Use two mutex; one between producers and the other between producers and
  consumer to use a condition. Not sure how much benefit can get from this.

* Producers and consumer produce or consume as many items as it could as long
  as ther are items to use. consumer wait and producer signals ONLY when there
  is no item

* *signal-lost* Only when there is no more items, consumer and producers
  engages in condition. There is a consumer which waits first.

<lpi-example>
* Use one mutex for many producers and single consumer.
 
* Producers send a signal when produce each item and single consumer reads as
  many items as items are available.

* *signal-lost* Can lost some but not a problem since consumer consumes as
  many as it could when signaled. There is a consumer which waits first.

For both, although some signal can be lost, there is a thread which waits
first and gets signaled.


<case>
Application calls async APIs in a short space of time and which uses mutex and
condition variable to make it sync call fashion to other component.

typedef struct
{
  // predicate
  bool                   response_received;
  SYSTEMUTIL_THR_Mutex_t mutex;
  SYSTEMUTIL_SYNC_Cond_t cond;

} GUIDE_AsyncCallbackStruct;

// global
static GUIDE_AsyncCallbackStruct f_planner_window_cond;

// async call
SYSTEM_STATUS GUIDE_API_ProgrammeInstance_GetBookingProperties()
{
  /* In order to make this interface reentrant create a new request with each
   * querry of planner for the current programme instance. When this API is
   * called simultaneously for an identical programme instance, each querry to
   * planner shall work with the associated request and does not impact each
   * other. 
   */

  // note: lock and clear predicate
  GuidePrepareForAsyncCallback(&f_planner_window_cond)
  { 
    SYSTEMUTIL_THR_MutexLock(&asyncCallbackStruct->mutex); 
    asyncCallbackStruct->response_received = false;
  }

  // register callback
  GUIDE_Int_PLANNER_API_Session_RetrieveProgInstBookingPropsInTimeWindow
    (l_planner_session, GuideProgrammeInstanceBookingPropsListener);

  // wait on condition
  GuideWaitForAsyncCallback(&f_planner_window_cond);
  {
    // note:
    // uses `predicate` which is part of global structure

    while ((asyncCallbackStruct->response_received == false) 
        && (sys_status == SYSTEMUTIL_STATUS_OK))
    {
      sys_status = SYSTEMUTIL_SYNC_CondTimedWait(&asyncCallbackStruct->cond, 
          &asyncCallbackStruct->mutex, &timeout_value);
    }

    SYSTEMUTIL_THR_MutexUnlock(&asyncCallbackStruct->mutex));
  }
}

// callback
static void GuideProgrammeInstanceBookingPropsListener(PLANNER_API_UserTag user_tag)
{
  `GuideSignalCallbackReceived`(&f_planner_window_cond)
  {
    /* Lock iterator_callback_mutex */
    if (SYSTEMUTIL_THR_MutexLock(&asyncCallbackStruct->mutex))
    {
      asyncCallbackStruct->response_received = true;      // set predicate

      /* Signal the condition variable */

      // use `pthread_cond_signal`(&cond->cond)); compared to
      // SYSTEM_STATUS SYSTEMUTIL_SYNC_CondBroadcast(SYSTEMUTIL_SYNC_Cond_t* cond);

      if (SYSTEMUTIL_STATUS_OK!=SYSTEMUTIL_SYNC_CondSignal(&asyncCallbackStruct->cond));

      /* Unlock iterator_callback_mutex */
      if (SYSTEMUTIL_STATUS_OK != SYSTEMUTIL_THR_MutexUnlock(&asyncCallbackStruct->mutex));
    }
  }
}


Looks fine on the face of it. However, the issue is that timeout happens when
calls this function multiple times in a very short time space. 

The log shows:

// first API call (thread #01)
^[2018:02:02 07:32:42]0946740080.938130 
!ENTER  -G_Sess       		< F:GUIDE_API_ProgrammeInstance_GetBookingProperties L:2016 > |-> GUIDE_API_ProgrammeInstance_GetBookingProperties

// wait on condition
^[2018:02:02 07:32:42]0946740080.938333
!ENTER  -G_Sess       		< F:GuidePrepareForAsyncCallback L:2330 > |-> GuidePrepareForAsyncCallback
^[2018:02:02 07:32:42]0946740080.938504
!ENTER  -G_Sess       		< F:GuideWaitForAsyncCallback L:2385 > |-> GuideWaitForAsyncCallback


// second API call (thread #02)
^[2018:02:02 07:32:42]0946740080.950809
!ENTER  -G_Sess       		< F:GUIDE_API_ProgrammeInstance_GetBookingProperties L:2016 > |-> GUIDE_API_ProgrammeInstance_GetBookingProperties

// wait on condition
!ENTER  -G_Sess       		< F:GuidePrepareForAsyncCallback L:2330 > |-> GuidePrepareForAsyncCallback
`^[2018:02:02 07:32:42]0946740080.951211`
!ENTER  -G_Sess       		< F:GuideWaitForAsyncCallback L:2385 > |-> GuideWaitForAsyncCallback


// first callback and signaled, asyncCallbackStruct->response_received == true
^[2018:02:02 07:32:43]0946740081.090678 !ENTER  -PLANNER      		< F:PLANNER_BookingStatusHandler L:1515 > |-> PLANNER_BookingStatusHandler
^[2018:02:02 07:32:43]0946740081.138914 !ENTER  -G_Sess       		< F:GuideSignalCallbackReceived L:2426 > |-> GuideSignalCallbackReceived
^[2018:02:02 07:32:43]0946740081.138939 !EXIT   -G_Sess       		< F:GuideSignalCallbackReceived L:2450 > <-| GuideSignalCallbackReceived : return_status = 0x0
^[2018:02:02 07:32:43]0946740081.139015 !EXIT   -PLANNER      		< F:PLANNER_BookingStatusHandler L:1574 > <-| PLANNER_BookingStatusHandler : return_status = 0x6402000


== Here lost the second signal when signaled while handling the first and before the second waits on.
==
== // second callback and signaled, asyncCallbackStruct->response_received == true
== ^[2018:02:02 07:32:43]0946740081.139066 !ENTER  -PLANNER      		< F:PLANNER_BookingStatusHandler L:1515 > |-> PLANNER_BookingStatusHandler
== 
== // gets condition from the first, change predicate, release lock. runs after the condifion.
== ^[2018:02:02 07:32:43]0946740081.139336 !EXIT
== !EXIT   -G_Sess       		< F:GuideWaitForAsyncCallback L:2410 > <-| `GuideWaitForAsyncCallback` : return_status = 0x3c00000
== 
== // second callback contines
== ^[2018:02:02 07:32:43]0946740081.171939 !ENTER  -G_Sess       		< F:GuideSignalCallbackReceived L:2426 > |-> GuideSignalCallbackReceived
== ^[2018:02:02 07:32:43]0946740081.171962 !EXIT   -G_Sess       		< F:GuideSignalCallbackReceived L:2450 > <-| GuideSignalCallbackReceived : return_status = 0x0
== ^[2018:02:02 07:32:43]0946740081.172035 !EXIT   -PLANNER      		< F:PLANNER_BookingStatusHandler L:1574 > <-| PLANNER_BookingStatusHandler : return_status = 0x6402000


// exit API call
!EXIT   -G_Sess       		< F:GUIDE_API_ProgrammeInstance_GetBookingProperties L:2435 > 
  <-| GUIDE_API_ProgrammeInstance_GetBookingProperties : return_status = 0x3c00000


// third API call (thread #03)
^[2018:02:02 07:32:43]0946740081.172588 !ENTER  -G_Sess       		< F:GUIDE_API_ProgrammeInstance_GetBookingProperties L:2016 > |-> GUIDE_API_ProgrammeInstance_GetBookingProperties

// wait on condition
^[2018:02:02 07:32:43]0946740081.172956 !ENTER  -G_Sess       		< F:GuideWaitForAsyncCallback L:2385 > |-> GuideWaitForAsyncCallback


// third callback and signaled
^[2018:02:02 07:32:43]0946740081.177932 !ENTER  -PLANNER      		< F:PLANNER_BookingStatusHandler L:1515 > |-> PLANNER_BookingStatusHandler
^[2018:02:02 07:32:43]0946740081.232369 !ENTER  -G_Sess       		< F:GuideSignalCallbackReceived L:2426 > |-> GuideSignalCallbackReceived
^[2018:02:02 07:32:43]0946740081.232394 !EXIT   -G_Sess       		< F:GuideSignalCallbackReceived L:2450 > <-| GuideSignalCallbackReceived : return_status = 0x0
^[2018:02:02 07:32:43]0946740081.232455 !EXIT   -PLANNER      		< F:PLANNER_BookingStatusHandler L:1574 > <-| PLANNER_BookingStatusHandler : return_status = 0x6402000

// gets condition, change predicate, release lock. runs after the condifion.
== 406907:NDS: ^[2018:02:02 07:32:43]0946740081.232499 !EXIT   -G_Sess       		< p:0x00000211 P:APP t:0x2c148520 T:no name M:guide_api_common.c F:GuideWaitForAsyncCallback L:2410 > <-| GuideWaitForAsyncCallback : return_status = 0x3c00000

// exit API call
^[2018:02:02 07:32:43]0946740081.233073 !EXIT   -G_Sess       		< F:GUIDE_API_ProgrammeInstance_GetBookingProperties L:2435 > <-| GUIDE_API_ProgrammeInstance_GetBookingProperties : return_status = 0x3c00000


// 30 sec timeout happens
^[2018:02:02 07:33:12]0946740110.953873 !WARN   -G_Sess       		< F:GuideWaitForAsyncCallback L:2398 > SYSTEMUTIL_SYNC_CondTimedWait() FAILED sys_status=0x11c2100d 


{linux-sync-cond-signal-can-be-lost} wake-up-issue
A condition variable holds `no state information.` It is simply a mechanism
for communicating information about the application's state. 

`If no thread is waiting on the condition variable at the time` that it is
signaled, then the signal is lost. 

A thread that later waits on the condition variable will unblock only when the
variable is signaled once more.

Solution?

* Use broadcast?
* Use something to remember events such as queue?


// delivered fix which makes predicate an entry per session and this is not
// global. 
//
// What may happen here is that since predicate was global, when it's signed
// and waiting thread waked up, there is no gurantee that predicate is true
// and waiting thread waits again and timeout happens. This is the same effect
// as signal lost. Might say signal is swallowed. 

SYSTEM_STATUS GUIDE_API_ProgrammeInstance_GetBookingProperties()
{
  // wait on `predicate` which is not global and is an instance of each
  // session

  while (l_request_handle != 
      request_object_data->request.cached_window.proginst_list_request_handle)
  {
    if (SYSTEMUTIL_STATUS_OK != SYSTEMUTIL_SYNC_CondTimedWait
        (&f_planner_window_cond.cond, &f_planner_window_cond.mutex, &timeout_value))
    {
      DIAG_LOG_WARN(g_guide_diag_segment_id, ("SYSTEMUTIL_SYNC_CondTimedWait() FAILED"));
      l_status = GUIDE_API_STATUS_FAILED;
      break;
    }
  }
}

// callback
static void GuideProgrammeInstanceBookingPropsListener(PLANNER_API_UserTag user_tag)
{
  request_data->request.cached_window.proginst_list_request_handle = user_tag;
}


={============================================================================
*kt_linux_core_202* linux-sync-cond-lpi-example

30.2 Signaling Changes of State: Condition Variables

Have a number of threads that produce some “result units” that are consumed by
the main thread, and that we use a mutex-protected variable, avail, to
represent the number of produced units awaiting consumption

namespace cxx_condition
{
  void errExitEN(int errnum, const char *format, ...)
  {
    cout << errnum << ", " << format << endl;
  }
 
  // LPI 30c
  // \tlpi-181022-dist.tar\tlpi-dist\threads\prod_no_condvar.c
  // A simple POSIX threads producer-consumer example that doesn't use a condition
  // variable.

  static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;
  static pthread_cond_t cond = PTHREAD_COND_INITIALIZER;

  // *sync-cond-predicate* which is global
  static int avail = 0;

  // producer

  static void* threadFunc_no_cond_var(void *arg)
  {
    // okay
    // int cnt = *((int *)arg);

    int cnt = *static_cast<int*>(arg);

    int s, j;

    // each thread has given the number of unit to produce from main argv

    for (j = 0; j < cnt; j++) 
    {
      sleep(1);

      /* Code to produce a unit omitted */

      s = pthread_mutex_lock(&mtx);
      if (s != 0)
        errExitEN(s, "pthread_mutex_lock");

      // Let consumer know another unit is available
      avail++;

      s = pthread_mutex_unlock(&mtx);
      if (s != 0)
        errExitEN(s, "pthread_mutex_unlock");
    }

    return NULL;
  }

  static void* threadFunc_with_cond_var(void *arg)
  {
    // okay
    // int cnt = *((int *)arg);

    int cnt = *static_cast<int*>(arg);

    int s, j;

    // each thread has given the number of unit to produce from main argv

    for (j = 0; j < cnt; j++) 
    {
      sleep(1);

      /* Code to produce a unit omitted */

      s = pthread_mutex_lock(&mtx);
      if (s != 0)
        errExitEN(s, "pthread_mutex_lock");

      // Let consumer know another unit is available
      avail++;

      s = pthread_mutex_unlock(&mtx);
      if (s != 0)
        errExitEN(s, "pthread_mutex_unlock");

      // wake sleeping consumer
      s = pthread_cond_signal(&cond);
      if (s != 0)
        errExitEN(s, "pthread_cond_unlock");

    }

    return NULL;
  }

} // namespace


// multiple producers and single consumer. no condition and all participates in
// the contention. 
//
// this wastes CPU time, because all, consumer and producer continually loops,
// checking the state of the variable avail.
//
// A condition variable remedies this and allows a thread to sleep (wait) until
// another thread notifies (signals) it that it must do something. 

TEST(Condition, ProducerAndConsumer_LPI_No_CondVar)
{
  using namespace cxx_condition;

  int const NUM_OF_PRODUCERS = 10;
  int const NUM_OF_ITEMS_TO_PRODUCE = 100;

  pthread_t tid;
  int s, j;
  int totRequired;            /* Total number of units that all
                                 threads will produce */
  int numConsumed;            /* Total units so far consumed */
  bool done;
  time_t t;

  t = time(NULL);

  // Create all producers

  totRequired = 0;

  // for (j = 1; j < argc; j++) {
  //   totRequired += atoi(argv[j]);

  //   s = pthread_create(&tid, NULL, threadFunc, argv[j]);
  //   if (s != 0)
  //     errExitEN(s, "pthread_create");
  // }

  for (j = 1; j < NUM_OF_PRODUCERS; j++) {
    totRequired += NUM_OF_ITEMS_TO_PRODUCE;

    s = pthread_create(&tid, NULL, threadFunc_no_cond_var, (void *)&NUM_OF_ITEMS_TO_PRODUCE);
    if (s != 0)
      errExitEN(s, "pthread_create");
  }

  // single consumer
  // Use a polling loop to check for available units

  numConsumed = 0;
  done = false;

  for (;;) {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_lock");

    // Consume all available units
    while (avail > 0) {

      // Do something with produced unit

      numConsumed++;
      avail--;

      // printf("T=%ld: numConsumed=%d\n", (long) (time(NULL) - t),
      //     numConsumed);

      done = numConsumed >= totRequired;
    }

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_unlock");

    if (done)
      break;

    // Perhaps do other work here that does not require mutex lock
  } // for end

  EXPECT_THAT(numConsumed, 900);
}

TEST(Condition, ProducerAndConsumer_LPI_CondVar)
{
  using namespace cxx_condition;

  int const NUM_OF_PRODUCERS = 10;
  int const NUM_OF_ITEMS_TO_PRODUCE = 100;

  pthread_t tid;
  int s, j;
  int totRequired;            /* Total number of units that all
                                 threads will produce */
  int numConsumed;            /* Total units so far consumed */
  bool done;
  time_t t;

  t = time(NULL);

  // Create all producers

  totRequired = 0;

  // for (j = 1; j < argc; j++) {
  //   totRequired += atoi(argv[j]);
  //
  //   s = pthread_create(&tid, NULL, threadFunc, argv[j]);
  //   if (s != 0)
  //     errExitEN(s, "pthread_create");
  // }

  for (j = 1; j < NUM_OF_PRODUCERS; j++) {
    totRequired += NUM_OF_ITEMS_TO_PRODUCE;

    s = pthread_create(&tid, NULL, threadFunc_with_cond_var, (void *)&NUM_OF_ITEMS_TO_PRODUCE);
    if (s != 0)
      errExitEN(s, "pthread_create");
  }

  // single consumer
  // Use a condition variable to check for available units

  numConsumed = 0;
  done = false;

  for (;;) {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_lock");

    // *linux-sync-cond-spurious-wakeup*
    // the pthread_cond_wait() call is placed within a while loop rather than an
    // if statement

    while (avail == 0)
    {
      s = pthread_cond_wait(&cond, &mtx);
      if (s != 0)
        errExitEN(s, "pthread_cond_wait");
    }

    // at this point, `mtx` is locked
    
    // Consume all available units
    while (avail > 0) {

      // Do something with produced unit

      numConsumed++;
      avail--;

      // printf("T=%ld: numConsumed=%d\n", (long) (time(NULL) - t),
      //     numConsumed);

      done = numConsumed >= totRequired;
    }

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_unlock");

    if (done)
      break;

    // Perhaps do other work here that does not require mutex lock
  } // for end

  EXPECT_THAT(numConsumed, 900);
}

[ RUN      ] Condition.ProducerAndConsumer_LPI_No_CondVar
[       OK ] Condition.ProducerAndConsumer_LPI_No_CondVar (100099 ms)
[ RUN      ] Condition.ProducerAndConsumer_LPI_CondVar
[       OK ] Condition.ProducerAndConsumer_LPI_CondVar (100036 ms)


={============================================================================
*kt_linux_core_202* linux-sync-cond-unp-example

UNP-XX, Consumer and Producer

<ex-01> 
// Multiple producer(write) and one consumer(read). No sync between producer
// and consumer and only sync between producers because consumer starts only
// when writes finishes.

#include <stdio.h>
#include <pthread.h>
#include <sys/errno.h>

#define MAXNITEMS     1000000
#define MAXNTHREADS   100

#define min(a,b) ((a) < (b) ? (a) : (b))
#define max(a,b) ((a) > (b) ? (a) : (b))

void 
Pthread_create
(pthread_t* tid, const pthread_attr_t* attr, void *(*func)(void*), void*arg)
{
  int n;

  if(( n = pthread_create( tid, attr, func, arg )) == 0 )
    return;

  errno = n;
  fprintf( stderr, "pthread_create error(%d)", n );
}

// shared by all threads. once set, not changed.
int nitems;

struct {
  pthread_mutex_t mutex;
  int buff[MAXNITEMS];
  int nput;                     // *predicate* next index to write
  int nval;                     // *predicate* next val to write
} shared = { PTHREAD_MUTEX_INITIALIZER };

void *produce(void *), *consume(void *);

int main( int argc, char** argv )
{
  int i, nthreads, count[MAXNTHREADS]={0};
  pthread_t tid_produce[MAXNTHREADS]={0}, tid_consume;

  if( argc != 3 )
  {
    fprintf( stderr, "usuage: prodcons2 <#items> <#threads>\n");
    exit(1);
  }

  nitems = min( atoi( argv[1] ), MAXNITEMS );
  nthreads = min( atoi( argv[2] ), MAXNTHREADS );

  // start all producer threads
  for( i=0; i < nthreads; ++i )
  {
    count[i] = 0;
    Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
  }

  // wait for all the producer threads
  for( i=0; i < nthreads; ++i ) 
  {
    pthread_join( tid_produce[i], NULL );
    printf("tid[%d] count[%d] = %d\n", tid_produce[i], i, count[i] );
  }

  // start, then wait for the consumer thread
  Pthread_create(&tid_consume, NULL, consume, NULL );
  pthread_join(tid_consume, NULL );

  exit(0);
}

void* produce(void* arg)
{
  printf("run tid[%d] \n", pthread_self());

  for(;;) 
  {
    pthread_mutex_lock( &shared.mutex );

    // nitems is the max num of shared.buff. When buff is full, we are done.
    if( shared.nput >= nitems )
    {
      printf("done tid[%d] \n", pthread_self());
      pthread_mutex_unlock(&shared.mutex);
      return NULL;
    }

    shared.buff[ shared.nput ] = shared.nval;
    shared.nput++; ~
    shared.nval++; ~

    pthread_mutex_unlock( &shared.mutex );

    // inc(count[i]) is the num of items that this thread wrote. Why not in
    // the critical section? Because each thread has its own
    *((int*)arg) += 1;
  }
}

void* consume(void* arg)
{
  int i;

  // see use of global vars; nitems and shared.buff
  for( i=0; i < nitems; i++ )
  {
    if( shared.buff[i] != i )
      printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
  }

  printf("consume done\n" );

  return NULL;
}


$ ./a.out 1000000 5
run tid[-1252185280] 
run tid[-1243792576] 
run tid[-1235399872] 
run tid[-1227007168] 
run tid[-1218614464] 
done tid[-1252185280] 
done tid[-1243792576] 
done tid[-1227007168] 
done tid[-1218614464] 
tid[-1218614464] count[0] = 198088
tid[-1227007168] count[1] = 220562
done tid[-1235399872] 
tid[-1235399872] count[2] = 254805
tid[-1243792576] count[3] = 162778
tid[-1252185280] count[4] = 163767
consume done


<ex-02>
// Now consumer runs at the same time and runs when there is data to read. All
// is accessing the same data and are in the `same mutex group`, `same contention` 
//
// Problem is that when consumer has a lock and wakes up, consumer do polling
// or spinning to check data. The consumer and producer has the equal
// opportunity to run since it is 'indeterminate' which thread will succeed in
// acquiring a lock.
//
// Uses shared mutex between producers and consumer. If items to read are
// ready then returns. Otherwise, do loops. The changes from the previous is:

int main()
{
  // ...

  // start all producer threads
  for( i=0; i < nthreads; ++i )
  {
    count[i] = 0;
    Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
  }

  // do not waits for producers to finish off and starts consumer right away
  Pthread_create(&tid_consume, NULL, consume, NULL );

  // ...
}

void consume_wait(int i)
{
  for(;;) 
  {
    pthread_mutex_lock(&shared.mutex);

    if( i < shared.nput ) {
      pthread_mutex_unlock(&shared.mutex);

      // item is ready
      return; 
    }

    // item is not ready so continue waiting
    pthread_mutex_unlock(&shared.mutex);
  }
}

void* consume(void* arg)
{
  // reading index
  int i;

  // see use of global vars; nitems and shared.buff
  for( i=0; i < nitems; i++ )
  { >
    // polling until ith item is ready. i is current read index.
    consume_wait(i);

    // check/consume
    if( shared.buff[i] != i )
      printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
  }

  printf("con: done\n" );

  return NULL;
}


<ex-03>
// Use condition variable


// // shared by all threads
// int nitems;
// 
// struct {
//   pthread_mutex_t mutex;
//   int buff[MAXNITEMS];
//   int nput;                     // next index to write
//   int nval;                     // next val to write
// } shared = { PTHREAD_MUTEX_INITIALIZER };


// Take buff out of shared state
// Use two mutexes for read and write. May be slightly better but not sure how
// much.

int nitems;
int buff[MAXNITEMS];

struct {
  pthread_mutex_t mutex;
  int nput;                     // next index to write
  int nval;                     // next val to write
} put = { PTHREAD_MUTEX_INITIALIZER };

struct {
  pthread_mutex_t mutex;
  pthread_cond_t cond;
  int ready;                    // *predicate* 

} nready = { PTHREAD_MUTEX_INITIALIZER, PTHREAD_COND_INITIALIZER };

void* produce(void* arg)
{
  printf("pro: run tid[%d] \n", pthread_self());

  for(;;) 
  {
    pthread_mutex_lock( &put.mutex );

    // buff is full, we are done.
    if( put.nput >= nitems )
    {
      printf("pro: no more. done tid[%d] \n", pthread_self());
      pthread_mutex_unlock(&put.mutex);
      return NULL;
    }

    buff[ put.nput ] = put.nval;
    put.nput++;
    put.nval++;

    pthread_mutex_unlock( &put.mutex );

    // note: changed to use cond-var 
    // {
    pthread_mutex_lock( &nready.mutex );

    // "ready" is 0 and means there is no items to read. Now there is one item
    // produced and signal cond-var. So "ready" is 'predicate' and the number
    // of items avaialbe to read ( number of produce - number of consume ).

    if( nready.ready == 0 )
      pthread_cond_signal( &nready.cond );

    nready.ready++;

    pthread_mutex_unlock( &nready.mutex );
    // }

    // inc(count[i]) is the num of items that this thread wrote. Why not in
    // the critical section? Because each thread has its own
    *((int*)arg) += 1;
  }
}

void* consume(void* arg)
{
  int i;

  // Q: Better if consumer reads all when has a lock? Yes, like a LPI example.
  for( i=0; i < nitems; i++ )
  { >
    // receieve cond-var
    pthread_mutex_lock( &nready.mutex );

    // Always test the condition again when wakes up because
    // *spurious-wakeups* can occur. Unlock "nready.mutex" and wait. 

    while( nready.ready == 0 ) 
      pthread_cond_wait( &nready.cond, &nready.mutex);

    // When returns from pthread_cond_wait, "nready.mutex" is locked
    // automatically and there is a item to read.
    
    nready.ready--;

    pthread_mutex_unlock( &nready.mutex );

    if( buff[i] != i )
      printf("con: err: buff[%d] = %d\n", i, buff[i] );
  }

  printf("con: done\n" );

  return NULL;
}


={============================================================================
*kt_linux_core_202* linux-sync-cond-order linux-sync-cond-spurious-wakeup

<use-cond-on-signal-side>
LPI and UNP uses different call order of _mutex_unlock and _cond_signal.

POSIX recommends the followings although SUSv3 permits them to be done in
either order and LPI said this may yieid `better performance`:

s = pthread_mutex_unlock(&mtx);
s = pthread_cond_signal(&cond);

or

s = pthread_cond_signal(&cond);
s = pthread_mutex_unlock(&mtx);


30.2.2 Signaling and Waiting on Condition Variables

[Butenhof, 1996] points out that, on some implementations, unlocking the mutex
and then signaling the condition variable may yield better performance than
  performing these steps in the reverse sequence. 

If the mutex is unlocked only after the condition variable is signaled, the
thread performing pthread_cond_wait() may wake up while the mutex is still
locked, and then immediately go back to sleep again when it finds that the
mutex is locked. This results in two superfluous context switches. Some
implementations eliminate this problem by employing a technique called wait
morphing, which moves the signaled thread 

  from `the condition variable wait queue` to `the mutex wait queue`
  
without performing a context switch if the mutex is locked.


<ex> 
Should always set variable first and then signal the change to other tasks,
not the other way round. Although here it is done under locked mutex, which
makes it safe, however it is still worth following a good practice.

from:
    g_mutex_lock(&writer->mutex);
    {
        old_thread = writer->writer_thread;
        if (old_thread)
        {
            g_cond_signal(&writer->cond);
        }
        `writer->writer_thread = NULL;`
    }
    g_mutex_unlock(&writer->mutex);

to:
    g_mutex_lock(&writer->mutex);
    {
        old_thread = writer->writer_thread;
        if (old_thread)
        {
            `writer->writer_thread = NULL;`
            g_cond_signal(&writer->cond);
        }
    }
    g_mutex_unlock(&writer->mutex);


<use-cond-on-wait-side> <linux-sync-cond-spurious-wakeups>

30.2.3 Testing a Condition Variable’s Predicate

Each condition variable has an associated `predicate` involving one or more
`shared variables.` In these example, nready.ready == 0  and (avail == 0) are
the predicate.

This demonstrates a general design principle: a pthread_cond_wait() call must
be governed by a while loop rather than an if statement. 

Because, on return from pthread_cond_wait(), that is when signaled, there are
`no guarantees` about the state of the predicate; therefore, we should
immediately `recheck` the predicate and resume sleeping if it is not in the
desired state. 

So use while-loop always to test the condition again when wakes up because we
can’t make any assumptions about the state of the predicate upon return from
pthread_cond_wait(), including spurious wakeups, for following reasons:

  while (avail == 0) {            /* Wait for something to consume */
    s = pthread_cond_wait(&cond, &mtx);
    if (s != 0)
      errExitEN(s, "pthread_cond_wait");
  }

  while( nready.ready == 0 )
    pthread_cond_wait( &nready.cond, &nready.mutex );

  * Other threads may be woken up first. Perhaps several threads were
    waiting to acquire the mutex associated with the condition variable. Even
    if the thread that signaled the mutex set the predicate to the desired
    state, it is still possible that another thread might acquire the mutex
    first and change the state of the associated shared variable(s), and thus
    the state of the predicate.

  * Designing for "loose" predicates may be simpler. Sometimes, it is easier
    to design applications based on condition variables that indicate
    `possibility rather than certainty` In other words, signaling a condition
    variable would mean "there may be something" for the signaled thread to do
    rather than "there is something" to do. Using this approach, the condition
    variable can be signaled based on approximations of the predicate's state,
    and the signaled thread can ascertain if there really is something to do
    by rechecking the predicate.

  * Spurious wake-ups can occur. On some implementations, a thread waiting on
    a condition variable may be woken up even though no other thread actually
    signaled the condition variable. Such spurious wake-ups are a (rare)
    consequence of the techniques required for efficient implementation on
    some multiprocessor systems, and are explicitly 'permitted' by SUSv3.


={============================================================================
*kt_linux_core_157* linux-sync-cond-advanced

Advanced thread-synchronization primitives: These facilities include barriers,
readwrite locks, and spin locks.


={============================================================================
*kt_linux_core_202* linux-sync-cond-examples

<ex>
typedef struct
{
  GstNexusMgr* mgr;

  GstNexusCallbackId callback_id;

  int reason;

  /* The members below are specific to callback_id. */
  int playpump_num;
  EXUS_Callback delegate_callback;
  void* delegate_context;
} GstNexusMgrCallbackInfo;


// producer, callback from hardware

static void gst_nexus_mgr_generic_1st_level_cb(void* context, int reason)
{
  /* ctx points to a member of mgr->callback_contexts, which we can read
   * but not write from this thread.
   */
  const GstNexusMgrCallbackInfo* ctx = context;

  GstNexusMgrCallbackInfo* posted_cb = g_malloc(sizeof(GstNexusMgrCallbackInfo));
  memcpy(posted_cb, ctx, sizeof(GstNexusMgrCallbackInfo));
  posted_cb->reason = reason;

  gst_nexus_mgr_enqueue_callback(mgr, posted_cb);

  /* mgr->callback_cond is associated with mgr->resource_lock rather than
   * with mgr->pending_callbacks_mutex. We are not allowed to lock
   * mgr->resource_lock in 1st level callbacks. Fortunately, signalling
   * a condition variable doesn't require locking the corresponding mutex.
   */
  // *cond-signal* Q: where's unlock?
  g_cond_signal(&mgr->callback_cond);
}


note: follows the general principle. use while on the predicate which is to
check if queue has items to process.

static gpointer mgr_callback_thread(gpointer context)
{
  GstNexusMgr* mgr = context;

  g_mutex_lock(&mgr->resource_lock);

  while(!mgr->callback_thread_exiting) {

    // deque cb item
    GstNexusMgrCallbackInfo* cb = mgr_dequeue_callback(mgr);

    // no item in the queue to process. wait and release a lock
    if (!cb){

      // *cond-wait*
      g_cond_wait(&mgr->callback_cond, &mgr->resource_lock);
      continue;
    }

    // use cb item
    mgr_2nd_level_cb_table[cb->callback_id](cb);
    g_free(cb);
  }

  g_mutex_unlock(&mgr->resource_lock);

  return NULL;
}


<ex>
Two writer(source) threads which pushes data to each queue and the
writer_thread which is single reader and reads data from each queue and writes
it to a hardware buffer. Since there is a single reader, less concern of
spurious wakeups.

Uses two predicates.

// producers
gboolean pes_data_source_produce_data(PesDataSource* source, PesStream* data)
{
  g_mutex_lock(&writer->mutex);
  g_mutex_lock(&source->mutex);

  g_queue_push_tail(source->chunks, chunk);

  g_mutex_unlock(&source->mutex);

  // *cond-signal* used different order
  g_cond_signal(&writer->cond);
  g_mutex_unlock(&writer->mutex);
}

// consumer
gpointer writer_thread(gpointer context)
{
  ConcurrentPesWriter* writer = context;
  GThread* this_thread = g_thread_self();

  g_mutex_lock(&writer->mutex);

  // note: predicate two which is to check the shutdown.

  // concurrent_pes_writer_start() / _stop() will overwrite
  // writer->writer_thread with another value, which is how we notice we need to
  // shut down.

  while (this_thread == writer->writer_thread)
  {
    GQueue* queue = NULL;
    gint64 timeout;

    if(!g_mutex_trylock (writer->resource_lock)) {
      /* Might not be able to get the resource_lock because another
         thread already has the lock and is trying to shut down this
         writer_thread */
      goto nap_time;
    }

    // note: predicate one which is a queue check

    while ((queue = find_best_queue_for_consumption_locked(writer)))
    {
      // get a buffer
      rc = NEXUS_Playpump_GetBuffer(
          writer->playpump_handle, &buffer, &buffer_size);

      if (buffer_size == 0)
      {
        break;
      }

      // say a write is completed 
      rc = NEXUS_Playpump_WriteComplete(
          writer->playpump_handle, 0, bytes_just_written);
      if (NEXUS_SUCCESS != rc)
      {
        break;
      }
    } // end of write while ((queue = ...))

    g_mutex_unlock (writer->resource_lock);

nap_time:
    // *cond-wait* 
    /* sleep for a while, or until more data is added to a queue */
    g_cond_wait_until(&writer->cond, &writer->mutex, timeout);

  } // end of thread while

  g_mutex_unlock(&writer->mutex);

  return NULL;
}


<ex>
This uses a single mutex, managerMutex, to guard access to APIs and callbacks.

void XX::API1(...)
{
    boost::unique_lock<boost::mutex> statusLock(managerMutex);

    if(activePlayer)
    {
        // Suspend player and wait for StatusEvent saying MR is stopped
        activePlayer->suspend();
        managerCondition.wait(statusLock);
    }

    mediaRouter->addListener(dispatcher, shared_from_this());
}

void XX::API2(...)
{
    const boost::unique_lock<boost::mutex> statusLock(managerMutex);
    ...
}

void XX::CALLBACK(const EventValue::Enum statusEvent)
{
    if(statusEvent == StatusEventValue::stopped)
    {
        boost::unique_lock<boost::mutex> statusLock(managerMutex);
        managerCondition.notify_one();
    }
}


={============================================================================
*kt_linux_core_202* linux-sync-between-processes

As with semaphore, mutex and cond-var uses 'global' structure and if these are
shared in a shared memory, these can be used between processes. If not, it is
for threads in a process.

In {ref-LPI} p881, process-shared mutexes and condition variables. They are
  'not' available on all UNIX systems, and so are not commonly employed for
  process synchronization.

note: As said, if share mutex structure in shared memory, then what is
process-shared mutex?


{implicit-sync}
This is synchronization handled by kernel not by applicaion. Such as pipe and
message-q. For example,

grep pattern chapters.* | wc -l

Writing by producer and reading by consumer are handled by kernel. Does it
mean that grep and wc runs at the same time? not one after one.


={============================================================================
*kt_linux_core_202* sync: read-write lock

From {ref-UNP} but no such a thing in {ref-LPI}, so may be old way but surely in posix but may be
different to this since this lock is before posix standard. See {ref-UNP} note.

To distinguish between obtaining the read-write lock for reading and for writing. The rules:

1. Any number of threads can hold a given read-write lock for reading as long as no threads holds
the the lock for writing.

2. A read-write lock can be allocated for writing only if no thread hold the lock for reading or
writing.

Stated another way, any threads can have read access to a data as long as no thread is modifying
that. A data can be modified only if no other thread is reading or modifying the data.

In application, the data is read more often than the data is modified, and these can benefit from
using read-write locks instead of mutex locks. 

can provide more concurrency than a plain mutex lock when the data is read more than it is written
and known as shared-exclusive locking since shared lock for reading and exclusive lock for writing.
Multiple readers and one writer problem or readers-writer locks.


{apis}

// to get read lock. blocks the calling if there are writers
int   pthread_rwlock_rdlock(pthread_rwlock_t *);

// to get write lock. blocks the calling if there are readers or writers
int   pthread_rwlock_wrlock(pthread_rwlock_t *);

int   pthread_rwlock_unlock(pthread_rwlock_t *);
int   pthread_rwlock_tryrdlock(pthread_rwlock_t *);
int   pthread_rwlock_trywrlock(pthread_rwlock_t *);

int   pthread_rwlock_init(pthread_rwlock_t *, const pthread_rwlockattr_t *);
int   pthread_rwlock_destroy(pthread_rwlock_t *);
int   pthread_rwlockattr_destroy(pthread_rwlockattr_t *);
int   pthread_rwlockattr_init(pthread_rwlockattr_t *);

// to share the lock between different processes
int   pthread_rwlockattr_setpshared(pthread_rwlockattr_t *, int); pthread_t
int   pthread_rwlockattr_getpshared(const pthread_rwlockattr_t *, int *);


{example-implementation}
\unpv22e.tar\unpv22e\my_rwlock_cancel\

Can be implemented using mutexes and condition variables. This is an implementation which gives
preference to waiting writers but there are other alternatives.

typedef struct {
	pthread_mutex_t 	rw_mutex;			// lock on this struct
	pthread_cond_t 	rw_condreaders;	// for reader threads waiting
	pthread_cond_t 	rw_condwriters;	// for writer threads waiting

	// [KT]
	// when struct is inited, set to RW_MAGIC and used by all functions to check that the caller is
	// passing a pointer to an initialized lock and set to 0 when the lock is destroyed.
	int 					rw_magic;
	int 					rw_nwaitreaders;
	int 					rw_nwaitwriters;

	// the current status of the read-write lock. only one of these can exist at a time: -1 indicates
	// a write lock, 0 is lock available, and an value greater than 0 menas that many read locks are
	// held.
	int 					rw_refcount; 
} pthread_rwlock_t;


#define RW_MAGIC 0x19283746

/* init and destroy */
int
pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		result;

	if (attr != NULL)
		return(EINVAL);		/* not supported */

	if ( (result = pthread_mutex_init(&rw->rw_mutex, NULL)) != 0)
		goto err1;
	if ( (result = pthread_cond_init(&rw->rw_condreaders, NULL)) != 0)
		goto err2;
	if ( (result = pthread_cond_init(&rw->rw_condwriters, NULL)) != 0)
		goto err3;
	rw->rw_nwaitreaders = 0;
	rw->rw_nwaitwriters = 0;
	rw->rw_refcount = 0;
	rw->rw_magic = RW_MAGIC;

	return(0);

err3:
	pthread_cond_destroy(&rw->rw_condreaders);
err2:
	pthread_mutex_destroy(&rw->rw_mutex);
err1:
	return(result);			/* an errno value */
}

void
Pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		n;

	if ( (n = pthread_rwlock_init(rw, attr)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_init error");
}

int
pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);
	if (rw->rw_refcount != 0 ||
		rw->rw_nwaitreaders != 0 || rw->rw_nwaitwriters != 0)
		return(EBUSY);

	pthread_mutex_destroy(&rw->rw_mutex);
	pthread_cond_destroy(&rw->rw_condreaders);
	pthread_cond_destroy(&rw->rw_condwriters);
	rw->rw_magic = 0;

	return(0);
}

void
Pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	int		n;

	if ( (n = pthread_rwlock_destroy(rw)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_destroy error");
}


// rdlock
// A problem exists in this function: if the calling thread blocks in the call to pthread_cond_wait
// and the thread is then canceled, the thread terminates while it holds the mutex lock, and the
// counter rw_nwaitreaders is wrong. The same problem exists in our implentation of
// pthred_rwlock_wrlock. Can correct these problem in {}

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// 4give preference to waiting writers. cannot get a read lock if a) rw_refcount < 0 (meaning
	// there is a writer holding the lock and b) if threads are waiting to get a write lock.
	// [KT] if. case that there are writers
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}

	// [KT] else. case that there are no writers
	if (result == 0)
		rw->rw_refcount++;		/* another reader has a read lock */

	pthread_mutex_unlock(&rw->rw_mutex);
	return (result);
}

/* tryrdlock */
int
pthread_rwlock_tryrdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0)
		result = EBUSY;			/* held by a writer or waiting writers */
	else
		rw->rw_refcount++;		/* increment count of reader locks */

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


/* wrlock */
int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// [KT] if there are other readers or writers
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}

	// [KT] else there are no readers and writers
	if (result == 0)
		rw->rw_refcount = -1;

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


// unlock. used by both reader and writer
int
pthread_rwlock_unlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount > 0)
		rw->rw_refcount--;			// releasing a reader
	else if (rw->rw_refcount == -1)
		rw->rw_refcount = 0;			// releasing a writer
	else
		err_dump("rw_refcount = %d", rw->rw_refcount); // cannot be since it is to unlock

	// 4give preference to waiting writers over waiting readers
	//
	// {Q} The ref-UNP says: notice that we do not grant any additional read locks as soon as a
	// writer is waiting; otherwise, a stream of continual read requests could block a waiting writer
	// forever. For this reason, we need two separate if tests and cannot write
	//
	// if( rw->rw_nwaitwriters > 0 && rw->rw_refcount == 0 )
	// ...
	// 
	// could also omit the test of rw->rw_refcount, but that can result in calls to
	// pthread_cond_signal when read locks are still allocated, which is less efficient.
	//
	if (rw->rw_nwaitwriters > 0) {
		if (rw->rw_refcount == 0)
			result = pthread_cond_signal(&rw->rw_condwriters);		// signal single writer
	} else if (rw->rw_nwaitreaders > 0)
		result = pthread_cond_broadcast(&rw->rw_condreaders);		// signal all readers

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


{example}

#include	"unpipc.h"
#include	"pthread_rwlock.h"

pthread_rwlock_t	rwlock = PTHREAD_RWLOCK_INITIALIZER;

void	 *thread1(void *), *thread2(void *);
pthread_t	tid1, tid2;

int
main(int argc, char **argv)
{
	void	*status;
	Pthread_rwlock_init(&rwlock, NULL);

	Set_concurrency(2);
	Pthread_create(&tid1, NULL, thread1, NULL);
	sleep(1);		/* let thread1() get the lock */
	Pthread_create(&tid2, NULL, thread2, NULL);

	Pthread_join(tid2, &status);
	if (status != PTHREAD_CANCELED)
		printf("thread2 status = %p\n", status);

	Pthread_join(tid1, &status);
	if (status != NULL)
		printf("thread1 status = %p\n", status);

	printf("rw_refcount = %d, rw_nwaitreaders = %d, rw_nwaitwriters = %d\n",
		   rwlock.rw_refcount, rwlock.rw_nwaitreaders,
		   rwlock.rw_nwaitwriters);
	Pthread_rwlock_destroy(&rwlock);
	/* 4returns EBUSY error if cancelled thread does not cleanup */

	exit(0);
}

void *
thread1(void *arg)
{
	Pthread_rwlock_rdlock(&rwlock);
	printf("thread1() got a read lock\n");
	sleep(3);		/* let thread2 block in pthread_rwlock_wrlock() */
	pthread_cancel(tid2);
	sleep(3);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

void *
thread2(void *arg)
{
	printf("thread2() trying to obtain a write lock\n");
	Pthread_rwlock_wrlock(&rwlock);

	// followings are never get executed since it gets canceled.
	printf("thread2() got a write lock\n");
	sleep(1);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

When run this, the program is hung. The occurred steps are:

1. the second trys to get write lock and blocks on pthread_cond_wait.
2. the first returns from slepp(3) and cancel the second.

3. when the second is canceled while it is blocked in a condition variable wait, the mutex is
reacquired before calling the first cleanup hander (even if not installed any handers, but the mutex
is still reacquired before the thread is canceled.) Therefore, when the secondis canceled, it holds
the mutex lock for the read-write lock.

4. the first calls pthread_rwlock_unlock, but it blocks forever in its call to pthread_mutex_lock
because the mutex is still locked by the first thread that was canceled. [KT] this means cancel
terminates a thread immediately and do not continue the rest in pthread_rwlock_wrlock. Hence still
locked. 

If remove the call to pthread_rwlock_unlock in the thread1 func, the main will print:

rw_refcount = 1, rw_nwaitreaders = 0, rw_nwaitwriters = 1
pthread_rwlock_destroy error: Device busy

The correctio is simple and this is addition to pthread_rwlock_rdlock:

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		pthread_cleanup_push( rwlock_cancelrdwait, (void*)rw); ~
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelrdwait */
static void
rwlock_cancelrdwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitreaders--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelrdwait */ 

int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		pthread_cleanup_push(rwlock_cancelwrwait, (void *) rw); ~
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelwrwait */
static void
rwlock_cancelwrwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitwriters--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelwrwait */


={============================================================================
*kt_linux_core_230* linux-sync-file-lock

File locks: File locks are a synchronization method explicitly designed to
coordinate the actions of multiple processes operating [on-the-same-file].
They can also be used to coordinate access to other shared resources. File
locks come in two flavors: read (shared) locks and write (exclusive) locks.
Any number of processes can hold a read lock on the same file (or region of a
    file). However, when one process holds a write lock on a file (or file
      region), other processes are prevented from holding either read or write
    locks on that file (or file region). Linux provides file-locking
    facilities via the flock() and fcntl() system calls. The flock() system
    call provides a simple locking mechanism, allowing processes to place a
    shared or an exclusive lock on an entire file.  Because of its limited
    functionality, flock() locking facility is rarely used nowadays. The
    fcntl() system call provides record locking, allowing processes to place
    multiple read and write locks on different regions of the same file.


={============================================================================
*kt_linux_core_260* linux-sync-atomic

https://gcc.gnu.org/onlinedocs/gcc-4.9.4/gcc/_005f_005fsync-Builtins.html#_005f_005fsync-Builtins

6.51 Legacy __sync Built-in Functions for Atomic Memory Access

The following built-in functions are intended to be compatible with those
described in the Intel Itanium Processor-specific Application Binary Interface,
section 7.4. As such, they depart from the normal GCC practice of using the
‘__builtin_’ prefix, and further that they are overloaded such that they work on
multiple types.

The definition given in the Intel documentation allows only for the use of the
types int, long, long long as well as their unsigned counterparts. GCC allows
any integral scalar or pointer type that is 1, 2, 4 or 8 bytes in length.

Not all operations are supported by all target processors. If a particular
operation cannot be implemented on the target processor, a warning is generated
and a call an external function is generated. The external function carries the
same name as the built-in version, with an additional suffix ‘_n’ where n is the
size of the data type.  In most cases, these built-in functions are considered a
full barrier. That is, no memory operand is moved across the operation, either
forward or backward. Further, instructions are issued as necessary to prevent
the processor from speculating loads across the operation and from queuing
stores after the operation.

All of the routines are described in the Intel documentation to take “an
optional list of variables protected by the memory barrier”. It's not clear what
is meant by that; it could mean that only the following variables are protected,
or it could mean that these variables should in addition be protected. At
present GCC ignores this list and protects all variables that are globally
accessible. If in the future we make some use of this list, an empty list will
continue to mean all globally accessible variables.

type __sync_fetch_and_add (type *ptr, type value, ...)
type __sync_fetch_and_sub (type *ptr, type value, ...)
type __sync_fetch_and_or (type *ptr, type value, ...)
type __sync_fetch_and_and (type *ptr, type value, ...)
type __sync_fetch_and_xor (type *ptr, type value, ...)
type __sync_fetch_and_nand (type *ptr, type value, ...)

These built-in functions perform the operation suggested by the name, and
returns the value that had previously been in memory. That is,

          { tmp = *ptr; *ptr op= value; return tmp; }
          { tmp = *ptr; *ptr = ~(tmp & value); return tmp; }   // nand

Note: GCC 4.4 and later implement __sync_fetch_and_nand as *ptr = ~(tmp & value)
instead of *ptr = ~tmp & value. 

// skipped

//Source: http://golubenco.org/atomic-operations.html
#ifndef _ATOMIC_H
#define _ATOMIC_H

/* Check GCC version, just to be safe */
#if !defined(__GNUC__) || (__GNUC__ < 4) || (__GNUC_MINOR__ < 1)
# error atomic.h works only with GCC newer than version 4.1
#endif /* GNUC >= 4.1 */

/**
 * Atomic type.
 */
typedef struct {
	volatile int counter;
} atomic_t;

#define ATOMIC_INIT(i)  { (i) }

/**
 * Read atomic variable
 * @param v pointer of type atomic_t
 *
 * Atomically reads the value of @v.
 */
#define atomic_read(v) ((v)->counter)

/**
 * Set atomic variable
 * @param v pointer of type atomic_t
 * @param i required value
 */
#define atomic_set(v,i) (((v)->counter) = (i))

/**
 * Add to the atomic variable
 * @param i integer value to add
 * @param v pointer of type atomic_t
 */
static inline void atomic_add( int i, atomic_t *v )
{
	(void)__sync_add_and_fetch(&v->counter, i);
}

/**
 * Subtract the atomic variable
 * @param i integer value to subtract
 * @param v pointer of type atomic_t
 *
 * Atomically subtracts @i from @v.
 */
static inline void atomic_sub( int i, atomic_t *v )
{
	(void)__sync_sub_and_fetch(&v->counter, i);
}

/**
 * Subtract value from variable and test result
 * @param i integer value to subtract
 * @param v pointer of type atomic_t
 *
 * Atomically subtracts @i from @v and returns
 * true if the result is zero, or false for all
 * other cases.
 */
static inline int atomic_sub_and_test( int i, atomic_t *v )
{
	return !(__sync_sub_and_fetch(&v->counter, i));
}

/**
 * Increment atomic variable
 * @param v pointer of type atomic_t
 *
 * Atomically increments @v by 1.
 */
static inline void atomic_inc( atomic_t *v )
{
	(void)__sync_fetch_and_add(&v->counter, 1);
}

/**
 * @brief decrement atomic variable
 * @param v: pointer of type atomic_t
 *
 * Atomically decrements @v by 1.  Note that the guaranteed
 * useful range of an atomic_t is only 24 bits.
 */
static inline void atomic_dec( atomic_t *v )
{
	(void)__sync_fetch_and_sub(&v->counter, 1);
}

/**
 * @brief Decrement and test
 * @param v pointer of type atomic_t
 *
 * Atomically decrements @v by 1 and
 * returns true if the result is 0, or false for all other
 * cases.
 */
static inline int atomic_dec_and_test( atomic_t *v )
{
	return !(__sync_sub_and_fetch(&v->counter, 1));
}

/**
 * @brief Increment and test
 * @param v pointer of type atomic_t
 *
 * Atomically increments @v by 1
 * and returns true if the result is zero, or false for all
 * other cases.
 */
static inline int atomic_inc_and_test( atomic_t *v )
{
	return !(__sync_add_and_fetch(&v->counter, 1));
}

/**
 * @brief add and test if negative
 * @param v pointer of type atomic_t
 * @param i integer value to add
 *
 * Atomically adds @i to @v and returns true
 * if the result is negative, or false when
 * result is greater than or equal to zero.
 */
static inline int atomic_add_negative( int i, atomic_t *v )
{
	return (__sync_add_and_fetch(&v->counter, i) < 0);
}

#endif


https://gcc.gnu.org/onlinedocs/gcc-4.9.4/gcc/_005f_005fatomic-Builtins.html#_005f_005fatomic-Builtins

6.52 Built-in functions for memory model aware atomic operations

// skipped


{atomic-operations}

For full articles:
http://www.informit.com/guides/content.aspx?g=cplusplus&seqNum=469

Atomicity

An atomic operation is a sequence of one or more machine instructions that are
executed sequentially, without interruption. By default, any sequence of two
or more machine instructions isn't atomic since the operating system may
suspend the execution of the current sequence of operations in favor of
another task. If you want to ensure that a sequence of operations is atomic
you must use some form of locking or other types of synchronization. 

Without that, the only guarantee you have is that a single machine instruction
is always atomic. the CPU will not interrupt a single instruction in the
middle. [KT] Not entirely true. 

We can conclude from that minimal guarantee that if you can prove that your compiler translates a
certain C++ statement into a single machine instruction, that C++ statement is naturally atomic
meaning, the programmer doesn't have to use explicit locking to enforce the atomic execution of that
statement.  

Which C++ Statements are Naturally Atomic?

Obviously, there are very few universal rules of thumb because each hardware architecture might
translate the same C++ statement differently. Many textbooks tell you that the unary ++ and --
operators, when applied to integral types and pointers, are guaranteed to be atomic. Historically,
when Dennis Ritchie and Brian Kernighan designed C, they added these operators to the language
because they wanted to take advantage of the fast INC (increment) assembly directive that many
machines supported. However, there is no guarantee in the C or C++ standards that these operators
shall be atomic. Ritchie and Kernighan were more concerned about speed rather than atomicity.

You shouldn't make assumptions about the atomicity of C++ statements without examining the output of
your compiler. In some cases, you might discover that what appears to be a single C++ statement is
in fact translated into a long and complex set of machine instructions. 


Epilog

The multithreading support of C++0x consists of a thread class as well as a standard atomics library
that guarantees the atomicity of logical and arithmetic operations. I will introduce the C++0x
atomics library in a separate column.


From C++11:

Data-dependency ordering: atomics and memory model 	N2664 	GCC 4.4
(memory_order_consume)


From StackOverflow:

The increment-memory machine instruction on an X86 is atomic only if you use it with a LOCK prefix.

x++ in C and C++ doesn't have atomic behavior. If you do unlocked increments, due to races in which
processor is reading and writing X, if two separate processors attempt an increment, you can end up
with just one increment or both being seen (the second processor may have read the initial value,
incremented it, and written it back after the first writes its results back).

I believe that C++11 offers atomic increments, and most vendor compilers have an idiomatic way to
cause an atomic increment of certain built-in integer types (typically int and long); see your
compiler reference manual.

If you want to increment a "large value" (say, a multiprecision integer), you need to do so with
using some standard locking mechanism such as a semaphore.

Note that you need to worry about atomic reads, too. On the x86, reading a 32 or 64 bit value
happens to be atomic if it is 64-bit word aligned. That won't be true of a "large value"; again
you'll need some standard lock.

{rmw-operations}
The RMW(read-modify-write) is:

<from-wikipedia>
A class of atomic operations such as test-and-set, fetch-and-add, and compare-and-swap which both
read a memory location and write a new value into it simultaneously, either with a completely new
value or some function of the previous value. These operations prevent race conditions in
multi-threaded applications. Typically they are used to implement mutexes or semaphores. These
atomic operations are also heavily used in non-blocking synchronization.

{atomic-non-atomic-operations}
http://preshing.com/20130618/atomic-vs-non-atomic-operations/

Much has already been written about atomic operations on the web, usually with a focus on atomic
read-modify-write (RMW) operations. However, those arenât the only kinds of atomic operations. There
are also atomic loads and stores, which are equally important. In this post, Iâll compare atomic
loads and stores to their non-atomic counterparts at both the processor level and the C/C++ language
level. Along the way, weâll clarify the C++11 concept of a âdata raceâ.

Automic operations: automic loads and stores + automic read-modify-write operations

An operation acting on shared memory is atomic if it completes in a single step relative to other
threads. When an atomic store is performed on a shared variable, no other thread can observe the
modification half-complete. When an atomic load is performed on a shared variable, it reads the
entire value as it appeared at a single moment in time. Non-atomic loads and stores do not make
those guarantees.

Without those guarantees, lock-free programming would be impossible, since you could never let
different threads manipulate a shared variable at the same time. We can formulate it as a rule:

Any time two threads operate on a shared variable concurrently, and one of those operations performs
a write, both threads must use atomic operations.

If you violate this rule, and either thread uses a non-atomic operation, youâll have what the C++11
standard refers to as a [data-race] (not to be confused with Javaâs concept of a data race, which is
different, or the more general race condition). [Q] what is the general race condition?

The C++11 standard doesnât tell you why data races are bad; only that if you have one, âundefined
behaviorâ will result (section 1.10.21). The real reason why such data races are bad is actually
quite simple: They result in [torn-reads] and [torn-writes].

A memory operation can be non-atomic because it uses multiple CPU instructions, non-atomic even when
using a single CPU instruction, or non-atomic because youâre writing portable code and you simply
canât make the assumption. Letâs look at a few examples.


<Non-Atomic Due to Multiple CPU Instructions>

Suppose you have a 64-bit global variable, initially zero.

uint64_t sharedValue = 0;

At some point, you assign a 64-bit value to this variable.

void storeValue()
{
    sharedValue = 0x100000002;
}

When you compile this function for 32-bit x86 using GCC, it generates the following machine code.

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	DWORD PTR sharedValue, 2
        mov	DWORD PTR sharedValue+4, 1
        ret
        ...

As you can see, the compiler implemented the 64-bit assignment using two separate machine
instructions. The first instruction sets the lower 32 bits to 0x00000002, and the second sets the
upper 32 bits to 0x00000001. Clearly, this assignment operation is not atomic. If sharedValue is
accessed concurrently by different threads, several things can now go wrong:

- If a thread calling storeValue is preempted between the two machine instructions, it will leave
the value of 0x0000000000000002 in memory â a torn write. At this point, if another thread reads
sharedValue, it will receive this completely bogus value which nobody intended to store.

- Even worse, if a thread is preempted between the two instructions, and another thread modifies
sharedValue before the first thread resumes, it will result in a permanently torn write: the upper
32 bits from one thread, the lower 32 bits from another.

- On multicore devices, it isnât even necessary to preempt one of the threads to have a torn write.
When a thread calls storeValue, any thread executing on a different core could read sharedValue at a
moment when only half the change is visible.

Reading concurrently from sharedValue brings its own set of problems:

uint64_t loadValue()
{
    return sharedValue;
}

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	eax, DWORD PTR sharedValue
        mov	edx, DWORD PTR sharedValue+4
        ret
        ...

Here too, the compiler has implemented the load operation using two machine instructions: The first
reads the lower 32 bits into eax, and the second reads the upper 32 bits into edx. In this case, if
a concurrent store to sharedValue becomes visible between the two instructions, it will result in a
torn read â even if the concurrent store was atomic.

These problems are not just theoretical. Mintomicâs test suite includes a test case called
test_load_store_64_fail, in which one thread stores a bunch of 64-bit values to a single variable
using a plain assignment operator, while another thread repeatedly performs a plain load from the
same variable, validating each result. On a multicore x86, this test fails consistently, as
expected.


<Non-Atomic in a single CPU Instructions>

A memory operation can be non-atomic even when performed by a single CPU instruction. For example,
  the ARMv7 instruction set includes the strd instruction, which stores the contents of two 32-bit
  source registers to a single 64-bit value in memory.

strd r0, r1, [r2]

On some ARMv7 processors, this instruction is not atomic. When the processor sees this instruction,
	it actually performs [two-separate-32-bit-stores] under the hood (section A3.5.3). Once again,
	another thread running on a separate core has the possibility of observing a torn write.
	Interestingly, a torn write is even possible on a single-core device: A system interrupt â say,
	for a scheduled thread context switch â can actually occur between the two internal 32-bit
	stores! In this case, when the thread resumes from the interrupt, it will restart the strd
	instruction all over again.

As another example, itâs well-known that on x86, a 32-bit mov instruction is atomic if the memory
operand is naturally aligned, but non-atomic otherwise. In other words, atomicity is [only]
guaranteed when the 32-bit integer is located at an address which is an exact multiple of 4.


<All C/C++ Operations Are Presumed Non-Atomic>

In C and C++, every operation is presumed non-atomic unless otherwise specified by the compiler or
hardware vendor â even plain 32-bit integer assignment.

uint32_t foo = 0;

void storeFoo()
{
    foo = 0x80286;
}

The language standards have nothing to say about atomicity in this case. Maybe integer assignment is
atomic, maybe it isnât. Since non-atomic operations donât make any guarantees, plain integer
assignment in C is non-atomic by definition.

In practice, we usually know more about our target platforms than that. For example, itâs common
knowledge that on all modern x86, x64, Itanium, SPARC, ARM and PowerPC processors, plain 32-bit
integer assignment is atomic [as-long-as] the target variable is naturally aligned. You can verify it
by consulting your processor manual and/or compiler documentation. In the games industry, I can tell
you that a lot of 32-bit integer assignments rely on this particular guarantee.

Nonetheless, when writing truly portable C and C++, thereâs a long-standing tradition of pretending
that we donât know anything more than what the language standards tell us. Portable C and C++ is
designed to run on every possible computing device past, present and imaginary. Personally, I like
to imagine a machine where memory can only be changed by mixing it up first:

On such a machine, you definitely wouldnât want to perform a concurrent read at the same time as a
plain assignment; you could end up reading a completely random value.

<CPP11>
In C++11, there is finally a way to perform truly portable atomic loads and stores: the C++11 atomic
library. Atomic loads and stores performed using the C++11 atomic library would even work on the
imaginary computer above - even if it means the C++11 atomic library must secretly [lock] a mutex to
make each operation atomic. Thereâs also the Mintomic library which I released last month, which
doesnât support as many platforms, but works on several older compilers, is hand-optimized and is
guaranteed to be lock-free.


Relaxed Atomic Operations

Letâs return to the original sharedValue example from earlier in this post. Weâll rewrite it using
Mintomic so that all operations are performed atomically on every platform Mintomic supports. First,
we must declare sharedValue as one of Mintomicâs atomic data types.

#include <mintomic/mintomic.h>

mint_atomic64_t sharedValue = { 0 };

The mint_atomic64_t type guarantees correct memory alignment for atomic access on each platform.
This is important because, for example, the GCC 4.2 compiler for ARM bundled with Xcode 3.2.5
doesnât guarantee that plain uint64_t will be 8-byte aligned.

In storeValue, instead of performing a plain, non-atomic assignment, we must call
mint_store_64_relaxed.

void storeValue()
{
    mint_store_64_relaxed(&sharedValue, 0x100000002);
}

Similarly, in loadValue, we call mint_load_64_relaxed.

uint64_t loadValue()
{
    return mint_load_64_relaxed(&sharedValue);
}

Using C++11âs terminology, these functions are now data race-free. When executing concurrently,
		there is absolutely no possibility of a torn read or write, whether the code runs on
		ARMv6/ARMv7 (Thumb or ARM mode), x86, x64 or PowerPC. If youâre curious how
		mint_load_64_relaxed and mint_store_64_relaxed actually work, both functions expand to an
		inline cmpxchg8b instruction on x86; for other platforms, consult Mintomicâs implementation.

Hereâs the exact same thing written in C++11 instead:

#include <atomic>

std::atomic<uint64_t> sharedValue(0);

void storeValue()
{
    sharedValue.store(0x100000002, std::memory_order_relaxed);
}

uint64_t loadValue()
{
    return sharedValue.load(std::memory_order_relaxed);
}

Youâll notice that both the Mintomic and C++11 examples use relaxed atomics, as evidenced by the
_relaxed suffix on various identifiers. The _relaxed suffix is a reminder that, just as with plain
loads and stores, no guarantees are made about memory ordering.

The only difference between a relaxed atomic load (or store) and a non-atomic load (or store) is
that relaxed atomics guarantee atomicity. No other difference is guaranteed.

In particular, it is still legal for the memory effects of a relaxed atomic operation to be
reordered with respect to instructions which follow or precede it in program order, either due to
compiler reordering or memory reordering on the processor itself. The compiler could even perform
optimizations on redundant relaxed atomic operations, just as with non-atomic operations. In all
cases, the operation remains atomic.

When manipulating shared memory concurrently, I think itâs good practice to always use Mintomic or
C++11 atomic library functions, even in cases where you know that a plain load or store would
already be atomic on your target platform. An atomic library function serves as a reminder that
elsewhere, the variable is the target of concurrent data access.

Hopefully, itâs now a bit more clear why the Worldâs Simplest Lock-Free Hash Table uses Mintomic
library functions to manipulate shared memory concurrently from different threads.


{lock-free-programming}
http://preshing.com/20120612/an-introduction-to-lock-free-programming/

An Introduction to Lock-Free Programming

Lock-free programming is a challenge, not just because of the complexity of the task itself, but
because of how difficult it can be to penetrate the subject in the first place.

I was fortunate in that my first introduction to lock-free (also known as lockless) programming was
Bruce Dawsonâs excellent and comprehensive white paper, Lockless Programming Considerations. And
like many, Iâve had the occasion to put Bruceâs advice into practice developing and debugging
lock-free code on platforms such as the Xbox 360.

Since then, a lot of good material has been written, ranging from abstract theory and proofs of
correctness to practical examples and hardware details. Iâll leave a list of references in the
footnotes. At times, the information in one source may appear orthogonal to other sources: For
instance, some material assumes sequential consistency, and thus sidesteps the memory ordering
issues which typically plague lock-free C/C++ code. The new C++11 atomic library standard throws
another wrench into the works, challenging the way many of us express lock-free algorithms.

In this post, Iâd like to re-introduce lock-free programming, first by defining it, then by
distilling most of the information down to a few key concepts. Iâll show how those concepts relate
to one another using flowcharts, then weâll dip our toes into the details a little bit. At a
minimum, any programmer who dives into lock-free programming should already understand how to write
correct multithreaded code using mutexes, and other high-level synchronization objects such as
semaphores and events.  

What Is It?

People often describe lock-free programming as programming without mutexes, which are also referred
to as [locks]. Thatâs true, but itâs only [part] of the story. The generally accepted definition, based
on academic literature, is a bit more broad. At its essence, lock-free is a property used to
describe some code, without saying too much about how that code was actually written.

Basically, if some part of your program satisfies the following conditions, then that part can
rightfully be considered lock-free. Conversely, if a given part of your code doesnât satisfy these
conditions, then that part is not lock-free.

<definition>
(This was a flow chart)

Are you programming with multiple threads? or interrupt, signal handlers, etc?
-> Yes

Do the threads access shared memeory?
-> Yes

Can the threads block each other? ie. is there some way to schedule the threads which would
'lock-up' indefinitely?
-> No

It is lock-free programming.

In this sense, the lock in lock-free [does not refer directly to mutexes], but rather to the
possibility of âlocking upâ the entire application in some way, whether itâs deadlock, livelock â or
even due to hypothetical thread scheduling decisions made by your worst enemy. That last point
sounds funny, but itâs key. Shared mutexes are ruled out trivially, because as soon as one thread
obtains the mutex, your worst enemy could simply never schedule that thread again. Of course, real
operating systems donât work that way - weâre merely defining terms.

Hereâs a simple example of an operation which contains no mutexes, but is still not lock-free.
Initially, X = 0. As an exercise for the reader, consider how two threads could be scheduled in a
way such that neither thread exits the loop.

while (X == 0)
{
    X = 1 - X;
}

Nobody expects a large application to be entirely lock-free. Typically, we identify a [specific-set]
of lock-free operations out of the whole codebase. For example, in a lock-free queue, there might be
a handful of lock-free operations such as push, pop, perhaps isEmpty, and so on.

Herlihy & Shavit, authors of The Art of Multiprocessor Programming, tend to express such operations
as class methods, and offer the following succinct definition of lock-free (see slide 150): 
	
"In an infinite execution, infinitely often some method call finishes." In other words, as long as
the program is able to keep calling those lock-free operations, the number of completed calls keeps
increasing, no matter what. It is algorithmically impossible for the system to lock up during those
operations. {Q} Didn't get that.

<why-use>
One important consequence of lock-free programming is that if you suspend a single thread, it will
never prevent other threads from making progress, as a group, through their own lock-free
operations. This hints at the value of lock-free programming when writing interrupt handlers and
real-time systems, where certain tasks must complete within a certain time limit, no matter what
state the rest of the program is in.

A final precision: Operations that are designed to block do not disqualify the algorithm. For
example, a queueâs pop operation may intentionally block when the queue is empty. The remaining
codepaths can still be considered lock-free.

<TODO> there is more in this page.


==============================================================================
*kt_linux_core_261*	sync: ref: locks aren't slow; lock contention is

<contention-and-frequency>
For example, this post measures the performance of a lock under heavy conditions: each thread must
hold the lock to do any work (high contention), and the lock is held for an extremely short interval
of time (high frequency)

But donât disregard locks yet. One good example of a place where locks perform admirably, in real
software, is when protecting the memory allocator. Doug Leaâs Malloc is a popular memory allocator
in video game development, but itâs single threaded, so we need to protect it using a lock. During
gameplay, itâs not uncommon to see multiple threads hammering the memory allocator, say around 15000
times per second. While loading, this figure can climb to 100000 times per second or more. Itâs not
a big problem, though. As youâll see, locks handle the workload like a champ.
{Q} what's this memory allocator?

Lock Contention Benchmark

In this test, we spawn a thread which generates random numbers, using a custom Mersenne Twister
implementation. For example, suppose we want to acquire the lock 15000 times per second, and keep it
held 50% of the time. (whole working time) 

This is code example to show 50% lock:

QueryPerformanceCounter(&start);
for (;;)
{
  // Do some work without holding the lock
  workunits = (int) (random.poissonInterval(averageUnlockedCount) + 0.5f);
  for (int i = 1; i < workunits; i++)
    random.integer();       // Do one work unit
  workDone += workunits;

  QueryPerformanceCounter(&end);
  elapsedTime = (end.QuadPart - start.QuadPart) * ooFreq;
  if (elapsedTime >= timeLimit)
    break;

  // Do some work while holding the lock
  EnterCriticalSection(&criticalSection);
  workunits = (int) (random.poissonInterval(averageLockedCount) + 0.5f);
  for (int i = 1; i < workunits; i++)
    random.integer();       // Do one work unit
  workDone += workunits;
  LeaveCriticalSection(&criticalSection);

  QueryPerformanceCounter(&end);
  elapsedTime = (end.QuadPart - start.QuadPart) * ooFreq;
  if (elapsedTime >= timeLimit)
    break;
}

Now suppose we launch two such threads, each running on a different core. Each thread will hold the
lock during 50% of the time when it can perform work, but if one thread tries to acquire the lock
while the other thread is holding it, it will be forced to wait. This is known as lock contention.

(Tested on dual core) When we run the above scenario, we find that each thread spends roughly 25% of
its time waiting, and 75% of its time doing actual work. Together, both threads achieve a net
performance of 1.5x compared to the single-threaded case.

I ran several variations of the test on a 2.66 GHz quad-core Xeon, from 1 thread, 2 threads, all the
way up to 4 threads, each running on its own core. I also varied the duration of the lock, from the
trivial case where the the lock is never held, all the way up to the maximum where each thread must
hold the lock for 100% of its workload. In all cases, the lock frequency remained constant â threads
acquired the lock 15000 times for each second of work performed.

<KT>
The graph shows that go up to 4x when 0% lock duration and down below 1x when 100% lock duration. 0%
means that there is no sharing between threads hence no lock is needed.

The results were interesting. For short lock durations, up to say 10%, the system achieved very high
parallelism. Not perfect parallelism, but close. Locks are fast!

To put the results in perspective, I analyzed the memory allocator lock in a multithreaded game
engine using this profiler. During gameplay, with 15000 locks per second coming from 3 threads, the
lock duration was in the neighborhood of just 2%. Thatâs well within the comfort zone on the left
side of the diagram.

These results also show that once the lock duration passes 90%, thereâs no point using multiple
threads anymore. A single thread performs better. Most surprising is the way the performance of 4
threads drops off a cliff around the 60% mark! This looked like an anomaly, so I re-ran the tests
several additional times, even trying a different testing order. The same behavior happened
consistently. My best hypothesis is that the experiment hits some kind of snag in the Windows
scheduler, but I didnât investigate further.

Lock Frequency Benchmark

Even a lightweight mutex has overhead. As my next post shows, a pair of lock/unlock operations on a
Windows Critical Section takes about 23.5 ns on the CPU used in these tests. Therefore, 15000 locks
per second is low enough that lock overhead does not significantly impact the results. But what
happens as we turn up the dial on lock frequency?

The algorithm offers very fine control over the amount of work performed between one lock and the
next, so I performed a new batch of tests using smaller amounts: from a very fine-grained 10 ns
between locks, all the way up to 31 Î¼s, which corresponds to roughly 32000 acquires per second. Each
test used exactly two threads:

As you might expect, for very high lock frequencies, the overhead of the lock itself begins to dwarf
the actual work being done. Several benchmarks youâll find online, including the one linked earlier
fall into the bottom-right corner of this chart. At such frequencies, youâre talking about some
seriously short lock times â on the scale of a few CPU instructions. The good news is that, when the
work between locks is that simple, a lock-free implementation is more likely to be feasible.

At the same time, the results show that locking up to 320000 times per second (3.1 Î¼s between
    successive locks) is not unreasonable. In game development, the memory allocator may flirt with
this frequency during load times. You can still achieve more than 1.5x parallelism if the lock
duration is short.

Weâve now seen a wide spectrum of lock performance: cases where it performs great, and cases where
the application slows to a crawl. Iâve argued that the lock around the memory allocator in a game
engine will often achieve excellent performance. Given this example from the real world, it cannot
be said that all locks are slow. Admittedly, itâs very easy to abuse locks, but one shouldnât live
in too much fear â any resulting bottlenecks will show up during careful profiling. When you
consider how reliable locks are, and the relative ease of understanding them (compared to lock-free
    techniques), locks are actually pretty awesome sometimes.

The goal of this post was to give locks a little respect where deserved - corrections are welcome. I
also realize that locks are used in a wide variety of industries and applications, and it may not
always be so easy to strike a good balance in lock performance. If youâve found that to be the case
in your own experience, I would love to hear from you in the comments.

{DN}
Then the bottomlien is that try to minimize the lock contention rather than try to find out which
one is fast. Of course, should use better one but the lock contention is far more important.


==============================================================================
*kt_linux_core_262*	sync: ref: always use a lightweight mutex {mutex-vs-semaphore}

http://preshing.com/20111124/always-use-a-lightweight-mutex/
Always Use a Lightweight Mutex

Have started wondering since seen this article which says that critical section is faster than mutex
in windows. This critical section is called as lightweight mutex which is equivalent to pthread
mutex in Linux. Does it mean that pthread mutex is faster than posix semaphore? Tried the same
approach that this article had and it seems not. Like ref-LPI, there is no big difference.

This is the result ran on the real Linux server machine which has multiple processors and as can
see, semaphore is slightly slower than using a mutex but not significant. This seems different from
what this article said.

<snippet>
Now, suppose you have a thread which acquires a Critical Section 100000 times per second, and there
are no other threads competing for the lock. Based on the above figures, you can expect to pay
between 0.2% and 0.6% in lock overhead. Not too bad! At lower frequencies, the overhead becomes
negligible.

<KT> Here shows the result in the graphic which is 58.7ns on Core 2 Duo and 23.5ns on Xeon for
windows critical section. Think that Core 2 means dual core and Xeon is single so means that there
are lock contention for Core 2 and not for Xeon. That's why he mean that there is 0.6% overhead for
locking. Then it's more about experimenting of mutl-core.

Naturally, Ubuntu 11.10 provides a lock implementation using the POSIX Threads API as well. Itâs
another lightweight mutex, based on a Linux-specific construct known as a futex. A pair of
pthread_mutex_lock/pthread_mutex_unlock calls takes about 66 ns on my Core 2 Duo. You can even share
this implementation between processes, but I didnât test that.

In my previous post, I argued against the misconception that locks are slow and provided some data
to support the argument. At this point, it should be clear that if you arenât using a lightweight
mutex, the entire argument goes out the window. Iâm fairly sure that the existence of heavy lock
implementations has only added to this misconception over the years.

Some of you old-timers may point out ancient platforms where a heavy lock was the only
implementation available, or when a semaphore had to be used for the job. But it seems all modern
platforms offer a lightweight mutex. And even if they didnât, you could write your own lightweight
mutex at the application level, even sharing it between processes, provided youâre willing to live
with certain caveats. Youâll find one example in my followup post, Roll Your Own Lightweight Mutex.


{when-with-no-threads}
keitee.park@magnum ~
$ ./sem
sem run
56:645
56:702   [57]
keitee.park@magnum ~
$ ./mtx
mtx run
3:710
3:773    [63]

keitee.park@magnum ~
$ ./sem
sem run
5:663
5:719    [56]
keitee.park@magnum ~
$ ./mtx
mtx run
6:894
6:957    [63]

keitee.park@magnum ~
$ ./sem
sem run
8:103
8:160    [57]
keitee.park@magnum ~
$ ./mtx
mtx run
9:294
9:356    [62]

<for-semaphore-example>
#include <stdio.h>
#include <time.h>
#include <fcntl.h>
#include <semaphore.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

  unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

int main()
{
  int s = 0, loop = 0;
  int flags, opt;
  mode_t perms;
  unsigned int value;
  sem_t *sem;

  flags = O_CREAT | O_EXCL;
  perms = S_IRUSR | S_IWUSR;

  fprintf( stderr, "sem run\n");

  // will create /dev/shm/sem.demo
  sem = sem_open("/demo", flags, perms, 1 );
  if(sem == SEM_FAILED)
    fprintf( stderr, "fail on sem_open");

  get_time_ms();

  for(loop = 0; loop < MAXLOOP; loop++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "fail on sem_wait");

    s = sem_post(sem);
    if (s != 0)
    fprintf(stderr, "fail on sem_post");
  }

  get_time_ms();

  s = sem_unlink("/demo");
  if(s != 0)
    fprintf(stderr, "fail on sem_unlink\n" );

}

<for-mutex-example>
#include <stdio.h>
#include <time.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

  unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;

int main()
{
  int s = 0, loop = 0;

  fprintf( stderr, "mtx run\n");

  get_time_ms();

  for(loop = 0; loop < MAXLOOP; loop++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "pthread_mutex_unlock");
  }

  get_time_ms();
}


{when-with-two-threads}
keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
14:946
15:370      [424]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
24:498
24:829      [331]
main: this is the end

keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
33:705
34:207      [502]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
35:993
36:460      [467]
main: this is the end

keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
2:673
3:125       [452]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
4:792
5:224       [432]
main: this is the end

<for-semaphore-example>
#include <stdio.h>
#include <time.h>
#include <fcntl.h>
#include <semaphore.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

	unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static sem_t *sem;
static unsigned int loopA;
static unsigned int loopB;

static void* threadFuncA( void *arg )
{
  int s = 0;

  for(loopA = 0; loopA < MAXLOOP; loopA++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "TA: fail on sem_wait\n");

    s = sem_post(sem);
    if (s != 0)
    fprintf( stderr, "TA: fail on sem_post\n");
  }
}

static void* threadFuncB( void *arg )
{
  int s = 0;

  for(loopB = 0; loopB < MAXLOOP; loopB++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "TB: fail on sem_wait\n");

    s = sem_post(sem);
    if (s != 0)
    fprintf( stderr, "TB: fail on sem_post\n");
  }
}

int main()
{
  int flags, opt;
  mode_t perms;
  pthread_t tA, tB;
  int s;

  flags = O_CREAT | O_EXCL;
  perms = S_IRUSR | S_IWUSR;

  // will create /dev/shm/sem.demo
  sem = sem_open("/demo", flags, perms, 1 );
  if(sem == SEM_FAILED)
    fprintf( stderr, "main: fail on sem_open\n");

  fprintf( stderr, "main: this is the second sem run\n");
  get_time_ms();

  s = pthread_create( &tA, NULL, threadFuncA, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(A)\n");

  s = pthread_create( &tB, NULL, threadFuncB, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(B)\n");

  s = pthread_join( tA, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(A)\n");

  s = pthread_join( tB, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(B)\n");

  get_time_ms();
  fprintf( stderr, "main: this is the end\n");

  s = sem_unlink("/demo");
  if(s != 0)
    fprintf(stderr, "main: fail on sem_unlink\n" );
}

<for-mutex-example>
#include <stdio.h>
#include <time.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

	unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;
static unsigned int loopA;
static unsigned int loopB;

static void* threadFuncA( void *arg )
{
  int s = 0;

  for(loopA = 0; loopA < MAXLOOP; loopA++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "TA: pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "TA: pthread_mutex_unlock");
  }
}

static void* threadFuncB( void *arg )
{
  int s = 0;

  for(loopB = 0; loopB < MAXLOOP; loopB++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "TB: pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "TB: pthread_mutex_unlock");
  }
}

int main()
{
  pthread_t tA, tB;
  int s;

  fprintf( stderr, "main: this is the second mtx run\n");
  get_time_ms();

  s = pthread_create( &tA, NULL, threadFuncA, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(A)\n");

  s = pthread_create( &tB, NULL, threadFuncB, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(B)\n");

  s = pthread_join( tA, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(A)\n");

  s = pthread_join( tB, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(B)\n");

  get_time_ms();
  fprintf( stderr, "main: this is the end\n");
}


={============================================================================
*kt_linux_core_263*	sync: ref: lock-free code: a false sense of security

{one}
http://www.drdobbs.com/cpp/lock-free-code-a-false-sense-of-security/210600279

Lock-Free Code: A False Sense of Security By Herb Sutter, September 08, 2008

Writing lock-free code can confound anyoneâeven expert programmers, as Herb shows this month.

Given that lock-based synchronization has serious problems [1], it can be tempting to think
lock-free code must be the answer. Sometimes that is true. In particular, it's useful to have
libraries provide hash tables and other handy types whose implementations are internally
synchronized using lock-free techniques, such as Java's ConcurrentHashMap, so that we can use those
types safely from multiple threads without external synchronization and without having to understand
the subtle lock-free implementation details.
{Q} Java's ConcurrentHashMap

<two-drawbacks-for-lock-free-code>
But replacing locks wholesale by writing your own lock-free code is not the answer. Lock-free code
has two major drawbacks. First, it's not broadly useful for solving typical problems-lots of basic
data structures, even doubly linked lists, still have no known lock-free implementations. Coming up
with a new or improved lock-free data structure will still earn you at least a published paper in a
refereed journal, and sometimes a degree.

Second, it's hard even for experts. It's easy to write lock-free code that appears to work, but it's
very difficult to write lock-free code that is correct and performs well. Even good magazines and
refereed journals have published a substantial amount of lock-free code that was actually broken in
subtle ways and needed correction.

To illustrate, let's dissect some peer-reviewed lock-free code that was published here in DDJ just
two months ago [2]. The author, Petru Marginean, has graciously allowed me to dissect it here so
that we can see what's wrong and why, what lessons we should learn, and how to write the code
correctly. That someone as knowledgable as Petru, who has published many good and solid articles,
can get this stuff wrong should be warning enough that lock-free coding requires great care.
<KT> See Lock-Free Queues in below.

A Limited Lock-Free Queue

<limitation-or-assumption-of-q>
Marginean's goal was to write a limited lock-free queue that can be used safely without internal or
external locking. To simplify the problem, the article imposed some significant restrictions,
including that the queue must only be used from two threads with specific roles: one Producer thread
that inserts into the queue, and one Consumer thread that removes items from the queue.

Marginean uses a nice technique that is designed to prevent conflicts between the writer and reader:

o The producer and consumer always work in separate parts of the underlying list, so that their work
won't conflict. At any given time, the first "unconsumed" item is the one after the one iHead refers
to, and the last (most recently added) "unconsumed" item is the one before the one iTail refers to.

o The consumer increments iHead to tell the producer that it has consumed another item in the queue.

o The producer increments iTail to tell the consumer that another item is now available in the
queue. <o> Only the producer thread ever actually modifies the queue.<o> That means the producer is
responsible, not only for adding into the queue, but also for removing consumed items. To maintain
separation between the producer and consumer and prevent them from doing work in adjacent nodes, the
producer won't clean up the most recently consumed item (the one referred to by iHead).

<KT> The consumer only changes iterator and do not modify list itself.

Q     ...   <- begin()
      ...
      ...
      [ ]   <- head, -> consumer, dummy. uses tail to check empty, publish head and consume.
      [ ]   <- first unconsumed item
      ...
      [ ]   <- last unconsumed item
      [ ]   <- tail, <- producer, end(). add, publish tail and uses head to trim unused. 
               end() and push_back()

The idea is reasonable; only the implementation is fatally flawed. Here's the original code, written
in C++ and using an STL doubly linked list<T> as the underlying data structure. I've reformatted the
code slightly for presentation, and added a few comments for readability: 

// Original code from [1] (broken without external locking)
//
template <typename T>
struct LockFreeQueue {
  private:
    std::list<T> list;
    typename std::list<T>::iterator iHead, iTail;

  public:
    LockFreeQueue() {
      list.push_back(T());        // add dummy separator
      iHead = list.begin();
      iTail = list.end();
    }

    // Produce is called on the producer thread only:

    void Produce(const T& t) {
      list.push_back(t);               // add the new item
      iTail = list.end();              // <publish> it
      list.erase(list.begin(), iHead); // trim unused nodes
    }

    // Consume is called on the consumer thread only:

    bool Consume(T& t) {
      typename std::list<T>::iterator iNext = iHead;
      ++iNext;
      if (iNext != iTail) {         // if queue is nonempty
        iHead = iNext;              // <publish> that we took an item
        t = *iHead;                 // copy it back to the caller
        return true;                // and report success
      }
      return false;                 // else report queue was empty
    }
};

<lock-free-variable> <two-key-property>
The fundamental reason that the code is broken is that it has race conditions on both would-be
lock-free variables, iHead and iTail. To avoid a race, a lock-free variable must have two key
properties that we need to watch for and guarantee: atomicity and ordering. These variables are
neither.

Atomicity

First, reads and writes of a lock-free variable must be atomic. For this reason, lock-free variables
are typically no larger than the machine's native word size, and are usually pointers (C++), object
references (Java, .NET), or integers. Trying to use an ordinary list<T>::iterator variable as a
lock-free shared variable isn't a good idea and can't reliably meet the atomicity requirement, as we
will see.

Let's consider the races on iHead and iTail in these lines from Produce and Consume:

void Produce(const T& t) {
  ...
  iTail = list.end();
  list.erase(list.begin(), iHead);
}
 
bool Consume(T& t) {
  ...
  if (iNext != iTail) {
    iHead = iNext;
  ...   
  }
}

If reads and writes of iHead and iTail are not atomic, then Produce could read a partly updated (and
therefore corrupt) iHead and try to dereference it, and Consume could read a corrupt iTail and
fall off the end of the queue. Marginean does note this requirement:

"Reading/writing list<T>::iterator is atomic on the machine upon which you run the application." [2]

Alas, atomicity is necessary but not sufficient (see next section), and not supported by
list<T>::iterator. First, in practice, many list<T>::iterator implementations I examined are [larger]
than the native machine/pointer size, which means that they can't be read or written with atomic
loads and stores on most architectures. Second, in practice, even if they were of an appropriate
size, you'd have to add other decorations to the variable to ensure atomicity, for example to
require that the variable be properly [aligned] in memory.

Finally, the code isn't valid ISO C++. The 1998 C++ Standard said nothing about concurrency, and so
provided no such guarantees at all. The upcoming second C++ standard that is now being finalized
C++0x, does include a memory model and thread support, and explicitly forbids it. In brief, C++0x
says that the answer to questions such as, "What do I need to do to use a list<T> mylist
thread-safely?" is "Same as any other object"âif you know that an object like mylist is shared, you
must externally synchronize access to it, including via iterators, by protecting all such uses with
locks, else you've written a race [3]. Note: Using C++0x's std::atomic<> is not an option for
list<T>::iterator, because atomic<T> requires T to be a bit-copyable type, and STL types and their
iterators aren't guaranteed to be that.

Ordering Problems in Produce

Second, reads and writes of a lock-free variable must occur in an expected order, which is nearly
always the exact order they appear in the program source code. But compilers, processors, and caches
love to optimize reads and writes, and will helpfully reorder, invent, and remove memory reads and
writes unless you prevent it from happening. 

<to-prevent-reordering>
The right prevention happens implicitly when you use mutex locks or ordered atomic variables; {Q}
why mutex prevent this? 

C++0x std::atomic, Java/.NET volatile; you can also do it explicitly, but with considerably more
effort, using ordered API calls e.g., Win32 InterlockedExchange or memory fences/barriers e.g.,
Linux mb.  Trying to write lock-free code without using any of these tools can't possibly work.

Consider again this code from Produce, and ignore that the assignment iTail isn't atomic as we look
for other problems:

list.push_back(t);  // A: add the new item
iTail = list.end(); // B: publish it

This is a classic publication race because lines A and B can be (partly or entirely) reordered. For
example, let's say that some of the writes to the T object's members are delayed until after the
write to iTail, which publishes that the new object is available; then the consumer thread can see a
partly assigned T object.

What is the minimum necessary fix? We might be tempted to write a memory barrier between the two
lines:

// Is this change enough?
list.push_back(t);  // A: add the new item
mb();               // full fence
iTail = list.end(); // B: publish it

Before reading on, think about it and see if you're convinced that this is (or isn't) right.

Have you thought about it? As a starter, here's one issue: Although list.end is probably unlikely to
perform writes, it's possible that it might, and those are side effects that need to be complete
before we publish iTail. 

The general issue is that you can't make assumptions about the side effects of library functions you
call, and you have to make sure they're fully performed before you publish the new state. 

So a slightly improved version might try to store the result of list.end into a local unshared
variable and assign it after the barrier:

// Better, but is it enough?
list.push_back(t);
tmp = list.end();
mb();                // full fence
iTail = tmp;

Unfortunately, this still isn't enough. Besides the fact that assigning to iTail isn't atomic and
that we still have a race on iTail in general, compilers and processors can also invent writes to
iTail that break this code. Let's consider write invention in the context of another problem area:
Consume.

Ordering Problems in Consume

Here's another reordering problem, this time from Consume:

++iNext;
if (iNext != iTail) {
  iHead = iNext;        // C
  t = *iHead;           // D

Note that Consume updates iHead to advertise that it has consumed another item before it actually
  reads the item's value. Is that a problem? We might think it's innocuous, because the producer
  always leaves the iHead item alone to stay at least one item away from the part of the list the
  consumer is using.

It turns out this code is broken regardless of which order we write lines C and D, because the
compiler or processor or cache can reorder either version in unfortunate ways. Consider what happens
if the consumer thread performs a consecutive two calls to Consume: The memory reads and writes
performed by those two calls could be reordered so that iHead is incremented twice before we copy
the two list nodes' values, and then we have a problem because the producer may try to remove
nodes the consumer is still using. [KT] Think that this is a problem besides odering and lock-free
variable.

Note: This doesn't mean the compiler or processor transformations are broken; they're not. Rather
the code is racy and has insufficient synchronization, and so it breaks the memory model guarantees
and makes such transformations possible and visible.

Reordering isn't the only issue. Another problem is that compilers and processors can invent writes,
so they could inject a transient value:

// Problematic compiler/processor transformation
if (iNext != iTail) {
  iHead = 0xDEADBEEF;
  iHead = iNext;
  t = *iHead;

Clearly, that would break the producer thread, which would read a bad value for iHead. More likely,
the compiler or processor might speculate that most of the time iNext != iTail:

// Another problematic transformation
//
__temp = iHead;
iHead = iNext;  // speculatively set to iNext

if (iNext == iTail) {   // note: inverted test!
  iHead = __temp;   // undo if we guessed wrong
} else {
  t = *iHead;

<invariant>
But now iHead could equal iTail, which breaks the essential invariant that iHead must never equal
iTail, on which the whole design depends.

Can we solve these problems by writing line D before C, then separating them with a full fence? Not
entirely: That will prevent most of the aforementioned optimizations, but it will not eliminate all
of the problematic invented writes. More is needed.

Next Steps

These are a sample of the concurrency problems in the original code. Marginean showed a good
algorithm, but the implementation is broken because it uses an inappropriate type and performs
insufficient synchronization/ordering. Fixing the code will require a rewrite, because we need to
change the data structure and the code to let us use proper ordered atomic lock-free variables. But
how? Next month, we'll consider a fixed version. Stay tuned.

Notes

[1] H. Sutter, "The Trouble With Locks," C/C++ Users Journal, March 2005.
(www.ddj.com/cpp/184401930).

[2] P. Marginean, "Lock-Free Queues," Dr. Dobb's Journal, July 2008. (www.ddj.com/208801974).

[3] B. Dawes, et al., "Thread-Safety in the Standard Library," ISO/IEC JTC1/SC22/WG21 N2669, June
2008. (www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2669.htm).


{two}
http://www.drdobbs.com/parallel/lock-free-queues/208801974?pgno=1
Lock-Free Queues By Petru Marginean, July 01, 2008

<note-begin>
This article as written assumes a sequentially consistent model. In particular, the code relies on
specific order of instructions in both Consumer and Producer methods. However, without inserting
proper memory barrier instructions, these instructions can be reordered with unpredictable results
(see, for example, the classic Double-Checked Locking problem).

Another issue is using the standard std::list<T>. While the article mentions that it is the
developer responsibility to check that the reading/writing std::list<T>::iterator is atomic, this
turns out to be too restrictive. While gcc/MSVC++2003 has 4-byte iterators, the MSVC++2005 has
8-byte iterators in Release Mode and 12-byte iterators in the Debug Mode.

The solution to prevent this is to use memory barriers/volatile variables. 
The downloadable code featured at the top of this article has fixed that issue. ~

Many thanks to Herb Sutter who signaled the issue and helped me fix the code. --P.M.
<note-end>

Queues can be useful in a variety of systems involving data-stream processing. Typically, you have a
data source producing dataârequests coming to a web server, market feeds, or digital telephony
packetsâat a variable pace, and you need to process the data as fast as possible so there are no
losses. To do this, you can push data into a queue using one thread and process it using a different
threadâa good utilization of resources on multicore processors. One thread inserts data into the
queue, and the other reads/deletes elements from the queue. Your main requirement is that a
high-rate data burst does not last longer than the system's ability to accumulate data while the
consumer thread handles it. [KT] This means no overlow? 

The queue you use has to be threadsafe to prevent race conditions when inserting/removing data from
multiple threads. For obvious reasons, it is necessary that the queue mutual exclusion mechanism add
as little overhead as possible.

In this article, I present a lock-free queue (the source code for the lockfreequeue class is
available online; see www.ddj.com/code/) in which one thread can write to the queue and another
read from itâat the same time without any locking. /0807/lockfreequeue/

To do this, the code implements these requirements:

<conditions>
o There is a single writer (Producer) and single reader (Consumer). When you have multiple producers
and consumers, you can still use this queue with some external locking. You cannot have multiple
producers writing at the same time (or multiple consumers consuming the data simultaneously), but
you can have one producer and one consumer (2x threads) accessing the queue at the same time
(Responsibility: developer).

o When inserting/erasing to/from an std::list<T>, the iterators for the existing elements must
remain valid (Responsibility: library implementor).

o <Only> one thread modifies the queue; the producer thread both adds/erases elements in the queue
(Responsibility: library implementor).

o Beside the underlying std::list<T> used as the container, the lock-free queue class also holds two
iterators pointing to the not-yet-consumed range of elements; each is modified by one thread and
read by the other (Responsibility: library implementor).

o Reading/writing list<T>::iterator is <atomic> on the machine upon which you run the application. If
they are not on your implementation of STL, you should check whether the raw pointer's operations
are atomic. You could easily replace the iterators to be mentioned shortly with raw pointers in the
code (Responsibility: machine). <KT> ?

<KT> Here big condition is that one producer and consumer model, only producer modify a list, and
updating iterators between threads are atomic.

Because I use Standard C++, the code is portable under the aforementioned "machine" assumption: 

template <typename T>
struct LockFreeQueue
{
    LockFreeQueue();
    void Produce(const T& t);
    bool Consume(T& t);

  private:
    typedef std::list<T> TList;
    TList list;
    typename TList::iterator iHead, iTail;   <KT> why not 'TList::iterator iHead'?
};

Considering how simple this code is, you might wonder how can it be threadsafe. The magic is due to
design, not implementation. Take a look at the implementation of the Produce() and Consume()
methods. The Produce() method looks like this: 

void Produce(const T& t)
{
  list.push_back(t);
  iTail = list.end();
  list.erase(list.begin(), iHead);
}

To understand how this works, mentally separate the data from LockFreeQueue<T> into two groups:

o The list and the iTail iterator, modified by the Produce() method (Producer thread).
o The iHead iterator, modified by the Consume() method (Consumer thread). 

<o>
Produce() is the [only] method that changes the list (adding new elements and erasing the consumed
elements), and it is essential that [only] one thread ever calls Produce()âit's the Producer
thread! The iterator (iTail) (only manipulated by the Producer thread) changes it only after a new
element is added to the list. This way, when the Consumer thread is reading the iTail element, the
new added element is ready to be used. The Consume() method tries to read all the elements between
iHead and iTail (excluding both ends). 
<o>

bool Consume(T& t)
{
  typename TList::iterator iNext = iHead;
  ++iNext;
  if (iNext != iTail)
  {
    iHead = iNext;
    t = *iHead;
    return true;
  }
  return false;
}

This method reads the elements, but doesn't remove them from the list. Nor does it access the list
directly, but through the iterators. They are guaranteed to be valid after std::list<T> is modified,
so no matter what the Producer thread does to the list, you are safe to use them.

The std::list<T> maintains an element (pointed to by iHead) that is considered already read. For
this algorithm to work even when the queue was just created, I add an empty T() element in the
constructor of the LockFreeQueue<T> (see Figure 1): 

LockFreeQueue()
{
  list.push_back(T());
  iHead = list.begin();
  iTail = list.end();
}

<discussion-when-queue-is-empty>
Consume() may fail to read an element (and return false). Unlike traditional lock-based queues, this
queue works fast when the queue is not empty, but needs an external locking or polling method to
wait for data. Sometimes you want to wait if there is no element available in the queue, and avoid
returning false. A naive approach to waiting is: 

T Consume()
{
  T tmp;
  while (!Consume(tmp))
    ;
  return tmp;
}

This Consume() method will likely heat up one of your CPUs red-hot to 100-percent use if there are
no elements in the queue. Nevertheless, this should have good performance when the queue is not
empty. However, if you think of it, a queue that's almost never empty is a sign of systemic trouble:
It means the consumer is unable to keep pace with the producer, and sooner or later, the system is
doomed to die of memory exhaustion. Call this approach NAIVE_POLLING. 

A friendlier Consume() function does some pooling and calls some sort of sleep() or yield() function
available on your system: 

T Consume(int wait_time = 1/*milliseconds*/)
{
  T tmp;
  while (!Consume(tmp))
  {
    Sleep(wait_time/*milliseconds*/);
  }
  return tmp;
}

The DoSleep() can be implemented using nanosleep() (POSIX) or Sleep() (Windows), or even better,
using boost::thread::sleep(), which abstracts away system-dependent nomenclature. Call this
approach SLEEP. Instead of simple polling, you can use more advanced techniques to signal the
Consumer thread that a new element is available. I illustrate this in Listing One using a
boost::condition variable.

#include <boost/thread.hpp>
#include <boost/thread/condition.hpp>
#include <boost/thread/xtime.hpp>
    
template <typename T>
struct WaitFreeQueue
{
    void Produce(const T& t)
    {
        queue.Produce(t);
        cond.notify_one();
    }
    bool Consume(T& t)
    {
        return queue.Consume(t);
    }
    T Consume(int wait_time = 1/*milliseconds*/)
    {
        T tmp;
        if (Consume(tmp))
            return tmp;

        // the queue is empty, try again (possible waiting...)
        boost::mutex::scoped_lock lock(mtx);
        while (!Consume(tmp))          // line A
        {
            boost::xtime t;
            boost::xtime_get(&t, boost::TIME_UTC);
            AddMilliseconds(t, wait_time);
            cond.timed_wait(lock, t);  // line B
        }
        return tmp;
    }
private:
    LockFreeQueue<T> queue;
    boost::condition cond;
    boost::mutex mtx;
};

I used the timed_wait() instead of the simpler wait() to solve a possible deadlock when Produce() is
called between line A and line B in Listing One. Then wait() will miss the notify_one() call and
have to wait for the next produced element to wake up. <If> this element never comes (no more produced
elements or if the Produce() call actually waits for Consume() to return), there's a deadlock.
Call this approach TIME_WAIT.

The lock is still wait-free as long as there are elements in the queue. In this case, the Consumer()
thread does no waiting and reads data as fast as possible (even with the Producer() that is
inserting new elements). Only when the queue is exhausted does locking occur. 
<KT> claims that it is lock-free as long as queue is not empty.

The Ping-Pong Test

To compare the three approaches (NAIVE_POLLING, SLEEP, and TIME_WAIT), I implemented a test called
"Ping-Pong" that is similar to the game of table tennis (the source code is available online). In
Figure 2, there are two identical queues between the threads T1 and T2. 

      queue 1
T1 ------------> T2
   <------------
      queue 2

You first load one of the queues with a number of "balls," then ask each thread to read from one
queue and write to the other. The result is a controlled infinite loop. By limiting the game to a
fixed number of reads/writes ("shots"), you get an understanding of how the queue behaves when
varying the waiting/sleep time and strategy and the number of "balls." The faster the game, the
better the performance. You should also check CPU usage to see how much of it is used for real work.

o "No ball" means "do nothing" (like two players waiting for the other to start). This gives you an
idea of how good the queues are when there is no dataâhow nervous the players are. Ideally, CPU
usage should be zero.
 
o "One ball" is like the real ping-pong game: Each player shoots and waits for the other to shoot.

o "Two (or more) balls" means both players could shoot at the same time, modulo collision and
waiting issues.


In a wait-free system, the more balls in the game, the better the performance gain compared to the
classic locking strategy. This is because wait-free is an optimistic concurrency control method
(works best when there is no contention), while classical lock-based concurrency control is
pessimistic (assumes contention happens and preemptively inserts locking).

Ready to play? Here is the Ping-Pong test command line:

$> ./pingpong [strategy] [timeout] [balls] [shots]

When you run the program, the tests show the results in the table shown in Figure 3:

o The best combination is the timed_wait() with a small wait time (1ms in the test for TIMED_WAIT).
It has a very fast response time and almost 0 percent CPU usage when the queue is empty.

o Even when the sleep time is 0 (usleep(0)), the worst seems to be the sleep() method, especially
when the queue is likely to be empty. (The number of shots in this case is 100-times smaller than
the other cases because of the long duration of the game.)

o The NO_WAIT strategy is [fast] but behaves worst when there are no balls (100-percent CPU usage to
do nothing). It has the best performance when there is a single ball.

Figure 4 presents a table with the results for a classic approach (see SafeQueue). These results
show that this queue is, on average, more than four-times slower than the LockFreeQueue. The
slowdown comes from the synchronization between threads. Both Produce() and Consume() have to wait
for each other to finish. CPU usage is almost 100 percent for this test (similar to the NO_WAIT
strategy, but not even close to its performance).

Final Considerations

The single-threaded code below shows the value of the list.size() when Producing/ Consuming
elements: 

LockFreeQueue<int> q;   // list.size() == 1
q.Produce(1);           // list.size() == 2
int i;
q.Consume(i);           // list.size() == still 2!;
                        // Consume() doesn't modify the list
q.Produce(i);           // list.size() == 2 again;

The size of the queue is 1 if Produce() was never called and greater than 1 if any element was
produced.

No matter how many times Consume() is called, the list's size will stay constant. It is Produce()
that is increasing the size (by 1); and if there were consumed elements, it will also delete them
from the queue. In a way, Produce() acts as a simple garbage collector. <o> The whole thread safety
comes from the fact that specific data is modified from single threads only. The synchronization
between threads is done using iterators (or pointers, whichever has atomic read/write operation on
your machine). <o> Also consider this code: 

usleep(1000);    // sleep 1 microsecond

On the face of it, this line of code makes a thread sleep for 1 microsecond, and
then continue. In reality, 1 microsecond is just a lower bound to the duration
of the call.

The man page for usleep() says, "The usleep() function suspends execution of the
calling process for (at least) usec microseconds. The sleep may be lengthened
slightly by any system activity or by the time spent processing the call or by
the granularity of system timers," or if you use the nanosleep() function.
"Therefore, nanosleep() always pauses for at least the specified time; however,
    it can take up to 10 ms longer than specified until the process becomes
    runnable again."

So if the process is not scheduled under a real-time policy, there's no
guarantee when your thread will be running again. I've done some tests and (to
        my surprise) there are situations when code such as: 

cond.timed_wait(lock, x);    // x = e.g. 1 millisecond

will actually wait for more than 1 second.


{three}
http://www.drdobbs.com/cpp/the-trouble-with-locks/184401930
The Trouble with Locks

By Herb Sutter, March 01, 2005

References

[1] Sutter, H. "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software," Dr.
Dobb's Journal, March 2005. Available online at http://www.gotw.ca/publications/concurrency-ddj.htm.

[2] Sutter, H. "The Concurrency Revolution," C/C++ Users Journal, February 2005. This is an
abbreviated version of [1].

[3] Alexandrescu, A. "Lock-Free Data Structures," C/C++ Users Journal, October 2004.

[4] Alexandrescu, A. and M. Michael. "Lock-Free Data Structures with Hazard Pointers," C/C++ Users
Journal, December 2004.

[5] http://blogs.msdn.com/cbrumme. 

Lock-based programming may not be the best approach to building large concurrent programs.

In my most recent articles [1, 2], I presented reasons why concurrency (for example, multithreading)
will be the next revolution in the way we develop software â a sea change of the same order as the
object-oriented revolution. I also stated that "the vast majority of programmers today don't grok
concurrency, just as the vast majority of programmers 15 years ago didn't yet grok objects."

In this column, I'd like to consider just one question that several people wrote to ask, namely: "Is
concurrency really that hard?" In particular, a few readers felt that lock-based programming is well
understood; it is, after all, the status quo mainstream solution to concurrency control.

So, "is concurrency really that hard?" My short answer is this:

o Lock-based programming, our status quo, is difficult for experts to get right. Worse, it is also
fundamentally flawed for building large programs. This article focuses exclusively on lock-based
programming just because there's so much to say in even a 50,000-foot overview that I ran out of
room.

o Lock-free programming is difficult for gurus to get right. I'll save this for another time, but if
you're interested, you should check out Andrei Alexandrescu's recent articles for a sampling of the
issues in lock-free programming and hazard pointers [3, 4]. (Aside: Yes, I'm implying Andrei is a
guru. I hope he doesn't mind my outing him in public like this. I don't think it was much of a
secret.) (More aside: The hazard pointer work shows in particular why, if you're writing
lock-free data structures, you really really want garbage collection. You can do it yourself
without garbage collection, but it's like working with knives that are sharp on both edges and
don't have handles. But that's another article. Specifically, it's Andrei's other article.)

Unfortunately, today's reality is that only thoughtful experts can write explicitly concurrent
programs that are correct and efficient. This is because today's programming models for concurrency
are subtle, intricate, and fraught with pitfalls that easily (and frequently) result in unforeseen
races (i.e., program corruption) deadlocks (i.e., program lockup) and performance cliffs (e.g.,
priority inversion, convoying, and sometimes complete loss of parallelism and/or even worse
performance than a single-threaded program). And even when a correct and efficient concurrent
program is written, it takes great care to maintain â it's usually brittle and difficult to maintain
correctly because current programming models set a very high bar of expertise required to reason
reliably about the operation of concurrent programs, so that apparently innocuous changes to a
working concurrent program can (and commonly do, in practice) render it entirely or intermittently
nonworking in unintended and unexpected ways. Because getting it right and keeping it right is so
difficult, at many major software companies there is a veritable priesthood of gurus who write and
maintain the core concurrent code.

Some people think I'm overstating this, so let me amplify. In this article, I'll focus on just the
narrow question of how to write a lock-based program correctly, meaning that it works (avoids data
corruption) and doesn't hang (avoids deadlock and livelock). That's pretty much the minimum
requirement to write a program that runs at all.

Question: Is the following code thread-safe? If it is, why is it safe? If it isn't always, under
what conditions is it thread-safe?

T Add( T& a, T& b ) {
  T result;
  // ... read a and b and set result to
  // proper values ...
  return result;
}

There are a lot of possibilities here. Let's consider some of the major ones.

Lock-Based Solutions?

Assume that reading a T object isn't an atomic operation. Then, if a and/or b are accessible from
another thread, we have a classic race condition: While we are reading the values of a and/or b,
some other thread might be changing those objects, resulting in blowing up the program; if you're
lucky, say, by causing the object to follow an internal pointer some other thread just deleted; or
reading corrupt values.

How would you solve that? Please stop and think about it before reading on...

Ready? Okay: Now please stop a little longer and think about your solution some more, and consider
whether it might have any further holes, before reading on...

Now that you've thought about it deeply, let's consider some alternatives.

Today's typical lock-based approach is to acquire locks so that uses of a and b on one thread won't
interleave. Typically, this is done by acquiring a lock on an explicit synchronization object [a
mutex, for instance] that covers both objects, or by acquiring locks on an implicit mutex associated
with the objects themselves. To acquire a lock that covers both objects, Add has to know what that
lock is, either because a and b and their lock are globals [but then why pass a and b as
parameters?] or because the caller acquires the lock outside of Add [which is usually preferable].
To acquire a lock on each object individually, we could write:

T SharedAdd( T& a, T& b ) {
  T result;
  lock locka( a );  // lock is a helper
  // whose constructor acquires a lock
  lock lockb( b );  // and whose
  // destructor releases the lock
  // ... read a and b and set result to
  //  proper values ...
  return result;
} // release the locks 


{four}
http://www.drdobbs.com/parallel/writing-lock-free-code-a-corrected-queue/210604448?pgno=1
Writing Lock-Free Code: A Corrected Queue

By Herb Sutter, September 29, 2008

Notes

[1] H. Sutter. âLock-Free Code: A False Sense of Securityâ (DDJ, September 2008).
(www.ddj.com/cpp/210600279).

[2] P. Marginean. "Lock-Free Queues" (DDJ, July 2008). (www.ddj.com/208801974).

[3] This is just like a canonical exception safety patternâdo all the work off to the side, then
commit to accept the new state using nonthrowing operations only. "Think in transactions" applies
everywhere, and should be ubiquitous in the way we write our code.

[4] Compare-and-swap (CAS) is the most widely available fundamental lock-free operation and so I'll
focus on it here. However, some systems instead provide the equivalently powerful
load-linked/store-conditional (LL/SC) instead.


As we saw last month [1], lock-free coding is hard even for experts. There, I dissected a published
lock-free queue implementation [2] and examined why the code was quite broken. This month, let's see
how to do it right.

Lock-Free Fundamentals

When writing lock-free code, always keep these essentials well in mind:

Key concepts. Think in transactions. Know who owns what data. Key tool. The ordered atomic variable. 

When writing a lock-free data structure, "to think in transactions" means to make sure that each
operation on the data structure is atomic, all-or-nothing with respect to other concurrent
operations on that same data. The typical coding pattern to use is to do work off to the side, then
"publish" each change to the shared data with a single atomic write or compare-and-swap. [3] Be sure
that concurrent writers don't interfere with each other or with concurrent readers, and pay special
attention to any operations that delete or remove data that a concurrent operation might still be
using.

Be highly aware of who owns what data at any given time; mistakes mean races where two threads think
they can proceed with conflicting work. You know who owns a given piece of shared data right now by
looking at the value of the ordered atomic variable that says who it is. To hand off ownership of
some data to another thread, do it at the end of a transaction with a single atomic operation that
means "now it's your's."

An ordered atomic variable is a "lock-free-safe" variable with the following properties that make it
safe to read and write across threads without any explicit locking:

Atomicity. Each individual read and write is guaranteed to be atomic with respect to all other reads
and writes of that variable. The variables typically fit into the machine's native word size, and so
are usually pointers (C++), object references (Java, .NET), or integers. Order. Each read and write
is guaranteed to be executed in source code order. Compilers, CPUs, and caches will respect it and
not try to optimize these operations the way they routinely distort reads and writes of ordinary
variables. Compare-and-swap (CAS) [4]. There is a special operation you can call using a syntax like
variable.compare_exchange( expectedValue, newValue ) that does the following as an atomic operation:
If variable currently has the value expectedValue, it sets the value to newValue and returns true;
else returns false. A common use is if(variable.compare_exchange(x,y)), which you should get in the
habit of reading as, "if I'm the one who gets to change variable from x to y." 

Ordered atomic variables are spelled in different ways on popular platforms and environments. For
example:

volatile in C#/.NET, as in volatile int. volatile or * Atomic* in Java, as in volatile int,
AtomicInteger. atomic<T> in C++0x, the forthcoming ISO C++ Standard, as in atomic<int>. 

In the code that follows, I'm going to highlight the key reads and writes of such a variable; these
variables should leap out of the screen at you, and you should get used to being very aware of every
time you touch one.

If you don't yet have ordered atomic variables yet on your language and platform, you can emulate
them by using ordinary but aligned variables whose reads and writes are guaranteed to be naturally
atomic, and enforce ordering by using either platform-specific ordered API calls (such as Win32's
InterlockedCompareExchange for compare-and-swap) or platform-specific explicit memory
fences/barriers (for example, Linux mb).


={============================================================================
*kt_linux_core_264*	conc: ref: the free lunch is over 
http://www.gotw.ca/publications/concurrency-ddj.htm

The Free Lunch Is Over
A Fundamental Turn Toward Concurrency in Software

By Herb Sutter

The biggest sea change in software development since the OO revolution is knocking at the door, and
its name is Concurrency.

This article appeared in Dr. Dobb's Journal, 30(3), March 2005. A much briefer version under the
title "The Concurrency Revolution" appeared in C/C++ Users Journal, 23(2), February 2005.

Update note: The CPU trends graph last updated August 2009 to include current data and show the
trend continues as predicted. The rest of this article including all text is still original as first
posted here in December 2004.

Your free lunch will soon be over. What can you do about it? What are you doing about it?

The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have
run out of room with most of their traditional approaches to boosting CPU performance. Instead of
driving clock speeds and straight-line instruction throughput ever higher, they are instead turning
en masse to hyperthreading and multicore architectures. Both of these features are already available
on chips today; in particular, multicore is available on current PowerPC and Sparc IV processors,
   and is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004 In-Stat/MDR Fall
   Processor Forum was multicore devices, as many companies showed new or updated multicore
   processors. Looking back, itâs not much of a stretch to call 2004 the year of multicore.

And that puts us at a fundamental turning point in software development, at least for the next few
years and for applications targeting general-purpose desktop computers and low-end servers (which
    happens to account for the vast bulk of the dollar value of software sold today). In this
article, Iâll describe the changing face of hardware, why it suddenly does matter to software, and
how specifically the concurrency revolution matters to you and is going to change the way you will
likely be writing software in the future.

Arguably, the free lunch has already been over for a year or two, only weâre just now noticing.  

<The Free Performance Lunch>

Thereâs an interesting phenomenon thatâs known as âAndy giveth, and Bill taketh away.â No matter how
fast processors get, software consistently finds new ways to eat up the extra speed. Make a CPU ten
times as fast, and software will usually find ten times as much to do (or, in some cases, will feel
    at liberty to do it ten times less efficiently). Most classes of applications have enjoyed free
and regular performance gains for several decades, even without releasing new versions or doing
anything special, because the CPU manufacturers (primarily) and memory and disk manufacturers
(secondarily) have reliably enabled ever-newer and ever-faster mainstream systems. Clock speed isnât
the only measure of performance, or even necessarily a good one, but itâs an instructive one: Weâre
used to seeing 500MHz CPUs give way to 1GHz CPUs give way to 2GHz CPUs, and so on. Today weâre in
the 3GHz range on mainstream computers.

The key question is: When will it end? After all, Mooreâs Law predicts exponential growth, and
clearly exponential growth canât continue forever before we reach hard physical limits; light isnât
getting any faster. The growth must eventually slow down and even end. (Caveat: Yes, Mooreâs Law
    applies principally to transistor densities, but the same kind of exponential growth has
    occurred in related areas such as clock speeds. Thereâs even faster growth in other spaces, most
    notably the data storage explosion, but that important trend belongs in a different article.)

If youâre a software developer, chances are that you have already been riding the âfree lunchâ wave
of desktop computer performance. Is your applicationâs performance borderline for some local
operations? âNot to worry,â the conventional (if suspect) wisdom goes; âtomorrowâs processors will
have even more throughput, and anyway todayâs applications are increasingly throttled by factors
other than CPU throughput and memory speed (e.g., theyâre often I/O-bound, network-bound,
    database-bound).â Right?

Right enough, in the past. But dead wrong for the foreseeable future.

The good news is that processors are going to continue to become more powerful. The bad news is
that, at least in the short term, the growth will come mostly in directions that do not take most
current applications along for their customary free ride.

Over the past 30 years, CPU designers have achieved performance gains in three main areas, the first
two of which focus on straight-line execution flow:

clock speed
execution optimization
cache

Increasing clock speed is about getting more cycles. Running the CPU faster more or less directly
means doing the same work faster.

Optimizing execution flow is about doing more work per cycle. Todayâs CPUs sport some more powerful
instructions, and they perform optimizations that range from the pedestrian to the exotic, including
pipelining, branch prediction, executing multiple instructions in the same clock cycle(s), and even
reordering the instruction stream for out-of-order execution. These techniques are all designed to
make the instructions flow better and/or execute faster, and to squeeze the most work out of each
clock cycle by reducing latency and maximizing the work accomplished per clock cycle.

Chip designers are under so much pressure to deliver ever-faster CPUs that theyâll risk changing the
meaning of your program, and possibly break it, in order to make it run faster

Brief aside on instruction reordering and memory models: Note that some of what I just called
âoptimizationsâ are actually far more than optimizations, in that they can change the meaning of
programs and cause visible effects that can break reasonable programmer expectations. This is
significant. CPU designers are generally sane and well-adjusted folks who normally wouldnât hurt a
fly, and wouldnât think of hurting your codeâ¦ normally. But in recent years they have been willing
to pursue aggressive optimizations just to wring yet more speed out of each cycle, even knowing full
well that these aggressive rearrangements could endanger the semantics of your code. Is this Mr.
Hyde making an appearance? Not at all. That willingness is simply a clear indicator of the extreme
pressure the chip designers face to deliver ever-faster CPUs; theyâre under so much pressure that
theyâll risk changing the meaning of your program, and possibly break it, in order to make it run
faster. Two noteworthy examples in this respect are write reordering and read reordering: Allowing a
processor to reorder write operations has consequences that are so surprising, and break so many
programmer expectations, that the feature generally has to be turned off because itâs too difficult
for programmers to reason correctly about the meaning of their programs in the presence of arbitrary
write reordering. Reordering read operations can also yield surprising visible effects, but that is
more commonly left enabled anyway because it isnât quite as hard on programmers, and the demands for
performance cause designers of operating systems and operating environments to compromise and choose
models that place a greater burden on programmers because that is viewed as a lesser evil than
giving up the optimization opportunities.

Finally, increasing the size of on-chip cache is about staying away from RAM. Main memory continues
to be so much slower than the CPU that it makes sense to put the data closer to the processorâand
you canât get much closer than being right on the die. On-die cache sizes have soared, and today
most major chip vendors will sell you CPUs that have 2MB and more of on-board L2 cache. (Of these
    three major historical approaches to boosting CPU performance, increasing cache is the only one
    that will continue in the near term. Iâll talk a little more about the importance of cache later
    on.)

Okay. So what does this mean?

A fundamentally important thing to recognize about this list is that all of these areas are
concurrency-agnostic. Speedups in any of these areas will directly lead to speedups in sequential
(nonparallel, single-threaded, single-process) applications, as well as applications that do make
use of concurrency. Thatâs important, because the vast majority of todayâs applications are
single-threaded, for good reasons that Iâll get into further below.

Of course, compilers have had to keep up; sometimes you need to recompile your application, and
target a specific minimum level of CPU, in order to benefit from new instructions (e.g., MMX, SSE)
  and some new CPU features and characteristics. But, by and large, even old applications have
  always run significantly fasterâeven without being recompiled to take advantage of all the new
  instructions and features offered by the latest CPUs.

That world was a nice place to be. Unfortunately, it has already disappeared.

<Obstacles, and Why You Donât Have 10GHz Today>

CPU performance growth as we have known it hit a wall two years ago. Most people have only recently
started to notice.

You can get similar graphs for other chips, but Iâm going to use Intel data here. Figure 1 graphs
the history of Intel chip introductions by clock speed and number of transistors. The number of
transistors continues to climb, at least for now. Clock speed, however, is a different story.

Around the beginning of 2003, youâll note a disturbing sharp turn in the previous trend toward
ever-faster CPU clock speeds. Iâve added lines to show the limit trends in maximum clock speed;
instead of continuing on the previous path, as indicated by the thin dotted line, there is a sharp
flattening. It has become harder and harder to exploit higher clock speeds due to not just one but
several physical issues, notably heat (too much of it and too hard to dissipate), power consumption
(too high), and current leakage problems.

Quick: Whatâs the clock speed on the CPU(s) in your current workstation? Are you running at 10GHz?
On Intel chips, we reached 2GHz a long time ago (August 2001), and according to CPU trends before
2003, now in early 2005 we should have the first 10GHz Pentium-family chips. A quick look around
shows that, well, actually, we donât. Whatâs more, such chips are not even on the horizonâwe have no
good idea at all about when we might see them appear.

Well, then, what about 4GHz? Weâre at 3.4GHz alreadyâsurely 4GHz canât be far away? Alas, even 4GHz
seems to be remote indeed. In mid-2004, as you probably know, Intel first delayed its planned
introduction of a 4GHz chip until 2005, and then in fall 2004 it officially abandoned its 4GHz plans
entirely. As of this writing, Intel is planning to ramp up a little further to 3.73GHz in early 2005
(already included in Figure 1 as the upper-right-most dot), but the clock race really is over, at
least for now; Intelâs and most processor vendorsâ future lies elsewhere as chip companies
aggressively pursue the same new multicore directions.

Weâll probably see 4GHz CPUs in our mainstream desktop machines someday, but it wonât be in 2005.
Sure, Intel has samples of their chips running at even higher speeds in the labâbut only by heroic
efforts, such as attaching hideously impractical quantities of cooling equipment. You wonât have
that kind of cooling hardware in your office any day soon, let alone on your lap while computing on
the plane.

<Myths and Realities: 2 x 3GHz < 6 GHz>

So a dual-core CPU that combines two 3GHz cores practically offers 6GHz of processing power. Right?

Wrong. Even having two threads running on two physical processors doesnât mean getting two times the
performance. Similarly, most multi-threaded applications wonât run twice as fast on a dual-core box.
They should run faster than on a single-core CPU; the performance gain just isnât linear, thatâs
all.

Why not? First, there is coordination overhead between the cores to ensure cache coherency (a
    consistent view of cache, and of main memory) and to perform other handshaking. Today, a two- or
four-processor machine isnât really two or four times as fast as a single CPU even for
multi-threaded applications. The problem remains essentially the same even when the CPUs in question
sit on the same die.

Second, unless the two cores are running different processes, or different threads of a single
process that are well-written to run independently and almost never wait for each other, they wonât
be well utilized. (Despite this, I will speculate that todayâs single-threaded applications as
    actually used in the field could actually see a performance boost for most users by going to a
    dual-core chip, not because the extra core is actually doing anything useful, but because it is
    running the adware and spyware that infest many usersâ systems and are otherwise slowing down
    the single CPU that user has today. I leave it up to you to decide whether adding a CPU to run
    your spyware is the best solution to that problem.)

If youâre running a single-threaded application, then the application can only make use of one core.
There should be some speedup as the operating system and the application can run on separate cores,
      but typically the OS isnât going to be maxing out the CPU anyway so one of the cores will be
      mostly idle. (Again, the spyware can share the OSâs core most of the time.) 
  

<TANSTAAFL: Mooreâs Law and the Next Generation(s)>

âThere ainât no such thing as a free lunch.â âR. A. Heinlein, The Moon Is a Harsh Mistress

Does this mean Mooreâs Law is over? Interestingly, the answer in general seems to be no. Of course,
     like all exponential progressions, Mooreâs Law must end someday, but it does not seem to be in
     danger for a few more years yet. Despite the wall that chip engineers have hit in juicing up
     raw clock cycles, transistor counts continue to explode and it seems CPUs will continue to
     follow Mooreâs Law-like throughput gains for some years to come.  The key difference, which is
     the heart of this article, is that the performance gains are going to be accomplished in
     fundamentally different ways for at least the next couple of processor generations. And most
     current applications will no longer benefit from the free ride without significant redesign.

For the near-term future, meaning for the next few years, the performance gains in new chips will be
fueled by three main approaches, only one of which is the same as in the past. The near-term future
performance growth drivers are:

hyperthreading
multicore
cache

Hyperthreading is about running two or more threads in parallel inside a single CPU. Hyperthreaded
CPUs are already available today, and they do allow some instructions to run in parallel. A limiting
factor, however, is that although a hyper-threaded CPU has some extra hardware including extra
registers, it still has just one cache, one integer math unit, one FPU, and in general just one each
of most basic CPU features. Hyperthreading is sometimes cited as offering a 5% to 15% performance
boost for reasonably well-written multi-threaded applications, or even as much as 40% under ideal
conditions for carefully written multi-threaded applications. Thatâs good, but itâs hardly double,
           and it doesnât help single-threaded applications.

Multicore is about running two or more actual CPUs on one chip. Some chips, including Sparc and
PowerPC, have multicore versions available already. The initial Intel and AMD designs, both due in
2005, vary in their level of integration but are functionally similar. AMDâs seems to have some
initial performance design advantages, such as better integration of support functions on the same
die, whereas Intelâs initial entry basically just glues together two Xeons on a single die. The
performance gains should initially be about the same as having a true dual-CPU system (only the
    system will be cheaper because the motherboard doesnât have to have two sockets and associated
    âglueâ chippery), which means something less than double the speed even in the ideal case, and
just like today it will boost reasonably well-written multi-threaded applications. Not
single-threaded ones.

Finally, on-die cache sizes can be expected to continue to grow, at least in the near term. Of these
three areas, only this one will broadly benefit most existing applications. The continuing growth in
on-die cache sizes is an incredibly important and highly applicable benefit for many applications,
  simply because space is speed. Accessing main memory is expensive, and you really donât want to
  touch RAM if you can help it. On todayâs systems, a cache miss that goes out to main memory often
  costs 10 to 50 times as much getting the information from the cache; this, incidentally, continues
  to surprise people because we all think of memory as fast, and it is fast compared to disks and
  networks, but not compared to on-board cache which runs at faster speeds. If an applicationâs
  working set fits into cache, weâre golden, and if it doesnât, weâre not. That is why increased
  cache sizes will save some existing applications and breathe life into them for a few more years
  without requiring significant redesign: As existing applications manipulate more and more data,
  and as they are incrementally updated to include more code for new features, performance-sensitive
  operations need to continue to fit into cache. As the Depression-era old-timers will be quick to
  remind you, âCache is king.â

(Aside: Hereâs an anecdote to demonstrate âspace is speedâ that recently hit my compiler team. The
 compiler uses the same source base for the 32-bit and 64-bit compilers; the code is just compiled
 as either a 32-bit process or a 64-bit one. The 64-bit compiler gained a great deal of baseline
 performance by running on a 64-bit CPU, principally because the 64-bit CPU had many more registers
 to work with and had other code performance features. All well and good. But what about data? Going
 to 64 bits didnât change the size of most of the data in memory, except that of course pointers in
 particular were now twice the size they were before. As it happens, our compiler uses pointers much
 more heavily in its internal data structures than most other kinds of applications ever would.
 Because pointers were now 8 bytes instead of 4 bytes, a pure data size increase, we saw a
 significant increase in the 64-bit compilerâs working set. That bigger working set caused a
 performance penalty that almost exactly offset the code execution performance increase weâd gained
 from going to the faster processor with more registers. As of this writing, the 64-bit compiler
 runs at the same speed as the 32-bit compiler, even though the source base is the same for both and
 the 64-bit processor offers better raw processing throughput. Space is speed.)

But cache is it. Hyperthreading and multicore CPUs will have nearly no impact on most current
applications.

So what does this change in the hardware mean for the way we write software? By now youâve probably
noticed the basic answer, so letâs consider it and its consequences.

<What This Means For Software: The Next Revolution>

In the 1990s, we learned to grok objects. The revolution in mainstream software development from
structured programming to object-oriented programming was the greatest such change in the past 20
years, and arguably in the past 30 years. There have been other changes, including the most recent
(and genuinely interesting) naissance of web services, but nothing that most of us have seen during
our careers has been as fundamental and as far-reaching a change in the way we write software as the
object revolution.

Until now.

Starting today, the performance lunch isnât free any more. Sure, there will continue to be generally
applicable performance gains that everyone can pick up, thanks mainly to cache size improvements.
But if you want your application to benefit from the continued exponential throughput advances in
new processors, it will need to be a well-written concurrent (usually multithreaded) application.
And thatâs easier said than done, because not all problems are inherently parallelizable and because
concurrent programming is hard.

I can hear the howls of protest: âConcurrency? Thatâs not news! People are already writing
concurrent applications.â Thatâs true. Of a small fraction of developers.

Remember that people have been doing object-oriented programming since at least the days of Simula
in the late 1960s. But OO didnât become a revolution, and dominant in the mainstream, until the
1990s. Why then? The reason the revolution happened was primarily that our industry was driven by
requirements to write larger and larger systems that solved larger and larger problems and exploited
the greater and greater CPU and storage resources that were becoming available. OOPâs strengths in
abstraction and dependency management made it a necessity for achieving large-scale software
development that is economical, reliable, and repeatable.

Concurrency is the next major revolution in how we write software

Similarly, weâve been doing concurrent programming since those same dark ages, writing coroutines
and monitors and similar jazzy stuff. And for the past decade or so weâve witnessed incrementally
more and more programmers writing concurrent (multi-threaded, multi-process) systems. But an actual
revolution marked by a major turning point toward concurrency has been slow to materialize. Today
the vast majority of applications are single-threaded, and for good reasons that Iâll summarize in
the next section.

By the way, on the matter of hype: People have always been quick to announce âthe next software
development revolution,â usually about their own brand-new technology. Donât believe it. New
technologies are often genuinely interesting and sometimes beneficial, but the biggest revolutions
in the way we write software generally come from technologies that have already been around for some
years and have already experienced gradual growth before they transition to explosive growth. This
is necessary: You can only base a software development revolution on a technology thatâs mature
enough to build on (including having solid vendor and tool support), and it generally takes any new
software technology at least seven years before itâs solid enough to be broadly usable without
performance cliffs and other gotchas. As a result, true software development revolutions like OO
happen around technologies that have already been undergoing refinement for years, often decades.
Even in Hollywood, most genuine âovernight successesâ have really been performing for many years
before their big break.

Concurrency is the next major revolution in how we write software. Different experts still have
different opinions on whether it will be bigger than OO, but that kind of conversation is best left
to pundits. For technologists, the interesting thing is that concurrency is of the same order as OO
both in the (expected) scale of the revolution and in the complexity and learning curve of the
technology.

<Benefits and Costs of Concurrency>

There are two major reasons for which concurrency, especially multithreading, is already used in
mainstream software. The first is to logically separate naturally independent control flows; for
example, in a database replication server I designed it was natural to put each replication session
on its own thread, because each session worked completely independently of any others that might be
active (as long as they werenât working on the same database row). The second and less common reason
to write concurrent code in the past has been for performance, either to scalably take advantage of
multiple physical CPUs or to easily take advantage of latency in other parts of the application; in
my database replication server, this factor applied as well and the separate threads were able to
scale well on multiple CPUs as our server handled more and more concurrent replication sessions with
many other servers.

There are, however, real costs to concurrency. Some of the obvious costs are actually relatively
unimportant. For example, yes, locks can be expensive to acquire, but when used judiciously and
properly you gain much more from the concurrent execution than you lose on the synchronization, if
you can find a sensible way to parallelize the operation and minimize or eliminate shared state.

Perhaps the second-greatest cost of concurrency is that not all applications are amenable to
parallelization. Iâll say more about this later on.

Probably the greatest cost of concurrency is that concurrency really is hard: The programming model,
         meaning the model in the programmerâs head that he needs to reason reliably about his
         program, is much harder than it is for sequential control flow.

Everybody who learns concurrency thinks they understand it, ends up finding mysterious races they
thought werenât possible, and discovers that they didnât actually understand it yet after all. As
the developer learns to reason about concurrency, they find that usually those races can be caught
by reasonable in-house testing, and they reach a new plateau of knowledge and comfort. What usually
doesnât get caught in testing, however, except in shops that understand why and how to do real
stress testing, is those latent concurrency bugs that surface only on true multiprocessor systems,
       where the threads arenât just being switched around on a single processor but where they
       really do execute truly simultaneously and thus expose new classes of errors. This is the
       next jolt for people who thought that surely now they know how to write concurrent code: Iâve
       come across many teams whose application worked fine even under heavy and extended stress
       testing, and ran perfectly at many customer sites, until the day that a customer actually had
       a real multiprocessor machine and then deeply mysterious races and corruptions started to
       manifest intermittently. In the context of todayâs CPU landscape, then, redesigning your
       application to run multithreaded on a multicore machine is a little like learning to swim by
       jumping into the deep endâgoing straight to the least forgiving, truly parallel environment
       that is most likely to expose the things you got wrong. Even when you have a team that can
       reliably write safe concurrent code, there are other pitfalls; for example, concurrent code
       that is completely safe but isnât any faster than it was on a single-core machine, typically
       because the threads arenât independent enough and share a dependency on a single resource
       which re-serializes the programâs execution. This stuff gets pretty subtle.

The vast majority of programmers today donât grok concurrency, just as the vast majority of
programmers 15 years ago didnât yet grok objects

Just as it is a leap for a structured programmer to learn OO (whatâs an object? whatâs a virtual
    function? how should I use inheritance? and beyond the âwhatsâ and âhows,â why are the correct
    design practices actually correct?), itâs a leap of about the same magnitude for a sequential
programmer to learn concurrency (whatâs a race? whatâs a deadlock? how can it come up, and how do I
    avoid it? what constructs actually serialize the program that I thought was parallel? how is the
    message queue my friend? and beyond the âwhatsâ and âhows,â why are the correct design practices
    actually correct?).

The vast majority of programmers today donât grok concurrency, just as the vast majority of
programmers 15 years ago didnât yet grok objects. But the concurrent programming model is learnable,
            particularly if we stick to message- and lock-based programming, and once grokked it
            isnât that much harder than OO and hopefully can become just as natural. Just be ready
            and allow for the investment in training and time, for you and for your team.

(I deliberately limit the above to message- and lock-based concurrent programming models. There is
 also lock-free programming, supported most directly at the language level in Java 5 and in at least
 one popular C++ compiler. But concurrent lock-free programming is known to be very much harder for
 programmers to understand and reason about than even concurrent lock-based programming. Most of the
 time, only systems and library writers should have to understand lock-free programming, although
 virtually everybody should be able to take advantage of the lock-free systems and libraries those
 people produce. Frankly, even lock-based programming is hazardous.)

<What It Means For Us>

Okay, back to what it means for us.

1. The clear primary consequence weâve already covered is that applications will increasingly need
to be concurrent if they want to fully exploit CPU throughput gains that have now started becoming
available and will continue to materialize over the next several years. For example, Intel is
talking about someday producing 100-core chips; a single-threaded application can exploit at most
1/100 of such a chipâs potential throughput. âOh, performance doesnât matter so much, computers just
keep getting fasterâ has always been a naÃ¯ve statement to be viewed with suspicion, and for the near
future it will almost always be simply wrong.

Applications will increasingly need to be concurrent if they want to fully exploit continuing
exponential CPU throughput gains

Efficiency and performance optimization will get more, not less, important

Now, not all applications (or, more precisely, important operations of an application) are amenable
to parallelization. True, some problems, such as compilation, are almost ideally parallelizable. But
others arenât; the usual counterexample here is that just because it takes one woman nine months to
produce a baby doesnât imply that nine women could produce one baby in one month. Youâve probably
come across that analogy before. But did you notice the problem with leaving the analogy at that?
Hereâs the trick question to ask the next person who uses it on you: Can you conclude from this that
the Human Baby Problem is inherently not amenable to parallelization? Usually people relating this
analogy err in quickly concluding that it demonstrates an inherently nonparallel problem, but thatâs
actually not necessarily correct at all. It is indeed an inherently nonparallel problem if the goal
is to produce one child. It is actually an ideally parallelizable problem if the goal is to produce
many children! Knowing the real goals can make all the difference. This basic goal-oriented
principle is something to keep in mind when considering whether and how to parallelize your
software.

2. Perhaps a less obvious consequence is that applications are likely to become increasingly
CPU-bound. Of course, not every application operation will be CPU-bound, and even those that will be
affected wonât become CPU-bound overnight if they arenât already, but we seem to have reached the
end of the âapplications are increasingly I/O-bound or network-bound or database-boundâ trend,
    because performance in those areas is still improving rapidly (gigabit WiFi, anyone?) while
    traditional CPU performance-enhancing techniques have maxed out. Consider: Weâre stopping in the
    3GHz range for now. Therefore single-threaded programs are likely not to get much faster any
    more for now except for benefits from further cache size growth (which is the main good news).
    Other gains are likely to be incremental and much smaller than weâve been used to seeing in the
    past, for example as chip designers find new ways to keep pipelines full and avoid stalls, which
    are areas where the low-hanging fruit has already been harvested. The demand for new application
    features is unlikely to abate, and even more so the demand to handle vastly growing quantities
    of application data is unlikely to stop accelerating. As we continue to demand that programs do
    more, they will increasingly often find that they run out of CPU to do it unless they can code
    for concurrency.

There are two ways to deal with this sea change toward concurrency. One is to redesign your
applications for concurrency, as above. The other is to be frugal, by writing code that is more
efficient and less wasteful. This leads to the third interesting consequence:

3. Efficiency and performance optimization will get more, not less, important. Those languages that
already lend themselves to heavy optimization will find new life; those that donât will need to find
ways to compete and become more efficient and optimizable. Expect long-term increased demand for
performance-oriented languages and systems.

4. Finally, programming languages and systems will increasingly be forced to deal well with
concurrency. The Java language has included support for concurrency since its beginning, although
mistakes were made that later had to be corrected over several releases in order to do concurrent
programming more correctly and efficiently. The C++ language has long been used to write heavy-duty
multithreaded systems well, but it has no standardized support for concurrency at all (the ISO C++
    standard doesnât even mention threads, and does so intentionally), and so typically the
concurrency is of necessity accomplished by using nonportable platform-specific concurrency features
and libraries. (Itâs also often incomplete; for example, static variables must be initialized only
    once, which typically requires that the compiler wrap them with a lock, but many C++
    implementations do not generate the lock.) Finally, there are a few concurrency standards,
    including pthreads and OpenMP, and some of these support implicit as well as explicit
    parallelization. Having the compiler look at your single-threaded program and automatically
    figure out how to parallelize it implicitly is fine and dandy, but those automatic
    transformation tools are limited and donât yield nearly the gains of explicit concurrency
    control that you code yourself. The mainstream state of the art revolves around lock-based
    programming, which is subtle and hazardous. We desperately need a higher-level programming model
    for concurrency than languages offer today; I'll have more to say about that soon.

<Conclusion>

If you havenât done so already, now is the time to take a hard look at the design of your
application, determine what operations are CPU-sensitive now or are likely to become so soon, and
identify how those places could benefit from concurrency. Now is also the time for you and your team
to grok concurrent programmingâs requirements, pitfalls, styles, and idioms.

A few rare classes of applications are naturally parallelizable, but most arenât. Even when you know
exactly where youâre CPU-bound, you may well find it difficult to figure out how to parallelize
those operations; all the most reason to start thinking about it now. Implicitly parallelizing
compilers can help a little, but donât expect much; they canât do nearly as good a job of
parallelizing your sequential program as you could do by turning it into an explicitly parallel and
threaded version.

Thanks to continued cache growth and probably a few more incremental straight-line control flow
optimizations, the free lunch will continue a little while longer; but starting today the buffet
will only be serving that one entrÃ©e and that one dessert. The filet mignon of throughput gains is
still on the menu, but now it costs extraâextra development effort, extra code complexity, and extra
testing effort. The good news is that for many classes of applications the extra effort will be
worthwhile, because concurrency will let them fully exploit the continuing exponential gains in
processor throughput.


={============================================================================
*kt_linux_core_265* sync: case: subtle race

This is to make sure that callback is called exactly once when there is an event
which calls code below. Thing is that if not remove it, it will keep adding
callbacks. So if there is callabck which is already in a queue, then remove the
new. The assumption was that the same events happens more but need to call
callback once asynchronously.

{
  // this add a callback to the glib main thread which is different from the
  // current thread.
  guint task_handle = g_idle_add( video_changed_message_callback, sink);

  // this set sink->task_handle atomic variable with new tast_handle when it
  // is 0 and return TRUE when it succeed.
  if (0 != g_atomic_int_compare_and_exchange(&sink->task_handle, 0,
        task_handle))
  {
    // There is already a task waiting to run, so we can cancel this one.
    g_source_remove(temporary_change_task_handle);
  }
}

The first problem is the wrong interpretation of the return value of
g_atomic_int_compare_and_exchange(). It doesn't return the old value of the
atomic variable. It returns TRUE if exchange took place and FALSE otherwise.
Because of that, we were proceeding to g_source_remove() when the atomic
variable went from zero to a non-zero value and it returns TRUE, which is wrong. 

This cause a race:

1. If the main thread managed to execute the newly added idle callback before
the background thread reached g_source_remove(), we were getting the following
error from g_source_remove():

GLib-CRITICAL **: Source ID 3 was not found when attempting to remove it

2. Otherwise, the idle callback wasn't executed at all!

3. When tried out, saw that both g_source_remove and callback works okay.

All in all, this is racy.

Even after changing the code above to:

{
  guint task_handle = g_idle_add( video_changed_message, sink);
  if (!g_atomic_int_compare_and_exchange(&sink->task_handle, 0,
        task_handle))
  {
    // There is already a task waiting to run, so we can cancel this one.
    g_source_remove(task_handle);
  }
}


There are still a subtle race condition. Consider the following sequence of
events:

1. One instance of this particular idle callback is queued. The atomic variable
   becomes non-zero.

2. Another instance is queued. Because the atomic variable is non-zero, we go
   inside the if.

3. Another thread (the one running GLib main loop) executes both of the queued
   callbacks.

4. The thread that entered the "if" finally reached g_source_remove(), which now
   acts on a non-existing source.

It turns out we don't have to execute the callback on the main thread, as all it
does is posting a message on GstBus, which does asynchronous delivery anyway. So
remove callback and do what it does in the current function.

Q: What if still needs callback approach?


={============================================================================
*kt_linux_core_266* sync: case: sync with no lock

  obj1                    obj2

    \                   /

  // callback called by external thread
  callback_func( object* obj, ... )
  {
    // access and set obj but no lock on objs
    deq_from_q_of_callbaks();
    run_callback();
  }

  ^
  |

  callback_to_hw()
  {
    enq_to_q_of_callbacks();
  }
  
  thread1             thread2

This callback is registered to two threads with different obj pointer. When gets
  called, picks up the obj and use it. But why no locks?

Since callback thread use queue, it's serialized and no need to have lock on
objects.


# ============================================================================
#{
==============================================================================
*kt_linux_core_300*	case: own semaphore and mutex class using pthred cond var

POSIX semaphore are system calls which means expensive. Is it possible to implement semaphore without it?

{class-semaphore}

This is for linux. When count is 0, waits and there is no upper limit. Also see that use one mutex
with many condition variables for semaphores.


{util-class}

Just to provide util funcs to all instances since these are static. Also SetPriority is not used.

class CThreadSelf
{
private:
	CThreadSelf(void) {}

public:
	// Returns the ID of the current executing thread.
	'static' int Id(void)
	{ return (int)pthread_self(); }

	'static' bool SetPriority(int priority);
	{
#if defined _LINUX

		  switch (priority)
		  {
			 case CThread::PRIORITY_HIGH: // [note] class type member
				 setpriority(PRIO_PROCESS, pthread_self(), -10); // [note] man setpriority
		       // param.sched_priority = 60;
				 break;
			 case CThread::PRIORITY_NORMAL:
				 setpriority(PRIO_PROCESS, pthread_self(), 0);
		       // param.sched_priority = 50;
				 break;
			 case CThread::PRIORITY_LOW:
		       // param.sched_priority = 40;
				 setpriority(PRIO_PROCESS, pthread_self(), 10);
				 break;
			 default:
				 return false;
		  }

#elif defined _WIN32
	}
};


{semaphore} [KT] the case uses containment(composition) to have implementation.

Use init count but no max count. 0 means to wait and other values means it is okay to get. Used as a
class memeber.

{Q} why need this? code says it calls sched_yield whenever sem count reaches 16.

#define	CONFIG_MAXIMUM_YIELD_COUNTER 16
static unsigned char semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;

pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER; [KT] this is global

struct PSemaphore
{
	pthread_cond_t  cond;
	int             count;
};

class Semaphore
{
	 private:
	 	PSemaphore* m_id;

	 public:
		Semaphore() { m_id = NULL; }
		virtual ~Semaphore() { assert( FlagCreate() == false); }

		bool Create(int count) // initial count
		{
			 pthread_mutex_lock(&mtx);

			 assert( FlagCreate() == false );

			 pthread_cond_t cond = PTHREAD_COND_INITIALIZER;

			 m_id = 'new' PSemaphore;	// new and m_id is not null
			 assert( m_id != NULL );
			 
			 m_id->cond = cond;  // [KT] is it okay as it is local variable?
			 m_id->count = count;

			 pthread_mutex_unlock(&mtx);

			 return m_id != NULL;
		}

		// return true when created
		bool FlagCreate() { return m_id != NULL; }

		virtual void Destory(void)
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  pthread_cond_destroy(&m_id->cond);
			  delete m_id;

			  m_id = NULL;

			  pthread_mutex_unlock(&mtx);
		}

		void Take()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  while (m_id->count <= 0)
			  {
				  pthread_cond_wait(&m_id->cond, &mtx);
			  }

			  m_id->count--;

			  pthread_mutex_unlock(&mtx);
		}

		void Give()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  m_id->count++;

			  pthread_cond_signal(&m_id->cond);

			  pthread_mutex_unlock(&mtx);

			  if (!semCounter--) {
				  sched_yield();
				  semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;
			  }
		}

		void Try(unsigned long msec = 0)
		{
			  if (msec == (unsigned long) INFINITY)
			  {
				  Take();

				  return true;
			  }

			  pthread_mutex_lock(&TimeMutex);
			  ASSERT(FlagCreate() == true);

			  struct timeval  now;
			  struct timespec timeout;
			  int             ret = 0;
			  bool            tf;


			  if (msec == 0)
			  {
				  if (m_id->count <= 0)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }
			  else
			  {
				  while ((m_id->count <= 0) && (ret != ETIMEDOUT))
				  {
					  gettimeofday(&now, NULL);
					  timeout.tv_sec  = now.tv_sec + msec / 1000;
					  timeout.tv_nsec = now.tv_usec + msec % 1000 * 1000;

					  while (timeout.tv_nsec > 1000000)
					  {
						  timeout.tv_nsec -= 1000000;
						  timeout.tv_sec++;
					  }

					  timeout.tv_nsec *= 1000;

					  ret = pthread_cond_timedwait(&m_id->cond, &TimeMutex, &timeout);
				  }

				  if (ret == ETIMEDOUT)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }

			  pthread_mutex_unlock(&TimeMutex);

			  return tf;
		}
};


{use-of-semaphore-one}

To make sure that an user can set prio once a thread is created.

class CThread
{
   PCSemaphore m_pidSync;

   Create()
   {
      m_pidSync.Create(0);
   }

   bool PCThread::SetPriority(int priority)
   {
      ASSERT(FlagCreate() == true);

      if (m_pid == -1)
      {
         m_pidSync.Take();
      }
      ...
   }

   inline void CThreadRun(CThread* thread)
   {
      thread->m_sync[0].Take();

      thread->m_pid = pthread_self();
      thread->m_pidSync.Give();

      thread->t_Main(); [KT] while loop on event get()

      thread->m_sync[1].Give();

      return;
   }
};


{mutex} 

CDerivedA: CMutex
 - thread   - Semaphore : uses global mutex

CDerivedB: CMutex
 - thread   - Semaphore : uses global mutex

Using sync always happens in the same thread. The case uses inheritance to have implementation. This
is based on the fact that mutex is a binary semaphore. Created with 1. Used to give the derived
class the lock/unlock feature to control interface access. That is, sync feature to class objects.
If need other use of sem, then have sem as a member.

class CMutex
{
	 private:
		  PCSemaphore 	m_sem; // [note] use of semaphore
		  int				m_tid;
		  int				m_count;

	 public:
	 	  // [note] no ctor?
	 	  virtual ~Mutex() { assert( FlagCreate() == false); }

		  bool Create(void)
		  {
				assert(FlagCreate() == false);

				if (m_sem.Create(1) == false) // [KT] create a sem
				{
					return false;
				}

				m_threadId = 0;

				return true;
		  }

		  virtual void Destroy(void)
		  {
				ASSERT(FlagCreate() == true);
				m_sem.Destroy();
		  }

		  bool FlagCreate(void) { return m_sem.FlagCreate(); }

		  void Lock(void)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;
				  return;
				}

				m_sem.Take();

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

		  }

		  bool Unlock(void);
		  {
				assert(FlagCreate() == true);

				if (m_threadId != CThreadSelf::Id())
				{
					return false;
				}

				m_count--;

				if (m_count > 0)
				{
				  return true;
				}

				m_threadId = 0;

				m_sem.Give();

				return true;
		  }

		  bool Try(unsigned long msec = 0)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;

				  return true;
				}

				if (m_sem.Try(msec) == false)
				{
				  return false;
				}

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

				return true;

		  }
};	

{cqueue}

struct PTEvent
{
	friend class PCQueue;
	friend class PCTask;

private:
	void* sync;

public:
	class PCHandler* receiver; //!< The Pointer to the handler that receives the event
	unsigned long    type;     //!< The event type

	//! Parameters of an event
	/*!
	 * The union of event parameters are 8-bytes long. That is, it can carry
	 * 2 long integers, 4 short integers, or 8 characters.
	 */
	union
	{
		long  l[2];
		short s[4];
		char  c[8];
	} param;
};

class CQueue : public CMutex
{
	private:
      PCSemaphore m_sem; // [KT] used to say that events are avaiable
		PTEvent     m_event[CONFIG_QUEUE_SIZE];

	bool CQueue::Create(void)
	{
		ASSERT(FlagCreate() == false);

		if (CMutex::Create() == false)
		{ return false; }

		if (m_sem.Create(0) == false)
		{
			CMutex::Destroy();
			return false;
		}

		m_in   = 0;		// [KT] this is {queue-contiguous-implementation} in *kt_dev*
		m_out  = 0;		// in(tail), out(head), size(count)
		m_size = 0;

		return true;
	}

	bool CQueue::Put(PTEvent* event, bool sync=false, bool priority=false)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		PCSemaphore sem;

		if (sync == false)
		{
			event->sync = NULL;
		}
		else
		{
			if (sem.Create(0) == false)
			{
				return false;
			}

			event->sync = &sem;
		}

		Lock();	// CMutex::Lock();

		int size;

		size = (priority == false) ? CONFIG_EMEGENCY_QUEUE_SIZE(16) : 0;
		size += m_size;

		if (size >= CONFIG_QUEUE_SIZE(256) )
		{
			Unlock();

			if (sync == true)
			{
				sem.Destroy();
			}

			PCDebug::Print("ERROR: Event queue full");

			return false;
		}

		if (priority == true)
		{
			m_out          = (m_out + CONFIG_QUEUE_SIZE - 1) % CONFIG_QUEUE_SIZE;
			m_event[m_out] = *event;
			m_size++;
		}
		// [KT] copy in event and inc count
		else
		{
			m_event[m_in] = *event;
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
			m_size++;
		}

		m_sem.Give();

      // [KT] As see how Get() uses m_sem, use m_sem to see if event are avaiable like a count or
      // length and then if there are call Lock() to lock access to this objects. May have some
      // performance gain from this.
		
      Unlock();	

		if (sync == true)
		{
			sem.Take();
			sem.Destroy();
		}

		return true;
	}

	bool PCQueue::Get(PTEvent* event, unsigned long msec = INFINITY)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		while (true)
		{
			if (m_sem.Try(msec) == false)
			{
				return false;
			}

			Lock();

			if (m_size == 0)
			{
				Unlock();

				continue;
			}

			*event = m_event[m_out];
			m_out  = (m_out + 1) % CONFIG_QUEUE_SIZE;
			m_size--;

			Unlock();

			return true;
		}
	}

};


{when-seek-one-specific}

{Q} This suggest that there are events for all(broadcast) and for specific ones. Move all event
between [head+1 ... tail-1] after tail. why? priority?

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver)
{ }

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver, unsigned long type)
{
	ASSERT(FlagCreate() == true);

	bool done = 0;

	Lock();

	unsigned long size = m_size;

	while (size-- != 0)
	{
		if (done == false && m_event[m_out].receiver == receiver && m_event[m_out].type == type)
		{
			*event = m_event[m_out];
			m_size--;
			done = true;
		}
		else
		{
			m_event[m_in] = m_event[m_out];
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
		}

		m_out = (m_out + 1) % CONFIG_QUEUE_SIZE;
	}

	if (done)
	{
		m_sem.Try();
	}

	Unlock();

	return done;
}

This q implementation uses q per thread. The different approach is to have q that threads share.
See *kt_linux_core_014* for msg q between threads


==============================================================================
*kt_linux_core_301*  case: use of mutex and thread class

This is case study using Mutex, Queue and Thread classes.

{CThread}

'inline' void CThreadRun(CThread* thread)
{
	thread->m_sync[0].Take();  // [KT] wait until signaled

#ifdef	_LINUX
	thread->m_pid = pthread_self();
	thread->m_pidSync.Give();
#endif

	thread->t_Main();

	thread->m_sync[1].Give();  // [KT] no use

	return;
}

'static' void* _Process(void* param)
{
	CThreadRun((CThread*)param);

	return NULL;
}

class CThread
{
private:

	int         m_id;
	int			m_pid;
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];

	friend void CThreadRun(CThread* thread); // [KT] inline friend

protected:

	virtual void t_Main(void) = 0;
	/*!< This function is the thread main virtual function.  As soon as a thread starts, this
	 * function is called.  When this function returns, the thread is terminated.
	 *
	 * You have to define this function when you define a new class that inherits CThread class.
	 */

public:

	//! Configuration constants
	enum PTConfigType
	{
		CONFIG_STACK_SIZE = 4096
	};

	enum PTPriorityType
	{
		PRIORITY_HIGH,
		PRIORITY_NORMAL,
		PRIORITY_LOW,
	};

	virtual ~CThread(void) { ASSERT(FlagCreate() == false); }

	bool Create(const char* name, unsigned long stackSize = CONFIG_STACK_SIZE);
	 {
		 ASSERT(FlagCreate() == false);

		 if (m_sync[0].Create(0) == false)
		 {
			 return false;
		 }

		 if (m_sync[1].Create(0) == false)
		 {
			 m_sync[0].Destroy();
			 return false;
		 }

#elif defined _LINUX

		 pthread_attr_t attr;
		 pthread_attr_init(&attr);
		 pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
		 pthread_attr_setstacksize(&attr, stackSize);
		 //struct sched_param schedParam;
		 //schedParam.sched_priority = 50;
		 //pthread_attr_getschedparam(&attr, &schedParam);
		 //pthread_attr_setschedpolicy(&attr, SCHED_RR);

		 m_pid = -1;
		 m_pidSync.Create(0);

		 if (pthread_create((pthread_t*)&m_id, &attr, _Process, this) != 0)
		 {
			 pthread_attr_destroy(&attr);
			 
			 ASSERT(!"CThread not created");

			 m_sync[0].Destroy();
			 m_sync[1].Destroy();

			 return false;
		 }

		 pthread_attr_destroy(&attr); 
		 
		 m_sync[0].Give();   // [KT] now a thread can run
		 return true;
	 }

	//! Check if the instance was created
	bool FlagCreate(void) { return m_sync[0].FlagCreate(); }
	//! Destroy the instance
	virtual void Destroy(void);

	//! Returns the ID of the CThread.
	int Id(void)
	{
		 ASSERT(FlagCreate() == true);
		 return m_id;
	}

	//! Set the priority value for the thread
	bool SetPriority(int priority);
};

The class hiarachy is:

class CThread
{
private:
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];
};

class CQueue : public CMutex

class CTask: private CThread, public CQueue, public CHandler
{
   private:
      void t_Main(void);

   protected:
      virtual bool t_Create(void);
};

class CSIEngineBase : public PCTask

class CSIVoiceEngine : public CSIEngineBase

class CSIEngineManager


<run>

CThread:
	CThreadRun: 
      thread->t_Main();	[pure-virtual] [main-start]

CTask:
              	t_Main()                 [main-end] {template-method} in kt_dev_txt
					{
				   	t_Create();                      [virtual] [create-start]
                 	while (ExecuteEvent() == true)   [thread-loop]
						//	virtual bool ExecuteEvent
						//	{
						//		 PTEvent event;
						//		 Get(&event, m_msec); [copy-in-event]
						//		 event.receiver->OnEvent(&event);
						//	}

					  	t_Destory();
					}

CSIEngineBase:
                                                 
CSIVoiceEngine:
                                                  t_Create() [create-end]
																     SendSelfEvent( OWN_EVENT_TYPE );

<event>

CHandler:
	protected:
	virtual bool t_OnEvent(const PTEvent* event) = 0;

	public:
	inline bool OnEvent
	{
      t_OnEvent() [pure-virtual] [event-start]
	}

CTask: public CHandler
	No OnEvent which means use CHandler one

	protectd:
	bool PCTask::t_OnEvent(const PTEvent* event)
	{
		has default basic event handling
	}

	static Send( PTEvent* event, bool sync = false, bool priority = false);
	  [to-other-task]
	  if ((sync == false) || (event->receiver->Task()->Id() != PCThreadSelf::Id()))
	  {
		 return event->receiver->Task()->Put(event, sync, priority);
		 {
			  In CQueue::Put, use copy-ctor of event structure to copy it into receiver's taks
			  m_event[].

			  m_event[i] *event;
		 }
	  }
	  [self] ends up with a func call
	  event->receiver->OnEvent(event);
		   
CSIEngineBase:
   public:
      bool SendSelfEvent(int nEventType)
      {
         PTEvent evt;
         evt.receiver = this; [event-to-self]
         evt.type = nEventType;
         Send(&evt);   // CTask::
      }

  protected:
      virtual bool t_OnEvent(const PTEvent* event)
      { return true; }

		virtual bool t_OnEvent( PTEvent* ) [event-end]
      {
		   PCTask::t_OnEvent(event); {hook-operation} in {template-method}
		   t_ProcessEvent(event); [virtual] [start]
      }

CSIVoiceEngine:
   SendSelfEvent( OWN_EVENT_TYPE );  // CSIEngineBase::

   virtual bool t_ProcessEvent(event);           [end]
   {
	   handle OWN_EVENT_TYPE;
   }


<create>

CThread:
   Create:
	  pthread_create

CTask:
	public:
   Create()
	  PCQueue::Create();
	  PCHandler::Create(this);
	  CThread::Create(stackSize);

CSIEngineBase: public CTask
	public:
   Create(const char* name)
      return CTask::Create(name);					[create]

CSIVoiceEngine: public CSIEngineBase
   no Create:

// {design-note}
// This can be a application manager which creates all applications and call Create() on them. This
// is platform wide and each application can override t_Create and init their own thing without
// knowing Create() calls made from outside.

class CSIEngineManager:
{
	 private:
	 CSIEngineBase* m_pVoiceEngine;

	 static CSIEngineManager* CSIEngineManager::GetInstance(void) {factory-method}
	 {
		 if(m_EngineManagerInstance == NULL)
		 {
			 m_EngineManagerInstance = 'new' CSIEngineManager;
		 }

		 return m_EngineManagerInstance;
	 }

	 CSIEngineManager::GetEngineInstance
	 {
		  m_pVoiceEngine =  'new' CSIVoiceEngine;
		  m_pVoiceEngine->Create("SIVoiceEngine"); // [create] and use inherited implementation
	 }
}

From review, t_ prefix means a primitive operation and to be overridden and all works are using the
most derived class object. Therefore, dreived version will be used for virtuals as shown
{template-method}


==============================================================================
*kt_linux_core_302*  case: analysis of 200 and 201 case

{how-this-work}

CUserClass instance:
                                       
CUserClass {
   // Eash CTask has a Q and thread
   CTask { CQueue, CHandler : CQueue { CMutex }
      CThread
         : pthread( staic _Process )
      }
}
  
static _Process(this) will run the derived class CTask t_Main which has a message loop. So this
_Process is a template code for all threads. 

Get(&event); in which all threads use the same code for thread routine

CUserB instance:
                                       
CUserB {
   CTask {
      CThread
         : pthread( staic _Process )
      }
}

Mutex class via inheritance:
Mutex #01     Mutex #02    Mutex #03     Mutex #04    Mutex #04  
(Semaphore)   (Semaphore)  (Semaphore)   (Semaphore)  (Semaphore)

Sem member in a class:
Sepmphore #01  Sepmphore #02 ...


Regarding q, each task has a q and other task can call put to insert a mesg to receiver's q.

{Q} Eash has its own pthread cond in Semaphore but all use a single global mutex for signaling. How
about performance? Is it better solution?


==============================================================================
*kt_linux_core_303*  case: msg q between threads

This uses stl q and sems to read, write and count(length) lock:


/** Maximum length of message queue name */
#define MQ_NAME_LENGTH 5

/** Queue Magic identifier Corresponds to ASCII QuEu*/
#define MQ_MAGIC 0x51754575

typedef struct PFMMessageQueueInfo_t_
{
    char                name[ MQ_NAME_LENGTH ];
    PCSemaphore         *readsem;
    PCMutex             *writeLock;
    PCMutex             *readLock;
    uint32_t            magic;
    std::queue<SPfmMessage>* container;
} PFMMessageQueueInfo_t;


extern "C"
{
///////////////////////////////////////////////////
// PFM Queue Create
///////////////////////////////////////////////////
HPfmQueue
pfmQueueCreate(const char* name, uint32_t max_size)
{
    PFMMessageQueueInfo_t *qptr;

    // Allocate queue controll structure
    qptr = (PFMMessageQueueInfo_t *)pfmMalloc( sizeof(PFMMessageQueueInfo_t) );
    if(!qptr)
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc\n");
        return PFM_NULL_HANDLE;
    }

    // create storage container
    qptr->container = new std::queue<SPfmMessage>();
    if (qptr->container == NULL)
    {
        fprintf(stderr,
            "pfmQueueCreate failed to alloc storage\n");
        pfmFree(qptr);
        return PFM_NULL_HANDLE;
    }

    // Create & initialize Read mutex for read serialization
    qptr->readLock = new PCMutex;
    if( qptr->readLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc read mutex\n");

        delete qptr->container;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    qptr->readLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init read mutex\n");

        delete qptr->container;
        delete qptr->readLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create & initialize Write mutex for write serialization
    qptr->writeLock = new PCMutex;
    if( qptr->writeLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }
    qptr->writeLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create and initialize read semaphore
    qptr->readsem = new PCSemaphore;
    if( qptr->readsem == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc semaphore\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    //Initialize read semaphore so it will "block" on try.
    qptr->readsem->Create(0);
    if( !qptr->readsem->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc sem\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        delete qptr->readsem;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Copy semaphore name
    if( !name )
    {
        char nameTmp[] = "SEM";
        PCString::Copy( qptr->name,nameTmp,MQ_NAME_LENGTH);
    }
    else
    {
        PCString::Copy( qptr->name,name,MQ_NAME_LENGTH);
    }

    qptr->name[ (MQ_NAME_LENGTH-1) ] = (char)NULL;
    qptr->magic = MQ_MAGIC;

    return (HPfmQueue)qptr;
}

///////////////////////////////////////////////////
// PFM Queue Destroy
///////////////////////////////////////////////////
pfmerr_t
pfmQueueDestroy(HPfmQueue h)
{
    // Validate input
    if( h == PFM_NULL_HANDLE)
    {
        pfmAssert( h != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)h;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Mark queue as invalid.
    qptr->magic = 0;

    // Release Reader (if exists) & Grab read/write locks.
    // This shall prevent "use while destruct" scenario.
    // Mutexes can be destroyed while locked.
    qptr->readsem->Give();
    qptr->readLock->Lock();
    qptr->writeLock->Lock();

    if( qptr->readsem->FlagCreate() )
    {
        qptr->readsem->Destroy();
    }
    if( qptr->readLock->FlagCreate() )
    {
        qptr->readLock->Destroy();
    }
    if( qptr->writeLock->FlagCreate() )
    {
        qptr->writeLock->Destroy();
    }

    delete qptr->readsem;
    delete qptr->readLock;
    delete qptr->writeLock;

    delete qptr->container;

    pfmFree( qptr );

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Receive
///////////////////////////////////////////////////
pfmerr_t
pfmQueueReceive(HPfmQueue q, uint32_t timeout_ms, SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    unsigned long readLockTickStart;
    unsigned long readLockTickEnd;
    unsigned long tickDiff;
    uint32_t    waitTime;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Convert PFM infinite timeout to Shadwo's infinite timeout
    switch( timeout_ms )
    {
        case PFM_WAIT_NONE:
            waitTime = 0;
            break;
        case PFM_WAIT_FOREVER:
            waitTime = INFINITY;
            break;
        default:
            waitTime = timeout_ms;
            break;
    }

    // Do thread serialized reading
    readLockTickStart = PCTime::Tick();
    if( !qptr->readLock->Try(waitTime) )
    {
        return ERR_TIMEDOUT;
    }

    // If waiting for data, do the tick calculation to
    // potentially reduce wait value
    if( waitTime )
    {
        readLockTickEnd = PCTime::Tick();
        tickDiff = pfmTickDiff( readLockTickStart, readLockTickEnd );

        // Check if we have time to hang on a samephore
        if( tickDiff > waitTime )
        {
            // We have waited longer on a mutex, so wait as short as possible
            waitTime = 0;
        }
        else
        {
            waitTime -= tickDiff;
        }
    }

    // Consume semaphore as it is signalled after every insertion
    qptr->readsem->Try(waitTime);

    // Check if there's data in the queue. This should only happen
    // on wait with timeout. Should not occour in infinite timeout
    // scenario
    if (qptr->container->empty())
    {
        pfmAssert(waitTime != (uint32_t)INFINITY );
        qptr->readLock->Unlock();
        return ERR_TIMEDOUT;
    }

    // read message
    *msg = qptr->container->front();
    qptr->container->pop();

    // This should be the last possible place where
    // a dying queue could be trapped (unlock fails).
    if( !qptr->readLock->Unlock() )
    {
        return ERR_SYS;
    }

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Send
///////////////////////////////////////////////////
pfmerr_t
pfmQueueSend(HPfmQueue q, const SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    pfmerr_t res;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Grab an read Lock
    qptr->writeLock->Lock();

    qptr->container->push(*msg);

    // Inform readers that there's data avaliable.
    qptr->readsem->Give();

    //Unlock queue
    if( !qptr->writeLock->Unlock() )
    {
        // This should handle a dying queue
        res = ERR_SYS;
    }
    else
    {
        res = ERR_OK;
    }

    return res;
}


={============================================================================
*kt_linux_busy_001* busy-links
http://www.busybox.net/source.html
http://www.busybox.net/downloads/BusyBox.html
https://git.busybox.net/busybox/tree/coreutils
https://busybox.net/downloads/


={============================================================================
*kt_linux_busy_001* busy-build

see *spk-build-busybox*


={============================================================================
*kt_linux_busy_001* busy-build-mount-issue

http://lists.busybox.net/pipermail/busybox/2008-November/067975.html

// /etc/fstab
# <file system> <mount pt>     <type>   <options>         <dump> <pass>
/dev/root       /              ext2     rw,noauto         0      1

-sh-3.2# mount
rootfs on / type rootfs (rw)

-sh-3.2# mount --bind / x
mount: mounting / on x failed: Invalid argument

note:
It seems that there is a problem to do bind mount from rootfs.


={============================================================================
*kt_linux_gcc_400* gcc-doc

https://gcc.gnu.org/onlinedocs/gcc/index.html#Top


={============================================================================
*kt_linux_gcc_400* gcc-ld gcc-linker gnu-ld

https://ftp.gnu.org/old-gnu/Manuals/ld-2.9.1/html_node/ld_toc.html
ftp://ftp.gnu.org/old-gnu/Manuals/ld-2.9.1/html_mono/ld.html

<check-search> 
--verbose[=NUMBER] Display the version number for ld and list the linker
emulations supported. Display which input files can and cannot be opened. 
    
Display the `linker script` being used by the linker. If the optional NUMBER
argument > 1, plugin symbol status will also be displayed. 

<ex>
/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-ld --verbose
GNU ld version 2.17.50-brcm 20070220
  Supported emulations:
   elf32btsmip
   elf32ltsmip
   elf32btsmipn32
   elf64btsmip
   elf32ltsmipn32
   elf64ltsmip
using internal linker script:
==================================================
/* Script for -z combreloc: combine and sort reloc sections */
OUTPUT_FORMAT("elf32-tradbigmips", "elf32-tradbigmips",
	      "elf32-tradlittlemips")
OUTPUT_ARCH(mips)
ENTRY(__start)
SEARCH_DIR("/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/mips-linux-uclibc/lib");
SECTIONS
{ ...  }


GNU ld (GNU Binutils) 2.25.1
  Supported emulations:
   elf32btsmip
   elf32ltsmip
   elf32btsmipn32
   elf64btsmip
   elf32ltsmipn32
   elf64ltsmip
using internal linker script:
==================================================
/* Script for -z combreloc: combine and sort reloc sections */
/* Copyright (C) 2014 Free Software Foundation, Inc.
   Copying and distribution of this script, with or without modification,
   are permitted in any medium without royalty provided the copyright
   notice and this notice are preserved.  */
OUTPUT_FORMAT("elf32-tradbigmips", "elf32-tradbigmips",
              "elf32-tradlittlemips")
OUTPUT_ARCH(mips)
ENTRY(__start)
SEARCH_DIR("=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/lib"); SEARCH_DIR("=/usr/local/lib"); SEARCH_DIR("=/lib"); SEARCH_DIR("=/usr/lib");
SECTIONS
{ ...  }



https://sourceware.org/binutils/docs/ld/Options.html#Options
LD(1)                        GNU Development Tools                       LD(1)


https://sourceware.org/binutils/docs/ld/index.html#Top
This file documents the gnu linker ld (GNU Binutils) version 2.25. 

2.1 Command Line Options

<linker-mapfile>
-Map=mapfile

Print a link map to the file mapfile. See the description of the -M option,
above.


-l namespec
--library=namespec
    Add the archive or object file specified by namespec to the list of files
    to link. This option may be used any number of times. If namespec is of
    the form :filename, ld will search the library path for a file called
    `filename`, otherwise it will search the library path for a file called
    `libnamespec.a`.

    On systems which support shared libraries, ld may also search for files
    other than libnamespec.a.  Specifically, on ELF and SunOS systems, ld will
    search a directory for a library called libnamespec.so before searching
    for one called libnamespec.a. (By convention, a ".so" extension indicates
        a shared library.) Note that this behavior does not apply to
      :filename, which always specifies a file called filename.

    The linker will search an archive only once, at the location where it is
    specified on the command line. If the archive defines a symbol which was
    undefined in some object which appeared before the archive on the command
    line, the linker will include the appropriate file(s) from the archive.
    However, an undefined symbol in an object appearing later on the command
    line will not cause the linker to search the archive again.

    See the -( option for a way to force the linker to search archives
        multiple times.

    You may list the same archive multiple times on the command line.


-L searchdir
--library-path=searchdir
    Add path `searchdir` to the list of paths that `ld` will search for
    `archive libraries and ld control scripts.` You may use this option any
    number of times. The directories are searched in the order in which they
    are specified on the command line.

    Directories specified on the command line are searched before the default
    directories. 
   
    All -L options apply to all `-l options`, regardless of the order in which
    the options appear. 
   
    -L options do not affect how ld searches for a linker script unless -T
    option is specified.

    If searchdir begins with "=", then the "=" will be replaced by the sysroot
    prefix, a path specified when the linker is configured.

    The default set of paths searched (without being specified with -L)
    depends on which emulation mode ld is using, and in some cases also on how
    it was configured.

    The paths can also be specified in a link script with the "SEARCH_DIR"
    command.  Directories specified this way are searched at the point in
    which the linker script appears in the command line.

*gcc-link-rpath*
-rpath=dir
    Add a directory to the runtime library search path. This is used when
    linking an ELF executable with shared objects. All -rpath arguments are
    concatenated and passed to the runtime linker, which uses them to locate
    shared objects at runtime.  The -rpath option is also used when locating
    shared objects which are needed by shared objects explicitly included in
    the link; see the description of the -rpath-link option. 
   
    If -rpath is not used when linking an ELF executable, the contents of the
    environment variable "LD_RUN_PATH" will be used if it is defined.

    The -rpath option may also be used on SunOS.  By default, on SunOS, the
    linker will form a runtime search patch out of all the -L options it is
    given.  

    If a -rpath option is used, the runtime search path will be formed
    `exclusively` using the -rpath options, ignoring the -L options.  This can
    be useful when using gcc, which adds many -L options which may be on NFS
    mounted file systems.

    For compatibility with other ELF linkers, if the -R option is followed by
    a directory name, rather than a file name, it is treated as the -rpath
    option.

--sysroot=directory
    Use directory as the location of the sysroot, overriding the
    configure-time default. This option is only supported by linkers that were
    configured using --with-sysroot. 

-EB
    Link big-endian objects. This affects the default output format.

-EL
    Link little-endian objects. This affects the default output format.


-Ifile
--dynamic-linker=file
Set the name of the dynamic linker. This is only meaningful when generating
dynamically linked ELF executables. The default dynamic linker is normally
correct; don't use this unless you know what you are doing.

https://blog.jessfraz.com/post/top-10-favorite-ldflags/

-shared *gcc-option-link*
-Bshareable
Create a shared library. This is currently only supported on ELF, XCOFF and
SunOS platforms. 


--start-group archives --end-group *gcc-link-order*
The archives should be a list of archive files. They may be either explicit
file names, or `-l' options. The specified archives are searched repeatedly
until no new undefined references are created. Normally, an archive is
searched only once `in the order that it is specified on the command line.` 

If a symbol in that archive is needed to resolve an undefined symbol referred
to by an object in an archive that appears later on the command line, the
linker would not be able to resolve that reference. 

By grouping the archives, they all be searched repeatedly until all possible
references are resolved. Using this option has a significant performance cost.
It is best to use it only when there are unavoidable circular references
between two or more archives.


={============================================================================
*kt_linux_gcc_400* gcc-ld-linker-script

3 Linker Scripts

Every link is controlled by a linker script. This script is written in the
`linker command language.`

The main purpose of the linker script is 

  * to describe how the sections in the input files should be mapped into the
    output file, and 
  
  * to control the memory layout of the output file. 

Most linker scripts do nothing more than this.  However, when necessary, the
linker script can also direct the linker to perform many other operations,
using the commands described below.

The linker always uses a linker script. If you do not supply one yourself, the
linker will use a `default script that is compiled into the linker executable.`

You can use the `--verbose` command line option to display the default linker
script. Certain command line options, such as `-r' or `-N', will affect the
default linker script.

You may supply your own linker script by using the `-T' command line option.
When you do this, your linker script will replace the default linker script.

You may also use linker scripts implicitly by naming them as input files to
the linker, as though they were files to be linked. See Implicit Linker
Scripts. 


3.1 Basic Linker Script Concepts

We need to define some basic concepts and vocabulary in order to describe the
linker script language.

The linker combines input files into a single output file. The output file and
each input file are in a special data format known as an object file format.
Each file is called an object file. The output file is often called an
executable, but for our purposes we will also call it an object file. Each
object file has, among other things, a list of sections. We sometimes refer to
a section in an input file as an input section; similarly, a section in the
output file is an output section.

Each section in an object file has a name and a size. Most sections also have
an associated block of data, known as the section contents. A section may be
marked as `loadable`, which means that the contents should be loaded into memory
when the output file is run. A section with no contents may be allocatable,
which means that an area in memory should be set aside, but nothing in
  particular should be loaded there (in some cases this memory must be zeroed
      out). A section which is neither loadable nor allocatable typically
  contains some sort of debugging information.

Every loadable or allocatable output section has two addresses. The first is
the `VMA`, or virtual memory address. This is the address the section will have
when the output file is run. The second is the `LMA`, or load memory address.
This is the address at which the section will be loaded. In most cases the two
addresses will be the same. An example of when they might be different is when
a data section is loaded into ROM, and then copied into RAM when the program
starts up (this technique is often used to initialize global variables in a
    ROM based system). In this case the ROM address would be the LMA, and the
RAM address would be the VMA.

You can see the sections in an object file by using the objdump program with
the `-h' option.

<symbols-are>
Every object file also has a list of symbols, known as the symbol table. A
symbol may be defined or undefined. Each symbol has a name, and each defined
symbol has an address, among other information. If you compile a C or C++
program into an object file, you will get a defined symbol for every defined
function and global or static variable. Every undefined function or global
variable which is referenced in the input file will become an undefined
symbol.

You can see the symbols in an object file by using the nm program, or by using
the objdump program with the `-t' option. 


3.2 Linker Script Format

Linker scripts are text files.

You write a linker script as a series of commands. Each command is either a
keyword, possibly followed by arguments, or an assignment to a symbol. You may
separate commands using semicolons. Whitespace is generally ignored.

Strings such as file or format names can normally be entered directly. If the
file name contains a character such as a comma which would otherwise serve to
separate file names, you may put the file name in double quotes. There is no
way to use a double quote character in a file name.

You may include comments in linker scripts just as in C, delimited by 
/* and */. As in C, comments are syntactically equivalent to whitespace. 


3.3 Simple Linker Script Example

Many linker scripts are fairly simple.

The simplest possible linker script has just one command: `SECTIONS`. You use
the SECTIONS command to describe the memory layout of the output file.

The `SECTIONS' command is a powerful command. Here we will describe a simple
use of it. Let's assume your program consists only of code, initialized data,
and uninitialized data. These will be in the `.text', `.data', and `.bss'
  sections, respectively. Let's assume further that these are the only
  sections which appear in your input files.

For this example, let's say that the code should be loaded at address 0x10000,
and that the data should start at address 0x8000000. Here is a linker script
which will do that:

     SECTIONS
     {
       . = 0x10000;
       .text : { *(.text) }
       . = 0x8000000;
       .data : { *(.data) }
       .bss : { *(.bss) }
     }

You write the SECTIONS command as the keyword 'SECTIONS', followed by a series
  of `symbol assignments` and `output section descriptions` enclosed in curly
  braces.

The first line inside the `SECTIONS' command of the above example sets the
value of the special symbol `.`, which is `the location counter` If you do not
specify the address of an output section in some other way (other ways are
    described later), the address is set from the current value of the
location counter. The location counter is then incremented by the size of the
output section. At the start of the `SECTIONS' command, the location counter
has the value 0.

The second line defines an output section, `.text`. The colon is required
syntax which may be ignored for now. Within the curly braces after the output
section name, `you list the names of the input sections` which should be
placed into this output section. The * is a wildcard which matches any file
name. The expression *(.text)' means all .text input sections in all input
files.

Since the location counter is 0x10000 when the output section .text is
defined, the linker will set the address of the .text section in the output
file to be 0x10000.

The remaining lines define the .data and .bss sections in the output file.
The linker will place the .data output section at address 0x8000000. After the
linker places the .data output section, the value of the location counter will
be 0x8000000 plus the size of the .data output section. The effect is that the
linker will place the .bss output section immediately after the .data output
section in memory.

The linker will ensure that each output section has `the required alignment`, by
increasing the location counter if necessary. In this example, the specified
addresses for the .text and .data sections will probably satisfy any
alignment constraints, but the linker may have to create a small gap between
the .data and .bss sections.

That's it! That's a simple and complete linker script. 


3.4.1 Setting the Entry Point

The first instruction to execute in a program is called the entry point. You
can use the ENTRY linker script command to set the entry point. The argument
is a symbol name:

     ENTRY(symbol)

There are several ways to set the entry point. The linker will set the entry
point by trying each of the following methods in order, and stopping when one
of them succeeds:

    the `-e' entry command-line option;

    the ENTRY(symbol) command in a linker script;

    the value of a target specific symbol, if it is defined; For many targets
      this is start, but PE and BeOS based systems for example check a list of
      possible entry symbols, matching the first one found.  the address of
      the first byte of the `.text' section, if present;

    The address 0. 


3.4.2 Commands Dealing with Files

Several linker script commands deal with files. 

INPUT(file, file, ...)
INPUT(file file ...)
    The INPUT command directs the linker to include `the named files` in the
    link, as though they were named on the command line.

    For example, if you always want to include subr.o any time you do a link,
        but you can't be bothered to put it on every link command line, then
          you can put `INPUT (subr.o)' in your linker script.

    In fact, if you like, you can list all of your input files in the linker
    script, and then invoke the linker with nothing but a -T option.

    In case a sysroot prefix is configured, and the filename starts with the
    '/' character, and the script being processed was located inside the
    sysroot prefix, the filename will be looked for in the sysroot prefix.
    Otherwise, the linker will try to open the file in the current directory.
    If it is not found, the linker will search through the archive library
    search path. The sysroot prefix can also be forced by specifying = as the
    first character in the filename path. See also the description of '-L' in
    Command Line Options.

    If you use 'INPUT (-lfile)', ld will transform the name to libfile.a, as
    with the command line argument '-l'.

    When you use the INPUT command in an implicit linker script, the files
    will be included in the link at the point at which the linker script file
    is included. This can affect archive searching. 

GROUP(file, file, ...)
GROUP(file file ...)
    The GROUP command is like INPUT, except that the named files should all be
    `archives`, and they are searched repeatedly until no new undefined
    references are created. See the description of '-(' in Command Line
        Options.

AS_NEEDED(file, file, ...)
AS_NEEDED(file file ...)
    This construct can appear only inside of the INPUT or GROUP commands,
    among other filenames. The files listed will be handled as if they appear
    directly in the INPUT or GROUP commands, with the exception of ELF shared
    libraries, that will be added only when they are actually needed. This
    construct essentially enables --as-needed option for all the files listed
    inside of it and restores previous --as-needed resp. --no-as-needed
    setting afterwards.

SEARCH_DIR(path)
    The SEARCH_DIR command adds path to the list of paths where ld looks for
    archive libraries. Using SEARCH_DIR(path) is exactly like using '-L path'
    on the command line (see Command Line Options). If both are used, then the
    linker will search both paths. Paths specified using the command line
    option are searched first. 


3.4.3 Commands Dealing with Object File Formats

A couple of linker script commands deal with object file formats.

OUTPUT_FORMAT(bfdname)
OUTPUT_FORMAT(default, big, little)
    The OUTPUT_FORMAT command names the BFD format to use for the output file
    (see BFD). Using OUTPUT_FORMAT(bfdname) is exactly like using '--oformat
    bfdname' on the command line (see Command Line Options). If both are used,
    the command line option takes precedence.

    You can use OUTPUT_FORMAT with three arguments to use different formats
    based on the '-EB' and `-EL' command line options. This permits the linker
    script to set the output format based on the desired endianness.

    If neither '-EB' nor '-EL' are used, then the output format will be the
    first argument, default. If '-EB' is used, the output format will be the
    second argument, big. If `-EL' is used, the output format will be the
    third argument, little.

    For example, the default linker script for the MIPS ELF target uses this
    command:

              OUTPUT_FORMAT(elf32-bigmips, elf32-bigmips, elf32-littlemips)
         
    This says that the default format for the output file is 'elf32-bigmips',
    but if the user uses the '-EL' command line option, the output file will
      be created in the 'elf32-littlemips' format. 


3.4.5 Other Linker Script Commands

OUTPUT_ARCH(bfdarch)
    Specify a particular output machine architecture. The argument is one of
    the names used by the BFD library (see BFD). You can see the architecture
    of an object file by using the objdump program with the `-f' option. 


5 BFD

The linker accesses object and archive files using the BFD libraries. These
libraries allow the linker to use the same routines to operate on object files
whatever the object file format. A different object file format can be
supported simply by creating a new BFD back end and adding it to the library.
To conserve runtime memory, however, the linker and associated tools are
usually configured to support only a subset of the object file formats
available. You can use objdump -i (see objdump) to list all the formats
available for your configuration.

As with most implementations, BFD is a compromise between several conflicting
requirements. The major factor influencing BFD design was efficiency: any time
used converting between formats is time which would not have been spent had
BFD not been involved. This is partly offset by abstraction payback; since BFD
simplifies applications and back ends, more time and care may be spent
optimizing algorithms for a greater speed.

One minor artifact of the BFD solution which you should bear in mind is the
potential for information loss. There are two places where useful information
can be lost using the BFD mechanism: during conversion and during output. See
BFD information loss.

    BFD outline: How it works: an outline of BFD 

<ex>
/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-objdump -i
BFD header file version 2.17.50-brcm 20070220
elf32-tradbigmips
 (header big endian, data big endian)
  mips
elf32-tradlittlemips
 (header little endian, data little endian)
  mips
...


<case>
CNXT embedded platform.

The problem is that try to use fixed specific path to build

/home/ci/views/ci_fosh_pecosa_linux_int/vobs/SYSTEM_BIN/arm1176_gcc_x86_linux_01/bin/../lib/gcc/arm-linux-uclibcgnueabi/4.1.1/../../../../arm-linux-uclibcgnueabi/bin/ld:
cannot find /home/wgrobler/platforms/CnxtPecos/M2.1/SRC/host/linux/tools-4.1.1/lib/libc.so.0
collect2: ld returned 1 exit status

So look into a directory one by one in libraries search parh to find libc.so
and libc.a. 

[bin]$ ./arm-linux-uclibcgnueabi-gcc -print-search-dirs
install: /home/wgrobler/platforms/CnxtPecos/M2.1/SRC/host/linux/tools-4.1.1/lib/gcc/arm-linux-uclibcgnueabi/4.1.1/
...
libraries:=/home/ci/views/ci_fosh_pecosa_linux_int/vobs/SYSTEM_BIN/arm1176_gcc_x86_linux_01/bin/../lib/gcc/arm-linux-uclibcgnueabi/4.1.1/
:/home/ci/views/ci_fosh_pecosa_linux_int/vobs/SYSTEM_BIN/arm1176_gcc_x86_linux_01/bin/../lib/gcc/
:/home/wgrobler/platforms/CnxtPecos/M2.1/SRC/host/linux/tools-4.1.1/lib/gcc/arm-linux-uclibcgnueabi/4.1.1/
:/usr/lib/gcc/arm-linux-uclibcgnueabi/4.1.1/
:/home/ci/views/ci_fosh_pecosa_linux_int/vobs/SYSTEM_BIN/arm1176_gcc_x86_linux_01/bin/../lib/gcc/arm-linux-uclibcgnueabi/4.1.1/../../../../arm-linux-uclibcgnueabi/lib/arm-linux-uclibcgnueabi/4.1.1/
:/home/ci/views/ci_fosh_pecosa_linux_int/vobs/SYSTEM_BIN/arm1176_gcc_x86_linux_01/bin/../lib/gcc/arm-linux-uclibcgnueabi/4.1.1/../../../../arm-linux-uclibcgnueabi/lib/
:/home/wgrobler/platforms/CnxtPecos/M2.1/SRC/host/linux/tools-4.1.1/arm-linux-uclibcgnueabi/lib/arm-linux-uclibcgnueabi/4.1.1/
:/home/wgrobler/platforms/CnxtPecos/M2.1/SRC/host/linux/tools-4.1.1/arm-linux-uclibcgnueabi/lib/


No luck. However, when have *tool-strace* log, found libc.so using libraries
path and try to find the real library looking at the content of libc.so. The
libc.so is a text file.

[lib]$ cat libc.so
/* GNU ld script
 * Use the shared library, but some functions are only in
 * the static library, so try that secondarily. */
GROUP ( /home/wgrobler/platforms/CnxtPecos/M2.1/SRC/host/linux/tools-4.1.1/lib/libc.so.0
    /home/wgrobler/platforms/CnxtPecos/M2.1/SRC/host/linux/tools-4.1.1/lib/uclibc_nonshared.a  )

When fixed as below, works fine.

/* GNU ld script
 * Use the shared library, but some functions are only in
 * the static library, so try that secondarily. */
GROUP ( libc.so.0 uclibc_nonshared.a  )


={============================================================================
*kt_linux_gcc_400* gcc-asm

https://maksimdan.gitbooks.io/intel-x86-assembly-gcc/content/inline_assembly.html

https://gcc.gnu.org/onlinedocs/gcc/Extended-Asm.html#AssemblerTemplate
6.45.2 Extended Asm - Assembler Instructions with C Expression Operands

With extended asm you can read and write C variables from assembler and
perform jumps from assembler code to C labels. 

`extended asm` syntax uses colons (‘:’) to delimit the operand parameters after
the assembler template:

     asm [volatile] ( AssemblerTemplate
                      : OutputOperands
                      [ : InputOperands
                      [ : Clobbers ] ])

The asm keyword is a GNU extension. When writing code that can be compiled
  with -ansi and the various -std options, use `__asm__ instead of asm` (see
      Alternate Keywords). 

AssemblerTemplate
    This is a literal string that is `the template for the assembler code` It
    is a combination of fixed text and `tokens` that refer to the input,
    output, and goto parameters. See AssemblerTemplate. 


6.45.2.2 Assembler Template

An assembler template is a literal string containing assembler instructions.
The compiler replaces tokens in the template that refer to inputs, outputs,
and goto labels, and `then outputs the resulting string to the assembler` 
  
The string can contain any instructions recognized by the assembler, including
directives. `GCC does not parse the assembler instructions themselves` and does
not know what they mean or even whether they are valid assembler input.
However, it does count the statements (see Size of an asm). 


Remarks

The asm statement allows you to include assembly instructions directly within
C code. This may help you `to maximize performance` in time-sensitive code or to
access assembly instructions that are not readily available to C programs.

Note that `extended asm` statements must be inside a function. Only `basic asm`
may be outside functions (see Basic Asm). Functions declared with the naked
attribute also require basic asm (see Function Attributes).

While the uses of asm are many and varied, it may help to think of an asm
statement as a series of low-level instructions that convert input parameters
to output parameters. So a simple (if not particularly useful) example for
i386 using asm might look like this:

     int src = 1;
     int dst;
     
     asm ("mov %1, %0\n\t"
         "add $1, %0"
         : "=r" (dst)
         : "r" (src));
     
     printf("%d\n", dst);

This code copies src to dst and add 1 to dst. 


6.45.2.3 Output Operands

An asm statement has zero or more output operands indicating the names of C
variables modified by the assembler code.

In this i386 example, old (referred to in the template string as %0) and *Base
(as %1) are outputs and Offset (%2) is an input:

     bool old;
     
     __asm__ ("btsl %2,%1\n\t" // Turn on zero-based bit #Offset in Base.
              "sbb %0,%0"      // Use the CF to calculate old.
        : "=r" (old), "+rm" (*Base)
        : "Ir" (Offset)
        : "cc");
     
     return old;


https://gcc.gnu.org/onlinedocs/gcc-4.8.2/gcc/Extended-Asm.html#Extended-Asm
6.41 Assembler Instructions with C Expression Operands

In an assembler instruction using asm, you can specify the operands of the
instruction using C expressions. `This means you need not guess` which registers
or memory locations contain the data you want to use. 

You must specify an assembler instruction template much like what appears in a
machine description, plus an `operand constraint string` for each operand.

For example, here is how to use the 68881's fsinx instruction:

     asm ("fsinx %1,%0" : "=f" (result) : "f" (angle));

Here angle is the C expression for the input operand while result is that of
  the output operand. 
  
Each has ‘"f"’ as its operand constraint, saying that a `floating-point`
`register` is required. The ‘=’ in ‘=f’ indicates that the operand is an
output; all output operands' constraints must use ‘=’. The constraints use the
same language used in the machine description (see Constraints).

Each operand is described by an operand-constraint string followed by the C
expression in parentheses. A colon separates the `assembler template` from the
first output operand and another separates the last output operand from the
first input, if any. Commas separate the operands within each group. The total
number of operands is currently limited to 30; this limitation may be lifted
in some future version of GCC. 


6.42.1 Simple Constraints

The simplest kind of constraint is a string full of letters, each of which
describes one kind of operand that is permitted. Here are the letters that are
allowed: 

‘r’
    A register operand is allowed provided that it is in a general register. 


={============================================================================
*kt_linux_gcc_400* gcc-libgcc gcc-ucmpdi2-error

https://gcc.gnu.org/onlinedocs/gccint/Libgcc.html#Libgcc
<gcc-source>/gcc/doc/gccint.info

4 The GCC low-level runtime library

GCC provides a low-level runtime library, libgcc.a or libgcc_s.so.1 on some
platforms. `GCC generates calls to routines in this library automatically`,
whenever it needs to perform some operation that is too complicated to emit
  inline code for.

Most of the routines in libgcc handle arithmetic operations that the target
processor cannot perform directly. This includes integer multiply and divide
on some machines, and all floating-point and fixed-point operations on other
machines. libgcc also includes routines for exception handling, and a handful
of miscellaneous operations.

Some of these routines can be defined in mostly machine-independent C. Others
must be hand-written in assembly language for each processor that needs them.

GCC will also generate calls to C library routines, such as memcpy and memset,
in some cases. The set of routines that GCC may possibly use is documented in
  Other Builtins.

These routines take arguments and return values of a specific machine mode,
not a specific C type. See Machine Modes, for an explanation of this concept.
  For illustrative purposes, in this chapter the floating point type float is
  assumed to correspond to SFmode; double to DFmode; and long double to both
  TFmode and XFmode. Similarly, the integer types int and unsigned int
  correspond to SImode; long and unsigned long to DImode; and long long and
  unsigned long long to TImode. 


4.1 Routines for integer arithmetic

The integer arithmetic routines are used on platforms that `don't provide`
hardware support for arithmetic operations on some modes. 


4.1.2 Comparison functions

The following functions implement integral comparisons. These functions
implement a low-level compare, upon which the higher level comparison
operators (such as less than and greater than or equal to) can be constructed.
The returned values lie in the range zero to two, to allow the high-level
operators to be implemented by testing the returned result using either signed
or unsigned comparison.

 Runtime Function: int __ucmpdi2 (unsigned long a, unsigned long b)
 Runtime Function: int __ucmpti2 (unsigned long long a, unsigned long long b)

    These functions perform an unsigned comparison of a and b. If a is less
    than b, they return 0; if a is greater than b, they return 2; and if a and
    b are equal they return 1. 


kyoupark@ukstbuild2:~/asn/gcc/gcc-4.2.0$ ack __ucmpdi2
gcc/libgcc2.c
1145:__ucmpdi2 (DWtype a, DWtype b)

gcc/libgcc-std.ver
112:  __ucmpdi2

gcc/config/v850/lib1funcs.asm
2247:   .global ___ucmpdi2
2248:   .type   ___ucmpdi2,@function
2249:___ucmpdi2:
2258:   .size ___ucmpdi2, . - ___ucmpdi2

gcc/config/rs6000/darwin-libgcc.10.4.ver
73:___ucmpdi2

gcc/config/rs6000/darwin-libgcc.10.5.ver
86:___ucmpdi2

gcc/config/i386/darwin-libgcc.10.4.ver
78:___ucmpdi2

gcc/config/i386/darwin-libgcc.10.5.ver
82:___ucmpdi2

gcc/config/darwin-64.c
55:word_type __ucmpdi2 (uDI x, uDI y);
72:word_type __ucmpdi2 (uDI x, uDI y) { return x < y ? 0 : x == y ? 1 : 2; }

gcc/libgcc2.h
268:#define __ucmpdi2   __NDW(ucmp,2)
341:extern word_type __ucmpdi2 (DWtype, DWtype);


<how>
 Runtime Function: unsigned long __udivmoddi4 (unsigned long a, unsigned long b, unsigned long *c)
 Runtime Function: unsigned long long __udivmodti4 (unsigned long long a, unsigned long long b, unsigned long long *c)

    These functions calculate both the quotient and remainder of the unsigned
    division of a and b. The return value is the quotient, and the remainder
    is placed in variable pointed to by c. 

linux-2.6.18.8-022-libgcc.patch

diff -ruNp linux/arch/mips/brcmstb/lib/udivdi3.c linux.new/arch/mips/brcmstb/lib/udivdi3.c
--- linux/arch/mips/brcmstb/lib/udivdi3.c	1969-12-31 16:00:00.000000000 -0800
+++ linux.new/arch/mips/brcmstb/lib/udivdi3.c	2008-07-18 18:34:04.000000000 -0700
@@ -0,0 +1,244 @@
+/* More subroutines needed by GCC output code on some machines.  */
+/* Compile this one with gcc.  */
+/* Copyright (C) 1989, 92-98, 1999 Free Software Foundation, Inc.
+
+This file is part of GNU CC.
+
+GNU CC is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2, or (at your option)
+any later version.
+
+GNU CC is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with GNU CC; see the file COPYING.  If not, write to
+the Free Software Foundation, 59 Temple Place - Suite 330,
+Boston, MA 02111-1307, USA.  */
+
+/* As a special exception, if you link this library with other files,
+   some of which are compiled with GCC, to produce an executable,
+   this library does not by itself cause the resulting executable
+   to be covered by the GNU General Public License.
+   This exception does not however invalidate any other reasons why
+   the executable file might be covered by the GNU General Public License.
+ */
+/* support functions required by the kernel. based on code from gcc-2.95.3 */
+/* I Molton     29/07/01 */
+
+#include <linux/module.h>
+#include "gcclib.h"
+#include "longlong.h"
+
+const UQItype __clz_tab[] =
+{
+  0,1,2,2,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,
+  6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,
+  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,
+  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,
+  8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
+  8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
+  8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
+  8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
+};
+
+UDItype
+__udivmoddi4 (UDItype n, UDItype d, UDItype *rp)


={============================================================================
*kt_linux_gcc_400* gcc-option-output

https://gcc.gnu.org/onlinedocs/gcc/Overall-Options.html#Overall-Options

3.2 Options Controlling the Kind of Output

Compilation can involve up to `four stages`: preprocessing, compilation proper,
assembly and linking, always in that order. 
  
GCC is capable of preprocessing and compiling several files either into
several assembler input files, or into one assembler input file; then each
assembler input file produces an object file, and linking combines all the
object files (those newly compiled, and those specified as input) into an
executable file.

For any given input file, the file name `suffix determines` what kind of
compilation is done: 

You can specify the input language explicitly with the -x option:

-x language
    Specify explicitly the language for the following input files (rather than
        letting the compiler choose a default based on the file name suffix).
    This option applies to all following input files until the next -x option.

    Possible values for language are:

              c  c-header  cpp-output
              c++  c++-header  c++-cpp-output
              objective-c  objective-c-header  objective-c-cpp-output
              objective-c++ objective-c++-header objective-c++-cpp-output
              assembler  assembler-with-cpp
              ada
              f77  f77-cpp-input f95  f95-cpp-input
              go

-x none
    Turn off any specification of a language, so that subsequent files are
    handled according to their file name suffixes (as they are if -x has not
        been used at all). 

-fverbose-asm
    Put extra commentary information in the generated assembly code to make it
    more readable. This option is generally only of use to those who actually
    need to read the generated assembly code (perhaps while debugging the
        compiler itself).

    -fno-verbose-asm, the default, causes the extra information to be omitted
    and is useful when comparing two assembler files.

    The added comments include:

        information on the compiler version and command-line options, the
        source code lines associated with the assembly instructions, in the
        form FILENAME:LINENUMBER:CONTENT OF LINE, hints on which high-level
        expressions correspond to the various assembly instruction operands. 


-S
    Stop after the stage of compilation proper; do not assemble. The output is
    `in the form of an assembler code file` for each non-assembler input file
    specified.

    By default, the assembler file name for a source file is made by replacing
    the suffix ‘.c’, ‘.i’, etc., with ‘.s’.

    Input files that don’t require compilation are ignored.


<gcc-option-verbose> gcc-debug
-v

Print (on standard error output) the commands executed to run the stages of
compilation. Also print the version number of the compiler driver program and
of the preprocessor and the compiler proper. 

note: can see spec and include path

-###

Like -v except the commands are `not executed` and arguments are quoted unless
they contain only alphanumeric characters or ./-_. This is useful for shell
scripts to capture the driver-generated command lines. 


note: the -### flag to see the exact compilation steps and notice which
libraries get picked up, etc. (note that to see -lstdc++ a C++ source compiled
    with g++4.8 is needed instead).


-print-search-dirs 

note: to check path for lib


={============================================================================
*kt_linux_gcc_400* gcc-option-developers

https://gcc.gnu.org/onlinedocs/gcc/Developer-Options.html#Developer-Options

3.17 GCC Developer Options

This section describes command-line options that are primarily of interest to
GCC developers, including options to support compiler testing and
investigation of compiler bugs and compile-time performance problems. 

This includes options that produce debug dumps at various points in the
compilation; that print statistics such as memory use and execution time; and
that print information about GCC's configuration, such as where it searches
for libraries. You should rarely need to use any of these options for ordinary
  compilation and linking tasks. 

-print-file-name=library
    Print the full absolute name of the library file library that would be
    used when linking-and don't do anything else. With this option, GCC does
    not compile or link anything; it just prints the file name.

-dumpmachine
    Print the compiler's target machine (for example, ‘i686-pc-linux-gnu’)and
    don't do anything else.

-dumpversion
    Print the compiler version (for example, 3.0)and don't do anything else.

-dumpspecs
    Print the compiler's built-in specsand don't do anything else. (This is
        used when GCC itself is being built.) See Spec Files. 

<gcc-save-temp>
To produce three extra files with .i, .s and .o extension. The temporary files
produced by `-save-temps` flag in one go can be produced one by one by using
the gcc flags -E, -C and -S at each of the preprocessing, compilation and
assembly steps respectively.

note: 
-save-temps is in debugging options.


={============================================================================
*kt_linux_gcc_400* gcc-option-directory-search

https://gcc.gnu.org/onlinedocs/gcc/Directory-Options.html#Directory-Options

3.15 Options for Directory Search

These options specify directories to search for header files, for libraries
and for parts of the compiler: 

Directories specified with -iquote apply only to the quote form of the
directive, #include "file".  

Directories specified with -I, -isystem, or -idirafter apply to lookup for
both the #include "file" and #include <file> directives.

-Ldir
Add directory dir to the list of directories to be searched for -l. 
note: used when compile.

-Lsearchdir 
--library-path=searchdir 

Add path searchdir to the list of paths that ld will search for archive
libraries and ld control scripts. You may use this option any number of times.
The directories are searched in the order in which they are specified on the
command line. Directories specified on the command line are searched before
the default directories. All -L options apply to all -l options, regardless of
the order in which the options appear. 

If searchdir begins with =, then the = will be replaced by the sysroot prefix,
a path specified when the linker is configured. 

The default set of paths searched (without being specified with `-L') depends
on which emulation mode ld is using, and in some cases also on how it was
configured. See Environment. 

The paths can also be specified in a link script with the SEARCH_DIR command.
Directories specified this way are searched at the point in which the linker
script appears in the command line. 


={============================================================================
*kt_linux_gcc_400* gcc-option-link

http://gcc.gnu.org/onlinedocs/gcc/Link-Options.html#Link-Options

3.14 Options for Linking

-llibrary
-l library
Search the library `named library` when `linking`. 

*lib-static-link-order* *gcc-link-order*
See link errors although all are defined in the library? Becuase:

It makes a difference where in the command you write this option; the linker
searches and processes libraries and object files in the order they are
specified. Thus, 'foo.o -lz bar.o' searches library 'z' after file foo.o but
before bar.o. If bar.o refers to functions in 'z', those functions may not be
loaded.

<ex>
$(TESTS) : $(TARGET).o gtest.a gmock.a
	$(CXX) $(CPPFLAGS) $(CXXFLAGS) -lpthread -ltinyxml2 ^$ -o $@

g++ -isystem /home/kitp/works/googletest/googletest/include -isystem
/home/kitp/works/googletest/googlemock/include -isystem
/home/kitp/works/tinyxml2 -std=c++11 -g -Wall -Wextra -pthread
-L/home/kitp/works/tinyxml2 -lpthread -ltinyxml2 tinyxml_test.o gtest.a
gmock.a -o tinyxml_test_test

tinyxml_test.o: In function `TinyXml_X_Test::TestBody()':
/home/kitp/git/kb/code-cxx/t_ex_ccook/tinyxml_test.cpp:36: 
  undefined reference to `tinyxml2::XMLDocument::XMLDocument(bool, tinyxml2::Whitespace)'

$(TESTS) : $(TARGET).o gtest.a gmock.a
	$(CXX) $(CPPFLAGS) $(CXXFLAGS) -lpthread $^ -ltinyxml2 -o $@

g++ -isystem /home/kitp/works/googletest/googletest/include -isystem
  /home/kitp/works/googletest/googlemock/include -isystem
  /home/kitp/works/tinyxml2 -std=c++11 -g -Wall -Wextra -pthread
  -L/home/kitp/works/tinyxml2 -lpthread tinyxml_test.o gtest.a gmock.a
  -ltinyxml2 -o tinyxml_test_test


The linker searches a standard list of directories for the library, which is
actually a file named liblibrary.a. The linker then uses this file as if it
had been specified precisely by name.

*gcc-link-search*
The directories searched include several standard system directories plus any
that you specify with -L.

Normally the files found this way are library files-archive files whose
members are object files. The linker handles an archive file by scanning
through it for members which define symbols that have so far been referenced
but not defined. But if the file that is found is an ordinary object file, it
is linked in the usual fashion. 

The only difference between using an -l option and specifying a file name is
that -l surrounds library with 'lib' and '.a' and searches several
directories.


-static
    On systems that support dynamic linking, this prevents linking with the
    shared libraries. On other systems, this option has no effect.

// ok
$ g++ -static t_01.cc -L/usr/local/lib -lboost_program_options

// nk. undefined error like when use:
// $ g++ -static t_01.cc 
$ g++ -static -L/usr/local/lib -lboost_program_options t_01.cc

-shared
    Produce a shared object which can then be linked with other objects to
    form an executable. Not all systems support this option. For predictable
    results, you must also specify the same set of options used for
    compilation (-fpic, -fPIC, or model suboptions) when you specify this
    linker option.1 

<gcc-strip>
-s
  Remove all symbol table and relocation information from the executable.


https://gcc.gnu.org/onlinedocs/gcc/Code-Gen-Options.html#Code-Gen-Options

3.16 Options for Code Generation Conventions

-fpic
    Generate position-independent code (PIC) suitable for use in a shared
    library, `if supported for the target machine` Such code accesses all
    constant addresses through a global offset table (GOT). The dynamic loader
    resolves the GOT entries when the program starts (the dynamic loader is
        not part of GCC; it is part of the operating system). If the GOT size
    for the linked executable exceeds a machine-specific maximum size, you get
      an error message from the linker indicating that -fpic does not work; in
        that case, recompile with -fPIC instead. (These maximums are 8k on the
            SPARC, 28k on AArch64 and 32k on the m68k and RS/6000. The x86 has
            no such limit.)

    Position-independent code requires special support, and therefore works
    only on certain machines. For the x86, GCC supports PIC for System V but
    not for the Sun 386i. Code generated for the IBM RS/6000 is always
    position-independent.

    When this flag is set, the macros __pic__ and __PIC__ are defined to 1.

-fPIC
    If supported for the target machine, emit position-independent code,
       suitable for dynamic linking and avoiding any limit on the size of the
         global offset table. This option makes a difference on AArch64, m68k,
       PowerPC and SPARC.

    Position-independent code requires special support, and therefore works
    only on certain machines.

    When this flag is set, the macros __pic__ and __PIC__ are defined to 2.


-Xlinker option
Pass option as an option to the linker. You can use this to supply
system-specific linker options that GCC does not recognize.

If you want to pass an option that takes a separate argument, you must use
-Xlinker twice, once for the option and once for the argument. For example, to
pass -assert definitions, you must write -Xlinker -assert -Xlinker
definitions. It does not work to write -Xlinker "-assert definitions", because
this passes the entire string as a single argument, which is not what the
linker expects.

When using the GNU linker, it is usually more convenient to pass arguments to
linker options using the option=value syntax than as separate arguments. For
example, you can specify -Xlinker -Map=output.map rather than -Xlinker -Map
-Xlinker output.map. Other linkers may not support this syntax for
command-line options.

<ex>
// ok
$ g++ -static -std=c++0x -Xlinker -Map=output.map t_string.cpp

// does really work
LDFLAGS=" -s -Xlinker -rpath /lib -Xlinker -rpath-link $MIPS/lib/gcc/mips-linux/4.2.0"


<ex> *gcc-link-order*
// single line originally

-Xlinker --start-group 
/home/kyoupark/asan/i686-nptl-linux-gnu-vstb-630/i686-nptl-linux-gnu/lib/libasan.a
-L/home/kyoupark/STB_SW_vstb_clean/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/LittleEndian/release/lib
-lm -lpthread -lrt -ldl -ltirpc -lsky_airplay_helper -lminizip -ljsoncpp -lglib
-lgio -lgobject -ldbus -lz -lasan
-Wl,-rpath-link=/home/kyoupark/asan/i686-nptl-linux-gnu-vstb-630/i686-nptl-linux-gnu/lib
-rdynamic
-Wl,-rpath-link=/home/kyoupark/STB_SW_vstb_clean/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/LittleEndian/release_dbg/lib/
-L/home/kyoupark/STB_SW_vstb_clean/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/LittleEndian/release_dbg/lib/
-Xlinker -R/NDS/lib
/home/kyoupark/asan/i686-nptl-linux-gnu-vstb-630/i686-nptl-linux-gnu/lib/libstdc++.a
-lusb
-L/home/kyoupark/STB_SW_vstb_clean/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/NDS_INTEL_X86_LNUX_MRFUSION_01/LittleEndian/fs/NDS/lib
-Xlinker --end-group -o


*gcc-pass-option-to-linker*
-Wl,option
Pass option as an option to the linker. If option contains commas, it is split
into multiple options at the commas. You can use this syntax to pass an
argument to the option. For example, -Wl,-Map,output.map passes -Map
output.map to the linker. When using the GNU linker, you can also get the same
effect with -Wl,-Map=output.map.


-nostdlib
Do not use the standard system startup files or libraries when linking. No
startup files and only the libraries you specify are passed to the linker, and
options specifying linkage of the system libraries, such as -static-libgcc or
-shared-libgcc, are ignored.

The compiler may generate calls to memcmp, memset, memcpy and memmove. These
entries are usually resolved by entries in libc. These entry points should be
supplied through some other mechanism when this option is specified.

One of the standard libraries bypassed by -nostdlib and -nodefaultlibs is
libgcc.a, a library of internal subroutines which GCC uses to overcome
shortcomings of particular machines, or special needs for some languages. (See
    Interfacing to GCC Output, for more discussion of libgcc.a.) In most
cases, you need libgcc.a even when you want to avoid other standard libraries.
In other words, when you specify -nostdlib or -nodefaultlibs you should
usually specify -lgcc as well. This ensures that you have no unresolved
references to internal GCC library subroutines. (An example of such an
    internal subroutine is '__main', used to ensure C++ constructors are
    called; see collect2.) 


={============================================================================
*kt_linux_gcc_400* gcc-option-link-order gcc-link-order

{gcc-compliation-process}
Four phase: preprocessing, compilation, assembly and linking. 

<case> 
Seen the case that during the porting work, built all files but when starts
some funcs which are part of files which are already built, seeing more errors
of missing header files and undefined symbols. Why?

From online:

Once the object file containing the machine code is produced in the step
above, the linking step makes sure that all the undefined symbols in code are
resolved.

Enable warnings using -Wall flag, then you will see warning: Implicit call to
function bprint() and Implicit call to function aprint(). It's is basically
compiler recognizes this function during Linker stage and this does not give
any error.

So gcc assumes implicit calls in compilation phase and starts to emit errors
when 'actually' use it which lead to link it. See the below for a correct
answer.


{gcc-link-order}

Used different in link with the same objects. out file has different md5sum
depending the link order:

$ gcc simplemain.o simplefunc.o
$ gcc simplefunc.o simplemain.o

$ md5sum a.out
50b5f88fe5ed8aef16a3decd63a1dce7  a.out
$ md5sum a.out.one 
c33b4d44f06f21e0b378d8d1473b07d8  a.out.one


<reference>
http://eli.thegreenplace.net/2013/07/09/library-order-in-static-linking/

Library order in static linking

July 9th, 2013 at 5:56 am

I'll start with a slightly sneaky but educational example. Suppose we have
this code:

volatile char src[] = {1, 2, 3, 4, 5};
volatile char dst[50] = { 0 };

void* memcpy(void* dst, void* src, int len);

int main(int argc, const char* argv[])
{
  memcpy(dst, src, sizeof(src) / sizeof(src[0]));
  return dst[4];
}

It runs just fine and the return value is 5. Now, suppose this is part of a
larger project that consists of many object files and libraries, and somewhere
within the project there is a 'library' that contains this code:

void memcpy(char* aa, char* bb, char* cc) {
  int i;
  for (i = 0; i < 100; ++i) {
    cc[i] = aa[i] + bb[i];
  }
}

If the previous snippet gets linked with this library, what happens? Would you
expect it to still return 5? Return something else? Crash? 

`The answer is: it depends` - the result can be either correct or a
segmentation fault. It depends on the order in which the objects and libraries
in the project were fed to the linker.

// see *case-link*

If you fully understand why this depends on linking order, as well as how to
avoid the problem (and more serious problems, like circular dependencies) then
congratulate yourself and move on - this article is probably not for you.
Otherwise, read on.


The basics

Let's start by defining the scope of this article: first, my examples are
demonstrating the use of the gcc and binutils toolchain on Linux. Compatible
toolchains (like clang instead of gcc) apply too. 

Second, the discussion here resolves around `static linking` that's done at
compile/link time.

*symbol-visibility*
`To understand why-linking-order-matters`, it's first instructional to
understand how the linker works with respect to linking libraries and objects
together. Just as a quick reminder - an object file both provides (exports)
  external symbols to other objects and libraries, and expects (imports)
  symbols from other objects and libraries. For example, in this C code:

int imported(int);

static int internal(int x) {
    return x * 2;
}

int exported(int x) {
    return imported(x) * internal(x);
}

The names of the functions speak for themselves. Let's compile it and look at
the symbol table:

*tool-nm*
$ gcc -c x.c
$ nm x.o
000000000000000e T exported
                 U imported         // undefined
0000000000000000 t internal         // *cxx-static* 'lowercase' in nm means local.

This means: exported is an external symbol - defined in the object file and
`visible from the outside.` imported is an undefined symbol; in other words, the
linker is expected to find it elsewhere. When we talk about linking later, the
term undefined can become confusing - so it helps to remember that this is
where it comes from originally. internal is defined within the object but
`invisible from the outside.` 

A library is simply a 'collection' of object files. Just a bunch of object
files glued together. Creating a library is a very trivial operation that
doesn't do anything special besides placing many object files into the same
file. This in itself is important, because a horde of object files is not
convenient to deal with. For example, on my system libc.a (the static version
    of the C library) consists of almost 1500 object files. It's way nicer to
just carry libc.a around.  


The linking process {gcc-link-algorithm}

This section defines the linking process in a somewhat dry, algorithmic
manner. This process is the key to understanding why linking order matters.

Consider a linker invocation:

$ gcc main.o -L/some/lib/dir -lfoo -lbar -lbaz

// note:
// The following do not work:
// gcc -lfoo main.c

The linker is almost always invoked through the-compiler-`driver-gcc` when
compiling C or C++ code. This is because the driver knows how to provide the
correct command-line arguments to the linker itself (gnu-ld) with all the
support libraries, etc. We'll see more of this later.

Anyhow, as you can see the object files and libraries are provided in a
certain order on the command-line, from left to right. This is the linking
order. Here's what the linker does:


<symbol-table>
The linker maintains a symbol table. This symbol table does a bunch of things,
    but among them is keeping two lists:

1. A list of symbols 'exported' by all the objects and libraries encountered
so far.

2. A list of 'undefined' symbols that the encountered objects and libraries
requested to import and were not found yet.


<for-object-file>
When the linker encounters a new object-file, it looks at:

1.The symbols it exports: these are added to the list of exported symbols
mentioned above. If any symbol is in the undefined list, it's removed from
there because it has now been found. If any symbol has already been in the
exported list, we get a "multiple definition" error: two different objects
export the same symbol and the linker is confused.

2. The symbols it imports: these are added to the list of undefined symbols,
  unless they can be found in the list of exported symbols.


<for-library>
note: the key is that linker tries to find a symbol `only when` it's in the
undefine list in the symbol table.

When the linker encounters a new library, things are a bit more interesting.
The linker goes over all the objects in the library. For each one, it 'first'
looks at the symbols it exports.

1. If any of the symbols it(library) exports are on the undefined list, the
object is 'added' to the link and the next step is executed. Otherwise, the
next step is skipped. This means that if it is not used, this object is not be
added to the link. 

2. If the object (from the libray) has been added to the link, it's treated as
described above - its undefined and exported symbols get added to the symbol
table. 

3. Finally, if any of the objects in the library has been included in the
link, the library is 'rescanned' again - it's possible that symbols imported
by the included object can be found in other objects within the same library.

When the linker finishes, it looks at the symbol table. If any symbols remain
in the undefined list the linker will throw an `"undefined reference" error`

For example, when you create an executable and forget to include the file with
the main function, you'll get something like:

/usr/lib/x86_64-linux-gnu/crt1.o: In function '_start':
(.text+0x20): undefined reference to 'main'
collect2: ld returned 1 exit status


<gcc-link-rescan>
Note that after the linker has looked at a library, it won't look at it again.
Even if it exports symbols that may be needed by some later library. The only
time where a linker goes back to rescan objects it has already seen, happens
within a single library - as mentioned above, once an object from some library
is taken into the link, all other objects in the same library will be
rescanned. Flags passed to the linker can tweak this process - again, we'll
see some examples later.


<gcc-link-added-when-really-used>
Also note that when a library is examined, an object file within it can be
left out of the link if it does not provide symbols that the symbol table
needs. `This is a very important feature of static linking.` The C library I
mentioned before makes a heavy use of this feature, by mostly splitting itself
to an-object-per-function. So, for example if the only C standard library
function your code 'uses' is strlen, only strlen.o will be taken into the link
from libc.a - and your executable will be very small.

Simple examples 
// these examples works on ubuntu as they are shown.

The previous section can be hard to digest, so here are some simple examples
that show the process in action.

Let's start with the most basic case, of linking two objects together:

$ cat simplefunc.c

int func(int i) {
    return i + 21;
}

$ cat simplemain.c

int func(int);

int main(int argc, const char* argv[])
{
    return func(argc);
}

$ gcc -c simplefunc.c
$ gcc -c simplemain.c
$ gcc simplefunc.o simplemain.o
$ ./a.out ; echo $?
22

// $? means the exit status which is return value in this case.

note: <key> linking order only matters when use a libaray

:~/work$ nm simplefunc.o
00000000 T func

:~/work$ nm simplemain.o 
         U func
00000000 T main

*gcc-link-order*
`Since these are object files, the linking order does 'not' matter.` Object
files are always taken into the link. We can pass them to the linker in
reversed order and it still works: 

$ gcc simplemain.o simplefunc.o
$ ./a.out ; echo $?
22

Now let's do something different. Let's put simplefunc.c into a library:

$ ar r libsimplefunc.a simplefunc.o    // ar rs to skip ranlib command.
$ ranlib libsimplefunc.a
$ gcc  simplemain.o -L. -lsimplefunc
$ ./a.out ; echo $?
22

Works like a charm. But note what happens if the linking order is reversed now:

$ gcc  -L. -lsimplefunc  simplemain.o
simplemain.o: In function 'main':
simplemain.c:(.text+0x15): undefined reference to 'func'
collect2: ld returned 1 exit status


Understanding the linking algorithm outlined above makes this case simple to
explain. When the linker encounters libsimplefunc.a, it still hasn't seen
simplemain.o, which means that func is not yet on the undefined list. When the
linker looks into the library it sees simplefunc.o that exports func. But
since it doesn't need func, this object file is 'not' included in the link.

When the linker does reach simplemain.o and sees that func is, indeed
required, it's added to the undefined list (because it's not on the exported
    list). The linker then reaches the end of the link and func is still
undefined.

See how this doesn't happen in the previous linking order - since simplemain.o
comes first, func is on the undefined list before the linker sees the library,
      so the object file exporting it does get included.

This brings us to the most important corollary of the linking process outlined
above:

If object or library AA needs a symbol from library BB, then AA should come
before library BB in the command-line invocation of the linker. 


Circular dependency

The corollary above is an important summary of the linking process - it's
certainly much more practical to keep in mind because it's so short. But it
makes one wonder - 

what happens if AA needs a symbol from BB, but BB also needs a symbol from AA? 

While officially this isn't a good programming practice, in reality it happens
quite a lot. But AA can't come both before and after BB on the command-line,
right? That's just silly. Wait, is it, really?

Let's see an example and start simple. Imagine that instead of simplefunc.c,
  the func symbol is provided thus:

$ cat func_dep.c
int bar(int);

int func(int i) {
    return bar(i + 1);
}

$ cat bar_dep.c
int func(int);

int bar(int i) {
    if (i > 3)
        return i;
    else
        return func(i);
}

These two files depend on each other and get placed into different libraries.
If we link them in one order, we fail:

$ gcc  simplemain.o -L. -lbar_dep -lfunc_dep
./libfunc_dep.a(func_dep.o): In function 'func':
func_dep.c:(.text+0x14): undefined reference to 'bar'
collect2: ld returned 1 exit status

However, the other order does work:

$ gcc  simplemain.o -L. -lfunc_dep -lbar_dep
$ ./a.out ; echo $?
4

Quiz: can you figure out why? Hint: just go over the linking process algorithm
with this command-line. What undefined symbols does the symbol table contain
when the linker first sees -lfunc_dep?


<analysis-on-okay-case>
simplemain:
         U func

symbol table: undefined: func          // needs func

func_dep.a:
         U bar  (import)
00000000 T func (export)               // found func and needs bar 

bar_dep.a:
00000000 T bar  (export)               // found bar
         U func (import)

<analysis-on-not-okay-case>
simplemain:
         U func

symbol table: undefined: func                      // needs func

bar_dep.a:
00000000 T bar  (export)
         U func (import)                           // still needs func

func_dep.a:
         U bar
00000000 T func (undefined reference to 'bar')     // found func and needs bar but link ends.

The symbol table has func but not bar. Hence, error.

But this is a very simple case. Let's look at a trickier one. We'll add a
dependency to bar on another function from libfunc_dep.a, but one that lives
in a different object:

$ cat bar_dep.c
int func(int);
int frodo(int);

int bar(int i) {
    if (i > 3)
        return frodo(i);
    else
        return func(i);
}

$ cat frodo_dep.c
int frodo(int i) {
    return 6 * i;
}

We'll recompile all these files into separate objects, and the libfunc_dep.a
library will now be:

$ ar r libfunc_dep.a func_dep.o frodo_dep.o
$ ranlib libfunc_dep.a

Here's a drawing of the libraries, with arrows showing the dependencies:

simplemain:
			U func

 libfunc_dep.a        libbar_dep.a
  - func_dep.o  ->  <- - bar_dep.o (need func and frodo)
  - frodo_dep.o     <-

Now linking fails no matter what order we list the libraries in:

U func; func and bar; bar and frodo

$ gcc  -L. simplemain.o -lfunc_dep -lbar_dep
./libbar_dep.a(bar_dep.o): In function 'bar':
bar_dep.c:(.text+0x17): undefined reference to 'frodo'
collect2: ld returned 1 exit status

U func; nothing for bar_dep; func and bar

$ gcc  -L. simplemain.o -lbar_dep -lfunc_dep
./libfunc_dep.a(func_dep.o): In function 'func':
func_dep.c:(.text+0x14): undefined reference to 'bar'
collect2: ld returned 1 exit status

To solve this, consider that it's perfectly valid to list a library more than
once on the link; so in fact, we can provide libfunc_dep.a both before and
after libbar_dep.a:

$ gcc  -L. simplemain.o -lfunc_dep -lbar_dep -lfunc_dep
$ ./a.out ; echo $?
24

Another quiz: will the same trick work providing -lbar_dep twice? Why not?

$ gcc  -L. simplemain.o -lbar_dep -lfunc_dep -lbar_dep

U func; nothing for bar_dep; func and bar(U); bar

./libbar.a(bar_dep.o): In function `bar':
bar_dep.c:(.text+0x13): undefined reference to `frodo'
collect2: ld returned 1 exit status

Becase frodo is in the libaray but was not added to the link because of
object-per-function-linking.


Using linker flags to control the process

As I've mentioned above, the linker has a number of interesting flags that can
be used to control the process in a fine-grained manner. For example, circular
dependency problems can be easily resolved with --start-group and --end-group.
Here's an instructive portion from man ld:

*gcc-link-order*
-start-group archives -end-group

The specified archives are searched repeatedly until no new undefined
references are created.  Normally, an archive is searched only once in the
order that it is specified on the command line. If a symbol in that archive is
needed to resolve an undefined symbol referred to by an object in an archive
that appears later on the command line, the linker would not be able to
resolve that reference. By grouping the archives, they all be searched
repeatedly until all possible references are resolved.

Using this option has a significant performance cost. It is best to use it
only when there are unavoidable circular references between two or more
archives.


Here's how this helps in our case:

$ gcc simplemain.o -L. -Wl,--start-group -lbar_dep -lfunc_dep -Wl,--end-group
$ ./a.out ; echo $?
24

It's interesting to note the "significant performance cost" warning in the
excerpt above. This explains why the linking process is the way it is.
Presumably, linkers could just re-scan the whole library list until no new
symbols got resolved. This would eliminate most circular-dependency and
linking order problems in the world, but it would also be slow. Linking is
already `a critical part of the compilation time` of large systems, since it
looks at the whole program and requires quite a bit of memory. It's better to
make it as fast as possible for well-behaved programs (that got their linking
    order right), and provide special options like groups for the difficult
circular dependency cases.

There's at least one another linker flag that can help us resolve the circular
dependency here. We can use the --undefined flag to tell the linker - "buddy,
           here's a symbol I want you to add to the undefined list". In our
           case this makes the link error go away even though the libraries
           are specified only once:

$ gcc simplemain.o -L. -Wl,--undefined=bar -lbar_dep -lfunc_dep
$ ./a.out ; echo $?
24


Figuring out why this works is left as an exercise to the reader.


Back to the original example

Let's go back to the example this article started with. main assumes it gets
the correct memcpy from the C library, but the memcpy it gets linked with does
something else. Assuming the memcpy here was packed into the libstray_memcpy.a
library:

$ gcc  -L. main_using_memcpy.o -lstray_memcpy
$ ./a.out
Segmentation fault (core dumped)

This is the expected behavior. Since -lstray_memcpy was provided after
main_using_memcpy.o on the command-line, it gets linked in. But what happens
if the order is reversed:

$ gcc  -L. -lstray_memcpy main_using_memcpy.o
$ ./a.out ; echo $?
5

*gcc-link-c-library*
The program links and works correctly. The reason for this is simple: even
`without us explicitly asking for it, gcc asks the linker to link the C library`
as well. The full linker invocation command of gcc is pretty complex, and can
be examined by passing the -### flag to gcc. But in our case this amounts to:

$ gcc  -L. -lstray_memcpy main_using_memcpy.o -lc

// note that -lc at the end


When the linker sees -lstray_memcpy, the symbol table does not yet have an
undefined entry for memcpy, so the object file with the wrong function does
not get linked. The linker adds this undefined entry only after it sees
main_using_memcpy.o. Then, when it reaches -lc, the object file holding memcpy
from the C library does get linked in because by now memcpy is on the
undefined list.


Conclusion

The algorithm used by the linker to resolve symbols between objects and
libraries is pretty simple. As long as you keep it in mind, linker errors and
related problems should be easy to understand. If you still run into
problematic situations you're not sure how to resolve, this article mentioned
two tools that can be very useful in debugging such problems: one is nm, which
shows the symbol table of an object or a whole library. 

The other is the *gcc-check* -### flag that gcc accepts and as a result shows
the full commands it passes to the underlying tools.

// KT. When use this option, it appears that linker cannot detect
duplicated symbols and use one of symbols. There are duplicated in the same
lib archive but picked up wrong one. 


-Xlinker --start-group 
/home/kyoupark/asan/i686-nptl-linux-gnu-vstb-630/i686-nptl-linux-gnu/lib/libasan.a
-L/home/kyoupark/STB_SW_vstb_clean/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/LittleEndian/release/lib
-lm -lpthread -lrt -ldl -ltirpc -lsky_airplay_helper -lminizip -ljsoncpp -lglib
-lgio -lgobject -ldbus -lz -lasan
-Xlinker --end-group -o


={============================================================================
*kt_linux_gcc_400* gcc-option-warning

https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#Warning-Options

3.8 Options to Request or Suppress Warnings

-Werror
  Make all warnings into errors.

-Werror=
Make the specified warning into an error. The specifier for a warning is
appended; for example -Werror=switch turns the warnings controlled by -Wswitch
into errors. This switch takes a negative form, to be used to negate -Werror
for specific warnings; for example -Wno-error=switch makes -Wswitch warnings
  not be errors, even when -Werror is in effect.

The warning message for each controllable warning includes the option that
controls the warning. That option can then be used with -Werror= and
-Wno-error= as described above. (Printing of the option in the warning message
    can be disabled using the -fno-diagnostics-show-option flag.)

Note that specifying -Werror=foo automatically implies -Wfoo. However,
     -Wno-error=foo does not imply anything.

<e>
option that is set from -Wall

error: unknown warning option '-Werror=unused-but-set-variable';
  did you mean '-Werror=unused-const-variable'? [-Werror,-Wunknown-warning-option]


-Wall
  This enables all the warnings about constructions that some users consider
  questionable, and that are easy to avoid (or modify to prevent the warning),
  even in conjunction with macros. This also enables some language-specific
  warnings described in C++ Dialect Options and Objective-C and Objective-C++
  Dialect Options.

-Wall turns on the following warning flags:
  ...
  -Wnarrowing (only for C++)

-Wextra
  This enables some extra warning flags that are not enabled by -Wall. (This
      option used to be called -W. The older name is still supported, but the
      newer name is more descriptive.)

  -Wclobbered  
  -Wempty-body  
  -Wignored-qualifiers 
  -Wmissing-field-initializers  
  -Wmissing-parameter-type (C only)  
  -Wold-style-declaration (C only)  
  -Woverride-init  
  -Wsign-compare  
  -Wtype-limits  
  -Wuninitialized  
  -Wunused-parameter (only with -Wunused or -Wall) 
  -Wunused-but-set-parameter (only with -Wunused or -Wall)  


<gcc-warining-on-off>

You can request many specific warnings with options beginning with `-W`, for
example -Wimplicit to request warnings on implicit declarations. Each of these
specific warning options also has a negative form beginning `-Wno-` to turn
off warnings; for example, -Wno-implicit. This manual lists only one of the
two forms, whichever is not the default. For further language-specific options
also refer to C++ Dialect Options and Objective-C and Objective-C++ Dialect
Options.


Some options, such as -Wall and -Wextra, turn on other options, such as
`-Wunused`, which may turn on further options, such as -Wunused-value. The
combined effect of positive and negative forms is that more specific options
have priority over less specific ones, independently of their position in the
command-line. For options of the same specificity, the last one takes effect.
Options enabled or disabled via pragmas (see Diagnostic Pragmas) take effect
as if they appeared at the end of the command-line.


-Wunused-but-set-variable

Warn whenever a local variable is assigned to, but otherwise unused (aside
    from its declaration). This warning is enabled by -Wall.
To suppress this warning use the unused attribute (see Variable Attributes).
This warning is also enabled by -Wunused, which is enabled by -Wall.


3.5 Options Controlling C++ Dialect *gcc-cpp-options*

This section describes the command-line options that are only meaningful for
C++ programs. You can also use most of the GNU compiler options regardless of
what language your program is in. 

-Wno-narrowing (C++ and Objective-C++ only)
  For C++11 and later standards, narrowing conversions are diagnosed by
  default, as required by the standard. A narrowing conversion from a constant
  produces an error, and a narrowing conversion from a non-constant produces a
  warning, `but -Wno-narrowing suppresses the diagnostic.`

  Note that this does not affect the meaning of well-formed code; narrowing
  conversions are still considered ill-formed in SFINAE contexts.

  With -Wnarrowing in C++98, warn when a narrowing conversion prohibited by
  C++11 occurs within ‘{ }’, e.g.

  int i = { 2.2 }; // error: narrowing from double to int

  This flag is included in -Wall and -Wc++11-compat.

<ex>
error: narrowing conversion of ''\37777777777'' from 'char' to 'unsigned char' inside { } [-Wnarrowing]

typedef enum {
  FORWARD_STATUS,
  ...
} x;

		unsigned char	buf[] = {(char)FORWARD_STATUS,(char)0xff};


<gcc-option-warning-unused-parameter> -Wunused-parameter

// $ g++ -std=c++11 -Werror -Wall -Wextra t_cxx.cpp 
// t_cxx.cpp:13:39: error: unused parameter ‘arg3’ [-Werror=unused-parameter]
//  void t_cxx_01(int arg1, int arg2, int arg3)
//                                        ^
// cc1plus: all warnings being treated as errors

void t_cxx_01(int arg1, int arg2, int arg3)
{
    std::cout << "args: arg1: " << arg1 
        << ", arg2: " << arg2 << endl;
}

// OK

void t_cxx_02(int arg1, int arg2, int)
{
    std::cout << "args: arg1: " << arg1 
        << ", arg2: " << arg2 << endl;
}

// https://stackoverflow.com/questions/1486904/how-do-i-best-silence-a-warning-about-unused-variables
// You can put it in "(void)var;" expression (does nothing) so that a compiler
// sees it is used. This is portable between compilers.
//
// #define UNUSED(expr) do { (void)(expr); } while (0)
//
// UNUSED(arg3);

void t_cxx_03(int arg1, int arg2, int arg3)
{
    (void)arg3;

    std::cout << "args: arg1: " << arg1 
        << ", arg2: " << arg2 << endl;
}


={============================================================================
*kt_linux_gcc_400* gcc-option-cpp gcc-cpp

http://gcc.gnu.org/onlinedocs/cpp/

/usr/bin/cpp

The C preprocessor, often known as cpp, is a "macro processor" that is used
automatically by the C compiler to transform your program before compilation.
It is called a macro processor because it allows you to define 'macros', which
are brief abbreviations for longer constructs. 

https://gcc.gnu.org/onlinedocs/gcc/Preprocessor-Options.html#Preprocessor-Options

3.12 Options Controlling the Preprocessor

These options control the C preprocessor, which is run on each C source file
before actual compilation.

If you use the -E option, nothing is done except preprocessing. Some of these
options make sense only together with -E because they cause the preprocessor
output to be unsuitable for actual compilation.

In addition to the options listed here, there are a number of options to
control search paths for include files documented in Directory Options.
Options to control preprocessor diagnostics are listed in Warning Options. 

-include file
    Process file as if #include "file" appeared as the first line of the
    primary source file. However, the first directory searched for file is the
    preprocessor's working directory instead of the directory containing the
    main source file. If not found there, it is searched for in the remainder
    of the #include "..." search chain as normal.

    If multiple -include options are given, the files are included in the
    order they appear on the command line. 

-imacros file
    Exactly like -include, except that any output produced by scanning file is
    thrown away. Macros it defines remain defined. This allows you to acquire
    all the macros from a header without also processing its declarations.

    All files specified by -imacros are processed before all files specified
    by -include. 

<ex>
note: does generate a.out

gcc -imacros "/usr/include/linux/version.h" -x c sample.c


={============================================================================
*kt_linux_gcc_203* gcc-option-cpp-search gcc-cpp-isystem

-isystem dir 

Mark it as a system directory, so that it gets the same special treatment as
is applied to the standard system directories. See System Headers. 

*option-sysroot*
If dir begins with =, then the = will be replaced by the sysroot prefix; see
--sysroot and -isysroot. 


2.8 System Headers

The header files declaring interfaces to the operating system and runtime
libraries often cannot be written in strictly conforming C. Therefore, GCC
gives code found in system headers 'special' treatment. All warnings, other
than those generated by '#warning' are suppressed while GCC is processing a
system header. Macros defined in a system header are immune to a few warnings
wherever they are expanded. This immunity is granted on an ad-hoc basis, when
we find that a warning generates lots of false positives because of code in
macros defined in system headers.

Normally, only the headers found in specific directories are considered system
headers. These directories are determined when GCC is compiled. There are,
however, two ways to make normal headers into system headers.

The -isystem command line option adds its argument to the list of directories
to search for headers, just like -I. Any headers found in that directory will
be considered system headers. 

All directories named by -isystem are searched after all directories named by
-I, no matter what their order was on the command line. If the same directory
is named by both -I and -isystem, the -I option is ignored. GCC provides an
informative message when this occurs if -v is used.


={============================================================================
*kt_linux_gcc_400* gcc-option-cpp-dependancies

// ????
// <wall-option>
// -Wall
// 
// Turns on all optional warnings which are desirable for normal code. At present
// this is -Wcomment, -Wtrigraphs, -Wmultichar and a warning about integer
// promotion causing a change of sign in #if expressions. Note that many of the
// preprocessor's warnings are on by default and have no options to control them. 
// 
// note: it is different from -Wall in warning options.

<dependancies-output>
Found that there is .deps dir which has dependancy output.

$(AM_V_CC)$(COMPILE) -MT $@ -MD -MP -MF ./deps/$*.Tpo -c -o $@ $<

-DNDEBUG -O2 -pipe -MT nexusMgr.lo -MD -MP -MF .deps/nexusMgr.Tpo -c src/nexusMgr.c  -fPIC -DPIC -o
.libs/nexusMgr.o

-MP
This option instructs CPP to add a phony target for each dependency other than
the main file, causing each to depend on nothing. These dummy rules work
around errors make gives if you remove header files without updating the
Makefile to match.

    This is typical output:

              test.o: test.c test.h
              
              test.h:

-MD
-MD is equivalent to -M -MF file, except that -E is not implied. The driver
determines file based on whether an -o option is given. If it is, the driver
uses its argument but with a suffix of .d, otherwise it takes the name of the
input file, removes any directory components and suffix, and applies a .d
suffix.

If -MD is used in conjunction with -E, any -o switch is understood to specify
the dependency output file (see -MF), but if used without -E, each -o is
understood to specify a target object file.

Since -E is not implied, -MD can be used to generate a dependency output file
as a side-effect of the compilation process. 


-M
Instead of outputting the result of preprocessing, output a rule suitable for
`make` describing the dependencies of the main source file. The preprocessor
outputs one make rule containing the object file name for that source file, a
colon, and the names of 'all' the 'included' files, including those coming
from -include or -imacros command line options.

Unless specified explicitly (with -MT or -MQ), the object file name consists
of the name of the source file with any suffix replaced with object file
suffix and with any leading directory parts removed. If there are many
included files then the rule is split into several lines using \ newline. The
rule has no commands.

This option does not suppress the preprocessor's debug output, such as -dM. To
'avoid' mixing such debug output with the dependency rules you should
explicitly specify the dependency output file with -MF, or use an environment
variable like DEPENDENCIES_OUTPUT (see Environment Variables). Debug output
will still be sent to the regular output stream as normal.

Passing -M to the driver implies -E, and suppresses warnings with an implicit
-w.

note: gcc -M tgetopt.c

tgetopt.o: tgetopt.c /usr/include/unistd.h /usr/include/features.h \
 /usr/include/x86_64-linux-gnu/bits/predefs.h \
 /usr/include/x86_64-linux-gnu/sys/cdefs.h \
 /usr/include/x86_64-linux-gnu/bits/wordsize.h \
 /usr/include/x86_64-linux-gnu/gnu/stubs.h \
 ...


-MM
    Like -M but do not mention header files that are found in system header
    directories, nor header files that are included, directly or indirectly,
from such a header.

    This implies that the choice of angle brackets or double quotes in an
    ‘#include’ directive does not in itself determine whether that header will
    appear in -MM dependency output. This is a slight change in semantics from
    GCC versions 3.0 and earlier. 


-MT target
    Change the target of the rule emitted by dependency generation. By default
    CPP takes the name of the main input file, deletes any directory
    components and any file suffix such as ‘.c’, and appends the platform's
    usual object suffix. The result is the target.

    An -MT option will set the target to be exactly the string you specify. If
    you want multiple targets, you can specify them as a single argument to
    -MT, or use multiple -MT options.

    For example, -MT '$(objpfx)foo.o' might give

              $(objpfx)foo.o: foo.c


-MF file
When used with -M or -MM, `specifies a file to write the dependencies to` If
no -MF switch is given the preprocessor sends the rules to the same place it
would have sent preprocessed output.

When used with the driver options -MD or -MMD, -MF overrides the default
dependency output file. 


={============================================================================
*kt_linux_gcc_400* gcc-option-cpp-check-defines

<cpp-debug>

-E
Stop after the preprocessing stage; do not run the compiler proper. The output
is in the form of preprocessed source code, which is sent to the standard
output.

Input files that don’t require preprocessing are ignored. 

If you use the -E option, `nothing is done except preprocessing` Some of these
options make sense only together with -E because they cause the preprocessor
output to be 'unsuitable' for actual compilation. 

$ g++ -std=c++0x -E useargs.cpp > out.txt
$ g++ -std=c++0x -E useargs.cpp -o out.txt

note:
Shows code replaced with define values.


*cpp-debug-cpp-check-define*
-dCHARS -dletters
Says to make debugging dumps during compilation as specified by letters. The
flags documented here are those relevant to the preprocessor.

CHARS is a sequence of one or more of the following characters, and must not
be preceded by a space.  Other characters are interpreted by the compiler
proper, or reserved for future versions of GCC, and so are silently ignored.
If you specify characters whose behavior conflicts, the result is undefined.

-dM
   Instead of the normal output, `generate a list of '#define' directives` for
   all the macros defined during the execution of the preprocessor, including
   predefined macros. This gives you a way of 'finding' out what is predefined
   in your version of the `preprocessor`. Assuming you have no file foo.h, the
   command

   touch foo.h; cpp -dM foo.h

   will show all the predefined macros.

   If you use -dM without the -E option, -dM is interpreted as a synonym for
   -fdump-rtl-mach. See Debugging Options. 

   note:
   Have to use -E tgether to see user defines.

https://gcc.gnu.org/onlinedocs/cpp/Invocation.html#Invocation

Most often when you use the C preprocessor you do not have to invoke it
explicitly: the C compiler does so automatically. However, the preprocessor is
sometimes useful on its own. You can invoke the preprocessor either with the
cpp command, or via gcc -E. In GCC, the preprocessor is actually integrated
with the compiler rather than a separate program, and both of these commands
invoke GCC and tell it to stop after the preprocessing phase.


<cpp-check-typedef>
echo '#include <stdlib.h>' | cpp -I/usr/include | grep -n size_t
gcc -E sample-two.c | egrep '\<size_t\>'

note:
Both do the job but using option -E requires c file.

g++ -E -dM - < /dev/null | awk '/GXX_ABI/ {print $3}'

This command will display 102 for g++ compilers using version 1 of the C++
ABI, and 1002 for g++ compilers using version 2 of the C++ ABI.


<cpp-check-macro>
gcc -dM -E -std=gnu11 - < /dev/null | grep STDC_VER.

$ mipsel-linux-gcc -dM -E - < /dev/null | ag GNUC
#define __GNUC_PATCHLEVEL__ 3
#define __GNUC__ 4
#define __GNUC_MINOR__ 5
#define __GNUC_GNU_INLINE__ 1

<ex>
$ cpp -dM tgetopt.c
$ echo '#include <errno.h>' | cpp -dM

<ex>
$ /opt/toolchains/humax-dtr_t1000/bin/mipsel-linux-g++ -E -dM -std=c++0x -x c++ /dev/null | ag _G
#define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 1
#define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 1
#define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4 1
#define __GNUC_PATCHLEVEL__ 5
#define __GXX_EXPERIMENTAL_CXX0X__ 1 ~
#define __GXX_WEAK__ 1
#define __GNUC__ 4
#define __GXX_RTTI 1
#define __GNUG__ 4
#define __GXX_ABI_VERSION 1002
#define __GCC_HAVE_BUILTIN_MIPS_CACHE 1
#define __GNUC_MINOR__ 4
#define __GNUC_GNU_INLINE__ 1
#define _GNU_SOURCE 1
$ /opt/toolchains/humax-dtr_t1000/bin/mipsel-linux-g++ -E -dM -x c++ /dev/null | ag _G
#define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 1
#define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 1
#define __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4 1
#define __GNUC_PATCHLEVEL__ 5
#define __GXX_WEAK__ 1
#define __GNUC__ 4
#define __GXX_RTTI 1
#define __GNUG__ 4
#define __GXX_ABI_VERSION 1002
#define __GCC_HAVE_BUILTIN_MIPS_CACHE 1
#define __GNUC_MINOR__ 4
#define __GNUC_GNU_INLINE__ 1
#define _GNU_SOURCE 1

<ex>
/home/kyoupark/asan/i686-nptl-linux-gnu-vstb-630/bin/i686-nptl-linux-gnu-gcc
-Wall -Wmissing-prototypes -Wshadow -Wpointer-arith -Wstrict-prototypes
-Wmissing-declarations -Werror -std=c99 -MD -MF
out/release_dbg/vstb/release_dbg/deps/glue/src/ceej_main.d -D_XOPEN_SOURCE=600
-D__USE_XOPEN2K -D_GNU_SOURCE -DCDI -DCDI_IOCTL_DEFINITIONS="ioctl_magic.h"
-DLINT_ERRORS_ONLY= -DLINT_RESTORE= -DPLATFORM_VSTB -DIS_LITTLE_ENDIAN
-D__LITTLE_ENDIAN -fdata-sections -ffunction-sections
-Wno-error=unused-but-set-variable -Os -fno-strict-aliasing -g -DRELEASE_DBG
-DDIAG_RELEASE_DBG -DJPA_GFX_SURFACE_DEBUG -DSI_BUILD
-DJPA_HEAPDUMP_CRASH_WORKAROUND -Imodules/jvm-glue/src/c/inc
-Imodules/jpa-impl/src/c/inc -Imodules/jvm/inc -I.
-I/home/kyoupark/STB_SW_o/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/components/darwin_platform_components/darwin_atg/inc
-Iout/release_dbg/vstb/release_dbg/include/adaptors -E -dM
modules/jvm-glue/src/c/src/ceej_main.c


={============================================================================
*kt_linux_gcc_400* gcc-option-cpp-where-comes-from

<cpp-debug>
-dCHARS -dletters

-dN
    Like -dD, but emit only the macro names, not their expansions. 

-dI
    Output ‘#include’ directives in addition to the result of preprocessing. 


<cpp-output>
https://gcc.gnu.org/onlinedocs/cpp/Preprocessor-Output.html

9 Preprocessor Output

When the C preprocessor is used with the C, C++, or Objective-C compilers, it
is integrated into the compiler and communicates a stream of binary tokens
directly to the compiler's parser. However, it can also be used in the more
conventional standalone mode, where it produces textual output.

The output from the C preprocessor looks much like the input, except that all
preprocessing directive lines have been replaced with blank lines and all
comments with spaces. Long runs of blank lines are discarded.

The ISO standard specifies that it is implementation defined whether a
preprocessor preserves whitespace between tokens, or replaces it with e.g. a
single space. In GNU CPP, whitespace between tokens is collapsed to become a
single space, with the exception that the first token on a non-directive line
is preceded with sufficient spaces that it appears in the same column in the
preprocessed output that it appeared in the original source file. This is so
the output is easy to read. CPP does not insert any whitespace where there was
none in the original source, except where necessary to prevent an accidental
token paste.

Source file name and line number information is conveyed by lines of the form

     # linenum filename flags

These are called linemarkers. They are inserted as needed into the output (but
    never within a string or character constant). They mean that the following
line originated in file filename at line linenum. filename will never contain
any non-printing characters; they are replaced with octal escape sequences.

After the file name comes zero or more flags, which are ‘1’, ‘2’, ‘3’, or ‘4’.
If there are multiple flags, spaces separate them. Here is what the flags
mean:

‘1’
    This indicates the start of a new file.

‘2’
    This indicates returning to a file (after having included another file).

‘3’
    This indicates that the following text comes from a system header file, so
    certain warnings should be suppressed.

‘4’
    This indicates that the following text should be treated as being wrapped
    in an implicit extern "C" block. 

As an extension, the preprocessor accepts linemarkers in non-assembler input
files. They are treated like the corresponding ‘#line’ directive, (see Line
    Control), except that trailing flags are permitted, and are interpreted
with the meanings described above. If multiple flags are given, they must be
in ascending order.

Some directives may be duplicated in the output of the preprocessor. These are
‘#ident’ (always), ‘#pragma’ (only if the preprocessor does not handle the
    pragma itself), and ‘#define’ and ‘#undef’ (with certain debugging
      options). If this happens, the ‘#’ of the directive will always be in
    the first column, and there will be no space between the ‘#’ and the
    directive name. If macro expansion happens to generate tokens which might
    be mistaken for a duplicated directive, a space will be inserted between
    the ‘#’ and the directive name. 

<ex>
// xcommon.c
#include "xcommon.h"

// xcommon.h
#include <limits.h>

# 14 "../../support/include/xcommon.h" 2
#include <limits.h>
# 14 "../../support/include/xcommon.h"
# 1 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include/limits.h" 1 3 4

#include "syslimits.h"
# 11 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include/limits.h" 3 4
# 1 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include/syslimits.h" 1 3 4

#define _GCC_NEXT_LIMITS_H
#include_next <limits.h>
# 7 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include/syslimits.h" 3 4
# 1 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include/limits.h" 1 3 4
# 122 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include/limits.h" 3 4
#include_next <limits.h>
# 122 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include/limits.h" 3 4
# 1 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/limits.h" 1 3 4
# 24 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/limits.h" 3 4
#define _LIBC_LIMITS_H_

// /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/limits.h
// #ifdef	__USE_POSIX
// /* POSIX adds things to <limits.h>.  */
// # include <bits/posix1_lim.h>
// #endif

# 144 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/limits.h" 3 4
#include <bits/posix1_lim.h>
# 144 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/limits.h" 3 4
# 1 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/bits/posix1_lim.h" 1 3 4
# 26 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/bits/posix1_lim.h" 3 4
#define _BITS_POSIX1_LIM_H

// /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/bits/posix1_lim.h
// /* Get the implementation-specific values for the above.  */
// #include <bits/local_lim.h>

#include <bits/local_lim.h>
# 130 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/bits/posix1_lim.h" 3 4
# 1 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/bits/local_lim.h" 1 3 4
# 26 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/bits/local_lim.h" 3 4

// /* The kernel sources contain a file with all the needed information.  */
// #include <linux/limits.h>

#include <linux/limits.h>
# 36 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/bits/local_lim.h" 3 4
# 1 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/sys-include/linux/limits.h" 1 3 4

#define PATH_MAX


={============================================================================
*kt_linux_gcc_400* gcc-option-code-gen

3.16 Options for Code Generation Conventions

https://gcc.gnu.org/onlinedocs/gcc/Code-Gen-Options.html#Code-Gen-Options

-fpic or -fPIC

Generate position-independent code (PIC) suitable for use in a shared library,
if supported for the target machine. Such code accesses all constant addresses
  through a global offset table (GOT). The dynamic loader resolves the GOT
    entries when the program starts (the dynamic loader is not part of GCC; it
        is part of the operating system). If the GOT size for the linked
    executable exceeds a machine-specific maximum size, you get an error
    message from the linker indicating that -fpic does not work; in that case,
            recompile with -fPIC instead. (These maximums are 8k on the SPARC
                and 32k on the m68k and RS/6000. The 386 has no such limit.)

Position-independent code requires special support, and therefore works only
on certain machines.  For the 386, GCC supports PIC for System V but not for
the Sun 386i. Code generated for the IBM RS/6000 is always
position-independent.

When this flag is set, the macros __pic__ and __PIC__ are defined to 1. 


={============================================================================
*kt_linux_gcc_400* gcc-option-optimization

https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#Optimize-Options

3.10 Options That Control Optimization

These options control various sorts of optimizations.

*when-no-optimization*
Without any optimization option, the compiler's goal is to reduce the cost of
compilation and to make debugging produce the expected results. 

Statements are independent: if you stop the program with a breakpoint between
statements, you can then assign a new value to any variable or change the
program counter to any other statement in the function and get exactly the
results you expect from the source code.

Turning on optimization flags makes the compiler attempt to improve the
performance and/or code size at the expense of compilation time and possibly
the ability to debug the program.

The compiler performs optimization based on the knowledge it has of the
program. Compiling multiple files at once to a single output file mode allows
the compiler to use information gained from all of the files when compiling
each of them.

Not all optimizations are controlled directly by a flag. Only optimizations
that have a flag are listed in this section.

Most optimizations are only enabled if an -O level is set on the command line.
`Otherwise they are disabled`, even if individual optimization flags are
specified.

// note: do not work on bcm gcc
Depending on the target and how GCC was configured, a slightly different set
of optimizations may be enabled at each -O level than those listed here. You
can invoke GCC with -Q --help=optimizers to find out the exact set of
optimizations that are enabled at each level. 

See Overall Options, for examples. 

-O
-O1
  Optimize. Optimizing compilation takes somewhat more time, and a lot more
  memory for a large function.

  With -O, the compiler tries to reduce code size and execution time, without
  performing any optimizations that take a great deal of compilation time.

  -O turns on the following optimization flags:

-O2
  Optimize even more. GCC performs nearly all supported optimizations that do
  not involve a space-speed tradeoff. As compared to -O, this option increases
  both compilation time and the performance of the generated code.

  -O2 turns on all optimization flags specified by -O. It also turns on the
  following optimization flags:

-Os
  Optimize for size. -Os enables all -O2 optimizations that do not typically
  increase code size. It also performs further optimizations designed to
  reduce code size.

  -Os disables the following optimization flags:

  -falign-functions  -falign-jumps  -falign-loops -falign-labels
  -freorder-blocks  -freorder-blocks-algorithm=stc
  -freorder-blocks-and-partition  -fprefetch-loop-arrays

*gcc-optimization-off*
-O0
  Reduce compilation time and make debugging produce the expected results.
  `This is the default`

-Og
  Optimize debugging experience. -Og enables optimizations that do not
  interfere with debugging. It should be the optimization level of choice for
  the standard edit-compile-debug cycle, offering a reasonable level of
  optimization while maintaining fast compilation and a good debugging
  experience.


<gcc-frame-pointer>
-fomit-frame-pointer
  Don't keep the frame pointer in a register for functions that don't need
  one.  This avoids the instructions to save, set up and restore frame
  pointers; it also makes an extra register available in many functions. It
  also makes debugging impossible on some machines.

  On some machines, such as the VAX, this flag has no effect, because the
  standard calling sequence automatically handles the frame pointer and
  nothing is saved by pretending it doesn't exist. The machine-description
  macro FRAME_POINTER_REQUIRED controls whether a target machine supports this
  flag.  See Register Usage.

  Enabled at levels -O, -O2, -O3, -Os. 

  From LPI 41.

  In addition, on some architectures, such as x86-32, the
  `-fomit-frame-pointer` option should not be specified because this makes
  debugging impossible. On some architectures, such as x86-64, this option is
  enabled by default since it doesn't prevent debugging. For the same reason,
executables and libraries should not be stripped of debugging information
  using strip(1).

  From ASAN,

  To get nicer stacktraces, use `-fno-omit-frame-pointer`


={============================================================================
*kt_linux_gcc_400* gcc-option-inline gcc-gnu-extension

6 Extensions to the C Language Family

GNU C provides several language features not found in ISO standard C. 

(The -pedantic option directs GCC to print a warning message if any of these
 features is used.) To test for the availability of these features in
conditional compilation, check for a predefined macro __GNUC__, which is
always defined under GCC.

These extensions are available in C and Objective-C. Most of them are also
available in C++. See Extensions to the C++ Language, for extensions that
apply only to C++.

Some features that are in ISO C99 but not C90 or C++ are also, as extensions,
accepted by GCC in C90 mode and in C++.


6.39 An Inline Function is As Fast As a Macro

To declare a function inline, use the inline keyword in its declaration, like
this:

`static inline` int
inc (int *a)
{
  return (*a)++;
}

If you are writing a header file to be included in ISO C90 programs, write
__inline__ instead of inline. See Section 6.45 [Alternate Keywords], page 443.

The three types of inlining behave similarly in `two important cases:` 

when the inline keyword is used on a static function, like the example above,

and when a function is first declared without using the inline keyword and
then is defined with inline, like this:

`extern` int inc (int *a);
`inline` int
inc (int *a)
{
  return (*a)++;
}

In both of these common cases, the program behaves the same `as if you had not`
used the inline keyword, except for its speed.

When a function is both inline and static, if all calls to the function are
integrated into the caller, and the function’s address is never used, then the
function’s own assembler code is never referenced. In this case, GCC does not
actually output assembler code for the function, unless you specify the option
‘-fkeep-inline-functions’.

// dosen't seem to work
// Using ‘-Winline’ warns when a function marked inline could not be
// substituted, and gives the reason for the failure.

GCC does not inline any functions when not optimizing unless you specify the
‘always_inline’ attribute for the function, like this:

/* Prototype. */
inline void foo (const char) __attribute__((always_inline));


The remainder of this section is specific to GNU C90 inlining.

When an inline function is not static, then the compiler must assume that
there may be calls from other source files; since a global symbol can be
defined only once in any program, the function must not be defined in the
other source files, so the calls therein cannot be integrated. Therefore, a
`non-static inline function is always compiled on its own` in the usual
fashion.

If you specify both inline and extern in the function definition, then the
definition is used only for inlining. In no case is the function compiled on
its own, not even if you refer to its address explicitly. Such an address
becomes an external reference, as if you had only declared the function, and
had not defined it.

This combination of inline and extern has almost the effect of a macro. The
way to use it is to put a function definition in a header file with these
keywords, and put another copy of the definition (lacking inline and extern)
  in a library file. The definition in the header file causes most calls to
  the function to be inlined. If any uses of the function remain, they refer
  to the single copy in the library.


<ex>
Under the same codes and build options, gcc 4.x links okay but gcc 6.x not.
Codes are:

#define MHWMEMORYINLINE __inline__

// CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/mem_allocator.c
MHWMEMORYINLINE SYSTEM_STATUS MHWMemGetIndex(MEMMAN_API_MemoryPoolHandle *pool,
                                             uint32_t *theIndex)
{
}

// CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/mem_allocator.h
MHWMEMORYINLINE SYSTEM_STATUS MHWMemGetIndex(MEMMAN_API_MemoryPoolHandle * pool,
                                             uint32_t * theIndex);

// CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/memman_fx.c
#include "mem_allocator.h"

SYSTEM_STATUS MEMMAN_API_GetIndexP(MEMMAN_API_MemoryPoolHandle *pool,
                                   uint32_t *theIndex)
{
    status = MHWMemGetIndex(pool, theAllocator, theElement, theIndex);
}

In 6.x case, mem_allocator.o do not have definitions and see undefined symbol
errors in final link stage.

CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/mem_allocator.h:42:31: warning: inline function 'MHWMemGetIndex' declared but never defined
 MHWMEMORYINLINE SYSTEM_STATUS MHWMemGetIndex(MEMMAN_API_MemoryPoolHandle * pool,

STB_SW/CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/memman_fx.c:359: undefined reference to `MHWMemGetIndex'

Have tried with/without -O? and staic __inline__ but do not fix the issue.
When not use inline at all, it builds okay.

https://runtimeverification.com/blog/?p=21

ISO C inline semantics

Many developers are not completely familiar with the semantics of the inline
function specifier in C99 and C11. For example, consider the following
program:

inline int foo(void) { return 5; }
int main() {
return foo();
}

When I compile this program with gcc -std=c11 test.c -O2, everything looks
great. I have no idea that I have written an undefined C program. Suddenly one
day for whatever reason I forget to specify -O2 when compiling, and see an
error: undefined reference to 'foo'. What is going on?

Well, it turns out you have fallen afoul of one of the side effects of the way
GCC inlines functions. When I compile with -O2, GCC performs inline
substitution as one of its optimizing passes. This causes the function foo to
be inlined at its call site in the main function, such that the main function
no longer contains any function calls to foo. As a result of this, the program
compiles successfully.

But wait, you say, why does that matter? Isn't foo defined right there? Well,
    no, not really. According to the C11 standard section 6.7.4, which deals
    with the inline specifier, a function with external linkage (ie,
        generally, one not declared with the static keyword), in which all
    declarations of the function at file scope include the inline keyword but
    not the extern keyword, is what is called an inline definition. Inline
    definitions do not count as what's called an external definition of a
    function (ie, storage allocated in the binary for the function's compiled
        instructions). So actually, the definition you provided for foo is
    never provided to the linker, causing the linker error you saw above. You
    can fix this error in one of three ways: by giving the function internal
    linkage (ie, specifying static inline), by turning your definition into an
    external definition (ie,

extern int foo(void);
inline int foo(void) {
  return 5;
}

), or by keeping the inline definition, but providing an external definition
in another file. In the last case, the C compiler is free to choose to either
link against the external definition, or inline the inline definition. Which
behavior you get at any given call site is unspecified, and GCC typically
performs performance analysis to try to guess which will generate better
performance. Any of these three alterations will result in a defined C
program.


={============================================================================
*kt_linux_gcc_400* gcc-option-debug

3.9 Options for Debugging Your Program

https://gcc.gnu.org/onlinedocs/gcc/Debugging-Options.html#Debugging-Options

GCC allows you to use -g with -O. The shortcuts taken by optimized code may
occasionally produce surprising results: some variables you declared may not
exist at all; flow of control may briefly move where you did not expect it; some
statements may not be executed because they compute constant results or their
values are already at hand; some statements may execute in different places
because they have been moved out of loops.

Nevertheless it proves possible to debug optimized output. This makes it
reasonable to use the optimizer for programs that might have bugs.

If you are not using some other optimization option, consider using -Og with
-g. `With no -O option at all`, some compiler passes that collect information
useful for debugging do not run at all, so that -Og may result in a better
debugging experience. 


-g
    Produce debugging information in the operating system's native format
    (stabs, COFF, XCOFF, or DWARF 2). GDB can work with this debugging
    information.

    On most systems that use stabs format, -g enables use of extra debugging
    information that only GDB can use; this extra information makes debugging
    work better in GDB but probably makes other debuggers crash or refuse to
    read the program. If you want to control for certain whether to generate the
    extra information, use -gstabs+, -gstabs, -gxcoff+, -gxcoff, or -gvms (see
            below).

-ggdb
    Produce debugging information for use by GDB. This means to use the most
    expressive format available (DWARF, stabs, or the native format if neither
            of those are supported), including GDB extensions if at all
    possible. 

-g`level`
-ggdb`level`
-gstabslevel
-gcofflevel
-gxcofflevel
-gvmslevel
    Request debugging information and also use level to specify how much
    information. The default level is 2.

    Level 0 produces no debug information at all. Thus, -g0 negates -g.

    Level 1 produces minimal information, enough for making backtraces in parts
    of the program that you don't plan to debug. This includes descriptions of
    functions and external variables, and line number tables, but no information
    about local variables.

    *gdb-cpp*
    Level 3 includes extra information, such as `all the macro definitions`
    present in the program. Some debuggers support macro expansion when you
    use -g3.

    -gdwarf does not accept a concatenated debug level, to avoid confusion with
    -gdwarf-level. Instead use an additional -glevel option to change the debug
    level for DWARF.


<ex>
#define CONST_VALUE (10)

// when no macro expansion

(gdb) p CONST_VALUE
No symbol "CONST_VALUE" in current context.

// when use either -ggdb3 or -g3

20	  cout << "value: " << CONST_VALUE << endl;
(gdb) n
value: 10
22	  string str("this is string");
(gdb) p CON
CONCAT       CONST_VALUE  
(gdb) p CONST_VALUE
$1 = 10


={============================================================================
*kt_linux_gcc_400* gcc-option-instrument

https://gcc.gnu.org/onlinedocs/gcc/Instrumentation-Options.html#Instrumentation-Options

3.11 Program Instrumentation Options

GCC supports a number of command-line options that control adding run-time
instrumentation to the code it normally generates. For example, one purpose of
instrumentation is collect profiling statistics for use in finding program hot
spots, code coverage analysis, or profile-guided optimizations. Another class
of program instrumentation is adding run-time checking to detect programming
errors like invalid pointer dereferences or out-of-bounds array accesses, as
well as deliberately hostile attacks such as stack smashing or C++ vtable
hijacking. There is also a general hook which can be used to implement other
forms of tracing or function-level instrumentation for debug or program
analysis purposes.

-fstack-protector
    Emit extra code to check for buffer overflows, such as stack smashing
    attacks. This is done by adding a guard variable to functions with
    vulnerable objects. This includes functions that call alloca, and
    functions with buffers larger than 8 bytes. The guards are initialized
    when a function is entered and then checked when the function exits. If a
    guard check fails, an error message is printed and the program exits.


={============================================================================
*kt_linux_gcc_400* gcc-pragma

// stops compilation
#error "unknwon cpu - you need to find out the stack grows downward or upward"

// get a mesg during compilation like:
// NDS_pthread.c:43:4: warning: #warning "xxx2" 
#warning "xxx2"


={============================================================================
*kt_linux_gcc_400* gcc-attribute

5.25 Attribute Syntax

This section describes the syntax with which __attribute__ may be used, and
the constructs to which attribute specifiers bind, for the C language. Some
details may vary for C++ and Objective-C. Because of infelicities in the
grammar for attributes, some forms described here may not be successfully
parsed in all cases.

There are some problems with the semantics of attributes in C++. For example,
there are no manglings for attributes, although they may affect code
  generation, so problems may arise when attributed types are used in
  conjunction with templates or overloading. Similarly, typeid does not
  distinguish between types with different attributes. Support for attributes
  in C++ may be restricted in future to attributes on declarations only, but
  not on nested declarators.

See Function Attributes, for details of the semantics of attributes applying
to functions. 

See Variable Attributes, for details of the semantics of attributes applying
to variables. 

See Type Attributes, for details of the semantics of attributes applying to
structure, union and enumerated types.


https://gcc.gnu.org/onlinedocs/gcc-4.1.2/gcc/Function-Attributes.html

5.24 Declaring Attributes of Functions

In GNU C, you declare certain things about functions which help the compiler
optimize function calls and check your code more carefully.

The keyword __attribute__ allows you to specify special attributes when making a
declaration. This keyword is followed by an attribute specification inside
`double parentheses` The following attributes are currently defined for
functions on all targets: 

noreturn, returns_twice, noinline, always_inline, flatten, pure, const,
nothrow, sentinel, format, format_arg, no_instrument_function, section,
constructor, destructor, used, unused, deprecated, `weak`, malloc, `alias`,
warn_unused_result, nonnull and externally_visible. 
  
Several other attributes are defined for functions on particular target
systems.

alias ("target")
The alias attribute causes the declaration to be emitted as an alias for
another symbol, which must be specified. For instance,

          void __f () { /* Do something. */; }
          void f () __attribute__ ((weak, alias ("__f")));
     
declares `f' to be a weak alias for `__f'. In C++, the mangled name for the
  target must be used. It is an error if `__f' is not defined in the same
  translation unit.

Not all target machines support this attribute. 


unused
    This attribute, attached to a function, means that the function is meant to
    be possibly unused. GCC will not produce a warning for this function.

note: can be used on object?

#define ZINC_UNUSED __attribute__((unused))

try
{
  ...
}
catch(ZINC_UNUSED const NS_RUBIDIUM_SYSTEM::NetworkProblemException& e)
{
  ...
}

used
    This attribute, attached to a function, means that code must be emitted for
    the function even if it appears that the function is 'not' referenced. This
    is useful, for example, when the function is referenced only in inline
    assembly.


format (archetype, string-index, first-to-check)

The format attribute specifies that a function takes printf, scanf, strftime
or strfmon style arguments which should be type-checked against a format
string. For example, the declaration:

          extern int
          my_printf (void *my_object, const char *my_format, ...)
                __attribute__ ((format (printf, 2, 3)));
     
causes the compiler to check the arguments in calls to my_printf for
consistency with the printf style format string argument my_format.

The parameter `archetype` determines how the format string is interpreted, and
should be printf, scanf, strftime or strfmon. (You can also use __printf__,
    __scanf__, __strftime__ or __strfmon__.) The parameter string-index
specifies which argument is the format string argument (starting from 1),
          while first-to-check is the number of the first argument to check
          against the format string. For functions where the arguments are not
          available to be checked (such as vprintf), specify the third
          parameter as zero. In this case the compiler only checks the format
          string for consistency. For strftime formats, the third parameter is
          required to be zero. Since non-static C++ methods have an implicit
          this argument, the arguments of such methods should be counted from
          two, not one, when giving values for string-index and
          first-to-check.

In the example above, the format string (my_format) is the second argument of
the function my_print, and the arguments to check start with the third
argument, so the correct parameters for the format attribute are 2 and 3.


<ex>
#define FORMAT(f, a)  __attribute__((format(printf, f, a)))

FORMAT(1,2)
void Report(const char *format, ...)
{
  va_list args;
  va_start(arg, format);
  SharedPrintCode(true, format, args);
  va_end(args);
}


The format attribute allows you to identify your own functions which take
format strings as arguments, so that GCC can check the calls to these
functions for errors. The compiler always (unless -ffreestanding or
    -fno-builtin is used) checks formats for the standard library functions
printf, fprintf, sprintf, scanf, fscanf, sscanf, strftime, vprintf, vfprintf
and vsprintf whenever such warnings are requested (using -Wformat), so there
is no need to modify the header file stdio.h. In C99 mode, the functions
snprintf, vsnprintf, vscanf, vfscanf and vsscanf are also checked. Except in
strictly conforming C standard modes, the X/Open function strfmon is also
checked as are printf_unlocked and fprintf_unlocked. See Options Controlling C
Dialect.

The target may provide additional types of format checks. See Format Checks
Specific to Particular Target Machines. 


<gcc-attribute-visibility> *symbol-visibility*

visibility ("visibility_type")

The visibility attribute `on ELF targets` causes the declaration to be emitted
with default, hidden, protected or internal visibility.

void __attribute__ ((visibility ("protected")))
f () { /* Do something. */; }

int i __attribute__ ((visibility ("hidden")));
         

See the ELF gABI for complete details, but the short story is:

default
  Default visibility is the normal case for ELF. This value is available for
  the visibility attribute to override other options that may change the
  assumed visibility of symbols.

#define ZINC_EXPORT __attribute__((visibility("default")))


hidden
  Hidden visibility indicates that the 'symbol' will not be placed into the
  dynamic symbol table, so no other module (executable or shared library) can
  reference it 'directly'. 

  // Then 'indirectly'?


internal 
  Internal visibility is like hidden visibility, but with additional processor
  specific semantics.  Unless otherwise specified by the psABI, GCC defines
  internal visibility to mean that the function is never called from another
  module. Note that hidden symbols, while they cannot be referenced directly
  by other modules, can be referenced indirectly via function pointers. By
  indicating that a symbol cannot be called from outside the module, GCC may
  for instance omit the load of a PIC register since it is known that the
  calling function loaded the correct value.  

protected 
  Protected visibility indicates that the symbol will be placed in the dynamic
  symbol table, but that references within the defining module will bind to
  the local symbol. That is, the symbol cannot be overridden by another
  module. 

Not all ELF targets support this attribute. 


3.18 Options for Code Generation Conventions

-fvisibility=default | internal | hidden | protected
    Set the default ELF image symbol visibility to the specified option-all
    symbols will be marked with this `unless overridden within the code.` Using
    this feature can very substantially improve linking and load times of shared
    object libraries, produce more optimized code, provide near-perfect API
    export and prevent symbol clashes. It is strongly recommended that you use
    this in any shared objects you distribute.

    Despite the nomenclature, default always means public ie; available to be
    linked against from outside the shared object. protected and internal are
    pretty useless in real-world usage so the only other commonly used option
    will be hidden. The "default" if -fvisibility isn't specified is default,
    i.e., make every symbol public-this causes the same behavior as previous
    versions of GCC.

    A good explanation of the benefits offered by ensuring ELF symbols have the
    correct visibility is given by "How To Write Shared Libraries" by Ulrich
    Drepper (which can be found at http://people.redhat.com/~drepper/) - however
    a superior solution made possible by this option to marking things hidden
    when the default is public is to make the default hidden and mark things
    public.  This is the norm with DLL's on Windows and with -fvisibility=hidden
    and __attribute__ ((visibility("default"))) instead of __declspec(dllexport)
    you get almost identical semantics with identical syntax. This is a great
    boon to those working with cross-platform projects.

    For those adding visibility support to existing code, you may find `#pragma
    GCC visibility' of use. This works by you enclosing the declarations you
    wish to set visibility for with (for example) `#pragma GCC visibility
    push(hidden)' and `#pragma GCC visibility pop'. Bear in mind that symbol
    visibility should be viewed as part of the API interface contract and thus
    all new code should always specify visibility when it is not the default ie;
    declarations only for use within the local DSO should always be marked
    explicitly as hidden as so to avoid PLT indirection overheads—making this
    abundantly clear also aids readability and self-documentation of the code.
    Note that due to ISO C++ specification requirements, operator new and
    operator delete must always be of default visibility.

    An overview of these techniques, their benefits and how to use them is at
    http://gcc.gnu.org/wiki/Visibility. 


Visibility Attributes and Pragmas for GCC C++ Libraries

A traditional problem when writing C++ libraries is that the number of visible
ELF symbols can become huge, even when symbols are not used externally and
therefore need not be made public.  

GCC versions 4.02 and later provide the new –fvisibility=value option and
related internal attributes to enable you to control this behavior in a
fashion similar to the mechanism provided by __declspec(dllexport) in
Microsoft’s C++ compilers. The new –fhidden option takes two possible values:
default, to continue to export all symbols from the current object file (which
    is the default value if this option is not specified), and hidden, which
causes g++ not to export the symbols from the current object module. These two
visibility cases are reflected by two new per-function/class attributes for
use in C++ applications: __attribute__ ((visibility("default"))) and
__attribute__ ((visibility("hidden"))).

By default, ELF symbols are still exported. To prevent ELF symbols from being
exported from a specific object file, use the –fvisibility=hidden option when
compiling that file. This can lead to increased complexity when creating a C++
library using a single Makefile because it requires that you either set manual
compilation options for each object file contained in a shared library, or
that you add this option to your global compilation flags for the library and
therefore do not export any symbols from any of a shared library’s component
object files. This can be a pain and is actually the wrong thing if you need
to make certain symbols visible externally for debugging purposes or for
catching exceptions for throwable entities within a library.

Caution: Being able to catch an exception for any user-defined type in a
binary other than the one that threw the exception requires a typeinfo lookup.
Because typeinfo information is part of the information that is hidden by the
–fvisibility=hidden option or the visibility __attribute__ attribute
specification, you must be very careful not to hide symbols for any class or
method that can throw exceptions across object file boundaries.

The best mechanism for making selected symbols visible is through a
combination of using the visibility attributes and the –fvisibility=hidden
command-line option. 

Method 1: use "default" attribute and "hidden" command option
that is to say something to show and otherwise to hide

For any structs, classes, or methods whose symbols must be externally visible,
    add the __attribute__ ((visibility("default"))) attribute to their
    definition, as in the following example:

class MyClass
{
  int i;
  __attribute__ ((visibility("default"))) void MyMethod();
  …
};

You can then add the –fvisibility=hidden option to the generic Makefile
compilation flags for all of your object files, which will cause symbols to be
hidden for all structs, classes, and methods that are not explicitly marked
with the default visibility attribute. When the source module containing this
code fragment is compiled, symbols are hidden by default, but those for the
MyMethod() method would be externally visible.


Method 2: use "hidden" attribute and no command option, "default"
that is to say something to hide

A slightly more elegant and memorable solution to symbol visibility is to
define a local visibility macro in a global header file shared by all of the
object modules in a library, and then put that macro in front of the
declaration for any class or method definition whose symbols you do not want
to export. In the converse of the previous example, if you want to export
symbols by default but restrict those for selected classes and methods, this
might look something like the following:

#define LOCAL __attribute__ ((visibility("hidden")))
class MyClass
{
  int i;
  LOCAL void MyMethod();
  ...
};

The source module containing this sample code could be compiled without using
the –fvisibility=value option and would export all symbols from MyClass with
the exception of those in the method MyMethod().


={============================================================================
*kt_linux_gcc_400* gcc-build-execution-env

http://askubuntu.com/questions/133389/no-such-file-or-directory-but-the-file-exists

There are three cases where you can get the message “No such file or directory”:

    The file doesn't exist. I presume you've checked that the file does exist
    (perhaps because the shell completes it).

    There is a file by that name, but it's a dangling symbolic link.

    The file exists, and you can even read it, and yet when you try to execute
    it you're told that the file doesn't exist.
    note: this is what I see when run binary built with glibc on a box with
    uclibc.

The error message in this last case is admittedly confusing. What it's telling
you is that a key component of the runtime environment necessary to run the
program is missing. Unfortunately, the channel through which the error is
reported only has room for the error code and not for this extra information
that it's really the runtime environment that's to blame. If you want the
technical version of this explanation, read Getting “Not found” message when
running a 32-bit binary on a 64-bit system.

The file command will tell you just what this binary is. With a few
exceptions, you can only run a binary for the processor architecture that your
release of Ubuntu is for. The main exception is that you can run 32-bit (x86,
    a.k.a. IA32) binaries on 64-bit (amd64, a.k.a. x86_64) systems.


={============================================================================
*kt_linux_gcc_400* gcc-toolchain-target-triplet

{triplet}
http://wiki.osdev.org/Target_Triplet

Target Triplets describe a platform on which code runs and are a core concept
in the GNU build system. They contain three fields: the name of the CPU
family/model, the vendor, and the operating system name. You can view the
unambiguous target triplet for your current system by running:

gcc -dumpmachine

<ex> pc vm
kyoupark@kit-debian:~/github$ gcc -dumpmachine
i586-linux-gnu

mips-linux-uclibc-gcc -dumpmachine
mips-linux-uclibc


Structure

Target triplets have this simple structure:

machine-vendor-operatingsystem

For instance, a FreeBSD system could be written as:

x86_64-unknown-freebsd

Notice how the vendor field is mostly irrelevant and is usually 'pc' for
32-bit x86 systems or 'unknown' or 'none' for other systems. The simple
three-field target triplet we have seen so far is unambiguous and easy to
parse. However, since the vendor field is mostly unused, the GNU build system
allows you to leave out the vendor field; the build system will automatically
insert a default vendor part when it disambiguates your target triplet. For
instance, this allows you to type:

x86_64-freebsd

The build system will then automatically deduce that the vendor is the default
(unknown) if it wishes to know the unambiguous target triplet. Note that
parsing target triplets are a bit more tricky, as sometimes the operating
system field `can be two fields`:

x86_64-unknown-linux-gnu

This gets a bit worse since the vendor field can be left out:

x86_64-linux-gnu

<check-toolchain-target-triplet>

note: /home/kyoupark/si-logs/gcc-build/gcc-4.8.2/config.sub

This is most definitely ambiguous. Most autoconf-based packages ship with a
huge shell script called `config.sub` whose function is to disambiguate target
triplet using a long list of known CPUs and known operating systems. 


Rationale

Target triplets are intended to be systematic unambiguous platform names
(well, after disambiguation). They lets build systems understand exactly which
system the code will run on and allows enabling platform-specific features
automatically. In any compilation setting, there are usually three platforms
involved (which might be the same three ones):

    `Build Platform`: This is the platform on which the compilation tools are
    executed.

    `Host Platform`: This is the platform on which the code will eventually run.

    `Target Platform`: If this is a compiler, this is the platform that the
    compiler will generate code for. 

This means that up to three differently-targeted compilers might be in play
(if you are building a GCC on platform A, which will run on platform B, which
 produces executables for platform C). This problem is solved by simply
prefixing the compilation tools with the target triplet. When you build a
cross-compiler, the installed executables will be prefixed with the specified
target triplet:

i686-elf-gcc

This prevents the wrong compiler from being used (and prevents things from the
    build machine to leak onto the target machine) if build systems are
carefully to prefix all compilation tools with the target prefix. 


<from-autoconf>
16.7 Specifying the System Type

There may be some features `configure' cannot figure out automatically,
but needs to determine by `the type of machine the package will run on`

Usually, assuming the package is built to be run on the _same_
architectures, `configure' can figure that out, but if it prints a
message saying it cannot guess the machine type, give it the
`--build=TYPE' option.  TYPE can either be a short name for the system
type, such as `sun4', or a canonical name which has the form:

     CPU-COMPANY-SYSTEM

where SYSTEM can have one of these forms:

     OS
     KERNEL-OS

   See the file `config.sub` for the possible values of each field.  If
   `config.sub' isn't included in this package, then this package doesn't need
   to know the machine type.

   If you are _building_ compiler tools for cross-compiling, you should use
   the option `--target=TYPE` to select the type of system they will produce
   code for.

   If you want to _use_ a cross compiler, that generates code for a platform
   different from the build platform, you should specify the "host" platform
   (i.e., that on which the generated programs will eventually be run) with
   `--host=TYPE`.


<how-to-change-gcc-to-have-own-triplet>
Target Triplets for Operating Systems Development

For instance, if you develop your own operating system and modify GCC to add a
new target triplet, yours could be: http://wiki.osdev.org/OS_Specific_Toolchain


<in-configure>

case "${target}" in
  *-*-chorusos)
    ;;

<ex> mips little endian
mipsel-linux-addr2line  mipsel-linux-as       mipsel-linux-elfedit
mipsel-linux-ld      mipsel-linux-nm       mipsel-linux-objdump

<ex> x86_64
x86 are all little endian so no need to have big endian triplet.


={============================================================================
*kt_linux_gcc_400* gcc-build-option

<gcc-build-options>
https://gcc.gnu.org/install/configure.html

To get the debug library
https://gcc.gnu.org/onlinedocs/libstdc++/manual/debug.html

--enable-libstdcxx-debug

This sets the binary name such as $install/bin/gcc4.9.2
--program-suffix=4.9.2 \

This sets the installation directory
--prefix=$HOME/gcc-build/install


<gcc-check-spec>
the `MIPSel` (little endian) 


={============================================================================
*kt_linux_gcc_400* gcc-int-internal

https://gcc.gnu.org/onlinedocs/gccint/index.html#Top

Introduction

This manual documents the internals of the GNU compilers, including how to
port them to new targets and some information about how to write front ends
for new languages. It corresponds to the compilers (GCC) version 7.0.1. The
  use of the GNU compilers is documented in a separate manual. See
    Introduction.

This manual is mainly a reference manual rather than a tutorial. It discusses
how to contribute to GCC (see Contributing), the characteristics of the
machines supported by GCC as hosts and targets (see Portability), how GCC
relates to the ABIs on such systems (see Interface), and the characteristics
of the languages for which GCC front ends are written (see Languages). It then
describes the GCC source tree structure and build system, some of the
interfaces to GCC front ends, and how support for a target system is
implemented in GCC. 


6.3.9 Anatomy of a Target Back End

gcc-4.2.0-20070124-patched/gcc/config/mips$ ls
24k.md   4600.md  6000.md         crtn.asm     iris5.h            linux64.h       mips-dsp.md    mips-modes.def      netbsd.h       sdb.h       t-iris           t-r3900        t-slibgcc-irix  vxworks.h
3000.md  4k.md    7000.md         dbxmdebug.h  iris6.h            linux.h         mips-dspr2.md  mips.opt            openbsd.h      sde.h       t-iris6          t-rtems        t-sr71k         windiss.h
4000.md  5000.md  74k.md          elf.h        iris.h             linux-unwind.h  mips.h         mips.opt.orig       predicates.md  sde.h.orig  t-isa3264        t-sb1          t-vr
4100.md  5400.md  9000.md         elfoabi.h    irix-crti.asm      mips16.S        mips.h.orig    mips-protos.h       r3900.h        sr71k.md    t-libgcc-mips16  t-sde          t-vxworks
4130.md  5500.md  constraints.md  elforion.h   irix-crtn.asm      mips.c          mips.md        mips-protos.h.orig  rtems.h        t-elf       t-linux64        t-sdemtk       vr4120-div.S
4300.md  5k.md    crti.asm        generic.md   libgcc-mips16.ver  mips.c.orig     mips.md.orig   mips-ps-3d.md       sb1.md         t-gofast    t-mips           t-sdemtk.orig  vr.h

A back end for a target architecture in GCC has the following parts: 

A directory `machine` under gcc/config, 
  containing a machine description machine.md file (see Machine Descriptions), 
  header files machine.h and machine-protos.h and 
  a source file machine.c (see Target Description Macros and Functions), 
  possibly a target Makefile fragment t-machine (see The Target Makefile Fragment), 
  and maybe some other files. 
    
The names of these files may be changed from the defaults given by explicit
specifications in config.gcc.

If necessary, a file machine-modes.def in the machine directory, containing
additional machine modes to represent condition codes. See Condition Code, for
further details.  An optional machine.opt file in the machine directory,
        containing a list of target-specific options. You can also add other
          option files using the extra_options variable in config.gcc. See
          Options.  Entries in config.gcc (see The config.gcc File) for the
          systems with this target architecture. 


17 Target Description Macros and Functions

In addition to the file machine.md, a machine description includes a C header
file conventionally given the name machine.h and a C source file named
machine.c. The header file defines numerous macros that convey the information
about the target machine that does not fit into the scheme of the .md file.
The file tm.h should be a link to machine.h. The header file config.h includes
tm.h and most compiler source files include config.h. The source file defines
a variable targetm, which is a structure containing pointers to functions and
data relating to the target machine. machine.c should also contain their
definitions, if they are not defined elsewhere in GCC, and other functions
called through the macros defined in the .h file. 


https://gcc.gnu.org/onlinedocs/gccint/RTL.html#RTL
13 RTL Representation

The last part of the compiler work is done on a low-level intermediate
representation called Register Transfer Language. In this language, the
instructions to be output are described, pretty much one by one, in an
algebraic form that describes what the instruction does.

RTL is inspired by Lisp lists. It has both an internal form, made up of
structures that point at other structures, and a textual form that is used in
the machine description and in printed debugging dumps. The textual form uses
nested parentheses to indicate the pointers in the internal form. 


https://gcc.gnu.org/onlinedocs/gccint/Machine-Desc.html
16 Machine Descriptions

A machine description has two parts: a file of instruction patterns (.md file)
and a C header file of macro definitions.

The .md file for a target machine contains `a pattern` for each instruction that
the target machine supports (or at least each instruction that is worth
    telling the compiler about). It may also contain comments. A semicolon
causes the rest of the line to be a comment, unless the semicolon is inside a
quoted string.

See the next chapter for information on the C header file. 


16.1 Overview of How the Machine Description is Used

There are three main conversions that happen in the compiler:

    The front end reads the source code and builds a parse tree.

    The parse tree is used to generate `an RTL insn list` based on named
    instruction patterns.

    The insn list is matched against the RTL templates `to produce assembler code` 

For the generate pass, only the names of the insns matter, from either a named
define_insn or a define_expand. The compiler will choose the pattern with the
right name and apply the operands according to the documentation later in this
chapter, without regard for the RTL template or operand constraints. Note that
the names the compiler looks for are hard-coded in the compiler - it will
ignore unnamed patterns and patterns with names it doesn't know about, but if
you don't provide a named pattern it needs, it will abort.

If a define_insn is used, the template given is inserted into the insn list.
If a define_expand is used, one of three things happens, based on the
condition logic. The condition logic may manually create new insns for the
insn list, say via emit_insn(), and invoke DONE. For certain named patterns,
     it may invoke FAIL to tell the compiler to use an alternate way of
       performing that task. If it invokes neither DONE nor FAIL, the template
       given in the pattern is inserted, as if the define_expand were a
       define_insn.

Once the insn list is generated, various optimization passes convert, replace,
and rearrange the insns in the insn list. This is where the define_split and
  define_peephole patterns get used, for example.

Finally, the insn list's RTL is matched up with the RTL templates in the
define_insn patterns, and those patterns are used to emit the final assembly
code. For this purpose, each named define_insn acts like it's unnamed, since
the names are ignored. 


16.2 Everything about Instruction Patterns

A `define_insn` expression is used to define instruction patterns to which
insns may be matched. A define_insn expression contains an incomplete RTL
expression, with pieces to be filled in later, operand constraints that
restrict how the pieces can be filled in, and an output template or C code to
generate the assembler output.

A define_insn is an RTL expression containing four or five operands: 

1. An optional name. 

The presence of a name indicate that this instruction pattern can perform a
certain standard job for the RTL-generation pass of the compiler. This pass
knows certain names and will use the instruction patterns with those names, if
the names are defined in the machine description.

The absence of a name is indicated by writing an empty string where the name
should go. Nameless instruction patterns are never used for generating RTL
code, but they may permit several simpler insns to be combined later on.

Names that are not thus known and used in RTL-generation have no effect; they
are equivalent to no name at all.

For the purpose of debugging the compiler, you may also specify a name
beginning with the ‘*’ character. Such a name is used only for identifying the
instruction in RTL dumps; it is equivalent to having a nameless pattern for
all other purposes. Names beginning with the ‘*’ character are not required to
be unique.


2. The RTL template: This is a vector of incomplete RTL expressions which
describe the semantics of the instruction (see RTL Template). It is incomplete
because it may contain match_operand, match_operator, and match_dup
expressions that stand for operands of the instruction.

If the vector has multiple elements, the RTL template is treated as a parallel
expression.


3. The condition: This is a string which contains a C expression. When the
compiler attempts to match RTL against a pattern, the condition is evaluated.
If the condition evaluates to true, the match is permitted. The condition may
be an empty string, which is treated as always true.

For a named pattern, the condition may not depend on the data in the insn
being matched, but only the target-machine-type flags. The compiler needs to
test these conditions during initialization in order to learn exactly which
named instructions are available in a particular run.

For nameless patterns, the condition is applied only when matching an
individual insn, and only after the insn has matched the pattern's recognition
template. The insn's operands may be found in the vector operands.

For an insn where the condition has once matched, it cannot later be used to
control register allocation by excluding certain register or value
combinations.


4. The output template or output statement: This is either a string, or a
fragment of C code which `returns a string`

When simple substitution isn't general enough, you can specify a piece of C
code to compute the output. See Output Statement.


5. The insn attributes: This is an optional vector containing the values of
attributes for insns matching this pattern (see Insn Attributes). 


16.3 Example of define_insn

Here is an example of an instruction pattern, taken from the machine
description for the 68000/68020.

     (define_insn "tstsi"
       [(set (cc0)
             (match_operand:SI 0 "general_operand" "rm"))]
       ""
       "*
     {
       if (TARGET_68020 || ! ADDRESS_REG_P (operands[0]))
         return \"tstl %0\";
       return \"cmpl #0,%0\";
     }")

This can also be written using braced strings:

     (define_insn "tstsi"
       [(set (cc0)
             (match_operand:SI 0 "general_operand" "rm"))]
       ""
     {
       if (TARGET_68020 || ! ADDRESS_REG_P (operands[0]))
         return "tstl %0";
       return "cmpl #0,%0";
     })

This describes an instruction which sets the condition codes based on the
  value of a general operand. It has no condition, so any insn with an RTL
  description of the form shown may be matched to this pattern. The name
  ‘tstsi’ means “test a SImode value” and tells the RTL generation pass that,
  when it is necessary to test such a value, an insn to do so can be
    constructed using this pattern.

The output control string is a piece of C code which chooses which output
template to return based on the kind of operand and the specific type of CPU
for which code is being generated.

‘"rm"’ is an operand constraint. Its meaning is explained below. 


={============================================================================
*kt_linux_gcc_400* gcc-int-spec-file

https://gcc.gnu.org/onlinedocs/gcc/Spec-Files.html

3.19 Specifying Subprocesses and the Switches to Pass to Them

`gcc is a driver program.` It performs its job by invoking a sequence of other
programs to do the work of compiling, assembling and linking. GCC interprets
its command-line parameters and uses these to deduce which programs it should
invoke, and which command-line options it ought to place on their command
lines. 

This behavior is controlled by `spec strings` In most cases there is one spec
string for each program that GCC can invoke, but a few programs have multiple
spec strings to control their behavior. The spec strings `built into GCC` can be
overridden by using the -specs= command-line switch to specify a spec file.

Spec files are plain-text files that are used `to construct spec strings.` They
consist of a sequence of directives separated by blank lines. The type of
directive is determined `by the first non-whitespace character` on the line,
which can be one of the following:

%command
    Issues a command to the spec file processor. The commands that can appear
    here are:

    %include <file>
        Search for file and insert its text at the current point in the specs
        file. 

*[spec_name]:
    This tells the compiler to create, override or delete the `named spec string` 
    All lines after this directive up to the next directive or blank line are
    considered to be the text for the spec string. 
    
    If this results in an empty string then the spec is deleted. (Or, if the
        spec did not exist, then nothing happens.) 

    Otherwise, if the spec does not currently exist a new spec is created. If
    the spec does exist then its contents are overridden by the text of this
    directive, unless the first character of that text is the ‘+’ character,
    in which case the text is appended to the spec. 

[suffix]:
    Creates a new ‘[suffix] spec’ pair. All lines after this directive and up
      to the next directive or blank line are considered to make up the spec
      string for the indicated suffix. When the compiler encounters an input
      file with the named suffix, it processes the spec string in order to
      work out `how to compile that file`. 
      
      For example:

              .ZZ:
              z-compile -input %i

    This says that any input file whose name ends in ‘.ZZ’ should be passed to
    the program ‘z-compile’, which should be invoked with the command-line
    switch -input and with the result of performing the ‘%i’ substitution.
    (See below.)

<override-spec>
    GCC already has an extensive list of suffixes built into it. This
    directive adds an entry to the end of the list of suffixes, but since the
    list is searched from the end backwards, it is effectively possible to
    override earlier entries using this technique. 


GCC has the following spec strings built into it. Spec files can override
these strings or create their own. Note that individual targets can also add
their own spec strings to this list.

     asm          Options to pass to the assembler
     asm_final    Options to pass to the assembler post-processor
     cpp          Options to pass to the C preprocessor
     cc1          Options to pass to the C compiler
     cc1plus      Options to pass to the C++ compiler
     endfile      Object files to include at the end of the link
     link         Options to pass to the linker
     lib          Libraries to include on the command line to the linker
     libgcc       Decides which GCC support library to pass to the linker
     linker       Sets the name of the linker
     predefines   Defines to be passed to the C preprocessor
     signed_char  Defines to pass to CPP to say whether char is signed
                  by default
     startfile    Object files to include at the start of the link

<ex>
Here is a small example of a spec file:

     %rename lib                 old_lib
     
     *lib:
     --start-group -lgcc -lc -leval1 --end-group %(old_lib)

This example renames the spec called ‘lib’ to ‘old_lib’ and then overrides the
previous definition of ‘lib’ with a new one. The new definition adds in some
extra command-line options before including the text of the old definition.


`spec strings` are a list of command-line options to be passed to their
corresponding program. 

In addition, the spec strings can contain ‘%’ `to substitute` variable text or
to conditionally insert text `into the command line` Using these constructs it
is possible to generate quite complex command lines.

Here is a table of all `defined ‘%’-sequences for spec strings.` Note that
spaces are not generated automatically around the results of expanding these
sequences. Therefore you can concatenate them together or combine them with
constant text in a single argument.

%G
    Process the libgcc spec. This is a spec string for deciding which GCC
    support library is included on the command line to the linker. 

%(name)
    Substitute the contents of spec string name at this point. 

%{S}
    Substitutes the -S switch, if that switch is given to GCC. If that switch
      is not specified, this substitutes nothing. 
      
    Note that the leading dash is omitted when specifying this option, and it
    is automatically inserted if the substitution is performed. Thus the spec
    string ‘%{foo}’ matches the command-line option -foo and `outputs` the
    command-line option -foo. 

%{S:X}
    Substitutes X, if the -S switch is given to GCC.

%{!S:X}
    Substitutes X, if the -S switch is not given to GCC.

%{S*:X}
    Substitutes X if one or more switches whose names start with -S are
      specified to GCC. Normally X is substituted only once, no matter how
      many such switches appeared. However, if %* appears somewhere in X, then
      X is substituted once for each matching switch, with the %* replaced by
      the part of that switch matching the *.

    If %* appears as the last part of a spec sequence then a space is added
    after the end of the last substitution. If there is more text in the
    sequence, however, then a space is not generated. This allows the %*
    substitution to be used as part of a larger string. For example, a spec
    string like this:

              %{mcu=*:--script=%*/memory.ld}

    when matching an option like `-mcu=newchip` produces:

              --script=newchip/memory.ld


The conditional text X in a %{S:X} or similar construct may contain other
nested ‘%’ constructs or spaces, or even newlines. They are processed as
usual, as described above. Trailing white space in X is ignored. White space
may also appear anywhere on the left side of the colon in these constructs,
    except between . or * and the corresponding word.

The -O, -f, -m, and -W switches are handled specifically in these constructs.
If another value of -O or the negated form of a -f, -m, or -W switch is found
later in the command line, the earlier switch value is ignored, except with
{S*} where S is just one letter, which passes all matching options.

The character ‘|’ at the beginning of the predicate text is used to indicate
that a command should be piped to the following command, but only if -pipe is
specified.

It is built into GCC which switches take arguments and which do not. (You
    might think it would be useful to generalize this to allow each compiler's
    spec to say which switches take arguments. But this cannot be done in a
    consistent fashion. GCC cannot even decide which input files have been
    specified without knowing which switches take arguments, and it must know
    which input files to compile in order to tell which compilers to run).

GCC also knows implicitly that arguments starting in -l are to be treated as
compiler output files, and passed to the linker in their proper position among
the other output files. 


<ex>
~/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-uclibc-gcc -dumpspecs
*asm:
%{G*} %(endian_spec) %{mips1} %{mips2} %{mips3} %{mips4} %{mips32} %{mips32r2} %{mips64} %{mips16:%{!mno-mips16:-mips16}} %{mno-mips16:-no-mips16} %{mips3d:-mips3d} %{msmartmips} %{mno-smartmips} %{mdsp} %{m
dspr2} %{mt} %{mmt} %{mno-mt} %{mfix-vr4120} %{mfix-vr4130} %(subtarget_asm_optimizing_spec) %(subtarget_asm_debugging_spec) %{mabi=*} %{!mabi*: %(asm_abi_default_spec)} %{mgp32} %{mgp64} %{march=*} %{mxgot:
-xgot} %{mfp32} %{mfp64} %{mshared} %{mno-shared} %{msym32} %{mno-sym32} %{mtune=*} %{v} %(subtarget_asm_spec)

*asm_debug:
%{gstabs*:--gstabs}%{!gstabs*:%{g*:--gdwarf2}}

*asm_final:


*asm_options:
%a %Y %{c:%W{o*}%{!o*:-o %w%b%O}}%{!c:-o %d%w%u%O}

*invoke_as:
%{!S:-o %|.s |
 as %(asm_options) %|.s %A }

*cpp:
%(subtarget_cpp_spec)

*cpp_options:
%(cpp_unique_options) %1 %{m*} %{std*&ansi&trigraphs} %{W*&pedantic*} %{w} %{f*} %{g*:%{!g0:%{!fno-working-directory:-fworking-directory}}} %{O*} %{undef} %{save-temps:-fpch-preprocess}

*cpp_debug_options:
%{d*}

*cpp_unique_options:
%{C|CC:%{!E:%eGCC does not support -C or -CC without -E}} %{!Q:-quiet} %{nostdinc*} %{C} %{CC} %{v} %{I*&F*} %{P} %I %{MD:-MD %{!o:%b.d}%{o*:%.d%*}} %{MMD:-MMD %{!o:%b.d}%{o*:%.d%*}} %{M} %{MM} %{MF*} %{MG}
%{MP} %{MQ*} %{MT*} %{!E:%{!M:%{!MM:%{MD|MMD:%{o*:-MQ %*}}}}} %{remap} %{g3:-dD} %{H} %C %{D*&U*&A*} %{i*} %Z %i %{fmudflap:-D_MUDFLAP -include mf-runtime.h} %{fmudflapth:-D_MUDFLAP -D_MUDFLAPTH -include mf-
runtime.h} %{E|M|MM:%W{o*}}

*trad_capable_cpp:
cc1 -E %{traditional|ftraditional|traditional-cpp:-traditional-cpp}

*cc1:
%{gline:%{!g:%{!g0:%{!g1:%{!g2: -g1}}}}} %{G*} %{EB:-meb} %{EL:-mel} %{EB:%{EL:%emay not use both -EB and -EL}} %{save-temps: } %(subtarget_cc1_spec)

*cc1_options:
%{pg:%{fomit-frame-pointer:%e-pg and -fomit-frame-pointer are incompatible}} %1 %{!Q:-quiet} -dumpbase %B %{d*} %{m*} %{a*} %{c|S:%{o*:-auxbase-strip %*}%{!o*:-auxbase %b}}%{!c:%{!S:-auxbase %b}} %{g*} %{O*}
 %{W*&pedantic*} %{w} %{std*&ansi&trigraphs} %{v:-version} %{pg:-p} %{p} %{f*} %{undef} %{Qn:-fno-ident} %{--help:--help} %{--target-help:--target-help} %{!fsyntax-only:%{S:%W{o*}%{!o*:-o %b.s}}} %{fsyntax-o
nly:-o %j} %{-param*} %{fmudflap|fmudflapth:-fno-builtin -fno-merge-constants} %{coverage:-fprofile-arcs -ftest-coverage}

*cc1plus:


*link_gcc_c_sequence:
%{static:--start-group} %G %L %{static:--end-group}%{!static:%G}

*link_ssp:
%{fstack-protector|fstack-protector-all:-lssp_nonshared -lssp}

*endfile:
%{shared|pie:crtendS.o%s;:crtend.o%s} crtn.o%s

*link:
%{!static:--eh-frame-hdr} %(endian_spec)   %{shared:-shared}   %{!shared:     %{!ibcs:       %{!static:         %{rdynamic:-export-dynamic}         %{!dynamic-linker:-dynamic-linker %{mglibc:%{muclibc:%e-mgl
ibc and -muclibc used together}/lib/ld.so.1;:/lib/ld-uClibc.so.0}}}         %{static:-static}}}

*lib:
%{pthread:-lpthread} %{shared:-lc} %{!shared:   %{profile:-lc_p} %{!profile:-lc}}

*mfwrap:
 %{static: %{fmudflap|fmudflapth:  --wrap=malloc --wrap=free --wrap=calloc --wrap=realloc --wrap=mmap --wrap=munmap --wrap=alloca} %{fmudflapth: --wrap=pthread_create}} %{fmudflap|fmudflapth: --wrap=main}

*mflib:
%{fmudflap|fmudflapth: -export-dynamic}

*link_gomp:


*libgcc:
%{static|static-libgcc:-lgcc -lgcc_eh}%{!static:%{!static-libgcc:%{!shared-libgcc:-lgcc --as-needed -lgcc_s --no-as-needed}%{shared-libgcc:-lgcc_s%{!shared: -lgcc}}}}

*startfile:
%{!shared: %{pg|p|profile:gcrt1.o%s;pie:Scrt1.o%s;:crt1.o%s}}    crti.o%s %{static:crtbeginT.o%s;shared|pie:crtbeginS.o%s;:crtbegin.o%s}

*switches_need_spaces:


*cross_compile:
1

*version:
4.2.0

*multilib:
. ;

*multilib_defaults:
EB mips1 mabi=32

*multilib_extra:


*multilib_matches:


*multilib_exclusions:


*multilib_options:


*linker:
collect2

*link_libgcc:
%D

*md_exec_prefix:


*md_startfile_prefix:


*md_startfile_prefix_1:


*startfile_prefix_spec:


*sysroot_spec:
--sysroot=%R

*sysroot_suffix_spec:


*sysroot_hdrs_suffix_spec:


*subtarget_cc1_spec:
%{profile:-p}

*subtarget_cpp_spec:
%{posix:-D_POSIX_SOURCE} %{pthread:-D_REENTRANT}

*subtarget_asm_optimizing_spec:
%{noasmopt:-O0} %{!noasmopt:%{O:-O2} %{O1:-O2} %{O2:-O2} %{O3:-O3}}

*subtarget_asm_debugging_spec:
%{g} %{g0} %{g1} %{g2} %{g3} %{ggdb:-g} %{ggdb0:-g0} %{ggdb1:-g1} %{ggdb2:-g2} %{ggdb3:-g3} %{gstabs:-g} %{gstabs0:-g0} %{gstabs1:-g1} %{gstabs2:-g2} %{gstabs3:-g3} %{gstabs+:-g} %{gstabs+0:-g0} %{gstabs+1:-
g1} %{gstabs+2:-g2} %{gstabs+3:-g3} %{gcoff:-g} %{gcoff0:-g0} %{gcoff1:-g1} %{gcoff2:-g2} %{gcoff3:-g3} %{gcoff*:-mdebug} %{!gcoff*:-no-mdebug}

*subtarget_asm_spec:
%{!mno-abicalls:%{!mgnu-plts:-KPIC}}

*asm_abi_default_spec:
-mabi=32

*endian_spec:
%{!EL:%{!mel:-EB}} %{EL|mel:-EL}

*link_command:
%{!fsyntax-only:%{!c:%{!M:%{!MM:%{!E:%{!S:    %(linker) %l %{pie:-pie} %X %{o*} %{A} %{d} %{e*} %{m} %{N} %{n} %{r}    %{s} %{t} %{u*} %{x} %{z} %{Z} %{!A:%{!nostdlib:%{!nostartfiles:%S}}}    %{static:} %{L*
} %(mfwrap) %(link_libgcc) %o    %{fopenmp:%:include(libgomp.spec)%(link_gomp)} %(mflib)    %{fprofile-arcs|fprofile-generate|coverage:-lgcov}    %{!nostdlib:%{!nodefaultlibs:%(link_ssp) %(link_gcc_c_sequenc
e)}}    %{!A:%{!nostdlib:%{!nostartfiles:%E}}} %{T*} }}}}}}


={============================================================================
*kt_linux_gcc_400* gcc-int-ucmpdi2-error

{gcc-ucmpdi2-error}
Outputs are different depending on compiler used and turns out that
gcc puts `__ucmpdi2` symbol although there is no use in the source. brcm gcc
supports some options which own gcc doesn't.


<when-use-bcm-gcc> see no __ucmpdi2

/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc -Wp,-MD,arch/mips/math-emu/.ieee754dp.o.d  -nostdinc -isystem /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include -D__KERNEL__ -Iinclude  -include include/linux/autoconf.h -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -fno-strict-aliasing -fno-common -Os  -mabi=32 -G 0 -mno-abicalls -fno-pic -pipe -msoft-float -ffreestanding  -march=mips32 -Wa,-mips32 -Wa,--trap -Wno-uninitialized -Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic -fomit-frame-pointer -g  -fno-stack-protector -Wdeclaration-after-statement -Wno-pointer-sign   -Os -I/home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/usr/include -I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include -D"KBUILD_STR(s)=\#s" -D"KBUILD_BASENAME=KBUILD_STR(ieee754dp)"  -D"KBUILD_MODNAME=KBUILD_STR(ieee754dp)" -c -o arch/mips/math-emu/ieee754dp.o arch/mips/math-emu/ieee754dp.c
/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc -Wp,-MD,arch/mips/math-emu/.ieee754dp.o.d  -nostdinc -isystem /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include -D__KERNEL__ -Iinclude  -include include/linux/autoconf.h -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -fno-strict-aliasing -fno-common -Os  -mabi=32 -G 0 -mno-abicalls -fno-pic -pipe -msoft-float -ffreestanding  -march=mips32 -Wa,-mips32 -Wa,--trap -Wno-uninitialized -Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic -fomit-frame-pointer -g  -fno-stack-protector -Wdeclaration-after-statement -Wno-pointer-sign   -Os -I/home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/usr/include -I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include -D"KBUILD_STR(s)=\#s" -D"KBUILD_BASENAME=KBUILD_STR(ieee754dp)"  -D"KBUILD_MODNAME=KBUILD_STR(ieee754dp)" -c -o ieee754dp.o arch/mips/math-emu/ieee754dp.c

/linux-2.6.18.8/arch/mips/math-emu$ nm ieee754dp.o
         U __ashldi3
         U __ieee754dp_spcvals
         U __lshrdi3
000000f4 t get_rounding
         U ieee754_xcpt
000000a4 T ieee754dp_bestnan
00000000 T ieee754dp_class
00000228 T ieee754dp_format
00000068 T ieee754dp_isnan
0000008c T ieee754dp_issnan
0000077c T ieee754dp_nanxcpt
000001a4 T ieee754dp_xcpt

<when-use-own-gcc> see __ucmpdi2
cd /home/nds-uk/kyoupark/ext4/project_build_mips/ams-drx890/linux-2.6.18.8
/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/mips-linux-gnu-gcc -Wp,-MD,arch/mips/math-emu/.ieee754dp.o.d  -nostdinc -isystem /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/include -D__KERNEL__ -Iinclude  -include include/linux/autoconf.h -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -fno-strict-aliasing -fno-common -Os  -mabi=32 -G 0 -mno-abicalls -fno-pic -pipe -msoft-float -ffreestanding  -march=mips32 -Wa,-mips32 -Wa,--trap -Wno-uninitialized -Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic -fomit-frame-pointer -g  -fno-stack-protector -Wdeclaration-after-statement -Wno-pointer-sign   -Os -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include -D"KBUILD_STR(s)=\#s" -D"KBUILD_BASENAME=KBUILD_STR(ieee754dp)"  -D"KBUILD_MODNAME=KBUILD_STR(ieee754dp)" -c -o ieee754dp.o arch/mips/math-emu/ieee754dp.c
/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -Wp,-MD,arch/mips/math-emu/.ieee754dp.o.d  -nostdinc -isystem /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/include -D__KERNEL__ -Iinclude  -include include/linux/autoconf.h -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -fno-strict-aliasing -fno-common -Os -mabi=32 -G 0 -mno-abicalls -fno-pic -pipe -msoft-float -ffreestanding -march=mips32 -Wa,-mips32 -Wa,--trap -Wno-uninitialized -Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic -fomit-frame-pointer -g  -fno-stack-protector -Wdeclaration-after-statement -Wno-pointer-sign   -Os -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include -D"KBUILD_STR(s)=\#s" -D"KBUILD_BASENAME=KBUILD_STR(ieee754dp)" -D"KBUILD_MODNAME=KBUILD_STR(ieee754dp)" -c -o ieee754dp.o arch/mips/math-emu/ieee754dp.c

/linux-2.6.18.8/arch/mips/math-emu$ nm ieee754dp.o
         U __ashldi3
         U __ieee754dp_spcvals
         U __lshrdi3
         U __ucmpdi2 ~
0000011c t get_rounding
         U ieee754_xcpt
000000d0 T ieee754dp_bestnan
00000000 T ieee754dp_class
000001a8 T ieee754dp_format
00000060 T ieee754dp_isnan
000000c4 T ieee754dp_issnan
0000063c T ieee754dp_nanxcpt
00000738 T ieee754dp_xcpt


<diff-gcc-v-option>
/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/mips-linux-gnu-gcc -Wp,-MD,arch/mips/math-emu/.ieee754dp.o.d  -nostdinc -isystem /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/include -D__KERNEL__ -Iinclude  -include include/linux/autoconf.h -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -fno-strict-aliasing -fno-common -Os  -mabi=32 -G 0 -mno-abicalls -fno-pic -pipe -msoft-float -ffreestanding  -march=mips32 -Wa,-mips32 -Wa,--trap -Wno-uninitialized -Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic -fomit-frame-pointer -g  -fno-stack-protector -Wdeclaration-after-statement -Wno-pointer-sign   -Os -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include -D"KBUILD_STR(s)=\#s" -D"KBUILD_BASENAME=KBUILD_STR(ieee754dp)"  -D"KBUILD_MODNAME=KBUILD_STR(ieee754dp)" -c -o ieee754dp.o arch/mips/math-emu/ieee754dp.c

Using built-in specs.
Target: mips-linux-uclibc

Configured with: ../gcc-4.2.0-20070124/configure
--prefix=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/
--build=mips-linux --host=mips-linux --target=mips-linux-uclibc
--with-build-sysroot=/usr/src/redhat/BUILD/build_uClibc
--enable-languages=c,c++ --disable-__cxa_atexit --enable-target-optspace
--with-gnu-ld --with-float=hard --enable-threads
--infodir=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/info
--mandir=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/man
--with-arch=mips32 --disable-libmudflap --disable-nls --with-gnu-plts

Thread model: posix
gcc version 4.2.0 20070124 (prerelease) - BRCM 10tsHound-20140508

 /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../libexec/gcc/mips-linux-uclibc/4.2.0/cc1
 -quiet -nostdinc -v 
 -Iinclude -Iinclude/asm-mips/mach-brcmstb
 -Iinclude/asm-mips/mach-generic
 -I/home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/include
 -I/home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/usr/include
 -I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include
 -iprefix /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/
 -D__KERNEL__ -DKBUILD_STR(s)=\#s -DKBUILD_BASENAME=KBUILD_STR(ieee754dp)
 -DKBUILD_MODNAME=KBUILD_STR(ieee754dp) 
 -isystem /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include
 -include include/linux/autoconf.h -MD arch/mips/math-emu/.ieee754dp.o.d
 arch/mips/math-emu/ieee754dp.c 
 -G 0 -quiet 
 -dumpbase ieee754dp.c 
 -mabi=32
 -mno-abicalls 
 -msoft-float 
 -march=mips32 
 -mgnu-plts ~
 -mno-shared
 -minterlink-mips16 ~
 -auxbase-strip ieee754dp.o 
 -g -Os -Os -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -Wno-uninitialized 
 -Wdeclaration-after-statement -Wno-pointer-sign -version
 -fno-strict-aliasing -fno-common -fno-pic -ffreestanding
 -fomit-frame-pointer -fno-stack-protector -o - |
 /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/bin/as
 -G 0 -EB -g -no-mdebug -mabi=32 -march=mips32 -mno-shared -v -mips32 --trap
 -o ieee754dp.o -

GNU assembler version 2.17.50 (mips-linux-uclibc) using BFD version 2.17.50-brcm 20070220
#include "..." search starts here:
#include <...> search starts here:
 include
 include/asm-mips/mach-brcmstb
 include/asm-mips/mach-generic
 /home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/include
 /home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/usr/include
 /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include
 /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include
End of search list.
GNU C version 4.2.0 20070124 (prerelease) - BRCM 10tsHound-20140508 (mips-linux-uclibc)
	compiled by GNU C version 4.1.2 20080704 (Red Hat 4.1.2-54).
GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: 1c2c8d62cf6432a1d25d35e4fc70875a


/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/mips-linux-gnu-gcc -Wp,-MD,arch/mips/math-emu/.ieee754dp.o.d  -nostdinc -isystem /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/include -D__KERNEL__ -Iinclude  -include include/linux/autoconf.h -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -fno-strict-aliasing -fno-common -Os  -mabi=32 -G 0 -mno-abicalls -fno-pic -pipe -msoft-float -ffreestanding  -march=mips32 -Wa,-mips32 -Wa,--trap -Wno-uninitialized -Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic -fomit-frame-pointer -g  -fno-stack-protector -Wdeclaration-after-statement -Wno-pointer-sign   -Os -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include -D"KBUILD_STR(s)=\#s" -D"KBUILD_BASENAME=KBUILD_STR(ieee754dp)"  -D"KBUILD_MODNAME=KBUILD_STR(ieee754dp)" -v -c -o ieee754dp.o arch/mips/math-emu/ieee754dp.c

Using built-in specs.
Target: mips-linux-gnu

Configured with: ../gcc-4.3.0/configure
--prefix=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install
--target=mips-linux-gnu --enable-languages=c,c++ --disable-libmudflap
--disable-multilib --disable-nls --disable-__cxa_atexit
--enable-target-optspace 
--with-gnu-ld --with-float=hard --enable-threads
--with-gnu-plts --with-arch=mips32

note: no disable-multilib?

Thread model: posix
gcc version 4.3.0 (GCC) 

COLLECT_GCC_OPTIONS='-nostdinc' '-isystem' '/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/include' '-D__KERNEL__' '-Iinclude' '-include' 'include/linux/autoconf.h' '-Wall' '-Wundef' '-Wstrict-prototypes' '-Wno-trigraphs' '-fno-strict-aliasing' '-fno-common' '-Os' '-mabi=32' '-G' '0' '-mno-abicalls' '-fno-pic' '-pipe' '-msoft-float' '-ffreestanding' '-march=mips32' '-Wno-uninitialized' '-Iinclude/asm-mips/mach-brcmstb' '-Iinclude/asm-mips/mach-generic' '-fomit-frame-pointer' '-g' '-fno-stack-protector' '-Wdeclaration-after-statement' '-Wno-pointer-sign' '-Os' '-I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/include' '-I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/usr/include' '-I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include' '-DKBUILD_STR(s)=\#s' '-DKBUILD_BASENAME=KBUILD_STR(ieee754dp)' '-DKBUILD_MODNAME=KBUILD_STR(ieee754dp)' '-v' '-c' '-o' 'ieee754dp.o' '-mllsc' '-mno-shared'

 /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../libexec/gcc/mips-linux-gnu/4.3.0/cc1
 -quiet -nostdinc -v 
 -Iinclude -Iinclude/asm-mips/mach-brcmstb
 -Iinclude/asm-mips/mach-generic
 -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/include
 -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/usr/include
 -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include
 -iprefix /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/
 -D__KERNEL__ -DKBUILD_STR(s)=\#s -DKBUILD_BASENAME=KBUILD_STR(ieee754dp)
 -DKBUILD_MODNAME=KBUILD_STR(ieee754dp) 
 -isystem /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/include
 -include include/linux/autoconf.h -MD arch/mips/math-emu/.ieee754dp.o.d
 arch/mips/math-emu/ieee754dp.c 
 -G 0 -quiet 
 -dumpbase ieee754dp.c 
 -mabi=32
 -mno-abicalls 
 -msoft-float 
 -march=mips32 
 -mllsc ~
 -mno-shared 
 -auxbase-strip ieee754dp.o 
 -g -Os -Os -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -Wno-uninitialized 
 -Wdeclaration-after-statement -Wno-pointer-sign -version
 -fno-strict-aliasing -fno-common -fno-pic -ffreestanding
 -fomit-frame-pointer -fno-stack-protector -o - |
  /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/../../../../mips-linux-gnu/bin/as
  -G 0 -EB -g -no-mdebug -mabi=32 -march=mips32 -mno-shared -v -mips32 --trap
  -o ieee754dp.o -

GNU assembler version 2.18 (mips-linux-gnu) using BFD version (GNU Binutils) 2.18
#include "..." search starts here:
#include <...> search starts here:
 include
 include/asm-mips/mach-brcmstb
 include/asm-mips/mach-generic
 /home/nds-uk/kyoupark/ext3/build_mips/staging_dir/include
 /home/nds-uk/kyoupark/ext3/build_mips/staging_dir/usr/include
 /home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include
 /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/include
End of search list.
GNU C (GCC) version 4.3.0 (mips-linux-gnu)
	compiled by GNU C version 4.4.7 20120313 (Red Hat 4.4.7-16), GMP version 4.2.4, MPFR version 2.4.0.
GGC heuristics: --param ggc-min-expand=100 --param ggc-min-heapsize=131072
Compiler executable checksum: a7616c4b28ed1fe8ba7a7bd00e321c4d
COMPILER_PATH=/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../libexec/gcc/mips-linux-gnu/4.3.0/:/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../libexec/gcc/:/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/../../../../mips-linux-gnu/bin/
LIBRARY_PATH=/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/:/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/:/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/../../../../mips-linux-gnu/lib/
COLLECT_GCC_OPTIONS='-nostdinc' '-isystem' '/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/include' '-D__KERNEL__' '-Iinclude' '-include' 'include/linux/autoconf.h' '-Wall' '-Wundef' '-Wstrict-prototypes' '-Wno-trigraphs' '-fno-strict-aliasing' '-fno-common' '-Os' '-mabi=32' '-G' '0' '-mno-abicalls' '-fno-pic' '-pipe' '-msoft-float' '-ffreestanding' '-march=mips32' '-Wno-uninitialized' '-Iinclude/asm-mips/mach-brcmstb' '-Iinclude/asm-mips/mach-generic' '-fomit-frame-pointer' '-g' '-fno-stack-protector' '-Wdeclaration-after-statement' '-Wno-pointer-sign' '-Os' '-I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/include' '-I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/usr/include' '-I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include' '-DKBUILD_STR(s)=\#s' '-DKBUILD_BASENAME=KBUILD_STR(ieee754dp)' '-DKBUILD_MODNAME=KBUILD_STR(ieee754dp)' '-v' '-c' '-o' 'ieee754dp.o' '-mllsc' '-mno-shared'


<run-own-gcc-manually>
// must quote ( like \( note: what is this?

/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../libexec/gcc/mips-linux-gnu/4.3.0/cc1 -quiet -nostdinc -v -Iinclude -Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include -iprefix /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/ -D__KERNEL__ -DKBUILD_STR\(s\)=\#s -DKBUILD_BASENAME=KBUILD_STR\(ieee754dp\) -DKBUILD_MODNAME=KBUILD_STR\(ieee754dp\) -isystem /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/include -include include/linux/autoconf.h -MD arch/mips/math-emu/.ieee754dp.o.d arch/mips/math-emu/ieee754dp.c -G 0 -quiet -dumpbase ieee754dp.c -mabi=32 -mno-abicalls -msoft-float -march=mips32 -mgnu-plts -mno-shared -minterlink-mips16 -auxbase-strip ieee754dp.o -g -Os -Os -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -Wno-uninitialized -Wdeclaration-after-statement -Wno-pointer-sign -version -fno-strict-aliasing -fno-common -fno-pic -ffreestanding -fomit-frame-pointer -fno-stack-protector -o - | /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/../../../../mips-linux-gnu/bin/as -G 0 -EB -g -no-mdebug -mabi=32 -march=mips32 -mno-shared -v -mips32 --trap -o ieee754dp.o -
cc1: error: unrecognized command line option "-mgnu-plts"

// /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../libexec/gcc/mips-linux-gnu/4.3.0/cc1
// -quiet -nostdinc -v -Iinclude -Iinclude/asm-mips/mach-brcmstb
// -Iinclude/asm-mips/mach-generic
// -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/include
// -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/usr/include
// -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include
// -iprefix
// /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/
// -D__KERNEL__ -DKBUILD_STR\(s\)=\#s -DKBUILD_BASENAME=KBUILD_STR\(ieee754dp\)
// -DKBUILD_MODNAME=KBUILD_STR\(ieee754dp\) -isystem
// /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/include
// -include include/linux/autoconf.h -MD arch/mips/math-emu/.ieee754dp.o.d
// arch/mips/math-emu/ieee754dp.c -G 0 -quiet -dumpbase ieee754dp.c -mabi=32
// -mno-abicalls -msoft-float -march=mips32 -mgnu-plts -mno-shared
// -minterlink-mips16 -auxbase-strip ieee754dp.o -g -Os -Os -Wall -Wundef
// -Wstrict-prototypes -Wno-trigraphs -Wno-uninitialized
// -Wdeclaration-after-statement -Wno-pointer-sign -version -fno-strict-aliasing
// -fno-common -fno-pic -ffreestanding -fomit-frame-pointer -fno-stack-protector
// -o - |
// /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/../../../../mips-linux-gnu/bin/as
// -G 0 -EB -g -no-mdebug -mabi=32 -march=mips32 -mno-shared -v -mips32 --trap -o
// ieee754dp.o -


<check-cc-output>
// b.o, use brcm gcc
/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../libexec/gcc/mips-linux-uclibc/4.2.0/cc1 \
-quiet -nostdinc -v -Iinclude -Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic -I/home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/usr/include -I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include -iprefix /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/ \
-D__KERNEL__ -DKBUILD_STR\(s\)=\#s -DKBUILD_BASENAME=KBUILD_STR\(ieee754dp\) -DKBUILD_MODNAME=KBUILD_STR\(ieee754dp\) -isystem /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include -include include/linux/autoconf.h -MD arch/mips/math-emu/.ieee754dp.o.d arch/mips/math-emu/ieee754dp.c -G 0 -quiet -dumpbase ieee754dp.c \
-mabi=32 -mno-abicalls -msoft-float -march=mips32 -mgnu-plts -mno-shared -minterlink-mips16 \
-auxbase-strip ieee754dp.o -g -Os -Os -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -Wno-uninitialized -Wdeclaration-after-statement -Wno-pointer-sign -version -fno-strict-aliasing -fno-common -fno-pic -ffreestanding -fomit-frame-pointer -fno-stack-protector -o b.o 

// a.o, use own gcc
/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../libexec/gcc/mips-linux-gnu/4.3.0/cc1 \
-quiet -nostdinc -v -Iinclude -Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/ext3/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include -iprefix /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/ -D__KERNEL__ -DKBUILD_STR\(s\)=\#s -DKBUILD_BASENAME=KBUILD_STR\(ieee754dp\) -DKBUILD_MODNAME=KBUILD_STR\(ieee754dp\) -isystem /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/include -include include/linux/autoconf.h -MD arch/mips/math-emu/.ieee754dp.o.d arch/mips/math-emu/ieee754dp.c -G 0 -quiet -dumpbase ieee754dp.c \
-mabi=32 -mno-abicalls -msoft-float -march=mips32 \
-mno-shared -minterlink-mips16 -auxbase-strip ieee754dp.o -g -Os -Os -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -Wno-uninitialized -Wdeclaration-after-statement -Wno-pointer-sign -version -fno-strict-aliasing -fno-common -fno-pic -ffreestanding -fomit-frame-pointer -fno-stack-protector -o a.o 

$ ack ucmpdi2 a.o
        .globl  __ucmpdi2
        jal     __ucmpdi2

but no in b.o

// same for other versions
// /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../libexec/gcc/mips-linux-gnu/4.3.0/cc1 \
// -quiet -nostdinc -v -Iinclude -Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic \
// -I/home/nds-uk/kyoupark/ext4/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/ext4/build_mips/staging_dir/usr/include \
// -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include -iprefix /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/ -D__KERNEL__ -DKBUILD_STR\(s\)=\#s -DKBUILD_BASENAME=KBUILD_STR\(ieee754dp\) -DKBUILD_MODNAME=KBUILD_STR\(ieee754dp\) -isystem /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../lib/gcc/mips-linux-gnu/4.3.0/include -include include/linux/autoconf.h -MD arch/mips/math-emu/.ieee754dp.o.d arch/mips/math-emu/ieee754dp.c -G 0 -quiet -dumpbase ieee754dp.c \
// -mabi=32 -mno-abicalls -msoft-float -march=mips32 \
// -mno-shared -minterlink-mips16 -auxbase-strip ieee754dp.o -g -Os -Os -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -Wno-uninitialized -Wdeclaration-after-statement -Wno-pointer-sign -version -fno-strict-aliasing -fno-common -fno-pic -ffreestanding -fomit-frame-pointer -fno-stack-protector -o a.o 
// 
// /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-10-install/bin/../libexec/gcc/mips-linux-gnu/4.8.2/cc1 \
// -quiet -nostdinc -v -Iinclude -Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic \
// -I/home/nds-uk/kyoupark/ext4/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/ext4/build_mips/staging_dir/usr/include \
// -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-10-install/mips-linux-gnu/include -iprefix /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-10-install/bin/../lib/gcc/mips-linux-gnu/4.8.2/ -D__KERNEL__ -DKBUILD_STR\(s\)=\#s -DKBUILD_BASENAME=KBUILD_STR\(ieee754dp\) -DKBUILD_MODNAME=KBUILD_STR\(ieee754dp\) \
// -isystem /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-10-install/bin/../lib/gcc/mips-linux-gnu/4.8.2/include -include include/linux/autoconf.h -MD arch/mips/math-emu/.ieee754dp.o.d arch/mips/math-emu/ieee754dp.c -G 0 -quiet -dumpbase ieee754dp.c \
// -mabi=32 -mno-abicalls -msoft-float -march=mips32 \
// -mno-shared -minterlink-mips16 -auxbase-strip ieee754dp.o -g -Os -Os -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs -Wno-uninitialized -Wdeclaration-after-statement -Wno-pointer-sign -version -fno-strict-aliasing -fno-common -fno-pic -ffreestanding -fomit-frame-pointer -fno-stack-protector -o a.o 


<own-gcc-cc-help> see no -gnuplt support
/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-try-install/bin/../libexec/gcc/mips-linux-gnu/4.3.0/cc1 --help
The following options are specific to the language Ada:
  -feliminate-unused-debug-types 	[enabled]
  -gnat<options>              		

The following options are specific to the language C:
 No options with the desired characteristics were found

The following options are specific to the language C++:
 No options with the desired characteristics were found

The following options are specific to the language Fortran:
  -J<directory>               		
  -Waliasing                  		
  -Wampersand                 		
  -Wcharacter-truncation      		
  -Wimplicit-interface        		
  -Wline-truncation           		
  -Wnonstd-intrinsics         		
  -Wsurprising                		
  -Wtabs                      		
  -Wunderflow                 		
  -fall-intrinsics            		
  -fautomatic                 		
  -fbackslash                 		
  -fbacktrace                 		
  -fblas-matmul-limit=        		
  -fconvert=big-endian        		
  -fconvert=little-endian     		
  -fconvert=native            		
  -fconvert=swap              		
  -fcray-pointer              		
  -fd-lines-as-code           		
  -fd-lines-as-comments       		
  -fdefault-double-8          		
  -fdefault-integer-8         		
  -fdefault-real-8            		
  -fdollar-ok                 		
  -fdump-core                 		
  -fdump-parse-tree           		
  -fexternal-blas             		
  -ff2c                       		
  -ffixed-form                		
  -ffixed-line-length-<n>     		
  -ffixed-line-length-none    		
  -ffpe-trap=[..]             		
  -ffree-form                 		
  -ffree-line-length-<n>      		
  -ffree-line-length-none     		
  -fimplicit-none             		
  -finit-character=           		
  -finit-integer=             		
  -finit-local-zero           		
  -finit-logical=             		
  -finit-real=                		
  -fintrinsic-modules-path    		
  -fmax-errors=<n>            		
  -fmax-identifier-length=<n> 		
  -fmax-stack-var-size=<n>    		
  -fmax-subrecord-length=<n>  		
  -fmodule-private            		
  -fpack-derived              		
  -frange-check               		
  -frecord-marker=4           		
  -frecord-marker=8           		
  -frecursive                 		
  -frepack-arrays             		
  -fsecond-underscore         		
  -fsign-zero                 		
  -funderscoring              		
  -static-libgfortran         		
  -std=f2003                  		
  -std=f95                    		
  -std=gnu                    		
  -std=legacy                 		

The following options are specific to the language Java:
  -Wextraneous-semicolon      		[disabled]
  -Wout-of-date               		[enabled]
  -Wredundant-modifiers       		[disabled]
  --CLASSPATH                 		
  -fassert                    		[enabled]
  --bootclasspath=<path>      		
  -fbootstrap-classes         		[disabled]
  -fcheck-references          		[disabled]
  --classpath=<path>          		
  -femit-class-file           		[disabled]
  -femit-class-files          		[disabled]
  --encoding=<encoding>       		
  --extdirs=<path>            		
  -ffilelist-file             		[disabled]
  -fforce-classes-archive-check 	[disabled]
  -fhash-synchronization      		[disabled]
  -findirect-classes          		[enabled]
  -findirect-dispatch         		[disabled]
  -fjni                       		[disabled]
  -foptimize-static-class-initialization 	[disabled]
  -freduced-reflection        		[disabled]
  -fsource=                   		
  -fstore-check               		[enabled]
  -ftarget=                   		
  -fuse-boehm-gc              		[disabled]
  -fuse-divide-subroutine     		[enabled]

The following options are specific to the language ObjC:
 No options with the desired characteristics were found

The following options are specific to the language ObjC++:
  -fobjc-call-cxx-cdtors      		[disabled]

The following options are specific to the language Treelang:
  -flexer-trace               		
  -fparser-trace              		

The following options are language-related:
  -A<question>=<answer>       		
  -C                          		
  -CC                         		
  -D<macro>[=<val>]           		
  -F <dir>                    		
  -H                          		
  -MD                         		
  -MG                         		
  -MMD                        		
  -MQ <target>                		
  -P                          		
  -U<macro>                   		
  -Wabi                       		[disabled]
  -Waddress                   		[disabled]
  -Wassign-intercept          		[disabled]
  -Wbad-function-cast         		[disabled]
  -Wc++-compat                		[disabled]
  -Wc++0x-compat              		[disabled]
  -Wcast-qual                 		[disabled]
  -Wchar-subscripts           		[disabled]
  -Wclobbered                 		[enabled]
  -Wcomment                   		
  -Wcomments                  		
  -Wconversion                		[disabled]
  -Wctor-dtor-privacy         		[disabled]
  -Wdeclaration-after-statement 	[disabled]
  -Wdeprecated                		[enabled]
  -Wdiv-by-zero               		[enabled]
  -Weffc++                    		[disabled]
  -Wempty-body                		[enabled]
  -Wendif-labels              		
  -Werror                     		[disabled]
  -Werror-implicit-function-declaration 	
  -Wfloat-equal               		[disabled]
  -Wformat                    		
  -Wformat-contains-nul       		[disabled]
  -Wformat-extra-args         		[disabled]
  -Wformat-nonliteral         		[disabled]
  -Wformat-security           		[disabled]
  -Wformat-y2k                		[disabled]
  -Wformat-zero-length        		[disabled]
  -Wignored-qualifiers        		[enabled]
  -Wimplicit-function-declaration 	[enabled]
  -Wimplicit-int              		[disabled]
  -Wimport                    		
  -Winit-self                 		[disabled]
  -Wint-to-pointer-cast       		[enabled]
  -Winvalid-offsetof          		[enabled]
  -Winvalid-pch               		
  -Wlong-long                 		[enabled]
  -Wmain                      		
  -Wmissing-braces            		[disabled]
  -Wmissing-declarations      		[disabled]
  -Wmissing-field-initializers 		[enabled]
  -Wmissing-format-attribute  		[disabled]
  -Wmissing-include-dirs      		
  -Wmissing-parameter-type    		[enabled]
  -Wmissing-prototypes        		[disabled]
  -Wmultichar                 		
  -Wnested-externs            		[disabled]
  -Wnon-template-friend       		[enabled]
  -Wnon-virtual-dtor          		[disabled]
  -Wnonnull                   		[disabled]
  -Wnormalized=<id|nfc|nfkc>  		
  -Wold-style-cast            		[disabled]
  -Wold-style-declaration     		[enabled]
  -Wold-style-definition      		[disabled]
  -Woverlength-strings        		[enabled]
  -Woverloaded-virtual        		[disabled]
  -Woverride-init             		[enabled]
  -Wparentheses               		[disabled]
  -Wpmf-conversions           		[enabled]
  -Wpointer-arith             		[disabled]
  -Wpointer-sign              		[enabled]
  -Wpointer-to-int-cast       		[enabled]
  -Wpragmas                   		[enabled]
  -Wprotocol                  		[enabled]
  -Wredundant-decls           		[disabled]
  -Wreorder                   		[disabled]
  -Wreturn-type               		[disabled]
  -Wselector                  		[disabled]
  -Wsequence-point            		[disabled]
  -Wsign-compare              		[enabled]
  -Wsign-conversion           		[enabled]
  -Wsign-promo                		[disabled]
  -Wstrict-null-sentinel      		
  -Wstrict-prototypes         		[disabled]
  -Wstrict-selector-match     		[disabled]
  -Wsynth                     		[disabled]
  -Wsystem-headers            		[disabled]
  -Wtraditional               		[disabled]
  -Wtraditional-conversion    		[disabled]
  -Wtrigraphs                 		
  -Wtype-limits               		[enabled]
  -Wundeclared-selector       		[disabled]
  -Wundef                     		
  -Wunknown-pragmas           		
  -Wunused-macros             		
  -Wvariadic-macros           		
  -Wvla                       		[enabled]
  -Wwrite-strings             		[disabled]
  -ansi                       		
  -d<letters>                 		
  -faccess-control            		
  -falt-external-templates    		
  -fasm                       		
  -fbuiltin                   		
  -fcheck-new                 		
  -fcond-mismatch             		
  -fconserve-space            		
  -fconst-string-class=<name> 		
  -fdefault-inline            		
  -fdirectives-only           		
  -fdollars-in-identifiers    		
  -femit-struct-debug-baseonly 		
  -femit-struct-debug-detailed=<spec-list> 	
  -femit-struct-debug-reduced 		
  -fenforce-eh-specs          		
  -fexec-charset=<cset>       		
  -fextended-identifiers      		
  -ffor-scope                 		
  -ffreestanding              		
  -ffriend-injection          		[disabled]
  -fgnu-keywords              		
  -fgnu-runtime               		
  -fgnu89-inline              		[enabled]
  -fhosted                    		
  -fhuge-objects              		
  -fimplement-inlines         		
  -fimplicit-inline-templates 		
  -fimplicit-templates        		
  -finput-charset=<cset>      		
  -flax-vector-conversions    		
  -fms-extensions             		
  -fnext-runtime              		
  -fnil-receivers             		
  -fobjc-direct-dispatch      		[disabled]
  -fobjc-exceptions           		[disabled]
  -fobjc-gc                   		[disabled]
  -fobjc-sjlj-exceptions      		[enabled]
  -fopenmp                    		[disabled]
  -foperator-names            		
  -foptional-diags            		
  -fpch-preprocess            		
  -fpermissive                		
  -fpreprocessed              		
  -freplace-objc-classes      		
  -frepo                      		
  -frtti                      		
  -fshort-double              		
  -fshort-enums               		
  -fshort-wchar               		
  -fshow-column               		[disabled]
  -fsigned-bitfields          		
  -fsigned-char               		
  -fstats                     		
  -ftabstop=<number>          		
  -ftemplate-depth-<number>   		
  -fno-threadsafe-statics     		
  -funsigned-bitfields        		
  -funsigned-char             		
  -fuse-cxa-atexit            		
  -fuse-cxa-get-exception-ptr 		
  -fvisibility-inlines-hidden 		
  -fvisibility-ms-compat      		[disabled]
  -fvtable-gc                 		
  -fvtable-thunks             		
  -fweak                      		
  -fwide-exec-charset=<cset>  		
  -fworking-directory         		
  -fxref                      		
  -fzero-link                 		
  -gen-decls                  		
  -idirafter <dir>            		
  -imacros <file>             		
  -imultilib                  		
  -include <file>             		
  -iprefix <path>             		
  -iquote <dir>               		
  -isysroot <dir>             		
  -isystem <dir>              		
  -iwithprefix <dir>          		
  -iwithprefixbefore <dir>    		
  -nostdinc                   		
  -nostdinc++                 		
  -o <file>                   		
  -pedantic                   		[disabled]
  -pedantic-errors            		
  -print-objc-runtime-info    		
  -print-pch-checksum         		
  -remap                      		
  -std=c++0x                  		
  -std=c++98                  		
  -std=c89                    		
  -std=c99                    		
  -std=c9x                    		
  -std=gnu++0x                		
  -std=gnu++98                		
  -std=gnu89                  		
  -std=gnu99                  		
  -std=gnu9x                  		
  -std=iso9899:1990           		
  -std=iso9899:199409         		
  -std=iso9899:1999           		
  -std=iso9899:199x           		
  -traditional-cpp            		
  -trigraphs                  		
  -undef                      		
  -w                          		[disabled]

The --param option recognizes the following as parameters:
  salias-max-implicit-fields  The maximum number of fields in a structure
                              variable without direct structure accesses that
                              GCC will attempt to track separately
  salias-max-array-elements   The maximum number of elements in an array for
                              wich we track its elements separately
  sra-max-structure-size      The maximum structure size (in bytes) for which
                              GCC will use by-element copies
  sra-max-structure-count     The maximum number of structure fields for which
                              GCC will use by-element copies
  sra-field-structure-ratio   The threshold ratio between instantiated fields
                              and the total structure size
  struct-reorg-cold-struct-ratio The threshold ratio between current and
                              hottest structure counts
  max-inline-insns-single     The maximum number of instructions in a single
                              function eligible for inlining
  max-inline-insns-auto       The maximum number of instructions when
                              automatically inlining
  max-inline-insns-recursive  The maximum number of instructions inline
                              function can grow to via recursive inlining
  max-inline-insns-recursive-auto The maximum number of instructions non-inline
                              function can grow to via recursive inlining
  max-inline-recursive-depth  The maximum depth of recursive inlining for
                              inline functions
  max-inline-recursive-depth-auto The maximum depth of recursive inlining for
                              non-inline functions
  min-inline-recursive-probability Inline recursively only when the probability
                              of call being executed exceeds the parameter
  max-variable-expansions-in-unroller If -fvariable-expansion-in-unroller is
                              used, the maximum number of times that an
                              individual variable will be expanded during loop
                              unrolling
  min-vect-loop-bound         If -ftree-vectorize is used, the minimal loop
                              bound of a loop to be considered for vectorization
  max-delay-slot-insn-search  The maximum number of instructions to consider to
                              fill a delay slot
  max-delay-slot-live-search  The maximum number of instructions to consider to
                              find accurate live register information
  max-pending-list-length     The maximum length of scheduling's pending
                              operations list
  large-function-insns        The size of function body to be considered large
  large-function-growth       Maximal growth due to inlining of large function
                              (in percent)
  large-unit-insns            The size of translation unit to be considered
                              large
  inline-unit-growth          how much can given compilation unit grow because
                              of the inlining (in percent)
  inline-call-cost            expense of call operation relative to ordinary
                              arithmetic operations
  large-stack-frame           The size of stack frame to be considered large
  large-stack-frame-growth    Maximal stack frame growth due to inlining (in
                              percent)
  max-gcse-memory             The maximum amount of memory to be allocated by
                              GCSE
  max-gcse-passes             The maximum number of passes to make when doing
                              GCSE
  gcse-after-reload-partial-fraction The threshold ratio for performing partial
                              redundancy elimination after reload
  gcse-after-reload-critical-fraction The threshold ratio of critical edges
                              execution count that permit performing redundancy
                              elimination after reload
  max-unrolled-insns          The maximum number of instructions to consider to
                              unroll in a loop
  max-average-unrolled-insns  The maximum number of instructions to consider to
                              unroll in a loop on average
  max-unroll-times            The maximum number of unrollings of a single loop
  max-peeled-insns            The maximum number of insns of a peeled loop
  max-peel-times              The maximum number of peelings of a single loop
  max-completely-peeled-insns The maximum number of insns of a completely
                              peeled loop
  max-completely-peel-times   The maximum number of peelings of a single loop
                              that is peeled completely
  max-once-peeled-insns       The maximum number of insns of a peeled loop that
                              rolls only once
  max-unswitch-insns          The maximum number of insns of an unswitched loop
  max-unswitch-level          The maximum number of unswitchings in a single
                              loop
  max-iterations-to-track     Bound on the number of iterations the brute force
                              # of iterations analysis algorithm evaluates
  max-iterations-computation-cost Bound on the cost of an expression to compute
                              the number of iterations
  sms-max-ii-factor           A factor for tuning the upper bound that swing
                              modulo scheduler uses for scheduling a loop
  sms-dfa-history             The number of cycles the swing modulo scheduler
                              considers when checking conflicts using DFA
  sms-loop-average-count-threshold A threshold on the average loop count
                              considered by the swing modulo scheduler
  hot-bb-count-fraction       Select fraction of the maximal count of
                              repetitions of basic block in program given basic
                              block needs to have to be considered hot
  hot-bb-frequency-fraction   Select fraction of the maximal frequency of
                              executions of basic block in function given basic
                              block needs to have to be considered hot
  align-threshold             Select fraction of the maximal frequency of
                              executions of basic block in function given basic
                              block get alignment
  align-loop-iterations       Loops iterating at least selected number of
                              iterations will get loop alignement.
  max-predicted-iterations    The maximum number of loop iterations we predict
                              statically
  tracer-dynamic-coverage-feedback The percentage of function, weighted by
                              execution frequency, that must be covered by
                              trace formation. Used when profile feedback is
                              available
  tracer-dynamic-coverage     The percentage of function, weighted by execution
                              frequency, that must be covered by trace
                              formation. Used when profile feedback is not
                              available
  tracer-max-code-growth      Maximal code growth caused by tail duplication
                              (in percent)
  tracer-min-branch-ratio     Stop reverse growth if the reverse probability of
                              best edge is less than this threshold (in percent)
  tracer-min-branch-probability-feedback Stop forward growth if the probability
                              of best edge is less than this threshold (in
                              percent). Used when profile feedback is available
  tracer-min-branch-probability Stop forward growth if the probability of best
                              edge is less than this threshold (in percent).
                              Used when profile feedback is not available
  max-crossjump-edges         The maximum number of incoming edges to consider
                              for crossjumping
  min-crossjump-insns         The minimum number of matching instructions to
                              consider for crossjumping
  max-grow-copy-bb-insns      The maximum expansion factor when copying basic
                              blocks
  max-goto-duplication-insns  The maximum number of insns to duplicate when
                              unfactoring computed gotos
  max-cse-path-length         The maximum length of path considered in cse
  max-cse-insns               The maximum instructions CSE process before
                              flushing
  lim-expensive               The minimum cost of an expensive expression in
                              the loop invariant motion
  iv-consider-all-candidates-bound Bound on number of candidates below that all
                              candidates are considered in iv optimizations
  iv-max-considered-uses      Bound on number of iv uses in loop optimized in
                              iv optimizations
  iv-always-prune-cand-set-bound If number of candidates in the set is smaller,
                              we always try to remove unused ivs during its
                              optimization
  scev-max-expr-size          Bound on size of expressions used in the scalar
                              evolutions analyzer
  omega-max-vars              Bound on the number of variables in Omega
                              constraint systems
  omega-max-geqs              Bound on the number of inequalities in Omega
                              constraint systems
  omega-max-eqs               Bound on the number of equalities in Omega
                              constraint systems
  omega-max-wild-cards        Bound on the number of wild cards in Omega
                              constraint systems
  omega-hash-table-size       Bound on the size of the hash table in Omega
                              constraint systems
  omega-max-keys              Bound on the number of keys in Omega constraint
                              systems
  omega-eliminate-redundant-constraints When set to 1, use expensive methods to
                              eliminate all redundant constraints
  vect-max-version-for-alignment-checks Bound on number of runtime checks
                              inserted by the vectorizer's loop versioning for
                              alignment check
  vect-max-version-for-alias-checks Bound on number of runtime checks inserted
                              by the vectorizer's loop versioning for alias
                              check
  max-cselib-memory-locations The maximum memory locations recorded by cselib
  max-flow-memory-locations   The maximum memory locations recorded by flow
  ggc-min-expand              Minimum heap expansion to trigger garbage
                              collection, as a percentage of the total size of
                              the heap
  ggc-min-heapsize            Minimum heap size before we start collecting
                              garbage, in kilobytes
  max-reload-search-insns     The maximum number of instructions to search
                              backward when looking for equivalent reload
  max-aliased-vops            The maximum number of virtual operators that a
                              function is allowed to have before triggering
                              memory partitioning heuristics
  avg-aliased-vops            The average number of virtual operators that
                              memory statements are allowed to have before
                              triggering memory partitioning heuristics
  max-sched-region-blocks     The maximum number of blocks in a region to be
                              considered for interblock scheduling
  max-sched-region-insns      The maximum number of insns in a region to be
                              considered for interblock scheduling
  min-spec-prob               The minimum probability of reaching a source
                              block for interblock speculative scheduling
  max-sched-extend-regions-iters The maximum number of iterations through CFG
                              to extend regions
  max-sched-insn-conflict-delay The maximum conflict delay for an insn to be
                              considered for speculative motion
  sched-spec-prob-cutoff      The minimal probability of speculation success
                              (in percents), so that speculative insn will be
                              scheduled.
  max-last-value-rtl          The maximum number of RTL nodes that can be
                              recorded as combiner's last value
  integer-share-limit         The upper bound for sharing integer constants
  min-virtual-mappings        Minimum number of virtual mappings to consider
                              switching to full virtual renames
  virtual-mappings-ratio      Ratio between virtual mappings and virtual
                              symbols to do full virtual renames
  ssp-buffer-size             The lower bound for a buffer to be considered for
                              stack smashing protection
  max-jump-thread-duplication-stmts Maximum number of statements allowed in a
                              block that needs to be duplicated when threading
                              jumps
  max-fields-for-field-sensitive Maximum number of fields in a structure before
                              pointer analysis treats the structure as a single
                              variable
  max-sched-ready-insns       The maximum number of instructions ready to be
                              issued to be considered by the scheduler during
                              the first scheduling pass
  prefetch-latency            The number of insns executed before prefetch is
                              completed
  simultaneous-prefetches     The number of prefetches that can run at the same
                              time
  l1-cache-size               The size of L1 cache
  l1-cache-line-size          The size of L1 cache line
  l2-cache-size               The size of L2 cache
  use-canonical-types         Whether to use canonical types
  max-partial-antic-length    Maximum length of partial antic set when
                              performing tree pre optimization
  sccvn-max-scc-size          Maximum size of a SCC before SCCVN stops
                              processing a function
  df-double-queue-threshold-factor Multiplier used for determining the double-
                              queueing threshold

The following options control compiler warning messages:
  -Waggregate-return          		[disabled]
  -Warray-bounds              		[disabled]
  -Wattributes                		[enabled]
  -Wcast-align                		[disabled]
  -Wcoverage-mismatch         		[disabled]
  -Wdeprecated-declarations   		[enabled]
  -Wdisabled-optimization     		[disabled]
  -Wextra                     		
  -Winline                    		[disabled]
  -Wlarger-than-<number>      		
  -Wlogical-op                		[disabled]
  -Wmissing-noreturn          		[disabled]
  -Woverflow                  		[enabled]
  -Wpacked                    		[disabled]
  -Wpadded                    		[disabled]
  -Wshadow                    		[disabled]
  -Wstack-protector           		[disabled]
  -Wstrict-aliasing           		
  -Wstrict-aliasing=          		0xffffffff
  -Wstrict-overflow           		
  -Wstrict-overflow=          		0xffffffff
  -Wswitch                    		[disabled]
  -Wswitch-default            		[disabled]
  -Wswitch-enum               		[disabled]
  -Wuninitialized             		[disabled]
  -Wunreachable-code          		[disabled]
  -Wunsafe-loop-optimizations 		[disabled]
  -Wunused-function           		[disabled]
  -Wunused-parameter          		[disabled]
  -Wunused-value              		[disabled]
  -Wunused-variable           		[disabled]
  -Wvolatile-register-var     		[disabled]

The following options control optimizations:
  -falign-jumps               		[disabled]
  -falign-labels              		[disabled]
  -falign-loops               		[enabled]
  -fargument-alias            		[enabled]
  -fargument-noalias          		[disabled]
  -fargument-noalias-anything 		[disabled]
  -fargument-noalias-global   		[disabled]
  -fasynchronous-unwind-tables 		[disabled]
  -fbranch-count-reg          		[enabled]
  -fbranch-probabilities      		[disabled]
  -fbranch-target-load-optimize 	[disabled]
  -fbranch-target-load-optimize2 	[disabled]
  -fbtr-bb-exclusive          		[disabled]
  -fcaller-saves              		[disabled]
  -fcommon                    		[enabled]
  -fcprop-registers           		[disabled]
  -fcrossjumping              		[disabled]
  -fcse-follow-jumps          		[disabled]
  -fcse-skip-blocks           		[disabled]
  -fcx-limited-range          		[disabled]
  -fdata-sections             		[disabled]
  -fdce                       		[enabled]
  -fdefer-pop                 		[disabled]
  -fdelayed-branch            		[disabled]
  -fdelete-null-pointer-checks 		[disabled]
  -fdse                       		[enabled]
  -fearly-inlining            		[enabled]
  -fexceptions                		[disabled]
  -fexpensive-optimizations   		[disabled]
  -ffinite-math-only          		[disabled]
  -ffloat-store               		[disabled]
  -fforward-propagate         		[disabled]
  -fgcse                      		[disabled]
  -fgcse-after-reload         		[disabled]
  -fgcse-las                  		[disabled]
  -fgcse-lm                   		[enabled]
  -fgcse-sm                   		[disabled]
  -fguess-branch-probability  		[disabled]
  -fif-conversion             		[disabled]
  -fif-conversion2            		[disabled]
  -finline-functions-called-once 	[enabled]
  -finline-small-functions    		[disabled]
  -fipa-cp                    		[disabled]
  -fipa-matrix-reorg          		[disabled]
  -fipa-pta                   		[disabled]
  -fipa-pure-const            		[disabled]
  -fipa-reference             		[disabled]
  -fipa-type-escape           		[disabled]
  -fivopts                    		[enabled]
  -fjump-tables               		[enabled]
  -fmerge-all-constants       		[disabled]
  -fmerge-constants           		[disabled]
  -fmodulo-sched              		[disabled]
  -fmove-loop-invariants      		[enabled]
  -fnon-call-exceptions       		[disabled]
  -fomit-frame-pointer        		[disabled]
  -foptimize-register-move    		[disabled]
  -foptimize-sibling-calls    		[disabled]
  -fpack-struct               		[disabled]
  -fpack-struct=<number>      		
  -fpeel-loops                		[disabled]
  -fpeephole                  		[enabled]
  -fpeephole2                 		[disabled]
  -fpredictive-commoning      		[disabled]
  -fprefetch-loop-arrays      		[disabled]
  -freg-struct-return         		[disabled]
  -fregmove                   		[disabled]
  -frename-registers          		[enabled]
  -freorder-blocks            		[disabled]
  -freorder-blocks-and-partition 	[disabled]
  -freorder-functions         		[disabled]
  -frerun-cse-after-loop      		[enabled]
  -freschedule-modulo-scheduled-loops 	[disabled]
  -frounding-math             		[disabled]
  -frtl-abstract-sequences    		[disabled]
  -fsched-interblock          		[enabled]
  -fsched-spec                		[enabled]
  -fsched-spec-load           		[disabled]
  -fsched-spec-load-dangerous 		[disabled]
  -fsched-stalled-insns       		[disabled]
  -fsched-stalled-insns-dep   		[enabled]
  -fsched2-use-superblocks    		[disabled]
  -fsched2-use-traces         		[disabled]
  -fschedule-insns            		[disabled]
  -fschedule-insns2           		[disabled]
  -fsection-anchors           		[disabled]
  -fsignaling-nans            		[disabled]
  -fsigned-zeros              		[enabled]
  -fsingle-precision-constant 		[disabled]
  -fsplit-ivs-in-unroller     		[enabled]
  -fsplit-wide-types          		[disabled]
  -fstrict-aliasing           		[disabled]
  -fthread-jumps              		[disabled]
  -ftoplevel-reorder          		[enabled]
  -ftrapping-math             		[enabled]
  -ftrapv                     		[disabled]
  -ftree-ccp                  		[disabled]
  -ftree-ch                   		[disabled]
  -ftree-copy-prop            		[disabled]
  -ftree-copyrename           		[disabled]
  -ftree-cselim               		[enabled]
  -ftree-dce                  		[disabled]
  -ftree-dominator-opts       		[disabled]
  -ftree-dse                  		[disabled]
  -ftree-fre                  		[disabled]
  -ftree-loop-im              		[enabled]
  -ftree-loop-ivcanon         		[enabled]
  -ftree-loop-linear          		[disabled]
  -ftree-loop-optimize        		[enabled]
  -ftree-lrs                  		[disabled]
  -ftree-pre                  		[disabled]
  -ftree-reassoc              		[enabled]
  -ftree-salias               		[disabled]
  -ftree-scev-cprop           		[enabled]
  -ftree-sink                 		[disabled]
  -ftree-sra                  		[disabled]
  -ftree-store-ccp            		[disabled]
  -ftree-ter                  		[disabled]
  -ftree-vect-loop-version    		[enabled]
  -ftree-vectorize            		[disabled]
  -ftree-vrp                  		[disabled]
  -funit-at-a-time            		[disabled]
  -funroll-all-loops          		[disabled]
  -funroll-loops              		[disabled]
  -funsafe-loop-optimizations 		[disabled]
  -funsafe-math-optimizations 		[disabled]
  -funswitch-loops            		[disabled]
  -funwind-tables             		[disabled]
  -fvar-tracking              		[enabled]
  -fvar-tracking-uninit       		[disabled]
  -fvariable-expansion-in-unroller 	[disabled]
  -fvect-cost-model           		[disabled]
  -fvpt                       		[disabled]
  -fweb                       		[enabled]
  -fwhole-program             		[disabled]
  -fwrapv                     		[disabled]

The following options are target specific:
  -mabi=ABI                   		
  -mabicalls                  		[enabled]
  -mad                        		[disabled]
  -march=ISA                  		
  -mbranch-cost=COST          		0
  -mbranch-likely             		[disabled]
  -mcheck-zero-division       		[enabled]
  -mcode-readable=SETTING     		
  -mdivide-breaks             		[disabled]
  -mdivide-traps              		[enabled]
  -mdmx                       		[disabled]
  -mdouble-float              		[enabled]
  -mdsp                       		[disabled]
  -mdspr2                     		[disabled]
  -meb                        		[enabled]
  -mel                        		[disabled]
  -membedded-data             		[disabled]
  -mexplicit-relocs           		[enabled]
  -mextern-sdata              		[enabled]
  -mfix-r4000                 		[disabled]
  -mfix-r4400                 		[disabled]
  -mfix-sb1                   		[disabled]
  -mfix-vr4120                		[disabled]
  -mfix-vr4130                		[disabled]
  -mfix4300                   		[disabled]
  -mflip-mips16               		[disabled]
  -mflush-func=FUNC           		_flush_cache
  -mfp-exceptions             		[enabled]
  -mfp32                      		[enabled]
  -mfp64                      		[disabled]
  -mfused-madd                		[enabled]
  -mglibc                     		[enabled]
  -mgp32                      		[enabled]
  -mgp64                      		[disabled]
  -mgpopt                     		[enabled]
  -mhard-float                		[enabled]
  -minterlink-mips16          		[disabled]
  -mipsN                      		
  -mips16                     		[disabled]
  -mips3d                     		[disabled]
  -mllsc                      		[disabled]
  -mlocal-sdata               		[enabled]
  -mlong-calls                		[disabled]
  -mlong32                    		[enabled]
  -mlong64                    		[disabled]
  -mmemcpy                    		[disabled]
  -mmips-tfile                		[disabled]
  -mmt                        		[disabled]
  -mno-flush-func             		[disabled]
  -mno-mdmx                   		[disabled]
  -mno-mips16                 		[enabled]
  -mno-mips3d                 		[enabled]
  -mpaired-single             		[disabled]
  -mshared                    		[enabled]
  -msingle-float              		[disabled]
  -msmartmips                 		[disabled]
  -msoft-float                		[disabled]
  -msplit-addresses           		[enabled]
  -msym32                     		[disabled]
  -mtune=PROCESSOR            		
  -muclibc                    		[disabled]
  -muninit-const-in-rodata    		[disabled]
  -mvr4130-align              		[disabled]
  -mxgot                      		[disabled]

The following options are language-independent:
  --help                      		
  --help=<class>              		
  --param <param>=<value>     		
  --target-help               		
  -G<number>                  		
  -O<number>                  		
  -Os                         		
  -W                          		
  -Werror=                    		
  -Wfatal-errors              		[disabled]
  -aux-info <file>            		
  -dumpbase <file>            		
  -fPIC                       		[disabled]
  -fPIE                       		[disabled]
  -falign-functions           		[disabled]
  -fassociative-math          		[disabled]
  -fauto-inc-dec              		[enabled]
  -fbounds-check              		[disabled]
  -fcall-saved-<register>     		
  -fcall-used-<register>      		
  -fcheck-data-deps           		[disabled]
  -fdbg-cnt-list              		
  -fdbg-cnt=                  		
  -fdebug-prefix-map=         		
  -fdiagnostics-show-location=[once|every-line] 	
  -fdiagnostics-show-option   		
  -fdump-<type>               		
  -fdump-noaddr               		[disabled]
  -fdump-unnumbered           		[disabled]
  -feliminate-dwarf2-dups     		[disabled]
  -feliminate-unused-debug-symbols 	[disabled]
  -femit-class-debug-always   		[disabled]
  -ffixed-<register>          		
  -fforce-addr                		
  -ffunction-cse              		[enabled]
  -ffunction-sections         		[disabled]
  -fident                     		[enabled]
  -finhibit-size-directive    		[disabled]
  -finline                    		[disabled]
  -finline-limit=<number>     		
  -finstrument-functions      		[disabled]
  -finstrument-functions-exclude-file-list= 	
  -finstrument-functions-exclude-function-list= 	
  -fipa-struct-reorg          		[disabled]
  -fkeep-inline-functions     		[disabled]
  -fkeep-static-consts        		[enabled]
  -fleading-underscore        		[enabled]
  -floop-optimize             		
  -fmath-errno                		[enabled]
  -fmem-report                		[disabled]
  -fmerge-debug-strings       		[enabled]
  -fmessage-length=<number>   		
  -fmodulo-sched-allow-regmoves 	[disabled]
  -fmudflap                   		[disabled]
  -fmudflapir                 		[disabled]
  -fmudflapth                 		[disabled]
  -fopenmp-ssa                		[disabled]
  -fpcc-struct-return         		[enabled]
  -fpic                       		[disabled]
  -fpie                       		[disabled]
  -fpost-ipa-mem-report       		[disabled]
  -fpre-ipa-mem-report        		[disabled]
  -fprofile                   		[disabled]
  -fprofile-arcs              		[disabled]
  -fprofile-generate          		
  -fprofile-use               		
  -fprofile-values            		[disabled]
  -frandom-seed=<string>      		
  -freciprocal-math           		[disabled]
  -frecord-gcc-switches       		[disabled]
  -frerun-loop-opt            		
  -fsched-stalled-insns-dep=<number> 	
  -fsched-stalled-insns=<number> 	
  -fsched-verbose=<number>    		
  -fsee                       		[disabled]
  -fstack-check               		[disabled]
  -fstack-limit-register=<register> 	
  -fstack-limit-symbol=<name> 		
  -fstack-protector           		[disabled]
  -fstack-protector-all       		[disabled]
  -fstrength-reduce           		
  -fstrict-overflow           		[disabled]
  -fsyntax-only               		[disabled]
  -ftest-coverage             		[disabled]
  -ftime-report               		[disabled]
  -ftls-model=[global-dynamic|local-dynamic|initial-exec|local-exec] 	
  -ftracer                    		[disabled]
  -ftree-parallelize-loops=   		0x1
  -ftree-store-copy-prop      		
  -ftree-vectorizer-verbose=<number> 	
  -fverbose-asm               		[disabled]
  -fvisibility=[default|internal|hidden|protected] 	
  -fzero-initialized-in-bss   		[enabled]
  -g                          		
  -gcoff                      		
  -gdwarf-2                   		
  -ggdb                       		
  -gstabs                     		
  -gstabs+                    		
  -gvms                       		
  -gxcoff                     		
  -gxcoff+                    		
  -p                          		[disabled]
  -pie                        		
  -quiet                      		[disabled]
  -shared                     		

<brcm-gcc-cc-help>
/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../libexec/gcc/mips-linux-uclibc/4.2.0/cc1 --target-help

Target specific options:
  -mabi=ABI                   Generate code that conforms to the given ABI
  -mabicalls                  Generate code that can be used in SVR4-style
                              dynamic objects
  -mad                        Use PMC-style 'mad' instructions
  -malign-locals              Force 32-bit data alignment for local variables
  -march=ISA                  Generate code for the given ISA
  -mbranch-cost=COST          Override branch instruction cost with COST
  -mbranch-likely             Use Branch Likely instructions, overriding the
                              architecture default
  -mcheck-zero-division       Trap on integer divide by zero
  -mdivide-breaks             Use branch-and-break sequences to check for
                              integer divide by zero
  -mdivide-traps              Use trap instructions to check for integer divide
                              by zero
  -mdouble-float              Allow hardware floating-point instructions to
                              cover both 32-bit and 64-bit operations
  -mdsp                       Use MIPS-DSP instructions
  -mdspr2                     Use MIPS-DSP REV 2 instructions
  -meb                        Use big-endian byte order
  -mel                        Use little-endian byte order
  -membedded-data             Use ROM instead of RAM
  -mexplicit-relocs           Use NewABI-style %reloc() assembly operators
  -mfix-r4000                 Work around certain R4000 errata
  -mfix-r4400                 Work around certain R4400 errata
  -mfix-sb1                   Work around errata for early SB-1 revision 2 cores
  -mfix-vr4120                Work around certain VR4120 errata
  -mfix-vr4130                Work around VR4130 mflo/mfhi errata
  -mfix4300                   Work around an early 4300 hardware bug
  -mflip-mips16               Switch on/off mips16 ASE in-between functions
  -mflush-func=FUNC           Use FUNC to flush the cache before calling stack
                              trampolines
  -mfp-exceptions             FP exceptions are enabled
  -mfp32                      Use 32-bit floating-point registers
  -mfp64                      Use 64-bit floating-point registers
  -mfused-madd                Generate floating-point multiply-add instructions
  -mglibc                     Use GNU libc instead of uClibc

  <gnu-plt>
  -mgnu-plts                  When generating -mabicalls code, allow ~
                              executables to use PLTs and copy relocations ~

  -mgp32                      Use 32-bit general registers
  -mgp64                      Use 64-bit general registers
  -mhard-float                Allow the use of hardware floating-point
                              instructions
  -minterlink-mips16          Generate code that can be safely linked with
                              MIPS16 code.
  -mipsN                      Generate code for ISA level N
  -mips16                     Generate mips16 code
  -mips16e                    Deprecated, alias to -mips16
  -mips3d                     Use MIPS-3D instructions
  -mlong-calls                Use indirect calls
  -mlong32                    Use a 32-bit long type
  -mlong64                    Use a 64-bit long type
  -mmemcpy                    Don't optimize block moves
  -mmips-tfile                Use the mips-tfile postpass
  -mno-flush-func             Do not use a cache-flushing function before
                              calling stack trampolines
  -mno-mips16                 Generate normal-mode code
  -mno-mips3d                 Do not use MIPS-3D instructions
  -mpaired-single             Use paired-single floating-point instructions
  -mshared                    When generating -mabicalls code, make the code
                              suitable for use in shared libraries
  -msingle-float              Restrict the use of hardware floating-point
                              instructions to 32-bit operations
  -msoft-float                Prevent the use of all hardware floating-point
                              instructions
  -msplit-addresses           Optimize lui/addiu address loads
  -msym32                     Assume all symbols have 32-bit values
  -mtune=PROCESSOR            Optimize the output for PROCESSOR
  -muclibc                    Use uClibc instead of GNU libc
  -muninit-const-in-rodata    Put uninitialized constants in ROM (needs
                              -membedded-data)
  -mvr4130-align              Perform VR4130-specific alignment optimizations
  -mxgot                      Lift restrictions on GOT size
  -smartmips                  Use Smartmips instructions


={============================================================================
*kt_linux_gcc_400* gcc-build-sysroot

https://gcc.gnu.org/onlinedocs/gcc/Directory-Options.html#Directory-Options

--sysroot=`dir`
    Use dir as the logical root directory `for headers and libraries` 
    
    For example, if the compiler normally searches for headers in /usr/include
    and libraries in /usr/lib, it instead searches `dir`/usr/include and
    `dir`/usr/lib.

    If you use both this option and the -isysroot option, then the --sysroot
    option applies to libraries, but the -isysroot option applies to header
    files.

    The GNU linker (beginning with version 2.16) has the necessary support for
    this option. If your linker does not support this option, the header file
    aspect of --sysroot still works, but the library aspect does not. 


https://gcc.gnu.org/install/configure.html

Cross-Compiler-Specific Options

The following options only apply to building cross compilers.

--with-sysroot
--with-sysroot=dir
    Tells GCC to consider dir as the root of a tree that contains (a subset
        of) the root filesystem of the `target operating system` 
    
    `Target system headers, libraries and run-time object files` will be
    searched for in there. 

    More specifically, this acts `as if --sysroot=dir was added` to the `default
    options` of the built compiler. 
    
    The specified directory is not copied into the install tree, unlike the
    options --with-headers and --with-libs that this option obsoletes. 
    
    The default value, in case --with-sysroot is not given an argument, is
    ${gcc_tooldir}/sys-root. If the specified directory is a subdirectory of
    ${exec_prefix}, then it will be found relative to the GCC binaries if the
    installation tree is moved.

    This option affects the system root for the compiler used to build target
    libraries (which runs on the build system) and the compiler newly
    installed with make install; it does not affect the compiler which is used
    to build GCC itself.

    If you specify the --with-native-system-header-dir=dirname option then the
    compiler will search that directory within dirname for native system
    headers rather than the default /usr/include. 

    note: will not create sysroot directory.


http://wiki.osdev.org/GCC_Cross-Compiler
--with-sysroot 
tells `binutils` to enable sysroot support in the cross-compiler by pointing
it to a default empty directory. By default the linker refuses to use sysroots
for no good technical reason, while gcc is able to handle both cases at
  runtime. This will be useful later on. 

<gcc-sysroot>
--sysroot=directory
    Use directory as the location of the sysroot, overriding the
    configure-time default. This option `is only supported` by linkers that were
    configured using --with-sysroot. 


={============================================================================
*kt_linux_gcc_400* gcc-build-tc-failed-instructions

<1>
// note:
// DO NOT USE THIS method since fails to build gcc and there seems to be a
// problem in source distibution package.
//
// https://gcc.gnu.org/bugzilla/show_bug.cgi?id=48378
//
// Get gcc source from the distribution
// ------------------------------------
// 
// $ sudo apt-get source libstdc++6
// Reading package lists... Done
// Building dependency tree       
// Reading state information... Done
// Picking 'gcc-4.7' as source package instead of 'libstdc++6'
// ... (starts downloading)


<1>
http://eli.thegreenplace.net/2014/01/16/building-gcc-4-8-from-source-on-ubunu-12-04

// note:
// 4.8.2, 4.9.2, 4.9.4, all requires these:
// configure: error: Building GCC requires GMP 4.2+, MPFR 2.4.0+ and MPC 0.8.0+.
//
// However, depending on where you builds gcc, this passes without error and
// gets built. eg. 4.8.2 is configured on build server but not on debian VM.
//
// regardless of using:
// --with-gmp=/usr/local/lib --with-mpc=/usr/lib --with-mpfr=/usr/lib \
//
// // from configure.ac
// # The library versions listed in the error message below should match
// # the HARD-minimums enforced above.
//   if test x$have_gmp != xyes; then
//     AC_MSG_ERROR([Building GCC requires GMP 4.2+, MPFR 2.4.0+ and MPC 0.8.0+.
// Try the --with-gmp, --with-mpfr and/or --with-mpc options to specify
// their locations.  Source code for these libraries can be found at
// their respective hosting sites as well as at
// ftp://gcc.gnu.org/pub/gcc/infrastructure/.  See also
// http://gcc.gnu.org/install/prerequisites.html for additional info.  If
// you obtained GMP, MPFR and/or MPC from a vendor distribution package,
// make sure that you have installed both the libraries and the header
// files.  They may be located in separate packages.])
//   fi
// fi
//
// GMP is a free library for arbitrary precision arithmetic, operating on signed
// integers, rational numbers, and floating-point numbers. There is no practical
// limit to the precision except the ones implied by the available memory in the
// machine GMP runs on. GMP has a rich set of functions, and the functions have a
// regular interface.  
// 
// The main target applications for GMP are cryptography applications and
// research, Internet security applications, algebra systems, computational
// algebra research, etc. 
// 
// The MPFR library is a C library for multiple-precision floating-point
// computations with correct rounding. MPFR has continuously been supported by
// the INRIA and the current main authors come from the Caramba and AriC
// project-teams at Loria (Nancy, France) and LIP (Lyon, France) respectively;
// see more on the credit page. MPFR is based on the GMP multiple-precision
// library.
// 
// Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily
// high precision and correct rounding of the result. It extends the principles
// of the IEEE-754 standard for fixed precision real floating point numbers to
// complex numbers, providing well-defined semantics for every operation. At the
// same time, speed of operation at high precision is a major design goal. 
// 
// sudo apt-get install libgmp3-dev
// sudo apt-get install libmpfr-dev
// sudo apt-get install libmpc-dev

mkdir from-gnu
wget ftp://ftp.gnu.org/gnu/gcc/gcc-4.9.2/gcc-4.9.2.tar.bz2
tar xjvf gcc-4.9.2.tar.bz2 
mkdir build

// ../gcc-4.8.2/configure --disable-checking --enable-languages=c,c++ \
// --enable-multiarch --enable-shared --enable-threads=posix \
// --enable-libstdcxx-debug \
// --program-suffix=4.8.2 \
// --with-gmp=/usr/local/lib --with-mpc=/usr/lib --with-mpfr=/usr/lib \
// --without-included-gettext --with-system-zlib --with-tune=generic \
// --prefix=$HOME/gcc-build/install

// from ASAN how to build gcc
../gcc-4.8.2/configure --prefix=$HOME/toolchains --enable-languages=c,c++

make -j8
make install

$ cd /tmp
$ cat > test.c
int main() { return 42; }
$ $HOME/install/gcc-4.8.2/bin/gcc4.8 test.c
$ ./a.out; echo $?
42


<2> *gcc-build-mips*
note:
make fails for both 4.9.4 and 4.8.2 as well.

http://llvm.org/docs/GettingStarted.html#getting-a-modern-host-c-toolchain

wget https://ftp.gnu.org/gnu/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2

// wget https://ftp.gnu.org/gnu/gcc/gcc-4.8.2/gcc-4.8.2.tar.bz2.sig
// wget https://ftp.gnu.org/gnu/gnu-keyring.gpg
// signature_invalid=`gpg --verify --no-default-keyring --keyring ./gnu-keyring.gpg gcc-4.8.2.tar.bz2.sig`
// if [ $signature_invalid ]; then echo "Invalid signature" ; exit 1 ; fi

tar -xvjf gcc-4.8.2.tar.bz2
cd gcc-4.8.2

// downloads required packages
./contrib/download_prerequisites

cd ..
mkdir gcc-4.8.2-build
cd gcc-4.8.2-build
../gcc-4.8.2/configure --prefix=$HOME/gcc-bin --enable-languages=c,c++ --target=mips --program-suffix=4.9.4

make -j$(nproc)

checking for suffix of object files... configure: error: in `/home/NDS-UK/kyoupark/asn/gcc/gcc-4.9.2-build/mips-unknown-linux-gnu/libgcc':
configure: error: cannot compute suffix of object files: cannot compile
See `config.log' for more details.
make[1]: *** [configure-target-libgcc] Error 1
make[1]: Leaving directory `/home/NDS-UK/kyoupark/asn/gcc/gcc-4.9.2-build'
make: *** [all] Error 2

For more details, check out the excellent GCC wiki entry, where I got most of
this information from. (http://gcc.gnu.org/wiki/InstallingGCC)


<3>
https://www.linux-mips.org/wiki/Toolchains
If you're building for big-endian MIPS, your TARGET should be
mips-unknown-linux-gnu instead.

Have tried this target but failes on the same.


={============================================================================
*kt_linux_gcc_400* gcc-build-tc-buildroot-build

https://www.uclibc.org/toolchains.html
https://buildroot.org/download.html
wget --no-check-certificate https://buildroot.org/downloads/buildroot-2009.02.tar.gz
wget --no-check-certificate https://buildroot.org/downloads/buildroot-2016.11.tar.gz


{menuconfig}
this requires to have `apt-get install libncurses5-dev` if not installed.
`saves configuration to: broot-latest/buildroot-2016.08.1/.config`

make menuconfig

<wget-setting-in-menuconfig>
Connecting to cdn.kernel.org|151.101.36.69|:443... connected.
ERROR: certificate common name `k.ssl.fastly.net' doesn't match requested host name `cdn.kernel.org'.
To connect to cdn.kernel.org insecurely, use `--no-check-certificate'.

How to set?

You can add the necessary option for wget in the configuration.

Build options -> Commands -> Wget command.

Add what's recommended "--no-check-certificate" to the options already present
there. That will make it ignore all self-signed certificates.  The problem is
alioth.debian.org using a self-signed certificate instead of a proper one.
Regards.


// {error-buildroot-linux-2.6.18.8}
// <error>
// >>> linux-headers 2.6.18.8 Downloading
// --2016-09-23 14:27:50--  https://cdn.kernel.org/pub/linux/kernel/v2.6/linux-2.6.18.8.tar.xz
// Resolving cdn.kernel.org... 151.101.36.69
// 
// Build options -> Mirrors and Download locations 
// use https://cdn.kernel.org/pub
// 
// <error>
// // fixed by changing makefile
// // output/build/linux-headers-2.6.18/Makefile
// //
// // @896
// // headers_install: include/linux/version.h
// // #	$(Q)unifdef -Ux /dev/null
// 
// (cd /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18; PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" /usr/bin/make -j25 ARCH=mips HOSTCC="/usr/lib64/ccache/gcc" HOSTCFLAGS="" HOSTCXX="/usr/lib64/ccache/g++" INSTALL_HDR_PATH=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18/usr headers_install)
// make[1]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18'
//   CHK     include/linux/version.h
//   UPD     include/linux/version.h
// make[1]: unifdef: Command not found
// make[1]: *** [headers_install] Error 127
// make[1]: Leaving directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18'
// make: *** [/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18/.stamp_configured] Error 2
// 
// 
// <error>
// // fixed by changing version via menuconfig
// if ! support/scripts/check-kernel-headers.sh  /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/sysroot  4.7; then exit 1; fi
// Incorrect selection of kernel headers: expected 4.7.x, got 2.6.x
// make: *** [/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18/.stamp_staging_installed] Error 1
// 
// <error>
// >>> uclibc 1.0.17 Building
// /usr/bin/make -j25 -C /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/uclibc-1.0.17 \
//  ARCH="mips" \
//  CROSS_COMPILE="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2015.08.1/output/host/usr/bin/mips-buildroot-linux-uclibc-" UCLIBC_EXTRA_CFLAGS=" " \ 
//  HOSTCC="/usr/lib64/ccache/gcc" headers
// make[1]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/uclibc-1.0.17'
//   MKDIR libm/mips
//   MKDIR include/bits
//   MKDIR libubacktrace/mips
//   GEN include/bits/uClibc_config.h
//   LN include/semaphore.h
//   LN include/pthread.h
//   LN include/bits/libc-lock.h
//   LN include/bits/stdio-lock.h
//   LN include/bits/pthreadtypes.h
//   LN include/bits/semaphore.h
//   LN include/sgidefs.h
//   LN include/regdef.h
//   GEN include/bits/sysnum.h
// ERROR: Could not generate syscalls.
// Make sure that you have properly installed kernel headers.
// Your .config KERNEL_HEADERS="" was set to:
// /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18/usr/include/
// make[2]: *** [include/bits/sysnum.h] Error 1
// make[1]: *** [include/bits/uClibc_config.h] Error 2
// make[1]: Leaving directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/uclibc-1.0.17'
// make: *** [/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/uclibc-1.0.17/.stamp_built] Error 2
// 
// errors from:
// 
// /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/uclibc-1.0.17/Makefile.in
// 
// 	$(Q)if grep -q __NR_ $@; then true; else \
// 		rm -f $@; \
// 		echo "ERROR: Could not generate syscalls."; \
// 		echo "Make sure that you have proper kernel headers."; \
// 		echo "Your .config in KERNEL_HEADERS=\"\" was set to:"; \
// 		echo "${KERNEL_HEADERS}"; \
// 		exit 1; \
// 	fi
// 
// <how-buildroot-set-uclibc>
// How to fix? BCM uses 0.9.29 from uclibc but not uclibc-ng and also uclibc-ng
// starts from 1.0.0.
// 
// when untar buildroot without changes:
// kyoupark@ukstbuild2:~/asn/broot-latest/buildroot-2016.08.1/package/uclibc$ ll
// -rw-r--r--    1 kyoupark ccusers  1473 Sep 21 21:53 0001-arm-fix-compile-in-thumb-mode.patch
// -rw-r--r--    1 kyoupark ccusers  1904 Sep 21 21:53 0002-sunrpc-Do-not-use-alloca-in-clntudp_call.patch
// -rw-r--r--    1 kyoupark ccusers  2263 Sep 21 21:53 0003-ARC-Support-syscall-ABI-v4.patch
// -rw-r--r--    1 kyoupark ccusers  5727 Sep 21 21:53 Config.in
// -rw-r--r--    1 kyoupark ccusers   137 Sep 21 21:53 uclibc.hash
// -rw-r--r--    1 kyoupark ccusers 14833 Sep 21 21:53 uclibc.mk
// -rw-r--r--    1 kyoupark ccusers  1131 Sep 21 21:53 uClibc-ng.config
//
// kyoupark@ukstbuild2:~/asn/broot-latest/buildroot-2016.08.1/package/uclibc$ more uclibc.hash
// # From http://www.uclibc-ng.org/
// sha256  a2e7207634c19997e8b9f3e712182d80d42aaa85ce3462eff1a9bce812aaf354        uClibc-ng-1.0.17.tar.xz
// 
// How about 1.0.0 from uclib-ng since to avoid to know how to build uclibc.
// 
// $ cp -r uclibc/ uclibc-org
// $ change uclibc.hash to use 1.0.0
// $ change uclibc.mk
//   UCLIBC_VERSION = 1.0.0
//   UCLIBC_SOURCE = uClibc-ng-$(UCLIBC_VERSION).tar.xz
// $ delete all patches
// $ make clean
// $ make
// 
// SAME ERROR.
// 
// HOW about uclibc?
// 
// UCLIBC_VERSION = 0.9.30.1
// UCLIBC_SOURCE = uClibc-$(UCLIBC_VERSION).tar.bz2
// UCLIBC_SITE = https://www.uclibc.org/downloads
// UCLIBC_LICENSE = LGPLv2.1+
// UCLIBC_LICENSE_FILES = COPYING.LIB
// UCLIBC_INSTALL_STAGING = YES
// 
// ERROR: Could not generate syscalls.
// Make sure that you have proper kernel headers.
// Your .config in KERNEL_HEADERS="" was set to:
// /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18/usr/include/


{make-help}
8.1 make tips

This is a collection of tips that help you make the most of Buildroot.

Display all commands executed by make:
$ make V=1 <target>

Display the list of boards with a defconfig:
$ make list-defconfigs

Display `all available targets`:
$ make help


busybox-menuconfig only works when busybox is enabled;

linux-menuconfig and linux-savedefconfig only work when linux is enabled;

uclibc-menuconfig is only available when the uClibc C library is selected in
the internal toolchain backend;

/buildroot-2016.08.1$ make help
Cleaning:
  clean                  - delete all files created by build
  distclean              - delete all non-source files (including .config)

Build:
  all                    - make world
  toolchain              - build toolchain

Configuration:
  menuconfig             - interactive curses-based configurator
  oldconfig              - resolve any unresolved symbols in .config
  silentoldconfig        - Same as oldconfig, but quietly, additionally update deps
  olddefconfig           - Same as silentoldconfig but sets new symbols to their default value
  randconfig             - New config with random answer to all options
  defconfig              - New config with default answer to all options
                             BR2_DEFCONFIG, if set, is used as input
  savedefconfig          - Save current config to BR2_DEFCONFIG (minimal config)
  allyesconfig           - New config where all options are accepted with yes
  allnoconfig            - New config where all options are answered with no
  randpackageconfig      - New config with random answer to package options
  allyespackageconfig    - New config where pkg options are accepted with yes
  allnopackageconfig     - New config where package options are answered with no

Package-specific:
  <pkg>                  - Build and install <pkg> and all its dependencies
  <pkg>-source           - Only download the source files for <pkg>
  <pkg>-extract          - Extract <pkg> sources
  <pkg>-patch            - Apply patches to <pkg>
  <pkg>-depends          - Build <pkg>'s dependencies
  <pkg>-configure        - Build <pkg> up to the configure step
  <pkg>-build            - Build <pkg> up to the build step
  <pkg>-graph-depends    - Generate a graph of <pkg>'s dependencies
  <pkg>-dirclean         - Remove <pkg> build directory
  <pkg>-reconfigure      - Restart the build from the configure step
  <pkg>-rebuild          - Restart the build from the build step

busybox:
  busybox-menuconfig     - Run BusyBox menuconfig

uclibc:
  `uclibc-menuconfig`      - Run uClibc menuconfig

Documentation:
  manual                 - build manual in all formats
  manual-html            - build manual in HTML
  manual-split-html      - build manual in split HTML
  manual-pdf             - build manual in PDF
  manual-text            - build manual in text
  manual-epub            - build manual in ePub
  graph-build            - generate graphs of the build times
  graph-depends          - generate graph of the dependency tree
  graph-size             - generate stats of the filesystem size
  list-defconfigs        - list all defconfigs (pre-configured minimal systems)

Miscellaneous:
  source                 - download all sources needed for offline-build
  source-check           - check selected packages for valid download URLs
  external-deps          - list external packages used
  legal-info             - generate info about license compliance

  make V=0|1             - 0 => quiet build (default), 1 => verbose build
  make O=dir             - Locate all output files in "dir", including .config

For further details, see README, generate the Buildroot manual, or consult
it on-line at http://buildroot.org/docs.html


{rebuild}
8.2 Understanding when a full rebuild is necessary

Buildroot does not attempt to detect what parts of the system should be
rebuilt when the system configuration is changed through make menuconfig, make
xconfig or one of the other configuration tools. In some cases, Buildroot
should rebuild the entire system, in some cases, only a specific subset of
packages. But detecting this in a completely reliable manner is very
difficult, and therefore the Buildroot developers have decided to simply not
attempt to do this.  

Instead, it is the responsibility of the user to know when a full rebuild is
necessary. As a hint, here are a few rules of thumb that can help you
understand how to work with Buildroot:


* When the toolchain configuration is changed, `a complete rebuild` generally is
  needed. Changing the toolchain configuration often involves changing the
  compiler version, the type of C library or its configuration, or some other
  fundamental configuration item, and these changes have an impact on the
  entire system.

Generally speaking, when you’re facing a build error and you’re unsure of the
potential consequences of the configuration changes you’ve made, do a full
rebuild.


<full-rebuild>
full rebuild is achieved by running:
$ make V=1 clean all


8.3 Understanding how to rebuild packages

The easiest way to rebuild a single package from scratch is to remove its
build directory in `output/build` Buildroot will then re-extract,
      re-configure, re-compile and re-install this package from scratch. You
      can ask buildroot to do this with the make `<package>-dirclean` command.

On the other hand, if you only want to restart the build process of a package
`from its compilation step`, you can run make `<package>-rebuild`, followed by
`make or make <package>` It will restart the compilation and installation of
the package, but not from scratch: it basically re-executes make and make
install inside the package, so it will only rebuild files that changed.

If you want to restart the build of a package `from its configuration step`,
   you can run `make <package>-reconfigure`, followed by make or make
   <package>. It will restart the configuration, compilation and installation
   of the package.

Internally, Buildroot creates so-called stamp files to keep track of which
build steps have been completed for each package. They are stored in the
package build directory, output/build/<package>-<version>/ and are named
.stamp_<stepname>.  The commands detailed above simply manipulate these stamp
files to force Buildroot to restart a specific set of steps of a package build
process.

Further details about package special make targets are explained in Section
8.12.5.


8.5 Building out-of-tree

As default, everything built by Buildroot is stored in the directory output in
the Buildroot tree. Buildroot also supports building out of tree with a syntax
similar to the Linux kernel. To use it, add O=<directory> to the make command
line:

$ make O=/tmp/build

Or:

$ cd /tmp/build; make O=$PWD -C path/to/buildroot

All the output files will be located under /tmp/build. If the O path does not
exist, Buildroot will create it.

When using out-of-tree builds, the Buildroot .config and temporary files are
also stored in the output directory. This means that you can safely run
multiple builds in parallel using the same source tree as long as they use
unique output directories.  For ease of use, Buildroot generates a Makefile
wrapper in the output directory - so after the first run, you no longer need
to pass O=<...> and -C <...>, simply run (in the output directory):

$ make <target>


8.8 Graphing the dependencies between packages

One of Buildroot’s jobs is to know the dependencies between packages, and make
sure they are built in the right order. These dependencies can sometimes be
quite complicated, and for a given system, it is often not easy to understand
why such or such package was brought into the build by Buildroot.  In order to
help understanding the dependencies, and therefore better understand what is
the role of the different components in your embedded Linux system, Buildroot
is capable of generating dependency graphs.  To generate a dependency graph of
the full system you have compiled, simply run:

make graph-depends

You will find the generated graph in output/graphs/graph-depends.pdf. If your
system is quite large, the dependency graph may be too complex and difficult
to read. It is therefore possible to generate the dependency graph just for a
given package:

make <pkg>-graph-depends

You will find the generated graph in output/graph/<pkg>-graph-depends.pdf.

By default, the dependency graphs are generated in the PDF format. However, by
passing the BR2_GRAPH_OUT environment variable, you can switch to other output
formats, such as PNG, PostScript or SVG. All formats supported by the -T
option of the dot tool are supported. BR2_GRAPH_OUT=svg make graph-depends
The graph-depends behaviour can be controlled by setting options in the
BR2_GRAPH_DEPS_OPTS environment variable. The accepted options are:


8.10 Graphing the filesystem size contribution of packages

When your target system grows, it is sometimes useful to understand how much
each Buildroot package is contributing to the overall root filesystem size. To
help with such an analysis, Buildroot collects data about files installed by
each package and using this data, generates a graph and CSV files detailing
the size contribution of the different packages.  To generate these data after
a build, run:


8.12.4 Location of downloaded packages

The various tarballs that are downloaded by Buildroot are all stored in
BR2_DL_DIR, which by default is the dl directory. If you want to keep a
complete version of Buildroot which is known to be working with the associated
tarballs, you can make a copy of this directory. This will allow you to
regenerate the toolchain and the target filesystem with exactly the same
versions.

The following line should be added to <~/.bashrc>.

export BR2_DL_DIR=<shared download location>

The download location can also be set in the .config file, with the BR2_DL_DIR
option. Unlike most options in the .config file, this value is overridden by
the BR2_DL_DIR environment variable.


BR2_DL_DIR to override the directory in which Buildroot stores/retrieves
downloaded files Note that the Buildroot download directory can also be set
from the configuration interface, so through the Buildroot .config file; this
is the recommended way of setting it.


8.12.5 Package-specific make targets

Running make <package> builds and installs that particular package and its
dependencies.  For packages relying on the Buildroot infrastructure, there are
numerous special make targets that can be called independently like this:

make <package>-<target>

The package build targets are (in the order they are executed):

Additionally, there are some other useful make targets:

rebuild 

Re-run the compilation commands - this only makes sense when using the
OVERRIDE_SRCDIR feature or when you modified a file directly in the build
directory

reconfigure 

Re-run the configure commands, then rebuild - this only makes sense when using
the OVERRIDE_SRCDIR feature or when you modified a file directly in the build
directory

<ex>
# broot-latest/buildroot-2016.08.1/package/gcc
# gcc is a special package, not named gcc, but gcc-initial and
# gcc-final, but patches are nonetheless stored in package/gcc in the
# tree, and potentially in BR2_GLOBAL_PATCH_DIR directories as well.

# this works
$ make V=1 host-gcc-initial-reconfigure


{rebuild-package}
8.12.6 Using Buildroot during development

The normal operation of Buildroot is to download a tarball, extract it,
    configure, compile and install the software component found inside this
    tarball. 

The source code is extracted in output/build/<package>-<version>, which is a
temporary directory: whenever make clean is used, this directory is entirely
removed, and re-created at the next make invocation.

Even when a Git or Subversion repository is used as the input for the package
source code, Buildroot creates a tarball out of it, and then behaves as it
normally does with tarballs.

This behavior is well-suited when Buildroot is used mainly as an integration
tool, to build and integrate all the components of an embedded Linux system. 

  <why>
  However, if one uses Buildroot during the development of certain components
  of the system, this behavior is not very convenient: 

  one would instead like to make a small change to the source code of one
  package, and be able to quickly rebuild the system with Buildroot.

Making changes directly in output/build/<package>-<version> `is not` an
appropriate solution, because this directory is removed on make clean.

Therefore, Buildroot provides a specific mechanism for this use case: the
<pkg>_OVERRIDE_SRCDIR mechanism. Buildroot reads an override file, which
allows the user to tell Buildroot the location of the source for certain
packages. By default this override file is named `local.mk` and located in the
top directory of the Buildroot source tree, but a different location can be
specified through the BR2_PACKAGE_OVERRIDE_FILE configuration option.


In this override file, Buildroot expects to find lines of the form:

<pkg1>_OVERRIDE_SRCDIR = /path/to/pkg1/sources
<pkg2>_OVERRIDE_SRCDIR = /path/to/pkg2/sources

For example:

LINUX_OVERRIDE_SRCDIR = /home/bob/linux/
BUSYBOX_OVERRIDE_SRCDIR = /home/bob/busybox/

When Buildroot finds that for a given package, an <pkg>_OVERRIDE_SRCDIR has
been defined, it will no longer attempt to download, extract and patch the
package. Instead, it will directly use the source code available in in the
specified directory and make clean will not touch this directory. 

This allows to point Buildroot to your own directories, that can be managed by
Git, Subversion, or any other version control system. To achieve this,
  Buildroot will use rsync to copy the source code of the component from the
  specified <pkg>_OVERRIDE_SRCDIR to output/build/<package>-custom/.

This mechanism is best used in conjunction with the make <pkg>-rebuild and
make <pkg>-reconfigure targets.

A `make <pkg>-rebuild all` sequence will rsync the source code from
<pkg>_OVERRIDE_SRCDIR to output/ build/<package>-custom (thanks to rsync, only
    the modified files are copied), and restart the build process of just this
package.

In the example of the linux package above, the developer can then make a
source code change in /home/bob/linux and then run:

make linux-rebuild all

and in a matter of seconds gets the updated Linux kernel image in
output/images. Similarly, a change can be made to the BusyBox source code in
/home/bob/busybox, and after:

make busybox-rebuild all

the root filesystem image in output/images contains the updated BusyBox.


={============================================================================
*kt_linux_gcc_400* gcc-build-tc-buildroot-internal

<build-output>
Chapter 4 Buildroot quick start

Buildroot output is stored in a single directory, output/. This directory
contains several subdirectories:

images/ 

where all the images (kernel image, bootloader and root filesystem images) are
stored. These are the files you need to put on your target system.

build/ 

where all the components are built (this includes tools needed by Buildroot on
    the host and packages compiled for the target). This directory contains
one subdirectory for each of these components.  

staging/ 

which contains a hierarchy similar to a root filesystem hierarchy. This
directory contains the headers and libraries of the cross-compilation
toolchain and all the userspace packages selected for the target. 
However, this directory is not intended to be the root filesystem for the
target: it contains a lot of development files, `unstripped` binaries and
libraries that make it far too big for an embedded system. These development
files are used to compile libraries and applications for the target that
depend on other libraries.

target/ 

which contains almost the complete root filesystem for the target: everything
needed is present except the device files in /dev/ (Buildroot can’t create
    them because Buildroot doesn’t run as root and doesn’t want to run as
    root). Also, it doesn’t have the correct permissions (e.g. setuid for the
      busybox binary). Therefore, this directory should not be used on your
    target. Instead, you should use one of the images built in the images/
    directory. 
If you need an extracted image of the root filesystem for booting over NFS,
   then use the tarball image generated in images/ and extract it as root.
   Compared to staging/, target/ contains only the files and libraries needed
   to run the selected target applications: the development files (headers,
       etc.) are not present, the binaries are stripped.

host/ 

contains the installation of tools compiled for the host that are needed for
the proper execution of Buildroot, including the cross-compilation toolchain.

<ex>
kyoupark@ukstbuild2:~/asn/broot-latest/buildroot-2016.08.1/output/host/usr$ ll
drwxr-xr-x 2 kyoupark ccusers   4096 Jan 19 09:40 bin/
drwxr-xr-x 2 kyoupark ccusers   4096 Jan 19 09:28 include/
drwxr-xr-x 5 kyoupark ccusers   4096 Jan 19 09:40 lib/
drwxr-xr-x 4 kyoupark ccusers   4096 Jan 19 09:33 libexec/
drwxr-xr-x 6 kyoupark ccusers   4096 Jan 19 09:40 mips-buildroot-linux-gnu/
drwxr-xr-x 9 kyoupark ccusers   4096 Jan 19 09:40 share/

<config>
# Config.in

source "toolchain/Config.in"

# toolchain/Config.in

# Should be selected for glibc or eglibc
config BR2_TOOLCHAIN_USES_GLIBC
	bool
	select BR2_USE_WCHAR
	select BR2_ENABLE_LOCALE
	select BR2_TOOLCHAIN_HAS_THREADS
	select BR2_TOOLCHAIN_HAS_THREADS_DEBUG
	select BR2_TOOLCHAIN_HAS_THREADS_NPTL
	select BR2_TOOLCHAIN_HAS_SHADOW_PASSWORDS
	select BR2_TOOLCHAIN_SUPPORTS_PIE

Once everything is configured, the configuration tool generates a .config file
that contains the entire configuration. This file will be read by the
top-level Makefile.


<makefile>
Chapter 14 How Buildroot works

As mentioned above, Buildroot is basically a `set of Makefiles` that download,
   configure, and compile software with the correct options. It also includes
   patches for various software packages - mainly the ones involved in the
   cross-compilation toolchain (gcc, binutils and uClibc).

There is basically one Makefile per software package, and they are named with
the .mk extension. Makefiles are split into many different parts.

  The toolchain/ directory contains the Makefiles and associated files for all
  software related to the cross-compilation toolchain: binutils, gcc, gdb,
  kernel-headers and uClibc.

  The arch/ directory contains the definitions for all the processor
  architectures that are supported by Buildroot.

  The package/ directory contains the Makefiles and associated files for all
  user-space tools and libraries that Buildroot can compile and add to the
  target root filesystem. There is one sub-directory per package.

  The linux/ directory contains the Makefiles and associated files for the
  Linux kernel.

  The boot/ directory contains the Makefiles and associated files for the
  bootloaders supported by Buildroot.

  The system/ directory contains support for system integration, e.g. the
  target filesystem skeleton and the selection of an init system.

  The fs/ directory contains the Makefiles and associated files for software
  related to the generation of the target root filesystem image.

Each directory contains at least 2 files:
  something.mk is the Makefile that downloads, configures, compiles and
  installs the package something.

  Config.in is a part of the configuration tool description file. It describes
  the options related to the package.

The main Makefile performs the following steps (once the configuration is
    done):

  Create all the output directories: staging, target, build, etc. in the
output directory (output/ by default, another value can be specified using O=)

  Generate the toolchain target. When an internal toolchain is used, this
  means generating the cross-compilation toolchain. When an external toolchain
  is used, this means checking the features of the external toolchain and
  importing it into the Buildroot environment.

  Generate all the targets listed in the TARGETS variable. This variable is
  filled by all the individual components’ Makefiles.  Generating these
  targets will trigger the compilation of the userspace packages (libraries,
      programs), the kernel, the bootloader and the generation of the root
  filesystem images, depending on the configuration.


Chapter 20

Debugging Buildroot

It is possible to instrument the steps Buildroot does when building packages.
Define the variable BR2_INSTRUMENTATIO N_SCRIPTS to contain the path of one or
more scripts (or other executables), in a space-separated list, you want
called `before and after each step` The scripts are called in sequence, with
three parameters:

  start or end to denote the start (resp. the end) of a step;
  the name of the step about to be started, or which just ended;
  the name of the package.

For example :
make BR2_INSTRUMENTATION_SCRIPTS="/path/to/my/script1 /path/to/my/script2"

The list of steps is:
The script has access to the following variables:


<initial>
>>> host-gcc-initial 4.9.4 Configuring
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build

note: where cwd is then?
ln -sf ../configure /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build/configure
(cd /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build && rm -rf config.cache; PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CXXFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib/pkgconfig:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null ./configure --prefix="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr" --sysconfdir="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/etc" --localstatedir="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/var" --enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html --disable-doc --disable-docs --disable-documentation --disable-debug --with-xmlto=no --with-fop=no --disable-dependency-tracking  --target=mips-buildroot-linux-uclibc --with-sysroot=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/sysroot --disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib --with-gmp=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --with-mpfr=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --with-pkgversion="Buildroot 2016.08.1" --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath --disable-libsanitizer --enable-tls --disable-libmudflap --enable-threads --with-mpc=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --without-isl --without-cloog --with-float=soft --disable-decimal-float --with-arch="mips32" --with-abi="32" --enable-languages=c --disable-shared --without-headers --disable-threads --with-newlib --disable-largefile --disable-nls  )

configure: loading site script /dev/null
checking build system type... x86_64-pc-linux-gnu

>>> host-gcc-initial 4.9.4 Building
PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" PKG_CONFIG="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib/pkgconfig"  /usr/bin/make -j25 gcc_cv_libc_provides_ssp=no all-gcc all-target-libgcc -C /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build
make[1]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build'
mkdir -p -- ./libiberty


<final>
for p in   "http://ftpmirror.gnu.org"/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2; do \
		if test ! -e /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/`basename $p` ; then \
			echo ">>> host-gcc-final 4.9.4 Downloading" ; \
			break ; \
		fi ; \
	done
if test -n "" ; then case "" in file) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b cp -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- /gcc-4.9.4.tar.bz2 && exit ;; scp) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b scp -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- '/gcc-4.9.4.tar.bz2' && exit ;; *) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b wget -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- '/gcc-4.9.4.tar.bz2' && exit ;; esac ; fi ; if test "" = "y" ; then exit 1 ; fi ; if test -n ""http://ftpmirror.gnu.org"/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2" ; then case "http" in git) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b git -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2   -- "http://ftpmirror.gnu.org"/gcc/gcc-4.9.4 4.9.4 host-gcc-final-4.9.4 && exit ;; svn) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b svn -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2  -- "http://ftpmirror.gnu.org"/gcc/gcc-4.9.4 4.9.4 host-gcc-final-4.9.4 && exit ;; cvs) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b cvs -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2  -- ftpmirror.gnu.org/gcc/gcc-4.9.4 4.9.4 gcc-final host-gcc-final-4.9.4 && exit ;; bzr) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b bzr -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2  -- "http://ftpmirror.gnu.org"/gcc/gcc-4.9.4 4.9.4 host-gcc-final-4.9.4 && exit ;; file) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b cp -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- ftpmirror.gnu.org/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2 && exit ;; scp) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b scp -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- 'ftpmirror.gnu.org/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2' && exit ;; hg) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b hg -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2  -- "http://ftpmirror.gnu.org"/gcc/gcc-4.9.4 4.9.4 host-gcc-final-4.9.4 && exit ;; *) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b wget -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- 'http://ftpmirror.gnu.org/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2' && exit ;; esac ; fi ; if test -n "http://sources.buildroot.net" ; then 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b wget -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- 'http://sources.buildroot.net/gcc-4.9.4.tar.bz2' && exit ; fi ; exit 1
gcc-4.9.4.tar.bz2: OK (sha512: 93abb78e16277454f41a8e9810f41f66c0fdffdc539a762ff6b67d3037f78db971378683fd2ebf707d1d51c059fad2161fe42d110c330027f40214b7db0f3efe)
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4
touch /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/.stamp_downloaded

>>> host-gcc-final 4.9.4 Extracting
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4
bzcat /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 | tar --strip-components=1 -C /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4 --exclude='libjava/*'  --exclude='libgo/*'  --exclude='gcc/testsuite/*'  --exclude='libstdc++-v3/testsuite/*'   -xf -
chmod -R +rw /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/libstdc++-v3/testsuite/
echo "all:" > /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/libstdc++-v3/testsuite/Makefile.in
echo "install:" >> /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/libstdc++-v3/testsuite/Makefile.in
touch /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/.stamp_extracted

>>> host-gcc-final 4.9.4 Configuring
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build
ln -sf ../configure /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/configure
(cd /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build && rm -rf config.cache; PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CXXFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib/pkgconfig:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " 
./configure --prefix="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr" 
--sysconfdir="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/etc" --enable-static  
--target=mips-buildroot-linux-uclibc 
--with-sysroot=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/sysroot --disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib --with-gmp=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --with-mpfr=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --with-pkgversion="Buildroot 2016.08.1" --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath 
--disable-libsanitizer --enable-tls --disable-libmudflap --enable-threads --with-mpc=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --without-isl --without-cloog 
--with-float=soft --disable-decimal-float 
--with-arch="mips32" --with-abi="32" 
--enable-languages=c --with-build-time-tools=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/bin --enable-shared --disable-libgomp  )
checking build system type... x86_64-pc-linux-gnu

>>> host-gcc-final 4.9.4 Building
PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" PKG_CONFIG="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib/pkgconfig"  /usr/bin/make -j25 gcc_cv_libc_provides_ssp=no -C /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build
make[1]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build'
make[2]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build'


={============================================================================
*kt_linux_gcc_400* gcc-build-buildroot-external-toolchain

{toolchain-selection}
6.1 Cross-compilation toolchain

Buildroot provides two solutions for the cross-compilation toolchain:

  The internal toolchain backend, called Buildroot toolchain in the
  configuration interface.

  The external toolchain backend, called External toolchain in the
  configuration interface.


6.1.1 Internal toolchain backend

The internal toolchain backend is the backend where Buildroot builds by itself
a cross-compilation toolchain, before building the userspace applications and
libraries for your target embedded system.

This backend supports several C libraries: uClibc, glibc and musl.

Once you have selected this backend, a number of options appear. The most
important ones allow to:

<kernel-version>
linux version on runtime >= linux version on toolchain 

Change the version of the Linux kernel headers used to build the toolchain.
This item deserves a few explanations. In the process of building a
cross-compilation toolchain, `the C library is being built` 

This library provides the `interface` between userspace applications and the
Linux kernel. In order to know how to "talk" to the Linux kernel, the C
library needs to have access to the Linux kernel headers (i.e. the .h files
    from the kernel), which define the interface between userspace and the
kernel (system calls, data structures, etc.). 

Since this interface is `backward compatible`, the version of the Linux kernel
headers used to build your toolchain `do not need to match exactly`  the version
of the Linux kernel you intend to run on your embedded system. 

They only need to have a version `equal or older` to the version of the Linux
kernel you intend to run. If you use kernel headers that are more recent than
the Linux kernel you run on your embedded system, then the C library might be
using interfaces that are not provided by your Linux kernel.

It is worth noting that whenever one of those options is modified, then the
entire toolchain and system must be rebuilt. See Section 8.2.

Drawbacks of this backend:

Rebuilding the toolchain is needed when doing make clean, which takes time. If
you're trying to reduce your build time, consider using the External toolchain
backend.


6.1.2 External toolchain backend

The external toolchain backend allows to use existing pre-built
cross-compilation toolchains.

  Use a completely custom external toolchain. This is particularly useful for
  toolchains generated using crosstool-NG or with Buildroot itself. To do
  this, select the Custom toolchain solution in the Toolchain list. You need
  to fill the Toolchain path, Toolchain prefix and External toolchain C
  library options. Then, you have to tell Buildroot what your external
  toolchain supports. 
  
  If your external toolchain uses the glibc library, you only have to tell
  whether your toolchain supports C++ or not and whether it has built-in RPC
  support. 
  
  If your external toolchain uses the uClibc library, then you have to tell
  Buildroot if it supports RPC, wide-char, locale, program invocation, threads
  and C++. 
  
  At the beginning of the execution, Buildroot will tell you if the selected
  options do not match the toolchain configuration.


Advantages of this backend:

  Avoids the build time of the cross-compilation toolchain, which is often
  very significant in the overall build time of an embedded Linux system.

  Not limited to uClibc: glibc and eglibc toolchains are supported.


6.1.2.1 External toolchain wrapper

When using an external toolchain, Buildroot generates a wrapper program, that
transparently passes the appropriate options (according to the configuration)
  to the external toolchain programs. 
  
In case you need to debug this wrapper to check exactly what arguments are
passed, you can set the `environment variable BR2_DEBUG_WRAPPER` to either one
of:

0, empty or not set: no debug
1: trace all arguments on a single line
2: trace one argument per line

// makefile (from `toolchain/toolchain-external/toolchain-external.mk', line 247)
TOOLCHAIN_EXTERNAL_CFLAGS = -march=$(CC_TARGET_ARCH_) -mabi=$(CC_TARGET_ABI_) -EB $(call qstrip,$(BR2_TARGET_OPTIMIZATION)) -msoft-float

BR2_GCC_TARGET_ARCH="mips32"
BR2_GCC_TARGET_ABI="32"

// buildroot-2016.08.1-mips-only/toolchain/toolchain-external/toolchain-external.mk
ifeq ($(call qstrip,$(BR2_GCC_TARGET_CPU_REVISION)),)
CC_TARGET_CPU_ := $(call qstrip,$(BR2_GCC_TARGET_CPU))
else
CC_TARGET_CPU_ := $(call qstrip,$(BR2_GCC_TARGET_CPU)-$(BR2_GCC_TARGET_CPU_REVISION))
endif

CC_TARGET_ARCH_ := $(call qstrip,$(BR2_GCC_TARGET_ARCH))
CC_TARGET_ABI_ := $(call qstrip,$(BR2_GCC_TARGET_ABI))
CC_TARGET_FPU_ := $(call qstrip,$(BR2_GCC_TARGET_FPU))
CC_TARGET_FLOAT_ABI_ := $(call qstrip,$(BR2_GCC_TARGET_FLOAT_ABI))
CC_TARGET_MODE_ := $(call qstrip,$(BR2_GCC_TARGET_MODE))

# march/mtune/floating point mode needs to be passed to the external toolchain
# to select the right multilib variant
ifneq ($(CC_TARGET_ARCH_),)
TOOLCHAIN_EXTERNAL_CFLAGS += -march=$(CC_TARGET_ARCH_)
TOOLCHAIN_EXTERNAL_TOOLCHAIN_WRAPPER_ARGS += -DBR_ARCH='"$(CC_TARGET_ARCH_)"'
endif
ifneq ($(CC_TARGET_CPU_),)
TOOLCHAIN_EXTERNAL_CFLAGS += -mcpu=$(CC_TARGET_CPU_)
TOOLCHAIN_EXTERNAL_TOOLCHAIN_WRAPPER_ARGS += -DBR_CPU='"$(CC_TARGET_CPU_)"'
endif
ifneq ($(CC_TARGET_ABI_),)
TOOLCHAIN_EXTERNAL_CFLAGS += -mabi=$(CC_TARGET_ABI_)
TOOLCHAIN_EXTERNAL_TOOLCHAIN_WRAPPER_ARGS += -DBR_ABI='"$(CC_TARGET_ABI_)"'
endif


{case-with-external-settings}
note: 
1. used external toolchain from <gcc-build-glibc-mips>
2. since spk uses buildroot and is to check if normal buildroot works okay
when use external toolchain setting and to find out glibc libraries to copy.

# Config.in
source "toolchain/Config.in"

# toolchain/Config.in

config BR2_TOOLCHAIN_EXTERNAL
	bool "External toolchain"
	help
	  Select if you want to use an existing cross-compiling
	  toolchain. Buildroot can either download automatically a
	  toolchain, or use an already installed toolchain.

# toolchain/toolchain-external/Config.in

config BR2_TOOLCHAIN_EXTERNAL_PREINSTALLED
	bool "Pre-installed toolchain"
	help
	  Select this option if you want to use a pre-installed toolchain.
	  Specify the path to this toolchain in BR2_TOOLCHAIN_EXTERNAL_PATH.

endchoice

# .config
#
# Toolchain
#
BR2_TOOLCHAIN=y
BR2_TOOLCHAIN_USES_GLIBC=y
BR2_TOOLCHAIN_EXTERNAL=y

#
# Toolchain External Options
#
BR2_TOOLCHAIN_EXTERNAL_CUSTOM=y
BR2_TOOLCHAIN_EXTERNAL_PREINSTALLED=y
BR2_TOOLCHAIN_EXTERNAL_PATH="/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install"
BR2_TOOLCHAIN_EXTERNAL_CUSTOM_PREFIX="mips-unknown-linux"
BR2_TOOLCHAIN_EXTERNAL_PREFIX="mips-unknown-linux"
BR2_TOOLCHAIN_EXTERNAL_GLIBC=y
BR2_TOOLCHAIN_EXTERNAL_GCC_4_8=y
BR2_TOOLCHAIN_EXTERNAL_HEADERS_REALLY_OLD=y
BR2_TOOLCHAIN_EXTERNAL_CUSTOM_GLIBC=y
BR2_TOOLCHAIN_EXTERNAL_HAS_SSP=y
BR2_TOOLCHAIN_EXTERNAL_INET_RPC=y
BR2_TOOLCHAIN_EXTERNAL_CXX=y
BR2_TOOLCHAIN_EXTRA_EXTERNAL_LIBS=""

#
# Toolchain Generic Options
#
BR2_TOOLCHAIN_HAS_NATIVE_RPC=y
BR2_USE_WCHAR=y
BR2_ENABLE_LOCALE=y
BR2_INSTALL_LIBSTDCPP=y
BR2_TOOLCHAIN_HAS_THREADS=y
BR2_TOOLCHAIN_HAS_THREADS_DEBUG=y
BR2_TOOLCHAIN_HAS_THREADS_NPTL=y
BR2_TOOLCHAIN_HAS_SHADOW_PASSWORDS=y
BR2_TOOLCHAIN_HAS_SSP=y
BR2_TOOLCHAIN_SUPPORTS_PIE=y
BR2_USE_MMU=y
BR2_TARGET_OPTIMIZATION=""
BR2_TARGET_LDFLAGS=""
BR2_TOOLCHAIN_HEADERS_AT_LEAST="2.6"
BR2_TOOLCHAIN_GCC_AT_LEAST_4_3=y
BR2_TOOLCHAIN_GCC_AT_LEAST_4_4=y
BR2_TOOLCHAIN_GCC_AT_LEAST_4_5=y
BR2_TOOLCHAIN_GCC_AT_LEAST_4_6=y
BR2_TOOLCHAIN_GCC_AT_LEAST_4_7=y
BR2_TOOLCHAIN_GCC_AT_LEAST_4_8=y
BR2_TOOLCHAIN_GCC_AT_LEAST="4.8"
BR2_TOOLCHAIN_HAS_SYNC_1=y
BR2_TOOLCHAIN_HAS_SYNC_2=y
BR2_TOOLCHAIN_HAS_SYNC_4=y
BR2_TOOLCHAIN_HAS_LIBATOMIC=y
BR2_TOOLCHAIN_HAS_ATOMIC=y

config BR2_TOOLCHAIN_EXTERNAL_CUSTOM_GLIBC
	bool "glibc/eglibc"
	depends on !BR2_STATIC_LIBS
	select BR2_TOOLCHAIN_EXTERNAL_GLIBC
	help
	  Select this option if your external toolchain uses the GNU C
	  library (available from https://www.gnu.org/software/libc/)
	  or its variant the eglibc library (http://www.eglibc.org/).

	  Note: eglibc is a variant of glibc that (among other things)
	  can be configured to exclude some of its features. Using a
	  toolchain with eglibc configured to exclude key features may
	  cause build failures to some packages.

comment "glibc only available with shared lib support"
	depends on BR2_STATIC_LIBS


<makefile>
Makefile

include toolchain/*.mk
include toolchain/*/*.mk

toolchain/helpers.mk
                     
toolchain/toolchain-external.mk

170:TOOLCHAIN_EXTERNAL_CROSS = $(TOOLCHAIN_EXTERNAL_BIN)/$(TOOLCHAIN_EXTERNAL_PREFIX)-
171:TOOLCHAIN_EXTERNAL_CC = $(TOOLCHAIN_EXTERNAL_CROSS)gcc$(TOOLCHAIN_EXTERNAL_SUFFIX)

# TOOLCHAIN_EXTERNAL_CC = /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc 


<steps-for-package>
# do not run for this package
# >>> toolchain-external  Extracting
# >>> toolchain-external  Patching
>>> toolchain-external  Configuring
# >>> toolchain-external  Building
>>> toolchain-external  Installing to staging directory
>>> toolchain-external  Copying external toolchain sysroot to staging...
>>> toolchain-external  Fixing libtool files
>>> toolchain-external  Installing to target

# toolchain/toolchain-external/toolchain-external.mk
# ifeq ($(BR2_TOOLCHAIN_EXTERNAL_GLIBC)$(BR2_TOOLCHAIN_EXTERNAL_UCLIBC),y)
# TOOLCHAIN_EXTERNAL_LIBS += libatomic.so.* libc.so.* libcrypt.so.* libdl.so.* libgcc_s.so.* libm.so.* libnsl.so.* libresolv.so.* librt.so.* libutil.so.*
#
# ifeq ($(BR2_STATIC_LIBS),)
# define TOOLCHAIN_EXTERNAL_INSTALL_TARGET_LIBS
# 	$(Q)$(call MESSAGE,"Copying external toolchain libraries to target...")
# 	$(Q)for libs in $(TOOLCHAIN_EXTERNAL_LIBS); do \
# 		echo "KT: TOOLCHAIN_EXTERNAL_LIBS: ($(TOOLCHAIN_EXTERNAL_LIBS)"; \
# 		$(call copy_toolchain_lib_root,$$libs); \
# 	done
# endef
# endif


<libs-for-glibc>
KT: TOOLCHAIN_EXTERNAL_LIBS: (libatomic.so.* libc.so.* libcrypt.so.* libdl.so.* libgcc_s.so.* libm.so.* libnsl.so.* libresolv.so.* librt.so.* libutil.so.* ld*.so.* libpthread.so.* libnss_files.so.* libnss_dn
s.so.* libmvec.so.* libstdc++.so.*

>>> toolchain  Extracting
>>> toolchain  Patching


# buildroot-2016.11/package/pkg-generic.mk
## Configure
#$(BUILD_DIR)/%/.stamp_configured:
#	@$(call step_start,configure)
#	@$(call MESSAGE,"Configuring")
#	$(foreach hook,$($(PKG)_PRE_CONFIGURE_HOOKS),$(call $(hook))$(sep))
#	$($(PKG)_CONFIGURE_CMDS)
#	$(foreach hook,$($(PKG)_POST_CONFIGURE_HOOKS),$(call $(hook))$(sep))
#	@$(call step_end,configure)
#	$(Q)touch $@
#
# >>> toolchain-external  Configuring
# __CROSS_CC=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc ; ${__CROSS_CC} -v > /dev/null 2>&1 ; if test $? -ne 0 ; then echo "Cannot execute cross-compiler '${__CROSS_CC}'" ; exit 1 ; fi
# __CROSS_CC=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc ; vendor=`${__CROSS_CC} -dumpmachine | cut -f2 -d'-'` ; if test "${vendor}" = "angstrom" ; then echo "Angstrom toolchains are not pure toolchains: they contain" ; echo "many other libraries than just the C library, which makes" ; echo "them unsuitable as external toolchains for build systems" ; echo "such as Buildroot." ; exit 1 ; fi; with_sysroot=`${__CROSS_CC} -v 2>&1 |sed -r -e '/.* --with-sysroot=([^[:space:]]+)[[:space:]].*/!d; s//\1/'`; if test "${with_sysroot}"  = "/" ; then echo "Distribution toolchains are unsuitable for use by Buildroot," ; echo "as they were configured in a way that makes them non-relocatable,"; echo "and contain a lot of pre-built libraries that would conflict with"; echo "the ones Buildroot wants to build."; exit 1; fi; libc_a_path=`${__CROSS_CC} -print-file-name=libc.a` ; if test "${libc_a_path}" = "libc.a" ; then echo "Unable to detect the toolchain sysroot, Buildroot cannot use this toolchain." ; exit 1 ; fi ; sysroot_dir="$(printf $(readlink -f $(LANG=C ${__CROSS_CC} -print-file-name=libc.a)) | sed -r -e 's:(usr/)?lib(32|64)?([^/]*)?/([^/]*/)?libc\.a::')" ; echo "KT: sysroot_dir: ${sysroot_dir}"; if test -z "${sysroot_dir}" ; then echo "External toolchain doesn't support --sysroot. Cannot use." ; exit 1 ; fi
# KT: sysroot_dir: /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/
# SYSROOT_DIR="$(printf $(readlink -f $(LANG=C /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc -print-file-name=libc.a)) | sed -r -e 's:(usr/)?lib(32|64)?([^/]*)?/([^/]*/)?libc\.a::')" ; #	echo "KT :  #		$(printf $(readlink -f $(LANG=C /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc -print-file-name=libc.a)) | sed -r -e 's:(usr/)?lib(32|64)?([^/]*)?/([^/]*/)?libc\.a::') :  #		2.6"; if ! support/scripts/check-kernel-headers.sh  #		$(printf $(readlink -f $(LANG=C /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc -print-file-name=libc.a)) | sed -r -e 's:(usr/)?lib(32|64)?([^/]*)?/([^/]*/)?libc\.a::')  #		2.6; then exit 1; fi; expected_version="4.8" ; if [ -z "${expected_version}" ]; then printf "Internal error, gcc version unknown (no GCC_AT_LEAST_X_Y selected)\n"; exit 1 ; fi; real_version=`/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc --version | sed -r -e '1!d; s/^[^)]+\) ([^[:space:]]+).*/\1/;'` ; if [[ ! "${real_version}" =~ ^${expected_version}\. ]] ; then printf "Incorrect selection of gcc version: expected %s.x, got %s\n" "${expected_version}" "${real_version}" ; exit 1 ; fi; if test "" = "y" ; then __CROSS_CC="/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc -march=mips32 -mabi=32 -EB -msoft-float" ; __CROSS_READELF=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-readelf ; EXT_TOOLCHAIN_TARGET=`LANG=C ${__CROSS_CC} -v 2>&1 | grep ^Target | cut -f2 -d ' '` ; if ! echo ${EXT_TOOLCHAIN_TARGET} | grep -qE 'eabi(hf)?$' ; then echo "External toolchain uses the unsuported OABI" ; exit 1 ; fi ; if ! echo 'int main(void) {}' | ${__CROSS_CC} -x c -o /home/kyoupark/buildroot-2016.11/output/build/.br-toolchain-test.tmp - ; then rm -f /home/kyoupark/buildroot-2016.11/output/build/.br-toolchain-test.tmp*; abistr_='EABI'; abistr_='EABIhf'; echo "Incorrect ABI setting: ${abistr_y} selected, but toolchain is incompatible"; exit 1 ; fi ; rm -f /home/kyoupark/buildroot-2016.11/output/build/.br-toolchain-test.tmp* ; fi ; if test "y" = "y" ; then __CROSS_CXX=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-g++ ; ${__CROSS_CXX} -v > /dev/null 2>&1 ; if test $? -ne 0 ; then echo "C++ support is selected but is not available in external toolchain" ; exit 1 ; fi ; fi ; if test "" = "y" ; then __CROSS_FC=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gfortran ; __o=/home/kyoupark/buildroot-2016.11/output/build/.br-toolchain-test-fortran.tmp ; printf 'program hello\n\tprint *, "Hello Fortran!\\n"\nend program hello\n' | ${__CROSS_FC} -x f95 -o ${__o} - ; if test $? -ne 0 ; then rm -f ${__o}* ; echo "Fortran support is selected but is not available in external toolchain" ; exit 1 ; fi ; rm -f ${__o}*  ; fi ; if test "" = "y" ; then SYSROOT_DIR="${SYSROOT_DIR}"; if ! test -f ${SYSROOT_DIR}/usr/include/bits/uClibc_config.h ; then echo "Incorrect selection of the C library"; exit -1; fi; UCLIBC_CONFIG_FILE=${SYSROOT_DIR}/usr/include/bits/uClibc_config.h ; IS_IN_LIBC=`grep -q "#define __ARCH_USE_MMU__ 1" ${UCLIBC_CONFIG_FILE} && echo y` ; if [ -z "BR2_USE_MMU" ] ; then if [ "${IS_IN_LIBC}" != "y" ] ; then echo "MMU support not available in C library, toolchain unsuitable for Buildroot" ; exit 1 ; fi ; else if [ "y" != "y" -a "${IS_IN_LIBC}" = "y" ] ; then echo "MMU support available in C library, please enable BR2_USE_MMU" ; exit 1 ; fi ; if [ "y" = "y" -a "${IS_IN_LIBC}" != "y" ] ; then echo "MMU support not available in C library, please disable BR2_USE_MMU" ; exit 1 ; fi ; fi ; IS_IN_LIBC=`grep -q "#define __UCLIBC_HAS_LFS__ 1" ${UCLIBC_CONFIG_FILE} && echo y` ; if [ -z "" ] ; then if [ "${IS_IN_LIBC}" != "y" ] ; then echo "Large file support not available in C library, toolchain unsuitable for Buildroot" ; exit 1 ; fi ; else if [ "" != "y" -a "${IS_IN_LIBC}" = "y" ] ; then echo "Large file support available in C library, please enable " ; exit 1 ; fi ; if [ "" = "y" -a "${IS_IN_LIBC}" != "y" ] ; then echo "Large file support not available in C library, please disable " ; exit 1 ; fi ; fi ; IS_IN_LIBC=`grep -q "#define __UCLIBC_HAS_IPV6__ 1" ${UCLIBC_CONFIG_FILE} && echo y` ; if [ -z "" ] ; then if [ "${IS_IN_LIBC}" != "y" ] ; then echo "IPv6 support not available in C library, toolchain unsuitable for Buildroot" ; exit 1 ; fi ; else if [ "" != "y" -a "${IS_IN_LIBC}" = "y" ] ; then echo "IPv6 support available in C library, please enable " ; exit 1 ; fi ; if [ "" = "y" -a "${IS_IN_LIBC}" != "y" ] ; then echo "IPv6 support not available in C library, please disable " ; exit 1 ; fi ; fi ; IS_IN_LIBC=`grep -q "#define __UCLIBC_HAS_RPC__ 1" ${UCLIBC_CONFIG_FILE} && echo y` ; if [ -z "BR2_TOOLCHAIN_HAS_NATIVE_RPC" ] ; then if [ "${IS_IN_LIBC}" != "y" ] ; then echo "RPC support not available in C library, toolchain unsuitable for Buildroot" ; exit 1 ; fi ; else if [ "y" != "y" -a "${IS_IN_LIBC}" = "y" ] ; then echo "RPC support available in C library, please enable BR2_TOOLCHAIN_HAS_NATIVE_RPC" ; exit 1 ; fi ; if [ "y" = "y" -a "${IS_IN_LIBC}" != "y" ] ; then echo "RPC support not available in C library, please disable BR2_TOOLCHAIN_HAS_NATIVE_RPC" ; exit 1 ; fi ; fi ; IS_IN_LIBC=`grep -q "#define __UCLIBC_HAS_LOCALE__ 1" ${UCLIBC_CONFIG_FILE} && echo y` ; if [ -z "BR2_ENABLE_LOCALE" ] ; then if [ "${IS_IN_LIBC}" != "y" ] ; then echo "Locale support not available in C library, toolchain unsuitable for Buildroot" ; exit 1 ; fi ; else if [ "y" != "y" -a "${IS_IN_LIBC}" = "y" ] ; then echo "Locale support available in C library, please enable BR2_ENABLE_LOCALE" ; exit 1 ; fi ; if [ "y" = "y" -a "${IS_IN_LIBC}" != "y" ] ; then echo "Locale support not available in C library, please disable BR2_ENABLE_LOCALE" ; exit 1 ; fi ; fi ; IS_IN_LIBC=`grep -q "#define __UCLIBC_HAS_WCHAR__ 1" ${UCLIBC_CONFIG_FILE} && echo y` ; if [ -z "BR2_USE_WCHAR" ] ; then if [ "${IS_IN_LIBC}" != "y" ] ; then echo "Wide char support not available in C library, toolchain unsuitable for Buildroot" ; exit 1 ; fi ; else if [ "y" != "y" -a "${IS_IN_LIBC}" = "y" ] ; then echo "Wide char support available in C library, please enable BR2_USE_WCHAR" ; exit 1 ; fi ; if [ "y" = "y" -a "${IS_IN_LIBC}" != "y" ] ; then echo "Wide char support not available in C library, please disable BR2_USE_WCHAR" ; exit 1 ; fi ; fi ; IS_IN_LIBC=`grep -q "#define __UCLIBC_HAS_THREADS__ 1" ${UCLIBC_CONFIG_FILE} && echo y` ; if [ -z "BR2_TOOLCHAIN_HAS_THREADS" ] ; then if [ "${IS_IN_LIBC}" != "y" ] ; then echo "Thread support not available in C library, toolchain unsuitable for Buildroot" ; exit 1 ; fi ; else if [ "y" != "y" -a "${IS_IN_LIBC}" = "y" ] ; then echo "Thread support available in C library, please enable BR2_TOOLCHAIN_HAS_THREADS" ; exit 1 ; fi ; if [ "y" = "y" -a "${IS_IN_LIBC}" != "y" ] ; then echo "Thread support not available in C library, please disable BR2_TOOLCHAIN_HAS_THREADS" ; exit 1 ; fi ; fi ; IS_IN_LIBC=`grep -q "#define __PTHREADS_DEBUG_SUPPORT__ 1" ${UCLIBC_CONFIG_FILE} && echo y` ; if [ -z "BR2_TOOLCHAIN_HAS_THREADS_DEBUG" ] ; then if [ "${IS_IN_LIBC}" != "y" ] ; then echo "Thread debugging support not available in C library, toolchain unsuitable for Buildroot" ; exit 1 ; fi ; else if [ "y" != "y" -a "${IS_IN_LIBC}" = "y" ] ; then echo "Thread debugging support available in C library, please enable BR2_TOOLCHAIN_HAS_THREADS_DEBUG" ; exit 1 ; fi ; if [ "y" = "y" -a "${IS_IN_LIBC}" != "y" ] ; then echo "Thread debugging support not available in C library, please disable BR2_TOOLCHAIN_HAS_THREADS_DEBUG" ; exit 1 ; fi ; fi ; IS_IN_LIBC=`grep -q "#define __UCLIBC_HAS_THREADS_NATIVE__ 1" ${UCLIBC_CONFIG_FILE} && echo y` ; if [ -z "BR2_TOOLCHAIN_HAS_THREADS_NPTL" ] ; then if [ "${IS_IN_LIBC}" != "y" ] ; then echo "NPTL thread support not available in C library, toolchain unsuitable for Buildroot" ; exit 1 ; fi ; else if [ "y" != "y" -a "${IS_IN_LIBC}" = "y" ] ; then echo "NPTL thread support available in C library, please enable BR2_TOOLCHAIN_HAS_THREADS_NPTL" ; exit 1 ; fi ; if [ "y" = "y" -a "${IS_IN_LIBC}" != "y" ] ; then echo "NPTL thread support not available in C library, please disable BR2_TOOLCHAIN_HAS_THREADS_NPTL" ; exit 1 ; fi ; fi ; elif test "" = "y" ; then SYSROOT_DIR="${SYSROOT_DIR}"; if test ! -f ${SYSROOT_DIR}/lib/libc.so -o -e ${SYSROOT_DIR}/lib/libm.so ; then echo "Incorrect selection of the C library" ; exit -1; fi ; else SYSROOT_DIR="${SYSROOT_DIR}"; if test `find ${SYSROOT_DIR}/ -maxdepth 2 -name 'ld-linux*.so.*' -o -name 'ld.so.*' -o -name 'ld64.so.*' | wc -l` -eq 0 ; then echo "Incorrect selection of the C library"; exit -1; fi; if [ "y" != "y" ] ; then echo "MMU support available in C library, please enable BR2_USE_MMU" ; exit 1 ; fi ; IS_IN_LIBC=`test -f ${SYSROOT_DIR}/usr/include/rpc/rpc.h && echo y` ; if [ "y" != "y" -a "${IS_IN_LIBC}" = "y" ] ; then echo "RPC support available in C library, please enable BR2_TOOLCHAIN_EXTERNAL_INET_RPC" ; exit 1 ; fi ; if [ "y" = "y" -a "${IS_IN_LIBC}" != "y" ] ; then echo "RPC support not available in C library, please disable BR2_TOOLCHAIN_EXTERNAL_INET_RPC" ; exit 1 ; fi ; fi
# __CROSS_CC=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc ; __HAS_SSP=`echo 'void main(){}' | ${__CROSS_CC} -fstack-protector -x c - -o /home/kyoupark/buildroot-2016.11/output/build/.br-toolchain-test.tmp >/dev/null 2>&1 && echo y` ; if [ "y" != "y" -a "${__HAS_SSP}" = "y" ] ; then echo "SSP support available in this toolchain, please enable BR2_TOOLCHAIN_EXTERNAL_HAS_SSP" ; exit 1 ; fi ; if [ "y" = "y" -a "${__HAS_SSP}" != "y" ] ; then echo "SSP support not available in this toolchain, please disable BR2_TOOLCHAIN_EXTERNAL_HAS_SSP" ; exit 1 ; fi ; rm -f /home/kyoupark/buildroot-2016.11/output/build/.br-toolchain-test.tmp*
# touch /home/kyoupark/buildroot-2016.11/output/build/toolchain-external/.stamp_configured

# Checks for an already installed toolchain: check the toolchain
# location, check that it is usable, and then verify that it
# matches the configuration provided in Buildroot: ABI, C++ support,
# kernel headers version, type of C library and all C library features.
define TOOLCHAIN_EXTERNAL_CONFIGURE_CMDS
	$(Q)$(call check_cross_compiler_exists,$(TOOLCHAIN_EXTERNAL_CC))
	$(Q)$(call check_unusable_toolchain,$(TOOLCHAIN_EXTERNAL_CC))
	$(Q)SYSROOT_DIR="$(call toolchain_find_sysroot,$(TOOLCHAIN_EXTERNAL_CC))" ; \
	$(call check_kernel_headers_version,\
		$(call toolchain_find_sysroot,$(TOOLCHAIN_EXTERNAL_CC)),\
		$(call qstrip,$(BR2_TOOLCHAIN_HEADERS_AT_LEAST))); \
	$(call check_gcc_version,$(TOOLCHAIN_EXTERNAL_CC),\
		$(call qstrip,$(BR2_TOOLCHAIN_GCC_AT_LEAST))); \
	if test "$(BR2_arm)" = "y" ; then \
		$(call check_arm_abi,\
			"$(TOOLCHAIN_EXTERNAL_CC) $(TOOLCHAIN_EXTERNAL_CFLAGS)",\
			$(TOOLCHAIN_EXTERNAL_READELF)) ; \
	fi ; \
	if test "$(BR2_INSTALL_LIBSTDCPP)" = "y" ; then \
		$(call check_cplusplus,$(TOOLCHAIN_EXTERNAL_CXX)) ; \
	fi ; \
	if test "$(BR2_TOOLCHAIN_HAS_FORTRAN)" = "y" ; then \
		$(call check_fortran,$(TOOLCHAIN_EXTERNAL_FC)) ; \
	fi ; \
	if test "$(BR2_TOOLCHAIN_EXTERNAL_UCLIBC)" = "y" ; then \
		$(call check_uclibc,$${SYSROOT_DIR}) ; \
	elif test "$(BR2_TOOLCHAIN_EXTERNAL_MUSL)" = "y" ; then \
		$(call check_musl,$${SYSROOT_DIR}) ; \
	else \
		$(call check_glibc,$${SYSROOT_DIR}) ; \
	fi
	$(Q)$(call check_toolchain_ssp,$(TOOLCHAIN_EXTERNAL_CC))
endef


kyoupark@kit-debian:~/buildroot-2016.11$ make V=1
>>> toolchain-external  Configuring
__CROSS_CC=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc ; ${__CROSS_CC} -v > /dev/null 2>&1 ; 
...
cc1: fatal error: /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux//usr/include/linux/version.h: No such file or directory
compilation terminated.
make: *** [/home/kyoupark/buildroot-2016.11/output/build/toolchain-external/.stamp_configured] Error 1


	sysroot_dir="$(call toolchain_find_sysroot,$${__CROSS_CC})" ; \
	if test -z "$${sysroot_dir}" ; then \
		echo "External toolchain doesn't support --sysroot. Cannot use." ; \
		exit 1 ; \
	fi

  $(Q)$(call check_unusable_toolchain,$(TOOLCHAIN_EXTERNAL_CC))


#
# Check for toolchains known not to work with Buildroot.
# - For the Angstrom toolchains, we check by looking at the vendor part of
#   the host tuple.
# - Exclude distro-class toolchains which are not relocatable.
# - Exclude broken toolchains which return "libc.a" with -print-file-name.
#
# note: - Exclude toolchains which doesn't support --sysroot option.
#
# $1: cross-gcc path
#
check_unusable_toolchain = \
	sysroot_dir="$(call toolchain_find_sysroot,$${__CROSS_CC})" ; \
	echo "KT: sysroot_dir: $${sysroot_dir}"; \
	if test -z "$${sysroot_dir}" ; then \
		echo "External toolchain doesn't support --sysroot. Cannot use." ; \
		exit 1 ; \
	fi

define toolchain_find_sysroot
$$(printf $(call `toolchain_find_libc_a`,$(1)) | sed -r -e 's:(usr/)?lib(32|64)?([^/]*)?/([^/]*/)?libc\.a::')
endef

# Returns the location of the libc.a file for the given compiler + flags
define toolchain_find_libc_a
$$(readlink -f $$(LANG=C $(1) -print-file-name=libc.a))
endef

    # translated to:
    # sysroot_dir="$(printf echo "$(readlink -f $(LANG=C ${__CROSS_CC} -print-file-name=libc.a))"

    # KT: sysroot_dir: /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/
    # LANG=C /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc print-file-name=libc.a
    #
    # $ LANG=C /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc --print-file-name=libc.a
    # /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/lib/gcc/mips-unknown-linux/4.8.2/../../../../mips-unknown-linux/lib/libc.a
    #
    # ~/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/lib$ ls -al libc.a
    # -rw-r--r-- 1 kyoupark ccusers 16059902 Jan 14 22:47 libc.a


<glibc-external-libs>
>>> toolchain-external  Installing to target
>>> toolchain-external  Copying external toolchain libraries to target..."
KT: TOOLCHAIN_EXTERNAL_LIBS: 
libatomic.so.* libc.so.* libcrypt.so.* libdl.so.* libgcc_s.so.* libm.so.*
libnsl.so.* libresolv.so.* librt.so.* libutil.so.* ld*.so.* libpthread.so.*
libnss_files.so.* libnss_dns.so.* libmvec.so.* libstdc++.so.*

-rwxr-xr-x  1 kyoupark ccusers  747596 Jan 19 17:14 ld-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      10 Jan 19 17:14 ld.so.1 -> ld-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      18 Jan 19 17:14 libatomic.so.1 -> libatomic.so.1.0.0*
-rwxr-xr-x  1 kyoupark ccusers   84308 Jan 19 17:14 libatomic.so.1.0.0*
-rwxr-xr-x  1 kyoupark ccusers 9163960 Jan 19 17:14 libc-2.20.so*
-rwxr-xr-x  1 kyoupark ccusers  126036 Jan 19 17:14 libcrypt-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      16 Jan 19 17:14 libcrypt.so.1 -> libcrypt-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      12 Jan 19 17:14 libc.so.6 -> libc-2.20.so*
-rwxr-xr-x  1 kyoupark ccusers   95868 Jan 19 17:14 libdl-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      13 Jan 19 17:14 libdl.so.2 -> libdl-2.20.so*
-rwxr-xr-x  1 kyoupark ccusers 1522964 Jan 19 17:14 libgcc_s.so.1*
-rwxr-xr-x  1 kyoupark ccusers 1406292 Jan 19 17:14 libm-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      12 Jan 19 17:14 libm.so.6 -> libm-2.20.so*
-rwxr-xr-x  1 kyoupark ccusers  497208 Jan 19 17:14 libnsl-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      14 Jan 19 17:14 libnsl.so.1 -> libnsl-2.20.so*
-rwxr-xr-x  1 kyoupark ccusers   80844 Jan 19 17:14 libnss_dns-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      18 Jan 19 17:14 libnss_dns.so.2 -> libnss_dns-2.20.so*
-rwxr-xr-x  1 kyoupark ccusers  228156 Jan 19 17:14 libnss_files-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      20 Jan 19 17:14 libnss_files.so.2 -> libnss_files-2.20.so*
-rwxr-xr-x  1 kyoupark ccusers  869968 Jan 19 17:14 libpthread-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      18 Jan 19 17:14 libpthread.so.0 -> libpthread-2.20.so*
-rwxr-xr-x  1 kyoupark ccusers  316352 Jan 19 17:14 libresolv-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      17 Jan 19 17:14 libresolv.so.2 -> libresolv-2.20.so*
-rwxr-xr-x  1 kyoupark ccusers  167768 Jan 19 17:14 librt-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      13 Jan 19 17:14 librt.so.1 -> librt-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      19 Jan 19 17:14 libstdc++.so.6 -> libstdc++.so.6.0.18*
-rwxr-xr-x  1 kyoupark ccusers 5560176 Jan 19 17:14 libstdc++.so.6.0.18*
-rwxr-xr-x  1 kyoupark ccusers    2405 Jan 19 17:14 libstdc++.so.6.0.18-gdb.py*
-rwxr-xr-x  1 kyoupark ccusers   25964 Jan 19 17:14 libutil-2.20.so*
lrwxrwxrwx  1 kyoupark ccusers      15 Jan 19 17:14 libutil.so.1 -> libutil-2.20.so*


<check-glibc>
SYSROOT_DIR=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/
find /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/ -maxdepth 2 -name 'ld-linux*.so.*' -o -name 'ld.so.*' -o -name 'ld64.so.*'
/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/lib/ld.so.1

<error>
toolchain-external/Config.in

choice
	bool "External toolchain kernel headers series"
	default BR2_TOOLCHAIN_EXTERNAL_HEADERS_REALLY_OLD
	help
	  Set to the kernel headers version that were used to build
	  this external toolchain.

	  This is used to hide/show some packages that have strict
	  requirements on the version of kernel headers.

	  If unsure what version your toolchain is using, you can look
	  at the value of LINUX_VERSION_CODE in linux/version.h in your
	  toolchain. The Linux version is M.m.p, with:
	    M = ( LINUX_VERSION_CODE >> 16 ) & 0xFF
	    m = ( LINUX_VERSION_CODE >> 8  ) & 0xFF
	    p = ( LINUX_VERSION_CODE >> 0  ) & 0xFF
	$(call check_kernel_headers_version,\
		$(call toolchain_find_sysroot,$(TOOLCHAIN_EXTERNAL_CC)),\
		$(call qstrip,$(BR2_TOOLCHAIN_HEADERS_AT_LEAST))); \


config BR2_TOOLCHAIN_EXTERNAL_HEADERS_REALLY_OLD
	bool "2.6.x"

endchoice

toolchain-common.in

# This order guarantees that the highest version is set, as kconfig
# stops affecting a value on the first matching default.
config BR2_TOOLCHAIN_HEADERS_AT_LEAST
   ...
	default "3.0"  if BR2_TOOLCHAIN_HEADERS_AT_LEAST_3_0
	default "2.6"

This ends to set BR2_TOOLCHAIN_HEADERS_AT_LEAST "2.6". WHY do this on HOSTCC?

# Check the specified kernel headers version actually matches the
# version in the toolchain.
#
# $1: sysroot directory
# $2: kernel version string, in the form: X.Y
#
check_kernel_headers_version = \
	if ! support/scripts/check-kernel-headers.sh $(1) $(2); then \
		exit 1; \
	fi

# $1: /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/ 
# $2: 2.6


# support/scripts/check-kernel-headers.sh
#!/bin/sh

SYSROOT="${1}"
# Make sure we have enough version components
HDR_VER="${2}.0.0"

HDR_M="${HDR_VER%%.*}"
HDR_V="${HDR_VER#*.}"
HDR_m="${HDR_V%%.*}"

EXEC="$(mktemp -t check-headers.XXXXXX)"

# We do not want to account for the patch-level, since headers are
# not supposed to change for different patchlevels, so we mask it out.
# This only applies to kernels >= 3.0, but those are the only one
# we actually care about; we treat all 2.6.x kernels equally.
${HOSTCC} -imacros "${SYSROOT}/usr/include/linux/version.h" \
          -x c -o "${EXEC}" - <<_EOF_
#include <stdio.h>
#include <stdlib.h>

int main(int argc __attribute__((unused)),
         char** argv __attribute__((unused)))
{
    if((LINUX_VERSION_CODE & ~0xFF)
        != KERNEL_VERSION(${HDR_M},${HDR_m},0))
    {
        printf("Incorrect selection of kernel headers: ");
        printf("expected %d.%d.x, got %d.%d.x\n", ${HDR_M}, ${HDR_m},
               ((LINUX_VERSION_CODE>>16) & 0xFF),
               ((LINUX_VERSION_CODE>>8) & 0xFF));
        return 1;
    }
    return 0;
}
_EOF_

"${EXEC}"
ret=${?}
rm -f "${EXEC}"
exit ${ret}


<float-hard-soft-error>
/usr/bin/make -f scripts/Makefile.build obj=scripts
  gcc -Wp,-MD,applets/.applet_tables.d  -Wall -Wstrict-prototypes -O2 -fomit-frame-pointer       -o applets/applet_tables applets/applet_tables.c
  applets/applet_tables include/applet_tables.h include/NUM_APPLETS.h
  applets/usage_compressed include/usage_compressed.h applets
  gcc -Wp,-MD,applets/.usage_pod.d  -Wall -Wstrict-prototypes -O2 -fomit-frame-pointer      -Iinclude -Iinclude -o applets/usage_pod applets/usage_pod.c
  /home/kyoupark/buildroot-2016.11/output/host/usr/bin/mips-unknown-linux-gcc -Wp,-MD,applets/.applets.o.d   
  -std=gnu99 -Iinclude -Ilibbb  -include include/autoconf.h 
  -D_GNU_SOURCE -DNDEBUG -D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE 
  -D_FILE_OFFSET_BITS=64 -D"BB_VER=KBUILD_STR(1.25.1)" 
  -DBB_BT=AUTOCONF_TIMESTAMP -D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE 
  -D_FILE_OFFSET_BITS=64  -Os  -Wall -Wshadow -Wwrite-strings 
  -Wundef -Wstrict-prototypes -Wunused -Wunused-parameter -Wunused-function 
  -Wunused-value -Wmissing-prototypes -Wmissing-declarations -Wno-format-security 
  -Wdeclaration-after-statement -Wold-style-definition -fno-builtin-strlen 
  -finline-limit=0 -fomit-frame-pointer -ffunction-sections 
  -fdata-sections -fno-guess-branch-probability -funsigned-char 
  -static-libgcc -falign-functions=1 -falign-jumps=1 -falign-labels=1 
  -falign-loops=1 -fno-unwind-tables -fno-asynchronous-unwind-tables 
  -fno-builtin-printf -Os     -D"KBUILD_STR(s)=#s" -D"KBUILD_BASENAME=KBUILD_STR(applets)"  -D"KBUILD_MODNAME=KBUILD_STR(applets)" -c -o applets/applets.o applets/applets.c
In file included from /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/include/features.h:389:0,
                 from /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/include/byteswap.h:21,
                 from include/platform.h:152,
                 from include/libbb.h:13,
                 from include/busybox.h:8,
                 from applets/applets.c:9:
/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/include/gnu/stubs.h:8:33: fatal error: gnu/stubs-o32_soft.h: No such file or directory
 # include <gnu/stubs-o32_soft.h>
                                 ^
compilation terminated.

/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/include/gnu/stubs.h
#include <sgidefs.h>

#if !defined(__mips_nan2008) && defined(__mips_soft_float) && (_MIPS_SIM == _MIPS_SIM_ABI32)
# include <gnu/stubs-o32_soft.h>
#endif

/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc -dM -E -x c /dev/null | ag mips
#define _MIPS_ISA _MIPS_ISA_MIPS1
#define _MIPS_TUNE "mips1"
#define __mips_abicalls 1
#define __mips_fpr 32
#define __mips__ 1
#define MIPSEB 1
#define mips 1
#define _MIPS_ARCH_MIPS1 1
#define __MIPSEB__ 1
#define _MIPSEB 1
#define _MIPS_ARCH "mips1"
#define _MIPS_TUNE_MIPS1 1
#define _MIPS_SZPTR 32
#define _mips 1
#define _MIPS_SZINT 32
#define __MIPSEB 1
#define _MIPS_SIM _ABIO32
#define _MIPS_FPSET 16
`#define __mips_hard_float 1`
#define __mips 1
#define _MIPS_SZLONG 32

asn/gcc/gcc-4.8.2-glibc-mips-install/mips-unknown-linux/include/gnu$ ll
-rw-r--r--  1 kyoupark ccusers  1263 Jan 14 22:46 libc-version.h
-rw-r--r--  1 kyoupark ccusers 18285 Jan 14 22:46 lib-names.h
-rw-r--r--  1 kyoupark ccusers  1815 Jan 14 22:46 stubs.h
-rw-r--r--  1 kyoupark ccusers   894 Jan 13 22:05 stubs-o32_hard.h

note: 
Should look for hard one. WHY looks for soft one? GLIBC bug? Found that gcc
from buildroot/output/host/usr/bin shows different soft/hard option from the
external toolchain setting. HOW is it possible? Turns out `wrapper` sets
different float setting.


#if !defined(__mips_nan2008) && defined(__mips_hard_float) && (_MIPS_SIM == _MIPS_SIM_ABI32)
# include <gnu/stubs-o32_hard.h>
#endif


<gcc-build-glibc-mips>
--with-float=type
    These configure options provide default values for the -mschedule=,
    -march=, -mtune=, -mabi=, and -mfpu= options and for -mhard-float or
    -msoft-float. As with --with-cpu, which switches will be accepted and
    acceptable values of the arguments depend on the target. 


<buildroot-arch-abi>
--with-float=soft --disable-decimal-float 
--with-arch="mips32" --with-abi="32" 

buildroot says:
# march/mtune/floating point mode needs to be passed to the external toolchain
# to select the right multilib variant


$ BR2_DEBUG_WRAPPER=2 make V=1

/home/kyoupark/buildroot-2016.11/output/host/usr/bin/mips-unknown-linux-gcc 
-Wp,-MD,applets/.applets.o.d
-c -o applets/applets.o applets/applets.c

Toolchain wrapper executing:
    '/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/bin/mips-unknown-linux-gcc'
    '--sysroot'
    '/home/kyoupark/buildroot-2016.11/output/host/usr/mips-buildroot-linux-gnu/sysroot'
    '-mabi=32'
    '-msoft-float'
    '-march=mips32'
    ...

The reason:

# .config
BR2_SOFT_FLOAT=y

# toolchain/toolchain-external/toolchain-external.mk

ifeq ($(BR2_SOFT_FLOAT),y)
TOOLCHAIN_EXTERNAL_CFLAGS += -msoft-float
TOOLCHAIN_EXTERNAL_TOOLCHAIN_WRAPPER_ARGS += -DBR_SOFTFLOAT=1
endif


<error> <why-sysroot-necessary>
/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install/lib/gcc/mips-unknown-linux/4.8.2/../../../../mips-unknown-linux/bin/ld:
this linker was not configured to use sysroots
collect2: error: ld returned 1 exit status
scripts/Makefile.build:264: recipe for target 'applets/built-in.o' failed

builds okay when use <gcc-build-glibc-mips-sysroot-null> and 
BR2_PACKAGE_STRACE is not set


={============================================================================
*kt_linux_gcc_400* gcc-build-spk-build-vmlinux

add toolchain in PATH

PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

export PATH=/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin:$PATH


darwin-spk-1.19/Obj/ams-drx890_debug/project_build_mips/ams-drx890/root

cd darwin-spk-1.19/Obj/ams-drx890_debug/project_build_mips/ams-drx890/linux-2.6.18.8

echo "!!********* Build drivers *********!!"
make CROSS_COMPILE=mips-linux-gnu- modules
make modules

echo "!!********* Re-build initramfs*********!!"
touch ../root/VERSION.gz

# when use own toolchain
make CROSS_COMPILE=mips-linux-gnu-
# when use brcm toolchain
make


echo "!!********* Copy vmlinux to tftp location  [YOU MAY NEED TO ADD SUDO PASSWORD] *********!!"
cp vmlinux vmlinux-stripped
mips-linux-strip vmlinux-stripped
sudo cp vmlinux-stripped /var/lib/tftpboot/vmlinux_drx595

ls -al ./vmlinux ./vmlinux-stripped /var/lib/tftpboot/vmlinux_drx595

echo "!!********* done!*********!!"


pushd /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_INPUT_DEL_2/DEL_AMS_BCM_MIPS4K_LNUX_DARWIN_01/build_platform/image_generation

+ ./generate-otv-image.sh /home/nds-uk/kyoupark/spk-out-use-own-brcm/binaries/ams-drx890/vmlinux -E 4.X
./generate-otv-image.sh /home/nds-uk/kyoupark/spk-out-use-own-brcm/project_build_mips/ams-drx890/linux-2.6.18.8/vmlinux -E 4.X
./generate-otv-image.sh /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/project_build_mips/ams-drx890/linux-2.6.18.8/vmlinux -E 4.X


={============================================================================
*kt_linux_gcc_400* gcc-build-spk-toolchain

MIPS:
The toolchain used is the Broadcom NPTL toolchain with GCC4.2.

The SPK assumes that the symlink /opt/toolchains/uclibc-crosstools points to
the correct toolchain. This is the default symlink to the Broadcom install.

Configure Build Options:

Following configure build options are supported to modify standard configurations:

initramfs:

This enables initramfs (i.e. the kernel binary will contain a rootfs which is
    loaded into RAM). By default, a kernel with no internal rootfs is built.

noinitramfs:

This disables initramfs. This is the default state for all configs, and will
only need to be used if you have modified the configurations.

otv

Configures the kernel for the OTV bootloader. On some platforms, this modifies
the kernel configuration.

cfe

Configures the kernel for the CFE bootloader. This is the default
configuration. This option may be useful to undo the changes made by the “otv”
config option.


<spk-source>
/darwin-spk-1.19/target/device/Sky/kernel-patches/third-party/arch/mips/README.txt

                    stblinux delta from linux-mips.org
                    ----------------------------------

OVERVIEW


The Broadcom reference Linux distribution includes:

 * stblinux-2.6.18 source tarball for the kernel
 * vmlinuz* binary kernel images
 * uclinux-rootfs source tarball for the root filesystem
 * Various binary rootfs images (jffs2, squashfs, etc.)
 * crosstools* source RPM for the toolchain
 * crosstools* binary RPMs and tarballs

These are the only OFFICIAL released bundles that were tested and approved
by Broadcom.  However, as a courtesy, we provide the same code in the form
of patches, to help you integrate our changes into your own kernels.

Broadcom does not guarantee that any of these patches can be used
independently.  Everything is in there for a reason, and we cannot be
responsible for the results of mixing and matching.  If you experience a
problem with a customized kernel, you MUST reproduce it on the reference
Linux distribution before asking Broadcom for assistance.


BASELINE

Broadcom stblinux-2.6.18 is derived from Linux 2.6.18.8 from
linux-mips.org. Direct link:

http://www.linux-mips.org/pub/linux/mips/kernel/v2.6/linux-2.6.18.8.tar.gz

uClibc, binutils, and gcc were all derived from snapshots.  The snapshot
tarballs and all local Broadcom patches can be found in the toolchain SRPM.

To view the SRPM changelog:

rpm -qp --changelog \
    crosstools-linux-2.6.18.0-uclibc-nptl-0.9.29-20070423-4.2-10ts.src.rpm | \
    less

To install the SRPM and examine the sources:

rpm -i \
    crosstools-linux-2.6.18.0-uclibc-nptl-0.9.29-20070423-4.2-10ts.src.rpm

cd /usr/src/redhat

The RPM "spec" file will appear under SPECS/ .  The tarballs and patches are
in SOURCES/ .  Build instructions and options can be found in the spec
file.

The uClibc-nptl snapshot came from uclibc.org and the author of the port.
The binutils, gcc, and gdb source archives came from MIPS Technologies,
through the Timesys LinuxLink service.

/usr/src/redhat/RPMS/i386
-rw-r--r-- 1 root root  2982978 Feb 10 07:55 crosstools_hf-linux-2.6.18.0-uclibc-nptl-0.9.29-20070423-common-4.2-11tsHound.i386.rpm
-rw-r--r-- 1 root root 38979402 Feb 10 07:56 crosstools_hf-linux-2.6.18.0-uclibc-nptl-0.9.29-20070423-mips-4.2-11tsHound.i386.rpm
-rw-r--r-- 1 root root 38726268 Feb 10 07:56 crosstools_hf-linux-2.6.18.0-uclibc-nptl-0.9.29-20070423-mipsel-4.2-11tsHound.i386.rpm

The SPK assumes that the symlink /opt/toolchains/uclibc-crosstools points to
the correct toolchain. This is the default symlink to the Broadcom install.


<rpm-install>
su
rpm -i \
    crosstools-linux-2.6.18.0-uclibc-nptl-0.9.29-20070423-4.2-10ts.src.rpm

# centos
install to /usr/src/redhat/SOURCES

# debian
will install in ~/rpmbuild like /home/kyoupark/rpmbuild but will not if there
is a such directory.


<vm-for-rpm-build>
centos vm:
sky, drx890
root, skysky
gcc version 4.1.2 20080704 (Red Hat 4.1.2-55)
// have shared folder, Downloads, with host pc and /media/sf_Downloads

debian:
gcc version 4.9.2 (Debian 4.9.2-10)

// must specify buildlog output since there is a check in spec file before
// running build_tool() or can comment out this check.
//
// have to comment out clean in the spec file in order not to delete built
// toolchain. do not know why.
// # %clean
// # rm -rf %{tools_dir_prefix}

cd /usr/src/redhat/SPECS
rpmbuild -ba crosstools-linux-2.6.18.0-uclibc-0.9.29-nptl-20070423-4.2-10ts.spec  2>&1 |tee  buildlog 


+ /usr/bin/bzip2 -dc /usr/src/redhat/SOURCES/linux-libc-headers-2.6.18.0.tar.bz2
+ tar -xvvf -
drwxr-xr-x root/root         0 2007-05-08 03:40:59 linux-libc-headers-2.6.18.0/
+ cd linux-libc-headers-2.6.18.0
++ /usr/bin/id -u
+ '[' 0 = 0 ']'
+ /bin/chown -Rhf root .
++ /usr/bin/id -u
+ '[' 0 = 0 ']'
+ /bin/chgrp -Rhf root .
+ /bin/chmod -Rf a+rX,u+w,g-w,o-w .
+ echo 'Patch #90 (linux-libc-headers-2.6.18.0-brcm-cachectl.patch):'

// define patches
Patch3003: uClibc-%{uclibc_version}-3003-plt-ldso.patch


// decide which patches to apply
%setup -T -b 0 -n uClibc-%{uclibc_version}
%patch009 -p1
%patch010 -p1
%patch011 -p1
%patch012 -p1
%patch013 -p1
%patch014 -p1
%patch015 -p1
%patch3003 -p1
...

+ echo 'Patch #9 (uClibc-nptl-0.9.29-20070423-config.patch):'
+ patch -p1 -s
+ echo 'Patch #10 (uClibc-nptl-0.9.29-20070423-mips-gprof.patch):'
+ patch -p1 -s
...

note:
So BUILD has all packages which are patched.


// crosstools-linux-2.6.18.0-uclibc-0.9.29-nptl-20070423-4.2-11tsHound.spec
build_tool()
{
  arch=$1
  gnu_arch=$arch-linux-uclibc
  export PATH=%{tools_dir_prefix}/bin:$PATH

  pushd ${RPM_BUILD_DIR}

  #
  # Set the arch for the build/devel system, or let it be automaticly set.
  #
  build_arch=$arch-linux

+ build_tool mips
+ arch=mips
+ gnu_arch=mips-linux-uclibc
+ export PATH=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/bin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/X11R6/bin:/root/bin
+ pushd /usr/src/redhat/BUILD
+ build_arch=mips-linux

  # Configure uClibc and header files
  
  # note: not build but do config
  # Build uClibc
  #
  pushd uClibc-%{uclibc_version}

  mkdir -p %{tools_dir_prefix}/$gnu_arch/include
  mkdir -p ${RPM_BUILD_DIR}/build_uClibc/usr/include
  mkdir -p ${RPM_BUILD_DIR}/build_uClibc/usr/lib
  mkdir -p ${RPM_BUILD_DIR}/build_uClibc/lib

+ `pushd uClibc-nptl-0.9.29-20070423`
+ mkdir -p /usr/src/redhat/BUILD/build_uClibc/usr/include
+ mkdir -p /usr/src/redhat/BUILD/build_uClibc/usr/lib
+ mkdir -p /usr/src/redhat/BUILD/build_uClibc/lib

  # Copy in the locale tarball
  cp ${RPM_SOURCE_DIR}/uClibc-locale-030818.tgz extra/locale

  # Copy in the architecture config and fix it up
  cp extra/Configs/Config.rpm.$arch ./.config

  if test %{float_type} = soft; then
    # Modify the uClibc config for soft float
    perl -p -i -e 's,.*HAS_FPU.*,# UCLIBC_HAS_FPU is not set\nUCLIBC_HAS_SOFT_FLOAT=y,g' ./.config
  fi

  # Modify the uClibc config for profiling
  perl -p -i -e 's,.*UCLIBC_PROFILING.*,# UCLIBC_PROFILING is not set,g' ./.config

  # Disable library stripping
  perl -p -i -e 's,.*DOSTRIP.*,# DOSTRIP is not set,g' ./.config

  make oldconfig

+ marks the end of oldconfig
+ Manuel's hidden warnings (UCLIBC_MJN3_ONLY) [N/y/?] n

+ cp /usr/src/redhat/SOURCES/uClibc-locale-030818.tgz extra/locale
+ cp extra/Configs/Config.rpm.mips ./.config
+ test hard = soft
+ perl -p -i -e 's,.*UCLIBC_PROFILING.*,# UCLIBC_PROFILING is not set,g' ./.config
+ perl -p -i -e 's,.*DOSTRIP.*,# DOSTRIP is not set,g' ./.config
+ make oldconfig

  # Create symlinks
  ln -sf ${RPM_BUILD_DIR}/%{kernel_top_dir}/linux include
  ln -sf ${RPM_BUILD_DIR}/%{kernel_top_dir}/asm-mips include/asm
  ln -sf ${RPM_BUILD_DIR}/%{kernel_top_dir}/asm-generic include/asm-generic

+ make PREFIX=/usr/src/redhat/BUILD/build_uClibc/ DEVEL_PREFIX=/usr/ RUNTIME_PREFIX=/usr/src/redhat/BUILD/build_uClibc/ HOSTCC=gcc KERNEL_HEADERS=/usr/src/redhat/BUILD/build_uClibc/usr/include CROSS=mips-linux-uclibc- pregen install_dev
  make PREFIX=${RPM_BUILD_DIR}/build_uClibc/			\
       DEVEL_PREFIX=/usr/					\
       RUNTIME_PREFIX=${RPM_BUILD_DIR}/build_uClibc/		\
       HOSTCC=gcc						\
       KERNEL_HEADERS=${RPM_BUILD_DIR}/build_uClibc/usr/include	\
       CROSS=$gnu_arch-						\
       pregen install_dev
  popd

+ ln -sf /usr/src/redhat/BUILD/linux-libc-headers-2.6.18.0/include/linux include
+ ln -sf /usr/src/redhat/BUILD/linux-libc-headers-2.6.18.0/include/asm-mips include/asm
+ ln -sf /usr/src/redhat/BUILD/linux-libc-headers-2.6.18.0/include/asm-generic include/asm-generic
+ make PREFIX=/usr/src/redhat/BUILD/build_uClibc/ DEVEL_PREFIX=/usr/ 
  RUNTIME_PREFIX=/usr/src/redhat/BUILD/build_uClibc/ HOSTCC=gcc 
  KERNEL_HEADERS=/usr/src/redhat/BUILD/build_uClibc/usr/include CROSS=mips-linux-uclibc- pregen install_dev

  #
  # Configure, build and install binutils
  #
  rm -rf build-binutils-%{binutils_version}
  mkdir build-binutils-%{binutils_version}
  pushd build-binutils-%{binutils_version}
  ../binutils-%{binutils_version}/configure			\
    --prefix=%{tools_dir_prefix}				\
    --build=$build_arch 					\
    --host=$build_arch	 					\
    --target=$gnu_arch						\
    --disable-werror						\
    --disable-nls						\
    --enable-multilib

+ ../binutils-2.17.50/configure --prefix=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809 --build=mips-linux --host=mips-linux --target=mips-linux-uclibc --disable-werror --disable-nls --enable-multilib
// + ../binutils-2.17.50/configure
// --prefix=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809
// --build=mips-linux --host=mips-linux --target=mips-linux-uclibc
// --disable-werror --disable-nls --enable-multilib

  make %{parallel}
  make info
  make install
  make install-info
  popd
  
  #
  # Update gcc version number
  #
  pushd gcc-%{gcc_version}
  perl -p -i -e "s/BRCM_VER/%{build_number}-%{build_date}/" gcc/version.c
  popd

  #
  # Build first stage C compiler
  #
  rm -rf build-gcc-%{gcc_version}-initial
  mkdir build-gcc-%{gcc_version}-initial
  pushd build-gcc-%{gcc_version}-initial

  # Currently the unwind stuff seems to work for staticly linked apps but
  # not dynamic.  So use setjmp/longjmp exceptions by default.
  #
  ../gcc-%{gcc_version}/configure 				\
    --prefix=%{tools_dir_prefix}				\
    --build=$build_arch 					\
    --host=$build_arch						\
    --target=$gnu_arch						\
    --with-build-sysroot=${RPM_BUILD_DIR}/build_uClibc		\
    --enable-languages=c					\
    --disable-shared						\
    --disable-__cxa_atexit 					\
    --enable-target-optspace 					\
    --with-gnu-ld						\
    --with-float=%{float_type}					\
    --enable-threads						\
    --disable-multilib						\
    --with-gnu-plts

+ ../gcc-4.2.0-20070124/configure
`--prefix=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809`
--build=mips-linux --host=mips-linux --target=mips-linux-uclibc
`--with-build-sysroot=/usr/src/redhat/BUILD/build_uClibc` --enable-languages=c
--disable-shared --disable-__cxa_atexit --enable-target-optspace --with-gnu-ld
--with-float=hard --enable-threads --disable-multilib --with-gnu-plts

  make %{parallel} all-gcc
  make install-gcc
  popd

  #
  # Build uClibc (which was configured above).
  #
  pushd uClibc-%{uclibc_version}

// must run the previous step to have .config.
// pushd /usr/src/redhat/BUILD/uClibc-nptl-0.9.29-20070423
// export PATH=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/bin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/X11R6/bin:/root/bin
// export PATH=/home/nds-uk/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/X11R6/bin:/root/bin
+ make KERNEL_HEADERS=/usr/src/redhat/BUILD/build_uClibc/usr/include PREFIX= DEVEL_PREFIX=/ RUNTIME_PREFIX=/ CROSS=mips-linux-uclibc- all
  make KERNEL_HEADERS=${RPM_BUILD_DIR}/build_uClibc/usr/include	\
    PREFIX=							\
    DEVEL_PREFIX=/						\
    RUNTIME_PREFIX=/						\
    CROSS=$gnu_arch- all

// to get processed output
// mips-linux-uclibc-gcc -c ldso/ldso/ldso.c -E -include ./include/libc-symbols.h -Wall -Wstrict-prototypes -fno-strict-aliasing -mips32 -mtune=mips32 -mabi=32 -fstack-protector -fno-builtin -nostdinc -I./include -I. -fsigned-char -Os -funit-at-a-time -fno-tree-loop-optimize -fno-tree-dominator-opts -fno-strength-reduce -mno-split-addresses -I./libpthread/nptl -I./libpthread/nptl/sysdeps/unix/sysv/linux/mips -I./libpthread/nptl/sysdeps/mips -I./libpthread/nptl/sysdeps/unix/sysv/linux -I./libpthread/nptl/sysdeps/pthread -I./libpthread/nptl/sysdeps/pthread/bits -I./libpthread/nptl/sysdeps/generic -I./ldso/ldso/mips -I./ldso/include -I/usr/src/redhat/BUILD/build_uClibc/usr/include -isystem /opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include -DNDEBUG -fstack-protector-all -mno-split-addresses -fPIC -DSHARED -DNOT_IN_libc -DIS_IN_rtld -fno-stack-protector -fno-omit-frame-pointer -I./ldso/ldso/mips -I./ldso/include -I./ldso/ldso -DUCLIBC_RUNTIME_PREFIX="/" -DUCLIBC_LDSO="ld-uClibc.so.0" -DLDSO_ELFINTERP="mips/elfinterp.c"
//
// mips-linux-uclibc-gcc -c ldso/ldso/ldso.c -o ldso/ldso/ldso.oS -include ./include/libc-symbols.h -Wall -Wstrict-prototypes -fno-strict-aliasing -mips32 -mtune=mips32 -mabi=32 -fstack-protector -fno-builtin -nostdinc -I./include -I. -fsigned-char -Os -funit-at-a-time -fno-tree-loop-optimize -fno-tree-dominator-opts -fno-strength-reduce -mno-split-addresses -I./libpthread/nptl -I./libpthread/nptl/sysdeps/unix/sysv/linux/mips -I./libpthread/nptl/sysdeps/mips -I./libpthread/nptl/sysdeps/unix/sysv/linux -I./libpthread/nptl/sysdeps/pthread -I./libpthread/nptl/sysdeps/pthread/bits -I./libpthread/nptl/sysdeps/generic -I./ldso/ldso/mips -I./ldso/include -I/usr/src/redhat/BUILD/build_uClibc/usr/include -isystem /opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include -DNDEBUG -fstack-protector-all -mno-split-addresses -fPIC -DSHARED -DNOT_IN_libc -DIS_IN_rtld -fno-stack-protector -fno-omit-frame-pointer -I./ldso/ldso/mips -I./ldso/include -I./ldso/ldso -DUCLIBC_RUNTIME_PREFIX="/" -DUCLIBC_LDSO="ld-uClibc.so.0" -DLDSO_ELFINTERP="mips/elfinterp.c"
// mips-linux-uclibc-gcc -c ldso/ldso/ldso.c -o ldso/ldso/ldso.oS -include
// ./include/libc-symbols.h -Wall -Wstrict-prototypes -fno-strict-aliasing
// -mips32 -mtune=mips32 -mabi=32 -fstack-protector -fno-builtin -nostdinc
// -I./include -I. -fsigned-char -Os -funit-at-a-time -fno-tree-loop-optimize
// -fno-tree-dominator-opts -fno-strength-reduce -mno-split-addresses
// -I./libpthread/nptl -I./libpthread/nptl/sysdeps/unix/sysv/linux/mips
// -I./libpthread/nptl/sysdeps/mips
// -I./libpthread/nptl/sysdeps/unix/sysv/linux
// -I./libpthread/nptl/sysdeps/pthread
// -I./libpthread/nptl/sysdeps/pthread/bits
// -I./libpthread/nptl/sysdeps/generic -I./ldso/ldso/mips -I./ldso/include
// -I/usr/src/redhat/BUILD/build_uClibc/usr/include -isystem
// /opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include
// -DNDEBUG -fstack-protector-all -mno-split-addresses -fPIC -DSHARED
// -DNOT_IN_libc -DIS_IN_rtld -fno-stack-protector -fno-omit-frame-pointer
// -I./ldso/ldso/mips -I./ldso/include -I./ldso/ldso
// -DUCLIBC_RUNTIME_PREFIX="/" -DUCLIBC_LDSO="ld-uClibc.so.0"
// -DLDSO_ELFINTERP="mips/elfinterp.c"

// mips-linux-uclibc-gcc -c ldso/ldso/mips/resolve.S -o ldso/ldso/mips/resolve.oS -include ./include/libc-symbols.h -Wall -Wstrict-prototypes -fno-strict-aliasing -mips32 -mtune=mips32 -mabi=32 -fstack-protector -fno-builtin -nostdinc -I./include -I. -fsigned-char -Os -funit-at-a-time -fno-tree-loop-optimize -fno-tree-dominator-opts -fno-strength-reduce -mno-split-addresses -I./libpthread/nptl -I./libpthread/nptl/sysdeps/unix/sysv/linux/mips -I./libpthread/nptl/sysdeps/mips -I./libpthread/nptl/sysdeps/unix/sysv/linux -I./libpthread/nptl/sysdeps/pthread -I./libpthread/nptl/sysdeps/pthread/bits -I./libpthread/nptl/sysdeps/generic -I./ldso/ldso/mips -I./ldso/include -I/usr/src/redhat/BUILD/build_uClibc/usr/include -isystem /opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include -DNDEBUG -fstack-protector-all -mno-split-addresses -fPIC -DSHARED -fstack-protector-all -DNOT_IN_libc -DIS_IN_rtld -fno-stack-protector -fno-omit-frame-pointer -I./ldso/ldso/mips -I./ldso/include -I./ldso/ldso -DUCLIBC_RUNTIME_PREFIX="/" -DUCLIBC_LDSO="ld-uClibc.so.0" -D__ASSEMBLER__ -Wa,--noexecstack
// mips-linux-uclibc-gcc -c ldso/ldso/mips/resolve.S -o
// ldso/ldso/mips/resolve.oS -include ./include/libc-symbols.h -Wall
// -Wstrict-prototypes -fno-strict-aliasing -mips32 -mtune=mips32 -mabi=32
// -fstack-protector -fno-builtin -nostdinc -I./include -I. -fsigned-char -Os
// -funit-at-a-time -fno-tree-loop-optimize -fno-tree-dominator-opts
// -fno-strength-reduce -mno-split-addresses -I./libpthread/nptl
// -I./libpthread/nptl/sysdeps/unix/sysv/linux/mips
// -I./libpthread/nptl/sysdeps/mips
// -I./libpthread/nptl/sysdeps/unix/sysv/linux
// -I./libpthread/nptl/sysdeps/pthread
// -I./libpthread/nptl/sysdeps/pthread/bits
// -I./libpthread/nptl/sysdeps/generic -I./ldso/ldso/mips -I./ldso/include
// -I/usr/src/redhat/BUILD/build_uClibc/usr/include -isystem
// /opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/bin/../lib/gcc/mips-linux-uclibc/4.2.0/include
// -DNDEBUG -fstack-protector-all -mno-split-addresses -fPIC -DSHARED
// -fstack-protector-all -DNOT_IN_libc -DIS_IN_rtld -fno-stack-protector
// -fno-omit-frame-pointer -I./ldso/ldso/mips -I./ldso/include -I./ldso/ldso
// -DUCLIBC_RUNTIME_PREFIX="/" -DUCLIBC_LDSO="ld-uClibc.so.0" -D__ASSEMBLER__
// -Wa,--noexecstack

+ make KERNEL_HEADERS=/usr/src/redhat/BUILD/build_uClibc/usr/include PREFIX= DEVEL_PREFIX=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/mips-linux-uclibc/ RUNTIME_PREFIX=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/mips-linux-uclibc/ CROSS=mips-linux-uclibc- install_runtime install_dev
  make KERNEL_HEADERS=${RPM_BUILD_DIR}/build_uClibc/usr/include	\
    PREFIX=							\
    DEVEL_PREFIX=%{tools_dir_prefix}/$gnu_arch/			\
    RUNTIME_PREFIX=%{tools_dir_prefix}/$gnu_arch/		\
    CROSS=$gnu_arch-	 					\
    install_runtime install_dev

// + make KERNEL_HEADERS=/usr/src/redhat/BUILD/build_uClibc/usr/include PREFIX=
// DEVEL_PREFIX=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/mips-linux-uclibc/
// RUNTIME_PREFIX=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/mips-linux-uclibc/
// CROSS=mips-linux-uclibc- install_runtime install_dev

  ln -snf include %{tools_dir_prefix}/$gnu_arch/sys-include
+ ln -snf include /opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/mips-linux-uclibc/sys-include

  popd

  #
  # Build final C compiler and C++ support, using the newly built uClibc.
  #
  rm -rf build-gcc-%{gcc_version}-final
  mkdir build-gcc-%{gcc_version}-final
  pushd build-gcc-%{gcc_version}-final

  #
  # WARNING!!!  Do not use --with-gxx-include-dir!!!  WARNING!!!
  #
  # mjn3: Incorrectly setting the g++ include dir has broken a number
  #       of uClibc toolchains in the past.  There is logic in some gcc
  #       versions that attempts to detect path aliases and which can
  #        break the include dir search order.  Best to just leave it be.
  #
  ../gcc-%{gcc_version}/configure				\
    --prefix=%{tools_dir_prefix}/				\
    --build=$build_arch						\
    --host=$build_arch						\
    --target=$gnu_arch						\
    --with-build-sysroot=${RPM_BUILD_DIR}/build_uClibc		\
    --enable-languages=c,c++					\
    --disable-__cxa_atexit 					\
    --enable-target-optspace					\
    --with-gnu-ld						\
    --with-float=%{float_type}					\
    --enable-threads						\
    --infodir=%{tools_dir_prefix}/info				\
    --mandir=%{tools_dir_prefix}/man				\
    --with-arch=mips32						\
    --disable-libmudflap					\
    --disable-nls						\
    --with-gnu-plts

+ ../gcc-4.2.0-20070124/configure
  --prefix=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/
  --build=mips-linux --host=mips-linux --target=mips-linux-uclibc
  --with-build-sysroot=/usr/src/redhat/BUILD/build_uClibc
  --enable-languages=c,c++ --disable-__cxa_atexit --enable-target-optspace
  --with-gnu-ld --with-float=hard --enable-threads
  --infodir=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/info
  --mandir=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-11tsHound_uclibc-nptl-0.9.29-20070423_20160809/man
  --with-arch=mips32 --disable-libmudflap --disable-nls --with-gnu-plts

  make %{parallel} all							\
    CFLAGS_FOR_TARGET="%{library_flags}"			\
    CXXFLAGS_FOR_TARGET="%{library_flags}"

  make info
  make install
  make install-info

  # If we build libstdc++ with -mips16, scripts/testsuite_flags will
  # cause the libstdc++ testsuite to use -mips16 (unlike the other
  # testsuites).  If we built the library with -Os, the testsuite will
  # also use -Os, but some of the tests require -O2.  Change the
  # testsuite flags to the default "-O2 -g".
  perl -p -i -e "s/-Os -g.*/-O2 -g/" $gnu_arch/libstdc++-v3/scripts/testsuite_flags

  popd

  #
  # Relink uClibc against final libgcc
  #
  pushd uClibc-%{uclibc_version}

  rm -rf lib

  make KERNEL_HEADERS=${RPM_BUILD_DIR}/build_uClibc/usr/include	\
    PREFIX=							\
    DEVEL_PREFIX=/						\
    RUNTIME_PREFIX=/						\
    CROSS=$gnu_arch- all

  make KERNEL_HEADERS=${RPM_BUILD_DIR}/build_uClibc/usr/include	\
    PREFIX=							\
    DEVEL_PREFIX=%{tools_dir_prefix}/$gnu_arch/			\
    RUNTIME_PREFIX=%{tools_dir_prefix}/$gnu_arch/		\
    CROSS=$gnu_arch-	 					\
    install_runtime install_dev

  popd

  #
  # Build uClibc utils (which were configured above).
  #
  pushd uClibc-%{uclibc_version}
  make KERNEL_HEADERS=${RPM_BUILD_DIR}/build_uClibc/usr/include	\
    PREFIX=%{tools_dir_prefix}/$gnu_arch/target-apps/ 		\
    DEVEL_PREFIX=						\
    RUNTIME_PREFIX=						\
    CROSS=$gnu_arch-	 					\
    utils install_utils

  make -C utils							\
    KERNEL_HEADERS=${RPM_BUILD_DIR}/build_uClibc/usr/include	\
    PREFIX=%{tools_dir_prefix}/$gnu_arch/ 			\
    CROSS=$gnu_arch-	 					\
    clean

  # We build uClibc in place, so clean it up for the next arch.
  make distclean
  popd

  // %if %{enable_gdb}
  // #
  // # Build gdbserver for the target
  // #
  // rm -rf build-gdbserver%{gdb_version}
  // mkdir build-gdbserver%{gdb_version}
  // pushd build-gdbserver%{gdb_version}

  // AR=$gnu_arch-ar						\
  // AS=$gnu_arch-as						\
  // LD=$gnu_arch-ld						\
  // NM=$gnu_arch-nm						\
  // CC=$gnu_arch-gcc						\
  // GCC=$gnu_arch-gcc						\
  // CXX=$gnu_arch-g++						\
  // RANLIB=$gnu_arch-ranlib					\
  // gdb_cv_func_sigsetjmp=yes					\
  // ac_cv_type_uintptr_t=yes					\
  // gt_cv_func_gettext_libintl=yes				\
  // ac_cv_func_dcgettext=yes					\
  // bash_cv_func_strcoll_broken=no				\
  // bash_cv_must_reinstall_sighandlers=no				\
  // bash_cv_func_sigsetjmp=present				\
  // bash_cv_have_mbstate_t=yes					\
  // ../gdb-%{gdb_version}/gdb/gdbserver/configure			\
  //   --prefix=%{tools_dir_prefix}/$gnu_arch/target-apps/usr	\
  //   --target=$gnu_arch						\
  //   --build=$build_arch						\
  //   --host=$gnu_arch						\
  //   --mandir=%{tools_dir_prefix}/man				\
  //   --infodir=%{tools_dir_prefix}/info				\
  //   --disable-nls						\
  //   --without-uiout --disable-gdbmi				\
  //   --disable-tui --disable-gdbtk --without-x			\
  //   --without-included-gettext

  // make %{parallel} all
  // make install
  // popd

  // #
  // # Build gdb for the host
  // #
  // rm -rf build-gdb-%{gdb_version}
  // mkdir build-gdb-%{gdb_version}
  // pushd build-gdb-%{gdb_version}
  // gdb_cv_func_sigsetjmp=yes					\
  // ../gdb-%{gdb_version}/configure				\
  //   --prefix=%{tools_dir_prefix}				\
  //   --target=$gnu_arch						\
  //   --build=$build_arch						\
  //   --host=$build_arch						\
  //   --disable-nls						\
  //   --disable-gdbserver						\
  //   --without-included-gettext

  // make %{parallel} all-gdb
  // make info
  // #make install-gdb
  // make install
  // make install-info
  // #install gdb/gdb %{tools_dir_prefix}/bin/$gnu_arch-gdb
  // #install gdb/gdb %{tools_dir_prefix}/$gnu_arch/bin

  // popd
  // %endif

  #
  # Build sstrip for the host.
  #
  # Even though it is independent of the target arch, provide symlinks
  # so that it can be invoked like all other cross tools.
  #
  pushd sstrip

  make all

  install sstrip %{tools_dir_prefix}/bin

  ln -snf sstrip %{tools_dir_prefix}/bin/$gnu_arch-sstrip
  ln -snf sstrip %{tools_dir_prefix}/bin/$arch-linux-sstrip
  ln -snf sstrip %{tools_dir_prefix}/bin/$arch-uclibc-sstrip
  ln -snf ../../bin/sstrip %{tools_dir_prefix}/$gnu_arch/bin/sstrip

  popd

  #
  # Now strip all binaries.
  #

  # Remove the script to avoid strip failures.
  rm -f %{tools_dir_prefix}/bin/*-gccbug

  # Do some additional housekeeping.
  rm -f %{tools_dir_prefix}/lib/libiberty.a
  rm -f %{tools_dir_prefix}/gcc/libiberty.a
  rm -f %{tools_dir_prefix}/gcc/libmmalloc.a

  strip %{tools_dir_prefix}/bin/*
  strip %{tools_dir_prefix}/$gnu_arch/bin/*

  $gnu_arch-strip %{tools_dir_prefix}/$gnu_arch/target-apps/sbin/*
  $gnu_arch-strip %{tools_dir_prefix}/$gnu_arch/target-apps/usr/bin/*

  #
  # Create the backward compatibility symlinks.
  #

  pushd %{tools_dir_prefix}/bin
  for F in $gnu_arch-* ; do \
	ln -snf $F $arch-linux${F##$gnu_arch} ;\
	ln -snf $F $arch-uclibc${F##$gnu_arch} ; \
  done
  popd
  ln -snf $gnu_arch %{tools_dir_prefix}/$arch-linux
  ln -snf $gnu_arch %{tools_dir_prefix}/$arch-uclibc

  # Directory name changed in gcc 3.4.x from lib/gcc-libc/ to lib/gcc/.
  if [ -d %{tools_dir_prefix}/lib/gcc-lib ] ; then 
    ln -snf $gnu_arch %{tools_dir_prefix}/lib/gcc-lib/$arch-linux
    ln -snf $gnu_arch %{tools_dir_prefix}/lib/gcc-lib/$arch-uclibc
  fi
  if [ -d %{tools_dir_prefix}/lib/gcc ] ; then 
    ln -snf $gnu_arch %{tools_dir_prefix}/lib/gcc/$arch-linux
    ln -snf $gnu_arch %{tools_dir_prefix}/lib/gcc/$arch-uclibc
  fi
}


note: when not use --disable-multilib, no libgcc_eh.a and fails on
build-glibc.

make -j4 all-gcc
make install-gcc


={============================================================================
*kt_linux_gcc_400* gcc-toolchain-spec gcc-spec

toolchain spec comparisons.

<brcm-toolchain> see *gcc-options-verbose* for -###
$ ./mips-buildroot-linux-gnu-gcc -v 2>&1 | grep ^Configured | tr " " "\n"

/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-uclibc-gcc -### x.c

Using built-in specs.
Target: mips-linux-uclibc
Configured with: ../gcc-4.2.0-20070124/configure
--prefix=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/
--build=mips-linux --host=mips-linux --target=mips-linux-uclibc
--with-build-sysroot=/usr/src/redhat/BUILD/build_uClibc
--enable-languages=c,c++ --disable-__cxa_atexit --enable-target-optspace
--with-gnu-ld --with-float=hard --enable-threads
--infodir=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/info
--mandir=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/man
--with-arch=mips32 --disable-libmudflap --disable-nls --with-gnu-plts

Thread model: posix
gcc version 4.2.0 20070124 (prerelease) - BRCM 10tsHound-20140508

 // cc1
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../libexec/gcc/mips-linux-uclibc/4.2.0/cc1"
 "-quiet" "-iprefix"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/"
 "x.c" "-quiet" "-dumpbase" "x.c" "-march=mips32" "-mhard-float" "-mgnu-plts"
 "-mno-shared" "-minterlink-mips16" "-auxbase" "x" "-o" "/tmp/ccW3IwoX.s"

 // as
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/bin/as"
 "-EB" "-no-mdebug" "-mabi=32" "-march=mips32" "-mno-shared" "-o"
 "/tmp/cck3ulQO.o" "/tmp/ccW3IwoX.s"

 // colletc2
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../libexec/gcc/mips-linux-uclibc/4.2.0/collect2"
 "--eh-frame-hdr" "-EB" "-dynamic-linker" "/lib/ld-uClibc.so.0"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib/crt1.o"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib/crti.o"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/crtbegin.o"
 "-L/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0"
 "-L/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc"
 "-L/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib"
 "/tmp/cck3ulQO.o" "-lgcc" "--as-needed" "-lgcc_s" "--no-as-needed" "-lc"
 "-lgcc" "--as-needed" "-lgcc_s" "--no-as-needed"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/crtend.o"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib/crtn.o"


<own-toolchain>
$ ./gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -### sample.c
Using built-in specs.
Target: mips-linux-gnu
Configured with: ../gcc-4.2.0-20070124-patched/configure --target=mips-linux-gnu --prefix=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install --enable-threads=no --disable-shared --with-float=hard --disable-__cxa_atexit --enable-target-optspace --disable-libmudflap --disable-nls --with-gnu-ld --with-gnu-plts --disable-multilib --enable-languages=c : (reconfigured) ../gcc-4.2.0-20070124-patched/configure --target=mips-linux-gnu --prefix=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install --enable-languages=c,c++ --disable-__cxa_atexit --enable-target-optspace --with-gnu-ld --with-float=hard --enable-threads --with-arch=mips32 --disable-libmudflap --disable-nls --with-gnu-plts
Thread model: posix
gcc version 4.2.0 20070124 (prerelease) - BRCM BRCM_VER

 "/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../libexec/gcc/mips-linux-gnu/4.2.0/cc1"
 "-quiet" "-iprefix"
 "/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/"
 "sample.c" "-quiet" "-dumpbase" "sample.c" "-march=mips32" "-mhard-float"
 "-mgnu-plts" "-mno-shared" "-minterlink-mips16" "-auxbase" "sample" "-o"
 "/tmp/ccBPoJEH.s"

 "/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/../../../../mips-linux-gnu/bin/as"
 "-EB" "-no-mdebug" "-mabi=32" "-march=mips32" "-mno-shared" "-o"
 "/tmp/cc90b7Hu.o" "/tmp/ccBPoJEH.s"

 "/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../libexec/gcc/mips-linux-gnu/4.2.0/collect2"
 "--eh-frame-hdr" "-EB" "-dynamic-linker" "/lib/ld.so.1"
 "/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/../../../../mips-linux-gnu/lib/crt1.o"
 "/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/../../../../mips-linux-gnu/lib/crti.o"
 "/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/crtbegin.o"
 "-L/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0"
 "-L/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc"
 "-L/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/lib/gcc/mips-linux-gnu/4.2.0"
 "-L/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/../../../../mips-linux-gnu/lib"
 "-L/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/lib/gcc/mips-linux-gnu/4.2.0/../../../../mips-linux-gnu/lib"
 "/tmp/cc90b7Hu.o" "-lgcc" "--as-needed" "-lgcc_s" "--no-as-needed" "-lc"
 "-lgcc" "--as-needed" "-lgcc_s" "--no-as-needed"
 "/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/crtend.o"
 "/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/../../../../mips-linux-gnu/lib/crtn.o"


<toolchain-spec-x86>
kyoupark@st-castor-03:~/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build$ ~/STB_SW/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-gnu/bin/i686-nptl-linux-gnu-gcc -### sample.c
Using built-in specs.
COLLECT_GCC=/home/kyoupark/STB_SW/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-gnu/bin/i686-nptl-linux-gnu-gcc
COLLECT_LTO_WRAPPER=/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../libexec/gcc/i686-nptl-linux-gnu/4.9.1/lto-wrapper
Target: i686-nptl-linux-gnu

Configured with: /home/pete/old_chain/.build/src/gcc-4.9.1/configure --build=i686-build_pc-linux-gnu --host=i686-build_pc-linux-gnu --target=i686-nptl-linux-gnu --prefix=/home/pete/x-tools/i686-nptl-linux-gn
u --with-sysroot=/home/pete/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sysroot --enable-languages=c,c++ --with-arch=i686 --with-pkgversion='crosstool-NG 1.21.0' --enable-__cxa_atexit --disable-libmudfla
p --disable-libgomp --disable-libssp --disable-libquadmath --disable-libquadmath-support 
--disable-libsanitizer 
--with-gmp=/home/pete/old_chain/.build/i686-nptl-linux-gnu/buildtools --with-mpfr=/home/pete/ol
d_chain/.build/i686-nptl-linux-gnu/buildtools --with-mpc=/home/pete/old_chain/.build/i686-nptl-linux-gnu/buildtools --with-isl=/home/pete/old_chain/.build/i686-nptl-linux-gnu/buildtools --with-cloog=/home/pe
te/old_chain/.build/i686-nptl-linux-gnu/buildtools --with-libelf=/home/pete/old_chain/.build/i686-nptl-linux-gnu/buildtools --with-host-libstdcxx='-static-libgcc -Wl,-Bstatic,-lstdc++ -lm' --enable-threads=p
osix --enable-target-optspace --disable-nls --disable-multilib --with-local-prefix=/home/pete/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sysroot --enable-c99 --enable-long-long

Thread model: posix
gcc version 4.9.1 (crosstool-NG 1.21.0)
COLLECT_GCC_OPTIONS='-mtune=generic' '-march=i686'
 /home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../libexec/gcc/i686-nptl-linux-gnu/4.9.1/cc1 -quiet -iprefix /home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../
lib/gcc/i686-nptl-linux-gnu/4.9.1/ -isysroot /home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../i686-nptl-linux-gnu/sysroot sample.c -quiet -dumpbase sample.c "-mtune=generic" "-march=
i686" -auxbase sample -o /tmp/ccSzT2O1.s
COLLECT_GCC_OPTIONS='-mtune=generic' '-march=i686'
 /home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../lib/gcc/i686-nptl-linux-gnu/4.9.1/../../../../i686-nptl-linux-gnu/bin/as --32 -o /tmp/ccQhQDLK.o /tmp/ccSzT2O1.s
COMPILER_PATH=/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../libexec/gcc/i686-nptl-linux-gnu/4.9.1/:/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../libexe
c/gcc/:/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../lib/gcc/i686-nptl-linux-gnu/4.9.1/../../../../i686-nptl-linux-gnu/bin/
LIBRARY_PATH=/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../lib/gcc/i686-nptl-linux-gnu/4.9.1/:/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../lib/gcc/:/h
ome/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../lib/gcc/i686-nptl-linux-gnu/4.9.1/../../../../i686-nptl-linux-gnu/lib/:/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_
01/bin/../i686-nptl-linux-gnu/sysroot/lib/:/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../i686-nptl-linux-gnu/sysroot/usr/lib/
COLLECT_GCC_OPTIONS='-mtune=generic' '-march=i686'
 /home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../libexec/gcc/i686-nptl-linux-gnu/4.9.1/collect2 "--sysroot=/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../
i686-nptl-linux-gnu/sysroot" --eh-frame-hdr -m elf_i386 -dynamic-linker /lib/ld-linux.so.2 /home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../i686-nptl-linux-gnu/sysroot/usr/lib/crt1.o
 /home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../i686-nptl-linux-gnu/sysroot/usr/lib/crti.o /home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../lib/gcc/i686-np
tl-linux-gnu/4.9.1/crtbegin.o -L/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../lib/gcc/i686-nptl-linux-gnu/4.9.1 -L/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_0
1/bin/../lib/gcc -L/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../lib/gcc/i686-nptl-linux-gnu/4.9.1/../../../../i686-nptl-linux-gnu/lib -L/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i6
86_nptl_gcc_x86_linux_01/bin/../i686-nptl-linux-gnu/sysroot/lib -L/home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../i686-nptl-linux-gnu/sysroot/usr/lib /tmp/ccQhQDLK.o -lgcc --as-need
ed -lgcc_s --no-as-needed -lc -lgcc --as-needed -lgcc_s --no-as-needed /home/kyoupark/STB_SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../lib/gcc/i686-nptl-linux-gnu/4.9.1/crtend.o /home/kyoupark/STB_
SW/deps/SYSTEM_BIN_3/i686_nptl_gcc_x86_linux_01/bin/../i686-nptl-linux-gnu/sysroot/usr/lib/crtn.o


={============================================================================
*kt_linux_gcc_400* gcc-build-spk-using-own-toolchain

{buildroot-version-used-in-spk}
file:///home/kyoupark/si-logs/spk/darwin-spk-1.19/docs/buildroot.html
$LastChangedDate: 2008-12-16 09:00:11 +0000 (Tue, 16 Dec 2008) $

<versions>
The nearest from buildroot.org
buildroot-2009.02-rc1.tar.bz2                      2009-01-16 19:35  7.0M

but checked buildroot-2009.02 and do not have glibc package

linux-2.6.18.tar.bz2     20-Sep-2006 03:56   40M  
linux-2.6.18.tar.gz      20-Sep-2006 03:56   50M  
linux-2.6.18.tar.sign    08-Aug-2013 19:27  665   
linux-2.6.18.tar.xz      20-Sep-2006 03:56   33M  

binutil GNU ld version 2.17.50-brcm 20070220

glibc-2.4.tar.bz2	          06-Mar-2006 06:30 	14M	 
glibc-2.4.tar.gz	          06-Mar-2006 06:30 	20M	 

glibc-2.8.tar.bz2	          2009-Feb-26 16:48:15	15.2M	application/x-bzip
glibc-2.8.tar.gz	          2009-Mar-10 17:05:47	20.4M	application/x-gzip


<spk-buildroot-to-use-external-toolchain>
// SPK itself is configured to use external toolchain.
toolchain/Config.in

config BR2_TOOLCHAIN_EXTERNAL
	bool "External binary toolchain"

source "toolchain/export-toolchain/Config.in"

toolchain/external-toolchain/Config.in

if BR2_TOOLCHAIN_EXTERNAL
config BR2_TOOLCHAIN_EXTERNAL_LIB_C
	string "The core C library from the external toolchain"
	default "libc.so.0"
	help
	  Specify the core C shared library found in the external
	  toolchain. This is required in addition to any other
	  libraries to be copied.

config BR2_TOOLCHAIN_EXTERNAL_LIBS
	string "Libraries to copy from the external toolchain"
	default "ld-uClibc.so.0 libcrypt.so.0 libdl.so.0 libgcc_s.so libm.so.0 libnsl.so.0 libpthread.so.0 libresolv.so.0 librt.so.0 libutil.so.0"
	help
	  A space separated list of the shared libraries to be copied
	  from the external toolchain into the root filesystem. Only
	  the top-level name is needed, i.e. libc.so, libpthread.so as
	  the actual shared library symlinked to will be copied also.

config BR2_TOOLCHAIN_EXTERNAL_STRIP
        bool
        default y
        prompt "Strip shared libraries"
	help
	  Strip shared libraries copied from the external toolchain.
endif

// The core C library from the external toolchain (BR2_TOOLCHAIN_EXTERNAL_LIB_C) 
// [libc.so.0] libc.so.0
// Libraries to copy from the external toolchain (BR2_TOOLCHAIN_EXTERNAL_LIBS) 
// [ld-uClibc.so.0 libcrypt.so.0 libdl.so.0 libgcc_s.so libm.so.0 libnsl.so.0
// libpthread.so.0 libresolv.so.0 librt.so.0 libutil.so.0] 
//

toolchain/dependencies/dependencies.mk
toolchain/dependencies/dependencies.sh

Checking build system dependencies:
BUILDROOT_DL_DIR clean:				Ok
CC clean:					Ok
CXX clean:					Ok
CPP clean:					Ok
CFLAGS clean:					Ok
INCLUDES clean:					Ok
CXXFLAGS clean:					Ok
which installed:				Ok
sed works:					Ok (/bin/sed)
GNU make version '3.81':			Ok
C compiler '/usr/lib64/ccache/gcc'
C compiler version '4.4.7':			Ok
C++ compiler '/usr/lib64/ccache/g++'
C++ compiler version '4.4.7':			Ok
awk installed:					Ok
bison installed:				Ok
flex installed:					Ok
gettext installed:				Ok
makeinfo installed:				Ok
Build system dependencies:			Ok

toolchain/external/ext-tool.mk

uclibc: dependencies $(TARGET_DIR)/lib/$(strip $(subst ",, $(BR2_TOOLCHAIN_EXTERNAL_LIB_C)))

$(TARGET_DIR)/lib/$(strip $(subst ",, $(BR2_TOOLCHAIN_EXTERNAL_LIB_C))):
#"))
	mkdir -p $(TARGET_DIR)/lib
	@$(call copy_toolchain_lib_root, $(strip $(subst ",, $(BR2_TOOLCHAIN_EXTERNAL_LIB_C))), /lib, $(BR2_TOOLCHAIN_EXTERNAL_STRIP))
#")))
	for libs in $(strip $(subst ",, $(BR2_TOOLCHAIN_EXTERNAL_LIBS))); do \
		$(call copy_toolchain_lib_root, $$libs, /lib, $(BR2_TOOLCHAIN_EXTERNAL_STRIP)); \
	done
	$(call copy_toolchain_sysroot)


<spk-patches>
file://///kit-debian/anonymous/si-logs/spk/darwin-spk-1.19/docs/patches.html

How the patching is done

Patches are applied in buildroot by running a shell script called
toolchain/patch-kernel.sh with three arguments. The first argument is the
target directory where the source code to be patched is saved. The second
argument is the directory where the patch is saved. The third argument is the
filename pattern to match when looking in the patch directory. The third
argument can include wildcards to select multiple patch files. 


+ ./scripts/spk-apply-patches.sh
toolchain/patch-kernel.sh /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/linux-2.6.18.8 \
  toolchain/kernel-headers \
  linux-2.6.18.8-\*.patch{,.gz,.bz2}

if [ -d target/device/Sky/kernel-patches ] ; then \
	toolchain/patch-kernel.sh /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/linux-2.6.18.8 \
    target/device/Sky/kernel-patches \
	 linux-2.6.18.8\*.patch{,.gz,.bz2} ; \
fi

# Apply third-party patches common for all architectures
# Specific to a particular Kernel Linux 2.6 Version e.g. 2.6.18.8
toolchain/patch-kernel.sh /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/linux-2.6.18.8 \
  target/device/Sky/kernel-patches/third-party \
  linux-2.6.18.8\*.patch{,.gz,.bz2}

# Apply common hardening patches
toolchain/patch-kernel.sh /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/linux-2.6.18.8 \
  target/device/Sky/kernel-patches/hardening \
  linux-2.6.18.8\*.patch{,.gz,.bz2}

# Apply architecture-specific third-party patches
toolchain/patch-kernel.sh /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/linux-2.6.18.8 target/device/Sky/kernel-patches/third-party/arch/"mips" \
		linux-2.6.18.8\*.patch{,.gz,.bz2}

# Apply architecture-specific hardening patches
toolchain/patch-kernel.sh /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/linux-2.6.18.8 target/device/Sky/kernel-patches/hardening/arch/"mips" \
		linux-2.6.18.8\*.patch{,.gz,.bz2}

# Board-specific patches are applied by the code below
touch /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/linux-2.6.18.8/.patched
touch /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/linux-2.6.18.8/.patched.arch
if [ -d target/device/Sky/ams-drx890/kernel-patches/ ] ; then \
	toolchain/patch-kernel.sh /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/linux-2.6.18.8 target/device/Sky/ams-drx890/kernel-patches/ \
		linux-2.6.18.8\*.patch{,.gz,.bz2} ; \
	fi

# Allow busybox patches.
toolchain/patch-kernel.sh /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/busybox-1.13.1 \
  package/busybox busybox-1.13.1-\*.patch


<spk-uclibc>
Where does libuClibc-0.9.29.so come from when it doesn't get built? From patch
file and means that it's `delivered as a binary`

~/si-logs/spk/darwin-spk-1.19/target/device/Sky/ams-drx890/rootfs/debug/lib$ ls -al
drwxr-xr-x  3 kyoupark ccusers 4096 Oct 14  2013 .
drwxr-xr-x 16 kyoupark ccusers 4096 Oct 14  2013 ..
-rw-r--r--  1 kyoupark ccusers    0 Oct 14  2013 .empty
drwxr-xr-x  3 kyoupark ccusers 4096 Oct 14  2013 modules

/home/kyoupark/si-logs/spk/darwin-spk-1.19/patches
-rw-r--r--  1 kyoupark ccusers 2535439 Mar 15  2016 darwin-spk-1.19-0001-drx890-uclibc-update-v3.patch

WARNING: patch 'patches/darwin-spk-1.19-0001-drx890-uclibc-update-v3.patch' can be applied -p0 or -p1. Using -p0.
Applying patch 'patches/darwin-spk-1.19-0001-drx890-uclibc-update-v3.patch'...
patching file target/device/Sky/ams-drx890/rootfs/debug/lib/libuClibc-0.9.29.so
patching file target/device/Sky/ams-drx890/rootfs/preprod/lib/libuClibc-0.9.29.so
patching file target/device/Sky/ams-drx890/rootfs/prod/lib/libuClibc-0.9.29.so


<spk-rootfs>
mkdir -p /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root
if ! [ -d "/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root/bin" ]; then \
		if [ -d "target/device/Sky/ams-drx890/rootfs/"debug"" ]; then \
			cp -fa target/device/Sky/ams-drx890/rootfs/"debug"/* \
         /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root/; \
		fi; \
		touch /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/.fakeroot.00000; \
	fi


<spk-output-to-test-without-changes>
mkdir /home/nds-uk/kyoupark/spk-out
cd /home/nds-uk/kyoupark/STB_SW/FUSIONOS_INPUT_DEL_2/DEL_AMS_BCM_MIPS4K_LNUX_DARWIN_01/build_platform/spk/darwin-spk-1.19
./configure O=/home/nds-uk/kyoupark/spk-out ams-drx890 debug initramfs otv fullstack noconsole |& tee ext.log
make O=/home/nds-uk/kyoupark/spk-out rootfs-clean
make O=/home/nds-uk/kyoupark/spk-out |& tee ext.log
find /home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/ -type d | xargs chmod 755


{spk-changes-to-use-own-toolchain}
// make changes to SPK to use own toolchain, tar it, and copy it to where SPK
// is. So overwrite SPK with modified one and the build system will use the
// modified one.

cd /home/nds-uk/kyoupark/si_logs/spk
tar cjvf darwin-spk-1.19-20131014.tar.bz2 darwin-spk-1.19-use-own
tar cjvf darwin-spk-1.19-20131014.tar.bz2 darwin-spk-1.19
cp darwin-spk-1.19-20131014.tar.bz2 ~/STB_SW/FUSIONOS_INPUT_DEL_2/DEL_AMS_BCM_MIPS4K_LNUX_DARWIN_01/build_platform/spk/
// cd /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_INPUT_DEL_2/DEL_AMS_BCM_MIPS4K_LNUX_DARWIN_01/build_platform/spk


*buildroot-debug.config* target/device/Sky/ams-drx890/buildroot-debug.config

// BR3_TOOLCHAIN_EXTERNAL_LIB_C="libc.so.0"
// BR2_TOOLCHAIN_EXTERNAL_LIBS="ld-uClibc.so.0 libcrypt.so.0 libdl.so.0 libgcc_s.so libm.so.0 libnsl.so.0 libpthread.so.0 libresolv.so.0 librt.so.0 libutil.so.0"
// BR2_TOOLCHAIN_EXTERNAL_PATH="/home/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2"
// BR2_TOOLCHAIN_EXTERNAL_PREFIX="$(ARCH)-linux"

BR2_TOOLCHAIN_EXTERNAL_LIB_C="libc.so.0"
BR2_TOOLCHAIN_EXTERNAL_LIBS="libatomic.so.* libc.so.* libcrypt.so.* libdl.so.* libgcc_s.so.* libm.so.* libnsl.so.* libresolv.so.* librt.so.* libutil.so.* ld*.so.* libpthread.so.* libnss_files.so.* libnss_dn s.so.* libmvec.so.* libstdc++.so.*"
BR2_TOOLCHAIN_EXTERNAL_PATH="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-10-install"
BR2_TOOLCHAIN_EXTERNAL_PREFIX="mips-linux-gnu"


*toolchain/external-toolchain/ext-tool.mk*
darwin-spk-1.19-use-own/toolchain/external-toolchain/ext-tool.mk

this file do two things:

1. copy libraries to target directory.

copy_toolchain_lib_root = \

copy_toolchain_lib_root lib=libc.so.0 dst=/lib
SKIPPING copy_toolchain_lib_root lib=libuClibc-0.9.29.so dst=/lib
for libs in ld-uClibc.so.0 libcrypt.so.0 libdl.so.0 libgcc_s.so libm.so.0 libnsl.so.0 libpthread.so.0 libresolv.so.0 librt.so.0 libutil.so.0; do \
		LIB="$libs"; DST="/lib"; STRIP="y"; SYSROOT="" ; if test -z y; 
      then PREFIX=`/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc -v 2>&1 | grep ^Configured | tr " " "\n" | grep -- "--prefix" | head -1 | cut -f2 -d=`; SYSROOT_DIR=`echo ""/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2"" | sed -e "s|\(.*\)${PREFIX}|\1|"` ; SYSROOT="--sysroot ${SYSROOT_DIR}" ; fi ; LIB_DIR=`/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc ${SYSROOT} -print-file-name=${LIB} | sed -e "s,${LIB}\$,,"`; if test -z "${LIB_DIR}"; then echo "copy_toolchain_lib_root: lib=${LIB} not found"; exit -1; fi; if test -z ; then LINKS="-type l"; fi; LIB="$libs"; for FILE in `find ${LIB_DIR} -maxdepth 1 ${LINKS} -name "${LIB}*"`; do LIB=`basename ${FILE}`; while test \! -z "${LIB}"; do if test -f /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root${DST}/${LIB}; then echo "SKIPPING copy_toolchain_lib_root lib=${LIB} dst=${DST}"; chmod +x "/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root${DST}/${LIB}"; case "${STRIP}" in (0 | n | no) ;; (*) /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-strip "/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root${DST}/${LIB}"; ;; esac; else rm -fr /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root${DST}/${LIB}; mkdir -p /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root${DST}; echo "copy_toolchain_lib_root lib=${LIB} dst=${DST}"; if test -h ${LIB_DIR}/${LIB}; then cp -d ${LIB_DIR}/${LIB} /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root${DST}/; elif test -f ${LIB_DIR}/${LIB}; then /usr/bin/install -D -m0755 ${LIB_DIR}/${LIB} /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root${DST}/${LIB}; case "${STRIP}" in (0 | n | no) ;; (*) /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-strip "/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root${DST}/${LIB}"; ;; esac; else exit -1; fi; fi; LIB="`readlink ${LIB_DIR}/${LIB}`"; done; done; echo -n; \
	done
copy_toolchain_lib_root lib=ld-uClibc.so.0 dst=/lib
copy_toolchain_lib_root lib=ld-uClibc-0.9.29.so dst=/lib
copy_toolchain_lib_root lib=libcrypt.so.0 dst=/lib
...

// logs
+ LIB=libc.so.0
+ DST=/lib
+ STRIP=y
+ SYSROOT=
++ pwd
+ echo 'KT: pwd=/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_INPUT_DEL_2/DEL_AMS_BCM_MIPS4K_LNUX_DARWIN_01/build_platform/spk/darwin-spk-1.19'

// do check if it really exist 
++ find /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib/ -maxdepth 1 -type l -name 'libc.so.0*'

// libc.so.0 is not exist
// ${TARGET_DIR} /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root
+ test -f /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root/lib/libc.so.0
+ rm -fr /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root/lib/libc.so.0
+ mkdir -p /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root/lib

+ mkdir -p /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root/lib
+ echo 'copy_toolchain_lib_root lib=libc.so.0 dst=/lib'
+ test -h /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib//libc.so.0
+ cp -d /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib//libc.so.0 \
    /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/`project_build_mips/ams-drx890/root/lib/`

cp -d /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib//libc.so.0 ~/yy

// libc.so.0 is a symbolic link to libuClibc-0.9.29.so. Here copies only a link
// but not actual file and skips libuClibc-0.9.29.so. Means that it's already
// copied and there.
++ readlink /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib//libc.so.0
+ LIB=libuClibc-0.9.29.so
+ test '!' -z libuClibc-0.9.29.so
+ test -f /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/project_build_mips/ams-drx890/root/lib/libuClibc-0.9.29.so
+ echo 'SKIPPING copy_toolchain_lib_root lib=libuClibc-0.9.29.so dst=/lib'

SO need to se set LIB_DIR since the own toolchain do not support
"--print-file-name0" which has no outpus.

// set ${LIB_DIR}
// $ /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc --print-file-name=libc.so.0
// /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib/libc.so.0
// $ /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc --print-file-name=libc.so.0 |sed -e "s,libc.so.0,,"
// /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib/
// so:
// /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/mips-linux-uclibc/lib


2. copy whole toolchain to staging directory.  

// logs. ${SYSROOT_DIR}
+ echo "/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2" | sed -e "s|\(.*\)${PREFIX}|\1|"
+ /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2

+ echo -e "\nCopying toolchain files from $${SYSROOT_DIR}\n" ;\
+ Copying toolchain files from /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2

+ cp -a /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/info /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/lib /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/libexec /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/man /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mipsel-linux /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mipsel-linux-uclibc /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux-uclibc /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/nds /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/out.txt /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/
+ xargs chmod 755
+ find /home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir -type d

// SYSROOT sets null when use --with-sysroot since no match from gcc -v:

// <brcm-mips-gcc-spec>
// $ /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc -v 2>&1 | grep ^Configured | tr " " "\n"
// 3:Configured
// with:
// ../gcc-4.2.0-20070124/configure
// --prefix=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/
// --build=mips-linux
// --host=mips-linux
// --target=mips-linux-uclibc
// --with-build-sysroot=/usr/src/redhat/BUILD/build_uClibc
// --enable-languages=c,c++
// --disable-__cxa_atexit
// --enable-target-optspace
// --with-gnu-ld
// --with-float=hard
// --enable-threads
// --infodir=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/info
// --mandir=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/man
// --with-arch=mips32
// --disable-libmudflap
// --disable-nls
// --with-gnu-plts

SO set SYSROOT and exit condition to prevent it from copying from "/*".


--- darwin-spk-1.19/toolchain/external-toolchain/ext-tool.mk    2017-02-17 08:16:54.494561690 +0000
+++ darwin-spk-1.19-use-own/toolchain/external-toolchain/ext-tool.mk    2017-04-07 12:29:00.204730854 +0100
@@ -6,6 +6,7 @@
 # $3: strip (y|n)       default is to strip
 #
 copy_toolchain_lib_root = \
+       set -x; \
        LIB="$(strip $1)"; \
        DST="$(strip $2)"; \
        STRIP="$(strip $3)"; \
@@ -17,7 +18,8 @@ copy_toolchain_lib_root = \
                SYSROOT="--sysroot $${SYSROOT_DIR}" ;\
        fi ;\
  \
-       LIB_DIR=`$(TARGET_CC) $${SYSROOT} -print-file-name=$${LIB} | sed -e "s,$${LIB}\$$,,"`; \
+       LIB_DIR="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/mips-linux-gnu/lib"; \
+       echo "KT $${LIB_DIR}"; \
  \
        if test -z "$${LIB_DIR}"; then \
                echo "copy_toolchain_lib_root: lib=$${LIB} not found"; \
@@ -89,6 +91,7 @@ copy_toolchain_lib_root = \
 #      echo "SYSROOT_DIR = $${SYSROOT_DIR}" ;\

 copy_toolchain_sysroot = \
+       set -x; \
        SYSROOT_DIR=`$(TARGET_CC) -v 2>&1 | grep ^Configured | tr " " "\n" | grep -- "--with-sysroot" | head -1 | cut -f2 -d=`; \
        if [ ! -z "$${SYSROOT_DIR}" ]; then \
                SYSROOT_DIR=`echo "$${SYSROOT_DIR}/" | sed -e "s|//|/|g"` ;\
@@ -96,11 +99,13 @@ copy_toolchain_sysroot = \
        fi ;\
  \
        if [ -z "$${SYSROOT_DIR}" ]; then \
-               PREFIX=`$(TARGET_CC) -v 2>&1 | grep ^Configured | tr " " "\n" | grep -- "--prefix" | head -1 | cut -f2 -d=`; \
-               SYSROOT_DIR=`echo "$(BR2_TOOLCHAIN_EXTERNAL_PATH)" | sed -e "s|\(.*\)$${PREFIX}|\1|"` ;\
+               SYSROOT_DIR=`$(TARGET_CC) -v 2>&1 | grep ^Configured | tr " " "\n" | grep -- "--prefix" | head -1 | cut -f2 -d=`; \
+               if [ -z "$${SYSROOT_DIR}" ]; then \
+                       exit 1; \
+               fi ;\
        fi ;\
  \
-       echo -e "\nCopying toolchain files from $${SYSROOT_DIR}\n" ;\
+       echo -e "\n KT: Copying toolchain files from $${SYSROOT_DIR}\n" ;\
        cp -a $${SYSROOT_DIR}/* $(STAGING_DIR)/ ; \
        find $(STAGING_DIR) -type d | xargs chmod 755 ;\


*make_image.sh*
// which builds spk
// 1. set output directory
// 2. set to use own toolchain

#!/bin/bash -x

# Set up SPK variables
function set_up_spk() {
  # KT: SPK_BUILD_OUTPUT_DIR=${OUTPUT}/${PROJECT}/SPK/${SPK_PLATFORM}-${SPK_VARIANT}
  SPK_BUILD_OUTPUT_DIR=/home/nds-uk/kyoupark/spk-out-use-own-brcm

  # KT
  ## nds_si_devel profile does not boot up with noconsole - but not required as of Nov '09.
  #if [ "${PROFILE}" ==  "sky_prod" ] || [ "${PROFILE}" ==  "sky_tpt" ] || [ "${PROFILE}" ==  "sky_trials" ] || [ "${PROFILE}" ==  "mem_trials" ] || [ "${PROFILE}" ==  "cust_trials" ]; then
  #  SPK_CONFIG+=" noconsole"
  #fi
  SPK_CONFIG+=" console"
}

# Configure SPK
function configure_spk() {

  ## Set toolchain path
  # sed -i "s?.*\(BR2_TOOLCHAIN_EXTERNAL_PATH\).*?\1=\"$NDS_FUSIONOS_TOOLCHAIN_PATH\"?" target/device/Sky/$SPK_PLATFORM/buildroot-${SPK_VARIANT}.config

  # note: NDS_FUSIONOS_TOOLCHAIN_PATH is not defined here.
  # configure scrip will cp so change config script directly instead rather
  # than change it here until know how to do these in here.
  # note: this do not work. why?
  NDS_FUSIONOS_TOOLCHAIN_PATH = "/home/nds-uk/kyoupark/si_logs/brcm/crosstools" 
  
  # Configure the SPK
  ./configure O=${SPK_BUILD_OUTPUT_DIR} $SPK_PLATFORM $SPK_VARIANT $SPK_CONFIG
}


*configure*
To allow changes in configure and to add V=1 to make command.

 # if [ $? -ne 0 ]; then
 #    echo "SPK patches failed to apply. Aborting."
 #    echo ""
 #    reset_tmpdir
 #    exit 1
 # fi

 # new_md5=`md5sum $0 | cut -d' ' -f1`
 # if [ "$original_md5" != "$new_md5" ]; then
 #    echo "Configure script modified by patch, aborting and restarting.. "
 #    echo "Calling  $0 with \"$ARGS\""
 #    exec $0 $ARGS
 #    reset_tmpdir
 #    exit 0
 # fi

# add V=1 to all make
make V=1 O=$OBJDIR SPK_CONFIGURED= oldconfig


*errors-when-build-spk-with-own-toolchain*

<error> in kernel build due to asm errors when use own built gcc
// brcm tool builds okay but 4.8.2/4.9.2 builds fails.
// fails to output assembler code but preprocessing result is the same as
// working case. So gcc difference is a cause

/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-10-install/bin/mips-linux-gnu-gcc \
-Wp,-MD,arch/mips/kernel/.asm-offsets.s.d  -nostdinc -isystem \
/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-10-install/bin/../lib/gcc/mips-linux-gnu/4.8.2/include \
-D__KERNEL__ -Iinclude  -include include/linux/autoconf.h -Wall -Wundef \
-Wstrict-prototypes -Wno-trigraphs -fno-strict-aliasing -fno-common -Os \
-mabi=32 -G 0 -mno-abicalls -fno-pic -pipe -msoft-float -ffreestanding \
-march=mips32 -Wa,-mips32 -Wa,--trap -Wno-uninitialized \
-Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic \
-fomit-frame-pointer -g  -fno-stack-protector -Wdeclaration-after-statement \
-Wno-pointer-sign   -Os \
-I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include \
-I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include \
-I/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include \
-D"KBUILD_STR(s)=#s" -D"KBUILD_BASENAME=KBUILD_STR(asm_offsets)" \
-D"KBUILD_MODNAME=KBUILD_STR(asm_offsets)" -fverbose-asm -S -o asm-offsets.s arch/mips/kernel/asm-offsets.c

// to use -E
/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-10-install/bin/mips-linux-gnu-gcc \
-Wp,-MD,arch/mips/kernel/.asm-offsets.s.d  -nostdinc -isystem \
/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-mips-10-install/bin/../lib/gcc/mips-linux-gnu/4.8.2/include \
-D__KERNEL__ -Iinclude  -include include/linux/autoconf.h -Wall -Wundef \
-Wstrict-prototypes -Wno-trigraphs -fno-strict-aliasing -fno-common -Os \
-mabi=32 -G 0 -mno-abicalls -fno-pic -pipe -msoft-float -ffreestanding \
-march=mips32 -Wa,-mips32 -Wa,--trap -Wno-uninitialized \
-Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic \
-fomit-frame-pointer -g  -fno-stack-protector -Wdeclaration-after-statement \
-Wno-pointer-sign   -Os \
-I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include \
-I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include \
-I/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include \
-D"KBUILD_STR(s)=#s" -D"KBUILD_BASENAME=KBUILD_STR(asm_offsets)" \
-D"KBUILD_MODNAME=KBUILD_STR(asm_offsets)" -fverbose-asm -E arch/mips/kernel/asm-offsets.c

/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc \
-Wp,-MD,arch/mips/kernel/.asm-offsets.s.d  -nostdinc -isystem \
/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/include \
-D__KERNEL__ -Iinclude  -include include/linux/autoconf.h -Wall -Wundef \
-Wstrict-prototypes -Wno-trigraphs -fno-strict-aliasing -fno-common -Os \
-mabi=32 -G 0 -mno-abicalls -fno-pic -pipe -msoft-float -ffreestanding \
-march=mips32 -Wa,-mips32 -Wa,--trap -Wno-uninitialized \
-Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic \
-fomit-frame-pointer -g  -fno-stack-protector -Wdeclaration-after-statement \
-Wno-pointer-sign   -Os \
-I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include \
-I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include \
-I/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include \
-D"KBUILD_STR(s)=#s" -D"KBUILD_BASENAME=KBUILD_STR(asm_offsets)" \
-D"KBUILD_MODNAME=KBUILD_STR(asm_offsets)" -fverbose-asm -S -o \
arch/mips/kernel/asm-offsets.s arch/mips/kernel/asm-offsets.c

/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc \
-Wp,-MD,arch/mips/kernel/.asm-offsets.s.d  -nostdinc -isystem \
/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/include \
-D__KERNEL__ -Iinclude  -include include/linux/autoconf.h -Wall -Wundef \
-Wstrict-prototypes -Wno-trigraphs -fno-strict-aliasing -fno-common -Os \
-mabi=32 -G 0 -mno-abicalls -fno-pic -pipe -msoft-float -ffreestanding \
-march=mips32 -Wa,-mips32 -Wa,--trap -Wno-uninitialized \
-Iinclude/asm-mips/mach-brcmstb -Iinclude/asm-mips/mach-generic \
-fomit-frame-pointer -g  -fno-stack-protector -Wdeclaration-after-statement \
-Wno-pointer-sign   -Os \
-I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include \
-I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include \
-I/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include \
-D"KBUILD_STR(s)=#s" -D"KBUILD_BASENAME=KBUILD_STR(asm_offsets)" \
-D"KBUILD_MODNAME=KBUILD_STR(asm_offsets)" -fverbose-asm -E arch/mips/kernel/asm-offsets.c

// #define constant(string, member) \
// 	__asm__("\n@@@" string "%X0" : : "ri" (member))
// 
// void output_mm_defines(void)
// {
// 	text("/* Size of struct page  */");
// 	size("#define STRUCT_PAGE_SIZE   ", struct page);
// 	linefeed;
// 	text("/* Linux mm_struct offsets. */");
// 	offset("#define MM_USERS      ", struct mm_struct, mm_users);
// 	offset("#define MM_PGD        ", struct mm_struct, pgd);
// 	offset("#define MM_CONTEXT    ", struct mm_struct, context);
// 	linefeed;
// 	constant("#define _PAGE_SIZE     ", PAGE_SIZE);
// }

// processed-output from -E
//
// void output_mm_defines(void)
// {
//  __asm__("\n@@@" "/* Size of struct page  */");
//  __asm__("\n@@@" "#define STRUCT_PAGE_SIZE   " "%0" : : "i" (sizeof(struct page)));
//  __asm__("\n@@@" "");
//  __asm__("\n@@@" "/* Linux mm_struct offsets. */");
//  __asm__("\n@@@" "#define MM_USERS      " "%0" : : "i" ((&(((struct mm_struct *)((void *)0))->mm_users))));
//  __asm__("\n@@@" "#define MM_PGD        " "%0" : : "i" ((&(((struct mm_struct *)((void *)0))->pgd))));
//  __asm__("\n@@@" "#define MM_CONTEXT    " "%0" : : "i" ((&(((struct mm_struct *)((void *)0))->context))));
//  __asm__("\n@@@" "");
//  __asm__("\n@@@" "#define _PAGE_SIZE     " "%X0" : : "ri" ((1UL << 12)));
// }

arch/mips/kernel/asm-offsets.c: In function 'output_mm_defines':
arch/mips/kernel/asm-offsets.c:25:2: error: invalid 'asm': invalid use of '%X'
  __asm__("\n@@@" string "%X0" : : "ri" (member))
  ^
arch/mips/kernel/asm-offsets.c:229:2: note: in expansion of macro 'constant'
  constant("#define _PGD_T_LOG2    ", PGD_T_LOG2);
  ^
arch/mips/kernel/asm-offsets.c:25:2: error: invalid 'asm': invalid use of '%X'
  __asm__("\n@@@" string "%X0" : : "ri" (member))
  ^
arch/mips/kernel/asm-offsets.c:230:2: note: in expansion of macro 'constant'
  constant("#define _PMD_T_LOG2    ", PMD_T_LOG2);
  ^
arch/mips/kernel/asm-offsets.c:25:2: error: invalid 'asm': invalid use of '%X'
  __asm__("\n@@@" string "%X0" : : "ri" (member))
  ^
arch/mips/kernel/asm-offsets.c:231:2: note: in expansion of macro 'constant'
  constant("#define _PTE_T_LOG2    ", PTE_T_LOG2);
  ^

change patch as:
target/device/Sky/kernel-patches/third-party/arch/mips/linux-2.6.18.8-000-kernel.org-to-mips.org.patch

diff -Naur linux-2.6.18.8-kernel.org/arch/mips/kernel/asm-offsets.c linux-2.6.18.8-mips.org/arch/mips/kernel/asm-offsets.c
--- linux-2.6.18.8-kernel.org/arch/mips/kernel/asm-offsets.c	2007-02-23 23:52:30.000000000 +0000
+++ linux-2.6.18.8-mips.org/arch/mips/kernel/asm-offsets.c	2007-03-10 12:05:57.000000000 +0000
@@ -22,7 +22,7 @@
 #define offset(string, ptr, member) \
 	__asm__("\n@@@" string "%0" : : "i" (_offset(ptr, member)))
 #define constant(string, member) \
-	__asm__("\n@@@" string "%x0" : : "ri" (member))
# +	__asm__("\n@@@" string "%X0" : : "ri" (member))
+	__asm__("\n@@@" string "%0" : : "ri" (member))
 #define size(string, size) \
 	__asm__("\n@@@" string "%0" : : "i" (sizeof(size)))
 #define linefeed text("")

cd /home/nds-uk/kyoupark/spk-out-use-own-brcm/project_build_mips/ams-drx890/linux-2.6.18.8
/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2 arch/mips/kernel/asm-offsets.s arch/mips/kernel/asm-offsets.c


2.this error happens as well when use normal spk

patching file ./target/device/Sky/ams-drx890/kernel-patches/linux-2.6.18.8-045-drx890-td_update_full_name.patch
Unable to apply patch 'patches/darwin-spk-1.19-0023-drx890-um-fs-helper-and-klog2hdd-on-panic-reboot.patch'


<error> ucmpdi2-error. see *gcc-int-ucmpdi2-error*

arch/mips/math-emu/built-in.o: In function `ieee754dp_format':
(.text+0x2ea4): undefined reference to `__ucmpdi2'
arch/mips/math-emu/built-in.o: In function `ieee754dp_format':
(.text+0x2ea4): relocation truncated to fit: R_MIPS_26 against `__ucmpdi2'
arch/mips/math-emu/built-in.o: In function `ieee754dp_mul':
(.text+0x47b8): undefined reference to `__ucmpdi2'
arch/mips/math-emu/built-in.o: In function `ieee754dp_mul':
(.text+0x47b8): relocation truncated to fit: R_MIPS_26 against `__ucmpdi2'
arch/mips/math-emu/built-in.o: In function `ieee754dp_mul':
(.text+0x47f8): undefined reference to `__ucmpdi2'
arch/mips/math-emu/built-in.o: In function `ieee754dp_mul':
(.text+0x47f8): relocation truncated to fit: R_MIPS_26 against `__ucmpdi2'
arch/mips/math-emu/built-in.o: In function `ieee754dp_sub':
(.text+0x4de0): undefined reference to `__ucmpdi2'
arch/mips/math-emu/built-in.o: In function `ieee754dp_sub':
(.text+0x4de0): relocation truncated to fit: R_MIPS_26 against `__ucmpdi2'
arch/mips/math-emu/built-in.o: In function `ieee754dp_sub':
(.text+0x4e6c): undefined reference to `__ucmpdi2'
arch/mips/math-emu/built-in.o: In function `ieee754dp_sub':
(.text+0x4e6c): relocation truncated to fit: R_MIPS_26 against `__ucmpdi2'
arch/mips/math-emu/built-in.o:(.text+0x5548): more undefined references to `__ucmpdi2' follow
arch/mips/math-emu/built-in.o: In function `ieee754dp_add':
(.text+0x5548): relocation truncated to fit: R_MIPS_26 against `__ucmpdi2'
arch/mips/math-emu/built-in.o: In function `ieee754dp_add':
(.text+0x55d4): relocation truncated to fit: R_MIPS_26 against `__ucmpdi2'
make[1]: *** [.tmp_vmlinux1] Error 1
make[1]: Leaving directory `/home/NDS-UK/kyoupark/ext3/project_build_mips/ams-drx890/linux-2.6.18.8'
make: *** [/home/nds-uk/kyoupark/ext3/project_build_mips/ams-drx890/linux-2.6.18.8/vmlinux] Error 2


note: these two errors forces to use gcc 4.2.0 


<error> LIBNL
note:
it seems to vary on which builds fails depending on gcc used.

#BR2_PACKAGE_LIBNL=y

  DEP doc.c
  MAKE libnl.a
make[2]: *** [all] Error 2
make[1]: *** [all] Error 2
make[1]: Leaving directory `/home/NDS-UK/kyoupark/spk-out-use-own-brcm/build_mips/libnl-1'
make: *** [/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/libnl-1/lib/libnl.a] Error 2


<error> PATH_MAX
#BR2_PACKAGE_NFS_UTILS=y

make[3]: Entering directory `/home/NDS-UK/kyoupark/ext4/build_mips/nfs-utils-1.0.10/support/nfs'

xcommon.c: In function 'canonicalize':
xcommon.c:106: error: 'PATH_MAX' undeclared (first use in this function)
xcommon.c:106: error: (Each undeclared identifier is reported only once
xcommon.c:106: error: for each function it appears in.)
xcommon.c:106: warning: unused variable 'canonical'


/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc -DHAVE_CONFIG_H -I. -I. -I../../support/include  -I../../support/include -D_FILE_OFFSET_BITS=64 -D_GNU_SOURCE -Wall  -pipe -Os  -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/usr/include -I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include  -DUTS_RELEASE='".."' -MT xcommon.o -MD -MP -MF ".deps/xcommon.Tpo" -c -o xcommon.o xcommon.c
/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc -DHAVE_CONFIG_H -I. -I. -I../../support/include  -I../../support/include -D_FILE_OFFSET_BITS=64 -D_GNU_SOURCE -Wall  -pipe -Os  -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/usr/include -I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include  -DUTS_RELEASE='".."' -E -dN xcommon.c

/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -DHAVE_CONFIG_H -I. -I. -I../../support/include  -I../../support/include -D_FILE_OFFSET_BITS=64 -D_GNU_SOURCE -Wall  -pipe -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include  -DUTS_RELEASE='".."' -E -dN -dI xcommon.c > out4.txt
/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -DHAVE_CONFIG_H -I. -I. -I../../support/include  -I../../support/include -D_FILE_OFFSET_BITS=64 -D_GNU_SOURCE -Wall  -pipe -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include  -DUTS_RELEASE='".."' -MT xcommon.o -MD -MP -MF ".deps/xcommon.Tpo" -c -o xcommon.o xcommon.c

make /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/lib/gcc/mips-linux-gnu/4.2.0/include/limits.h the same as
/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/lib/gcc/mips-linux-uclibc/4.2.0/include/limits.h

diff /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/lib/gcc/mips-linux-gnu/4.2.0/include/limits.h /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/lib/gcc/mips-linux-uclibc/4.2.0/include/limits.h
cp /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/lib/gcc/mips-linux-uclibc/4.2.0/include/limits.h /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/lib/gcc/mips-linux-gnu/4.2.0/include/limits.h


<error> ARG_MAX
make[2]: Entering directory `/home/NDS-UK/kyoupark/spk-out-use-own-brcm/build_mips/netkit-rsh-0.17/rshd'
rshd.c: In function 'doit':
rshd.c:342: error: 'ARG_MAX' undeclared (first use in this function)
rshd.c:342: error: (Each undeclared identifier is reported only once
rshd.c:342: error: for each function it appears in.)

/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc  -O2 -Dsocklen_t=size_t -D_GNU_SOURCE -Wall -W -Wpointer-arith -Wbad-function-cast -Wcast-qual -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wnested-externs -Winline  -E -dN -dI rshd.c -c > out4.txt
~/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-uclibc-gcc -O2 -Dsocklen_t=size_t -D_GNU_SOURCE -Wall -W -Wpointer-arith -Wbad-function-cast -Wcast-qual -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wnested-externs -Winline  -E -dN -dI rshd.c -c > out4.txt

/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include/bits/local_lim.h
/* Have to remove ARG_MAX?  */
#ifdef __undef_ARG_MAX
# undef ARG_MAX
# undef __undef_ARG_MAX
#endif

/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/mips-linux-uclibc/include/bits/local_lim.h
not have

so removes that from a header. Q: this header is not part of linux header?
Where does it come from and hot does it get created?


<error> this is not part of toolchain and is in libc(uclibc or glibc). nds
specific. remove it from spk.

mkdir -p /home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/nds/ifnameindex
cp -rv .//package/sky/nds/ifnameindex/* /home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/nds/ifnameindex
`.//package/sky/nds/ifnameindex/if_nameindex.c' -> `/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/nds/ifnameindex/if_nameindex.c'
`.//package/sky/nds/ifnameindex/ifnameindex.mk' -> `/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/nds/ifnameindex/ifnameindex.mk'
`.//package/sky/nds/ifnameindex/if_nameindex-test.c' -> `/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/nds/ifnameindex/if_nameindex-test.c'
`.//package/sky/nds/ifnameindex/Makefile' -> `/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/nds/ifnameindex/Makefile'
`.//package/sky/nds/ifnameindex/README.txt' -> `/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/nds/ifnameindex/README.txt'

/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc
-Os
-I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/include
-I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/usr/include
-I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include
-L/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/lib
-L/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/usr/lib
-shared -ldl -lpthread -o libifnameindex.so if_nameindex.o


make[1]: Entering directory `/home/NDS-UK/kyoupark/spk-out-use-own-brcm/build_mips/nds/ifnameindex'
/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/
build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/
nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include -c -fPIC -pthread -o if_nameindex.o if_nameindex.c
if_nameindex.c:38:27: error: libc-internal.h: No such file or directory
if_nameindex.c:39:28: error: bits/stackinfo.h: No such file or directory
if_nameindex.c:77:3: warning: #warning unknown stack
make[1]: *** [if_nameindex.o] Error 1

/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc -Os  -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/usr/include -I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include -Os  -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/usr/include -I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include -c -fPIC -pthread -dN -dI if_nameindex.c > out.txt
/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc -Os  -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/usr/include -I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include -Os  -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/STB_SW/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/picasso/SPK/ams-drx890-debug/build_mips/staging_dir/usr/include -I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include -c -fPIC -pthread -o if_nameindex.o if_nameindex.c


<error> STRACE
#BR2_PACKAGE_STRACE=y


<spk-image-genarate>
pushd /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_INPUT_DEL_2/DEL_AMS_BCM_MIPS4K_LNUX_DARWIN_01/build_platform/image_generation
./generate-otv-image.sh /home/nds-uk/kyoupark/spk-out-use-own-brcm/binaries/ams-drx890/vmlinux -E 4.X
./generate-otv-image.sh /home/nds-uk/kyoupark/spk-out-use-own-brcm/project_build_mips/ams-drx890/linux-2.6.18.8/vmlinux -E 4.X


*errors-when-run-image-built-with-own-toolchain*

<error> from busybox init when boot up with a image built with glibc and own
toolchain.

[0946684815.429327]Freeing unused kernel memory: 5892k freed
/init[0946684815.430327]Kernel panic - not syncing: Attempted to kill init!
: error while lo[0946684815.431327] ading shared lib<0>Rebooting in 1 seconds..raries: libcrypt.so.1: cannot open shared object file: No such file or directory

kyoupark@ukstbuild2:/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux-uclibc/lib$ ll libcrypt.*
-r--r--r-- 1 kyoupark ccusers 16378 Jun 19  2014 libcrypt.a
lrwxrwxrwx 1 kyoupark ccusers    13 Feb 17 13:38 libcrypt.so -> libcrypt.so.0
lrwxrwxrwx 1 kyoupark ccusers    18 Feb 17 13:38 libcrypt.so.0 -> libcrypt-0.9.29.so

kyoupark@ukstbuild2:~/spk-out/project_build_mips/ams-drx890/root/lib$ ll libcrypt*
-rwxr-xr-x 1 kyoupark ccusers 12356 Feb  1 14:18 libcrypt-0.9.29.so*
lrwxrwxrwx 1 kyoupark ccusers    18 Feb  1 14:10 libcrypt.so.0 -> libcrypt-0.9.29.so*

kyoupark@ukstbuild2:~/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/lib$ ll libcrypt.*
-rw-r--r-- 1 kyoupark ccusers 138748 Feb 21 14:42 libcrypt.a
lrwxrwxrwx 1 kyoupark ccusers     13 Feb 21 14:42 libcrypt.so -> libcrypt.so.1*
lrwxrwxrwx 1 kyoupark ccusers     16 Feb 21 14:42 libcrypt.so.1 -> libcrypt-2.13.so*

kyoupark@ukstbuild2:~/spk-out-use-own-brcm/project_build_mips/ams-drx890/root/lib$ ll libcrypt*
-rwxr-xr-x 1 kyoupark ccusers 39688 Feb 22 09:26 libcrypt-2.13.so*
lrwxrwxrwx 1 kyoupark ccusers    16 Feb 22 09:20 libcrypt.so.1 -> libcrypt-2.13.so*

~/spk-out-use-own-brcm/binaries/ams-drx890/x/bin$ readelf -d busybox
 0x00000001 (NEEDED)                     Shared library: [libcrypt.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so.6]

why cannot find it when there is a library in the image? need to change ld
system dir. see *ld-system-dir*


<mini-init> see *mini-init*
works when build it as static and init instead and gets working as dynamic
later. However, still bash and busybox sh do not work.

// minit.c
#include <stdio.h>

int main(int argc, char *argv)
{
    int i;
    printf("Hello minit!\n");
    for(i = 0; i < 999999999; i++)
    {
        printf("....minit> %d\n", i);
        sleep(3);
    }
}

have to rootfs.mips.initramfs_list to use as init


<error> <libgcc> <invalid-elf-header> when try bash. do not work even when
build it as static.

not find libgcc_s.so when use own toolchanin which causes "invalid ELF header"
error when runs an app.

// note: ????
// note: TODO needs to check since this means that do not use cross toolchain
// can build *libgcc* after binutil step
// 
// cross gcc without C/C++ library
// http://wiki.osdev.org/GCC_Cross-Compiler
// 
// cd build-gcc
// ../gcc-x.y.z/configure --target=$TARGET --prefix="$PREFIX" --disable-nls --enable-languages=c,c++ --without-headers
// make all-gcc
// make all-target-libgcc
// make install-gcc
// make install-target-libgcc


*what-libgcc*
The libgcc is a low-level support library that the compiler expects available
at compile time. Linking against libgcc provides integer, floating point,
   decimal, stack unwinding (useful for exception handling) and other support
   functions. 


From *gcc-build-glibc-reference* uses gcc4.8.2/4.9.2

The step 4, glibc headers and startup files, builds dummy libc.so
The step 5, compiler support libraries, builds libgcc_s.so

$ make -j4 all-target-libgcc

see it has dummy libc

$ readelf -d libgcc_s.so.1
 0x00000001 (NEEDED)                     Shared library: `[libc.so]`
 0x0000000e (SONAME)                     Library soname: [libgcc_s.so.1]

The step 6, standard c library, really builds libc.so

/* GNU ld script
   Use the shared library, but some functions are only in
   the static library, so try that secondarily.  */
OUTPUT_FORMAT(elf32-tradbigmips)
GROUP ( /home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-reference/mips-linux-gnu/lib/libc.so.6 /home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-reference/mips-linux-gnu/lib/libc_nonshared.a  AS_NEEDED ( /home/nds-u
k/kyoupark/asn/gcc/gcc-glibc-mips-reference/mips-linux-gnu/lib/ld.so.1 ) )

The step 7, gcc second pass build, changes libgcc_s.so.1 and see it now has
real libc.

$ readelf -d libgcc_s.so.1
 0x00000001 (NEEDED)                     Shared library: `[libc.so.6]`
 0x0000000e (SONAME)                     Library soname: [libgcc_s.so.1]


*when-use-own-toolchain* 

Like when use brcm toolchain, see no use of libgcc when use reference
toolchain.

$ ./gcc/gcc-glibc-mips-reference/bin/mips-linux-gnu-gcc sample.c
$ ls -al a.out
-rwxr-xr-x 1 kyoupark ccusers 9640 Apr  5 08:29 a.out

$ readelf -d a.out
 0x00000001 (NEEDED)                     Shared library: `[libc.so.6]`


However, the issue is when use own toolchain, 420, it builds libgcc_s.so in
first-pass gcc which is all-gcc and never update it since then. As shown
above, this makes libgcc has wrong libc name in it. Then binary links with
libgcc and cannot find libc when runs.


$ ./gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc sample.c
// asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/../../../../mips-linux-gnu/lib/libgcc_s.so

$ readelf -d a.out

 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: `[libc.so.6]`

$ readelf -d libgcc_s.so.1

 0x00000001 (NEEDED)                     Shared library: `[libc.so]`
 0x0000000e (SONAME)                     Library soname: [libgcc_s.so.1]


this was cases when try bash or ps which needs libgcc. see libc.so which has
correct libc name.

$ readelf -d bash

 0x00000001 (NEEDED)                     Shared library: [libdl.so.2]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so.6]

 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so]  // script

$ more libc.so
/* GNU ld script
   Use the shared library, but some functions are only in
   the static library, so try that secondarily.  */
OUTPUT_FORMAT(elf32-tradbigmips)
GROUP ( libc.so.6 libc_nonshared.a  AS_NEEDED ( ld.so.1 ) )

-rw-r--r-- 1 kyoupark ccusers      220 Apr  4 10:48 libc.so
lrwxrwxrwx 1 kyoupark ccusers       12 Mar 16 12:03 libc.so.6 -> libc-2.13.so*


*when-use-brcm-toolchain* see no use of libgcc.
two differences. not use libgcc and libc.so

~/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-uclibc-gcc sample.c

$ readelf -d a.out

 0x00000001 (NEEDED)                     Shared library: [libc.so.0]

$ readelf -d libgcc_s.so.1

 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
 0x0000000e (SONAME)                     Library soname: [libgcc_s.so.1]

$ readelf -d bash

 0x00000001 (NEEDED)                     Shared library: [libdl.so.0]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]

$ more libc.so
/* GNU ld script
 * Use the shared library, but some functions are only in
 * the static library, so try that secondarily. */
GROUP ( libc.so.0 uclibc_nonshared.a  )

-r--r--r-- 1 kyoupark ccusers     166 Jun 19  2014 libc.so
lrwxrwxrwx 1 kyoupark ccusers      19 Mar 23 08:09 libc.so.0 -> libuClibc-0.9.29.so


Fixed this and see *gcc-build-glibc-mips-own* This also fixes this rpath
warning which see only when build with own toolchain before.

$ /home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc minit.c
$ asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc minit.c

/home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/../lib/gcc/mips-linux-gnu/4.2.0/../../../../mips-linux-gnu/bin/ld: warning: libc.so, needed by /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-
install/bin/../lib/gcc/mips-linux-gnu/4.2.0/../../../../mips-linux-gnu/lib/libgcc_s.so, not found (try using -rpath or -rpath-link)


note: why needs libgcc when use own toolchain?


*to-use-own-toolchain-for-bash-or-others* 
1. check modified ld 
strings ld-2.13.so | grep KT

1. copy libgcc_s.so.1 to root and update

cd /home/nds-uk/kyoupark/spk-out-use-own-brcm/project_build_mips/ams-drx890/root/lib
cp ~/asn/gcc/gcc-glibc-brcm-mips-install-two/mips-linux-gnu/lib/libgcc_s.so.1 .

/home/nds-uk/kyoupark/spk-out-use-own-brcm/binaries/ams-drx890/rootfs.mips.initramfs_list
file /lib/ld-2.13.so /home/nds-uk/kyoupark/spk-out-use-own-brcm/project_build_mips/ams-drx890/root/lib/ld-2.13.so 755 0 0
file /lib/libgcc_s.so.1 /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/project_build_mips/ams-drx890/root/lib/libgcc_s.so.1 755 0 0

1. to use bash as a init

// rootfs.mips.initramfs_list
slink /init sbin/init 777 0 0

note: have to "../bin/sh"

slink /init ../bin/sh 777 0 0


<error> when try busybox sh
busybox sh keeps restarting when sh is not configured in busybox

KT: process '/bin/sh' (pid 16) exited. (stat 65280) Scheduling for restart.
KT: respawn(once)
KT: process '/bin/sh' (pid 17) exited. (stat 65280) Scheduling for restart.
KT: respawn(once)


<changes-to-busybox-to-use-ash>
*spk-build-busybox* *busy-build* build for spk

/usr/bin/make -j1
CC=/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-gcc
CROSS_COMPILE="/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-"
\
CROSS="/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-"
PREFIX="/home/nds-uk/kyoupark/spk-out/project_build_mips/ams-drx890/root" \
ARCH=mips \ EXTRA_CFLAGS="-Os
-I/home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/include
-I/home/nds-uk/kyoupark/spk-out/build_mips/staging_dir/usr/include
-I/home/NDS-UK/kyoupark/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/mips-linux/include"
-C /home/nds-uk/kyoupark/spk-out/project_build_mips/ams-drx890/busybox-1.13.1


PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

cd /home/nds-uk/kyoupark/spk-out-use-own-brcm/project_build_mips/ams-drx890/busybox-1.13.1

/usr/bin/make -j1 CC=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-/bin/mips-linux-gnu-gcc \
CROSS_COMPILE="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-" \
CROSS="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-" \
PREFIX="/home/nds-uk/kyoupark/spk-out-use-own-brcm/project_build_mips/ams-drx890/root" \
ARCH=mips \
EXTRA_CFLAGS="-Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" 


/usr/bin/make -j1 CC=mips-linux-gnu-gcc \
CROSS_COMPILE="mips-linux-gnu-" \
CROSS="mips-linux-gnu-" \
PREFIX="/home/nds-uk/kyoupark/spk-out-use-own-brcm/project_build_mips/ams-drx890/root" \
ARCH=mips \
EXTRA_CFLAGS="-Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include \
-I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include \
-I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/mips-linux-gnu/include" 


to check what applets, commands, are supported.

include/applet_tables.h

const char applet_names[] ALIGN1 = ""
"awk" "\0"
"basename" "\0"
"brctl" "\0"

int (*const applet_main[])(int argc, char **argv) = {
awk_main,
basename_main,
brctl_main,
...
};


libbb/appletlib.c

int main(int argc UNUSED_PARAM, char **argv)
{
	parse_config_file(); /* ...maybe, if FEATURE_SUID_CONFIG */

   // note:
   // gets printed whenever runs commands on busybox

	full_write2_str(applet_name);
	full_write2_str(" KT: run applet\n");

	run_applet_and_exit(applet_name, argv);
}

note: 
to check init working

init/init.c

int init_main(int argc UNUSED_PARAM, char **argv)
{
	/* Hello world */
	message(MAYBE_CONSOLE | L_LOG, "KT: init started: %s", bb_banner);
    sleep(2);
}


[0946684820.742326]Freeing unused kernel memory: 7936k freed
         1:      KT: system_dirs: /lib/
init KT: run applet                                 // from libbb/appletlib.c
KT: init started: BusyBox v1.13.1 (2017-03-24 14:44:53 GMT)
KT: init started: 2
KT: init started: 4
command='/bin/ash' action=2 tty='/dev/ttyS0'

command='/sbin/reboot' action=32 tty=''

command='/usr/bin/killall syslogd' action=64 tty='/dev/null'

command='/bin/umount -a -r' action=64 tty='/dev/null'

command='/sbin/swapoff -a' action=64 tty='/dev/null'

KT: init started: 5
KT: init started: 6
KT: run(sysint)
KT: run_actions(1)
KT: wait(sysint)
KT: run_actions(8)
KT: wait(once)
KT: run_actions(16)
KT: respawn(once)
KT: run_actions(6)
KT: starting pid 16, tty '/dev/ttyS0': '/bin/ash'
KT: EXE: /bin/ash                         // static void init_exec(const char *command)
        16:      KT: system_dirs: /lib/   // from glibc
ash KT: run applet                        // from libbb/appletlib.c

BusyBox v1.13.1 (2017-03-24 14:44:53 GMT) built-in shell (ash)
Enter 'help' for a list of built-in commands.

        16:      KT: RPATH...        16:         KT: LD_LIB...        16:        KT: RUNPATH...        16:   KT: cache...        16:         KT: default...
        16:      KT: cannot open...        16:   KT: RPATH...        16:         KT: LD_LIB...        16:    KT: RUNPATH...        16:       KT: cache...        16:         KT: default...
        16:      KT: RPATH...        16:         KT: LD_LIB...        16:        KT: RUNPATH...        16:   KT: cache...        16:         KT: default...
        16:      KT: RPATH...        16:         KT: LD_LIB...        16:        KT: RUNPATH...        16:   KT: cache...        16:         KT: default...
/ #
/ #
/ # ls
        17:      KT: system_dirs: /lib/
ls KT: run applet
VERSION.gz  fifo        linuxrc     root        tmp
bin         home        mnt         sbin        usr
dev         init        opt         scs         var
etc         lib         proc        sys


// rootfs.mips.initramfs_list
slink /init ../bin/ash 777 0 0
slink /bin/ash busybox 777 0 0


# /etc/inittab
# Put a getty on the serial port
#
# note: ash
ttyS0::respawn:/bin/ash

# Stuff to do for the 3-finger salute
::ctrlaltdel:/sbin/reboot

# Stuff to do before rebooting
null::shutdown:/usr/bin/killall syslogd
null::shutdown:/bin/umount -a -r
null::shutdown:/sbin/swapoff -a


// change config to enable ash
#
# Shells
#
CONFIG_FEATURE_SH_IS_ASH=y
CONFIG_ASH=y

#
# Ash Shell Options
#
CONFIG_ASH_BASH_COMPAT=y
CONFIG_ASH_JOB_CONTROL=y
# CONFIG_ASH_READ_NCHARS is not set
# CONFIG_ASH_READ_TIMEOUT is not set
CONFIG_ASH_ALIAS=y
CONFIG_ASH_MATH_SUPPORT=y
# CONFIG_ASH_MATH_SUPPORT_64 is not set
# CONFIG_ASH_GETOPTS is not set
CONFIG_ASH_BUILTIN_ECHO=y
CONFIG_ASH_BUILTIN_PRINTF=y
CONFIG_ASH_BUILTIN_TEST=y
# CONFIG_ASH_CMDCMD is not set
CONFIG_ASH_MAIL=y
CONFIG_ASH_OPTIMIZE_FOR_SIZE=y


<error> seg fault when run apps on ash running

Considerd other ways to debug such as chroot, strace, usb mount, gdb server or
nfs mount to get core file out from the box. Nothing viable since can do
little on ash running environment without running app.

The solution is to run glibc on uclibc system. *linux-lib-ld-use-different-version*

<ex>
Using Multiple versions of glibc using LD_LIBRARY_PATH env variable.
https://sourceware.org/ml/libc-help/2008-12/msg00045.html

Hi, all

I just built different version of glibc from Fedora core 9 and I tried to
execute some applications (e.g. Firefox, Mysql) using this library.  First I
tried it by changing LD_LIBRARY_PATH environmental variable like below, but it
crashed with core dump.

 $ export LD_LIBRARY_PATH=/tmp/lib-2.9/lib:$LD_LIBRARY_PATH
 $ echo $LD_LIBRARY_PATH
 /tmp/lib-2.9/lib
 $ /bin/ls
 Segmentation fault (core dumped)

What I did next was using 'ld-linux.so.2' loader and it could exeute the
command.

$/tmp/lib-2.9/lib/ld-linux.so.2 --library-path /tmp/lib-2.9/lib /bin/ls

But I still want to find cleaner solution to have multiple versions of glic in
a single machine. Can anyone help me with this?

Thanks for your help in advance


<ex>
cp new-ld.so /lib
LD_LIBRARY_PATH=/new version path/lib ./a.out


<ex> <best>
https://unix.stackexchange.com/questions/272606/locally-installing-glibc-2-23-causes-all-programs-to-segfault
Because of a mismatch of ld-linux-x86-64.so.2 (man ld.so) and libc.so.

If you want to run gdb under the LD_LIBRARY_PATH setting, run as follows:

export LD_LIBRARY_PATH=~/local/lib
/lib64/ld-linux-x86-64.so.2 --library-path /lib64 /usr/bin/gdb /bin/ls

//  --library-path PATH   use given PATH `instead of` content of the environment
//                        variable LD_LIBRARY_PATH

This runs /usr/bin/gdb in the old library environment and /bin/ls in the new
library environment. Similarly, you can run only one command in the new
library environment as follows:

export LD_LIBRARY_PATH=~/local/lib
~/local/lib/ld-linux-x86-64.so.2 /bin/echo


<ex>
-sh-3.2# LD_LIBRARY_PATH=/mnt/tmp/asn/glibc-libs/lib ./a.out
*** KT: force LD_DEBUG=libs ***
      8417:      KT: system_dirs: /lib/
      8417:     find library=libgcc_s.so.1 [0]; searching
      8417:      KT: RPATH...      8417:         KT: LD_LIB...      8417:        search path=/mnt/tmp/asn/glibc-libs/lib/tls:/mnt/tmp/asn/glibc-libs/lib                (LD_LIBRARY_PATH)
      8417:     KT:  trying file=/mnt/tmp/asn/glibc-libs/lib/tls/libgcc_s.so.1
      8417:     KT:  trying file=/mnt/tmp/asn/glibc-libs/lib/libgcc_s.so.1
      8417:      KT: RUNPATH...      8417:       KT: default...
      8417:
      8417:     find library=libc.so.6 [0]; searching
      8417:      KT: RPATH...      8417:         KT: LD_LIB...      8417:        search path=/mnt/tmp/asn/glibc-libs/lib                (LD_LIBRARY_PATH)
      8417:     KT:  trying file=/mnt/tmp/asn/glibc-libs/lib/libc.so.6
      8417:      KT: RUNPATH...      8417:       KT: default...
      8417:
Segmentation fault (core dumped)


<ex> *current-working*

note: currently use spk-out-use-brcm and install-two.

/home/nds-uk/kyoupark/asn/gcc
glibc-2.13/             // for source to work
glibc-2.13-build/       // for build

$ ../glibc-2.13/configure --prefix=/home/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-03/mips-linux-gnu --build=x86_64-redhat-linux-gnu --host=mips-linux-gnu --without-cvs --disable-profile --without-gd --
with-headers=/home/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-03/mips-linux-gnu/include --disable-debug --disable-sanity-checks --enable-obsolete-rpc libc_cv_forced_unwind=yes libc_cv_c_cleanup=yes --enabl
e-kernel=2.6.18 --with-__thread --with-tls --enable-shared --without-fp --enable-add-ons=nptl,../glibc-ports-2.13

cd glibc-2.13-build

PREFIX=/home/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-03
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

make

// copy built ld to mount directory
cp /home/kyoupark/asn/gcc/glibc-2.13-build/elf/ld.so /home/kyoupark/asn/glibc-libs/lib/ld-2.13.so
ls -al /home/kyoupark/asn/glibc-libs/lib/ld-2.13.so

telnet 10.209.60.47
mkdir /mnt/tmp
// mount -o rw -t nfs 10.209.62.232:/home/NDS-UK/kyoupark/ /mnt/tmp
mount -o rw -t nfs 10.209.62.106:/home/kyoupark/ /mnt/tmp
cd /mnt/tmp/asn/glibc-libs
ulimit -c unlimited
echo '%p.COR' >/proc/sys/kernel/core_pattern

// -sh-3.2# 
export LD_DEBUG=all
./lib/ld-2.13.so --library-path /mnt/tmp/asn/glibc-libs/lib ./minit-dynamic


glibc-2.13-for-source/  // for browsing with ports source           
glibc-2.13-org/         // org


-sh-3.2# export LD_DEBUG=all
-sh-3.2# ./lib/ld-2.13.so --library-path /mnt/tmp/asn/glibc-libs/lib ./minit-dynamic
*** KT: not force LD_DEBUG ***
      1759:      KT: dynamic string...      1759:       file=./minit-dynamic [0];  generating link map
      1759:       dynamic: 0x0040017c  base: 0x00000000   size: 0x000019e0
      1759:         entry: 0x004005e0  phdr: 0x00400034  phnum:          8
      1759:
      1759:      KT: system_dirs: /lib/
      1759:
      1759:     file=libgcc_s.so.1 [0];  needed by ./minit-dynamic [0]
      1759:     find library=libgcc_s.so.1 [0]; searching
      1759:      KT: RPATH...      1759:         KT: LD_LIB...      1759:        search path=/mnt/tmp/asn/glibc-libs/lib/tls:/mnt/tmp/asn/glibc-libs/lib                (LD_LIBRARY_PATH)
      1759:     KT:  trying file=/mnt/tmp/asn/glibc-libs/lib/tls/libgcc_s.so.1
      1759:     KT:  trying file=/mnt/tmp/asn/glibc-libs/lib/libgcc_s.so.1
      1759:      KT: RUNPATH...      1759:       KT: default...
      1759:
      1759:     file=libgcc_s.so.1 [0];  generating link map
      1759:       dynamic: 0x2aaac10c  base: 0x2aaac000   size: 0x0000f940
      1759:         entry: 0x2aaae680  phdr: 0x2aaac034  phnum:          6
      1759:
      1759:
      1759:     file=libc.so.6 [0];  needed by ./minit-dynamic [0]
      1759:     find library=libc.so.6 [0]; searching
      1759:      KT: RPATH...      1759:         KT: LD_LIB...      1759:        search path=/mnt/tmp/asn/glibc-libs/lib                (LD_LIBRARY_PATH)
      1759:     KT:  trying file=/mnt/tmp/asn/glibc-libs/lib/libc.so.6
      1759:      KT: RUNPATH...      1759:       KT: default...
      1759:
      1759:     file=libc.so.6 [0];  generating link map
      1759:       dynamic: 0x2aabc1ac  base: 0x2aabc000   size: 0x00159e50
      1759:         entry: 0x2aad32a4  phdr: 0x2aabc034  phnum:         11
      1759:
      1759:     checking for version `GLIBC_2.0' in file /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0] required by file ./minit-dynamic [0]
      1759:     checking for version `GLIBC_2.0' in file /mnt/tmp/asn/glibc-libs/lib/libgcc_s.so.1 [0] required by file ./minit-dynamic [0]
      1759:     checking for version `GLIBC_2.2' in file /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0] required by file /mnt/tmp/asn/glibc-libs/lib/libgcc_s.so.1 [0]
      1759:     checking for version `GLIBC_2.0' in file /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0] required by file /mnt/tmp/asn/glibc-libs/lib/libgcc_s.so.1 [0]
      1759:     checking for version `GLIBC_PRIVATE' in file ./lib/ld-2.13.so [0] required by file /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0]
      1759:     checking for version `GLIBC_2.3' in file ./lib/ld-2.13.so [0] required by file /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0]
      1759:     checking for version `GLIBC_2.2' in file ./lib/ld-2.13.so [0] required by file /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0]
      1759:
      1759:     relocation processing: /mnt/tmp/asn/glibc-libs/lib/libc.so.6 (lazy)
      1759:     symbol=argp_err_exit_status;  lookup in file=./minit-dynamic [0]
      1759:     symbol=argp_err_exit_status;  lookup in file=/mnt/tmp/asn/glibc-libs/lib/libgcc_s.so.1 [0]
      1759:     symbol=argp_err_exit_status;  lookup in file=/mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0]
      1759:     binding file /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0] to /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0]: normal symbol `argp_err_exit_status' [GLIBC_2.2]
      1759:     symbol=optarg;  lookup in file=./minit-dynamic [0]
      1759:     symbol=optarg;  lookup in file=/mnt/tmp/asn/glibc-libs/lib/libgcc_s.so.1 [0]
      1759:     symbol=optarg;  lookup in file=/mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0]
      1759:     binding file /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0] to /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0]: normal symbol `optarg' [GLIBC_2.0]
      1759:     symbol=opterr;  lookup in file=./minit-dynamic [0]
      1759:     symbol=opterr;  lookup in file=/mnt/tmp/asn/glibc-libs/lib/libgcc_s.so.1 [0]
      1759:     symbol=opterr;  lookup in file=/mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0]
      1759:     ...                                                                                                                                                                                      [21/1297]
      1759:     relocation processing: /mnt/tmp/asn/glibc-libs/lib/libgcc_s.so.1 (lazy)
      1759:     ...
      1759:     relocation processing: ./minit-dynamic (lazy)
      1759:     symbol=_init;  lookup in file=./minit-dynamic [0]
      1759:     binding file ./minit-dynamic [0] to ./minit-dynamic [0]: normal symbol `_init'
      1759:     symbol=__gmon_start__;  lookup in file=./minit-dynamic [0]
      1759:     symbol=__gmon_start__;  lookup in file=/mnt/tmp/asn/glibc-libs/lib/libgcc_s.so.1 [0]
      1759:     symbol=__gmon_start__;  lookup in file=/mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0]
      1759:     symbol=__gmon_start__;  lookup in file=./lib/ld-2.13.so [0]
Segmentation fault
-sh-3.2#

<gdb-core>
cd /home/kyoupark/asn/glibc-libs
~/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-uclibc-gdb

(gdb) 
set solib-search-path ./lib
file minit-dynamic
core X.COR

Reading symbols from /home/NDS-UK/kyoupark/asn/glibc-libs/lib/ld-2.13.so...done.
Loaded symbols for ./lib/ld-2.13.so

Core was generated by `./lib/ld-2.13.so --library-path /mnt/tmp/asn/glibc-libs/lib ./minit-dynamic'.
Program terminated with signal 11, Segmentation fault.
#0  0x5555c600 in _dl_relocate_object (scope=0x2aaa828c, reloc_mode=<value optimized out>, consider_profiling=0) at ../../glibc-ports-2.13/sysdeps/mips/dl-machine.h:755
755           gotplt = (ElfW(Addr) *) D_PTR (l, l_info[DT_MIPS (PLTGOT)]);

#0  0x5555c600 in _dl_relocate_object (scope=0x2aaa828c, reloc_mode=<value optimized out>, consider_profiling=0) at ../../glibc-ports-2.13/sysdeps/mips/dl-machine.h:755
#1  0x55554da4 in dl_main (phdr=<value optimized out>, phnum=<value optimized out>, user_entry=0x7fdfea60, auxv=0x7fdfee64) at rtld.c:2256
#2  0x55567a14 in _dl_sysdep_start (start_argptr=<value optimized out>, dl_main=0x55552658 <dl_main>) at ../elf/dl-sysdep.c:244
#3  0x55550e64 in _dl_start_final (arg=0x7fdfee20, info=0x7fdfeac8) at rtld.c:336
#4  0x55551784 in _dl_start (arg=0x7fdfee20) at rtld.c:564
#5  0x55550894 in __start () at ../../glibc-ports-2.13/sysdeps/mips/dl-machine.h:83
Backtrace stopped: frame did not save the PC


$ export LD_LIBRARY_PATH=/mnt/tmp/asn/glibc-libs/lib
$ ./lib/ld-2.13.so --library-path /mnt/tmp/asn/glibc-libs/lib ./uclibc-gdb ./minit-dynamic

// use uclibc for gdb

export LD_LIBRARY_PATH=/mnt/tmp/asn/glibc-libs/lib
/lib/ld-uClibc-0.9.29.so --library-path /lib ./uclibc-gdb ./minit-dynamic


./uclibc-gdb -d /mnt/tmp/asn/gcc/glibc-2.13 ./lib/ld-2.13.so 
b dl_main
run --library-path /mnt/tmp/asn/glibc-libs/lib ./minit-dynamic
(gdb) run --library-path /mnt/tmp/asn/glibc-libs/lib ./minit-dynamic
Starting program: /mnt/tmp/asn/glibc-libs/lib/ld-2.13.so --library-path /mnt/tmp/asn/glibc-libs/lib ./minit-dynamic
warning: Can not parse XML target description; XML support was disabled at compile time
Cannot access memory at address 0x20cb4


#0  0x2aab4160 in ?? () from /home/NDS-UK/kyoupark/asn/glibc-libs/lib/ld.so.1
warning: GDB can't find the start of the function at 0x2aab4160.

    GDB is unable to find the start of the function at 0x2aab4160
and thus can't determine the size of that function's stack frame.
This means that GDB may be unable to access that stack frame, or
the frames below it.
    This problem is most likely caused by an invalid program counter or
stack pointer.
    However, if you think GDB should simply search farther back
from 0x2aab4160 for code which looks like the beginning of a
function, you can increase the range of the search using the `set
heuristic-fence-post' command.
#1  0x2aab4160 in ?? () from /home/NDS-UK/kyoupark/asn/glibc-libs/lib/ld.so.1
warning: GDB can't find the start of the function at 0x2aab4160.
Backtrace stopped: previous frame identical to this frame (corrupt stack?)

~/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-addr2line -e lib/ld.so.1 0x2aab4160


{to-port-glibc-for-mips}
To port patches for uclibc ld to glibc

// paches for brcm uclibc
uClibc-nptl-0.9.29-20070423-3003-plt-ldso.patch

--- uClibc-nptl-0.9.29-20070423.orig/ldso/include/dl-elf.h      2008-03-09 16:42:38.000000000 +0000
+++ uClibc-nptl-0.9.29-20070423/ldso/include/dl-elf.h   2008-03-09 16:42:41.000000000 +0000


*current-working*
To modify glibc ld to have BRCM mips patches.


// elf/dl-reloc.c
_dl_relocate_object ()
{
  // 1, elf/dynamic-link.h
  ELF_DYNAMIC_RELOCATE (l, lazy, consider_profiling);

  // /* This can't just be an inline function because GCC is too dumb
  //    to inline functions containing inlines themselves.  */
  // # define ELF_DYNAMIC_RELOCATE(map, lazy, consider_profile) \
  //   do {									      \
  //     int edr_lazy = `elf_machine_runtime_setup` ((map), (lazy),		      \
  // 					      (consider_profile));	      \
  //     ELF_DYNAMIC_DO_REL ((map), edr_lazy);				      \
  //     ELF_DYNAMIC_DO_RELA ((map), edr_lazy);				      \
  //   } while (0)
  // 
  // #endif
  //
  // this leads to :
  //
  // /* Set up the loaded object described by L so its stub function
  //    will jump to the on-demand fixup code __dl_runtime_resolve.  */
  // 
  // auto inline int
  // __attribute__((always_inline))
  // elf_machine_runtime_setup (struct link_map *l, int lazy, int profile);


  // 2, glibc-ports-2.13/sysdeps/mips/dl-machine.h
  `elf_machine_runtime_setup` (struct link_map *l, int lazy, int profile)
  {
# ifndef RTLD_BOOTSTRAP
    ElfW(Addr) *got;
    extern void _dl_runtime_resolve (ElfW(Word));
    extern void _dl_runtime_pltresolve (void);
    extern int _dl_mips_gnu_objects;

    if (lazy)
    {
      /* The GOT entries for functions have not yet been filled in.
         Their initial contents will arrange when called to put an
         offset into the .dynsym section in t8, the return address
         in t7 and then jump to _GLOBAL_OFFSET_TABLE[0].  */
      got = (ElfW(Addr) *) D_PTR (l, l_info[DT_PLTGOT]);

      /* This function will get called to fix up the GOT entry indicated by
         the register t8, and then jump to the resolved address.  */
      got[0] = (ElfW(Addr)) &`_dl_runtime_resolve`;

      /* Store l to _GLOBAL_OFFSET_TABLE[1] for gnu object. The MSB
         of got[1] of a gnu object is set to identify gnu objects.
         Where we can store l for non gnu objects? XXX  */
      if ((got[1] & ELF_MIPS_GNU_GOT1_MASK) != 0)
        got[1] = ((ElfW(Addr)) l | ELF_MIPS_GNU_GOT1_MASK);
      else
        _dl_mips_gnu_objects = 0;
    }

    /* Relocate global offset table.  */
    elf_machine_got_rel (l, lazy);

    /* If using PLTs, fill in the first two entries of .got.plt.  */
    if (l->l_info[DT_JMPREL] && lazy)
    {
      ElfW(Addr) *gotplt;

      // changes:
      // K:\asn\gcc\glibc-ports-2.13\sysdeps\mips\dl-machine.h
      // K:\asn\gcc\glibc-ports-2.13\sysdeps\mips\dl-trampoline.c
      // K:\asn\gcc\glibc-2.13\elf\dynamic-link.h
      //
      // _dl_dprintf(), _dl_debug_vdprintf() in elf/dl-misc.c

      // *todo-1* 
      // 1. modify glibc-2.13/elf/dynamic-link.h, elf_get_dynamic_info() based
      // on uclibc/ldso/include/dl-elf.h, _dl_parse_dynamic_info()

      // *todo-2* 
      // 2. modify glibc-ports-2.13/sysdeps/mips/dl-machine.h,
      // elf_machine_runtime_setup() to use DT_PLTGOT
      
      // note: 
      // DT_MIPS_PLTGOT cause crash since there is no such entry in the dynaic
      // header of an image and l_info[84] is null.
      //
      // l_info size: 516, l_info[129], DT_MIPS (PLTGOT) (84)
      //
      // glibc-ports-2.13/sysdeps/mips/dl-machine.h
      //
      // /* Translate a processor specific dynamic tag to the index
      //    in l_info array.  */
      // #define DT_MIPS(x) (DT_MIPS_##x - DT_LOPROC + DT_NUM)
      //
      // #define DT_MIPS_PLTGOT	     0x70000032
      // #define DT_LOPROC	0x70000000	/* Start of processor-specific */
      // #define DT_NUM		34		/* Number used */
      //
      // each target defines DT_THISPROCNUM macros. for mips:
      //
      // DT_MIPS_NUM      1677 elf/elf.h        #define DT_MIPS_NUM           0x35
      // DT_THISPROCNUM     22 sysdeps/mips/dl-dtprocnum.h #define DT_THISPROCNUM        DT_MIPS_NUM
      //
      // from core
      // Core was generated by `./lib/ld-2.13.so --library-path /mnt/tmp/asn/glibc-libs/lib ./minit-dynamic'.
      // Program terminated with signal 11, Segmentation fault.
      // #0  0x5555c600 in _dl_relocate_object (scope=0x2aaa828c, reloc_mode=<value optimized out>, consider_profiling=0) at `../../glibc-ports-2.13/sysdeps/mips/dl-machine.h:755`
      // 755           gotplt = (ElfW(Addr) *) D_PTR (l, l_info[DT_MIPS (PLTGOT)]);
      //
      // *link-map-struct*
      // include/link.h. this is defined in elf/link.h but seems not used.
      // struct link_map
      // {
      //    ElfW(Dyn) *`l_info`[DT_NUM + DT_THISPROCNUM + DT_VERSIONTAGNUM
		//      + DT_EXTRANUM + DT_VALNUM + DT_ADDRNUM];
      // }
      //
      // (ElfW(Addr) *) D_PTR (l, l_info[DT_MIPS (PLTGOT)]);
      //
      // #ifdef DL_RO_DYN_SECTION
      // # define D_PTR(map, i) ((map)->i->d_un.d_ptr + (map)->l_addr)
      // #else
      // # define D_PTR(map, i) (map)->i->d_un.d_ptr
      // #endif

      // gotplt = (ElfW(Addr) *) D_PTR (l, l_info[DT_MIPS (PLTGOT)]); to
      gotplt = (ElfW(Addr) *) D_PTR (l, l_info[DT_PLTGOT]);

      /* If a library is prelinked but we have to relocate anyway,
         we have to be able to undo the prelinking of .got.plt.
         The prelinker saved the address of .plt for us here.  */
      if (gotplt[1])
        l->l_mach.plt = gotplt[1] + l->l_addr;

      *todo-3* 
      // // _dl_runtime_pltresolve is assembly function in sysdeps/mips/dl-trampoline.c
      // gotplt[0] = (ElfW(Addr)) &`_dl_runtime_pltresolve`;
      // gotplt[1] = (ElfW(Addr)) l;

      // to
      gotplt[0] = (ElfW(Addr)) &_dl_runtime_resolve;
      gotplt[1] = (ElfW(Addr)) l;
      gotplt[2] = (ElfW(Addr)) &_dl_runtime_ptlresolve;

      // gotplt[2] = (ElfW(Addr)) &_dl_linux_resolve;
    }
# endif
    return lazy;
  }
}


*todo-3* 
modify glibc-ports-2.13/sysdeps/mips/dl-trampoline.c to add two _dl_xxx
functions added above.

_dl_runtime_resolve exist in glibc but slightly different

<dl_runtime_reslove-glic-version>

macros expands to asm code depending on 32 or 64 bit defines like:

	" STRINGXP(PTR_SUBIU) "  $29, " STRINGXP(ELF_DL_FRAME_SIZE) "\n\
	becomes "subd $29, 40 "\n\

asm ("\n\
	move	$4, $24\n\
	move	$5, $15\n\
	move	$6, $3\n\
	move	$7, $2\n\
	jal	__dl_runtime_resolve\n\
	" ELF_DL_RESTORE_ARG_REGS "\
	" STRINGXP(RESTORE_GP64) "\n\
	" STRINGXP(PTR_ADDIU) "	$29, " STRINGXP(ELF_DL_FRAME_SIZE) "\n\
	move	$25, $2\n\
	jr	$25\n\
	.end	_dl_runtime_resolve\n\
	.previous\n\
");

__dl_runtime_resolve (ElfW(Word) sym_index,
		      ElfW(Word) return_address,
		      ElfW(Addr) old_gpreg,
		      ElfW(Addr) stub_pc)
{}


<dl_runtime_reslove-uclibc-version>

	# Setup functions args and call __dl_runtime_resolve
	move	$4, $24
	move	$5, $3
	jal	__dl_runtime_resolve

   // // same as glibc version
	// # Restore function arguments from stack to registers
	// lw	$31, 36($29)
	// lw	$4, 16($29)
	// lw	$5, 20($29)
	// lw	$6, 24($29)
	// lw	$7, 28($29)
   //
	// # Do a tail call to the original function
	// addiu	$29, 40

	move	$25, $2
	jr	$2                    // changed by brcm-patch 
.end	_dl_runtime_resolve

unsigned long __dl_runtime_resolve(unsigned long sym_index,
	unsigned long old_gpreg)
{
  unsigned long *got = (unsigned long *) (old_gpreg - OFFSET_GP_GOT);
  struct elf_resolve *tpnt = (struct elf_resolve *) got[1];
}

so difference is due to difference of function prototype.

<dl_linux_resolver>

_dl_linux_resolver(struct elf_resolve *tpnt, int reloc_entry)
{
  rel_addr = (char *)tpnt->dynamic_info[DT_JMPREL];
}



={============================================================================
*kt_linux_gcc_400* gcc-build-spk-gdb gcc-build-spk-bash

<spk-gdb>
/home/nds-uk/kyoupark/si_logs/spk/darwin-spk-1.19-use-own/toolchain/gdb/6.7.1
-rw-rw-r--  1 kyoupark ccusers  920 Oct 14  2013 600-fix-compile-flag-mismatch.patch

/home/nds-uk/kyoupark/spk-out/toolchain_build_mips
cp /home/nds-uk/kyoupark/si_logs/spk/darwin-spk-1.19-use-own/dl/gdb-6.7.1.tar.bz2 .

Applying 600-fix-compile-flag-mismatch.patch using plaintext: 
patching file gdb/gdbserver/configure
patching file gdb/configure
cp -f package/gnuconfig/config.sub package/gnuconfig/config.guess /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/toolchain_build_mips/gdb-6.7.1
touch /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/toolchain_build_mips/gdb-6.7.1/.unpacked
mkdir -p /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/gdb-6.7.1-target
(
 cd /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/gdb-6.7.1-target; \

		gdb_cv_func_sigsetjmp=yes \
		PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/bin:/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/bin:/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/toolchain_build_mips/bin:/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ar" AS="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" LD="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" NM="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-nm" CC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" GCC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" CPP="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-cpp -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" CXX="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-g++ -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" FC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gfortran " RANLIB="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ranlib" STRIP="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-strip" OBJCOPY="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-objcopy" AR_FOR_BUILD="/usr/bin/ar" AS_FOR_BUILD="/usr/bin/as" CC_FOR_BUILD="/usr/lib64/ccache/gcc" GCC_FOR_BUILD="/usr/lib64/ccache/gcc" CXX_FOR_BUILD="/usr/lib64/ccache/g++" FC_FOR_BUILD="/usr/bin/ld" LD_FOR_BUILD="/usr/bin/ld" CFLAGS_FOR_BUILD="" CXXFLAGS_FOR_BUILD="" LDFLAGS_FOR_BUILD="" FCFLAGS_FOR_BUILD="" AR_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ar" AS_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" CC_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc" LD_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" NM_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-nm" DEFAULT_ASSEMBLER="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" DEFAULT_LINKER="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" PKG_CONFIG_SYSROOT_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir" PKG_CONFIG="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/bin/pkg-config" PKG_CONFIG_PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/lib/pkgconfig:" STAGING_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir" \
		CFLAGS_FOR_TARGET="-Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include -L/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/lib -L/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/lib -Wno-error" \
		CFLAGS="-Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include -L/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/lib -L/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/lib -Wno-error" \
		ac_cv_type_uintptr_t=yes gt_cv_func_gettext_libintl=yes ac_cv_func_dcgettext=yes gdb_cv_func_sigsetjmp=yes bash_cv_func_strcoll_broken=no bash_cv_must_reinstall_sighandlers=no bash_cv_func_sigsetjmp=present bash_cv_have_mbstate_t=yes \
		/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/toolchain_build_mips/gdb-6.7.1/configure \
		--cache-file=/dev/null \
		--build=x86_64-pc-linux-gnu \
		--host=mips-linux-gnu \
		--target=mips-linux-gnu \
		--prefix=/usr \
		--disable-nls \
		--without-uiout  \
		--disable-tui --disable-gdbtk --without-x \
		--disable-sim --enable-gdbserver \
		--without-included-gettext \
		--disable-werror \
		 \
)

<use-static>
http://stackoverflow.com/questions/9364685/how-i-can-static-build-gdb-from-source
note: --enable-static do not work but LDFLAGS=-static works

PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

gdb_cv_func_sigsetjmp=yes \
PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/bin:/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/bin:/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/toolchain_build_mips/bin:/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ar" AS="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" LD="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" NM="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-nm" CC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" GCC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" CPP="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-cpp -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" CXX="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-g++ -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" FC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gfortran " RANLIB="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ranlib" STRIP="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-strip" OBJCOPY="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-objcopy" AR_FOR_BUILD="/usr/bin/ar" AS_FOR_BUILD="/usr/bin/as" CC_FOR_BUILD="/usr/lib64/ccache/gcc" GCC_FOR_BUILD="/usr/lib64/ccache/gcc" CXX_FOR_BUILD="/usr/lib64/ccache/g++" FC_FOR_BUILD="/usr/bin/ld" LD_FOR_BUILD="/usr/bin/ld" CFLAGS_FOR_BUILD="" CXXFLAGS_FOR_BUILD="" LDFLAGS_FOR_BUILD="" FCFLAGS_FOR_BUILD="" AR_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ar" AS_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" CC_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc" LD_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" NM_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-nm" DEFAULT_ASSEMBLER="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" DEFAULT_LINKER="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" PKG_CONFIG_SYSROOT_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir" PKG_CONFIG="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/bin/pkg-config" PKG_CONFIG_PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/lib/pkgconfig:" STAGING_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir" \
CFLAGS_FOR_TARGET="-Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include -L/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/lib -L/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/lib -Wno-error" \
CFLAGS="-Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include -L/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/lib -L/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/lib -Wno-error" \
ac_cv_type_uintptr_t=yes gt_cv_func_gettext_libintl=yes ac_cv_func_dcgettext=yes gdb_cv_func_sigsetjmp=yes bash_cv_func_strcoll_broken=no bash_cv_must_reinstall_sighandlers=no bash_cv_func_sigsetjmp=present bash_cv_have_mbstate_t=yes \
/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/toolchain_build_mips/gdb-6.7.1/configure \
--cache-file=/dev/null \
--build=x86_64-pc-linux-gnu \
--host=mips-linux-gnu \
--target=mips-linux-gnu \
--prefix=/usr \
--disable-nls \
--without-uiout  \
--disable-tui --disable-gdbtk --without-x \
--disable-sim --enable-gdbserver \
--without-included-gettext \
--disable-werror

touch /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/gdb-6.7.1-target/.configured
/usr/bin/make -j1 CC=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc \
LDFLAGS=-static \
MT_CFLAGS="-Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" \
-C /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/gdb-6.7.1-target


PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

gdb_cv_func_sigsetjmp=yes \
PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/bin:/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/bin:/home/nds-uk/kyoupark/spk-out-use-own-brcm/toolchain_build_mips/bin:/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-ar" AS="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-as" LD="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-ld" NM="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-nm" CC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-gcc -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/mips-linux-gnu/include" GCC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-gcc -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/mips-linux-gnu/include" CPP="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-cpp -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/mips-linux-gnu/include" CXX="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-g++ -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/mips-linux-gnu/include" FC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-gfortran " RANLIB="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-ranlib" STRIP="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-strip" OBJCOPY="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-objcopy" AR_FOR_BUILD="/usr/bin/ar" AS_FOR_BUILD="/usr/bin/as" CC_FOR_BUILD="/usr/lib64/ccache/gcc" GCC_FOR_BUILD="/usr/lib64/ccache/gcc" CXX_FOR_BUILD="/usr/lib64/ccache/g++" FC_FOR_BUILD="/usr/bin/ld" LD_FOR_BUILD="/usr/bin/ld" CFLAGS_FOR_BUILD="" CXXFLAGS_FOR_BUILD="" LDFLAGS_FOR_BUILD="" FCFLAGS_FOR_BUILD="" AR_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-ar" AS_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-as" CC_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-gcc" LD_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-ld" NM_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-nm" DEFAULT_ASSEMBLER="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-as" DEFAULT_LINKER="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-ld" PKG_CONFIG_SYSROOT_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir" PKG_CONFIG="/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/bin/pkg-config" PKG_CONFIG_PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/lib/pkgconfig:" STAGING_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir" \
CFLAGS_FOR_TARGET="-Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/mips-linux-gnu/include -L/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/lib -L/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/lib -Wno-error" \
CFLAGS="-Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/mips-linux-gnu/include -L/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/lib -L/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/lib -Wno-error" \
ac_cv_type_uintptr_t=yes gt_cv_func_gettext_libintl=yes ac_cv_func_dcgettext=yes gdb_cv_func_sigsetjmp=yes bash_cv_func_strcoll_broken=no bash_cv_must_reinstall_sighandlers=no bash_cv_func_sigsetjmp=present bash_cv_have_mbstate_t=yes \
/home/nds-uk/kyoupark/spk-out-use-own-brcm/toolchain_build_mips/gdb-6.7.1/configure \
--cache-file=/dev/null \
--build=x86_64-pc-linux-gnu \
--host=mips-linux-gnu \
--target=mips-linux-gnu \
--prefix=/usr \
--disable-nls \
--without-uiout  \
--disable-tui --disable-gdbtk --without-x \
--disable-sim --enable-gdbserver \
--without-included-gettext \
--disable-werror

touch /home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/gdb-6.7.1-target/.configured
/usr/bin/make -j1 CC=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/bin/mips-linux-gnu-gcc \
LDFLAGS=-static \
MT_CFLAGS="-Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-two/mips-linux-gnu/include" \
-C /home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/gdb-6.7.1-target


// gdb_cv_func_sigsetjmp=yes \
// PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/bin:/home/nds-uk/kyoupark/spk-out-use-own-brcm/toolchain_build_mips/bin:/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" \
// AR="mips-linux-gnu-ar" AS="mips-linux-gnu-as" LD="mips-linux-gnu-ld" \
// NM="mips-linux-gnu-nm" CC="mips-linux-gnu-gcc -Os \
// -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include \
// -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include \
// -I${PREFIX}/mips-linux-gnu/include \
// GCC="mips-linux-gnu-gcc -Os \
// -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include \
// -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include \
// -I${PREFIX}/mips-linux-gnu/include \
// CPP="mips-linux-gnu-cpp -Os \
// -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include \
// -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include \
// -I${PREFIX}/mips-linux-gnu/include \
// CXX="mips-linux-gnu-g++ -Os \
// -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include \
// -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include \
// -I${PREFIX}/mips-linux-gnu/include \
// FC="mips-linux-gnu-gfortran" \
// RANLIB="mips-linux-gnu-ranlib" \
// STRIP="mips-linux-gnu-strip" \
// OBJCOPY="mips-linux-gnu-objcopy" \
// AR_FOR_BUILD="/usr/bin/ar" AS_FOR_BUILD="/usr/bin/as" \
// CC_FOR_BUILD="/usr/lib64/ccache/gcc" GCC_FOR_BUILD="/usr/lib64/ccache/gcc" \
// CXX_FOR_BUILD="/usr/lib64/ccache/g++" FC_FOR_BUILD="/usr/bin/ld" \
// LD_FOR_BUILD="/usr/bin/ld" CFLAGS_FOR_BUILD="" CXXFLAGS_FOR_BUILD="" \
// LDFLAGS_FOR_BUILD="" FCFLAGS_FOR_BUILD="" \
// AR_FOR_TARGET="mips-linux-gnu-ar" \
// AS_FOR_TARGET="mips-linux-gnu-as" \
// CC_FOR_TARGET="mips-linux-gnu-gcc" \
// LD_FOR_TARGET="mips-linux-gnu-ld" \
// NM_FOR_TARGET="mips-linux-gnu-nm" \
// DEFAULT_ASSEMBLER="mips-linux-gnu-as" \
// DEFAULT_LINKER="mips-linux-gnu-ld" \
// PKG_CONFIG_SYSROOT_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir" \
// PKG_CONFIG="/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/bin/pkg-config" \
// PKG_CONFIG_PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/lib/pkgconfig:" \
// STAGING_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir" \
// CFLAGS_FOR_TARGET="-Os \
// -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include \
// -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include \
// -I${PREFIX}/mips-linux-gnu/include \
// -L/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/lib \
// -L/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/lib \
// -Wno-error" \ 
// CFLAGS="-Os -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include \
// -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include \
// -I${PREFIX}/mips-linux-gnu/include \
// -L/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/lib \
// -L/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/lib \
// -Wno-error" \ 
// ac_cv_type_uintptr_t=yes gt_cv_func_gettext_libintl=yes \
// ac_cv_func_dcgettext=yes gdb_cv_func_sigsetjmp=yes \
// bash_cv_func_strcoll_broken=no bash_cv_must_reinstall_sighandlers=no \
// bash_cv_func_sigsetjmp=present bash_cv_have_mbstate_t=yes \
// /home/nds-uk/kyoupark/spk-out-use-own-brcm/toolchain_build_mips/gdb-6.7.1/configure \
// --cache-file=/dev/null \
// --build=x86_64-pc-linux-gnu \
// --host=mips-linux-gnu \
// --target=mips-linux-gnu \
// --prefix=/usr \
// --disable-nls \
// --without-uiout \
// --disable-tui --disable-gdbtk --without-x \ --disable-sim --enable-gdbserver \
// --without-included-gettext --disable-werror
// 
// touch /home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/gdb-6.7.1-target/.configured
// /usr/bin/make -j1 CC=mips-linux-gnu-gcc \
// LDFLAGS=-static \
// MT_CFLAGS="-Os -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/staging_dir/usr/include -I${PREFIX}/mips-linux-gnu/include" \
// -C /home/nds-uk/kyoupark/spk-out-use-own-brcm/build_mips/gdb-6.7.1-target


<spk-bash>
kyoupark@ukstbuild2:~/si_logs/spk/darwin-spk-1.19/package/bash$ ls
bash32-001  bash32-004  bash32-007  bash32-010  bash32-013  bash32-016  bash32-019  bash32-022  bash32-025  bash32-028  bash32-031  bash32-034-noreconf.patch            bash32-remove-bzero-dependancy.patch
bash32-002  bash32-005  bash32-008  bash32-011  bash32-014  bash32-017  bash32-020  bash32-023  bash32-026  bash32-029  bash32-032  bash32-035-curses.patch.arm          bash.mk
bash32-003  bash32-006  bash32-009  bash32-012  bash32-015  bash32-018  bash32-021  bash32-024  bash32-027  bash32-030  bash32-033  bash32-fix-termap-header-path.patch  Config.in

cd /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/bash-3.2; 
rm -rf config.cache;

PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/bin:/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/bin:/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/toolchain_build_mips/bin:/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" \
AR="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ar" \
AS="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" \
LD="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" \
NM="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-nm" \
CC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" GCC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" \
CPP="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-cpp -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" \
CXX="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-g++ -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" \
FC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gfortran " \
RANLIB="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ranlib" \
STRIP="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-strip" \
OBJCOPY="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-objcopy" \
AR_FOR_BUILD="/usr/bin/ar" AS_FOR_BUILD="/usr/bin/as" CC_FOR_BUILD="/usr/lib64/ccache/gcc" GCC_FOR_BUILD="/usr/lib64/ccache/gcc" CXX_FOR_BUILD="/usr/lib64/ccache/g++" FC_FOR_BUILD="/usr/bin/ld" LD_FOR_BUILD="/usr/bin/ld" \
CFLAGS_FOR_BUILD="" CXXFLAGS_FOR_BUILD="" LDFLAGS_FOR_BUILD="" FCFLAGS_FOR_BUILD="" \
AR_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ar" \
AS_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" \
CC_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc" \ 
LD_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" \ 
NM_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-nm" \ 
DEFAULT_ASSEMBLER="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" \
DEFAULT_LINKER="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" \
PKG_CONFIG_SYSROOT_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir" \
PKG_CONFIG="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/bin/pkg-config" \
PKG_CONFIG_PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/lib/pkgconfig:" \
STAGING_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir" \
ac_cv_lbl_unaligned_fail=yes \
ac_cv_func_mmap_fixed_mapped=yes \
ac_cv_func_memcmp_working=yes \
ac_cv_have_decl_malloc=yes \
gl_cv_func_malloc_0_nonnull=yes \
ac_cv_func_malloc_0_nonnull=yes \
ac_cv_func_calloc_0_nonnull=yes \
ac_cv_func_realloc_0_nonnull=yes \
ac_cv_c_bigendian=yes \
CCFLAGS_FOR_BUILD="" \
ac_cv_func_setvbuf_reversed=no \
ac_cv_have_decl_sys_siglist=yes \
bash_cv_job_control_missing=present \
bash_cv_sys_named_pipes=present \
bash_cv_unusable_rtsigs=no \
bash_cv_func_ctype_nonascii=yes \
bash_cv_decl_under_sys_siglist=yes \
bash_cv_ulimit_maxfds=yes \
bash_cv_getcwd_malloc=yes \
bash_cv_func_sigsetjmp=present \
bash_cv_printf_a_format=yes \
./configure \
--target=mips-linux-gnu \
--host=mips-linux-gnu \
--build=x86_64-pc-linux-gnu \
--prefix=/usr \
--exec-prefix=/usr \
--bindir=/usr/bin \
--sbindir=/usr/sbin \
--libdir=/lib \
--libexecdir=/usr/lib \
--sysconfdir=/etc \
--datadir=/usr/share \
--localstatedir=/var \
--mandir=/usr/share/man \
--infodir=/usr/share/info \
--includedir=/usr/include \
--disable-nls \
--without-curses \
--enable-alias \
--without-bash-malloc

PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

touch /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/bash-3.2/.configured
/usr/bin/make -j1 CC=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc CC_FOR_BUILD="/usr/lib64/ccache/gcc" -C /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/bash-3.2

-rwxr-xr-x 1 kyoupark ccusers 2251065 Mar 17 16:04 bash

<use-static>
/usr/bin/make -j1 \
LDFLAGS=-static \
CC=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc \
CC_FOR_BUILD="/usr/lib64/ccache/gcc" -C /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/bash-3.2

-rwxr-xr-x 1 kyoupark ccusers 5905254 Mar 21 15:11 bash


PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/bin:/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/bin:/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/toolchain_build_mips/bin:/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" \
AR="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ar" \
AS="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" \
LD="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" \
NM="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-nm" \
CC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" GCC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" \
CPP="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-cpp -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" \
CXX="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-g++ -Os  -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/include -I/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/include -I/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/include" \
FC="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gfortran " \
RANLIB="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ranlib" \
STRIP="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-strip" \
OBJCOPY="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-objcopy" \
AR_FOR_BUILD="/usr/bin/ar" AS_FOR_BUILD="/usr/bin/as" CC_FOR_BUILD="/usr/lib64/ccache/gcc" GCC_FOR_BUILD="/usr/lib64/ccache/gcc" CXX_FOR_BUILD="/usr/lib64/ccache/g++" FC_FOR_BUILD="/usr/bin/ld" LD_FOR_BUILD="/usr/bin/ld" \
CFLAGS_FOR_BUILD="" CXXFLAGS_FOR_BUILD="" LDFLAGS_FOR_BUILD="" FCFLAGS_FOR_BUILD="" \
AR_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ar" \
AS_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" \
CC_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc" \ 
LD_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" \ 
NM_FOR_TARGET="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-nm" \ 
DEFAULT_ASSEMBLER="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-as" \
DEFAULT_LINKER="/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-ld" \
PKG_CONFIG_SYSROOT_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir" \
PKG_CONFIG="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/bin/pkg-config" \
PKG_CONFIG_PATH="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir/usr/lib/pkgconfig:" \
STAGING_DIR="/home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/build_mips/staging_dir" \
CCFLAGS_FOR_BUILD="" \
./configure \
--target=mips-linux-gnu \
--host=mips-linux-gnu \
--build=x86_64-pc-linux-gnu \
--prefix=/usr \
--exec-prefix=/usr \
--bindir=/usr/bin \
--sbindir=/usr/sbin \
--libdir=/lib \
--libexecdir=/usr/lib \
--sysconfdir=/etc \
--datadir=/usr/share \
--localstatedir=/var \
--mandir=/usr/share/man \
--infodir=/usr/share/info \
--includedir=/usr/include \
--disable-nls \
--without-curses \
--enable-alias \
--without-bash-malloc

/usr/bin/make -j1 LDFLAGS=-static 


={============================================================================
*kt_linux_gcc_400* gcc-toolchain-reference gcc-build

http://preshing.com/20141119/how-to-build-a-gcc-cross-compiler/

note:
binutils-2.24, linux-3.17.2, glibc-2.20
4.9.2 builds with exact version and settings in the blog.
4.8.2 builds with no changes on prerequsites.

// ok, vstb builds
CT_BINUTILS_VERSION="2.28"
CT_KERNEL_VERSION="4.10.8"
CT_LIBC_VERSION="2.25"
CT_CC_GCC_VERSION="4.9.4"

// ok, 
CT_TOOLCHAIN_PKGVERSION="for-host-32"
CT_KERNEL_VERSION="4.9"
CT_BINUTILS_VERSION="2.28"
CT_LIBC_VERSION="2.24"
CT_CC_GCC_VERSION="6.3.0"

binutils-2.28, linux-3.14.43, glibc-2.25 gcc 4.9.4 builds on build server
binutils-2.28, linux-3.14.43, glibc-2.25 gcc 4.9.4 not builds on VM when do
manually and to use cross-ng

note:
Since do not support sysroot *gcc-sysroot* for this build, need to have same
directory layout if want to move toolchain built to other servers.


Required Packages
=================
Starting with a clean Debian system, you must first install a few packages:

$ sudo apt-get install g++ make gawk

Everything else will be built from source. Create a new directory somewhere,
and download the following source packages. 

*gcc-source* *gcc-get*
wget https://ftp.gnu.org/gnu/binutils/binutils-2.24.tar.gz
wget http://ftpmirror.gnu.org/gcc/gcc-4.9.2/gcc-4.9.2.tar.gz
wget --no-check-certificate https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.17.2.tar.xz
wget --no-check-certificate http://ftpmirror.gnu.org/glibc/glibc-2.20.tar.xz

wget --no-check-certificate ftp://gcc.gnu.org/pub/gcc/infrastructure/isl-0.12.2.tar.bz2
wget --no-check-certificate ftp://gcc.gnu.org/pub/gcc/infrastructure/cloog-0.18.1.tar.gz

wget https://ftp.gnu.org/gnu/binutils/binutils-2.28.tar.gz
wget http://ftpmirror.gnu.org/gcc/gcc-4.9.4/gcc-4.9.4.tar.gz
wget --no-check-certificate https://mirrors.edge.kernel.org/pub/linux/kernel/v3.x/linux-3.14.43.tar.xz
wget --no-check-certificate http://mirror.koddos.net/gnu/glibc/glibc-2.25.tar.xz

The first four packages  Binutils, GCC, the Linux kernel and Glibc  are the
main ones. We could have installed the next three packages in binary form
using our system's package manager instead, but that tends to provide older
versions. The last two packages, ISL and CLooG, are `optional`, but they enable
a few more optimizations in the compiler we’re about to build.

wget --no-check-certificate https://mirrors.edge.kernel.org/pub/linux/kernel/v4.x/linux-4.9.tar.gz

How The Pieces Fit Together
===========================
By the time we're finished, we will have built each of the following programs
and libraries. First, we'll build the tools on the left, then we'll use those
tools to build the programs and libraries on the right. 


Host Programs                             
(Instruction set: x64)                    

C cross-compiler
aarch64-linux-gcc built from GCC

C++ cross-compiler
aarch64-linux-gcc built from GCC

cross-assember, cross-linker
aarch64-linux-ld built from binutils
aarch64-linux-as built from binutils

      ||
      \/

Target
instruction set: aarch64

+---------------------------------+
|     a.out                       |
|       +-------------------------+
|       | standard C++ library    |
|       | libstdc++.so from GCC   |
+-------+-------------------------+
|   standard C library            |
|   libc.so from GCC              |
+---------------------------------+
|   Linux kernel                  |
+---------------------------------+

The compilers on the left will invoke the assembler & linker as part of their
job. All the other packages we downloaded, such as MPFR, GMP and MPC, will be
linked into the compilers themselves.

The diagram on the right represents a sample program, a.out, running on the
target OS, built using the cross compiler and linked with the target system's
standard C and C++ libraries. The standard C++ library makes calls to the
standard C library, and the C library makes direct system calls to the AArch64
Linux kernel.

Note that instead of using Glibc as the standard C library implementation, we
could have used Newlib, an alternative implementation. `Newlib` is a popular C
library implementation for embedded devices. Unlike Glibc, Newlib doesn’t
require a complete OS on the target system  just a thin hardware abstraction
layer called Libgloss. Newlib doesn’t have regular releases; instead, you’re
meant to pull the source directly from the Newlib CVS repository. One
limitation of Newlib is that currently, it doesn’t seem to support building
multithreaded programs for AArch64. That’s why I chose not to use it here.


Build Steps
===============================
Choose an installation directory, and make sure you have write permission to
it. In the steps that follow, I'll install the new toolchain to /opt/cross.

$ sudo mkdir -p /opt/cross
$ sudo chown jeff /opt/cross


0. PATH
===========
Throughout the entire build process, make sure the installation's bin
subdirectory is in your PATH environment variable. You can remove this
directory from your PATH later, but most of the build steps expect to find
aarch64-linux-gcc and other host tools via the PATH by default.

// for arm
PREFIX=`/opt/cross`
export PATH=/opt/cross/bin:$PATH
export PATH=${PREFIX}/bin:$PATH

// for mips
PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-reference
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

*build-gcc-x86*
cd /home/kyoupark/asan/gcc
mkdir i686-nptl-linux-gnu-gcc
PREFIX=/home/kyoupark/asan/gcc/i686-nptl-linux-gnu-gcc
TARGET=i686-nptl-linux-gnu
export PATH=${PREFIX}/bin:$PATH


0. SYSTEM ROOT: ${PREFIX}=`/opt/cross/{target}`
===========
Pay particular attention to the stuff that gets installed under
`/opt/cross/aarch64-linux/`. This directory is considered the system root of an
imaginary AArch64 Linux target system. A self-hosted AArch64 Linux compiler
could, in theory, use all the headers and libraries placed here. Obviously,
none of the programs built for the host system, such as the cross-compiler
itself, will be installed to this directory.


1. Binutils: installed to "${PREFIX}/bin"
===========
note:
installs /gcc-glibc-mips-reference/mips-linux-gnu/lib/ldscripts and where it
installs it depends on on binutil version. For example, binutil 2.17 installs
it in the different directory.

This step builds and installs the cross-assembler, cross-linker, and other tools.

$ mkdir build-binutils-xx
$ cd build-binutils
$ ../binutils-2.24/configure --prefix=/opt/cross --target=aarch64-linux --disable-multilib
$ make -j4
$ make install
$ cd ..

// for mips
../binutils-2.24/configure --prefix=${PREFIX} --target=${TARGET} --disable-multilib
make -j4
make install

// for x86
../binutils-2.28/configure --prefix=${PREFIX} --target=${TARGET} --disable-multilib

// installed
rwxr-xr-x 5 kyoupark kyoupark 4096 Sep 19 15:19 share/
drwxr-xr-x 2 kyoupark kyoupark 4096 Sep 19 15:19 bin/
drwxr-xr-x 4 kyoupark kyoupark 4096 Sep 19 15:19 i686-nptl-linux-gnu/


*gcc-build-target-triplet*
We've specified aarch64-linux as the target system type. Binutils's configure
script will recognize that this target is different from the machine we're
building on, and configure a cross-assembler and cross-linker as a result. 

The tools `will be installed to /opt/cross/bin,` their names prefixed by
aarch64-linux-. 

`--disable-multilib` means that we only want our Binutils installation to work
with programs and libraries using the AArch64 instruction set, and not any
related instruction sets such as AArch32.


note: 
failed to build 2.24 but build 2.25.1 on debian VM.
kyoupark@kit-debian:~/si-logs/gcc/build-binutil$ uname -a
Linux kit-debian 3.16.0-4-686-pae #1 SMP Debian 3.16.7-ckt25-2+deb8u3 (2016-07-02) i686 GNU/Linux

kyoupark@kit-debian64:~/asan/gcc$ uname -a
Linux kit-debian64 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux

kyoupark@kit-debian:~/si-logs/gcc/build-binutil$ gcc --version
gcc (Debian 4.9.2-10) 4.9.2
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

note:
http://wiki.osdev.org/GCC_Cross-Compiler

`--disable-nls` tells binutils not to include native language support. This is
basically optional, but reduces dependencies and compile time. It will also
result in English-language diagnostics, which the people on the Forum
understand when you ask your questions. ;-)


2. Linux Kernel Headers: installed to ${PREFIX}/${TARGET}/include
=======================
This step installs the Linux kernel header files to ${INSTALL_HDR_PATH}/include,
which will ultimately allow programs built using our new toolchain to make
  system calls to the AArch64 kernel in the target environment.

$ cd linux-3.17.2
$ make ARCH=arm64 INSTALL_HDR_PATH=`/opt/cross/aarch64-linux` headers_install
$ cd ..

We could even have done this before installing Binutils.

Because the Linux kernel is a different open-source project from the others,
it has a `different way of identifying the target` CPU architecture:
ARCH=arm64


*build-gcc-mips*
cd linux-3.17.2
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install

*build-gcc-x86*
cd linux-3.17.2
make ARCH=x86 INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install

// this installs to ${PREFIX}/${TARGET}/include
// asm  asm-generic  linux  mtd  rdma  scsi  sound  video

All of the remaining steps involve building GCC and Glibc. The trick is that
there are parts of GCC which depend on parts of Glibc already being built, and
vice versa. We can't build either package in a single step; we need to go back
and forth between the two packages and build their components in a way that
satisfies their dependencies.


<build-order>
GCC components          Glibc componenets
C/C++ compilers
                    ->  standard C library headers and starup files
                        stdio.h,... crti.o/libc.so
Compiler support    <-
library
libgcc.a/libgcc.so  ->  standard C library
                        libc.a/libc.so
standard C++ 
library             <-
libstdc++.a/libstdc++.so 


3. C/C++ Compilers (pass 1): installed to ${PREFIX}/bin
==================
This step will build `GCC's C and C++ cross-compilers only`, and install them to
/opt/cross/bin. It won't invoke those compilers to build any libraries just
yet.


Extract all the source packages.

$ for f in *.tar*; do tar xf $f; done

Create symbolic links from the GCC directory to some of the other directories.
These five packages are dependencies of GCC, and when the symbolic links are
present, GCC's build script will build them automatically.

$ cd gcc-4.9.2

// note:
// https://gcc.gnu.org/install/download.html
// 
// Downloading GCC
// 
// If you also intend to build binutils (either to upgrade an existing
// installation or for use in place of the corresponding tools of your OS),
// unpack the binutils distribution either in the same directory or a separate
// one. In the latter case, add symbolic links to any components of the
// binutils you intend to build alongside the compiler (bfd, binutils, gas,
// gprof, ld, opcodes, ...) to the directory containing the GCC sources.
// 
// Likewise the GMP, MPFR and MPC libraries can be automatically built
// together with GCC. You may simply run the contrib/download_prerequisites
// script in the GCC source directory to set up everything. Otherwise unpack
// the GMP, MPFR and/or MPC source distributions in the directory containing
// the GCC sources and rename their directories to gmp, mpfr and mpc,
// respectively (or use symbolic links with the same name). 


// run this script which will download and set up sym links
./contrib/download_prerequisites

// when downloads fails, do manually and make sym links like:
wget https://ftp.gnu.org/gnu/mpfr/mpfr-2.4.2.tar.bz2
wget https://ftp.gnu.org/gnu/gmp/gmp-4.3.2.tar.bz2
...
ln -s ../mpfr-3.1.2 mpfr
ln -s ../gmp-6.0.0 gmp
ln -s ../mpc-1.0.2 mpc
...


*build-gcc-reference*
$ mkdir -p build-gcc
$ cd build-gcc
$ ../gcc-4.9.2/configure --prefix=/opt/cross --target=aarch64-linux --enable-languages=c,c++ --disable-multilib
$ ../gcc-4.9.4/configure --prefix=/home/kyoupark/asan/gcc/i686-nptl-linux-gnu-gcc --target=i686-nptl-linux-gnu --enable-languages=c,c++ --disable-multilib CFLAGS=-g3 -Og CXXFLAGS=-g3 -Og
build-glibc-refe
$ make -j4 all-gcc
$ make install-gcc
$ cd ..

*build-gcc-mips*
cd build-gcc
../gcc-4.8.2/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ --disable-multilib
make -j4 all-gcc
make install-gcc

*build-gcc-x86*
cd build-gcc
../gcc-4.9.4/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ --disable-multilib


Because we've specified --target=aarch64-linux, the build script looks for the
Binutils cross-tools we built in step 1 with names prefixed by aarch64-linux-.
Likewise, the C/C++ compiler names will be prefixed by aarch64-linux-.

`--enable-languages=c,c++` prevents other compilers in the GCC suite, such as
Fortran, Go or Java, from being built.

// note:
// must use "all-gcc". Since when runs "make", build fails on:
// 
// shows error:
// 
// checking linker for .eh_frame personality relaxation... no
// *** This configuration requires the GNU assembler
// make[1]: *** [configure-gcc] Error 1
// make[1]: Leaving directory `/home/NDS-UK/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu'
// 
// // gcc/configure.ac
// 
// # Mips and HP-UX need the GNU assembler.
// # Linux on IA64 might be able to use the Intel assembler.
// 
// case "$target" in
//   mips*-*-* | *-*-hpux* )
//     if test x$gas_flag = xyes \
//     else
//       echo "*** This configuration requires the GNU assembler" >&2
//       exit 1
//     fi
//     ;;
// esac
// 
// try to add "--with-gnu-as" since this will set $gas_flag:
// 
// Checking multilib configuration for libgcc...
// mkdir -p -- mipsel-linux-gnu/libgcc
// `Configuring in mipsel-linux-gnu/libgcc`
// configure: creating cache ./config.cache
// checking build system type... x86_64-unknown-linux-gnu
// checking host system type... mipsel-unknown-linux-gnu
// checking for --enable-version-specific-runtime-libs... no
// checking for a BSD-compatible install... /usr/bin/install -c
// checking for gawk... gawk
// checking for mipsel-linux-gnu-ar... mipsel-linux-gnu-ar
// checking for mipsel-linux-gnu-lipo... mipsel-linux-gnu-lipo
// checking for mipsel-linux-gnu-nm... /home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu/./gcc/nm
// checking for mipsel-linux-gnu-ranlib... mipsel-linux-gnu-ranlib
// checking for mipsel-linux-gnu-strip... mipsel-linux-gnu-strip
// checking whether ln -s works... yes
// checking for mipsel-linux-gnu-gcc... /home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu/./gcc/xgcc -B/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu/./gcc/ -B/home/nds-uk/kyoupark/gcc-bin/mipsel-linux-gnu/bin/ -B/home/nds-uk/kyoupark/gcc-bin/mipsel-linux-gnu/lib/ -isystem /home/nds-uk/kyoupark/gcc-bin/mipsel-linux-gnu/include -isystem /home/nds-uk/kyoupark/gcc-bin/mipsel-linux-gnu/sys-include
// checking for suffix of object files... configure: error: in `/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu/mipsel-linux-gnu/libgcc':
// configure: error: cannot compute suffix of object files: cannot compile
// See `config.log' for more details.
// make[1]: *** [configure-target-libgcc] Error 1
// make[1]: Leaving directory `/home/NDS-UK/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu'
// make: *** [all] Error 2
// 
// try to use "--disable-miltilib" in order not to use multilib:
// try to use own binutil but fails on the same.
// 
// https://ftp.gnu.org/gnu/binutils/
// binutils-2.24.tar.bz2	2013-12-02 05:09 	22M	
// 
// http://wiki.osdev.org/Cross-Compiler_Successful_Builds
// 4.8.2 Yes 	Yes 
//       2.24, 2.23.2 	
// 
// ../binutils-2.24/configure --target=mipsel-linux-gnu --prefix=/home/nds-uk/kyoupark/asn/install --with-sysroot --disable-nls --disable-werror
// make
// make install
// 
// export PATH=~/asn/install/bin:$PATH

// 
// --disable-nls is the same as for binutils above.
// 
// --without-headers tells GCC not to rely on any C library (standard or runtime)
// being present for the target. 
//
// --without-headers
//  Tells GCC `not use any target headers` from a libc when building a cross
//  compiler. When crossing to GNU/Linux, you need the headers so GCC can
//  build the exception handling for libgcc. 
//
// Now you have a "naked" cross-compiler. It `does not have` access to a C
// library or C runtime yet, so you cannot use any of the standard includes or
// create runnable binaries. But it is quite `sufficient to compile the kernel`
//
// How this compiler is not able to compile normal C programs. The
// cross-compiler will spit errors whenever you want to #include any of the
// standard headers (except for a select few that actually are
// platform-independent, and generated by the compiler itself). This is quite
// correct - you don't have a standard library for the target system yet!
// 
// The C standard defines two different kinds of executing environments -
// "freestanding" and "hosted". While the definition might be rather fuzzy for
// the average application programmer, it is pretty clear-cut when you're
// doing OS development: A kernel is "freestanding", everything you do in user
// space is "hosted". A "freestanding" environment needs to provide only a
// `subset` of the C library: float.h, iso646.h, limits.h, stdalign.h, stdarg.h,
// stdbool.h, stddef.h, stdint.h and stdnoreturn.h (as of C11). All of these
// consist of typedef s and #define s "only", so you can implement them
// without a single .c file in sight. 


note: installs in ${prefix}/bin
-rwxr-xr-x 2 kyoupark ccusers 2259227 Dec  6 09:59 mips-unknown-linux-uclibc-c++*
-rwxr-xr-x 1 kyoupark ccusers 2253021 Dec  6 09:59 mips-unknown-linux-uclibc-cpp*
-rwxr-xr-x 2 kyoupark ccusers 2259227 Dec  6 09:59 mips-unknown-linux-uclibc-g++*
-rwxr-xr-x 2 kyoupark ccusers 2251225 Dec  6 09:59 mips-unknown-linux-uclibc-gcc*
-rwxr-xr-x 2 kyoupark ccusers 2251225 Dec  6 09:59 mips-unknown-linux-uclibc-gcc-4.8.2*
-rwxr-xr-x 1 kyoupark ccusers  110602 Dec  6 09:59 mips-unknown-linux-uclibc-gcc-ar*
-rwxr-xr-x 1 kyoupark ccusers  110546 Dec  6 09:59 mips-unknown-linux-uclibc-gcc-nm*
-rwxr-xr-x 1 kyoupark ccusers  110558 Dec  6 09:59 mips-unknown-linux-uclibc-gcc-ranlib*
-rwxr-xr-x 1 kyoupark ccusers 1241641 Dec  6 09:59 mips-unknown-linux-uclibc-gcov*


4. glibc Headers and Startup Files: installed to ${PREFIX}/${TARGET}/include,lib
===============================================
note:
Before this, must have cross gcc, binuitl built, installed, and have it in the
PATH.

In this step, we install Glibc's `standard C library headers` to
/opt/cross/aarch64-linux/include. 

We also use the C compiler built in step 3 to compile the library's startup
files and install them to /opt/cross/aarch64-linux/lib. 

Finally, we create a couple of `dummy files`, *libc.so* and stubs.h, which are
expected in step 5, but which will be replaced in step 6.


$ mkdir -p build-glibc
$ cd build-glibc
$ ../glibc-2.20/configure --prefix=/opt/cross/aarch64-linux --build=$MACHTYPE --host=aarch64-linux --target=aarch64-linux 
  --with-headers=/opt/cross/aarch64-linux/include --disable-multilib libc_cv_forced_unwind=yes
$ make install-bootstrap-headers=yes install-headers
$ make -j4 csu/subdir_lib
$ install csu/crt1.o csu/crti.o csu/crtn.o /opt/cross/aarch64-linux/lib
$ aarch64-linux-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o /opt/cross/aarch64-linux/lib/libc.so
$ touch /opt/cross/aarch64-linux/include/gnu/stubs.h
$ cd ..

--prefix=/opt/cross/aarch64-linux tells Glibc's configure script where it
should `install its headers and libraries` Note that it's different from the
usual --prefix.

Despite some contradictory information out there, Glibc's configure script
currently requires us to specify all three --build, --host and --target system
types.

$MACHTYPE is a predefined environment variable which describes the machine
running the build script. --build=$MACHTYPE is needed because in step 6, the
build script will compile some additional tools which run as part of the build
process itself.

// kyoupark@ukstbuild2:~/asn/gcc/glibc-2.20-build$ echo $MACHTYPE
// x86_64-redhat-linux-gnu

<host-target>
--host has `a different meaning` here than we've been using so far. In Glibc's
configure, both the --host and --target options are meant to describe the
system on which Glibc's libraries will ultimately run.

We install the C library's startup files, crt1.o, crti.o and crtn.o, to the
installation directory manually. There's doesn't seem to a make rule that does
this without having other side effects.

note:
check on --with-headers which is about linux header

note:
config error if not --host=mips-linux and --target=${TARGET} are the same.

// ../glibc-2.20/configure --prefix=${PREFIX}/${TARGET} \
// --build=$MACHTYPE --host=mips-linux --target=${TARGET} \
// --with-headers=${PREFIX}/${TARGET}/include \
// --disable-multilib libc_cv_forced_unwind=yes

*build-gcc-mips*
cd glibc-2.20-build
../glibc-2.20/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=${TARGET} --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-multilib libc_cv_forced_unwind=yes

make install-bootstrap-headers=yes install-headers
make -j4 csu/subdir_lib
install csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
touch ${PREFIX}/${TARGET}/include/gnu/stubs.h

~/asn/gcc/gcc-glibc-mips-reference/mips-linux-gnu/lib
-rwxr-xr-x 1 kyoupark ccusers 3152 Apr  6 07:39 crt1.o*
-rwxr-xr-x 1 kyoupark ccusers 1980 Apr  6 07:39 crti.o*
-rwxr-xr-x 1 kyoupark ccusers 1680 Apr  6 07:39 crtn.o*
drwxr-xr-x 2 kyoupark ccusers 4096 Apr  6 07:35 ldscripts/
-rwxr-xr-x 1 kyoupark ccusers 1701 Apr  6 07:39 libc.so*


*build-gcc-x86*
cd builddir
../glibc-2.25/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=${TARGET} --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-multilib libc_cv_forced_unwind=yes

make install-bootstrap-headers=yes install-headers
make -j4 csu/subdir_lib

// fails on VM on "make -j4 csu/subdir_lib" but works on server

make[2]: Entering directory '/home/kyoupark/asan/gcc/glibc-2.25/csu'
gawk -f ../scripts/gen-as-const.awk ../sysdeps/i386/nptl/tcb-offsets.sym \ |
gcc -S -o /home/kyoupark/asan/gcc/build-glibc/tcb-offsets.hT3 -std=gnu11
-fgnu89-inline  -O2 -Wall -Werror -Wundef -Wwrite-strings
-fmerge-all-constants -fno-stack-protector -frounding-math -g
-Wstrict-prototypes -Wold-style-definition -mpreferred-stack-boundary=4
-ftls-model=initial-exec      -I../include
-I/home/kyoupark/asan/gcc/build-glibc/csu
-I/home/kyoupark/asan/gcc/build-glibc  -I../sysdeps/unix/sysv/linux/i386/i686
-I../sysdeps/i386/i686/nptl  -I../sysdeps /unix/sysv/linux/i386
-I../sysdeps/unix/sysv/linux/x86  -I../sysdeps/i386/nptl
-I../sysdeps/unix/sysv/linux/include -I../sysdeps/unix/sysv/linux
-I../sysdeps/nptl  -I../sysdeps/pthread  -I../sysdeps/gnu
-I../sysdeps/unix/inet  -I../sy sdeps/unix/sysv  -I../sysdeps/unix/i386
-I../sysdeps/unix  -I../sysdeps/posix  -I../sysdeps/i386/i686/fpu/multiarch
-I../sysdeps/i386/i686/fpu  -I../sysdeps/i386/i686/multiarch
-I../sysdeps/i386/i686  -I../sysdeps/i386/fpu  -I../sysdep s/x86/fpu/include
-I../sysdeps/x86/fpu  -I../sysdeps/i386  -I../sysdeps/x86
-I../sysdeps/wordsize-32  -I../sysdeps/ieee754/ldbl-96/include
-I../sysdeps/ieee754/ldbl-96  -I../sysdeps/ieee754/dbl-64
-I../sysdeps/ieee754/flt-32  -I../sysde ps/ieee754  -I../sysdeps/generic  -I..
-I../libio -I. -nostdinc -isystem /usr/lib/gcc/x86_64-linux-gnu/4.9/include
-isystem /usr/lib/gcc/x86_64-linux-gnu/4.9/include-fixed -isystem
/home/kyoupark/asan/gcc/i686-nptl-linux-gnu-gcc/i686-nptl -linux-gnu/include
-D_LIBC_REENTRANT -include /home/kyoupark/asan/gcc/build-glibc/libc-modules.h
-DMODULE_NAME=libc -include ../include/libc-symbols.h       -x c - \ -MD -MP
-MF /home/kyoupark/asan/gcc/build-glibc/tcb-offsets.h.dT -MT
'/home/kyoupark/asan/gcc/build-glibc/tcb-offsets.h.d
/home/kyoupark/asan/gcc/build-glibc/tcb-offsets.h'

install csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
touch ${PREFIX}/${TARGET}/include/gnu/stubs.h

note:
If miss out these, then cause libgcc in step 5 failed with errors around gcov
and pthred related.


5. Compiler Support Library: *libgcc* http://wiki.osdev.org/Libgcc
===========================
This step `uses the cross-compilers` built in step 4 to build the compiler
support library. The compiler support library contains some C++ exception
handling boilerplate code, among other things. This library depends on the
startup files installed in step 4. The library itself is needed in step 6.

Unlike some other guides, we don't need to re-run GCC's configure. We're just
building additional targets in the same configuration.

$ cd build-gcc
$ make -j4 all-target-libgcc
$ make install-target-libgcc
$ cd ..

Two static libraries, libgcc.a and libgcc_eh.a, are installed to
/opt/cross/lib/gcc/aarch64-linux/4.9.2/.

*libgcc*
A shared library, libgcc_s.so, is installed to /opt/cross/aarch64-linux/lib64.

cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc

-rwxr-xr-x 1 kyoupark ccusers    1701 Apr  6 07:39 libc.so*
lrwxrwxrwx 1 kyoupark ccusers      13 Apr  6 07:40 libgcc_s.so -> libgcc_s.so.1
-rw-r--r-- 1 kyoupark ccusers 1522196 Apr  6 07:40 libgcc_s.so.1

$ readelf -d libgcc_s.so.1

 0x00000001 (NEEDED)                     Shared library: [libc.so]
 0x0000000e (SONAME)                     Library soname: [libgcc_s.so.1]

// /home/NDS-UK/kyoupark/asn/gcc/gcc-4.8.2-build/mips-linux-gnu/libgcc/libgcc_s.so.1


5.5 When needs kernel header?
===============================================
We won't actually build the target system's Linux kernel, but we do need the
kernel header files in order to build the target system's standard C library.

The Linux kernel header files won't actually be used until step 6, when we
build the standard C library, although the configure script in step 4 expects
them to be already installed.  


6. Standard C Library (back to glibc)
=====================
In this step, we finish off the Glibc package, which builds the standard C
library and installs its files to /opt/cross/aarch64-linux/lib/. The static
library is named libc.a and the shared library is libc.so.

$ cd build-glibc
$ make -j4
$ make install
$ cd ..

make -j4
make install


7. Standard C++ Library (back to gcc) *gcc-libstdc++*
=======================

note:
run this step when make changes libs like *tool-asan* in gcc

../gcc-4.9.4/configure --prefix=/home/kyoupark/asan/gcc/i686-nptl-linux-gnu-gcc --target=i686-nptl-linux-gnu --enable-languages=c,c++ --disable-multilib \
CFLAGS="-g3 -Og" CXXFLAGS="-g3 -Og"


Finally, we finish off the GCC package, which builds the standard C++ library
and installs it to /opt/cross/aarch64-linux/lib64/. It depends on the C
library built in step 6. The resulting static library is named libstdc++.a and
the shared library is libstdc++.so.

$ cd build-gcc
$ make -j5
$ make install
$ cd ..

make -j5
make install

note:
This changes:

$ readelf -d libgcc_s.so.1
 0x00000001 (NEEDED)                     Shared library: [libc.so.6]
 0x0000000e (SONAME)                     Library soname: [libgcc_s.so.1]

$ readelf -d libgcc_s.so.1.org
 0x00000001 (NEEDED)                     Shared library: [libc.so]
 0x0000000e (SONAME)                     Library soname: [libgcc_s.so.1]


8. Check
=======================
// from 
// BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-uclibc-gcc --version
// mips-linux-uclibc-gcc (GCC) 4.2.0 20070124 (prerelease) - BRCM 10tsHound-20140508

a.out: ELF 32-bit MSB executable, MIPS, MIPS32 version 1 (SYSV), dynamically linked (uses shared libs), not stripped

// note: 
// from GCC built here. "2.6.32" comes from the build server but not from
// header used.

a.out_from_gcc_482: ELF 32-bit MSB executable, MIPS, MIPS-I version 1 (SYSV), dynamically linked (uses shared libs), \
for GNU/Linux 2.6.32, with unknown capability 0x41000000 = 0xf676e75, with unknown capability 0x10000 = 0x70401, not stripped


={============================================================================
*kt_linux_gcc_400* gcc-build-c-library

Why doing this? Since see this error when run ASAN on target which has uclibc:

-sh-3.2# ./a.out_from_mips_glib_samc_wdlink
FATAL: kernel too old


={============================================================================
*kt_linux_gcc_400* gcc-toolchain-glibc-mips buildroot-glic-mips-internal-toolchain

<01> fail: gcc-4.8.2, glibc-2.20, linux-2.6.19
Have to use Linux 2.6.32 or later for glibc-2.20

<02> okay: gcc-4.8.2, glibc-2.20, linux-2.6.33
<03> okay: gcc-4.8.2, glibc-2.20, linux-2.6.32
<04> fail: gcc-4.8.2, glibc-2.20, linux-2.6.32, float-soft
<05> fail: gcc-4.8.2, glibc-2.20, linux-2.6.32, sysroot
<06> okay: gcc-4.8.2, glibc-2.20, linux-2.6.32, null sysroot
<07> okay: gcc-4.8.2, glibc-2.20, linux-2.6.32, binuitls-2.25.1, null sysroot, float-hard
<08> okay: gcc-4.8.2, glibc-2.13, linux-2.6.32, binutils-2.19.1, float-hard
<09> okay: gcc-4.3.0, glibc-2.13, linux-2.6.18.8-from-mips, binutils-2.18, float-hard
<10> okay: gcc-4.8.2, glibc-2.13, linux-2.6.18.8-from-mips, binutils-2.25.1, null sysroot, float-hard
<11> okay: gcc-4.2.0-org, glibc-2.13, linux-2.6.18.8-from-mips, binutils-2.25.1, float-hard *cross-ng*
<12> okay: gcc-4.2.0-bcm, glibc-2.13, linux-2.6.18.8-from-mips, binutils-2.17.50, float-hard
<13-test> okay: gcc-4.2.0-org, glibc-2.13, linux-2.6.18.8-from-mips, binutils-2.24, float-hard

<1> fail: gcc-4.8.2, glibc-2.20, linux-2.6.19
Have to use Linux 2.6.32 or later for glibc-2.20

// 1. use a specific(old) kernel version
// wget --no-check-certificate https://www.kernel.org/pub/linux/kernel/v2.6/linux-2.6.19.tar.xz
// 
// 2. 
// INSTALL_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2-own
// 
// 3. 
// export PATH=$INSTALL_PATH/bin:$PATH
// 
// 4.
// ../binutils-2.24/configure --target=mips-linux --prefix=${INSTALL_PATH} --with-sysroot --disable-nls --disable-multilib
// make -j4
// make install
// 
// 5.
// cd linux-2.6.19
// make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-linux headers_install
// 
// $ make ARCH=mips INSTALL_HDR_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2-own/mips-linux headers_install
//   CHK     include/linux/version.h
//   UPD     include/linux/version.h
//   HOSTCC  scripts/basic/fixdep
// scripts/basic/fixdep.c: In function ‘traps’:
// scripts/basic/fixdep.c:371: warning: dereferencing type-punned pointer will break strict-aliasing rules
// scripts/basic/fixdep.c:373: warning: dereferencing type-punned pointer will break strict-aliasing rules
//   HOSTCC  scripts/basic/docproc
//   HOSTCC  scripts/unifdef
// scripts/unifdef.c:209: error: conflicting types for ‘getline’
// /usr/include/stdio.h:673: note: previous declaration of ‘getline’ was here
// make[1]: *** [scripts/unifdef] Error 1
// make: *** [headers_install] Error 2
// 
// <linux-kernel-header-fix>
// To fix, chage getline to parseline in scripts/unifdef.c
// 
// 6.
// ../gcc-4.8.2-clean/configure --prefix=${INSTALL_PATH} --target=mips-linux --enable-languages=c,c++ --disable-multilib --disable-nls
// ../gcc-4.8.2-clean/configure --prefix=${INSTALL_PATH} --target=mips-linux --enable-languages=c,c++ --disable-multilib --disable-nls --without-headers --with-newlib
// make -j4 all-gcc
// make install-gcc
// 
// 7.
// ../glibc-2.20/configure --prefix=${INSTALL_PATH}/mips-linux \
// --build=$MACHTYPE --host=mips-linux --target=mips-linux \
// --with-headers=${INSTALL_PATH}/mips-linux/include \
// --disable-multilib libc_cv_forced_unwind=yes
// 
// make install-bootstrap-headers=yes install-headers
// make -j4 csu/subdir_lib
// 
// configure: error: GNU libc requires kernel header files from
// Linux 2.6.32 or later to be installed before configuring.


<3> okay: gcc-4.8.2, glibc-2.20, linux-2.6.32

PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install
TARGET=mips-unknown-linux
export PATH=${PREFIX}/bin:$PATH


mkdir build-binutils
cd build-binutils
../binutils-2.25.1/configure --target=${TARGET} --prefix=${PREFIX} --disable-nls --disable-multilib
make -j4
make install

// *buildroot-log*
// >>> host-binutils 2.25.1 Configuring
// (cd /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-binutils-2.25.1/ && rm -rf config.cache; 
// PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CXXFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" ac_cv_prog_MAKEINFO=missing CONFIG_SITE=/dev/null 
// ./configure --prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr" 
// --sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/etc" 
// --localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/var" 
// --enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html --disable-doc --disable-docs --disable-documentation --disable-debug --with-xmlto=no --with-fop=no 
// --disable-dependency-tracking  --disable-multilib --disable-werror 
// --target=mips-buildroot-linux-gnu --disable-shared --enable-static 
// --with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-gnu/sysroot 
// --enable-poison-system-directories --disable-sim --disable-gdb

cd ../linux-2.6.32
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install


cd build-gcc, build-first-pass-gcc
../gcc-4.8.2/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ \
--disable-libmudflap --disable-multilib --disable-nls 
make -j4 all-gcc
make install-gcc


// >>> host-gcc-initial 4.9.4 Configuring
// mkdir -p /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build
// ln -sf ../configure /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build/configure
// (cd /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build && rm -rf config.cache; PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CXXFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null 
// ./configure
// --prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr"
// --sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/etc"
// --localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/var"
// --enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html
// --disable-doc --disable-docs --disable-documentation --disable-debug
// --with-xmlto=no --with-fop=no --disable-dependency-tracking
// --target=mips-buildroot-linux-gnu
// --with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-gnu/sysroot
// --enable-languages=c 
// --disable-__cxa_atexit 
// --with-gnu-ld 
// --with-float=soft 
// --disable-libssp --disable-multilib
// --with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr
// --with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr
// --with-pkgversion="Buildroot 2016.08.1"
// --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath
// --enable-tls --disable-libmudflap --enable-threads
// --with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr
// --without-isl --without-cloog 
// --disable-decimal-float
// --with-arch="mips32" --with-abi="32" 
// --disable-shared
// --without-headers --disable-threads --with-newlib --disable-largefile
// --disable-nls  )
//
// >>> host-gcc-final 4.9.4 Configuring
// mkdir -p /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build
// ln -sf ../configure /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/configure
// (cd /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build && rm -rf config.cache; PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CXXFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " 
// ./configure
// --prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr"
// --sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/etc"
// --enable-static  --target=mips-buildroot-linux-gnu
// --with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-gnu/sysroot
// --disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib
// --with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr
// --with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr
// --with-pkgversion="Buildroot 2016.08.1"
// --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath
// --enable-tls --disable-libmudflap --enable-threads
// --with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr
// --without-isl --without-cloog --with-float=soft --disable-decimal-float
// --with-arch="mips32" --with-abi="32" --enable-languages=c,c++
// --with-build-time-tools=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-gnu/bin
// --enable-shared --disable-libgomp  )

cd build-glibc
../glibc-2.20/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=mips-unknown-linux --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-multilib libc_cv_forced_unwind=yes

// >>> glibc 2.23 Configuring
// mkdir -p /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/glibc-2.23/build
// # Do the configuration
// (cd /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/glibc-2.23/build; PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-ar" AS="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-as" LD="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-ld" NM="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-nm" CC="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-gcc" GCC="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-gcc" CPP="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-cpp" CXX="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-g++" FC="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-gfortran" F77="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-gfortran" RANLIB="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-ranlib" READELF="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-readelf" STRIP="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-strip" OBJCOPY="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-objcopy" OBJDUMP="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-objdump" AR_FOR_BUILD="/usr/bin/ar" AS_FOR_BUILD="/usr/bin/as" CC_FOR_BUILD="/usr/lib64/ccache/gcc" GCC_FOR_BUILD="/usr/lib64/ccache/gcc" CXX_FOR_BUILD="/usr/lib64/ccache/g++" LD_FOR_BUILD="/usr/bin/ld" CPPFLAGS_FOR_BUILD="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CFLAGS_FOR_BUILD="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CXXFLAGS_FOR_BUILD="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS_FOR_BUILD="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" FCFLAGS_FOR_BUILD="" DEFAULT_ASSEMBLER="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-as" DEFAULT_LINKER="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-gnu-ld" CPPFLAGS="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64" CFLAGS="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " LDFLAGS="" FCFLAGS=" -Os " FFLAGS=" -Os " PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" STAGING_DIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-gnu/sysroot" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -mabi=32" CPPFLAGS="" CXXFLAGS="-O2 -mabi=32" /bin/sh /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/glibc-2.23/
// ./configure ac_cv_path_BASH_SHELL=/bin/bash libc_cv_forced_unwind=yes
// libc_cv_ssp=no --target=mips-buildroot-linux-gnu
// --host=mips-buildroot-linux-gnu --build=x86_64-pc-linux-gnu --prefix=/usr
// --enable-shared --without-fp  --with-pkgversion="Buildroot" --without-cvs
// --disable-profile --without-gd --enable-obsolete-rpc --enable-kernel=3.2
// --with-headers=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-gnu/sysroot/usr/include)

make install-bootstrap-headers=yes install-headers
make -j4 csu/subdir_lib

install csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
touch ${PREFIX}/${TARGET}/include/gnu/stubs.h


cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc


cd build-glibc
make -j5
make install


cd build-gcc
make -j5
make install


<4> fail: gcc-4.8.2, glibc-2.20, linux-2.6.32, float-soft

PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-install
TARGET=mips-unknown-linux
export PATH=${PREFIX}/bin:$PATH

mkdir build-binutils
cd build-binutils
../binutils-2.25.1/configure --target=${TARGET} --prefix=${PREFIX} --disable-nls --disable-multilib
make -j4
make install


cd ../linux-2.6.32
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install


cd build-gcc, build-first-pass-gcc
../gcc-4.8.2/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ \
--disable-__cxa_atexit \
--enable-target-optspace \
--with-gnu-ld \
--with-float=soft \
--enable-threads \
--with-arch=mips32 \
--with-abi="32" \
--disable-libmudflap \
--with-gnu-plts \
--disable-multilib --disable-nls 

make -j4 all-gcc
make install-gcc


cd build-glibc
../glibc-2.20/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=mips-unknown-linux --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-multilib libc_cv_forced_unwind=yes

# note: due to PATH for toolchain
# running configure fragment for sysdeps/unix/sysv/linux/mips
# configure: error: could not determine if compiler is using hard or soft floating point ABI
# 
# # asn/gcc/glibc-2.20/sysdeps/unix/sysv/linux/mips/configure.ac
# 
# AC_COMPILE_IFELSE(
#   [AC_LANG_PROGRAM([
#     #if !defined(__mips_soft_float)
#     #error Not soft ABI
#     #endif])],
#   [libc_mips_float=soft],
#   [AC_COMPILE_IFELSE(
#     [AC_LANG_PROGRAM([
#       #if !defined(__mips_hard_float)
#       #error Not hard ABI
#       #endif])],
#     [libc_mips_float=hard],
#     [])])
# 
# if test -z "$libc_mips_float"; then
#   AC_MSG_ERROR([could not determine if compiler is using hard or soft floating point ABI])
# fi

make install-bootstrap-headers=yes install-headers
make -j4 csu/subdir_lib

install csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so

# note: warning when use `soft`
#/tmp/ccj5unPV.s: Assembler messages:
#/tmp/ccj5unPV.s: Warning: .gnu_attribute 4,3 requires `softfloat'

touch ${PREFIX}/${TARGET}/include/gnu/stubs.h


cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc


cd build-glibc
make -j5

/home/kyoupark/asn/gcc/glibc-2.20-build/math/fegetround.os.dt -MT /home/kyoupark/asn/gcc/glibc-2.20-build/math/fegetround.os
../sysdeps/mips/fpu/fegetround.c: In function 'fegetround':
../sysdeps/mips/fpu/fegetround.c:31:15: error: '_FPU_RC_MASK' undeclared (first use in this function)
   return cw & _FPU_RC_MASK;
               ^
stubs-o32_hard.h not generated yet


<5> fail: gcc-4.8.2, glibc-2.20, linux-2.6.32, sysroot

mkdir gcc-4.8.2-glibc-mips-wsysroot-build
PREFIX=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install
TARGET=mips-unknown-linux
export PATH=${PREFIX}/bin:$PATH

mkdir build-binutils
cd build-binutils

../binutils-2.25.1/configure --target=${TARGET} --prefix=${PREFIX} --disable-nls --disable-multilib \
--with-sysroot=${PREFIX}/${TARGET}/sysroot
make -j4
make install

cd ../linux-2.6.32
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET}/sysroot/usr headers_install

// cd build-gcc
// ../gcc-4.8.2/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ \
// --disable-libmudflap --disable-multilib --disable-nls
// make -j4 all-gcc
// make install-gcc

cd build-gcc
../gcc-4.8.2/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ \
--disable-libmudflap --disable-multilib --disable-nls \
--with-sysroot=${PREFIX}/${TARGET}/sysroot
make -j4 all-gcc
make install-gcc


// cd build-glibc
// ../glibc-2.20/configure --prefix=${PREFIX}/${TARGET} \
// --build=$MACHTYPE --host=mips-unknown-linux --target=${TARGET} \
// --with-headers=${PREFIX}/${TARGET}/include \
// --disable-multilib libc_cv_forced_unwind=yes

cd build-glibc
../glibc-2.20/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=mips-unknown-linux --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/sysroot/usr/include \
--disable-multilib libc_cv_forced_unwind=yes

make install-bootstrap-headers=yes install-headers
make -j5 csu/subdir_lib
// install csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib
// ${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
// touch ${PREFIX}/${TARGET}/include/gnu/stubs.h$ mkdir /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/sysroot/usr/lib
// installs ${PREFIX}/${TARGET}/include. HOW to make it to install in
// ${PREFIX}/${TARGET}/sysroot/usr/include?

cd build-glibc
../glibc-2.20/configure --prefix=${PREFIX}/${TARGET}/sysroot/usr \
--build=$MACHTYPE --host=mips-unknown-linux --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/sysroot/usr/include \
--disable-multilib libc_cv_forced_unwind=yes

make install-bootstrap-headers=yes install-headers
make -j5 csu/subdir_lib

mkdir ${PREFIX}/${TARGET}/sysroot/usr/lib
install csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/sysroot/usr/lib
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/sysroot/usr/lib/libc.so
touch ${PREFIX}/${TARGET}/sysroot/usr/include/gnu/stubs.h


cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc


cd build-glibc
make -j5
make install


cd build-gcc
make -j5

/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/bin/ld:
cannot find /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/sysroot/usr/lib/libc.so.6
inside      /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/sysroot

/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/bin/ld: cannot find /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/sysroot/usr/lib/libc_nonsha
red.a inside /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/sysroot
/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/bin/ld: cannot find /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/sysroot/usr/lib/ld.so.1 ins
ide /home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/sysroot
collect2: error: ld returned 1 exit status
Makefile:921: recipe for target 'libgcc_s.so' failed
make[2]: *** [libgcc_s.so] Error 1

note:
WHY not find these files when there are? WHY sysroot do not work?

From this build:
SEARCH_DIR("=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install/mips-unknown-linux/lib"); SEARCH_DIR("=/usr/local/lib"); SEARCH_DIR("=/lib"); SEARCH_DIR("=/usr/lib");

From buildroot-builds:
SEARCH_DIR("=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-gnu/lib"); SEARCH_DIR("=/usr/local/lib"); SEARCH_DIR("=/lib"); SEARCH_DIR("=/usr/lib");

Same and in case no sysroot, all libraries goes to
  install/mips-unknown-linux/lib so not a problem.


<6> okay: gcc-4.8.2, glibc-2.20, linux-2.6.32, null sysroot

PREFIX=/home/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install
PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-install
TARGET=mips-unknown-linux

export PATH=${PREFIX}/bin:$PATH

mkdir build-binutils
cd build-binutils

../binutils-2.25.1/configure --target=${TARGET} --prefix=${PREFIX} --disable-nls --disable-multilib \
--with-sysroot
make -j4
make install

cd ../linux-2.6.32
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install


cd build-gcc
../gcc-4.8.2/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ \
--disable-libmudflap --disable-multilib --disable-nls
make -j4 all-gcc
make install-gcc


cd build-glibc
../glibc-2.20/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=mips-unknown-linux --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-multilib libc_cv_forced_unwind=yes

make install-bootstrap-headers=yes install-headers
make -j5 csu/subdir_lib
install csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
touch ${PREFIX}/${TARGET}/include/gnu/stubs.h


cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc


cd build-glibc
make -j5
make install


cd build-gcc
make -j5
make install


<7> okay: gcc-4.8.2, glibc-2.20, linux-2.6.32, null sysroot, float-hard

PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-glibc-mips-wsysroot-woption-install
TARGET=mips-unknown-linux
export PATH=${PREFIX}/bin:$PATH

cd build-binutils
../binutils-2.25.1/configure --target=${TARGET} --prefix=${PREFIX} --disable-nls --disable-multilib \
--with-sysroot
make -j4
make install

cd ../linux-2.6.32
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install


cd build-gcc, build-first-pass-gcc
../gcc-4.8.2/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ \
--disable-libmudflap --disable-multilib --disable-nls \
--disable-__cxa_atexit --enable-target-optspace \
--with-gnu-ld --with-float=hard --enable-threads \
--with-gnu-plts --with-arch=mips32 
make -j4 all-gcc
make install-gcc


cd build-glibc
../glibc-2.20/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=mips-unknown-linux --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-multilib libc_cv_forced_unwind=yes

make install-bootstrap-headers=yes install-headers
make -j5 csu/subdir_lib
install csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
touch ${PREFIX}/${TARGET}/include/gnu/stubs.h


cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc


cd build-glibc
make -j5
make install


cd build-gcc
make -j5
make install


<8> okay: gcc-4.8.2, glibc-2.13, linux-2.6.32, binutils-2.19.1, float-hard

// http://ftpmirror.gnu.org/binutils/binutils-2.18.tar.bz2 # makeinfo error
// http://ftpmirror.gnu.org/binutils/binutils-2.18a.tar.bz2 # makeinfo error
// http://ftpmirror.gnu.org/binutils/binutils-2.20.1.tar.bz2 # builds okay
// http://ftpmirror.gnu.org/binutils/binutils-2.22.tar.gz # builds okay

note: aims to 2011, linux-2.6.32, binutils-2.19.1, glibc-2.13
note: fails on glic when use linux-2.6.19 which from 2006.

wget http://ftpmirror.gnu.org/binutils/binutils-2.19.1.tar.bz2 # builds okay.


Errors when try to build glibc for mips:

// cd build-glibc
// ../glibc-2.10.1/configure --prefix=${INSTALL_PATH}/${TARGET} \
// --build=$MACHTYPE --host=mips-unknown-linux --target=${TARGET} \
// --with-headers=${INSTALL_PATH}/${TARGET}/include \
// --disable-multilib libc_cv_forced_unwind=yes
// 
// configure: running configure fragment for add-on nptl
// checking sysdep dirs... configure: error: The mips is not supported.

see linux-lib-libc-glibc-port


PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-glibc-2.13-mips-w-install
TARGET=mips-unknown-linux
export PATH=${PREFIX}/bin:$PATH

mkdir build-binutils
cd binutils-2.19.1-build

../binutils-2.19.1/configure --target=${TARGET} --prefix=${PREFIX} --disable-nls --disable-multilib \
--with-sysroot
make -j4
make install


cd ../linux-2.6.32
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install


cd build-gcc, build-first-pass-gcc
../gcc-4.8.2/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ \
--disable-libmudflap --disable-multilib --disable-nls \
--disable-__cxa_atexit --enable-target-optspace \
--with-gnu-ld --with-float=hard --enable-threads \
--with-gnu-plts --with-arch=mips32 
make -j4 all-gcc
make install-gcc


// # 36@nptl/sysdeps/pthread/configure.in
// configure: error: the compiler must support C cleanup handling
// libc_cv_c_cleanup=yes \

cd glibc-2.13-build
../glibc-2.13/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=mips-unknown-linux --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-multilib libc_cv_forced_unwind=yes libc_cv_c_cleanup=yes \
--enable-add-ons=nptl,../glibc-ports-2.13

make install-bootstrap-headers=yes install-headers
make -j5 csu/subdir_lib
install csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
touch ${PREFIX}/${TARGET}/include/gnu/stubs.h


cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc


cd build-glibc
make -j5
make install


cd build-gcc
make -j5
make install


<9> okay: gcc-4.3.0, glibc-2.13, linux-2.6.18.8-from-mips, binutils-2.18, float-hard

PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

// 2011-08-31 06:49
// wget http://ftpmirror.gnu.org/binutils/binutils-2.18.tar.bz2 

cd build-binutils
../binutils-2.18/configure --target=${TARGET} --prefix=${PREFIX} --disable-nls --disable-multilib \
--with-sysroot
make -j4
make install


// ../binutils-2.17/configure --target=${TARGET} --prefix=${PREFIX} --disable-nls --disable-multilib \
// --with-sysroot
// make -j4
//
// cc1: warnings being treated as errors
// ../../binutils-2.17/bfd/ecoff.c: In function '_bfd_ecoff_write_armap':
// ../../binutils-2.17/bfd/ecoff.c:3090: error: 'rehash' may be used uninitialized in this function
// gcc -DHAVE_CONFIG_H -I. -I../../binutils-2.17/bfd -I. -I. -I../../binutils-2.17/bfd -I../../binutils-2.17/bfd/../include -I../../binutils-2.17/bfd/../intl -I../intl -W -Wall -Wstrict-prototypes -Wmissing-prototypes -Werror -g -O2 -c ../../binutils-2.17/bfd/elf64-gen.c -o elf64-gen.o
// make[4]: *** [ecoff.lo] Error 1

// ../binutils-2.18/configure --target=${TARGET} --prefix=${PREFIX} --disable-nls --disable-multilib \
// --with-sysroot
// make -j4
/bin/sh ../../../binutils-2.18/bfd/doc/../../move-if-change elf.tmp elf.texi
restore=: && backupdir=".am$$" && \
        rm -rf $backupdir && mkdir $backupdir && \
        if (/home/nds-uk/kyoupark/asn/gcc/binutils-2.18/missing makeinfo --split-size=5000000 --split-size=5000000 --version) >/dev/null 2>&1; then \
          for f in bfd.info bfd.info-[0-9] bfd.info-[0-9][0-9] bfd.i[0-9] bfd.i[0-9][0-9]; do \
            if test -f $f; then mv $f $backupdir; restore=mv; else :; fi; \
          done; \
        else :; fi && \
        if /home/nds-uk/kyoupark/asn/gcc/binutils-2.18/missing makeinfo --split-size=5000000 --split-size=5000000   -I ../../../binutils-2.18/bfd/doc \
         -o bfd.info `test -f 'bfd.texinfo' || echo '../../../binutils-2.18/bfd/doc/'`bfd.texinfo; \
        then \
          rc=0; \
        else \
          rc=$?; \
          $restore $backupdir/* `echo "./bfd.info" | sed 's|[^/]*$||'`; \
        fi; \
        rm -rf $backupdir; exit $rc
WARNING: `makeinfo' is missing on your system.  You should only need it if
         you modified a `.texi' or `.texinfo' file, or any other file
         indirectly affecting the aspect of the manual.  The spurious
         call might also be the consequence of using a buggy `make' (AIX,
         DU, IRIX).  You might want to install the `Texinfo' package or
         the `GNU make' package.  Grab either from any GNU archive site.
make[3]: *** [bfd.info] Error 1


--- configure   2007-08-06 21:29:40.000000000 +0100
+++ ../../binutils-2.18/configure       2017-01-27 09:40:13.086559122 +0000
@@ -6128,7 +6128,7 @@ case " $build_configdirs " in
     # For an installed makeinfo, we require it to be from texinfo 4.4 or
     # higher, else we use the "missing" dummy.
     if ${MAKEINFO} --version \
-       | egrep 'texinfo[^0-9]*([1-3][0-9]|4\.[4-9]|[5-9])' >/dev/null 2>&1; then
+       | egrep 'texinfo[^0-9]*(4\.([6-9]|[1-9][0-9])|[5-9]|[1-9][0-9])' >/dev/null 2>&1; then


$ makeinfo --version
makeinfo (GNU texinfo) 4.13

Copyright (C) 2008 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

$ makeinfo --version | egrep 'texinfo[^0-9]*(4\.([6-9]|[1-9][0-9])|[5-9]|[1-9][0-9])'
makeinfo (GNU texinfo) 4.13


note: this is important to use linux from mips
cd ../linux-2.6.18.8-from-mips
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install

// copy unifdef binary to $PATH
// kyoupark@ukstbuild2:~/asn/gcc$ ls linux-2.6.19/scripts/unifdef*
// linux-2.6.19/scripts/unifdef  linux-2.6.19/scripts/unifdef.c

note:
tried gcc 4.8.2 and glibc 2.20 config fails since requires > binuitl 2.20

wget --no-check-certificate https://ftp.gnu.org/gnu/gcc/gcc-4.2.0/gcc-4.2.0.tar.bz2
wget --no-check-certificate https://ftp.gnu.org/gnu/gcc/gcc-4.2.4/gcc-4.2.4.tar.bz2
wget --no-check-certificate https://ftp.gnu.org/gnu/gcc/gcc-4.3.0/gcc-4.3.0.tar.bz2

wget --no-check-certificate http://ftpmirror.gnu.org/mpfr/mpfr-2.4.0.tar.bz2
wget --no-check-certificate http://ftpmirror.gnu.org/gmp/gmp-4.2.4.tar.gz


cd build-gcc, build-first-pass-gcc-all

// builds fails
// ../gcc-4.2.0/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ \
// 
// I../../gcc-4.2.0/gcc/../libdecnumber -I../libdecnumber -fexceptions -fvisibility=hidden -DHIDE_EXPORTS -c ../../gcc-4.2.0/gcc/unwind-dw2.c -o libgcc/./unwind-dw2.o
// In file included from ./gthr-default.h:1,
//                  from ../../gcc-4.2.0/gcc/gthr.h:114,
//                  from ../../gcc-4.2.0/gcc/unwind-dw2.c:42:
// ../../gcc-4.2.0/gcc/gthr-posix.h:43:21: error: pthread.h: No such file or directory
// ../../gcc-4.2.0/gcc/gthr-posix.h:44:20: error: unistd.h: No such file or directory

// 09/03/2008
../gcc-4.3.0/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ \
--disable-libmudflap --disable-multilib --disable-nls \
--disable-__cxa_atexit --enable-target-optspace \
--with-gnu-ld --with-float=hard --enable-threads \
--with-gnu-plts --with-arch=mips32 
make -j4 all-gcc
make install-gcc


// glibc-2.13.tar.xz 2011-02-01
// wget --no-check-certificate http://ftpmirror.gnu.org/glibc/glibc-2.13.tar.gz
// wget --no-check-certificate http://ftpmirror.gnu.org/glibc/glibc-ports-2.13.tar.gz

cd glibc-2.13-build
../glibc-2.13/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=mips-linux-gnu --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-multilib libc_cv_forced_unwind=yes libc_cv_c_cleanup=yes \
--enable-add-ons=nptl,../glibc-ports-2.13

make install-bootstrap-headers=yes install-headers

// note: when forget to change --host=mips-linux-gnu from
// --host-mips-unknown-linus
//
// /usr/bin/install -c -m 644 /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/gnu/lib-names.h /home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-try-install/mips-linux-gnu/include/gnu/lib-names.h
// /usr/bin/install: cannot stat `/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/gnu/lib-names.h': No such file or directory

make -j5 csu/subdir_lib
install csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
touch ${PREFIX}/${TARGET}/include/gnu/stubs.h


cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc


cd build-glibc
make -j5
make install


cd build-gcc
make -j5
make install


<10> okay: gcc-4.8.2, glibc-2.13, linux-2.6.18.8-from-mips, binutils-2.25.1, null sysroot, float-hard

PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-mips-10-install
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

cd build-binutils
../binutils-2.25.1/configure --target=${TARGET} --prefix=${PREFIX} --disable-nls --disable-multilib \
--with-sysroot
make -j4
make install


note: this is important to use linux from mips
cd ../linux-2.6.18.8-from-mips
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install

// copy unifdef binary to $PATH

cd build-gcc, *build-first-pass* *gcc-all*
../gcc-4.8.2/configure --prefix=${PREFIX} --target=${TARGET} --enable-languages=c,c++ \
--disable-libmudflap --disable-multilib --disable-nls \
--disable-__cxa_atexit --enable-target-optspace \
--with-gnu-ld --with-float=hard --enable-threads \
--with-gnu-plts --with-arch=mips32 
make -j4 all-gcc
make install-gcc


cd glibc-2.13-build
../glibc-2.13/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=mips-linux-gnu --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-multilib libc_cv_forced_unwind=yes libc_cv_c_cleanup=yes \
--enable-add-ons=nptl,../glibc-ports-2.13

make install-bootstrap-headers=yes install-headers
make -j5 csu/subdir_lib
install csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
touch ${PREFIX}/${TARGET}/include/gnu/stubs.h

note: fails when use binutil 2.18 so back to 2.25.1


cd build-gcc, *build-second-pass*
make -j4 all-target-libgcc
make install-target-libgcc


cd build-glibc
make -j5
make install


cd build-gcc
make -j5
make install


<11> okay: gcc-4.2.0-org, glibc-2.13, linux-2.6.18.8-from-mips, binutils-2.25.1, float-hard

PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

1
======
cd build-binutils

// gcc-4.2.4, glibc-2.13, linux-2.18.8 : OK
// *cross-ng* mipsel-unknown-linux-gnu, gcc-4.2.4, glibc-2.13, linux-4.3
//
// [EXTRA]    Configuring binutils
// [DEBUG]    Extra config passed: '--enable-ld=yes --enable-gold=no --enable-plugins --with-pkgversion=crosstool-NG crosstool-ng-1.22.0 --disable-multilib --disable-nls'
// [DEBUG]    ==> Executing: 'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe '
// 'LDFLAGS= ' '/home/kyoupark/cross/.build/src/binutils-2.25.1/configure'
// '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
// '--target=mipsel-unknown-linux-gnu'
// '--prefix=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu' '--disable-werror'
// '--enable-ld=yes' '--enable-gold=no' '--enable-plugins'
// '--with-pkgversion=crosstool-NG crosstool-ng-1.22.0' '--disable-multilib'
// '--disable-nls' '--with-float=soft'
// '--with-sysroot=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot' 

cd build-binutils
../binutils-2.25.1/configure --target=${TARGET} --prefix=${PREFIX} --disable-nls --disable-multilib
make -j4
make install


1 all-gcc
======
cd build-gcc, *build-first-pass* *all-gcc*

// note: i686-build_pc-linux-gnu-gcc is /usr/bin/gcc
// 
// [INFO ]  Installing pass-1 core C gcc compiler
// [DEBUG]    Entering '/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/build/build-cc-gcc-core-pass-1'
// [EXTRA]    Configuring core C gcc compiler
// [DEBUG]    Copying headers to install area of core C compiler
// [DEBUG]    ==> Executing: 'cp' '-a' '/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr/include' '/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools/mipsel-unknown-linux-gnu/include' 
// [DEBUG]    Extra config passed: '--with-newlib --enable-threads=no --disable-shared --with-arch=mips1 --with-abi=32 --with-float=soft --enable-__cxa_atexit --with-cloog=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools --with-libelf=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools --enable-lto --enable-target-optspace --with-mips-plt --disable-libgomp --disable-libmudflap --disable-libssp --disable-nls --disable-multilib'
// [DEBUG]    ==> Executing: 'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc'
// 'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe ' 'LDFLAGS=  -lstdc++ -lm'
// '/home/kyoupark/cross/.build/src/gcc-4.2.4/configure'
// '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
// '--target=mipsel-unknown-linux-gnu'
// '--prefix=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools'
// '--with-local-prefix=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot'
// '--with-sysroot=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot'
// '--with-newlib' '--enable-threads=no' '--disable-shared' '--with-arch=mips1'
// '--with-abi=32' '--with-float=soft' '--enable-__cxa_atexit'
// '--with-cloog=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools'
// '--with-libelf=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools'
// '--enable-lto' '--enable-target-optspace' '--with-mips-plt'
// '--disable-libgomp' '--disable-libmudflap' '--disable-libssp' '--disable-nls'
// '--disable-multilib' '--enable-languages=c' 
// 
// [EXTRA]    Building gcc
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l' 'all-gcc' 
// [EXTRA]    Installing gcc
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l' 'install-gcc' 


// OK - `soft` and OK - `hard`
CFLAGS='-O2 -g -pipe' CXXFLAGS='-O2 -g -pipe' LDFLAGS=' -lstdc++ -lm' \
../gcc-4.2.0/configure --target=${TARGET} --prefix=${PREFIX} \
--with-newlib --enable-threads=no --disable-shared \
--with-arch=mips1 --with-abi=32 --with-float=soft --enable-__cxa_atexit \
--enable-lto --enable-target-optspace --with-mips-plt --disable-libgomp \
--disable-libmudflap --disable-libssp --disable-nls \
--disable-multilib --enable-languages=c,c++ 
make -j4 -l all-gcc
make -l install-gcc


// NOT OK since --enable-threads sets -DHAVE_GTHR_DEFAULT and this fails to
// build files in gcc/unwind-xx.c so `have to use --enable-threads=no` 
//
// ../gcc-4.2.0/configure --target=${TARGET} --prefix=${PREFIX} \
// --enable-languages=c \
// --disable-shared \
// --disable-libmudflap --disable-nls \
// --disable-__cxa_atexit --enable-target-optspace \
// --with-gnu-ld --with-float=hard --enable-threads \
// --disable-multilib  \
// --with-gnu-plts

// OK with less options
../gcc-4.2.0/configure --target=${TARGET} --prefix=${PREFIX} \
--enable-threads=no --disable-shared \
--with-float=hard \
--disable-__cxa_atexit --enable-target-optspace \
--disable-libmudflap --disable-nls \
--with-gnu-ld --with-gnu-plts \
--disable-multilib --enable-languages=c
make -j4 -l all-gcc
make -l install-gcc


2 kernel headers
======

// [EXTRA]    Installing kernel headers
// [DEBUG]    ==> Executing: '/usr/bin/make' '-C'
// '/home/kyoupark/cross/.build/src/linux-4.3'
// 'CROSS_COMPILE=mipsel-unknown-linux-gnu-'
// 'O=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/build/build-kernel-headers'
// 'ARCH=mips'
// 'INSTALL_HDR_PATH=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr'
// 'V=0' 'headers_install' 
//
// [EXTRA]    Checking installed headers
// [DEBUG]    ==> Executing: '/usr/bin/gmake' '-C'
// '/home/NDS-UK/kyoupark/cross/.build/src/linux-4.3'
// 'CROSS_COMPILE=mipsel-unknown-linux-gnu-'
// 'O=/home/NDS-UK/kyoupark/cross/.build/mipsel-unknown-linux-gnu/build/build-kernel-headers'
// 'ARCH=mips'
// 'INSTALL_HDR_PATH=/home/nds-uk/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr'
// 'V=0' 'headers_check' 

cd ../linux-2.6.18.8-from-mips
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install


3 glibc startup files
======

// [INFO ]  Installing C library headers & start files
// [DEBUG]    Entering '/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/build/build-libc-startfiles'
// [EXTRA]    Configuring C library
// [DEBUG]    Using gcc for target    : '/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools/bin/mipsel-unknown-linux-gnu-gcc'
// [DEBUG]    Configuring with addons : 'nptl,ports'
// [DEBUG]    Extra config args passed: '--disable-debug --disable-sanity-checks --enable-obsolete-rpc --enable-kernel=4.3.0 --with-__thread --with-tls --enable-shared --without-fp --enable-add-ons=nptl,ports --with-pkgversion=crosstool-NG crosstool-ng-1.22.0'
// [DEBUG]    Extra CC args passed    : ' -U_FORTIFY_SOURCE  -EL -march=mips1 -mabi=32    -msoft-float  -O2 '
// [DEBUG]    Extra flags (multilib)  : ''
// [DEBUG]    ==> Executing: 'BUILD_CC=i686-build_pc-linux-gnu-gcc' 'CFLAGS=
//   -U_FORTIFY_SOURCE  -EL -march=mips1 -mabi=32    -msoft-float  -O2 '
//   'CC=mipsel-unknown-linux-gnu-gcc    ' 'AR=mipsel-unknown-linux-gnu-ar'
//   'RANLIB=mipsel-unknown-linux-gnu-ranlib' '/bin/bash'
//   '/home/kyoupark/cross/.build/src/glibc-2.13/configure' '--prefix=/usr'
//   '--build=i686-build_pc-linux-gnu' '--host=mipsel-unknown-linux-gnu'
//   '--cache-file=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/build/build-libc-startfiles/config.cache'
//   '--without-cvs' '--disable-profile' '--without-gd'
//   '--with-headers=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr/include'
//   '--disable-debug' '--disable-sanity-checks' '--enable-obsolete-rpc'
//   '--enable-kernel=4.3.0' '--with-__thread' '--with-tls' '--enable-shared'
//   '--without-fp' '--enable-add-ons=nptl,ports' '--with-pkgversion=crosstool-NG
//   crosstool-ng-1.22.0' 
//
// [EXTRA]    Installing C library headers
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l'
// 'install_root=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot'
// 'install-bootstrap-headers=yes' 'BUILD_CFLAGS=-O2 -g  ' 'BUILD_LDFLAGS=  '
// 'install-headers' 
// [DEBUG]    ==> Executing: 'touch' '/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr/include/gnu/stubs.h' 
// [DEBUG]    ==> Executing: 'cp' '-v' '/home/kyoupark/cross/.build/src/glibc-2.13/include/features.h' '/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr/include/features.h' 
// [DEBUG]    ==> Executing: 'cp' '-v' 'bits/stdio_lim.h' '/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr/include/bits/stdio_lim.h' 
// [DEBUG]    ==> Executing: 'cp' '-v' 'misc/syscall-list.h' '/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr/include/bits/syscall.h' 
// [EXTRA]    Installing C library start files
// [DEBUG]    ==> Executing: 'mkdir' '-p' '/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr/lib' 
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l' 'BUILD_CFLAGS=-O2 -g  ' 'BUILD_LDFLAGS=  ' 'csu/subdir_lib' 
// [DEBUG]    ==> Executing: 'cp' 'csu/crt1.o' 'csu/crti.o' 'csu/crtn.o' '/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr/lib' 
// [DEBUG]    ==> Executing: '/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools/bin/mipsel-unknown-linux-gnu-gcc' '-nostdlib' '-nostartfiles' '-shared' '-x' 'c' '/dev/null' '-o' '/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr/lib/libc.so' 

// use soft
// CFLAGS='-U_FORTIFY_SOURCE  -EL -march=mips1 -mabi=32    -msoft-float  -O2'
// ../glibc-2.13/configure --prefix=${PREFIX}/${TARGET} \
// --build=$MACHTYPE --host=${TARGET} \
// --without-cvs --disable-profile --without-gd \
// --with-headers=${PREFIX}/${TARGET}/include \
// --disable-debug --disable-sanity-checks --enable-obsolete-rpc \
// libc_cv_forced_unwind=yes libc_cv_c_cleanup=yes \
// --enable-kernel=2.6.18 --with-__thread --with-tls --enable-shared \
// --without-fp --enable-add-ons=nptl,../glibc-ports-2.13
// 
// make install-bootstrap-headers=yes BUILD_CFLAGS='-O2 -g' install-headers
// touch ${PREFIX}/${TARGET}/include/gnu/stubs.h
// cp -v ../glibc-2.13/include/features.h ${PREFIX}/${TARGET}/include/features.h 
// cp -v ./bits/stdio_lim.h ${PREFIX}/${TARGET}/include/bits/stdio_lim.h
// cp -v ./misc/syscall-list.h ${PREFIX}/${TARGET}/include/bits/syscall.h
// ${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
// make -j2 -l BUILD_CFLAGS='-O2 -g' csu/subdir_lib
// cp csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib

use `hard`
CFLAGS='-U_FORTIFY_SOURCE  -EL -march=mips1 -mabi=32    -mhard-float  -O2'
../glibc-2.13/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=${TARGET} \
--without-cvs --disable-profile --without-gd \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-debug --disable-sanity-checks --enable-obsolete-rpc \
libc_cv_forced_unwind=yes libc_cv_c_cleanup=yes \
--enable-kernel=2.6.18 --with-__thread --with-tls --enable-shared \
--without-fp --enable-add-ons=nptl,../glibc-ports-2.13

make install-bootstrap-headers=yes BUILD_CFLAGS='-O2 -g' install-headers
touch ${PREFIX}/${TARGET}/include/gnu/stubs.h
cp -v ../glibc-2.13/include/features.h ${PREFIX}/${TARGET}/include/features.h 
cp -v ./bits/stdio_lim.h ${PREFIX}/${TARGET}/include/bits/stdio_lim.h
cp -v ./misc/syscall-list.h ${PREFIX}/${TARGET}/include/bits/syscall.h
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
make -j2 -l BUILD_CFLAGS='-O2 -g' csu/subdir_lib
cp csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib


3 libgcc
======

cd build-gcc, *build-second-pass*

note: this is different compared with the previous tries since not call
all-target-libgcc.

// [INFO ]  Installing pass-2 core C gcc compiler
// [DEBUG]    Entering '/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/build/build-cc-gcc-core-pass-2'
// [EXTRA]    Configuring core C gcc compiler
// [DEBUG]    Copying headers to install area of core C compiler
// [DEBUG]    ==> Executing: 'cp' '-a' '/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr/include' '/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools/mipsel-unknown-linux-gnu/include' 
// [DEBUG]    Extra config passed: '--enable-shared --with-arch=mips1 --with-abi=32 --with-float=soft --enable-__cxa_atexit --with-cloog=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools --with-libelf=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools --enable-lto --enable-target-optspace --with-mips-plt --disable-libgomp --disable-libmudflap --disable-libssp --disable-nls --disable-multilib'
// [DEBUG]    ==> Executing: 'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc'
// 'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe ' 'LDFLAGS=  -lstdc++ -lm'
// '/home/kyoupark/cross/.build/src/gcc-4.2.4/configure'
// '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
// '--target=mipsel-unknown-linux-gnu'
// '--prefix=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools'
// '--with-local-prefix=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot'
// '--with-sysroot=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot'
// '--enable-shared' '--with-arch=mips1' '--with-abi=32' '--with-float=soft'
// '--enable-__cxa_atexit'
// '--with-cloog=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools'
// '--with-libelf=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools'
// '--enable-lto' '--enable-target-optspace' '--with-mips-plt'
// '--disable-libgomp' '--disable-libmudflap' '--disable-libssp' '--disable-nls'
// '--disable-multilib' '--enable-languages=c' 
// [EXTRA]    Building gcc
// [DEBUG]    ==> Executing: '/usr/bin/gmake' '-j25' '-l' 'all-gcc' 'all-target-libgcc' 
// [EXTRA]    Installing gcc
// [DEBUG]    ==> Executing: '/usr/bin/gmake' '-j25' '-l' 'install-gcc' 'install-target-libgcc' 

// // OK soft
// CFLAGS='-O2 -g -pipe' CXXFLAGS='-O2 -g -pipe' LDFLAGS=' -lstdc++ -lm' \
// ../gcc-4.2.0/configure --target=${TARGET} --prefix=${PREFIX} \
// --enable-shared \
// --with-arch=mips1 --with-abi=32 --with-float=soft --enable-__cxa_atexit \
// --enable-lto --enable-target-optspace --with-mips-plt --disable-libgomp \
// --disable-libmudflap --disable-libssp --disable-nls \
// --disable-multilib --enable-languages=c 
// make -j4 -l all-gcc
// make -l install-gcc
// 
// // OK `hard`
// ../gcc-4.2.0/configure --target=${TARGET} --prefix=${PREFIX} \
// --enable-shared \
// --with-arch=mips1 --with-abi=32 --with-float=hard --enable-__cxa_atexit \
// --enable-lto --enable-target-optspace --with-mips-plt --disable-libgomp \
// --disable-libmudflap --disable-libssp --disable-nls \
// --disable-multilib --enable-languages=c 
// make -j4 -l all-gcc
// make -l install-gcc

note: this is difference from glibc-reference

// OK
../gcc-4.2.0/configure --target=${TARGET} --prefix=${PREFIX} \
--enable-languages=c,c++ \
--disable-__cxa_atexit --enable-target-optspace \
--with-gnu-ld --with-float=hard --enable-threads \
--with-arch=mips32 --disable-libmudflap --disable-nls --with-gnu-plts
make -j4 -l all-gcc
make -l install-gcc


// [INFO ]  Installing C library
// [DEBUG]    Entering '/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/build/build-libc-final'
// [EXTRA]    Configuring C library
// [DEBUG]    Using gcc for target    : '/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools/bin/mipsel-unknown-linux-gnu-gcc'
// [DEBUG]    Configuring with addons : 'nptl,ports'
// [DEBUG]    Extra config args passed: '--disable-debug --disable-sanity-checks --enable-obsolete-rpc --enable-kernel=4.3.0 --with-__thread --with-tls --enable-shared --without-fp --enable-add-ons=nptl,ports --with-pkgversion=crosstool-NG crosstool-ng-1.22.0'
// [DEBUG]    Extra CC args passed    : ' -U_FORTIFY_SOURCE  -EL -march=mips1 -mabi=32    -msoft-float  -O2 '
// [DEBUG]    Extra flags (multilib)  : ''
// [DEBUG]    ==> Executing: 'BUILD_CC=i686-build_pc-linux-gnu-gcc' 'CFLAGS= -U_FORTIFY_SOURCE  -EL -march=mips1 -mabi=32    -msoft-float  -O2 ' 'CC=mipsel-unknown-linux-gnu-gcc    ' 'AR=mipsel-unknown-linux-gnu-ar' 'RANLIB=mipsel-unknown-linux-gnu-ranlib' '/bin/bash' '/home/kyoupark/cross/.build/src/glibc-2.13/configure' '--prefix=/usr' '--build=i686-build_pc-linux-gnu' '--host=mipsel-unknown-linux-gnu' '--cache-file=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/build/build-libc-final/config.cache' '--without-cvs' '--disable-profile' '--without-gd' '--with-headers=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot/usr/include' '--disable-debug' '--disable-sanity-checks' '--enable-obsolete-rpc' '--enable-kernel=4.3.0' '--with-__thread' '--with-tls' '--enable-shared' '--without-fp' '--enable-add-ons=nptl,ports' '--with-pkgversion=crosstool-NG crosstool-ng-1.22.0' 
// [EXTRA]    Building C library
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l' 'BUILD_CFLAGS=-O2 -g  ' 'BUILD_LDFLAGS=  ' 'all' 
// [EXTRA]    Installing C library
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l' 'BUILD_CFLAGS=-O2 -g  ' 'BUILD_LDFLAGS=  ' 'install_root=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot' 'install' 

cd build-glibc
// make BUILD_CFLAGS='-O2 -g' -j5
// make BUILD_CFLAGS='-O2 -g' install
make -j5
make install


// [INFO ]  Installing final gcc compiler
// [DEBUG]    Entering '/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/build/build-cc-gcc-final'
// [EXTRA]    Configuring final gcc compiler
// [DEBUG]    Extra config passed: '--enable-languages=c,c++ --with-arch=mips1 --with-abi=32 --with-float=soft --enable-__cxa_atexit --disable-libmudflap --disable-libgomp --disable-libssp --with-cloog=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools --with-libelf=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools --enable-lto --enable-threads=posix --enable-target-optspace --enable-plugin --with-mips-plt --disable-nls --disable-multilib'
// [DEBUG]    ==> Executing: 'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc' 'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe ' 'LDFLAGS=  -lstdc++ -lm' 'CFLAGS_FOR_TARGET= -EL -march=mips1 -mabi=32    -msoft-float ' 'CXXFLAGS_FOR_TARGET= -EL -march=mips1 -mabi=32    -msoft-float ' 'LDFLAGS_FOR_TARGET= -Wl,-EL ' '/home/kyoupark/cross/.build/src/gcc-4.2.4/configure' '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu' '--target=mipsel-unknown-linux-gnu' '--prefix=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu' '--with-sysroot=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot' '--enable-languages=c,c++' '--with-arch=mips1' '--with-abi=32' '--with-float=soft' '--enable-__cxa_atexit' '--disable-libmudflap' '--disable-libgomp' '--disable-libssp' '--with-cloog=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools' '--with-libelf=/home/kyoupark/cross/.build/mipsel-unknown-linux-gnu/buildtools' '--enable-lto' '--enable-threads=posix' '--enable-target-optspace' '--enable-plugin' '--with-mips-plt' '--disable-nls' '--disable-multilib' '--with-local-prefix=/home/kyoupark/x-tools/mipsel-unknown-linux-gnu/mipsel-unknown-linux-gnu/sysroot' '--enable-long-long' 
// [EXTRA]    Building final gcc compiler
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l' 'all' 
// [EXTRA]    Installing final gcc compiler
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l' 'install' 

cd build-gcc
make -j5
make install


<12> okay: gcc-4.2.0-bcm, glibc-2.13, linux-2.6.18.8-from-mips, binutils-2.17.50, float-hard

PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

../binutils-2.17.50/configure --prefix=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install --target=mips-linux-gnu --disable-werror --disable-nls --enable-multilib target_alias=mips-linux-gnu \
--no-create --no-recursion

../binutils-2.17.50/configure --prefix=${PREFIX} --target=${TARGET} --disable-werror --disable-nls --enable-multilib \
target_alias=mips-linux-gnu \
--no-create --no-recursion
make -j4
make install

../gcc-4.2.0-20070124-patched/configure --target=${TARGET} --prefix=${PREFIX} \
--enable-threads=no --disable-shared \
--with-float=hard \
--disable-__cxa_atexit --enable-target-optspace \
--disable-libmudflap --disable-nls \
--with-gnu-ld --with-gnu-plts \
--disable-multilib --enable-languages=c
make -j4 -l all-gcc
make -l install-gcc


cd ../linux-2.6.18.8-from-mips/include
rename linux, asm-mips, asm-generic to -org
cp linux-libc-headers-2.6.18.0/include/linux linux
cp linux-libc-headers-2.6.18.0/include/asm-mips asm-mips
cp linux-libc-headers-2.6.18.0/include/asm-generic asm-mips
cp Kbuild files from org to new directories

+ ln -sf /usr/src/redhat/BUILD/linux-libc-headers-2.6.18.0/include/linux include
+ ln -sf /usr/src/redhat/BUILD/linux-libc-headers-2.6.18.0/include/asm-mips include/asm
+ ln -sf /usr/src/redhat/BUILD/linux-libc-headers-2.6.18.0/include/asm-generic include/asm-generic

cd ../linux-2.6.18.8-from-mips
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install


// use `hard`
CFLAGS='-U_FORTIFY_SOURCE  -EL -march=mips1 -mabi=32    -mhard-float  -O2'
../glibc-2.13/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=${TARGET} \
--without-cvs --disable-profile --without-gd \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-debug --disable-sanity-checks --enable-obsolete-rpc \
libc_cv_forced_unwind=yes libc_cv_c_cleanup=yes \
--enable-kernel=2.6.18 --with-__thread --with-tls --enable-shared \
--without-fp --enable-add-ons=nptl,../glibc-ports-2.13

make install-bootstrap-headers=yes BUILD_CFLAGS='-O2 -g' install-headers
touch ${PREFIX}/${TARGET}/include/gnu/stubs.h
cp -v ../glibc-2.13/include/features.h ${PREFIX}/${TARGET}/include/features.h 
cp -v ./bits/stdio_lim.h ${PREFIX}/${TARGET}/include/bits/stdio_lim.h
cp -v ./misc/syscall-list.h ${PREFIX}/${TARGET}/include/bits/syscall.h
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
make -j2 -l BUILD_CFLAGS='-O2 -g' csu/subdir_lib
cp csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib


../gcc-4.2.0-20070124-patched/configure --target=${TARGET} --prefix=${PREFIX} \
--enable-languages=c,c++ \
--disable-__cxa_atexit --enable-target-optspace \
--with-gnu-ld --with-float=hard --enable-threads \
--with-arch=mips32 --disable-libmudflap --disable-nls --with-gnu-plts
make -j4 -l all-gcc
make -l install-gcc


cd build-glibc
make -j5
make install


cd build-gcc
make -j5
make install


={============================================================================
*kt_linux_gcc_400* gcc-toolchain-glibc-mips-own

<14-test> okay: gcc-4.2.0-brcm, glibc-2.13, linux-2.6.18.8-from-mips, binutils-2.17.50, float-hard

PREFIX=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-03
TARGET=mips-linux-gnu
export PATH=${PREFIX}/bin:$PATH

1. binutils

// note: do not create makefile. ???
// ../binutils-2.17.50/configure --prefix=${PREFIX} --target=${TARGET} --disable-werror --disable-nls --enable-multilib \
//--no-create --no-recursion

../binutils-2.17.50/configure --prefix=${PREFIX} --target=mips-linux-gnu --disable-werror --disable-nls --enable-multilib 
make -j4
make install


2. first-pass gcc

cd gcc-build
../gcc-4.2.0-20070124-patched/configure --target=${TARGET} --prefix=${PREFIX} \
--enable-threads=no --disable-shared \
--with-float=hard \
--disable-__cxa_atexit --enable-target-optspace \
--disable-libmudflap --disable-nls \
--with-gnu-ld --with-gnu-plts \
--disable-multilib --enable-languages=c
make -j4 -l all-gcc
make -l install-gcc


3. kernel headers

cd ../linux-2.6.18.8-from-mips/include
rename linux, asm-mips, asm-generic to -org
cp linux-libc-headers-2.6.18.0/include/linux linux
cp linux-libc-headers-2.6.18.0/include/asm-mips asm-mips
cp linux-libc-headers-2.6.18.0/include/asm-generic asm-mips
cp Kbuild files from org to new directories

note:
faild to find ./include/asm-mips/isadep.h in spk-gdb build. why not copied?
cp /home/nds-uk/kyoupark/asn/gcc/linux-2.6.18.8-from-mips/include/asm-mips/isadep.h ~/asn/gcc/gcc-glibc-brcm-mips-install-two/mips-linux-gnu/include/asm


cd ../linux-2.6.18.8-from-mips
make ARCH=mips INSTALL_HDR_PATH=${PREFIX}/${TARGET} headers_install


4. glibc headers and start-up files

CFLAGS='-U_FORTIFY_SOURCE  -EL -march=mips1 -mabi=32    -mhard-float  -O2' \
CFLAGS='-U_FORTIFY_SOURCE  -EL -march=mips1 -mabi=32    -mhard-float  -g3' \

/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/config.h:3:3: error: #error "glibc cannot be compiled without optimization"

/* config.h.  Generated from config.h.in by configure.  */
#if !defined __ASSEMBLER__ && !defined _ISOMAC && !defined __OPTIMIZE__
# error "glibc cannot be compiled without optimization"
#endif

CFLAGS='-U_FORTIFY_SOURCE  -EL -march=mips1 -mabi=32    -mhard-float  -Og' \
checking for suffix of object files... configure: error: in `/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build':
configure: error: cannot compute suffix of object files: cannot compile

CFLAGS='-O -g3' \
/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/libc_pic.os: In function `_mcount':
../sysdeps/generic/unwind-pe.c:(.debug_macinfo+0x7db399c): relocation truncated to fit: R_MIPS_HI16 against `_gp_disp'
collect2: ld returned 1 exit status

CFLAGS='-O -g' \
../glibc-2.13/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=${TARGET} \
--without-cvs --disable-profile --without-gd \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-debug --disable-sanity-checks --enable-obsolete-rpc \
libc_cv_forced_unwind=yes libc_cv_c_cleanup=yes \
--enable-kernel=2.6.18 --with-__thread --with-tls --enable-shared \
--without-fp --enable-add-ons=nptl,../glibc-ports-2.13

make install-bootstrap-headers=yes BUILD_CFLAGS='-O2 -g' install-headers
touch ${PREFIX}/${TARGET}/include/gnu/stubs.h
cp -v ../glibc-2.13/include/features.h ${PREFIX}/${TARGET}/include/features.h 
cp -v ./bits/stdio_lim.h ${PREFIX}/${TARGET}/include/bits/stdio_lim.h
cp -v ./misc/syscall-list.h ${PREFIX}/${TARGET}/include/bits/syscall.h
${TARGET}-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o ${PREFIX}/${TARGET}/lib/libc.so
make -j2 -l BUILD_CFLAGS='-O2 -g' csu/subdir_lib
cp csu/crt1.o csu/crti.o csu/crtn.o ${PREFIX}/${TARGET}/lib


5. second-pass gcc *gcc-second-pass*

cd build-gcc, *gcc-second-pass*

note:
this is different --enable-languages=c,c++ and build and install libgcc_s.so.1

$ readelf -d libgcc_s.so.1

 0x00000001 (NEEDED)                     Shared library: [libc.so]
 0x0000000e (SONAME)                     Library soname: [libgcc_s.so.1]

if move this up before gcc-second-pass, build fails since has dependancy on
libgcc.

../gcc-4.2.0-20070124-patched/configure --target=${TARGET} --prefix=${PREFIX} \
--enable-languages=c,c++ \
--disable-__cxa_atexit --enable-target-optspace \
--with-gnu-ld --with-float=hard --enable-threads \
--with-arch=mips32 --disable-libmudflap --disable-nls --with-gnu-plts

make -j4 -l all-gcc
make -l install-gcc

// but this from glibc-reference 
//
// make -j4 all-target-libgcc
// make install-target-libgcc
// 
// but fails 
// make -j4 all-target-libgcc
// make: *** No rule to make target `all-target-libgcc'.  Stop.


6. libc

note: 
updates libc.so which is linker script

cd build-glibc
make -j5
make install


<build-ld-so> note: librtld
mips-linux-gnu-gcc -mabi=32   -nostdlib -nostartfiles -shared -o /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/ld.so			\
		  -Wl,-z,relro -Wl,-z,defs 	\
		  /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/librtld.os -Wl,--version-script=/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/ld.map		\
		  -Wl,-soname=ld.so.1 -T /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/ld.so.lds

ln -s ld.so /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/ld.so.1

mips-linux-gnu-gcc -mabi=32 -nostdlib -nostartfiles -o
/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/iconv/iconvconfig
-Wl,-dynamic-linker=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install-03/mips-linux-gnu/lib/ld.so.1
-Wl,-z,relro  /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/csu/crt1.o
/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/csu/crti.o ``mips-linux-gnu-gcc -mabi=32  --print-file-name=crtbegin.o``
/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/iconv/iconvconfig.o
/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/iconv/strtab.o
/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/iconv/xmalloc.o
/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/iconv/hash-string.o
-Wl,-rpath-link=/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build:/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/math:/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf:/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/dlfcn:/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/nss:/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/nis:/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/rt:/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/resolv:/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/crypt:/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/nptl
/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/libc.so.6
/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/libc_nonshared.a
/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/ld.so  -lgcc -lgcc_eh
``mips-linux-gnu-gcc -mabi=32  --print-file-name=crtend.o``
/home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/csu/crtn.o


<install-ld-so> <install-libc>
/usr/bin/install -c /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/ld.so ${PREFIX}/mips-linux-gnu/lib/ld-2.13.so.new
mv -f ${PREFIX}/mips-linux-gnu/lib/ld-2.13.so.new ${PREFIX}/mips-linux-gnu/lib/ld-2.13.so

/usr/bin/install -c /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/libc.so ${PREFIX}/mips-linux-gnu/lib/libc-2.13.so.new
mv -f ${PREFIX}/mips-linux-gnu/lib/libc-2.13.so.new ${PREFIX}/mips-linux-gnu/lib/libc-2.13.so

rm -f ${PREFIX}/mips-linux-gnu/lib/ld.so.1
ln -s ld-2.13.so ${PREFIX}/mips-linux-gnu/lib/ld.so.1


note:
so do it again to build libgcc_s.so.1

$ readelf -d ./gcc/libgcc_s.so.1
 0x00000001 (NEEDED)                     Shared library: [libc.so.6]
 0x0000000e (SONAME)                     Library soname: [libgcc_s.so.1]

../gcc-4.2.0-20070124-patched/configure --target=${TARGET} --prefix=${PREFIX} \
--enable-languages=c,c++ \
--disable-__cxa_atexit --enable-target-optspace \
--with-gnu-ld --with-float=hard --enable-threads \
--with-arch=mips32 --disable-libmudflap --disable-nls --with-gnu-plts

make -j4 -l all-gcc
make -l install-gcc


cd build-gcc
make -j5
make install

<todo>
edit libc.so

<todo>
see <error> PATH_MAX
cp /home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/lib/gcc/mips-linux-uclibc/4.2.0/include/limits.h /home/NDS-UK/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/lib/gcc/mips-linux-gnu/4.2.0/include/limits.h

<todo>
change this and see <error> ARG_MAX
gcc-glibc-brcm-mips-install/mips-linux-gnu/include/bits/local_lim.h


={============================================================================
*kt_linux_gcc_400* gcc-toolchain-tool-asan

gcc/gcc.c:7683:   /* -fsanitize= and -fsanitize-recover= can take
gcc/opts.c:958:    error_at (loc, "-fsanitize-recover=thread is not supported");
gcc/opts.c:961:    error_at (loc, "-fsanitize-recover=leak is not supported");

note: without sysroot

<enalbe-asan>

// gcc-4.8.2/configure.ac

# Disable libsanitizer on unsupported systems.
if test -d ${srcdir}/libsanitizer; then
  if test x$enable_libsanitizer = x; then
     AC_MSG_CHECKING([for libsanitizer support])

     # "test -n" true if the length of string is non-zero.

     if (srcdir=${srcdir}/libsanitizer; \
        . ${srcdir}/configure.tgt; \
        test -n "$UNSUPPORTED")
     then
         AC_MSG_RESULT([no])
         noconfigdirs="$noconfigdirs target-libsanitizer"
     else
         AC_MSG_RESULT([yes])
     fi
  fi
fi


// gcc-4.8.2/libsanitizer/configure.tgt

# Filter out unsupported systems.
case "${target}" in
  x86_64-*-linux* | i?86-*-linux*)
	if test x$ac_cv_sizeof_void_p = x8; then
		TSAN_SUPPORTED=yes
	fi
	;;
  powerpc*-*-linux*)
	;;
  sparc*-*-linux*)
	;;
  x86_64-*-darwin[1]* | i?86-*-darwin[1]*)
	TSAN_SUPPORTED=no
	;;
  *)
	UNSUPPORTED=1
	;;
esac

add this not to set(return) UNSUPPORTED as noted not to `filter out`:

  mips*-*-linux*)
	;;


0. Use either AC_CHECK or "--enable-libsanitizer".

configure:3202: checking for libsanitizer support
configure:3212: result: yes

../gcc-4.8.2/configure -C --prefix=/home/nds-uk/kyoupark/asn/install-4.8.2 --target=mips-linux --enable-languages=c,c++ --disable-multilib --enable-libsanitizer

# when use --enable-libsanitizer
# check on logs when runs "gcc/configure" to see which is excluded from.
*** This configuration is not supported in the following subdirectories:
     target-libitm gnattools target-libada target-libgfortran target-libgo target-libffi target-libbacktrace target-zlib target-libjava target-libobjc target-boehm-gc
    (Any other directories should still work fine.)


1. Run make in the step of the reference.

$ make
52:Configuring in ./fixincludes
160:Configuring in ./libiberty
654:Configuring in ./intl
775:Configuring in ./gmp
2199:Configuring in ./lto-plugin
2299:Configuring in ./gcc
2697:Configuring in ./mpfr
3536:Configuring in ./mpc
3877:Configuring in build-i686-pc-linux-gnu/libiberty
4343:Configuring in build-i686-pc-linux-gnu/fixincludes
4465:Configuring in ./zlib
4571:Configuring in ./libbacktrace
4722:Configuring in ./libcpp
4889:Configuring in ./libdecnumber
6168:Configuring in mips-linux/libgcc

$ make all-gcc
8:Configuring in ./gmp
1432:Configuring in ./lto-plugin
1515:Configuring in ./libiberty
1985:Configuring in ./intl
2106:Configuring in ./gcc
2499:Configuring in ./mpfr
3339:Configuring in ./mpc
3680:Configuring in build-x86_64-unknown-linux-gnu/libiberty
4132:Configuring in build-x86_64-unknown-linux-gnu/fixincludes
4254:Configuring in ./zlib
4360:Configuring in ./libbacktrace
4511:Configuring in ./libcpp
4678:Configuring in ./libdecnumber
4759:Configuring in ./fixincludes

When do "make" fails on libgcc and "make all-gcc" will not build "asan". 

The answer:

  @Maxim:
  Ah, no, you should do all 7 steps from the reference and just after that
  rebuild GCC like this:

  1) rm - rf *
  2) configure ... (from step 3)
  3) make - j12

You should not have problems with libgcc on that step (after you did all 7 steps from the reference).


2. do clean build as in the reference and back to step 3 and apply patches as
Maxim suggested. 

note: build outputs is in 
/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-mips/mips-linux

diff -up -r gcc-4.8.2-clean/gcc/config/mips/linux-common.h gcc-4.8.2/gcc/config/mips/linux-common.h
diff -up -r gcc-4.8.2-clean/gcc/config/mips/linux.h gcc-4.8.2/gcc/config/mips/linux.h

// Define TARGET_ASAN_SHADOW_OFFSET somewhere in gcc/config/mips/mips.c. You
// can see an example how to do this in gcc/config/arm/arm.c
diff -up -r gcc-4.8.2-clean/gcc/config/mips/mips.c gcc-4.8.2/gcc/config/mips/mips.c

// copy from 4.9.2 asan source which has mips support
diff -up -r gcc-4.8.2-clean/libsanitizer/asan/asan_linux.cc gcc-4.8.2/libsanitizer/asan/asan_linux.cc

// Add mips*-*-linux* (or whatever triplet you use) entry in
// libsanitizer/configure.tgt. This should enable libsanitizer build for MIPS.
diff -up -r gcc-4.8.2-clean/libsanitizer/configure.tgt gcc-4.8.2/libsanitizer/configure.tgt


../gcc-4.8.2/configure -C --prefix=/home/nds-uk/kyoupark/asn/install-4.8.2 --target=mips-linux --enable-languages=c,c++ --disable-multilib --enable-libsanitizer
make
make install


3. "-fstack-protector"

@Maxim:
Note that I've added  -fstack-protector flag to avoid "cc1: warning:
-fsanitize=address and -fsanitize=kernel-address are not supported for this
target" error on compilation step. This is weird, but for some reason GCC
folks use this flag to control FRAME_GROWS_DOWNWARD macro om MIPS.  GCC's ASan
doesn't support targets with FRAME_GROWS_DOWNWARD == 0 so I just added
-fstack-protector as a workaround.

$ mips-linux-gcc -fsanitize=address  <gcc_tree_location>/gcc/testsuite/c-c++-common/asan/heap-overflow-1.c -fstack-protector
$ qemu-mips -L $SYSROOT -R 0 ./a.out 

note: If misses out patch in mips.c then will get this error even if use
-fstack-protector.

./bin/mips-linux-gcc -fsanitize=address -Wl,-rpath=/home/NDS-UK/kyoupark/asn/install-4.8.2/mips-linux/lib ~/uaf.c
/home/nds-uk/kyoupark/asn/x.c:1:0: warning: -fsanitize=address not supported for this target [enabled by default]
 #include <stdio.h>
 ^
./bin/mips-linux-gcc -fsanitize=address -Wl,-rpath=/home/NDS-UK/kyoupark/asn/install-4.8.2/mips-linux/lib ~/uaf.c -fstack-protector


// why not clean??
// rm
rm -rf /home/kyoupark/STB_SW/THIRD_PARTY_LIBRARIES/BSKYB_JTH/build/applications/Picasso/picasso/jpa/out


={============================================================================
*kt_linux_gcc_400* gcc-build-mips-uclibc

note: based on cross-ng

use a specific kernel version
wget --no-check-certificate https://www.kernel.org/pub/linux/kernel/v2.6/linux-2.6.19.tar.xz

INSTALL_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2-uc-two
export PATH=$INSTALL_PATH/bin:$PATH


1. binutil
===========

// [INFO ]  Installing binutils for host
// [DEBUG]    Entering '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-binutils-host-i686-build_pc-linux-gnu'
// [EXTRA]    Configuring binutils
// [DEBUG]    Extra config passed: '--enable-ld=yes --enable-gold=no --enable-plugins --with-pkgversion=crosstool-NG crosstool-ng-1.22.0 --disable-multilib --disable-nls'
// [DEBUG]    ==> Executing: 'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe '
//   'LDFLAGS= ' '/home/kyoupark/cross/work/.build/src/binutils-2.25.1/configure'
//     '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
//     '--target=mips-unknown-linux-uclibc'
//     '--prefix=/home/kyoupark/x-tools/mips-unknown-linux-uclibc'
//     '--disable-werror' '--enable-ld=yes' '--enable-gold=no' '--enable-plugins'
//     '--with-pkgversion=crosstool-NG crosstool-ng-1.22.0' '--disable-multilib'
//     '--disable-nls' '--with-float=soft'
//     '--with-sysroot=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot' 
// [EXTRA]    Installing binutils
// [DEBUG]    ==> Executing: '/usr/bin/make' 'install' 

../binutils-2.24/configure --target=mips-unknown-linux-uclibc --prefix=${INSTALL_PATH} --disable-nls --disable-multilib --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4
make install


2. kernel headers
===========
note: 
differnt from cross-ng since if do it after gcc-post-1 as cross-ng then see
build errors.

// [EXTRA]    Installing kernel headers
// [DEBUG]    ==> Executing: '/usr/bin/make' '-C'
//   '/home/kyoupark/cross/work/.build/src/linux-4.3'
//     'CROSS_COMPILE=mips-unknown-linux-uclibc-'
//     'O=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-kernel-headers'
//     'ARCH=mips'
//     'INSTALL_HDR_PATH=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr'
//     'V=0' 'headers_install' 
// 
// /usr/bin/make -C /home/kyoupark/cross/work/.build/src/linux-4.3 CROSS_COMPILE=mips-unknown-linux-uclibc- \
// O=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-kernel-headers 
// ARCH=mips INSTALL_HDR_PATH=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr V=0 headers_install
//
// note:
// install to ${INSTALL_HDR_PATH}/include
//
// kyoupark@kit-debian:~/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/include$ ll
// total 72
// drwxr-xr-x 14 kyoupark ccusers  4096 Dec  1 15:48 ./
// drwxr-xr-x  4 kyoupark ccusers  4096 Dec  1 15:48 ../
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 asm/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 asm-generic/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 drm/
// drwxr-xr-x 25 kyoupark ccusers 20480 Dec  1 15:48 linux/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 misc/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 mtd/
// drwxr-xr-x  3 kyoupark ccusers  4096 Dec  1 15:48 rdma/
// drwxr-xr-x  3 kyoupark ccusers  4096 Dec  1 15:48 scsi/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 sound/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 uapi/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 video/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 xen/


cd linux-2.6.19
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr headers_install

cd linux-3.17.2
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr headers_install

// home/nds-uk/kyoupark/asn/install-4.8.2-own/mips-unknown-linux-uclibc/sysroot/usr


3. C/C++ Compilers (pass 1)
===========

// [INFO ]  Installing pass-1 core C gcc compiler
// [DEBUG]    Entering '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-cc-gcc-core-pass-1'
// [EXTRA]    Configuring core C gcc compiler

// note: this seems to do nothing
// [DEBUG]    Copying headers to install area of core C compiler
// [DEBUG]    ==> Executing: 'cp' '-a'
// '/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/include'
// '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools/mips-unknown-linux-uclibc/include' 
//
// [DEBUG]    ==> Executing: 'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc'
//   'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe ' 'LDFLAGS= '
//     '/home/kyoupark/cross/work/.build/src/gcc-5.2.0/configure'
//     '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
//     '--target=mips-unknown-linux-uclibc'
//     '--prefix=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-local-prefix=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
//     '--with-sysroot=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
//     '--with-newlib' '--enable-threads=no'  note: only in pass-1 
//     '--disable-shared'                     note: different from pass-2
//     '--with-pkgversion=crosstool-NG crosstool-ng-1.22.0' 
//     '--with-arch=mips1' '--with-abi=32' '--with-float=soft' '--enable-__cxa_atexit'
//     '--with-gmp=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-mpfr=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-mpc=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-isl=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-cloog=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-libelf=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--enable-lto' '--with-host-libstdcxx=-static-libgcc
//     -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' '--enable-target-optspace'
//     '--with-mips-plt' '--disable-libgomp' '--disable-libmudflap'
//     '--disable-libssp' '--disable-libquadmath' '--disable-libquadmath-support'
//     '--disable-nls' '--disable-multilib' '--enable-languages=c' 

../`gcc-4.8.2-clean`/configure --prefix=${INSTALL_PATH} --target=mips-unknown-linux-uclibc --enable-languages=c,c++ \
--disable-libmudflap --disable-multilib --disable-nls --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot

// ../gcc-4.8.2-clean/configure --prefix=${INSTALL_PATH} --target=mips-unknown-linux-uclibc --enable-languages=c,c++ --disable-multilib --disable-nls \
// --with-local-prefix=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot \
// --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot

make -j5 all-gcc
make install-gcc


4. Standard C Library Headers and Startup Files(uclibc)
===========

// note: untar to "build/src"
// 
// [INFO ]  =================================================================
// [INFO ]  Extracting and patching toolchain components
// [DEBUG]    Entering '/home/kyoupark/cross/work/.build/src'
// [EXTRA]    Extracting 'uClibc-ng-1.0.9'
// [DEBUG]    ==> Executing: 'mkdir' '-p' 'uClibc-ng-1.0.9' 
// [DEBUG]    ==> Executing: 'tar' '--strip-components=1' '-C' 'uClibc-ng-1.0.9' '-xv' '-f' '-' 
// 
//            /home/kyoupark/cross/work/.build/src/uClibc-ng-1.0.9
// 
// [INFO ]  =================================================================
// [INFO ]  Checking C library configuration
// [EXTRA]    Manage uClibc configuration
// [DEBUG]    ==> Executing: 'rm' '-f' '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/configs/uClibc.config' 
// [DEBUG]    ==> Executing: 'mkdir' '-p' '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/configs' 
// [DEBUG]    ==> Executing: 'cp'
//   '/home/kyoupark/cross/install/lib/crosstool-ng-1.22.0/contrib/uClibc-defconfigs/uClibc-ng.config'
//     '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/configs/uClibc.config' 
// 
// note: copy files from "build/src" to "build for header"
// 
// [INFO ]  =================================================================
// [INFO ]  Installing C library headers
// [EXTRA]    Copying sources to build dir
// [DEBUG]    ==> Executing: 'cp' '-av'
//   '/home/kyoupark/cross/work/.build/src/uClibc-ng-1.0.9'
//     '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/`build-libc-headers`' 

note: current directory '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc-headers'

// 
// note: after all, use "config" from "uClibc-ng.config"
// 
// [DEBUG]    ==> Executing: 'cp'
// '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/configs/uClibc.config'
//   '.config' 
//
// .config changes
// 
// from cross-ng
// KERNEL_HEADERS="/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/include"
//
// from BR
// TARGET_mips=y
// TARGET_ARCH="mips"
// FORCE_OPTIONS_FOR_ARCH=y
// CONFIG_MIPS_O32_ABI=y
// KERNEL_HEADERS="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1-mips-only/output/build/linux-headers-3.2.81/usr/include"
// CROSS_COMPILER_PREFIX="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1-mips-only/output/host/usr/bin/mips-buildroot-linux-uclibc-"
// 
// RUNTIME_PREFIX="/"
// DEVEL_PREFIX="/usr/"
// KERNEL_HEADERS="/home/nds-uk/kyoupark/asn/install-4.8.2-own/mips-unknown-linux-uclibc/sysroot/usr/include"
//
// [EXTRA]    Applying configuration
// [DEBUG]    ==> Executing: '/usr/bin/make'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'oldconfig' 
// 
// [ALL  ]    make[1]: Entering directory '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc-headers'
// [ALL  ]    * Restart config...
// [ALL  ]    *
// [ALL  ]    *
// [ALL  ]    * uClibc-ng 1.0.9 C Library Configuration
//
// 
// from INSTALL
//
// If you have an existing .config file, you can update this file
// using the
//
//       make oldconfig
//
// command, which will only ask you about new configuration options.

note: since binuitl is in PATH

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ oldconfig 

// kyoupark@ukstbuild2:~/asn/uclibc/uClibc-0.9.33.2-build$ make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ oldconfig
//   MKDIR include/config
//   HOSTCC-o extra/config/conf.o
//   GEN extra/config/zconf.tab.c
//   GEN extra/config/lex.zconf.c
//   GEN extra/config/zconf.hash.c
//   HOSTCC-o extra/config/zconf.tab.o
//   HOSTCC extra/config/conf
// #
// # configuration written to ./.config
// #

// note: build and install headers, startup, dummy c lib to "x-tools/sysroot"
// 
// [EXTRA]    Building headers
// [DEBUG]    ==> Executing: '/usr/bin/make'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'headers' 
//
// make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/ LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz headers

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ headers 

// [EXTRA]    Installing headers
// [DEBUG]    ==> Executing: '/usr/bin/make'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'install_headers' 
//
// [ALL  ]    make[1]: Entering directory '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc-headers'
// [ALL  ]      INSTALL include -> /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
// [ALL  ]    make[1]: Leaving directory '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc-headers'
// 
// note: this step really copies headers
// WHY "../sysroot/usr/include"? 
// since "install_headers.sh" uses:
// dstdir=${2:-`. ./.config 2>/dev/null && echo ${DEVEL_PREFIX}/include`}
//
// this is from .config
// DEVEL_PREFIX="/usr/"
//
// kyoupark@kit-debian:~/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc-headers$ make Q="echo " CROSS_COMPILE=mips-unknown-linux-uclibc- \
// UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/ LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz install_headers
// echo
// 
//   INSTALL include -> /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
// echo top_builddir=./ \
// ./extra/scripts/install_headers.sh \
// include /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
// top_builddir=./ ./extra/scripts/install_headers.sh include /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
//
// echo cd /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include && rm -f -r config generated internal cancel.h dl-osinfo.h jmpbuf-offsets.h jmpbuf-unwind.h hp-timing.h not-cancel.h _lfs_64.h bits/uClibc_arch_features.h bits/kernel_sigaction.h bits/kernel_stat.h bits/kernel_types.h bits/libc-lock.h bits/stdio-lock.h bits/syscalls.h bits/syscalls-common.h bits/uClibc_fpmax.h bits/uClibc_mutex.h bits/uClibc_pthread.h bits/uClibc_uintmaxtostr.h bits/uClibc_uwchar.h bits/uClibc_va_copy.h bits/sigcontextinfo.h bits/stackinfo.h atomic.h bits/atomic.h tls.h rpc/des_crypt.h rpc/key_prot.h rpc/rpc_des.h fenv.h bits/fenv.h bits/fenvinline.h fts.h libintl.h netinet/ip6.h netinet/icmp6.h execinfo.h iconv.h bits/uClibc_ctype.h rpc wordexp.h xlocale.h ustat.h sys/ustat.h bits/ustat.h
// cd /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
// echo cd /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include && rm -f -f wchar-stub.h
// cd /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
//
// make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe \
// PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/ LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz install_headers
// 

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install_headers 

// 
// this is wrong
// kyoupark@ukstbuild2:~/asn/uclibc/uClibc-0.9.33.2-build$ make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install_headers
//   INSTALL include -> /home/nds-uk/kyoupark/asn/install-4.8.2-own/mips-unknown-linux-uclibc/sysroot//usr/mips-linux-uclibc/usr/include

// [EXTRA]    Building start files
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'STRIPTOOL=true' 'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz'
//     'lib/crt1.o' 'lib/crti.o' 'lib/crtn.o' 

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ STRIPTOOL=true lib/crt1.o lib/crti.o lib/crtn.o

//   MKDIR lib
//   AS lib/crt1.o
// libc/sysdeps/linux/mips/crt1.S: Assembler messages:
// libc/sysdeps/linux/mips/crt1.S:117: Warning: no .cprestore pseudo-op used in PIC code
//   AS lib/crti.o
//   AS lib/crtn.o

// [EXTRA]    Building dummy shared libs
// [DEBUG]    ==> Executing: 'mips-unknown-linux-uclibc-gcc' '-nostdlib'
//   '-nostartfiles' '-shared' '-x' 'c' '/dev/null' '-o' 'libdummy.so' 

mips-unknown-linux-uclibc-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o libdummy.so

// [EXTRA]    Installing start files
// [DEBUG]    ==> Executing: '/usr/bin/install' '-c' '-m' '0644' 'lib/crt1.o'
//   'lib/crti.o' 'lib/crtn.o'
//     '/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/lib' 

note: "lib" is created before in cross-ng?
mkdir -p ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0644 lib/crt1.o lib/crti.o lib/crtn.o ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib

// [EXTRA]    Installing dummy shared libs
// [DEBUG]    ==> Executing: '/usr/bin/install' '-c' '-m' '0755' 'libdummy.so'
//   '/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/lib/libc.so'

/usr/bin/install -c -m 0755 libdummy.so ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib/libc.so


5. Compiler Support Library (libgcc)
===========

[INFO ]  =================================================================
[INFO ]  Installing pass-2 core C gcc compiler
[DEBUG]    Entering '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-cc-gcc-core-pass-2'
[EXTRA]    Configuring core C gcc compiler
[DEBUG]    Copying headers to install area of core C compiler
[DEBUG]    ==> Executing: 'cp' '-a' '/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/include' '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools/mips-unknown-linux-uclibc/include' 
[DEBUG]    Extra config passed: '--enable-shared --with-pkgversion=crosstool-NG crosstool-ng-1.22.0 --with-arch=mips1 --with-abi=32 --with-float=soft --enable-__cxa_atexit --with-gmp=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --with-mpfr=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --with-mpc=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --with-isl=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --with-cloog=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --with-libelf=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --enable-lto --with-host-libstdcxx=-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm --enable-target-optspace --with-mips-plt --disable-libgomp --disable-libmudflap --disable-libssp --disable-libquadmath --disable-libquadmath-support --disable-nls --disable-multilib'
[DEBUG]    ==> Executing: 'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc'
  'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe ' 'LDFLAGS= '
    '/home/kyoupark/cross/work/.build/src/gcc-5.2.0/configure'
    '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
    '--target=mips-unknown-linux-uclibc'
    '--prefix=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-local-prefix=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
    '--with-sysroot=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
    '--enable-shared'                       note: different from pass-1
    '--with-pkgversion=crosstool-NG crosstool-ng-1.22.0'
    '--with-arch=mips1' '--with-abi=32' '--with-float=soft' '--enable-__cxa_atexit'
    '--with-gmp=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-mpfr=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-mpc=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-isl=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-cloog=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-libelf=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--enable-lto' '--with-host-libstdcxx=-static-libgcc
    -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' '--enable-target-optspace'
    '--with-mips-plt' '--disable-libgomp' '--disable-libmudflap'
    '--disable-libssp' '--disable-libquadmath' '--disable-libquadmath-support'
    '--disable-nls' '--disable-multilib' '--enable-languages=c' 
  
note:
must do "make install-target-libgcc". Otherwise, fails on next uclibc build.

cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc

kyoupark@ukstbuild2:~/asn/install-4.8.2-own/mips-unknown-linux-uclibc/lib$ ls -al
total 1552
drwxr-xr-x 3 kyoupark ccusers    4096 Dec  2 09:22 .
drwxr-xr-x 5 kyoupark ccusers    4096 Nov 29 16:02 ..
drwxr-xr-x 2 kyoupark ccusers    4096 Nov 29 13:35 ldscripts
lrwxrwxrwx 1 kyoupark ccusers      13 Dec  2 09:22 libgcc_s.so -> libgcc_s.so.1
-rw-r--r-- 1 kyoupark ccusers 1576912 Dec  2 09:22 libgcc_s.so.1


6. Standard C Library (back to uclibc)
===========

// note: copy files from "build/src" to "build for libc"
//
// [INFO ]  Installing C library
// [EXTRA]    Copying sources to build dir
// [DEBUG]    ==> Executing: 'cp' '-av'
//   '/home/kyoupark/cross/work/.build/src/uClibc-ng-1.0.9'
//     '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc' 

// note: same as "build-libc-headers"
//
// [DEBUG]    ==> Executing: 'cp'
// '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/configs/uClibc.config'
//   '.config' 
// 
// [EXTRA]    Applying configuration
// [DEBUG]    ==> Executing: '/usr/bin/make'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'oldconfig' 

// [EXTRA]    Building C library
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j1'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'STRIPTOOL=true' 'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'pregen' 
// 
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l'
// 'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//   'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//   'STRIPTOOL=true' 'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'all' 
//
// [EXTRA]    Installing C library
// [DEBUG]    ==> Executing: '/usr/bin/make'
// 'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
// 'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
// 'STRIPTOOL=true' 'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'install' 

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ pregen
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ all

libc/libc_so.a(if_index.os): In function `__GI_if_nameindex':
if_index.c:(.text+0x328): undefined reference to `IFLA_RTA'
if_index.c:(.text+0x340): undefined reference to `IFLA_PAYLOAD'
collect2: error: ld returned 1 exit status
make: *** [lib/libc.so] Error 1

note:
Tried linux-3.17.2 instead and causes different error:

/home/nds-uk/kyoupark/asn/install-4.8.2-own/mips-unknown-linux-uclibc/sysroot/usr/include/linux/sysinfo.h:8:2: error: unknown type name '__kernel_long_t'
  __kernel_long_t uptime;  /* Seconds since boot */
  ^
make: *** [libc/inet/if_index.os] Error 1

<uclibc-note>
===========
Here fails to use uClibc-0.9.33.2 and tried uClibc-ng-1.0.17 instead with the
same kernel and built! However, failed with linux-2.6.19 around the same code
as uClibc case above.

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install


7. (back to gcc)
===========

// [INFO ]  =================================================================
// [INFO ]  Installing final gcc compiler
// [DEBUG]    ==> Executing: 'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc'
//   'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe ' 'LDFLAGS= '
//     'CFLAGS_FOR_TARGET= -EB -march=mips1 -mabi=32    -msoft-float '
//     'CXXFLAGS_FOR_TARGET= -EB -march=mips1 -mabi=32    -msoft-float '
//     'LDFLAGS_FOR_TARGET= -Wl,-EB '
//     '/home/kyoupark/cross/work/.build/src/gcc-5.2.0/configure'
//     '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
//     '--target=mips-unknown-linux-uclibc'
//     '--prefix=/home/kyoupark/x-tools/mips-unknown-linux-uclibc'
//     '--with-sysroot=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
//     '--enable-languages=c,c++' '--with-arch=mips1' '--with-abi=32'
//     '--with-float=soft' '--with-pkgversion=crosstool-NG crosstool-ng-1.22.0'
//     '--enable-__cxa_atexit' '--disable-libmudflap' '--disable-libgomp'
//     '--disable-libssp' '--disable-libquadmath' '--disable-libquadmath-support'
//     '--disable-libsanitizer'
//     '--with-gmp=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-mpfr=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-mpc=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-isl=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-cloog=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-libelf=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--enable-lto' '--with-host-libstdcxx=-static-libgcc
//     -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' '--enable-threads=posix'
//     '--enable-target-optspace' '--enable-plugin' '--with-mips-plt'
//     '--disable-nls' '--disable-multilib'
//     '--with-local-prefix=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
//     '--enable-long-long' 
// 
// [EXTRA]    Building final gcc compiler
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l' 'all' 
// 
// [EXTRA]    Installing final gcc compiler
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l' 'install' 
// 
// [EXTRA]    Housekeeping for final gcc compiler
// [DEBUG]    Entering '/home/kyoupark/x-tools/mips-unknown-linux-uclibc'
// [DEBUG]    ==> Executing: 'ln' '-sfv' 'mips-unknown-linux-uclibc-gcc'
//   '/home/kyoupark/x-tools/mips-unknown-linux-uclibc/bin/mips-unknown-linux-uclibc-cc' 

make -j5
make install


<commands>
INSTALL_PATH=/home/nds-uk/kyoupark/asn/install-3.8.2-uc-two
export PATH=$INSTALL_PATH/bin:$PATH

../binutils-2.24/configure --target=mips-unknown-linux-uclibc --prefix=${INSTALL_PATH} --disable-nls --disable-multilib --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4
make install

cd linux-3.17.2
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr headers_install

../gcc-4.8.2-clean/configure --prefix=${INSTALL_PATH} --target=mips-unknown-linux-uclibc --enable-languages=c,c++ \
--disable-libmudflap --disable-multilib --disable-nls --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4 all-gcc
make install-gcc

cd uClibc-ng-1.0.17
DEVEL_PREFIX="/usr/"
KERNEL_HEADERS="/home/nds-uk/kyoupark/asn/install-4.8.2-uc-two/mips-unknown-linux-uclibc/sysroot/usr/include"
make clean
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install_headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ STRIPTOOL=true lib/crt1.o lib/crti.o lib/crtn.o
mips-unknown-linux-uclibc-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o libdummy.so

mkdir -p ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0644 lib/crt1.o lib/crti.o lib/crtn.o ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0755 libdummy.so ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib/libc.so

cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc

cd uClibc-ng-1.0.17
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ pregen
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ all
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install

cd build-gcc
make -j5
make install


={============================================================================
*kt_linux_gcc_400* gcc-build-mips-uclibc-asan

/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-uclibc-wasan

INSTALL_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan-wsys
export PATH=$INSTALL_PATH/bin:$PATH

../binutils-2.24/configure --target=mips-unknown-linux-uclibc --prefix=${INSTALL_PATH} --disable-nls --disable-multilib --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4
make install

cd linux-3.17.2
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr headers_install

# this gcc-pass-1 expects to see kernel headers from
# ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr. How?

../gcc-4.8.2-wasan/configure --prefix=${INSTALL_PATH} --target=mips-unknown-linux-uclibc --enable-languages=c,c++ \
--enable-libsanitizer --disable-libmudflap --disable-multilib --disable-nls --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4 all-gcc
make install-gcc

cd uClibc-ng-1.0.17
# change KERNEL_HEADERS in .config if intall path is changed
DEVEL_PREFIX="/usr/"
KERNEL_HEADERS="/home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/sysroot/usr/include"
make clean
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install_headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ STRIPTOOL=true lib/crt1.o lib/crti.o lib/crtn.o
mips-unknown-linux-uclibc-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o libdummy.so
mkdir -p ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0645 lib/crt1.o lib/crti.o lib/crtn.o ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0755 libdummy.so ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib/libc.so

cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ pregen
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ all
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install

cd build-gcc
make -j5
make install


<error>
/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/bin/ld: 
  warning: libstdc++.so.6, needed by /home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/lib/libasan.so, not found (try using -rpath or -rpath-link)
/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/lib/libasan.so: undefined reference to `__libc_free'
/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/lib/libasan.so: undefined reference to `__libc_malloc'
collect2: error: ld returned 1 exit status

sanitizer_allocator.o:
00000000 r $LC0
00000054 r $LC1
0000007c r $LC2
         U _ZN11__sanitizer11CheckFailedEPKciS1_yy
00000074 T _ZN11__sanitizer12InternalFreeEPv
00000000 T _ZN11__sanitizer13InternalAllocEm
         U _ZN11__sanitizer17GetPageSizeCachedEv
00000124 T _ZN11__sanitizer17LowLevelAllocator8AllocateEm
0000024c T _ZN11__sanitizer27SetLowLevelAllocateCallbackEPFvmmE
00000264 T _ZN11__sanitizer35CallocShouldReturnNullDueToOverflowEmm
         U _ZN11__sanitizer9MmapOrDieEmPKc
00000000 b _ZN11__sanitizerL24low_level_alloc_callbackE
00000000 r _ZZN11__sanitizer17LowLevelAllocator8AllocateEmE12__FUNCTION__
         U __libc_free
         U __libc_malloc
         U _gp_disp

// sanitizer_allocator.cc
// FIXME: We should probably use more low-level allocator that would
// mmap some pages and split them into chunks to fulfill requests.
// #if defined(__linux__) && !defined(__ANDROID__)
// extern "C" void *__libc_malloc(__sanitizer::uptr size);
// extern "C" void __libc_free(void *ptr);
// # define LIBC_MALLOC __libc_malloc
// # define LIBC_FREE __libc_free
// #else  // __linux__ && !ANDROID
# include <stdlib.h>
# define LIBC_MALLOC malloc
# define LIBC_FREE free
// #endif  // __linux__ && !ANDROID

fixes undefined references


<error> rpath
install-4.8.2-uclibc-asan-wsys/bin/mips-unknown-linux-uclibc-gcc -fsanitize=address ~/uaf.c -fstack-protector \
-Wl,-rpath=/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/lib

fixes warning.


<error> 
# when run on target, see lots of library dependancies.

-sh-3.2# ./a.out_from_mips_asan
./a.out_from_mips_asan: can't load library 'libasan.so.0'
./a.out_from_mips_asan: can't load library 'libstdc++.so.6'
./a.out_from_mips_asan: can't load library 'ld-uClibc.so.1'
...

$ ../../bin/mips-unknown-linux-uclibc-readelf -d libasan.so.0.0.0 | grep NEEDED
4: 0x00000001 (NEEDED)                     Shared library: [libpthread.so.0]
5: 0x00000001 (NEEDED)                     Shared library: [libdl.so.0]
6: 0x00000001 (NEEDED)                     Shared library: [libstdc++.so.6]
7: 0x00000001 (NEEDED)                     Shared library: [libm.so.0]
8: 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
9: 0x00000001 (NEEDED)                     Shared library: [ld-uClibc.so.1]
10: 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]

use LD_LIBRARY_PATH


<error>
# see this when uses gdb to see core.

(gdb) file a.out_from_mips_asan
warning: core file may not match specified executable file.
Reading symbols from /home/NDS-UK/kyoupark/a.out_from_mips_asan...Dwarf Error: wrong version in compilation unit header (is 4, should be 2) [in module /home/NDS-UK/kyoupark/a.out_from_mips_asan]


<error>
install-4.8.2-uclibc-asan-wsys-uc109/bin/mips-unknown-linux-uclibc-gcc -fsanitize=address ~/uaf.c -fstack-protector -Wl,-rpath=/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys-uc109/mips-unknown-linux-uclibc/lib
install-4.8.2-uclibc-asan-wsys-uc109/bin/mips-unknown-linux-uclibc-gcc -fsanitize=address ~/uaf.c -Wl,-rpath=/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys-uc109/mips-unknown-linux-uclibc/lib

# sam.c
#include <stdio.h>

int main(int argc, char **argv)
{
    printf("====> \n" );
    printf("====> Hello, this is sample program.\n" );
    printf("====> \n" );
    return;
}

# tsm.c (sample thread app)
# uaf(asan).c (use after free app)

1. no seg fault for sam.c when use any mips build
2. seg fault for tsm.c when use mips build with uclibc-ng-1.0.17 and core
shows:

#1  0x2aabf414 in __pthread_initialize_minimal_internal () from /home/kyoupark/asn/mips-libs-from-build/libpthread.so.0
#2  0x2ab36874 in __uClibc_init () from /home/kyoupark/asn/mips-libs-from-build/libc.so.0

3. no seg fault for tsm.c when use mips build with uclibc-ng-1.0.9. This is
why built mips and uclibc-ng-109

4. seg fault for asan.c when use mips build with uclibc-ng-1.0.9 and shows the
same core as before.

-sh-3.2# LD_LIBRARY_PATH=/mnt/tmp/asn/mips-libs-from-uclibc109 ./a.out_from_mips_asan
Segmentation fault
-sh-3.2# LD_LIBRARY_PATH=/mnt/tmp/asn/mips-libs-from-uclibc109 ./a.out_from_mips_asan_from_109
Segmentation fault

5. With/without "-fstack-protector", makes a.out in different size but both
make a seg fault.

6. forum suggests to use glibc since ASAN is heavily connected to glibc.

install-4.8.2/bin/mips-linux-gcc sam.c

7. LD_LIBRARY_PATH=/mnt/tmp/asn/mips-libs-from-glibc ./a.out_from_mips_glibc_asan
-sh: ./a.out_from_mips_glibc_asan: No such file or directory

This is because to run glibc binary on uclibc environment.

8.
install-4.8.2/bin/mips-linux-gcc sam.c -Wl,--dynamic-linker=/mnt/tmp/asn/mips-libs-from-glibc/ld.so.1

-sh-3.2# ./a.out_from_mips_glib_samc_wdlink
FATAL: kernel too old

9. built tool with linux-2.6.33
install-4.8.2-glibc-k2633/bin/mips-unknown-linux-gcc sam.c -Wl,--dynamic-linker=/mnt/tmp/asn/mips-libs-from-glibc/ld.so.1

-sh-3.2# ./a.out_a.out_from_mips_glib_samc_wk2633
FATAL: kernel too old


<static>
./bin/mips-unknown-linux-uclibc-gcc -fsanitize=address ~/uaf.c -fstack-protector -lasan -L./mips-unknown-linux-uclibc/lib/

-Wl,-rpath=/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/lib


<without-sysroot>
INSTALL_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan-wosys
export PATH=$INSTALL_PATH/bin:$PATH

../binutils-2.24/configure --target=mips-unknown-linux-uclibc --prefix=${INSTALL_PATH} --disable-nls --disable-multilib
make -j4
make install

cd linux-3.17.2
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-unknown-linux-uclibc headers_install

cd gcc-4.8.2-build-uclibc-wasan
../gcc-4.8.2-wasan/configure --prefix=${INSTALL_PATH} --target=mips-unknown-linux-uclibc --enable-languages=c,c++ \
--enable-libsanitizer --disable-libmudflap --disable-multilib --disable-nls 
make -j4 all-gcc
make install-gcc

# change KERNEL_HEADERS in .config if intall path is changed
DEVEL_PREFIX=""
KERNEL_HEADERS="/home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan-wosys/mips-unknown-linux-uclibc/include"
make clean
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc install_headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc STRIPTOOL=true lib/crt1.o lib/crti.o lib/crtn.o
mips-unknown-linux-uclibc-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o libdummy.so
mkdir -p ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0645 lib/crt1.o lib/crti.o lib/crtn.o ${INSTALL_PATH}/mips-unknown-linux-uclibc/lib
/usr/bin/install -c -m 0755 libdummy.so ${INSTALL_PATH}/mips-unknown-linux-uclibc/lib/libc.so

cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc pregen
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc all
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc install

cd build-gcc
make -j5

// /home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan/mips-unknown-linux-uclibc/bin/ld: cannot find /lib/libc.so.1
// /home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan/mips-unknown-linux-uclibc/bin/ld: cannot find /lib/uclibc_nonshared.a
// /home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan/mips-unknown-linux-uclibc/bin/ld: cannot find /lib/ld-uClibc.so.1
// collect2: error: ld returned 1 exit status
// make[2]: *** [libgcc_s.so] Error 1

make install


={============================================================================
*kt_linux_gcc_400* gcc-build-mips-uclibc-asan-combinations

BCM       uses uClibc 0.9.29
BR        uses uClibc-ng-1.0.17.tar.xz
cross-ng  uses uClibc-ng-1.0.9
          tried uClibc-0.9.33.2 and linux-3.17.2  (not okay)
          tried uClibc-ng-1.0.17 and linux-3.17.2 (okay)
          tried uClibc-ng-1.0.17 and linux-2.6.19 (not okay)
          tried uClibc-ng-1.0.17 and linux-3.17.2 (okay) but runtime error.

see <uclibc-note>

so this is to try uClibc-ng-1.0.9 and linux-3.17.2

wget http://downloads.uclibc-ng.org/releases/1.0.9/uClibc-ng-1.0.9.tar.xz


INSTALL_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan-wsys-uc109
export PATH=$INSTALL_PATH/bin:$PATH

../binutils-2.24/configure --target=mips-unknown-linux-uclibc --prefix=${INSTALL_PATH} --disable-nls --disable-multilib --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4
make install

cd linux-3.17.2
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr headers_install

cd gcc-4.8.2-build-uclibc109
../gcc-4.8.2-wasan/configure --prefix=${INSTALL_PATH} --target=mips-unknown-linux-uclibc --enable-languages=c,c++ \
--enable-libsanitizer --disable-libmudflap --disable-multilib --disable-nls --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4 all-gcc
make install-gcc

cd uClibc-ng-1.0.9
# change KERNEL_HEADERS in .config if intall path is changed
DEVEL_PREFIX="/usr/"
KERNEL_HEADERS="/home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan-wsys-uc109/mips-unknown-linux-uclibc/sysroot/usr/include"
make clean
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install_headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ STRIPTOOL=true lib/crt1.o lib/crti.o lib/crtn.o
mips-unknown-linux-uclibc-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o libdummy.so
mkdir -p ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0645 lib/crt1.o lib/crti.o lib/crtn.o ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0755 libdummy.so ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib/libc.so

cd gcc-4.8.2-build-uclibc109
make -j4 all-target-libgcc
make install-target-libgcc

cd uClibc-ng-1.0.9
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ pregen
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ all
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install

cd gcc-4.8.2-build-uclibc109
make -j5
make install

note:
failes withe the same seg fault issue.


={============================================================================
*kt_linux_gcc_400* gcc-build-tc-mips-target-tree

From BRCM

├── bin
│   ├── mipsel-linux-addr2line -> mipsel-linux-uclibc-addr2line
│   ├── mipsel-linux-c++ -> mipsel-linux-uclibc-c++
│   ├── mipsel-linux-cpp -> mipsel-linux-uclibc-cpp
│   ├── mipsel-linux-g++ -> mipsel-linux-uclibc-g++
│   ├── mipsel-linux-gcc -> mipsel-linux-uclibc-gcc
│   ├── mipsel-linux-gcc-4.2.0 -> mipsel-linux-uclibc-gcc-4.2.0

│   ├── mips-linux-c++ -> mips-linux-uclibc-c++
│   ├── mips-linux-c++filt -> mips-linux-uclibc-c++filt
│   ├── mips-linux-cpp -> mips-linux-uclibc-cpp
│   ├── mips-linux-g++ -> mips-linux-uclibc-g++
│   ├── mips-linux-gcc -> mips-linux-uclibc-gcc
│   ├── mips-linux-gcc-4.2.0 -> mips-linux-uclibc-gcc-4.2.0

├── lib
│   └── gcc
│       ├── mipsel-linux -> mipsel-linux-uclibc
│       ├── mipsel-linux-uclibc
│       │   └── 4.2.0
│       │       ├── crtbegin.o
│       │       ├── crtbeginS.o
│       │       ├── crtbeginT.o
│       │       ├── crtend.o
│       │       ├── crtendS.o
│       │       ├── finclude
│       │       ├── include
│       │       │   ├── bits
│       │       │   │   └── huge_val.h
│       │       │   ├── decfloat.h
│       │       │   ├── float.h
│       │       │   ├── iso646.h
│       │       │   ├── limits.h
│       │       │   ├── linux
│       │       │   │   └── a.out.h
│       │       │   ├── obstack.h
│       │       │   ├── omp.h
│       │       │   ├── README
│       │       │   ├── ssp
│       │       │   │   ├── ssp.h
│       │       │   │   ├── stdio.h
│       │       │   │   ├── string.h
│       │       │   │   └── unistd.h
│       │       │   ├── stdarg.h
│       │       │   ├── stdbool.h
│       │       │   ├── stddef.h
│       │       │   ├── syslimits.h
│       │       │   ├── unwind.h
│       │       │   └── varargs.h
│       │       ├── install-tools
│       │       │   ├── gsyslimits.h
│       │       │   ├── include
│       │       │   │   ├── decfloat.h
│       │       │   │   ├── float.h
│       │       │   │   ├── iso646.h
│       │       │   │   ├── limits.h
│       │       │   │   ├── README
│       │       │   │   ├── stdarg.h
│       │       │   │   ├── stdbool.h
│       │       │   │   ├── stddef.h
│       │       │   │   ├── unwind.h
│       │       │   │   └── varargs.h
│       │       │   ├── macro_list
│       │       │   └── mkheaders.conf
│       │       ├── libgcc.a
│       │       ├── libgcc_eh.a
│       │       └── libgcov.a
│       ├── mips-linux -> mips-linux-uclibc
│       └── mips-linux-uclibc
│           └── 4.2.0
│               ├── crtbegin.o
│               ├── crtbeginS.o
│               ├── crtbeginT.o
│               ├── crtend.o
│               ├── crtendS.o
│               ├── finclude
│               ├── include
│               │   ├── bits
│               │   │   └── huge_val.h
│               │   ├── decfloat.h
│               │   ├── float.h
│               │   ├── iso646.h
│               │   ├── limits.h
│               │   ├── linux
│               │   │   └── a.out.h
│               │   ├── obstack.h
│               │   ├── omp.h
│               │   ├── README
│               │   ├── ssp
│               │   │   ├── ssp.h
│               │   │   ├── stdio.h
│               │   │   ├── string.h
│               │   │   └── unistd.h
│               │   ├── stdarg.h
│               │   ├── stdbool.h
│               │   ├── stddef.h
│               │   ├── syslimits.h
│               │   ├── unwind.h
│               │   └── varargs.h
│               ├── install-tools
│               │   ├── gsyslimits.h
│               │   ├── include
│               │   │   ├── decfloat.h
│               │   │   ├── float.h
│               │   │   ├── iso646.h
│               │   │   ├── limits.h
│               │   │   ├── README
│               │   │   ├── stdarg.h
│               │   │   ├── stdbool.h
│               │   │   ├── stddef.h
│               │   │   ├── unwind.h
│               │   │   └── varargs.h
│               │   ├── macro_list
│               │   └── mkheaders.conf
│               ├── libgcc.a
│               ├── libgcc_eh.a
│               └── libgcov.a
├── libexec
│   └── gcc
│       ├── mipsel-linux-uclibc
│       │   └── 4.2.0
│       │       ├── cc1
│       │       ├── cc1plus
│       │       ├── collect2
│       │       └── install-tools
│       │           ├── fixincl
│       │           ├── fixinc.sh
│       │           └── mkheaders
│       └── mips-linux-uclibc
│           └── 4.2.0
│               ├── cc1
│               ├── cc1plus
│               ├── collect2
│               └── install-tools
│                   ├── fixincl
│                   ├── fixinc.sh
│                   └── mkheaders

├── mipsel-linux -> mipsel-linux-uclibc
├── mipsel-linux-uclibc note: this is host tool.
│   ├── bin
│   │   ├── ar
│   │   ├── as
│   │   ├── c++
│   │   ├── g++
│   │   ├── gcc
│   │   ├── ld
│   │   ├── nm
│   │   ├── objdump
│   │   ├── ranlib
│   │   ├── sstrip -> ../../bin/sstrip
│   │   └── strip
│   ├── include
│   │   ├── alloca.h
│   │   ├── a.out.h
│   │   ├── ar.h
│   │   ├── arpa
│   │   │   ├── ftp.h
│   │   │   ├── inet.h
│   │   │   ├── nameser_compat.h
│   │   │   ├── nameser.h
│   │   │   ├── telnet.h
│   │   │   └── tftp.h
│   │   ├── asm
│   │   │   ├── a.out.h
│   │   │   ├── asm.h
│   │   ├── linux
│   ├── lib
│   │   ├── crt1.o
│   │   ├── crti.o
│   │   ├── crtn.o
│   │   ├── ldscripts

│   │   ├── ld-uClibc-0.9.29.so
│   │   ├── ld-uClibc.so.0 -> ld-uClibc-0.9.29.so
│   │   ├── libc.a
│   │   ├── libcrypt-0.9.29.so
│   │   ├── libcrypt.a
│   │   ├── libcrypt.so -> libcrypt.so.0
│   │   ├── libcrypt.so.0 -> libcrypt-0.9.29.so
│   │   ├── libc.so
│   │   ├── libc.so.0 -> libuClibc-0.9.29.so
│   │   ├── libdl-0.9.29.so
│   │   ├── libdl.a
│   │   ├── libdl.so -> libdl.so.0
│   │   ├── libdl.so.0 -> libdl-0.9.29.so
│   │   ├── libgcc_s.so -> libgcc_s.so.1
│   │   ├── libgcc_s.so.1
│   │   ├── libgomp.a
│   │   ├── libgomp.la
│   │   ├── libgomp.so -> libgomp.so.1.0.0
│   │   ├── libgomp.so.1 -> libgomp.so.1.0.0
│   │   ├── libgomp.so.1.0.0
│   │   ├── libgomp.spec
│   │   ├── libiberty.a
│   │   ├── libm-0.9.29.so
│   │   ├── libm.a
│   │   ├── libm.so -> libm.so.0
│   │   ├── libm.so.0 -> libm-0.9.29.so
│   │   ├── libnsl-0.9.29.so
│   │   ├── libnsl.a
│   │   ├── libnsl.so -> libnsl.so.0
│   │   ├── libnsl.so.0 -> libnsl-0.9.29.so
│   │   ├── libpthread-0.9.29.so
│   │   ├── libpthread.a
│   │   ├── libpthread_nonshared.a
│   │   ├── libpthread.so
│   │   ├── libpthread.so.0 -> libpthread-0.9.29.so
│   │   ├── libresolv-0.9.29.so
│   │   ├── libresolv.a
│   │   ├── libresolv.so -> libresolv.so.0
│   │   ├── libresolv.so.0 -> libresolv-0.9.29.so
│   │   ├── librt-0.9.29.so
│   │   ├── librt.a
│   │   ├── librt.so -> librt.so.0
│   │   ├── librt.so.0 -> librt-0.9.29.so
│   │   ├── libssp.a
│   │   ├── libssp.la
│   │   ├── libssp_nonshared.a
│   │   ├── libssp_nonshared.la
│   │   ├── libssp.so -> libssp.so.0.0.0
│   │   ├── libssp.so.0 -> libssp.so.0.0.0
│   │   ├── libssp.so.0.0.0
│   │   ├── libstdc++.a
│   │   ├── libstdc++.la
│   │   ├── libstdc++_pic.a
│   │   ├── libstdc++.so -> libstdc++.so.6.0.9
│   │   ├── libstdc++.so.6 -> libstdc++.so.6.0.9
│   │   ├── libstdc++.so.6.0.9
│   │   ├── libsupc++.a
│   │   ├── libsupc++.la
│   │   ├── libthread_db-0.9.29.so
│   │   ├── libthread_db.a
│   │   ├── libthread_db.so -> libthread_db.so.1
│   │   ├── libthread_db.so.1 -> libthread_db-0.9.29.so
│   │   ├── libuClibc-0.9.29.so
│   │   ├── libutil-0.9.29.so
│   │   ├── libutil.a
│   │   ├── libutil.so -> libutil.so.0
│   │   ├── libutil.so.0 -> libutil-0.9.29.so
│   │   ├── Scrt1.o
│   │   └── uclibc_nonshared.a
│   ├── sys-include -> include
│   └── target-apps
│       ├── sbin
│       │   └── ldconfig
│       └── usr
│           └── bin
│               ├── iconv
│               ├── ldd
│               └── mipsel-linux-uclibc-gdbserver

├── mips-linux -> mips-linux-uclibc
├── mips-linux-uclibc
│   ├── bin


={============================================================================
*kt_linux_gcc_400* gcc-build-tc-buildroot-mips-target-tree

├── bin
│   ├── faked
│   ├── fakeroot
│   ├── gawk
│   ├── igawk
│   ├── m4
│   ├── makedevs
│   ├── mips-buildroot-linux-gnu-addr2line
│   ├── mips-buildroot-linux-gnu-ar
│   ├── mips-buildroot-linux-gnu-as
│   ├── mips-buildroot-linux-gnu-c++ -> toolchain-wrapper
│   ├── mips-buildroot-linux-gnu-c++.br_real
│   ├── mips-buildroot-linux-gnu-cc -> toolchain-wrapper
│   ├── mips-buildroot-linux-gnu-cc.br_real
│   ├── mips-buildroot-linux-gnu-c++filt
│   ├── mips-buildroot-linux-gnu-cpp -> toolchain-wrapper
│   ├── mips-buildroot-linux-gnu-cpp.br_real
│   ├── mips-buildroot-linux-gnu-elfedit
│   ├── mips-buildroot-linux-gnu-g++ -> toolchain-wrapper
│   ├── mips-buildroot-linux-gnu-g++.br_real
│   ├── mips-buildroot-linux-gnu-gcc -> toolchain-wrapper
│   ├── mips-buildroot-linux-gnu-gcc-4.9.4 -> toolchain-wrapper
│   ├── mips-linux-addr2line -> mips-buildroot-linux-gnu-addr2line
│   ├── mips-linux-ar -> mips-buildroot-linux-gnu-ar
│   ├── mips-linux-as -> mips-buildroot-linux-gnu-as
│   ├── mips-linux-c++ -> toolchain-wrapper
│   ├── mips-linux-c++.br_real -> mips-buildroot-linux-gnu-c++.br_real
│   ├── mips-linux-cc -> toolchain-wrapper
│   ├── mips-linux-cc.br_real -> mips-buildroot-linux-gnu-cc.br_real
│   ├── mips-linux-c++filt -> mips-buildroot-linux-gnu-c++filt
│   ├── mips-linux-cpp -> toolchain-wrapper
│   ├── mips-linux-cpp.br_real -> mips-buildroot-linux-gnu-cpp.br_real
│   ├── mips-linux-elfedit -> mips-buildroot-linux-gnu-elfedit
│   ├── mips-linux-g++ -> toolchain-wrapper
│   ├── mips-linux-g++.br_real -> mips-buildroot-linux-gnu-g++.br_real
│   ├── mips-linux-gcc -> toolchain-wrapper
│   ├── mips-linux-gcc-4.9.4 -> toolchain-wrapper
│   └── toolchain-wrapper

├── lib
│   ├── gcc
│   │   └── mips-buildroot-linux-gnu
│   │       └── 4.9.4
│   │           ├── crtbegin.o
│   │           ├── crtbeginS.o
│   │           ├── crtbeginT.o
│   │           ├── crtend.o
│   │           ├── crtendS.o
│   │           ├── crtfastmath.o
│   │           ├── include
│   │           │   ├── float.h
│   │           │   ├── iso646.h
│   │           │   ├── loongson.h
│   │           │   ├── stdalign.h
│   │           │   ├── stdarg.h
│   │           ├── install-tools
│   │           │   ├── fixinc_list
│   │           │   ├── gsyslimits.h
│   │           │   ├── include
│   │           │   │   ├── limits.h
│   │           │   │   └── README
│   │           │   ├── macro_list
│   │           │   └── mkheaders.conf
│   │           ├── libgcc.a
│   │           ├── libgcc_eh.a
│   │           ├── libgcov.a
│   │           └── plugin
│   │               ├── gtype.state
│   │               └── include
│   │                   ├── ada
│   │                   │   └── gcc-interface
│   │                   │       └── ada-tree.def
│   │                   ├── alias.h
│   ├── ldscripts
│   ├── libfakeroot-0.so
│   ├── libfakeroot.la
│   ├── libfakeroot.so -> libfakeroot-0.so
│   ├── libgmp.la
│   ├── libgmp.so -> libgmp.so.10.3.1
│   ├── libgmp.so.10 -> libgmp.so.10.3.1
│   ├── libgmp.so.10.3.1
│   ├── libmpc.la
│   ├── libmpc.so -> libmpc.so.3.0.0
│   ├── libmpc.so.3 -> libmpc.so.3.0.0
│   ├── libmpc.so.3.0.0
│   ├── libmpfr.la
│   ├── libmpfr.so -> libmpfr.so.4.1.4
│   ├── libmpfr.so.4 -> libmpfr.so.4.1.4
│   └── libmpfr.so.4.1.4


├── libexec
│   ├── awk
│   │   ├── grcat
│   │   └── pwcat
│   └── gcc
│       └── mips-buildroot-linux-gnu
│           └── 4.9.4
│               ├── cc1
│               ├── cc1plus
│               ├── collect2
│               ├── install-tools
│               │   ├── fixincl
│               │   ├── fixinc.sh
│               │   ├── mkheaders
│               │   └── mkinstalldirs
│               ├── liblto_plugin.la
│               ├── liblto_plugin.so -> liblto_plugin.so.0.0.0
│               ├── liblto_plugin.so.0 -> liblto_plugin.so.0.0.0
│               ├── liblto_plugin.so.0.0.0
│               ├── lto1
│               ├── lto-wrapper
│               └── plugin
│                   └── gengtype

├── mips-buildroot-linux-gnu
│   ├── bin
│   │   ├── ar
│   │   ├── as
│   │   ├── ld
│   │   ├── ld.bfd
│   │   ├── nm
│   │   ├── objcopy
│   │   ├── objdump
│   │   ├── ranlib
│   │   └── strip
│   ├── include
│   │   └── c++

│   ├── lib
│   │   ├── libatomic.a
│   │   ├── libatomic.la
│   │   ├── libatomic.so -> libatomic.so.1.1.0
│   │   ├── libatomic.so.1 -> libatomic.so.1.1.0
│   │   ├── libatomic.so.1.1.0
│   │   ├── libgcc_s.so
│   │   ├── libgcc_s.so.1
│   │   ├── libstdc++.a
│   │   ├── libstdc++.la
│   │   ├── libstdc++.so -> libstdc++.so.6.0.20
│   │   ├── libstdc++.so.6 -> libstdc++.so.6.0.20
│   │   ├── libstdc++.so.6.0.20
│   │   ├── libstdc++.so.6.0.20-gdb.py
│   │   ├── libsupc++.a
│   │   └── libsupc++.la
│   └── sysroot
│       ├── bin
│       ├── etc
│       │   └── rpc
│       ├── lib
│       │   ├── ld-2.23.so
│       │   ├── ld.so.1 -> ld-2.23.so
│       │   ├── libanl-2.23.so
│       │   ├── libanl.so.1 -> libanl-2.23.so
│       │   ├── libatomic.a
│       │   ├── libatomic.la
│       │   ├── libatomic.so -> libatomic.so.1.1.0
│       │   ├── libatomic.so.1 -> libatomic.so.1.1.0
│       │   ├── libatomic.so.1.1.0
│       │   ├── libBrokenLocale-2.23.so
│       │   ├── libBrokenLocale.so.1 -> libBrokenLocale-2.23.so
│       │   ├── libc-2.23.so

│       ├── lib32 -> lib
│       ├── sbin
│       │   ├── ldconfig
│       │   └── sln
│       ├── usr
│       │   ├── bin
│       │   │   ├── catchsegv
│       │   │   ├── gencat
│       │   │   ├── getconf
│       │   │   ├── getent
│       │   │   ├── iconv
│       │   │   ├── ldd
│       │   │   ├── locale
│       │   │   ├── localedef
│       │   │   ├── makedb
│       │   │   ├── mtrace
│       │   │   ├── pcprofiledump
│       │   │   ├── pldd
│       │   │   ├── rpcgen
│       │   │   ├── sotruss
│       │   │   ├── sprof
│       │   │   ├── tzselect
│       │   │   └── xtrace
│       │   ├── include

={============================================================================
*kt_linux_gcc_400* gcc-build-uclibc-target-tree

# from-mips-glibc-without-sysroot

kyoupark@ukstbuild2:~/asn/install-4.8.2-mips-glibc/mips-linux/lib$ ls
audit           libasan_preinit.o        libcidn-2.20.so   libgcc_s.so       libmudflap.a           libnss_compat.so       libnss_nisplus-2.20.so  librpcsvc.a          libstdc++.so
crt1.o          libasan.so               libcidn.so        libgcc_s.so.1     libmudflap.la          libnss_compat.so.2     libnss_nisplus.so       librt-2.20.so        libstdc++.so.6
crti.o          libasan.so.0             libcidn.so.1      libgomp.a         libmudflap.so          libnss_db-2.20.so      libnss_nisplus.so.2     librt.a              libstdc++.so.6.0.18
crtn.o          libasan.so.0.0.0         libc_nonshared.a  libgomp.la        libmudflap.so.0        libnss_db.so           libnss_nis.so           librt.so             libstdc++.so.6.0.18-gdb.py
gconv           libatomic.a              libcrypt-2.20.so  libgomp.so        libmudflap.so.0.0.0    libnss_db.so.2         libnss_nis.so.2         librt.so.1           libsupc++.a
gcrt1.o         libatomic.la             libcrypt.a        libgomp.so.1      libmudflapth.a         libnss_dns-2.20.so     libpcprofile.so         libSegFault.so       libsupc++.la
ld-2.20.so      libatomic.so             libcrypt.so       libgomp.so.1.0.0  libmudflapth.la        libnss_dns.so          libpthread-2.20.so      libssp.a             libthread_db-1.0.so
ldscripts       libatomic.so.1           libcrypt.so.1     libgomp.spec      libmudflapth.so        libnss_dns.so.2        libpthread.a            libssp.la            libthread_db.so
ld.so.1         libatomic.so.1.0.0       libc.so           libieee.a         libmudflapth.so.0      libnss_files-2.20.so   libpthread_nonshared.a  libssp_nonshared.a   libthread_db.so.1
libanl-2.20.so  libBrokenLocale-2.20.so  libc.so.6         libm-2.20.so      libmudflapth.so.0.0.0  libnss_files.so        libpthread.so           libssp_nonshared.la  libutil-2.20.so
libanl.a        libBrokenLocale.a        libdl-2.20.so     libm.a            libnsl-2.20.so         libnss_files.so.2      libpthread.so.0         libssp.so            libutil.a
libanl.so       libBrokenLocale.so       libdl.a           libmcheck.a       libnsl.a               libnss_hesiod-2.20.so  libresolv-2.20.so       libssp.so.0          libutil.so
libanl.so.1     libBrokenLocale.so.1     libdl.so          libmemusage.so    libnsl.so              libnss_hesiod.so       libresolv.a             libssp.so.0.0.0      libutil.so.1
libasan.a       libc-2.20.so             libdl.so.2        libm.so           libnsl.so.1            libnss_hesiod.so.2     libresolv.so            libstdc++.a          Mcrt1.o
libasan.la      libc.a                   libg.a            libm.so.6         libnss_compat-2.20.so  libnss_nis-2.20.so     libresolv.so.2          libstdc++.la         Scrt1.o


# from-mips-uclibc-with-sysroot

kyoupark@ukstbuild2:~/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/lib$ ls
ldscripts          libasan.so        libatomic.la        libgcc_s.so    libgomp.so        libssp.a             libssp.so        libstdc++.la         libstdc++.so.6.0.18-gdb.py
libasan.a          libasan.so.0      libatomic.so        libgcc_s.so.1  libgomp.so.1      libssp.la            libssp.so.0      libstdc++.so         libsupc++.a
libasan.la         libasan.so.0.0.0  libatomic.so.1      libgomp.a      libgomp.so.1.0.0  libssp_nonshared.a   libssp.so.0.0.0  libstdc++.so.6       libsupc++.la
libasan_preinit.o  libatomic.a       libatomic.so.1.0.0  libgomp.la     libgomp.spec      libssp_nonshared.la  libstdc++.a      libstdc++.so.6.0.18

kyoupark@ukstbuild2:~/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/sysroot/lib$ ls
ld-uClibc-1.0.17.so  libcrypt.so.0  libdl-1.0.17.so  libm.so.0         libnsl.so.1           libresolv-1.0.17.so  librt.so.0              libuClibc-1.0.17.so
ld-uClibc.so.0       libcrypt.so.1  libdl.so.0       libm.so.1         libpthread-1.0.17.so  libresolv.so.0       librt.so.1              libutil-1.0.17.so
ld-uClibc.so.1       libc.so.0      libdl.so.1       libnsl-1.0.17.so  libpthread.so.0       libresolv.so.1       libthread_db-1.0.17.so  libutil.so.0
libcrypt-1.0.17.so   libc.so.1      libm-1.0.17.so   libnsl.so.0       libpthread.so.1       librt-1.0.17.so      libthread_db.so.1       libutil.so.1

kyoupark@ukstbuild2:~/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/sysroot/usr/lib$ ls
crt1.o  libc_pic.a      libc.so      libm.a      libnsl_pic.a            libpthread_nonshared_pic.a  libresolv_pic.a  librt.so            libutil.a      uclibc_nonshared.a
crti.o  libcrypt.a      libdl.a      libm_pic.a  libnsl.so               libpthread_pic.a            libresolv.so     libthread_db.a      libutil_pic.a
crtn.o  libcrypt_pic.a  libdl_pic.a  libm.so     libpthread.a            libpthread.so               librt.a          libthread_db_pic.a  libutil.so
libc.a  libcrypt.so     libdl.so     libnsl.a    libpthread_nonshared.a  libresolv.a                 librt_pic.a      libthread_db.so     Scrt1.o


={============================================================================
*kt_linux_gcc_400* gcc-toolchain-cross-ng-mips-uclibc

// not on the build server since some tools are missing which requires root
// permission. should do on private VM or build server.

http://crosstool-ng.org/
wget http://crosstool-ng.org/download/crosstool-ng/crosstool-ng-1.23.0.tar.xz

// build, install ct-ng and add it to path
tar xjf crosstool-ng-VERSION.tar.bz2
cd crosstool-ng-VERSION
./configure --prefix=/home/kyoupark/asan/cross/install/bin

sudo apt-get install libtool-bin
sudo apt-get install libncurses-dev

make
make install
export PATH="${PATH}:/home/kyoupark/asan/cross/help2man-1.47.4:`/home/kyoupark/asan/cross/install/bin/bin`"

Then, you are ready to use crosstool-NG.

`create a place to work in`, then list the existing samples (pre-configured
    toolchains that are known to build and work) to see if one can fit your
actual needs. Sample names are `4-part tuples`, such as
arm-unknown-linux-gnueabi. 

In the following, we'll use that as a sample name; adapt to your needs:

mkdir /a/directory/to/build/your/toolchain
cd /a/directory/to/build/your/toolchain
ct-ng help
ct-ng list-samples

29:[G..]   i686-centos6-linux-gnu
30:[G..]   i686-centos7-linux-gnu
31:[G..]   i686-nptl-linux-gnu
32:[G..]   i686-ubuntu12.04-linux-gnu
33:[G..]   i686-ubuntu14.04-linux-gnu
34:[G..]   i686-ubuntu16.04-linux-gnu
35:[G.X]   i686-w64-mingw32
47:[G.X]   i686-w64-mingw32,nios2-spico-elf

ct-ng show-i686-nptl-linux-gnu
kyoupark@kit-debian64:~/asan/cross-build$ ct-ng show-i686-nptl-linux-gnu
[G..]   i686-nptl-linux-gnu
    OS             : linux-4.10.8
    Companion libs : gmp-6.1.2 mpfr-3.1.5 isl-0.16.1 mpc-1.0.3 libelf-0.8.13 expat-2.2.0 ncurses-6.0
    binutils       : binutils-2.28
    C compilers    : gcc  |  6.3.0
    Languages      : C,C++
    C library      : glibc-2.25 (threads: nptl)
    Tools          : duma-2_5_15 gdb-7.12.1 ltrace-0.7.3 strace-4.16

once you know what sample to use, configure ct-ng to use it and get .config
file to use:

note: this will overwrite .config from the specificed sample config. 
$ ct-ng i686-nptl-linux-gnu                                                                                                                                                                          
  CONF  config/config.in
#
# configuration written to .config
#

***********************************************************

Initially reported by: YEM
URL: http://ymorin.is-a-geek.org/

***********************************************************

Now configured for "i686-nptl-linux-gnu"

If no sample really fits your needs:

choose the one closest to what you want (see above), and start building it
(see above, too) this ensures sure it is working for your machine, before
trying to do more advanced tests fine-tune the configuration, and re-run the
build, with:

ct-ng menuconfig
ct-ng build


// has to set --no-check-certificate
// mips-ar2315-linux-gnu 			linux 	4.3 	2.25.1 	gcc 	5.2.0 	glibc 	2.22 	nptl 	soft 	C, C++ 	Giammarco Zacheo 	2015-11-14 
ct-ng mips-ar2315-linux-gnu

ct-ng mipsel-unknown-linux-gnu
ct-ng V=2 build


samples are configured to install in
"${HOME}/x-tools/arm-unknown-linux-gnueabi" by default. This should be OK for
a first time user, so you can now build your toolchain:

ct-ng build

finally, you can set access to your toolchain, and call your new
cross-compiler with :

export PATH="${PATH}:${HOME}/x-tools/arm-unknown-linux-gnueabi/bin"
arm-unknown-linux-gnueabi-gcc

Note 2: If you call ct-ng --help you will get help for make(2). This is
because ct-ng is in fact a make(2) script. There is no clean workaround for
this. 

// to build from existing config and build.log
Re-building an existing toolchain

${CT_TARGET}-ct-ng.config >.config
ct-ng oldconfig


<to-set-custom-string>
Mark the toolchain as a VSTB one:
    CT_TOOLCHAIN_PKGVERSION="vstb"
    CT_TOOLCHAIN_BUGURL="http://vstb.cisco.com"

$ ./i686-nptl-linux-gnu-gcc --version
i686-nptl-linux-gnu-gcc (crosstool-NG crosstool-ng-1.23.0 - vstb) `4.9.4`
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


<ct-ng-issue>
when use custom kernel, see build errors:

[INFO ]  Installing kernel headers
[EXTRA]    Installing kernel headers
[ERROR]    make[1]: *** No rule to make target 'headers_install'.  Stop.


CT_KERNEL_LINUX_CUSTOM_LOCATION="/home/kyoupark/asan/cross-build-for-host/"
CT_KERNEL_LINUX_CUSTOM_LOCATION="${PWD}/linux-${CT_KERNEL_VERSION}.tar.xz"

the issue is that ct-ng use this value to make a sym link to kernel source
under .build dir. so check if that link is right.


<ct-prefix>
// install under {CMAKE_BUILD}
CT_PREFIX_DIR="${PWD}/x-tools/${CT_TARGET}"

// install to ~/x-tools
CT_PREFIX_DIR="${CT_PREFIX:-${HOME}/x-tools}/${CT_HOST:+HOST-${CT_HOST}/}${CT_TARGET}"


<steps>
kyoupark@kit-debian:~/cross/work$ ct-ng list-steps
Available build steps, in order:
  - libc_check_config
  - companion_libs_for_build
  - binutils_for_build
  - companion_libs_for_host
  - binutils_for_host
  - cc_core_pass_1
  - kernel_headers
  - libc_start_files
  - cc_core_pass_2
  - libc
  - cc_for_build
  - cc_for_host
  - libc_post_cc
  - companion_libs_for_target
  - binutils_for_target
  - debug
  - test_suite
  - finish


Environment variables (see /home/kyoupark/cross/install/share/doc/crosstool-ng/crosstool-ng-1.22.0/0 - Table of content.txt):
  STOP=step          - Stop the build just after this step (list with list-steps)
  RESTART=step       - Restart the build just before this step (list with list-steps)

export PATH=/home/kyoupark/cross/install/bin:${PATH}
export PATH=/home/kyoupark/asan/cross/install/bin/bin:${PATH}


STOP=binutils_for_host ct-ng build
STOP=cc_core_pass_1 ct-ng build
STOP=libc_start_files V=2 ct-ng build
STOP=cc_core_pass_2 ct-ng build

// RESTART=cc_core_pass_2 ct-ng build
// 
// [ERROR]  You asked to restart a non-restartable build
// [ERROR]  This happened because you didn't set CT_DEBUG_CT_SAVE_STEPS
// [ERROR]  in the config options for the previous build, or the state
// [ERROR]  directory for the previous build was deleted.
// [ERROR]  I will stop here to avoid any carnage


<build-log>
~/cross/work$ ct-ng V=2 build

note:see build.log

/home/kyoupark/cross/install/lib/crosstool-ng-1.22.0/scripts/crosstool-NG.sh
[INFO ]  Performing some trivial sanity checks
[INFO ]  Build started 20161129.124349
[INFO ]  Building environment variables
[WARN ]  Directory '/home/kyoupark/src' does not exist.
[WARN ]  Will not save downloaded tarballs to local storage.
[EXTRA]  Preparing working directories
[EXTRA]  Installing user-supplied crosstool-NG configuration
[EXTRA]  =================================================================
[EXTRA]  Dumping internal crosstool-NG configuration
[EXTRA]    Building a toolchain for:
[EXTRA]      build  = i686-pc-linux-gnu
[EXTRA]      host   = i686-pc-linux-gnu
[EXTRA]      target = mips-unknown-linux-uclibc
[EXTRA]  Dumping internal crosstool-NG configuration: done in 0.26s (at 00:02)
[INFO ]  =================================================================
[INFO ]  Retrieving needed toolchain components' tarballs
[INFO ]  Retrieving needed toolchain components' tarballs: done in 0.12s (at 00:02)

[INFO ]  =================================================================
[INFO ]  Extracting and patching toolchain components
[EXTRA]    Extracting 'uClibc-ng-1.0.9'
[INFO ]  Extracting and patching toolchain components: done in 0.12s (at 00:03)

[INFO ]  =================================================================
[INFO ]  Checking C library configuration
[EXTRA]    Manage uClibc configuration
[INFO ]  Checking C library configuration: done in 0.17s (at 00:03)
[INFO ]  =================================================================
[INFO ]  Installing binutils for host
[EXTRA]    Configuring binutils
[EXTRA]    Building binutils
[EXTRA]    Installing binutils

[DEBUG]    ==> Executing: 'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc'
  'CFLAGS_FOR_BUILD=' 'CFLAGS=-O2 -g -pipe
    -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include
    ' 'CXXFLAGS=-O2 -g -pipe
    -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include
    '
    'LDFLAGS=-L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/lib
    ' '/bin/bash'
    '/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/binutils-2.28/configure'
    '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
    '--target=i686-nptl-linux-gnu'
    '--prefix=/home/kyoupark/x-tools/i686-nptl-linux-gnu' '--disable-werror'
    '--enable-ld=default' '--enable-gold=yes' '--enable-threads'
    '--enable-plugins' '--with-pkgversion=crosstool-NG crosstool-ng-1.23.0 -
    for-host-32' '--disable-multilib' '--disable-sim' '--disable-gdb'
      '--disable-nls'
        '--with-sysroot=/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sysroot' 

[ALL  ]    libtool: install:
/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/tools/bin/install
  -c size
  /home/kyoupark/x-tools/i686-nptl-linux-gnu/bin/./i686-nptl-linux-gnu-size

[ALL  ]      /bin/bash ./libtool   --mode=install
  /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/tools/bin/install
    -c objdump
    '/home/kyoupark/x-tools/i686-nptl-linux-gnu/bin/./i686-nptl-linux-gnu-objdump'

// 
[ALL  ]      /bin/bash ./libtool   --mode=install
/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/tools/bin/install
  -c size
  '/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/x-tools/i686-nptl-linux-gnu/bin/./i686-nptl-linux-gnu-size'

[INFO ]  Installing binutils for host: done in 175.28s (at 08:22)
[INFO ]  =================================================================
[INFO ]  Installing pass-1 core C gcc compiler
[EXTRA]    Configuring core C gcc compiler
[EXTRA]    Building gcc
[EXTRA]    Installing gcc
[EXTRA]    Housekeeping for final gcc compiler
[INFO ]  Installing pass-1 core C gcc compiler: done in 1045.75s (at 25:47)
[INFO ]  =================================================================
[INFO ]  Installing kernel headers
[EXTRA]    Installing kernel headers
[EXTRA]    Checking installed headers
[INFO ]  Installing kernel headers: done in 10.09s (at 25:57)
[INFO ]  =================================================================
[INFO ]  Installing C library headers
[EXTRA]    Copying sources to build dir
[EXTRA]    Applying configuration
[EXTRA]    Building headers
[EXTRA]    Installing headers
[EXTRA]    Building start files
[EXTRA]    Building dummy shared libs
[EXTRA]    Installing start files
[EXTRA]    Installing dummy shared libs
[INFO ]  Installing C library headers: done in 13.40s (at 26:11)
[INFO ]  =================================================================
[INFO ]  Installing pass-2 core C gcc compiler
[EXTRA]    Configuring core C gcc compiler
[EXTRA]    Building gcc
[EXTRA]    Installing gcc
[EXTRA]    Housekeeping for final gcc compiler
[INFO ]  Installing pass-2 core C gcc compiler: done in 1338.26s (at 48:29)
[INFO ]  =================================================================
[INFO ]  Installing C library
[EXTRA]    Copying sources to build dir
[EXTRA]    Applying configuration
[EXTRA]    Building C library
[EXTRA]    Installing C library
[INFO ]  Installing C library: done in 126.24s (at 50:35)
[INFO ]  =================================================================
[INFO ]  Installing final gcc compiler
[EXTRA]    Configuring final gcc compiler

[DEBUG]    Entering '/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final'
[EXTRA]    Configuring final gcc compiler
[DEBUG]    Extra config passed: '--enable-languages=c,c++ --with-arch=i686 --with-pkgversion=crosstool-NG crosstool-ng-1.23.0 - for-host-32 --enable-__cxa_atexit --disable-libmudflap --disable-libgomp --disable-libssp --disable-libquadmath --disable-libquadmath-support --enable-libsanitizer --enable-libmpx --with-gmp=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools --with-mpfr=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools --with-mpc=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools --with-isl=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools --enable-lto --with-host-libstdcxx=-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm --enable-threads=posix --enable-target-optspace --enable-plugin --enable-gold --disable-nls --disable-multilib'

[DEBUG]    ==> Executing: 'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc'
  'CFLAGS=-O2 -g -pipe
    -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include
    ' 'CFLAGS_FOR_BUILD=-O2 -g -pipe
    -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include
    ' 'CXXFLAGS=-O2 -g -pipe
    -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include
    ' 'CXXFLAGS_FOR_BUILD=-O2 -g -pipe
    -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include
    '
    'LDFLAGS=-L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/lib
    -lstdc++ -lm' 'CFLAGS_FOR_TARGET=    -march=i686      '
    'CXXFLAGS_FOR_TARGET=    -march=i686      ' 'LDFLAGS_FOR_TARGET=    '
    '/bin/bash'
    '/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/configure'
    '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
    '--target=i686-nptl-linux-gnu'
    '--prefix=/home/kyoupark/x-tools/i686-nptl-linux-gnu'
    '--with-sysroot=/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sysroot'
    '--enable-languages=c,c++' '--with-arch=i686'
    '--with-pkgversion=crosstool-NG crosstool-ng-1.23.0 - for-host-32'
    '--enable-__cxa_atexit' '--disable-libmudflap' '--disable-libgomp'
    '--disable-libssp' '--disable-libquadmath' '--disable-libquadmath-support'
    '--enable-libsanitizer' '--enable-libmpx'
    '--with-gmp=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools'
    '--with-mpfr=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools'
    '--with-mpc=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools'
    '--with-isl=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools'
    '--enable-lto' '--with-host-libstdcxx=-static-libgcc
    -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' '--enable-threads=posix'
    '--enable-target-optspace' '--enable-plugin' '--enable-gold'
    '--disable-nls' '--disable-multilib'
    '--with-local-prefix=/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sysroot'
    '--enable-long-long' 

'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc' 'CFLAGS=-O2 -g -pipe -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include  ' 'CFLAGS_FOR_BUILD=-O2 -g -pipe -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include  ' 'CXXFLAGS=-O2 -g -pipe -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include  ' 'CXXFLAGS_FOR_BUILD=-O2 -g -pipe -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include  ' 'LDFLAGS=-L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/lib   -lstdc++ -lm' 'CFLAGS_FOR_TARGET=    -march=i686      ' 'CXXFLAGS_FOR_TARGET=    -march=i686      ' 'LDFLAGS_FOR_TARGET=    ' '/bin/bash' '/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/configure' '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu' '--target=i686-nptl-linux-gnu' '--prefix=/home/kyoupark/x-tools/i686-nptl-linux-gnu' '--with-sysroot=/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sysroot' '--enable-languages=c,c++' '--with-arch=i686' '--with-pkgversion=crosstool-NG crosstool-ng-1.23.0 - for-host-32' '--enable-__cxa_atexit' '--disable-libmudflap' '--disable-libgomp' '--disable-libssp' '--disable-libquadmath' '--disable-libquadmath-support' '--enable-libsanitizer' '--enable-libmpx' '--with-gmp=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools' '--with-mpfr=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools' '--with-mpc=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools' '--with-isl=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools' '--enable-lto' '--with-host-libstdcxx=-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' '--enable-threads=posix' '--enable-target-optspace' '--enable-plugin' '--enable-gold' '--disable-nls' '--disable-multilib' '--with-local-prefix=/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sysroot' '--enable-long-long' 

// CC and CXX uses host version
[DEBUG]  ==> Executing: 'chmod' '700' '/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/bin/i686-build_pc-linux-gnu-as' 
[DEBUG]    Missing: 'i686-pc-linux-gnu-dlltool' or 'i686-pc-linux-gnu-dlltool' or 'dlltool' : not required.
[DEBUG]    'i686-build_pc-linux-gnu-gcc' -> '/usr/bin/gcc'
[DEBUG]  ==> Executing: 'chmod' '700' '/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/bin/i686-build_pc-linux-gnu-gcc' 
[DEBUG]    'i686-build_pc-linux-gnu-g++' -> '/usr/bin/g++'

// to delete old configs
find . -name config.cache -exec rm -f {} \;

// to use bintuils
export PATH="/home/kyoupark/x-tools/i686-nptl-linux-gnu/bin:$PATH"

// to move build dir
// source dir is /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src
cd '/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final'

// to change build flags (for host)
CC_FOR_BUILD=/usr/bin/gcc \
CFLAGS="-Og -g3 -pipe -I/mnt/bcb7779e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include" \
CFLAGS_FOR_BUILD="-Og -g3 -pipe -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include" \
CXXFLAGS="-Og -g3 -pipe -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include" \
CXXFLAGS_FOR_BUILD="-Og -g3 -pipe -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/include" \
LDFLAGS="-L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools/lib   -lstdc++ -lm" \
CFLAGS_FOR_TARGET="    -march=i686" \
CXXFLAGS_FOR_TARGET="    -march=i686" \
LDFLAGS_FOR_TARGET= \
/bin/bash /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/configure --build=i686-build_pc-linux-gnu --host=i686-build_pc-linux-gnu --target=i686-nptl-linux-gnu --prefix=/home/kyoupark/x-tools/i686-nptl-linux-gnu --with-sysroot=/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sysroot --enable-languages=c,c++ --with-arch=i686 \
--with-pkgversion=crosstool-NG crosstool-ng-1.23.0-for-host-32 --enable-__cxa_atexit --disable-libmudflap --disable-libgomp --disable-libssp --disable-libquadmath --disable-libquadmath-support --enable-libsanitizer --enable-libmpx --with-gmp=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools --with-mpfr=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools --with-mpc=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools --with-isl=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/buildtools --enable-lto \
--with-host-libstdcxx="-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm" --enable-threads=posix --enable-target-optspace --enable-plugin --enable-gold --disable-nls --disable-multilib --with-local-prefix=/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sysroot --enable-long-long 


<for-vstb-630>
// to use bintuils
export PATH="/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/x-tools/i686-nptl-linux-gnu/bin:$PATH"
            '/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/x-tools/i686-nptl-linux-gnu/bin/./i686-nptl-linux-gnu-size'

// /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/src/gcc-6.3.0
cd /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final

// to change build flags (for vstb 630)

[DEBUG]    ==> Executing: 

CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc \
CFLAGS="-Og -g3 -pipe -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/i686-nptl-linux-gnu/buildtools/include" \
CFLAGS_FOR_BUILD="-Og -g3 -pipe -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/i686-nptl-linux-gnu/buildtools/include" \
CXXFLAGS="-Og -g3 -pipe -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/i686-nptl-linux-gnu/buildtools/include" \
CXXFLAGS_FOR_BUILD="-O3 -g -pipe -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/i686-nptl-linux-gnu/buildtools/include" \
LDFLAGS="-L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/i686-nptl-linux-gnu/buildtools/lib -static -lstdc++ -lm" \ 
CFLAGS_FOR_TARGET='    -march=i686' \
CXXFLAGS_FOR_TARGET='    -march=i686' \
LDFLAGS_FOR_TARGET= \
/bin/bash /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/src/gcc-6.3.0/configure --build=i686-build_pc-linux-gnu --host=i686-build_pc-linux-gnu --target=i686-nptl-linux-gnu --prefix=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/x-tools/i686-nptl-linux-gnu --with-sysroot=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sysroot --enable-languages=c,c++ --with-arch=i686 \
--with-pkgversion='crosstool-NG crosstool-ng-1.23.0 - vstb' --with-bugurl=http://vstb.cisco.com --disable-sjlj-exceptions \
--enable-__cxa_atexit --enable-libmudflap --enable-libgomp --disable-libssp --disable-libquadmath --disable-libquadmath-support \
--enable-libsanitizer --enable-libmpx \
--with-gmp=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/i686-nptl-linux-gnu/buildtools \
--with-mpfr=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/i686-nptl-linux-gnu/buildtools \
--with-mpc=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/i686-nptl-linux-gnu/buildtools \
--with-isl=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/.build/i686-nptl-linux-gnu/buildtools \
--disable-lto --with-host-libstdcxx='-static-libgcc -Wl,-Bstatic,-lstdc++ -lm' \
--enable-threads=posix --without-long-double-128 --enable-linker-build-id \
--with-linker-hash-style=gnu --disable-plugin --disable-nls --enable-tls \
--disable-multilib --with-local-prefix=/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-vstb-630/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sysroot --enable-long-long 


[EXTRA]    Building final gcc compiler
[DEBUG]    ==> Executing: 'make' '-j3' '-l' 'all' 

[EXTRA]    Installing final gcc compiler
[DEBUG]    ==> Executing: 'make' 'install' 

[EXTRA]    Housekeeping for final gcc compiler
[INFO ]  Installing final gcc compiler: done in 1458.59s (at 74:54)
[INFO ]  =================================================================

// where to copy
+COMPILER_ROOT := /home/kyoupark/asan/i686-nptl-linux-gnu-vstb-630


<compile-single-file> 

[ALL  ]    make[5]: Entering directory '/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libsanitizer/asan'

cd '/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libsanitizer/asan'
// xgcc is gcc binary

// to get preprocessed output using *cpp-e*
[ALL  ]    libtool: compile:
/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/./gcc/xgcc
  -shared-libgcc
  -B/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/./gcc
  -nostdinc++
  -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/src
  -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/src/.libs
  -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/libsupc++/.libs
  -B/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/bin/
  -B/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/lib/
  -isystem
  /home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/include
  -isystem
  /home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sys-include
  -D_GNU_SOURCE -D_DEBUG -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS
  -D__STDC_LIMIT_MACROS -DASAN_HAS_EXCEPTIONS=1 -DASAN_NEEDS_SEGV=1
  -DCAN_SANITIZE_UB=0 -D_OBSTACK_SIZE_T=SIZE_T -I.
  -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/asan
  -I.. -I
  /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/include
  -I
  /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer
  -Wall -W -Wno-unused-parameter -Wwrite-strings -pedantic -Wno-long-long
  -fPIC -fno-builtin -fno-exceptions -fno-rtti -fomit-frame-pointer
  -funwind-tables -fvisibility=hidden -Wno-variadic-macros -fno-ipa-icf
  -I../../libstdc++-v3/include
  -I../../libstdc++-v3/include/i686-nptl-linux-gnu
  -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/../libstdc++-v3/libsupc++
  -std=gnu++11 -march=i686 -D_GNU_SOURCE -g -Os -MT asan_rtl.lo -MD -MP -MF
  .deps/asan_rtl.Tpo -c

  `/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/asan/asan_rtl.cc`

  -o asan_rtl.o >/dev/null 2>&1

/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/./gcc/xgcc -shared-libgcc -B/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/./gcc -nostdinc++ -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/src -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/src/.libs -L/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/i686-nptl-linux-gnu/build/build-cc-gcc-final/i686-nptl-linux-gnu/libstdc++-v3/libsupc++/.libs -B/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/bin/ -B/home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/lib/ -isystem /home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/include -isystem /home/kyoupark/x-tools/i686-nptl-linux-gnu/i686-nptl-linux-gnu/sys-include -D_GNU_SOURCE -D_DEBUG -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -DASAN_HAS_EXCEPTIONS=1 -DASAN_NEEDS_SEGV=1 -DCAN_SANITIZE_UB=0 -D_OBSTACK_SIZE_T=SIZE_T -I. -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/asan -I.. -I /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/include -I /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer -Wall -W -Wno-unused-parameter -Wwrite-strings -pedantic -Wno-long-long -fPIC -fno-builtin -fno-exceptions -fno-rtti -fomit-frame-pointer -funwind-tables -fvisibility=hidden -Wno-variadic-macros -fno-ipa-icf -I../../libstdc++-v3/include -I../../libstdc++-v3/include/i686-nptl-linux-gnu -I/mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/../libstdc++-v3/libsupc++ -std=gnu++11 -march=i686 -D_GNU_SOURCE -g -Os -MT asan_rtl.lo -MD -MP -MF .deps/asan_rtl.Tpo -E /mnt/bcb7778e-e87e-4609-857b-cada2826a487/cross-build-for-host/.build/src/gcc-6.3.0/libsanitizer/asan/asan_rtl.cc > asan_rtl.cpp


={============================================================================
*kt_linux_arch_001* arch-endian

representation

this is how {.byte 11} in MIPS is stores in memory:

high                                low
0000 0000 0000 0000 0000 0000 0000 1011

big endian(e.g. intel)  little(e.g. spacc)
0000 0000               0000 1011         low
0000 0000               0000 0000
0000 0000               0000 0000
0000 1011               0000 0000         high

<check-runtime>
uint8_t ebuf[4];
uint32_t *ebuf32 = (uint32_t *)ebuf;

ebuf32[0] = 0x01000000;
if(ebuf[0] == 0x01) system_endian = TBIG_ENDIAN;
else system_endian = TLITTLE_ENDIAN;


={============================================================================
*kt_linux_arch_001* arch-mips-registers mips-instruction

http://msdn.microsoft.com/en-us/library/aa448706.aspx 

Register Name   Common Name   Description

$2-$3           v0-v1         Function result registers.
                              Not preserved across function calls.

return $v0, $v1 

The return value is placed in registers $v0, and if necessary, in $v1. In
general, this makes calling functions a little easier. In particular, the
calling function usually does not need to place anything on the stack for the
function being called. 

However, this is clearly not a panacea. In particular, imagine main() calls
foo().  Arguments are passed using $a0 and $a1, say. What happens when foo()
calls bar()? If foo() has to pass arguments too, then by convention, it's
supposed to pass them using $a0 and $a1, etc. What if foo() needs the argument
values from main() afterwards? To prevent its own arguments from getting
overwritten, foo() needs to save the arguments to the stack.  Thus, we don't
entirely avoid using the stack. 


$4-$7           a0-a3         Function argument registers.
                              Not preserved across function calls.
args $a0, $a1, $a2, $a3 

There are four registers used to pass arguments: $a0, $a1, $a2, $a3. 

If a function has more than four arguments, or if any of the arguments is a
large structure that's passed by value, `then the stack is used.`

There must be a set procedure for passing arguments that's known to everyone
based on the types of the functions. That way, the caller of the function
knows how to pass the arguments, and the function being called knows how to
access them.  Clearly, if this protocol is not established and followed, the
function being called would not get its arguments properly, and would likely
compute bogus values or, worse, crash. 

<mips-ra>
$31	ra	
Return address register, saved by the calling function. Available for use
after saving.

<mips-sa>
$16-$23, $30	s0-s8	
Saved registers to use freely. `Preserved across function calls.` These
registers must be saved before use by the called function.

<mips-gp>
$28	gp	Global pointer.
Not used in Windows CE and may be used as save register for called functions.

(gdb) info registers

          zero       at       v0       v1       a0       a1       a2       a3
 R0   00000000 10008401 2aadf310 00d40b70 2aadc000 00000000 00000001 00000001 
            t0       t1       t2       t3       t4       t5       t6       t7
 R8   00000000 00008400 10008400 831a8000 00000002 00000000 831a9e7a 00000000 
            s0       s1       s2       s3       s4       s5       s6       s7
 R16  00a12870 009cc130 0098c4bc 00000001 00000004 00000008 009cd2fc 009cc23c 
            t8       t9       k0       k1       gp      {sp}      s8      {ra}
 R24  831a9d9a 2aada010 00000000 00000000 2aae5010 009cc078 00860000 2aad2590 
            sr       lo       hi      bad    cause       pc
      00008413 00000000 00000000 00d40b74 0080000c 2aad0cac 
           fsr      fir
      00000000 00000000 

<ex>
Example to show reg changes between frames.

(gdb) bt
#0  0x2aaf02d4 in ?? () from /data/home/NDS-UK/parkkt/fob/gdb/lib/libpthread.so.0
#1  0x00b100c8 in SYSTEMITC_API_PendEvent (qhandle=<value optimized out>, timeout=-1) at systemitc.c:375
#2  0x00520fd0 in SrmLowPriorityMain (data=<value optimized out>) at srm_main.c:1597
#3  0x00b328d0 in SYSTEMUTIL_THR_P_ThreadPrologue (arg=<value optimized out>) at systemutil_thread.c:869
#4  0x2aad2f9c in pthread_join () from /data/home/NDS-UK/parkkt/fob/gdb/lib/libpthread.so.0
Backtrace stopped: previous frame inner to this frame (corrupt stack?)

(gdb) bt
#0  0x2aaf02d4 in ?? () from /data/home/NDS-UK/parkkt/fob/gdb/lib/libpthread.so.0
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00f28b4c 2aaf02c0 00000000 00000000 2ab88540 0139fbb0 00d40000 00b100c8 
            sr       lo       hi      bad    cause       pc
      00008413 00000000 00000000 2b4bae74 00800020 2aaf02d4 
      
#1  0x00b100c8 in SYSTEMITC_API_PendEvent (qhandle=<value optimized out>, timeout=-1) at systemitc.c:375
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00f28b4c 2aaf02c0 00000000 00000000 2ab88540 0139fbb0 00d40000 00b100c8 
            sr       lo       hi      bad    cause       pc
      00008413 00000000 00000000 2b4bae74 00800020 00b100c8 
            
#2  0x00520fd0 in SrmLowPriorityMain (data=<value optimized out>) at srm_main.c:1597
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00f28b4c 2aaf02c0 00000000 00000000 2ab88540 0139fbe8 00d40000 00520fd0 
            sr       lo       hi      bad    cause       pc
      00008413 00000000 00000000 2b4bae74 00800020 00520fd0 

(SP diff: 0139fe38 - 0139fbe8 = 0x250, 592)      

#3  0x00b328d0 in SYSTEMUTIL_THR_P_ThreadPrologue (arg=<value optimized out>) at systemutil_thread.c:869
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00f28b4c 2aaf02c0 00000000 00000000 2ab88540 0139fe38 00f0eacc 00b328d0 
            sr       lo       hi      bad    cause       pc
      00008413 00000000 00000000 2b4bae74 00800020 00b328d0 

(0139feb0 - 0139fe38 = 0x78, 128)
       
#4  0x2aad2f9c in pthread_join () from /data/home/NDS-UK/parkkt/fob/gdb/lib/libpthread.so.0
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00f28b4c 2aaf02c0 00000000 00000000 2ab88540 0139feb0 00f0eacc 2aad2f9c 
            sr       lo       hi      bad    cause       pc
      00008413 00000000 00000000 2b4bae74 00800020 2aad2f9c 

40 registers in total and 40*4 = 160 bytes


<branch-and-jump>
Branches allow for conditions. But allowing for conditions takes up more bits
in the instruction. Therefore, a branch's address is only 2^16 bits and only
allows you to branch 2^15 - 1 instructions backward or 2^15 instructions
forward.

A jump is unconditional and the bits saved by leaving out the condition can be
used for the address. A jump allows for a 26 bit address and so can jump much
further in the code than a branch. At the expense of not being conditional.


<mips-instruction>

I-Type(immediate)

|31      26|25    21|20     16|15              0|
|    op    |   rs   |    rt   |    immediate    |

<arch-alignment>
when a word(4 bytes) is loaded and stored the memory address must be a
multiple of four. this is called an `alignment restriction.` addresses that
are a multiple of four are called `word aligned.` this resctiction makes the
hardware simpler and faster.

The MIPS instruction that loads a word into a register is the lw instruction.
The store word instruction is sw. Each must specify a register and a memory
address. 

  `A MIPS instruction is 32 bits (always).`
  `A MIPS memory address is 32 bits (always).`
  
How can a load or store instruction specify an address where there is not
enough space for instruction and address?

An instruction that refers to memory `uses a base register and an offset.` The
base register is a general purpose register that contains a 32-bit address.
The offset is a 16-bit signed integer contained in the instruction. The sum of
the address in the base register with the (sign-extended) offset forms the
memory address. 

For example:

if you want to load a byte by using the 'lb' instruction.

lb t1, 0x180003fb

it's not working and it simply stores the hexadecimal value into the register.
in this case 0x18000000. Because addresses are always signed 16-bit
displacements relative to a register 

so you'd wite to work

lui t1, 0x1800		
lbu t1, 0x3fb(t1)


http://www.mrc.uidaho.edu/mrc/people/jff/digital/MIPSir.html

LB -- Load byte

Description:
A byte is loaded into a register from the specified address.

Operation:
$t = MEM[$s + offset]; advance_pc (4);

Syntax:
lb $t, offset($s)

Encoding:
1000 00ss ssst tttt iiii iiii iiii iiii


https://web.cse.ohio-state.edu/~crawfis.3/cse675-02/Slides/MIPS%20Instruction%20Set.pdf

General notes:

a. Rs, Rt, and Rd specify general purpose registers

b. Square brackets ([]) indicate “the contents of”

c. [PC] specifies the address of the instruction in execution

d. I specifies part of instruction and its subscripts indicate bit positions
of sub-fields

e. || indicates concatenation of bit fields

f. Superscripts indicate repetition of a binary value

g. M{i} is a value (contents) of the word beginning at the memory address i

h. m{i} is a value (contents) of the byte at the memory address i

i. all integers are in 2’s complement representation if not indicated as
unsigned 


// <mips-lui>
// 27. load upper immediate: lui instruction
//  +--------+-------+--------+-----------------------+
//  | 001111 | 00000 | Rt     | immediate |
//  +--------+-------+--------+-----------------------+
// Effects of the instruction: 
//  Rt <-- [I(sub)15-0)] || 0(super)16; 
//  PC <-- [PC] + 4
// Assembly format: lui Rt,immediate

// LUI -- The immediate value is shifted left 16 bits and stored in the
// register and the lower 16 bits are zeroes due to *cpp-shift* 
// lui $t, imm  // same as $t = (imm << 16)

// <mips-lbu>
// 24. load unsigned byte: lbu instruction
//  +--------+-------+-------+-----------------------+
//  | 100100 | Rs | Rt | offset |
//  +--------+-------+-------+-----------------------+
// Effects of the instruction: Rt<-- 024 || m{[Rs] + [I15]16 || [I15..0]}
//  PC <-- [PC] + 4
//  (If an illegal memory address then exception processing)
// Assembly format: lbu Rt,offset(Rs) 


Similarly, to store t1 into address 0x8009f000, you'd write:

# SW -- Store word. The contents of $t is stored at the specified address. 	
# sw $t, offset($s). MEM[$s + offset] = $t

lui at, 0x800a
sw t1, 0xf000(at) 

# Notice that in this case, the upper part of the address has to be
# incremented by one, since the lower part is negative.  0xf000(at) is
# negative and equates to -0x1000. so 0x800a0000 - 0x1000 = 0x8009f000

If you just want to load or store into an address between 0xffff8000 and
0x7fff, you can skip the lui and use the register which various assemblers
call zr, zero or 0 as the base address. (??)

As you may already know, to load an arbitrary 32-bit value into a register,
   you use lui and addiu.  Let's say you want to load 0xfedcba98 into v0. This
     becomes:

lui v0, 0xfedd
addiu v0, v0, 0xba98 # Add immediate unsigned (no overflow). addiu $t, $s, imm ($t = $s + imm)

Although addiu stands for "add immediate unsigned", the operand is
nevertheless sign extended.

Example 02)
sw $12 , 0xFFF8($13)    or    sw $12 , -8($13)

Both are the same.


<mips-jal> update *mips-ra*

jal - jump and link
-----------------------
copy program counter to register $ra (return address register) Jumps to the
calculated address and stores the return address in $31. Unconditionally jumps
to a specified location and puts the return address in a general register. By
default, the return address is placed in register $31.

jal target # $31 = PC + 8 (!NOTE: it's 8)

0000 11ii iiii iiii iiii iiii iiii iiii 

j - jump
------------------------
Jumps to the calculated address. The instruction j $31 returns from the a
"jal" call.  

j target
0000 10ii iiii iiii iiii iiii iiii iiii


<mips-lw>

   45c54:       8c440094        lw      a0,148(v0)    // <-

Description: A word is loaded into a register from the specified address. 

Operation: $t = MEM[$s + offset]; advance_pc (4); Syntax: lw $t, offset($s)

The whole instruction means: load a0 with the address in v0 + 148. 


={============================================================================
*kt_linux_arch_001* arch-mips-stack arch-stack

http://www.cs.umd.edu/class/sum2003/cmsc311/Notes/Mips/stack.html

To think about what's required, let's think about what happens in a function
call. 

  * When a function call is executed, the arguments need to be evaluated to
    values at least, for C-like programming languages.  

  * Then, control flow jumps to the body of the function, and code begins
    executing there.  

  * Once a return statement has been encountered, we're done with the
    function, and return back to the function call. 

Programming languages make functions easy to maintain and write by giving each
function its own section of memory to operate in. For example, suppose you
have the following function. 

int pickMin( int x, int y, int z ) {
  int min = x ;
  if ( y < min )
    min = y ;
  if ( z < min )
    min = z ;
  return min ;
}

You declare parameters x, y, and z. You also declare local variables, min. You
know that these variables won't interfere with other variables in other
functions, even if those functions use the same variable names. In fact, you
also know that these variables won't interfere with separate invocations of
itself. 

For example, consider this recursive function, 

    int fact( int n ) {
      if ( n == 0 )
        return 1 ;
      else
        return fact( n - 1 ) * n ;
    }

Each call to fact produces a new memory location for n. Thus, each separate
call (or invocation) to fact has its own copy of n. How does this get
implemented? In order to understand function calls, you need to understand the
stack, and you need to understand how assembly languages like MIPS deal with
the stack. 


{arch-sp} *stack-pointer*

When a program starts executing, a certain contiguous section of memory is set
aside for the program called the stack. Let's look at a stack. 

low addr  +---------+ stack limit (overflow)
          |         | 
          |         | /|\
			 |         |  |
			 |=========| sp (grows up towards low addr)
			 |         |
high addr +---------+ stack bottom

The stack-pointer is usually a register that contains `the top of the stack.`
The stack pointer contains the smallest address x such that any address
smaller than x is considered garbage, and any address greater than or equal to
x is considered valid. The shaded region of the diagram represents valid parts
of the stack. 


<push-and-pop>
You can push one or more registers, by setting the stack pointer to a smaller
value. Usually by subtracting 4 times the number of registers to be pushed on
the stack and copying the registers to the stack.  

You can pop one or more registers, by copying the data from the stack to the
registers, then to add a value to the stack pointer. Usually adding 4 times
the number of registers to be popped on the stack

Thus, pushing is a way of saving the contents of the register to stack, and
popping is a way of restoring the contents of the register from stack. 

Some ISAs have an explicit push and pop instruction. However, MIPS does not.

However, you can get the same behavior as push and pop by manipulating the
stack pointer directly. 

The stack pointer, by convention, is r29(sp). That is, it's register 29.
Here's how to implement the equivalent of push $r2 in MIPS, which is to push
register $r2 onto the stack. 

push:  addi $sp, $sp, -4  # Decrement stack pointer by 4
       sw   $r3, 0($sp)   # Save $r3 to stack

Here's a diagram of a push operation. 


<sp-convention>
You might wonder why it's necessary to update the stack pointer. Couldn't you
just do the following? 

push:  sw $r3, -4($sp)   # Copy $r3 to stack

Certainly, this is equivalent in behavior as far as register 3 being saved to
the stack. However we'd like to maintain the invariant (condition) that all
addresses greater than or equal to the stack pointer hold valid data, and all
the addresses less than the stack pointer hold invalid data.  It just makes
bookkeeping that much easier if we follow these conventions.  

Popping off the stack is the opposite of pushing on the stack. First, you copy
the data from the stack to the register, then you adjust the stack pointer. 

pop:  lw   $r3, 0($sp)   # Copy from stack to $r3
      addi $sp, $sp, 4   # Increment stack pointer by 4


<call-by-value>
If you've ever made the error of returning a pointer to a local variable or to
a parameter that was passed by value and wondered why the value stayed valid
initially, but later on got corrupted, you should now know the reason. The
data still stays on the garbage part of the stack until the next push
operation overwrites it. That's when the data gets corrupted.

push and pop for 3 regs

push:  addi $sp, $sp, -12  # Decrement stack pointer by 12
       sw   $r2, 0($sp)   # Save $r2 to stack
       sw   $r3, 4($sp)   # Save $r3 to stack
       sw   $r4, 8($sp)   # Save $r4 to stack

pop:  sw   $r2, 0($sp)   # Copy from stack to $r2
      sw   $r3, 4($sp)   # Copy from stack to $r3
      sw   $r4, 8($sp)   # Copy from stack to $r4
      addi $sp, $sp, 12  # Increment stack pointer by 12


{arch-stack-frame} *frame-pointer* *stack-frame*
The following is more of general when there is a frame pointer so not the same
as MIPS. 

// The stack frame is a function call and scope. Arg is copied into a stack
// and hence need *cpp-copy-ctor*

Let's now see how the stack is used to implement functions. For each function
call, there's a section of the stack reserved for the function. This is
usually called a stack frame. Let's imagine we're starting in main() in a C
program. The stack looks something like this: 

We'll call this the stack frame for main(). A stack frame exists whenever a
(called) function has started, but yet to complete.

Suppose, inside of body of main() there's a call to foo(). Suppose foo() takes
two arguments. 

  `One way to pass the arguments to foo() is through the stack.`

Thus, there needs to be assembly language code in main() to "push" arguments
for foo() onto the the stack. The result looks like: 

As you can see, by placing the arguments on the stack, the stack frame for
main() has increased in size. We also reserved some space for the return
value. The return value is computed by foo(), so it will be filled out once
foo() is done. 

Once we get into code for foo(), the function foo() may need local variables,
     so foo() needs to push some space on the stack, which looks like: 
 

   |          |
SP +----------+   ----
   | foo      |	foo(arg1, arg2) stack frame
   |          |
   +----------+   ----
FP | ret val  |   main()'s sp now since sp has increased
   +----------+
   | args     |
   +----------+
   | args     |
   +----------+
   | main     |   main()'s sp before
   |          |
   |          |
   +----------+   ----

foo() can access the arguments passed to it from main() because the code in
main() places the arguments just `as foo() expects it.` 

We've added a new pointer called FP which stands for frame pointer. The frame
pointer points to the location where the stack pointer was, just before foo()
  moved the stack pointer for foo()'s own local variables. 

Almost all architectures have one register dedicated to point to the `end` of
the stack; the `stack pointer`

<frame-pointer> <not-for-mips>
Many have a second register which points to `the start of the currently active`
stack frame; the `frame pointer` 

That's SP means the current level and FP means the start of current stack
frame.

When your program is started, the stack has only one frame, that of the
function main.  This is called the initial frame or the outermost frame.


Having a frame pointer is convenient when a function is likely to move the
stack pointer several times throughout the course of running the function. The
idea is to keep the frame pointer fixed for the duration of foo()'s stack
frame. The stack pointer, in the meanwhile, can change values.  Thus, we can
use the frame pointer to compute the locations in memory for both arguments as
well as local variables. Since it doesn't move, the computations for those
locations should be some fixed offset from the frame pointer. 

And, once it's time to exit foo(), you just have to set the stack pointer to
where the frame pointer is, which effectively pops off foo()'s stack frame.
It's quite handy to have a frame pointer.  


We can imagine the stack growing if foo() calls another function, say, bar().
foo() would push arguments on the stack just as main() pushed arguments on the
stack for foo().

So when we exit foo() the stack looks just as it did before we pushed on
foo()'s stack frame, `except this time the return value has been filled in.`

Once main() has the return value, it can pop that and the arguments to foo()
  off the stack. 

<leaf-function>
Each called function in a program allocates a stack frame on the runtime
stack, if necessary. A frame is allocated for each `non-leaf function` and for
each leaf function that requires stack storage. A non-leaf function is one
that calls other function(s); a `leaf function` is one that does not itself
make any function calls. Stack frames are allocated on the run-time stack; 

the stack grows downward from high addresses to low addresses. 

A function allocates a stack frame by subtracting the size of the stack frame
from $sp on entry to the function. This $sp adjustment must occur before $sp
is used within the function and prior to any jump or branch instructions. 


base offset | contents       | frame
+-----------+----------------+--------------
$sp       +0| arg build      |   low	
            | area           |   
            +----------------+   current
            | f/point regs   |
				| save area      |	
            +----------------+
            | generl regs    |
				| save area      |	
            +----------------+   
            | local and temp |	
+-----------+----------------+--------------
old $sp   +0| space for      |	
            | incoming args  |
            | args 1-4       |	
            +----------------+
         +16| (if present)   |
            | incoming args  |	
            | passed in sp   |	previous
            +----------------+
            | unspecified    |	high
            | ...            |	
            | variable size  |	
            +----------------+


{standard-called-function-rules}
By convention, there is a set of rules that must be followed by every function
that allocates a stack frame. Following this set of rules ensures that, given
an arbitrary program counter, return address register $31, and stack pointer,
   there is `a deterministic way of performing stack backtracing` 

These rules also make possible programs that translate already compiled
absolute code into position-independent

There is only one exit from a function that contains a stack adjustment: a
jump register instruction that transfers control to the location in the return
address register $31. This instruction, including the contents of its branch
delay slot, mark the end of function.

MIPS stack frame alignment. Although the architecture requires only word
alignment, software convention and the operating system require every stack
frame to be doubleword (8 byte) aligned.

In the previous discussion of function calls, we said that arguments are
pushed on the stack and space for the return value is also pushed. This is how
CPUs used to do it. With the RISC revolution (admittedly, nearly 20 years old
    now) and large numbers of registers used in typical RISC machines,
     `the-goal-is-to-try-and-avoid-using-the-stack` 

Why? The stack is in physical memory, which is RAM. Compared to accessing
registers, accessing memory is much slower; probably on the order of 100 to
500 times as slow to access RAM than to access a register. 


<ex> stack asm example (mips)

// C

typedef struct {
  int struct_one;
  int struct_two;
  int struct_three;
} ENTRY;

int func_second(int arg1, int arg2)
{
  int local_val = 0;

  return local_val = arg1*2 + arg2;
}

int func_first(int arg1, int arg2, int arg3, int arg4, int arg5, int arg6, ENTRY entry)
{
  int local_val = 0;
  int one = 0, two = 0;

  one = arg5+arg1+entry.struct_one;
  two = arg6+arg2+entry.struct_two;

  local_val = arg1+arg2+arg3+arg4+func_second(one, two);

  return local_val;
}

int main(int argc, char* argv[])
{
  int ret = 0;
  ENTRY node = {0};

  node.struct_one = 10;
  node.struct_two = 20;
  node.struct_three = 30;

  ret = func_first( 1, 2, 3, 4, 5, 6, node );
  printf("ret = %d\n",  ret);
  return 0;
}

// asm

int func_second(int arg1, int arg2)
{
  400590:	27bdffe8 	addiu	sp,sp,-24
  400594:	afbe0010 	sw	s8,16(sp)	 // *do-not-save-ra*
  400598:	03a0f021 	move	s8,sp     // s8 <- sp
  40059c:	afc40018 	sw	a0,24(s8)
  4005a0:	afc5001c 	sw	a1,28(s8)
	int local_val = 0;
  4005a4:	afc00008 	sw	zero,8(s8)
	
	return local_val = arg1*2 + arg2;	
  4005a8:	8fc20018 	lw	v0,24(s8)
  4005ac:	00021840 	sll	v1,v0,0x1
  4005b0:	8fc2001c 	lw	v0,28(s8)
  4005b4:	00621021 	addu	v0,v1,v0
  4005b8:	afc20008 	sw	v0,8(s8)
  4005bc:	8fc20008 	lw	v0,8(s8)
}
  4005c0:	03c0e821 	move	sp,s8     // sp <- s8
  4005c4:	8fbe0010 	lw	s8,16(sp)
  4005c8:	27bd0018 	addiu	sp,sp,24	 // pop
  4005cc:	03e00008 	jr	ra				 // return to ra2
  4005d0:	00000000 	nop

004005d4 <func_first>:

As can see, no copying of struct argument happens although uses pass by value.
How about passing this struct to second func? this struct will be copied again
in stack?

int func_first(int arg1, int arg2, int arg3, int arg4, int arg5, int arg6, ENTRY entry)
{
  4005d4:	27bdffc8 addiu sp,sp,-56
  4005d8:	afbf0030 sw    ra,48(sp)   // save ra
  4005dc:	afbe002c sw    s8,44(sp)
  4005e0:	afb00028 sw    s0,40(sp)
  4005e4:	03a0f021 move  s8,sp       // <- save sp to s8
  4005e8:	afc40038 sw    a0,56(s8)   // -> save args in stack
  4005ec:	afc5003c sw    a1,60(s8)
  4005f0:	afc60040 sw    a2,64(s8)
  4005f4:	afc70044 sw    a3,68(s8)
	int local_val = 0;
  4005f8:	afc00020 	sw	zero,32(s8)
	int one = 0, two = 0;
  4005fc:	afc0001c 	sw	zero,28(s8)
  400600:	afc00018 	sw	zero,24(s8)
	
	one = arg5+arg1+entry.struct_one;
  400604:	8fc30048 	lw	v1,72(s8)	// arg5
  400608:	8fc20038 	lw	v0,56(s8)	// arg1
  40060c:	00621821 	addu	v1,v1,v0
  400610:	8fc20050 	lw	v0,80(s8)	// struct.one
  400614:	00621021 	addu	v0,v1,v0
  400618:	afc2001c 	sw	v0,28(s8)
	two = arg6+arg2+entry.struct_two;
  40061c:	8fc3004c 	lw	v1,76(s8)	// arg6
  400620:	8fc2003c 	lw	v0,60(s8)	// arg2
  400624:	00621821 	addu	v1,v1,v0
  400628:	8fc20054 	lw	v0,84(s8)
  40062c:	00621021 	addu	v0,v1,v0
  400630:	afc20018 	sw	v0,24(s8)
	
	local_val = arg1+arg2+arg3+arg4+func_second(one, two);
  400634:	8fc30038 	lw	v1,56(s8)
  400638:	8fc2003c 	lw	v0,60(s8)
  40063c:	00621821 	addu	v1,v1,v0
  400640:	8fc20040 	lw	v0,64(s8)
  400644:	00621821 	addu	v1,v1,v0
  400648:	8fc20044 	lw	v0,68(s8)
  40064c:	00628021 	addu	s0,v1,v0
  400650:	8fc4001c 	lw	a0,28(s8)
  400654:	8fc50018 	lw	a1,24(s8)
  400658:	0c100164 	jal	400590 <func_second>
  40065c:	00000000 	nop
  400660:	02021021 	addu	v0,s0,v0	// local_val = ... + return;
  400664:	afc20020 	sw	v0,32(s8)		 
	
	return local_val;
  400668:	8fc20020 	lw	v0,32(s8)    // save return
}
  40066c:	03c0e821 	move	sp,s8
  400670:	8fbf0030 	lw	ra,48(sp)    // get ra
  400674:	8fbe002c 	lw	s8,44(sp)
  400678:	8fb00028 	lw	s0,40(sp)
  40067c:	27bd0038 	addiu	sp,sp,56  // pop sp
  400680:	03e00008 	jr	ra
  400684:	00000000 	nop

00400688 <main>:

int main(int argc, char* argv[])
{
  400688:	27bdffb8 	addiu	sp,sp,-72   // push
  40068c:	afbf0044 	sw	ra,68(sp)	   // save ra return
  400690:	afbe0040 	sw	s8,64(sp)
  400694:	03a0f021 	move	s8,sp
  400698:	afc40048 	sw	a0,72(s8)
  40069c:	afc5004c 	sw	a1,76(s8)
	int ret = 0;
  4006a0:	afc00030 	sw	zero,48(s8)
	ENTRY node = {0};
  4006a4:	afc00034 	sw	zero,52(s8)	 // save struct in stack
  4006a8:	afc00038 	sw	zero,56(s8)
  4006ac:	afc0003c 	sw	zero,60(s8)
	
	node.struct_one = 10;
  4006b0:	2402000a 	li	v0,10
  4006b4:	afc20034 	sw	v0,52(s8)
	node.struct_two = 20;
  4006b8:	24020014 	li	v0,20
  4006bc:	afc20038 	sw	v0,56(s8)
	node.struct_three = 30;
  4006c0:	2402001e 	li	v0,30
  4006c4:	afc2003c 	sw	v0,60(s8)
	
	ret = func_first( 1, 2, 3, 4, 5, 6, node );
  4006c8:	24020005 	li	v0,5		  // args in stack
  4006cc:	afa20010 	sw	v0,16(sp)  
  4006d0:	24020006 	li	v0,6
  4006d4:	afa20014 	sw	v0,20(sp)
  4006d8:	8fc20034 	lw	v0,52(s8)  // -> load and move struct
  4006dc:	8fc30038 	lw	v1,56(s8)
  4006e0:	8fc4003c 	lw	a0,60(s8)
  4006e4:	afa20018 	sw	v0,24(sp)  // <-
  4006e8:	afa3001c 	sw	v1,28(sp)
  4006ec:	afa40020 	sw	a0,32(sp)
  4006f0:	24040001 	li	a0,1
  4006f4:	24050002 	li	a1,2
  4006f8:	24060003 	li	a2,3
  4006fc:	24070004 	li	a3,4
                                        // -> update ra *mips-ra* and jump
  400700:	0c100175 	jal	4005d4 <func_first>
                                        // <- returns from the call
  400704:	00000000 	nop
  400708:	afc20030 	sw	v0,48(s8)    // get return value
	printf("ret = %d\n",  ret);	
  40070c:	3c020040 	lui	v0,0x40
  400710:	244407e0 	addiu	a0,v0,2016
  400714:	8fc50030 	lw	a1,48(s8)
  400718:	0c100124 	jal	400490 <printf@plt>
  40071c:	00000000 	nop
	return 0;
  400720:	00001021 	move	v0,zero
}
  400724:	03c0e821 	move	sp,s8
  400728:	8fbf0044 	lw	ra,68(sp)
  40072c:	8fbe0040 	lw	s8,64(sp)
  400730:	27bd0048 	addiu	sp,sp,72    // pop  
  400734:	03e00008 	jr	ra
  400738:	00000000 	nop
  40073c:	00000000 	nop


<odd-asms>
// case where no push and pop
00788400 <GFX_IMAGE_JPEG_INTERFACE_InitErrorManager>:
  788400:	3c030079 	lui	v1,0x79
  788404:	24638618 	addiu	v1,v1,-31208
  788408:	ac830000 	sw	v1,0(a0)
  78840c:	3c030079 	lui	v1,0x79
  788410:	24638370 	addiu	v1,v1,-31888
  788414:	ac830004 	sw	v1,4(a0)
  788418:	3c030079 	lui	v1,0x79
  78841c:	24638584 	addiu	v1,v1,-31356
  788420:	ac830008 	sw	v1,8(a0)
  788424:	3c030079 	lui	v1,0x79
  788428:	24638470 	addiu	v1,v1,-31632
  78842c:	ac83000c 	sw	v1,12(a0)
  788430:	3c030079 	lui	v1,0x79
  788434:	246383ec 	addiu	v1,v1,-31764
  788438:	ac830010 	sw	v1,16(a0)
  78843c:	3c0300ed 	lui	v1,0xed
  788440:	2463cbc4 	addiu	v1,v1,-13372
  788444:	ac830070 	sw	v1,112(a0)
  788448:	00801021 	move	v0,a0
  78844c:	2403007c 	li	v1,124
  788450:	ac830074 	sw	v1,116(a0)
  788454:	ac800068 	sw	zero,104(a0)
  788458:	ac80006c 	sw	zero,108(a0)
  78845c:	ac800014 	sw	zero,20(a0)
  788460:	ac800078 	sw	zero,120(a0)
  788464:	ac80007c 	sw	zero,124(a0)
  788468:	03e00008 	jr	ra
  78846c:	ac800080 	sw	zero,128(a0)

// two pops
0046bcd0 <NDSJNI_ThrowException>:
  46bcd0:	27bdffd8 	addiu	sp,sp,-40
  46bcd4:	afb20020 	sw	s2,32(sp)
  46bcd8:	afbf0024 	sw	ra,36(sp)
  46bcdc:	afb1001c 	sw	s1,28(sp)
  46bce0:	afb00018 	sw	s0,24(sp)
  46bce4:	8c900004 	lw	s0,4(a0)
  46bce8:	00a09021 	move	s2,a1
  46bcec:	8e110000 	lw	s1,0(s0)
  46bcf0:	8e390018 	lw	t9,24(s1)
  46bcf4:	0320f809 	jalr	t9
  46bcf8:	02002021 	move	a0,s0
  46bcfc:	1040000a 	beqz	v0,46bd28 <NDSJNI_ThrowException+0x58>
  46bd00:	02002021 	move	a0,s0
  46bd04:	8e390038 	lw	t9,56(s1)
  46bd08:	02403021 	move	a2,s2
  46bd0c:	8fbf0024 	lw	ra,36(sp)
  46bd10:	8fb20020 	lw	s2,32(sp)
  46bd14:	8fb1001c 	lw	s1,28(sp)
  46bd18:	8fb00018 	lw	s0,24(sp)
  46bd1c:	00402821 	move	a1,v0
  46bd20:	03200008 	jr	t9
  46bd24:	27bd0028 	addiu	sp,sp,40
  46bd28:	8fbf0024 	lw	ra,36(sp)
  46bd2c:	8fb20020 	lw	s2,32(sp)
  46bd30:	8fb1001c 	lw	s1,28(sp)
  46bd34:	8fb00018 	lw	s0,24(sp)
  46bd38:	03e00008 	jr	ra
  46bd3c:	27bd0028 	addiu	sp,sp,40


<stack-frames>

 ----------------------------------- { second
  *do-not-save-ra* since it's leaf function
  4. get ra(ra3) and return

 ----------------------------------- { first
  offset, 56(00) : sp
  1. save ra(ra2)
  2. save args

  (16) : arg5
  (20) : arg6
  (24) : struct_one
  (28) : struct_one
  (32) : struct_one

  3. save ra(ra3) and call second
  4. get ra from the frame and return

  04(48) : ra(ra2)
  00(56) : 

 ----------------------------------- { main
  offset, 72(00) : sp

  1. save register ra(ra1) to 68(sp) in the frame and this ra is set by the
     caller and will be used to return to the call site.
     from register to stack frame 

  2. save args, locals, and struct in the frame

  a0-a3 : args
  (16)  : arg5          +56 = 72
  (20)  : arg6          +56 = 76
  (24)  : struct_one    +56 = 80
  (28)  : struct_two    +56 = 84
  (32)  : struct_three

  3. call. update ra register(ra2) and call first
  use ra register to pass return address to inner calls.

  4. return.get ra from the frame and return to the caller(standard library)

  04(68) : ra(ra1) use 4(word) from here
  00(72) : 

 ----------------------------------- { standard
  update ra(ra1) and call main

<frame-has-all-necessary-to-run>
so a frame has ra, args, locals and struct itself or pointer to a struct.


<difference-with-fomit-frame-pointer>

// no -fomit-frame-pointer

00400590 <func_second>:
  400590:	27bdffe8 	addiu	sp,sp,-24
  400594:	afbe0010 	sw	s8,16(sp)
  400598:	03a0f021 	move	s8,sp
  40059c:	afc40018 	sw	a0,24(s8)
  4005a0:	afc5001c 	sw	a1,28(s8)
  4005a4:	afc00008 	sw	zero,8(s8)
  4005a8:	8fc20018 	lw	v0,24(s8)
  4005ac:	00021840 	sll	v1,v0,0x1
  4005b0:	8fc2001c 	lw	v0,28(s8)
  4005b4:	00621021 	addu	v0,v1,v0
  4005b8:	afc20008 	sw	v0,8(s8)
  4005bc:	8fc20008 	lw	v0,8(s8)
  4005c0:	03c0e821 	move	sp,s8
  4005c4:	8fbe0010 	lw	s8,16(sp)
  4005c8:	27bd0018 	addiu	sp,sp,24
  4005cc:	03e00008 	jr	ra
  4005d0:	00000000 	nop


// with -fomit-frame-pointer

$ ~/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-uclibc-gcc -fomit-frame-pointer sample.c
$ ~/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-uclibc-objdump -D a.out > sample_out_when_use_omit_

00400590 <func_second>:
  400590:	27bdfff0 	addiu	sp,sp,-16
  400594:	afa40010 	sw	a0,16(sp)
  400598:	afa50014 	sw	a1,20(sp)
  40059c:	afa00008 	sw	zero,8(sp)
  4005a0:	8fa20010 	lw	v0,16(sp)
  4005a4:	00021840 	sll	v1,v0,0x1
  4005a8:	8fa20014 	lw	v0,20(sp)
  4005ac:	00621021 	addu	v0,v1,v0
  4005b0:	afa20008 	sw	v0,8(sp)
  4005b4:	8fa20008 	lw	v0,8(sp)
  4005b8:	27bd0010 	addiu	sp,sp,16
  4005bc:	03e00008 	jr	ra
  4005c0:	00000000 	nop


// with -Os and no -fomit-frame-pointer

00400590 <func_second>:
  400590:	00041040 	sll	v0,a0,0x1
  400594:	03e00008 	jr	ra
  400598:	00a21021 	addu	v0,a1,v0


={============================================================================
*kt_linux_arch_001* arch-mips-backtrace mips-membuster

As shown above, use this pattern at the entry of every non-leaf function
although these are not in sequencial:

A function using a stack frame has the following instruction as the first one: 

addiu sp,sp,-<frame_size>

We make use of this to detect starting address of a function. This works
better than using 'j ra' instruction to signify end of the previous function
(for e.g. functions like boot() or panic() do not actually emit a 'j ra'
 instruction).	the abi does not require that the addiu instruction be the
first one.   

#define MIPS_START_OF_FUNCTION(ins)     (((ins) & 0xffff8000) == 0x27bd8000)

0x27bd 8000(001001.11101.11101.1000*)

addiu $t(11101 $29(sp)), $s(sp), imm 					# $t = $s + imm

// push
0x27BDFFE0 # in_frame=1 case. addiu sp, sp, -32

// pop
0x27BD0020 # in_frame=0 case. addiu sp, sp, 32

<ex>

  400688:	27bdffb8 	addiu	sp,sp,-72   // push sp
  40068c:	afbf0044 	sw	ra,68(sp)	   // save ra to return
  ...
  a8d3b4:	27bd0020 	addiu	sp,sp,32    // pop sp


<ex> from APP_Process

// push is not always the first
0040dd10 <AEM_JAVA_DeregisterCleanup_DeleteAllApplicationData>:
  40dd10:	3c0400f6 	lui	a0,0xf6
  40dd14:	27bdffe0 	addiu	sp,sp,-32
  ...

00578144 <PLANNER_BookedProgramme_GetRepetitionFrequency>:
  578144:	27bdffa8 	addiu	sp,sp,-88
  578148:	afb5004c 	sw	s5,76(sp)
  57814c:	27a70020 	addiu	a3,sp,32
  578150:	00c0a821 	move	s5,a2
  578154:	2406000f 	li	a2,15
  578158:	afb40048 	sw	s4,72(sp)
  57815c:	afb30044 	sw	s3,68(sp)
  578160:	afb20040 	sw	s2,64(sp)
  578164:	afb00038 	sw	s0,56(sp)
  578168:	afbf0050 	sw	ra,80(sp) ~
  ...

  578500:	03e00008 	jr	ra
  578504:	27bd0058 	addiu	sp,sp,88

00577e6c <PLANNER_BookedProgramme_StoreUint32InPCAT>:
  577e6c:	27bdffb8 	addiu	sp,sp,-72
  577e70:	38e30002 	xori	v1,a3,0x2
  577e74:	afb40040 	sw	s4,64(sp)
  577e78:	afb3003c 	sw	s3,60(sp)
  577e7c:	24020012 	li	v0,18
  577e80:	00c09821 	move	s3,a2
  577e84:	24140002 	li	s4,2
  577e88:	24060011 	li	a2,17
  577e8c:	afb20038 	sw	s2,56(sp)
  577e90:	0043300b 	movn	a2,v0,v1
  577e94:	27a70028 	addiu	a3,sp,40
  577e98:	24020003 	li	v0,3
  577e9c:	02809021 	move	s2,s4
  577ea0:	afb00030 	sw	s0,48(sp)
  577ea4:	afbf0044 	sw	ra,68(sp) ~
  ...

  57813c:	03e00008 	jr	ra
  578140:	27bd0048 	addiu	sp,sp,72


<build-tables>
// scan from loaded memory
/* generates output. needs initialized frame info tab */
void parse_text_raw_mips(uint32_t *text, uint32_t text_addr, uint32_t text_size, uint32_t swap, 
							uint32_t *syms, uint16_t *info_idx, uint32_t tcount, uint32_t *frame_inf )
{
  uint32_t *p;
  uint16_t frame_sz, ret_addr;
  int i, in_frame;
  uint32_t ins, idx, cnt, frame_addr;

  frame_sz = 0;

  p = text;
  in_frame = 0;
  cnt=0;

  // scan through all text code by int size
  for(i=0; i < text_size>>2; i++)
  {
    ins = p[i];

    // push, get the start address of a frame
    if((ins &  0xffff8000) == 0x27bd8000)
    {
      frame_sz = (-ins) & 0x7fff;

      // text_addr is load address so get a loaded address of a frame.
      frame_addr = (i << 2) + text_addr;
    }

    // ra
    if ((ins & 0xffff8000) == 0xafbf0000)
    {
      // ra offset and frame size pair must be found in the *frame-info-table*
      // and get a index to frame info table and save it to the info index
      // table.
      ret_addr = (ins) & 0x7fff;
      for(idx=0; idx < tcount; idx++)
      {
        /* always expect a match here */
        if( ((ret_addr << 16) | frame_sz) == frame_inf[idx]) break;
      }

      if(idx == tcount) printf("This should not happen, contact me\n");

      // cnt is # of total frame
      //
      // *symbol-info-table* has loading address of each frame
      syms[cnt] = frame_addr;

      // *info-index-table* has index to *frame-info-table* of each frame. By
      // doing this, make a map between symbol info table to frame info table.
      info_idx[cnt] = idx;

      cnt++;
    }
  }

  return;
}


// #define TMP_FTAB_SIZE		1024
//
// scan from loaded memory(text) and return count the total number of frames,
// that is # of "save ra" found.
//
// and build frame info table. not use text_addr since this binary is already 
// loaded into memory.

int count_text_raw_mips(uint32_t *text, uint32_t text_addr, uint32_t text_size, uint32_t swap, 
						uint32_t *count, uint32_t *tcount, uint32_t *frame_inf)
{
  tcnt = 0;

  // since a instruction is 4 bytes
  for(i=0; i < text_size>>2; i++)
  {
#ifdef BI_ENDIAN_SUPPORT
    ins = cond_swap32(p[i], swap);
#else
    ins = p[i];
#endif				

    // when see "push" and get the size of stack frame
    if((ins &  0xffff8000) == 0x27bd8000)
    {
      // 27bdffb8 	addiu	sp,sp,-72  // push sp
      //
      // >>> hex(0x27bdffb8 & 0xffff8000)
      // '0x27bd8000'
      //
      // >>> 15 bits, bin(0x7fff)
      // '0b111111111111111'

      frame_sz = (-ins) & 0x7fff;
      in_frame =1;
    }

    // when see "pop"
    if((ins &  0xffff8000) == 0x27bd0000)
    {
      in_frame=0;
    }

    // *frame-info-table* To get and pack ra offest and frame size
    //
    // Whenever see "save ra" while scanning whole text area, save offset of
    // ra to save in the stack and the size of current stack frame. 
    //
    // cnt is the total number of ra instruction seen and tcnt is number of
    // entry in the frame info table
    //
    // WHY do (offset<<16 | frame_sz)?
    //
    // "To save memory, use 16 bits for each ra and frame size and pack them
    // into single 4 bytes."
    //
    // This has simply offset size which are byte offset and frame info table
    // do need to have the same entry as the symbol table since different
    // symbols may have the same offset sizes. 
    //
    // # of total frames from scanning is 23018 -> # of entrys in frame info
    // table is 448 when runs on APP_Process
    //
    // so make a value from ra offset and frame size and save it into the
    // table.
    //
    // 0040bbd4 <_init>:
    //   40bbd4:	3c1c00b1 	lui	gp,0xb1
    //   40bbd8:	279c055c 	addiu	gp,gp,1372
    //   40bbdc:	0399e021 	addu	gp,gp,t9
    //   40bbe0:	27bdffe0 	addiu	sp,sp,-32         // frame size
    //   40bbe4:	afbc0010 	sw	gp,16(sp)
    //   40bbe8:	afbf001c 	sw	ra,28(sp)            // ra offset
    // 
    // 0040c920 <__do_global_dtors_aux>:
    //   40c920:	3c0200f2 	lui	v0,0xf2
    //   40c924:	90428c50 	lbu	v0,-29616(v0)
    //   40c928:	27bdffe0 	addiu	sp,sp,-32
    //   40c92c:	afbf001c 	sw	ra,28(sp)
    //
    // // when the second "sw ra" do not count as "save ra"
    // 007f07cc <stacktrace_start>:
    //   7f07cc:	27bdffd0 	addiu	sp,sp,-48
    //   7f07d0:	afb10024 	sw	s1,36(sp)
    //   7f07d4:	afbf0028 	sw	ra,40(sp)
    //   7f07d8:	afb00020 	sw	s0,32(sp)
    //   7f07dc:	00808821 	move	s1,a0
    //   7f07e0:	ac9d0004 	sw	sp,4(a0)
    //   7f07e4:	ac9f0010 	sw	ra,16(a0)
    //
    // *tool-objdump* dumps all but here we only care about text code.
    //
    // Like these, both maps into the same entry in frame info table since
    // both has the same size of frame size and ra offset.
    //
    // "To use frame info table later, use"
    //
    // info = frame_info[info_index[i-1]];
    // info >> 18
    //
    // not 16? since ra offset is bytes and use it as 4 bytes int by /4.
    //
    // count: foffset(0x0000bbe0): push. ins(0x27bdffe0) -ins(0xd8420020) frame_sz(0x20)
    // count: j(0), frame_info[  0] = 001c0020. ret_addr = 0000001c(28)
    // count: frame_info[  0] >> 18 = 00000007(7)
   
    if ((ins & 0xffff8000) == 0xafbf0000)
    {
      // get ra offset in stack
      ret_addr = (ins) & 0x7fff;

      // is it already in the frame table?
      for(j=0; j < tcnt; j++)
      {
        if(frame_inf[j] == ((ret_addr << 16) | frame_sz) ) break;
      }
      
      // if not in the frame table then
      if(j == tcnt)
      { 
        // add it to the frame table
        frame_inf[tcnt] = ((ret_addr << 16) | frame_sz);
        DPRINT("frame_info[%3d] = %.8X\n", tcnt, frame_inf[tcnt]);
        tcnt++;
        if(tcnt >= *tcount) return -1;
      }

      // note: increase after so have +1
      cnt++;
    }
  }

  // return counts

  *count = cnt;
  *tcount = tcnt;
  return cnt;
}

// function    jump                  symbol/frame 
// address     table in 4K           table
// 
// f->func =>  [======] (index to symbol table and frame info table)
//             [======]
//             [======]
//             [======]
//             --------
//             [======]
//             [======]
//             [======]
//             [======]
// 
// One jump table entry covers 4K which may have many symbols in it. So when
// found a jump table index and then get -2 symbols back from the corresponding
// symbol and move forward from that to find the closest symbol. Use estimation.

void membuster_plus_init(void)
{
  stacktrace_init(mb_bin_path)
  {
    // 1. parse elf header to get settings:
    //    endian?, swap?, arch?, need table? (yes for mips)

    parse_fn = parse_text_raw_mips;
    count_fn = count_text_raw_mips;

    // 2. build tables
    // parse elf program header to get PT_LOAD properties.
    //
    // build structure by scanning the text two times
    //
    // syms[# of total frame found] = address of a frame
    // info_index[# of toral frame frond] = index to (unique) frame table
    // frame_info[# of unique frame] = unique frame tag(computed)

    for(n=0; n < n_phdr; n++)
    {
      if(phdr[n].p_type == PT_LOAD)
      {
        // use printf
        // #define DPRINT if(DEBUG_ON == 1)printf

        DPRINT("PT_LOAD header: offset=%d vaddr=%x paddr=%x fsize=%d msize=%d flags=%x\n", 
            phdr[n].p_offset, phdr[n].p_vaddr, phdr[n].p_paddr, phdr[n].p_filesz, phdr[n].p_memsz, phdr[n].p_flags); 

        if(phdr[n].p_flags & PF_X)
        {
          // get start and end
          // PT_LOAD header(3): type=1 offset=0 
          // vaddr=0x400000 paddr=0x400000 fsize=11297060 msize=11297060 flags=0x5

          text_start = phdr[n].p_vaddr;
          text_end = phdr[n].p_vaddr + phdr[n].p_memsz;

          // text_start 400000 - text_end ec6124
          //
          // >>> hex(0xac6124+0x00400000)
          // '0xec6124'
          // >>> 0xec6124-0x400000
          // 11297060, '0xac6124'

          if(need_table)
          {
            DPRINT("parse text %x - %x\n", text_start, text_end);

            text = text_start;
            tcount = TMP_FTAB_SIZE;

            frame_info = malloc(TMP_FTAB_SIZE * sizeof(uint32_t));

            // count: cnt(23008), tcnt(448).
            count_fn(text, phdr[n].p_vaddr, phdr[n].p_filesz, 
                need_swap, &count, &tcount, frame_info);

            // resize frame table
            frame_info = realloc(frame_info, tcount * sizeof(uint32_t));
            DPRINT("found %d frames %d fram_tab entries\n", count, tcount);

            // *symbol-info-table-invalid* 
            // note symbols have +1 more since that # total frame found is 23007
            // but count_fn returns +1 since count is increased after to use array.
            //
            // [0-23007] and alloc one more and set the last, 23009,
            // [0-23008], as invalid
            //
            // found 23008 frames -> 448 fram_tab entries 
            // alloc symbols=92036 for frames 23009
            //
            // <ex>
            // count(23116): loffset(0x00d88854): ret. ins(0xafbf4351), j(484), tcnt(485)
            // count: cnt(23117), tcnt(486).
            // alloc symbols=92472 for frames 23118
            //
            // save syms[23116] = 00d31e60 info[23116]= 485, ins(0xafbf4351) frame_addr(0x00d31e60) 
            // parse done

            symbols = (uint32_t *) malloc((count+1) * sizeof(uint32_t));
            symbols[count] = 0x0FFFFFFF;

            info_index = (uint16_t *) malloc(count * sizeof(uint16_t));
            memset(info_index, 0xFF, count * sizeof(uint16_t));

            parse_fn(text, phdr[n].p_vaddr, phdr[n].p_filesz, 
                need_swap, symbols, info_index, tcount, frame_info);

            // the last
            // save syms[23007] = 00c61e6c info[23007]= 447

            DPRINT("parse done\n");
          }
        }
      }
    }

    // 3. build jump table
    //
    // #define MAX_JUMP_TAB_SIZE 	8192 (0x2000)
    //
    // count is # of total frames

    if(need_table)
    {
      jump_table_size = MAX_JUMP_TAB_SIZE; 

      // alloc jump_table=32768

      /* prepare a jump table (index table into the function addresses) */
      jump_table = malloc(sizeof(uint32_t) * jump_table_size);
      memset(jump_table, 0x00, sizeof(uint32_t) * jump_table_size);

      // *jump-info-table* maps to index of *symbol-info-table*
      //
      // TO SAVE MEM AND LOOK-UP TIME!
      //
      // >>> bin(8191), 13 bits and "& (jump_table_size-1)" do modulo % operation
      // '0b1 1111 1111 1111'

      // count=23008
      //
      // jt: symbols[0]= 0x0040bbe0(0x0000040b), first_jump_val=0x0000040b
      // jt: update jump_table[0x0040b] = 0
      
      first_jump_val = ((symbols[0] >> 12) & (jump_table_size-1));
      jump_table[first_jump_val] = 0;
      jump_val = first_jump_val;

      // jt: symbols[00001]= 0x0040c928(0x0040c924, 0x0000040c), jump_val=0x0000040b 
      // jt: !eg: update jump_table[0x0040c] = 0
      //
      // note: as you can see, some symbols falls into the same 4K page size
      // and are skipped. Later, maps to the same jump info table index.
      //
      // jt: symbols[00002]= 0x0040c9b4(0x0040c9b0, 0x0000040c), jump_val=0x0000040c 
      // jt: symbols[00003]= 0x0040cfa0(0x0040cf9c, 0x0000040c), jump_val=0x0000040c 
      //
      // note: only when calculated jump_val is different from the previous
      // jump_val, that is a different page, maps to the different jump info
      // table index.
      // 
      // jt: symbols[00004]= 0x0040d010(0x0040d00c, 0x0000040d), jump_val=0x0000040c 
      // jt: !eg: update jump_table[0x0040d] = 3
      //
      // jt: symbols[00005]= 0x0040d1ec(0x0040d1e8, 0x0000040d), jump_val=0x0000040d 
      // jt: symbols[00006]= 0x0040dc60(0x0040dc5c, 0x0000040d), jump_val=0x0000040d 
      // jt: symbols[00007]= 0x0040dd14(0x0040dd10, 0x0000040d), jump_val=0x0000040d 
      // jt: symbols[00008]= 0x0040dd34(0x0040dd30, 0x0000040d), jump_val=0x0000040d 
      // jt: symbols[00009]= 0x0040de50(0x0040de4c, 0x0000040d), jump_val=0x0000040d 
      //
      // jt: symbols[00010]= 0x0040e388(0x0040e384, 0x0000040e), jump_val=0x0000040d 
      // jt: !eg: update jump_table[0x0040e] = 9

      for(i=0; i < count; i++)
      {
        // note: WHY -4? Probaly becuase the start of a frame(symbol) do not
        // always start from the beginning?

        if((((symbols[i+1]-4) >> 12) & (jump_table_size-1)) != jump_val)
        {
          jump_val = (((symbols[i+1]-4) >> 12) & (jump_table_size-1));

          // note: -1
          jump_table[jump_val] = i;
        }

      }

      // ...
      // jt: !eg: update jump_table[0x00c61] = 23004
      //
      // jt: symbols[23008]= 0x0fffffff(0x0ffffffb, 0x0000ffff), jump_val=0x00000c61 
      // jt: !eg: update jump_table[0x01fff] = 23007
      // jt: jump_count= 7157 
      //
      // # of frames is 23018 -> 8192

      // note that set [beg-1] = [beg]
      /* this is so that we can safely go one entry below the first without a if statement */
      if(first_jump_val != 0)jump_table[first_jump_val - 1] = jump_table[first_jump_val];

      // note that this is array index calculation to get size
      /* plus 1 since it's a range including the last value */
      jump_count = (jump_val - first_jump_val)+1;

      // jump_count= 7157.  (0x01bf5) (0x01fff-0x0040b) +1= 7157
      // 0x0040b+7157 = 0x02000
     
      // fill empty slots which represent 4K address space which do not have
      // symbols belongs to that and set these slots to the one found before.
      // So repeat the found 4K slot on following empty slots until see the
      // new one.

      /* fill up the holes in the jump table */
      for(i=first_jump_val; i < (first_jump_val + jump_count); i++)
      {
        if(jump_table[i]==0)jump_table[i] = jump_table[i-1];
        DPRINT("jump table[%.5d] = %5d\n", i, jump_table[i]);
      }
    }
  }
}


<membuster-init>

typedef struct _BT_LIST
{
	uint32_t checksum;
	uint16_t pos;
	uint16_t len;
	uint32_t size;
} BT_LIST;

typedef struct _ALLOC_LIST
{
	uint32_t addr;
	uint32_t time;
	uint32_t size;
	uint16_t bt_index;
	uint16_t type;
} ALLOC_LIST;

struct MEMMAN_API_MemoryPoolHandleTag
{
#ifdef MEMMAN_MEMBUSTER
	uint32_t *bt_data;
	uint32_t bt_data_idx, bt_list_idx, bt_list_len, bt_data_len;
	uint32_t alloc_list_idx, alloc_list_len;
	int32_t recent_freed[5];
	BT_LIST *bt_list;
	ALLOC_LIST *alloc_list;
	uint32_t   *bt_temp; /* temp buffer for holding backtraces */
#ifdef MEMBUSTER_DEBUG
	uint32_t alloc_search_time, bt_search_time, fletcher_time, bt_time, free_time;
#endif
#endif /* MEMBUSTER */
};


// CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/memman_pinit.c
// note: it's per a pool
static SYSTEM_STATUS PoolCreate(MEMMAN_API_MemoryPoolHandle **handle, char *name, uint32_t flags)
{
  poolHandle->alloc_list = malloc(sizeof(ALLOC_LIST) * ALLOC_LIST_INIT_SIZE);
  poolHandle->bt_list    = malloc(sizeof(BT_LIST) * BT_LIST_INIT_SIZE);
  poolHandle->bt_data    = malloc(sizeof(uint32_t) * BT_DATA_INIT_SIZE);

  // note comes from `max_call_in_bt = MAX_CALL_DEPTH;`
  poolHandle->`bt_temp`    = malloc(get_max_call_in_bt() * sizeof(uint32_t));

  poolHandle->alloc_list_len = ALLOC_LIST_INIT_SIZE;

  // note: represent total num of alloc of a pool
  /* 1 because the first index is found through recently freed index mechanism */
  poolHandle->alloc_list_idx = 1; 

  // note: represent total num of bt of a pool
  poolHandle->bt_list_idx = 0;
  poolHandle->bt_list_len = BT_LIST_INIT_SIZE;

  poolHandle->bt_data_len = BT_DATA_INIT_SIZE;
  poolHandle->bt_data_idx = 0;
  // ...
  *handle = poolHandle;
}

uint32_t MHWMemCheckBank(MEMMAN_API_MemoryPoolHandle * mpool, uint32_t size, uint32_t mem_nb_bank)
{
	membuster_plus_pool_oom(mpool);
}

MEMMAN_API_Init
{
	membuster_plus_init();
}


<from-user>
// instrumented memman ifs

MEMMAN_API_Handle *MEMMAN_API_AllocP(MEMMAN_API_MemoryPoolHandle *pool, uint32_t size, int32_t gcollect)
void   *MEMMAN_API_AllocStaticP(MEMMAN_API_MemoryPoolHandle * pool, uint32_t size)        
void   *MEMMAN_API_CallocStaticP(MEMMAN_API_MemoryPoolHandle * pool, uint32_t nbelem, uint32_t size)
void   *MEMMAN_API_AllocStaticAlignP(MEMMAN_API_MemoryPoolHandle * pool, uint32_t size, uint32_t align)
void   *MEMMAN_API_AllocMaxStaticP(MEMMAN_API_MemoryPoolHandle * pool, uint32_t * paramsize)
void   *MEMMAN_API_ReallocStaticP(MEMMAN_API_MemoryPoolHandle * pool, void *ptr, uint32_t askedSize)
void   *MEMMAN_API_ReallocStaticAlignP(MEMMAN_API_MemoryPoolHandle * pool, void *ptr, uint32_t askedSize, uint32_t align)
{
  //  alloc calls
  ptr = (void *) MHWMemAllocStatic(pool, size);

  // CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/mbuster_pinit.c
  // add list
  // alloc_list_add(MEMMAN_API_MemoryPoolHandle *pool, uint32_t addr, uint32_t size, uint32_t type)
  alloc_list_add(pool, (uint32_t)ptr, size, ALLOC_TYPE_MV);
  {
    // struct stacktrace_frame f;
    // 	struct stacktrace_frame
    // 	{
    // 	  int             size;       /* Frame size */
    // 	  uint32_t *      fp;         /* Frame pointer */
    // 	  uint32_t *      pc;         /* Program counter */
    // 	  uint32_t *      func;       /* Start of current function */
    // 	  uint32_t *      caller;     /* Address of caller */
    // 	};

    // this is the very first function
    // as it's already in the called frame, sp was moved and ra has return addr.
    //
    // *nolinine*
    // int __attribute__ ((noinline)) stacktrace_start(struct stacktrace_frame *f)

    // starts from 0 which means i represent num of calls in bt_temp
    i = 0;

    stacktrace_start(&f);
    {
      // set f for current function frame
      {
        // run asm code to get sp and ra to the caller
        /* __builtin_frame_address(0) doesn't appear work with -fomit-frame-pointer */
        /* so we get the stack pointer and hope it's not been trashed */
        f->fp     = $sp;
        f->caller = $ra;
        f->func   = stacktrace_start;

        // note: use 25 bits address and 4K page in the jump table. Starts from
        // the -1 from the jump table due to how jump table is constructed and
        // get the symbol index.
        //
        // >>> bin(0x1FFF000) '0b1111111111111000000000000'

        // note: -1 again
        i = jump_table[(((uint32_t)(f->func) & 0x1FFF000) >> 12)-1];

        // to get a nearest addres in the sym table
        while((uint32_t)(f->func) >= symbols[i])
        {
          i++;
        }

        // to get a frame size but use -1 since use estimation.
        f->size = frame_info[info_index[i-1]] & 0xFFFF;
      }

      // get the caller from the current
      {
        /* Unwind frame - below if statement is on purpose, as it prevents 
           a nasty optimization that breaks unwinding*/
        i = stacktrace_unwind(f);
      }

      if(i == -1)return -1;
      else return 0;
    }

    // note: now current f->caller is `alloc_list_add`

    // build a backtrace(bt) of this call from a user into bt_temp array
    while(stacktrace_unwind(&f) != -1)
    {
      pool->bt_temp[i++] = f.caller;
    }

    // calculate a checksum of bt_temp, that is a whole bt
    checksum = fletcher32(pool->bt_temp, i);

    // get a idx of next free alloc slot from the alloc list array
    `idx` = get_alloc_index(pool);

    // get a idx of bt which matches to checksum from the bt list and if not
    // found one and which means new one, call bt_add
    `bt_idx` = bt_find(pool, checksum);
    if(bt_idx==-1)
    {
      // note: i is number of calls in the bt_temp
      bt_idx = bt_add(pool, pool->bt_temp, i, checksum);
      { 
        // get a next free index of bt_list
        idx = pool->bt_list_idx++; 

        // set bt_list
        pool->bt_list[idx].len      = len;
        pool->bt_list[idx].checksum = checksum;
        pool->bt_list[idx].pos      = pool->bt_data_idx;

        // bt_data[] = bt_temp
        memcpy(&(pool->bt_data[pool->bt_data_idx]), bt, len * sizeof(uint32_t));
        pool->bt_data_idx += len;

        return idx;
      }

      pool->bt_list[bt_idx].size = 0;
    }

    //
    pool->alloc_list[idx].addr = ptr;                  // allocated ptr from memman	
    pool->alloc_list[idx].size = size;                 // allocated size
    pool->alloc_list[idx].bt_index = (uint16_t)bt_idx; // idx of this bt
    pool->alloc_list[idx].type = type;                 // allocated type
    pool->alloc_list[idx].time = time_get();
    pool->bt_list[bt_idx].size += size;                // accumulated size for this bt
  } // end alloc_list_add
}

note:

alloc_list[x]
  has bt[y]
    : 7c8db8 7a6c54 7b1a98 7b64b8 7c3670 7c4850 7a7524 7986e0 5101d8 50abe8 e91518


SYSTEM_STATUS MEMMAN_API_FreeP(MEMMAN_API_MemoryPoolHandle *pool, MEMMAN_API_Handle *handle)
SYSTEM_STATUS MEMMAN_API_FreeStaticP(MEMMAN_API_MemoryPoolHandle * pool, void *ptr)
{
	alloc_list_remove(pool, (uint32_t) addr);
	{
		# search alloc list for this addr and if found, mark it freed.
		if(pool->alloc_list[i].addr == addr)
			pool->bt_list[pool->alloc_list[i].bt_index].size -= pool->alloc_list[i].size;
			add_recent_freed(pool, idx of alloc list);
	}
}


#if __mips__ 
/* unwinding of stack frame */
int __attribute__ ((noinline)) stacktrace_unwind(struct stacktrace_frame *f)
{
  int count, i;
  uint32_t info;

  count = 0;

  /* Unwind frame pointer */
  f->pc = f->caller;

  // note that we are in the stack now and stack grows to low address:
  //
  // base offset | contents       | frame
  // +-----------+----------------+--------------
  // $sp       +0| arg build      |   low	
  //             | area           |   
  //             +----------------+   current
  //             | ra             |
  //             +----------------+
  //             ...
  // +-----------+----------------+--------------
  // old $sp   +0|                |	
  //             |                |	high
  //
  // to get the sp of the previous frame, move the current sp back to the
  // previous by adding offset (increase in address).
  //
  // note that do /4 since offset is byte offset and we use sp in 4 bytes
  // unit.

  // move sp back
  f->fp += f->size >> 2;

  // caller address
  f->func = f->pc;

  /* don't go further down if it's inside the stdlib */
  /*if((uint32_t)f->func & 0xFF000000)return -1;
  if(((uint32_t)f->func) < 0x00400000)return -1;*/

  if((uint32_t)(f->func) < text_start || (uint32_t)(f->func) > text_end)
	  `return -1;`

  i = jump_table[(((uint32_t)(f->func) & 0x1FFF000) >> 12)-1];
  TDPRINT("caller2 %x jmp %d\n", f->caller, i);
  /*if(i==0)return -1;*/
  
  TDPRINT("sym2 %x\n", symbols[i]);
  while((uint32_t)(f->func) >= symbols[i])
  {
  	i++;
	TDPRINT("sym2x %x\n", symbols[i]);
  }

  // note: fix!

  /* get function start addr from the symbol table */
  f->func = (uint32_t *)symbols[i-1];

  // *frame-info-table*
  /* if we've hit a function with no info, we're done */
  if( info_index[i-1] == 0xFFFF ) `return -1;`

  info = frame_info[info_index[i-1]];
  /* Get size of frame */
  f->size = info & 0xFFFF;

  // WHY -2, 8 bytes back? since *mips-jal* moves PC+8?

  /* Get caller */
  f->caller = (uint32_t *)f->fp[info >> 18] - 2;
  TDPRINT("frame2 %x\n", f->caller);

  return 0;
}


<fix-to-membuster-crash>
APP process crashes when press a yellow button to launch iapp and found that
one of functions during unwinding comes from libPicasso.a and which do not
have push, save ra, and pop pattern. Therefore, not in symbol table
membuster maintains. The fix is to return in those cases: 

    if (0x0FFFFFFF == symbols[i])
    {
      printf("KT: =============================\n");
      printf("KT: su: RETURN: i(%d), f->func(0x%8x)\n", i, f->func);
      return -1;
    }

Some functions called are:

KT: su: RETURN: i(23081), f->func(0x  e4fff8)
KT: su: RETURN: i(23081), f->func(0x  f00700)

$ mips-uclibc-addr2line -f -s -e APP_Process e4fff8 f00700
Lpriv192
adaptors-register-natives.c:0
Lpriv266
adaptors-register-natives.c:0


/* GENERATED CODE - DO NOT EDIT */
./build/applications/Picasso/picasso/jpa/out/release_dbg/amsdrx890/release_dbg/generated-source/adaptors-register-callbacks.c

build/applications/Picasso/picasso/jpa/Makefile:249:            ADAPTOR_LIBS="$(abspath $(OUTDIR_PLATFORM_DEPENDENT)/libPicasso.a)"\

// dump from libPicasso.a

00000ec0 <Lpriv27564>:
     ec0:	3c1c0000 	lui	gp,0x0
     ec4:	279c0000 	addiu	gp,gp,0
     ec8:	0399e021 	addu	gp,gp,t9
     ecc:	1220001b 	beqz	s1,f3c <Lpriv27564+0x7c>
     ed0:	ac310000 	sw	s1,0(at)
     ed4:	8f01fffc 	lw	at,-4(t8)
     ed8:	30210002 	andi	at,at,0x2
     edc:	14200017 	bnez	at,f3c <Lpriv27564+0x7c>
     ee0:	8f990000 	lw	t9,0(gp)
     ee4:	00000000 	nop
     ee8:	8f390000 	lw	t9,0(t9)
     eec:	8f390000 	lw	t9,0(t9)
     ef0:	8e21fffc 	lw	at,-4(s1)
     ef4:	30210002 	andi	at,at,0x2
     ef8:	10200010 	beqz	at,f3c <Lpriv27564+0x7c>
     efc:	8f810000 	lw	at,0(gp)
     f00:	00000000 	nop
     f04:	8c210000 	lw	at,0(at)
     f08:	0319c823 	subu	t9,t8,t9
     f0c:	8c210000 	lw	at,0(at)
     f10:	0019c302 	srl	t8,t9,0xc
     f14:	0018c080 	sll	t8,t8,0x2
     f18:	0019c9c2 	srl	t9,t9,0x7
     f1c:	0038c021 	addu	t8,at,t8
     f20:	24010001 	li	at,1
     f24:	0321c804 	sllv	t9,at,t9
     f28:	c3010000 	ll	at,0(t8)
     f2c:	00390825 	or	at,at,t9
     f30:	e3010000 	sc	at,0(t8)
     f34:	1020fffc 	beqz	at,f28 <Lpriv27564+0x68>
     f38:	00000000 	nop
     f3c:	03e00008 	jr	ra
     f40:	00000000 	nop
	...


<ex>
note:
* do now show MEMMAN_API_AllocStaticP
* the line number from *tool-addr2line* is not exact match

pool GUIDE size=590248:
size=36 (6030e0 615f60 601714 9d5c6c ) [dcab057f]

$ mips-uclibc-addr2line -f -s -e ./APP_Process 6030e0 615f60 601714 9d5c6c
Guide_Util_Alloc_Debug
guide_api_common.c:270
GuideScheduleSimpleProgrammeInstanceListChangedHandler
guide_api_schedule.c:4590
MessageThread
guide_api_common.c:1113
VRMIPCClientGenericUserInterfaceCB
vrm_ipc_client.c:1029

// DARWIN_PLATFORM/DARWIN_GUIDE/src/guide_api_common.c

void* Guide_Util_Alloc_Debug(MEMMAN_API_MemoryPoolHandle *pool, uint32_t size, const char *file, int line)
{
   void *ptlBuffer = NULL;

   DIAG_DECLARE_FUNCTION("Guide_Util_Alloc_Debug");
   DIAG_LOG_FN_ENTRY(g_guide_diag_memory_segment_id);

#if (defined(MEMMAN_FORCE_ALLOC_DEBUG))
   ptlBuffer = MEMMAN_API_AllocStaticPDebug(pool, size, NULL, file, line);
#else
   ptlBuffer = MEMMAN_API_AllocStaticP(pool, size);
#endif
   // ...
}


<membuster-check-from-thread>

mb_init_sleep  = 300;
mb_check_sleep = 180;

static void membuster_thread(void)
{
  sleep(mb_init_sleep); /* wait 5 minutes before checking allocations */
  
  while(1)
  {
    membuster_plus_check_all();
    {
      "start check at: %.3d:%.2d:%.2d.%.3d\n",
    }

    sleep(mb_check_sleep);
  }
}


// old: start check at: 000:04:38.883
// WHY??
start check at: 1192:59:51.721


// membuster_plus_check_all() runs a loop for all pools. List allocs which has
// the same bt index, that is the same bt, for each pool 

pool SFM_MEMORY_POOL size=741680:
size=400 (c7e800 c7f078 c7d0f8 108357c ) [c2d7e24b]
allocs:4
size=336 (c80a38 c7a45c e92fd8 e96904 e98330 e4a54c e4c908 d1ab10 d3a178 108357c ) [9094c3ba]
allocs:6
size=3528 (c80a38 c7a45c c98104 cb4000 cad48c caf924 cb15bc cd3254 e8cd2c d0a00c d0da4c d0c934 cc03d8 cc0e48 cc17a0 cc23f0 ccf108 ccb150 e48010 e4c220 d1ab10 d3a178 108357c ) [e9045d3a]
allocs:63
size=900 (c7e800 c7f240 c7d0f8 108357c ) [cbbfe413]
allocs:9
size=210 (c8235c c7e858 c7f240 c7d0f8 108357c ) [14350890]
allocs:6
size=1700 (c7e800 c7ee18 c7d338 108357c ) [bdb7e22b]
allocs:17
size=630 (c8235c c7e858 c7ee18 c7d338 108357c ) [062d06a8]
allocs:18
pool usage: 17800/741680 peak: 17912

pool SFM_SectionPool size=1923072:

...


<membuster-check-from-oom>
// found that OOM from log is `far behind` in OOM in MEM.TXT.

void   *MHWMemAllocStatic(MEMMAN_API_MemoryPoolHandle *pool, uint32_t size)
{
  MEMORY_PRINT_W(("Cannot allocate %d bytes in pool " POOLNAME_FMT ", not enough memory", size, PNAME(pool)));
  if (smax <= size)
  {
    MEMORY_PRINT_W(("   Biggest block is %d bytes", smax));
  }
  DUMP_USED_BLOCKS(pool);
}


#define DUMP_USED_BLOCKS(POOL) \
	(void) membuster_plus_pool_oom(POOL); \
	(void) MemCheckMemory(POOL, MEMSTATUS_LOGLEVEL(MEMMAN_API_MEMSTATUS_ALL, MEMSTATUS_LOGLEVEL_WARN))


void membuster_plus_pool_oom(MEMMAN_API_MemoryPoolHandle *p)
{
#ifdef MEMBUSTER_OOM_ONLY
    membuster_plus_check_pool(0, mb_file_path, p);
#else
    membuster_create_output_path();
    membuster_plus_check_pool(mb_output, mb_file_path, p);
#endif /* MEMBUSTER_OOM_ONLY */
}


// static void membuster_plus_check_pool() is used by MEMMAN API but not from
// membuster thread.
// CMS_SYSTEM_INFRASTRUCTURE/MEMMAN/src/mbuster_pinit.c

static void membuster_plus_check_pool()
{
  // bt from bt_data

  sprintf(buf+(9*x), "%08x ", 0xFFFFFFFF & (p->bt_data[b[j].pos + x]));

  // to MEM_xx file
  fprintf(of,
      "OOM Detected at: %.3d:%.2d:%.2d.%.3d\n",
      hour,
      minute,
      sec,
      msec);

  // to LOG
  MEMORY_PRINT_E(("MEMBUSTER: pool %s, count %d size %d, bt(%s)", p->name, `alloc_count`, `b[j].size`, buf));
}


M:mem_blockpool.c F:MHWMemCheckBank L:310 > CHECK_BANK Size:1448 FreeSize:788 Bank:1 PoolMAX:387248 
M:mem_static.c F:MHWMemAllocStatic L:75 > Cannot allocate 2580 bytes in pool " "MW_HTTP_POOL"
M:mem_static.c F:MHWMemAllocStatic L:78 > Biggest block is 712 bytes
M:mbuster_pinit.c 
  F:membuster_plus_check_pool L:755 > 
  MEMBUSTER: pool MW_HTTP_POOL, count 1 size 520, bt(007aff90 00ef5c30 007afe08 00795b3c 006e1a60 00ef52d4 )
...


={============================================================================
*kt_linux_arch_001* arch-backtrace

https://eli.thegreenplace.net/2015/programmatic-access-to-the-call-stack-in-c/

Programmatic access to the call stack in C++

July 15, 2015 at 05:33 Tags Debuggers , C & C++

Sometimes when working on a large project, I find it useful to figure out all
the places from which some function or method is called. Moreover, more often
than not I don't just want the immediate caller, but the whole call stack.
This is most useful in two scenarios - when debugging and when trying to
figure out how some code works.

One possible solution is to use a debugger - run the program within a
debugger, place a breakpoint in the interesting place, examine call stack when
stopped. While this works and can sometimes be very useful, I personally
prefer a more `programmatic approach.` 

I want to change the code in a way that will print out the call stack in every
place I find interesting. Then I can use grepping and more sophisticated tools
to analyze the call logs and thus gain a better understanding of the workings
of some piece of code.

In this post, I want to present a relatively simple method to do this. It's
aimed mainly at Linux, but should work with little modification on other
Unixes (including OS X).


Obtaining the backtrace - libunwind

I'm aware of three reasonably well-known methods of accessing the call stack
programmatically:

o The gcc builtin macro __builtin_return_address: very crude, low-level
approach. This obtains the return address of the function on each frame on the
stack. Note: just the address, not the function name. So extra processing is
required to obtain the function name.

o glibc's backtrace and backtrace_symbols: can obtain the actual symbol names
for the functions on the call stack.

o libunwind

Between the three, I strongly prefer libunwind, as it's the most modern,
        widespread and portable solution. It's also more flexible than
          backtrace, being able to provide extra information such as values of
          CPU registers at each stack frame.

Moreover, in the zoo of system programming, libunwind is the closest to the
"official word" you can get these days. For example, gcc can use libunwind for
implementing zero-cost C++ exceptions (which requires stack unwinding when an
    exception is actually thrown) [2]. LLVM also has a re-implementation of
the libunwind interface in libc++, which is used for unwinding in LLVM
toolchains based on this library.

[1] AFAIK, gcc indeed uses libunwind by default on some architectures, though
it uses an alternative unwinder on others. Please correct me if I'm missing
  something here.


Code sample

Here's a complete code sample for using libunwind to obtain the backtrace from
an arbitrary point in the execution of a program. Refer to the libunwind
documentation for more details about the API functions invoked here:

http://www.nongnu.org/libunwind/docs.html

#define UNW_LOCAL_ONLY
#include <libunwind.h>
#include <stdio.h>

// Call this function to get a backtrace.
void backtrace() {
  unw_cursor_t cursor;
  unw_context_t context;

  // Initialize cursor to current frame for local unwinding.
  unw_getcontext(&context);
  unw_init_local(&cursor, &context);

  // Unwind frames one by one, going up the frame stack.
  while (unw_step(&cursor) > 0) {
    unw_word_t offset, pc;
    unw_get_reg(&cursor, UNW_REG_IP, &pc);
    if (pc == 0) {
      break;
    }
    printf("0x%lx:", pc);

    char sym[256];
    if (unw_get_proc_name(&cursor, sym, sizeof(sym), &offset) == 0) {
      printf(" (%s+0x%lx)\n", sym, offset);
    } else {
      printf(" -- error: unable to obtain symbol name for this frame\n");
    }
  }
}

void foo() {
  backtrace(); // <-------- backtrace here!
}

void bar() {
  foo();
}

int main(int argc, char **argv) {
  bar();

  return 0;
}


libunwind is easy to install from source or as a package. I just built it from
source with the usual configure, make and make install sequence and placed it
into /usr/local/lib.

// git clone git://git.sv.gnu.org/libunwind.git
// 
// * General Build Instructions
// 
// In general, this library can be built and installed with the following
// commands:
// 
//         $ ./autogen.sh # Needed only for building from git. Depends on libtool.
//         $ ./configure
//         $ make
//         $ make install prefix=PREFIX

Once you have libunwind installed in a place the compiler can find [2],
     compile the code snippet with:

[2] If your libunwind is in a non-standard location, you'll need to provide
additional -I and -L flags.

gcc -o libunwind_backtrace -Wall -g libunwind_backtrace.c -lunwind

Finally, run:

kyoupark@kit-debian64:~/git/kb/code-linux/ex_backtrace$ 
LD_LIBRARY_PATH=/usr/local/lib ./unwind_c_out
0x40094e: (foo+0xe)
0x40095e: (bar+0xe)
0x400979: (main+0x19)
0x7f6bbb661b45: (__libc_start_main+0xf5)
0x400789: (_start+0x29)


0000000000400940 <foo>:
foo():
/home/kyoupark/git/kb/code-linux/ex_backtrace/unwind_c.c:32
  400940:	55                   	push   rbp
  400941:	48 89 e5             	mov    rbp,rsp
/home/kyoupark/git/kb/code-linux/ex_backtrace/unwind_c.c:33
  400944:	b8 00 00 00 00       	mov    eax,0x0
  400949:	e8 08 ff ff ff       	call   400856 <backtrace>
/home/kyoupark/git/kb/code-linux/ex_backtrace/unwind_c.c:34
  40094e:	5d                   	pop    rbp
  40094f:	c3                   	ret    

0000000000400950 <bar>:
bar():
/home/kyoupark/git/kb/code-linux/ex_backtrace/unwind_c.c:36
  400950:	55                   	push   rbp
  400951:	48 89 e5             	mov    rbp,rsp
/home/kyoupark/git/kb/code-linux/ex_backtrace/unwind_c.c:37
  400954:	b8 00 00 00 00       	mov    eax,0x0
  400959:	e8 e2 ff ff ff       	call   400940 <foo>
/home/kyoupark/git/kb/code-linux/ex_backtrace/unwind_c.c:38
  40095e:	5d                   	pop    rbp
  40095f:	c3                   	ret    

note that this is the case when build it with gcc but not g++

So we get the complete call stack at the point where backtrace is called. We
can obtain the function symbol names and the address of the instruction where
the call was made (more precisely, the `return address` which is the next
    instruction).

Sometimes, however, we want not only the caller's name, but also the call
location (source file name + line number). This is useful when one function
calls another from multiple locations and we want to pinpoint which one is
actually part of a given call stack. libunwind gives us the call address, but
nothing beyond. Fortunately, it's all in the DWARF information of the binary,
        and given the address we can extract the exact call location in a
        number of ways. The simplest is probably to call addr2line:

$ addr2line 0x400968 -e libunwind_backtrace
libunwind_backtrace.c:37

We pass the PC address to the left of the bar frame to addr2line and get the
file name and line number.

Alternatively, we can use the dwarf_decode_address example from pyelftools to
obtain the same information:

$ python <path>/dwarf_decode_address.py 0x400968 libunwind_backtrace
Processing file: libunwind_backtrace
Function: bar
File: libunwind_backtrace.c
Line: 37

If printing out the exact locations is important for you during the backtrace
call, you can also go fully programmatic by using `libdwarf` to open the
executable and read this information from it, in the backtrace call. There's a
section and a code sample about a very similar task in my blog post on
debuggers.


C++ and mangled function names

The code sample above works well, but these days one is most likely writing
C++ code and not C, so there's a slight problem. In C++, names of functions
and methods are mangled. This is essential to make C++ features like function
overloading, namespaces and templates work. Let's say the actual call sequence
is:

namespace ns {

template <typename T, typename U>
void foo(T t, U u) {
  backtrace(); // <-------- backtrace here!
}

}  // namespace ns

template <typename T>
struct Klass {
  T t;
  void bar() {
    ns::foo(t, true);
  }
};

int main(int argc, char** argv) {
  Klass<double> k;
  k.bar();

  return 0;
}

The backtrace printed will then be:

0x400b3d: (_ZN2ns3fooIdbEEvT_T0_+0x17)
0x400b24: (_ZN5KlassIdE3barEv+0x26)
0x400af6: (main+0x1b)
0x7fc02c0c4ec5: (__libc_start_main+0xf5)
0x4008b9: (_start+0x29)

Oops, that's not nice. While some seasoned C++ veterans can usually make sense
of simple mangled names (kinda like system programmers who can read text from
    hex ASCII), when the code is heavily templated this can get ugly very
quickly.

One solution is to use a command-line tool - c++filt:

$ c++filt _ZN2ns3fooIdbEEvT_T0_
void ns::foo<double, bool>(double, bool)

However, it would be nicer if our backtrace dumper would print the demangled
name directly. Luckily, this is pretty easy to do, using the cxxabi.h API
that's part of libstdc++ (more precisely, libsupc++). libc++ also provides it
in the low-level libc++abi. `All we need to do is call abi::__cxa_demangle.`

Here's a complete example:

#define UNW_LOCAL_ONLY
#include <cxxabi.h>
#include <libunwind.h>
#include <cstdio>
#include <cstdlib>

void backtrace() {
  unw_cursor_t cursor;
  unw_context_t context;

  // Initialize cursor to current frame for local unwinding.
  unw_getcontext(&context);
  unw_init_local(&cursor, &context);

  // Unwind frames one by one, going up the frame stack.
  while (unw_step(&cursor) > 0) {
    unw_word_t offset, pc;
    unw_get_reg(&cursor, UNW_REG_IP, &pc);
    if (pc == 0) {
      break;
    }
    std::printf("0x%lx:", pc);

    char sym[256];
    if (unw_get_proc_name(&cursor, sym, sizeof(sym), &offset) == 0) {

      // {
      char* nameptr = sym;
      int status;
      char* demangled = abi::__cxa_demangle(sym, nullptr, nullptr, &status);
      if (status == 0) {
        nameptr = demangled;
      }
      std::printf(" (%s+0x%lx)\n", nameptr, offset);
      std::free(demangled);
      // }

    } else {
      std::printf(" -- error: unable to obtain symbol name for this frame\n");
    }
  }
}

namespace ns {

template <typename T, typename U>
void foo(T t, U u) {
  backtrace(); // <-------- backtrace here!
}

}  // namespace ns

template <typename T>
struct Klass {
  T t;
  void bar() {
    ns::foo(t, true);
  }
};

int main(int argc, char** argv) {
  Klass<double> k;
  k.bar();

  return 0;
}

This time, the backtrace is printed with all names nicely demangled:

kyoupark@kit-debian64:~/git/kb/code-linux/ex_backtrace$ 
LD_LIBRARY_PATH=/usr/local/lib ./unwind_cpp_out

0x400a4b: (foo()+0x9)
0x400a56: (bar()+0x9)
0x400a6c: (main+0x14)
0x7f9a3cc0fb45: (__libc_start_main+0xf5)
0x400849: (_start+0x29)


<gcc-builtin>
https://gcc.gnu.org/onlinedocs/gcc/Return-Address.html

6.49 Getting the Return or Frame Address of a Function

These functions may be used to get information about the callers of a function.

Built-in Function: void * __builtin_return_address (unsigned int level)

This function returns the return address of the current function, or of one of
its callers. The level argument is number of frames to scan up the call stack.

`A value of 0 yields the return address of the current function`, a value of 1
yields the return address of the caller of the current function, and so forth.
When inlining the expected behavior is that the function returns the address
of the function that is returned to. To work around this behavior use the
noinline function attribute.

The level argument must be a constant integer.

On some machines it may be impossible to determine the return address of any
function other than the current one; in such cases, or when the top of the
stack has been reached, this function returns 0 or a random value. In
addition, __builtin_frame_address may be used to determine if the top of the
stack has been reached.

Additional post-processing of the returned value may be needed, see
__builtin_extract_return_addr.

Calling this function with a nonzero argument can have unpredictable effects,
        including crashing the calling program. As a result, calls that are
        considered unsafe are diagnosed when the -Wframe-address option is in
        effect. Such calls should only be made in debugging situations.


Built-in Function: void * __builtin_extract_return_addr (void *addr)

The address as returned by __builtin_return_address may have to be fed through
this function to get the actual encoded address. For example, on the 31-bit
S/390 platform the highest bit has to be masked out, or on SPARC platforms an
offset has to be added for the true next instruction to be executed.

If no fixup is needed, this function simply passes through addr.

Built-in Function: void * __builtin_frob_return_address (void *addr)

This function does the reverse of __builtin_extract_return_addr.


Built-in Function: void * __builtin_frame_address (unsigned int level)

This function is similar to __builtin_return_address, but it returns the
address of the function frame rather than the return address of the function.
Calling __builtin_frame_address with a value of 0 yields the frame address of
the current function, a value of 1 yields the frame address of the caller of
the current function, and so forth.

*x86-asm-ebp*
The frame is the area on the stack that holds local variables and saved
registers. The frame address is normally the address of the first word pushed
on to the stack by the function. However, the exact definition depends upon
the processor and the calling convention. If the processor has a dedicated
frame pointer register, and the function has a frame, then
__builtin_frame_address returns the value of the frame pointer register.

On some machines it may be impossible to determine the frame address of any
function other than the current one; in such cases, or when the top of the
stack has been reached, this function returns 0 if the first frame pointer is
properly initialized by the startup code.

Calling this function with a nonzero argument can have unpredictable effects,
        including crashing the calling program. As a result, calls that are
        considered unsafe are diagnosed when the -Wframe-address option is in
        effect. Such calls should only be made in debugging situations.


<ex>
`regarding symbol resolution`, the current glibc (version 2.3.1 at the time of
    this writing) allows users to obtain the function name and offset only on
systems based on the ELF binary format. Furthermore, static symbols' names
cannot be resolved internally, because they cannot be accessed by the dynamic
linking facilities. In this case, the external command `addr2line` can be used
instead.

*gcc-builtin*
The implementation of backtrace() in the glibc library contains
platform-specific code for each platform, which is based either on GCC
internal variables (__builtin_frame_address and __builtin_return_address) or
on assembly code.

you still have to resolve the return addresses into function names, an
operation dependent on the binary format you are using. In the case of ELF, it
is performed by using a dynamic linker internal function (_dl_addr(), see
    glibc-x.x.x/sysdeps/generic/elf/backtracesyms.c).

The key one is backtrace(), which navigates the stack frames from the calling
point to the beginning of the program and provides an array of return
addresses.

use backtrace_symbols(). This function transforms a list of return addresses,
as returned by backtrace(), into a list of strings, each containing the
  function name offset within the function and the return address.

backtrace_symbols() internally calls malloc() and, thus, can fail if the
memory heap is corrupted--which might be the case if you are dealing with a
fault signal handler. If you need to resolve the return addresses in such a
situation, calling backtrace_symbols_fd() is safer, because it directly writes
to the given file descriptor without allocating memory. The same reasoning
implies that it is safer to use either static or automatic (non dynamic)
storage space for the array passed to backtrace().

In order to convert an address to a function name, the last two functions rely
on symbol information `to be available inside the program itself.` To enable
this feature, compile your program with the -rdynamic option (see man dlopen
    for more details).

  -rdynamic
  Pass the flag ‘-export-dynamic’ to the ELF linker, on targets that support
  it. This instructs the linker to add all symbols, not only used ones, to the
  dynamic symbol table. This option is needed for some uses of dlopen or to
  allow obtaining backtraces from within a program.)


https://www.gnu.org/software/libc/manual/html_node/Backtraces.html

// backtrace from glibc

BACKTRACE(3)
Linux Programmer's Manual                                                                            BACKTRACE(3)

NAME
       backtrace, backtrace_symbols, backtrace_symbols_fd - support for
       application self-debugging

SYNOPSIS
       #include <execinfo.h>

       int backtrace(void **buffer, int size);

       char **backtrace_symbols(void *const *buffer, int size);

       void backtrace_symbols_fd(void *const *buffer, int size, int fd);


<ex>
// Here's an example program that installs a SIGSEGV handler and prints a
// stacktrace to stderr when it segfaults.

#include <stdio.h>
#include <execinfo.h>
#include <signal.h>
#include <stdlib.h>


void handler(int sig) {
  void *array[10];
  size_t size;

  // get void*'s for all entries on the stack
  size = backtrace(array, 10);

  // print out all the frames to stderr
  fprintf(stderr, "Error: signal %d:\n", sig);
  backtrace_symbols_fd(array, size, 2);
  exit(1);
}

void baz() {
 int *foo = (int*)-1; // make a bad pointer
  printf("%d\n", *foo);       // causes segfault
}

void bar() { baz(); }
void foo() { bar(); }

int main(int argc, char **argv) {
  signal(SIGSEGV, handler);   // install our handler
  foo(); // this will call foo, bar, and baz.  baz segfaults.
}


// on x86_64 VM and note that do not show function name

$ gcc -g t_backtrace.c
$ ./a.out
Error: signal 11:
./a.out[0x4006d2]
/lib/x86_64-linux-gnu/libc.so.6(+0x350e0)[0x7ff5b192b0e0]
./a.out[0x40072b]
./a.out[0x40074e]
./a.out[0x40075e]
./a.out[0x400788]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf5)[0x7ff5b1917b45]
./a.out[0x4005e9]


<ex>
do not build on x86_64 VM and do not print names when changed it to use rip
for x86_64

#ifndef _GNU_SOURCE
#define _GNU_SOURCE
#endif
#ifndef __USE_GNU
#define __USE_GNU
#endif

#include <execinfo.h>
#include <signal.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <ucontext.h>
#include <unistd.h>

/* This structure mirrors the one found in /usr/include/asm/ucontext.h */
typedef struct _sig_ucontext {
 unsigned long     uc_flags;
 struct ucontext   *uc_link;
 stack_t           uc_stack;
 struct sigcontext uc_mcontext;
 sigset_t          uc_sigmask;
} sig_ucontext_t;

void crit_err_hdlr(int sig_num, siginfo_t * info, void * ucontext)
{
  void *             array[50];
  void *             caller_address;
  char **            messages;
  int                size, i;
  sig_ucontext_t *   uc;

  uc = (sig_ucontext_t *)ucontext;

  // Get the address at the time the signal was raised from the EIP (x86)

  // btrace_name.c:36:44: error: ‘struct sigcontext’ has no member named ‘eip’
  // caller_address = (void *) uc->uc_mcontext.eip;
  //                                           ^
  // caller_address = (void *) uc->uc_mcontext.rip;   

  caller_address = (void *) uc->uc_mcontext.eip;   

  fprintf(stderr, "signal %d (%s), address is %p from %p\n", 
      sig_num, strsignal(sig_num), info->si_addr, 
      (void *)caller_address);

  size = backtrace(array, 50);

  /* overwrite sigaction with caller's address */
  array[1] = caller_address;

  messages = backtrace_symbols(array, size);

  /* skip first stack frame (points here) */
  for (i = 1; i < size && messages != NULL; ++i)
  {
    fprintf(stderr, "[bt]: (%d) %s\n", i, messages[i]);
  }

  free(messages);

  exit(EXIT_FAILURE);
}

int crash()
{
 char * p = NULL;
 *p = 0;
 return 0;
}

int foo4()
{
 crash();
 return 0;
}

int foo3()
{
 foo4();
 return 0;
}

int foo2()
{
 foo3();
 return 0;
}

int foo1()
{
 foo2();
 return 0;
}

int main(int argc, char ** argv)
{
 struct sigaction sigact;

 sigact.sa_sigaction = crit_err_hdlr;
 sigact.sa_flags = SA_RESTART | SA_SIGINFO;

 if (sigaction(SIGSEGV, &sigact, (struct sigaction *)NULL) != 0)
 {
  fprintf(stderr, "error setting signal handler for %d (%s)\n",
    SIGSEGV, strsignal(SIGSEGV));

  exit(EXIT_FAILURE);
 }

 foo1();

 exit(EXIT_SUCCESS);
}


// output on 32?
signal 11 (Segmentation fault), address is (nil) from 0x8c50
[bt]: (1) ./test(crash+0x24) [0x8c50]
[bt]: (2) ./test(foo4+0x10) [0x8c70]
[bt]: (3) ./test(foo3+0x10) [0x8c8c]
[bt]: (4) ./test(foo2+0x10) [0x8ca8]
[bt]: (5) ./test(foo1+0x10) [0x8cc4]
[bt]: (6) ./test(main+0x74) [0x8d44]
[bt]: (7) /lib/libc.so.6(__libc_start_main+0xa8) [0x40032e44]

// output on 64
signal 11 (Segmentation fault), address is (nil) from 0x400926
[bt]: (1) ./btrace_name_out() [0x400926]
[bt]: (2) ./btrace_name_out() [0x400926]
[bt]: (3) ./btrace_name_out() [0x400939]
[bt]: (4) ./btrace_name_out() [0x400949]
[bt]: (5) ./btrace_name_out() [0x400959]
[bt]: (6) ./btrace_name_out() [0x400969]
[bt]: (7) ./btrace_name_out() [0x4009f9]
[bt]: (8) /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf5) [0x7f8f8c623b45]
[bt]: (9) ./btrace_name_out() [0x400729]


<ex>
The problem is that there is no backtrace.c for mips.

./uClibc-ng-1.0.17/libubacktrace/arm/backtrace.c
./uClibc-ng-1.0.17/libubacktrace/backtrace.c
./uClibc-0.9.33.2/libubacktrace/arm/backtrace.c
./uClibc-0.9.33.2/libubacktrace/backtrace.c


./uClibc-0.9.33.2/libubacktrace/arm/backtrace.c

/*
 * Perform stack unwinding by using the _Unwind_Backtrace.
 *
 */
int backtrace (void **array, int size);

// _Unwind_Backtrace comes from the gcc.

static void backtrace_init (void)
{
	void *handle = dlopen ("libgcc_s.so.1", RTLD_LAZY);
	if (handle == NULL
		|| ((unwind_backtrace = dlsym (handle, "_Unwind_Backtrace")) == NULL)
		|| ((unwind_vrs_get = dlsym (handle, "_Unwind_VRS_Get")) == NULL)) {
		printf("libgcc_s.so.1 must be installed for backtrace to work\n");
		abort();
	}
}


<ex>
In running gst-launch, see the call traces made automatically as shown.

Caught SIGSEGV
#0  0x7753cd1c in __lll_lock_wait () from /lib/libpthread.so.0
#1  0x775446fc in pthread_mutex_lock () from /lib/libpthread.so.0
#2  0x772c4fcc in g_mutex_lock () from /opt/zinc-trunk/oss/lib/libglib-2.0.so.0
#3  0x75b6f2ac in gst_nexus_mgr_set_rate (mgr=0x48e0b0, settings=0x47ff18, res=0x0, rate=1) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexusmgr.c:1097
#4  0x75b74aa0 in gst_nexus_sink_play_locked (sink=0x47fd18) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexussink.c:1531
#5  gst_nexus_sink_change_state_locked (transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING, sink=0x47fd18) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexussink.c:1354
#6  gst_nexus_sink_change_state (element=<optimized out>, transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexussink.c:1326
#7  0x77464ed8 in gst_element_change_state (element=0x47fd18, transition=<optimized out>) at gstelement.c:2602
#8  0x77465aac in gst_element_set_state_func (element=0x47fd18, state=<optimized out>) at gstelement.c:2558
#9  0x7743a954 in gst_bin_element_set_state (next=GST_STATE_PLAYING, current=GST_STATE_PAUSED, start_time=0, base_time=592515640814054, element=0x47fd18, bin=0x489078) at gstbin.c:2328
#10 gst_bin_change_state_func (element=0x489078, transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING) at gstbin.c:2665
#11 0x77490e24 in gst_pipeline_change_state (element=0x489078, transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING) at gstpipeline.c:474
#12 0x77464ed8 in gst_element_change_state (element=0x489078, transition=<optimized out>) at gstelement.c:2602
#13 0x77465aac in gst_element_set_state_func (element=0x489078, state=<optimized out>) at gstelement.c:2558
#14 0x00404c5c in main (argc=28, argv=0x7f9c2c94) at gst-launch.c:1095
Spinning.  Please run 'gdb gst-launch-1.0 15054' to continue debugging, Ctrl-C
to quit, or Ctrl-\ to dump core.


gstreamer-1.5.0/tools/gst-launch.c

static void
fault_handler_sighandler (int signum)
{
  fault_restore ();

  /* printf is used instead of g_print(), since it's less likely to
   * deadlock */
  switch (signum) {
    case SIGSEGV:
      fprintf (stderr, "Caught SIGSEGV\n");
      break;
    case SIGQUIT:
      if (!quiet)
        printf ("Caught SIGQUIT\n");
      break;
    default:
      fprintf (stderr, "signo:  %d\n", signum);
      break;
  }

  fault_spin ();
}

// tools/gst-launch.c
static void
fault_spin (void)
{
  int spinning = TRUE;

  glib_on_error_halt = FALSE;
  g_on_error_stack_trace ("gst-launch-" GST_API_VERSION);

  wait (NULL);

  /* FIXME how do we know if we were run by libtool? */
  fprintf (stderr,
      "Spinning.  Please run 'gdb gst-launch-" GST_API_VERSION " %d' to "
      "continue debugging, Ctrl-C to quit, or Ctrl-\\ to dump core.\n",
      (gint) getpid ());
  while (spinning)
    g_usleep (1000000);
}

static void
fault_restore (void)
{
  struct sigaction action;

  memset (&action, 0, sizeof (action));
  action.sa_handler = SIG_DFL;

  sigaction (SIGSEGV, &action, NULL);
  sigaction (SIGQUIT, &action, NULL);
}

static void
fault_setup (void)
{
  struct sigaction action;

  memset (&action, 0, sizeof (action));
  action.sa_handler = fault_handler_sighandler;

  sigaction (SIGSEGV, &action, NULL);
  sigaction (SIGQUIT, &action, NULL);
}

int
main (int argc, char *argv[])
{
  ...

#ifdef G_OS_UNIX
    fault_setup ();
#endif

}

https://developer.gnome.org/glib/stable/glib-Warnings-and-Assertions.html#g-on-error-stack-trace

void
g_on_error_stack_trace (const gchar *prg_name);

Invokes gdb, which attaches to the current process and shows a stack trace.
Called by g_on_error_query() when the "[S]tack trace" option is selected. You
can get the current process's program name with g_get_prgname(), assuming that
you have called gtk_init() or gdk_init().  This function may cause different
actions on non-UNIX platforms.

Parameters
prg_name
the program name, needed by gdb for the "[S]tack trace" option


={============================================================================
*kt_linux_arch_001* arch-mips-for-and-while

<asm-while-loop> in C
int main()
{
  int i = 0;

  while( i < 10 )
  {
    printf("w iter %d\n", i );
    ++i;
  }

  return 0;
}

#	when use -g (debug)
#
int main()
{
  400590:       27bdffd8        addiu   sp,sp,-40	     # push	:
  400594:       afbf0024        sw      ra,36(sp)	     # {copy-return-addr-into-a-stack}
  400598:       afbe0020        sw      s8,32(sp)	     # save s8(saved reg) into a stack. 
                                                        # s8 has one of s0-s7 that has values are preserved across function calls.
  40059c:       03a0f021        move    s8,sp	        # save sp to s8.
        int i = 0;
  4005a0:       afc00018        sw      zero,24(s8)     # int i = 0. save into a stack.			

        while( i < 10 )
  4005a4:       08100173        j       4005cc <main+0x3c> # { jump
  4005a8:       00000000        nop
        {
                printf("w iter %d\n", i );

{4005ac}:       3c020040        lui     v0,0x40		          # -> v0 = v0 << 16.
  4005b0:       244406a0        addiu   a0,v0,1696	          # a0 = v0 + 1696. a0-a3 used for passing args to functions.
  4005b4:       8fc50018        lw      a1,24(s8)			    # a1 = int i. 
  4005b8:       0c100124        jal     400490 <printf@plt>  # }} jump and link.
  4005bc:       00000000        nop
                ++i;

  4005c0:       8fc20018        lw      v0,24(s8)	    # v0 = int i.
  4005c4:       24420001        addiu   v0,v0,1		    # v0 = int i +1. inc i.
  4005c8:       afc20018        sw      v0,24(s8)	    # int i = v0.

{4005cc}:       8fc20018        lw      v0,24(s8)	    # -> v0 = int i 
  4005d0:       2842000a        slti    v0,v0,10	    # i < 10. 
  # slti(set on less than immediate. tests if one register is less than a constant.)
  4005d4:       1440fff5        bnez    v0,4005ac <main+0x1c>	
  # if( v0 != 0 ) then jump. bne(branch on not equal). v0 is 1(true).
  4005d8:       00000000        nop                    # }
        }

        return 0;
  4005dc:       00001021        move    v0,zero		# {copy-return-value}
}
  4005e0:       03c0e821        move    sp,s8
  4005e4:       8fbf0024        lw      ra,36(sp)
  4005e8:       8fbe0020        lw      s8,32(sp)
  4005ec:       27bd0028        addiu   sp,sp,40
  4005f0:       03e00008        jr      ra
  4005f4:       00000000        nop
  
#	when use -Os
#  
00400590 <main>:
  400590:       27bdffd8        addiu   sp,sp,-40						# push. dec sp -40.
  400594:       3c020040        lui     v0,0x40							# v0 = v0 << 16.
  400598:       afb1001c        sw      s1,28(sp)						# save arg
  40059c:       afb00018        sw      s0,24(sp)						# save arg
  4005a0:       afbf0020        sw      ra,32(sp)						# save ra
  4005a4:       24510678        addiu   s1,v0,1656						# a1 = v0 + 1656.
  4005a8:       00008021        move    s0,zero							# s0 = 0. s0(int i) is free to use.
  4005ac:       02002821        move    a1,s0							# a1 = s0.
  																						# {
  4005b0:       0c100124        jal     400490 <printf@plt>			# call printf. as int i is 0 to start
  4005b4:       02202021        move    a0,s1							# a0 = s1
  4005b8:       26100001        addiu   s0,s0,1							# s0 = s0+1. 
  4005bc:       2402000a        li      v0,10							# v0 = 10. 
  4005c0:       1602fffb        bne     s0,v0,4005b0 <main+0x20>	# if( s0 != v0 )then jump.
                                                                  # }
  4005c4:       02002821        move    a1,s0							# a1 = s0(int i). exit while
  4005c8:       8fbf0020        lw      ra,32(sp)						# pop. recover ra.
  4005cc:       8fb1001c        lw      s1,28(sp)
  4005d0:       8fb00018        lw      s0,24(sp)
  4005d4:       00001021        move    v0,zero							# return 0.
  4005d8:       03e00008        jr      ra
  4005dc:       27bd0028        addiu   sp,sp,40


<asm-for-loop>
#include <stdio.h>

int main()
{
  int i = 0;

  for(i; i < 10; i++)
    printf("w iter %d\n", i );

  return 0;
}

# when use -Os. same as while case
#
00400590 <main>:
  400590:       27bdffd8        addiu   sp,sp,-40
  400594:       3c020040        lui     v0,0x40
  400598:       afb1001c        sw      s1,28(sp)
  40059c:       afb00018        sw      s0,24(sp)
  4005a0:       afbf0020        sw      ra,32(sp)
  4005a4:       24510678        addiu   s1,v0,1656
  4005a8:       00008021        move    s0,zero
  4005ac:       02002821        move    a1,s0
                                                             # {
  4005b0:       0c100124        jal     400490 <printf@plt>
  4005b4:       02202021        move    a0,s1
  4005b8:       26100001        addiu   s0,s0,1
  4005bc:       2402000a        li      v0,10
  4005c0:       1602fffb        bne     s0,v0,4005b0 <main+0x20>
                                                             # }
  4005c4:       02002821        move    a1,s0
  4005c8:       8fbf0020        lw      ra,32(sp)
  4005cc:       8fb1001c        lw      s1,28(sp)
  4005d0:       8fb00018        lw      s0,24(sp)
  4005d4:       00001021        move    v0,zero
  4005d8:       03e00008        jr      ra
  4005dc:       27bd0028        addiu   sp,sp,40


={============================================================================
*kt_linux_arch_001* arch-mips-power-on-reset

POR(power on reset) (MIPS)

when POR, the start address by reset vector is 0xBFC0 0000

exception vector locations.

the reset, soft reset, and NMI exceptions are always vectored to location
0xBFC0_0000.  debug exceptions are vectored to location 0xBFC0_0480 or to
location 0xFF20_0200 if the ProbEn bit is 0 or 1, respectively, itn eht EJTAG
control register(ECR).


={============================================================================
*kt_linux_arch_001* arch-mips-qemu tool-qemu

QEMU is a generic open source processor and system emulator. It achieves good
performance from using a Just-in-time compilation. 

https://www.linux-mips.org/wiki/QEMU

$ qemu-mips -L $SYSROOT -R 0 ./a.out 
$ qemu-mipsel -cpu 74Kf -E LD_LIBRARY_PATH=/usr/mipsel-linux-gnu/lib/ a.out


1.1 Features

QEMU has two operating modes:

Full system emulation. 

In this mode, QEMU emulates a full system (for example a PC), including one or
several processors and various peripherals. It can be used to launch different
Operating Systems without rebooting the PC or to debug system code.

has the following features:

* QEMU uses a full software MMU for maximum portability.
* QEMU can optionally use an in-kernel accelerator, like kvm. The accelerators
  execute most of the guest code natively, while continuing to emulate the
  rest of the machine.

* Various hardware devices can be emulated and in some cases, host devices
  (e.g. serial and parallel ports, USB, drives) can be used transparently by
  the guest Operating System. Host device passthrough can be used for talking
  to external physical peripherals (e.g. a webcam, modem or tape drive).

* Symmetric multiprocessing (SMP) support. Currently, an in-kernel accelerator
  is required to use more than one host CPU for emulation.


User mode emulation. 

In this mode, QEMU can launch processes compiled for one CPU on another CPU.
It can be used to launch the Wine Windows API emulator (http://www.winehq.org)
or to ease cross-compilation and cross-debugging.

has the following features:

* Generic Linux system call converter, including most ioctls.
* clone() emulation using native CPU clone() to use Linux scheduler for threads.
* Accurate signal handling by remapping host signals to target signals.


2.3.1 Standard options

-cpu model
Select CPU model (-cpu help for list and additional feature selection)

$ qemu-mips -cpu help

$ qemu-mips ./minit-static
$ qemu-mips -E LD_LIBRARY_PATH=/home/kyoupark/test_qemu/glibc-libs/lib minit-static


4.3 Linux User space emulator

4.3.1 Quick Start

In order to launch a Linux process, QEMU needs the process executable itself
and all the target (x86) dynamic libraries used by it.

4.3.3 Command line options

-g port
Wait gdb connection to port


={============================================================================
*kt_linux_arch_001* arch-x86-stack

https://eli.thegreenplace.net/2011/02/04/where-the-top-of-the-stack-is-on-x86/
Where the top of the stack is on x86

I've noticed more than once that some programmers are confused about the
direction in which the stack grows on x86, and what "top of the stack" and
"bottom of the stack" mean. It appears that this confusion is caused by a
basic mismatch in the way people are used to thinking about stacks, and in the
way the stack on x86 actually behaves [1].

// [1] It doesn't help that some online resources mistakenly call the top of
// the stack "bottom". The version presented here is the correct one of x86,
// since it relies on terminology defined in Intel's x86 architecture manuals

In this article, I intend to resolve this confusion with a few helpful
diagrams.


The stack analogy

Back to the basics. The stack analogy is sometimes demonstrated to new
students of computing with a stack of plates. You push a plate onto the stack
and pop a plate off the stack. The top of the stack is where your next plate
goes when pushing, and from where you take a plate when popping.


Hardware stacks

In computers, the stack is usually a specially treated region of memory. In
the abstract sense, the analogy applies - you push data by placing it on the
top of the stack, and pop data by taking it from the top of the stack. Note
that this doesn't address `the issue of where the top of the stack is` located
in memory.


The stack in x86

Herein lies the source of the confusion. Intel's x86 architecture places its
stack "head down". It starts at some address and grows down to a lower
address. Here's how it looks:


So when we say "top of the stack" on x86, we actually mean the lowest address
in the memory area occupied by the stack. This may be unnatural for some
people [2]. As long as we keep the diagram shown above firmly in mind,
however, we should be OK.

// [2] You may try to fix the confusion by viewing memory with its low
// addresses at the top and high addresses at the bottom. While this would
// indeed make stack movement more natural, it would also mean that increasing
// some memory address would take it down in the graphical representation,
// which is probably even more counter-intuitive.

While we're at it, let's see how some common idioms of x86 assembly
programming map to this graphical representation.


Pushing and popping data with the stack pointer

*x86-asm-esp*
The x86 architecture reserves a special register for working with the stack -
ESP (Extended Stack Pointer). The ESP, by definition, always points to the top
of the stack:

// Whereas most of the registers have lost their special purposes in the
// modern instruction set, by convention, two are reserved for special
// purposes  the stack pointer (ESP) and the base pointer (EBP).

In this diagram, address 0x9080ABCC is the top of the stack. The word located
in it is some "foo" and ESP contains the address 0x9080ABCC - in other words,
points to it.

// [3] There are several instructions x86 defines in the "push family". I'm
// demonstrating push since it's the simplest and most generally applicable.

To push new data onto the stack we use the push instruction [3]. What push
does is first decrement esp by 4, and then store its operand in the location
esp points to. So this:

*x86-asm-push*
push eax

Is actually equivalent to this:

sub esp, 4
mov [esp], eax

Taking the previous diagram as the starting point, and supposing that eax held
the venerable value 0xDEADBEEF, after the push the stack will look as follows:

high
            |             |
0x9080ABCC  |   foo       |
0x9080ABC8  | 0xDEADBEEF  |   <- esp
            |             |
low


Similarly, the pop instruction takes a value off the top of stack and places
it in its operand, increasing the stack pointer afterwards. In other words,
this:

pop eax

Is equivalent to this:

mov eax, [esp]
add esp, 4

So, again, taking the previous diagram (after the push) as a starting point,
pop eax will do the following:

And the value 0xDEADBEEF will be written into eax. Note that 0xDEADBEEF also
stays at address 0x9080ABC8, since we did nothing to overwrite it yet.


Stack frames and calling conventions

When looking at assembly code generated from C, you will find a lot of
interesting patterns. Perhaps the most recognizable pattern is the way
parameters are passed into functions using the stack, and the way local
variables are allocated on the stack [4].

// [4] This only applies to some calling conventions and architectures, of
// course. In others, some parameters are passed in registers.

I'll demonstrate this with a simple C program:

int foobar(int a, int b, int c)
{
    int xx = a + 2;
    int yy = b + 3;
    int zz = c + 4;
    int sum = xx + yy + zz;

    return xx * yy * zz + sum;
}

int main()
{
    return foobar(77, 88, 99);
}

Both the arguments passed into foobar and the local variables of that
function, along with some other data, are going to be stored on the stack when
foobar is called. This set of data on the stack is called a frame for this
function. Right before the return statement, the stack frame for foobar looks
like this:

high

`epb +` 16  [       c         ]
epb + 12  [       b         ]
epb + 8   [       a         ]
ebp + 4   [ return address  ]
ebp       | saved ebp       | <- ebp
`epb -` 4   |       xx        |
epb - 8   |       yy        |
epb - 12  |       zz        |
ebp - 16  | sum             | <- esp

low

The green data, marked with [], were pushed onto the stack by the calling
function, `caller`, and the blue ones, ||, by foobar itself, `callee`.

Compiled with gcc into assembly as follows:

*gcc-asm* *x86-asm*
gcc -masm=intel -S z.c -o z.s

The following assembly listing is generated for foobar. I commented it heavily
for easy understanding:

_foobar:
    ; ebp must be preserved across calls. Since
    ; this function `modifies` it, it must be
    ; saved. save ebp of the previous frame
    ;
    push    ebp   // esp -= 4, [esp] = ebp 

    ; From now on, ebp points to the current stack
    ; frame of the function
    ;
    mov     ebp, esp

    ; Make space on the stack for local variables
    ;
    sub     esp, 16

    ; eax <-- a. eax += 2. then store eax in xx
    ;
    mov     eax, DWORD PTR [ebp+8]
    add     eax, 2
    mov     DWORD PTR [ebp-4], eax

    ; eax <-- b. eax += 3. then store eax in yy
    ;
    mov     eax, DWORD PTR [ebp+12]
    add     eax, 3
    mov     DWORD PTR [ebp-8], eax

    ; eax <-- c. eax += 4. then store eax in zz
    ;
    mov     eax, DWORD PTR [ebp+16]
    add     eax, 4
    mov     DWORD PTR [ebp-12], eax

    ; add xx + yy + zz and store it in sum
    ;
    mov     eax, DWORD PTR [ebp-8]
    mov     edx, DWORD PTR [ebp-4]
    lea     eax, [edx+eax]
    add     eax, DWORD PTR [ebp-12]
    mov     DWORD PTR [ebp-16], eax

    ; *x86-asm-return* *x86-asm-eax*
    ; EAX which was needed to return the result
    ; Compute final result into eax, which
    ; stays there until return
    ;
    mov     eax, DWORD PTR [ebp-4]
    imul    eax, DWORD PTR [ebp-8]
    imul    eax, DWORD PTR [ebp-12]
    add     eax, DWORD PTR [ebp-16]

    ; The leave instruction here is equivalent to:
    ;
    ;   mov esp, ebp
    ;   pop ebp     // ebp = [esp], esp += 4
    ;
    ; Which cleans the allocated locals and restores
    ; ebp.
    ;
    leave

    ; 8. Finally, we return to the caller by executing a ret instruction. This
    ; instruction will find and remove the appropriate return address from the
    ; stack.
    ret

Since esp keeps moving as the function executes, ebp (`base pointer`, also
    known as `frame pointer` in other architectures) is used as a convenient
anchor relatively to which all function arguments and locals can be found.
Arguments are above ebp in the stack (hence the positive offset when accessing
    them), while locals are below ebp in the stack.


*x86-asm-ebx*
http://www.cs.virginia.edu/~evans/cs216/guides/x86.html
https://aaronbloomfield.github.io/pdr/book/x86-32bit-ccc-chapter.pdf
https://aaronbloomfield.github.io/pdr/book/x86-64bit-ccc-chapter.pdf

3. Next, the values of any registers that are designated callee-saved that
will be used by the function must be saved. To save registers, push them onto
the stack. The callee-saved registers are EBX, EDI and ESI (ESP and EBP will
also be preserved by the call convention, but need not be pushed on the
stack during this step).

callee-saved mean?

Save the value of registers that the function will modify. This function uses
EDI and ESI.


<ex>
note that this function do not use leave and may be because this function do
not have local variables so don't need to increase esp?

0000043c <ml_func>:
 43c:   55                      push   ebp
 43d:   89 e5                   mov    ebp,esp
 43f:   e8 16 00 00 00          call   45a <__i686.get_pc_thunk.cx>
>`444`:   81 c1 b0 1b 00 00       add    ecx,`0x1bb0`
 44a:   8b 81 f0 ff ff ff       mov    eax,DWORD PTR [ecx-0x10]
 450:   8b 00                   mov    eax,DWORD PTR [eax]
 452:   03 45 08                add    eax,DWORD PTR [ebp+0x8]
 455:   03 45 0c                add    eax,DWORD PTR [ebp+0xc]
 458:   5d                      pop    ebp
 459:   c3                      ret


<arch-x86-64-stack>
https://eli.thegreenplace.net/2011/09/06/stack-frame-layout-on-x86-64

[1] This architecture goes by many names. Originated by AMD and dubbed AMD64,
  it was later implemented by Intel, which called it IA-32e, then EM64T and
  finally Intel 64. It's also being called x86-64. But I like the name x64 -
  it's nice and short.

Argument passing

I'm going to simplify the discussion here on purpose and focus on
integer/pointer arguments [3]. 

According to the ABI, the first 6 integer or pointer arguments to a function
are passed in registers. 

The first is placed in rdi, the second in rsi, the third in rdx, and then rcx,
    r8 and r9. Only the 7th argument and onwards are passed on the stack.


<x86-asm-return-value>
see use of eax after the call

41: result = call_two(a1_, a2_, 100);

/home/kyoupark/git/kb/asan/stack.c:41
 6ca:	83 ec 04             	sub    esp,0x4                ; -4, ebp-28

; 3 args + ra, 16 and call
 6cd:	6a 64                	push   0x64                   ; 100 
 6cf:	ff 75 f0             	push   DWORD PTR [ebp-0x10]   ; a2
 6d2:	ff 75 f4             	push   DWORD PTR [ebp-0xc]    ; a1
 6d5:	e8 e6 fe ff ff       	call   5c0 <call_two>
 6da:	83 c4 10             	add    esp,0x10               ; +16

; save return to result var
 6dd:	89 45 e8             	mov    DWORD PTR [ebp-0x18],eax   ; ebp-24 = eax


;  call void function and no use of eax
;  call_one(10, 20);

   0x08048824 <+63>:    call   0x804876f <call_one>
   0x08048829 <+68>:    add    esp,0x10
   0x0804882c <+71>:    sub    esp,0xc
   0x0804882f <+74>:    push   0x8048ae0
   0x08048834 <+79>:    call   0x80485a0 <puts@plt>

;  call int return function and use it
;  result = call_two(10, 20, 30);
;  printf("main: calls two result %ld\n", result );

   0x08048845 <+96>:    call   0x80486f4 <call_two>
   0x0804884a <+101>:   add    esp,0x10
   0x0804884d <+104>:   mov    DWORD PTR [ebp-0xc],eax


={============================================================================
*kt_linux_arch_001* arch-x86-asm

*gcc-asm*

3.17 Hardware Models and Configurations

-masm=dialect
Output assembly instructions using selected dialect. Supported choices are
‘intel’ or ‘att’ (the default). Darwin does not support ‘intel’.

-fverbose-asm
Put extra commentary information in the generated assembly code to make it
more readable. This option is generally only of use to those who actually need
to read the generated assembly code (perhaps while debugging the compiler
    itself).  ‘-fno-verbose-asm’, the default, causes the extra information to
be omitted and is useful when comparing two assembler files.


3.17.16 Intel 386 and AMD x86-64 Options

-m32
-m64
-mx32 

Generate code for a 32-bit or 64-bit environment. The ‘-m32’ option sets int,
long, and pointer types to 32 bits, and generates code that runs on any i386
  system.  The ‘-m64’ option sets int to 32 bits and long and pointer types to
  64 bits, and generates code for the x86-64 architecture. For Darwin only the
  ‘-m64’ option also turns off the ‘-fno-pic’ and ‘-mdynamic-no-pic’ options.
  The ‘-mx32’ option sets int, long, and pointer types to 32 bits, and
  generates code for the x86-64 architecture.


http://asm.sourceforge.net/howto/gas.html

<gcc-asm-att>
What is this AT&T syntax

Because GAS was invented to support a 32-bit unix compiler, it uses standard
AT&T syntax, which resembles a lot the syntax for standard m68k assemblers,
and is standard in the UNIX world. This syntax is neither worse, nor better
  than the Intel syntax. It's just different. When you get used to it, you
  find it much more regular than the Intel syntax, though a bit boring.

Here are the major caveats about GAS syntax:

`Register names are prefixed with %`, so that registers are %eax, %dl and so
on, instead of just eax, dl, etc. This makes it possible to include external C
symbols directly in assembly source, without any risk of confusion, or any
need for ugly underscore prefixes.


The `order of operands` is source(s) first, and destination last, as opposed
to the Intel convention of destination first and sources last. Hence, what in
Intel syntax is mov eax,edx (move contents of register edx into register eax)
will be in GAS syntax mov %edx,%eax.


The operand size is specified as a `suffix` to the instruction name. The suffix
is b for (8-bit) byte, w for (16-bit) word, and l for (32-bit) long. For
instance, the correct syntax for the above instruction would have been movl
%edx,%eax. However, gas does not require strict AT&T syntax, so the suffix is
optional when size can be guessed from register operands, and else defaults to
32-bit (with a warning).


Immediate operands are marked with a $ prefix, as in 

addl $5,%eax 
(add immediate long value 5 to register %eax).

// The `immediate value` is fixed (not variable), and is coded into the
// instruction itself. Immediate values are prefixed with $.


Missing operand prefix indicates that it is memory-contents; hence 

movl $foo,%eax 
puts the address of variable foo into register %eax, but 

movl foo,%eax 
puts the contents of variable foo into register %eax.


Indexing or `indirection` is done by enclosing the index register or indirection
memory cell address in parentheses, as in 

testb $0x80,17(%ebp) 

(test the high bit of the byte value at offset 17 from the cell pointed to by
 %ebp).


*x86-asm-addressing*
Addressing modes

When we write code that has loops, often one register holds the base address
of an array and another register holds the current index being processed.
Although it’s possible to manually compute the address of the element being
processed, the x86 ISA provides a more elegant solution – there are memory
addressing modes that let you add and multiply certain registers together.
This is probably easier to illustrate than describe:

movb (%eax,%ecx), %bh         // means bh = *(eax + ecx);.

movb -10(%eax,%ecx,4), %bh    // means bh = *(eax + (ecx * 4) - 10);.

The address format is offset(base, index, scale), where offset is an integer
constant (can be positive, negative, or zero), base and index are 32-bit
registers (but a few combinations are disallowed), and scale is either
{1,2,4,8}. For example if an array holds a series of 64-bit integers, we would
use scale = 8 because each element is 8 bytes long.

The memory addressing modes are valid wherever a memory operand is permitted.
Thus if you can write sbbl %eax, (%eax), then you can certainly write sbbl
%eax, (%eax,%eax,2) if you need the indexing capability. Also note that the
address being computed is a temporary value that is not saved in any register.
This is good because if you wanted to compute the address explicitly, you
would need to allocate a register for it, and having only 8 GPRs is rather
tight when you want to store other variables.

There is one special instruction that uses memory addressing but does not
actually access memory. The `leal (load effective address)` instruction
computes the final memory address according to the addressing mode, and stores
the result in a register. For example, 
    
leal 5(%eax,%ebx,8), %ecx     // means ecx = eax + ebx*8 + 5;. 

Note that this is entirely an arithmetic operation and does not involve
dereferencing a memory address.

http://www.cs.virginia.edu/~evans/cs216/guides/x86.html
Addressing Memory

Modern x86-compatible processors are capable of addressing up to 2**32 bytes
of memory: memory addresses are 32-bits wide. In the examples above, where we
used labels to refer to memory regions, these labels are actually replaced by
the assembler with 32-bit quantities that specify addresses in memory. In
addition to supporting referring to memory regions by labels (i.e. constant
    values), the x86 provides a flexible scheme for computing and referring to
memory addresses: up to two of the 32-bit registers and a 32-bit signed
constant can be added together to compute a memory address. One of the
registers can be optionally pre-multiplied by 2, 4, or 8.  The addressing
modes can be used with many x86 instructions (we'll describe them in the next
    section). Here we illustrate some examples using the mov instruction that
moves data between registers and memory. This instruction has two operands:
the first is the destination and the second specifies the source.  Some
examples of mov instructions using address computations are:

; Move the 4 bytes in memory at the address contained in EBX into EAX
mov eax, [ebx]	

; Move the contents of EBX into the 4 bytes at memory address var. (Note, var
; is a 32-bit constant).

mov [var], ebx	

; Move 4 bytes at memory address ESI + (-4) into EAX
mov eax, [esi-4]	

; Move the contents of CL into the byte at address ESI+EAX
mov [esi+eax], cl	

; Move the 4 bytes of data at address ESI+4*EBX into EDX
mov edx, [esi+4*ebx]    	


Some examples of invalid address calculations include:
mov eax, [ebx-ecx]	; Can only add register values
mov [eax+esi+edi], ebx    	; At most 2 registers in address computation


Size Directives

In general, the intended size of the of the data item at a given memory
address can be inferred from the assembly code instruction in which it is
referenced. For example, in all of the above instructions, the size of the
memory regions `could be inferred from the size of the register operand.` When
we were loading a 32-bit register, the assembler could infer that the region
of memory we were referring to was 4 bytes wide. When we were storing the
value of a one byte register to memory, the assembler could infer that we
wanted the address to refer to a single byte in memory. 

However, in some cases the size of a referred-to memory region is ambiguous.
Consider the instruction mov [ebx], 2. Should this instruction move the value
2 into the single byte at address EBX? Perhaps it should move the 32-bit
integer representation of 2 into the 4-bytes starting at address EBX. Since
either is a valid possible interpretation, the assembler must be explicitly
directed as to which is correct. The size directives BYTE PTR, WORD PTR, and
DWORD PTR serve this purpose, indicating sizes of 1, 2, and 4 bytes
respectively.

For example:

; Move 2 into the single byte at the address stored in EBX.
mov BYTE PTR [ebx], 2	

; Move the 16-bit integer representation of 2 into the 2 bytes starting at the
; address in EBX.

mov WORD PTR [ebx], 2	

; Move the 32-bit integer representation of 2 into the 4 bytes starting at the
; address in EBX.

mov DWORD PTR [ebx], 2    	


*x86-asm-flag*
https://www.nayuki.io/page/a-fundamental-introduction-to-x86-assembly-programming

4. Flags register and comparisons

There is a 32-bit register named eflags which is implicitly read or written in
many instructions. In other words, its value plays a role in the instruction
execution, but the register is not mentioned in the assembly code.

Arithmetic instructions such as addl usually update eflags based on the computed
result. The instruction would set or clear flags like carry (CF), overflow (OF),
  sign (SF), parity (PF), zero (ZF), etc. Some instructions read the flags – for
  example adcl adds two numbers and uses the carry flag as a third operand: adcl
  %ebx, %eax means eax = eax + ebx + cf;. Some instructions set a register based
  on a flag – for example setz %al sets the 8-bit register al to 0 if ZF is
  clear or 1 if ZF is set. Some instructions directly affect a single flag bit,
  such as cld clearing the direction flag (DF).


*x86-asm-test*  

test — Logical Compare

Computes the bit-wise logical AND of first operand (source 1 operand) and the
second operand (source 2 operand) and sets the SF, ZF, and PF status flags
according to the result. The result is then discarded.

; jump if cl is zero
test cl, cl   ; set ZF to 1 if cl == 0
je 0x804f430  ; jump if ZF == 1

instruction would set or clear flags like carry (CF), overflow (OF), sign
(SF), parity (PF), zero (ZF), etc.


cmp  Compare

Compare the values of the two specified operands, setting the condition codes
in the machine status word appropriately. This instruction is equivalent to
the sub instruction, except the result of the subtraction is discarded instead
of replacing the first operand.

Syntax
cmp <reg>,<reg>
cmp <reg>,<mem>
cmp <mem>,<reg>
cmp <reg>,<con>

Example
cmp DWORD PTR [var], 10
jeq loop

If the 4 bytes stored at location var are equal to the 4-byte integer constant
10, jump to the location labeled loop.


*x86-asm-jmp*
Although jmp is unconditional, it has sibling instructions that look at the
state of `eflags`, and either jumps to the label if the condition is met or
otherwise advances to the next instruction below. 

Conditional jump instructions include:  ja (jump if above), jle (jump if less
    than or equal), jo (jump if overflow), jnz (jump if non-zero), et cetera.
There are 16 of them in all, and some have synonyms – e.g. jz (jump if zero)
is the same as 

`je (jump if equal)`, 
ja (jump if above) is the same as jnbe (jump if not below or equal). 

An example of using conditional jump:

jc skip  /* If carry flag is on, then jump away */
/* Otherwise CF is off, then execute this stuff */
notl %eax
/* Implicitly fall into the next instruction */
skip:
adcl %eax, %eax

Label addresses are fixed in the code when it is compiled, but it is also
possible to jump to an arbitrary memory address computed at run time. In
particular, it is possible to jump to the value of a register: jmp *%ecx
essentially means to copy ecx’s value into eip, the instruction pointer
register.


Now is a perfect time to discuss a concept that was glossed over in section 1
about instructions and execution. Each instruction in assembly language is
ultimately translated into 1 to 15 bytes of machine code, and these machine
instructions are strung together to create an executable file. The CPU has a
32-bit register named `eip (extended instruction pointer)` which, during
program execution, holds the memory address of the current instruction being
executed.  Note that there are very few ways to read or write the eip
register, hence why it behaves very differently from the 8 main
general-purpose registers.  Whenever an instruction is executed, the CPU knows
how many bytes long it was, and advances eip by that amount so that it points
to the next instruction.


http://www.cs.virginia.edu/~evans/cs216/guides/x86.html

*x86-asm-registares-32*
               16       8    8
EAX       |       AX | AH | AL |
EBX       |       BX | BH | BL |
ECX       |       CX | CH | CL |
EDX       |       DX | DH | DL |
ESI       |          |    |    |
EDI       |          |    |    |
ESP       |          |    |    |
EBP       |          |    |    |

For the EAX, EBX, ECX, and EDX registers, subsections may be used. For
example, the least significant 2 bytes of EAX can be treated as a 16-bit
register called AX. The least significant byte of AX can be used as a single
8-bit register called AL, while the most significant byte of AX can be used as
a single 8-bit register called AH. These names refer to the same physical
register. When a two-byte quantity is placed into DX, the update affects the
value of DH, DL, and EDX. These sub-registers are mainly hold-overs from
older, 16-bit versions of the instruction set. However, they are sometimes
convenient when dealing with data that are smaller than 32-bits (e.g. 1-byte
    ASCII characters).


lea — Load effective address

The lea instruction places the address specified by its second operand into
the register specified by its first operand. Note, the contents of the memory
location are not loaded, only the effective address is computed and placed
into the register. This is useful for obtaining a pointer into a memory
region.

Syntax
lea <reg32>,<mem>

Examples

; the address calculated from EBX+4*ESI is placed in EDI.
lea edi, [ebx+4*esi]
lea eax, [var] — the value in var is placed in EAX.
lea eax, [val] — the value val is placed in EAX.


shl, shr — Shift Left, Shift Right

These instructions shift the bits in their first operand's contents left and
right, padding the resulting empty bit positions with zeros. The shifted
operand can be shifted up to 31 places. The number of bits to shift is
specified by the second operand, which can be either an 8-bit constant or the
register CL. In either case, shifts counts of greater then 31 are performed
modulo 32.

Syntax
shl <reg>,<con8>
shl <mem>,<con8>
shl <reg>,<cl>
shl <mem>,<cl>

shr <reg>,<con8>
shr <mem>,<con8>
shr <reg>,<cl>
shr <mem>,<cl>

Examples
shl eax, 1 — Multiply the value of EAX by 2 (if the most significant bit is 0)

shr ebx, cl — Store in EBX the floor of result of dividing the value of EBX by
2n wheren is the value in CL.


set`cc` — Set Byte on Condition

`setne` Set byte if not equal (ZF=0).

Sets the destination operand to 0 or 1 depending on the settings of the status
flags (CF, SF, OF, ZF, and PF) in the EFLAGS register. The destination operand
points to a byte register or a byte in memory. The condition code suffix (cc)
  indicates the condition being tested for.

The terms “above” and “below” are associated with the CF flag and refer to the
relationship between two unsigned integer values. The terms “greater” and
“less” are associated with the SF and OF flags and refer to the relationship
between two signed integer values.

Many of the SETcc instruction opcodes have alternate mnemonics. For example,
     SETG (set byte if greater) and SETNLE (set if not less or equal) have the
     same opcode and test for the same condition: ZF equals 0 and SF equals
     OF. These alternate mnemonics are provided to make code more
     intelligible. Appendix B, “EFLAGS Condition Codes,” in the Intel® 64 and
     IA-32 Architectures Software Developer’s Manual, Volume 1, shows the
     alternate mnemonics for various test conditions.

Some languages represent a logical one as an integer with all bits set. This
representation can be obtained by choosing the logically opposite condition
for the SETcc instruction, then decrementing the result. For example, to test
for overflow, use the SETNO instruction, then decrement the result.

The reg field of the ModR/M byte is not used for the SETCC instruction and
those opcode bits are ignored by the processor.


*x86-asm-mov*
movzx  Move with Zero-Extend

                    ; 11000011 11101110
mov     bx, 0C3EEh  ; Sign bit of bl is now 1: BH == 1100 0011, BL == 1110 1110
movsx   ebx, bx     ; Load signed 16-bit value into 32-bit register and sign-extend
                    ; EBX is now equal FFFFC3EEh

movzx   dx, bl      ; Load unsigned 8-bit value into 16-bit register and zero-extend
                    ; DX is now equal 00EEh


*x86-asm-and* *x86-asm-test*

and, or, xor — Bitwise logical and, or and exclusive or

These instructions perform the specified logical operation (logical bitwise
    and, or, and exclusive or, respectively) on their operands, placing the
result in the first operand location.  Syntax

and <reg>,<reg>
or <reg>,<reg>
xor <reg>,<reg>

Examples
and eax, 0fH — clear all but the last 4 bits of EAX.
xor edx, edx — set the contents of EDX to zero.


={============================================================================
*kt_linux_core_400* linux-elf-readelf

<elf-sections>
The .text section contains the executable program code. 

The .rodata section contains constant data in your program. 

The .data section generally contains initialized global data used by the C
library prologue code and can contain large initialized data items from your
application.  The .sdata section is used for smaller initialized global data
items and exists only on some architectures. Some processor architectures can
make use of optimized data access when the attributes of the memory area are
known. The .sdata and .sbss sections enable these optimizations. 

The .bss and .sbss sections contain uninitialized (global) data in your program.
These sections occupy no space in the program image their memory space is
allocated and initialized to zero on program startup by C library prologue code.


{example-analysis}
/*
** This is sample program to see how elfs is made and allocated
**
** ktpark
*/

/* bss */
int kt_bss_vars[100];

/* data */
int kt_data_vars[100]={0x01};

/* constant */
char* kt_const = "this is constant string array.\n";

int main( int argc, char** argv )
{
  int kt_local_bss_vars[100];
  int kt_local_data_vars[100]={0xFF};
  char* kt_local_const = "this is local constant string array.\n";
  int i;

  printf("\n\n this is sample program to see elf.\n\n");

  for(i=0; i <= 10;i++)
  {
    sleep(1000);
  }

  printf("\nend of program.\n");
}
--

# free (before)
              total         used         free       shared      buffers
  Mem:       116472        18052        98420            0         8192
 Swap:            0            0            0
Total:       116472        18052        98420

# free (after)
              total         used         free       shared      buffers
  Mem:       116472        18068        98404            0         8192
 Swap:            0            0            0
Total:       116472        18068        98404

16K used.

{example-analysis}
#include <stdio.h>

int bss_var; /* Uninitialized global variable */
int data_var = 1; /* Initialized global variable */

int main(int argc, char **argv)
{
  void *stack_var; /* Local variable on the stack */
  stack_var = (void *)main; /* Don't let the compiler optimize it out */

  printf("Hello, World! Main is executing at %p\n", stack_var);
  printf("This address (%p) is in our stack frame\n", &stack_var);

  /* bss section contains uninitialized data */
  printf("This address (%p) is in our bss section\n", &bss_var);

  /* data section contains initializated data */
  printf("This address (%p) is in our data section\n", &data_var);

  return 0;
}

root@amcc:~# ./hello
Hello, World! Main is executing at 0x10000418
This address (0x7ff8ebb0) is in our stack frame
This address (0x10010a1c) is in our bss section
This address (0x10010a18) is in our data section


{section-and-nm-map}
(from the readelf of kernel)
Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .init             PROGBITS      40018000 008000 01c000 00 WAX  0   0 32
  [ 2] .text             PROGBITS      40034000 024000 2af998 00  AX  0   0 32
  [ 3] .pci_fixup        PROGBITS        402e4000 2d4000 000490 00   A  0   0  4
  [ 4] __ksymtab         PROGBITS        402e4490 2d4490 004020 00   A  0   0  4
  [ 5] __ksymtab_gpl     PROGBITS        402e84b0 2d84b0 000f40 00   A  0   0  4
  [ 6] __ksymtab_gpl_fut PROGBITS        402e93f0 2d93f0 000018 00   A  0   0  4
  [ 7] __ksymtab_strings PROGBITS        402e9408 2d9408 00b4d8 00   A  0   0  4
  [ 8] __param           PROGBITS        402f5000 2e5000 0004b0 00   A  0   0  4
  [ 9] .data             PROGBITS        402f8000 2e8000 05c210 00  WA  0   0 32
  [10] .bss              NOBITS          40354220 344210 020438 00  WA  0   0 32
  [11] .comment          PROGBITS        00000000 344210 002e68 00      0   0  1
  [12] .ARM.attributes   ARM_ATTRIBUTES  00000000 347078 000010 00      0   0  1
  [13] .debug_abbrev     PROGBITS        00000000 347088 0b55ee 00      0   0  1
  [14] .debug_info       PROGBITS        00000000 3fc676 16124fa 00      0   0  1
  [15] .debug_line       PROGBITS        00000000 1a0eb70 17d764 00      0   0  1
  [16] .debug_pubnames   PROGBITS        00000000 1b8c2d4 0210e5 00      0   0  1
  [17] .debug_str        PROGBITS        00000000 1bad3b9 09173a 01  MS  0   0  1
  [18] .debug_aranges    PROGBITS        00000000 1c3eaf3 006408 00      0   0  1
  [19] .debug_frame      PROGBITS        00000000 1c44efc 05b4b0 00      0   0  4
  [20] .debug_loc        PROGBITS        00000000 1ca03ac 23ee57 00      0   0  1
  [21] .debug_ranges     PROGBITS        00000000 1edf203 069400 00      0   0  1
  [22] .shstrtab         STRTAB          00000000 1f48603 000113 00      0   0  1
  [23] .symtab           SYMTAB          00000000 1f48b00 07f8a0 10     24 26436  4
  [24] .strtab           STRTAB          00000000 1fc83a0 05e3c9 00      0   0  1
  
(from the map)
0000000040034000 T _text
00000000402f54b0 A _etext
00000000402f54b0-0000000040034000=0x2C14B0(2,888,880)

Why these are different in size??


={============================================================================
*kt_linux_core_400* linux-elf load-elf

https://lwn.net/Articles/631631/

The previous article in this series described the general mechanisms that the
Linux kernel has for executing programs as a result of a user-space call to
execve(). However, the particular format handlers described in that article
each deferred the process of execution to an inner call to
search_binary_handler(). That recursion almost always ends with the invocation
of an ELF binary program, which is the subject of this article.

The ELF format

*elf-handler* <linux-elf-source>

The ELF (Executable and Linkable Format) format is the main binary format in
use on modern Linux systems, and support for it is implemented in the file
`fs/binfmt_elf.c.` It's also a slightly complicated format for the kernel to
handle; the main load_elf_binary() function spans over 400 lines, and the ELF
support code is more than four times as big as the code that supports the old
a.out format.


<elf-format>
linux-2.6.18.8-from-mips/include/linux/elf.h

#ifndef _LINUX_ELF_H
#define _LINUX_ELF_H

#include <linux/types.h>
#include <linux/auxvec.h>
#include <linux/elf-em.h>
#include <asm/elf.h>
...

<elf-header>
To locate the program header, the ELF header is needed. The ELF header is the
only data structure which has a fixed place in the file, starting at 0.

typedef struct
{
  unsigned char	e_ident[EI_NIDENT];	/* Magic number and other info */
  Elf32_Half	e_type;			/* Object file type */
  Elf32_Half	e_machine;		/* Architecture */
  Elf32_Word	e_version;		/* Object file version */
  Elf32_Addr	e_entry;		/* Entry point virtual address */
  Elf32_Off	e_phoff;		/* Program header table file offset */
  Elf32_Off	e_shoff;		/* Section header table file offset */
  Elf32_Word	e_flags;		/* Processor-specific flags */
  Elf32_Half	e_ehsize;		/* ELF header size in bytes */
  Elf32_Half	e_phentsize;		/* Program header table entry size */
  Elf32_Half	e_phnum;		/* Program header table entry count */
  Elf32_Half	e_shentsize;		/* Section header table entry size */
  Elf32_Half	e_shnum;		/* Section header table entry count */
  Elf32_Half	e_shstrndx;		/* Section header string table index */
} Elf32_Ehdr;


<elf-executable> <elf-program-header>
An ELF file for an executable program (rather than a shared library or an
    object file) must always contain a `program header table` near the start
of the file, after the `ELF header`; each entry in this table provides
information that is needed to run the program.

The kernel only really cares about `three types of program header entries.`

The first type is the `PT_LOAD` segment, which describes areas of the new
program's running memory. This includes code and data sections that come from
the executable file, together with the size of a BSS section. The BSS will be
filled with zeroes (thus only its length needs to be stored in the executable
    file). 

The second entry of interest is a PT_INTERP entry, which identifies the
run-time linker needed to assemble the complete program; for the time being,
we'll assume a statically linked ELF binary and return to dynamic linking
  later. Finally, the kernel also gets a single bit of information from a
  PT_GNU_STACK entry, if present, which indicates whether the program's stack
  should be made executable or not.

(This article only focuses on what's needed to load an ELF program, rather
 than exploring all of the details of the format. The interested reader can
 find much more information via the references linked from Wikipedia's ELF
 article or by exploring real binaries with the objdump tool.)


Program Headers:
  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
  PHDR           0x000034 0x00400034 0x00400034 0x00100 0x00100 R E 0x4
  INTERP         0x000134 0x00400134 0x00400134 0x0000d 0x0000d R   0x1
      [Requesting program interpreter: /lib/ld.so.1]
  REGINFO        0x000164 0x00400164 0x00400164 0x00018 0x00018 R   0x4
  LOAD           0x000000 0x00400000 0x00400000 0x00924 0x00924 R E 0x1000 ~
  LOAD           0x000924 0x00401924 0x00401924 0x00090 0x000bc RW  0x1000 ~
  DYNAMIC        0x00017c 0x0040017c 0x0040017c 0x000f8 0x000f8 RWE 0x4
  NOTE           0x000144 0x00400144 0x00400144 0x00020 0x00020 R   0x4
  NULL           0x000000 0x00000000 0x00000000 0x00000 0x00000     0x4

<elf-program-header>
typedef struct
{
  Elf32_Word	p_type;			/* Segment type */
  Elf32_Off	p_offset;		/* Segment file offset */
  Elf32_Addr	p_vaddr;		/* Segment virtual address */
  Elf32_Addr	p_paddr;		/* Segment physical address */
  Elf32_Word	p_filesz;		/* Segment size `in file` */
  Elf32_Word	p_memsz;		/* Segment size `in memory` */
  Elf32_Word	p_flags;		/* Segment flags */
  Elf32_Word	p_align;		/* Segment alignment */
} Elf32_Phdr;

/* Legal values for p_type (segment type).  */
#define	PT_NULL		0		/* Program header table entry unused */
#define PT_LOAD		1		/* Loadable program segment */
#define PT_DYNAMIC	2		/* Dynamic linking information */
#define PT_GNU_RELRO	0x6474e552	/* Read-only after relocation */

/* Legal values for p_flags (segment flags).  */
#define PF_X		(1 << 0)	/* Segment is executable */
#define PF_W		(1 << 1)	/* Segment is writable */
#define PF_R		(1 << 2)	/* Segment is readable */


Dynamically linked programs

So far we've assumed the program being executed is statically linked and
skipped over steps that would be triggered by the presence of a PT_INTERP
entry in the ELF program header. However, most programs are dynamically
linked, meaning that required shared libraries have to be located and linked
at run-time. This is performed by `the runtime linker` (typically something
    like /lib64/ld-linux-x86-64.so.2), and the identity of this linker is
specified by the PT_INTERP program header entry.

To cope with a runtime linker, the ELF handler first reads the ELF interpreter
file name into scratch space, then opens the executable file with `open_exec()`.

note:
erros when renames runtime linker to make it unavailable.

-sh-3.2# ls
-sh: /bin/ls: No such file or directory


The first 128 bytes of the file are read into the bprm->buf scratch area,
replacing the contents of the original program file and allowing access to the
  ELF header of the interpreter program which must therefore be an ELF binary
  itself, rather than any other format.
 

<elf-dual-nature>
LAL, Figure 3-10: Two views of an ELF file

linking view and execution view, adapted from fig 1-1 in Intel TIS document

Linking View                            Execution View

linkable                                executable
`sections`                                `segments`
              +---------------------+
              | ELF header          |
              +---------------------+
optional      | program header tbl  |   describe segments
ignored       +---------------------+
           -> |                     |
              +-----                |
sections   -> |                     |   <- segment 1
              +-----                |
           -> |                     |
              +---------------------+
           -> |                     |
              +-----                |
sections   -> |                     |   <- segment x
              +-----                |
           -> |                     |
              +---------------------+
describe      | section header tbl  |   optional, ignored
sections      +---------------------+


ELF files have an unusual dual nature, Figure 3-10. Compilers, assemblers, and
linkers treat the file as a set of logical `sections` described by a section
header table, while the system loader treats the file as a set of `segments`
described by a program header table.

`A single segment will usually consist of several sections.` For example, a
‘loadable read-only’ segment could contain sections for executable code,
read-only data, and symbols for the dynamic linker.

Relocatable files have section tables, executable files have program header
tables, and `shared objects have both.` The sections are intended for further
processing by a linker, while the segments are intended to be mapped into
memory.

// tool-readelf
// 
//        -a
//        --all
//            Equivalent to specifying --file-header, --program-headers,
//            --sections, --symbols, --relocs, --dynamic, --notes 
//            and --version-info.
// 
//        -e
//        --headers
//            Display all the headers in the file.  Equivalent to -h -l -S.
// 
//                [-h|--file-header]
//                [-l|--program-headers|--segments]
//                [-S|--section-headers|--sections]
// 
// note: to speficy section
//
//        -x <number or name>
//        --hex-dump=<number or name>
//            Displays the contents of the indicated section as a hexadecimal
//            bytes.  A number identifies a particular section by index in the
//            section table; any other string identifies all sections with that
//            name in the object file.


 Section to Segment mapping:
  Segment Sections...
   00
   01     .interp
   02     .reginfo
   03     .interp .note.ABI-tag .reginfo .dynamic .hash .dynsym .dynstr .gnu.version .gnu.version_r .rel.plt .init .plt .text .fini .rodata .eh_frame
   04     .ctors .dtors .jcr .data .rld_map .got.plt .got .sdata .sbss .bss
   05     .dynamic
   06     .note.ABI-tag
   07


<elf-dynamic-section>
/* Dynamic section entry.  */

typedef struct
{
  Elf32_Sword	d_tag;			/* Dynamic entry type */
  union
    {
      Elf32_Word d_val;			/* Integer value */
      Elf32_Addr d_ptr;			/* Address value */
    } d_un;
} Elf32_Dyn;


/* Legal values for d_tag (dynamic entry type).  */
#define DT_NULL       0 /* Marks end of dynamic section */
#define DT_NEEDED     1 /* Name of needed library */
...
#define DT_NUM      34  /* Number used, 0x22 */
#define DT_LOOS     0x6000000d	/* Start of OS-specific */
#define DT_HIOS     0x6ffff000	/* End of OS-specific */
#define DT_LOPROC   0x70000000	/* Start of processor-specific */
#define DT_HIPROC   0x7fffffff	/* End of processor-specific */
#define DT_PROCNUM  DT_MIPS_NUM	/* Most used by any processor */

Dynamic section at offset 0x17c contains 26 entries:
  Tag        Type                         Name/Value
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so.6]
 0x0000000c (INIT)                       0x4004e0
 0x0000000d (FINI)                       0x4008b0
 0x00000004 (HASH)                       0x400274
 0x00000005 (STRTAB)                     0x40038c
 0x00000006 (SYMTAB)                     0x4002bc
 0x0000000a (STRSZ)                      200 (bytes)
 0x0000000b (SYMENT)                     16 (bytes)
 `0x70000016` (MIPS_RLD_MAP)               0x401960
 0x00000015 (DEBUG)                      0x0
 0x00000003 (PLTGOT)                     0x401980
 `0x70000001` (MIPS_RLD_VERSION)           1
 0x70000005 (MIPS_FLAGS)                 NOTPOT
 0x70000006 (MIPS_BASE_ADDRESS)          0x400000
 0x7000000a (MIPS_LOCAL_GOTNO)           10
 0x70000011 (MIPS_SYMTABNO)              13
 0x70000012 (MIPS_UNREFEXTNO)            39
 `0x70000013` (MIPS_GOTSYM)                0xb
 0x00000014 (PLTREL)                     REL
 0x00000017 (JMPREL)                     0x4004b0
 0x00000002 (PLTRELSZ)                   48 (bytes)
 0x6ffffffe (VERNEED)                    0x400470
 0x6fffffff (VERNEEDNUM)                 2
 0x6ffffff0 (VERSYM)                     0x400454
 0x00000000 (NULL)                       0x0


$ ./gcc/gcc-glibc-mips-reference/bin/mips-linux-gnu-gcc --static sample.c
$ readelf -d a.out

There is no dynamic section in this file.


<elf-section-header>
A section header table index is a subscript into this array. The ELF header’s
e_shoff member gives the byte offset from the beginning of the file to the
section header table; e_shnum tells how many entries the section header table
contains; e_shentsize gives the size in bytes of each entry.  Some section
header table indexes are reserved; an object file will not have sections for
these special indexes.

typedef struct
{
  Elf32_Word	sh_name;		/* Section name (string tbl index) */
  Elf32_Word	sh_type;		/* Section type */
  Elf32_Word	sh_flags;		/* Section flags */
  Elf32_Addr	sh_addr;		/* Section virtual addr at execution */
  Elf32_Off	sh_offset;		/* Section file offset */
  Elf32_Word	sh_size;		/* Section size in bytes */
  Elf32_Word	sh_link;		/* Link to another section */
  Elf32_Word	sh_info;		/* Additional section information */
  Elf32_Word	sh_addralign;		/* Section alignment */
  Elf32_Word	sh_entsize;		/* Entry size if section holds table */
} Elf32_Shdr;


/* Special section indices.  */
#define SHN_UNDEF	0		/* Undefined section */

#define SHN_MIPS_ACOMMON    0xff00	/* Allocated common symbols */
#define SHN_MIPS_TEXT	    0xff01	/* Allocated test symbols.  */
#define SHN_MIPS_DATA	    0xff02	/* Allocated data symbols.  */
#define SHN_MIPS_SCOMMON    0xff03	/* Small common symbols */
#define SHN_MIPS_SUNDEFINED 0xff04	/* Small undefined symbols */

/* Legal values for sh_type (section type).  */
#define SHT_NULL	  0		/* Section header table entry unused */
#define SHT_PROGBITS	  1		/* Program data */


<elf-symbol> <linker-undefined-resolve>
A symbol is basically a name and a value. Many symbols represent static
objects in the original source codethat is, objects which exist in a single
place for the duration of the program. For example, in an object file
generated from C code, there will be a symbol for each `function` and for each
`global and static variable.` 

The `value of such a symbol` is simply an `offset` into the contents. This type
of symbol is known as a `defined symbol.` It is important not to confuse the
value of the symbol representing the variable my_global_var with the value of
my_global_var itself. The value of the symbol is roughly the address of the
variable:

Symbols are also used to indicate a `reference` to a name defined in a
different object file. Such a reference is known as an `undefined symbol.`

During the linking process, the linker will assign an address to each defined
symbol, and will `resolve` each undefined symbol by finding a defined symbol
with the same name.

/* Symbol table entry.  */

typedef struct
{
  Elf32_Word	st_name;		/* Symbol name (`string tbl index`) */
  Elf32_Addr	st_value;		/* Symbol value */
  Elf32_Word	st_size;		/* Symbol size */
  unsigned char	st_info;		/* Symbol type and binding */
  unsigned char	st_other;		/* Symbol visibility */
  Elf32_Section	st_shndx;		/* Section index */
} Elf32_Sym;

// from readelf
  [40] .symtab           SYMTAB          00000000 001c68 000620 10     41  69  4
  [41] .strtab           STRTAB          00000000 002288 00037e 00      0   0  1

// code to show a relationship between symtab and strtab
	symtab = (ElfW(Sym) *)(intptr_t)tpnt->dynamic_info[DT_SYMTAB];
	strtab = (char *)tpnt->dynamic_info[DT_STRTAB];
	symname = strtab + symtab[symtab_index].st_name;

// #define DT_STRTAB	5		/* Address of string table */
// #define DT_SYMTAB	6		/* Address of symbol table */


/* Legal values for ST_BIND subfield of st_info (symbol binding).  */

#define STB_LOCAL	0		/* Local symbol */
#define STB_GLOBAL	1		/* Global symbol */
#define STB_WEAK	2		/* Weak symbol */
#define STB_NUM		3		/* Number of defined types.  */
#define STB_LOOS	10		/* Start of OS-specific */
#define STB_HIOS	12		/* End of OS-specific */
#define STB_LOPROC	13		/* Start of processor-specific */
#define STB_HIPROC	15		/* End of processor-specific */

/* Legal values for ST_TYPE subfield of st_info (symbol type).  */

#define STT_NOTYPE	0		/* Symbol type is unspecified */
#define STT_OBJECT	1		/* Symbol is a data object */
#define STT_FUNC	2		/* Symbol is a code object */
#define STT_SECTION	3		/* Symbol associated with a section */
#define STT_FILE	4		/* Symbol's name is file name */
#define STT_COMMON	5		/* Symbol is a common data object */
#define STT_TLS		6		/* Symbol is thread-local data object*/
#define STT_NUM		7		/* Number of defined types.  */
#define STT_LOOS	10		/* Start of OS-specific */
#define STT_HIOS	12		/* End of OS-specific */
#define STT_LOPROC	13		/* Start of processor-specific */
#define STT_HIPROC	15		/* End of processor-specific */

/* Symbol table indices are found in the hash buckets and chain table
   of a symbol hash table section.  This special index value indicates
   the end of a chain, meaning no further symbols are found in that bucket.  */

#define STN_UNDEF	0		/* End of a chain.  */

<elf-symbol-st-other>
/* How to extract and insert information held in the st_other field.  */

#define ELF32_ST_VISIBILITY(o)	((o) & 0x03)

/* Symbol visibility specification encoded in the st_other field.  */
#define STV_DEFAULT	0		/* Default symbol visibility rules */
#define STV_INTERNAL	1		/* Processor specific hidden class */
#define STV_HIDDEN	2		/* Sym unavailable in other modules */
#define STV_PROTECTED	3		/* Not preemptible, not exported */


={============================================================================
*kt_linux_core_400* linux-elf-relocation

A relocation is a computation to perform `on the contents.` Most relocations
refer to a symbol and to an offset within the contents. Many relocations will
also provide an additional operand, known as the `addend`.

// from one of program linker operations.
Read the contents data and the relocations. `Apply the relocations` to the
contents. Write the result to the output file.

The contents are what memory should look like during the execution of the
program. They contain the machine code generated by the compiler and assembler
(known as text). They contain the values of initialized variables (data).
They contain static unnamed data like string constants and switch tables
(read-only data or rdata). They contain uninitialized variables, in which case
the array of bytes is generally omitted and assumed to contain only zeroes
(bss).

http://eli.thegreenplace.net/2011/08/25/load-time-relocation-of-shared-libraries/

Load-time relocation of shared libraries

This article's aim is to explain how a modern operating system makes it
possible to use shared libraries with load-time relocation. It focuses on the
Linux OS running on 32-bit x86, but the general principles apply to other OSes
and CPUs as well.

Note that shared libraries have many names - shared libraries, shared objects,
dynamic shared objects (DSOs), dynamically linked libraries (DLLs - if you're
    coming from a Windows background). For the sake of consistency, I will try
  to just use the name "shared library" throughout this article.


<elf-entry>
Loading executables

Linux, similarly to other OSes with virtual memory support, loads executables
to a fixed memory address. If we examine the ELF header of some random
executable, we'll see an Entry point address:

$ readelf -h /usr/bin/uptime
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF32
  [...] some header fields
  `Entry point address:               0x8048470`
  [...] some header fields

// e_entry 
//
// This member gives the virtual address to which the system first transfers
// control, thus starting the process. If the file has no associated entry
// point, this member holds zero.

This is placed by the linker to tell the OS where to start executing the
  executable's code [1]. And indeed if we then load the executable with GDB
  and examine the address 0x8048470, we'll see the first instructions of the
  executable's .text segment there.


<linktime-relocations>
What this means is that the linker, when linking the executable, can fully
resolve all `internal` symbol references (to functions and data) to fixed and
final locations. The linker does some relocations of its own [2], but
eventually the output it produces contains no additional relocations.


// [2] Link-time relocation happens in the process of combining multiple
// object files into an executable (or shared library). It involves quite a
// lot of relocations to resolve symbol references between the object files.
// Link-time relocation is a more complex topic than load-time relocation, and
// I won't cover it in this article.
//
// `PIC is not the same as relocatable code.` Relocatable code is code whose
// address may be assigned at `linktime`. Position independent code is code
// whose address may be assigned at `runtime`. 
// 
// // Linkers, Ian Lance Taylor, linktime relocation 
// Again, a relocation is a computation to perform `on the contents.` Most
// relocations refer to a symbol and to an offset within the contents. 
// 
// During the linking process, the linker will perform all of the relocation
// computations as directed.  A relocation in an object file may refer to an
// undefined symbol. If the linker is unable to resolve that symbol, it will
// normally issue an error (but not always: for some symbol types or some
// relocation types an error may not be appropriate)
// 
// // Linkers, Ian Lance Taylor, dynamic linker do dynamic or loadtime
// relocation
// 
// When the program linker creates a shared library, it does not yet know
// which virtual address that shared library will run at. In fact, in
// different processes, the same shared library will run at different address,
// depending on the decisions made by the dynamic linker. This means that
// shared library code `must be position independent.` More precisely, it must
// be position independent after the dynamic linker has finished loading it.
// It is always possible for the dynamic linker to convert any piece of code
// to run at any virtula address, given sufficient relocation information. 
// 
// However, performing the reloc computations must be done every time the
// program starts, implying that it will start more slowly. 
//
// Therefore, any shared library system seeks to generate position independent
// code which requires a `minimal` number of relocations to be applied at
// runtime, while still running at close to the runtime efficiency of position
// dependent code.

Or does it? Note that I emphasized the word `internal` in the previous
paragraph. As long as the executable needs no shared libraries [3], it needs
no relocations. But if it does use shared libraries (as do the vast majority
    of Linux applications), symbols taken from these shared libraries need to
be relocated, because of how shared libraries are loaded.

// [3] This can be made possible by compiling all your libraries into static
// libraries (with ar combining object files instead gcc -shared), and
// providing the -static flag to gcc when linking the executable - to avoid
// linkage with the shared version of libc.


Loading shared libraries

  // why use loadtime relocation
  Unlike executables, when shared libraries are being built, the linker can't
  assume a known load address for their code. The reason for this is simple.
  Each program can use any number of shared libraries, and there's simply no
  way to know in advance where any given shared library will be loaded in the
  process's virtual memory. 

Many solutions were invented for this problem over the years, but in this
article I will just focus on the ones currently used by Linux.

But first, let's briefly examine the problem. Here's some sample C code
which I compile into a shared library:

int myglob = 42;

int ml_func(int a, int b)
{
    myglob += a;
    return b + myglob;
}

Note how ml_func references myglob a few times. When translated to x86
assembly, this will involve a mov instruction to pull the value of myglob from
its location in memory into a register. 

`mov requires an absolute address` - so how does the linker know which address
to place in it? The answer is - it doesn't. As I mentioned above, shared
libraries have no pre-defined load address - it will be decided at runtime.

In Linux, the dynamic loader [5] is a piece of code responsible for preparing
programs for running. One of its tasks is to load shared libraries from disk
into memory, when the running executable requests them. When a shared library
is loaded into memory, `it is then adjusted for` its newly determined load
location. It is the job of the dynamic loader to solve the problem presented
in the previous paragraph.

// [5] Also called "dynamic linker". It's a shared object itself (though it
// can also run as an executable), residing at /lib/ld-linux.so.2 (the last
// number is the SO version and may be different).


There are two main approaches to solve this problem in Linux ELF shared
libraries:

  Load-time relocation
  Position independent code (PIC)

Although PIC is the more common and nowadays-recommended solution, in this
article I will focus on load-time relocation. Eventually I plan to cover both
approaches and write a separate article on PIC, and I think starting with
load-time relocation will make PIC easier to explain later. 

// note:
// This article is about "relocation" since not works on 64 bits and even 32
// bits with different gcc shows different results such as so file do not have
// program header.
//
// errors on 64 so builds only on 32 bits
// kyoupark@kit-debian64:~/git/kb/code-linux/ex_shared$ gcc -shared -o limmlreloc.so ml_mainreloc.o
// /usr/bin/ld: ml_mainreloc.o: relocation R_X86_64_PC32 against symbol `myglob' can not be used when making a shared object; recompile with -fPIC
// /usr/bin/ld: final link failed: Bad value
// collect2: error: ld returned 1 exit status
// gcc -g -fpic -c ml_main.c -o ml_mainreloc.o
// gcc -shared -o libmlreloc.so ml_mainreloc.o


Linking the shared library for load-time relocation

To create a shared library that has to be relocated at load-time, I'll compile
it `without the -fPIC flag` (which would otherwise trigger PIC generation):

gcc -g -c ml_main.c -o ml_mainreloc.o
gcc -shared -o libmlreloc.so ml_mainreloc.o


The first interesting thing to see is the entry point of libmlreloc.so:

$ readelf -h libmlreloc.so
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF32
  [...] some header fields
  Entry point address:               0x3b0
  [...] some header fields

For simplicity, the linker just links the shared object for address 0x0 (the
    .text section starting at 0x3b0), knowing that the loader will move it
anyway. Keep this fact in mind - it will be useful later in the article.

// from own build and can see the same fro tool-objdump
// Section Headers:
//  [11] .text             PROGBITS        000003b0 0003d0 000155 00  AX  0   0 16


Now let's look at the disassembly of the shared library, focusing on ml_func:

*tool-objdump*
$ objdump -d -Mintel libmlreloc.so

libmlreloc.so:     file format elf32-i386

[...] skipping stuff

// int myglob = 42;
// 
// int ml_func(int a, int b)
// {
//     myglob += a;
//     return b + myglob;
// }

0000046c <ml_func>:
 46c: 55                      push   ebp
 46d: 89 e5                   mov    ebp,esp
 46f: a1 00 00 00 00          mov    eax,`ds:0x0`               // fetch myglob
 474: 03 45 08                add    eax,DWORD PTR [ebp+0x8]  // myglob + a
 477: a3 00 00 00 00          mov    `ds:0x0`,eax               // save back to myglob
 47c: a1 00 00 00 00          mov    eax,`ds:0x0`               // fetch myglob
 481: 03 45 0c                add    eax,DWORD PTR [ebp+0xc]  // myglob + b
 484: 5d                      pop    ebp
 485: c3                      ret

[...] skipping stuff

After the first two instructions which are part of the prologue [6], we see
the compiled version of myglob += a [7]. The value of myglob is taken from
memory into eax, incremented by a (which is at ebp+0x8) and then placed back
into memory.

But wait, the mov takes myglob? Why? It appears that the actual operand of mov
is just 0x0 [8]. 

// eax,`ds:0x0`

What gives? This is how relocations work. The linker places some provisional
pre-defined value (0x0 in this case) into the instruction stream, and then
creates a special `relocation entry` pointing to this place.

// [8] I'm looking at the left-hand side of the output of objdump, where the
// raw memory bytes are. a1 00 00 00 00 means mov to eax with operand 0x0,
// which is interpreted by the disassembler as `ds:0x0`.  


$ objdump -d ml_reloc.o

ml_reloc.o:     file format elf32-i386

Disassembly of section .text:

00000000 <ml_func>:
   0:   55                      push   %ebp
   1:   89 e5                   mov    %esp,%ebp
   3:   e8 fc ff ff ff          call   4 <ml_func+0x4>
   8:   05 01 00 00 00          add    $0x1,%eax
   d:   8b 88 00 00 00 00       mov    0x0(%eax),%ecx
  13:   8b 55 08                mov    0x8(%ebp),%edx
  16:   01 ca                   add    %ecx,%edx
  18:   89 90 00 00 00 00       mov    %edx,0x0(%eax)
  1e:   8b 90 00 00 00 00       mov    0x0(%eax),%edx
  24:   8b 45 0c                mov    0xc(%ebp),%eax
  27:   01 d0                   add    %edx,%eax
  29:   5d                      pop    %ebp
  2a:   c3                      ret

Disassembly of section .text.__x86.get_pc_thunk.ax:

00000000 <__x86.get_pc_thunk.ax>:
   0:   8b 04 24                mov    (%esp),%eax
   3:   c3                      ret

// note see that differnece when use -Mintel

$ objdump -d -Mintel ml_reloc.o

ml_reloc.o:     file format elf32-i386

Disassembly of section .text:

00000000 <ml_func>:
   0:   55                      push   ebp
   1:   89 e5                   mov    ebp,esp
   3:   e8 fc ff ff ff          call   4 <ml_func+0x4>
   8:   05 01 00 00 00          add    eax,0x1
   d:   8b 88 00 00 00 00       mov    ecx,DWORD PTR [eax+0x0]
  13:   8b 55 08                mov    edx,DWORD PTR [ebp+0x8]
  16:   01 ca                   add    edx,ecx
  18:   89 90 00 00 00 00       mov    DWORD PTR [eax+0x0],edx
  1e:   8b 90 00 00 00 00       mov    edx,DWORD PTR [eax+0x0]
  24:   8b 45 0c                mov    eax,DWORD PTR [ebp+0xc]
  27:   01 d0                   add    eax,edx
  29:   5d                      pop    ebp
  2a:   c3                      ret

Disassembly of section .text.__x86.get_pc_thunk.ax:

00000000 <__x86.get_pc_thunk.ax>:
   0:   8b 04 24                mov    eax,DWORD PTR [esp]
   3:   c3                      ret


// *x86-asm*
// [6] If you're not familiar with how x86 structures its stack frames, this
// would be a good time to read this article.  
// https://eli.thegreenplace.net/2011/02/04/where-the-top-of-the-stack-is-on-x86/
//
// *tool-objdump*
// [7] You can provide the -l flag to objdump to add C source lines into the
// disassembly, making it clearer what gets compiled to what. I've omitted it
// here to make the output shorter.  
//
// kyoupark@kt-office-debian:~/works$ cat -n ml_main.c
//      1  int myglob = 42;
//      2
//      3  int ml_func(int a, int b)
//      4  {
//      5      myglob += a;
//      6      return b + myglob;
//      7  }
// 
// $ objdump -d -Mintel -l ml_reloc.o
//
// 00000000 <ml_func>:
// ml_func():
// /home/kyoupark/works/ml_main.c:4
//    0:   55                      push   ebp
//    1:   89 e5                   mov    ebp,esp
//    3:   e8 fc ff ff ff          call   4 <ml_func+0x4>
//    8:   05 01 00 00 00          add    eax,0x1
// /home/kyoupark/works/ml_main.c:5
//    d:   8b 88 00 00 00 00       mov    ecx,DWORD PTR [eax+0x0]
//   13:   8b 55 08                mov    edx,DWORD PTR [ebp+0x8]
//   16:   01 ca                   add    edx,ecx
//   18:   89 90 00 00 00 00       mov    DWORD PTR [eax+0x0],edx
// /home/kyoupark/works/ml_main.c:6
//   1e:   8b 90 00 00 00 00       mov    edx,DWORD PTR [eax+0x0]
//   24:   8b 45 0c                mov    eax,DWORD PTR [ebp+0xc]
//   27:   01 d0                   add    eax,edx
// /home/kyoupark/works/ml_main.c:7
//   29:   5d                      pop    ebp
//   2a:   c3                      ret


Let's examine the relocation entries for this shared library:

// note: cannot see the same on 32 bits and gcc 6.3.0

*tool-readelf*
$ readelf -r libmlreloc.so

Relocation section '.rel.dyn' at offset 0x2fc contains 7 entries:
 Offset     Info    Type            Sym.Value  Sym. Name
00002008  00000008 R_386_RELATIVE
00000470  00000401 R_386_32          0000200C   myglob
00000478  00000401 R_386_32          0000200C   myglob
0000047d  00000401 R_386_32          0000200C   myglob
[...] skipping stuff


<loadtime-relocations>
The `rel.dyn` section of ELF is reserved for dynamic (load-time) relocations, to
be consumed by the dynamic loader. There are 3 relocation entries for myglob
in the section showed above, since there are 3 references to myglob in the
disassembly. (three ds:0x0) Let's decipher the first one.

It says: go to offset 0x470 in this object (shared library), and apply
relocation of type `R_386_32` to it for symbol myglob. If we consult the ELF
spec we see that relocation type 

`R_386_32 means`: take the value at the offset specified in the entry, add the
address of the symbol to it, and place it back into the offset.

What do we have at offset 0x470 in the object? Recall this instruction from
the disassembly of ml_func:

46f:  a1 00 00 00 00          mov    eax,ds:0x0
         ^^^^^^^^^^^
         replace this dword

a1 encodes the mov instruction, so its operand starts at the next address
which is 0x470. This is the 0x0 we see in the disassembly. So back to the
relocation entry, we now see it says: add the address of myglob to the operand
of that mov instruction. 

In other words it tells the dynamic loader - once you perform actual address
assignment, `put the real address of myglob into 0x470`, thus replacing the
operand of mov by the correct symbol value. Neat, huh?

<relocation-entry-and-symbol-table>
Note also the "Sym. value" column in the relocation section, which contains
0x200C for myglob. This is the `offset` of myglob in the virtual memory image of
the shared library (which, recall, the linker assumes is just loaded at 0x0).
This value can also be examined by looking at the `symbol table` of the library,
     for example with nm:

*tool-nm*
$ nm libmlreloc.so
[...] skipping stuff
0000200c D myglob

This output also provides the offset of myglob inside the library. D means the
symbol is in the initialized data section (.data).


Load-time relocation in action

To see the load-time relocation in action, I will use our shared library from
a simple driver executable. When running this executable, the OS will load the
shared library and relocate it appropriately.

*gdb-address-random* *ld-address-random*
Curiously, due to the address space layout randomization feature which is
enabled in Linux, relocation is relatively difficult to follow, because every
time I run the executable, the libmlreloc.so shared library gets placed in a
different virtual memory address [9].

// *tool-ldd*
// [9] So ldd invoked on the executable will report a different load address
// for the shared library each time it's run.
// note: this is true. try ldd /bin/sh

This is a rather weak deterrent, however. There is a way to make sense in it
all. But first, let's talk about the segments our shared library consists of:

*tool-readelf*
$ readelf --segments libmlreloc.so

Elf file type is DYN (Shared object file)
Entry point 0x3b0
There are `6 program headers`, starting at offset 52

Program Headers:
  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
  LOAD           0x000000 0x00000000 0x00000000 0x004e8 0x004e8 R E 0x1000
  LOAD           0x000f04 0x00001f04 0x00001f04 0x0010c 0x00114 RW  0x1000
  DYNAMIC        0x000f18 0x00001f18 0x00001f18 0x000d0 0x000d0 RW  0x4
  NOTE           0x0000f4 0x000000f4 0x000000f4 0x00024 0x00024 R   0x4
  GNU_STACK      0x000000 0x00000000 0x00000000 0x00000 0x00000 RW  0x4
  GNU_RELRO      0x000f04 0x00001f04 0x00001f04 0x000fc 0x000fc R   0x1

 Section to Segment mapping:
  Segment Sections...
   00     .note.gnu.build-id .hash .gnu.hash .dynsym .dynstr .gnu.version \
     .gnu.version_r .rel.dyn .rel.plt .init .plt .text .fini .eh_frame
   01     .ctors .dtors .jcr .dynamic .got .got.plt .data .bss
   02     .dynamic
   03     .note.gnu.build-id
   04
   05     .ctors .dtors .jcr .dynamic .got

To follow the myglob symbol, we're interested in the `second segment` listed
here. Note a couple of things:

  In the section to segment mapping in the bottom, segment 01 is said to
  contain the `.data` section, which is the home of myglob

  // >>> hex(0x1f04+0x10c)
  //'0x2010'

  The VirtAddr column specifies that the second segment starts at 0x1f04 and
  has size 0x10c, meaning that it extends until 0x2010 and thus contains
  myglob which is at 0x200C.


*dl-iterate*
Use a nice tool Linux gives us to examine the `load-time linking process` - the
dl_iterate_phdr function, which allows an application to inquire at runtime
which shared libraries it has loaded, and more importantly - take a peek at
their program headers.

So I'm going to write the following code into driver.c:

// glibc-2.13/elf/dl-iteratephdr.c
//
// hidden_proto (__dl_iterate_phdr)
// int
// __dl_iterate_phdr (int (*callback) (struct dl_phdr_info *info,
// 				    size_t size, void *data), void *data)
// {}

#define _GNU_SOURCE
#include <link.h>
#include <stdlib.h>
#include <stdio.h>

static int header_handler(struct dl_phdr_info* info, size_t size, void* data)
{
    printf("name=%s (%d segments) address=%p\n",
            info->dlpi_name, info->dlpi_phnum, (void*)info->dlpi_addr);
    for (int j = 0; j < info->dlpi_phnum; j++) {
         printf("\t\t header %2d: address=%10p\n", j,
             (void*) (info->dlpi_addr + info->dlpi_phdr[j].p_vaddr));
         printf("\t\t\t type=%u, flags=0x%X\n",
                 info->dlpi_phdr[j].p_type, info->dlpi_phdr[j].p_flags);
    }
    printf("\n");
    return 0;
}


extern int ml_func(int, int);

int main(int argc, const char* argv[])
{
    dl_iterate_phdr(header_handler, NULL);

    int t = ml_func(argc, argc);
    return t;
}

header_handler implements the callback for dl_iterate_phdr. It will get called
for all libraries and report their names and load addresses, along with all
their segments. It also invokes ml_func, which is taken from the libmlreloc.so
shared library.

To compile and link this driver with our shared library, run:

gcc -g -c driver.c -o driver.o
// gcc -std=c99 -g -c driver.c -o driver.o
gcc -o driver driver.o -L. -lmlreloc


/usr/include/link.h


*gdb-address-random* *dl-iterate*
Running the driver stand-alone we get the information, but for each run the
`addresses are different.` So what I'm going to do is `run it under gdb` [10], see
what it says, and then use gdb to further query the process's memory space:

// [10] Experienced readers will probably note that I could use `i shared` to
// get the load-address of the shared library. However, i shared only mentions
// the load location of the whole library (or, even more accurately, its entry
// point), and I was interested in the segments.

// note 
// LD_LIBRARY_PATH=. ./driver to run driver itself and it under gdb
// export LD_LIBRARY_PATH=. to run driver under gdb

 $ gdb -q driver
 Reading symbols from driver...done.
 (gdb) b driver.c:31
 Breakpoint 1 at 0x804869e: file driver.c, line 31.
 (gdb) r
 Starting program: driver
 [...] skipping output
 name=./libmlreloc.so (6 segments) address=0x12e000
                header  0: address=  0x12e000
                        type=1, flags=0x5
                header  1: address=  `0x12ff04`
                        type=1, flags=0x6
                header  2: address=  0x12ff18
                        type=2, flags=0x6
                header  3: address=  0x12e0f4
                        type=4, flags=0x4
                header  4: address=  0x12e000
                        type=1685382481, flags=0x6
                header  5: address=  0x12ff04
                        type=1685382482, flags=0x4

[...] skipping output
 Breakpoint 1, main (argc=1, argv=0xbffff3d4) at driver.c:31
 31    }
 (gdb)


Since driver reports all the libraries it loads (even implicitly, like libc or
    the dynamic loader itself), the output is lengthy and I will just focus on
the report about libmlreloc.so. 

// note that ldd output can show load address

Note that `the 6 segments` are the same segments reported by readelf, but this
time `relocated into their final memory locations.`

// see that 6 segments in readelf matches with dl-iterate output

Let's do some math. The output says libmlreloc.so was placed in virtual
address 0x12e000. We're interested in the second segment, which as we've seen
in readelf is at 0x1f04. Indeed, we see in the output it was loaded to
address 0x12ff04. And since myglob is at offset 0x200c in the file, we'd
expect it to now be at address 0x13000c.

// note: why 0x13000c?
// 0x12e000 + 0x200c = 0x13000c. so the offset 0x200c is from the load address.
// 
// then how nm results matches up with? From program header, can know the size
// of each segment?
//
//  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
//  LOAD           0x000000 0x00000000 0x00000000 0x004e8 0x004e8 R E 0x1000
//  LOAD           0x000f04 0x0000`1f04` 0x00001f04 0x0010c 0x00114 RW  0x1000
//
// 0x12ff04 - 0x12e000 = 0x`1f04`
//
// hex(0x12e000+0x1f04+0x10c)
// '0x130010'
//
// note that this is beyond all segments


// `R_386_32 means`: take the value at the offset specified in the entry, add
// the address of the symbol to it, and place it back into the offset.
//
// "the address of the symbol" means the address of the symbol `once loaded`


So, let's ask GDB:

(gdb) p &myglob
$1 = (int *) 0x13000c

Excellent! But what about the code of ml_func which refers to myglob? Let's
ask GDB again:

*gdb-disassemble*
(gdb) set disassembly-flavor intel
(gdb) disas ml_func

Dump of assembler code for function ml_func:
   0x0012e46c <+0>:   push   ebp
   0x0012e46d <+1>:   mov    ebp,esp
   0x0012e46f <+3>:   mov    eax,ds:0x13000c ~
   0x0012e474 <+8>:   add    eax,DWORD PTR [ebp+0x8]
   0x0012e477 <+11>:  mov    ds:0x13000c,eax
   0x0012e47c <+16>:  mov    eax,ds:0x13000c
   0x0012e481 <+21>:  add    eax,DWORD PTR [ebp+0xc]
   0x0012e484 <+24>:  pop    ebp
   0x0012e485 <+25>:  ret
End of assembler dump.

As expected, `the real address of myglob was placed` in all the mov instructions
referring to it, just as the relocation entries specified.


Relocating function calls

So far this article demonstrated relocation of `data references` - using the
global variable myglob as an example. Another thing that needs to be relocated
is `code references` - in other words, function calls. This section is a brief
guide on how this gets done. The pace is much faster than in the rest of this
article, since I can now assume the reader understands what relocation is all
about.

Without further ado, let's get to it. I've modified the code of the shared
library to be the following:

int myglob = 42;

int ml_util_func(int a)
{
    return a + 1;
}

int ml_func(int a, int b)
{
    int c = b + ml_util_func(a);
    myglob += c;
    return b + myglob;
}

ml_util_func was added and it's being used by ml_func. Here's the disassembly
of ml_func in the linked shared library:

000004a7 <ml_func>:
 4a7:   55                      push   ebp
 4a8:   89 e5                   mov    ebp,esp
 4aa:   83 ec 14                sub    esp,0x14
 4ad:   8b 45 08                mov    eax,DWORD PTR [ebp+0x8]
 4b0:   89 04 24                mov    DWORD PTR [esp],eax
 4b3:   e8 fc ff ff ff          call   4b4 <ml_func+0xd> ~
 4b8:   03 45 0c                add    eax,DWORD PTR [ebp+0xc]
 4bb:   89 45 fc                mov    DWORD PTR [ebp-0x4],eax
 4be:   a1 00 00 00 00          mov    eax,ds:0x0
 4c3:   03 45 fc                add    eax,DWORD PTR [ebp-0x4]
 4c6:   a3 00 00 00 00          mov    ds:0x0,eax
 4cb:   a1 00 00 00 00          mov    eax,ds:0x0
 4d0:   03 45 0c                add    eax,DWORD PTR [ebp+0xc]
 4d3:   c9                      leave
 4d4:   c3                      ret

// note: relocation is needed for a function in the same file.

What's interesting here is the instruction at address 0x4b3 - it's the call to
ml_util_func. Let's dissect it:

e8 is the opcode for call. The argument of this call is the offset `relative`
to the next instruction. In the disassembly above, this argument is
0xfffffffc, or simply -4. So the call currently points to itself.(4b4) This
clearly isn't right - but let's not forget about relocation. Here's what the
relocation section of the shared library looks like now:

$ readelf -r libmlreloc.so

Relocation section '.rel.dyn' at offset 0x324 contains 8 entries:
 Offset     Info    Type            Sym.Value  Sym. Name
00002008  00000008 R_386_RELATIVE
000004b4  00000502 R_386_PC32        0000049c   ml_util_func ~
000004bf  00000401 R_386_32          0000200c   myglob
000004c7  00000401 R_386_32          0000200c   myglob
000004cc  00000401 R_386_32          0000200c   myglob
[...] skipping stuff

If we compare it to the previous invocation of readelf -r, we'll notice a new
entry added for ml_util_func. This entry points at address 0x4b4 which is the
argument of the call instruction, and its type is `R_386_PC32`. This relocation
type is more complicated than R_386_32, but not by much.

`R_386_PC32 means`: 
take the value at the offset specified in the entry, add the address of the
symbol to it, subtract the address of the offset itself, and place it back
into the word at the offset. 

Recall that this relocation is done at load-time, when the final load
addresses of the symbol and the relocated offset itself are already known.
These final addresses participate in the computation.

What does this do? Basically, it's a `relative relocation`, taking its location
into account and thus suitable for arguments of instructions with relative
addressing (which the e8 call is). I promise it will become clearer once we
get to the real numbers.

I'm now going to build the driver code and run it under GDB again, to see this
relocation in action. Here's the GDB session, followed by explanations:

 $ gdb -q driver
 Reading symbols from driver...done.
 (gdb) b driver.c:31
 Breakpoint 1 at 0x804869e: file driver.c, line 31.
 (gdb) r
 Starting program: driver
 [...] skipping output
 name=./libmlreloc.so (6 segments) address=0x12e000
               header  0: address=  0x12e000
                       type=1, flags=0x5
               header  1: address=  0x12ff04
                       type=1, flags=0x6
               header  2: address=  0x12ff18
                       type=2, flags=0x6
               header  3: address=  0x12e0f4
                       type=4, flags=0x4
               header  4: address=  0x12e000
                       type=1685382481, flags=0x6
               header  5: address=  0x12ff04
                       type=1685382482, flags=0x4

[...] skipping output
Breakpoint 1, main (argc=1, argv=0xbffff3d4) at driver.c:31
31    }
(gdb)  set disassembly-flavor intel

(gdb) disas ml_util_func
Dump of assembler code for function ml_util_func:
   `0x0012e49c` <+0>:   push   ebp
   0x0012e49d <+1>:   mov    ebp,esp
   0x0012e49f <+3>:   mov    eax,DWORD PTR [ebp+0x8]
   0x0012e4a2 <+6>:   add    eax,0x1
   0x0012e4a5 <+9>:   pop    ebp
   0x0012e4a6 <+10>:  ret
End of assembler dump.

(gdb) disas /r ml_func
Dump of assembler code for function ml_func:
   0x0012e4a7 <+0>:    55     push   ebp
   0x0012e4a8 <+1>:    89 e5  mov    ebp,esp
   0x0012e4aa <+3>:    83 ec 14       sub    esp,0x14
   0x0012e4ad <+6>:    8b 45 08       mov    eax,DWORD PTR [ebp+0x8]
   0x0012e4b0 <+9>:    89 04 24       mov    DWORD PTR [esp],eax
   `0x0012e4b3` <+12>:   e8 e4 ff ff ff call   `0x12e49c` <ml_util_func>
   0x0012e4b8 <+17>:   03 45 0c       add    eax,DWORD PTR [ebp+0xc]
   0x0012e4bb <+20>:   89 45 fc       mov    DWORD PTR [ebp-0x4],eax
   0x0012e4be <+23>:   a1 0c 00 13 00 mov    eax,ds:0x13000c
   0x0012e4c3 <+28>:   03 45 fc       add    eax,DWORD PTR [ebp-0x4]
   0x0012e4c6 <+31>:   a3 0c 00 13 00 mov    ds:0x13000c,eax
   0x0012e4cb <+36>:   a1 0c 00 13 00 mov    eax,ds:0x13000c
   0x0012e4d0 <+41>:   03 45 0c       add    eax,DWORD PTR [ebp+0xc]
   0x0012e4d3 <+44>:   c9     leave
   0x0012e4d4 <+45>:   c3     ret
End of assembler dump.

0x0012e4a7-0x0012e49c

The important parts here are:

  1. In the printout from driver we see that the first segment (the code
      segment) of libmlreloc.so has been mapped to 0x12e000 [11] 

  // [11] What, 0x12e000 again? Didn't I just talk about load-address
  // randomization? It turns out the dynamic loader can be manipulated to turn
  // this off, for purposes of debugging. This is exactly what GDB is doing.
  
  2. ml_util_func was loaded to address 0x0012e49c

  3. The address of the relocated offset is 0x0012e4b4

  // 4b3:   e8 fc ff ff ff          call   4b4 <ml_func+0xd> 
 
  4. The call in ml_func to ml_util_func was patched to place 0xffffffe4 in
  the argument (I disassembled ml_func with the /r flag to show raw hex in
      addition to disassembly), which is interpreted as the correct offset to
  ml_util_func.


Obviously we're most interested in `how (4) was done.` Again, it's time for some
math. Interpreting the R_386_PC32 relocation entry mentioned above, we have:

  Take the (original) value at the offset specified in the entry (0xfffffffc), 
     
  add the address of the symbol to it (0x0012e49c), 

  subtract the address of the offset itself (0x0012e4b4), and place it back
  into the word at the offset. 

  // 0xfffffffc+0x0012e49c-0x0012e4b4

Everything is done assuming 32-bit 2-s complement, of course. The result is
0xffffffe4, as expected.

// As shown above, it was 0xfffffffc, -4, since it reads 4 bytes already so -4
// from 0xffffffff. 
//
// Here when see relocated address, so should add 4 to the difference 
// 
// 0x0012e4b3 and reads 4 bytes. becomes 0x12e4b7 and goes back 23+4:
// >>> hex(0x12e4b7-(23+4))
// '0x12e49c'
// 
// >>> 0xffffffff-0xffffffe4
// 27L


Extra credit: Why was the call relocation needed?

This is a "bonus" section that discusses some peculiarities of the
implementation of shared library loading in Linux. If all you wanted was to
understand how relocations are done, you can safely skip it.

When trying to understand the call relocation of ml_util_func, I must admit I
scratched my head for some time. Recall that the argument of call is a
relative offset. Surely the offset between the call and ml_util_func itself
doesn't change when the library is loaded - they both are in the code segment
which gets moved as one whole chunk. `So why is the relocation needed at all?`

// this is the same question I had. explains why PIC function call are only
// for non-static functions.

Here's a small experiment to try: go back to the code of the shared library,
  add `static` to the declaration of ml_util_func. Re-compile and look at the
  output of readelf -r again.

// static make it file scope

Done? Anyway, I will reveal the outcome - the relocation is gone! Examine the
disassembly of ml_func - there's now a correct offset placed as the argument
of call - `no relocation required.` What's going on?

*linux-ld-load-rule*                
When tying global symbol references to their actual definitions, the dynamic
loader `has some rules` about the order in which shared libraries are searched.
The user can also influence this order by setting the LD_PRELOAD environment
variable.

There are too many details to cover here, so if you're really interested
you'll have to take a look at the ELF standard, the dynamic loader man page
and do some Googling. In short, however, when ml_util_func is global, it may
be overridden in the executable or another shared library, so when linking our
shared library, the linker can't just assume the offset is known and hard-code
it [12]. 

It makes `all references to global symbol relocatable` in order to allow the
dynamic loader to decide how to resolve them. This is why declaring the
function static makes a difference - since it's no longer global or exported,
the linker can hard-code its offset in the code.


// [12] Unless it's passed the -Bsymbolic flag. Read all about it in the man
// page of ld.


Extra credit #2: Referencing shared library data from the executable

Again, this is a bonus section that discusses an advanced topic. It can be
skipped safely if you're tired of this stuff.

In the example above, myglob was only used internally in the shared library.
What happens if we reference it from the program (driver.c)? After all, myglob
is a global variable and thus visible externally.

Let's modify driver.c to the following (note I've removed the segment
    iteration code):

#include <stdio.h>

extern int ml_func(int, int);
extern int myglob;

int main(int argc, const char* argv[])
{
    printf("addr myglob = %p\n", (void*)&myglob);
    int t = ml_func(argc, argc);
    return t;
}

It now prints the address of myglob. The output is:

addr myglob = 0x804a018

Wait, something doesn't compute here. Isn't myglob in the shared library's
address space? 0x804xxxx looks like the program's address space. What's going
on?

`Recall that the program/executable is not relocatable`, and thus its data
addresses have to bound at link time. Therefore, the linker has to create a
`copy` of the variable in the program's address space, and the dynamic loader
will use that as the relocation address. This is similar to the discussion in
the previous section - in a sense, myglob in the main program `overrides` the
one in the shared library, and according to the global symbol lookup rules,
    it's being used instead. If we examine ml_func in GDB, we'll see the
    correct reference made to myglob:

0x0012e48e <+23>:      a1 18 a0 04 08 mov    eax,ds:0x804a018

This makes sense because a R_386_32 relocation for myglob still exists in
libmlreloc.so, and the dynamic loader makes it point to the correct place
where myglob now lives.

This is all great, but something is missing. myglob is initialized in the
shared library (to 42) - how does this initialization value get to the address
space of the program? It turns out there's `a special relocation entry` that the
linker builds into the program (so far we've only been examining relocation
    entries in the shared library):

$ readelf -r driver

Relocation section '.rel.dyn' at offset 0x3c0 contains 2 entries:
 Offset     Info    Type            Sym.Value  Sym. Name
08049ff0  00000206 R_386_GLOB_DAT    00000000   __gmon_start__
0804a018  00000605 R_386_COPY        `0804a018`   myglob
[...] skipping stuff

`R_386_COPY` relocation for myglob. It simply means: copy the value
from the symbol's address `into this offset.` The dynamic loader performs this
when it loads the shared library. How does it know how much to copy? The
symbol table section contains the size of each symbol; for example the size
for myglob in the .symtab section of libmlreloc.so is 4.

I think this is a pretty cool example that shows how the process of executable
linking and loading is orchestrated together. The linker puts special
instructions in the output for the dynamic loader to consume and execute.


Conclusion

Load-time relocation is one of the methods used in Linux (and other OSes) to
resolve internal data and code references in shared libraries when loading
them into memory. These days, position independent code (PIC) is a more
popular approach, and 

`some modern systems (such as x86-64) no longer support load-time relocation.`

// appears that x86 32 bits and gcc 6.3.0 no longer use relocation and produce
// PIC binary and that's why see difference from this article when tried. 


Still, I decided to write an article on load-time relocation for two reasons.
First, load-time relocation has a couple of advantages over PIC on some
systems, especially in terms of performance. Second, load-time relocation is
IMHO simpler to understand without prior knowledge, which will make PIC easier
to explain in the future. 

Regardless of the motivation, I hope this article has helped to shed some
light on the magic going behind the scenes of linking and loading shared
libraries in a modern OS.


={============================================================================
*kt_linux_core_400* linux-elf-pic linux-elf-plt

http://eli.thegreenplace.net/2011/11/03/position-independent-code-pic-in-shared-libraries/

Position Independent Code (PIC) in shared libraries

I've described the need for special handling of shared libraries while loading
them into the process's address space in a previous article. Briefly, when the
linker creates a shared library, it doesn't know in advance where it might be
loaded. This creates a problem for the data and code references within the
library, which should be somehow made to point to the correct memory
locations.

There are two main approaches to solve this problem in Linux ELF shared
libraries:

Load-time relocation
Position independent code (PIC)

Load-time relocation was already covered. Here, I want to explain the second
approach - PIC.

// LAL, Linkers and Loaders, 8
// 
// One popular solution to the dilemma of loading the same program at
// different addresses is position independent code (PIC). The idea is simple,
// separate the code from the data and generate code that `won’t change`
// regardless of the address at which it’s loaded. That way the code can be
// shared among all processes, with only data pages being private to each
// process.
// 
// On modern architectures, it’s not difficult to generate PIC executable
// code. Jumps and branches are generally either PC-relative or relative to a
// base register set at runtime, so no load-time relocation is required for
// them. 
// 
// The problem is with data addressing. The code can’t contain any direct data
// addresses, since those would be relocatable and wouldn’t be PIC. 
// 
// The usual solution is to create a table of data addresses in a data page
// and keep a `pointer to that table in a register`, so the code can use indexed
// addressing relative to that register to pick up the data. This works at the
// cost of an extra indirection for each data reference, but there’s still the
// question of how to get the initial data address into the register.


I originally planned to focus on both x86 and x64 (a.k.a. x86-64) in this
article, but as it grew longer and longer I decided it won't be practical. So,
it will explain only how PIC works on x86, picking this older architecture
  specifically because (unlike x64) it wasn't designed with PIC in mind, so
  implementing PIC on it is a bit trickier. A future (hopefully much shorter)
  article will build upon the foundation of this one to explain how PIC is
  implemented on x64.


Some problems of load-time relocation

As we've seen in the previous article, load-time relocation is a fairly
straightforward method, and it works. PIC, however, is much more popular
nowadays, and is usually the recommended method of building shared libraries.


Why is this so?

Load-time relocation has a couple of problems: it takes time to perform, and
it makes the text section of the library `non-shareable`.

  First, the performance problem. If a shared library was linked with
  load-time relocation entries, it will take some time to actually perform
  these relocations when the application is loaded. You may think that the
  cost shouldn't be too large - after all, the loader doesn't have to scan
  through the whole text section - it should only look at the relocation
  entries. But if a complex piece of software loads multiple large shared
  libraries at start-up, and each shared library must first have its load-time
  relocations applied, these costs can build up and result in a noticeable
  delay in the start-up time of the application.

  // this is why use pic
  Second, `the non-shareable text section problem`, which is somewhat more
  serious. One of the main points of having shared libraries in the first
  place, is saving RAM. Some common shared libraries are used by multiple
  applications. If the text section (where the code is) of the shared library
  can only be loaded into memory once (and then mapped into the virtual
      memories of many processes), considerable amounts of RAM can be saved.
  But this is not possible with load-time relocation, since when using this
  technique the text section has to be modified at load-time to apply the
  relocations. Therefore, for each application that loaded this shared
  library, it will have to be wholly placed in RAM again [1]. Different
  applications won't be able to really share it.

// [1] Unless all applications load this library into the exact same virtual
// memory address. But this usually isn't done on Linux.

Moreover, having a `writable text section` (it must be kept writable, to allow
    the dynamic loader to perform the relocations) poses a security risk,
making it easier to exploit the application.

As we'll see in this article, PIC mostly mitigates these problems.


PIC - introduction

The idea behind PIC is simple - add an additional level of indirection to all
global data and function references in the code. By cleverly utilizing some
artifacts of the linking and loading processes, it's possible to make the text
section of the shared library truly position independent, in the sense that it
can be easily mapped into different memory addresses without needing to change
one bit. In the next few sections I will explain in detail how this feat is
achieved.


<offset-between-text-and-data>
Key insight #1 - offset between text and data sections

One of the key insights on which PIC relies is the offset between the text and
data sections, `known to the linker at link-time.` When the linker combines
several object files together, it collects their sections (for example, all
    text sections get unified into a single large text section). Therefore,
the linker knows both about the sizes of the sections and about their relative
  locations.

For example, the text section may be immediately followed by the data section,
so 
  
  // way to get offset to data section
  the offset from any given instruction in the text section to the beginning
  of the data section is just the size of the text section minus the offset of
  the instruction from the beginning of the text section - and both these
  quantities are known to the linker.

In the diagram above, the code section was loaded into some address (unknown
    at link-time) 0xXXXX0000 (the X-es literally mean "don't care"), and the
data section right after it at offset 0xXXXXF000. Then, if some instruction at
offset 0x80 in the code section wants to reference stuff in the data section,
the linker knows the relative offset (0xEF80 in this case) and can encode it
  in the instruction.

Note that it wouldn't matter if another section was placed between the code
and data sections, or if the data section preceded the code section. Since the
linker knows the sizes of all sections and decides where to place them, the
insight holds.

// Since the GOT is in the same loadable ELF file as the code that references
// it, and `the relative addresses within a file don’t change` regardless of
// where the program is loaded, the code can locate the GOT with a relative
// address, load the address of the GOT into a register, and then load
// pointers from the GOT whenever it needs to address static data.


*x86-asm-get-ip*
Key insight #2 - making an IP-relative offset work on x86

// trick to get `ip or pc` and to get GOT addresss in essence

The above is only useful if we can actually put the `relative offset` to work.
But data references (i.e. in the `mov` instruction) on x86 require `absolute`
addresses. So, what can we do?

If we have a relative address and need an absolute address, what's missing is
the value of the instruction pointer (since, by definition, the relative
    address is relative to the instruction's location). There's no instruction
to obtain the value of the instruction pointer on x86, but we can use a simple
trick to get it. Here's some assembly pseudo-code that demonstrates it:

    call TMPLABEL
TMPLABEL:
    pop ebx

What happens here is:

The CPU executes call TMPLABEL, which causes it to save the address of the
next instruction (the pop ebx) on stack and jump to the label.  

Since the instruction at the label is pop ebx, it gets executed next. It pops
a value from the stack `into ebx` But this value is the address of the
instruction itself, so ebx now effectively contains the value of the
`instruction pointer.`


The Global Offset Table (GOT) *elf-got*

With this at hand, we can finally get to the implementation of
position-independent data addressing on x86. It is accomplished by means of a
GOT.

// GOT is used for global and static variables.

A GOT is simply a table of addresses, `residing in the data section.` Suppose
some instruction in the code section wants to refer to a variable. Instead of
referring to it directly by absolute address (which would require a
    relocation), it refers to an entry in the GOT. Since the GOT is in a known
place in the data section, this reference is relative and known to the linker.

The GOT entry, in turn, will contain the absolute address of the variable:

   -> +----------------------+
      | mov ...,...          | --+
text  +----------------------+   |
      |                      |   | relative offset
      |                      |   | 
   -> +----------------------|   | 
      |                      |   |
      +----------------------+   |
data  | GOT#0 var #1 address |   |
      | GOT#1 var #2 address | <-+
      | GOT#2 var #3 address |
      | GOT#3 var #4 address |
      | GOT#4 var #5 address |
      | GOT#X                |
      +----------------------|
      | data sections        |
   -> +----------------------+

In pseudo-assembly, we replace an absolute addressing instruction:

// from

; Place the value of the variable in edx
mov edx, [ADDR_OF_VAR]

// to

With displacement addressing from a register, along with an extra indirection:

; 1. `Somehow get the address of the GOT into ebx`
lea ebx, ADDR_OF_GOT

; 2. Suppose ADDR_OF_VAR is stored at offset 0x10
;    in the GOT. Then this will place ADDR_OF_VAR
;    into edx.
mov edx, DWORD PTR [ebx + 0x10]

; 3. Finally, access the variable and place its
;    value into edx.
mov edx, DWORD PTR [edx]

So, we've gotten rid of a relocation in the code section by redirecting
variable references through the GOT. But we've also created a relocation in
the data section. Why? Because the GOT still has to contain the absolute
address of the variable for the scheme described above to work. So what have
we gained?

// still need data relocation since GOT entries need absoule address


A lot, it turns out. A `relocation in the data section` is much less
problematic than one in the code section, for two reasons (which directly
    address the two main problems of load-time relocation of code described in
    the beginning of the article):

  Relocations in the code section are required per variable reference, while
    in the GOT we only need `to relocate once per variable.` There are likely
    much more references to variables than variables, so this is more
    efficient.  
  
  The data section is writable and not shared between processes anyway, so
  adding relocations to it does no harm. Moving relocations from the code
  section, however, allows to make it read-only and share it between
  processes.


<elf-pic-ex> <elf-got-ex>
PIC with `data references` through GOT - an example

I will now show a complete example that demonstrates the mechanics of PIC:

int myglob = 42;

int ml_func(int a, int b)
{
    return myglob + a + b;
}


This chunk of code will be compiled into a shared library (using the `-fpic`
    and -shared flags as appropriate) named libmlpic_dataonly.so.

Let's take a look at its disassembly, focusing on the ml_func function:

0000043c <ml_func>:
 43c:   55                      push   ebp
 43d:   89 e5                   mov    ebp,esp
 43f:   e8 16 00 00 00          call   45a <__i686.get_pc_thunk.cx>
>`444`:   81 c1 b0 1b 00 00       add    ecx,`0x1bb0`
 44a:   8b 81 f0 ff ff ff       mov    eax,DWORD PTR [ecx-0x10]
 450:   8b 00                   mov    eax,DWORD PTR [eax]
 452:   03 45 08                add    eax,DWORD PTR [ebp+0x8]
 455:   03 45 0c                add    eax,DWORD PTR [ebp+0xc]
 458:   5d                      pop    ebp
 459:   c3                      ret

0000045a <__i686.get_pc_thunk.cx>:
 45a:   8b 0c 24                mov    ecx,DWORD PTR [esp]
 45d:   c3                      ret

// from office-debian
//
// 00000530 <ml_func>:
//  530:   55                      push   ebp
//  531:   89 e5                   mov    ebp,esp
//  533:   53                      push   ebx
//  534:   e8 c7 fe ff ff          call   400 <__x86.get_pc_thunk.bx>
//  539:   81 c3 c7 1a 00 00       add    ebx,0x1ac7
//  53f:   ff 93 f0 ff ff ff       call   DWORD PTR [ebx-0x10]
//  545:   89 d8                   mov    eax,ebx
//  547:   8b 90 e8 ff ff ff       mov    edx,DWORD PTR [eax-0x18]
//  54d:   8b 0a                   mov    ecx,DWORD PTR [edx]
//  54f:   8b 55 08                mov    edx,DWORD PTR [ebp+0x8]
//  552:   01 d1                   add    ecx,edx
//  554:   8b 90 e8 ff ff ff       mov    edx,DWORD PTR [eax-0x18]
//  55a:   89 0a                   mov    DWORD PTR [edx],ecx
//  55c:   8b 80 e8 ff ff ff       mov    eax,DWORD PTR [eax-0x18]
//  562:   8b 10                   mov    edx,DWORD PTR [eax]
//  564:   8b 45 0c                mov    eax,DWORD PTR [ebp+0xc]
//  567:   01 d0                   add    eax,edx
//  569:   5b                      pop    ebx
//  56a:   5d                      pop    ebp
//  56b:   c3                      ret
//
// 00000400 <__x86.get_pc_thunk.bx>:
//  400:   8b 1c 24                mov    ebx,DWORD PTR [esp]
//  403:   c3                      ret
//  404:   66 90                   xchg   ax,ax
//  406:   66 90                   xchg   ax,ax
//  408:   66 90                   xchg   ax,ax
//  40a:   66 90                   xchg   ax,ax
//  40c:   66 90                   xchg   ax,ax
//  40e:   66 90                   xchg   ax,ax

// function-prolog
// the address of the GOT is held in the register %ecx once offset is added.
// This register is initialized at the entry to each function in position
// independent code.

I'm going to refer to instructions by their addresses (the left-most number in
    the disassembly). This address is the offset from the load address of the
shared library.

*x86-asm-get-ip*
  At 43f, the address of the next instruction is placed into ecx, by means of
  the technique described in the "key insight #2" section above.

  At 444, a known constant offset from the instruction to the place where the
  GOT is located is added to ecx. So `ecx now serves as a base pointer to GOT.`

  At 44a, a value is taken from [ecx - 0x10], which is a GOT entry, and placed
  into eax. This is the address of myglob.

  At 450 the indirection is done, and the value of myglob is placed into eax.

  Later the parameters a and b are added to myglob and the value is returned
  (by keeping it in eax).

We can also query the shared library with readelf -S to see where the GOT
section was placed:

Section Headers:
  [Nr] Name     Type            Addr     Off    Size   ES Flg Lk Inf Al
  <snip>
  [19] .got     PROGBITS        0000`1fe4` 000fe4 000010 04  WA  0   0  4
  [20] .got.plt PROGBITS        00001ff4 000ff4 000014 04  WA  0   0  4
  <snip>

Let's do some math to check the computation done by the compiler to find
myglob. As I mentioned above, the call to __i686.get_pc_thunk.cx places the
address of the next instruction into ecx. That address is 0x444 [2]. 

// [2] 0x444 (and all other addresses mentioned in this computation) is
// relative to the load address of the shared library, which is unknown until
// an executable actually loads it at runtime. Note how it doesn't matter in
// the code since it only juggles relative addresses.

// 0x1bb0 is offset to GOT as shown in Key insight #1 - offset between text
// and data sections
// 
// >>> hex(0x444+0x1bb0)  ; next addr + offset
// '0x1ff4'
//
// >>> hex(0x1ff4-0x10)   ; -0x16 to get the start of GOT
// '0x1fe4'


The next instruction then adds 0x1bb0 to it, and the result in ecx is going to
be 0x1ff4. Finally, to actually obtain the GOT entry holding the address of
myglob, displacement addressing is used - [ecx - 0x10], so the entry is at
`0x1fe4`, which is the first entry in the GOT according to the section header.

Why there's another section whose name starts with .got will be explained
later in the article [3]. Note that `the compiler chooses` to point ecx to after
the GOT and then `use negative offsets to obtain entries.` This is fine, as long
as the math works out. And so far it does.

// [3] The astute reader may wonder why .got is a separate section at all.
// Didn't I just show in the diagrams that it's located in the data section?
// In practice, it is. I don't want to get into the distinction between ELF
// sections and segments here, since that would take use too far away from the
// point. But briefly, any number of "data" sections can be defined for a
// library and mapped into a read-write segment. This doesn't really matter,
// as long as the ELF file is organized correctly. Separating the data segment
// into different logical sections provides modularity and makes the linker's
// job easier.


<got-relocation>
// Like the PLT, the GOT does not exist in an .o file, but is created by the
// program linker. The program linker will create the dynamic relocations
// which the dynamic linker will use to initialize the GOT at runtime. Unlike
// the PLT, the dynamic linker always fully initializes the GOT when the
// program starts.
//
// The program linker will create dynamic relocations for each entry in the
// GOT, telling the dynamic linker how to initialize the entry. These
// relocations are of type GLOB_DAT.

There's something we're still missing, however. How does the address of myglob
actually get into the GOT slot at 0x1fe4? Recall that I mentioned a
relocation, so let's find it:

> readelf -r libmlpic_dataonly.so

Relocation section '.rel.dyn' at offset 0x2dc contains 5 entries:
 Offset     Info    Type            Sym.Value  Sym. Name
00002008  00000008 R_386_RELATIVE
0000`1fe4`  00000406 R_386_`GLOB_DAT`    0000200c   myglob
..snip..

Note the relocation section for myglob, pointing to address 0x1fe4, as
expected. The relocation is of type R_386_GLOB_DAT, which simply tells the
dynamic loader - "put the actual value of the symbol (i.e. its address) into
that offset". 

So everything works out nicely. All that's left is to check how it actually
looks when the library is loaded. We can do this by writing a simple "driver"
executable that links to libmlpic_dataonly.so and calls ml_func, and then
running it through GDB.

*gdb-setenv*
> gdb driver
[...] skipping output
(gdb) `set environment LD_LIBRARY_PATH=.`
(gdb) `break ml_func`
[...]
(gdb) run
Starting program: [...]pic_tests/driver

Breakpoint 1, ml_func (a=1, b=1) at ml_reloc_dataonly.c:5
5         return myglob + a + b;
(gdb) set disassembly-flavor intel *gdb-disas*
(gdb) disas ml_func
Dump of assembler code for function ml_func:
   0x0013143c <+0>:   push   ebp
   0x0013143d <+1>:   mov    ebp,esp
   0x0013143f <+3>:   call   0x13145a <__i686.get_pc_thunk.cx>
   `0x00131444` <+8>:   add    ecx,0x1bb0
=> 0x0013144a <+14>:  mov    eax,DWORD PTR [ecx-0x10]
   0x00131450 <+20>:  mov    eax,DWORD PTR [eax]
   0x00131452 <+22>:  add    eax,DWORD PTR [ebp+0x8]
   0x00131455 <+25>:  add    eax,DWORD PTR [ebp+0xc]
   0x00131458 <+28>:  pop    ebp
   0x00131459 <+29>:  ret
End of assembler dump.
(gdb) i registers
eax            0x1    1
ecx            0x132ff4       1257460   ~
[...] skipping output

The debugger has entered ml_func, and stopped at IP 0x0013144a [4]. 

// ecx is 0x00131444+0x1bb0=0x132ff4

We see that ecx holds the value 0x132ff4 (which is the address of the
    instruction plus 0x1bb0, as explained before). Note that at this point, at
runtime, these are absolute addresses - the shared library has already been
loaded into the address space of the process.

So, the GOT entry for myglob is at [ecx - 0x10]. Let's check what's there:

// 0x132ff4-0x10=0x132fe4

(gdb) x 0x132fe4
0x132fe4:     0x0013300c

So, we'd expect 0x0013300c to be the address of myglob. Let's verify:

(gdb) p &myglob
$1 = (int *) 0x13300c

Indeed, it is!


Function calls in PIC

Alright, so this is how data addressing works in position independent code.
But what about function calls? Theoretically, the exact same approach could
work for function calls as well. Instead of call actually containing the
address of the function to call, let it contain the address of a known GOT
entry, and fill in that entry during loading.

But this is not how function calls work in PIC. What actually happens is a bit
more complicated. Before I explain how it's done, a few words about the
motivation for such a mechanism.


The lazy binding optimization

When a shared library refers to some function, the real address of that
function is not known until load time. Resolving this address is called
`binding`, and it's something the dynamic loader does when it loads the shared
library into the process's memory space. 

This binding process is non-trivial, since the loader has to actually look up
the function symbol in special tables [5].

// [5] Shared library ELF objects actually come with special hash table
// sections for this purpose.

So, resolving each function takes time. Not a lot of time, but it adds up
since the amount of functions in libraries is typically much larger than the
amount of global variables. Moreover, most of these resolutions are done in
vain, because in a typical run of a program only a fraction of functions
actually get called (think about various functions handling error and special
    conditions, which typically don't get called at all).

So, `to speed up this process`, a clever lazy binding scheme was devised.

"Lazy" is a generic name for a family of optimizations in computer
programming, where work is delayed until the last moment when it's actually
needed, with the intention of avoiding doing this work if its results are
never required during a specific run of a program. Good examples of laziness
are copy-on-write and lazy evaluation.

This lazy binding scheme is attained by adding yet another level of
indirection - the PLT.


The Procedure Linkage Table (PLT) *elf-plt*

<trampoline>
`The PLT is part of the executable text section`, consisting of a set of entries
(one for each external function the shared library calls). Each PLT entry is a
short chunk of executable code. Instead of calling the function directly, the
code calls an entry in the PLT, which then takes care to call the actual
function. This arrangement is sometimes called a "trampoline". Each PLT entry
also has a corresponding entry in the GOT which contains the actual offset to
the function, `but only when the dynamic loader resolves it.` I know this is
confusing, but hopefully it will be come clearer once I explain the details in
the next few paragraphs and diagrams.

// To support dynamic linking, `each ELF shared libary and each executable`
// that uses shared libraries has a Procedure Linkage Table (PLT).
// 
// Linkers, Ian Lance Taylor 
//
// Position independent code `will call non-static functions` via PLT. This
// PLT does not exist in .o files. In an .o file, use of the PLT is indicated
// by a `special relocation.` When the program linker processes such a
// relocation, it will create an entry in the PLT. It will adjust the
// instruction such that it becomes a PC-relative call to the PLT entry.
// PC-relative calls are inherently position independent and thus do not
// require a relocation entry themselves.
// 
// The program linker will `create a relocation for the PLT entry` which tells
// the dynamic linker `which symbol is associated with that entry.` This
// process reduces the number of dynamic relocations in the shared library
// from one per function call to one per function `called.`
// 
// However, by default, the dynamic linker will not actually apply a
// relocation to the PLT until some code actually calls the function in
// question. This also speeds up startup time, in that many invocations of a
// program will not call every possible function.
//
// The PLT adds a level of indirection `for function calls` analogous to that
// provided by the GOT for data. The PLT also permits `lazy evaluation`, that is,
// not resolving procedure addresses until they’re called for the first time.
// 
//       program
//    -> +--------------|
//       | call (codes) | -> to plt
// text  +--------------+
//       | PLT          | <- reference from call
//       +              | -> to got
//    -> +--------------| 
//       |              |
//       +--------------+
// data  | GOT          | -> to calls to other library
//       +--------------|    which has the same structure, plt and got
//       |              |
//    -> +--------------+
// 
// PLT in read-only text and GOT in data

As the previous section mentioned, PLTs allow lazy resolution of functions.
When the shared library is first loaded, the function calls have `not` been
resolved yet:

Explanation:

  In the code, a function func is called. The compiler translates it to a call
  to func@plt, which is some N-th entry in the PLT.

  The PLT consists of a special first entry, followed by a bunch of
  identically structured entries, one for each function needing resolution.

  <resolver>
  Each PLT entry but the first consists of these parts:
    1. A jump to a location which is specified in a corresponding GOT entry
    2. Preparation of arguments for a "resolver" routine
    3. Call to the resolver routine, which resides in the first entry of the PLT

  The first PLT entry is a `call to a resolver routine`, which is located in the
  dynamic loader itself [6]. This routine resolves the actual address of the
  function. More on its action a bit later.

// [6] The dynamic loader on Linux is just another shared library which gets
// loaded into the address space of all running processes.


  // GOT[1] GOT[2]
  //
  // Linkers, Ian Lance Taylor 
  //
  // first PLT entry is special, and looks like this:
  //
  // pushl 4(%ebx)
  // jmp *8(%ebx)
  //
  // This references the second and third entries in the GOT. The dynamic
  // linker will initialize them to have appropriate values for a callback
  // into the dynamic linker itself. The dynamic linker will use the index
  // pushed by the first code sequence to find the JMP_SLOT relocation.

  // The first PLT entry is special. This references the second and third
  // entries in the GOT. The dynamic linker will initialize them to have
  // appropriate values for a callback into the dynamic linker itself.

  Before the function's actual address has been resolved, the Nth GOT entry
  just points to after the jump. This is why this arrow in the diagram is
  colored differently - it's not an actual jump, just a pointer.

What happens when func is called for the first time is this:

  PLT[n] is called and jumps to the address pointed to in GOT[n].

  This address points into PLT[n] itself, to the preparation of arguments for
  the resolver.

  The resolver is then called.

  The resolver performs resolution of the actual address of func, places its
  actual address into GOT[n] and calls func.


Code:
  call func@PLT

PLT:
  PLT[0]:
    call resolver
  ...

  PLT[n]:                         GOT:
    jmp *GOT[n]                     GOT[n]:
    prepare resolver(push l #index)   <- addr : address of second instruction
    jmp PLT[0]                                  go back to PLT[n]


// The dynamic linker will use the index pushed by the first code sequence to
// find the JMP_SLOT relocation.

After the first call, the diagram looks a bit differently:

  PLT[n]:                         GOT:
    jmp *GOT[n]                     GOT[n]:
    prepare resolver                  addr -> `resolved address`
    jmp PLT[0]


Note that GOT[n] now points to the actual func [7] instead of back into the
PLT. So, when func is called again:

// [7] I placed func in a separate code section, although in theory this could
// be the same one where the call to func is made (i.e. in the same shared
// library). The "extra credit" section of this article has information about
// why a call to an external function in the same shared library needs PIC (or
// relocation) as well.

  PLT[n] is called and jumps to the address pointed to in GOT[n].
  GOT[n] points to func, so this just transfers control to func.

In other words, now func is being actually called, without going through the
resolver, at the cost of one additional jump. That's all there is to it,
really. 
  
// this is why use lazy and plt for function calls rather than simply using
// GOT.

This mechanism allows `lazy resolution of functions`, and no resolution at all
for functions that aren't actually called.

It also leaves the code/text section of the library completely position
independent, since the only place where an absolute address is used is the
GOT, which resides in the data section and will be relocated by the dynamic
loader. Even the PLT itself is PIC, so it can live in the read-only text
section.

I didn't get into much details regarding the resolver, but it's really not
important for our purpose here. 

<resolver-do-symbol-resolution> *ld-so-resolver*
The resolver is simply a chunk of low-level code in the loader that does
`symbol resolution.`

The arguments prepared for it in each PLT entry, along with a suitable
relocation entry, help it know about the symbol that needs resolution and
about the GOT entry to update.


<elf-got-function>
PIC with `function calls` through PLT and GOT - an example

Once again, to fortify the hard-learned theory with a practical demonstration,
     here's a complete example showing function call resolution using the
     mechanism described above. I'll be moving forward a bit faster this time.

Here's the code for the shared library:
    
// gcc -g -c -fPIC ml_main.c -o ml_mainreloc.o
// gcc -shared -o libmlreloc.so ml_mainreloc.o

int myglob = 42;

int ml_util_func(int a)
{
    return a + 1;
}

int ml_func(int a, int b)
{
    int c = b + ml_util_func(a);
    myglob += c;
    return b + myglob;
}

This code will be compiled into libmlpic.so, and the focus is going to be on
the call to ml_util_func from ml_func. Let's first disassemble ml_func:

00000477 <ml_func>:
 477:   55                      push   ebp
 478:   89 e5                   mov    ebp,esp
 47a:   53                      push   ebx
 47b:   83 ec 24                sub    esp,0x24
 47e:   e8 e4 ff ff ff          call   467 <__i686.get_pc_thunk.bx>
>483:   81 c3 71 1b 00 00       add    ebx,0x1b71
 489:   8b 45 08                mov    eax,DWORD PTR [ebp+0x8]
 48c:   89 04 24                mov    DWORD PTR [esp],eax
 48f:   e8 0c ff ff ff          call   3a0 <ml_util_func@plt> ~
 <... snip more code>

The interesting part is the call to ml_util_func@plt. Note also that the
address of GOT is in `ebx` (this time). Here's what ml_util_func@plt looks
like (it's in an executable section called .plt):

000003a0 <ml_util_func@plt>:
 3a0:   ff a3 14 00 00 00       jmp    DWORD PTR [ebx+0x14]
 `3a6`:   68 10 00 00 00          push   0x10
 3ab:   e9 c0 ff ff ff          jmp    370 <_init+0x30>

Recall that each PLT entry consists of three parts:

1. A jump to an address specified in GOT (this is the jump to [ebx+`0x14`])
2. Preparation of arguments for the resolver
3. Call to the resolver

The resolver (PLT entry 0) resides at address 0x370, but it's of no interest
to us here. What's more interesting is to see what the GOT contains. For that,
   we first have to do some math.

The "get IP" trick in ml_func was done on address 0x483, to which 0x1b71 is
added. So the base of the GOT is at 0x1ff4. We can take a peek at the GOT
contents with readelf [8]:

// [8] Recall that in the data reference example I promised to explain why
// there are apparently two GOT sections in the object: `.got and .got.plt` 
//
// Now it should become obvious that this is just to conveniently split the
// GOT entries required for global data from GOT entries required for the PLT.
// This is also why when the GOT offset is computed in functions, it points to
// .got.plt, which comes right after .got. This way, negative offsets lead us
// to .got, while positive offsets lead us to .got.plt. While convenient, such
// an arrangement is by no means compulsory. Both parts could be placed into a
// single .got section.

> readelf -x `.got.plt` libmlpic.so

Hex dump of section '.got.plt':
  0x00001ff4 241f0000 00000000 00000000 86030000 $...............
  0x00002004 96030000 a6030000                   ........
                      ^^^^

// >>> hex(0x2004-0x1ff4)
// '0x10'

The GOT entry ml_util_func@plt looks at is at offset +0x14, or `0x2008`. From
above, the word at that location is 0x3a6, which is the address of the push
instruction in ml_util_func@plt. (see above)

To help the dynamic loader do its job, a relocation entry is also added and
specifies which place in the GOT to relocate for ml_util_func:

> readelf -r libmlpic.so
[...] snip output

Relocation section '.rel.plt' at offset 0x328 contains 3 entries:
 Offset     Info    Type            Sym.Value  Sym. Name
00002000  00000107 R_386_JUMP_SLOT   00000000   __cxa_finalize
00002004  00000207 R_386_JUMP_SLOT   00000000   __gmon_start__
0000`2008`  00000707 R_386_`JUMP_SLOT`   0000046c   ml_util_func

// this is relocation for got.plt and relocation entry is kind of command to
// dynamic loader

The last line means that the dynamic loader should place the value (address)
of symbol ml_util_func into 0x2008 (which, recall, is the GOT entry for this
function).

// go to 00002008 and update it with 0000046c

It would be interesting to see this GOT entry modification actually happen
after the first call. Let's once again use GDB for the inspection.

> gdb driver
[...] skipping output
(gdb) set environment LD_LIBRARY_PATH=.
(gdb) break ml_func
Breakpoint 1 at 0x80483c0
(gdb) run
Starting program: /pic_tests/driver

Breakpoint 1, ml_func (a=1, b=1) at ml_main.c:10
10        int c = b + ml_util_func(a);
(gdb)

// is inside ml_main

We're now before the first call to ml_util_func. Recall that GOT is pointed to
by ebx in this code. Let's see what's in it:

(gdb) i registers ebx
ebx            0x132ff4

And the offset to the entry we need is at [ebx+0x14]:

(gdb) x/w 0x133008
0x133008:     0x001313a6

Yep, the 0x3a6 ending, looks right. Now, let's step until `after the call` to
ml_util_func and check again:

// It would be interesting to see this GOT entry modification actually happen
// after the first call. 

(gdb) step
ml_util_func (a=1) at ml_main.c:5
5         return a + 1;

(gdb) x/w 0x133008
0x133008:     0x0013146c

The value at 0x133008 was changed. Hence, 0x0013146c should be the real
address of ml_util_func, placed in there by the dynamic loader:

(gdb) p &ml_util_func
$1 = (int (*)(int)) 0x13146c <ml_util_func>

Just as expected.


Controlling if and when the resolution is done by the loader

This would be a good place to mention that the process of lazy symbol
resolution performed by the dynamic loader can be configured with some
environment variables (and corresponding flags to ld when linking the shared
    library). This is sometimes useful for special performance requirements or
debugging.

The LD_BIND_NOW env var, when defined, tells the dynamic loader to always
perform the resolution for all symbols at start-up time, and not lazily. You
can easily verify this in action by setting this env var and re-running the
previous sample with GDB. You'll see that the GOT entry for ml_util_func
contains its real address even before the first call to the function.

Conversely, the LD_BIND_NOT env var tells the dynamic loader not to update the
GOT entry at all. Each call to an external function will then go through the
dynamic loader and be resolved anew.

The dynamic loader is configurable by other flags as well. I encourage you to
go over man ld.so - it contains some interesting information.


The costs of PIC

This article started by stating the problems of load-time relocation and how
the PIC approach fixes them. But PIC is also not without problems. 

One immediately apparent cost is the extra indirection required for all
external references to data and code in PIC. That's an extra memory load for
each reference to a global variable, and for each call to a function. How
problematic this is in practice depends on the compiler, the CPU architecture
and the particular application.

// register for GOT?

Another, less apparent cost, is the increased register usage required to
implement PIC. In order to avoid locating the GOT too frequently, it makes
sense for the compiler to generate code that keeps its address in a register
(usually ebx). But that ties down a whole register just for the sake of GOT.

While not a big problem for RISC architectures that tend to have a lot of
general purposes registers, it presents a performance problem for
architectures like x86, which has a small amount of registers. PIC means
having one general purpose register less, which adds up indirect costs since
now more memory references have to be made.


Conclusion

This article explained what position independent code is, and how it helps
create shared libraries with shareable read-only text sections. There are some
tradeoffs when choosing between PIC and its alternative (load-time
    relocation), and the eventual outcome really depends on a lot of factors,
          like the CPU architecture on which the program is going to run.

That said, PIC is becoming more and more popular. Some non-Intel architectures
like SPARC64 force PIC-only code for shared libraries, and many others (for
    example, ARM) include IP-relative addressing modes to make PIC more
efficient. Both are true for the successor of x86, the x64 architecture. I
will discuss PIC on x64 in a future article.

The focus of this article, however, has not been on performance considerations
or architectural decisions. My aim was to explain, given that PIC is used, how
it works. If the explanation wasn't clear enough - please let me know in the
comments and I will try to provide more information.


note:
All relocations in PIC uses symbol values(actual absolute address) from
relocation entry of dynamic section which is the same as ones from symbol
tables. That means program linker knows all and makes dynamic sections for all
relocations.


<elf-pic-64>
https://eli.thegreenplace.net/2011/11/11/position-independent-code-pic-in-shared-libraries-on-x64

The previous article explained how position independent code (PIC) works, with
code compiled for the x86 architecture as an example. I promised to cover PIC
on x64 [1] in a separate article, so here we are. This article will go into
much less detail, since it assumes an understanding of how PIC works in
theory. In general, the idea is similar for both platforms, but some details
differ because of unique features of each architecture.

RIP-relative addressing

On x86, while function references (with the call instruction) use relative
offsets from the instruction pointer, data references (with the mov
    instruction) only support absolute addresses. As we've seen in the
previous article, this makes PIC code somewhat less efficient, since PIC by
its nature requires making all offsets IP-relative; absolute addresses and
position independence don't go well together.

x64 fixes that, with a new "RIP-relative addressing mode", which is the
default for all 64-bit mov instructions that reference memory (it's used for
    other instructions as well, such as lea). A quote from the "Intel
Architecture Manual vol 2a":

  A new addressing form, RIP-relative (relative instruction-pointer)
  addressing, is implemented in 64-bit mode. An effective address is formed by
  adding displacement to the 64-bit RIP of the next instruction.  The
  displacement used in RIP-relative mode is 32 bits in size. Since it should
  be useful for both positive and negative offsets, roughly +/- 2GB is the
  maximal offset from RIP supported by this addressing mode.

x64 PIC with data references - an example

For easier comparison, I will use the same C source as in the data reference
example of the previous article:

int myglob = 42;

int ml_func(int a, int b)
{
    return myglob + a + b;
}

Let's look at the disassembly of ml_func:

00000000000005ec <ml_func>:
 5ec:   55                      push   rbp
 5ed:   48 89 e5                mov    rbp,rsp
 5f0:   89 7d fc                mov    DWORD PTR [rbp-0x4],edi
 5f3:   89 75 f8                mov    DWORD PTR [rbp-0x8],esi
>5f6:   48 8b 05 db 09 20 00    mov    rax,QWORD PTR [rip+0x2009db]
 5fd:   8b 00                   mov    eax,DWORD PTR [rax]
 5ff:   03 45 fc                add    eax,DWORD PTR [rbp-0x4]
 602:   03 45 f8                add    eax,DWORD PTR [rbp-0x8]
 605:   c9                      leave
 606:   c3                      ret

The most interesting instruction here is at 0x5f6: it places the address of
myglobal into rax, by referencing an entry in the GOT. As we can see, it uses
RIP relative addressing. Since it's relative to the address of the next
instruction, what we actually get is 0x5fd + 0x2009db = `0x200fd8`. So the GOT
entry holding the address of myglob is at 0x200fd8. Let's check if it makes
sense:

$ readelf -S libmlpic_dataonly.so
There are 35 section headers, starting at offset 0x13a8:

Section Headers:
  [Nr] Name              Type             Address           Offset
       Size              EntSize          Flags  Link  Info  Align

[...]
  [20] .got              PROGBITS         0000000000`200fc8`  00000fc8
       0000000000000020  0000000000000008  WA       0     0     8
[...]

// >>> hex(0x200fc8+8)
// '0x200fd0'
// >>> hex(0x200fc8+16)
// '0x200fd8'

GOT starts at 0x200fc8, so myglob is in its third entry. We can also see the
relocation inserted for the GOT reference to myglob:

$ readelf -r libmlpic_dataonly.so

Relocation section '.rela.dyn' at offset 0x450 contains 5 entries:

  Offset          Info           Type           Sym. Value    Sym. Name + Addend
[...]
000000200fd8  000500000006 R_X86_64_GLOB_DAT 0000000000201010 myglob + 0
[...]

// still not actual address but has all `instruction` to make all right when
// loaded.

Indeed, a relocation entry for 0x200fd8 telling the dynamic linker to place
the address of myglob into it once the final address of this symbol is known.

So it should be quite clear how the address of myglob is obtained in the code.
The next instruction in the disassembly (at 0x5fd) then dereferences the
address to get the value of myglob into eax [2].

// [2] Into eax and not rax because the type of myglob is int, which is still
// 32-bit on x64.


<ex>
In x86 assembly language WORD , DOUBLEWORD (DWORD) and QUADWORD (QWORD)
are used for 2, 4 and 8 byte sizes

$ objdump -d -Mintel ml_main.o 

ml_main.o:     file format elf64-x86-64

Disassembly of section .text:

0000000000000000 <ml_func>:
   0:   55                      push   rbp
   1:   48 89 e5                mov    rbp,rsp
   4:   89 7d fc                mov    DWORD PTR [rbp-0x4],edi
   7:   89 75 f8                mov    DWORD PTR [rbp-0x8],esi
   a:   48 8b 05 00 00 00 00    mov    rax,QWORD PTR [rip+0x0]        # 11 <ml_func+0x11>
  11:   8b 10                   mov    edx,DWORD PTR [rax]
  13:   8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]
  16:   01 c2                   add    edx,eax
  18:   48 8b 05 00 00 00 00    mov    rax,QWORD PTR [rip+0x0]        # 1f <ml_func+0x1f>
  1f:   89 10                   mov    DWORD PTR [rax],edx
  21:   48 8b 05 00 00 00 00    mov    rax,QWORD PTR [rip+0x0]        # 28 <ml_func+0x28>
  28:   8b 10                   mov    edx,DWORD PTR [rax]
  2a:   8b 45 f8                mov    eax,DWORD PTR [rbp-0x8]
  2d:   01 d0                   add    eax,edx
  2f:   5d                      pop    rbp
  30:   c3                      ret    


// as you can see, modified with offset

$ objdump -d -Mintel libdata.so

0000000000000690 <ml_func>:
 690:   55                      push   rbp
 691:   48 89 e5                mov    rbp,rsp
 694:   89 7d fc                mov    DWORD PTR [rbp-0x4],edi
 697:   89 75 f8                mov    DWORD PTR [rbp-0x8],esi
 69a:   48 8b 05 97 02 20 00    mov    rax,QWORD PTR [rip+0x200297]        # 200938 <_DYNAMIC+0x1c8>
 6a1:   8b 10                   mov    edx,DWORD PTR [rax]
 6a3:   8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]
 6a6:   01 c2                   add    edx,eax
 6a8:   48 8b 05 89 02 20 00    mov    rax,QWORD PTR [rip+0x200289]        # 200938 <_DYNAMIC+0x1c8>
 6af:   89 10                   mov    DWORD PTR [rax],edx
 6b1:   48 8b 05 80 02 20 00    mov    rax,QWORD PTR [rip+0x200280]        # 200938 <_DYNAMIC+0x1c8>
 6b8:   8b 10                   mov    edx,DWORD PTR [rax]
 6ba:   8b 45 f8                mov    eax,DWORD PTR [rbp-0x8]
 6bd:   01 d0                   add    eax,edx
 6bf:   5d                      pop    rbp
 6c0:   c3                      ret    

$ readelf -S
  [19] .got              PROGBITS         0000000000200930  00000930
       0000000000000030  0000000000000008  WA       0     0     8
  [20] .got.plt          PROGBITS         0000000000200960  00000960
       0000000000000028  0000000000000008  WA       0     0     8

// the 2nd element from .got start, 200930
>>> hex(0x200297+0x6a1)
'0x200938'

$ readelf -r libdata.so                                                                                                                                                             

Relocation section '.rela.dyn' at offset 0x438 contains 9 entries:
  Offset          Info           Type           Sym. Value    Sym. Name + Addend
000000200938  000700000006 R_X86_64_GLOB_DAT 0000000000200990 myglob + 0

// output from driver

name=./libdata.so (6 segments) address=0x7ffff7bdb000
                 header  0: address=0x7ffff7bdb000
                         type=1, flags=0x5
                 header  1: address=0x7ffff7ddb758
                         type=1, flags=0x6
                 header  2: address=0x7ffff7ddb770
                         type=2, flags=0x6
                 header  3: address=0x7ffff7bdb190
                         type=4, flags=0x4
                 header  4: address=0x7ffff7bdb6d0
                         type=1685382480, flags=0x4
                 header  5: address=0x7ffff7bdb000
                         type=1685382481, flags=0x6


(gdb) disas ml_func
Dump of assembler code for function ml_func:
   0x00007ffff7bdb690 <+0>:     push   rbp
   0x00007ffff7bdb691 <+1>:     mov    rbp,rsp
   0x00007ffff7bdb694 <+4>:     mov    DWORD PTR [rbp-0x4],edi
   0x00007ffff7bdb697 <+7>:     mov    DWORD PTR [rbp-0x8],esi
=> 0x00007ffff7bdb69a <+10>:    mov    rax,QWORD PTR [rip+0x200297]        # 0x7ffff7ddb938
   0x00007ffff7bdb6a1 <+17>:    mov    edx,DWORD PTR [rax]
   0x00007ffff7bdb6a3 <+19>:    mov    eax,DWORD PTR [rbp-0x4]
   0x00007ffff7bdb6a6 <+22>:    add    edx,eax
   0x00007ffff7bdb6a8 <+24>:    mov    rax,QWORD PTR [rip+0x200289]        # 0x7ffff7ddb938
   0x00007ffff7bdb6af <+31>:    mov    DWORD PTR [rax],edx
   0x00007ffff7bdb6b1 <+33>:    mov    rax,QWORD PTR [rip+0x200280]        # 0x7ffff7ddb938
   0x00007ffff7bdb6b8 <+40>:    mov    edx,DWORD PTR [rax]
   0x00007ffff7bdb6ba <+42>:    mov    eax,DWORD PTR [rbp-0x8]
   0x00007ffff7bdb6bd <+45>:    add    eax,edx
   0x00007ffff7bdb6bf <+47>:    pop    rbp
   0x00007ffff7bdb6c0 <+48>:    ret    
End of assembler dump.

// .got start + loaded base
>>> hex(0x200930+0x00007ffff7bdb000)
'0x7ffff7ddb930'

// the satrt address of 2nd .got element and note that it's 64 bits(8 bytes)
>>> hex(0x200297+0x00007ffff7bdb6a1)
'0x7ffff7ddb938'

(gdb) x/4xw 0x7ffff7ddb930
0x7ffff7ddb930: 0x00000000      0x00000000      0xf7ddb990      0x00007fff
(gdb) x/2xg 0x7ffff7ddb930
0x7ffff7ddb930: 0x0000000000000000      0x00007ffff7ddb990

// 0x7fff f7dd b938
(gdb) x/xg 0x7ffff7ddb938 
0x7ffff7ddb938: 0x00007ffff7ddb990

// myglob loaded address
(gdb) p &myglob
$2 = (int *) 0x7ffff7ddb990 <myglob>



x64 PIC with function calls - an example

Now let's see how function calls work with PIC code on x64. Once again, we'll
use the same example from the previous article:

int myglob = 42;

int ml_util_func(int a)
{
    return a + 1;
}

int ml_func(int a, int b)
{
    int c = b + ml_util_func(a);
    myglob += c;
    return b + myglob;
}

Disassembling ml_func, we get:

000000000000064b <ml_func>:
 64b:   55                      push   rbp
 64c:   48 89 e5                mov    rbp,rsp
 64f:   48 83 ec 20             sub    rsp,0x20
 653:   89 7d ec                mov    DWORD PTR [rbp-0x14],edi
 656:   89 75 e8                mov    DWORD PTR [rbp-0x18],esi
 659:   8b 45 ec                mov    eax,DWORD PTR [rbp-0x14]
 65c:   89 c7                   mov    edi,eax
 65e:   e8 fd fe ff ff          call   560 <ml_util_func@plt>
 [... snip more code ...]

The call is, as before, to ml_util_func@plt. Let's see what's there:

0000000000000560 <ml_util_func@plt>:
 560:   ff 25 a2 0a 20 00       jmp    QWORD PTR [rip+0x200aa2]
 566:   68 01 00 00 00          push   0x1
 56b:   e9 d0 ff ff ff          jmp    540 <_init+0x18>

So, the GOT entry holding the actual address of ml_util_func is at 0x200aa2 +
0x566 = `0x201008`.

And there's a relocation for it, as expected:

$ readelf -r libmlpic.so

Relocation section '.rela.dyn' at offset 0x480 contains 5 entries:
[...]

Relocation section '.rela.plt' at offset 0x4f8 contains 2 entries:
  Offset          Info           Type           Sym. Value    Sym. Name + Addend
[...]
000000201008  000600000007 R_X86_64_JUMP_SLO 000000000000063c ml_util_func + 0


<ex>
$ objdump -d -Mintel libdata.so

00000000000005d0 <ml_util_func@plt>:
 5d0:   ff 25 52 04 20 00       jmp    QWORD PTR [rip+0x200452]        # 200a28 <_GLOBAL_OFFSET_TABLE_+0x20>
 5d6:   68 01 00 00 00          push   0x1
 5db:   e9 d0 ff ff ff          jmp    5b0 <_init+0x28>

00000000000006f0 <ml_util_func>:
 6f0:   55                      push   rbp
 6f1:   48 89 e5                mov    rbp,rsp
 6f4:   89 7d fc                mov    DWORD PTR [rbp-0x4],edi
 6f7:   8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]
 6fa:   83 c0 01                add    eax,0x1
 6fd:   5d                      pop    rbp
 6fe:   c3                      ret    

00000000000006ff <ml_func>:
 6ff:   55                      push   rbp
 700:   48 89 e5                mov    rbp,rsp
 703:   48 83 ec 20             sub    rsp,0x20
 707:   89 7d ec                mov    DWORD PTR [rbp-0x14],edi
 70a:   89 75 e8                mov    DWORD PTR [rbp-0x18],esi
 70d:   8b 45 ec                mov    eax,DWORD PTR [rbp-0x14]
 710:   89 c7                   mov    edi,eax
 712:   e8 b9 fe ff ff          call   5d0 <ml_util_func@plt>
 .. snip ..


  [19] .got              PROGBITS         00000000002009d8  000009d8
       0000000000000030  0000000000000008  WA       0     0     8
  [20] .got.plt          PROGBITS         0000000000`200a08`  00000a08
       0000000000000030  0000000000000008  WA       0     0     8

// got for ml_util_func@plt
>>> hex(0x200452+0x5d6)
'0x200a28'


Relocation section '.rela.plt' at offset 0x540 contains 3 entries:
  Offset          Info           Type           Sym. Value    Sym. Name + Addend
000000200a20  000300000007 R_X86_64_JUMP_SLO 0000000000000000 __gmon_start__ + 0
000000`200a28`  000a00000007 R_X86_64_JUMP_SLO 00000000000006f0 ml_util_func + 0
000000200a30  000600000007 R_X86_64_JUMP_SLO 0000000000000000 __cxa_finalize + 0

name=./libfunc.so (6 segments) address=0x7ffff7bdb000
                 header  0: address=0x7ffff7bdb000
                         type=1, flags=0x5
                 header  1: address=0x7ffff7ddb810
                         type=1, flags=0x6
                 header  2: address=0x7ffff7ddb828
                         type=2, flags=0x6
                 header  3: address=0x7ffff7bdb190
                         type=4, flags=0x4
                 header  4: address=0x7ffff7bdb760
                         type=1685382480, flags=0x4
                 header  5: address=0x7ffff7bdb000
                         type=1685382481, flags=0x6

(gdb) disas ml_func
Dump of assembler code for function ml_func:
   0x00007ffff7bdb6ff <+0>:     push   rbp
   0x00007ffff7bdb700 <+1>:     mov    rbp,rsp
   0x00007ffff7bdb703 <+4>:     sub    rsp,0x20
   0x00007ffff7bdb707 <+8>:     mov    DWORD PTR [rbp-0x14],edi
   0x00007ffff7bdb70a <+11>:    mov    DWORD PTR [rbp-0x18],esi
=> 0x00007ffff7bdb70d <+14>:    mov    eax,DWORD PTR [rbp-0x14]
   0x00007ffff7bdb710 <+17>:    mov    edi,eax
   0x00007ffff7bdb712 <+19>:    call   0x7ffff7bdb5d0 <ml_util_func@plt>
  .. snip ..

(gdb) disas 0x7ffff7bdb5d0
Dump of assembler code for function ml_util_func@plt:
   0x00007ffff7bdb5d0 <+0>:     jmp    QWORD PTR [rip+0x200462]        # 0x7ffff7ddba38
   0x00007ffff7bdb5d6 <+6>:     push   0x1
   0x00007ffff7bdb5db <+11>:    jmp    0x7ffff7bdb5b0
End of assembler dump.

// got.plt entry
>>> hex(0x00007ffff7bdb5d6+0x200462)
'0x7ffff7ddba38'

// can see it's not updated yet. 
(gdb) x/wg 0x7ffff7ddba38
0x7ffff7ddba38: 0x00007ffff7bdb5d6

// udpated after function is called
(gdb) x/wg 0x7ffff7ddba38
0x7ffff7ddba38: 0x00007ffff7bdb6f0

(gdb) disas 0x00007ffff7bdb6f0
Dump of assembler code for function ml_util_func:
   0x00007ffff7bdb6f0 <+0>:     push   rbp
   0x00007ffff7bdb6f1 <+1>:     mov    rbp,rsp
   0x00007ffff7bdb6f4 <+4>:     mov    DWORD PTR [rbp-0x4],edi
   0x00007ffff7bdb6f7 <+7>:     mov    eax,DWORD PTR [rbp-0x4]
   0x00007ffff7bdb6fa <+10>:    add    eax,0x1
   0x00007ffff7bdb6fd <+13>:    pop    rbp
   0x00007ffff7bdb6fe <+14>:    ret    
End of assembler dump.


Performance implications

In both examples, it can be seen that PIC on x64 requires `less instructions`
than on x86. On x86, the GOT address is loaded into some base register (ebx by
    convention) in two steps - first the address of the instruction is
obtained with a special function call, and then the offset to GOT is added.

Both steps aren't required on x64, since the relative offset to GOT is known
to the linker and can simply be `encoded in the instruction` itself with RIP
relative addressing.

When calling a function, there's also no need to prepare the GOT address in
ebx for the trampoline, as the x86 code does, since the trampoline just
accesses its GOT entry directly through RIP-relative addressing.

So PIC on x64 still requires extra instructions when compared to non-PIC code,
   but the additional cost is smaller. The indirect cost of tying down a
   register to use as the GOT pointer (which is painful on x86) is also gone,
   since no such register is needed with RIP-relative addressing [3]. All in
   all, x64 PIC results in a much smaller performance hit than on x86, making
   it much more attractive. So attractive, in fact, that it's the default
   method for writing shared libraries for this architecture.

// [3] By the way, it would be much less "painful" to tie down a register on
// x64, since it has twice as many GPRs as x86


Extra credit: Non-PIC code on x64

Not only does gcc encourage you to use PIC for shared libraries on x64, it
`requires it by default.` For instance, if we compile the first example without
-fpic [4] and then try to link it into a shared library (with -shared), we'll
get an error from the linker, something like this:

// [4] It also happens if we explicitly specify we don't want PIC by passing
// -fno-pic to gcc.

/usr/bin/ld: ml_nopic_dataonly.o: relocation R_X86_64_PC32 against 
  symbol `myglob' can not be used when making a shared object; recompile with -fPIC
/usr/bin/ld: final link failed: Bad value
collect2: ld returned 1 exit status

What's going on? Let's look at the disassembly of ml_nopic_dataonly.o [5]:

// [5] Note that unlike other disassembly listings we've been looking at in
// this and the previous article, this is an object file, not a shared library
// or executable. Therefore it will contain some relocations for the linker.

0000000000000000 <ml_func>:
   0:   55                      push   rbp
   1:   48 89 e5                mov    rbp,rsp
   4:   89 7d fc                mov    DWORD PTR [rbp-0x4],edi
   7:   89 75 f8                mov    DWORD PTR [rbp-0x8],esi
>  a:   8b 05 00 00 00 00       mov    eax,DWORD PTR [rip+0x0]
  10:   03 45 fc                add    eax,DWORD PTR [rbp-0x4]
  13:   03 45 f8                add    eax,DWORD PTR [rbp-0x8]
  16:   c9                      leave
  17:   c3                      ret

Note how myglob is accessed here, in instruction at address 0xa. It expects
the linker to patch in a relocation to the actual location of myglob into the
operand of the instruction (so no GOT redirection is required):

$ readelf -r ml_nopic_dataonly.o

Relocation section '.rela.text' at offset 0xb38 contains 1 entries:
  Offset          Info           Type           Sym. Value    Sym. Name + Addend
00000000000c  000f00000002 R_X86_64_PC32     0000000000000000 myglob - 4
[...]

Here is the R_X86_64_PC32 relocation the linker was complaining about. It just
can't link an object with such relocation into a shared library. Why? Because
the displacement of the mov (the part that's added to rip) must fit in 32
bits, and when a code gets into a shared library, we just can't know in
advance that 32 bits will be enough. After all, this is a full 64-bit
architecture, with a vast address space. The symbol may eventually be found in
some shared library that's farther away from the reference than 32 bits will
allow to reference. This makes R_X86_64_PC32 an invalid relocation for shared
libraries on x64.

But can we still somehow create non-PIC code on x64? Yes! We should be
instructing the compiler to use the "large code model", by adding the
-mcmodel=large flag. The topic of code models is interesting, but explaining
it would just take us too far from the real goal of this article [6]. So I'll
just say briefly that a code model is a kind of agreement between the
programmer and the compiler, where the programmer makes a certain promise to
the compiler about the size of offsets the program will be using. In exchange,
    the compiler can generate better code.

It turns out that to make the compiler generate non-PIC code on x64 that
actually pleases the linker, only the large code model is suitable, because
it's the least restrictive. Remember how I explained why the simple relocation
isn't good enough on x64, for fear of an offset which will get farther than 32
bits away during linking? Well, the large code model basically gives up on all
offset assumptions and uses the largest 64-bit offsets for all its data
references. This makes load-time relocations always safe, and enables non-PIC
code generation on x64. Let's see the disassembly of the first example
compiled without -fpic and with -mcmodel=large:

0000000000000000 <ml_func>:
   0:   55                      push   rbp
   1:   48 89 e5                mov    rbp,rsp
   4:   89 7d fc                mov    DWORD PTR [rbp-0x4],edi
   7:   89 75 f8                mov    DWORD PTR [rbp-0x8],esi
   a:   48 b8 00 00 00 00 00    mov    rax,0x0
  11:   00 00 00
  14:   8b 00                   mov    eax,DWORD PTR [rax]
  16:   03 45 fc                add    eax,DWORD PTR [rbp-0x4]
  19:   03 45 f8                add    eax,DWORD PTR [rbp-0x8]
  1c:   c9                      leave
  1d:   c3                      ret

The instruction at address 0xa places the address of myglob into eax. Note
that its argument is currently 0, which tells us to expect a relocation. Note
also that it has a full 64-bit address argument. Moreover, the argument is
absolute and not RIP-relative [7]. Note also that two instructions are
actually required here to get the value of myglob into eax. This is one reason
why the large code model is less efficient than the alternatives.

Now let's see the relocations:

$ readelf -r ml_nopic_dataonly.o

Relocation section '.rela.text' at offset 0xb40 contains 1 entries:
  Offset          Info           Type           Sym. Value    Sym. Name + Addend
00000000000c  000f00000001 R_X86_64_64       0000000000000000 myglob + 0
[...]

Note the relocation type has changed to R_X86_64_64, which is an absolute
relocation that can have a 64-bit value. It's acceptable by the linker, which
will now gladly agree to link this object file into a shared library.

Some judgmental thinking may bring you to ponder why the compiler generated
code that isn't suitable for load-time relocation by default. The answer to
this is simple. Don't forget that code also tends to get directly linked into
executables, which don't require load-time relocations at all. Therefore, by
default the compiler assumes the small code model to generate the most
efficient code. If you know your code is going to get into a shared library,
          and you don't want PIC, then just tell it to use the large code
          model explicitly. I think gcc's behavior makes sense here.

Another thing to think about is why there are no problems with PIC code using
the small code model. The reason is that the GOT is always located in the same
shared library as the code that references it, and unless a single shared
library is big enough for a 32-bit address space, there should be no problems
addressing the PIC with 32-bit RIP-relative offsets. Such huge shared
libraries are unlikely, but in case you're working on one, the AMD64 ABI has a
"large PIC code model" for this purpose.

Conclusion

This article complements its predecessor by showing how PIC works on the x64
architecture. This architecture has a new addressing mode that helps PIC code
be faster, and thus makes it more desirable for shared libraries than on x86,
   where the cost is higher. Since x64 is currently the most popular
   architecture used in servers, desktops and laptops, this is important to
   know. Therefore, I tried to focus on additional aspects of compiling code
   into shared libraries, such as non-PIC code. If you have any questions
   and/or suggestions on future directions to explore, please let me know in
   the comments or by email.


={============================================================================
*kt_linux_core_400* linux-elf-loading

LAL, Linkers and Loaders, 3

<elf-interp-section>
An unusual section type is .interp which contains the name of a program to use
as an interpreter. If this section is present, rather than running the program
directly, the system `runs the interpreter and passes it the ELF file` as an
argument. Unix has for many years had self-running interpreted text files,
using

#! /path/to/interpreter

as the first line of the file. ELF extends this facility to interpreters which
run non-text programs. In practice this is used to call the run-time dynamic
linker to load the program and link in any required shared libraries.

The auxiliary vector contains beside the two aforementioned values several
more values which allow the dynamic linker to avoid several systam calls. The
elf.h defines a number of AT_ prefix. These are the tags for the entries in
the vector.

/* Auxiliary vector.  */

/* This vector is normally only used by the program interpreter.  The
   usual definition in an ABI supplement uses the name auxv_t.  The
   vector is not usually defined in a standard <elf.h> file, but it
   can't hurt.  We rename it to avoid conflicts.  The sizes of these
   types are an arrangement between the exec server and the program
   interpreter, so we don't fully specify them here.  */

typedef struct
{
  uint32_t a_type;		/* Entry type */
  union
    {
      uint32_t a_val;		/* Integer value */
      /* We use to have pointer elements added here.  We cannot do that,
	 though, since it does not work when using 32-bit definitions
	 on 64-bit platforms and vice versa.  */
    } a_un;
} Elf32_auxv_t;

/* Legal values for a_type (entry type).  */

#define AT_NULL		0		/* End of vector */
#define AT_IGNORE	1		/* Entry should be ignored */
#define AT_EXECFD	2		/* File descriptor of program */
#define AT_PHDR		3		/* Program headers for program */
#define AT_PHENT	4		/* Size of program header entry */
#define AT_PHNUM	5		/* Number of program headers */
#define AT_PAGESZ	6		/* System page size */
#define AT_BASE		7		/* Base address of interpreter */
#define AT_ENTRY	9		/* Entry point of program */


See *linux-elf-source*

// from dl-startup.c of uclubc
//
// * Fortunately, the linker itself leaves a few clues lying around, and when the
// * kernel starts the image, there are a few further clues.  First of all, there
// * is Auxiliary Vector Table information sitting on which is provided to us by
// * the kernel, and which includes information about the load address that the
// * program interpreter was loaded at, the number of sections, the address the
// * application was loaded at and so forth.  Here this information is stored in
// * the array auxvt.  For details see linux/fs/binfmt_elf.c where it calls
// * NEW_AUX_ENT() a bunch of time....


<address-binding>
The basic job of any linker or loader is simple: it `binds` more abstract names
to more concrete names, which permits programmers to write code using the more
abstract names. That is, it takes a name written by a programmer such as
getline and binds it to "the location 612 bytes from the beginning of the
executable code in module iosys."


Address binding: a historical perspective

Libraries of code compound the `address assignment problem.`

The `relocating loader` allowed the authors and users of the subprograms to
write each subprogram as though it would start at location zero, and to defer
the actual address binding until the subprograms were linked with a particular
main program.

But with operating systems, the program had to share the computer’s memory
with the operating system and perhaps even with other programs, This means
that the actual addresses at which the program would be running weren’t known
until the operating system loaded the program into memory, deferring final
address binding past link time to load time.

Linkers and loaders now divided up the work, with linkers doing `part` of the
address binding, assigning relative addresses within each program, and the
loader doing a `final relocation` step to assign actual addresses.

note:
link time binding(relocation) and load time or run time address
binding(relocation).

  Relocation is the process of assigning load addresses to the various parts
  of the program, adjusting the code and data in the program to reflect the
  assigned addresses. In many systems, relocation happens more than once. It’s
  quite common for a linker to create a program from multiple subprograms, and
  create one linked output program that starts at zero, with the various
  subprograms relocated to locations within the big program. 

  Then when the program is loaded, the system picks the actual load address
  and the linked program is relocated as a whole to the load address.


But computers with hardware relocation invariably run more than one program,
frequently multiple copies of the same program. Some parts of the program are
  the same among all running instance (the executable code, in particular),
while other parts are unique to each instance.

Compilers and assemblers were modified to create object code in multiple
  sections, with one section for read only code and another section for
  writable data, the linker had to be able to combine all of sections of each
  type so that the linked program would have all the code in one place and all
  of the data in another.

Most systems now provide shared libraries for programs to use, so that all the
programs that use a library can share a single copy of it.

Sometimes the binding is delayed even farther than that; with full-fledged
dynamic linking, the addresses of called procedures aren’t bound until the
first call. 

Furthermore, programs can bind to libraries as the programs are running,
  loading libraries in the middle of program execution.

  Symbol resolution: When a program is built from multiple subprograms, the
  references from one subprogram to another are made using symbols; a main
  program might use a square root routine called sqrt, and the math library
  defines sqrt. A linker resolves the symbol by noting the location assigned
  to sqrt in the library, and patching the caller’s object code to so the call
  instruction refers to that location.


The line between relocation and symbol resolution can be fuzzy. Since linkers
already can resolve references to symbols, one way to handle code relocation
is to assign a symbol to the base address of each part of the program, and
treat relocatable addresses as references to the base address symbols.


Two-pass linking

When a linker runs, it first has to scan the input files to find the sizes of
the segments and to collect the definitions and references of all of the
symbols. It `creates` a segment table listing all of the segments defined in the
input files, and `a symbol table` with all of the symbols imported or exported.

Using the data from the first pass, 
the linker `assigns numeric locations to symbols`, determines the sizes and
  location of the segments in the output address space, and figures out where
  everything goes in the output file.

The second pass uses the information collected in the first pass to control
the actual linking process. It reads and relocates the object code,
`substituting numeric addresses for symbol references`, and adjusting memory
  addresses in code and data to reflect relocated segment addresses, and
  writes the relocated code to the output file. It then writes the output
  file, generally with header information, the relocated segments, and symbol
  table information. 

*ld-so-dynamic*
If the program uses dynamic linking, the symbol table contains the info the
runtime linker will need to resolve dynamic symbols.


<elf-got-plt>
Position-independent code

One popular solution to the dilemma of loading the same program at different
addresses is position independent code (PIC). The idea is simple, separate the
code from the data and generate code that won’t change regardless of the
address at which it’s loaded. That way the code can be shared among all
processes, with only data pages being private to each process.


Its designers noticed that an ELF executable consists of a group of code pages
followed by a group of data pages, and regardless of where in the address
space the program is loaded, the offset from the code to the data doesn’t
change. So if the code can load its own address into a register, the data will
be at a known distance from that address, and references to data in the
program’s own data segment can use efficient based addressing with fixed
offsets.

The linker creates a global offset table (GOT) containing pointers to all of
the global data that the executable file addresses.

If a procedure needs to refer to global or static data, it’s up to the
procedure itself to load up the address of the GOT. The details vary by
architecture, but the 386 code is typical:

  call .L2 ;; push PC in on the stack
.L2:
  popl %ebx ;; PC into register EBX
  addl $_GLOBAL_OFFSET_TABLE_+[.-.L2],%ebx    ;; adjust ebx to GOT address

It consists of a call instruction to the immediately following location, which
has the effect of pushing the PC on the stack but not jumping, then a pop to
get the saved PC in a register and an add immediate of the difference between
the address the GOT and address the target of the call. In an object file
generated by a compiler, there’s a special R_386_GOTPC relocation item for the
operand of the addl instruction. 

It tells the linker to substitute in the offset from the current instruction
to the base address of the GOT, and also serves as a flag to the linker to
build a GOT in the output file. In the output file, there’s no relocation
needed for the instruction since the distance from the addl to the GOT is
fixed. 

note: this output is output in memory for DSO.


={============================================================================
*kt_linux_core_400* linux-elf-start

https://eli.thegreenplace.net/2012/08/13/how-statically-linked-programs-run-on-linux

How statically linked programs run on Linux

In this article I want to explore what happens when a statically linked
program gets executed on Linux. By statically linked I mean a program that
does not require any shared objects to run, even the ubiquitous libc. In
reality, most programs one encounters on Linux aren't statically linked, and
do require one or more shared objects to run. 
  
However, the running sequence of such programs is more involved, which is why
  I want to present statically linked programs first. It will serve as a good
  basis for understanding, allowing me to explore most of the mechanisms
  involved with less details getting in the way. In a future article I will
  cover the dynamic linking process in detail.

The Linux kernel

Program execution begins in the Linux kernel. To run a program, a process will
call a function from the *call-exec* family. The functions in this family are
all very similar, differing only in small details regarding the manner of
passing arguments and environment variables to the invoked program. What they
all end up doing is issuing the sys_execve system call to the Linux kernel.

*call-sys_execve* does a lot of work to prepare the new program for execution.
Explaining it all is far beyond the scope of this article - a good book on
kernel internals can be helpful to understand the details [1]. I'll just focus
on the stuff useful for our current discussion.

// [1] Or just read the source, if you're brave.

As part of its job, the kernel must read the program's executable file from
disk into memory and prepare it for execution. The kernel knows how to handle
a lot of binary file formats, and tries to open the file with different
handlers until it succeeds (this happens in the function search_binary_handler
    in fs/exec.c). We're only interested in ELF here, however; for this format
the action happens in function `load_elf_binary` (in fs/binfmt_elf.c).

The kernel reads the ELF header of the program, and looks for a PT_INTERP
segment to see if an interpreter was specified. 

`Here the statically linked vs. dynamically linked distinction kicks in.` For
statically linked programs, there is no PT_INTERP segment. This is the
scenario this article covers.
*tool-readelf-l*

// static link means to link statically for all libraries but not just one of
// them.

The kernel then goes on mapping the program's segments into memory, according
to the information contained in the ELF program headers. Finally, it passes
the execution, by directly modifying the IP register, to the entry address
read from the ELF header of the program (e_entry). Arguments are passed to the
program on the stack (the code responsible for this is in `create_elf_tables`).

Here's the stack layout when the program is called, for x64:

// see picture

At the top of the stack is argc, the amount of command-line arguments. It is
followed by all the arguments themselves (each a char*), terminated by a zero
pointer. Then, the environment variables are listed (also a char* each),
  terminated by a zero pointer. 

The observant reader will notice that this argument layout is not what one
usually expects in main. This is because main is not really the entry point of
the program, as the rest of the article shows.


Program entry point

So, the Linux kernel reads the program's entry address from the ELF header.
Let's now explore how this address gets there.

Unless you're doing something very funky, the final program binary image is
probably being created by the system linker - *gnu-ld* By default, ld looks for a
special symbol called `_start` in one of the object files linked into the
program, and `sets the entry point to the address of that symbol.` This will be
simplest to demonstrate with an example written in assembly (the following is
NASM syntax):

section    .text
    ; The _start symbol must be declared for the linker (ld)
    global _start

_start:
    ; Execute sys_exit call. Argument: status -> ebx
    mov     eax, 1
    mov     ebx, 42
    int     0x80

This is a very basic program that simply returns 42. Note that it has the
_start symbol defined. Let's build it, examine the ELF header and its
disassembly:

*tool-nasm*
$ nasm -f elf64 nasm_rc.asm -o nasm_rc.o
$ ld -o nasm_rc64 nasm_rc.o

$ readelf -h nasm_rc64
ELF Header:
  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF64
  ...
  Entry point address:               0x400080
  ...
$ objdump -d nasm_rc64

nasm_rc64:     file format elf64-x86-64


Disassembly of section .text:

0000000000400080 <_start>:
  400080:     b8 01 00 00 00          mov    $0x1,%eax
  400085:     bb 2a 00 00 00          mov    $0x2a,%ebx
  40008a:     cd 80                   int    $0x80


As you can see, the entry point address in the ELF header was set to 0x400080,
  which also happens to be the address of _start.

ld looks for _start by default, but this behavior can be modified by either
the --entry command-line flag, or by providing an ENTRY command in a custom
linker script.


The entry point in C code

We're usually not writing our code in assembly, however. For C/C++ the
situation is different, because the entry point familiar to users is the main
function and not the _start symbol. Now it's time to explain how these two are
related.

Let's start with this simple C program which is functionally equivalent to the
assembly shown above:

int main() {
    return 42;
}

I will compile this code into an object file and then attempt to link it with
ld, like I did with the assembly:

$ gcc -c c_rc.c
$ ld -o c_rc c_rc.o
ld: warning: cannot find entry symbol _start; defaulting to 00000000004000b0

Whoops, ld can't find the entry point. It tries to guess using a default, but
it won't work - the program will segfault when run. ld obviously needs some
additional object files where it will find the entry point. But which object
files are these? Luckily, we can use gcc to find out. gcc can act as a full
compilation driver, invoking ld as needed. Let's now use gcc to link our
object file into a program. Note that the -static flag is passed to force
static linking of the C library and the gcc runtime library:

$ gcc -o c_rc -static c_rc.o
$ c_rc; echo $?
42

*gcc-ld-verbose* *gcc-debug*
It works. So how does gcc manage to do the linking correctly? We can pass the
-Wl,-verbose flag to gcc which will spill the list of objects and libraries it
passed to the linker. Doing this, we'll see additional object files like
crt1.o and the whole libc.a static library (which has objects with telling
    names like libc-start.o). C code does not live in a vacuum. To run, it
requires some support libraries such as the gcc runtime and libc.

Since it obviously linked and ran correctly, the program we built with gcc
should have a _start symbol at the right place. Let's check [2]:

$ readelf -h c_rc
ELF Header:
  Magic:   7f 45 4c 46 02 01 01 03 00 00 00 00 00 00 00 00
  Class:                             ELF64
  ...
  Entry point address:               0x4003c0
  ...

$ objdump -d c_rc | grep -A15 "<_start"
00000000004003c0 <_start>:
  4003c0:     31 ed                   xor    %ebp,%ebp
  4003c2:     49 89 d1                mov    %rdx,%r9
  4003c5:     5e                      pop    %rsi
  4003c6:     48 89 e2                mov    %rsp,%rdx
  4003c9:     48 83 e4 f0             and    $0xfffffffffffffff0,%rsp
  4003cd:     50                      push   %rax
  4003ce:     54                      push   %rsp
  4003cf:     49 c7 c0 20 0f 40 00    mov    $0x400f20,%r8
  4003d6:     48 c7 c1 90 0e 40 00    mov    $0x400e90,%rcx
  4003dd:     48 c7 c7 d4 04 40 00    mov    $0x4004d4,%rdi
  4003e4:     e8 f7 00 00 00          callq  4004e0 <__libc_start_main>
  4003e9:     f4                      hlt
  4003ea:     90                      nop
  4003eb:     90                      nop

// $ objdump -d -Mintel start | grep -A15 "<_start"                                                                                                                                           
// 946:0000000000400e6e <_start>:
// 947-  400e6e:   31 ed                   xor    ebp,ebp
// 948-  400e70:   49 89 d1                mov    r9,rdx
// 949-  400e73:   5e                      pop    rsi
// 950-  400e74:   48 89 e2                mov    rdx,rsp
// 951-  400e77:   48 83 e4 f0             and    rsp,0xfffffffffffffff0
// 952-  400e7b:   50                      push   rax
// 953-  400e7c:   54                      push   rsp
// 954-  400e7d:   49 c7 c0 70 16 40 00    mov    r8,0x401670
// 955-  400e84:   48 c7 c1 e0 15 40 00    mov    rcx,0x4015e0
// 956-  400e8b:   48 c7 c7 8e 0f 40 00    mov    rdi,0x400f8e
// 957-  400e92:   e8 09 01 00 00          call   400fa0 <__libc_start_main>
// 958-  400e97:   f4                      hlt    
// 959-  400e98:   0f 1f 84 00 00 00 00    nop    DWORD PTR [rax+rax*1+0x0]
// 960-  400e9f:   00 


Indeed, 0x4003c0 is the address of _start and it's the program entry point.
However, what is all that code at _start? Where does it come from, and what
does it mean?


Decoding the start sequence of C code

The startup code shown above comes from glibc - the GNU C library, where for
x64 ELF it lives in the file sysdeps/x86_64/start.S [3]. Its goal is to
prepare the arguments for a function named __libc_start_main and call it. This
function is also part of glibc and lives in csu/libc-start.c. Here is its
signature, formatted for clarity, with added comments to explain what each
argument means:

int __libc_start_main(
         /* Pointer to the program's main function */
         (int (*main) (int, char**, char**),
         /* argc and argv */
         int argc, char **argv,
         /* Pointers to initialization and finalization functions */
         __typeof (main) init, void (*fini) (void),
         /* Finalization function for the dynamic linker */
         void (*rtld_fini) (void),
         /* End of stack */
         void* stack_end)

Anyway, with this signature and the AMD64 ABI in hand, we can map the
arguments passed to __libc_start_main from _start:

main:      rdi <-- $0x4004d4
argc:      rsi <-- [RSP]
argv:      rdx <-- [RSP + 0x8]
init:      rcx <-- $0x400e90
fini:      r8  <-- $0x400f20
rdld_fini: r9  <-- rdx on entry
stack_end: on stack <-- RSP

You'll also notice that the stack is aligned to 16 bytes and some garbage is
pushed on top of it (rax) before pushing rsp itself. This is to conform to the
AMD64 ABI. Also note the hlt instruction at address 0x4003e9. It's a safeguard
in case __libc_start_main did not exit (as we'll see, it should). hlt can't be
executed in user mode, so this will raise an exception and crash the process.

Examining the disassembly, it's easy to verify that 0x4004d4 is indeed main,
          0x400e90 is __libc_csu_init and 0x400f20 is __libc_csu_fini. There's
          another argument the kernel passes _start - a finish function for
          shared libraries to use (in rdx). We'll ignore it in this article.


The C library start function

Now that we understood how it's being called, what does __libc_start_main
actually do? Ignoring some details that are probably too specialized to be
interesting in the scope of this article, here's a list of things that it does
for a statically linked program:

Figure out where the environment variables are on the stack.
Prepare the auxiliary vector, if required.
Initialize thread-specific functionality (pthreads, TLS, etc.)
Perform some security-related bookkeeping (this is not really a separate step, but is trickled all through the function).
Initialize libc itself.
Call the program initialization function through the passed pointer (init).
Register the program finalization function (fini) for execution on exit.
Call main(argc, argv, envp)
Call exit with the result of main as the exit code.


<elf-entry>
https://eli.thegreenplace.net/2011/01/27/how-debuggers-work-part-2-breakpoints/

Digression - process addresses and entry point

Frankly, 0x8048096 itself doesn't mean much, it's just a few bytes away from
the beginning of the text section of the executable. If you look carefully at
the dump listing above, you'll see that the text section starts at 0x08048080.
This tells the OS to map the text section starting at this address in the
virtual address space given to the process. On Linux these addresses can be
absolute (i.e. the executable isn't being relocated when it's loaded into
    memory), because with the virtual memory system each process gets its own
chunk of memory and sees the whole 32-bit address space as its own (called
    "linear" address).

If we examine the ELF [5] header with readelf, we get:

$ readelf -h traced_printer2
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF32
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           Intel 80386
  Version:                           0x1
  Entry point address:               0x8048080
  Start of program headers:          52 (bytes into file)
  Start of section headers:          220 (bytes into file)
  Flags:                             0x0
  Size of this header:               52 (bytes)
  Size of program headers:           32 (bytes)
  Number of program headers:         2
  Size of section headers:           40 (bytes)
  Number of section headers:         4
  Section header string table index: 3

Note the "entry point address" section of the header, which also points to
0x8048080. So if we interpret the directions encoded in the ELF file for the
OS, it says:

Map the text section (with given contents) to address 0x8048080
Start executing at the entry point - address 0x8048080

But still, why 0x8048080? For historic reasons, it turns out. Some googling
led me to a few sources that claim that the first 128MB of each process's
address space were reserved for the stack. 128MB happens to be 0x8000000,
which is where other sections of the executable may start. 0x8048080, in
  particular, is the default entry point used by the Linux ld linker. This
  entry point can be modified by passing the -Ttext argument to ld.

To conclude, there's nothing really special in this address and we can freely
change it. As long as the ELF executable is properly structured and the entry
point address in the header matches the real beginning of the program's code
(text section), we're OK.

<ex>

$ readelf -h hello_test
ELF Header:
  Entry point address:               0x4005e0

$ objdump -d hello_test
Disassembly of section .text:

00000000004005e0 <_start>:
  4005e0:       31 ed                   xor    %ebp,%ebp

$ readelf -h a.out
ELF Header:
  Entry point address:               0x4006c0

$ objdump -d a.out
Disassembly of section .text:

00000000004006c0 <_start>:
  4006c0:       31 ed                   xor    %ebp,%ebp


http://www.tldp.org/LDP/LGNET/issue84/hawk.html

How main() is executed on Linux

note: this is elf from debian VM.

$ readelf -h a-ng.out
  Entry point address:               0x8048300

$ objdump -S a-ng.out

Disassembly of section .text:

08048300 <_start>:
 8048300:	31 ed                	xor    %ebp,%ebp
 8048302:	5e                   	pop    %esi           // get argc
 8048303:	89 e1                	mov    %esp,%ecx      // get argv
 8048305:	83 e4 f0             	and    $0xfffffff0,%esp
 8048308:	50                   	push   %eax           // 0
 8048309:	54                   	push   %esp           // stack_end
 804830a:	52                   	push   %edx           // _rtlf_fini?
 804830b:	68 d0 84 04 08       	push   $0x80484d0     // __libc_csu_fini
 8048310:	68 60 84 04 08       	push   $0x8048460     // __libc_csu_init
 8048315:	51                   	push   %ecx           // argv
 8048316:	56                   	push   %esi           // argc
 8048317:	68 fb 83 04 08       	push   $0x80483fb     // main
 804831c:	e8 cf ff ff ff       	call   80482f0 <__libc_start_main@plt>
 8048321:	f4                   	hlt    
 8048322:	66 90                	xchg   %ax,%ax
 8048324:	66 90                	xchg   %ax,%ax
 8048326:	66 90                	xchg   %ax,%ax
 8048328:	66 90                	xchg   %ax,%ax
 804832a:	66 90                	xchg   %ax,%ax
 804832c:	66 90                	xchg   %ax,%ax
 804832e:	66 90                	xchg   %ax,%ax


080482f0 <__libc_start_main@plt>:
 80482f0:	ff 25 fc 96 04 08    	jmp    *0x80496fc
 80482f6:	68 10 00 00 00       	push   $0x10
 80482fb:	e9 c0 ff ff ff       	jmp    80482c0 <_init+0x2c>

080483fb <main>:
 80483fb:	8d 4c 24 04          	lea    0x4(%esp),%ecx

08048460 <__libc_csu_init>:
 8048460:	55                   	push   %ebp

080484d0 <__libc_csu_fini>:
 80484d0:	f3 c3                	repz ret 

According to this stack frame, esi, ecx, edx, esp, eax registers should be
filled with appropriate values before __libc_start_main() is executed. And
clearly this registers are not set by the startup assembly instructions shown
before. Then, who sets these registers? Now I guess the only thing left. The
kernel. Now let's go back to our third question.

When we execute a program by entering a name on shell, this is what happens on Linux.

  The shell calls the kernel system call "execve" with argc/argv. 

  The kernel system call handler gets control and start handling the system
  call. In kernel code, the handler is "sys_execve". On x86, the user-mode
  application passes all required parameters to kernel with the following
  registers.

  ebx : pointer to program name string
  ecx : argv array pointer
  edx : environment variable array pointer.
 
  The generic execve kernel system call handler, which is do_execve, is
  called.  What it does is set up a data structure and copy some data from
  user space to kernel space and finally calls search_binary_handler(). Linux
  can support more than one executable file format such as a.out and ELF at
  the same time. For this functionality, there is a data structure "struct
  linux_binfmt", which has a function pointer for each binary format loader.
  And search_binary_handler() just looks up an appropriate handler and calls
  it. In our case, load_elf_binary() is the handler. To explain each detail of
  the function would be lengthy/boring work. So I'll not do that. If you are
  interested in it, read a book about it. As a picture tells a thousand words,
a thousand lines of source code tells ten thousand words (sometimes). Here is
  the bottom line of the function. It first sets up kernel data structures for
  file operation to read the ELF executable image in. Then it sets up a kernel
  data structure: code size, data segment start, stack segment start, etc. And
  it allocates user mode pages for this process and copies the argv and
  environment variables to those allocated page addresses. Finally, argc, the
  argv pointer, and the envrioronment variable array pointer are pushed to
  user mode stack by create_elf_tables(), and start_thread() starts the
  process execution rolling. 


When the _start assembly instruction gets control of execution, the stack
frame looks like this. 

Stack Top   -------------
            argc
            -------------
            argv pointer
            -------------
            env pointer
            ------------- 

And the assembly instructions gets all information from stack by

pop %esi          <--- get argc
move %esp, %ecx   <--- get argv
                   actually the argv address is the same as the current stack
                   pointer.

And now we are all set to start executing.


What about the other registers?

For esp, this is used for stack end in application program. After popping all
necessary information, the _start rountine simply adjusts the stack pointer
(esp) by turning off lower 4 bits from esp register. This perfectly makes
sense since actually, to our main program, that is the end of stack. 


About the assembly instructions

Where are all those codes from? It's part of GCC code. You can usually find
all the object files for the code at /usr/lib/gcc-lib/i386-redhat-linux/XXX
and /usr/lib where XXX is gcc version. File names are crtbegin.o,crtend.o,
gcrt1.o.


Summing up

Here is what happens. 

GCC build your program with crtbegin.o/crtend.o/gcrt1.o And the other default
libraries are dynamically linked by default. Starting address of the
executable is set to that of _start.

Kernel loads the executable and setup text/data/bss/stack, especially, kernel
allocate page(s) for arguments and environment variables and pushes all
necessary information on stack.

Control is pased to _start. _start gets all information from stack setup by
kernel, sets up argument stack for __libc_start_main, and calls it. 

__libc_start_main initializes necessary stuffs, especially C library(such as
    malloc) and thread environment and calls our main. 

our main is called with main(argv, argv) Actually, here one interesting point
is the signature of main. __libc_start_main thinks main's signature as
main(int, char **, char **) If you are curious, try the following prgram.

main(int argc, char** argv, char** env)
{
    int i = 0;
    while(env[i] != 0)
    {
       printf("%s\n", env[i++]);
    }
    return(0);
}


Conclusion

On Linux, our C main() function is executed by the cooperative work of GCC,
   libc and Linux's binary loader.

References

__libc_start_main          glibc source 
                           ./sysdeps/generic/libc-start.c 

sys_execve                 linux kernel source code 
                           arch/i386/kernel/process.c 

do_execve                  linux kernel source code 
                           fs/exec.c 

struct linux_binfmt        linux kernel source code 
                           include/linux/binfmts.h 

load_elf_binary            linux kernel source code
                           fs/binfmt_elf.c 

create_elf_tables          linux kernel source code 
                           fs/binfmt_elf.c 

start_thread               linux kernel source code 
                           include/asm/processor.h

={============================================================================
*kt_linux_core_400* linux-abi

An application binary interface (ABI) is a set of rules specifying how a
binary executable should exchange information with some service (e.g., the
    kernel or a library) at run time. Among other things, an ABI specifies
which registers and stack locations are used to exchange this information, and
what meaning is attached to the exchanged values. Once compiled for a
particular ABI, a binary executable should be able to run on any system
presenting the same ABI. This contrasts with a standardized API (such as
    SUSv3), which guarantees portability only for applications compiled from
source code.


={============================================================================
*kt_linux_core_400* linux-lib-libc-glibc check-libc-version

LPI. 2.5 File I/O Model

The stdio library

To perform file I/O, C programs typically employ I/O functions contained in
the standard C library. This set of functions, referred to as the stdio
library, includes fopen(), fclose(), scanf(), printf(), fgets(), fputs(), and
so on. The stdio functions are layered on top of the I/O system calls (open(),
    close(), read(), write(), and so on).


<ex>
$ nm libc.a | grep libc_malloc
5440:00005130 T __libc_malloc
5441:00000004 D __libc_malloc_initialized
5579:         U __libc_malloc_initialized

$ nm libuClibc-1.0.17.so | grep malloc
784:00054850 T __uc_malloc
785:000775e0 B __uc_malloc_failed
1513:00051820 T malloc


<check-libc-version>
LPI. 3.3 The Standard C Library; The GNU C Library (glibc)

$ ldd /bin/echo
        linux-vdso.so.1 =>  (0x00007fff089ff000)
        libc.so.6 => /lib64/libc.so.6 (0x0000003ba8000000)
        /lib64/ld-linux-x86-64.so.2 (0x0000003ba7c00000)


$ /lib64/libc.so.6
GNU C Library stable release version 2.12, by Roland McGrath et al.
Copyright (C) 2010 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.
There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE.
Compiled by GNU CC version 4.4.7 20120313 (Red Hat 4.4.7-9).
Compiled on a Linux 2.6.32 system on 2015-01-19.
Available extensions:
        The C stubs add-on version 2.1.2.
        crypt add-on version 2.1 by Michael Glad and others
        GNU Libidn by Simon Josefsson
        Native POSIX Threads Library by Ulrich Drepper et al
        BIND-8.2.3-T5B
        RT using linux kernel aio
libc ABIs: UNIQUE IFUNC
For bug reporting instructions, please see:
<http://www.gnu.org/software/libc/bugs.html>.

note: How this shared object prints out messages when run?

In some Linux distributions, the GNU C library resides at a pathname other
than /lib/libc.so.6.

$ /lib/i386-linux-gnu/i686/cmov/libc.so.6
GNU C Library (Debian GLIBC 2.19-18+deb8u4) stable release version 2.19, by Roland McGrath et al.
Copyright (C) 2014 Free Software Foundation, Inc.


<libc-version-macro>
From version 2.0 onward, glibc defines two constants, __GLIBC__ and
__GLIBC_MINOR__, that can be tested at compile time (in #ifdef statements). On
a system with glibc 2.12 installed, these constants would have the values 2
and 12.  

However, these constants are of limited use in a program that is compiled on
one system but run on another system with a different glibc. To handle this
possibility, a program can call the gnu_get_libc_version() function to
determine the version of glibc available at run time.


<source>
https://www.gnu.org/software/libc/sources.html

Download sources

Releases are available by source branch checkout via git and tarball via ftp.

Checkout the latest glibc 2.25 stable release:

git clone git://sourceware.org/git/glibc.git
cd glibc
git checkout --track -b local_glibc-2.25 origin/release/2.25/master

Release tarballs are available via anonymous ftp at http://ftp.gnu.org/gnu/glibc/ and its mirrors.


={============================================================================
*kt_linux_core_400* linux-lib-libc-glibc-port

Errors when try to build glibc for mips:

// cd build-glibc
// ../glibc-2.10.1/configure --prefix=${INSTALL_PATH}/${TARGET} \
// --build=$MACHTYPE --host=mips-unknown-linux --target=${TARGET} \
// --with-headers=${INSTALL_PATH}/${TARGET}/include \
// --disable-multilib libc_cv_forced_unwind=yes
// 
// configure: running configure fragment for add-on nptl
// checking sysdep dirs... configure: error: The mips is not supported.


Have most sources in sysdeps.

kyoupark@ukstbuild2:~/asn/gcc/glibc-ports-2.13$ ll
total 204
drwxr-xr-x  5 kyoupark ccusers  4096 May  4 16:04 ./
drwxr-xr-x 28 kyoupark ccusers  4096 May 17 11:14 ../
-rw-r--r--  1 kyoupark ccusers    71 Jan 25  2011 Banner
drwxr-xr-x  2 kyoupark ccusers  4096 Jan 25  2011 bare/
-rw-r--r--  1 kyoupark ccusers  1437 Jan 25  2011 ChangeLog
-rw-r--r--  1 kyoupark ccusers   347 Jan 25  2011 ChangeLog.aix
-rw-r--r--  1 kyoupark ccusers  7826 Jan 25  2011 ChangeLog.alpha
-rw-r--r--  1 kyoupark ccusers  9053 Jan 25  2011 ChangeLog.am33
-rw-r--r--  1 kyoupark ccusers 44842 Jan 25  2011 ChangeLog.arm
-rw-r--r--  1 kyoupark ccusers   725 Jan 25  2011 ChangeLog.cris
-rw-r--r--  1 kyoupark ccusers 25015 Jan 25  2011 ChangeLog.hppa
-rw-r--r--  1 kyoupark ccusers 24289 Jan 25  2011 ChangeLog.m68k
-rw-r--r--  1 kyoupark ccusers 33221 Jan 25  2011 ChangeLog.mips
-rw-r--r--  1 kyoupark ccusers  4314 Jan 25  2011 ChangeLog.powerpc
drwxr-xr-x  2 kyoupark ccusers  4096 Jan 25  2011 data/
-rw-r--r--  1 kyoupark ccusers  1435 Jan 25  2011 Makefile
-rw-r--r--  1 kyoupark ccusers  2461 Jan 25  2011 README
drwxr-xr-x 21 kyoupark ccusers  4096 Jan 25  2011 sysdeps/


Like all glibc add-ons, this must be used by specifying the directory in
the --enable-add-ons option when running glibc's configure script.

glibc-2.8/README

This directory contains the version 2.7 release of the GNU C Library.

When working with Linux kernels, the GNU C Library version 2.4 is
intended primarily for use with Linux kernel version 2.6.0 and later.
We only support using the NPTL implementation of pthreads, which is now
the default configuration.  Most of the C library will continue to work
on older Linux kernels and many programs will not require a 2.6 kernel
to run correctly.  However, pthreads and related functionality will not
work at all on old kernels and we do not recommend using glibc 2.4 with
any Linux kernel prior to 2.6.

All Linux kernel versions prior to 2.6.16 are known to have some bugs that
may cause some of the tests related to pthreads in "make check" to fail.
If you see such problems, please try the test suite on the most recent
Linux kernel version that you can use, before pursuing those bugs further.

The code for other CPU configurations supported by volunteers outside of
the core glibc maintenance effort is contained in the separate `ports'
add-on.  You can find glibc-ports-2.7 distributed separately in the
same place where you got the main glibc distribution files.
Currently these configurations are known to work using the `ports' add-on:

        arm-*-linux-gnu         Requires Linux 2.6.15 for NPTL, no SMP support
        arm-*-linux-gnueabi     Requires Linux 2.6.16-rc1 for NPTL, no SMP
        mips-*-linux-gnu        Requires Linux 2.6.12 for NPTL
        mips64-*-linux-gnu      Requires Linux 2.6.12 for NPTL

The ports distribution also contains code for other configurations that
do not work or have not been maintained recently, but will be of use to
anyone trying to make a new configuration work.  If you are interested
in doing a port, please contact the glibc maintainers; see
http://www.gnu.org/software/libc/ for more information.

*** On GNU/Linux systems it is normal to compile GNU libc with the
*** `nptl' add-on.  Without that, the library will be
*** incompatible with normal GNU/Linux systems.
*** If you really mean to not use this add-on, run configure again
*** using the extra parameter `--disable-sanity-checks'.

https://www.gnu.org/software/libc/manual/html_node/Configuring-and-compiling.html

<ex>
../glibc-2.13/configure --prefix=${PREFIX}/${TARGET} \
--build=$MACHTYPE --host=mips-unknown-linux --target=${TARGET} \
--with-headers=${PREFIX}/${TARGET}/include \
--disable-multilib libc_cv_forced_unwind=yes libc_cv_c_cleanup=yes \
--enable-add-ons=nptl,../glibc-ports-2.13


={============================================================================
*kt_linux_core_400* linux-lib-libc-glibc-testing

https://sourceware.org/glibc/wiki/Testing/Testsuite

Testing with a cross-compiler

While building and testing glibc on a system with native tools is the simplest
way to do glibc testing, it is possible to build a cross-compilation toolchain
that includes glibc and run the glibc testsuite using that toolchain.

note:
glibc 2.13 don't have this script but 2.20 has.

To do cross-compilation testing you need to use the scripts/cross-test-ssh.sh
script in the glibc sources and do the build on the host machine in a
directory that is also visible, with the same path, on the target machine. It
is also necessary to be able to use ssh to access the target machine from the
host machine. There are details on this in the cross-test-ssh.sh script. This
script can be used to run "make check", "make xcheck", or "make bench".

Because both the host and target machines are touching files during the make,
changes on one machine need to be immediately visible on the other machine.
  This is not the default behaviour with NFS due to caching. You may need to
  use the 'noac' option on NFS in order to get this to work. This problem can
  cause messages similar to the following when running make:

touch: cannot touch `/home/sellcey/gcc/gcc_cross_testing/obj-mips-mti-linux-gnu/glibc/obj_default/localedata/de_DE.ISO-8859-1/LC_CTYPE': No such file or directory

If you build a cross-toolchain by building binutils, an initial GCC, glibc,
and then rebuild GCC (a common build sequence), then running the glibc
  testsuite may result in glibc getting rebuilt before any tests are run. This
  is because the glibc tests have make dependencies on glibc and glibc has
  make dependencies on GCC (or at least some GCC headers). If your second GCC
  build overwrites the first one then make will see this change and rebuild
  many parts (if not all) of glibc before doing any testing. If you know you
  are going to run the glibc testsuite you may want to rebuild glibc after
  rebuilding GCC and before running "make check". You cannot skip the second
  GCC build because the initial GCC (if configured using --without-headers)
  will not have all the headers and libraries needed to build the glibc test
  programs.


={============================================================================
*kt_linux_core_400* linux-lib-libc-uclibc

http://www.etalabs.net/compare_libcs.html
Comparison of C/POSIX standard library implementations for Linux


http://www.musl-libc.org/
musl New standard C library. musl is lightweight, fast, simple, free, and
  strives to be correct in the sense of standards-conformance and safety.

<uclibc>
http://elinux.org/Toolchains#C_library
uClibc is an alternate C library, which features a much smaller footprint.
This library can be an interesting alternative if flash space and/or memory
footprint is an issue. However, the space advantages gained using uClibc are
becoming less important as the price of memory & flash continues to drop. It
is still useful C library for embedded systems without MMU.

http://www.uclibc.org/
http://git.uclibc.org/uClibc/

git clone git://uclibc.org/uClibc.git

note: this is the last update.
15 May 2012, uClibc 0.9.33.2 Released
uClibc-0.9.33.2 was released today. 

wget --no-check-certificate https://www.uclibc.org/downloads/uClibc-0.9.33.2.tar.xz
wget --no-check-certificate https://www.uclibc.org/downloads/old-releases/uClibc-0.9.28.3.tar.bz2

1. 
    To configure uClibc, you can run:

            make menuconfig

2. To enable WORDEXP

#
# Big and Tall
#
UCLIBC_HAS_WORDEXP=y


<uclibc-ng>
http://www.uclibc-ng.org/
Latest Release
*NEWS* 1.0.18 (Codename Delirium Nocturnum) released 27.09.2016 *NEWS* 

uClibc-ng is a spin-off of uClibc (from Erik Andersen) from
http://www.uclibc.org. Our main goal is to provide regulary a stable and
tested release.

wget http://downloads.uclibc-ng.org/releases/1.0.17/uClibc-ng-1.0.17.tar.xz

uclibc-ng starts from 1.0.0.


<to-config-ld>
http://git.uclibc.org/uClibc/plain/extra/Configs/Config.in

config SUPPORT_LD_DEBUG
	bool "Build the shared library loader with debugging support"
	depends on HAVE_SHARED
	help
	  Answer Y here to enable all the extra code needed to debug the uClibc
	  native shared library loader.  The level of debugging noise that is
	  generated depends on the LD_DEBUG environment variable...  Just set
	  LD_DEBUG to something like: 'LD_DEBUG=token1,token2,..  prog' to
	  debug your application.  Diagnostic messages will then be printed to
	  the stderr.

	  For now these debugging tokens are available:
	    detail        provide more information for some options
	    move          display copy processing
	    symbols       display symbol table processing
	    reloc         display relocation processing; detail shows the
	                  relocation patch
	    nofixups      never fixes up jump relocations
	    bindings      displays the resolve processing (function calls);
	                  detail shows the relocation patch
	    all           Enable everything!

	  The additional environment variable:
	    LD_DEBUG_OUTPUT=file
	  redirects the diagnostics to an output file created using
	  the specified name and the process id as a suffix.

	  An excellent start is simply:
	    $ LD_DEBUG=binding,move,symbols,reloc,detail ./appname
	  or to log everything to a file named 'logfile', try this
	    $ LD_DEBUG=all LD_DEBUG_OUTPUT=logfile ./appname

	  If you are doing development and want to debug uClibc's shared library
	  loader, answer Y.  Mere mortals answer N.


={============================================================================
*kt_linux_core_400* linux-lib-shared linux-shared

*LPI-41* 

{linux-static}

<create> ar-command
The archive also records various attributes of each of the component object
files, including file permissions, numeric user and group IDs, and last
modification time.

r (replace): 
  
Insert an object file into the archive, replacing any previous object file of
the same name. This is the standard method for creating and updating an archive.
Thus, we might build an archive with the following commands:

$ cc -g -c mod1.c mod2.c mod3.c
$ ar r libdemo.a mod1.o mod2.o mod3.o

t (table of contents): 
  
Display a table of contents of the archive. By default, this lists just the
names of the object files in the archive. By adding the v (verbose) modifier, we
additionally see all of the other attributes recorded in the archive for each
object file, as in the following example:

$ ar tv libdemo.a
rw-r--r-- 1000/100 1001016 Nov 15 12:26 2009 mod1.o
rw-r--r-- 1000/100 406668 Nov 15 12:21 2009 mod2.o
rw-r--r-- 1000/100 46672 Nov 15 12:21 2009 mod3.o

<staic-link>
Couple of ways in linking:

1. The first is to name the static library as part of the link command, as in
the following:

$ cc -g -c prog.c
$ cc -g -o prog prog.o libdemo.a

note: `linktime search`
2. can place the library in one of the `standard directories` searched by the
linker such as /usr/lib, and then specify the library name; the filename of
the library without the lib prefix and .a suffix using the `-l option`

$ cc -g -o prog prog.o -ldemo

3. If the library resides in a directory not normally searched by the linker,
  can specify that the linker should search this additional directory using
  the `-L option`

$ cc -g -o prog prog.o -Lmylibdir -ldemo

<only-used>
Although a static library may contain many object modules, the linker includes
'only' those modules that the program 'requires'.


{downside-of-static} *why-use-shared-library*
1. Duplicates in disk and ram spce.

2. If a change is required perhaps a security or bug fix to an object module
in a static library, then all executables using that module must be relinked
in order to incorporate the change. This disadvantage is further compounded by
the fact that the system administrator needs to be aware of which applications
were linked against the library.


{linux-shared}
Although the code of a shared library is shared among multiple processes, its
`variables are not.` Each process that uses the library has its own copies of
the `global and static variables` that are defined within the library.


{further-advantages}
* Because overall program size is smaller, in some cases, programs can be
loaded into memory and started more 'quickly'. This point holds true only for
large shared libraries that are already in use by another program.

* Such changes can be carried out even while running programs are using an
existing version of the shared library.


{cost-of-shared}
* Shared libraries are more `complex than static libraries`, both at the
  conceptual level, and at the practical level of creating shared libraries
  and building the programs that use them.

* Shared libraries 'must' be compiled to use position-independent code, which
  has a performance 'overhead' on most architectures because it requires the
  use of an extra register 

* Symbol relocation must be performed at run time. During symbol relocation,
  references to each symbol (a variable or function) in a shared library need
  to be modified to correspond to the actual run-time location at which the
  symbol is placed in virtual memory. Take a little more time to execute.


{linux-shared-pic} position-independent-code
These changes allow the code to be located at any virtual address at run time.
This is necessary for shared libraries, since there is no way of knowing at
link time where the shared library code will be located in memory.

In order to determine whether an existing object file has been compiled with
the -fPIC option, can check for the presence of the name _GLOBAL_OFFSET_TABLE_
in the object file's symbol table, using either of the following commands:

$ nm mod1.o | grep _GLOBAL_OFFSET_TABLE_
$ readelf -s mod1.o | grep _GLOBAL_OFFSET_TABLE_         // -s, --syms|--symbols

As shown above, this not necessarily means 'shared' object. It's only tell
-fPIC used.


<ex>
$ gcc -g -c -fPIC -Wall mod1.c mod2.c mod3.c
$ gcc -g -shared -o libfoo.so mod1.o mod2.o mod3.o

or 

$ gcc -g -fPIC -Wall mod1.c mod2.c mod3.c -shared -o libfoo.so

Unlike static, it is not possible to add or remove individual object modules
from a previously built shared library. As with normal executables, the object
files within a shared library no longer maintain distinct identities.


{shared-vs-static} do-not-use-single-line

$ cat main.c 
#nclude <stdio.h>

extern void foo(void);

int main(void)
{
  foo();
  return 0;
}

$ cat foo.c 
#include <stdio.h>

void foo(void)
{
  printf("foo: this is foo...\n");
}

$ ls -alR 
drwxr-xr-x 2 kpark kpark 4096 Feb  5 09:34 one
drwxr-xr-x 2 kpark kpark 4096 Feb  5 09:37 two
drwxr-xr-x 2 kpark kpark 4096 Feb  5 09:46 thr

<used-seperated-step> recommended
./one:
-rw-r--r-- 1 kpark kpark   73 Feb  5 09:32 foo.c
-rw-r--r-- 1 kpark kpark 1528 Feb  5 09:33 foo.o
-rwxr-xr-x 1 kpark kpark 6407 Feb  5 09:33 libfoo.so
-rw-r--r-- 1 kpark kpark   82 Feb  5 09:32 main.c
-rwxr-xr-x 1 kpark kpark 7120 Feb  5 09:34 one


$ gcc -c -fpic foo.c 
$ gcc -shared -o libfoo.so foo.o
$ gcc -o one main.c libfoo.so 

$ ./one 
./one: error while loading shared libraries: libfoo.so: cannot open shared
  object file: No such file or directory

$ readelf -d one | grep NEEDED
:4: 0x0000000000000001 (NEEDED)             Shared library: [`libfoo.so`]
:5: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ file libfoo.so 
libfoo.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), 
  dynamically linked,
  BuildID[sha1]=0x7c5aa43368b2a98af48a1558d7e2c5a010d238b0, not stripped

$ readelf -s libfoo.so | grep _OFFSET_TA
(standard input):64:    43: 0000000000200978     0 OBJECT  LOCAL  DEFAULT  \
                   ABS _GLOBAL_OFFSET_TABLE_

note: no SONAME is involved here and use a realname since so do not have SONAME.

$ nm one | grep foo
(standard input):32:                 U foo


<used-single-step>
./two:
-rw-r--r-- 1 kpark kpark   73 Feb  5 09:32 foo.c
-rw-r--r-- 1 kpark kpark 1528 Feb  5 09:37 libfoo.so
-rw-r--r-- 1 kpark kpark   82 Feb  5 09:32 main.c
-rwxr-xr-x 1 kpark kpark 6872 Feb  5 09:37 two

$ gcc -c -fpic foo.c -shared -o libfoo.so
$ gcc -o two main.c libfoo.so

$ ./two 
foo: this is foo...

$ readelf -d two | grep NEEDED
4: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ readelf -s libfoo.so | grep _OFFSET_TA
(standard input):14:    10: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  \
                   UND _GLOBAL_OFFSET_TABLE_

$ file libfoo.so 
libfoo.so: ELF 64-bit LSB 'relocatable', x86-64, version 1 (SYSV), not stripped

note: what does it mean? same as static link? looks like "libfoo.so" is not
shared object.

$ nm two | grep foo
(standard input):32:000000000040051c T foo


<used-static-build>
./thr:
-rw-r--r-- 1 kpark kpark   73 Feb  5 09:45 foo.c
-rw-r--r-- 1 kpark kpark 1480 Feb  5 09:46 foo.o
-rw-r--r-- 1 kpark kpark   82 Feb  5 09:45 main.c
-rwxr-xr-x 1 kpark kpark 6872 Feb  5 09:46 thr

$ gcc -c foo.c -o foo.o 
$ gcc -o thr main.c foo.o 

$ ./thr 
foo: this is foo...

$ file foo.o 
foo.o: ELF 64-bit LSB 'relocatable', x86-64, version 1 (SYSV), not stripped

$ readelf -s foo.o | grep _OFFSET_TA
$ readelf -d thr | grep NEEDED
4: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ nm thr | grep foo
(standard input):32:000000000040051c T foo


<static-option>
asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc minit.c
ls -al a.out
-rwxr-xr-x 1 kyoupark ccusers 9734 Mar  2 17:09 a.out

readelf -d a.out

Dynamic section at offset 0x17c contains 26 entries:
  Tag        Type                         Name/Value
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so.6]

asn/gcc/gcc-glibc-brcm-mips-install/bin/mips-linux-gnu-gcc -static minit.c
ls -al a.out
-rwxr-xr-x 1 kyoupark ccusers 2701077 Mar  2 17:10 a.out

readelf -d a.out

There is no dynamic section in this file.


={============================================================================
*kt_linux_core_400* linux-lib-static-how-works

In a try to use DUMA which replace malloc calls, should I have to change all
sources which uses malloc calls to have "include "duma.h"" and have to
recompile all?

How can be sure that duma_malloc is used in all static libraries used to build
a final binary?

// mod.c
// which do not include duma.h and not recompiled.

#include <stdio.h>
#include <stdlib.h>

void func_from_mod()
{
    printf("func_from_mod: calls malloc and use it\n");
    char *p = (char*)malloc(10*sizeof(char));
    p[0] = 'm';
    p[1] = 'o';
    p[2] = 'd';
    p[3] = 0;
    printf("func_from_mod: p = %s\n", p );
    free(p);
    p[0] = 'm';
    printf("func_from_mod: return \n");
}

$ gcc -g -c mod.c
$ ar r libmod mod.o

// testmain.c

#include <stdio.h>

func_from_mod();

$ nm mod.o
                 U free
0000000000000000 T func_from_mod
                 U malloc
                 U printf
                 U puts

See that global malloc is `external undefined` from glibc

$ gcc -g testmain.c mod.host.o -o out_duma_host
$ nm out_duma_host | grep malloc
44:                 U malloc@@GLIBC_2.2.5


<1>
// duma.h
      #define malloc(SIZE)                _duma_malloc(SIZE, __FILE__, __LINE__)

include duma header, see compile errors:

$ gcc -g -DUSE_DUMA testmain.c mod.host.o -o out_duma_host

/testmain.c:33: undefined reference to `_duma_malloc'
/testmain.c:36: undefined reference to `_duma_free'


<2>
// duma.c
void * malloc(size_t size)
{
  return _duma_malloc(size  DUMA_PARAMS_UK);
}


not include duma header which causes that duma defines global malloc in duma
library so see all `defined` malloc calls in the output binary from duma by
rebuilding the duma and relinking the binary, but not whole thing. 

$ gcc -g testmain.c mod.host.o libduma.a.for.buildsvr -lpthread -o out_duma_host
$ nm out_duma_host | grep malloc
59 :0000000000403451 T _duma_malloc
111:0000000000403e31 T malloc


See its working

=== DUMA: KIT: _duma_allocate
func_from_mod: p = mod
Segmentation fault (core dumped)


={============================================================================
*kt_linux_core_400* linux-lib-dependancy

To see library dependancy of a application. This is a `script file` and shows a
`dynamic-library-dependancy`. If not, shows not a dynamic executable message.

*libc-check-version*
The simplest approach is to pick a binary that you consider is typical (e.g.
    /bin/ls and run ldd on it.) One of the listed libraries should be libc -
check its version number.

*tool-ldd*
$ ldd /bin/ls
        libc.so.6 => /lib/libc.so.6 (0x4000e000)
        /lib/ld-linux.so.2 => /lib/ld-linux.so.2 (0x40000000)


{shows-if-can-load-dependency}
This is a result from target ldd version.

root# ldd /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so
checking sub-depends for 'not found' ~
checking sub-depends for '/usr/local/lib/libnexus.so'
checking sub-depends for '/opt/zinc/oss/lib/libgstbase-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgstmpegts-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgstreamer-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgobject-2.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libglib-2.0.so.0'
checking sub-depends for '/lib/libgcc_s.so.1'
checking sub-depends for '/lib/libpthread.so.0'
checking sub-depends for '/lib/libc.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgmodule-2.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libintl.so.8'
checking sub-depends for '/lib/libdl.so.0'
checking sub-depends for '/lib/libm.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libffi.so.5'
  libnexusMgr.so.0 => not found (0x00000000) ~
  libnexus.so => /usr/local/lib/libnexus.so (0x00000000)
  libgstbase-1.0.so.0 => /opt/zinc/oss/lib/libgstbase-1.0.so.0 (0x00000000)
  libgstmpegts-1.0.so.0 => /opt/zinc/oss/lib/libgstmpegts-1.0.so.0 (0x00000000)
  libgstreamer-1.0.so.0 => /opt/zinc/oss/lib/libgstreamer-1.0.so.0 (0x00000000)
  libgobject-2.0.so.0 => /opt/zinc/oss/lib/libgobject-2.0.so.0 (0x00000000)
  libglib-2.0.so.0 => /opt/zinc/oss/lib/libglib-2.0.so.0 (0x00000000)
  libgcc_s.so.1 => /lib/libgcc_s.so.1 (0x00000000)
  libpthread.so.0 => /lib/libpthread.so.0 (0x00000000)
  libc.so.0 => /lib/libc.so.0 (0x00000000)
  libgmodule-2.0.so.0 => /opt/zinc/oss/lib/libgmodule-2.0.so.0 (0x00000000)
  libintl.so.8 => /opt/zinc/oss/lib/libintl.so.8 (0x00000000)
  libdl.so.0 => /lib/libdl.so.0 (0x00000000)
  libm.so.0 => /lib/libm.so.0 (0x00000000)
  libffi.so.5 => /opt/zinc/oss/lib/libffi.so.5 (0x00000000)
  not a dynamic executable

<ex>

#include <stdio.h>

int main()
{
    printf("Hello, this is sample program.\n");
}

gcc sample.c


// debian vm libc case

@kit-debian:~$ readelf -d a.out | grep NEEDED
4: 0x00000001 (NEEDED)                     Shared library: [libc.so.6]

@kit-debian:~$ ldd a.out 
	linux-gate.so.1 (0xb77b5000)
	libc.so.6 => /lib/i386-linux-gnu/i686/cmov/libc.so.6 (0xb75e9000)
	/lib/ld-linux.so.2 (0xb77b8000)


// mips-cross-gcc
// same when use mips-unknown-linux-uclibc-readelf

@kit-debian:~$ readelf -d a.out | grep NEEDED
4: 0x00000001 (NEEDED)                     Shared library: [libc.so.1]

@kit-debian:~$ ldd a.out
	not a dynamic executable

<case>
The ldd is script. Although the binary uses shared library, ldd don't
understand it since it's cross compiled.

$ readelf -d nexus-inspect | grep NEED
(standard input):4: 0x00000001 (NEEDED)  Shared library: [libnexus.so]
(standard input):5: 0x00000001 (NEEDED)  Shared library: [libgcc_s.so.1]
(standard input):6: 0x00000001 (NEEDED)  Shared library: [libpthread.so.0]
(standard input):7: 0x00000001 (NEEDED)  Shared library: [libc.so.0]

$ ldd -v nexus-inspect
	not a dynamic executable


<ex>
mips-unknown-linux-uclibc-ldd is a script and not part of binutil build.

@kit-debian:~$ ./x-tools/mips-unknown-linux-uclibc/bin/mips-unknown-linux-uclibc-ldd \
  --root ./x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/ a.out 
        libc.so.1 => /lib/libc.so.1 (0xdeadbeef)
        ld-uClibc.so.1 => /lib/ld-uClibc.so.1 (0xdeadbeef)

// x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/lib
lrwxrwxrwx 1 kyoupark ccusers       18 Dec  2 10:17 libc.so.1 -> libuClibc-1.0.9.so*
-r-xr-xr-x 1 kyoupark ccusers   612272 Dec  2 10:17 libuClibc-1.0.9.so*


<ex>
$ ~/STB_SW_o/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-
gnu/bin/i686-nptl-linux-gnu-ldd PWM_Process
i686-nptl-linux-gnu-ldd: no root given
Try `i686-nptl-linux-gnu-ldd --help' for more information

$ ~/STB_SW_o/FUSIONOS_9/BLD_NDS_INTEL_X86_LNUX_MRFUSION_01/platform_cfg/linux/compiler/i686-nptl-linux-gnu/bin/i686-nptl-linux-gnu-ldd --root ~/si_logs/flash0-vstb-asan-build/flash0/fs/ PWM_Process
        libm.so.6 => /lib/libm.so.6 (0x8badf00d)
        libc.so.6 => /lib/libc.so.6 (0x8badf00d)
        ld-linux.so.2 => /lib/ld-linux.so.2 (0x8badf00d)
        libpthread.so.0 => /lib/libpthread.so.0 (0x8badf00d)
        librt.so.1 => /lib/librt.so.1 (0x8badf00d)
        libdl.so.2 => /lib/libdl.so.2 (0x8badf00d)
        libtirpc.so.1 not found
        libsky_airplay_helper.so.0 not found
        libminizip.so.1 not found
        libjsoncpp.so.1 not found
        libglib-2.0.so.0 not found
        libgio-2.0.so.0 not found
        libgobject-2.0.so.0 not found
        libdbus-1.so.3 not found
        libz.so.1 not found
        libasan.so.1 => /lib/libasan.so.1 (0x8badf00d)
        libstdc++.so.6 => /lib/libstdc++.so.6 (0x8badf00d)
        libgcc_s.so.1 => /lib/libgcc_s.so.1 (0x8badf00d)
        libusb.so => /NDS/lib/libusb.so (0xdeadc0de)
        libxpower.so.0 not found


={============================================================================
*kt_linux_core_400* linux-lib-ld linux-dynamic-linker

// Linkers, Ian Lance Taylor

I will refer to the version of the linker which creates the program as the
`program linker.` This type of shared libraries was a significant change to the
traditional program linker: it now had to build linking information which
could be used efficiently at runtime by the `dynamic linker.`  


LPI 41.4.3 Using a Shared Library

$ ./prog 
./prog: error while loading shared libraries: libfoo.so: cannot open shared
  object file: No such file or directory


This brings us to the second required step: dynamic linking, which is the task
of resolving the embedded library name at run time. This task is performed by
the dynamic linker (also called the dynamic linking loader or the run-time
linker).

The dynamic linker is `itself a shared library`, named /lib/ld-linux.so.2,
which is employed `by every ELF executable` that uses shared libraries.


<lib-shared-two-step-link>
Every program-including those that use shared libraries-goes through a
static-linking phase. At run time, a program that employs shared libraries
`additionally` undergoes dynamic linking.

Two steps must occur that are not required for programs that use static
libraries:

$ gcc -c -fpic foo.c 
$ gcc -shared -o libfoo.so foo.o
$ gcc -o one main.c libfoo.so          // note: embeds the name of lib

  * At `linktime`. Since the executable file 'no' longer contains copies of
    the object files that it requires, must have some mechanism for
    identifying the shared library that it needs at runtime.

This is done by `embedding the name of the shared library inside the executable`
during the link phase. DT_NEEDED tag in ELF.

$ readelf -d one | grep NEED
0x00000001 (NEEDED)  Shared library: [libfoo.so]    // note: "libfoo"
0x00000001 (NEEDED)  Shared library: [libc.so.6]

note: In order to embed the shared library in a executable that use so, requires
'so' file in linking.

  * At `runtime`, there must be some 'mechanism' for 'resolving' the embedded
    library name-that is, for finding the shared library file corresponding to
    the name specified in the executable file-and then loading the library
    into memory, if it is not already present.


42.1 Dynamically Loaded Libraries

When an executable starts, the `dynamic-linker` loads all of the shared
libraries in the program's `dynamic dependency list.` Sometimes, however, it
can be useful to load libraries at a 'later' time. For example, a plug-in is
loaded only when it is needed. This functionality is provided by an API to the
dynamic linker.


<lib-shared-intereter>
the kernel sees this an ELF file, then it looks for a piece of information
called the "requesting program interpreter" and it calls that program with
your program as argument. Only available for a executable.

<ex> uclibc

$ file busybox
busybox: setuid ELF 32-bit MSB executable, MIPS, MIPS32 version 1 (SYSV), dynamically linked, 
  interpreter /lib/ld-uClibc.so.0, corrupted section header size
busybox: setuid ELF 32-bit MSB executable, MIPS, MIPS32 version 1 (SYSV), dynamically linked, 
  interpreter /lib/ld.so.1, stripped


$ readelf -l busybox
ELF Header:
  Magic:   7f 45 4c 46 01 02 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF32
  Data:                              2's complement, big endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           MIPS R3000

There are no sections in this file.

There are no sections to group in this file.

Program Headers:
  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
  PHDR           0x000034 0x00400034 0x00400034 0x00100 0x00100 R E 0x4
  `INTERP`         0x000134 0x00400134 0x00400134 0x00014 0x00014 R   0x1
      [Requesting program interpreter: /lib/ld-uClibc.so.0]


// on the box
-rwxr-xr-x  1 kyoupark ccusers  29028 Mar  6 07:56 ld-uClibc-0.9.29.so*
lrwxrwxrwx  1 kyoupark ccusers     19 Mar  6 07:49 ld-uClibc.so.0 -> ld-uClibc-0.9.29.so*


// from redhat server
$ readelf -l /bin/echo

  INTERP         0x0000000000000200 0x0000000000400200 0x0000000000400200
                 0x000000000000001c 0x000000000000001c  R      1
      [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2]


The pathname /lib/ld-linux.so.2 is normally a symbolic link pointing to the
dynamic linker executable file. This file has the name ld-version.so, where
version is the glibc version installed on the system: for example, ld-2.11.so.
The pathname of the dynamic linker differs on some architectures. For example,
    on IA-64, the dynamic linker symbolic link is named
    /lib/ld-linux-ia64.so.2.


// ex-glibc
-rwxr-xr-x 1 kyoupark ccusers 746756 Dec 15 08:16 ld-2.20.so
lrwxrwxrwx 1 kyoupark ccusers     10 Dec 15 08:16 ld.so.1 -> ld-2.20.so


// see this usage help from glibc-2.20/elf/rtld.c

note:
-sh-3.2# ./ld-uClibc-0.9.29.so
Standalone execution is not supported yet

<check-ld-versoin>

$ /lib64/ld-linux-x86-64.so.2
`Usage: ld.so [OPTION]... EXECUTABLE-FILE [ARGS-FOR-PROGRAM...]`
You have invoked `ld.so', the helper program for shared library executables.
This program usually lives in the file `/lib/ld.so', and special directives
in executable files using ELF shared libraries tell the system's program
loader to load the helper program from this file.  This helper program loads
the shared libraries needed by the program executable, prepares the program
to run, and runs it. "You may invoke this helper program directly from the
command line to load and run an ELF executable file"; this is like executing
that file itself, but always uses this helper program from the file you
specified, instead of the helper program file specified in the executable
file you run.  This is mostly of use for maintainers to test new versions
of this helper program; chances are you did not intend to run this program.

  --list                list all dependencies and how they are resolved
  --verify              verify that given object really is a dynamically linked
                        object we can handle
  --library-path PATH   use given PATH instead of content of the environment
                        variable LD_LIBRARY_PATH
  --inhibit-rpath LIST  ignore RUNPATH and RPATH information in object names
                        in LIST
  --audit LIST          use objects named in LIST as auditors


$ /lib64/ld-linux-x86-64.so.2 /bin/echo 'Hello, world!'
Hello, world!

<ex> see that has the same result as *tool-ldd*
$ /lib64/ld-linux-x86-64.so.2 --list /bin/echo
        linux-vdso.so.1 =>  (0x00007fff94f8e000)
        libc.so.6 => /lib64/libc.so.6 (0x0000003ba8000000)
        /lib64/ld-linux-x86-64.so.2 (0x0000003ba7c00000)

https://blog.ksub.org/bytes/2016/07/23/ld.so-glibcs-dynanic-linker/loader/

There's another helpful environment variable that the dynamic loader pays
attention to: LD_TRACE_LOADED_OBJECTS. `This is what ldd actually uses` to
display the list of libraries that are loaded. If you compare the output of
setting this variable and the output of --list you'll notice that they are
slightly different. ldd will optionally set LD_VERBOSE=1 (to output version
    information) and LD_DEBUG=unused (to output unused shared libraries).


={============================================================================
*kt_linux_core_400* linux-lib-ld-glibc ld-gdb

https://sourceware.org/glibc/wiki/Debugging/Loader_Debugging

Debugging the Loader

This page describes different methods for debugging the GLIBC dynamic
linker/loader ld[64].so to achieve certain goals.

There are several loader environment scenarios:

1. Loader is in the system toolchain, e.g. /lib[64]/ld[64].so.1

2. Loader is in a self-contained toolchain, e.g.
/opt/toolchain/lib[64]/ld[64].so.1

3. Loader is in an install_root installed GLIBC directory, e.g.
/home/user/glibc/install_root/lib[64]/ld[64].so.1 (This is an intermediary
    install step of GLIBC performed using make install_root=path install and
    is used for packaging purposes).

4. Loader is in a non-installed GLIBC build directory, e.g.
/home/user/glibc/build/glibc[32|64]/elf/ld[64].so.1

The first two scenarios are relatively similar and debugging shouldn't provide
any great challenges other than the following:

The loader has demonstrated fault in its symbol resolver code. - You can step
into the loader resolver code from the application main procedure. Todo: Write
this section.

The third scenario and fourth scenarios are challenging because you have to
break the toolchain packages' assumptions about library paths. This is
described by the following section:

An alternate loader is being used and an application or library needs to be
debugged. - An application or library that uses an alternate loader must be
debugged by debugging the loader and directing the loader to debug the
application or library. This may require creating an application in order to
step into a library.


Debugging With An Alternate Loader

Note: The examples here were done on the Power Architecture. The symbols that
are used to break into the loader may vary by platform.

Note: For this example assume that GLIBC was configured and built into
/home/user/glibc/build/glibc[32|64]. It doesn't need to be installed in order
to test it. Our test application resides in /home/user/glibc/build/test.

Often times you may need to test an application against a GLIBC build that is
not installed into the system root. You can use the following method to debug
against a `non-installed GLIBC build` that was configured with any --prefix
(but not installed).

In order to runtime test against the GLIBC build one must invoke the newly
built loader, ld[64].so, from the GLIBC build directly passing ld[64].so the
application name and using the --library-path directive to identify the
location of the libraries built by the GLIBC build, e.g.

32-bit:
/home/user/glibc/build/glibc32/elf/ld.so.1 --library-path \
</home/user/glibc/build/glibc32/nptl:/home/user/glibc/build/glibc32/elf:etc> <application>

64-bit:
/home/user/glibc/build/glibc64/elf/ld64.so.1 --library-path \
</home/user/glibc/build/glibc64/nptl:/home/user/glibc/build/glibc64/elf:etc> <application>

If this GLIBC installation or the application that runs against it need to be
debugged one must direct GDB to first debug the newly installed loader and the
loader is directed to run the application to be debugged. We'll trap in the
loader and then step into the application or libraries. This can be
accomplished with a .gdb script.

<loader-breakpoint>
In order to be able to step into the application or the libraries that are
loaded we'll want to immediately trap in the loader. So we'll set a breakpoint
on the following loader symbol:

32-bit:
_dl_main_dispatch

64-bit:
_dl_start_user

~/asn/gcc/glibc-2.13-build$ nm libc.so | grep _dl_
~/asn/gcc/glibc-2.20-build$ nm libc.so | grep _dl_

2857:00016f18 t _dl_start

~/asn/gcc/glibc-2.13-build$ file libc.so
libc.so: ELF 32-bit MSB shared object, MIPS, MIPS32 version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, not stripped

https://sourceware.org/glibc/wiki/DynamicLoader

Dynamic Loader Operation

The following is an annotated and summarized view of the dynamic loader startup:

// CFLAGS='-O -g' \
// ../glibc-2.13/configure --prefix=${PREFIX}/${TARGET} \
// --build=$MACHTYPE --host=${TARGET} \
// --without-cvs --disable-profile --without-gd \
// --with-headers=${PREFIX}/${TARGET}/include \
// --disable-debug --disable-sanity-checks --enable-obsolete-rpc \
// libc_cv_forced_unwind=yes libc_cv_c_cleanup=yes \
// --enable-kernel=2.6.18 --with-__thread --with-tls --enable-shared \
// --without-fp --enable-add-ons=nptl,../glibc-ports-2.13

// sysdeps/mips/dl-machine.h
// /* Initial entry point code for the dynamic linker.
//    The C function `_dl_start' is the real entry point;
//    its return value is the user program's entry point.
//    Note how we have to be careful about two things:
// 
//    1) That we allocate a minimal stack of 24 bytes for
//       every function call, the MIPS ABI states that even
//       if all arguments are passed in registers the procedure
//       called can use the 16 byte area pointed to by $sp
//       when it is called to store away the arguments passed
//       to it.
// 
//    2) That under Linux the entry is named __start
//       and not just plain _start.  */
// 
#define RTLD_START asm (\
RTLD_START (sysdep/<arch>/dl-machine.h)

  // static ElfW(Addr) __attribute_used__ internal_function
  // _dl_start (void *arg)
  `_dl_start` (elf/rtld.c)
  {

  ELF_MACHINE_BEFORE_RTLD_RELOC (sysdep/<arch>/dl-machine.h)
  // ELF_DYNAMIC_RELOCATE (sysdep/<arch>/dl-machine.h, Relocate the loader)
  // /* Relocate ourselves so we can do normal function calls and
  // data access using the global offset table.  */
  ELF_DYNAMIC_RELOCATE (&bootstrap_map, 0, 0);

  // _dl_start_final (elf/rtld.c, bottom half of _dl_start)

#ifdef DONT_USE_BOOTSTRAP_MAP
  ElfW(Addr) entry = _dl_start_final (arg);
#else
  ElfW(Addr) entry = _dl_start_final (arg, &info);
#endif
  {
    /* Call the OS-dependent function to set up life so we can do things like
       file access.  It will call `dl_main' (below) to do all the real work
       of the dynamic linker, and then unwind our frame and run the user
       entry point on the same stack we entered on.  */
    // elf/dl-sysdep.c
    start_addr = _dl_sysdep_start (arg, &dl_main);
    {

      user_entry = (ElfW(Addr)) ENTRY_POINT;

      // dl_main (elf/rtld.c, main entry point for loader)
      (*dl_main) (phdr, phnum, &user_entry, _dl_auxv);
      static void
        `dl_main` (const ElfW(Phdr) *phdr,
            ElfW(Word) phnum,
            ElfW(Addr) *user_entry,
            ElfW(auxv_t) *auxv)
        {

          `process_envvars` (&mode);

          if (*user_entry == (ElfW(Addr)) ENTRY_POINT)
          {
            // Ho ho.  We are not the program interpreter!  We are the program
            // itself!  This means someone ran ld.so as a command.  

            rtld_is_main = true;

            _dl_fatal_printf ("\
                Usage: ld.so [OPTION]... EXECUTABLE-FILE [ARGS-FOR-PROGRAM...]\n\
                You have invoked `ld.so', the helper program for shared library executables.\n\
                This program usually lives in the file `/lib/ld.so', and special directives\n\
                in executable files using ELF shared libraries tell the system's program\n\
                loader to load the helper program from this file.  This helper program loads\n\
                the shared libraries needed by the program executable, prepares the program\n\
                to run, and runs it.  You may invoke this helper program directly from the\n\
                command line to load and run an ELF executable file; this is like executing\n\
                that file itself, but always uses this helper program from the file you\n\
                specified, instead of the helper program file specified in the executable\n\
                file you run.  This is mostly of use for maintainers to test new versions\n\
                of this helper program; chances are you did not intend to run this program.\n\
                \n\
                --list                list all dependencies and how they are resolved\n\
                --verify              verify that given object really is a dynamically linked\n\
                object we can handle\n\
                --library-path PATH   use given PATH instead of content of the environment\n\
                variable LD_LIBRARY_PATH\n\
                --inhibit-rpath LIST  ignore RUNPATH and RPATH information in object names\n\
                in LIST\n\
                --audit LIST          use objects named in LIST as auditors\n");


            if (__builtin_expect (mode, normal) == verify)
            {}
            else
            {
              *call-dl-map-object*
              // since use standalone more. 
              //
              // call-dl-map-object loops and load each:
              // {
              //    _dl_map_object_from_fd
              //    {
              //       _dl_debug_printf ("file=%s [%lu];  
              //         generating link map\n", name, nsid);
              //
              //       `elf_get_dynamic_info` (l, NULL);
              //    }
              // }
              //
              // 20286:      KT: dynamic string...     
              // 20286:       file=./minit-dynamic [0];  generating link map
              // 20286:     *** KT: elf from map obj ***
              // 20286:       dynamic: 0x0040017c  base: 0x00000000   size: 0x000019e0
              // 20286:         entry: 0x004005e0  phdr: 0x00400034  phnum:          8
              // 20286:
              // 20286:     *** KT: map obj ***
              // 20286:      KT: system_dirs: /lib/
              // 20286:
              // 20286:     file=libgcc_s.so.1 [0];  needed by ./minit-dynamic [0]
              // 20286:     find library=libgcc_s.so.1 [0]; searching
              // 20286:      KT: RPATH...     
              // 20286:         KT: LD_LIB...     20286:        search path=/mnt/tmp/asn/glibc-libs/lib/tls:/mnt/tmp/asn/glibc-libs/lib                (LD_LIBRARY_PATH)
              // 20286:     KT:  trying file=/mnt/tmp/asn/glibc-libs/lib/tls/libgcc_s.so.1
              // 20286:     KT:  trying file=/mnt/tmp/asn/glibc-libs/lib/libgcc_s.so.1
              // 20286:      KT: RUNPATH...     20286:       KT: default...
              // 20286:
              // 20286:     file=libgcc_s.so.1 [0];  generating link map
              // 20286:     *** KT: elf from map obj ***
              // 20286:       dynamic: 0x2aaac10c  base: 0x2aaac000   size: 0x0000f940
              // 20286:         entry: 0x2aaae680  phdr: 0x2aaac034  phnum:          6
              // 20286:
              // 20286:
              // 20286:     file=libc.so.6 [0];  needed by ./minit-dynamic [0]
              // 20286:     find library=libc.so.6 [0]; searching
              // 20286:      KT: RPATH...     20286:         KT: LD_LIB...     20286:        search path=/mnt/tmp/asn/glibc-libs/lib                (LD_LIBRARY_PATH)
              // 20286:     KT:  trying file=/mnt/tmp/asn/glibc-libs/lib/libc.so.6
              // 20286:      KT: RUNPATH...     20286:       KT: default...
              // 20286:
              // 20286:     file=libc.so.6 [0];  generating link map
              // 20286:     *** KT: elf from map obj ***
              // 20286:       dynamic: 0x2aabc1ac  base: 0x2aabc000   size: 0x00159e50
              // 20286:         entry: 0x2aad32a4  phdr: 0x2aabc034  phnum:         11
              
              _dl_map_object (NULL, rtld_progname, lt_library, 0,
                  __RTLD_OPENEXEC, LM_ID_BASE);

              _dl_debug_printf ("*** KT: map obj ***\n");
              
              goes to *calls-dl-relocate*
            }

          }
          else
          {
            // note: alloc *link-map*
            /* Create a link_map for the executable itself.
               This will be what dlopen on "" returns.  */
            main_map = _dl_new_object ((char *) "", "", lt_executable, NULL,
                __RTLD_OPENEXEC, LM_ID_BASE);
          }

          /* Scan the program header table for the dynamic section.  */
          for (ph = phdr; ph < &phdr[phnum]; ++ph);

          // note: parse dynamic section and fill l->l_info
          if (! rtld_is_main)
          {
            /* Extract the contents of the dynamic section for easy access.  */
            // `elf/dynamic-link.h elf_get_dynamic_info` (struct link_map *l, ElfW(Dyn) *temp)
            `elf_get_dynamic_info` (main_map, NULL);
            /* Set up our cache of pointers into the hash table.  */
            _dl_setup_hash (main_map);
          }

          // elf/dl-load.c for ld-search-order
          /* Map in the shared object file NAME.  */
          _dl_map_object (NULL, rtld_progname, lt_library, 0);

          // NEED_DL_SYSINFO is not defined for mips

          // *elf/dl-load.c*  fill in RPATH and LD_LIBRARY_PATH information);
          /* Initialize the data structures for the search paths for shared
             objects.  */
          `_dl_init_paths` (library_path);

          // elf/dl-debug.c, initialization of _r_debug and DT_DEBUG);
          /* Initialize _r_debug.  */
          struct r_debug *r = _dl_debug_initialize (GL(dl_rtld_map).l_addr,
              LM_ID_BASE);

          // KT: dl-lookup.c
          _dl_lookup_symbol_x();

          // elf/dl-reloc.c
          //
          // include/link.h
          // struct link_map
          // {
          //    ElfW(Dyn) *l_info[DT_NUM + DT_THISPROCNUM + DT_VERSIONTAGNUM
          //      + DT_EXTRANUM + DT_VALNUM + DT_ADDRNUM];
          // };
          //
          // glibc-2.13
          //
          // DT_THISPROCNUM     22 sysdeps/mips/dl-dtprocnum.h #define DT_THISPROCNUM	DT_MIPS_NUM
          // DT_MIPS_NUM      1677 elf/elf.h        #define DT_MIPS_NUM	     0x35
          // DT_VERSIONTAGNUM  752 elf/elf.h        #define DT_VERSIONTAGNUM 16
          // DT_EXTRANUM       759 elf/elf.h        #define DT_EXTRANUM	3
          // DT_VALNUM         713 elf/elf.h        #define DT_VALNUM 12
          // DT_ADDRNUM        734 elf/elf.h        #define DT_ADDRNUM 11

          if (prelinked)
          {}
          else
          {
            /* Now we have all the objects loaded.  Relocate them all except for
               the dynamic linker itself.  We do this in reverse order so that copy
               relocs of earlier objects overwrite the data written by later
               objects.  We do not re-relocate the dynamic linker itself in this
               loop because that could result in the GOT entries for functions we
               call being changed, and that would break us.  It is safe to relocate
               the dynamic linker out of order because it has no copy relocs (we
               know that because it is self-contained).  */

            if (l != &GL(dl_rtld_map))
            {
              *calls-dl-relocate*
              _dl_relocate_object (l, l->l_scope, GLRO(dl_lazy) ? RTLD_LAZY : 0,
                  consider_profiling);
            }
          }

          _dl_relocate_object (struct link_map *l)
          {
            *current-working*

            // 1759:     relocation processing: /mnt/tmp/asn/glibc-libs/lib/libc.so.6 (lazy)

            if (__builtin_expect (GLRO(dl_debug_mask) & DL_DEBUG_RELOC, 0))
              _dl_debug_printf ("\nrelocation processing: %s%s\n",
                  l->l_name[0] ? l->l_name : rtld_progname,
                  lazy ? " (lazy)" : "");

            ELF_DYNAMIC_RELOCATE (l, lazy, consider_profiling);
          }

          _dl_debug_bindings (const char *undef_name, struct link_map *undef_map)
          {
            if (GLRO(dl_debug_mask) & DL_DEBUG_BINDINGS)
            {

              // 1759:     binding file /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0] to /mnt/tmp/asn/glibc-libs/lib/libc.so.6 [0]: normal symbol `argp_err_exit_status' [GLIBC_2.2]
              
              _dl_debug_printf ("binding file %s [%lu] to %s [%lu]: %s symbol `%s'",
                  (reference_name[0]
                   ? reference_name
                   : (rtld_progname ?: "<main program>")),
                  undef_map->l_ns,
                  value->m->l_name[0] ? value->m->l_name : rtld_progname,
                  value->m->l_ns,
                  protected ? "protected" : "normal", undef_name);
              if (version)
                _dl_debug_printf_c (" [%s]\n", version->name);
              else
                _dl_debug_printf_c ("\n");
            }

            do_lookup_x (undef_name, new_hash, &old_hash, *ref);
          }

          DL_SYSDEP_INIT;
          DL_PLATFORM_INIT; (sysdeps/<arch>/dl-machine.h)
            /* Last action of _dl_sysdep_start return the user entry point */
            return user_entry;
        }
    }
  }

  /* Last action of _dl_start returns the user entry point */      
  ELF_MACHINE_START_ADDRESS (GL(dl_loaded), entry); (sysdep/<arch>/dl-machine.h)
    _dl_init (elf/dl-init.c, run DSO initializers)
    call_init (elf/dl-init.c)
    jump to return of _dl_start (sysdep/<arch>/dl-machine.h)
    /*User code is now executing*/
  }

<dl-runtime-resolve>
/* Machine-dependent ELF dynamic relocation inline functions.  Stub version.
 */
_dl_runtime_resolve  111 sysdeps/generic/dl-machine.h   extern void _dl_runtime_resolve (Elf32_Word);
_dl_runtime_resolve  124 sysdeps/generic/dl-machine.h       got[2] = (Elf32_Addr) &_dl_runtime_resolve;

/* Machine-dependent ELF dynamic relocation inline functions.  MIPS version.
 */
_dl_runtime_resolve  723 sysdeps/mips/dl-machine.h   extern void _dl_runtime_resolve (ElfW(Word));
_dl_runtime_resolve  737 sysdeps/mips/dl-machine.h       got[0] = (ElfW(Addr)) &_dl_runtime_resolve;
_dl_runtime_resolve  288 sysdeps/mips/dl-trampoline.c   " STRINGXV(SETUP_GP64 (0, _dl_runtime_resolve)) "\n\


NOTE: In order for gdb to be able to fully debug the application and
standalone GLIBC you need to invoke it through the new standalone loader.

Otherwise GDB will indicate that it's version of libthread_db is different
from the libpthread you're attempting to debug by giving the following error:

versions of libpthread and libthread_db do not match

So we need GDB to be able to find all of the libraries created by the
standalone GLIBC build.

32-bit:
#!/bin/bash

GLIBC="/home/user/glibc/build/glibc32"

# We need to make sure that gdb is linked against the standalone glibc so that
# it picks up the correct nptl_db/libthread_db.so. So that means invoking gdb
# using the standalone glibc's linker.

${GLIBC}/elf/ld.so.1 --library-path \
${GLIBC}:\
${GLIBC}/math:\
${GLIBC}/elf:\
${GLIBC}/dlfcn:\
${GLIBC}/nss:\
${GLIBC}/nis:\
${GLIBC}/rt:\
${GLIBC}/resolv:\
${GLIBC}/crypt:\
${GLIBC}/nptl:\
${GLIBC}/nptl_db:\
/usr/bin/gdb -x test.gdb -d /home/user/glibc/libc /home/user/glibc/build/glibc32/elf/ld.so.1

Note: the -d directive tells GDB where to find the library sources.


In order to direct the loader to execute the test application and load the
dependent libraries we'll use the --library-path directive.

note: glibc test-suite?

We can accomplished all of this easily using a GDB script: Note: if the
application you're debugging is a GLIBC test-suite application you must append
the --direct flag to the application invocation as you'll see in the following
examples.

> cat test.gdb

32-bit:
set environment C -E -x c-header
break _dl_main_dispatch
run --library-path
/home/user/glibc/build/glibc32:\
/home/user/glibc/build/glibc32/nptl:\
/home/user/glibc/build/glibc32/math:\
/home/user/glibc/build/glibc32/elf:\
/home/user/glibc/build/glibc32/dlfcn:\
/home/user/glibc/build/glibc32/nss:\
/home/user/glibc/build/glibc32/nis:\
/home/user/glibc/build/glibc32/rt:\
/home/user/glibc/build/glibc32/resolv:\
/home/user/glibc/build/glibc32/crypt:\
/home/user/glibc/build/glibc32/nptl:\
/home/user/glibc/build/glibc32/nptl_db \
/home/user/glibc/build/test/test --direct


={============================================================================
*kt_linux_core_400* linux-lib-ld-uclibc spk-toolchain-uclibc

*current-working*

<build-config>
#
# General Library Settings
#

# LDSO_RUNPATH is not set
#ifdef __LDSO_RUNPATH__


SUPPORT_LD_DEBUG=y
SUPPORT_LD_DEBUG_EARLY=y

In file included from ldso/ldso/ldso.c:46:
ldso/ldso/mips/elfinterp.c:125:2: error: #error "not implemented"
make: *** [ldso/ldso/ldso.oS] Error 1
[root@localhost uClibc-nptl-0.9.29-20070423]# 

ldso/mips/elfinterp.c

#if defined (__SUPPORT_LD_DEBUG__)
#error "not implemented"
#else
	*got_addr = new_addr;
#endif


note:
to print out before output routines are ready but these X_LD_DEBUG not
supported in mips.

#ifdef __SUPPORT_LD_DEBUG_EARLY__
# define SEND_STDERR_DEBUG(X) SEND_STDERR(X)
# define SEND_NUMBER_STDERR_DEBUG(X, add_a_newline) SEND_NUMBER_STDERR(X, add_a_newline)
# define SEND_ADDRESS_STDERR_DEBUG(X, add_a_newline) SEND_ADDRESS_STDERR(X, add_a_newline)
#else


<elf-source-struct>
// ldso/include/dl-hash.h
struct elf_resolve {
  /* These entries must be in this order to be compatible with the interface
   * used by gdb to obtain the list of symbols. */
  ...

  /*
   * These are only used with ELF style shared libraries
   */
  Elf_Symndx nchain;
  Elf_Symndx *chains;

  // note: after all, dynamic section has fixed number of items.
  // #define `DYNAMIC_SIZE` (DT_NUM+OS_NUM+ARCH_NUM)
  unsigned long `dynamic_info[DYNAMIC_SIZE]`;
};

// uClibc-nptl-0.9.29-20070423/ldso/ldso/dl-startup.c
_dl_start(unsigned long args)
{
  struct elf_resolve *tpnt = &tpnt_tmp;
  ElfW(Dyn) *dpnt;
   
  /* Auxiliary vector.  */
  ElfW(auxv_t) auxvt[AT_EGID + 1];

  // elf_machine_load_address() from ldso/ldso/mips/dl-sysdep.h
  /* locate the ELF header.   We need this done as soon as possible
   * (esp since SEND_STDERR() needs this on some platforms... */
  auxvt[AT_BASE].a_un.a_val = elf_machine_load_address();
  load_addr = auxvt[AT_BASE].a_un.a_val;
  header = (ElfW(Ehdr) *) auxvt[AT_BASE].a_un.a_val;


  // elf_machine_dynamic() from ldso/ldso/mips/dl-sysdep.h
  /* Locate the global offset table.  Since this code must be PIC
   * we can take advantage of the magic offset register, if we
   * happen to know what that is for this architecture.  If not,
   * we can always read stuff out of the ELF file to find it... */
  got = elf_machine_dynamic();
  dpnt = (ElfW(Dyn) *) (got + load_addr);

  _dl_memset(tpnt, 0, sizeof(struct elf_resolve));

  // ldso/ldso/mips/dl-startup.h
  // /* We can't call functions earlier in the dl startup process */
  // #define NO_FUNCS_BEFORE_BOOTSTRAP

  // ldso/include/dl-elf.h
  // _dl_parse_dynamic_info
  /* OK, that was easy.  Next `scan the DYNAMIC section of the image.`
     We are only doing ourself right now - we will have to do the rest later */

  SEND_STDERR_DEBUG("Scanning DYNAMIC section\n");
  tpnt->dynamic_addr = dpnt;

  // ldso/include/dl-elf.h
  static __always_inline
    _dl_parse_dynamic_info(dpnt, dynamic_info[] /* tpnt->dynamic_info */, NULL /*debug_addr */, load_addr)
    void __dl_parse_dynamic_info(ElfW(Dyn) *dpnt, unsigned long dynamic_info[], void *debug_addr, ElfW(Addr) load_off)
    {
      // note: updates dynamic_info for each DT tags and is within
      // DYNAMIC_SIZE.
      
      for (; dpnt->d_tag; dpnt++) {

        if (dpnt->d_tag < DT_NUM) {

          // note: *how-get-pltgot* 
          //
          // #define DT_PLTGOT	3  /* Processor defined value */
          // #define DT_NUM      34 /* Number used, 0x22 */
          //
          // /* OS and/or GNU dynamic extensions */
          // #define OS_NUM 1
          // 
          // // from dynamic header
          // 0x00000003 (PLTGOT)                     0x401980
          // 
          // ldso/ldso/mips/dl-sysdep.h
          // #define ARCH_NUM 3
          // #define DT_MIPS_GOTSYM_IDX	(DT_NUM + OS_NUM)
          // #define DT_MIPS_LOCAL_GOTNO_IDX	(DT_NUM + OS_NUM +1)
          // #define DT_MIPS_SYMTABNO_IDX	(DT_NUM + OS_NUM +2)

          dynamic_info[dpnt->d_tag] = dpnt->d_un.d_val;

        } else if (dpnt->d_tag < DT_LOPROC) {

          // #define DT_LOOS     0x6000000d	/* Start of OS-specific */
          // #define DT_HIOS     0x6ffff000	/* End of OS-specific */
          // #define DT_LOPROC   0x70000000	/* Start of processor-specific */

          if (dpnt->d_tag == DT_RELOCCOUNT)
            dynamic_info[DT_RELCONT_IDX] = dpnt->d_un.d_val;
        }

// ldso/ldso/mips/dl-sysdep.h
#ifdef ARCH_DYNAMIC_INFO
        else {
          ARCH_DYNAMIC_INFO(dpnt, dynamic_info, debug_addr);

            // note: here updates dynamic_info for DT_MIPS_LOCAL_GOTNO_IDX
            //
            // #define DT_MIPS_LOCAL_GOTNO_IDX	(DT_NUM + OS_NUM +1)
            //
            // else if (dpnt->d_tag == DT_MIPS_LOCAL_GOTNO) \
            //      dynamic[DT_MIPS_LOCAL_GOTNO_IDX] = dpnt->d_un.d_val; \
        }
#endif
      }

      // add offset to these DTs
#define ADJUST_DYN_INFO(tag, load_off) \
      do { \
        if (dynamic_info[tag]) \
        dynamic_info[tag] += load_off; \
      } while(0)

      ADJUST_DYN_INFO(DT_HASH, load_off);

      // note: *how-get-pltgot* 
      ADJUST_DYN_INFO(DT_PLTGOT, load_off);
      ADJUST_DYN_INFO(DT_STRTAB, load_off);
      ADJUST_DYN_INFO(DT_SYMTAB, load_off);
      ADJUST_DYN_INFO(DT_RELOC_TABLE_ADDR, load_off);
      ADJUST_DYN_INFO(DT_JMPREL, load_off);
    } // _dl_parse_dynamic_info()

// ldso/ldso/mips/dl-startup.h
#if defined(PERFORM_BOOTSTRAP_GOT)
  // /*
  //  * Here is a macro to perform `the GOT relocation. `This is only
  //  * used when bootstrapping the dynamic loader.
  //  */
  // #define PERFORM_BOOTSTRAP_GOT(tpnt)  \
  // do {                                 \
  //   ...
  // } while (0)

  SEND_STDERR_DEBUG("About to do specific GOT bootstrap\n");
  /* some arches (like MIPS) we have to tweak the GOT before relocations */
  PERFORM_BOOTSTRAP_GOT(tpnt);
#else
#endif

  /* Wahoo!!! */
  SEND_STDERR_DEBUG("Done relocating ldso; we can now use globals and make function calls!\n");

  /* Now we have done the mandatory linking of some things.  We are now
     free to start using global variables, since these things have all been
     fixed up by now. Still `no function calls outside of this library`,
     since the `dynamic resolver` is not yet ready. */

  // ldso.c
  _dl_get_ready_to_run(tpnt, load_addr, auxvt, envp, argv);
  {
    struct elf_resolve app_tpnt_tmp;
    struct elf_resolve *app_tpnt = &app_tpnt_tmp;

    /* Wahoo!!! We `managed to make a function call!`  Get malloc
     * setup so we `can use _dl_dprintf()` to print debug noise
     * instead of the SEND_STDERR macros used in dl-startup.c */

    // <not-support-standalone>
    // -sh-3.2# /lib/ld-uClibc.so.0
    // Standalone execution is not supported yet

    if (_start == (void *) auxvt[AT_ENTRY].a_un.a_val) {
      `_dl_dprintf`(_dl_debug_file, "Standalone execution is not supported yet\n");
      _dl_exit(1);
    }

    // note: check on LD_ envs
    /* Start to build the tables of the modules that are required for
     * this beast to run.  We start with the basic executable, and then
     * go from there.  Eventually we will run across ourself, and we
     * will need to properly deal with that as well.
     */

    // # HAVE_NO_SSP is not set
    // UCLIBC_HAS_SSP=y
    // # UCLIBC_HAS_SSP_COMPAT is not set
    // UCLIBC_BUILD_SSP=y

#ifdef __UCLIBC_HAS_SSP__
    /* Set up the stack checker's canary.  */
    stack_chk_guard = _dl_setup_stack_chk_guard ();
#endif

    /* At this point we are now free to examine the user application,
     * and figure out which libraries are supposed to be called.  Until
     * we have this list, we will not be completely ready for dynamic
     * linking.
     */

    /* Find the runtime load address of the main executable.  This may be
     * different from what the ELF header says for ET_DYN/PIE executables.
     */
    {
      unsigned int idx;
      ElfW(Phdr) *phdr = (ElfW(Phdr) *) auxvt[AT_PHDR].a_un.a_val;

      for (idx = 0; idx < auxvt[AT_PHNUM].a_un.a_val; idx++, phdr++)
        if (phdr->p_type == PT_PHDR) {

          // note: gets `load address`
          app_tpnt->loadaddr = (ElfW(Addr)) (auxvt[AT_PHDR].a_un.a_val - phdr->p_vaddr);
          break;
        }

      if (app_tpnt->loadaddr)
        _dl_debug_early("Position Independent Executable: "
            "app_tpnt->loadaddr=%x\n", app_tpnt->loadaddr);
    }

    ppnt = (ElfW(Phdr) *) auxvt[AT_PHDR].a_un.a_val;

    // `loop through program header table` (segments table)
    for (i = 0; i < auxvt[AT_PHNUM].a_un.a_val; i++, ppnt++) {

      // handle dynamic segment
      if (ppnt->p_type == PT_DYNAMIC) {

        // _dl_parse_dynamic_info 
        // note: second time and fill up dynamic_info
        _dl_parse_dynamic_info(dpnt, app_tpnt->dynamic_info, debug_addr, app_tpnt->loadaddr);

        /* OK, we have what we need - slip this one into the list. */
        /*
         * We call this function when we have just read an ELF library or executable.
         * We `add the relevant info to the symbol chain`, so that we can resolve all
         * externals properly.
         */
        // ldso/ldso/dl-hash.c
        // note: how many times does it get run? once?
        app_tpnt = _dl_add_elf_hash_table(_dl_progname, (char *)app_tpnt->loadaddr,
            app_tpnt->dynamic_info, ppnt->p_vaddr + app_tpnt->loadaddr, ppnt->p_filesz);
        {
          /*
           * This is the start of `the linked list` that describes all of the files present
           * in the system with pointers to all of the symbol, string, and hash tables,
           * as well as all of the other good stuff in the binary.
           */
          struct elf_resolve *_dl_loaded_modules = NULL;

		    tpnt = _dl_loaded_modules = (struct elf_resolve *) _dl_malloc(sizeof(struct elf_resolve));
        }

        _dl_loaded_modules->libtype = elf_executable;

        // note: get PLTGOT address from dynamic section and set up PLTGOT
        // 0, 1, 2 entry with appropriate values.
        
        // #define DT_PLTGOT	3		/* Processor defined value */
        lpnt = (unsigned long *) (app_tpnt->dynamic_info[DT_PLTGOT]);

        // FORCE_SHAREABLE_TEXT_SEGMENTS=y
        // 39 ldso/ldso/ldso.c #define ALLOW_ZERO_PLTGOT

        // ldso/mips/dl-sysdep.h from *patch-01*
        //
        // INIT_GOT(GOT_BASE,MODULE);
        // INIT_GOT(lpnt, _dl_loaded_modules);
        //
        // 	/* Fill in first two GOT entries `according to the ABI` */		\
        //
        //  note: _dl_runtime_resolve is asm code which calls
        //  __dl_runtime_reolve and this looks similar to the same name of
        //  glibc version.
        //
        // 	GOT_BASE[0] = (unsigned long) _dl_runtime_resolve;			\
        // 	GOT_BASE[1] = (unsigned long) MODULE;					\
        // 	idx = 2;								\ 
        // 	if (MODULE->dynamic_info[DT_JMPREL])					\ 
        // 		GOT_BASE[idx++] = (unsigned long) _dl_linux_resolve;		\ 
        //
        // note: `elf_resolve` MODULE. ldso/ldso/dl-hash.c
        // /*
        //  * This is the start of the linked list that describes all of the files present
        //  * in the system with pointers to all of the symbol, string, and hash tables,
        //  * as well as all of the other good stuff in the binary.
        //  */
        // struct elf_resolve *_dl_loaded_modules = NULL;
        //
      } // PT_DYNAMIC

      /* OK, fill this in - we did not have this before */
      if (ppnt->p_type == PT_INTERP) {
        char *ptmp;

        tpnt->libname = (char *) ppnt->p_vaddr + app_tpnt->loadaddr;

        /* Store the path where the shared lib loader was found
         * for later use
         */
        _dl_ldsopath = _dl_strdup(tpnt->libname);
        ptmp = _dl_strrchr(_dl_ldsopath, '/');
        if (ptmp != _dl_ldsopath)
          *ptmp = '\0';

        _dl_debug_early("Lib Loader: (%x) %s\n", tpnt->loadaddr, tpnt->libname);
      }
    }

    // LDSO_LDD_SUPPORT=y
    // LDSO_PRELOAD_FILE_SUPPORT is not set

    // note: applies to all in the list, _dl_loaded_modules
    /*
     * Relocation of the GOT entries for MIPS have to be done
     * after all the libraries have been loaded.
     */

    // note:
    // when looks into elfinterp.o in the build output, there is no object
    // file. not built? thing is that elfinterp.c and other c files gets built 
    // as part of ldso.c since in ldso.c, can see:
    // 
    // <ldso.c>
    //
    // /* Pull in the value of _dl_progname */
    // #include LDSO_ELFINTERP
    //
    // #include "dl-hash.c"
    // #include "dl-elf.c"
    //
    // <command-line for ldso.c build>
    //
    // -DLDSO_ELFINTERP="mips/elfinterp.c"
    //
    // ldso/ldso/mips/elfinterp.c
    // _dl_perform_mips_global_got_relocations(struct elf_resolve *tpnt, int lazy)
    _dl_perform_mips_global_got_relocations(_dl_loaded_modules, !unlazy);
    {
		/* Relocate the global GOT entries for the object */

      // ldso/ldso/mips/elfinterp.c from *patch-02* which change to handle
      // #define SHN_UNDEF 0 /* Undefined section */ when it is not lazy in
      // _dl_perform_mips_global_got_relocations.
    }

    /*
     * OK, now all of the kids are tucked into bed in their proper
     * addresses.  Now we go through and look for `REL and RELA records` that
     * indicate fixups to the GOT tables.  We need to do this in reverse
     * order so that COPY directives work correctly.
     */
    // ldso/dl-elf.c
    if (_dl_symbol_tables)
      if (_dl_fixup(_dl_symbol_tables, unlazy))
      {
        // ldso/ldso/dl-elf.c
        // _dl_fixup(struct dyn_elf *rpnt, int now_flag);

        // ldso/ldso/arm/elfinterp.c
        goof += _dl_parse_relocation_information(rpnt,
            tpnt->dynamic_info[DT_JMPREL],
            tpnt->dynamic_info[DT_PLTRELSZ])
        {
          // ldso/mips/elfinterp.c from *patch-02*
          // add handlings of these rel type:
          //
          // #define R_MIPS_COPY        126
          // #define R_MIPS_JUMP_SLOT   127
          //
          // $ readelf -r 
          //
          // Relocation section '.rel.plt' at offset 0x4b0 contains 6 entries:
          //  Offset     Info    Type            Sym.Value  Sym. Name
          // 00401964  0000017f R_MIPS_JUMP_SLOT  00400570   __deregister_frame_inf
          // 00401968  0000047f R_MIPS_JUMP_SLOT  00400580   __register_frame_info
          // 0040196c  0000057f R_MIPS_JUMP_SLOT  00400590   __libc_start_main
          // 00401970  0000077f R_MIPS_JUMP_SLOT  004005a0   printf
          // 00401974  0000087f R_MIPS_JUMP_SLOT  004005b0   sleep
          // 00401978  0000097f R_MIPS_JUMP_SLOT  004005c0   puts
        }
      }

    /* Find the real malloc function and make ldso functions use that from now on */
    _dl_malloc_function = (void* (*)(size_t)) (intptr_t) _dl_find_hash("malloc",
        _dl_symbol_tables, NULL, ELF_RTYPE_CLASS_PLT);
    // ldso/ldso/dl-hash.c
    // _dl_find_hash("calloc", _dl_symbol_tables, NULL, ELF_RTYPE_CLASS_PLT);
    // {
    //   // uses ARCH_SKIP_RELOC from *patch-01*
    // }

  } // end of _dl_get_ready_to_run


  /* Transfer control to the application.  */
  _dl_elf_main = (int (*)(int, char **, char **)) auxvt[AT_ENTRY].a_un.a_val;
}


<spk-brcm-patch-analysis>

1. when DT_JMPREL exist, which means 

  #define DT_JMPREL	23		/* Address of PLT relocs */

  there is plt relocs. Then set GOT[2] = _dl_linux_resolve.

/* Initialization sequence for the application/library GOT.  */
#define INIT_GOT(GOT_BASE,MODULE)						\
do {										\
	/* Fill in first two GOT entries according to the ABI */		\
	GOT_BASE[0] = (unsigned long) _dl_runtime_resolve;			\
	GOT_BASE[1] = (unsigned long) MODULE;					\
	idx = 2;								\ ~
	if (MODULE->dynamic_info[DT_JMPREL])					\ ~
		GOT_BASE[idx++] = (unsigned long) _dl_linux_resolve;		\ ~
										\
} while (0)

_dl_linux_resolve(void) is added in ldso/ldso/mips/resolve.S

looks like to set and to call:

$4 = GOT[1]
$5 = index of reloc

+.globl	_dl_linux_resolve

+	# Setup functions args and `call _dl_linux_resolver`
+	lw	$4, -4($15)
+	subu	$5, $24, $14
+	srl	$5, 2
+	jal	_dl_linux_resolver
+	move	$25, $2


ldso/ldso/mips/elfinterp.c

+unsigned long
+_dl_linux_resolver(struct elf_resolve *tpnt, int reloc_entry)
+{
+	int reloc_type;
+	ELF_RELOC *this_reloc;
+	char *strtab;
+	ElfW(Sym) *symtab;
+	int symtab_index;
+	char *rel_addr;
+	char *new_addr;
+	char **got_addr;
+	unsigned long instr_addr;
+	char *symname;
+
+	rel_addr = (char *)tpnt->dynamic_info[DT_JMPREL];
+	this_reloc = (ELF_RELOC *)(rel_addr) + reloc_entry;
+	reloc_type = ELF_R_TYPE(this_reloc->r_info);
+	symtab_index = ELF_R_SYM(this_reloc->r_info);
+
+	reloc_type = ELF_R_TYPE(this_reloc->r_info);
+	symtab_index = ELF_R_SYM(this_reloc->r_info);
+
+	symtab = (ElfW(Sym) *)(intptr_t)tpnt->dynamic_info[DT_SYMTAB];
+	strtab = (char *)tpnt->dynamic_info[DT_STRTAB];
+	symname = strtab + symtab[symtab_index].st_name;
+
+	/* Address of the .got.plt entry to fix up. */
+	instr_addr = (unsigned long)this_reloc->r_offset;
+	got_addr = (char **)instr_addr;
+
+	/* Get the new value of the GOT entry. */
+	new_addr = _dl_find_hash(symname, tpnt->symbol_scope, tpnt, ELF_RTYPE_CLASS_PLT);
+	if (unlikely(!new_addr)) {
+		_dl_dprintf(2, "%s: can't resolve symbol '%s' in lib '%s'.\n", _dl_progname, symname, tpnt->libname);
+		_dl_exit(1);
+	}
+
+	*got_addr = new_addr;
+
+	return (unsigned long)new_addr;
+}


.globl	_dl_runtime_resolve

	# Setup functions args and call __dl_runtime_resolve
	move	$4, $24
	move	$5, $3
	jal	__dl_runtime_resolve

unsigned long __dl_runtime_resolve(unsigned long sym_index,
	unsigned long old_gpreg)
{
	unsigned long *got = (unsigned long *) (old_gpreg - OFFSET_GP_GOT);
	struct elf_resolve *tpnt = (struct elf_resolve *) got[1];  // note
	Elf32_Sym *sym;
	char *strtab;
	unsigned long local_gotno;
	unsigned long gotsym;
	unsigned long new_addr;
	unsigned long instr_addr;
	char **got_addr;
	char *symname;

	gotsym = tpnt->dynamic_info[DT_MIPS_GOTSYM_IDX];
	local_gotno = tpnt->dynamic_info[DT_MIPS_LOCAL_GOTNO_IDX];

	sym = ((Elf32_Sym *) tpnt->dynamic_info[DT_SYMTAB]) + sym_index;
	strtab = (char *) tpnt->dynamic_info[DT_STRTAB];
	symname = strtab + sym->st_name;

	new_addr = (unsigned long) _dl_find_hash(symname,
			tpnt->symbol_scope, tpnt, ELF_RTYPE_CLASS_PLT);
	if (unlikely(!new_addr)) {
		_dl_dprintf(2, "%s: can't resolve symbol '%s' in lib '%s'.\n",
			_dl_progname, symname, tpnt->libname);
		_dl_exit (1);
	}

	/* Address of jump instruction to fix up */
	instr_addr = (unsigned long) (got + local_gotno + sym_index - gotsym);
	got_addr = (char **) instr_addr;

	*got_addr = (char*)new_addr;

	return new_addr;
}


<brcm-patch>
uClibc-nptl-0.9.29-20070423-3003-plt-ldso.patch

	* include/elf.h (STO_MIPS_PLT, R_MIPS_COPY, R_MIPS_JUMP_SLOT): Define.
	(R_MIPS_NUM): Bump to 128.
	* ldso/include/dl-elf.h (ELF_ST_VISIBILITY): Define.
	* ldso/ldso/dl-hash.c (ARCH_SKIP_RELOC): New macro.
	(_dl_find_hash): Use it.
	* ldso/ldso/mips/dl-sysdep.h (ARCH_SKIP_RELOC): Define.
	(INIT_GOT): Treat index 2 as a pointer to _dl_linux_resolve
	if there's a DT_JMPREL tag.
	(elf_machine_type_class, DL_NO_COPY_RELOCS): Delete.
	* ldso/ldso/mips/elfinterp.c (_dl_linux_resolve): Declare.
	(_dl_linux_resolver): New function.
	(_dl_parse_relocation_information): Handle R_MIPS_JUMP_SLOT
	and R_MIPS_COPY.
	(_dl_perform_mips_global_got_relocations): Use a PLT lookup
	when replacing lazy binding stub entries with the real address.
	Don't use PLT lookups otherwise.
	* ldso/ldso/mips/resolve.S (_dl_runtime_resolve:): Allow the
	move to $25 to be in the delay slot of the return jump.
	(_dl_linux_resolve): New function.

<1>
Index: uClibc-nptl-0.9.29-20070423/ldso/include/dl-elf.h
===================================================================
--- uClibc-nptl-0.9.29-20070423.orig/ldso/include/dl-elf.h	2008-03-09 16:42:38.000000000 +0000
+++ uClibc-nptl-0.9.29-20070423/ldso/include/dl-elf.h	2008-03-09 16:42:41.000000000 +0000
@@ -51,6 +51,7 @@ extern void _dl_protect_relro (struct el
 # define ELF_ST_TYPE(val) ELF64_ST_TYPE(val)
 # define ELF_R_SYM(i)     ELF64_R_SYM(i)
 # define ELF_R_TYPE(i)    ELF64_R_TYPE(i)
+# define ELF_ST_VISIBILITY(i) ELF64_ST_VISIBILITY(i)
 # ifndef ELF_CLASS
 #  define ELF_CLASS ELFCLASS64
 # endif
@@ -59,6 +60,7 @@ extern void _dl_protect_relro (struct el
 # define ELF_ST_TYPE(val) ELF32_ST_TYPE(val)
 # define ELF_R_SYM(i)     ELF32_R_SYM(i)
 # define ELF_R_TYPE(i)    ELF32_R_TYPE(i)
+# define ELF_ST_VISIBILITY(i) ELF32_ST_VISIBILITY(i)
 # ifndef ELF_CLASS
 #  define ELF_CLASS ELFCLASS32
# endif

// used in ARCH_SKIP_RELOC
//
// 1. ldso/include/dl-elf is differnt from include/elf.h.
// 2. add macros to use st_info
//
// typedef struct
// {
//   ...
//   unsigned char	st_info;   /* Symbol type and binding */
//   unsigned char	st_other;  /* Symbol visibility */
//   Elf32_Section	st_shndx;  /* Section index */
// } Elf32_Sym;


<2> <patch-01>
Index: uClibc-nptl-0.9.29-20070423/ldso/ldso/mips/dl-sysdep.h
===================================================================
--- uClibc-nptl-0.9.29-20070423.orig/ldso/ldso/mips/dl-sysdep.h	2008-03-09 16:42:38.000000000 +0000
+++ uClibc-nptl-0.9.29-20070423/ldso/ldso/mips/dl-sysdep.h	2008-03-09 16:42:41.000000000 +0000
@@ -30,6 +30,12 @@ else if (dpnt->d_tag == DT_MIPS_RLD_MAP)
      *(Elf32_Addr *)(dpnt->d_un.d_ptr) =  (Elf32_Addr) debug_addr; \
 } while (0)
  
+#define ARCH_SKIP_RELOC(type_class, sym)				\
+	(((sym)->st_shndx == SHN_UNDEF)					\
+	 & ((type_class)						\
+	    | (((sym)->st_other & ~ELF_ST_VISIBILITY (-1))		\
+	       != STO_MIPS_PLT)))
+
 /* Initialization sequence for the application/library GOT.  */
 #define INIT_GOT(GOT_BASE,MODULE)						\
 do {										\
@@ -42,9 +48,11 @@ do {										\
 	/* Fill in first two GOT entries according to the ABI */		\
 	GOT_BASE[0] = (unsigned long) _dl_runtime_resolve;			\
 	GOT_BASE[1] = (unsigned long) MODULE;					\
+	idx = 2;								\
+	if (MODULE->dynamic_info[DT_JMPREL])					\
+		GOT_BASE[idx++] = (unsigned long) _dl_linux_resolve;		\
 										\
 	/* Add load address displacement to all local GOT entries */		\
-	idx = 2;									\
 	while (idx < MODULE->dynamic_info[DT_MIPS_LOCAL_GOTNO_IDX])		\
 		GOT_BASE[idx++] += (unsigned long) MODULE->loadaddr;		\
 										\
@@ -71,10 +79,6 @@ void _dl_perform_mips_global_got_relocat
 #define ADDR_ALIGN 0xfff
 #define OFFS_ALIGN 0x7ffff000
  
-#define elf_machine_type_class(type)		ELF_RTYPE_CLASS_PLT
-/* MIPS does not have COPY relocs */
-#define DL_NO_COPY_RELOCS
-
 #define OFFSET_GP_GOT 0x7ff0
  
 static inline ElfW(Addr) *

// to:
//
// /* Initialization sequence for the application/library GOT.  */
// #define INIT_GOT(GOT_BASE,MODULE)						\
// do {										\
// 	unsigned long idx;							\
// 										\
// 	/* Check if this is the dynamic linker itself */			\
// 	if (MODULE->libtype == program_interpreter)				\
// 		continue;							\
// 										\
// 	/* Fill in first two GOT entries according to the ABI */		\
// 	GOT_BASE[0] = (unsigned long) _dl_runtime_resolve;			\
// 	GOT_BASE[1] = (unsigned long) MODULE;					\
// 	idx = 2;								\ ~
// 	if (MODULE->dynamic_info[DT_JMPREL])					\ ~
// 		GOT_BASE[idx++] = (unsigned long) _dl_linux_resolve;		\ ~
// 										\
// 	/* Add load address displacement to all local GOT entries */		\
// 	while (idx < MODULE->dynamic_info[DT_MIPS_LOCAL_GOTNO_IDX])		\
// 		GOT_BASE[idx++] += (unsigned long) MODULE->loadaddr;		\
// 										\
// } while (0)
//
// /* Special section indices.  */
//
// #define SHN_UNDEF	0		/* Undefined section */
// #define SHN_LORESERVE	0xff00		/* Start of reserved indices */
// ...
// if ARCH_SKIP_RELOC returns true then skip this reloc.
//
// /* MIPS specific values for `st_other'.  */
// #define STO_MIPS_DEFAULT		0x0
// #define STO_MIPS_INTERNAL		0x1
// #define STO_MIPS_HIDDEN			0x2
// #define STO_MIPS_PROTECTED		0x3
// #define STO_MIPS_PLT			0x10
// #define STO_MIPS_SC_ALIGN_UNUSED	0xff
//
// #define ELF32_ST_VISIBILITY(o)	((o) & 0x03)
// ((sym)->st_other & ~ELF_ST_VISIBILITY (-1))
//
// so test 3 bits. return 1 for values greater than 0x3 and return 0 for less
// values.
//
// ((sym)->st_other & b1111000)
//
// looks like to skip others other then PLT


<3>
Index: uClibc-nptl-0.9.29-20070423/include/elf.h
===================================================================
--- uClibc-nptl-0.9.29-20070423.orig/include/elf.h	2008-03-09 16:42:38.000000000 +0000
+++ uClibc-nptl-0.9.29-20070423/include/elf.h	2008-03-09 16:42:41.000000000 +0000
@@ -1539,6 +1539,7 @@ typedef struct
 #define STO_MIPS_INTERNAL		0x1
 #define STO_MIPS_HIDDEN			0x2
 #define STO_MIPS_PROTECTED		0x3
+#define STO_MIPS_PLT			0x10
 #define STO_MIPS_SC_ALIGN_UNUSED	0xff
  
 /* MIPS specific values for `st_info'.  */
@@ -1684,8 +1685,9 @@ typedef struct
 #define R_MIPS_TLS_TPREL64	48	/* TP-relative offset, 64 bit */
 #define R_MIPS_TLS_TPREL_HI16	49	/* TP-relative offset, high 16 bits */
 #define R_MIPS_TLS_TPREL_LO16	50	/* TP-relative offset, low 16 bits */
-/* Keep this the last entry.  */
-#define R_MIPS_NUM		51
+#define R_MIPS_COPY		126
+#define R_MIPS_JUMP_SLOT	127
+#define R_MIPS_NUM		128
  
 /* Legal values for p_type field of Elf32_Phdr.  */
 

<4>
Index: uClibc-nptl-0.9.29-20070423/ldso/ldso/dl-hash.c
===================================================================
--- uClibc-nptl-0.9.29-20070423.orig/ldso/ldso/dl-hash.c	2008-03-09 16:42:38.000000000 +0000
+++ uClibc-nptl-0.9.29-20070423/ldso/ldso/dl-hash.c	2008-03-09 16:42:41.000000000 +0000
@@ -30,6 +30,11 @@
  * SUCH DAMAGE.
  */
  
+#ifndef ARCH_SKIP_RELOC
+#define ARCH_SKIP_RELOC(type_class, sym) \
+	((type_class) & ((sym)->st_shndx == SHN_UNDEF))
+#endif
+
  
 /* Various symbol table handling functions, including symbol lookup */
  
@@ -189,7 +194,7 @@ char *_dl_find_hash(const char *name, st
 				&& ELF_ST_TYPE(sym->st_info) != STT_TLS
 #endif
 				)
-				|| (type_class & (sym->st_shndx == SHN_UNDEF)))
+				|| ARCH_SKIP_RELOC (type_class, sym))
 				continue;
  
 			if (ELF_ST_TYPE(sym->st_info) > STT_FUNC
@@ -283,7 +288,7 @@ char *_dl_find_hash2(const char *name, s
 				&& ELF_ST_TYPE(sym->st_info) != STT_TLS
 #endif
 				)
-				|| (type_class & (sym->st_shndx == SHN_UNDEF)))
+				|| ARCH_SKIP_RELOC (type_class, sym))
 				continue;
  
 			if (ELF_ST_TYPE(sym->st_info) > STT_FUNC

note: changes in elf.h and used in ARCH_SKIP_RELOC
1. used in _dl_find_hash()
2. use ARCH_SKIP_RELOC is defined in dl-sysdep.h

// from c file:
//
//        if ((sym->st_value == 0
// #if USE_TLS
//          && ELF_ST_TYPE(sym->st_info) != STT_TLS
// #endif
//          )
//          || ARCH_SKIP_RELOC (type_class, sym))
//          continue;
//
// from processed:
//
//    if ((sym->st_value == 0
// 
//     && ((sym->st_info) & 0xf) != 6
// 
//     )
//     || (((sym)->st_shndx == 0) & ((type_class) | (((sym)->st_other & ~((-1) & 0x03)) != 0x10))))
//     continue;
//     
// so ~((-1) & 0x03) is "11111111111111111100" and clears out 2 bits. want to
// see only STO_MIPS_PLT(0x10).


<5> <patch-02>
// _dl_linux_resolve uses _dl_linux_resolver()
Index: uClibc-nptl-0.9.29-20070423/ldso/ldso/mips/elfinterp.c
===================================================================
--- uClibc-nptl-0.9.29-20070423.orig/ldso/ldso/mips/elfinterp.c	2008-03-09 16:42:38.000000000 +0000
+++ uClibc-nptl-0.9.29-20070423/ldso/ldso/mips/elfinterp.c	2008-03-09 16:42:41.000000000 +0000
@@ -30,6 +30,7 @@
 #include "ldso.h"
 
 extern int _dl_runtime_resolve(void);
+extern int _dl_linux_resolve(void);
 
 #define OFFSET_GP_GOT 0x7ff0
 
@@ -83,6 +84,52 @@ unsigned long __dl_runtime_resolve(unsig
 	return new_addr;
 }
 
+unsigned long
+_dl_linux_resolver(struct elf_resolve *tpnt, int reloc_entry)
+{
+	int reloc_type;
+	ELF_RELOC *this_reloc;
+	char *strtab;
+	ElfW(Sym) *symtab;
+	int symtab_index;
+	char *rel_addr;
+	char *new_addr;
+	char **got_addr;
+	unsigned long instr_addr;
+	char *symname;
+
+	rel_addr = (char *)tpnt->dynamic_info[DT_JMPREL];
+	this_reloc = (ELF_RELOC *)(rel_addr) + reloc_entry;
+	reloc_type = ELF_R_TYPE(this_reloc->r_info);
+	symtab_index = ELF_R_SYM(this_reloc->r_info);
+
+	reloc_type = ELF_R_TYPE(this_reloc->r_info);
+	symtab_index = ELF_R_SYM(this_reloc->r_info);
+
+	symtab = (ElfW(Sym) *)(intptr_t)tpnt->dynamic_info[DT_SYMTAB];
+	strtab = (char *)tpnt->dynamic_info[DT_STRTAB];
+	symname = strtab + symtab[symtab_index].st_name;
+
+	/* Address of the .got.plt entry to fix up. */
+	instr_addr = (unsigned long)this_reloc->r_offset;
+	got_addr = (char **)instr_addr;
+
+	/* Get the new value of the GOT entry. */
+	new_addr = _dl_find_hash(symname, tpnt->symbol_scope, tpnt, ELF_RTYPE_CLASS_PLT);
+	if (unlikely(!new_addr)) {
+		_dl_dprintf(2, "%s: can't resolve symbol '%s' in lib '%s'.\n", _dl_progname, symname, tpnt->libname);
+		_dl_exit(1);
+	}
+
+#if defined (__SUPPORT_LD_DEBUG__)
+#error "not implemented"
+#else
+	*got_addr = new_addr;
+#endif
+
+	return (unsigned long)new_addr;
+}
+
 void _dl_parse_lazy_relocation_information(struct dyn_elf *rpnt,
 	unsigned long rel_addr, unsigned long rel_size)
 {
@@ -119,7 +166,6 @@ int `_dl_parse_relocation_information`(str
 			(unsigned long) rpnt->r_offset);
 		reloc_type = ELF32_R_TYPE(rpnt->r_info);
 		symtab_index = ELF32_R_SYM(rpnt->r_info);
-		symbol_addr = 0;
 
 		debug_sym(symtab,strtab,symtab_index);
 		debug_reloc(symtab,strtab,rpnt);
@@ -200,6 +246,35 @@ _dl_dprintf(2, "TLS_TPREL  : %s, %x, %x\
 			break;
 		case R_MIPS_NONE:
 			break;
+		case R_MIPS_JUMP_SLOT:
+			if (symtab_index) {
+				*reloc_addr = (unsigned long)_dl_find_hash(
+					strtab + symtab[symtab_index].st_name,
+					xpnt->dyn->symbol_scope, tpnt,
+					ELF_RTYPE_CLASS_PLT);
+			} else {
+				*reloc_addr = 0;
+			}
+			break;
+		case R_MIPS_COPY:
+			symbol_addr = (unsigned long)_dl_find_hash(
+					strtab + symtab[symtab_index].st_name,
+					xpnt->dyn->symbol_scope, tpnt,
+					ELF_RTYPE_CLASS_COPY);
+			if (symbol_addr) {
+#if defined (__SUPPORT_LD_DEBUG__)
+				if (_dl_debug_move)
+					_dl_dprintf(_dl_debug_file,
+						    "\n%s move %d bytes from %x to %x",
+						    strtab + symtab[symtab_index].st_name,
+						    symtab[symtab_index].st_size,
+						    symbol_addr, reloc_addr);
+#endif
+				_dl_memcpy((char *)reloc_addr,
+					   (char *)symbol_addr,
+					   symtab[symtab_index].st_size);
+			}
+			break;
 		default:
 			{
 				_dl_dprintf(2, "\n%s: ",_dl_progname);
@@ -254,21 +329,31 @@ void `_dl_perform_mips_global_got_relocat`
 		/* Relocate the global GOT entries for the object */
 		while (i--) {
 			if (sym->st_shndx == SHN_UNDEF) {
-				if (ELF32_ST_TYPE(sym->st_info) == STT_FUNC && sym->st_value && tmp_lazy) {
-					*got_entry = sym->st_value + (unsigned long) tpnt->loadaddr;
+				if (ELF32_ST_TYPE(sym->st_info) == STT_FUNC && sym->st_value) {
+					if (tmp_lazy)
+						*got_entry = sym->st_value + (unsigned long) tpnt->loadaddr;
+					else
+						*got_entry = (unsigned long) _dl_find_hash(strtab +
+							sym->st_name, tpnt->symbol_scope, tpnt,
+							ELF_RTYPE_CLASS_PLT);
 				}
 				else {
 					*got_entry = (unsigned long) _dl_find_hash(strtab +
-						sym->st_name, tpnt->symbol_scope, tpnt, ELF_RTYPE_CLASS_PLT);
+						sym->st_name, tpnt->symbol_scope, tpnt, 0);
 				}
 			}
 			else if (sym->st_shndx == SHN_COMMON) {
 				*got_entry = (unsigned long) _dl_find_hash(strtab +
-					sym->st_name, tpnt->symbol_scope, tpnt, ELF_RTYPE_CLASS_PLT);
+					sym->st_name, tpnt->symbol_scope, tpnt, 0);
 			}
 			else if (ELF32_ST_TYPE(sym->st_info) == STT_FUNC &&
-				*got_entry != sym->st_value && tmp_lazy) {
-				*got_entry += (unsigned long) tpnt->loadaddr;
+				*got_entry != sym->st_value) {
+				if (tmp_lazy)
+					*got_entry += (unsigned long) tpnt->loadaddr;
+				else
+					*got_entry = (unsigned long) _dl_find_hash(strtab +
+						sym->st_name, tpnt->symbol_scope, tpnt,
+						ELF_RTYPE_CLASS_PLT);
 			}
 			else if (ELF32_ST_TYPE(sym->st_info) == STT_SECTION) {
 				if (sym->st_other == 0)
@@ -276,7 +361,7 @@ void _dl_perform_mips_global_got_relocat
 			}
 			else {
 				*got_entry = (unsigned long) _dl_find_hash(strtab +
-					sym->st_name, tpnt->symbol_scope, tpnt, ELF_RTYPE_CLASS_PLT);
+					sym->st_name, tpnt->symbol_scope, tpnt, 0);
 			}
 
 			got_entry++;

<6>
Index: uClibc-nptl-0.9.29-20070423/ldso/ldso/mips/resolve.S
===================================================================
--- uClibc-nptl-0.9.29-20070423.orig/ldso/ldso/mips/resolve.S	2008-03-09 16:42:38.000000000 +0000
+++ uClibc-nptl-0.9.29-20070423/ldso/ldso/mips/resolve.S	2008-03-17 20:22:30.000000000 +0000
@@ -61,7 +61,61 @@ _dl_runtime_resolve:
 	# Do a tail call to the original function
 	addiu	$29, 40
 	move	$25, $2
-	jr	$25
+	jr	$2
 .end	_dl_runtime_resolve
+
+	# On entry:
+	#
+	#	$14 points to the beginning of the .got.plt section
+	#	$15 points to _GLOBAL_OFFSET_TABLE_ + 8
+	#	$24 points to the function's .got.plt entry
+.align	2
+.globl	_dl_linux_resolve
+.type	_dl_linux_resolve,@function
+.ent	_dl_linux_resolve
+_dl_linux_resolve:
+	.frame	$29, 48, $31
+	.set noreorder
+
+	# Compute GP.
+	.set noreorder
+	.cpload $25
+	.set reorder
+
+	addiu	$29, -48
+
+	# Save slot call pc.
+	.cprestore 40
+
+	# Save the function arguments.  Also save $2 and $3, which are used
+	# as inputs to __mips16_ret_* functions.
+	sw	$2, 16($29)
+	sw	$3, 20($29)
+	sw	$4, 24($29)
+	sw	$5, 28($29)
+	sw	$6, 32($29)
+	sw	$7, 36($29)
+	sw	$31, 44($29)
+
+	# Setup functions args and `call _dl_linux_resolver`
+	lw	$4, -4($15)
+	subu	$5, $24, $14
+	srl	$5, 2
+	jal	_dl_linux_resolver
+	move	$25, $2
+
+	# Restore the saved registers.
+	lw	$2, 16($29)
+	lw	$3, 20($29)
+	lw	$4, 24($29)
+	lw	$5, 28($29)
+	lw	$6, 32($29)
+	lw	$7, 36($29)
+	lw	$31, 44($29)
+
+	# Do a tail call to the original function.
+	addiu	$29, 48
+	jr	$25
+.end	_dl_linux_resolve
 .previous
 

uClibc-nptl-0.9.29-20070423-brcm-dl_calloc.patch:diff -urN uClibc-nptl-0.9.29-20070423/ldso/ldso/dl-tls.c uClibc-nptl-0.9.29-20070423-patched/ldso/ldso/dl-tls.c
uClibc-nptl-0.9.29-20070423-brcm-static-link.patch:diff -ru uClibc-nptl-0.9.29-20070423.rpm/ldso/ldso/dl-elf.c uClibc-nptl-0.9.29-20070423/ldso/ldso/dl-elf.c


<from-uclibc-git>
7f07b8deffa7eaea0cbab9e84503b7644a6b6f8e
7f07b8de 188 (Bernhard Reutner-Fischer 2008-12-22 09:58:25 +0000 189) 		if (reloc_type == R_MIPS_JUMP_SLOT || reloc_type == R_MIPS_COPY) {

commit 7f07b8deffa7eaea0cbab9e84503b7644a6b6f8e
Author: Bernhard Reutner-Fischer <rep.dot.nop@gmail.com>
Date:   Mon Dec 22 09:58:25 2008 +0000

    - non-pic support for MIPS (Catherine Moore, clm at codesourcery com)

diff --git a/include/elf.h b/include/elf.h
index 4c6d090..9f00970 100644
--- a/include/elf.h
+++ b/include/elf.h
@@ -1544,6 +1544,7 @@ typedef struct
 #define STO_MIPS_INTERNAL		0x1
 #define STO_MIPS_HIDDEN			0x2
 #define STO_MIPS_PROTECTED		0x3
+#define STO_MIPS_PLT			0x8
 #define STO_MIPS_SC_ALIGN_UNUSED	0xff
 
 /* MIPS specific values for `st_info'.  */
@@ -1689,8 +1690,11 @@ typedef struct
 #define R_MIPS_TLS_TPREL64	48	/* TP-relative offset, 64 bit */
 #define R_MIPS_TLS_TPREL_HI16	49	/* TP-relative offset, high 16 bits */
 #define R_MIPS_TLS_TPREL_LO16	50	/* TP-relative offset, low 16 bits */
+#define R_MIPS_GLOB_DAT		51
+#define R_MIPS_COPY		126
+#define R_MIPS_JUMP_SLOT        127
 /* Keep this the last entry.  */
-#define R_MIPS_NUM		51
+#define R_MIPS_NUM		128
 
 /* Legal values for p_type field of Elf32_Phdr.  */
 
@@ -1756,7 +1760,13 @@ typedef struct
 #define DT_MIPS_COMPACT_SIZE 0x7000002f /* (O32)Size of compact rel section. */
 #define DT_MIPS_GP_VALUE     0x70000030 /* GP value for aux GOTs.  */
 #define DT_MIPS_AUX_DYNAMIC  0x70000031 /* Address of aux .dynamic.  */
-#define DT_MIPS_NUM	     0x32
+/* The address of .got.plt in an executable using the new non-PIC ABI.  */
+#define DT_MIPS_PLTGOT	     0x70000032
+/* The base of the PLT in an executable using the new non-PIC ABI if that
+   PLT is writable.  For a non-writable PLT, this is omitted or has a zero
+   value.  */
+#define DT_MIPS_RWPLT        0x70000034
+#define DT_MIPS_NUM	     0x35
 
 /* Legal values for DT_MIPS_FLAGS Elf32_Dyn entry.  */
 
diff --git a/ldso/ldso/dl-hash.c b/ldso/ldso/dl-hash.c
index b44bd3a..a251aaf 100644
--- a/ldso/ldso/dl-hash.c
+++ b/ldso/ldso/dl-hash.c
@@ -160,6 +160,11 @@ check_match (const ElfW(Sym) *sym, char *strtab, const char* undef_name, int typ
 		/* undefined symbol itself */
 		return NULL;
 
+#ifdef __mips__
+    if (sym->st_shndx == SHN_UNDEF && !(sym->st_other & STO_MIPS_PLT))
+        return NULL;
+#endif
+
 	if (sym->st_value == 0)
 		/* No value */
 		return NULL;
diff --git a/ldso/ldso/mips/dl-sysdep.h b/ldso/ldso/mips/dl-sysdep.h
index 7287792..8f303fb 100644
--- a/ldso/ldso/mips/dl-sysdep.h
+++ b/ldso/ldso/mips/dl-sysdep.h
@@ -93,10 +93,11 @@ typedef struct
 
 #include <link.h>
 
-#define ARCH_NUM 3
+#define ARCH_NUM 4
 #define DT_MIPS_GOTSYM_IDX	(DT_NUM + OS_NUM)
 #define DT_MIPS_LOCAL_GOTNO_IDX	(DT_NUM + OS_NUM +1)
 #define DT_MIPS_SYMTABNO_IDX	(DT_NUM + OS_NUM +2)
+#define DT_MIPS_PLTGOT_IDX	(DT_NUM + OS_NUM +3)
 
 #define ARCH_DYNAMIC_INFO(dpnt,  dynamic, debug_addr) \
 do { \
@@ -106,6 +107,8 @@ else if (dpnt->d_tag == DT_MIPS_LOCAL_GOTNO) \
      dynamic[DT_MIPS_LOCAL_GOTNO_IDX] = dpnt->d_un.d_val; \
 else if (dpnt->d_tag == DT_MIPS_SYMTABNO) \
      dynamic[DT_MIPS_SYMTABNO_IDX] = dpnt->d_un.d_val; \
+else if (dpnt->d_tag == DT_MIPS_PLTGOT) \
+     dynamic[DT_MIPS_PLTGOT_IDX] = dpnt->d_un.d_val; \
 else if (dpnt->d_tag == DT_MIPS_RLD_MAP) \
      *(ElfW(Addr) *)(dpnt->d_un.d_ptr) =  (ElfW(Addr)) debug_addr; \
 } while (0)
@@ -114,6 +117,7 @@ else if (dpnt->d_tag == DT_MIPS_RLD_MAP) \
 #define INIT_GOT(GOT_BASE,MODULE)						\
 do {										\
 	unsigned long idx;							\
+	unsigned long *pltgot;							\
 										\
 	/* Check if this is the dynamic linker itself */			\
 	if (MODULE->libtype == program_interpreter)				\
@@ -123,6 +127,12 @@ do {										\
 	GOT_BASE[0] = (unsigned long) _dl_runtime_resolve;			\
 	GOT_BASE[1] = (unsigned long) MODULE;					\
 										\
+	pltgot = MODULE->dynamic_info[DT_MIPS_PLTGOT_IDX];			\
+	if (pltgot) {								\
+		pltgot[0] = (unsigned long) _dl_runtime_pltresolve;		\
+		pltgot[1] = (unsigned long) MODULE;				\
+	}									\
+										\
 	/* Add load address displacement to all local GOT entries */		\
 	idx = 2;									\
 	while (idx < MODULE->dynamic_info[DT_MIPS_LOCAL_GOTNO_IDX])		\
@@ -151,9 +161,9 @@ void _dl_perform_mips_global_got_relocations(struct elf_resolve *tpnt, int lazy)
 #define OFFS_ALIGN (0x10000000000UL-0x1000)
 #endif	/* O32 || N32 */
 
-#define elf_machine_type_class(type)		ELF_RTYPE_CLASS_PLT
-/* MIPS does not have COPY relocs */
-#define DL_NO_COPY_RELOCS
+#define elf_machine_type_class(type) \
+  ((((type) == R_MIPS_JUMP_SLOT) * ELF_RTYPE_CLASS_PLT)	\
+   | (((type) == R_MIPS_COPY) * ELF_RTYPE_CLASS_COPY))
 
 #define OFFSET_GP_GOT 0x7ff0
 
diff --git a/ldso/ldso/mips/elfinterp.c b/ldso/ldso/mips/elfinterp.c
index 1b03d94..0ed2757 100644
--- a/ldso/ldso/mips/elfinterp.c
+++ b/ldso/ldso/mips/elfinterp.c
@@ -30,6 +30,7 @@
 #include "ldso.h"
 
 extern int _dl_runtime_resolve(void);
+extern int _dl_runtime_pltresolve(void);
 
 #define OFFSET_GP_GOT 0x7ff0
 
@@ -83,6 +84,61 @@ unsigned long __dl_runtime_resolve(unsigned long sym_index,
 	return new_addr;
 }
 
+unsigned long
+__dl_runtime_pltresolve(struct elf_resolve *tpnt, int reloc_entry)
+{
+	int reloc_type;
+	ELF_RELOC *this_reloc;
+	char *strtab;
+	Elf32_Sym *symtab;
+	int symtab_index;
+	char *rel_addr;
+	char *new_addr;
+	char **got_addr;
+	unsigned long instr_addr;
+	char *symname;
+
+	rel_addr = (char *)tpnt->dynamic_info[DT_JMPREL];
+	this_reloc = (ELF_RELOC *)(intptr_t)(rel_addr + reloc_entry);
+	reloc_type = ELF32_R_TYPE(this_reloc->r_info);
+	symtab_index = ELF32_R_SYM(this_reloc->r_info);
+
+	symtab = (Elf32_Sym *)(intptr_t)tpnt->dynamic_info[DT_SYMTAB];
+	strtab = (char *)tpnt->dynamic_info[DT_STRTAB];
+	symname = strtab + symtab[symtab_index].st_name;
+
+	/* Address of the jump instruction to fix up. */
+	instr_addr = ((unsigned long)this_reloc->r_offset +
+		      (unsigned long)tpnt->loadaddr);
+	got_addr = (char **)instr_addr;
+
+	/* Get the address of the GOT entry. */
+	new_addr = _dl_find_hash(symname, tpnt->symbol_scope, tpnt, ELF_RTYPE_CLASS_PLT);
+	if (unlikely(!new_addr)) {
+		_dl_dprintf(2, "%s: can't resolve symbol '%s' in lib '%s'.\n", _dl_progname, symname, tpnt->libname);
+		_dl_exit(1);
+	}
+
+#if defined (__SUPPORT_LD_DEBUG__)
+	if ((unsigned long)got_addr < 0x40000000) {
+		if (_dl_debug_bindings) {
+			_dl_dprintf(_dl_debug_file, "\nresolve function: %s", symname);
+			if (_dl_debug_detail)
+				_dl_dprintf(_dl_debug_file,
+				            "\n\tpatched: %x ==> %x @ %x",
+				            *got_addr, new_addr, got_addr);
+		}
+	}
+	if (!_dl_debug_nofixups) {
+		*got_addr = new_addr;
+	}
+#else
+	*got_addr = new_addr;
+#endif
+
+	return (unsigned long)new_addr;
+}
+
 void _dl_parse_lazy_relocation_information(struct dyn_elf *rpnt,
 	unsigned long rel_addr, unsigned long rel_size)
 {
@@ -115,6 +171,7 @@ int _dl_parse_relocation_information(struct dyn_elf *xpnt,
 	got = (unsigned long *) tpnt->dynamic_info[DT_PLTGOT];
 
 	for (i = 0; i < rel_size; i++, rpnt++) {
+		char *symname = NULL;
 		reloc_addr = (unsigned long *) (tpnt->loadaddr +
 			(unsigned long) rpnt->r_offset);
 		reloc_type = ELF_R_TYPE(rpnt->r_info);
@@ -128,6 +185,16 @@ int _dl_parse_relocation_information(struct dyn_elf *xpnt,
 			old_val = *reloc_addr;
 #endif
 
+		if (reloc_type == R_MIPS_JUMP_SLOT || reloc_type == R_MIPS_COPY) {
+			symname = strtab + symtab[symtab_index].st_name;
+			symbol_addr = (unsigned long)_dl_find_hash(symname,
+								   tpnt->symbol_scope,
+								   tpnt,
+								   elf_machine_type_class(reloc_type));
+			if (unlikely(!symbol_addr && ELF32_ST_BIND(symtab[symtab_index].st_info) != STB_WEAK))
+				return 1;
+		}
+
 		switch (reloc_type) {
 #if _MIPS_SIM == _MIPS_SIM_ABI64
 		case (R_MIPS_64 << 8) | R_MIPS_REL32:
@@ -148,6 +215,24 @@ int _dl_parse_relocation_information(struct dyn_elf *xpnt,
 				*reloc_addr += (unsigned long) tpnt->loadaddr;
 			}
 			break;
+		case R_MIPS_JUMP_SLOT:
+			*reloc_addr = symbol_addr;
+			break;
+		case R_MIPS_COPY:
+			if (symbol_addr) {
+#if defined (__SUPPORT_LD_DEBUG__)
+				if (_dl_debug_move)
+					_dl_dprintf(_dl_debug_file,
+						    "\n%s move %d bytes from %x to %x",
+						    symname, symtab[symtab_index].st_size,
+						    symbol_addr, reloc_addr);
+#endif
+
+				_dl_memcpy((char *)reloc_addr,
+					   (char *)symbol_addr,
+					   symtab[symtab_index].st_size);
+			}
+			break;
 		case R_MIPS_NONE:
 			break;
 		default:
diff --git a/ldso/ldso/mips/resolve.S b/ldso/ldso/mips/resolve.S
index f5d988a..d7951a1 100644
--- a/ldso/ldso/mips/resolve.S
+++ b/ldso/ldso/mips/resolve.S
@@ -112,3 +112,54 @@ _dl_runtime_resolve:
 .end	_dl_runtime_resolve
 .previous
 
+/* Assembler veneer called from the PLT header code when using the
+   non-PIC ABI.
+
+   Code in each PLT entry puts the caller's return address into t7 ($15),
+   the PLT entry index into t8 ($24), the address of _dl_runtime_pltresolve
+   into t9 ($25) and the address of .got.plt into gp ($28).  __dl_runtime_pltresolve
+   needs a0 ($4) to hold the link map and a1 ($5) to hold the index into
+   .rel.plt (== PLT entry index * 4).  */
+
+	.text
+	.align	2
+	.globl	_dl_runtime_pltresolve
+	.type	_dl_runtime_pltresolve,@function
+	.ent	_dl_runtime_pltresolve
+_dl_runtime_pltresolve:
+	.frame	$29, 40, $31
+	.set noreorder
+	# Save arguments and sp value in stack.
+	subu    $29, 40
+	lw      $10, 4($28)
+	# Modify t9 ($25) so as to point .cpload instruction.
+	addiu   $25, 12
+	# Compute GP.
+	.cpload $25
+	.set reorder
+
+	/* Store function arguments from registers to stack */
+	sw	$15, 36($29)
+	sw	$4, 16($29)
+	sw	$5, 20($29)
+	sw	$6, 24($29)
+	sw	$7, 28($29)
+
+	/* Setup functions args and call __dl_runtime_pltresolve.  */
+	move	$4, $10
+	sll     $5, $24, 3
+	jal	__dl_runtime_pltresolve
+
+	/* Restore function arguments from stack to registers */
+	lw	$31, 36($29)
+	lw	$4, 16($29)
+	lw	$5, 20($29)
+	lw	$6, 24($29)
+	lw	$7, 28($29)
+
+	/* Do a tail call to the original function */
+	addiu	$29, 40
+	move	$25, $2
+	jr	$25
+	.end	_dl_runtime_pltresolve
+	.previous


={============================================================================
*kt_linux_core_400* linux-lib-ld-uclibc-fail-to-find-symbol

Q: HOW to check if linker finds a symbols? 

<1>
Another unfortunate consequence of this situation, together with poor
implementation of uClibc dynamic linker [2], is that some libraries fail to
load. This can happen in a scenario when a C program loads a C library which
loads another C library that depends on some C++ libraries. uClibc dynamic
linker in some of such cases doesn't resolve symbols in the right order,
especially for global C++ objects with constructors!

[2] It's been proved many times that uClibc dynamic linker implementation
available on devices is broken as it doesn't resolve symbols properly when their
dependencies are complex enough. Possibly upstream implementation works better
but the one available from Broadcom is a mix of a very old version and some
random backported patches.

This libgstcencdec.so is a C library which depends on another C library:
libdrm.so. The latter one depends on several C++ libraries, with one being
SystemAPI.so. Because SystemAPI.so now depends on dbus-c++-1, the dynamic linker
tries to load it. But due some unfortunate library and symbol ordering, it gets
it wrong which results in a SIGSEGV while scanning for plug-ins:

Program received signal SIGSEGV, Segmentation fault.

[Switching to Thread 0x77ff4000 (LWP 1548)]
0x00000000 in ?? ()
(gdb) bt
#0 0x00000000 in ?? ()
#1 0x7661d180 in global constructors keyed to eventloop_integration.cpp () 
  from /opt/zinc-trunk/lib/libdbus-c++-1.so.0

#2 0x7661d558 in __do_global_ctors_aux () from 
  /opt/zinc-trunk/lib/libdbus-c++-1.so.0


<2>
During development, works okay but suddenly failed to start during code
management.

After all, found that there is one function which thought was deleted but was in
the code to be called. However, there is no function definition since it was
removed but only calls to that function remained.

How could that possible without linking error? Since it was so file and run nm:

U        is undefined meaning extern

This was undefined and supposed to be linked when loaded. So when run
application, failed to find symbol and crashes. 

root        1.1M Sep 18 15:16 core.gst-plugin-scan.8785.HUMAX.1442585817
root        1.1M Sep 18 15:16 core.gst-plugin-scan.8786.HUMAX.1442585817
root      171.9M Sep 18 15:17 core.multiqueue1:src.8775.HUMAX.1442585819


<3>
http://stackoverflow.com/questions/1617286/easy-check-for-unresolved-symbols-in-shared-libraries

Q: To check if there are undefined symbol at compile time than dynamic loading
time?

A: (not checked yet)

Check out the linker option -z defs / --no-undefined. When creating a shared
object, it will cause the link to fail if there are unresolved symbols.

If you are using gcc to invoke the linker, you'll use the compiler -Wl option to
pass the option to the linker:

gcc -shared ... -Wl,-z,defs

As an example, consider the following file:

#include <stdio.h>

void forgot_to_define(FILE *fp);

void doit(const char *filename)
{
    FILE *fp = fopen(filename, "r");
    if (fp != NULL)
    {
        forgot_to_define(fp);
        fclose(fp);
    }
}

Now, if you build that into a shared object, it will succeed:

> gcc -shared -fPIC -o libsilly.so silly.c && echo succeeded || echo failed
succeeded

But if you add -z defs, the link will fail and tell you about your missing symbol:

> 
gcc -shared -fPIC -o libsilly.so silly.c -Wl,-z,defs && echo succeeded || echo failed
/tmp/cccIwwbn.o: In function `doit':
silly.c:(.text+0x2c): undefined reference to `forgot_to_define'
collect2: ld returned 1 exit status
failed


={============================================================================
*kt_linux_core_412* linux-lib-ld-search 
  
{ldconfig} ld-conf ld-so-cache

The ldconfig(8) program addresses two potential problems with shared libraries:

  Shared libraries can reside in a variety of directories. If the dynamic
  linker needed to search all of these directories in order to find a library,
  then loading libraries could be very slow.

  As new versions of libraries are installed or old versions are removed, the
  soname symbolic links may become out of date.

The ldconfig program solves these problems by performing two tasks:

1. It searches a standard set of directories and 

creates or `updates a cache file`, `/etc/ld.so.cache`, text file

to contain a list of the (latest minor versions of each of the) major library
  versions in all of these directories. The dynamic linker in turn uses this
  cache file when resolving library names at run time. 
  
To build the cache, ldconfig searches the directories specified in the file
`/etc/ld.so.conf` and then /lib and /usr/lib. The /etc/ld.so.conf file
consists of a list of directory pathnames (these should be specified as
    absolute pathnames), separated by newlines, spaces, tabs, commas, or
colons. In some distributions, the directory /usr/local/lib is included in
this list. (If not, we may need to add it manually.)

The command ldconfig p displays the current contents of /etc/ld.so.cache.

2. It examines the latest minor version (i.e., the version with the highest
    minor number) of each major version of each library to find the embedded
soname and then creates (or updates) relative symbolic links for each soname
in the same directory.


<ex>
/etc/ld.so.cache is required to run D-BUS daemon as non-root. This is due to
the fact that LD_LIBRARY_PATH is ignored when it calls setuid
yv-daemon-sandbox (this is intended behaviour). The dynamic linker does not
resolve shared object dependencies properly in this situation. Using RPATH
could help with that but it is generally not recommended and it is broken in
uClibc.

In order to create ld.so.cache you may create /etc/ld.so.conf with the
following content:

/opt/zinc/oss/lib
/opt/zinc/lib
/usr/local/lib
/opt/stagecraft-2.0/bin

and run ldconfig. You can get ldconfig as part of uClibc build or download
MIPSEL rootfs from the uClibc site and extract it from there. Note that you
should generate /etc/ld.so.cache with every build as the list of the shared
libraries may differ.


{ld-library-path}

41.10 Specifying Library Search Directories in an Object File

standard library directories (/lib, /usr/lib, or one of the directories listed
in /etc/ld.so.conf).

Some of these rules specify a set of `standard directories` in which shared
libraries normally reside. To inform the dynamic linker that a shared library
resides in a nonstandard directory

If LD_LIBRARY_PATH is defined, then the dynamic linker searches for the shared
library in the directories `before looking in the standard library directories.`

*sh-env-export* 
Creates an environment variable definition 'within' the process executing
prog. Since if you don't export the changes to an environment variable, they
won't be inherited by the child processes. The loader and our test program
didn't inherit the changes we made.

$ LD_LIBRARY_PATH=. ./one 
foo: this is foo...

$ ./one 
./one: error while loading shared libraries: libfoo.so: cannot open shared 
  object file: No such file or directory

$ export LD_LIBRARY_PATH=.
$ ./one 
foo: this is foo...


LD_LIBRARY_PATH is broken and should 'not' be used if at all possible since
LD_LIBRARY_PATH is great for quick tests and for systems on which you don't have
admin privileges. However, as a downside, exporting the LD_LIBRARY_PATH variable
means it may cause problems with other programs if you don't reset it to its
previous state when you're done.

<ex>
$ export LD_LIBRARY_PATH=$HOME/install/fftw/lib
$ ldd libfftw3_mpi.so |grep fftw
    libfftw3.so.3 => /usr/lib64/libfftw3.so.3 (0x00007f32b4794000)


<rpath>
During the static editing phase, we can insert into the executable a list of
directories that should be searched at run time for shared libraries. This is
useful if we have libraries that reside in fixed locations that are not among
the standard locations searched by the dynamic linker. To do this, we employ
the -rpath linker option when creating an executable:

$ gcc -g -Wall -Wl,-rpath,/home/mtk/pdir -o prog prog.c libdemo.so

An alternative to the rpath option is the LD_RUN_PATH environment variable.
This variable can be assigned a string containing a series of colon-separated
directories that are to be used as the rpath list `when building` the
executable file. LD_RUN_PATH is employed only if the rpath option is not
specified when building the executable.

By default, the linker creates the rpath list as a DT_RPATH tag. To have the
linker instead create the rpath list as a DT_RUNPATH entry, we must
additionally employ the enablenewdtags (enable new dynamic tags) linker
option.


// from ld linker docs:

2. Any directories specified by -rpath options. The difference between -rpath
and -rpath-link is that directories specified by -rpath options are included in
the executable and used at runtime, whereas the `-rpath-link` option is only
effective at link time. Searching -rpath in this way is only supported by native
linkers and cross linkers which have been configured with the --with-sysroot
option.

# readelf -d
 0x0000000f (RPATH) \
  Library rpath: [/home/NDS-UK/kyoupark/asn/mips-unknown-linux-uclibc/lib]


<ld-serch-order>
The difference between these two types of rpath lists is their relative
precedence with respect to the LD_LIBRARY_PATH environment variable when the
dynamic linker searches for shared libraries at run time: `DT_RPATH` has higher
precedence, while DT_RUNPATH has lower precedence


41.11 Finding Shared Libraries at Run Time

When resolving library dependencies, the dynamic linker first inspects each
dependency string to see if it contains a slash (/), which can occur if we
specified an explicit library pathname when linking the executable. If a slash
is found, then the dependency string is interpreted as a pathname (either
    absolute or relative), and the library is loaded using that pathname. 

Otherwise, The dynamic linker searches for the shared library using the
following rules:

  * If the executable has any directories listed in its DT_RPATH run-time
    library path list (rpath) and the executable does not contain a DT_RUNPATH
    list, then these directories are searched (in the order that they were
    supplied when linking the program).

  * If the LD_LIBRARY_PATH environment variable is defined, then each of the
    colon-separated directories listed in its value is searched in turn. 

  <security>
  If the executable is a set-user-ID or set-group-ID program, then
  LD_LIBRARY_PATH is ignored. This is a security measure to prevent users from
  tricking the dynamic linker into loading a private version of a library with
  the same name as a library required by the executable.

  * If the executable has any directories listed in its DT_RUNPATH run-time
    library path list, then these directories are searched (in the order that
    they were supplied when linking the program).

  * The file `/etc/ld.so.cache` is checked to see if it contains an entry for
    the library.

  * The directories /lib and /usr/lib are searched (in that order).

<from-ld-man>
LD.SO(8)                   Linux Programmer’s Manual                  LD.SO(8)

       The shared libraries needed by the program are searched for in the
       following order:

       o  (ELF  only) Using the directories specified in the DT_RPATH dynamic
       section attribute of the binary if present and DT_RUNPATH attribute
       does not exist.  Use of DT_RPATH is deprecated.

       o  Using the environment variable LD_LIBRARY_PATH.  Except if the
       executable is a set-user-ID/set-group-ID binary, in which case it is
       ignored.

       o  (ELF only) Using the directories specified in the DT_RUNPATH dynamic
       section attribute of the binary if present.

       o  From the cache file /etc/ld.so.cache which contains a compiled list
       of candidate libraries previously found in the augmented library path.
       If, however, the binary  was  linked with the -z nodeflib linker
       option, libraries in the default library paths are skipped.

       o  In the default path /lib, and then /usr/lib.  If the binary was
       linked with the -z nodeflib linker option, this step is skipped.


<lib-ld-system-dirs>
Where system dirs comes from? comes from SYSTEM_DIRS.

// glibc-2.13/elf/dl-load.c, ld-search-order

elf/dl-load.c:1512:/* Print search path.  */
elf/dl-load.c:1520:  _dl_debug_printf (" search path=");

/* Map in the shared object file NAME.  */

struct link_map *
internal_function
_dl_map_object (struct link_map *loader, const char *name,
  int type, int trace_mode, int mode, Lmid_t nsid)
{

  /* Finally, try the default path.  */
  if (fd == -1
      && ((l = loader ?: GL(dl_ns)[nsid]._ns_loaded) == NULL
        || __builtin_expect (!(l->l_flags_1 & DF_1_NODEFLIB), 1))
      && rtld_search_dirs.dirs != (void *) -1)
  {
    fd = open_path (name, namelen, mode & __RTLD_SECURE, &rtld_search_dirs,
        &realname, &fb, l, LA_SER_DEFAULT, &found_other_class);

  }
}

open_path (const char *name, size_t namelen, int secure,
	   struct r_search_path_struct *sps, char **realname)
{
  struct r_search_path_elem **dirs = sps->dirs;

  do
  {
      struct r_search_path_elem *this_dir = *dirs;
  }
  while (*++dirs != NULL);
}


[0946684818.923326]Freeing unused kernel memory: 7000k freed
*** KT: force LD[0946684818.925326]Kernel panic - not syncing: Attempted to kill init!
[0946684818.926326]
                             1: fin<0>Rebooting in 1 seconds..d library=libdl.so.2 [0]; searching
         1:      KT: RPATH...         1:         KT: LD_LIB...         1:        KT: RUNPATH...         1:   search cache=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/etc/ld.so.cache
         1:      KT: cache...         1:         KT: default...         1:       search path=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/lib/tls:/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/lib              (system search path)
         1:       trying file=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/lib/tls/libdl.so.2
         1:       trying file=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/lib/libdl.so.2
         1:
         1:      KT: cannot open.../init: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory

11198 1999-05-03  Ulrich Drepper  <drepper@cygnus.com>
11210         * elf/dl-load.c (systems_dirs): Moved into file scope.  Initialize
11211         from SYSTEM_DIRS macro.
11212         (system_dirs_len): New variable.  Contains lengths of system_dirs
11213         strings.
11214         (fillin_rpath): Rewrite for systems_dirs being a simple string.
11215         Improve string comparisons.  Change parameter trusted to be a flag.
11216         Change all callers.
11217         (_dt_init_paths): Improve using new format for system_dirs.

212 make[2]: Entering directory `/home/NDS-UK/kyoupark/asn/gcc/glibc-2.13/elf'
213 echo "KT: /home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/lib "
214 KT: /home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/lib
215 echo "KT: gawk -f gen-trusted-dirs.awk > /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/trusted-dirs.T"
216 KT: gawk -f gen-trusted-dirs.awk > /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/trusted-dirs.T
217 echo "/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/lib "    \
218   | gawk -f gen-trusted-dirs.awk > /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/trusted-dirs.T;
219 echo '#define DL_DST_LIB "lib"' >> /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/trusted-dirs.T
220 /bin/sh ../scripts/move-if-change /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/trusted-dirs.T /home/nds-uk/kyoupark/asn/gcc/glibc-2.13-build/elf/trusted-dirs.h

./elf/gen-trusted-dirs.awk

$ echo "/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/lib" | gawk -f gen-trusted-dirs.awk
#define SYSTEM_DIRS \
  "/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/lib/"

#define SYSTEM_DIRS_LEN \
  77

#define SYSTEM_DIRS_MAX_LEN     77

$ echo "/lib" | gawk -f gen-trusted-dirs.awk
#define SYSTEM_DIRS \
  "/lib/"

#define SYSTEM_DIRS_LEN \
  5

#define SYSTEM_DIRS_MAX_LEN     5

<ex> *lib-ld-search-order*
*** KT: force LD_DEBUG=libs ***
         1:      KT: system_dirs: /lib/
         1:   find library=libdl.so.2 [0]; searching
         1:      KT: RPATH...         
         1:      KT: LD_LIBRARY_PATH...         
         1:      KT: RUNPATH...         
         1:   search cache=/home/nds-uk/kyoupark/asn/gcc/gcc-glibc-brcm-mips-install/mips-linux-gnu/etc/ld.so.cache
         1:      KT: cache...         
         1:      KT: default...
         1:      search path=/lib/tls:/lib              (system search path)
         1:     KT:  trying file=/lib/tls/libdl.so.2
         1:     KT:  trying file=/lib/libdl.so.2        note: found


<libc-script>
file /lib/ld-2.13.so /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/project_build_mips/ams-drx890/root/lib/ld-2.13.so 755 0 0
file /lib/libgcc_s.so.1 /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/project_build_mips/ams-drx890/root/lib/libgcc_s.so.1 755 0 0
file /lib/libc.so /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/project_build_mips/ams-drx890/root/lib/libc.so 755 0 0
file /lib/libc_nonshared.a /home/nds-uk/kyoupark/spk-out-use-own-brcm-gdb/project_build_mips/ams-drx890/root/lib/libc_nonshared.a 755 0 0
slink /lib/libc.so.6 libc-2.13.so 777 0 0
slink /init ../bin/sh 777 0 1

  0x00000001 (NEEDED)                     Shared library: [libdl.so.2]
  0x00000001 (NEEDED)                     Shared library: [libc.so.6]
  0x00000001 (NEEDED)                     Shared library: [ld.so.1]


={============================================================================
*kt_linux_core_402* linux-lib-ld-resolve-order lib-preload

{lib-shared-resove-order} *ld-so-runtime-resolve*
Suppose that a global symbol (i.e., a 'function' or variable) is defined in
multiple locations, such as in an executable and in a shared library, or in
'multiple' shared libraries. How is a reference to that symbol resolved?

         prog                                   libfoo.so
 -----------------------------          -----------------------------
xyz() {                                xyz() {               
  printf("main-xyz\n");                  printf("foo-xyz\n");
}                                      }
                                       
main() {                               func() {
  func();    -->                         xyz();                
}                                      }       

$ gcc -g -c -fPIC -Wall -c foo.c
$ gcc -g -shared -o libfoo.so foo.o
$ gcc -g -o prog prog.c libfoo.so
$ LD_LIBRARY_PATH=. ./prog
main-xyz

The following semantics apply:

  A definition of a global symbol in the 'main' program 'overrides' a
  definition in a library.


<lib-resolve-order> lib-link-order
If a global symbol is defined in 'multiple' libraries, then a reference to
that symbol is bound to the `first definition found` by scanning libraries in
the `left-to-right order` in which they were listed on the link command line.

This applies for static and dynamic as well. For example:

gcc -g -std=c99 $(TARGET4)_main.c lib$(TARGET4)_1.so lib$(TARGET4)_2.so lib$(TARGET4)_3.so 
  -o $(TARGET4)_123_out

Dynamic section at offset 0xbf0 contains 27 entries:
  Tag        Type                         Name/Value
 0x0000000000000001 (NEEDED)             Shared library: [liborder_1.so]
 0x0000000000000001 (NEEDED)             Shared library: [liborder_2.so]
 0x0000000000000001 (NEEDED)             Shared library: [liborder_3.so]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

gcc -g -std=c99 $(TARGET4)_main.c lib$(TARGET4)_3.so lib$(TARGET4)_1.so lib$(TARGET4)_2.so 
  -o $(TARGET4)_321_out

Dynamic section at offset 0xbf0 contains 27 entries:
  Tag        Type                         Name/Value
 0x0000000000000001 (NEEDED)             Shared library: [liborder_3.so]
 0x0000000000000001 (NEEDED)             Shared library: [liborder_1.so]
 0x0000000000000001 (NEEDED)             Shared library: [liborder_2.so]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

gcc -g -std=c99 $(TARGET4)_main.c lib$(TARGET4)_2.so lib$(TARGET4)_3.so lib$(TARGET4)_1.so 
  -o $(TARGET4)_231_out

Dynamic section at offset 0xbf0 contains 27 entries:
  Tag        Type                         Name/Value
 0x0000000000000001 (NEEDED)             Shared library: [liborder_2.so]
 0x0000000000000001 (NEEDED)             Shared library: [liborder_3.so]
 0x0000000000000001 (NEEDED)             Shared library: [liborder_1.so]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]


`The order of runtime loading also follows the order of dynamic section`

When use:

	g++ -### -g $(TARGET5)_main.c lib$(TARGET5).so -ldl -o $@

libintercept.so -ldl "-lstdc++" -lm -lgcc_s -lgcc -lc -lgcc_s -lgcc \
    /usr/lib/gcc/x86_64-linux-gnu/4.9/crtend.o \
    /usr/lib/gcc/x86_64-linux-gnu/4.9/../../../x86_64-linux-gnu/crtn.o


<case>
Although these semantics make the transition from static to shared libraries
relatively straightforward, they can cause some problems. The most significant
problem is that these semantics conflict with the model of a shared library as
implementing a self-contained subsystem. 

By default, a shared library `can not guarantee` that a reference to one of its
own global symbols will actually be bound to the library's definition of that
symbol.

Consequently, the properties of a shared library can change when it is
aggregated into a larger unit. This can lead to applications breaking in
unexpected ways, and also makes it difficult to perform divide-and-conquer
debugging (i.e., trying to reproduce a problem using fewer or different shared
libraries).

In the above scenario, if we wanted to ensure that the invocation of xyz() in
the shared library actually called the version of the function defined
`within the library`, then we could use the `-Bsymbolic linker option` when
building the shared library:

$ gcc -g -c -fPIC -Wall -c foo.c
$ gcc -g -shared -Wl,-Bsymbolic -o libfoo.so foo.o
$ gcc -g -o prog prog.c libfoo.so
$ LD_LIBRARY_PATH=. ./prog
foo-xyz

The `Bsymbolic` linker option specifies that references to global symbols
within a shared library should be preferentially bound to definitions (if they
    exist) within that library. Note that, regardless of this option, calling
xyz() from the main program would always invoke the version of xyz() defined
in the main program.


<lib-resolve-use-name> *ld-so-runtime-resolve*
o So always picks up the main version and runtime resolution use `name.`

o If multiple shared library are used in a process, all constitute a single
  lookup space? NO. As noted above, symbols in main overrides others.

o For library calls, cannot use different call signature:

  when use:
  void *malloc(char size)
  
  gcc -g -std=c99 first_main.c libfirst.so -o first_out
  first_main.c:8:7: error: conflicting types for ‘malloc’
   void *malloc(char size)
         ^
  In file included from first_main.c:2:0:
  /usr/include/stdlib.h:466:14: note: previous declaration of ‘malloc’ was here
   extern void *malloc (size_t __size) __THROW __attribute_malloc__ __wur;


<case> crash
2014/07. Samsung. 

When moves application which was a process and uses static link to the shared
library application to be used by other processes. The codes which works well
before starts to fail since crash happens when try to create a thread using
custom thread library. 

Problem of linking? Problem of the thread library when used in shared library
application? Somehow linker picks up the wrong libaray since a debugger shows
odd address when thread creation call is made and causes a crash? 

Tried various directions and spent many days. Eventually, found out that
PCThread::Create() is a problem and works fine when changes parameter orders.

The problem was that the process loads a lot of shared library and one of
those has the same PCThread class in it `but different signature.` When our
library make a call, it picks it up from the other shared library in which has
different signature so crashes. Sovled when wraps PCThread class with a
namespace.

// Q: How could this happen since declaration would be also differnet? Should
// we see compile error before? 

The typical example is to use third party library and name conflict happens
when other parties develops a libaray separately. In static link, conflict
happens in link stage. In dynamic loading, this happens at run time and called
dynamic binding. So more difficult to find out.


solution to the case

o As the case-example, can use namespace which effectively make a different
  symbol.

o As shown here, can use linker option, `-Bsymbolic` to ensure that the
  invocation of the same symbol 'in' the shared library actually called the
  version of the function defined 'within' the library.

  $ gcc -shared -Wl,-Bsymbolic -o libfoo.so foo.o

  This shows the same main's foo version.

o Unlike the case-example, this example is a problem between main and shared
  library. This means that main version gets called for the above example but
  would solve the case-example since it force to pick up the one in the same
  library.


<case>
2015.05. YouView.

When uses a wrapper process which in short exec application given, hangs on
application launch and appears hang on this wrapper when looks at top and call
stack from gdb. This wrapper has other works but not relavent to this problem.

The odd thing is that it only happens on the different hardware platform with
the same wrapper source.

shell exec -> wrapper -> application      // fail and hang
shell exec -> application                 // okay

#0 0x2ab64fb0 in pthread_cond_init () from /lib/libc.so.0
#1 0x2ae12d50 in global constructors keyed to FutureContextBase.cpp () 
  from /opt/zinc-trunk/lib/libZincCommon.so.0
#2 0x2ae35ed4 in __do_global_ctors_aux () 
  from /opt/zinc-trunk/lib/libZincCommon.so.0
#3 0x2adcc59c in ?? () from /opt/zinc-trunk/lib/libZincCommon.so.0


Since known that pthread stub from libc was used from call stack and found
that `works okay when specify pthread library in LD_PRELOAD explicitly,` looked
at library dependancy and found difference between two platforms:

for not working platform

: wrapper                    
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]

: libxx which is preloaded before application
 0x00000001 (NEEDED)                     Shared library: [libTitaniumUtils.so.0]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]

: application
 ...
 0x00000001 (NEEDED)                     Shared library: [libdirect-1.4.so.15]
 0x00000001 (NEEDED)                     Shared library: [libpthread.so.0]
 0x00000001 (NEEDED)                     Shared library: [libdl.so.0]
 ...


for working platforms

: libxx which is preloaded before application
 ...
 0x00000001 (NEEDED)                     Shared library: [libstdc++.so.6]
 0x00000001 (NEEDED)                     Shared library: [libm.so.0]
 0x00000001 (NEEDED)                     Shared library: [libpthread.so.0]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]


So "libxx" has libpthread for working case and thought this explains the
problem since the stub from libc was used for not working case. While having
problem to mess with as-needed in the build system and libtool, eventually
turned out that the problem happens regardless of pthread so in libxx.


// *ld-so* and believe this is uclibc in this case

The vendor investigation is:

o dynamic loader for working platform is more powerful. for not working,
  ld-0.9.29.so, for working, ld-0.9.32.1.so

o libpthread in working platform is changed to meet the require of dynamic
loader.

When libpthread.so is not included in LD_PRELOAD and on working platform, 

*ld-so-runtime-resolve*
__dl_runtime_resolve will finish the relocation, for example before invoking
pthread_cond_init, the symbol still belongs to libc, but we really invoking
pthread_cont_init, the pthread_cond_init funciton will be relocated to
libpthread. this is implemented by __dl_runtime_resolve, of course
__dl_runtime_resolve is invoked by dynamic loader.

If open the log of dynamic loader, we can see this:

resolve function: pthread_cond_init
patched 0x77a313e0 ==> 0x77cbfa60 @ 0x77a6cef8 (reloacated)

However, we can't see this log for not working platform.

// believe that `relocated` means `resolved`


// conclusion then

This is a problem of *ld-so* which fails to relocate symbols. This is vendor's
conclusion but the same condition such as the same ld version and others works
okay on the other vendor's platform. Believe that it's uclibc bug and it is
fixed in the latest and the other vendor patched the old version.

int __pthread_cond_init (pthread_cond_t *cond, const pthread_condattr_t *cond_attr) 
{ 
if (__libc_pthread_functions.ptr___pthread_cond_init == ((void *)0)) 
	return 0; 

// {
// added by me to test if 
if (__libc_pthread_functions.ptr___pthread_cond_init == __pthread_cond_init)
	return 0;
// }

return __libc_pthread_functions.ptr___pthread_cond_init (cond, cond_attr); 
}

// which part of uclibc is a bug?


<libc-glibc-stub>
http://stackoverflow.com/questions/21092601/is-pthread-in-glibc-so-implemented-by-weak-symbol-to-provide-pthread-stub-functi

I know there is pthread.so to provide the functions similar with pthread in
glibc.so somebody said pthread in glibc provide stub only and will be replace
when explicit linking to lpthread.o my question is how to support it? using
weak symbol or other tech?


Yes, glibc uses a stub implementation of various pthread functions, so that
single threaded programs do not have to waste cycles doing things like locking
and unlocking mutexes, and yet do not have to link to a different C library
(like what is done in the Microsoft world, for instance).

For instance, according to POSIX, every time you call fputc(ch, stream), there
is mutex lock and unlock. If you don't want that, you call fputc_unlocked. But
when you do that, you're using a POSIX extension related to threading; it's
not an appropriate workaround for programs that don't use POSIX or don't use
the threading API.

The overriding of the stub pthread functions with the real ones (in the
    dynamic glibc) is not based on weak symbols. The shared library mechanism
makes it possible to override non-weak definitions.


note:
Weak symbols are a mechanism which allows for symbol overriding under 'static'
linking.

If you want a source for the above statement, here it is:

"Note that a definition in a DSO being weak has no effects. Weak definitions
only play a role in static linking." [Ulrich Drepper, "How To Write Shared
Libraries"].  http://www.akkadia.org/drepper/dsohowto.pdf

If you run nm on the static glibc on your system (if you have one), libc.a, you
will note that functions like pthread_mutex_lock are marked weak. In the dynamic
version, libc.so.<whatetever>, the functions are not marked weak.

*ld-so-dynamic* *tool-nm*
you should use nm -D or nm --dynamic to look at the symbols in a shared
library. nm will not produce anything on a shared library that is stripped. If
it does, you're looking at the debug symbols, not the dynamic symbols.


<from-embedded-libc> *libc-uclibc*
root# ls -al /lib/libc.so.0
lrwxrwxrwx    1 root     root            19 Jan  1  1970 
  /lib/libc.so.0 -> libuClibc-0.9.29.so

$ nm -D libc.so.0 | ag pthread
000574b0 T __libc_pthread_init
00056d1c T __pthread_attr_init_2_1
00056f58 T __pthread_cond_broadcast
00056f84 T __pthread_cond_destroy
00056fb0 T __pthread_cond_init
00056fdc T __pthread_cond_signal
00057034 T __pthread_cond_timedwait
00057008 T __pthread_cond_wait
0005730c T __pthread_exit
         w __pthread_initialize_minimal
         w __pthread_mutex_init
         w __pthread_mutex_lock
         w __pthread_mutex_trylock
         w __pthread_mutex_unlock
         w _pthread_cleanup_pop_restore
         w _pthread_cleanup_push_defer
00056cf0 T pthread_attr_destroy
00056d48 T pthread_attr_getdetachstate
00056da0 T pthread_attr_getinheritsched
00056df8 T pthread_attr_getschedparam
00056e50 T pthread_attr_getschedpolicy
00056ea8 T pthread_attr_getscope
00056d1c W pthread_attr_init
00056d74 T pthread_attr_setdetachstate
00056dcc T pthread_attr_setinheritsched
00056e24 T pthread_attr_setschedparam
00056e7c T pthread_attr_setschedpolicy
00056ed4 T pthread_attr_setscope
00056f58 W pthread_cond_broadcast
00056f84 W pthread_cond_destroy
00056fb0 W pthread_cond_init
00056fdc W pthread_cond_signal
00057034 W pthread_cond_timedwait
00057008 W pthread_cond_wait
00056f00 T pthread_condattr_destroy
00056f2c T pthread_condattr_init
00057060 T pthread_equal
0005708c T pthread_getschedparam
000570e4 T pthread_mutex_destroy
00057110 T pthread_mutex_init
> 00057168 T pthread_mutex_lock
0005713c T pthread_mutex_trylock
00057194 T pthread_mutex_unlock
000571ec T pthread_mutexattr_destroy
000571c0 T pthread_mutexattr_init
00057218 T pthread_mutexattr_settype
00057244 T pthread_self
00057270 T pthread_setcancelstate
0005729c T pthread_setcanceltype
000570b8 T pthread_setschedparam


{lib-preload} *linux-ld-load-rule*
For testing purposes, it can sometimes be useful to `selectively override`
functions and other symbols that would normally be found by the dynamic linker
using the search rules. 

To do this, we can define the environment variable LD_PRELOAD as a string
consisting of space-separated or colon-separated names of shared libraries that
should be loaded `before any other shared libraries` 

Since these libraries are loaded first, any functions they define will
automatically be used if required by the executable, thus overriding any other
functions of the `same name` that the dynamic linker would otherwise have
searched for.


$ LD_LIBRARY_PATH=. LD_PRELOAD=libfunc.so ./ex_preload 
====> 
====> this is uaf(use after free)..
====> 
my malloc: {
my malloc: }


<process-and-system-wide>
The LD_PRELOAD environment variable controls preloading on a `per-process`
basis. Alternatively, the file `/etc/ld.so.preload`, which lists libraries
separated by white space, can be used to perform the same task on a
system-wide basis. Libraries specified by LD_PRELOAD are loaded before those
specified in /etc/ld.so.preload. 

<security>
For security reasons, set-user-ID and set-group-ID programs ignore LD_PRELOAD.

<lib-shared-check-loading>
You can use the following command to test if the driver (the driver itself just
        dlopen() this lib) is happy to load:

LD_PRELOAD=/opt/zinc/lib/libyouviewrcu.so /bin/true

note:
Why "/bin/true"? This is a simple utility which returns 0(success) and to check
if libraries are loaded fine, run that with simple utility.

note:
have to use absolute path.

https://github.com/google/sanitizers/issues/796
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=64234


={============================================================================
*kt_linux_core_400* linux-lib-ld-debug ld-glibc

the dynamic loader is not part of GCC and is part of c library.

<ld-man>
http://man7.org/linux/man-pages/man8/ld.so.8.html
LD.SO(8)                   Linux Programmer’s Manual                  LD.SO(8)

NAME
       ld.so, ld-linux.so* - dynamic linker/loader

SYNOPSIS
       The  dynamic  linker  can be run either indirectly by running some
       dynamically linked program or library (in which case no command-line
           options to the dynamic linker can be passed and, in the ELF case,
           the dynamic linker which is stored in the .interp section of the
           program is executed) or directly by running:

       /lib/ld-linux.so.*  [OPTIONS] [PROGRAM [ARGUMENTS]]

ENVIRONMENT
       There are four important environment variables.

       LD_BIND_NOW
              (libc5; glibc since 2.1.1) If set to a non-empty string, causes
              the dynamic linker to resolve all symbols at program startup
              instead of deferring function  call  resolution to the point
              when they are first referenced.  This is useful when using a
              debugger.

       LD_LIBRARY_PATH
              A colon-separated list of directories in which to search for ELF
              libraries at execution-time.  Similar to the PATH environment
              variable.

       *linux-ld-load-rule*                
       LD_PRELOAD
              A whitespace-separated list of additional, user-specified, ELF
              shared libraries to be loaded before all others.  This can be
              used to selectively override functions in other shared
              libraries.  For set-user-ID/set-group-ID ELF binaries, only
              libraries in the standard search directories that are also
              set-user-ID will be loaded.

       LD_TRACE_LOADED_OBJECTS
              (ELF only) If set to a non-empty string, causes the program to
              list its dynamic library dependencies, as if run by ldd(1),
              instead of running normally.


       Then there are lots of more or less obscure variables, many obsolete or
       only for internal use.

       LD_DEBUG
              (glibc since 2.1) Output verbose debugging information about the
              dynamic linker.  If set to all prints all debugging information
              it has, if set to help prints a  help  message about which
              categories can be specified in this environment variable.  Since
              glibc 2.3.4, LD_DEBUG is ignored for set-user-ID/set-group-ID
              binaries.

Sometimes, it is useful to monitor the operation of the dynamic linker in
order to know, for example, where it is searching for libraries. We can use
the LD_DEBUG environment variable to do this. By setting this variable to one
(or more) of a set of standard keywords, we can obtain various kinds of
tracing information from the dynamic linker.

If we assign the value help to LD_DEBUG, the dynamic linker displays help
information about LD_DEBUG, and the specified command is not executed:

note: can be any command other 
$ LD_DEBUG=help ls  
Valid options for the LD_DEBUG environment variable are:

  libs        display library search paths
  reloc       display relocation processing
  files       display progress for input file
  symbols     display symbol table processing
  bindings    display information about symbol binding
  versions    display version dependencies
  all         all previous options combined
  statistics  display relocation statistics
  unused      determined unused DSOs
  help        display this help message and exit

To direct the debugging output into a file instead of standard output
a filename can be specified using the LD_DEBUG_OUTPUT environment variable.


$ LD_DEBUG=libs ls
     20177:	find library=libselinux.so.1 [0]; searching
     20177:	 search path=./tls/x86_64:./tls:./x86_64:.		(LD_LIBRARY_PATH)
     20177:	  trying file=./tls/x86_64/libselinux.so.1
     20177:	  trying file=./tls/libselinux.so.1
     20177:	  trying file=./x86_64/libselinux.so.1
     20177:	  trying file=./libselinux.so.1
     20177:	 search cache=/etc/ld.so.cache
     20177:	  trying file=/lib/x86_64-linux-gnu/libselinux.so.1
     20177:	
     20177:	find library=librt.so.1 [0]; searching
     20177:	 search path=./tls/x86_64:./tls:./x86_64:.		(LD_LIBRARY_PATH)
     20177:	  trying file=./tls/x86_64/librt.so.1
     20177:	  trying file=./tls/librt.so.1
     20177:	  trying file=./x86_64/librt.so.1
     20177:	  trying file=./librt.so.1
     ...
     20177:	
install-sh  ltmain.sh  Makefile.am  Makefile.in  missing  src $ 

note: 
ls command was executed. works on host since it uses glibc but not on a
target. use strace instead.

The PID value displayed at the start of each line and this is useful if we are
monitoring several processes (e.g., parent and child).

If desired, we can assign multiple options to LD_DEBUG by separating them with
commas (no spaces should appear). 

<symbol-option>
The output of the symbols option (which traces symbol resolution by the
    dynamic linker) is particularly voluminous.

LD_DEBUG is effective both for libraries implicitly loaded by the dynamic
linker and for libraries dynamically loaded by dlopen().

For security reasons, LD_DEBUG is (since glibc 2.2.5) ignored in set-user-ID
and set- group-ID programs.


<ex>
// glibc-2.13/elf/rtld.c

/* Process all environments variables the dynamic linker must recognize.
   Since all of them start with `LD_' we are a bit smarter while finding
   all the entries.  */
extern char **_environ attribute_hidden;

static void
process_envvars (enum mode *modep)
{
  _dl_printf ("*** KT: force LD_DEBUG=all ***\n");
  process_dl_debug ("all");

  /*
     _dl_next_ld_env_entry Walk through the environment of the process and return
     all entries starting with `LD_'. */

  while ((envline = _dl_next_ld_env_entry (&runp)) != NULL)

  case 5:
    /* Debugging of the dynamic linker?  */
    if (memcmp (envline, "DEBUG", 5) == 0)
    {
      process_dl_debug (&envline[6]);
      break;
    }
}


={============================================================================
*kt_linux_core_405* linux-lib-ld-api

LPI42.1

DLOPEN(3)

NAME
       dladdr, dlclose, dlerror, dlopen, dlsym, dlvsym - programming interface
       to dynamic linking loader

SYNOPSIS
       #include <dlfcn.h>

       void *dlopen(const char *filename, int flag);

       char *dlerror(void);

       void *dlsym(void *handle, const char *symbol);

       int dlclose(void *handle);

       `Link with -ldl.`


<dl-dlopen>

void *dlopen(const char *libfilename, int flags);

Returns library handle on success, or NULL on error

The dlopen() opens a shared library, returning a handle used by sub-sequent
calls. Loads the shared library named in libfilename into the calling process's
virtual address space and increments the count of open references to the
library.

If the shared library specified by libfilename contains dependencies on other
shared libraries, `dlopen() also automatically loads those libraries.` This
procedure occurs recursively if necessary. We refer to the set of such loaded
libraries as this library's dependency tree.

RTLD_LAZY

Undefined function symbols in the library should be resolved only as the code is
executed. If a piece of code requiring a particular symbol is not executed, that
symbol is 'never' resolved. Lazy resolution is performed `only for function`
references; references to variables are always resolved immediately. Specifying
the RTLD_LAZY flag provides behavior that corresponds to the normal operation of
the dynamic linker when loading the shared libraries identified in an
executable's dynamic dependency list.

It is also possible to include further values in flags. The following flags are
specified in SUSv3:

RTLD_GLOBAL

Symbols in this library and its dependency tree are made available for resolving
references in other libraries loaded by this process and also for lookups via
dlsym().


<dl-dlerror>
If we receive an error return from dlopen() or one of the other functions in the
dlopen API, we can use dlerror() to obtain a pointer to a string that indicates
the cause of the error.

#include <dlfcn.h>
const char *dlerror(void);

Returns pointer to error-diagnostic string, or NULL if 'no' error has occurred
since previous call to dlerror()

The dlerror() function returns NULL if no error has occurred since the last call
to dlerror(). We'll see how this is useful in the next section.

<ex>
(void) dlerror(); /* Clear dlerror() */


<dl-dlsym>
#include <dlfcn.h>

void *dlsym(void *handle, char *symbol);

Returns address of symbol, or NULL if symbol is not found

searches a library for a symbol (a string containing the name of a function or
    variable) and returns its address.

The value of a symbol returned by dlsym() 'may' be NULL, which is
'indistinguishable' from the "symbol not found" return. 

In order to differentiate the two possibilities, we must call dlerror()
beforehand (to make sure that any previously held error string is cleared) and
then if, after the call to dlsym(), dlerror() returns a non-NULL value, we
know that an error occurred.

There  are  two special pseudo-handles: 

RTLD_DEFAULT

The former will find the first occurrence of the desired symbol using the
default library search order.  

RTLD_NEXT *linux-dl-next*

The latter will find the next occurrence of a function in the search order
after the current library. This allows one to provide a wrapper around a
function in another shared library.

<ex>

From dlsym(3):

    Since the value of the symbol could actually be NULL (so that a NULL return
            from dlsym() need not indicate an error), the correct way to test
    for an error is to call dlerror(3) to clear any old error conditions, then
    call dlsym(), and then call dlerror(3) again, saving its return value into a
    variable, and check whether this saved value is not NULL.


typedef bool (*BcmInit)(void);
BcmInit bcmInit = (BcmInit)dlsym(nexusSharedLibHandle, "BcmNexus_Platform_Init");
const char* err = dlerror();

// This assumes that when `dlsym` is NULL, `dlerror` should return some value so
// err should not be NULL. However, dlerror can return NULL when dlsym return
// NULL and this code runs to else-if and can crash since bcmInit() is a call.
//
if (!bcmInit && err) {
    NICKEL_ERROR("dlsym failed: " << err);
} else if (!bcmInit()) { 
    ...
}


Should be:

if (!bcmInit) {
    const char* const err = dlerror();
    ERROR("dlsym failed: " << (err ? err : "(unknown error)"));
} else if (!bcmInit()) {


<dl-dlclose>
The dlclose() function closes a library previously opened by dlopen().


<dl-iterate>

DL_ITERATE_PHDR(3)

NAME
       dl_iterate_phdr - walk through list of shared objects

SYNOPSIS
       #define _GNU_SOURCE         /* See feature_test_macros(7) */
       #include <link.h>

       int dl_iterate_phdr(
                 int (*callback) (struct dl_phdr_info *info,
                                  size_t size, void *data),
                 void *data);

// skipped

// /usr/include/link.h
/* Data structure for communication from the run-time dynamic linker for
   loaded ELF shared objects.
   Copyright (C) 1995-2014 Free Software Foundation, Inc.
   This file is part of the GNU C Library.

   The GNU C Library is free software; you can redistribute it and/or
   modify it under the terms of the GNU Lesser General Public
   License as published by the Free Software Foundation; either
   version 2.1 of the License, or (at your option) any later version.

   The GNU C Library is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   Lesser General Public License for more details.

   You should have received a copy of the GNU Lesser General Public
   License along with the GNU C Library; if not, see
   <http://www.gnu.org/licenses/>.  */

#ifndef _LINK_H
#define _LINK_H 1

#include <features.h>
#include <elf.h>
#include <dlfcn.h>
#include <sys/types.h>
...

see *dl-iterate* example.

#define _GNU_SOURCE
#include <link.h>
#include <stdlib.h>
#include <stdio.h>

// kyoupark@kit-debian64:~/git/kb/code-linux/ex_shared/dl-iterate$ LD_LIBRARY_PATH=. ./driver_out
//
// name= (8 segments) address=(nil)
//                  header  0: address=  0x400040 type=6, flags=0x5
//                  header  1: address=  0x400200 type=3, flags=0x4
//                  header  2: address=  0x400000 type=1, flags=0x5
//                  header  3: address=  0x600ac8 type=1, flags=0x6
//                  header  4: address=  0x600ae0 type=2, flags=0x6
//                  header  5: address=  0x40021c type=4, flags=0x4
//                  header  6: address=  0x400974 type=1685382480, flags=0x4
//                  header  7: address=     (nil) type=1685382481, flags=0x6
// 
// name=linux-vdso.so.1 (4 segments) address=0x7ffc273f4000
//                  header  0: address=0x7ffc273f4000 type=1, flags=0x5
//                  header  1: address=0x7ffc273f4318 type=2, flags=0x4
//                  header  2: address=0x7ffc273f4818 type=4, flags=0x4
//                  header  3: address=0x7ffc273f4854 type=1685382480, flags=0x4
// 
// name=./libshared.so (6 segments) address=0x7f72375d7000
//                  header  0: address=0x7f72375d7000 type=1, flags=0x5
//                  header  1: address=0x7f72377d7758 type=1, flags=0x6
//                  header  2: address=0x7f72377d7770 type=2, flags=0x6
//                  header  3: address=0x7f72375d7190 type=4, flags=0x4
//                  header  4: address=0x7f72375d76d0 type=1685382480, flags=0x4
//                  header  5: address=0x7f72375d7000 type=1685382481, flags=0x6
// 
// name=/lib/x86_64-linux-gnu/libc.so.6 (10 segments) address=0x7f723722c000
//                  header  0: address=0x7f723722c040 type=6, flags=0x5
//                  header  1: address=0x7f723739a650 type=3, flags=0x4
//                  header  2: address=0x7f723722c000 type=1, flags=0x5
//                  header  3: address=0x7f72375cd748 type=1, flags=0x6
//                  header  4: address=0x7f72375d0ba0 type=2, flags=0x6
//                  header  5: address=0x7f723722c270 type=4, flags=0x4
//                  header  6: address=0x7f72375cd748 type=7, flags=0x4
//                  header  7: address=0x7f723739a66c type=1685382480, flags=0x4
//                  header  8: address=0x7f723722c000 type=1685382481, flags=0x6
//                  header  9: address=0x7f72375cd748 type=1685382482, flags=0x4
// 
// name=/lib64/ld-linux-x86-64.so.2 (7 segments) address=0x7f72377d8000
//                  header  0: address=0x7f72377d8000 type=1, flags=0x5
//                  header  1: address=0x7f72379f8c00 type=1, flags=0x6
//                  header  2: address=0x7f72379f8e70 type=2, flags=0x6
//                  header  3: address=0x7f72377d81c8 type=4, flags=0x4
//                  header  4: address=0x7f72377f5680 type=1685382480, flags=0x4
//                  header  5: address=0x7f72377d8000 type=1685382481, flags=0x6
//                  header  6: address=0x7f72379f8c00 type=1685382482, flags=0x4

// /usr/include/link.h
//
// struct dl_phdr_info
//   {
//     ElfW(Addr) dlpi_addr;
//     const char *dlpi_name;
//     const ElfW(Phdr) *dlpi_phdr;
//     ElfW(Half) dlpi_phnum;
//   };
// 
// /usr/include/elf.h
// 
// /* Program segment header.  */
// 
// typedef struct
// {
//   Elf32_Word	p_type;			/* Segment type */
//   Elf32_Off	p_offset;		/* Segment file offset */
//   Elf32_Addr	p_vaddr;		/* Segment virtual address */
//   Elf32_Addr	p_paddr;		/* Segment physical address */
//   Elf32_Word	p_filesz;		/* Segment size in file */
//   Elf32_Word	p_memsz;		/* Segment size in memory */
//   Elf32_Word	p_flags;		/* Segment flags */
//   Elf32_Word	p_align;		/* Segment alignment */
// } Elf32_Phdr;
// 
// Program Headers:
//   Type           Offset             VirtAddr           PhysAddr
//                  FileSiz            MemSiz              Flags  Align
//   PHDR           0x0000000000000040 0x0000000000400040 0x0000000000400040
//                  0x00000000000001c0 0x00000000000001c0  R E    8
//   INTERP         0x0000000000000200 0x0000000000400200 0x0000000000400200
//                  0x000000000000001c 0x000000000000001c  R      1
//       [Requesting program interpreter: /lib64/ld-linux-x86-64.so.2]

static int header_handler(struct dl_phdr_info* info, size_t size, void* data)
{
    printf("name=%s (%d segments) address=%p\n",
            info->dlpi_name, info->dlpi_phnum, (void*)info->dlpi_addr);

    for (int j = 0; j < info->dlpi_phnum; j++) {

         printf("\t\t header %2d: address=%10p type=%u, flags=0x%X\n", j,
             (void*) (info->dlpi_addr + info->dlpi_phdr[j].p_vaddr),
                 info->dlpi_phdr[j].p_type, info->dlpi_phdr[j].p_flags);

         // printf("\t\t\t type=%u, flags=0x%X\n",
         //        info->dlpi_phdr[j].p_type, info->dlpi_phdr[j].p_flags);
    }

    printf("\n");
    return 0;
}


extern int ml_func(int, int);

int main(int argc, const char* argv[])
{
    dl_iterate_phdr(header_handler, NULL);

    int t = ml_func(argc, argc);
    return t;
}


<dl-link>
To build programs that use the dlopen API on Linux, we must specify the -ldl
option, in order to link against the libdl library.


{access-symbols-in-main}
Sometimes, it is desirable instead to have x() in shared library invoke an
implementation of y() in the main program. In order to do this, we must make the
(global-scope) symbols in the main program available to the dynamic linker, by
linking the program using the --export-dynamic linker option:

$ gcc -Wl,--export-dynamic main.c (plus further options and arguments)

Equivalently, we can write the following:

$ gcc -export-dynamic main.c

Using either of these options allows a dynamically loaded library to access
global symbols in the main program.


={============================================================================
*kt_linux_core_400* lib-shared-soname

In the above example, libfoo is realname. As with a realname, soname is used in
linking and is embedded in the executable, and used by the linker at runtime.

Why soname? The purpose of the soname is to provide a level of 'indirection'
that permits an executable to use, at runtime, a version of the shared library
that is different from but compatible with the library against which it was
linked.

$ gcc -c -fpic foo.c

$ gcc -shared -Wl,-soname,libbar.so -o libfoo.so foo.o      
note: shall no space between -soname

$ gcc -shared -Wl,-soname -Wl,libbar.so -o libfoo.so foo.o  

$ readelf -d libfoo.so | grep SONAME              note: DT_SONAME in ELF
(standard input):5: 0x000000000000000e (SONAME)   Library soname: [libbar.so]

$ gcc -o one main.c libfoo.so

$ readelf -d one | grep NEED 
:4: 0x0000000000000001 (NEEDED)             Shared library: [`libbar.so`]
:5: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ ./one 
./one: error while loading shared libraries: libbar.so: cannot open shared 
  object file: No such file or directory

$ LD_LIBRARY_PATH=. ./one
./one: error while loading shared libraries: libbar.so: cannot open shared 
  object file: No such file or directory

note: Why fails this time? So set soname in so file itself and executable embeds
it instead of realname. Now ld looks for `libbar` instead but not libfoo.

<indirection>
When using a soname, one further step is required: we must create a symbolic
link from the soname to the real name of the library. This symbolic link must be
created in one of the directories searched by the dynamic linker. Thus, we could
run our program as follows:

$ ln -s libfoo.so libbar.so 
$ ll
lrwxrwxrwx 1 kpark kpark    9 Feb  5 11:22 libbar.so -> libfoo.so*
$ ./one 
foo: this is foo...


{versioning}
<requirement>
The same calling interface and are semantically equivalent (they achieve
    identical results). Such differing but compatible versions are referred to
as 'minor' versions of a shared library.

Occasionally, however, it is necessary to create a new 'major' version of a
library-one that is incompatible with a previous version. At the same time, it
must still be possible to continue running programs that require the older
version of the library.

<realname>
libdemo.so.1.0.1
libdemo.so.1.0.2        Minor version, compatible with version 1.0.1
libdemo.so.2.0.0        New major version, incompatible with version 1.*

<soname> minor-independent link
The soname of the shared library includes the same 'major' version identifier as
its corresponding real library name, but excludes the minor version identifier.
Thus, the soname has the form libname.so.major-id.

Usually, the soname is created as a relative symbolic link in the directory that
contains the real name. The following are some examples of sonames, along with
the real names to which they might be symbolically linked:

libdemo.so.1   -> libdemo.so.1.0.2
libdemo.so.2   -> libdemo.so.2.0.0

Normally, the soname corresponding to each major library version points to the
most recent 'minor' version within the major version. This setup allows for the
correct versioning semantics during the runtime operation of shared libraries. 

Because the static-linking phase embeds a copy of the soname in the executable
which has major, and the soname symbolic link may subsequently be modified to
point to a newer (minor) version of the shared library, it is possible to ensure
that an executable loads the most up-to-date minor version of the library at
runtime.

Furthermore, since different major versions of a library have different sonames,
  they can happily coexist and be accessed by the programs that require them.

note: Due to this indirection, possible to meet versioning requirement by
changing a sym link. After all, soname is to have 'indirection' to minor
version.

<linkername> version-independent link
The linker name, which is used when 'linking' an executable against the shared
library. The linker name is a symbolic link containing just the library name and
thus has the form libname.so. The linker name allows us to construct
version-independent link commands that automatically operate with the correct
version of the shared library.

libdemo.so ->  libdemo.so.2


real name(minor)   <-   soname(major)         <-   linker name         
libdemo.so.1.0.1        libdemo.so.1               libdemo.so
(regular file)          (symbolic link)            (symbolic link)
lib name.so.maj.min     libname.so.maj
Object code for         
library modules


$ gcc -g -c -fPIC -Wall mod1.c mod2.c mod3.c

created a realname and soname

$ gcc -g -shared -Wl,-soname,libdemo.so.1 -o libdemo.so.1.0.1 mod1.o mod2.o mod3.o

$ ln -s libdemo.so.1.0.1 libdemo.so.1
$ ln -s libdemo.so.1 libdemo.so

$ ls -l libdemo.so* | awk '{print $1, $9, $10, $11}'
lrwxrwxrwx libdemo.so -> libdemo.so.1
lrwxrwxrwx libdemo.so.1 -> libdemo.so.1.0.1
-rwxr-xr-x libdemo.so.1.0.1

$ gcc -g -Wall -o prog prog.c -L. -ldemo        
$ gcc -g -Wall -o prog prog.c `libdemo.so`
// as with exmple $ gcc -o one main.c libfoo.so

note: -ldemo or libdemo.so is linkername and use is in `static linking stage`

$ LD_LIBRARY_PATH=. ./prog
Called mod1-x1
Called mod2-x2

So three cases:

o the case when linkername is realname. Means that no soname is used. 
o the case when linkername is soname. This is one-level indirection. 
o the case when linkername is different from soname. This is two-level
indirection.

note: Even when linkername is different from soname, executable 'always' embeds
'soname' and the linkername is a name to use linking stage that must be exist
before doing linking. Checked with the above one example. 

Using two names and two indirections enable us to change major and minor libs to
use. 

When major changes, changes linkname link: need to link again 

libdemo.so -> libdemo.so.1          =>    libdemo.so -> libdemo.so.2

When minor changes, change soname link: no need to link.

libdemo.so.1 -> libdemo.so.1.0.1    =>    libdemo.so.1 -> libdemo.so.1.0.1

note: The above is convention but not mandatory so can have followings:

17 Dec 29 13:08 liblog4c.so -> liblog4c.so.3.1.0*
17 Dec 29 13:08 liblog4c.so.3 -> liblog4c.so.3.1.0*


={============================================================================
*kt_linux_core_400* linux-lib-shared-visibility

*term-abi*

A well-designed shared library should make visible only those symbols (functions
        and variables) that form part of its specified application binary
interface (ABI). The reasons for this are as follows:

* If the shared library designer accidentally exports unspecified interfaces,
  then authors of applications that use the library may choose to employ these
  interfaces. This creates a 'compatibility' problem for future upgrades of the
  shared library.

  The library developer expects to be able to change or remove any interfaces
  'other' than those in the documented ABI, while the library user expects to
  continue using the same interfaces (with the same semantics) that they
  currently employ.

* During run-time symbol resolution, any symbols that are exported by a shared
  library might interpose definitions that are provided in other shared
  libraries.

  // this is a problem as <resolve-real-problem>?

* Exporting unnecessary symbols increases the size of the dynamic symbol table
  that must be loaded at runtime. 
  
  // also means 'faster' loading time?

All of these problems can be minimized or avoided altogether if the library
designer ensures that only the symbols required by the library's specified ABI
are `exported` 

The following techniques can be used to control the export of symbols:

In a C program, we can use the static keyword to make a symbol private to a
source-code module, thus rendering it unavailable for binding by other object
files.

// why converse? anyway, *cxx-static* make bind local so those references
// won't be subject to runtime interposition.

As well as making a symbol private to a source-code module, the static keyword
also has a converse effect. If a symbol is marked as static, then all references
to the symbol 'in' the same source file will be bound to that definition of the
symbol. Consequently, these references won't be subject to run-time
interposition by definitions from other shared libraries in the manner described
in Section 41.12. 

This effect of the static keyword is similar to the `-Bsymbolic` linker option
described in Section 41.12, with the difference that the static keyword affects
a 'single' symbol within a single source file.

@ The GNU C complier, gcc, provides a compiler-specific attribute declaration
that performs a similar task to the static keyword:

void __attribute__ ((visibility("hidden")))
func(void) {
   /* Code */
}

see *gcc-attributes*

Whereas the static keyword limits the visibility of a symbol to a single source
code file, the hidden attribute makes the symbol available across all source
code files that compose the shared library, but prevents it from being visible
outside the library.

As with the static keyword, the hidden attribute also has the converse effect
of preventing symbol interposition at runtime.

o Version scripts (Section 42.3) can be used to precisely control symbol
visibility and to select the version of a symbol to which a reference is
bound.

o When dynamically loading a shared library (Section 42.1.1), the dlopen()
  RTLD_GLOBAL flag can be used to specify that the symbols defined by the
  library should be made available for binding by subsequently loaded
  libraries, and the ––export–dynamic linker option (Section 42.1.6) can be
  used to make the global symbols of the main program available to dynamically
  loaded libraries.

For further details on the topic of symbol visibility, see [Drepper, 2004 (b)].

<elf-weak-symbol>
A weak symbol is a half-hearted global symbol: if a definition is available
for an undefined weak symbol, the linker will use it, but if not the value
defaults to zero.


={============================================================================
*kt_linux_core_406* lib-shared-elf-edit 

http://nairobi-embedded.org/085_elf_editing_dt_needed.html

note:
this works when the length of library name is the same.

readelf -S libgcc_s.so.1
There are 38 section headers, starting at offset 0x43544:

Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .reginfo          MIPS_REGINFO    000000f4 0000f4 000018 18   A  0   0  4
  [ 2] .dynamic          DYNAMIC         0000010c 00010c 0000e8 08   A  5   0  4
  [ 3] .hash             HASH            000001f4 0001f4 000640 04   A  4   0  4
  [ 4] .dynsym           DYNSYM          00000834 000834 000c90 10   A  5   2  4
  [ 5] .dynstr           STRTAB          000014c4 0014c4 000c0a 00   A  0   0  1
  [ 6] .gnu.version      VERSYM          000020ce 0020ce 000192 02   A  4   0  2
  [ 7] .gnu.version_d    VERDEF          00002260 002260 000158 00   A  5  10  4
  [ 8] .rel.dyn          REL             000023b8 0023b8 000028 08   A  4   0  4
  [ 9] .init             PROGBITS        00002598 002598 000098 00  AX  0   0  4
  [10] .text             PROGBITS        00002630 002630 00c1f0 00  AX  0   0 16
  [11] .MIPS.stubs       PROGBITS        0000e820 00e820 0000a0 00  AX  0   0  4
  [12] .fini             PROGBITS        0000e8c0 00e8c0 000050 00  AX  0   0  4
  [13] .rodata           PROGBITS        0000e910 00e910 000490 00   A  0   0 16
  [14] .eh_frame_hdr     PROGBITS        0000eda0 00eda0 0001c4 00   A  0   0  4
  [15] .eh_frame         PROGBITS        0000f000 00f000 000704 00  WA  0   0  4
  [16] .ctors            PROGBITS        0000f704 00f704 000008 00  WA  0   0  4
  [17] .dtors            PROGBITS        0000f70c 00f70c 000008 00  WA  0   0  4
  [18] .jcr              PROGBITS        0000f714 00f714 000004 00  WA  0   0  4
  [19] .data             PROGBITS        0000f720 00f720 000020 00  WA  0   0 16
  [20] .got              PROGBITS        0000f740 00f740 0000d4 04 WAp  0   0 16
  [21] .sdata            PROGBITS        0000f814 00f814 000004 00 WAp  0   0  4
  [22] .bss              NOBITS          0000f820 00f818 000120 00  WA  0   0 16
  [23] .comment          PROGBITS        00000000 00f818 001ea0 00      0   0  1
  [24] .debug_aranges    MIPS_DWARF      00000000 0116b8 001470 00      0   0  8
  [25] .debug_pubnames   MIPS_DWARF      00000000 012b28 000ff5 00      0   0  1
  [26] .debug_info       MIPS_DWARF      00000000 013b1d 01353e 00      0   0  1
  [27] .debug_abbrev     MIPS_DWARF      00000000 02705b 00752f 00      0   0  1
  [28] .debug_line       MIPS_DWARF      00000000 02e58a 006538 00      0   0  1
  [29] .debug_frame      MIPS_DWARF      00000000 034ac4 001640 00      0   0  4
  [30] .debug_str        MIPS_DWARF      00000000 036104 002204 01  MS  0   0  1
  [31] .debug_loc        MIPS_DWARF      00000000 038308 008982 00      0   0  1
  [32] .mdebug.abi32     PROGBITS        00008982 040c8a 000000 00      0   0  1
  [33] .pdr              PROGBITS        00000000 040c8c 001d40 00      0   0  4
  [34] .debug_ranges     MIPS_DWARF      00000000 0429d0 000a10 00      0   0  8
  [35] .shstrtab         STRTAB          00000000 0433e0 000162 00      0   0  1
  [36] .symtab           SYMTAB          00000000 043b34 001f70 10     37 304  4
  [37] .strtab           STRTAB          00000000 045aa4 001326 00      0   0  1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings)
  I (info), L (link order), G (group), T (TLS), E (exclude), x (unknown)
  O (extra OS processing required) o (OS specific), p (processor specific)


readelf -S libgcc_s.so.1 | egrep -A1 '(\[Nr\]|dynstr)'
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
--
  [ 5] .dynstr           STRTAB          000014c4 0014c4 000c0a 00   A  0   0  1
  [ 6] .gnu.version      VERSYM          000020ce 0020ce 000192 02   A  4   0  2


={============================================================================
*kt_linux_core_406* linux-lib-shared-plt-hook 

https://github.com/namhyung/uftrace/wiki/Tutorial

Note that it showed the "puts" which is a library function you didn't write
(the same goes to "__monstartup" and "__cxa_atexit") as well as your own
functions in the program. Of course it doesn't show the internals of the
"puts" but it's good to see how long does the "puts" take. It uses a technique
called "PLT hooking" which redirects functions called from a
dynamically-linked program. 


// hook by modifying PLT entries
http://shadowwhowalks.blogspot.com/2013/01/android-hacking-hooking-system.html


//
https://github.com/kubo/plthook

Usage

If you have a library libfoo.so.1 and want to intercept a function call recv()
without modifying the library, put plthook.h and plthook_elf.c,
plthook_win32.c or plthook_osx.c in your source tree and add the following
code.


={============================================================================
*kt_linux_core_410* lib-shared: check libraries that process uses

cat /proc/NNNN/maps | awk '{print $6}' | grep '\.so' | sort | uniq

or

can use ldd.


={============================================================================
*kt_linux_core_411* lib-shared: case problem in open failure

/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so:     
  file format elf32-tradlittlemips

Dynamic Section:
  NEEDED               libnexusMgr.so.0 ~
  NEEDED               libnexus.so
  NEEDED               libgstbase-1.0.so.0
  NEEDED               libgstmpegts-1.0.so.0
  NEEDED               libgstreamer-1.0.so.0
  NEEDED               libgobject-2.0.so.0
  NEEDED               libglib-2.0.so.0
  NEEDED               libgcc_s.so.1
  NEEDED               libpthread.so.0
  NEEDED               libc.so.0
  SONAME               libgstnexus.so.0
  RPATH                /usr/local/lib
  RUNPATH              /usr/local/lib

From strace log:

open("/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so", O_RDONLY) = 60

// see how search path works

open("/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/opt/zinc-trunk/oss/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/opt/zinc-trunk/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/usr/local/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)

open("/opt/zinc-trunk/oss/lib/gstreamer-1.0/libnexusMgr.so.0", O_RDONLY) = -1
ENOENT (No such file or directory)

open("/opt/zinc-trunk/devel/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/opt/zinc-trunk/tests/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/usr/local/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/usr/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)

// this is error on strerr

write(2, "\n(w3cEngine:1641): GStreamer-WARN"..., 134
(w3cEngine:1641): GStreamer-WARNING **: 
  Failed to load plugin '/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so': 
  File not found
) = 134


note: Even though the real issue is that there is no Mgr.so which is the next
so, the error suggest that failed to open the starting so.


={============================================================================
*kt_linux_core_401* slib: --as-needed flag and link error

<manual>
https://sourceware.org/binutils/docs/ld/Options.html

--as-needed 
--no-as-needed

This option affects ELF DT_NEEDED tags for 'dynamic' libraries mentioned on the
command line 'after' the --as-needed option. 

<default>
Normally the linker will add a DT_NEEDED tag for each dynamic library mentioned
on the command line, 'regardless' of whether the library is actually needed or
not.

The --as-needed causes a DT_NEEDED tag to 'only' be emitted for a library that
"at that point in the link" satisfies a non-weak undefined symbol reference from
a regular object file or, if the library is not found in the DT_NEEDED lists of
other needed libraries, a non-weak undefined symbol reference from another
needed dynamic library. 

Object files or libraries appearing on the command line after the library in
question do not affect whether the library is seen as needed. This is similar to
the rules for extraction of object files from archives. --no-as-needed restores
the 'default' behaviour. 

<example>
$ gcc -c -fpic foo.c 
$ gcc -shared -o libfoo.so foo.o
$ gcc -o one main.c libfoo.so          <embedding-the-name-of-lib>

or 

$ gcc -L. -lfoo -o one main.c

$ nm libfoo.so | grep foo
(standard input):24:00000550 T foo

$ nm one | grep foo
(standard input):33:         U foo

$ readelf -d one | grep NEEDED
(standard input):4: 0x00000001 (NEEDED)   Shared library: [libfoo.so]
(standard input):5: 0x00000001 (NEEDED)   Shared library: [libc.so.6]


However, when tried:

$ gcc -Wl,--as-needed -L. -lfoo -o one main.c
/tmp/ccUshwwe.o: In function `main':
main.c:(.text+0x7): undefined reference to `foo'
collect2: error: ld returned 1 exit status

The followings are okay.

$ gcc -L. -lfoo -Wl,--as-needed -o one main.c
$ gcc -Wl,--as-needed -o one main.c libfoo.so

note: WHY this fails to link even if main really uses foo()?


<case>
There is an application which is said that it uses one shared library and this
library again uses the other shard library. 

The assumption is: application <- a.so <- b.so.

The problem happens when simply changes application source file to use functions
in a.so and statred to see "undefined symbols" of b.so from the linker. But
those symbols are not used in that application at all and the call that
applicaion uses don't have any reference to b.so as well. However other APIs of
a.so does have reference to b.so. Why is this?

When looked at application and library dependancies for okay case, there was no
reference to a.so library which has undefined symbols defined. Why was it okay
before?

Two problems:

1. Wrong assumption. For okay case, there was actually no reference between
application and a.so. That's why cannot see dependancies.

2. When build the new source, this code introduce a call to a.so and this call
has calls to the b.so. Now application need to know both a.so and b.so. Since
the application is the 'final' in the linking process and now see undefined
symbols which used in a.so but in the b.so. The problem is that b.so is dropped
to link of application due to as-needed option.

application          libMgr                  libFb
reference to Mgr  -> no reference to Fb
                     reference to Fb      ->


When fails:

-Wl,--as-needed -lMgr -lFb ...

/.../usr/local/lib/libMgr.so: undefined reference to `DirectFBSetOption' 
/.../usr/local/lib/libMgr.so: undefined reference to `DirectFBCreate' 
/.../usr/local/lib/libMgr.so: undefined reference to `DirectFBInit' 
  collect2: ld returned 1 exit status

When works:

-o application application.o
-lMgr -lFb ...
-Wl,--as-needed ...

So works fine when move --as-needed after necessary library.


<ex>

// main.c
#include <stdio.h>

extern void foo(void);

int main(void)
{
  // foo(5);
  foo();
  return 0;
}

// foo.c
#include <stdio.h>

extern void boo();

void foo()
{
  printf("foo: this is foo...\n");
  boo();
}

// boo.c
#include <stdio.h>

void boo()
{
  printf("boo: this is boo...\n");
}

$ gcc -c -fpic boo.c 
$ gcc -c -fpic foo.c
$ gcc -shared -o libfoo.so foo.o
$ gcc -shared -o libboo.so boo.o
$ gcc -o one main.c libfoo.so libboo.so

$ readelf -d one | ag NEEDED
 0x00000001 (NEEDED)                     Shared library: [libfoo.so]
 0x00000001 (NEEDED)                     Shared library: [libboo.so]
 0x00000001 (NEEDED)                     Shared library: [libc.so.6]

$ LD_LIBRARY_PATH=. ./one
foo: this is foo...
boo: this is boo...

$ gcc -o one main.c -Wl,--as-needed libfoo.so libboo.so
// same as $ gcc -o one main.c libfoo.so libboo.so

$ gcc -Wl,--as-needed libfoo.so libboo.so -o one main.c 
$ gcc -Wl,--as-needed -L. -lfoo -lboo -o one main.c 
/tmp/cc3ZjAgB.o: In function `main':
main.c:(.text+0x7): undefined reference to `foo'
collect2: error: ld returned 1 exit status

$ gcc -Wl,--as-needed -lfoo -lboo -o one main.c
/usr/bin/ld: cannot find -lfoo
/usr/bin/ld: cannot find -lboo
collect2: error: ld returned 1 exit status


<useful>
http://wiki.gentoo.org/wiki/Project:Quality_Assurance/As-needed#What_is_--as-needed.3F

What is --as-needed?

The --as-needed flag is passed to the GNU linker (GNU ld ). The flag tells the
linker to link in the produced binary only the libraries containing symbols
'actually' used by the binary itself. This binary can be either a final
executable or another library.

In theory, when linking something, only the needed libraries are passed to the
command line used to invoke the linker. But to workaround systems with broken
linkers or not using ELF format, many libraries declare some "dependencies" that
get pulled in while linking. A simple example can be found by looking at the
libraries declared as dependencies by gtk+ 2.0 :

libraries needed to link to gtk+ 2.0

$ pkg-config gtk+-2.0 --libs
-lgtk-x11-2.0 -lgdk-x11-2.0 -latk-1.0 -lgdk_pixbuf-2.0 -lm -lpangocairo-1.0 
-lpango-1.0 -lcairo -lgobject-2.0 -lgmodule-2.0 -ldl -lglib-2.0

If the application is just using functions from gtk+ 2.0, a simple link line
with -lgtk-x11-2.0 should make it build fine, but looking at which libraries are
needed and which are not from a package point of view is often an impossible
task. 

How can --as-needed be useful?

<improve-startup-time>
The use of the --as-needed flag allows the linker to avoid linking extra
libraries in a binary. This not only improves 'startup' times (as the loader does
    not have to load all the libraries for every step) but might avoid the full
initialization of things like KDE's KIO for a binary if it's not using the KIO
framework. 

More importantly, the use of --as-needed 'avoids' adding dependencies to a
binary that are prerequisites of one of its direct or indirect dependencies.
This is important because when a library changes SONAME after an ABI change, all
the binaries directly linking to it have to be 'rebuilt'. 

By linking only the libraries that are actually needed, the breakage due to an
ABI change is reduced. It is particularly useful when the ABI breakage happens
in a library used by some other high level library (like cairo , which is used
    directly by gtk+-2.0 , and gets linked indirectly in applications using the
    latter), as it prevents the rebuild of the final binaries and thus of the
packages carrying them.

It is also useful to check whether the dependencies stated by the documentation
are actually used by a package: it's not impossible that a package checks in a
configure script for some library, and then links to it, but without using it at
all because the code using it was removed or refactored or has not been written. 

<final-linking> only for executable but not library
Failure in final linking, undefined symbols

This is the most common error that happens while using --as-needed. It happens
during the final linking stage of an executable note: libraries don't create
problems, because they are allowed to have undefined symbols. The executable
linking stage dies because of an undefined symbol that is present in one of the
libraries fed to the command line. However, the library is not used by the
executable itself, thus it gets 'removed' by --as-needed.

This usually means that a library was not linked to another library, but was
using it, and then 'relying' on the final executable to link them together. This
behavior is also an extra encumbrance on developers using that library because
they have to check for the requirements.

The fix to this kind of problem is usually simple: just find which library
provides the symbols and which one is requiring them (the error message from the
    linker should contain the name of the latter). Then make sure that when the
library is linked from the source files it's also linked to the first. 


={============================================================================
*kt_linux_core_407* shared library: further information

Further information

Various information related to static and shared libraries can be found in the ar(1), gcc(1), ld(1),
ldconfig(8), ld.so(8), dlopen(3), and objdump(1) manual pages and in the info documentation for ld
  and readelf. [Drepper, 2004 (b)] covers many of the finer details of writing shared libraries on
  Linux. Further useful information can also be found in David Wheeler's Program Library HOWTO,
which is online at the LDP web site, http://www.tldp.org/. 

The GNU shared library scheme has many similarities to that implemented in Solaris, and therefore it
is worth reading Sun¿s Linker and Libraries Guide (available at http://docs.sun.com/) for further
information and examples. [Levine, 2000] provides an introduction to the operation of static and
dynamic linkers.

Information about GNU Libtool, a tool that shields the programmer from the implementation-specific
details of building shared libraries, can be found online at http://www.gnu.org/software/libtool and
in [Vaughan et al., 2000].

The document Executable and Linking Format, from the Tools Interface Standards committee, provides
details on ELF. This document can be found online at http://refspecs.freestandards.org/elf/elf.pdf.
[Lu, 1995] also provides a lot of useful detail on ELF.


={============================================================================
*kt_linux_core_408* shared library: md5sum

{can-use-on-library-to-check-integrity}
The idea is that can use md5sum on a library to confirm that it uses the exact same compile and link
options. The assumption is that if both party has the same build configuration, the library made
from the same source 'must' have the same md5 checksum. If that is the case, can use the md5
checksum as a quick way to check if both party has and uses the same build configuration.

Is it true?

https://gcc.gnu.org/ml/gcc-help/2010-01/msg00082.html
beaugy.a@free.fr wrote:

    Hi all,
    So, to put it in a nutshell, all my generated objects file are
    identical on dev1 and dev2 and object files contained in my
    convenience libraries are all identical. The only difference
    remaining, before I generate my binary, resides in the generated
    convenience libraries which are not identical, but their contents
    are. So AFAK, this slight difference shall not make the difference. So
    "why does gcc output (MD5 checksum) differs when I build a binary
    using the project object files (*.o) or the project convenience
    libraries (*.a)?" and "what can I do to fix that?".

Your *.o files are proceeded in a different order when on the command line and on the .a archive,
     putting symbols on different addresses, so obviously different binaries are produced.

Even if you do:
gcc 1.o 2.o
And:
gcc 2.o 1.o

you get binaries with different md5.

<example>
This is example from "*kt_dev_gcc_103* gcc link and ld"

$ cat simplefunc.c
int func(int i) {
    return i + 21;
}

$ cat simplemain.c
int func(int);

int main(int argc, const char* argv[])
{
    return func(argc);
}

$ gcc -c simplefunc.c
$ gcc -c simplemain.c
$ gcc simplefunc.o simplemain.o
$ ./a.out ; echo $?
22

:~/work$ nm simplefunc.o
00000000 T func

:~/work$ nm simplemain.o 
         U func
00000000 T main

$ ar r libsimplefunc.a simplefunc.o    // ar rs to skip ranlib command.
$ ranlib libsimplefunc.a
$ gcc simplemain.o -L. -lsimplefunc
$ ./a.out ; echo $?
22

Now we have *.o and *.a files and run the same build and ar command without changes in source. Just
make o and a files.

<o-files-are-the-same>
keitee@debian-keitee:~/work$ ll simplefunc.*
-rw-r--r-- 1 keitee keitee 848 Jan 20 22:15 simplefunc.o
-rw-r--r-- 1 keitee keitee 848 Jan 20 22:06 simplefunc.o.old
keitee@debian-keitee:~/work$ diff simplefunc.o simplefunc.o.old 
keitee@debian-keitee:~/work$ 

<a-files-are-different>
keitee@debian-keitee:~/work$ ll libsimplefunc.a*
-rw-r--r-- 1 keitee keitee 990 Jan 20 22:14 libsimplefunc.a
-rw-r--r-- 1 keitee keitee 990 Jan 20 22:14 libsimplefunc.a.old
keitee@debian-keitee:~/work$ diff libsimplefunc.a libsimplefunc.a.old 
Binary files libsimplefunc.a and libsimplefunc.a.old differ
keitee@debian-keitee:~/work$ 

The md5 checksum result shows the same.

keitee@debian-keitee:~/work$ md5sum simplefunc.o*
3acb728c611c4c936320d64c1b360633  simplefunc.o
3acb728c611c4c936320d64c1b360633  simplefunc.o.old

keitee@debian-keitee:~/work$ md5sum libsimplefunc.a*
87663beba78feef15106bf156a8f6ef3  libsimplefunc.a
02a369c6a4476eea407bec53beaa7b95  libsimplefunc.a.old

So even when make a library from the same object at 'different' time, it will create different
library file.

<Q> However, when do the md5sum on library files created from project build at different time, the
libraries are the same on diff and md5. WHY?

Tried to make a shared library with the same code:

gcc -fpic -shared -o simple.so simplefunc.c
ar rs libsimple.so simple.so

Again, simple.so files are the same but libsimple.so are differ. This means that when run ar, will
have different output files. Then how were the libraries the same for the above case?

<A> The answer is that do not use ar when make a shared library and then will the same md5sum.

-rw-r--r-- 1 kpark kpark 1528 Feb  5 10:02 foo.o
-rw-r--r-- 1 kpark kpark 1528 Feb  5 09:33 foo.o.old
-rwxr-xr-x 1 kpark kpark 6407 Feb  5 10:03 libfoo.so*
-rwxr-xr-x 1 kpark kpark 6407 Feb  5 09:33 libfoo.so.old*

$ diff foo.o foo.o.old 
$ diff libfoo.so libfoo.so.old 
$ md5sum foo.o*
1f87ad103b677a3090707fee9daaea33  foo.o
1f87ad103b677a3090707fee9daaea33  foo.o.old

$ md5sum libfoo.so*
e64fd5c673979f09360178f938e6e1b7  libfoo.so
e64fd5c673979f09360178f938e6e1b7  libfoo.so.old


={============================================================================
*kt_linux_core_411* slib: points to enhance performance

Again, reiterate pros and cons:

{downside-of-static}
1. Duplicates in disk and ram spce.

2. If a change is required perhaps a security or bug fix to an object module in
a static library, then all executables using that module must be relinked in
order to incorporate the change. This disadvantage is further compounded by the
fact that the system administrator needs to be aware of which applications were
linked against the library.


{what-is-shared}
Although the code of a shared library is shared among multiple processes, its
variables are not. Each process that uses the library has its own copies of the
global and static variables that are defined within the library.


{further-advantages}
o Because overall program size is smaller, in some cases, programs can be loaded
into memory and started more 'quickly'. This point holds true only for large
shared libraries that are already in use by another program.

o Such changes can be carried out even while running programs are using an
existing version of the shared library.


{cost-of-shared}
o Shared libraries are more 'complex' than static libraries, both at the
conceptual level, and at the practical level of creating shared libraries and
building the programs that use them.

o Shared libraries 'must' be compiled to use position-independent code, which
has a performance 'overhead' on most architectures because it requires the use
of an extra register 

o Symbol relocation must be performed at run time. During symbol relocation,
  references to each symbol (a variable or function) in a shared library need to
  be modified to correspond to the actual run-time location at which the symbol
  is placed in virtual memory. Take a little more time to execute.


{points-to-think}

From YV real cases:

1. Reordering LD_LIBRARY_PATH

Quote {
From the visualisation, its fairly obvious the there are many (failing) repeated
attempts to find common libraries in /opt/zinc/lib /opt/zinc/oss/lib, before
finding them in there actual location in /lib

In the "before" and "after" visualisations, you can see the significant (0.5)
improvement for reordering of LD_LIBRARY_PATH.
Quote }

To see what is the best LD_LIBRARY_PATH ordering, use strace to show all the
calls to open that succeed and all the calls that failed during the application
startup. 

It seems that reordering the LD_LIBRARY_PATH brings an improvement of 0.2s. The
precision of the profiling has to be taken into account: 0.2s is not much bigger
than the precision of 0.1s.


2. Reduce the code size

Moreover, #Including <zinc-common/logger.h> causes object code bloat. A symbol
for a boost::shared_ptr<LoggerCache> is emitted for every translation unit that
includes the header. This means that the symbol is duplicated over 800 times in
our stack, this means that the stack size is bigger and therefore that it will
take more time to load the shared libraries.

It seems that removing the logger brings an improvement of 0.25s. The precision
of the profiling has to be taken into account: 0.25s is not much bigger than the
precision of 0.10s.


3. Removing unnecessary library dependencies

In order to reduce the number of dependencies, John mentioned a linker option
that allows to keep the dependencies that are really needed: --as-needed.
Normally, it would be as easy as adding this option to the global LDFLAGS and
recompiling the stack: export LDFLAGS="-Wl,--as-needed ${LDFLAGS}"

dynamic libraries mentioned on the command line after the --as-needed option
Unfortunately, the --as-needed option will only apply to the dynamic libraries
that appears after it.  And unfortunately, autotools reorders the parameters and
the --as-needed ends up at the end of the compilation parameters.

Fortunately, we are not the only one using autotools and there are some patches
for fixing that: See

Why –as–needed doesn't work as expected for your libraries on your autotools
project

http://sigquit.wordpress.com/2011/02/16/why-asneeded-doesnt-work-as-expected-for-your-libraries-on-your-autotools-project/

for more details and for links to patches.

Another issue is that this might require some Makefile changes for projects that
forget to list some dependencies and that happened to get them by transitive
dependencies.

We can finally see that this change brings an improvement of 0.32s. We have to
keep in mind that in this case, the box is not busy at all. Therefore, the gain
of 0.32s could be much higher in production when the performance are IO bound.


4. Reducing exported symbols

By applying the flag -fvisibility=hidden

This causes an improvement of 0.4s with sandboxing off and a bit less with
sandboxing on (no idea why?). This is a considerable improvement.

note: This figure comes from a situation when FLASH application loads lots of
libraries from the mw stack.

5. Removing script

As seen previously, a lot of time is spent in Python. It also seems to be a
reasonable idea to remove the python scripts from the launch chain as they did
not have a real justification.



={============================================================================
*kt_linux_core_412* slib: as-needed and _GLOBAL_OFFSET_TABLE_

The case is that shared so has liked with another shared library as:

mipsel-linux-gcc -shared sqlite3.o -Wl,--as-needed 
  -Wl,-soname -Wl,libsqlite3.so.0 
  -o .libs/libsqlite3.so

note: no -shared or -fPIC used to make the second shared so.

mipsel-linux-gcc -Wl,--as-needed 
-o .libs/sqlite3 shell.o ./.libs/libsqlite3.so -ldl -lpthread

$ readelf -d sqlite3

 0x00000001 (NEEDED)                     Shared library: [libsqlite3.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]

The problem is that on certain target, for the same package, the link to make
the second so fails with link error:

./.libs/libsqlite3.so:(.got+0x0): multiple definition of `_GLOBAL_OFFSET_TABLE_'
collect2: ld returned 1 exit status
make: *** [xxx] Error 1


Q1: Founds that links works okay without as-needed. Why?
Q2: Why the same commands does work on the other target?

A: The reason for that is that 'ld' seems to have a bug:
                       
https://sourceware.org/ml/binutils/2013-02/msg00159.html

Re: binutils 2.19.92 linker broke with --as-needed flag

On Mon, Feb 04, 2013 at 05:58:37PM -0800, Vincent Wen wrote:
> The linker broke when as-need flag is added.
> 
>  /bin/sh ../libtool --tag=CC --mode=link mipsel-linux-gcc -I../include -g
> -O2 -Wl,--as-needed -o test-example test.o ../lib/libTestGcc.la
> libtool: link: mipsel-linux-gcc -I../include -g -O2 -Wl,--as-needed -o
> .libs/test-example test.o ../lib/.libs/libTestGcc.so -Wl,-rpath
> -Wl,/usr/local/lib
> ../lib/.libs/libTestGcc.so:(.got+0x0): multiple definition of
> `_GLOBAL_OFFSET_TABLE_'

I think mips was broken before --as-needed.  mips seems to want a
dynamic _GLOBAL_OFFSET_TABLE_ symbol in shared libs, presumably for
use by ld.so.  However, a global symbol will be resolved by ld.so
according to the ELF rules which will result in the symbol being
resolved to the first definition seen in a breadth first search of the
application and its shared libraries.  That means the value in the
first shared lib.  So the value seen in other shared libs is wrong.
_GLOBAL_OFFSET_TABLE_ must resolve locally.

So _GLOBAL_OFFSET_TABLE_ must at least be STV_PROTECTED, and could be
STV_HIDDEN as you do in your patch.

> --- a/bfd/elfxx-mips.c    2013-02-01 03:26:00.000000000 -0800
> +++ b/bfd/elfxx-mips.c    2013-02-01 03:26:16.000000000 -0800
> @@ -4681,6 +4681,7 @@
>    h->non_elf = 0;
>    h->def_regular = 1;
>    h->type = STT_OBJECT;
> +  h->other = STV_HIDDEN;
>    elf_hash_table (info)->hgot = h;
> 
>    if (info->shared

The trouble with this is that making it STV_HIDDEN results in no
dynamic _GLOBAL_OFFSET_TABLE_ symbol at all, due to the following code
in bfd_elf_link_record_dynamic_symbol.

      /* XXX: The ABI draft says the linker must turn hidden and
	 internal symbols into STB_LOCAL symbols when producing the
	 DSO. However, if ld.so honors st_other in the dynamic table,
	 this would not be necessary.  */
      switch (ELF_ST_VISIBILITY (h->other))
	{
	case STV_INTERNAL:
	case STV_HIDDEN:
	  if (h->root.type != bfd_link_hash_undefined
	      && h->root.type != bfd_link_hash_undefweak)
	    {
	      h->forced_local = 1;
	      if (!elf_hash_table (info)->is_relocatable_executable)
		return TRUE;
	    }

Now that code is also wrong, I think.  The "return TRUE" should never
happen.

-- 
Alan Modra
Australia Development Lab, IBM


Found that:

GNU ld (GNU Binutils for Debian) 2.22   : Link fails on mips
GNU ld (GNU Binutils) 2.19.1            : Likk okay on mips


={============================================================================
*kt_linux_core_500* sandbox

There are occasional situations where static libraries may be appropriate. If the program is to be
run in an environment (perhaps a chroot jail, for example) where shared libraries are unavailable.


={============================================================================
*kt_linux_core_600* linux-time

LPI 10 Time

Most computer architectures have a built-in `hardware clock` that enables the
kernel to measure real and process time.

{two-kinds}
Within a program, we may be interested in two kinds of time

* Real time: This is the time as measured either from some standard point
(`calendar time`) or from some fixed point (typically the start) in the life
of a process (elapsed or `wall clock time`). Obtaining the calendar time is
useful to programs that, for example, timestamp database records or files.
Measuring elapsed time is useful for a program that takes periodic actions or
makes regular measurements from some external input device.

* Process time: This is the amount of CPU time used by a process. Measuring
process time is useful for checking or optimizing the performance of a program
or algorithm.


{calender-time}
Regardless of geographic location, UNIX systems represent time internally as a
measure of `seconds since the Epoch`; that is, since midnight on the morning
of 1 January 1970, Universal Coordinated Time 

(`UTC`, previously known as Greenwich Mean Time, or GMT). 

This is approximately the date when the UNIX system came into being. Calendar
time is stored in variables of type `time_t`, an integer type specified by
SUSv3.

note: UTC == GMT


{time-t-type} *struct-timet*
On 32-bit Linux systems, `time_t, which is a signed integer`, can represent
dates in the range 13 December 1901 20:45:52 to 19 January 2038 03:14:07.
SUSv3 leaves the meaning of negative time_t values unspecified. 

Many current 32-bit UNIX systems face a theoretical Year 2038 problem, which
they may encounter before 2038, if they do calculations based on dates in the
future. This problem will be significantly alleviated by the fact that by
2038, probably all UNIX systems will have long become 64-bit and beyond.
However, 32-bit embedded systems, which typically have a much longer lifespan
than desktop hardware, may still be afflicted by the problem. Furthermore, the
problem will remain for any legacy data and applications that maintain time in
a 32-bit time_t format.

note: Appears that 32 bit also has 64 bits time_t? This is taken from debian
32 bits in VM.

Linux debian-keitee 3.2.0-4-486 #1 Debian 3.2.63-2+deb7u2 i686 GNU/Linux

__STD_TYPE __TIME_T_TYPE __time_t;	/* Seconds since the Epoch.  */

__TIME_T_TYPE      50 i386-linux-gnu/bits/typesizes.h #define __TIME_T_TYPE	__SLONGWORD_TYPE

__SLONGWORD_TYPE  103 i386-linux-gnu/bits/types.h #define __SLONGWORD_TYPE	long int


{gettimeofday-time}
The gettimeofday() system call returns the `calendar time` in the buffer
pointed to by tv.

#include <sys/time.h>

int gettimeofday(struct timeval *tv, struct timezone *tz);

Returns 0 on success, or -1 on error

*time-timeval*

struct timeval {
    time_t tv_sec;         /* Seconds since 00:00:00, 1 Jan 1970 UTC */
    suseconds_t tv_usec;   /* Additional `microseconds` (long int) */
};

The tz argument to gettimeofday() is a historical artifact. This argument is
now obsolete and should always be specified as NULL.


<time-call>
time() system call returns the number of seconds since the Epoch. i.e., the
same value that gettimeofday() returns in the tv_sec field of its tv argument.

#include <time.h>

time_t time(time_t *timep);

Returns number of seconds since the Epoch,or (time_t) -1 on error

If the timep argument is not NULL, the number of seconds since the Epoch is
also placed in the location to which timep points.

Since time() returns the same value in two ways, we often simply use the
following call without error checking:

t = time(NULL);

note:
The existence of time() as a system call is historical and now redundant; it
could be implemented as a library function that calls gettimeofday().


={============================================================================
*kt_linux_core_601* linux-time-conversion

Shows the functions used to convert between `time_t` values and other time
formats, including printable representations. These functions shield us from
the complexity brought to such conversions by timezones, daylight saving time
(DST) regimes, and localization issues.

* affected by TZ env variable
+ affected by locale 

Kernel   -> time()               time_t               -> ctime()*    fixed-format string
         <- stime()                                                  Tue Feb 1 21:39:46 2011\n\0

         -> gettimeofday()       struct timeval
         <- settimeofday()

         -> gmtime()             struct tm            -> asctime()
         -> localtime()*         (broken down time)
         <- mktime()*            struct tm            -> strftime()*+   user-formatted,
                                                      <- strptime()+    localized string


{to-printable-form} time-ctime
The ctime() function provides a simple method of converting a time_t value
`into printable form.` The ctime() function automatically accounts for local
timezone and DST settings when performing the conversion.

The returned string is statically allocated; future calls to ctime() will
overwrite it.

#include <time.h>

char *ctime(const time_t *timep);

Returns pointer to statically allocated string `terminated by newline` and \0 on
success, or NULL on error


<statically-allocated>
SUSv3 states that calls to any of the functions ctime(), gmtime(),
      localtime(), or asctime() may overwrite the statically allocated
      structure that is returned by any of the other functions. In other
      words, these functions may share single copies of the returned character
      array and tm structure, and this is done in some versions of glibc. If
      we need to maintain the returned information across multiple calls to
      these functions, we must save local copies.


{to-broken-down-from}
The gmtime() and localtime() functions convert a time_t value into a so-called
brokendown time. The broken-down time is placed in a statically allocated
structure whose address is returned as the function result.

#include <time.h>

struct tm *gmtime(const time_t *timep);
struct tm *localtime(const time_t *timep);

Both return a pointer to a statically allocated broken-down time structure on
success, or NULL on error

Unlike gmtime(), localtime() takes into account timezone and DST settings to
return a broken-down time corresponding to the system's local time.

*struct-tm*

struct tm {
    int tm_sec;      /* Seconds (0-60) */
    int tm_min;      /* Minutes (0-59) */
    int tm_hour;     /* Hours (0-23) */
    int tm_mday;     /* Day of the month (1-31) */
    int tm_mon;      /* Month (0-11) */
    int tm_year;     /* Year since 1900 */   note: 1900 but not 1970
    int tm_wday;     /* Day of the week (Sunday = 0)*/
    int tm_yday;     /* Day in the year (0-365; 1 Jan = 0)*/
    int tm_isdst;    /* Daylight saving time flag
                     > 0: DST is in effect;
                     = 0: DST is not effect;
                     < 0: DST information not available */
};

note:
When using the gmtime function, be sure to check its return value. If the
computer running the code doesn’t have a local time zone defined, the gmtime
function will be unable to compute the UTC time, and will return 0. If you
pass 0 to the asctime function, undefined behavior will result.


{between-broken-down-and-printable-form}
functions that convert a broken-down time to printable form, and vice versa.

<no-control-format>
#include <time.h>

char *asctime(const struct tm *timeptr);

Returns pointer to statically allocated string terminated by newline and \0 on
success, or NULL on error

  *to-get-utc*
  By contrast with ctime(), local timezone settings have no effect on
  asctime(), since it is converting a broken-down time that is typically
  either already localized via localtime() or in UTC as returned by gmtime().
  As with ctime(), we have no control over the format of the string produced
  by asctime().


<to-control-format>
The strftime() function provides us with more precise control when converting
a broken-down time into printable form. Given a broken-down time pointed to by
timeptr, strftime() places a corresponding null-terminated, date-plus-time
string in the buffer pointed to by outstr.

The string returned in outstr is formatted according to the specification in
format.

#include <time.h>

size_t strftime(char *outstr, size_t maxsize, const char *format, const struct
    tm *timeptr);

Returns number of bytes placed in outstr (excluding terminating null byte) on
success, or 0 on error


<ex>
// The local date and time is: Tue Jun 12 12:49:12 2018
// The local date and time is: Tue Jun 12 12:49:12 2018
// The UTC date and time is: Tue Jun 12 11:49:12 2018

TEST(Time, UseConventionalWay)
{
  // time_t now = time(0);
  auto now = time(0);

  cout << "The local date and time is: " << ctime(&now) << endl;

  // tm *localtm = localtime(&now);
  auto localtm = localtime(&now);
  cout << "The local date and time is: " << asctime(localtm) << endl;

  // tm *gmtm = gmtime(&now);
  auto gmtm = gmtime(&now);
  if (gmtm != nullptr)
  {
    cout << "The UTC date and time is: " << asctime(gmtm) << endl;
  }
}


#include <time.h>
#include "curr_time.h" /* Declares function defined here */

#define BUF_SIZE 1000

// Return a string containing the current time formatted according to the
// specification in 'format' (see strftime(3) for specifiers). If 'format' is
// NULL, we use "%c" as a specifier (which gives the date and time as for
// ctime(3), but without the trailing newline). Returns NULL on error.
//
// %c Date and time                       Tue Feb 1 21:39:46 2011
// %T Time (same as %H:%M:%S)             21:39:46

char * currTime(const char *format)
{
  static char buf[BUF_SIZE]; /* Nonreentrant */
  time_t t;
  size_t s;
  struct tm *tm;
  t = time(NULL);
  tm = localtime(&t);
  if (tm == NULL)
    return NULL;
  s = strftime(buf, BUF_SIZE, (format != NULL) ? format : "%c", tm);
  return (s == 0) ? NULL : buf;
}

fprintf( stdout, "time: %s\n", currTime(NULL));
fprintf( stdout, "time: %s\n", currTime("%T"));

$ ./a.out 
time: Fri May  8 00:55:29 2015
time: 00:55:29


{from-input-to-time-to-print}


={============================================================================
*kt_linux_core_602* linux-time-resolution jiffies

10-9  1 nanosecond   ns    one billionth of one second
10-6  1 microsecond  us    one millionth of one second
10-3  1 millisecond  ms    one thousandth of one second

10.6 The Software Clock (Jiffies)

The accuracy of various time-related system calls described in this book is
limited to the resolution of the system `software clock`, which measures time in
units called jiffies.

The size of a jiffy is defined by `the constant HZ within the kernel source code.`
This is the unit in which the kernel allocates the CPU to processes under the
roundrobin time-sharing scheduling algorithm.

Because CPU speeds have greatly increased since Linux was first implemented, in
kernel 2.6.0, the rate of the software clock was raised to 1000 hertz on Linux/
x86-32. The advantages of a higher software clock rate are that timers can
operate with greater accuracy and time measurements can be made with greater
precision.

However, it isn't desirable to set the clock rate to arbitrarily high values,
because each clock interrupt consumes a small amount of CPU time, which is time
    that the CPU can't spend executing processes.

Debate among kernel developers eventually resulted in the software clock rate
becoming a configurable kernel option (under Processor type and features, Timer
        frequency). Since kernel 2.6.13, the clock rate can be set to 100, 250
(the default), or 1000 HZ, giving jiffy values of 10, 4, and 1 milliseconds,
respectively.


={============================================================================
*kt_linux_core_602* linux-time-process-time

10.7 Process Time

Process time is the amount of CPU time used by a process since it was created.
For recording purposes, the kernel separates CPU time into the following two
components:

User CPU time is the amount of time spent executing in user mode. Sometimes
referred to as virtual time, this is the time that it appears to the program
that it has access to the CPU.

System CPU time is amount of time spent executing in kernel mode. This is the
time that the kernel spends executing system calls or performing other tasks
on behalf of the program (e.g., servicing page faults).

The times() system call retrieves process time information, returning it in
the structure pointed to by buf.

#include <sys/times.h>
clock_t times(struct tms *buf);

Returns number of clock ticks (sysconf(_SC_CLK_TCK)) since “arbitrary” time in
  past on success, or (clock_t) –1 on error

  struct tms {
    clock_t tms_utime; /* User CPU time used by caller */
    clock_t tms_stime; /* System CPU time used by caller */
    clock_t tms_cutime; /* User CPU time of all (waited for) children */
    clock_t tms_cstime; /* System CPU time of all (waited for) children */
  };

The `clock_t` data type used to type the four fields of the tms structure is
  an integer type that measures time `in units called clock ticks.` We can
  call sysconf(_SC_CLK_TCK) to obtain the number of clock ticks per second,
       and then divide a clock_t value by this number `to convert to seconds.`
         (We describe sysconf() in Section 11.2.)

On success, times() returns the elapsed (real) time in clock ticks since some
arbitrary point in the past. SUSv3 deliberately does not specify what this
point is, merely stating that it will be constant during the life of the
calling process. Therefore, the only portable use of this return value is to
measure elapsed time in the execution of the process by calculating the
difference in the value returned by pairs of times() calls. 

However, even for this use, the return value of times() is unreliable, since
it can overflow the range of clock_t, at which point the value would cycle to
start again at 0 (i.e., a later times() call could return a number that is
    lower than an earlier times() call). 

  The reliable way to measure the passage of elapsed time is to use
  gettimeofday() (described in Section 10.1).


*clock-call*
The `clock()` function provides a simpler interface for retrieving the process
time. It returns a single value that measures the total (i.e., user plus
    system) CPU time used by the calling process.

#include <time.h>
clock_t clock(void);

Returns total CPU time used by calling process measured in CLOCKS_PER_SEC, or
  (clock_t) –1 on error

The value returned by clock() is measured in units of CLOCKS_PER_SEC, so we
must divide by this value to arrive at the number of seconds of CPU time used
by the process. CLOCKS_PER_SEC is fixed at 1 million by POSIX.1, regardless of
the resolution of the underlying software clock (Section 10.6). The accuracy
of clock() is nevertheless limited to the resolution of the software clock.

note:
CLOCKS_PER_SEC is 1e+06 on VM which means microseconds on this system


{tool-time}

~/tizencore$ time -f "%E real,%U user,%S sys" ls -Fs
-f: command not found

real	0m0.143s
user	0m0.068s
sys	0m0.040s

$ /usr/bin/time -f "%E real,%U user,%S sys" ls -Fs
total 24
4 app-core/  4 appfw/  4 application/  4 app-service/  4 dlog/	4 README
0:00.00 real,0.00 user,0.00 sys

Users of the bash shell need to use an explicit path in order to run the
external time command and not the shell builtin variant. On system where time is
installed in /usr/bin, the first example would become /usr/bin/time wc
/etc/hosts


={============================================================================
*kt_linux_core_603* linux-time-timer

LPI 23.2 Scheduling and Accuracy of Timers

High-resolution timers

On modern Linux kernels, the preceding statement that timer resolution is
limited by the frequency of the software clock no longer holds true. Since
kernel 2.6.21, Linux optionally supports high-resolution timers. If this
support is enabled (via the CONFIG_HIGH_RES_TIMERS kernel configuration
    option), then the accuracy of the various timer and sleep interfaces that
we describe in this chapter is no longer constrained by the size of the kernel
jiffy. Instead, these calls can be as accurate as the underlying hardware
allows. On modern hardware, accuracy down to a microsecond is typical.

The availability of high-resolution timers can be determined by examining the
clock resolution returned by clock_getres(), described in Section 23.5.1.


23.4 Suspending Execution for a Fixed Interval (Sleeping)

Sometimes, we want to suspend execution of a process for a fixed amount of
time. While it is possible to do this using a combination of sigsuspend() and
the timer functions already described, it is easier to use one of the sleep
functions instead.

#include <unistd.h>
unsigned int sleep(unsigned int seconds);

Returns 0 on normal completion, or number of unslept seconds if prematurely
  terminated

SUSv3 leaves possible interactions of sleep() with alarm() and setitimer()
unspecified. On Linux, sleep() is implemented as a call to nanosleep()
(Section 23.4.2), with the consequence that there is no interaction between
sleep() and the timer functions.  However, on many implementations, especially
older ones, sleep() is implemented using alarm() and a handler for the SIGALRM
signal. For portability, we should avoid mixing the use of sleep() with
alarm() and setitimer().


23.4.2 High-Resolution Sleeping: nanosleep()

The nanosleep() function performs a similar task to sleep(), but provides a
number of advantages, including finer resolution when specifying the sleep
interval.

#define _POSIX_C_SOURCE 199309
#include <time.h>
int nanosleep(const struct timespec *request, struct timespec *remain);

Returns 0 on successfully completed sleep, or 1 on error or interrupted sleep

*struct-timespec*

  struct timespec {
    time_t tv_sec;  /* Seconds */
    long tv_nsec;   /* Nanoseconds */
  };

A further advantage of nanosleep() is that SUSv3 explicitly specifies that it
  should not be implemented using signals. This means that, unlike the
  situation with sleep(), we can portably mix calls to nanosleep() with calls
  to alarm() or setitimer().  Although it is not implemented using signals,
     nanosleep() may still be interrupted by a signal handler.


23.5.4 Improved High-Resolution Sleeping: clock_nanosleep()

Like nanosleep(), the Linux-specific clock_nanosleep() system call suspends
the calling process until either a specified interval of time has passed or a
signal arrives. In this section, we describe the features that distinguish
clock_nanosleep() from nanosleep().

By default (i.e., if flags is 0), the sleep interval specified in request is
relative (like nanosleep()). However, if we specify TIMER_ABSTIME in flags
(see the example in Listing 23-4), then request specifies an absolute time as
measured by the clock identified by clockid. This feature is essential in
applications that need `to sleep accurately` until a specific time. If we
instead try retrieving the current time, calculating the difference until the
desired target time, and doing a relative sleep, then there is a possibility
that the process may be preempted in the middle of these steps, and
consequently sleep for longer than desired.

As described in Section 23.4.2, this "oversleeping" problem is particularly
marked for a process that uses a loop to restart a sleep that is interrupted
by a signal handler. If signals are delivered at a high rate, then a relative
sleep (of the type performed by nanosleep()) can lead to large inaccuracies in
the time a process spends sleeping. We can avoid the oversleeping problem by
making an initial call to clock_gettime() to retrieve the time, adding the
desired amount to that time, and then calling clock_nanosleep() with the
TIMER_ABSTIME flag (and restarting the system call if it is interrupted by a
    signal handler).


={============================================================================
*kt_linux_core_603* linux-time-realtime

LPI-23.5 POSIX Clocks

POSIX clocks (originally defined in POSIX.1b) provide an API for accessing
clocks that measure time with `nanosecond precision` Nanosecond time values
are represented using the same `timespec structure`

The main system calls in the POSIX clocks API are `clock_gettime()`, which
retrieves the current value of a clock; clock_getres(), which returns the
resolution of a clock; and clock_settime(), which updates a clock.

note:
On Linux, programs using this API must be compiled with the `-lrt` option, in
order to link against the `librt (realtime) library.`


{call-clock-gettime}
The time value is returned in the timespec structure pointed to by tp.

Although the timespec structure affords nanosecond precision, the granularity
of the time value returned by clock_gettime() may be coarser than this. The
clock_getres() system call returns a pointer to a timespec structure
containing the resolution of the clock specified in clockid.

#define _POSIX_C_SOURCE 199309
#include <time.h>

int clock_gettime(clockid_t clockid, struct timespec *tp);
int clock_getres(clockid_t clockid, struct timespec *res);

Both return 0 on success, or -1 on error

<clockid>
The clockid_t data type is a type specified by SUSv3 for representing a clock
identifier and supports for POSIX.1b timers, as defined in
include/linux/time.h, are:

CLOCK_REALTIME                Settable system-wide real-time clock

The CLOCK_REALTIME clock is a system-wide clock that measures wall-clock time.

By contrast with the CLOCK_MONOTONIC clock, the setting of this clock can be
changed.

CLOCK_MONOTONIC               Nonsettable monotonic clock

SUSv3 specifies that the CLOCK_MONOTONIC clock measures time since some
“unspecified point in the past” that doesn’t change after system startup. This
clock is useful for applications that must not be affected by discontinuous
changes to the system clock (e.g., a manual change to the system time). 
`On Linux, this clock measures the time since system startup.`

CLOCK_PROCESS_CPUTIME_ID      Per-process CPU-time clock (since Linux 2.6.12)
CLOCK_THREAD_CPUTIME_ID       Per-thread CPU-time clock (since Linux 2.6.12)

The CLOCK_PROCESS_CPUTIME_ID clock measures the user and system CPU time
consumed by the calling process. The CLOCK_THREAD_CPUTIME_ID clock performs
the analogous task for an individual thread within a process.

note:
but `only CLOCK_REALTIME is mandatory` and widely supported on UNIX
implementations.

<linux>
Linux 2.6.28 adds a new clock type, CLOCK_MONOTONIC_RAW, to those listed in
Table 23-1. This is a nonsettable clock that is similar to CLOCK_MONOTONIC,
      but it gives access to a pure hardware-based time that is unaffected by
      NTP adjustments.  This nonstandard clock is intended for use in
      specialized clocksynchronization applications.

Linux 2.6.32 adds two more new clocks to those listed in Table 23-1:
CLOCK_REALTIME_COARSE and CLOCK_MONOTIC_COARSE. These clocks are similar to
CLOCK_REALTIME and CLOCK_MONOTONIC, but intended for applications that want to
obtain lower-resolution timestamps at minimal cost. These nonstandard clocks
don't cause any access to the hardware clock (which can be expensive for some
    hardware clock sources), and the resolution of the returned value is the
jiffy (Section 10.6).


={============================================================================
*kt_linux_core_604* linux-time-ns-stamp

typedef uint64_t u64;

// return nano, 10-9, secs

static u64 nsec() {
  struct timeval tv;
  if(gettimeofday(&tv, 0) < 0)
    return -1;
  return (u64)tv.tv_sec*1000*1000*1000 + tv.tv_usec*1000;
}

int main()
{
  be = nsec();
  sort(vec_byval.begin(), vec_byval.end(), compare_by_value);
  af = nsec();
  cout << "by value: diff(ns): " << af-be << endl;
  cout << "by value: diff(ms): " << (af-be)/(1000) << endl;
}


={============================================================================
*kt_linux_core_604* linux-time-ms-stamp

If clock has 1024 HZ resolution, then

1/1024 = 0.000.9765625 (the result is 976562.5 nanoseconds.)
1/1000 = 0.001.
1/2048 = 0.000.48828125

The unit is wider or shorter depending on a resolution.

*struct-timespec*

struct timespec {
  time_t   tv_sec;        /* seconds */
  long     tv_nsec;       /* `nano` seconds */
};

The CLOCK_REALTIME clock measures the amount of time that has elapsed since
epoch. note: since uses time_t. The tv_nsec field specifies a nanoseconds
value. It must be a number in the range 0 to 999,999,999.


To get nanos in total

(tv_sec * 10+9) + tv_nsec nanoseconds

and the following gets time in ms.

1 ms = 1 sec * 10+3        // multiply
1 ms = 1 nsec * 10-6       // devide

So this is to get time in ms and us.

static uint32_t get_time_ms()
{
    struct timespec ts = {0, 0};
    clock_gettime(CLOCK_REALTIME, &ts);
    return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000));
}

static uint32_t time_get_us()
{
    struct	 timespec ts;
    xclock_gettime(CLOCK_REALTIME, &ts);
    return (uint32_t)((ts.tv_sec * 1000000) + (ts.tv_nsec / 10000));
}


<timpstamp-in-ms-example>
1. Since start from 0, this makes timestamp in HH:MM:SS:MS since started but not shows days.
2. Do math as below. So if wants to have timestamps with years and days then use other way.

hour = (thi)/3600000; 				# hour in ms
msec = (thi - (hour*3600000));	# ms remains 
minute = msec / 60000; 				# mins 
msec = msec - (minute * 60000);	# ms remains
sec = msec / 1000; 					# secs
msec = msec - (sec * 1000); 		# ms remains

/*
 * time in MIPS
 */
#include <stdio.h>
#include <unistd.h>
#include <linux/unistd.h>
#include <errno.h> 
#include <stdlib.h>
#include <string.h>
#include <time.h>

typedef unsigned int uint32_t;

// note: starts from 0
static uint32_t tstart = 0;

#if __mips__ /* optimization for mips */
static inline void xclock_gettime(unsigned int which_clock, struct timeval * tv);

#define _syscall_clock_gettimeX(type,name,atype,a,btype,b) \
type x##name(atype a, btype b) \
{ \
    register unsigned long __a0 asm("$4") = (unsigned long) a; \
    register unsigned long __a1 asm("$5") = (unsigned long) b; \
    register unsigned long __a3 asm("$7"); \
    unsigned long __v0; \
    \
    __asm__ volatile ( \
            ".set\tnoreorder\n\t" \
            "li\t$2, %4\t\t\t# " #name "\n\t" \
            "syscall\n\t" \
            "move\t%0, $2\n\t" \
            ".set\treorder" \
            : "=&r" (__v0), "=r" (__a3) \
            : "r" (__a0), "r" (__a1), "i" (__NR_##name) \
            : "$2", "$8", "$9", "$10", "$11", "$12", "$13", "$14", "$15", "$24", \
            "memory"); \
}

_syscall_clock_gettimeX(void, clock_gettime, unsigned int, which_clock, struct timeval *, tv);

#endif

static uint32_t time_get(void)
{
    struct timespec ts;
#if __mips__
    xclock_gettime(CLOCK_REALTIME, &ts);
    printf("call xclock_getttime: ts.tv_sec = %ld, ts.tv_nsec = %ld\n", ts.tv_sec, ts.tv_nsec);
#else
    clock_gettime(CLOCK_REALTIME, &ts);
#endif
    return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000)) - tstart;
}

void main(int argc, char* agrv[]) 
{
    int hour = 0, minute = 0, sec = 0, msec = 0;
    uint32_t tlo =0, thi = 0;

    tstart = time_get();

    thi = time_get();
    hour = (thi)/3600000;
    msec = (thi - (hour*3600000));
    minute = msec / 60000;
    msec = msec - (minute * 60000);
    sec = msec / 1000;
    msec = msec - (sec * 1000);
    printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

    sleep(2);

    thi = time_get();
    hour = (thi)/3600000;
    msec = (thi - (hour*3600000));
    minute = msec / 60000;
    msec = msec - (minute * 60000);
    sec = msec / 1000;
    msec = msec - (sec * 1000);
    printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

    return;
}

$ ./a.out 
call xclock_getttime: ts.tv_sec = 946693753, ts.tv_nsec = 932727000
call xclock_getttime: ts.tv_sec = 946693753, ts.tv_nsec = 932829000
thi:   0, time: 000:00:00.000

call xclock_getttime: ts.tv_sec = 946693755, ts.tv_nsec = 933026000
thi:2001, time: 000:00:02.001


#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <time.h>

typedef unsigned int uint32_t;

static uint32_t tstart = 0;

static time_get(void)
{
    struct timespec ts;
    clock_gettime(CLOCK_REALTIME, &ts);
    return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000)) - tstart;
}

static char* time_stamp(void)
{
    uint32_t tdiff = time_get();

    static char buf[100];
    int hour = 0, minute = 0, sec = 0, msec = 0;
    hour = (tdiff)/3600000;
    msec = (tdiff - (hour*3600000));
    minute = msec / 60000;
    msec = msec - (minute * 60000);
    sec = msec / 1000;
    msec = msec - (sec * 1000);
    sprintf( buf, "%.2d:%.2d:%.2d.%.3d", hour, minute, sec, msec);

    return buf;
}

int main(int argc, char** argv)
{
    tstart = time_get();

    fprintf( stdout, "time: %s\n", time_stamp() );

    int i, j;
    for(i = 0; i < 1000000000; i++)
    {
        j = 30;
        j = j++ % 10;
    }

    fprintf( stdout, "time: %s\n", time_stamp() );

    exit(EXIT_SUCCESS);
}


$ ./a.out 
time: 00:00:00.000
time: 00:00:04.648


={============================================================================
*kt_linux_core_604* linux-time-us-stamp

<ex>
// perfcounter.h

#ifndef PERF_COUNTER_H_
#define PERF_COUNTER_H_

#include <string>

using std::string;

class PerfCounter
{
    public:
        PerfCounter();
        ~PerfCounter();

        void snap(const string &name_);
        void dump();

    private:
        // *cxx-nested-class*
        // since the original code uses class but makes it all public, use
        // struct instead.
        struct CounterData
        {
            CounterData(const string &name_) : name{name_}, pnext{nullptr} {}
            CounterData *pnext;
            struct timespec ts;
            string name;
        };

        CounterData *phead;
        CounterData *pend;

        // utility function
        CounterData *createSnap(const string &name_);
};

#endif // PERF_COUNTER_H_

// perfcounter.cpp

#include "perfcounter.h"
#include <iostream>     // std::cout
#include <sstream>
#include <cstdint>      // for uint64_t
#include <ctime>        // for clock_gettime

// g++ -std=c++11 perfcounter.cpp main.cpp

PerfCounter::PerfCounter()
{
    // create a start node
    phead = pend = createSnap("start");

    // phead = pend = new CounterData("start");
    // clock_gettime(CLOCK_MONOTONIC, &(phead->ts));
}

PerfCounter::~PerfCounter()
{
    // clean up a list
    for (CounterData *psnap = phead; psnap;)
    {
        phead = psnap->pnext;
        std::cout << "delete " << psnap->name << std::endl;
        delete psnap;
        psnap = phead;
    }
}

void PerfCounter::snap(const string &name_)
{
    CounterData *psnap = createSnap(name_);

    // CounterData *psnap = new CounterData(name_);
    // clock_gettime(CLOCK_MONOTONIC, &(psnap->ts));

    pend->pnext = psnap;
    pend = psnap;
}

// utility function to have common code in one place
// note that use of cpp-nested-class type, PerfCounter::CounterData. Otherwise,
// see compile errors.
//
// CounterData *PerfCounter::createSnap(const string &name_) {}
//
// perfcounter.cpp:45:1: error: ‘CounterData’ does not name a type
//  CounterData *PerfCounter::createSnap(const string &name_)
//  ^

PerfCounter::CounterData *PerfCounter::createSnap(const string &name_)
{
    CounterData *psnap = new CounterData(name_);
    clock_gettime(CLOCK_MONOTONIC, &(psnap->ts));
    return psnap;
}

// cpp-stringstream
void PerfCounter::dump()
{
    std::stringstream ss{};
    uint32_t countSnap{1};

    // only when there are two nodes to use
    for (CounterData *pstart = phead; 
            pstart && pstart->pnext; pstart = pstart->pnext)
    {
        ss << "snap: " << countSnap << ": ";
        ss << pstart->name << " -> " << pstart->pnext->name << " took ";

        // time diff in us from current to next
        uint64_t timeDiff = 
            (pstart->pnext->ts.tv_sec*1000000 + pstart->pnext->ts.tv_nsec/1000)-
            (pstart->ts.tv_sec*1000000 + pstart->ts.tv_nsec/1000);

        ss << timeDiff << "us" << std::endl;
        ++countSnap;
    }

    std::cout << ss.str();
}

// main.cpp
#include "perfcounter.h"

#include <iostream>
#include <sstream>
#include <boost/lexical_cast.hpp>

int main(int argc,char** argv)
{
    PerfCounter counter;
    
    for(int i=0;i<10000;++i)
    {
        int out;
        sscanf("42","%d",&out);
    }
    counter.snap("scanf int");

    for(int i=0;i<10000;++i)
    {
        int out;
        std::stringstream ss("42");
        ss >> out;
    }
    counter.snap("stringstream int");

    for(int i=0;i<10000;++i)
    {
        int out = boost::lexical_cast<int>("42");
    }
    counter.snap("boost::lexical_cast<int>");
    counter.dump();
}

// on debian VM 
snap: 1: start -> scanf int took 1050us
snap: 2: scanf int -> stringstream int took 7130us
snap: 3: stringstream int -> boost::lexical_cast<int> took 1922us
delete start
delete scanf int
delete stringstream int
delete boost::lexical_cast<int>

// on build host
snap: 1: start -> scanf int took 1917us
snap: 2: scanf int -> stringstream int took 7663us
snap: 3: stringstream int -> boost::lexical_cast<int> took 1488us
delete start
delete scanf int
delete stringstream int
delete boost::lexical_cast<int>


={============================================================================
*kt_linux_core_606* linux-time-usleep

{usleep}

usleep - suspend execution for microsecond intervals

#include <unistd.h>

int usleep(useconds_t usec);

note: copied from other article.

Also consider this code: 

usleep(1000);    // sleep 1 microsecond

On the face of it, this line of code makes a thread sleep for 1 microsecond
and then continue. In reality, 1 microsecond is just a lower bound to the
duration of the call.

The man page for usleep() says, "The usleep() function suspends execution of
the calling process for (at least) usec microseconds. The sleep may be
lengthened slightly by any system activity or by the time spent processing the
call or by the granularity of system timers," or if you use the nanosleep()
function. "Therefore, nanosleep() always pauses for at least the specified
time; however, it can take up to 10 ms longer than specified until the process
becomes runnable again."

So if the process is not scheduled under a real-time policy, there's no
guarantee when your thread will be running again. I've done some tests and (to
        my surprise) there are situations when code such as: 

cond.timed_wait(lock, x);    // x = e.g. 1 millisecond

will actually wait for more than 1 second.


{msleep}

void msleep(size_t milliseconds)
{
    usleep(milliseconds * 1000);
}


={============================================================================
*kt_linux_core_700* dbus

{dbus}
http://www.freedesktop.org/wiki/Software/dbus/

What is D-Bus?

D-Bus is a message bus system, a simple way for applications to talk to one
another. In addition to interprocess communication, D-Bus helps coordinate
process lifecycle; it makes it simple and reliable to code a "single instance"
application or daemon, and to launch applications and daemons on demand when
their services are needed.

D-Bus is an Inter-Process Communication (IPC) and Remote Procedure Calling (RPC)
    mechanism specifically designed for efficient and easy-to-use communication
    between processes running on the 'same' machine.


{tutorial}
http://dbus.freedesktop.org/doc/dbus-tutorial.html

D-Bus is a system for interprocess communication (IPC). Architecturally, it has
several layers:

<libdbus> low-level-binding
A library, libdbus, that allows two applications to connect to each other and
exchange messages.

libdbus only supports one-to-one connections, just like a raw network socket.
However, rather than sending byte streams over the connection, you send
'messages'. Messages have a header identifying the kind of message, and a body
containing a data payload. libdbus also abstracts the exact transport used
(sockets vs. whatever else), and handles details such as authentication.

<bus-deamon>
A `message-bus-daemon-executable`, built on libdbus, that multiple applications
can connect to. The daemon can route messages from one application to zero or
more other applications.

The message bus daemon forms the hub of a wheel. Each spoke of the wheel is a
one-to-one connection to an application using libdbus. 

An application sends a message to the bus daemon over its spoke, and the bus
daemon forwards the message to other connected applications as appropriate.
Think of the daemon as a 'router'. 

// from a box, dbus_glib-0.100.2
dbus       671  0.0  0.3   3644  2056 ?        Ss   Apr13   2:45 /opt/zinc/oss/bin/dbus-daemon --fork --print-pid 4 --print-address 6 --session


<binding> high-level binding
'wrapper' libraries or 'bindings' based on particular application frameworks.
For example libdbus-glib and libdbus-qt. There are also bindings to languages
such as Python. These wrapper libraries are the API most people should use, as
they simplify the details of D-Bus programming.  libdbus is intended to be a
low-level backend for the higher level bindings. Much of the libdbus API is only
useful for binding implementation. 


<session-and-system>
D-Bus applications. D-Bus is designed for 'two' specific 'cases':

The bus daemon has 'multiple' instances on a typical computer. The first
instance is a machine-global singleton, that is, a system daemon similar to
sendmail or Apache. This instance has heavy security restrictions on what
messages it will accept, and is used for systemwide communication. The other
instances are created one per user login session. These instances allow
applications in the user's session to communicate with one another.

The systemwide and per-user daemons are separate. Normal within-session IPC does
not involve the systemwide message bus process and vice versa. 

1. session-bus. Communication between desktop applications in the 'same' desktop
'session'; to allow integration of the desktop session as a whole, and address
issues of process lifecycle when do desktop components start and stop running.

2. system-bus. Communication between the desktop session and the operating
system, where the operating system would typically include the kernel and any
system daemons or processes. 

For the within-desktop-session use case, the GNOME and KDE desktops have
significant previous experience with different IPC solutions such as CORBA and
DCOP. D-Bus is built 'on' that experience and carefully tailored to meet the
needs of these desktop projects in particular. D-Bus may or may not be
appropriate for other applications; the FAQ has some comparisons to 'other' IPC
systems.


{concept}
From concept diagram

bus daemon process
==================
 DbusConnection      ->          message dispatcher         <- DbusConnection
 Instance            <-          if(message is signal)      -> Instance
   |                               broadcast
   |                             else
   |                               find destination
   |                               named by message
   |
   |                             (destination table)
   |                 <-          Connection 1
   |                             Connection 2               ->
   |                             "The Window Manager"
   | socket                      ...
   | (bidirectional              
   | message system
   |

application process 1                                   application process 2
===================
 DbusConnection
 Instance

 incoming                   outgoing call
 | locate object via       /|\
 | object path              | marshal method call
 |                          | to message
 | bindings marshal         |
 | to method call           |
\|/                         |

 C/C++ object              Bindings proxy
 instance                  instance

note: incoming from address to method


<address>
Applications using D-Bus are either 'servers' or 'clients'. A server listens for
incoming connections; a client connects to a server. Once the connection is
established, it is a symmetric flow of messages; the client-server distinction
only matters when setting up the connection.

If you're using the bus daemon, as you probably are, your application will be a
client of the bus daemon. That is, the bus daemon listens for connections and
your application initiates a connection to the bus daemon.

A D-Bus address specifies where a server will listen, and where a client will
connect. For example the address unix:path=/tmp/abcdef specifies that the server
will listen on a UNIX domain socket at the path /tmp/abcdef and the client will
connect to that socket. An address can also specify TCP/IP sockets, or any other
'transport' defined in future iterations of the D-Bus specification. 


<busname> for application
When each application connects to the bus daemon, the daemon immediately
'assigns' it a 'name' called the unique connection name. A unique name begins
with a ':' (colon) character. These names are never reused during the lifetime
of the bus daemon - that is, you know a given name will always refer to the same
application.

An example of a unique name might be :34-907. The numbers after the colon have
no meaning other than their 'uniqueness'.

    "When a name is mapped to a particular application's connection, that
    application is said to own that name."

    note: Each connection = each application = each bus name

Applications may ask to own 'additional' well-known names. For example, you
could write a specification to define a name called com.mycompany.TextEditor.
Your definition could specify that to own this name, an application should have
an object at the path /com/mycompany/TextFileManager supporting the interface
org.freedesktop.FileHandler.

    "Applications could then send messages to this bus name, object, and
    interface to execute method calls."

You could think of the unique names as IP addresses, and the well-known names as
domain names. So com.mycompany.TextEditor might map to something like :34-907
just as mycompany.com maps to something like 192.168.0.5. 

note: can monitor if other application is live or not

Names have a 'second' important 'use', other than routing messages. They are
used to track 'lifecycle'. When an application exits (or crashes), its
connection to the message bus will be closed by the operating system kernel. The
message bus then sends out 'notification' messages telling remaining
applications that the application's names have lost their owner. By tracking
these notifications, your application can reliably monitor the lifetime of
'other' applications. 


<object-path>
Your programming framework probably defines what an "object" is like; usually
with a base class. For example: java.lang.Object, GObject, QObject, python's
base Object, or whatever. Let's call this a native object. 

The low-level D-Bus protocol, and corresponding libdbus API, does not care about
native objects.

However, low-level protocal provides a concept called an `object-path`. The idea
of an object path is that higher-level bindings can 'name' native object
'instances', and allow remote applications to refer to them. 

The object path looks like a filesystem path, for example an object could be
named /org/kde/kspread/sheets/3/cells/4/5. Human-readable paths are nice, but
you are free to create an object named /com/mycompany/c5yo817y0c1y1c5b if it
makes sense for your application.

Namespacing object paths is smart, by starting them with the components of a
domain name you own (e.g. /org/kde). This keeps different code modules in the
same process from stepping on one another's toes. 


<interface>
Each object supports one or more interfaces. Think of an interface as a named
group of methods and signals, just as it is in GLib or Qt or Java. Interfaces
define the type of an object instance.

DBus identifies interfaces with a simple 'namespaced' string, something like
org.freedesktop.Introspectable. Most bindings will map these interface names
directly to the appropriate programming language construct, for example to Java
interfaces or C++ pure virtual classes. 


<method-and-signal>
Each object has members; the two kinds of member are 'methods' and 'signals'.
Methods are operations that can be invoked on an object, with optional input
(aka arguments or "in parameters") and output (aka return values or "out
        parameters"). 

Signals are 'broadcasts' from the object to any interested observers of the
'object'; signals may contain a data payload.

Both methods and signals are referred to by name, such as "Frobate" or
"OnClicked".

note: outgoing


<proxies>
A proxy object is a convenient 'native' object created to 'represent' a 'remote'
object in another process. The low-level DBus API involves manually creating a
method call message, sending it, then manually receiving and processing the
method reply message. 

Higher-level bindings provide proxies as an alternative. Proxies look like a
normal native object; but when you invoke a method on the proxy object, the
binding converts it into a DBus method call message, waits for the reply
message, unpacks the return value, and returns it from the native method.

 In pseudocode, programming without proxies might look like this:

Message message = new Message("/remote/object/path", "MethodName", arg1, arg2);
Connection connection = getBusConnection();
connection.send(message);
Message reply = connection.waitForReply(message);
if (reply.isError()) {

} else {
Object returnValue = reply.getReturnValue();
}
        
Programming with proxies might look like this:

Proxy proxy = new Proxy(getBusConnection(), "/remote/object/path");
Object returnValue = proxy.MethodName(arg1, arg2);
        

{big-picture} to specify call
Pulling all these concepts together, to specify a particular method call on a
particular object instance, a number of nested components have to be named:

Address -> [Bus Name] -> Path -> Interface -> Method

dbus.expose(
    // <1> path "/Zinc/Media/MediaRouterFactory";
    ObjectPath::MEDIA_ROUTER_FACTORY, 
    // <2> obj
    factoryMediaRouter.createMediaRouterFactory(),
    // <3>
    boost::make_shared<NonInheritingAdaptorFactory<MediaRouter> > (
        NS_ZINC_DBUS_BINDING::RefCountedAdaptorFactory<MediaRouter>(*bnm), conn, 
        "/Zinc/Media/MediaRouters/")
           );

method call sender=:1.127 -> dest=Zinc.MediaProxy serial=44 
path=/Zinc/Media/MediaRouterFactory; 
interface=Zinc.Media.MediaRouterFactory; 
member=createMediaRouter

// :1.129 == Zinc.MediaProxy
method call sender=:1.127 -> dest=:1.129 serial=48 
path=/Zinc/Media/MediaRouters/0; 
interface=Zinc.Media.MediaRouter; 
member=setSource
   string "http://=1460458249&e=1460501449&h=b42724e238b5e67cbc99e24d435763f3"
   int32 1

dbus-send --print-reply --type=method_call --dest='Zinc.MediaProxy' /Zinc/Media/MediaRouters/0
Zinc.Media.MediaRouter.setSource 
string:http://dash.bidi.int.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd int32:0


{dbus-messages}
There are 4 message 'types':

1. `method-call-messages` ask to invoke a method on an object.  you send a
method call message, and receive either a method return message or an error
message in reply. 

2. `method-return-messages` return the results of invoking a method.

3. `error-messages` return an exception caused by invoking a method.

4. `signal-messages` are notifications that a given signal has been emitted that
an 'event' has occurred. You could also think of these as "event" messages. 

A method call maps very simply to messages: you send a method call message, and
receive either a method return message or an error message in reply.

Each message has a header, including fields, and a body, including arguments.
You can think of the header as the routing information for the message, and the
body as the payload. Header fields might include the sender bus name,
     destination bus name, method or signal name, and so forth. One of the
     header fields is a type signature describing the values found in the body.
     For example, the letter "i" means "32-bit integer" so the signature "ii"
     means the payload has two 32-bit integers. 


{calling-a-method} Behind the Scenes
A method call in DBus consists of two messages; a method call message sent from
process A to process B, and a matching method reply message sent from process B
to process A. Both the call and the reply messages are routed through the bus
daemon. The caller includes a different `serial-number` in each call message,
    and the reply message includes this number to allow the caller to 'match'
    replies to calls.

The call message will contain any arguments to the method. The reply message may
indicate an error, or may contain data returned by the method.

A method invocation in DBus happens as follows:

* The language binding may provide a proxy, such that invoking a method on an
  in-process object invokes a method on a remote object in another process. If
  so, the application calls a method on the proxy, and the proxy constructs a
  method call message to send to the remote process.

* For more low-level APIs, the application may construct a method call message
  itself, without using a proxy.

* In either case, the method call message contains: a bus name belonging to the
  remote process; the name of the method; the arguments to the method; an object
  path inside the remote process; and optionally the name of the interface that
  specifies the method.

* The method call message is sent to the bus daemon.

* The bus daemon looks at the destination bus name. If a process owns that name,
  the bus daemon forwards the method call to that process. Otherwise, the
  bus daemon creates an error message and sends it back as the reply to the
  method call message.

* The receiving process unpacks the method call message. In a simple low-level
  API situation, it may immediately run the method and send a method reply
  message to the bus daemon. When using a high-level binding API, the binding
  might examine the object path, interface, and method name, and convert the
  method call message into an invocation of a method on a native object sucn as
  java.lang.Object, QObject, etc., then convert the return value from the native
  method into a method, reply message.

* The bus daemon receives the method reply message and sends it to the process
  that made the method call.

* The process that made the method call looks at the method reply and makes use
  of any return values included in the reply. The reply may also indicate that
  an error occurred. When using a binding, the method reply message may be
  converted into the return value of of a proxy method, or into an exception. 

The bus daemon 'never' reorders messages. That is, if you send two method call
messages to the same recipient, they will be received in the order they were
sent. The recipient is not required to reply to the calls in order, however; for
example, it may process each method call in a separate thread, and return reply
messages in an undefined order depending on when the threads complete. Method
calls have a unique serial number used by the method caller to match reply
messages to call messages. 


{emitting-a-signal} Behind the Scenes
A signal in DBus consists of a single message, sent by one process to any number
of other processes.  That is, a signal is a `unidirectional-broadcast`. The
signal may contain arguments (a data payload), but because it is a broadcast, it
'never' has a "return value." Contrast this with a method call where the method
call message has a matching method reply message.

The emitter (aka sender) of a signal has no knowledge of the signal recipients.
Recipients register with the bus daemon to receive signals based on "match
rules" - these rules would typically include the sender and the signal name. The
bus daemon sends each signal only to recipients who have expressed interest in
that signal.

A signal in DBus happens as follows:

* A signal message is created and sent to the bus daemon. When using the
  low-level API this may be done manually, with certain bindings it may be done
  for you by the binding when a native object emits a native signal or event.

* The signal message contains the name of the interface that specifies the
  signal; the name of the signal; the bus name of the process sending the
  signal; and any arguments

* Any process on the message bus can register "match rules" indicating which
  signals it is interested in. The bus has a list of registered match rules.

* The bus daemon examines the signal and determines which processes are
  interested in it. It sends the signal message to these processes.

* Each process receiving the signal decides what to do with it; if using a
  binding, the binding may choose to emit a native signal on a proxy object. If
  using the low-level API, the process may just look at the signal sender and
  name and decide what to do based on that. 


={============================================================================
*kt_linux_core_700* dbus-libdbus

Reference Implementation (dbus-daemon and libdbus)

Released versions of D-Bus can be downloaded from the releases directory on
dbus.freedesktop.org and are available in all major Linux distributions. If in
doubt, use your distribution's packages.

note: Use dbus-1.6.4.tar.bz2

The current stable branch is D-Bus 1.10.x. This is the recommended version for
most purposes.

The current legacy branch is D-Bus 1.8.x. This is still supported, but only for
security fixes: only use these versions when upgrading from older stable
releases, or preparing security updates for frozen/stable distributions.

Older branches are unsupported and are unlikely to have any more releases, but
distributors who still provide security support for an older version are invited
to share backported patches via the older branches in the same git repository.

The current development branch is D-Bus 1.11.x, which will lead to a 1.12.x
stable branch in future.


<doc>
https://dbus.freedesktop.org/doc/api/html/group__DBusThreads.html

DBUS_EXPORT dbus_bool_t dbus_threads_init_default 	(void) 	

Initializes threads.

If this function is not called, the D-Bus library will not lock any data
structures. If it is called, D-Bus will do locking, at some cost in efficiency.

Since D-Bus 1.7 it is safe to call this function from any thread, any number of
times (but it must be called before any other libdbus API is used).

In D-Bus 1.6 or older, this function must be called in the main thread before
any other thread starts. As a result, it is not sufficient to call this function
in a library or plugin, unless the library or plugin imposes a similar
requirement on its callers.

dbus_shutdown() reverses the effects of this function when it resets all global
state in libdbus.

Returns
    TRUE on success, FALSE if not enough memory 

Definition at line 438 of file dbus-threads.c.

References dbus_threads_init(), and NULL.

Referenced by _dbus_cmutex_new_at_location(), _dbus_condvar_new(), and
_dbus_rmutex_new_at_location().


={============================================================================
*kt_linux_core_700* dbus-bindings

https://www.freedesktop.org/wiki/Software/DBusBindings/

// used dbus_python-0.83.1

dbus-python

dbus-python is a binding for libdbus, the reference implementation of D-Bus. For
compatibility reasons, its API involves a lot of type-guessing (despite
        "explicit is better than implicit" and "resist the temptation to
        guess").

Since version 1.0.0 it supports both Python 2 and 3.

    Recent release history

    Releases are always available from http://dbus.freedesktop.org/releases/dbus-python/

    API and other documentation are at http://dbus.freedesktop.org/doc/dbus-python/

    dbus-python is maintained in git: dbus-python git web

    For users with commit access: git clone
    git+ssh://git.freedesktop.org/git/dbus/dbus-python

    For anonymous read only access: git clone
    git://anongit.freedesktop.org/git/dbus/dbus-python

    Bugs are tracked in the freedesktop.org bugzilla: search for dbus-python
    bugs or file a dbus-python bug


C++

dbus-cpp was started almost three years ago to provide a C++ API for D-Bus, but
is unfortunately abandoned since then. For this reason ?PaoloDurante wrote a
pure C++ binding (dbus-c++) while working on the OpenWengo softphone.


={============================================================================
*kt_linux_core_700* dbus-case

// MediaDaemon.cpp


int main(int argc, char *argv[]) {
  int result = EXIT_SUCCESS;

  try {
    MainLoop mainloop(BusName::MEDIA);

    // note: create instance
    MediaDaemon daemon(mainloop);

    mainloop.post(boost::bind(&MediaDaemon::start, &daemon, argc, argv));
    result = mainloop.run();
  } catch (std::exception& e) {
    std::cerr << e.what() << endl;
    result = EXIT_FAILURE;
  }

  return result;
}


class MediaDaemon : boost::noncopyable
{
public:
    MediaBackendDaemon(MainLoop& mainloop);
    ~MediaBackendDaemon();
    void start();
    void stop();
private:

    MainLoop& mainloop;
    SignalReceiver sr;

    // note:
    DBusService dbus;

    NS_ZINC::InlineDispatcher futureDispatcher;
    boost::scoped_ptr<BusNameMonitor> bnm;

    std::string getUserAgentString();
};

MediaDaemon::MediaDaemon(MainLoop& mainloop):
    mainloop(mainloop),
    sr(mainloop, createExitSignalHandler(boost::bind(&MediaBackendDaemon::stop, this))),

    // note:
    dbus(mainloop)
{
}

void MediaDaemon::start()
{
    sr.start();
    dbus.start();

    DBus::Connection conn = dbus.getConnection();

    const bool dashCencEnable = convertToSync(createAuthoriser())->isFeatureEnabled("yv-mediarouter-dash");

    boost::shared_ptr<Zinc::Media::Backend::Factory> factory =
        createIPCSrcBinFactory(
            boost::shared_ptr<NS_ZINC::Dispatcher>(&futureDispatcher),
            createSinkBinFactory(
                boost::shared_ptr<NS_ZINC::Dispatcher>(&futureDispatcher),
                dashCencEnable));

    bnm.reset(new BusNameMonitor(conn));

    <ex>
    dbus.expose(OBJECT_PATH_FACTORY, factory,
        boost::make_shared<NonInheritingAdaptorFactory<Zinc::Media::Backend::SinkInstance> >(
            RefCountedAdaptorFactory<Zinc::Media::Backend::SinkInstance>(*bnm), conn, OBJECT_PATH_CONTROL));

    <ex> 
    dbus.expose(
            ObjectPath::MEDIA_ROUTER_FACTORY, 
            factory.createMediaRouterFactory(),
            boost::make_shared<NonInheritingAdaptorFactory<MediaRouter> >(
                NS_ZINC_DBUS_BINDING::RefCountedAdaptorFactory<MediaRouter>(*bnm), conn, "/Zinc/Media/MediaRouters/")
              );

    dbus.request_name(BUS_NAME);
}


<wrapper>
MainLoop
: DBusMainLoopDispatch (wraps BusDispatch)

    ->
        BusDispatch : Dispatch, DefaultMainLoop
            (DefaultMainLoop do the real work)

/////////////////////////////////////
#ifndef ZINC_DBUS_BINDING_MAINLOOP_H_
#define ZINC_DBUS_BINDING_MAINLOOP_H_

#include "macros.h"

#include <dbus-c++/connection.h>
#include <dbus-c++/dispatcher.h>
#include <dbus-c++/eventloop-integration.h>
#include <zinc-common/AsynchronousEventDispatcher.h>

#include <boost/make_shared.hpp>
#include <boost/noncopyable.hpp>
#include <boost/shared_ptr.hpp>

NS_ZINC_DBUS_BINDING_OPEN

/**
 *  MainLoop class wraps dispatchers and make those dispatchers have a unique
 *  interface to work in different situation.
 *
 *  It adapts a single dispatcher loop such that it can be used as any types of
 *  dispatcher. It worth to do further modifying on the dispatchers to have
 *  fewer dispatcher types for simplification
 *
 */

class ZINC_EXPORT MainLoop : boost::noncopyable {
public:
   // note: uses for debugging but not the real bus name
	explicit MainLoop(const std::string& daemonName);

	/**
	 * Post a command to be run by any thread currently running the dispatcher
	 * loop.
	 */
	void post(boost::function<void(void)> cb);

    /**
     * Break the dispatching loop, allowing graceful shutdown
     */
    void leave();

	/**
	 * Run the dispatcher loop.
	 */
	int run();

	/**
	 * Get different dispatchers maintained in mainloop
	 */
	boost::shared_ptr<NS_ZINC::Dispatcher> getZincDispatcher();
	boost::shared_ptr<DBus::Dispatcher> getDbusDispatcher();
	boost::shared_ptr<DBus::DefaultMainLoop> getDefaultMainLoop();

private:
	void initialization();
	boost::shared_ptr<NS_ZINC::Dispatcher> dispatcher;
	boost::shared_ptr<DBus::Dispatcher> dbusDispatcher;
	boost::shared_ptr<DBus::DefaultMainLoop> defaultMainloop;
	std::string name;
};

NS_ZINC_DBUS_BINDING_CLOSE

#endif /* MAINLOOP_H_ */


/////////////////////////////////////

#include "../../include/dbus/MainLoop.h"

#include <iostream>

#include <cstdlib>

namespace {
/**
 * Type conversion class to convert from DBus::DefaultMainLoop to NS_ZINC::Dispatcher
 */
class DBusMainLoopDispatcher: public NS_ZINC::Dispatcher {

public:
	explicit DBusMainLoopDispatcher(boost::shared_ptr<DBus::DefaultMainLoop> d_) :
			d(d_) {
	}

	void post(boost::function<void(void)> fn) {
		d->post_function(fn);
	}

	void onWorkAdded() {
		d->add_external_work();
	}

	void onWorkRemoved() {
		d->remove_external_work();
	}

	boost::shared_ptr<DBus::DefaultMainLoop> d;
};

} //anon namespace

NS_ZINC_DBUS_BINDING_OPEN

MainLoop::MainLoop(const std::string& daemonName) : name(daemonName) {
	initialization();
}


// note: 
// create dbus-dispatcher and dbus-main-loop
//
// 'DBusMainLoopDispatcher' is to translate between zinc dispatcher and dbus
// dispatcher.

void MainLoop::initialization(){

	boost::shared_ptr<DBus::BusDispatcher> busDispatcher = 
       boost::make_shared<DBus::BusDispatcher>();

   // DBus::Dispatcher = DBus::BusDispatcher;
	dbusDispatcher = busDispatcher;

	// DBus::DefaultMainLoop = DBus::BusDispatcher;
   // HOW?
	defaultMainloop = busDispatcher;

   // create wrapper(defaultMainloop);
	dispatcher = boost::make_shared<DBusMainLoopDispatcher>(defaultMainloop);
}

void MainLoop::post(boost::function<void(void)> cb) {
	dispatcher->post(cb);
}


void MainLoop::leave() {
    dbusDispatcher->leave();
}


int MainLoop::run() {
    int result = EXIT_FAILURE;
    try {
        // note: run
        dbusDispatcher->run();

        result = EXIT_SUCCESS;
    } catch (const std::exception& e) {
        std::cerr << name << " ERROR: " << e.what() << std::endl;
        throw;
    } catch (...) {
        std::cerr << name << " ERROR: Unknown\n";
        throw;
    }
    return result;
}

boost::shared_ptr<NS_ZINC::Dispatcher> MainLoop::getZincDispatcher() {
	return dispatcher;
}
boost::shared_ptr<DBus::Dispatcher> MainLoop::getDbusDispatcher() {
	return dbusDispatcher;
}
boost::shared_ptr<DBus::DefaultMainLoop> MainLoop::getDefaultMainLoop() {
	return defaultMainloop;
}
NS_ZINC_DBUS_BINDING_CLOSE


<dbus-bus-dispatcher>

// Zinc/Zinc.DBus-C++/include/dbus-c++/eventloop-integration.h
// Zinc/Zinc.DBus-C++/include/dbus-c++/dispatcher.h
// has class DXXAPI Dispatcher

namespace DBus {

/* 
 * Glue between the event loop and the DBus library
 */
class DXXAPI BusDispatcher 
	: public Dispatcher, public DefaultMainLoop
{
public:
	BusDispatcher(boost::shared_ptr<ErrorConverter> = DefaultErrorConverter::get());

	~BusDispatcher();

	bool running() { return _running; }

	virtual void enter();

	virtual void leave();

	virtual void start_iteration();

	virtual Timeout* add_timeout(Timeout::Internal *);

	virtual void rem_timeout(Timeout *);

	virtual Watch* add_watch(DBusWatch*);

	virtual void rem_watch(Watch *);

	void wakeup();

	bool isThisDispatchThread() const {
		return DefaultMainLoop::isThisDispatchThread();
	}

	bool shouldQueueCommand() const {
		return DefaultMainLoop::shouldQueueCommand();
	}
	std::size_t run();
	std::size_t poll();

protected:
	
   // note:
	void post_function(boost::function<void ()> command)
	{
		DefaultMainLoop::post_function(command);
	}

private:

	volatile bool _running;
};

}


// Zinc/Zinc.DBus-C++/src/eventloop-integration.cpp

BusDispatcher::BusDispatcher(boost::shared_ptr<ErrorConverter> ec)
	: Dispatcher()
	, DefaultMainLoop(ec)
	, _running(false)
{
    // note: call to libdbus
    dbus_threads_init_default();
}

BusDispatcher::~BusDispatcher()
{
}

std::size_t BusDispatcher::run()
{
    debug_log("Running dispatcher %p with EventLoop %p", (Dispatcher*) this, (DefaultMainLoop*) this);
    start_iteration();

    DefaultMainLoop::LoopScope scopeLoop(this);

    std::size_t n = 0;
    while (_running && (has_cmds() || has_timeouts() || has_watches()
                || has_external_work())) {
        n += dispatch(BLOCK_FOR_WATCHES | BLOCK_FOR_TIMEOUTS
                | BLOCK_FOR_EXTERNAL_WORK);
    }

    debug_log("Finished running dispatcher %p with EventLoop %p.  Ran %u handlers.",
            (Dispatcher*) this, (DefaultMainLoop*) this, (unsigned) n);
    return n;
}


// <dbus-busdispatcher>
// From Makefile. DO not use these.
// 
// #am__objects_4 =  \
// #	libdbus_c___1_la-glib-integration.lo
// #am__objects_5 =  \
// #	libdbus_c___1_la-ecore-integration.lo
// 
// Assumes that uses Glib since there are two options: Ecore and Glib.
// 
// // Defines ABC
// // Zinc/Zinc.DBus-C++/include/dbus-c++/dispatcher.h
// 
// class DXXAPI Dispatcher;
// 
// 
// // Zinc/Zinc.DBus-C++/include/dbus-c++/ecore-integration.h
// // Zinc/Zinc.DBus-C++/include/dbus-c++/glib-integration.h
// 
// namespace DBus {
// namespace Glib {
// 
// class DXXAPI BusDispatcher : public Dispatcher
// {
// public:
// 
// 	BusDispatcher();
// 	~BusDispatcher();
// 
// 	void attach(GMainContext *);
// 
// 	void enter() {}
// 
// 	void leave() {}
// 
// 	Timeout *add_timeout(Timeout::Internal *);
// 
// 	void rem_timeout(Timeout *);
// 
// 	Watch* add_watch(DBusWatch*);
// 
// 	void rem_watch(Watch *);
// 
// 	void set_priority(int priority);
// 
// private:
// 
// 	GMainContext *_ctx;
// 	int _priority;
// 	GSource *_source;
// };
// 
// } /* namespace Glib */
// } /* namespace DBus */
// 
// 
// // Zinc/Zinc.DBus-C++/src/glib-integration.cpp
// 
// Glib::BusDispatcher::BusDispatcher()
// : _ctx(NULL), _priority(G_PRIORITY_DEFAULT), _source(NULL)
// {
// }


<dbus-default-main-loop>

// Zinc/Zinc.DBus-C++/include/dbus-c++/eventloop.h

/*
 *
 *  D-Bus++ - C++ bindings for D-Bus
 *
 *  Copyright (C) 2005-2009  Paolo Durante <shackan@gmail.com>
 */

#ifndef __DBUSXX_EVENTLOOP_H
#define __DBUSXX_EVENTLOOP_H

namespace DBus {

/*
 * these Default *classes implement a very simple event loop which
 * is used here as the default main loop, if you want to hook
 * a different one use the Bus *classes in eventloop-integration.h
 * or the Glib::Bus *classes as a reference
 */

class DXXAPI DefaultMainLoop
{
public:

	DefaultMainLoop(boost::shared_ptr<ErrorConverter>);

	virtual ~DefaultMainLoop();

	void wakeup();

	bool isThisDispatchThread() const;
	bool shouldQueueCommand() const;

	void post_function(boost::function<void ()>);

	/**
	 * Register for a callback after the specified amount of time.
	 *
	 * @param interval_ms The amount of time to wait before calling the
	 *                    callback in milliseconds
	 * @param repeat      A value of false will cause the callback to be called
	 *                    exactly once.  i.e. the callback is canceled
	 *                    immediately after it is first called.
	 * @param callback    The callback functor to call after/every interval_ms
	 *                    milliseconds.
	 *
	 * @return A pointer to a DefaultTimeout object which can be used to cancel
	 *         the timeout.
	 */
	DefaultTimeout* add_timeout(int interval_ms, bool repeat, boost::function<void (DefaultTimeout&)> callback);

	/**
	 * Register for a callback when a file descriptor becomes ready for I/O
	 *
	 * @param fd      The file descriptor to watch
	 * @param flags   Flags describing under what circumstances the callback
	 *                should be called.  These are the same as what should be
	 *                passed to poll as the events member of the pollfd
	 *                structure.  This will typically be several of POLLIN,
	 *                POLLPRI, POLLOUT, POLLRDHUP, POLLERR, POLLHUP and
	 *                POLLNVAL.  See man 2 poll for more information.
	 * @param handler The callback functor to call when the fd becomes ready
	 *                for I/O
	 *
	 * @return A pointer to a DefaultWatch object which can be used to stop
	 *         watching the file descriptor.
	 */
	DefaultWatch* add_watch(int fd, int flags, boost::function<void (DefaultWatch&)> handler);

	/**
	 * The way of doing graceful exit of a dispatcher is by removing works from
	 * it and letting it exit after it's run out of works.
	 *
	 * In order to avoid the dispatcher run down before all the Dispatcher::work
	 * objects destroyed, it is necessary to maintain a ref count of the works.
	 *
	 * Working with the ref count, Flags of BLOCK_FOR_EXTERNAL_WORK can be set
	 * to decide whether polling in dispatcher should block(for some timeout)
	 * when there is any work has not been finished on the dispatcher.
	 *
	 */
	void add_external_work();

	void remove_external_work();

	/**
	 * Do as much work (call as many callbacks) as possible without blocking
	 * on any of the watchers or timeouts.
	 *
	 * @return The number of callbacks that were called.
	 */
	size_t poll();
protected:

	virtual unsigned dispatch(unsigned flags);

	void enteredLoop();

	void leftLoop();

	bool has_cmds();
	bool has_timeouts();
	bool has_watches();

	/**
	 * check whether there is any work has not been finished on the dispatcher
	 */
	bool has_external_work();

	/**
	 * This struct follows RAII(Resource Acquisition Is Initialization), in
	 * order to make sure that the leftloop is always called even there is an
	 * exception thrown.
	 *
	 */
	struct LoopScope {
		LoopScope(DefaultMainLoop* loop_)
		 : loop(loop_) {
			loop->enteredLoop();
		}
		~LoopScope(){
			loop->leftLoop();
		}
		DefaultMainLoop* loop;
	};
private:
	unsigned processCommands();

	boost::mutex _mutex_cmds;
	std::list<boost::function<void ()> > _cmds;

	boost::mutex _mutex_t;
	std::list<DefaultTimeout> _timeouts;

	boost::mutex _mutex_w;
	std::list<DefaultWatch> _watches;

	unsigned externalWorkCount;

	int wakeupReadFd;
	int wakeupWriteFd;

	bool enteredDispatch;
	pthread_t dispatchThread;

	boost::shared_ptr<ErrorConverter> errorConverter;
};

} /* namespace DBus */

#endif//__DBUSXX_EVENTLOOP_H

// Zinc/Zinc.DBus-C++/src/eventloop.cpp

void DefaultMainLoop::post_function(boost::function<void ()> fn)
{
	std::list<boost::function<void ()> > new_cmd;
	new_cmd.push_back(fn);
	boost::mutex::scoped_lock lock(_mutex_cmds);
	_cmds.splice(_cmds.end(), new_cmd);
	lock.unlock();
	wakeup();
}


={============================================================================
*kt_linux_core_700* dbus-case-service

// The line 46 has no effect. WHAT is that for?
//
// template<typename T, typename A1>
// void expose_impl(DBus::Connection conn, const char* path,
//         boost::shared_ptr<T> obj, A1 a1) {
//     using namespace std;
// 46:	expose(conn, path, obj, a1);
// }
// 
// template<typename T, typename A1>
// void DBusService::expose(const char* path, boost::shared_ptr<T> obj, A1 a1) {
//     using namespace std;
// 145:	detail::expose_impl(conn, path, obj, a1);
//       objects.push_back(path);
// }
// 
// (gdb) bt
// #0  expose_impl<Zinc::Media::MediaRouterFactoryAsync, 
// boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > 
// (
//     a1=<error reading variable: access outside bounds of object referenced 
//          via synthetic pointer>, 
//     obj=<error reading variable: access outside bounds of object referenced 
//          via synthetic pointer>, 
//     conn=<incomplete type>, 
//     path=<optimized out>
// )
//     at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:46
// 
// #1  expose<
//  Zinc::Media::MediaRouterFactoryAsync, 
//  boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > 
//  (
//     a1=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
//     obj=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
//     this=0x7fff6510, 
//     path=<optimized out>
//  )
//     at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:145
// 
// #2  (anonymous namespace)::MediaDaemon::start (this=0x7fff64e4, argc=<optimized out>, argv=<optimized out>)
//     at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Nickel/Nickel.System.DBusServer/src/MediaDaemon.cpp:123
// #3  0x77e57404 in DBus::DefaultMainLoop::processCommands() () from /opt/zinc/lib/libdbus-c++-1.so.0
// #4  0x77e58ff0 in DBus::DefaultMainLoop::dispatch(unsigned int) () from /opt/zinc/lib/libdbus-c++-1.so.0
// #5  0x77e5a6c4 in DBus::BusDispatcher::run() () from /opt/zinc/lib/libdbus-c++-1.so.0
// #6  0x77ec24ec in zinc::binding::dbus::MainLoop::run (this=0x7fff64c8)
//     at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Zinc/Zinc.DBus.BindingRuntime/src/dbus/MainLoop.cpp:70
// #7  0x00408150 in main (argc=1, argv=0x7fff6864) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Nickel/Nickel.System.DBusServer/src/MediaDaemon.cpp:234
// 
// 
// (gdb) n
// expose<
//  Zinc::Media::MediaRouterFactoryAsync, 
//  boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > 
//  (
//     a1=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
//     obj=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
//     this=0x7fff6510, 
//     path=<optimized out>
//  )
//     at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:145
// 145		detail::expose_impl(conn, path, obj, a1);
//

/////////////////////////////////////
// Zinc.DBus.BindingRuntime/include/dbus/DBusService.h
/*
 * DBusService.h
 */

#ifndef ZINC_DBUS_BINDING_DBUSSERVICE_H_
#define ZINC_DBUS_BINDING_DBUSSERVICE_H_

#include "macros.h"
#include "MainLoop.h"
#include "MessageBusFactory.h"

#include <dbus-c++/connection.h>

#include <boost/make_shared.hpp>
#include <boost/shared_ptr.hpp>
#include <boost/foreach.hpp>

NS_ZINC_DBUS_BINDING_OPEN

/**
 * Those expose_impl(s) is to do with name-lookup.
 * Without this indirection, DBusService is unable to look outside class scope
 * to find additional expose methods as it already provides some
 */
namespace detail {

template<typename T>
void expose_impl(DBus::Connection conn, const char* path,
		boost::shared_ptr<T> obj) {
	using namespace std;
	expose(conn, path, obj);
}

// note:
template<typename T, typename A1>
void expose_impl(DBus::Connection conn, const char* path,
		boost::shared_ptr<T> obj, A1 a1) {
	using namespace std;

   // note: WHERE this leads to?
	expose(conn, path, obj, a1);
}

template<typename T, typename A1, typename A2>
void expose_impl(DBus::Connection conn, const char* path,
		boost::shared_ptr<T> obj, A1 a1, A2 a2) {
	using namespace std;
	expose(conn, path, obj, a1, a2);
}

} // detail namespace



/**
 * DBusService class is kind of wrapper of dbus, it maintains the connection
 * with bus, takes care of exposing and registering objects to the bus.
 *
 */
class ZINC_EXPORT DBusService {

public:
	/**
	 * Constructor
	 *
	 * @param mainloop is a wrapper of dispatchers
	 *
	 */
	DBusService(MainLoop& mainloop_);

	/**
	 * This method should be called when the dbus daemons start, in order to
	 * establish connection with dbus
	 */
	void start();

	/**
	 * Expose the given object on the bus
	 * By exposing this way the objects will be automatically unexposed when
	 * stop is called.
	 */
	template<typename T>
	void expose(const char* path, boost::shared_ptr<T> obj);

   // note:
	template<typename T, typename A1>
	void expose(const char* path, boost::shared_ptr<T> obj, A1 a1);

	template<typename T, typename A1, typename A2>
	void expose(const char* path, boost::shared_ptr<T> obj, A1 a1, A2 a2);

	/**
	 * The name will be automatically removed when stop() is called
	 */
	void request_name(const char* name);

	/**
	 * This method should be called when the dbus daemon ends, in order to close
	 * the connection
	 * with bus and also release names and unregister the object paths
	 */
	void stop();

	/**
	 * Unregister all object paths when the DBusService stops(the connection
	 * with bus stops)
	 */
	void onNamesReleased();

	/**
	 * Get connection that the DBusService maintains
	 */
	DBus::Connection getConnection();

private:
	DBus::Connection conn;
	std::vector<std::string> busNames;
	std::vector<std::string> objects;
	MainLoop& mainloop;

	/**
	 * Bus proxy used to access libdbus
	 */
	boost::shared_ptr<NS_ZINC_DBUS_BINDING::MessageBusAsync> bus_Async;
};

template<typename T>
void DBusService::expose(const char* path, boost::shared_ptr<T> obj) {
	using namespace std;
	detail::expose_impl(conn, path, obj);
	objects.push_back(path);
}

// note:
template<typename T, typename A1>
void DBusService::expose(const char* path, boost::shared_ptr<T> obj, A1 a1) {
	using namespace std;
	detail::expose_impl(conn, path, obj, a1);
	objects.push_back(path);
}

template<typename T, typename A1, typename A2>
void DBusService::expose(const char* path, boost::shared_ptr<T> obj, A1 a1,
		A2 a2) {
	using namespace std;
	detail::expose_impl(conn, path, obj, a1, a2);
	objects.push_back(path);
}

NS_ZINC_DBUS_BINDING_CLOSE

#endif /* DBUSSERVICE_H_ */


/////////////////////////////////////
/*
 * DBusService.cpp
 */

#include <zinc-common/macros.h>

#include "../../include/dbus/DBusService.h"
#include "../../include/dbus/MessageBus.h"
#include "../../include/dbus/DBusConnectionManager.h"
#include "../../include/dbus/detail/SingletonDBusErrorConverter.h"

#include <zinc-common/async/FutureBarrier.h>
#include <dbus-c++/connection.h>

NS_ZINC_DBUS_BINDING_OPEN

DBusService::DBusService(MainLoop& mainloop_) :
		mainloop(mainloop_) {
}

void DBusService::start() {
    // Zinc/Zinc.DBus-C++/include/dbus-c++/connection.h
    // class DXXAPI StandardConnectionFactory
    conn=DBus::StandardConnectionFactory(mainloop.getDbusDispatcher(), SingletonDBusErrorConverter::get()).connectToSessionBus();

    NS_ZINC_DBUS_BINDING::DBusConnectionManager::instance().setSessionConnection(conn);
    bus_Async = NS_ZINC_DBUS_BINDING::createMessageBusProxy(conn,mainloop.getZincDispatcher());
}

void DBusService::request_name(const char* name) {
	conn.request_name(name);
	busNames.push_back(name);
}

void DBusService::stop() {
	NS_ZINC::FutureBarrier bar(*mainloop.getZincDispatcher());
	BOOST_FOREACH(std::string name, busNames){
		bar.add(bus_Async->ReleaseName(name));
	}
	bar.setCallback(*mainloop.getZincDispatcher(),
			boost::bind(&DBusService::onNamesReleased, this));
}
void DBusService::onNamesReleased() {
	busNames.clear();
	BOOST_FOREACH(std::string path, objects){
		conn.unregister_object_path(path.c_str());
	}
	conn.set_callback_when_all_requests_replied(boost::bind(&DBus::Connection::close, _1));
}

DBus::Connection DBusService::getConnection(){
	return conn;
}

NS_ZINC_DBUS_BINDING_CLOSE


/////////////////////////////////////
/*
 * NonInheritingAdaptorFactory.h
 */

#ifndef ZINC_DBUS_BINDING_NONINHERITINGADAPTORFACTORY_H_
#define ZINC_DBUS_BINDING_NONINHERITINGADAPTORFACTORY_H_

#include "macros.h"

#include "PredestructDeleter.h"
#include "AdaptorFunctor.h"
#include "RefCountedAdaptorDecorator.h"
#include "InterfaceAdaptor.h"

#include <dbus-c++/types.h>
#include <dbus-c++/connection.h>

#include <boost/shared_ptr.hpp>
#include <boost/function.hpp>
#include <boost/foreach.hpp>

#include <string>
#include <sstream>

NS_ZINC_DBUS_BINDING_OPEN

class Connection;

/**
 * An implementation of the DBus-C++ AdaptorFactory concept.
 * 
 * WARNING: This is not thread-safe.  DBus internal locks must be exposed to
 *          make this thread-safe.
 */
template <class BaseT>
class NonInheritingAdaptorFactory : boost::noncopyable {
public:
    typedef AdaptorFunctor<typename NS_ZINC_DBUS_BINDING::adaptor<BaseT>::type> Adaptor;
    typedef RefCountedAdaptorDecorator<typename NS_ZINC_DBUS_BINDING::adaptor<BaseT>::type> RefCountedAdaptor;
    /**
     * Constructor
     * 
     * Factory should be a functor with the signature:
     *     std::string (boost::shared_ptr<BaseT> obj, boost::shared_ptr<Connection> connection, const std::string& basepath)
     * or equivalent.  The factory should expose the given object on the bus
     * generating a new unique path for that object and returning it.  When the
     * object drops off the bus the predestructor functor should be called.
     */
    template<class Factory>
    NonInheritingAdaptorFactory(Factory f, DBus::Connection connection_, const std::string& basePath_)
     : factory(f), connection(connection_), basePath(basePath_)
    {
    }

    /**
     * Expose the given object on the bus (if not already exposed) and return
     * the object path. 
     */
    std::string expose(boost::shared_ptr<BaseT> obj, const DBus::Message& msg = DBus::Message())
    {
        // First check if the object has yet been exposed:
        typedef std::pair<std::string, Adaptor*> AdaptorNamePair;
        std::vector<AdaptorNamePair> adaptors = connection.get_adaptors_of_type<Adaptor>();
        BOOST_FOREACH(AdaptorNamePair& i, adaptors) {
            if (i.second->getObject() == obj) {
                return i.first;
            }
        }
        typedef std::pair<std::string, RefCountedAdaptor*> RefCountedAdaptorNamePair;
        std::vector<RefCountedAdaptorNamePair> refcountedadaptors = connection.get_adaptors_of_type<RefCountedAdaptor>();
        BOOST_FOREACH(RefCountedAdaptorNamePair& i, refcountedadaptors) {
            if (i.second->getObject() == obj) {
                const char* dest = msg.destination();
                if (dest) {
                    i.second->getManager().addRef(connection, dest);
                }
                return i.first;
            }
        }
        return factory(obj, connection, basePath, msg);
    }
    /**
     * Return the object exposed at the given object path.  If one is not found
     * return NULL.  See caveat above.
     */
    boost::shared_ptr<BaseT> getFromPath(const std::string& path, const DBus::Message& = DBus::Message())
    {
        // This nested try, catch is in incredibly poor taste, sorry. -- Will
        // TODO: Make nicer
        try {
            try {
                return connection.get_adaptor_at_path<Adaptor>(path.c_str()).getObject();
            }
            catch (std::bad_cast&) {
                return connection.get_adaptor_at_path<RefCountedAdaptor>(path.c_str()).getObject();
            }
        }
        catch (DBus::Error&) {
            return boost::shared_ptr<BaseT>();
        }
    }
private:
    boost::function<std::string (boost::shared_ptr<BaseT>, DBus::Connection&, const std::string&, const DBus::Message&)> factory;
    DBus::Connection connection;
    const std::string basePath;
};

/**
 * A factory suitable to be provided to NonInheritingAdaptorFactory.  Should be
 * used for generated adaptors for objects that are to either stay on the bus
 * until teardown time or will be removed manually by calling into the
 * DBus::Connection (i.e. unrefcounted objects)
 */
template<class BaseT>
class ObjectAdaptorFactory
{
public:
    ObjectAdaptorFactory()
     : nextId(0)
    {
    }
    typedef std::string result_type;
    std::string operator() (boost::shared_ptr<BaseT> adaptee, DBus::Connection& conn, const std::string& root, const DBus::Message&)
    {
        std::stringstream ss;
        ss << root << nextId++;
        expose(conn, ss.str(), adaptee);
        return ss.str();
    }
private:
    int nextId;
};

/**
 * A factory suitable to be provided to NonInheritingAdaptorFactory.  Should be
 * used for generated adaptors for objects that are to stay on the bus for as
 * long as a remote process has a proxy for them.
 */
template<class BaseT>
class RefCountedAdaptorFactory
{
public:
    RefCountedAdaptorFactory(BusNameMonitor& bnm_)
     : nextId(0), bnm(bnm_)
    {
    }
    typedef std::string result_type;
    std::string operator() (boost::shared_ptr<BaseT> adaptee, DBus::Connection& conn, const std::string& root, const DBus::Message& msg)
    {
        const char* dest = msg.destination();
        if (dest == NULL) {
            throw std::runtime_error("RefCounted objects can only be created in calls or returns (i.e. messages that have a specific destination)");
        }
        std::stringstream ss;
        ss << root << nextId++;
        exposeRefCounted(conn, ss.str(), adaptee, bnm, dest);
        return ss.str();
    }
private:
    int nextId;
    BusNameMonitor& bnm;
};

NS_ZINC_DBUS_BINDING_CLOSE

#endif /* ZINC_DBUS_BINDING_NONINHERITINGADAPTORFACTORY_H_ */


={============================================================================
*kt_linux_core_701* dbus-introspection

{introspection}
D-Bus objects may support the interface org.freedesktop.DBus.Introspectable. This interface has one
method "Introspect" which takes no arguments and returns an XML string. The XML string describes the
interfaces, methods, and signals of the object. See the D-Bus specification for more details on this
introspection format. 


{signature-strings}
D-Bus uses a string-based type encoding mechanism called Signatures to describe the number and types
of arguments requried by methods and signals. Signatures are used for interface
declaration/documentation, data marshalling, and validity checking. Their string encoding uses a
simple, though expressive, format and a basic understanding of it is required for effective D-Bus
use. The table below lists the fundamental types and their encoding characters.


Character 	Code Data Type
y           8-bit unsigned integer
b           boolean value
n           16-bit signed integer
q           16-bit unsigned integer
i           32-bit signed integer
u           32-bit unsigned integer
x           64-bit signed integer
t           64-bit unsigned integer
d           double-precision floating point (IEEE 754)
s           UTF-8 string (no embedded nul characters)
o           D-Bus Object Path string
g           D-Bus Signature string
a           Array
(           Structure start
)           Structure end
v           Variant type (described below)
{           Dictionary/Map begin
}           Dictionary/Map end
h           Unix file descriptor

<example> xml has member and type defs as well
[17-02-2015 10:57:31.574183] signal sender=:1.0 -> dest=(null destination) serial=96 
path=/org/freedesktop/NetworkManager/Devices/0; interface=org.freedesktop.NetworkManager.Device; 
member=StateChanged

<node name="/" xmlns:tp="http://telepathy.freedesktop.org/wiki/DbusSpec#extensions-v0">
  <interface name="org.freedesktop.NetworkManager.Device">
    ...
    <signal name="StateChanged">
      <arg name="new_state" type="u" tp:type="NM_DEVICE_STATE">
        <tp:docstring>
          The new state of the device.
        </tp:docstring>
      </arg>
      <arg name="old_state" type="u" tp:type="NM_DEVICE_STATE">
        <tp:docstring>
          The previous state of the device.
        </tp:docstring>
      </arg>
      <arg name="reason" type="u" tp:type="NM_DEVICE_STATE_REASON">
        <tp:docstring>
          A reason for the state transition.
        </tp:docstring>
      </arg>
    </signal>

    <tp:enum name="NM_DEVICE_STATE" type="u">
      <tp:enumvalue suffix="UNKNOWN" value="0">
        <tp:docstring>
          The device is in an unknown state.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="UNMANAGED" value="1">
        <tp:docstring>
          The device is not managed by NetworkManager.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="UNAVAILABLE" value="2">
        <tp:docstring>
          The device cannot be used (carrier off, rfkill, etc).
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="DISCONNECTED" value="3">
        <tp:docstring>
          The device is not connected.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="PREPARE" value="4">
        <tp:docstring>
          The device is preparing to connect.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="CONFIG" value="5">
        <tp:docstring>
          The device is being configured.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="NEED_AUTH" value="6">
        <tp:docstring>
          The device is awaiting secrets necessary to continue connection.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="IP_CONFIG" value="7">
        <tp:docstring>
          The IP settings of the device are being requested and configured.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="ACTIVATED" value="8">
        <tp:docstring>
          The device is active.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="FAILED" value="9">
        <tp:docstring>
          The device is in a failure state following an attempt to activate it.
        </tp:docstring>
      </tp:enumvalue>
    </tp:enum>
    ...

<example>

<yv:member type="a{ss}" name="shortTitle">

->

std::map< std::string, std::string > shortTitle;


<introspect>
dbus-send  --session --type=method_call --print-reply --dest='org.freedesktop.DBus' / org.freedesktop.DBus.Introspectable.Introspect

method return sender=org.freedesktop.DBus -> dest=:1.255 reply_serial=2
   string "<!DOCTYPE node PUBLIC "-//freedesktop//DTD D-BUS Object Introspection 1.0//EN"
"http://www.freedesktop.org/standards/dbus/1.0/introspect.dtd">
<node>
  <interface name="org.freedesktop.DBus"> ~
    <method name="Hello">
      <arg direction="out" type="s"/>
    </method>
    ...
    <method name="ListNames"> ~
      <arg direction="out" type="as"/>
    </method>
    ...
  </interface>
  <interface name="org.freedesktop.DBus.Introspectable"> ~
    <method name="Introspect">
      <arg direction="out" type="s"/>
    </method>
  </interface>
</node>
"


={============================================================================
*kt_linux_core_702* dbus: dbus-send tool

dbus-send, distributed with D-Bus, allows you to invoke methods on services from
the command line.

http://dbus.freedesktop.org/doc/dbus-send.1.html

dbus-send — Send a message to a message bus

Synopsis

dbus-send [ --system | --session | --address=ADDRESS ] [--dest=NAME] 
   [ --print-reply [=literal]] [--reply-timeout=MSEC] [--type=TYPE] 
   OBJECT_PATH INTERFACE.MEMBER [CONTENTS...]

DESCRIPTION

The dbus-send command is used to send a message to a D-Bus message bus. See
http://www.freedesktop.org/software/dbus/ for more information about the big
picture.

note: system vs session

There are two well-known message buses: the systemwide message bus (installed on
    many systems as the "messagebus" service) and the per-user-login-session
message bus (started each time a user logs in). 

The --system and --session options direct dbus-send to send messages to the
system or session buses respectively. note: If neither is specified, dbus-send
sends to the session bus.

Nearly all uses of dbus-send must provide the --dest argument which is the name
of a connection on the bus to send the message to. If --dest is omitted, no
destination is set.

note: message is either method or signal.

The object path and the name of the message to send must always be specified.
Following arguments, if any, are the message contents (message arguments). These
are given as type-specified values and may include containers (arrays, dicts,
        and variants) as described below.

<contents>   ::= <item> | <container> [ <item> | <container>...]
<item>       ::= <type>:<value>
<container>  ::= <array> | <dict> | <variant>
<array>      ::= array:<type>:<value>[,<value>...]
<dict>       ::= dict:<type>:<type>:<key>,<value>[,<key>,<value>...]
<variant>    ::= variant:<type>:<value>
<type>       ::= string | int16 | uint 16 | int32 | uint32 | int64 | uint64 
                        | double | byte | boolean | objpath

D-Bus supports more types than these, but dbus-send currently does not. Also,
dbus-send does not permit empty containers or nested containers (e.g. arrays of
        variants).

Here is an example invocation:

  dbus-send --dest=org.freedesktop.ExampleName               \
            /org/freedesktop/sample/object/name              \
            org.freedesktop.ExampleInterface.ExampleMethod   \
            int32:47 string:'hello world' double:65.32       \
            array:string:"1st item","next item","last item"  \
            dict:string:int32:"one",1,"two",2,"three",3      \
            variant:int32:-8                                 \
            objpath:/org/freedesktop/sample/object/name

  dbus-send --session --type=method_call --print-reply --dest=Zinc.MediaProxy2
  /Zinc/Media/MediaRouters/0 Zinc.Media.MediaRouter.getSourceInformation

Note that the interface is separated from a method or signal name by a dot,
though in the actual protocol the interface and the interface member are
    separate fields.  

OPTIONS

The following options are supported:

--dest=NAME
    Specify the name of the connection to receive the message.

--print-reply
    'block' for a reply to the message sent, and print any reply received in a
    human-readable form. It also means the message type (--type=) is
    method_call.

--print-reply=literal
    Block for a reply to the message sent, and print the body of the reply. If
    the reply is an object path or a string, it is printed literally, with no
    punctuation, escape characters etc.

--reply-timeout=MSEC
    Wait for a reply for up to MSEC milliseconds. The default is
    implementation-defined, typically 25 seconds.

--system
    Send to the system message bus.

--session
    Send to the session message bus. (This is the default.)

--address=ADDRESS
    Send to ADDRESS.

--type=TYPE
    Specify method_call or signal (defaults to "signal").


={============================================================================
*kt_linux_core_703* dbus: lsdbus

Get the list of activateable bus names
dbus-send  --session --type=method_call --print-reply --dest='org.freedesktop.DBus' \
/ org.freedesktop.DBus.ListActivatableNames

to find the owners of all dbus connections
dbus-send  --session --type=method_call --print-reply --dest='org.freedesktop.DBus' \
/ org.freedesktop.DBus.ListNames

note: org.freedesktop.Dbus means the dbus itself and ListNmaes to get the list of names on the bus

method return sender=org.freedesktop.DBus -> dest=:1.81 reply_serial=2
   array [
      string "org.freedesktop.DBus"
      string ":1.7"
      string ":1.8"
      string "Zinc.BabySitter"
      string "Zinc.Application"
      string "org.freedesktop.NetworkManager"
      string "Zinc.Crb"
      string "Zinc.OEMSystemTime"
      string ":1.81"
      string "Zinc.ApplicationPackages"
      string ":1.41"
      string "Zinc.UsageCollection"
      string ":1.42"
      string ":1.65"
      string ":1.21"
      string ":1.43"
      string ":1.66"
      string ":1.44"
      string ":1.67"
      string ":1.45"
      string "org.freedesktop.NetworkManagerSystemSettings"
      string ":1.46"
      string ":1.69"
      string "Zinc.Media"
      string ":1.47"
      string ":1.48"
      string "Zinc.OEMSystem"
      string ":1.29"
      string "Zinc.ContentAcquisition"
      string "Zinc.Broker"
      string "org.freedesktop.Avahi"
      string "Zinc.DeviceSoftware"
      string "Zinc.RemoteBooking"
      string "Zinc.LinearSource"
      string "Zinc.DeviceManager"
      string "Zinc.Tuner"
      string ":1.70"
      string ":1.71"
      string "Zinc.OEMSystemManager"
      string ":1.72"
      string ":1.73"
      string ":1.75"
      string "Zinc.MetadataProxy"
      string ":1.53"
      string ":1.76"
      string "Zinc.Reminders"
      string ":1.34"
      string "Zinc.Metadata"
      string "Zinc.System"
      string ":1.0"
      string ":1.58"
      string "Zinc.RemoteDiagnostics"
      string ":1.4"
      string ":1.5"
      string ":1.18"
      string ":1.19"
   ]


# lsdbus
793   :1.0                     /opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
793   org.freedesktop.NetworkManager/opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
793   org.freedesktop.NetworkManagerSystemSettings/opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
831   :1.4                     /opt/zinc/bin/litaniumsystemmanagerd            
831   :1.5                     /opt/zinc/bin/litaniumsystemmanagerd            
831   Zinc.OEMSystemManager    /opt/zinc/bin/litaniumsystemmanagerd            
831   Zinc.System              /opt/zinc/bin/litaniumsystemmanagerd            
...


#cat /opt/zinc/devel/bin/lsdbus
#!/bin/sh -e

# Print a list of all dbus connections and the processes that own them.
# Taken from the wiki page:
# https://wiki.youview.co.uk/display/canvas/How+to+Introspect+DBus+from+the+Command+Line

# The format of the output is:
# pid | bus name | command

function ListBusNames {
    dbus-send --session --type=method_call --print-reply \
              --dest=org.freedesktop.DBus / \
              org.freedesktop.DBus.ListNames
}

function GetProcessID {
    dbus-send --session --type=method_call --print-reply \
              --dest=org.freedesktop.DBus / \
              org.freedesktop.DBus.GetConnectionUnixProcessID string:$1 2>/dev/null \
        | xargs -n1 | tail -1
}

for i in $(ListBusNames | grep string | cut -d'"' -f2)
do
    DCNAME=$i
    DCPID=$(GetProcessID $DCNAME)
    if [ -n "$DCPID" ]
    then
        DCPCMD=$(cat /proc/$DCPID/cmdline 2>/dev/null | xargs -0 echo) && \
               printf "%-6s%-25s%-40s%-8s%s\n" \
                  "$DCPID" "$DCNAME" "$DCPCMD"
    fi
done | sort -n


={============================================================================
*kt_linux_core_704* dbus: dbus-monitor tool

http://dbus.freedesktop.org/doc/dbus-monitor.1.html

Distributed with D-Bus, prints out traffic on the bus. You can filter the output by passing match
rules as arguments. 

dbus-monitor [ --system | --session | --address ADDRESS ] 
  [ --profile | --monitor ] [ watch expressions ]

<session>
There are two well-known message buses: the systemwide message bus; installed on
many systems as the "messagebus" service and the per-user-login-session message
bus; started each time a user logs in.  The --system and --session options
direct dbus-monitor to monitor the system or session buses respectively. 

note:
If neither is specified, dbus-monitor monitors the session bus.

<monitor>
dbus-monitor has two different output modes, the 'classic'-style monitoring mode
and profiling mode.  The profiling format is a compact format with a single line
per message and microsecond-resolution timing information. The --profile and
--monitor options select the profiling and monitoring output format
respectively. 

note: If neither is specified, dbus-monitor uses the monitoring output format.


<profile>
--profile
    Use the profiling output format.

[root@HUMAX /]# /opt/zinc-trunk/oss/bin/dbus-monitor --profile 
  "interface=Zinc.Media.MediaRouter,member=getSourceInformation" "type=method_call" "type=method_return"

sig	1442323921	119827	2	/org/freedesktop/DBus	org.freedesktop.DBus	NameAcquired
mc	1442323921	124985	3	:1.167	/org/freedesktop/DBus	org.freedesktop.DBus	AddMatch
mc	1442323921	128350	4	:1.167	/org/freedesktop/DBus	org.freedesktop.DBus	AddMatch
mc	1442323923	728265	1587	:1.72	/Zinc/Tuner/LinearPlaybackControl	Zinc.Tuner.LinearPlaybackControl	getSourceInformation
mc	1442323923	731820	1424	:1.80	/Zinc/Media/DefaultMediaRouter	Zinc.Media.MediaRouter	getSourceInformation
mr	1442323923	734371	772	1424	:1.80
mr	1442323923	735621	1425	1587	:1.72
mc	1442323927	846109	1588	:1.72	/Zinc/Broker/UnifiedEventRepository	Zinc.Broker.UnifiedEventRepository	getScheduleEvents
mc	1442323927	854912	3206	:1.22	/Zinc/Metadata/EventRepository	Zinc.Metadata.EventRepository	getScheduleEvents
mr	1442323927	868006	1366	3206	:1.22
mr	1442323927	884479	3207	1588	:1.72
mc	1442323927	890274	1589	:1.72	/Zinc/Broker/UnifiedEventRepository	Zinc.Broker.UnifiedEventRepository	getScheduleEvents
mc	1442323927	895714	3208	:1.22	/Zinc/Metadata/EventRepository	Zinc.Metadata.EventRepository	getScheduleEvents
mr	1442323927	901114	1367	3208	:1.22
mr	1442323927	906785	3209	1589	:1.72
mc	1442323927	910330	1590	:1.72	/Zinc/Broker/UnifiedEventRepository	Zinc.Broker.UnifiedEventRepository	getScheduleEvents


<watch-expr>
In order to get dbus-monitor to see the messages you are interested in, you
should specify a set of watch expressions as you would expect to be passed to
the dbus_bus_add_match function.

# display only method calls, returns and errors. No signals at all will be
# displayed but it does have the benefit of you not getting the PositionChange
# signals cluttering your display.

dbus-monitor "type=method_call" "type=method_return" "type=error"

dbus-monitor profile 
  "interface=Zinc.Application.ApplicationManager,member=launchApplication" \ 
  "interface=Zinc.Application.ApplicationManager,member=ApplicationLifecycleEvent"

dbus-monitor > /mnt/hd1/mylogs.log &
dbus-monitor | tee /mnt/hd1/mylogs.log &


# To launch Dbus-Monitor on your STB, and inspect MediaRouter activity, run the
# following command:

dbus-monitor "interface=Zinc.Media.MediaRouter,member=getSourceInformation"
  "type=method_call" "type=method_return"

<log>
[07-07-2015 07:45:04.848504] 
method call sender=:1.246 -> dest=Zinc.Media serial=2
path=/Zinc/Media/DefaultMediaRouter; interface=Zinc.Media.MediaRouter;
member=stop

[07-07-2015 07:45:04.864666] 
method return sender=:1.6 -> dest=:1.246 reply_serial=2

<>

// 1.247 becomes dbussenddaemon

[07-07-2015 07:45:05.164966] 
signal sender=org.freedesktop.DBus -> dest=(null destination) 
  serial=413 path=/org/freedesktop/DBus;
  interface=org.freedesktop.DBus; member=NameOwnerChanged string
  "Zinc.DBusSendDaemon" string "" string ":1.247"

[07-07-2015 07:45:05.165242] 
method call sender=:1.247 -> dest=org.freedesktop.DBus 
  serial=2 path=/org/freedesktop/DBus; 
  interface=org.freedesktop.DBus; member=RequestName
  string "Zinc.DBusSendDaemon"
  uint32 0

// dbussenddaemon fowards a call

[07-07-2015 07:45:07.926145] 
method call sender=:1.249 -> dest=Zinc.DBusSendDaemon serial=2
path=/Zinc/Media/MediaRouterFactory; interface=Zinc.Media.MediaRouterFactory;
  member=createMediaRouter string "Zinc.MediaProxy"

[07-07-2015 07:45:07.935001] 
method call sender=:1.247 -> dest=Zinc.MediaProxy serial=3
path=/Zinc/Media/MediaRouterFactory; interface=Zinc.Media.MediaRouterFactory;
  member=createMediaRouter


// fowards a reply

[07-07-2015 07:45:07.945512] 
method return sender=:1.231 -> dest=:1.247 reply_serial=3 object path
"/Zinc/Media/MediaRouters/1"

[07-07-2015 07:45:07.948415] 
method return sender=:1.247 -> dest=:1.249 reply_serial=2 object path
"/Zinc/Media/MediaRouters/1"


={============================================================================
*kt_linux_core_710* dbus: kdbus

{kdbus}
https://github.com/gregkh/presentation-kdbus
https://github.com/gregkh/kdbus


={============================================================================
*kt_linux_sete_005* admin-check running services

sudo service --status-all


={============================================================================
*kt_linux_sete_005* admin: check nvidia version

$ cat /proc/driver/nvidia/version 
NVRM version: NVIDIA UNIX x86_64 Kernel Module  340.96  Sun Nov  8 22:33:28 PST 2015
GCC version:  gcc version 4.8.4 (Debian 4.8.4-1) 


={============================================================================
*kt_linux_sete_005* admin-gnome: check gnome version

$ gnome-session --version
gnome-session 3.14.0


={============================================================================
*kt_linux_sete_005* admin-gnome: shortcuts

{activate-button}
To access your windows and applications, click the Activities button, or just
move your mouse pointer to the top-left hot corner. 

You can also press the 'super' key on your keyboard. You can see your windows
and applications in the overview.  You can also just start typing to search your
applications, files, folders and the web. note: this is window key.


{tile-windows}
Super+Left     " to left
Super+Right    " to right
Super+Up       " to max
Supaer+Down    " back to original size


{lock-screen}
Super-L


{keyboard-shortcuts}
https://wiki.gnome.org/Design/OS/KeyboardShortcuts
https://help.gnome.org/users/gnome-help/stable/shell-keyboard-shortcuts.html.en

To change key maps:

System Setting -> Keyboard -> Shortcuts

={============================================================================
*kt_linux_sete_005* admin: firefox key shortcuts

Ctrl+D                     Add Bookmark
Ctrl+B or Ctrl+I           Bookmarks

Backspace or Alt+<-        Back
Shft+Backspace or Alt+->   Forward

Ctrl+W or Ctrl+F4          Close Tab
Ctrl+T                     New Tab
Ctrl+Tab or Ctrl+PageDown  Next Tab

F5 or Ctrl+R               Reload


={============================================================================
*kt_linux_sete_005* admin: tbird key shortcuts

https://support.mozilla.org/en-US/kb/keyboard-shortcuts

Tagging and marking your messages

Mark Message as Read/Unread     M
Mark Thread as Read             R


={============================================================================
*kt_linux_tool_000* tool-vnc

// create a session
vncserver -localhost no -geometry 1900x1200 -depth 24

sudo apt-get install xrdp

Then run windows remote desktop and connect using ip or hostname. That's it.

<q-and-a>
I use Ubuntu on my desktop. When I am away from my desktop, I would like to
access the session using my Windows 7 laptop. Currently, I am using xrdp to
connect, but it starts up a remote session. Is there any way to just use the
same desktop session? I want to be able to pick up where I left off on the
desktop.

http://askubuntu.com/questions/235905/use-xrdp-to-connect-to-desktop-session

// original when installs

/etc/xrdp/xrdp.ini

[xrdp1]
name=sesman-Xvnc
lib=libvnc.so
username=ask
password=ask
ip=127.0.0.1
port=-1

// this works as said in the above link but not for dual 

[xrdp1]
name=sesman-Xvnc
lib=libvnc.so
username=
password=ask
ip=127.0.0.1
port=5900


={============================================================================
*kt_linux_set_000* gnome-application
  
change default application setting

Change a map between file type and default application in:

/usr/share/applications/defaults.list -> /etc/gnome/defaults.list


<application-file>
Place this file in the /usr/share/applications directory so that it is
accessible by everyone, or in ~/.local/share/applications if you only wish to
make it accessible to a single user. 


={============================================================================
*kt_linux_sete_101* gnome: workspace

https://help.gnome.org/users/gnome-help/stable/shell-workspaces.html.en

To add a workspace, drag and drop a window from an existing workspace onto the
empty workspace in the workspace selector. This workspace now contains the
window you have dropped, and a new empty workspace will appear below it.

default keymap:

Cult-Alt-Up       move to workspace above
Cult-Alt-Down     move to workspace below


={============================================================================
*kt_linux_sete_101* gnome: change wallpapers

http://fabhax.com/technology/change-wallpapers-in-gnome-3.4/


={============================================================================
*kt_linux_sete_105* web server

{nginx}

http://wiki.nginx.org/Main

$ sudo apt-get install nginx

Nginx has become one of the most important web servers over the last couple of
years. There's a reason for that. Instead of using the standard threaded- or
process-oriented architecture, it uses a scalable, event-driven (asynchronous)
  architecture. 

So not only is it incredibly light weight, it's highly scalable and memory usage
is far better suited for limited resource deployments. Nginx also handles simple
load balancing, fault tolerance, auto-indexing, virtual servers (both name- and
    IP-based), mod_rewrite, access control, and much more. Nginx can also serve
as a reverse proxy and an IMAP/POP3 proxy server.

Surprisingly, Nginx powers a few very high-profile sites, such as: Netflix,
  Hulu, Pinterest, Wordpress.com, and AirBnB.

Who is Nginx right for? The nice thing about this particular light weight HTTPD
daemon is that it doesn't perform like a lightweight server. Not only does it
run with minimal resources, it offers plenty of optional modules and addons. You
can find pre-built packages for Linux and BSD for easy installation. So if you
need a powerhouse server, in a lighter weight package, Nginx is the server for
you.

Nginx comes in at a 10 MB installation (versus the Apache 30 MB installation)
  and can give you up to a 35 percent performance increase (versus Apache).

<home>
$ ll /usr/share/nginx/www

<config>

/etc/nginx/sites-enabled/default 

server {
  #listen   80; ## listen for ipv4; this line is default and implied
  #listen   [::]:80 default_server ipv6only=on; ## listen for ipv6

  root /usr/share/nginx/www;
  index index.html index.htm;
  ...
}


<commands>

Use the following command:

# /etc/init.d/nginx restart
# /etc/init.d/nginx reload

# service nginx restart
# service nginx reload

However, recommend way is as follows. This should work with any Linux
distributions or Unix like operating systems:

# nginx -s reload
# /path/to/full/nginx -s reload


{proxy-pass}
http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass

Take a look at nginx's HttpProxyModule, which is where proxy_pass comes from.
The proxy_pass docs say:

This directive sets the address of the proxied server and the URI to which
location will be mapped.

So when you tell Nginx to proxy_pass, you're saying "Pass this request on to
this proxy URL".


<okay>
This is a configuration for local web server and want want to have is when there
are requests to the local server, it would redirect those to the actual server.

wget http://wll1p04345.dev.youview.co.uk/e/pseudolive/bbb/client_manifest.mpd;

To:

wget http://dash.bidi.int.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd;


location / {
# First attempt to serve request as file, then
# as directory, then fall back to displaying a 404.
# try_files $uri $uri/ /index.html;
# Uncomment to enable naxsi on this location
# include /etc/nginx/naxsi.rules
  proxy_pass http://dash.bidi.int.bbc.co.uk;
}

<not-okay>

location / {
# First attempt to serve request as file, then
# as directory, then fall back to displaying a 404.
try_files $uri $uri/ /index.html;

# Uncomment to enable naxsi on this location
# include /etc/nginx/naxsi.rules
  proxy_pass http://dash.bidi.int.bbc.co.uk;
}


={============================================================================
*kt_linux_sete_105* set: update adobe flash plugin

sudo update-flashplugin-nonfree --install


={============================================================================
*kt_linux_sete_200* which to install?

{pdf-viewer}
Okular


# ============================================================================
#{
={============================================================================
*kt_linux_ref_001* references

{ref-UNP}
Richard Stevens. Unix Network Programming Vol 2 Addision Wesley. 2nd Ed.
http://www.kohala.com/start/unpv22e/unpv22e.html

{ref-LPI}
The Linux Programming Interface: A Linux and UNIX System Programming Handbook 
Michael Kerrisk (Author) 

For sources, http://www.man7.org/tlpi/


-------------------------------------------------------------------------------
Copyright: see |ktkb|  vim:tw=100:ts=3:ft=help:norl:

