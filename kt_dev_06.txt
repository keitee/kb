*kt_dev_06*                                                                tw=100

kt.dev.boost

/^[#=]{
Use #{ for a group and ={ for a item

|kt_dev_boost_000| install, build and run
|kt_dev_boost_001| bind: to use member function
|kt_dev_boost_002| program_options
|kt_dev_boost_003| posix_time
|kt_dev_boost_004| typeof
|kt_dev_boost_005| is_same
|kt_dev_boost_006| lexical_cast
|kt_dev_boost_007| filesystem
|kt_dev_boost_008| boost-sync: condition 
*kt_dev_boost_009* boost-function
*kt_dev_boost_010* boost-intrusive-ptr: usage
*kt_dev_boost_010* boost-ptr-map:
*kt_dev_boost_010* boost-signal

|kt_dev_boost_100| asio
|kt_dev_boost_101| asio: tutorial
|kt_dev_boost_102| asio: deadline_timer
|kt_dev_boost_103| asio: io_service
|kt_dev_boost_104| asio: A guide to getting started with boost::asio


# ============================================================================
#{
={============================================================================
*kt_dev_boost_000* install, build and run

http://www.boost.org/doc/libs/1_58_0/more/getting_started/unix-variants.html

1. download and untar
tar --bzip2 -xf /path/to/boost_1_58_0.tar.bz2

2. build and install
3. include in source:

#include <boost/typeof/typeof.hpp>

using namespace boost;

// don't need if installed with defaults.
// 4. specify include directory
// $ g++ -g -std=c++0x -I /usr/local/boost_1_58_0 tboost.cpp 


{build}
If you want to use any of the separately-compiled Boost libraries, you'll need to acquire library
binaries.

Issue the following commands in the shell (don't type $; that represents the shell's prompt):

$ cd path/to/boost_1_58_0
$ ./bootstrap.sh --help

Select your configuration options and invoke ./bootstrap.sh again without the --help option. Unless
you have write permission in your system's /usr/local/ directory, you'll probably want to at least
use

$ sudo ./bootstrap.sh --prefix=path/to/installation/prefix
$ sudo ./b2

to install somewhere else. Also, consider using the --show-libraries and
--with-libraries=library-name-list options to limit the long wait you'll experience if you build
everything. Finally,

$ ./b2 install


{build-error}
5.4   In Case of Build Errors

The only error messages you see when building Boost—if any—should be related to
the IOStreams library's support of zip and bzip2 formats as described here.
Install the relevant development packages for libz and libbz2 if you need those
features. Other errors when building Boost libraries are cause for concern.

Do install and run ./b2 again.

$ sudo apt-get install libbz2-dev


{link}
$ g++ -g t_asio_01.cpp

/usr/local/include/boost/asio/error.hpp:230: undefined reference to `boost::system::system_category()'
collect2: error: ld returned 1 exit status

To solve this since The boost library you are using depends on the boost_system
library. (Not all of them do.)

$ g++ -g -lboost_system t_asio_01.cpp


$ cat ./boobl.sh 
#!/bin/bash
g++ -g -std=c++0x -lboost_system $1

$ ./a.out
./a.out: error while loading shared libraries: libboost_system.so.1.58.0: cannot
open shared object file: No such file or directory

$ LD_LIBRARY_PATH=/usr/local/lib ./a.out 


={============================================================================
*kt_dev_boost_001* bind: to use member function

This is member function pointer declaration:

void (SystemClientEventRepository::*masCallback)
(boost::shared_ptr<X>, const FutureValue< std::vector<Y> > &) =
                &SystemClientEventRepository::eventsReceived;

This is the use of bind:

bar.addWithCallback(fMASEvents, boost::bind(masCallback, this, sharedCmd, _1));

However, bind uses 3 arguments but the pointer declaration uses 2 arguments.  Why?


{from-boost-doc}
Using bind with pointers to members

Pointers to member functions and pointers to data members are 'not' function objects, because they
do not support operator(). For convenience, bind accepts member pointers as its first argument, and
the behavior is as if boost::mem_fn has been used to convert the member pointer into a function
object. In other words, the expression

bind(&X::f, args)

is equivalent to

bind<R>(mem_fn(&X::f), args)

where R is the return type of X::f (for member functions) or the type of the member (for data
        members.)

Example:

struct X
{
    bool f(int a);
};

X x;

shared_ptr<X> p(new X);

int i = 5;

bind(&X::f, ref(x), _1)(i);		// x.f(i)
bind(&X::f, &x, _1)(i);			//(&x)->f(i)
bind(&X::f, x, _1)(i);			// (internal copy of x).f(i)
bind(&X::f, p, _1)(i);			// (internal copy of p)->f(i)

The last two examples are interesting in that they produce "self-contained" function objects.
bind(&X::f, x, _1) stores a copy of x. bind(&X::f, p, _1) stores a copy of p, and since p is a
boost::shared_ptr, the function object retains a reference to its instance of X and will remain
valid even when p goes out of scope or is reset(). 

note:
This doc do not have more detail but the stackoverflow said

boost::function<void (int)> f2( boost::bind( &myclass::fun2, this, _1 ) );

to use member function and this seems to be the same with 

bind(&X::f, &x, _1)(i);			//(&x)->f(i)

So this binds to a particular object.


={============================================================================
*kt_dev_boost_002* program_options

#include <boost/program_options.hpp>
#include <boost/date_time.hpp>

void parseProgramOptions(int argc, char** argv)
{
    std::string output_filename;
    unsigned long timeout_seconds = 0;
    unsigned int window_size = 20;

    boost::program_options::options_description desc("\nWaits until the system becomes idle "
                                                     "and exits.\nOptions");
    desc.add_options()
            ("help,h", "Show this help")
            ("window_size,w", boost::program_options::value(&window_size),
                             "Size of moving-average window (in number of values)")
            ("sampling_period,s", boost::program_options::value(&sampling_period_ms),
                                 "Number of milliseconds between each sample")
            ("output,o", boost::program_options::value(&output_filename),
                                  "File name to print the values to (default=stdout)")
            ("exit_threshold,e", boost::program_options::value(&exit_threshold),
                                "If specified specified (in %) - program will exit if "
                                "CPU utilisation will fall below this value (default=1)")
            ("timeout,t", boost::program_options::value(&timeout_seconds),
                         "Timeout in seconds (default=infinite)")
            ;

    boost::program_options::variables_map vm;
    boost::program_options::store(boost::program_options::parse_command_line(argc, argv, desc), vm);
    boost::program_options::notify(vm);
    if (vm.count("help"))
    {
        std::cout << desc << std::endl;
        exit(EXIT_SUCCESS);
    }

    // resize moving average window
    ma.resize(window_size);

    started_at = boost::posix_time::microsec_clock::local_time();
    if(timeout_seconds > 0)
    {
        stop_at = started_at + boost::posix_time::seconds(timeout_seconds);
    }

    if(vm.count("output"))
    {
        file.open(output_filename.c_str());
        os = &file;
    }

    *os << "started at     : " << started_at.time_of_day() << "\n";
    *os << "sampling period: " << sampling_period_ms << " ms\n";
}


={============================================================================
*kt_dev_boost_003* posix_time

#include <boost/date_time.hpp>


={============================================================================
*kt_dev_boost_004* typeof

http://www.boost.org/doc/libs/1_58_0/doc/html/typeof.html

Motivation

<object-generator>
Today many template libraries supply object generators to simplify object creation by utilizing the
C++ template argument deduction facility. Consider std::pair. In order to instantiate this class
template and create a temporary object of this instantiation, one has to supply template parameters,
as well as parameters to the constructor:

std::pair<int, double>(5, 3.14159);

To avoid this 'duplication', STL supplies the std::make_pair object generator. When it is used, the
    types of template parameters are deduced from supplied function arguments:

std::make_pair(5, 3.14159);

For the temporary objects it is enough. However, when a named object needs to be allocated, the
    problem appears again:

std::pair<int, double> p(5, 3.14159);

The object generator no longer helps:

std::pair<int, double> p = std::make_pair(5, 3.14159);

It would be nice to deduce the type of the object (on the left) from the expression it is
    initialized with (on the right), but the current C++ syntax does not allow for this.

note: l

The above example demonstrates the essence of the problem but does not demonstrate its scale. Many
libraries, especially expression template libraries, create objects of really complex types, and go
a long way to hide this complexity behind object generators. Consider a nit Boost.Lambda functor:

_1 > 15 && _2 < 20

If one wanted to allocate a named copy of such an innocently looking functor, she would have to
specify something like this:

lambda_functor<
    lambda_functor_base<
        logical_action<and_action>,
        tuple<
            lambda_functor<
                lambda_functor_base<
                    relational_action<greater_action>,
                    tuple<
                        lambda_functor<placeholder<1> >,
                        int const
                    >
                >
            >,
            lambda_functor<
                lambda_functor_base<
                    relational_action<less_action>,
                    tuple<
                        lambda_functor<placeholder<2> >,
                        int const
                    >
                >
            >
        >
    >
>
f = _1 > 15 && _2 < 20;

Not exactly elegant. To solve this problem (as well as some other problems), the C++ standard
    committee is considering a few additions to the standard language, such as typeof/decltype and
    auto (see http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1607.pdf).

The typeof operator (or decltype, which is a slightly different flavor of typeof) allows one to
determine the type of an expression at compile time. Using typeof, the above example can be
simplified drastically:

typeof(_1 > 15 && _2 < 20) f = _1 > 15 && _2 < 20;

Much better, but some duplication still exists. The auto type solves the rest of the problem:

auto f = _1 > 15 && _2 < 20;

The purpose of the Boost.Typeof library is to provide a library-based solution, which could be used
    until the language-based facility is added to the Standard and becomes widely available. 


Tutorial

The above examples are possible because the Typeof Library knows about primitive types, such as int,
    double, char, etc. The Typeof Library also knows about most types and templates defined by the
        Standard C++ Library, but the appropriate headers need to be included to take advantage of
        this:


#include <boost/typeof/std/utility.hpp>

namespace ex3
{
    BOOST_AUTO(p, make_pair(1, 2));

    BOOST_STATIC_ASSERT((is_same<BOOST_TYPEOF(p), pair<int, int> >::value));
}

Here <boost/typeof/std/utility.hpp> includes <utility> and contains knowledge about templates
defined there. This naming convention applies in general, for example to let the Typeof Library
handle std::vector, include <boost/typeof/std/vector.hpp>, etc


={============================================================================
*kt_dev_boost_005* is_same

http://www.boost.org/doc/libs/1_50_0/libs/type_traits/doc/html/boost_typetraits/reference/is_same.html

is_same

template <class T, class U>
struct is_same : public true_type-or-false_type {};

Inherits: If T and U are the same types then inherits from true_type, otherwise inherits from
              false_type.

Header: #include <boost/type_traits/is_same.hpp> or #include <boost/type_traits.hpp>

Compiler Compatibility: If the compiler does not support partial-specialization of class templates,
         then this template can not be used with abstract, incomplete or function types.

Examples:

    is_same<int, int> inherits from true_type. 

    is_same<int, int>::type is the type true_type. 

    is_same<int, int>::value is an integral constant expression that evaluates to true. 

    is_same<int const, int>::value is an integral constant expression that evaluates to false. 

    is_same<int&, int>::value is an integral constant expression that evaluates to false. 

    is_same<T, T>::value_type is the type bool. 


#include <iostream>
#include <boost/typeof/typeof.hpp>

using namespace std;
using namespace boost;

int main(int argc, char** argv)
{
    int ival = 5;

    std::cout << "is the same type? " << is_same<decltype(ival), int>::value << std::endl;
}

$ g++ -g -std=c++0x -I /usr/local/boost_1_58_0 tboost.cpp 
$ ./a.out 
is the same type? 1


={============================================================================
*kt_dev_boost_006* lexical_cast

http://www.boost.org/doc/libs/1_58_0/doc/html/boost_lexical_cast.html

Motivation

Sometimes a value must be converted to a literal text form, such as an int
represented as a std::string, or vice-versa, when a std::string is interpreted
as an int. Such examples are common when converting between data types internal
to a program and representation external to a program, such as windows and
configuration files.

The standard C and C++ libraries offer a number of facilities for performing
such conversions. However, they vary with their ease of use, extensibility, and
safety.

For instance, there are a number of limitations with the family of standard C
functions typified by 'atoi':

o Conversion is supported in one direction only: from text to internal data
type. Converting the other way using the C library requires either the
inconvenience and compromised safety of the sprintf function, or the loss of
portability associated with non-standard functions such as itoa.

o The range of types supported is only a subset of the built-in numeric types,
namely int, long, and double.

o The range of types cannot be extended in a uniform manner. For instance,
conversion from string representation to complex or rational.

The standard C functions typified by strtol have the same basic limitations, but
offer finer control over the conversion process. However, for the common case
such control is often either not required or not used. The scanf family of
functions offer even greater control, but also lack safety and ease of use.

The standard C++ library offers stringstream for the kind of in-core formatting
being discussed. It offers a great deal of control over the formatting and
conversion of I/O to and from arbitrary types through text. However, for simple
conversions direct use of stringstream can be either clumsy (with the
    introduction of extra local variables and the loss of infix-expression
    convenience) or obscure (where stringstream objects are created as temporary
      objects in an expression). Facets provide a comprehensive concept and
    facility for controlling textual representation, but their perceived
    complexity and high entry level requires an extreme degree of involvement
    for simple conversions, and excludes all but a few programmers.

The lexical_cast function template offers a convenient and consistent form for
  supporting common conversions to and from arbitrary types when they are
  represented as text. The simplification it offers is in expression-level
  convenience for such conversions. For more involved conversions, such as where
  precision or formatting need tighter control than is offered by the default
  behavior of lexical_cast, the conventional std::stringstream approach is
  recommended. Where the conversions are numeric to numeric, boost::numeric_cast
  may offer more reasonable behavior than lexical_cast.

For a good discussion of the options and issues involved in string-based
formatting, including comparison of stringstream, lexical_cast, and others, see
Herb Sutter's article, The String Formatters of Manor Farm. Also, take a look at
the Performance section. 


<ex> to string

std::map<std::string, std::string> si;

static uint64_t fake_outputs = 0;

si["POST_REPAIR_OUTPUTS"] =
  boost::lexical_cast<std::string>(fake_outputs++);

<ex> to int

int EnvironmentAppIdStrategy::getAppId() const
{
    ApplicationRuntimeInstanceId appRuntimeInstanceId = 0;

    char* appIdFromEnv = getenv(APP_RUNTIME_INSTANCE_ID_ENV_VAR_NAME);

    // note: use stringstring to convert
 
    const std::string appIdAsString(appIdFromEnv);
    std::istringstream is(appIdAsString);
    is >> appRuntimeInstanceId;

    return appRuntimeInstanceId;
}

int EnvironmentAppIdStrategy::getAppId() const
{
    ApplicationRuntimeInstanceId appRuntimeInstanceId = 0;

    char* appIdFromEnv = getenv(APP_RUNTIME_INSTANCE_ID_ENV_VAR_NAME);

    return boost::lexical_cast<int>(appIdFromEnv);
}


Synopsis

lexical_cast
bad_lexical_cast
try_lexical_convert

Library features defined in boost/lexical_cast.hpp:

namespace boost
{
    class bad_lexical_cast;

    template<typename Target, typename Source>
      Target lexical_cast(const Source& arg);

    template <typename Target>
      Target lexical_cast(const AnyCharacterType* chars, std::size_t count);

    namespace conversion
    {
        template<typename Target, typename Source>
            bool try_lexical_convert(const Source& arg, Target& result);

        template <typename AnyCharacterType, typename Target>
            bool try_lexical_convert(const AnyCharacterType* chars, std::size_t count, Target& result);

    } // namespace conversion
} // namespace boost

lexical_cast

template<typename Target, typename Source>
  Target lexical_cast(const Source& arg);

Returns the result of streaming arg into a standard library string-based stream
and then out as a Target object. Where Target is either std::string or
std::wstring, stream extraction takes the whole content of the string, including
spaces, rather than relying on the default operator>> behavior. If the
conversion is unsuccessful, a bad_lexical_cast exception is thrown.

template <typename Target>
  Target lexical_cast(const AnyCharacterType* chars, std::size_t count);

Takes an array of count characters as input parameter and streams them out as a
Target object. If the conversion is unsuccessful, a bad_lexical_cast exception
is thrown. This call may be useful for processing nonzero terminated array of
characters or processing just some part of character array.

The requirements on the argument and result types for both functions are:

o Source is OutputStreamable, meaning that an operator<< is defined that takes a
std::ostream or std::wostream object on the left hand side and an instance of
the argument type on the right.

o Target is InputStreamable, meaning that an operator>> is defined that takes a
std::istream or std::wistream object on the left hand side and an instance of
the result type on the right.

o Target is CopyConstructible [20.1.3].

o Target is DefaultConstructible, meaning that it is possible to
default-initialize an object of that type [8.5, 20.1.4].

The character type of the underlying stream is assumed to be char. 


{peformance}
While we weren't looking Boost.Lexical_cast became "in most cases
boost::lexical_cast is faster than scanf, printf, std::stringstream". Yay for
upgrades - now there's no reason to use scanf for performance reasons.

http://www.boost.org/doc/libs/1_58_0/doc/html/boost_lexical_cast/
  performance.html#boost_lexical_cast.performance.tests_description

#include "PerfCounter.h"

#include <iostream>
#include <sstream>
#include <boost/lexical_cast.hpp>

int main(int argc,char** argv)
{
    PerfCounter counter;
    
    for(int i=0;i<10000;++i)
    {
        int out;
        sscanf("42","%d",&out);
    }
    counter.snap("scanf int");

    for(int i=0;i<10000;++i)
    {
        int out;
        std::stringstream ss("42");
        ss >> out;
    }
    counter.snap("stringstream int");

    for(int i=0;i<10000;++i)
    {
        int out = boost::lexical_cast<int>("42");
    }
    counter.snap("boost::lexical_cast<int>");
    std::cout << counter.dump() << std::endl;
}

// from a run on debian

$ ./a.out 
Start -> scanf int took 2112us
scanf int -> stringstream int took 27859us
stringstream int -> boost::lexical_cast<int> took 910us
boost::lexical_cast<int> -> end took 0us


<question>
// about the above code

> you're transforming the same value on every iteration, source of which is a
> const - might this be a subject of some optimisation / cache related thing?

Yeah, I was just thinking the same thing. I wouldn't be too surprised if the
compiler replaces the lexical_cast call with a constant result. Try with a
random number converted to a string.

<result>
Start -> scanf int took 22us
scanf int -> stringstream int took 45us
stringstream int -> boost::lexical_cast<int> took 3us
boost::lexical_cast<int> -> end took 0us

on the host and

Start -> scanf int took 46us
scanf int -> stringstream int took 392us
stringstream int -> boost::lexical_cast<int> took 11us
boost::lexical_cast<int> -> end took 4us

on the device.


={============================================================================
*kt_dev_boost_007* filesystem

// from code

// read all files from the given directory and populate those in vector
vector<boost::filesystem::path> configFiles(findMediaRouterPluginConfigFiles(configDir));

// "/opt/zinc-trunk/share/platform_data/mediarouter-plugin-config"
// /opt/zinc-trunk/share/platform_data/mediarouter-plugin-config/http-application%2Fdash%2Bxml.plugin-config
// /opt/zinc-trunk/share/platform_data/mediarouter-plugin-config/https-application%2Fdash%2Bxml.plugin-config

BOOST_FOREACH(const boost::filesystem::path& fullPath, configFiles)
{
    // "http-application%2Fdash%2Bxml"
    string fileStem = fullPath.stem();

    // fullPath.string() is:
    // "/opt/zinc-trunk/share/platform_data/mediarouter-plugin-config/http-application%2Fdash%2Bxml.plugin-config"
    //
}


={============================================================================
*kt_dev_boost_008* boost-sync: condition

http://www.boost.org/doc/libs/1_58_0/doc/html/thread/synchronization.html#thread.synchronization.condvar_ref

The classes condition_variable and condition_variable_any provide a mechanism
for one thread to wait for notification from another thread that a particular
  condition has become true. 
    
The general usage pattern is that one thread locks a mutex and then calls wait
on an instance of condition_variable or condition_variable_any. When the thread
is woken from the wait, then it checks to see if the appropriate condition is
now true, and continues if so. If the condition is not true, then the thread
then calls wait again to resume waiting. In the simplest case, this condition is
just a boolean variable:


boost::condition_variable cond;
boost::mutex mut;
bool data_ready;

void process_data();

void wait_for_data_to_process()
{
    boost::unique_lock<boost::mutex> lock(mut);
    while(!data_ready)
    {
        cond.wait(lock);
    }
    process_data();
}

Notice that the 'lock' is passed to wait: wait will atomically add the thread to
the set of threads waiting on the condition variable, and unlock the mutex. When
the thread is woken, the mutex will be locked again before the call to wait
returns. This allows other threads to acquire the mutex in order to update the
shared data, and ensures that the data associated with the condition is
correctly synchronized.

In the mean time, another thread sets the condition to true, and then calls
either notify_one or notify_all on the condition variable to wake one waiting
thread or all the waiting threads respectively.

void retrieve_data();
void prepare_data();

void prepare_data_for_processing()
{
    retrieve_data();
    prepare_data();
    {
        boost::lock_guard<boost::mutex> lock(mut);
        data_ready=true;
    }
    cond.notify_one();
}

Note that the same mutex is locked before the shared data is updated, but that
the mutex does not have to be locked across the call to notify_one.

This example uses an object of type condition_variable, but would work just as
well with an object of type condition_variable_any: condition_variable_any is
more general, and will work with any kind of lock or mutex, whereas
condition_variable 'requires' that the lock passed to wait is an instance of
boost::unique_lock<boost::mutex>. This enables condition_variable to make
optimizations in some cases, based on the knowledge of the mutex type;
condition_variable_any typically has a more complex implementation than
condition_variable.

-----------------
void wait(boost::unique_lock<boost::mutex>& lock)

Precondition:
    lock is locked by the current thread, and either no other thread is
    currently waiting on *this, or the execution of the mutex() member function
    on the lock objects supplied in the calls to wait or timed_wait in all the
    threads currently waiting on *this would return the same value as
    lock->mutex() for this call to wait.  

Effects:
    Atomically call lock.unlock() and blocks the current thread. The thread will
    unblock when notified by a call to this->notify_one() or this->notify_all(),
    or spuriously. When the thread is unblocked (for whatever reason), the lock
    is reacquired by invoking lock.lock() before the call to wait returns. The
    lock is also reacquired by invoking lock.lock() if the function exits with
    an exception.  

Postcondition:
    lock is locked by the current thread. 

Throws:
    boost::thread_resource_error if an error occurs. boost::thread_interrupted
    if the wait was interrupted by a call to interrupt() on the boost::thread
    object associated with the current thread of execution. 

-----------------
void notify_one()

Effects:
    If any threads are currently blocked waiting on *this in a call to wait or
    timed_wait, unblocks one of those threads.

Throws:
    Nothing. 


={============================================================================
*kt_dev_boost_009* boost-function

http://www.boost.org/doc/libs/1_59_0/doc/html/function/tutorial.html

Boost.Function has two syntactical forms: the preferred form and the portable
form. The preferred form fits more closely with the C++ language and reduces the
number of separate template parameters that need to be considered, often
improving readability; however, the preferred form is not supported on all
platforms due to compiler bugs. The compatible form will work on all compilers
supported by Boost.Function. Consult the table below to determine which
syntactic form to use for your compiler. 


Basic Usage

A function wrapper is defined simply by instantiating the function class
template with the desired return type and argument types, formulated as a C++
function type. 

Any number of arguments may be supplied, up to some implementation-defined limit
(10 is the default maximum). The following declares a function object wrapper f
that takes two int parameters and returns a float:


Preferred syntax

boost::function<float (int x, int y)> f;


Portable syntax

boost::function2<float, int, int> f;


By default, function object wrappers are 'empty', so we can create a function
  object to assign to f:

struct int_div { 
  float operator()(int x, int y) const { return ((float)x)/y; }; 
};

f = int_div();

Now we can use f to execute the underlying function object int_div:

std::cout << f(5, 3) << std::endl;

We are free to assign any compatible function object to f. If int_div had been
declared to take two long operands, the implicit conversions would have been
applied to the arguments without any user interference. The only limit on the
types of arguments is that they be CopyConstructible, so we can even use
references and arrays: 


={============================================================================
*kt_dev_boost_010* boost-intrusive-ptr: usage

http://baptiste-wicht.com/posts/2011/11/boost-intrusive_ptr.html

Boost intrusive_ptr : faster shared pointer

Baptiste Wicht

2011-11-14 08:40

This post will present the Boost intrusive_ptr and its usage in C++ programming.

One of the way I found was to replace some shared_ptr by some intrusive_ptr of
the Boost library.

It's a faster alternative of shared_ptr. Like its name indicates, it's
intrusive. The reference counter is 'included' directely in the managed class,
in the contrary of the shared_ptr where the reference counter has to be
  dynamically allocated to live 'aside' the object. 

This leads to some performances improvement. Considering memory, the footprint
of an intrusive_ptr is the same as the footprint of a raw pointer. This is not
the case for the shared_ptr that have a pointer to the object, a pointer to the
counter and the counter itself.

For example, if you have a class X:

  class X {
    std::string name;
    int age;
  };

And you use it in your code using a shared_ptr :

  void test(){
    std::shared_ptr<X> x(new X);

    std::cout << x->name << std::endl;
  }

and you want to use an intrusive_ptr, you have first to 'add' a reference
  counter inside the X class :

  class X {
    std::string name;
    int age;

    long references;
    X() : references(0) {}
  };

And you have to indicate to the intrusive_ptr where the reference counter can be
  found for this class :

  inline void intrusive_ptr_add_ref(X* x){
    ++x->references;
  }

  inline void intrusive_ptr_release(X* x){
      if(--x->references == 0) 
          delete x;
  }

And finally you can use the intrusive_ptr to replace your shared_ptr :

  void test(){
    boost::intrusive_ptr<X> x(new X);

    std::cout << x->name << std::endl;
  }

The smart pointer itself can be used exactly the same way as a shared_ptr. 
  
<support-multiple-types>
If you have several classes that are managed using an intrusive_ptr, you can use
a function template to tell Boost that all the reference counter are at the same
place :

  template<typename T>
    inline void intrusive_ptr_add_ref(T* expr){
      ++expr->references;
    }

  template<typename T>
    inline void intrusive_ptr_release(T* expr){
      if(--expr->references == 0)
        delete expr;
    }

As you can see, the pointer is very intrusive and needs some boilerplate code
  added to your application, but it can leads to some interesting improvements
  for classes very often dynamically allocated.

There is another advantage in using intrusive_ptr. As the reference counter is
  stored into the object itself, you can create several intrusive_ptr to the
  same object without any problem. This is not the case when you use a
  shared_ptr. Indeed, if you create two shared_ptr to the same dynamically
  allocated object, they will both have a 'different' references counter and at
  the end, you will end up with an object being deleted twice.

Of course, there are not only advantages. First of all, you have to declare a
field in every classes that you want to manage using an intrusive_ptr and you
have to declare functions to manage the reference. Then there are some
disadvantages when using this pointer type compared to a shared_ptr :

- It's impossible to create a weak_ptr from a intrusive_ptr 

- Code redundancy, you have to copy the reference counter in every class that
you want to use an intrusive_ptr with 

- You have to provide a function for every types that has to be used with
intrusive_ptr (only two functions if you use the template versions of the two
    functions)

To conclude, the boost::intrusive_ptr can be a good replacement of
std::shared_ptr in a performance critical application, but if you have no
performances problem, do not use it because it makes your code less clear. 

If you are concerned by performances when using std::shared_ptr, consider also
using std::make_shared to create your pointers, so that the reference counter
and the object itself will be allocated at the same 'place' and at the same
time, resulting in better performances. 

note: same place?

Another case where it's interesting to use an intrusive_ptr is when dealing with
libraries using a lot of raw pointers, because you can create several
intrusive_ptr to the same raw pointer without any problem.


={============================================================================
*kt_dev_boost_010* boost-intrusive-ptr: 

http://www.boost.org/doc/libs/master/libs/smart_ptr/intrusive_ptr.html

The intrusive_ptr class template stores a pointer to an object with an embedded
reference count. Every new intrusive_ptr instance increments the reference count
by using an unqualified call to the function intrusive_ptr_add_ref, passing it
the pointer as an argument. Similarly, when an intrusive_ptr is destroyed, it
calls intrusive_ptr_release; this function is responsible for destroying the
object when its reference count drops to zero. 

The user is expected to provide suitable definitions of these two functions. 

On compilers that support argument-dependent lookup, intrusive_ptr_add_ref and
intrusive_ptr_release should be defined in the namespace that corresponds to
their parameter; otherwise, the definitions need to go in namespace boost. The
library provides a helper base class template intrusive_ref_counter which may
help adding support for intrusive_ptr to user types.

The class template is parameterized on T, the type of the object pointed to.
intrusive_ptr<T> can be implicitly converted to intrusive_ptr<U> whenever T* can
be implicitly converted to U*.

The main reasons to use intrusive_ptr are:

  Some existing frameworks or OSes provide objects with embedded reference
  counts;

  The memory footprint of intrusive_ptr is the same as the corresponding raw
    pointer;

  intrusive_ptr<T> can be constructed from an arbitrary raw pointer of type T *.

As a general rule, if it isn't obvious whether intrusive_ptr better fits your
needs than shared_ptr, try a shared_ptr-based design first.


<get>

T * get() const; // never throws

    Returns: the stored pointer.

    Throws: nothing.


<assignment>

intrusive_ptr & operator=(intrusive_ptr const & r);
template<class Y> intrusive_ptr & operator=(intrusive_ptr<Y> const & r);
intrusive_ptr & operator=(T * r);

    Effects: Equivalent to intrusive_ptr(r).swap(*this).

    Returns: *this.


<reset>

void reset();

    Effects: Equivalent to intrusive_ptr().swap(*this).

void reset(T * r);

    Effects: Equivalent to intrusive_ptr(r).swap(*this).

void reset(T * r, bool add_ref);

    Effects: Equivalent to intrusive_ptr(r, add_ref).swap(*this).


<swap>

void swap(intrusive_ptr & b); // never throws

    Effects: Exchanges the contents of the two smart pointers.

    Throws: nothing.


<code>

// boost-1.57.0/boost/smart_ptr/intrusive_ptr.hpp

namespace boost
{

//
//  intrusive_ptr
//
//  A smart pointer that uses intrusive reference counting.
//
//  Relies on unqualified calls to
//  
//      void intrusive_ptr_add_ref(T * p);
//      void intrusive_ptr_release(T * p);
//
//          (p != 0)
//
//  The object is responsible for destroying itself.
//

template<class T> class intrusive_ptr
{
private:

    typedef intrusive_ptr this_type;

public:

    typedef T element_type;

    intrusive_ptr() BOOST_NOEXCEPT : px( 0 )
    {
    }

    intrusive_ptr( T * p, bool add_ref = true ): px( p )
    {
        if( px != 0 && add_ref ) intrusive_ptr_add_ref( px );
    }

#if !defined(BOOST_NO_MEMBER_TEMPLATES) || defined(BOOST_MSVC6_MEMBER_TEMPLATES)

    template<class U>
#if !defined( BOOST_SP_NO_SP_CONVERTIBLE )

    intrusive_ptr( intrusive_ptr<U> const & rhs,
        typename boost::detail::sp_enable_if_convertible<U,T>::type 
          = boost::detail::sp_empty() )

#else

    intrusive_ptr( intrusive_ptr<U> const & rhs )

#endif
    : px( rhs.get() )
    {
        if( px != 0 ) intrusive_ptr_add_ref( px );
    }

#endif

    intrusive_ptr(intrusive_ptr const & rhs): px( rhs.px )
    {
        if( px != 0 ) intrusive_ptr_add_ref( px );
    }

    ~intrusive_ptr()
    {
        if( px != 0 ) intrusive_ptr_release( px );
    }

    intrusive_ptr & operator=(T * rhs)
    {
        this_type(rhs).swap(*this);
        return *this;
    }

    ...
};


={============================================================================
*kt_dev_boost_010* boost-intrusive-ptr: case ex

This case uses reference count in GObject and no need to add reference count
member to the type.

inline
GObject* intrusive_ptr_add_ref(GObject* const p)
{
    return G_OBJECT(g_object_ref(p));
}

inline
void intrusive_ptr_release(GObject* const p)
{
    g_object_unref(p);
}

inline
GstElement* intrusive_ptr_add_ref(GstElement* const p)
{
    return GST_ELEMENT_CAST(gst_object_ref(p));
}

inline
void intrusive_ptr_release(GstElement* const p)
{
    gst_object_unref(p);
}

// defines more functions for other types

/**
 * Ideally `boost::intrusive_ptr` should be used directly but unfortunately its
 * constructor by default increments the reference counter which is not handy as
 * GStreamer (GObject) objects are returned with reference counter already
 * initialised.  Also specifying `incRef` as `false` for most of the instances
 * is inconvenient or sometimes even impossible.
 *
 * This thin class does _not_ increment the reference counter by default which
 * suits most of the use cases.
 */
template <typename T>
class RefObj : public boost::intrusive_ptr<T>
{
  public:
    inline RefObj() : boost::intrusive_ptr<T>() {}

    /**
     * Initialise the pointer but do _not_ increment reference counter by
     * default.
     */
    inline RefObj(T* const elem, const bool incRef = false)
      : boost::intrusive_ptr<T>(elem, incRef)
    {
    }

    /**
     * In some contexts it's necessary to retain the ownership of the object.
     * This happens in situations when a function takes ownership of the object
     * provided but the caller wants to keep a reference to the object as well.
     *
     * In other contexts this also means transferring ownership of the object as
     * the original object goes out of scope after a call to a function
     * expecting transfer of ownership.
     */
    inline T* retain() const
    {
      if (T* const p = this->get()) {
        return intrusive_ptr_add_ref(p);
      }

      return NULL;
    }
};


<case>

// as a member

boost::shared_ptr<VirtualBinFactory> typefindBinFactory;

mutable RefObj<GstElement> pipeline;

{
  // this is local object. increase reference count

  RefObj<GstElement> elem = gst_pipeline_new("pipeline");

  // from init()

  g_signal_connect(elem.get(), "child-added", ...);

  // this gets a pointer to bin and increase reference count. Still want to use
  // after init call.

  gst_bin_add_many(GST_BIN(elem.get()),
      typefindBin->getParent().retain(),
      srcBin->getParent().retain(),
      sinkBin->getParent().retain(), NULL);

  elem.swap(this->pipeline);
}

typefindBin->setSource(sinkBin->setDrmUrl(mediaLocator));


<case>

// create intrusive_ptr instance as a member 

RefObj<GstElement> uridecodebin;

// from setSource()
//
// this works since intrusive_ptr & operator=(T *r); and factory_make() increase
// reference count.

uridecodebin = gst_element_factory_make("uridecodebin", "uri-decoder");

if (gst_bin_add(GST_BIN(getParent().get()), uridecodebin.get())) 
{
  // increase reference count. Why this? Since gst_bin_add() take a ownership
  // but after that, still want to use.

  uridecodebin.retain();

  GstCaps *const caps = gst_caps_from_string(
      "application/dash+xml; "
      "application/x-hls; "
      "video/quicktime");

  g_object_set(uridecodebin.get(), "caps", caps, NULL);

  if (0 == g_signal_connect(uridecodebin.get(), "pad-added",
        G_CALLBACK(newPadCb),
        static_cast<gpointer>(this))) {
  }

  // note: WHY do not need to decrease reference count of uridecodebin?
}


={============================================================================
*kt_dev_boost_010* boost-ptr-map:

http://www.boost.org/doc/libs/1_57_0/libs/ptr_container/doc/ptr_container.html

namespace boost
{
    template
    < 
        T,
        class VoidPtrMultiMap,
        class CloneAllocator = heap_clone_allocator 
    >
    class ptr_multimap_adapter 
    {
    public: // typedefs
        typedef VoidPtrMap::key_type key_type;
        typedef T*                   mapped_type;
        typedef T&                   mapped_reference;
        typedef const T&             const_mapped_reference;
        typedef ...                  value_type;
        typedef ...                  reference;
        typedef ...                  const_reference;
        typedef ...                  pointer;
        typedef ...                  const_pointer;  
        
    public: // modifiers         
        iterator  insert( key_type& k, T* x ); 
        template< class U >
        iterator  insert( const key_type&, std::auto_ptr<U> x );                        

    public: // pointer container requirements
        void      transfer( iterator object, ptr_multimap_adapter& from );
        size_type transfer( iterator first, iterator last, ptr_multimap_adapter& from );
        template< class Range >
        size_type transfer( const Range& r, ptr_multimap_adapter& from );
        void      transfer( ptr_multimap_adapter& from );

    }; //  class 'ptr_multimap_adapter'

} // namespace 'boost'  


<ex>
Q: auto_ptr is deprecated. How to use ptr_map then since there is no insert
overload to use unique_ptr?

return getFactories().insert(boost::to_lower_copy(f->getMIME()),
  std::auto_ptr<JSPluginFactoryNP>(f)).second;


The stackoverflow shows this way:

bool registerFactory(std::unique_ptr<JSPluginFactoryNP> f)
{
  std::string lcMIME = boost::to_lower_copy(f->getMIME());
  return getFactories().insert(lcMIME, f.release()).second;
}

Here used the first overload:

iterator  insert( key_type& k, T* x ); 


={============================================================================
*kt_dev_boost_010* boost-signal

Introduction
===================================

The Boost.Signals2 library is an implementation of a managed signals and slots
system. Signals represent 'callbacks' with multiple targets, and are also called
publishers or events in similar systems. Signals are connected to some set of
slots, which are callback receivers (also called event targets or subscribers),
which are called when the signal is "emitted."

Signals and slots are managed, in that signals and slots (or, more properly,
    objects that occur as part of the slots) can track connections and are
capable of automatically disconnecting signal/slot connections when either is
destroyed. This enables the user to make signal/slot connections without
expending a great effort to manage the lifetimes of those connections with
regard to the lifetimes of all objects involved.

When signals are connected to multiple slots, there is a question regarding the
relationship between the return values of the slots and the return value of the
signals. Boost.Signals2 allows the user to specify the manner in which multiple
return values are combined.

This documentation describes a thread-safe 'variant' of the original
Boost.Signals library. There have been some changes to the interface to support
thread-safety, mostly with respect to automatic connection management. This
implementation was written by Frank Mori Hess. 


How to Read this Tutorial

This tutorial is not meant to be read linearly. Its top-level structure roughly
separates different concepts in the library (e.g., handling calling multiple
    slots, passing values to and from slots) and in each of these concepts the
basic ideas are presented first and then more complex uses of the library are
described later. Each of the sections is marked Beginner, Intermediate, or
Advanced to help guide the reader. The Beginner sections include information
that all library users should know; one can make good use of the Signals2
library after having read only the Beginner sections. The Intermediate sections
build on the Beginner sections with slightly more complex uses of the library.
Finally, the Advanced sections detail very advanced uses of the Signals2
library, that often require a solid working knowledge of the Beginner and
Intermediate topics; most users will not need to read the Advanced sections.


Hello, World! (Beginner)

The following example writes "Hello, World!" using signals and slots. First, we
create a signal sig, a signal that takes no arguments and has a void return
value. Next, we connect the hello function object to the signal using the
connect method. Finally, use the signal sig 'like' a function to call the slots,
which in turns invokes HelloWorld::operator() to print "Hello, World!".

struct HelloWorld
{
  void operator()() const
  {
    std::cout << "Hello, World!" << std::endl;
  }
};

{
  // Signal with no arguments and a void return value
  boost::signals2::signal<void ()> sig;

  // Connect a HelloWorld slot
  HelloWorld hello;
  sig.connect(hello);

  // Call all of the slots note: 'invoke'
  sig();
}


Calling Multiple Slots
===================================

Connecting Multiple Slots (Beginner)

<signal-slots-pair>
So we can make the Hello, World program more interesting by 'splitting' the work
of printing "Hello, World!" into two completely separate slots. The first slot
will print "Hello" and may look like this:

struct Hello
{
  void operator()() const
  {
    std::cout << "Hello";
  }
};

The second slot will print ", World!" and a newline, to complete the program.
The second slot may look like this:

struct World
{
  void operator()() const
  {
    std::cout << ", World!" << std::endl;
  }
};

Like in our previous example, we can create a signal sig that takes no arguments
and has a void return value. This time, we 'connect' 'both' a hello and a world
'slot' to the same signal, and when we call the signal both slots will be
called.

{
  boost::signals2::signal<void ()> sig;

  sig.connect(Hello());   note: multiple connects
  sig.connect(World());

  sig();
}

<slot-list>
By default, slots are pushed onto the 'back' of the slot list, so the output of
this program will be as expected:

Hello, World!


Connection Management
===================================

Disconnecting Slots (Beginner)

Slots aren't expected to exist indefinitely after they are connected. Often
slots are only used to receive a few events and are then disconnected, and the
programmer needs 'control' to decide when a slot should no longer be connected.

<connection-class>
The entry for managing connections explicitly is the boost::signals2::connection
class. 
  
The connection class uniquely represents the connection between a particular
signal and a particular slot. The connected() method checks if the signal and
slot are still connected, and the disconnect() method disconnects the signal and
slot if they are connected before it is called. Each call to the signal's
connect() method returns a connection object, which can be used to determine if
the connection still exists or to disconnect the signal and slot.

  boost::signals2::connection c = sig.connect(HelloWorld());
  std::cout << "c is connected\n";
  sig(); // Prints "Hello, World!"

  c.disconnect(); // Disconnect the HelloWorld object
  std::cout << "c is disconnected\n";
  sig(); // Does nothing: there are no connected slots


Giving a Slot Access to its Connection (Advanced)

You may encounter situations where you wish to disconnect or block a slot's
connection from 'within' the slot itself. For example, suppose you have a group
of asynchronous tasks, each of which emits a signal when it completes. You wish
to connect a slot to all the tasks to retrieve their results as each completes.
Once a given task completes and the slot is run, the slot no longer needs to be
connected to the completed task. Therefore, you may wish to clean up old
connections by having the slot disconnect its invoking connection when it runs.


<connection-object>
For a slot to disconnect (or block) its invoking connection, it must have access
to a signals2::connection object which references the invoking signal-slot
connection. The difficulty is, the connection object is returned by the
signal::connect method, and therefore is not available until after the slot is
already connected to the signal. This can be particularly troublesome in a
multi-threaded environment where the signal may be invoked concurrently by a
different thread while the slot is being connected.

Therefore, the signal classes provide signal::connect_extended methods, which
allow slots which take an extra argument to be connected to a signal. The extra
argument is a signals2::connection object which refers to the signal-slot
connection currently invoking the slot. signal::connect_extended uses slots of
the type given by the signal::extended_slot_type typedef.

The examples section includes an extended_slot program which demonstrates the
syntax for using signal::connect_extended. 


Thread-Safety
===================================

Introduction

The primary motivation for Boost.Signals2 is to provide a version of the
original Boost.Signals library which can be used safely in a multi-threaded
environment. This is achieved primarily through 'two' changes from the original
Boost.Signals API. One is the introduction of a new automatic connection
management scheme relying on shared_ptr and weak_ptr, as described in the
tutorial. The second change was the introduction of a Mutex template type
parameter to the signal class. 

This section details how the library employs these changes to provide
thread-safety, and the limits of the provided thread-safety.  


Signals and combiners

Each signal object default-constructs a Mutex object to protect its internal
state. Furthermore, a Mutex is created 'each' time a new slot is connected to
the signal, to protect the associated signal-slot connection.

A signal's mutex is automatically locked whenever any of the signal's methods
are called. The mutex is usually held until the method completes, however there
is one major exception to this rule. 

When a signal is 'invoked' by calling signal::operator(), the invocation first
acquires a lock on the signal's mutex. Then it obtains a handle to the signal's
slot list and combiner. Next it releases the signal's mutex, 'before' invoking
the combiner to 'iterate' through the slot list. Thus no mutexes are held by the
signal while a slot is executing.  

This design choice makes it impossible for user code running in a slot to
deadlock against any of the mutexes used internally by the Boost.Signals2
library. It also prevents slots from accidentally causing recursive locking
attempts on any of the library's internal mutexes. 

Therefore, if you invoke a signal concurrently from multiple threads, it is
possible for the signal's combiner to be invoked 'concurrently' and thus the
slots to execute concurrently.

During a combiner invocation, the following steps are performed in order to find
the next callable slot while iterating through the signal's slot list.

    The Mutex associated with the connection to the slot is locked.

    All the tracked weak_ptr associated with the slot are copied into temporary
    shared_ptr which will be kept alive until the invocation is done with the
    slot. If this fails due to any of the weak_ptr being expired, the connection
    is automatically disconnected. Therefore a slot will never be run if any of
    its tracked weak_ptr have expired, and none of its tracked weak_ptr will
    expire while the slot is running.

    The slot's connection is checked to see if it is blocked or disconnected,
    and then the connection's mutex is unlocked. If the connection was either
    blocked or disconnected, we start again from the beginning with the next
    slot in the slot list. Otherwise, we commit to executing the slot when the
    combiner next dereferences the slot call iterator (unless the combiner
        should increment the iterator without ever dereferencing it).

Note that since we unlock the connection's mutex before executing its associated
slot, it is possible a slot will still be executing after it has been
disconnected by a connection::disconnect(), if the disconnect was called
concurrently with signal invocation.

You may have noticed above that during signal invocation, the invocation only
obtains handles to the signal's slot list and combiner while holding the
signal's mutex. Thus concurrent signal invocations may still wind up accessing
the same slot list and combiner concurrently. So what happens if the slot list
is modified, for example by connecting a new slot, while a signal invocation is
in progress concurrently? If the slot list is already in use, the signal
performs a deep copy of the slot list before modifying it. Thus the a concurrent
signal invocation will continue to use the old unmodified slot list, undisturbed
by modifications made to the newly created deep copy of the slot list. Future
signal invocations will receive a handle to the newly created deep copy of the
slot list, and the old slot list will be destroyed once it is no longer in use.
Similarly, if you change a signal's combiner with signal::set_combiner while a
signal invocation is running concurrently, the concurrent signal invocation will
continue to use the old combiner undisturbed, while future signal invocations
will receive a handle to the new combiner.

The fact that concurrent signal invocations use the same combiner object means
you need to insure any custom combiner you write is thread-safe. So if your
combiner maintains state which is modified when the combiner is invoked, you may
need to protect that state with a mutex. Be aware, if you hold a mutex in your
combiner while dereferencing slot call iterators, you run the risk of deadlocks
and recursive locking if any of the slots cause additional mutex locking to
occur. One way to avoid these perils is for your combiner to release any locks
before dereferencing a slot call iterator. The combiner classes provided by the
Boost.Signals2 library are all thread-safe, since they do not maintain any state
across invocations.

Suppose a user writes a slot which connects another slot to the invoking signal.
Will the newly connected slot be run during the same signal invocation in which
the new connection was made? The answer is no. Connecting a new slot modifies
the signal's slot list, and as explained above, a signal invocation already in
progress will not see any modifications made to the slot list.

Suppose a user writes a slot which disconnects another slot from the invoking
signal. Will the disconnected slot be prevented from running during the same
signal invocation, if it appears later in the slot list than the slot which
disconnected it? This time the answer is yes. Even if the disconnected slot is
still present in the signal's slot list, each slot is checked to see if it is
disconnected or blocked immediately before it is executed (or not executed as
    the case may be), as was described in more detail above.

Connections and other classes

The methods of the signals2::connection class are thread-safe, with the
exception of assignment and swap. This is achived via locking the mutex
associated with the object's underlying signal-slot connection. Assignment and
swap are not thread-safe because the mutex protects the underlying connection
which a signals2::connection object references, not the signals2::connection
object itself. That is, there may be many copies of a signals2::connection
object, all of which reference the same underlying connection. There is not a
mutex for each signals2::connection object, there is only a single mutex
protecting the underlying connection they reference.

The shared_connection_block class obtains some thread-safety from the Mutex
protecting the underlying connection which is blocked and unblocked. The
internal reference counting which is used to keep track of how many
shared_connection_block objects are asserting blocks on their underlying
connection is also thread-safe (the implementation relies on shared_ptr for the
    reference counting). However, individual shared_connection_block objects
should not be accessed concurrently by multiple threads. As long as two threads
each have their own shared_connection_block object, then they may use them in
safety, even if both shared_connection_block objects are copies and refer to the
same underlying connection.

The signals2::slot class has no internal mutex locking built into it. It is
expected that slot objects will be created then connected to a signal in a
single thread. Once they have been copied into a signal's slot list, they are
protected by the mutex associated with each signal-slot connection.

The signals2::trackable class does NOT provide thread-safe automatic connection
management. In particular, it leaves open the possibility of a signal invocation
calling into a partially destructed object if the trackable-derived object is
destroyed in a different thread from the one invoking the signal.
signals2::trackable is only provided as a convenience for porting
single-threaded code from Boost.Signals to Boost.Signals2.


Porting from Boost.Signals to Boost.Signals2
===================================

The 'changes' made to the Boost.Signals2 API compared to the original
Boost.Signals library are summarized below. We also provide some notes on
dealing with each change while porting existing Boost.Signals code to
Boost.Signals2. 

-. signals2::signal::connect_extended()

The signals2::signal class has an additional 

typedef signals2::signal::extended_slot_type and 
signals2::signal::connect_extended() methods. 

These allow connection of slots which take an additional signals2::connection
argument, giving them thread-safe access to their signal/slot connection when
they are invoked. There is also a new ExtendedSlotFunction template parameter
for specifying the underlying slot function type for the new extended slots.

These additions should have no effect on porting unless you are also converting
your program from a single threaded program into a multi-threaded one. In that
case, if you have slots which need access to their signals2::connection to the
signal invoking them (for example to block or disconnect their connection) you
may wish to connect the slots with signals2::signal::connect_extended(). This
also requires adding an additional connection argument to the slot. More
information on how and why to use extended slots is available in the tutorial.


Class template signal
===================================

// In header: <boost/signals2/signal.hpp>

template<typename Signature, 
         typename Combiner = boost::signals2::optional_last_value<R>, 
         typename Group = int, typename GroupCompare = std::less<Group>, 

         typename SlotFunction = boost::function<Signature>, 
         typename ExtendedSlotFunction = 
            boost::function<R (const connection &, T1, T2, ..., TN)>, 

         typename Mutex = boost::signals2::mutex> 
class signal : public boost::signals2::signal_base {
public:
  // types
  ...
  typedef SlotFunction slot_function_type;         
  typedef typename signals2::slot<Signature, SlotFunction> slot_type;

  typedef ExtendedSlotFunction extended_slot_function_type;

  // note:
  typedef typename 
    signals2::slot<R (const connection &, T1, ..., TN), ExtendedSlotFunction> 
      extended_slot_type;

  typedef typename SlotFunction::result_type slot_result_type;           
  ...

1.

signal connection management

    connection connect(const slot_type& slot, connect_position at = at_back);
    connection connect(const group_type& group, const slot_type& slot, 
                       connect_position at = at_back);

    Effects:
    
    Connects the signal this to the incoming slot. If the slot is inactive,
    i.e., any of the slots's tracked objects have been destroyed, then the call
    to connect is a no-op. If the second version of connect is invoked, the slot
    is associated with the given group. The at parameter specifies where the
    slot should be connected: at_front indicates that the slot will be connected
    at the front of the list or group of slots and at_back indicates that the
    slot will be connected at the back of the list or group of slots.

    Returns:

    A signals2::connection object that references the newly-created connection
    between the signal and the slot; if the slot is inactive, returns a
    disconnected connection.

    Throws:

    This routine meets the strong exception guarantee, where any exception
    thrown will cause the slot to not be connected to the signal.

    Complexity:

    Constant time when connecting a slot without a group name or logarithmic in
    the number of groups when connecting to a particular group.

    Notes:

    It is unspecified whether connecting a slot while the signal is calling will
    result in the slot being called immediately.


2.
    connection connect_extended(const extended_slot_type& slot, 
                                connect_position at = at_back);
    connection connect_extended(const group_type& group, 
                                const extended_slot_type& slot, 
                                connect_position at = at_back);

    The connect_extended methods work the same as the connect methods, except
    they take slots of type extended_slot_type. This is useful if a slot needs
    to access the connection between it and the signal invoking it, for example
    if it wishes to disconnect or block its own connection.

4. 

void disconnect_all_slots();

    Effects:
    Disconnects all slots connected to the signal.

    Notes:
    May be called at any time within the lifetime of the signal, including
    during calls to the signal's slots.


<ex>

libs/signals2/example/extended_slot.cpp

// Example program for connecting an extended slot,
// using a signal's connect_extended and extended_slot_type.
//
// Copyright Frank Mori Hess 2009.
//
// Use, modification and
// distribution is subject to the Boost Software License, Version
// 1.0. (See accompanying file LICENSE_1_0.txt or copy at
// http://www.boost.org/LICENSE_1_0.txt)
// For more information, see http://www.boost.org

#include <boost/signals2/signal.hpp>
#include <iostream>
#include <string>

namespace bs2 = boost::signals2;

void single_shot_slot(const bs2::connection &conn, const std::string &message)
{
  conn.disconnect();
  std::cout << message;
}

int main()
{
  typedef bs2::signal<void (void)> sig_type;
  sig_type sig;

  {
    sig_type::extended_slot_type hello(&single_shot_slot, _1, "Hello");
    sig.connect_extended(hello);
  }

  sig();  // prints "Hello"

  {
    sig_type::extended_slot_type world(&single_shot_slot, _1, ", World!\n");
    sig.connect_extended(world);
  }

  sig();  // only prints ", World!\n" since hello slot has disconnected itself
  sig();  // prints nothing, world slot has disconnected itself

  return 0;
}


={============================================================================
*kt_dev_boost_100* asio

http://www.boost.org/doc/libs/1_58_0/doc/html/boost_asio.html

{overview}

<basic>
Basic Boost.Asio Anatomy

<async>

      Your Program

I/O Object        Your Completion Handler
eg. socket


      io_service

      OS

When an asynchronous operation is used, a different sequence of events occurs. 

1. Your program initiates the connect operation by calling the I/O object:

socket.async_connect(server_endpoint, your_completion_handler);

where your_completion_handler is a function or function object with the
signature:

void your_completion_handler(const boost::system::error_code& ec);

The exact signature required depends on the asynchronous operation being
performed. The reference documentation indicates the appropriate form for each
operation.

2. The I/O object forwards the request to the io_service.

3. The io_service signals to the operating system that it should start an
asynchronous connect.

Time passes. In the synchronous case this wait would have been contained
entirely within the duration of the connect operation.

4. The operating system indicates that the connect operation has completed by
placing the result on a 'queue', ready to be picked up by the io_service.

5. Your program must make a call to io_service::run() or to one of the similar
io_service member functions in order for the result to be retrieved. 

A call to io_service::run() 'blocks' while there are unfinished asynchronous
operations, so you would typically call it as soon as you have started your
first asynchronous operation.

6. While inside the call to io_service::run(), the io_service dequeues the
result of the operation, translates it into an error_code, and then passes it
to your completion handler.

This is a simplified picture of how Boost.Asio operates. You will want to
delve further into the documentation if your needs are more advanced, such as
extending Boost.Asio to perform other types of asynchronous operations. 


<threads-and-boost-asio>
Threads and Boost.Asio

Thread Safety

In general, it is safe to make concurrent use of distinct objects, but unsafe
to make concurrent use of a single object. However, types such as io_service
provide a stronger guarantee that it is safe to use a single object
concurrently.


Thread Pools note: scalable

Multiple threads may call io_service::run() to set up a pool of threads from
which completion handlers may be invoked. This approach may also be used with
io_service::post() to use a means to perform any computational tasks across a
thread pool.

Note that all threads that have joined an io_service's pool are considered
equivalent, and the io_service may distribute work across them in an arbitrary
fashion.  

Internal Threads

The implementation of this library for a particular platform may make use of
one or more internal threads to emulate asynchronicity. As far as possible,
    these threads must be invisible to the library user. In particular, the
    threads:

must not call the user's code directly; and

must block all signals.
note: this is what single dispatcher has in place.

This approach is complemented by the following guarantee:

Asynchronous completion handlers will only be called from threads that are
currently calling io_service::run(). 

Consequently, it is the library user's responsibility to create and manage all
threads to which the notifications will be delivered.

The reasons for this approach include:

By only calling io_service::run() from a single thread, the user's code can
avoid the development complexity associated with synchronisation. For example,
      a library user can implement scalable servers that are single-threaded
      (from the user's point of view).

A library user may need to perform initialisation in a thread shortly after
the thread starts and before any other application code is executed. For
example, users of Microsoft's COM must call CoInitializeEx before any other
COM operations can be called from that thread.

The library interface is decoupled from interfaces for thread creation and
management, and permits implementations on platforms where threads are not
available.


={============================================================================
*kt_dev_boost_101* asio: tutorial

<01> Timer.1 - Using a timer synchronously
#include <iostream>
#include <boost/asio.hpp>
#include <boost/date_time/posix_time/posix_time.hpp>

int main()
{
  boost::asio::io_service io;

  // Next we declare an object of type boost::asio::deadline_timer. The core
  // asio classes that provide I/O functionality (or as in this case timer
  // functionality) always take a reference to an io_service as their first
  // constructor argument. The second argument to the constructor sets the timer
  // to expire 5 seconds from now.

  boost::asio::deadline_timer t(io, boost::posix_time::seconds(5));

  // In this simple example we perform a blocking wait on the timer. That is,
  // the call to deadline_timer::wait() will not return until the timer has
  // expired, 5 seconds after it was created (i.e. not from when the wait
  // starts). 

  t.wait();

  std::cout << "Hello, world!" << std::endl;

  return 0;
}


<02> Timer.2 - Using a timer asynchronously

// This tutorial program demonstrates how to use asio's asynchronous callback
// functionality by modifying the program from tutorial Timer.1 to perform an
// asynchronous wait on the timer.

#include <iostream>
#include <boost/asio.hpp>
#include <boost/date_time/posix_time/posix_time.hpp>

// Using asio's asynchronous functionality 'means' having a callback function
// that will be called 'when' an asynchronous operation completes. In this
// program we define a function called print to be called when the asynchronous
// wait finishes.

void print(const boost::system::error_code& /*e*/)
{
  std::cout << "Hello, world!" << std::endl;
}

int main()
{
    boost::asio::io_service io;

    boost::asio::deadline_timer t(io, boost::posix_time::seconds(5));

    // Next, instead of doing a blocking wait as in tutorial Timer.1, we call
    // the deadline_timer::async_wait() function to perform an asynchronous
    // wait. When calling this function we pass the print callback handler that
    // was defined above.

    std::cout << "calls async wait..." << std::endl;

    // post a work for asio to do
    t.async_wait(&print);

    // Finally, we must call the io_service::run() member function on the
    // io_service object.

    // note: thread
    // The asio library provides a guarantee that callback handlers will only be
    // called from threads that are currently calling io_service::run().
    //
    // Therefore unless the io_service::run() function is called, the callback
    // for the asynchronous wait completion will never be invoked.

    // note: run() continue to run
    // The io_service::run() function will also continue to run while there is
    // still work to do. In this example, the 'work' is the asynchronous wait on
    // the timer, so the call will not return until the timer has expired and
    // the callback has completed.

    // It is important to remember to give the io_service some work to do before
    // calling io_service::run(). For example, if we had omitted the above call
    // to deadline_timer::async_wait(), the io_service would not have had any
    // work to do, and consequently io_service::run() would have returned
    // immediately.

    std::cout << "calls io.run..." << std::endl;

    io.run();

    std::cout << "ends..." << std::endl;

    return 0;
}

calls async wait...
calls io.run...

note: blocks on io.run() so wait for timer expired.

Hello, world!
ends...


<03> Timer.3 - Binding arguments to a handler
#include <iostream>
#include <boost/asio.hpp>
#include <boost/bind.hpp>
#include <boost/date_time/posix_time/posix_time.hpp>

// To implement a repeating timer using asio you need to change the timer's
// expiry time in your callback function, and to then start a new asynchronous
// wait. Obviously this means that the callback function will need to be able
// to access the timer object. To this end we add two new parameters to the
// print function:
//
// * A pointer to a timer object.  
// * A counter so that we can stop the program when the timer fires for the
// sixth time.
// 
// note: when to stop - no more work to do
// However you will observe that there is no explicit call to ask the
// io_service to stop. Recall that in tutorial Timer.2 we learnt that the
// io_service::run() function completes when there is no more "work" to do. By
// not starting a new asynchronous wait on the timer when count reaches 5, the
// io_service will run out of work and stop running. 

void print(const boost::system::error_code& /*e*/,
    boost::asio::deadline_timer* t, int* count)
{
  if (*count < 5)
  {
    std::cout << "callback: count: " << count << ", t: " << t << std::endl;
    std::cout << "callback: count: " << *count << std::endl;
    ++(*count);

    // Next we move the expiry time for the timer along by one second from the
    // previous expiry time. By calculating the new expiry time relative to
    // the old, we can ensure that the timer does not drift away from the
    // whole-second mark due to any delays in processing the handler.

    t->expires_at(t->expires_at() + boost::posix_time::seconds(1));

    // note: function object
    // Then we start a new asynchronous wait on the timer. As you can see, the
    // boost::bind() function is used to associate the extra parameters with
    // your callback handler. The deadline_timer::async_wait() function
    // expects a handler function (or function object) with the signature
    // void(const boost::system::error_code&). Binding the additional
    // parameters converts your print function into a function object that
    // matches the signature correctly.

    t->async_wait(boost::bind(print,
          boost::asio::placeholders::error, t, count));
  }
}

int main()
{
  boost::asio::io_service io;

  int count = 0;
  boost::asio::deadline_timer t(io, boost::posix_time::seconds(1));

  std::cout << "main: count: " << &count << ", t: " << &t << std::endl;

  // note: pass reference. is it okay when callback is called?

  t.async_wait(boost::bind(print,
        boost::asio::placeholders::error, &t, &count));

  std::cout << "main: calls run" << std::endl;
  io.run();

  std::cout << "main: final count is " << count << std::endl;

  return 0;
}

main: count: 0xbfcf0650, t: 0xbfcf062c
main: calls run
callback: count: 0xbfcf0650, t: 0xbfcf062c
callback: count: 0
callback: count: 0xbfcf0650, t: 0xbfcf062c
callback: count: 1
callback: count: 0xbfcf0650, t: 0xbfcf062c
callback: count: 2
callback: count: 0xbfcf0650, t: 0xbfcf062c
callback: count: 3
callback: count: 0xbfcf0650, t: 0xbfcf062c
callback: count: 4
main: final count is 5

note: see that &count and &t are the same between main and callback since:

Asynchronous completion handlers will only be called from threads that are
currently calling io_service::run(). 


<04> Timer.4 - Using a member function as a handler
#include <iostream>
#include <boost/asio.hpp>
#include <boost/bind.hpp>
#include <boost/date_time/posix_time/posix_time.hpp>

// The constructor of this class will take a reference to the io_service
// object and use it when initialising the timer_ member. The counter used to
// shut down the program is now also a member of the class. 

class printer
{
public:
  printer(boost::asio::io_service& io)
    : timer_(io, boost::posix_time::seconds(1)),         // note: interesting
      count_(0)
  {
    // The boost::bind() function works just as well with class member
    // functions as with free functions. Since all non-static class member
    // functions have an implicit this parameter, we need to bind this to the
    // function. As in tutorial Timer.3, boost::bind() converts our callback
    // handler (now a member function) into a function object that can be
    // invoked as though it has the signature void(const
    // boost::system::error_code&).

    // You will note that the boost::asio::placeholders::error placeholder is
    // not specified here, as the print member function does not accept an
    // error object as a parameter. 

    std::cout << "ctor: print: " << &printer::print << 
        ", this: " << this << std::endl;
    timer_.async_wait(boost::bind(&printer::print, this));
  }

  ~printer()
  {
    std::cout << "Final count is " << count_ << std::endl;
  }

  // The print member function is very similar to the print function from
  // tutorial Timer.3, except that it now operates on the class data members
  // instead of having the timer and counter passed in as parameters. 

  void print()
  {
    if (count_ < 5)
    {
      std::cout << "print: print: " << &printer::print << 
          ", this: " << this << std::endl;
      std::cout << "print: count: " << count_ << std::endl;
      ++count_;

      timer_.expires_at(timer_.expires_at() + boost::posix_time::seconds(1));
      timer_.async_wait(boost::bind(&printer::print, this));
    }
  }

private:
  boost::asio::deadline_timer timer_;
  int count_;
};

int main()
{
  boost::asio::io_service io;

  printer p1(io);
  printer p2(io);

  std::cout << "main: calls run" << std::endl;
  io.run();
  std::cout << "main: ends run" << std::endl;

  return 0;
}

ctor: print: 1, this: 0xbf956508
ctor: print: 1, this: 0xbf956530
main: calls run
print: print: 1, this: 0xbf956508
print: count: 0
print: print: 1, this: 0xbf956530
print: count: 0
print: print: 1, this: 0xbf956508
print: count: 1
print: print: 1, this: 0xbf956530
print: count: 1
print: print: 1, this: 0xbf956508
print: count: 2
print: print: 1, this: 0xbf956530
print: count: 2
print: print: 1, this: 0xbf956508
print: count: 3
print: print: 1, this: 0xbf956530
print: count: 3
print: print: 1, this: 0xbf956508
print: count: 4
print: print: 1, this: 0xbf956530
print: count: 4
main: ends run
dtor: final count is 5
dtor: final count is 5


<05> Timer.5 - Synchronising handlers in multithreaded programs

// This tutorial demonstrates the use of the boost::asio::strand class to
// synchronise callback handlers in a multithreaded program.

// The previous four tutorials avoided the issue of handler synchronisation by
// calling the io_service::run() function from one thread only. As you already
// know, the asio library provides a guarantee that callback handlers will
// only be called from threads that are currently calling io_service::run().
// Consequently, calling io_service::run() from only 'one' thread ensures that
// callback handlers cannot run concurrently.

// The single threaded approach is usually the best place to start when
// developing applications using asio. The downside is the limitations it
// places on programs, particularly servers, including:

// * Poor responsiveness when handlers can take a long time to complete.
// * An inability to scale on multiprocessor systems.

// If you find yourself running into these limitations, an alternative
// approach is to have a pool of threads 'calling' io_service::run(). However,
// as this allows handlers to execute concurrently, we need a method of
// synchronisation when handlers might be accessing a shared, thread-unsafe
// resource. 

#include <iostream>
#include <boost/asio.hpp>
#include <boost/thread/thread.hpp>
#include <boost/bind.hpp>
#include <boost/date_time/posix_time/posix_time.hpp>

// An boost::asio::strand guarantees that, for those handlers that are
// dispatched 'through' it, an executing handler will be allowed to complete
// before the next one is started. This is guaranteed irrespective of the
// number of threads that are calling io_service::run(). 
//
// Of course, the handlers may still execute concurrently with other handlers
// that were 'not' dispatched through an boost::asio::strand, or were
// dispatched through a 'different' boost::asio::strand object. 
//
// note: can use different io_service object.

class printer
{
public:
  printer(boost::asio::io_service& io)
    : strand_(io),
      timer1_(io, boost::posix_time::seconds(1)),
      timer2_(io, boost::posix_time::seconds(1)),
      count_(0)
  {

    // When initiating the asynchronous operations, each callback handler is
    // "wrapped" using the boost::asio::strand object. The strand::wrap()
    // function returns a new handler that automatically dispatches its
    // contained handler through the boost::asio::strand object. By wrapping
    // the handlers using the same boost::asio::strand, we are ensuring that
    // they cannot execute concurrently.

    timer1_.async_wait(strand_.wrap(boost::bind(&printer::print1, this)));
    timer2_.async_wait(strand_.wrap(boost::bind(&printer::print2, this)));
  }

  ~printer()
  {
    std::cout << "Final count is " << count_ << std::endl;
  }

  // In a multithreaded program, the handlers for asynchronous operations
  // should be synchronised if they access shared resources. In this tutorial,
  // the shared resources used by the handlers (print1 and print2) are
  // std::'cout' and the count_ data member. 
  //
  // In this example, use asio::strand to sync than sync primitive.

  void print1()
  {
    if (count_ < 10)
    {
      std::cout << "Timer 1: " << count_ << std::endl;
      ++count_;

      timer1_.expires_at(timer1_.expires_at() + boost::posix_time::seconds(1));
      timer1_.async_wait(strand_.wrap(boost::bind(&printer::print1, this)));
    }
  }

  void print2()
  {
    if (count_ < 10)
    {
      std::cout << "Timer 2: " << count_ << std::endl;
      ++count_;

      timer2_.expires_at(timer2_.expires_at() + boost::posix_time::seconds(1));
      timer2_.async_wait(strand_.wrap(boost::bind(&printer::print2, this)));
    }
  }

private:
  boost::asio::io_service::strand strand_;
  boost::asio::deadline_timer timer1_;
  boost::asio::deadline_timer timer2_;
  int count_;
};

// The main function now causes io_service::run() to be called from 'two'
// threads: the main thread and one additional thread. This is accomplished
// using an boost::thread object.

// Just as it would with a call from a single thread, 'concurrent' calls to
// io_service::run() will continue to execute while there is "work" left to
// do. The background thread will not exit until all asynchronous operations
// have completed. 

// note: two threads uses the same io. bind(run, &io) where &io is this since
// run() is member function but not run(io_service&).

int main()
{
  boost::asio::io_service io;
  printer p(io);
  boost::thread t(boost::bind(&boost::asio::io_service::run, &io));
  io.run();
  t.join();

  return 0;
}

$ g++ -g -std=c++0x -lboost_system -lboost_thread t_asio_04-2.cpp
$ LD_LIBRARY_PATH=/usr/local/lib ./a.out 
Timer 1: 0
Timer 2: 1
Timer 1: 2
Timer 2: 3
Timer 1: 4
Timer 2: 5
Timer 1: 6
Timer 2: 7
Timer 1: 8
Timer 2: 9
Final count is 10


={============================================================================
*kt_dev_boost_102* asio: deadline_timer

http://www.boost.org/doc/libs/1_58_0/doc/html/boost_asio/reference/basic_deadline_timer.html

basic_deadline_timer::async_wait

Start an asynchronous wait on the timer.

template<typename WaitHandler>
void-or-deduced async_wait(WaitHandler handler);

This function may be used to initiate an asynchronous wait against the timer.
It always returns immediately.

For each call to async_wait(), the supplied handler will be called exactly
once. The handler will be called when:

The timer has expired.
The timer was cancelled, in which case the handler is passed the error code
boost::asio::error::operation_aborted.

Parameters

handler

The handler to be called when the timer expires. Copies will be made of the
handler as required. The function signature of the handler must be:

void handler(
      const boost::system::error_code& error // Result of operation.
);

Regardless of whether the asynchronous operation completes immediately or not,
the handler will not be invoked from within this function.

note: 
See that 'hanlder' term means callback function and post() and aync_wait() add a
work to io_service.

Invocation of the handler will be performed in a manner equivalent to using
boost::asio::io_service::post().

// boost_1_58_0/boost/asio/detail/deadline_timer_service.hpp
//
  // Start an asynchronous wait on the timer.
  template <typename Handler>
  void async_wait(implementation_type& impl, Handler& handler)
  {
    // Allocate and construct an operation to wrap the handler.
    typedef wait_handler<Handler> op;
    typename op::ptr p = { boost::asio::detail::addressof(handler),
      boost_asio_handler_alloc_helpers::allocate(
        sizeof(op), handler), 0 };
    p.p = new (p.v) op(handler);

    impl.might_have_pending_waits = true;

    BOOST_ASIO_HANDLER_CREATION((p.p, "deadline_timer", &impl, "async_wait"));

    scheduler_.schedule_timer(timer_queue_, impl.expiry, impl.timer_data, p.p);
    p.v = p.p = 0;
  }

// boost_1_58_0/boost/asio/basic_deadline_timer.hpp
//
  template <typename WaitHandler>
  BOOST_ASIO_INITFN_RESULT_TYPE(WaitHandler,
      void (boost::system::error_code))
  async_wait(BOOST_ASIO_MOVE_ARG(WaitHandler) handler)
  {
    // If you get an error on the following line it means that your handler does
    // not meet the documented type requirements for a WaitHandler.
    BOOST_ASIO_WAIT_HANDLER_CHECK(WaitHandler, handler) type_check;

    return this->service.async_wait(this->implementation,
        BOOST_ASIO_MOVE_CAST(WaitHandler)(handler));
  }


={============================================================================
*kt_dev_boost_103* asio: io_service

http://www.boost.org/doc/libs/1_58_0/doc/html/boost_asio/reference/io_service.html

Provides core I/O functionality.

class io_service : noncopyable


* Synchronous and asynchronous operations

Synchronous operations on I/O objects implicitly run the io_service object for
an individual operation. The io_service functions run(), run_one(), poll() or
poll_one() must be called for the io_service to perform asynchronous operations
on behalf of a C++ program. 

Notification that an asynchronous operation has completed is delivered by
invocation of the associated handler. Handlers are invoked only by a thread that
is currently calling any overload of run(), run_one(), poll() or poll_one() for
the io_service. 


* Effect of exceptions thrown from handlers

If an exception is thrown from a handler, the exception is allowed to propagate
through the throwing thread's invocation of run(), run_one(), poll() or
poll_one(). No other threads that are calling any of these functions are
affected. It is then the responsibility of the application to catch the
exception.

After the exception has been caught, the run(), run_one(), poll() or poll_one()
call may be restarted without the need for an intervening call to reset(). This
allows the thread to rejoin the io_service object's thread pool 'without'
impacting any other threads in the pool.

For example:

boost::asio::io_service io_service;
...
for (;;)
{
  try
  {
    io_service.run();
    break; // run() exited normally
  }
  catch (my_exception& e)
  {
    // Deal with exception as appropriate.
  }
}


* Stopping the io_service from running out of work

note: when work is needed to keep io_service alive.

Some applications may need to prevent an io_service object's run() call from
returning when there is no more work to do. For example, the io_service may be
being run in a background thread that is launched prior to the application's
asynchronous operations. The run() call may be kept running by creating an
object of type io_service::work:

boost::asio::io_service io_service;
boost::asio::io_service::work work(io_service);
...

To effect a shutdown, the application will then need to call the io_service
object's stop() member function. This will cause the io_service run() call to
return as soon as possible, abandoning unfinished operations and without
permitting ready handlers to be dispatched.

Alternatively, if the application requires that all operations and handlers be
allowed to finish normally, the work object may be explicitly destroyed.

boost::asio::io_service io_service;
auto_ptr<boost::asio::io_service::work> work(new boost::asio::io_service::work(io_service));
...
work.reset(); // Allow run() to exit. 


* Member Functions

<post>
io_service::post

Request the io_service to invoke the given handler and return immediately.

template<typename CompletionHandler>
void-or-deduced post(CompletionHandler handler);

This function is used to ask the io_service to execute the given handler, but
    'without' allowing the io_service to call the handler from inside this
    function.

The io_service guarantees that the handler will only be called in a thread in
which the run(), run_one(), poll() or poll_one() member functions is currently
being invoked.  
        
Parameters

handler

The handler to be called. The io_service will make a 'copy' of the handler object
as required. The function signature of the handler must be:

void handler();

note: the signature of handler is different from one of deadline_timer.

Remarks

This function throws an exception only if:

the handler's asio_handler_allocate function; or the handler's copy constructor
throws an exception. 

<run>
io_service::run (1 of 2 overloads)

Run the io_service object's event processing loop.

std::size_t run();

The run() function blocks until all work has finished and there are no more
handlers to be 'dispatched', or until the io_service has been stopped.

Multiple threads may call the run() function to set up a pool of threads from
which the io_service may execute handlers. All threads that are waiting in the
pool are equivalent and the io_service may choose any one of them to invoke a
handler.

note: no order?

A normal exit from the run() function implies that the io_service object is
stopped (the stopped() function returns true). Subsequent calls to run(),
        run_one(), poll() or poll_one() will return immediately unless there is
        a prior call to reset().  
        
Return Value
The number of handlers that were executed.

Exceptions
boost::system::system_error. Thrown on failure. 

Remarks
The run() function must not be called from a thread that is currently calling
one of run(), run_one(), poll() or poll_one() on the same io_service object.

The poll() function may also be used to dispatch ready handlers, but without
blocking. 

note: here 'dispatch' means to get a handler from a io_service and calls it.
that is dispatch a user code.


* Types

<work>
io_service::work

Class to inform the io_service when it has work to do.

Member Functions

get_io_service 
Get the io_service associated with the work.

explicit work(boost::asio::io_service & io_service);
Constructor notifies the io_service that work is starting.
Copy constructor notifies the io_service that work is starting.

~work          
Destructor notifies the io_service that the work is complete.

The work class is used to 'inform' the io_service when work starts and finishes.
This ensures that the io_service object's run() function will not exit while
work is underway, and that it does exit when there is no unfinished work
remaining.

The work 'class' is copy-constructible so that it may be used as a data member
in a handler class.  It is not assignable. 

note: 'work object' and 'work'?
In other words, as long as an io_service has a work object associated with it,
   it will never run out of stuff to do.


<handler>
A free function as a completion handler:

void completion_handler()
{
  ...
}


={============================================================================
*kt_dev_boost_104* asio: A guide to getting started with boost::asio

http://www.gamedev.net/blog/950/entry-2249317-a-guide-to-getting-started-with-boostasio/?pg=1

A guide to getting started with boost::asio

Example 1f

#include <boost/asio.hpp>
#include <boost/shared_ptr.hpp>
#include <boost/thread.hpp>
#include <iostream>

boost::asio::io_service io_service;

void WorkerThread()
{
    std::cout << "Thread Start\n";
    io_service.run();
    std::cout << "Thread Finish\n";
}

int main( int argc, char * argv[] )
{
    boost::shared_ptr< boost::asio::io_service::work > work(
            new boost::asio::io_service::work( io_service )
            );

    std::cout << "Press [return] to exit." << std::endl;

    boost::thread_group worker_threads;
    for( int x = 0; x < 4; ++x )
    {
        worker_threads.create_thread( WorkerThread );
    }

    std::cin.get();

    io_service.stop();

    worker_threads.join_all();

    return 0;
}

What should really stand out is how simple and easy it is to make our threaded
programs 'scale'. By simply adding more worker threads, we can support more and
more concurrency for processing work through the io_service object. 

As mentioned before, if we had associated a work object with the io_service and
wanted to let all 'queued' work finish, we would not call stop but rather
destroy the work object. Care has to be taken though. If we want all work to
finish but keep giving the io_service more things to do, then it will never
exit! In that case, at some point, we would want to call the stop function to
ensure the system actually stops.


==============================================================================
Copyright: see |ktkb|                              vim:tw=100:ts=3:ft=help:norl:
