*kt_linux*                                                           tw=100, utf-8

/^[#=]{ 

#{ bash
#{ tool
#{ gdb
#{ linux-core
#{ syscall
#{ sysadmin
#{ syssetup

#{ bash
|kt_linux_bash_000| bash: reference
|kt_linux_bash_001| bash: history and reverse search
|kt_linux_bash_002| bash: variable <substitution> <expansion>
|kt_linux_bash_003| bash: if and test command primaries <string-comparison> <[[>
|kt_linux_bash_004| bash: globbing (wildcard)
|kt_linux_bash_005| bash: quoting <grep>
|kt_linux_bash_006| bash: shell and special parameters <shift-operator> <while>
|kt_linux_bash_007| bash: set var using other commands <dirname> <readlink> <if>
|kt_linux_bash_008| bash: for
|kt_linux_bash_009| bash: select
|kt_linux_bash_010| bash: case
|kt_linux_bash_011| bash: process options using while and getopts
|kt_linux_bash_012| bash: break and continue
|kt_linux_bash_013| bash: read
|kt_linux_bash_014| bash: function
|kt_linux_bash_015| bash: basename
|kt_linux_bash_016| bash: here document
|kt_linux_bash_017| bash-redirection
|kt_linux_bash_018| bash: ':' command
|kt_linux_bash_019| bash: subshell and multiple command <execution-env>
|kt_linux_bash_020| bash: list options
|kt_linux_bash_021| sh-exit: test on exit status
|kt_linux_bash_022| bash: && and ||
|kt_linux_bash_023| bash: debug
|kt_linux_bash_024| bash: builtin: exec
|kt_linux_bash_025| bash: builtin: set
*kt_linux_bash_025* bash: builtin: trap
*kt_linux_bash_025* bash: builtin: shift
|kt_linux_bash_026| bash: which one when there are script and exe binary in the same dir?
|kt_linux_bash_027| bash: test on file using which command
|kt_linux_bash_028| bash: bash: builtin: programmable completion
|kt_linux_bash_029| bash: variable arithmetic
|kt_linux_bash_030| bash: interactive shell or not
|kt_linux_bash_031| bash: double dash
|kt_linux_bash_032| bash: command line edit
|kt_linux_bash_033| bash: true and false
|kt_linux_bash_034| bash: wait
|kt_linux_bash_035| bash: help
|kt_linux_bash_036| bash: shell and environment variable
|kt_linux_bash_037| bash: builtin: command
|kt_linux_bash_038| bash: builtin: prepare
*kt_linux_bash_038* bash-substitude

|kt_linux_bash_100| bash: config: colour prompt
|kt_linux_bash_101| bash: config: how to know shell nesting level

|kt_linux_bash_200| bash: code: debug and printf
|kt_linux_bash_201| bash: code: array from 0 and return 1 for success like c
|kt_linux_bash_202| bash: code: recursion test
*kt_linux_bash_202* bash: code: get filenames from comment in the script

|kt_linux_bash_300| bash: faq: var# 


#{ tool
|kt_linux_tool_001| md5sum
|kt_linux_tool_002| tool-cut
|kt_linux_tool_003| tool-kill, tool-killall
|kt_linux_tool_004| dmesg
*kt_linux_tool_005* tool-check-version
|kt_linux_tool_006| cp
|kt_linux_tool_007| mkdir
|kt_linux_tool_008| strings
|kt_linux_tool_009| sort
*kt_linux_tool_010* tool-grep
|kt_linux_tool_011| find
|kt_linux_tool_012| make a empty file without touch
*kt_linux_tool_013* tool-xargs
|kt_linux_tool_014| ssh and putty {scp}
*kt_linux_tool_015* tool-pgrep, tool-pidof
|kt_linux_tool_016| screen
*kt_linux_tool_017* tool-ldd
|kt_linux_tool_018| ls
|kt_linux_tool_019| strace
|kt_linux_tool_020| time and date
|kt_linux_tool_021| tool-chmod
|kt_linux_tool_022| mknod
|kt_linux_tool_023| wc
|kt_linux_tool_024| du and df
|kt_linux_tool_025| ln
*kt_linux_tool_026* tool-rsync
|kt_linux_tool_027| awk
|kt_linux_tool_028| sed, print a range
|kt_linux_tool_029| pyserial and grabserial
|kt_linux_tool_030| diff and patch
|kt_linux_tool_031| tar
|kt_linux_tool_032| split
|kt_linux_tool_033| getconf
|kt_linux_tool_034| ps
|kt_linux_tool_035| wget, curl
|kt_linux_tool_036| nc
|kt_linux_tool_037| port checks
|kt_linux_tool_038| minicom
*kt_linux_tool_039* tool-mount: find rootfs
|kt_linux_tool_039| mount --bind and nfs
|kt_linux_tool_040| install
|kt_linux_tool_041| notify-send
*kt_linux_tool_042* tool-tail: print multiple files with filename
|kt_linux_tool_044| mc
|kt_linux_tool_045| graphbiz
|kt_linux_tool_046| mktemp
*kt_linux_tool_047* tool-env
|kt_linux_tool_048| term: gnome-terminal
*kt_linux_tool_049* term: terminator
|kt_linux_tool_050| tool-watch
*kt_linux_tool_051* font
*kt_linux_tool_051* tool-echo
*kt_linux_tool_051* tool-wireshark
*kt_linux_tool_051* tool-htop

|kt_linux_tool_100| package: apt-xxx to get package
|kt_linux_tool_101| package: pkg-config
|kt_linux_tool_102| package: dpkg and install deb file

#{ make and build
|kt_linux_tool_140| cmake
|kt_linux_tool_141| cmake: mix of c and cpp build
|kt_linux_tool_142| cmake: cflags
|kt_linux_tool_143| cmake: includes {message-keyword}
|kt_linux_tool_144| cmake: link group

*kt_linux_tool_150* autoconf:
*kt_linux_tool_150* autoconf: error on LDFLAGS

|kt_linux_tool_150| automake:
*kt_linux_tool_150* automake: standard targets
*kt_linux_tool_150* automake: directory variable
*kt_linux_tool_150* automake: distcheck
*kt_linux_tool_150* automake: example
*kt_linux_tool_150* automake: example: explained
*kt_linux_tool_150* automake: 03: variables
*kt_linux_tool_150* automake: 04: example packages
*kt_linux_tool_150* automake: 08: building programs and libraries
*kt_linux_tool_150* automake: 08: program variables
*kt_linux_tool_150* automake: 08: program variables: case issue
*kt_linux_tool_150* automake: 09: scripts
*kt_linux_tool_150* automake: 15: test and make check
|kt_linux_tool_160| ccache

|kt_linux_tool_200| bin-:
*kt_linux_tool_201* bin-readelf
|kt_linux_tool_202| binutil: check if it is built with debug symbols
*kt_linux_tool_202* binutil: call trace of debug and release build
|kt_linux_tool_202| bin-nm
|kt_linux_tool_203| binutil: objdump
|kt_linux_tool_204| binutil: addr2line
|kt_linux_tool_205| binutil: ld
*kt_linux_tool_250* bin-busybox

#{ gdb
|kt_linux_tool_300| gdb: term and links
*kt_linux_tool_300* gdb: sample session
*kt_linux_tool_301* gdb: start
*kt_linux_tool_301* gdb: start: options
*kt_linux_tool_301* gdb: start: repeatedly run a test in GDB, until it fails
*kt_linux_tool_301* gdb: start: what do during startup
*kt_linux_tool_301* gdb: log
*kt_linux_tool_301* gdb: shell command
*kt_linux_tool_301* gdb-cpp:
*kt_linux_tool_301* gdb-cpp: completion
*kt_linux_tool_302* gdb: general and help
|kt_linux_tool_303| gdb: about status
|kt_linux_tool_304| gdb: debugging info
|kt_linux_tool_305| gdb: run and env, args, and tty
*kt_linux_tool_306* gdb-attach: debug already running process
|kt_linux_tool_307| gdb: debug with multiple threads
*kt_linux_tool_308* gdb: call trace
*kt_linux_tool_309* gdb: tip: get a call trace promatically

|kt_linux_tool_320| gdb: examine sources and source path
|kt_linux_tool_321| gdb: sources and machine code
|kt_linux_tool_322| gdb: examine symbols
|kt_linux_tool_323| gdb: examine symbols loading
|kt_linux_tool_324| gdb: specify files
*kt_linux_tool_325* gdb-slib: difference between with -g and without -g
*kt_linux_tool_325* gdb-slib: load, search
*kt_linux_tool_325* gdb-slib: ex
|kt_linux_tool_325| gdb: specify files for shared library on remote
|kt_linux_tool_326| gdb: debugging with strace and gdb
|kt_linux_tool_327| gdb: debugging remote 
|kt_linux_tool_328| gdb: debugging remote real example

|kt_linux_tool_300| gdb: core dump setting
*kt_linux_tool_300* gdb: sample session
|kt_linux_tool_301| gdb: core dump analysis {frame-command}
|kt_linux_tool_302| gdb: kernel crash analysis
|kt_linux_tool_303| gdb: .gdbinit
|kt_linux_tool_307| gdb: commands for stepping
*kt_linux_tool_309* gdb-break
*kt_linux_tool_309* gdb-break: specifying a location
*kt_linux_tool_309* gdb-break: specifying a location in cpp
*kt_linux_tool_309* gdb-break: set
*kt_linux_tool_309* gdb-break: conditional ex
*kt_linux_tool_309* gdb-break: info, get you quickly where
*kt_linux_tool_309* gdb-break: set in shared
*kt_linux_tool_309* gdb-break: set watch
*kt_linux_tool_309* gdb-break: set catch
*kt_linux_tool_310* gdb-break: unset
*kt_linux_tool_310* gdb-break: disable
*kt_linux_tool_310* gdb-break: run command on break
*kt_linux_tool_310* gdb-dprintf
|kt_linux_tool_311| gdb: examine running

|kt_linux_tool_400| gdb: remote debugging
|kt_linux_tool_450| gdb: frontend tool: cgdb
*kt_linux_tool_251* gdb-fend: eclipse tip


#{ linux-performance
|kt_linux_perf_001| bootchart

#{ linux-core
|kt_linux_core_001| 
|kt_linux_core_002| stdin, stdout, and stderr
|kt_linux_core_003| check libc version and path
|kt_linux_core_004| system call errors and errno
|kt_linux_core_005| error handling codes from LPI
|kt_linux_core_006| process credential
|kt_linux_core_007| /dev/null
|kt_linux_core_008| score: file ownership and permissions

|kt_linux_core_050| signal
|kt_linux_core_051| signal: example: use signal as synchronization
|kt_linux_core_052| signal: kill: checking for the existence of a process

|kt_linux_core_100| process
|kt_linux_core_101| process: creation
|kt_linux_core_102| process: termination
|kt_linux_core_103| process: monitor child process
|kt_linux_core_104| process: zombie
|kt_linux_core_105| process: memory layout
|kt_linux_core_106| process: virtual memory, user and kernel stack, stack frame
|kt_linux_core_107| process: environment list
|kt_linux_core_108| process: group
|kt_linux_core_109| process: deamon
|kt_linux_core_110| process: execution, exec call, <exec-wrapper-example>

|kt_linux_core_150| thread vs process. LPI 29
|kt_linux_core_151| pthread and source
|kt_linux_core_152| pthread errno. LPI 29.2
|kt_linux_core_153| pthread compile
|kt_linux_core_154| pthread create and exit
|kt_linux_core_155| pthread join and detach. zombie-thread. LPI 29
|kt_linux_core_156| pthread thread id. LPI 29

|kt_linux_core_150| pthread {nptl}
|kt_linux_core_102| how to run three threads sequencially
|kt_linux_core_103| priority and schedule

|kt_linux_core_200| file io
|kt_linux_core_201| file io: further details
|kt_linux_core_202| file io: non-blocking
|kt_linux_core_203| file io: /dev/fd
|kt_linux_core_204| system resource limits

|kt_linux_core_200| ipc
|kt_linux_core_201| ipc: system v <ipcs-command>
|kt_linux_core_202| ipc: system v: shm
|kt_linux_core_203| ipc: server consideration

|kt_linux_core_250| ipc: posix

|kt_linux_core_300| ipc: socket: LPI 56

|kt_linux_core_101| ipc: pipe
|kt_linux_core_102| ipc: way to check pipe fd and setup
|kt_linux_core_103| ipc: fifo
|kt_linux_core_104| ipc: semantics of read() and write() on pipes and fifo
|kt_linux_core_105| ipc: which one to use {semaphores-versus-pthreads-mutexes}

|kt_linux_core_200| ipc: sync: semaphore 
*kt_linux_core_201* sync: mutex
*kt_linux_core_202* sync: deadlock
*kt_linux_core_202* sync-cond: why condition variable using cons and prod problem
*kt_linux_core_202* sync-cond: condition variable
*kt_linux_core_202* sync-cond: example:
*kt_linux_core_202* sync-cond: example: use single lock for api and callback
*kt_linux_core_202* sync: between processes
|kt_linux_core_220| sync: read-write lock

|kt_linux_core_230|  sync: file-lock

|kt_linux_core_240|  sync: common problems when use threads {race-condition}

|kt_linux_core_250|  sync: reentrant and thread-safe {thread-specific-data} {thread-local-storage}
|kt_linux_core_260|  sync: atomic operations {lock-free-programming}
|kt_linux_core_261|  sync: ref: locks aren't slow; lock contention is
|kt_linux_core_262|  sync: ref: always use a lightweight mutex {mutex-vs-semaphore}
|kt_linux_core_263|  conc: ref: lock-free code: a false sense of security
|kt_linux_core_264|  conc: ref: the free lunch is over 
*kt_linux_core_265* sync: case: subtle race
*kt_linux_core_266* sync: case: sync with no lock

|kt_linux_core_290|  ref: concurrency in C++. ch01
|kt_linux_core_291|  ref: concurrency in C++. ch02 {std::thread}
|kt_linux_core_293|  ref: concurrency in C++. ch03

|kt_linux_core_300|  case: own semaphore and mutex class using pthread cond-var {cqueue}
|kt_linux_core_301|  case: use of mutex and thread class
|kt_linux_core_302|  case: analysis of 200 and 201 case
|kt_linux_core_303|  case: msg q between threads

*kt_linux_core_400* uclibc
*kt_linux_core_400* uclibc: config ld_debug
*kt_linux_core_400* uclibc: fail to find symbol

*kt_linux_core_400* slib: shared library
*kt_linux_core_400* slib: dynamic linker
*kt_linux_core_401* slib: --as-needed flag and link error
*kt_linux_core_402* slib: search and resolve
*kt_linux_core_403* slib: case problem in name resolve. crash
*kt_linux_core_404* slib: case problem in name resolve. pthread stub 
*kt_linux_core_405* slib: dl, ABI, visibility
|kt_linux_core_406| shared library: preload, debug and monitor ld
|kt_linux_core_407| shared library: further information
|kt_linux_core_408| shared library: md5sum
|kt_linux_core_409| shared library: inspect dynamic sections
|kt_linux_core_410| shared library: check libraries that process uses
*kt_linux_core_411* slib: case problem in open failure
*kt_linux_core_411* slib: points to enhance performance
*kt_linux_core_412* slib: ld.conf and ld.cache
*kt_linux_core_412* slib: as-needed and _GLOBAL_OFFSET_TABLE_

|kt_linux_core_500| sandbox

|kt_linux_core_500|  file io

|kt_linux_core_600| time
|kt_linux_core_601| time: conversion
|kt_linux_core_602| time: resolution, jiffies
|kt_linux_core_603| time: posix clock, realtime
|kt_linux_core_604| time: ms timestamp
|kt_linux_core_605| time: us timestamp
|kt_linux_core_606| time: sleep

|kt_linux_core_700| dbus
|kt_linux_core_701| dbus: introspection
|kt_linux_core_702| dbus: dbus-send tool
|kt_linux_core_703| dbus: lsdbus tool
|kt_linux_core_704| dbus: dbus-monitor tool
|kt_linux_core_710| dbus: kdbus

|kt_linux_core_800| proc: /proc/mounts
|kt_linux_core_801| proc: /proc/PID/status
|kt_linux_core_802| proc: /proc/PID/maps
|kt_linux_core_803| proc: /proc/PID/exe
|kt_linux_core_804| proc: /proc/PID/fd. which file does process open?
|kt_linux_core_813| proc: /proc/sys/kernel, system resource limits
|kt_linux_core_814| proc: /proc/stat
*kt_linux_core_814* proc: /proc/PID/cmdline, get process name

#{ syscall
*kt_linux_sysc_001* syscall list
|kt_linux_sysc_001| pause
|kt_linux_sysc_002| alarm
|kt_linux_sysc_003| getenv, setenv

#{ sysadmin
|kt_linux_sysa_001| sys: cannot execute a file in cdrom, fstab

#{ syssetup
|kt_linux_sete_001| ubuntu: virtualbox
|kt_linux_sete_002| 
|kt_linux_sete_003| ubuntu: samba
|kt_linux_sete_004| ubuntu: nfs
|kt_linux_sete_005| ubuntu: check running services
|kt_linux_sete_006| ubuntu: connect from windows remote desktop
|kt_linux_sete_007| ubuntu: cpuinfo
|kt_linux_sete_008| ubuntu: change default application setting

|kt_linux_sete_100| gnome: windows
|kt_linux_sete_101| gnome: workspace
*kt_linux_sete_101* gnome: change wallpapers
|kt_linux_sete_102| win xserver
|kt_linux_sete_103| firefox key shortcuts
|kt_linux_sete_104| solarized color for ls and gnome
|kt_linux_sete_105| set: local web server
*kt_linux_sete_105* set: update adobe flash plugin

|kt_linux_sete_200| which to install?

|kt_linux_refe_001|  references


# ============================================================================
#{
={============================================================================
*kt_linux_bash_000* bash: reference

http://www.gnu.org/software/bash/

Advanced Bash-Scripting Guide
https://wiki.kldp.org/HOWTO/html/Adv-Bash-Scr-HOWTO/
http://www.tldp.org/LDP/abs/html/


Bash Hackers Wiki Frontpage
http://wiki.bash-hackers.org/start


={============================================================================
*kt_linux_bash_001* bash: history

{erase-duplicates}
# eliminate the continuous repeated entry across the whole history

export HISTCONTROL=erasedups

<history-per-terminal>
# in .bash_profile
echo $SSH_TTY
export HISTFILE=/home/NDS-UK/parkkt/.bash_history_$(echo $SSH_TTY | cut -f 4 -d'/')
export HISTFILESIZE=10000
export HISTSIZE=10000 

note: better to have TTY number in the prompt as well.


# run multiple commands from the history

fc [-e ename] [-lnr] [first] [last]
fc -s [pat=rep] [cmd]

Fix Command. In  the  first form, a range of commands from first to last is selected
from the history list.  First and last may be specified as a string (to locate  the  last
command  beginning  with  that string)  or  as  a  number (an index into the history list,
where a negative number is used as an offset from the current command number).

This is not quite what you want as it will launch an editor first, but that is probably a
good thing since it gives you a chance to double check that you have the correct commands
and even edit them using all the capabilities of your favorite editor. Once you save you
changes and exit the editor, the commands will be run.

e.g.

$ fc 100 120


{reverse-search-history}
http://www.gnu.org/software/bash/manual/html_node/Commands-For-History.html

<C-r> to search backward starting at the current line and moving 'up' through
the history as necessary. This is an incremental search.

Once you've found the command you have several options:

1. Press enter to run it
2. <C-r> to cycle through other commands that are filterd out with the letters
you've typed.
3. <C-g> to quit the search and back to the command line 'empty'-handed
4. Press ESC to take one and edit the command.


<edit-command-line>
To set vi mode for editing command line:

8.3.1 Readline Init File Syntax

There are only a few basic constructs allowed in the Readline init file. Blank lines are ignored.
Lines beginning with a `#' are comments. Lines beginning with a `$' indicate conditional constructs
(see section 8.3.2 Conditional Init Constructs). Other lines denote variable settings and key
bindings.

Variable Settings
You can modify the run-time behavior of Readline by altering the values of variables in Readline
using the set command within the init file. The syntax is simple:

    set variable value

Here, for example, is how to change from the default Emacs-like key binding to use vi line editing
commands:

    set editing-mode vi

To set binding to up/down key to history search:

# ~/.inputrc
"\e[A": history-search-backward
"\e[B": history-search-forward

or equivalently,

# ~/.bashrc
bind '"\e[A": history-search-backward'
bind '"\e[B": history-search-forward'

Normally, Up and Down are bound to the Readline functions previous-history and next-history
respectively. I prefer to bind PgUp/PgDn to these functions, instead of displacing the normal
operation of Up/Down.

# ~/.inputrc
"\e[5~": history-search-backward
"\e[6~": history-search-forward

After you modify ~/.inputrc, restart your shell or use Ctrl+X, Ctrl+R to tell it to re-read
~/.inputrc. By the way, if you're looking for relevant documentation: Bash uses The GNU Readline
Library for the shell prompt and history.


={============================================================================
*kt_linux_bash_002* bash: variable

{global-and-local}
Global variables or 'environment' variables are available in all shells. The env or printenv
commands can be used to display environment variables.

Local variables are only available in the current shell. Using the set built-in command without any
options will display a list of all variables (including environment variables) and functions.


{set-variable}
Putting spaces around the equal sign will cause errors.

VARNAME="value"

A variable created like the ones in the example above is only available to the current shell. It is
a local variable: child processes of the current shell will not be aware of this variable. In order
to pass variables to a subshell, we need to export them using the export built-in command.

export VARNAME="value"

note: parent can export variables to child but not vice versa.

<set-variable-for-child>
In the Bourne shell and its descendants (e.g., bash and the Korn shell), the following syntax can be
used to add values to the environment used to execute a single program, without affecting the parent
shell (and subsequent commands):

$ NAME=value program

This adds a definition to the environment of 'just' the child process executing the named program.
If desired, multiple assignments (delimited by white space) can precede the program name.

<reserved-variables>
PATH        A colon-separated list of directories in which the shell looks for commands.
PS1         The primary prompt string. The default value is "'\s-\v\$ '".
PS2         The secondary prompt string. The default value is "'> '".


<special-variable>
$$          Expands to the process ID of the shell.
$?          The exit status

{constant}
The readonly built-in marks each specified variable as unchangeable. When tried to set, displays
error but execution continues.

readonly CONST=100
echo "my const var is $CONST.."
CONST=200
echo "my const var is $CONST.."

bash: CONST: readonly variable


3.4 Shell Parameters

A parameter is an entity that stores values. It can be a 'name', a 'number', or one of the special
characters listed below. A variable is a parameter denoted by a 'name'. 

A variable has a value and zero or more attributes. Attributes are assigned using the declare
builtin command (see the description of the declare builtin in Bash Builtins).

A parameter is set if it has been assigned a value. Once a variable is set, it may be unset only by
using the unset builtin command.

A variable may be assigned to by a statement of the form

name=[value]

If value is not given, the variable is assigned the null string and the null string is a 'valid' value. 

All values undergo tilde 'expansion', parameter and variable expansion, command substitution,
arithmetic expansion, and quote removal. If the variable has its integer attribute set, then value
    is evaluated as an arithmetic expression even if the $((...)) expansion is not used (see
            Arithmetic Expansion). Word splitting is not performed, with the exception of "$@" as
    explained below. Filename expansion is not performed.  Assignment statements may also appear as
    arguments to the alias, declare, typeset, export, readonly, and local builtin commands. When in
    POSIX mode (see Bash POSIX Mode), these builtins may appear in a command after one or more
    instances of the command builtin and retain these assignment statement properties.


about "+=" operator:

In the context where an assignment statement is assigning a value to a shell variable or array index
(see Arrays), the ‘+=’ operator can be used to append to or add to the variable's previous value.
When ‘+=’ is applied to a variable for which the integer attribute has been set, value is evaluated
as an arithmetic expression and added to the variable's current value, which is also evaluated. When
‘+=’ is applied to an array variable using compound assignment (see Arrays), the variable's value is
not unset (as it is when using ‘=’), and new values are appended to the array beginning at one
greater than the array's maximum index (for indexed arrays), or added as additional key-value pairs
in an associative array. When applied to a string-valued variable, value is expanded and appended to
the variable’s value.

TODO: about nameref which is call by reference. 


3.4.1 Positional Parameters

The shell's command-line arguments.

A positional parameter is a parameter denoted by one or more digits, other than the single digit 0.
note: not use '0'.

Positional parameters are assigned from the shell's arguments when it is invoked, and may be
reassigned using the set builtin command. Positional parameter N may be referenced as ${N}, or as $N
when N consists of a single digit. Positional parameters may not be assigned to with assignment
statements. The set and shift builtins are used to set and unset them (see Shell Builtin Commands).
The positional parameters are temporarily replaced when a shell function is executed (see Shell
        Functions).

When a positional parameter consisting of more than a single digit is expanded, it must be enclosed
in braces. 


<no-parameter-not-set>
If not given in the command line, not set.

#!/bin/bash
echo $1 $2 $3 ' -> echo $1 $2 $3'

$./runsh.sh 1 2
1 2


3.4.2 Special Parameters

The shell treats several parameters specially. These parameters may only be referenced; assignment
to them is not allowed. 


*
($*) Expands to the positional parameters, starting from one. When the expansion is not within
double quotes, each positional parameter expands to a separate word. In contexts where it is
performed, those words are subject to further word splitting and pathname expansion. 

When the expansion occurs "within double quotes", it expands to a single word with the value of each
parameter separated by the first character of the IFS special variable. That is, "$*" is equivalent
to "$1c$2c..." where c is the first character of the value of the IFS variable. If IFS is unset, the
parameters are separated by spaces. If IFS is null, the parameters are joined without intervening
separators.

note: "unset" is different from "null"

<code>
This function is to make a single line from a array(list) separated by ":".

function list_join() {
    local sep="$1"
    shift

    (
        IFS="$sep"
        echo "$*"
    )
}

declare -ar preload_libs=(
    /usr/local/lib/libdirectfb.so
    /usr/local/lib/libdirect.so
    /usr/local/lib/libinit.so
    /lib/libpthread.so.0
)

vars+=(LD_PRELOAD=$(list_join ":" ${preload_libs[@]}))


@
($@) Expands to the positional parameters, starting from one.  When the expansion occurs "within
double quotes", each parameter expands to a separate word. That is, "$@" is equivalent to "$1"
"$2"... 

If the double-quoted expansion occurs within a word, the expansion of the first parameter is joined
with the beginning part of the original word, and the expansion of the last parameter is joined with
the last part of the original word. When there are no positional parameters, "$@" and $@ expand to
nothing (i.e., they are removed).

<arrayfy>
args=("$@");   # to get 'all' args and set it to array.
echo ${args[0]} ${args[1]} ${args[2]}

echo $@        # to print all args in a one go


0
($0) Expands to the name of the shell or shell script. This is set at shell initialization. If Bash
is invoked with a file of commands (see Shell Scripts), $0 is set to the name of that file. If Bash
is started with the -c option (see Invoking Bash), then $0 is set to the first argument after the
string to be executed, if one is present. Otherwise, it is set to the filename used to invoke Bash,
as given by argument zero.

<code>
This variable is commonly used to determine the behavior of scripts that can be invoked with more
than one name.

$ ln -s mytar listtar
$ ln -s mytar maketar

$ listtar or maketar

#!/bin/sh
case $0 in
   *listtar) TARGS="-tvf $1" ;;
   *maketar) TARGS="-cvf $1.tar $1" ;;
esac
tar $TARGS


#
($#) Expands to the number of positional parameters in decimal. the number of args passed in the
command line. exclued 0th.


?
($?) Expands to the exit status of the most recently executed foreground pipeline.



={============================================================================
*kt_linux_bash_003* bash: array

<indexed-and-associateive>
Bash provides one-dimensional indexed and associative array variables. Indexed arrays are referenced
using integers and are zero-based; associative arrays use arbitrary strings.

An indexed array is created automatically if any variable is assigned to using the syntax

name[subscript]=value

The subscript is treated as an arithmetic expression that must 'evaluate' to a number. 
note: evaluates which means subcript can be a variable.

To explicitly declare an array, use

declare -a name

Associative arrays are created using

declare -A name.

Attributes may be specified for an array variable using the declare and readonly builtins. Each
attribute applies to all members of an array.

Arrays are assigned to using compound assignments of the form

name=(value1 value2 ... )

where each value is of the form [subscript]=string. Indexed array assignments do not require
anything but string. When assigning to indexed arrays, if the optional subscript is supplied, that
index is assigned to; otherwise the index of the element assigned is the last index assigned to by
the statement plus one. Indexing starts at 'zero'. 


FRUIT[0]=apple
FRUIT[1]=banana
FRUIT[2]=orange

Or,

FRUIT=(apple plum blackberry)


<reference>
Any element of an array may be referenced using ${name[subscript]}. The braces are required to avoid
conflicts with the shell's filename expansion operators. 

If the subscript is ‘@’ or ‘*’, the word expands to all members of the array name. These subscripts
differ 'only' when the word appears within double quotes. If the word is double-quoted, ${name[*]}
expands to a single word with the value of each array member separated by the first character of the
    IFS variable, and ${name[@]} expands each element of name to a separate word. 

note: the difference is [*] uses IFS but [@] is not as with "special parameters"

When there are no array members, ${name[@]} expands to nothing.
If the double-quoted expansion occurs within a word, the expansion of the first parameter is joined
with the beginning part of the original word, and the expansion of the last parameter is joined with
the last part of the original word. This is analogous to the expansion of the special parameters ‘@’
and ‘*’. ${#name[subscript]} expands to the length of ${name[subscript]}. If subscript is ‘@’ or
‘*’, the expansion is the number of elements in the array. 

Referencing an array variable without a subscript is equivalent to referencing with a subscript of 0. 

If the subscript used to reference an element of an indexed array evaluates to a number less than
zero, it is interpreted as relative to one greater than the maximum index of the array, so negative
indices count back from the end of the array, and an index of -1 refers to the last element. 


echo "idx 0 = ${FRUIT[0]}"    # idx 0 = apple. just a string
echo "idx 1 = ${FRUIT[1]}"
echo "idx 2 = ${FRUIT[2]}"

echo "all = ${FRUIT[*]}"      # to reference all
echo "all = ${FRUIT[@]}"      # same as above. note: this is to array'fy' from a long string.


uri="$1"
[[ $uri =~ [1-9][0-9]* ]] && uri="${urls[$uri]}"


<example>
farm_hosts=(web03 web04 web05 web06 web07)

for i in ${farm_hosts[@]}; do
  su $login -c "scp $httpd_conf_new ${i}:${httpd_conf_path}"
  su $login -c "ssh $i sudo /usr/local/apache/bin/apachectl graceful"
done

<example> for use of array and here document
A cron job that fills an array with the possible candidates, uses date +%W to find the week of the
year, and does a modulo operation to find the correct index. The lucky person gets notified by
e-mail.


#!/bin/bash
# This is get-tester-address.sh
#
# First, we test whether bash supports arrays. (Support for arrays was only added recently.)
#
whotest[0]='test' || (echo 'Failure: arrays not supported in this version of bash.' && exit 2)
#
# Our list of candidates. (Feel free to add or
# remove candidates.)
#
wholist=(
  'Bob Smith <bob@example.com>'
  'Jane L. Williams <jane@example.com>'
  'Eric S. Raymond <esr@example.com>'
  'Larry Wall <wall@example.com>'
  'Linus Torvalds <linus@example.com>'
)
#
# Count the number of possible testers.
# (Loop until we find an empty string.)
#
count=0
while [ "x${wholist[count]}" != "x" ]
do
   count=$(( $count + 1 ))
done
#
# Now we calculate whose turn it is.
#
week=`date '+%W'`             # The week of the year (0..53).
week=${week#0}                # Remove possible leading zero.
let "index = $week % $count"  # week modulo count = the lucky person
email=${wholist[index]}       # Get the lucky person's e-mail address.
echo $email                   # Output the person's e-mail address.

This script is then used in other scripts, such as this one, which uses a here document:

email=`get-tester-address.sh` # Find who to e-mail.
hostname=`hostname` # This machine's name.

# Send e-mail to the right person.

mail $email -s '[Demo Testing]' <<EOF
  The lucky tester this week is: $email
  Reminder: the list of demos is here:
  http://web.example.com:8080/DemoSites
  (This e-mail was generated by $0 on ${hostname}.)
EOF


{unset}
The unset built-in is used to destroy arrays or member variables of an array:


{parameter-expansion}
3.5.3 Shell Parameter Expansion

The '$' character introduces parameter expansion, command substitution, or arithmetic expansion. The
parameter name or symbol to be expanded may be enclosed in 'braces', which are 'optional' but serve to
protect the variable to be expanded from characters immediately following it which could be
interpreted as part of the name. 

${parameter:-word} 
If parameter is null or unset, word is substituted for parameter. The value of parameter does not
change. In other words, if unset, word is used and if set, parameter used.

This use 'default' to make sure that the prompt is always set correctly.
PS1=${HOST:-localhost}"$ " ; export PS1 ;

This from often used in conditional tests and this set 80 when it is not defined before. That is
default. note: why not use {parameter:=word} form?

# [ -z STRING ] True of the length of "STRING" is zero.

[ -z "${COLUMNS:-}" ] && COLUMNS=80

It is a shorter notation for

if [ -z "${COLUMNS:-}" ]; then
   COLUMNS=80
fi

note: In this form, the value of parameter does not change. Here TEST is not defined.

$ echo $TEST
$ echo ${TEST:-test}
test
$ echo $TEST
$


${parameter:=word} 
If parameter is null or unset, parameter is set to the value of word.

$ echo $TEST
$ echo ${TEST:=test}
test
$ echo $TEST
test
$

note: Q? this works as it has default value when parameter is not set, null. what is it?
box_ip=${2-localhost}
box_port=${3-2033}


${parameter:+word}
If parameter is null or unset, nothing is substituted, otherwise the expansion of word is
substituted.


<substring-expansion>

${parameter:offset}
${parameter:offset:length}

The LENGTH parameter defines how many characters to keep, starting from the first character after
the offset point. If LENGTH is omitted, the remainder of the variable content is taken

${VAR:OFFSET:LENGTH}

$ STRING="thisisaverylongname"
$ echo ${STRING:4}
isaverylongname
$ echo ${STRING:6:5}
avery


{expansion-on-arg}
If parameter is @, the result is length positional parameters 'beginning' at offset. A negative
offset is taken relative to one greater than the greatest positional parameter, so an offset of -1
evaluates to the last positional parameter. It is an expansion error if length evaluates to a number
less than zero.

The following examples illustrate substring expansion using positional parameters:

$ set -- 1 2 3 4 5 6 7 8 9 0 a b c d e f g h
$ echo ${@:7}
7 8 9 0 a b c d e f g h
$ echo ${@:7:0}

$ echo ${@:7:2}
7 8

$ echo ${@:7:-2}
bash: -2: substring expression < 0

$ echo ${@: -7:2}
b c

note: this shows all args
$ echo ${@}
1 2 3 4 5 6 7 8 9 0 a b c d e f g h

$ echo ${@:0}
./bash 1 2 3 4 5 6 7 8 9 0 a b c d e f g h

$ echo ${@:0:2}
./bash 1


={============================================================================
*kt_linux_bash_003* bash: if and test command primaries

if TEST-COMMANDS; then CONSEQUENT-COMMANDS; fi

The TEST-COMMAND list is executed, and if its return status is zero, the CONSEQUENT-COMMANDS list is
  executed. The return status is the exit status of the last command executed, or zero if no
  condition tested true.

<primaries-on-conditional>
Primary        Meaning
[ -a FILE ]    True if FILE exists.
[ -x FILE ]    True if FILE exists and is excutable.
[ -f FILE ]    True if FILE exists and is a regular file.

[ -z string ]  True if the length of string is zero.
[ -n string ]  True if the length of string is non-zero.

note: can use on env variable.

if [ -z "${ZB_CFG}" ]; then
   # when not set
else
   # when set
fi


if [ -n "${ZB_FORCE_BRANCH}" ]; then
   # when set
else
   # when not set
fi

<string-comparison>
[ STRING1 == STRING2 ] 
True if the strings are equal. note: "=" may be used instead of "==" for strict POSIX compliance.

[ STRING1 != STRING2 ] True if the strings are not equal.

Regular expressions may also be used in comparisons:

gender="female"
if [[ "$gender" == f* ]]


{test-expr}
<single>
test
[

    test expr

Evaluate a conditional expression expr and return a status of 0 (true) or 1 (false). Each operator
and operand must be a separate argument. Expressions are composed of the primaries described below
in Bash Conditional Expressions. test does not accept any options, nor does it accept and ignore an
argument of -- as signifying the end of options.

When the [ form is used, the last argument to the command must be a ].

Expressions may be combined using the following operators, listed in decreasing order of precedence.
The evaluation depends on the number of arguments; see below. Operator precedence is used when there
are five or more arguments.

! expr 
True if expr is false.

( expr )
Returns the value of expr. This may be used to override the normal precedence of operators.

expr1 -a expr2
True if both expr1 and expr2 are true.

expr1 -o expr2
True if 'either' expr1 or expr2 is true. 

The test and [ builtins evaluate conditional expressions using a set of rules based on the number of
arguments. 

<double>
http://www.gnu.org/software/bash/manual/bash.html#Conditional-Constructs

note: Use [] whenever you want your script to be 'portable' across shells. Use [[]] if you want
conditional expressions not supported by [] and don't need to be portable.

[[ is bash's improvement to the [ command. It has several enhancements that make it a better choice
if you write scripts that target bash. My favorites are:

1. It is a syntactical feature of the shell, so it has some special behavior that [ doesn't have.
You no longer have to quote variables like mad because [[ handles empty strings and strings with
whitespace more intuitively. For example, with [ you have to write

    if [ -f "$FILE" ]

to correctly handle empty strings or file names with spaces in them. With [[ the quotes are
unnecessary:

    if [[ -f $FILE ]]

2. Because it is a syntactical feature, it lets you use && and || operators for boolean tests and < and
> for string comparisons. [ cannot do this because it is a regular command and &&, ||, <, and > are
not passed to regular commands as command-line arguments.

3. It has a wonderful =~ operator for doing regular expression matches. With [ you might write

    if [ "$ANSWER" = y -o "$ANSWER" = yes ]

    With [[ you can write this as

    if [[ $ANSWER =~ ^y(es)?$ ]]

It even lets you access the captured groups which it stores in BASH_REMATCH. For instance,
   ${BASH_REMATCH[1]} would be "es" if you typed a full "yes" above.

4. You get pattern matching aka globbing for free. Maybe you're less strict about how to type yes.
Maybe you're okay if the user types y-anything. Got you covered:

    if [[ $ANSWER = y* ]]

Keep in mind that it is a bash extension, so if you are writing sh-compatible scripts then you need
to stick with [. Make sure you have the #!/bin/bash shebang line for your script if you use double
brackets.


={============================================================================
*kt_linux_bash_004* bash: globbing (wildcard)

*              Matches zero or more occurrences of any character
?              Matches one occurrence of any character
[characters]   Matches one occurrence of any of the given characters
[!characters]  Negative match. e.g. $ ls [!a]* 


={============================================================================
*kt_linux_bash_005* bash: quoting

<backslash>
$ echo You owe \$1250
You owe $1250

<single>
Quote all inside the single quote.

$ echo '<-$1250.**>; (update?) [y|n]'
<-$1250.**>; (update?) [y|n]

<double> <use>
Do not quote all (see below) so that can use 'variables' inside the quoted string.

$ for parameter substitution.

All other \ characters are literal (not special).

`  for command substitution.
\$ to enable literal dollar signs.
\Â´ to enable literal backquotes.
\" to enable embedded double quotes.
\\ to enable embedded backslashes.

$ echo '$USER owes <-$1250.**>; [ as of (`date +%m/%d`) ]'
$USER owes <-$1250.**>; [ as of (`date +%m/%d`) ]

$ echo "$USER owes <-$1250.**>; [ as of (`date +%m/%d`) ]"
parkkt owes <-250.**>; [ as of (01/26) ]

<quote-parms-to-other-command> <use>
To prevent globbing by shell before passing params to grep command.

$ grep '[0-9][0-9]*$' report2 report7




={============================================================================
*kt_linux_bash_007* bash: set var using other commands

Two ways to set var by running other commands.

<one>
DIR=$(dirname $0)

<two>
if [[ "$dir" != /* ]]
then
   echo "if"
   CURDIR=`pwd`                       # back quote
   DIR=$(readlink -f $CURDIR/$DIR)
   echo "DIR:$DIR"
fi

if [[ "$dir" != /* ]]; then         # /* ?
   echo "if"
   CURDIR=`pwd`                       # back quote
   DIR=$(readlink -f $CURDIR/$DIR)
   echo "DIR:$DIR"
fi

note: here dirname and readlink are commands.

dirname - strip last component from file name. eg. /usr/bin/ -> /usr
readlink - print value of a symbolic link or canonical file name


={============================================================================
*kt_linux_bash_008* bash: for

<form>
for name in word1 word2 ... wordN
do
   list
done

<example> in command line
for f in $(ls); do echo "var is: $f"; done

note: It is important to remember that the then and fi are considered to be separated statements in
the shell. Therefore, when issued on the command line, they are separated by a semi-colon.

for f in $( ls /var/ ); do
   echo $f
done

note: 'no' need to specify index and to shift.

<example>
for FILE in $HOME/.bash*
do
   cp $FILE ${HOME}/public_html
   chmod a+r ${HOME}/public_html/${FILE}
done


={============================================================================
*kt_linux_bash_009* bash: select

<form>
select name in word1 word2 ... wordN
do
   list
done

<example>
select COM in comp1 comp2 comp3 all none
do
   echo "is in do"
done

$ ./sample.sh  
1) comp1
2) comp2
3) comp3
4) all
5) none
#?

note: Automatically make numbers and prompts. COM var will have whatever value entered. Also this
example do infinite loop until press C-c since no break statement in do.

<example>
select COM in comp1 comp2 comp3 all none
do
  case $COM in
      comp1 | comp2 | comp3) echo "sel is $COM" ;;
      all) echo "sel is $COM" ;;
      none)
          echo "sel is $COM"
          break
           ;;
       *) echo "ERROR. sel is $REPLY" ;;
   esac
done

You can change the prompt displayed by the select loop by altering the variable PS3. If PS3 is not
set, the default prompt, #?, is displayed. Otherwise the value of PS3 is used as the prompt to
display. For example, the commands

$ PS3="Please make a selection => " ; export PS3

note: All loops has 'do .. done' block and careful the place of 'do'.


={============================================================================
*kt_linux_bash_010* bash: case

Can use either in the loop or it alone.

<form>
case $variable in
 $condition1) or pattern1)
 command...
 ;;

 $condition2 )
 command...
 ;;
esac


<example>
case $PLATFORM in 
  sam_bcm )
    PLATFORM_DIR=SAM_BCM_MIPS4K_LNUX_DARWIN_03
    FUSIONOS=FUSIONOS_3
    ENDIAN_TYPE=BigEndian
    ;;    
  pace_bcm )
    PLATFORM_DIR=PACE_BCM_MIPS4K_LNUX_DARWIN_02
    FUSIONOS=FUSIONOS_3
    ENDIAN_TYPE=BigEndian
    ;;
  ams_bcm )
    PLATFORM_DIR=AMS_BCM_MIPS4K_LNUX_DARWIN_01
    FUSIONOS=FUSIONOS_2
    ENDIAN_TYPE=BigEndian
    ;;
  tmm_bcm )
    PLATFORM_DIR=TMM_BCM_MIPS4K_LNUX_DARWIN_02
    FUSIONOS=FUSIONOS_3
    ENDIAN_TYPE=BigEndian
    ;;
  * )
    echo "USAGE ERROR"
    usage
    exit
esac


<example> note the use of pattern
case "$TERM" in
   *term)
   TERM=xterm ;;
   network | dialup | unknown | vt[0-9][0-9][0-9])
   TERM=vt100 ;;
esac


<example> can get an input from a user
read case;
case $case in
    1) echo "You selected bash";;
    2) echo "You selected perl";;
    3) echo "You selected phyton";;
    4) echo "You selected c++";;
    5) exit
esac


={============================================================================
*kt_linux_bash_011* bash: process options using while and getopts

<form>
while command
do
   list
done

<example>
note: can use true only in the script but not command line.

while [ true ] ; do ls -l; echo Wait 1 sec ; sleep 1; done

while [ "$1" != "" ]; do
  case $1 in
    -h | -help | --help)
      usage
      exit
      ;;
    -p | --platform | -b | --box)
      shift
      PLATFORM=$1
      ;;
    --project)
      shift
      PROJECT=$1
      ;;
    release)
      RELEASE="true"
      ;;
    *)
      echo "USAGE ERROR"
      usage
      exit
      ;;
  esac
  shift
done

<example>
while [ $# -gt 0 ]; do
    case "$1" in
        (-h | -help | --help)
            grep '^#/' "$0" | cut -c 4-
            exit 0;;

        (-d | --debug)
            debug="1"
            shift;;

        (-l | --list)
            list_assets
            exit 0;;

        (*)
            break;;
    esac
done


{getopts} shell built-in

getopts optstring name [args]

getopts is used by shell scripts to parse positional parameters. 'optstring' contains the option
characters to be recognized; if a character is followed by a colon, the option is expected to have
an argument, which should be separated from it by whitespace. 

<option>
Each time it is invoked, getopts 'places' the next option in the shell variable 'name', initializing
name if it does not exist, and the 'index' of the next argument to be processed into the variable
'OPTIND'. OPTIND is initialized to 1 each time the shell or a shell script is invoked. 

<arg>
When an option requires an argument, getopts places that 'argument' into the variable OPTARG. 

<return>
When the end of options is encountered, getopts exits with a return value greater than zero. note:
false. OPTIND is set to the index of the first 'non'-option argument, and name is set to '?'.


={============================================================================
*kt_linux_bash_012* bash: break and continue


while :     # this cause infinite loop.
do
   read CMD
   case $CMD in
      [qQ]|[qQ][uU][iI][tT]) break ;;
   *) process $CMD ;;
   esac
done

<continue>
If one of the filenames in $FILES is not a file, this loop skips it, rather than exiting.

for FILE in $FILES ;
do
   if [ ! -f "$FILE" ] ; then
      echo "ERROR: $FILE is not a file."
      continue
   fi
   # process the file
done


={============================================================================
*kt_linux_bash_013* bash: read

The read command in bash is a magic builtin. It reads a line from stdin and assigns its contents to
shell variables. It also has a return code when EOF is reached, allowing a clean exit from a loop. 

A common use of input redirection in conjunction with the read command is the reading of a file one
line at a time using the while loop.

<example> really works!
#!/bin/bash
cat list | while read f; do
  echo "line $f"
done


<example> get token from a input line
Read three tokens and echo the second.

$ grep 'sda' DF.LOG | grep 'FAT'
/dev/sda1              63   976751999   488375968   1 FAT12
$ echo "/dev/sda1              63   976751999   488375968   1 FAT12" | while read a b c; do echo $b; done
63

In the script:

A=`grep 'sda' $F | grep 'FAT' | while read a b c;do echo $b;done`


<example>
# what it's trying to do is that search through the same errors in logs which was uploaded in
# Feburary and was under translation. For example,
# translation/Zone9-Box5_Feb_18_09_38_42b3e5c9130e1d758acdc71bb9d12b2a

ls translation/ \
| grep Feb | while read x; do pushd -n translation/$x; pwd; egrep --color -ano 'btreePageFromDbPage' .detailed_output; popd -n; done 

note: WHY?

pushd -n   

Suppresses the normal change of directory when adding directories to the stack, so that only the
stack is manipulated. 


{read-and-sub-process}

<example> This cause the creation of sub process?
while read LINE
do
   ... # manipulate file here
done < file


<example>
#!/bin/sh
if [ -f "$1" ] ; then
  i=0
  while read LINE
  do
    i=`echo "$i + 1" | bc`
    echo "inner $i..."
  done < "$1"
  echo $i
fi

echo $i

$ ./sbash.sh list 
inner 1...
inner 2...
inner 3...
inner 4...
4
4

note: in the original text, it got 0 than 4 since there is a sub-process created. However, when run
it on PC linux, got 4 as shown. Is it difference between PC and embedded shell? Seems not since the
below example seems to be run on build server.


<example>
scan_for()
{
  while read line
  do
      echo "$line"
  done
}

A=`mycat $1 | egrep -n "(VRM_JOB_START:|XTVFS_WriteEx)" | scan_for`


This example cannot get output inside a function. So make a changes to:

scan_for()
{
  cat x.tmp | while read line
  do
      echo "$line"
  done
}

A=`mycat $1 | egrep -n "(VRM_JOB_START:|XTVFS_WriteEx)" > x.tmp`
scan_for()

This enables us to use echo to get output but causes the other problem.

scan_for()
{
  STARTED=0
  MATCHED=0

  # export STARTED MATCHED LASTLINE

  cat x.tmp | while read line
  do
    case $line in
      *jtVRM=3*)
          if [ $STARTED -ne 1 ]
          then
             STARTED=1;
             STARTLINE=`echo $line`;
          fi
          ;;
       *XTVFS_WriteEx*E_FSSERVER_STATUS_INVALID_ALIGNMENT*)
          if [ $STARTED -eq 1 ]
          then
             MATCHED=1;
             LASTLINE=`echo $line`;

             # note: can get output of variables as expected
             echo "started = $STARTED, MATCHED=$MATCHED";

          break;
          fi
          ;;
    esac
  done

  # note: however, the output of variables are '0'
  echo "started = $STARTLINE";
  echo "matched = $LASTLINE";

  if [[ $STARTED -eq 1 && $MATCHED -eq 1 ]]
  then
      echo "$LASTLINE";    # note: here
  fi
}

[parkkt@ukstbuild02 SI-4063]$ ./PicassoShutdownFail job.nds
started = 1, MATCHED=1
started = 0
matched = 0
started = 0
matched = 0

The reason is that "That's a tricky one, because the "while read line do done" spawns a subshell."
So use "echo variable" inside the loop to make it available after the loop. But still not able to
see the output inside the loop.

A=`mycat $1 | egrep -n "(VRM_JOB_START:|XTVFS_WriteEx)" | scan_for`
echo "A = $A";

So A will have all lines which are ecohed from the subshell 


[parkkt@ukstbuild02 SI-4063]$ ./PicassoShutdownFail job.nds
--
A = started = 1, MATCHED=1

started = 2:1801:NDS: ^[08:14:11]0946714451.604390 !MIL -VRMS < p:000000b7 t:2b933100 T:VRM_SRV
M:job_man_srv.c F:JobManSrvStart L:03884 > VRM_JOB_START: jtVRM=3, jhVRM=0x3000385

matched = 290:243541:NDS: ^[09:11:44]0946717904.242499 !FATAL -MSM_MS < p:000000b7 t:0144ce00
T:FS_WT_THREAD1 M:fs_server.c F:PerformWrite L:01943 > XTVFS_WriteEx failed,returned value:
E_FSSERVER_STATUS_INVALID_ALIGNMENT 290:243541:NDS: ^[09:11:44]0946717904.242499 !FATAL -MSM_MS <
p:000000b7 t:0144ce00 T:FS_WT_THREAD1 M:fs_server.c F:PerformWrite L:01943 > XTVFS_WriteEx
failed,returned value: E_FSSERVER_STATUS_INVALID_ALIGNMENT

started = 1, MATCHED=1 started = 2 Time:[08:14:11]0946714451,  VRM_JOB_START: jtVRM=3 Picasso not
shutting down properly - maybe Jira SI-3770

[parkkt@ukstbuild02 SI-4063]$


={============================================================================
*kt_linux_bash_014* bash: function

note: must use bash and see how to pass args to function

#!/bin/bash
# BASH FUNCTIONS CAN BE DECLARED IN ANY ORDER

function function_B {
  echo "{ fb"
  echo Function B.
  echo "} fb"
}

function function_A {
  echo "{ fa"
  echo $1
  echo "} fa"
}

function function_D {
  echo "{ fd"
  echo Function D.
  echo "} fd"
}

# function function_C {
function_C () {
  echo "{ fc"
  echo $1
  echo "} fc"
}

# FUNCTION CALLS

# Pass parameter to function A
function_A "Function A via pass"
function_B

# Pass parameter to function C
function_C "Function C via pass"
function_D 

$ ./sbash.sh list 
{ fa
Function A via pass
} fa
{ fb
Function B.
} fb
{ fc
Function C via pass
} fc
{ fd
Function D.
} fd


{syntax}
function function_C() {
  echo "{ fc"
  echo $1
  echo "} fc"
}

function function_C {
  echo "{ fc"
  echo $1
  echo "} fc"
}

function_C () {
  echo "{ fc"
  echo $1
  echo "} fc"
}

note: this causes an error as:

function_C {
  echo "{ fc"
  echo $1
  echo "} fc"
}

./sample.sh: line 23: function_C: command not found
{ fc

} fc
./sample.sh: line 27: syntax error near unexpected token `}'
./sample.sh: line 27: `}'


{use-return-value}
get_available_space() {
    declare -a fs_info=( $(stat -f -c "%d %s" "$core_output_path") )
    echo $(( $(IFS="*"; echo "${fs_info[*]}") ))
}

# break the loop if the available space is above the threshold
[ "$(get_available_space)" -lt "$threshold" ] || break


function get_lsr_int_with_default() {
    key_name="$1"
    value="$(lsr-config "$key_name")"

    if [ -z "$value" ]; then
        value="$2"
    fi

    printf "$value"
}


={============================================================================
*kt_linux_bash_015* bash: basename

$ dirname /home/kt/kb
/home/kt
$ basename /home/kt/kb
kb

# DESCRIPTION
# Print NAME with any leading directory components removed. If 'specified', also remove a trailing
# SUFFIX.

$ basename cfgversion.out  
cfgversion.out
$ basename cfgversion.out .out
cfgversion


={============================================================================
*kt_linux_bash_016* bash: here document

The general form for a here document is

command << delimiter
document
delimiter

Here the shell interprets the << operator as an instruction to read 'input' until it finds a line
containing the specified delimiter. All the input lines up to the line containing the delimiter are
then fed into the standard 'input' of the command.

The delimiter must be a single word that does not contain spaces or tabs. For example, to print a
quick list of URLs, you could use the following here document:

lpr << MYURLS
http://www.csua.berkeley.edu/~ranga/
http://www.cisco.com/
http://www.marathon.org/story/
http://www.gnu.org/
MYURLS

For example, you can use the following command to create a file with the short list of URLs given
previously:

cat > urls << MYURLS
http://www.csua.berkeley.edu/~ranga/
http://www.cisco.com/
http://www.marathon.org/story/
http://www.gnu.org/
MYURLS


={============================================================================
*kt_linux_bash_017* bash-redirection

<form>
command n> file
command n>> file

<example>
One of the most common uses of file descriptors is to redirect STDOUT and STDERR
to separate files.  The basic syntax is

command 1> file1 2> file2

Often the STDOUT file descriptor, 1, is not written, so a shorter form of the
basic syntax is

command > file1 2> file2

Redirecting STDOUT and STDERR to the 'same' file

list > file 2>&1

n>&m. Here n and m are file descriptors (integers). If you let n=2 and m=1, you
see that STDERR is redirected to STDOUT. By redirecting STDOUT to a file, you
also redirect STDERR.

<short>
list &> file

Bash allows for both standard output and standard error to be redirected to the
file. This is the equivalent of > FILE 2>&1. note: cannot use tee in this case.

<ex> the all are the same
prog1 > /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log 2>&1
prog1 2>&1 | tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log

<use-tee>
prog1 |& tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log


<>
$ (ZB_CFG=humax.2100 zb-make-g listprojects) > 2100.log 2>&1


={============================================================================
*kt_linux_bash_018* bash: ':' command

<example> can use as 'no-op'
Can be used as a no-op, which is a command that does nothing and thus can be safely inserted
anywhere a command is needed for purely syntactical reasons:

if [ -x $CMD ]
then :
else
   echo Error: $CMD is not executable >&2
fi

In this example, assume you are not quite ready to write the code to follow the then statement. The
shell flags a syntax error if you leave that code out completely, so you insert the : command as a
temporary noop command that can be replaced by the desired code later.


<example>
Because the : always returns a successful result, it is sometimes used to create an infinite loop:

while :
do
  echo "Enter some input: \c"
  read INPUT
  [ "$INPUT" = stop ] && break
Done

Because the : always returns a successful or true result, the while loop will continue forever or
until a break is executed within the loop. Sometimes you might find that "while true" used in place
of while : but using the : is more efficient because it is a shell built-in command, whereas true is
a command that must be read from a disk file, if you are in the Bourne shell.


={============================================================================
*kt_linux_bash_019* bash: subshell and multiple commands

3.7.3 Command Execution Environment

The shell has an execution environment, which consists of the following:

open files inherited by the shell at invocation, as modified by redirections supplied to the exec builtin

the current working directory as set by cd, pushd, or popd, or inherited by the shell at invocation
the file creation mode mask as set by umask or inherited from the shell's parent

current traps set by trap

shell parameters that are set by variable assignment or with set or inherited from the shell's
parent in the environment

shell functions defined during execution or inherited from the shell's parent in the environment
options enabled at invocation (either by default or with command-line arguments) or by set options
enabled by shopt (see The Shopt Builtin)

shell aliases defined with alias (see Aliases)

various process IDs, including those of background jobs (see Lists), the value of $$, and the value of $PPID 

<separate>
When a simple command other than a builtin or shell function is to be executed, it is invoked in a
'separate' execution environment that consists of the following. Unless otherwise noted, the values
are inherited from the shell.

- the shell's open files, plus any modifications and additions specified by redirections to the command

- the current working directory
- the file creation mode mask

- note: shell variables and functions marked for export, along with variables exported for the
command, passed in the environment (see Environment)

A command invoked in this separate environment cannot affect the shell's execution environment.

Command substitution, commands grouped with parentheses, and asynchronous commands are invoked in a
subshell environment that is a duplicate of the shell environment, except that traps caught by the
shell are reset to the values that the shell inherited from its parent at invocation. 

Builtin commands that are invoked as part of a pipeline are also executed in a 'subshell' environment.
Changes made to the subshell environment cannot affect the shell's execution environment.

Subshells spawned to execute command substitutions inherit the value of the -e option from the
parent shell. When not in POSIX mode, Bash clears the -e option in such subshells.


3.2.2 Pipelines

A pipeline is a sequence of simple commands separated by one of the control operators | or |&.

The format for a pipeline is

[time [-p]] [!] command1 [ | or |& command2 ] ...

The output of each command in the pipeline is connected via a pipe to the input of the next command.
That is, each command reads the previous command's output. This connection is performed before any
redirections specified by the command.

If |& is used, command1's standard error, in addition to its standard output, is connected to
command2's standard input through the pipe; it is shorthand for 2>&1 |. This implicit redirection of
the standard error to the standard output is performed after any redirections specified by the
command.

If the pipeline is 'not' executed asynchronously (see Lists), the shell 'waits' for all commands in
the pipeline to complete.

'each' command in a pipeline is executed in its own subshell (see Command Execution Environment). The
exit status of a pipeline is the exit status of the last command in the pipeline, unless the
pipefail option is enabled (see The Set Builtin). 


3.2.3 Lists of Commands

A list is a sequence of one or more pipelines separated by one of the operators ;, &, &&, or ||, and
optionally terminated by one of ;, &, or a newline. 

Commands separated by a ; are executed 'sequentially'; the shell waits for each command to terminate
in turn. The return status is the exit status of the last command executed. 

If a command is terminated by the control operator &, the shell executes the command asynchronously
in a subshell. This is known as executing the command in the background.


3.2.4.3 Grouping Commands

Bash provides two ways to group a list of commands to be executed as a unit. When commands are
grouped, redirections may be applied to the entire command list. For example, the output of all the
commands in the list may be redirected to a single stream.

()

( list )

Placing a list of commands between parentheses causes a 'subshell' environment to be created (see
    Command Execution Environment), and each of the commands in list to be executed 'in' that
subshell. Since the list is executed in a subshell, variable assignments do not remain in effect
after the subshell completes.

<example>
$ global strlen                 # strlen() is not found
$ (cd /usr/src/lib; gtags)      # library source
$ (cd /usr/src/sys; gtags)      # kernel source


{}

{ list; }

Placing a list of commands between curly braces causes the list to be executed in the 'current'
shell context. No subshell is created. The semicolon (or newline) following list is required. 

In addition to the creation of a subshell, there is a subtle difference between these two constructs
due to historical reasons. The braces are reserved words, so they must be separated from the list by
blanks or other shell metacharacters. The parentheses are operators, and are recognized as separate
tokens by the shell even if they are not separated from the list by whitespace.

The exit status of both of these constructs is the exit status of list. 

note: must be spaces between {}.

$ { date;time; }
$ { date;time; } > mylog


={============================================================================
*kt_linux_bash_020* bash: list options

keitee@debian-keitee:~/github/kb$ set -o
allexport      	off
braceexpand    	on
emacs          	on
errexit        	off
errtrace       	off
functrace      	off
hashall        	on
histexpand     	on
history        	on
ignoreeof      	off
interactive-comments	on
keyword        	off
monitor        	on
noclobber      	off
noexec         	off
noglob         	off
nolog          	off
notify         	off
nounset        	off
onecmd         	off
physical       	off
pipefail       	off
posix          	off
privileged     	off
verbose        	off
vi             	off
xtrace         	off

{shopt}

shopt [-pqsu] [-o] [optname ...]

Toggle  the values of variables controlling optional shell behavior.  With no options, or with the
-p option, a 'list' of all settable options is  displayed, with  an indication of whether or not
each is set.  The -p option causes output to be displayed in a form that may be reused  as  input.
Other  options have the following meanings:

-s     Enable (set) each optname.


<direxpand>
shopt -s direxpand

direxpand

If set, bash replaces directory names with the results of word expansion when performing filename
completion.  This changes the  contents of  the  readline  editing buffer.  If not set, bash
attempts to preserve what the user typed.


$ shopt
autocd         	off
cdable_vars    	off
cdspell        	off
checkhash      	off
checkjobs      	off
checkwinsize   	on
cmdhist        	on
compat31       	off
compat32       	off
compat40       	off
compat41       	off
direxpand      	on
dirspell       	off
dotglob        	off
execfail       	off
expand_aliases 	on
extdebug       	off
extglob        	on
extquote       	on
failglob       	off
force_fignore  	on
globstar       	off
gnu_errfmt     	off
histappend     	on
histreedit     	off
histverify     	off
hostcomplete   	off
huponexit      	off
interactive_comments	on
lastpipe       	off
lithist        	off
login_shell    	off
mailwarn       	off
no_empty_cmd_completion	off
nocaseglob     	off
nocasematch    	off
nullglob       	off
progcomp       	on
promptvars     	on
restricted_shell	off
shift_verbose  	off
sourcepath     	on
xpg_echo       	off


={============================================================================
*kt_linux_bash_021* sh-exit: test on exit status

The $? expands to the exit status of the 'most' recently executed foreground pipeline. Can be used
in test construct.

The following example demonstrates that TEST COMMANDS might be 'any' UNIX command that returns an
exit status, and that if again returns an exit status of zero:

<example>
note: The grep return 0 when found and 1 when not found. The test in shell see that 0 is true and 1
is false.

kpark@wll1p04345:~$ if grep KB readme; then echo "found"; fi
readme:1:this is KB for keitee.
found

kpark@wll1p04345:~$ if ! grep KB readme; then echo "found"; fi
readme:1:this is KB for keitee.
kpark@wll1p04345:~$ 

kpark@wll1p04345:~$ if true; then echo "found"; fi
found

<example>
while kill -0 "${parent_pid}" &>/dev/null;
do
   usleep 500
done

<example>

#include <stdio.h>
#include <stdlib.h>

int main(int argc, char** argv)
{
  printf(" this is a main function.\n");

  if( 2 == argc )
    exit(EXIT_FAILURE);

  exit(EXIT_SUCCESS);
}

$ ./a.out && echo "return success"
 this is a main function.
return success
$ 

$ ./a.out xx && echo "return success"
 this is a main function.


<ex>
Stash away the output of program and only use its exit status to see if it has
found a element in the system.

# gst-inspect-1.0 nexussink &>/dev/null && echo "true" || echo "false"
true
# gst-inspect-1.0 xx &>/dev/null && echo "true" || echo "false"
false
#   


={============================================================================
*kt_linux_bash_022* bash: && and ||

if [ "$(whoami)" != 'root' ]; then
   echo "You have no permission to run $0 as non-root user."
   exit 1;
fi

With Bash, you can shorten this type of construct. The compact equivalent of the above test is as
follows:

[ "$(whoami)" != 'root' ] && ( echo you are using a non-privileged account; exit 1 )

Similar to the "&&" expression which indicates what to do if the test proves true, "||" specifies
what to do if the test is false.

<and-and-or>
3.2.3 Lists of Commands

An AND list has the form

command1 && command2

command2 is executed if, and only if, command1 returns an exit status of
zero(true).

An OR list has the form

command1 || command2

command2 is executed if, and only if, command1 returns a non-zero(false) exit status.

The return status of AND and OR lists is the exit status of the last command executed in the list. 

<example>
1. if loginscript is set(defined), run -f. 
   if file exist then test return true so no fail() call.
   if file not exist then test return false so run fail() call.
2. if loginscript is not set, no run -f and test return true(0) so no fail() call.

[[ -z "$loginscript" || -f "$loginscript" ]] ||
    fail "Loginscript '$loginscript' doesn't exist."

note: progress to next cond when only previous one is true. [[ cond || cond ]] || exec

<example>
To see if meet at least one of options.

    [[ -n "$release" || -n "$zincdir" || -n "$jenkinshost" || -n "$jenkinsurl" ||
       -n "$galliumurl" || -n "$localgallium" || -n "$platformconfigurl" ||
       "$virtualinputdriver" -ne 0 || -n "$irdriver" || "$enabledbustcp" -ne 0 ||
       "$directfbsrc" -ne 0 || "$killzincdaemons" -ne 0 || -n "$toolnames" ||
       -n "$networkmanager" || -n "$loginscript" || -n "$startupscript" ||
       "$forcefirsttime" -ne 0 || "$needsreboot" -ne 0 || "$vidmemcapture" -ne 0 ]] ||
    {
        fail "You must specify at least one of -[ZzjugcdeEtsknlbTfrv]."
    }

<example>

function x {
  ...

  [[ 
    "$cacheOptionCorrectAtExpectedPosition" = 1 &&
    "$cacheValueCorrectAtExpectedPosition" = 1 &&
    "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
    "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
    "$jarOptionCorrectAtExpectedPosition" = "1" && 
    "$jarValueCorrectAtExpectedPosition" = "1" && 
    "$urlOptionCorrectAtExpectedPosition" = "1" && 
    "$urlValueCorrectAtExpectedPosition" = "1" 
  ]] 
}

From the debug output, shows only 4 since stops as soon as see flase. This may be confusing when
debugging.

+ [[ 1 = 1 ]]
+ [[ 1 = 1 ]]
+ [[ 1 = 1 ]]
+ [[ 0 = 1 ]]


<example>
The lsr-config returns "0/1" or "true/false" when use --bool and the problem
is when a key is boolean and is not set in a database meaning 0, it still
returns "0" which is not null string. So "if [ -z ]" check always becomes
false.

The solution is that use variable's value depending on key value to set actual
variable.

#!/bin/sh

# when key is not set or returns "false" then enable_yv_media="".
# when key set and returns "true" then enable_yv_media="true".

enable_yv_media=""
[ "false" = "$(lsr-config --bool platform.settings.enable-yv-media || echo false)" ] || {
    enable_yv_media="true"
}

USE_YV_MEDIA_ROUTER="$enable_yv_media"

if [ -z "$USE_YV_MEDIA_ROUTER" ]; then
	echo "NOT SET. $USE_YV_MEDIA_ROUTER. WILL USE OEM MR"
else
	echo "SET. $USE_YV_MEDIA_ROUTER. WILL USE YV MR"
fi


={============================================================================
*kt_linux_bash_023* bash: debug

-x    " to start up the subshell in debug mode

#!/bin/bash -ex


={============================================================================
*kt_linux_bash_024* bash: builtin: exec

exec

exec [-cl] [-a name] [command [arguments]]

Two uses:

1. If command is supplied, it replaces the shell 'without' creating a new process. If command cannot
be executed for some reason, a non-interactive shell exits, unless the execfail shell option is
enabled. In that case, it returns failure. An interactive shell returns failure if the file cannot
be executed. 

Replace the shell with a given program (executing it, not as new process)

<example>
As you can see, the subshell is replaced by echo.

user@host:~$ PS1="supershell$ "
supershell$ bash
user@host:~$ PS1="subshell$ "
subshell$ exec echo hello
hello
supershell$ 

2. If no command is specified, redirections may be used to affect the current shell environment. If
there are no redirection errors, the return status is zero; otherwise the return status is non-zero.

Set redirections for the program to execute or for the current shell. If only redirections are
given, the redirections affect the current shell without executing any program. 


={============================================================================
*kt_linux_bash_025* bash: builtin: set

4.3.1 The Set Builtin

This builtin is so complicated that it deserves its own section. set allows you to change the values
of shell options and set the positional parameters, or to display the names and values of shell
variables.

set

    set [--abefhkmnptuvxBCEHPT] [-o option-name] [argument ...]
    set [+abefhkmnptuvxBCEHPT] [+o option-name] [argument ...]

If no options or arguments are supplied, set displays the names and values of all shell variables
and functions, sorted according to the current locale, in a format that may be reused as input for
setting or resetting the currently-set variables. Read-only variables cannot be reset. In POSIX
mode, only shell variables are listed.

When options are supplied, they set or unset shell attributes. Options, if specified, have the
following meanings: 

-- 

If no arguments follow this option, then the positional parameters are unset. Otherwise, the
positional parameters are set to the arguments, even if some of them begin with a -.


={============================================================================
*kt_linux_bash_025* bash: builtin: trap

4.1 Bourne Shell Builtins

trap

    trap [-lp] [arg] [sigspec …]

    The commands in arg are to be read and executed when the shell receives signal sigspec. If arg is absent (and there is a single sigspec) or equal to ‘-’, each specified signal’s disposition is reset to the value it had when the shell was started. If arg is the null string, then the signal specified by each sigspec is ignored by the shell and commands it invokes. If arg is not present and -p has been supplied, the shell displays the trap commands associated with each sigspec. If no arguments are supplied, or only -p is given, trap prints the list of commands associated with each signal number in a form that may be reused as shell input. The -l option causes the shell to print a list of signal names and their corresponding numbers. Each sigspec is either a signal name or a signal number. Signal names are case insensitive and the SIG prefix is optional.

    If a sigspec is 0 or EXIT, arg is executed when the shell exits. If a sigspec is DEBUG, the command arg is executed before every simple command, for command, case command, select command, every arithmetic for command, and before the first command executes in a shell function. Refer to the description of the extdebug option to the shopt builtin (see The Shopt Builtin) for details of its effect on the DEBUG trap. If a sigspec is RETURN, the command arg is executed each time a shell function or a script executed with the . or source builtins finishes executing.

    If a sigspec is ERR, the command arg is executed whenever a pipeline (which may consist of a single simple command), a list, or a compound command returns a non-zero exit status, subject to the following conditions. The ERR trap is not executed if the failed command is part of the command list immediately following an until or while keyword, part of the test following the if or elif reserved words, part of a command executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command’s return status is being inverted using !. These are the same conditions obeyed by the errexit (-e) option.

    Signals ignored upon entry to the shell cannot be trapped or reset. Trapped signals that are not being ignored are reset to their original values in a subshell or subshell environment when one is created.

    The return status is zero unless a sigspec does not specify a valid signal.

={============================================================================
*kt_linux_bash_025* bash: builtin: shift

shift [n]

Shift the positional parameters to the left by n. The positional parameters from n+1 … $# are
renamed to $1 … $#-n. Parameters represented by the numbers $# to $#-n+1 are unset. n must be a
non-negative number less than or equal to $#. If n is zero or greater than $#, the positional
parameters are not changed. If n is not supplied, it is assumed to be 1. The return status is zero
unless n is greater than $# or less than zero, non-zero otherwise.


See that use S1 all the way and $# decreases

#!/bin/bash
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"


$ ./sbash.sh 1 2 3 4 5 6 
this is 1 1, num 6
this is 1 2, num 5
this is 1 3, num 4
this is 1 4, num 3
this is 1 5, num 2

So can use 'loop' until there is no more args to process:

while [ "$1" != "" ]; do
   case $1 in
      -h | -help | --help)
         echo "help.."
         exit
         ;;
      release)
         echo "release..."
         exit
         ;;
      *)
         echo "else..."
         exit
         ;;
   esac
   shift
done

={============================================================================
*kt_linux_bash_026* bash: which one when there are script and exe binary in the same dir?

The executable binary gets executed.


={============================================================================
*kt_linux_bash_027* bash: test on file using which command

if which ssh-copy-id >/dev/null;
then
    ssh-copy-id -i $privatekey root@$stbip || true  # ssh-copy-id always returns failure
fi


={============================================================================
*kt_linux_bash_028* bash: builtin: programmable completion

https://www.gnu.org/software/bash/manual/html_node/Programmable-Completion-Builtins.html

Three builtin commands are available to manipulate the programmable completion facilities: one to
specify how the arguments to a particular command are to be completed, and two to modify the
completion as it is happening.

compgen

compgen [option] [word]

Generate possible completion matches for word according to the options, which may be any option
accepted by the complete builtin with the exception of -p and -r, and write the matches to the
standard output. 

The matches will be generated in the same way as if the programmable completion code had generated
them directly from a completion specification with the same flags. If word is specified, only those
completions matching word will be displayed.

The return value is true unless an invalid option is supplied, or no matches were generated.

note: compgen is the same as complete command but limited. use to 'generate' matches.

complete

complete [-abcdefgjksuv] [-o comp-option] [-DE] [-A action] [-G globpat] [-W wordlist]
    [-F function] [-C command] [-X filterpat]
    [-P prefix] [-S suffix] name [name ¿]
    complete -pr [-DE] [name ...

-A action
The action may be one of the following to generate a list of possible completions: 

function    Names of shell functions.

<example>
#!/bin/bash
# BASH FUNCTIONS CAN BE DECLARED IN ANY ORDER

function func_B {
  echo "{ fb"
  echo Function B.
  echo "} fb"
}

function func_A {
  echo "{ fa"
  echo $1
  echo "} fa"
}

function func_D {
  echo "{ fd"
  echo Function D.
  echo "} fd"
}

# function function_C {
function_C () {
  echo "{ fc"
  echo $1
  echo "} fc"
}

## foreach test
for test_fun in $(compgen -A function func_); do

  echo -e -n "\n$test_fun ... "

done


func_D ... kpark@wll1p04345:$ ./sample.sh 

func_A ... 
func_B ... 
func_D ... kpark@wll1p04345:$ 


={============================================================================
*kt_linux_bash_029* bash: variable arithmetic

The both makes the same result.

COUNT=10

while [ $COUNT -gt 0 ]; do
   echo Value of count is: $COUNT
   let COUNT=COUNT-1
done

COUNT=10

while [ $COUNT -gt 0 ]; do
   echo Value of count is: $COUNT
   COUNT=$(( $COUNT-1 ))
done


={============================================================================
*kt_linux_bash_030* bash: interactive shell or not


={============================================================================
*kt_linux_bash_031* bash: double dash

For example, from the below command line, what is "--"?

xxx.sh -o humax.1000 -- -r

The "--" signify the end of 'options'.

4.3.1 The Set Builtin

set [--abefhkmnptuvxBCEHPT] [-o option-name] [argument …]

-- 

If no arguments follow this option, then the positional parameters are unset. Otherwise, the
positional parameters are set to the arguments, even if some of them begin with a ‘-’.

$ set -- 1 2 3 4 5 6 7 8 9 0 a b c d e f g h
$ echo ${@:7}
7 8 9 0 a b c d e f g h
$ echo ${@:7:0}

$ echo ${@:7:2}
7 8

The double dash "--" means "end of command line flags" i.e. it tells ssh command not to try to parse
what comes after command line options. You will see something as follows when you use gcutil ssh
vmNameHere python wrapper. It will display and execute ssh as follows:

ssh -o UserKnownHostsFile=/dev/null -o CheckHostIP=no -o StrictHostKeyChecking=no -i
/Users/vivek/.ssh/google_compute_engine -A -p 22 nixcraft@server1.cyberciti.biz --

This syntax ensures that you can run commands on the remote server without ssh parsing them:

ssh nixcraft@server1.cyberciti.biz -- command1 --arg1 --arg2

The above syntax tell ssh not try to parse --arg1 and --arg2 after -- command line options. This
ensures that command1 will accept --arg1 and --arg2 as arguments.

## safe examples ##
ssh nixcraft@server1.cyberciti.biz -- --commandName --arg1 --arg2

This kind of behavior is mostly defined and handled by the ssh command and not by your bash/ksh/csh
shell. This is also true for many other commands. For example you can not create or view a file
named --file or -f using cat command

## fail ##
cat --file
cat -f

Instead try passing double dash "--" to instruct cat command not to try to parse what comes after
command line options:

## works ##
cat -- --file
cat -- -f

<example>
Can be used to pass args to a sub script:

script-one -o h.1000 -- -r

script-one:
   ...
	script-two "$@" "$host"

So the "-r" is passed to script-two as arg which is called from script-one.


={============================================================================
*kt_linux_bash_032* bash: command line edit

http://web.mit.edu/gnu/doc/html/features_7.html

C-a      : jump to the beginning of the line.
C-e      : jump to the end of the line.
Esc-b    : jump one word backward.
Esc-f    : jump one word forward. 

C-w            : cut text backward until space.
Esc-Backspace  : cut one word backward.
Esc-Delete     : cut one word forward.


Readline Killing Commands

C-k      : cut from current position until the end of the line.
C-y      : paste the most recently cut text. 

<case> use cut and paste

git init --bare /path/to/repo.git
git remote add origin /path/to/repo.git

Notice that the second command uses the same path at the end. Instead of typing that path twice, you
could copy and paste it from the first command, using this sequence of keystrokes:

Press the up arrow to bring back the previous command.

Press Ctrl-w to cut the path part: "/path/to/repo.git".

Press Ctrl-c to cancel the current command.

Type git remote add origin, and press Ctrl-y to paste the path. 

If you want to learn more, see the bash man page and search for "READLINE", "Commands for Moving"
and "Commands for Changing Text".


={============================================================================
*kt_linux_bash_033* bash: true and false

echo "---------------------"

if [ -n "${ZXXX:-}" ]; then
    echo "ths is non zero"
else
    echo "ths is zero"
fi

$ ZXXX=1 ./sample.sh 
ths is non zero

$ ZXXX=true ./sample.sh 
ths is non zero

$ ZXXX=2 ./sample.sh 
ths is non zero

$ ZXXX=0 ./sample.sh 
ths is non zero

$ ZXXX=false ./sample.sh 
ths is non zero

$ ZXXX= ./sample.sh 
ths is zero


={============================================================================
*kt_linux_bash_034* bash: wait

How to wait in a bash script for several subprocesses spawned from that script to finish?

$ help wait
wait: wait [id]
    Wait for job completion and return exit status.
    
    Waits for the process identified by ID, which may be a process ID or a
    job specification, and reports its termination status.  
    
    If ID is not given, waits for 'all' currently active child processes, and the return
    status is zero.  
    
    If ID is a a job specification, waits for all processes in the job's pipeline.
    
    Exit Status:
    Returns the status of ID; fails if ID is invalid or an invalid option is
    given.


={============================================================================
*kt_linux_bash_035* bash: help

$ help wait
wait: wait [id]
    Wait for job completion and return exit status.
    
    Waits for the process identified by ID, which may be a process ID or a
    job specification, and reports its termination status.  If ID is not
    given, waits for all currently active child processes, and the return
    status is zero.  If ID is a a job specification, waits for all processes
    in the job's pipeline.
    
    Exit Status:
    Returns the status of ID; fails if ID is invalid or an invalid option is
    given.


={============================================================================
*kt_linux_bash_036* bash: shell and environment variable

{problem}
When works on some issue, have to set some environment variables before running some commands which
are affected by these settings. This is repetative and thought it's better to have a script to set
these.

#!/bin/sh
export LD_LIBRARY_PATH="/opt/zinc-trunk/lib:${LD_LIBRARY_PATH}"
export LD_PRELOAD=/usr/local/lib/libdirectfb.so:/lib/libpthread.so.0

Great. Now runs commands like this:

$ ./run-settings.sh
$ commands..

However, start to see that the expected shared library are not loaded and the picture is complicated
since is doing investigation with gdb. So suspects gdb settings or configuration for not running as
expected. 

After all, the reason is that by setting env vars in the shell script and run commands outside of
script then command don't see these settings done in the script. Use env vars with default settings.


={============================================================================
*kt_linux_bash_037* bash: builtin: command

command

command [-pVv] command [arguments …]

Runs command with arguments 'ignoring' any shell function named command. Only shell builtin commands
or commands found by 'searching' the PATH are executed. If there is a shell function named ls,
running 'command ls' within the function will execute the external command ls instead of calling the
    function recursively. The -p option means to use a default value for PATH that is guaranteed to
    find all of the standard utilities. The return status in this case is 127 if command cannot be
    found or an error occurred, and the exit status of command otherwise.

If either the -V or -v option is supplied, a description of command is printed. The -v option causes
a single word indicating the command or file name used to invoke command to be displayed; the -V
option produces a more verbose description. In this case, the return status is zero if command is
found, and non-zero if not.

<code>
if [ -x "$(command -v nexus-inspect)" ]; then
    echo "Running on a Nexus enabled system"

note: -v returns a full path

++ command -v nexus-inspect
+ '[' -x /opt/zinc/bin/nexus-inspect ']'
+ echo 'Running on a Nexus enabled system'
Running on a Nexus enabled system


={============================================================================
*kt_linux_bash_038* bash: builtin: prepare

declare

declare [-aAfFgilnrtux] [-p] [name[=value] ...]

'declare' variables and give them attributes. If no names are given, then display the values of
variables instead.

    The -p option will display the attributes and values of each name. When -p is used with name arguments, additional options, other than -f and -F, are ignored.

    When -p is supplied without name arguments, declare will display the attributes and values of all variables having the attributes specified by the additional options. If no other options are supplied with -p, declare will display the attributes and values of all shell variables. The -f option will restrict the display to shell functions.

    The -F option inhibits the display of function definitions; only the function name and attributes are printed. If the extdebug shell option is enabled using shopt (see The Shopt Builtin), the source file name and line number where the function is defined are displayed as well. -F implies -f.

    The -g option forces variables to be created or modified at the global scope, even when declare is executed in a shell function. It is ignored in all other cases.

The following options can be used to restrict output to variables with the specified attributes or
to give variables attributes: 

-a
Each name is an indexed array variable (see Arrays).

-A
Each name is an associative array variable (see Arrays).

-r
Make names readonly. These names cannot then be assigned values by subsequent assignment statements
or unset.

={============================================================================
*kt_linux_bash_038* bash-substitude

<ex>
The both has the same result.

-LDFLAGS=`echo $LDFLAGS | sed -e 's/\s-Wl,--as-needed//g'`

+LDFLAGS="${LDFLAGS/[[:space:]]-Wl,--as-needed}"


={============================================================================
*kt_linux_bash_039* bash: builtin: export

export

export [-fn] [-p] [name[=value]]

Mark each name to be passed to child processes in the environment. If the -f option is supplied, the
names refer to shell functions; otherwise the names refer to shell variables. The -n option means to
no longer mark each name for export. 

If 'no' names are supplied, or if the -p option is given, a list of names of all exported variables
is displayed. The -p option displays output in a form that may be reused as input. If a variable
name is followed by =value, the value of the variable is set to value.

The return status is zero unless an invalid option is supplied, one of the names is not a valid
shell variable name, or -f is supplied with a name that is not a shell function.


={============================================================================
*kt_linux_bash_100* bash: config: colour prompt

Uncomment the line that says force_color_prompt=yes then logout and back in in the .bashrc.


={============================================================================
*kt_linux_bash_101* bash: config: how to know shell nesting level

SHLVL  Incremented by one each time an instance of bash is started.
$ echo $SHLVL


={============================================================================
*kt_linux_bash_102* bash: config: prompt

keitee@debian-keitee:~/github/kb$ echo $PS1
\[\e]0;\u@\h: \w\a\]${debian_chroot:+($debian_chroot)}\u@\h:\w\$

PS1="\[\e]0;\u@\h: \w\a\]${debian_chroot:+($debian_chroot)}\u@\h:\w\:$SHLVL$"

keitee@debian-keitee:~/github/kb\:2$


={============================================================================
*kt_linux_bash_200* bash: code: debug and printf

debug() {
    # Note: echo -e doesn't work on OS X's default bash (3.2).
    printf '\n\033[0;32m%s\n' "$*"
    tput sgr0
}


={============================================================================
*kt_linux_bash_201* bash: code: array from 0 and return 1 for success like c

Use array from index 0 and return 1 for success. Unlike a shell convention which use 0 for success.

// return 1 (okay) when found a string in a array at the expected position. otherwise, return 0 (fail).

function paramsContainStringAtPosition () {
    local stringToSearchFor="$1"
    local positionToExpectStringAt="$2"
    local stringToSearch="${@:3}"
    local stringToSearchAsArray=($stringToSearch)
    local currentParamPos=0;
    local currentSearchString
        
    for currentSearchString in "${stringToSearchAsArray[@]}"
        do
            if [ "$currentSearchString" == "$stringToSearchFor" ]; then
                if [ "$positionToExpectStringAt" == "$currentParamPos" ]; then
                    return 1;
                fi
            fi
            currentParamPos=$(( $currentParamPos+1 ))
        done
    return 0
}

function x {

   ... 
   paramsContainStringAtPosition "$expected_url" 7 "$@"
   urlValueCorrectAtExpectedPosition=$?

   // note: here convert c style return to shell return by comararing "return from function = 1"
   
   [[
     "$cacheOptionCorrectAtExpectedPosition" = 1 &&
     "$cacheValueCorrectAtExpectedPosition" = 1 &&
     "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
     "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
     "$jarOptionCorrectAtExpectedPosition" = "1" && 
     "$jarValueCorrectAtExpectedPosition" = "1" && 
     "$urlOptionCorrectAtExpectedPosition" = "1" && 
     "$urlValueCorrectAtExpectedPosition" = "1" 
   ]] 
}


={============================================================================
*kt_linux_bash_202* bash: code: recursion test

The same script file is used to define tests and routines to check result using exec trick and
communication between parent process that runs the the script and the subshell that runs each test
case.

test.sh

test_xxx () {
  set local vars

  run.sh args...        
  " run.sh to exec the subprocess with the given cmd so the subprocess will run the cmd.

}

test_xxx_handler() {

}

if [ -z "$test_handler" ]; then
   for each test case

   " create a subprocess
   " run a test case that runs the other script and exec call eventually.
   (
    set env vars to pass to subprocess.

    " note: use env var to pass data to a subprocess
    " note: use a single line to run

    cmd = test.sh \                       " set cmd as the same script so 
    test_handler="${test_xxx}_handler" \
    test_xxx
   )
   done

else

  xxx_handler     " that checks the results

fi

So the parent process runs the test.sh and drives the whole test. For each test, set env vars and
create a subprocess that exec and run the given cmd. By having cmd as the same test.sh and the
test_handler env var set, subprocess runs the same script and env var set. So run the different path
and run check call in this case. So looks like a recursion or callback.

<code>

test_that_generates_url() {

    local app_dir="$scratch_dir/app"
    createApp "$app_dir"

    runBrowser.sh --app "$app_dir" --data "$default_app_data_dir" \
        --app-launch-parameters \
        "launch_context.ui.youview.com" "portal" \
        "some.test.param" "some.test.param.value" \
        "test.param.spaces" "param value with spaces"
}


that_generates_url_handler() {

    echo "that_generates_url_handler() - Got args: '$@'"

    local expected_url="http://youview.tv/test-player?\
launch_context.ui.youview.com=portal&some.test.param=some.test.param.value&\
test.param.spaces=param%20value%20with%20spaces"

    paramsContainStringAtPosition "-cache" 0 "$@"
    cacheOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "/tmp/client-cache" 1 "$@"
    cacheValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-cache-size" 2 "$@"
    cacheSizeOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "0" 3 "$@"
    cacheSizeValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-jar" 4 "$@"
    jarOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "$default_cookie_jar_location" 5 "$@"
    jarValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-url" 6 "$@"
    urlOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "$expected_url" 7 "$@"
    urlValueCorrectAtExpectedPosition=$?

    [[ 
      "$cacheOptionCorrectAtExpectedPosition" = 1 &&
      "$cacheValueCorrectAtExpectedPosition" = 1 &&
      "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
      "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
      "$jarOptionCorrectAtExpectedPosition" = "1" && 
      "$jarValueCorrectAtExpectedPosition" = "1" && 
      "$urlOptionCorrectAtExpectedPosition" = "1" && 
      "$urlValueCorrectAtExpectedPosition" = "1" 
    ]] 
}


# In test configuration runBrowser.sh will not actually launch the browser but
# will instead call back into this test script recursively.  We then check the
# environment in the test to see if the browser would have been launched with
# the appropriate environment and parameters.  This is done with pairs of
# functions, one called test_xxx and the other xxx_handler.  test_x functions
# are tests and the xxx_handler functions are what will be run when we
# runBrowser.sh calls us back.
#
# The recursion and the callback are communicated through the environment
# variable $test_handler.
if [ -z "$test_handler" ]; then

        failures=0
        successes=0

        ## foreach test
        for test_fun in $(compgen -A function test_); do

            echo -e -n "\n$test_fun ... "

            setup
            (

                RUN_BROWSER_CMD="$this_script" \
                RUN_BROWSER_DATA_DIR="${datadir:-}" \
                GST_INSPECT_CMD="touch $scratch_dir/gst-inspect" \
                test_handler="${test_fun#test_*}_handler" \
                $test_fun

            ) >> $logfile 2>&1

            status=$?
            if [ $status -eq 0 ]; then
                echo "OK"
                ((++successes))
            else
                echo "FAILURE"
                ((++failures))
            fi

            [[ "$1" == '-v' || $status -ne 0 ]] && { cat $logfile; echo; }
            teardown &> /dev/null

        done

        printf "$invoked_as complete. %i PASSES %i FAILURES\n" \
            $successes $failures

        exit $failures

else

        $test_handler "$@"

fi


={============================================================================
*kt_linux_bash_202* bash: code: get filenames from comment in the script

#!/bin/bash -x

# Exit immediately if any unexpected error occurs
set -e

#/ DASH: unencrypted, pseudo-live
#/ http://dash.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd
#/
#/ DASH: encrypted, non-live
#/ https://ms3.co.uk/s/Big+Buck+Bunny+DASH+2#http://bbb/avc3/1/2drm_manifest.mpd
#/
#/ HLS: unencrypted, non-live
#/ http://184.72.239.149/vod/smil:bigbuckbunnyiphone.smil/playlist.m3u8
#/
declare -ar urls=($(grep '^#/ http' "$0" | cut -c 4-))

echo "urls 0 : ${urls[0]}"


={============================================================================
*kt_linux_bash_300* bash: faq: var#

The both shows the same result. What is this "${test_func#test*}" for?

test_handler="${test_fun#test*}_handler"
test_handler_two="${test_fun}_handler"
echo ">> $test_handler, $test_handler_two"


# ============================================================================
#{
={============================================================================
*kt_linux_tool_001* md5sum

md5sum - compute and check MD5 message digest

Another way let say you have more files to verify, you can create a text file, such as md5sum.txt

283158c7da8c0ada74502794fa8745eb  ubuntu-6.10-alternate-amd64.iso
549ef19097b10ac9237c08f6dc6084c6  ubuntu-6.10-alternate-i386.iso
5717dd795bfd74edc2e9e81d37394349  ubuntu-6.10-alternate-powerpc.iso
99c3a849f6e9a0d143f057433c7f4d84  ubuntu-6.10-desktop-amd64.iso
b950a4d7cf3151e5f213843e2ad77fe3  ubuntu-6.10-desktop-i386.iso
a3494ff33a3e5db83669df5268850a01  ubuntu-6.10-desktop-powerpc.iso
2f44a48a9f5b4f1dff36b63fc2115f40  ubuntu-6.10-server-amd64.iso
cd6c09ff8f9c72a19d0c3dced4b31b3a  ubuntu-6.10-server-i386.iso
6f165f915c356264ecf56232c2abb7b5  ubuntu-6.10-server-powerpc.iso
4971edddbfc667e0effbc0f6b4f7e7e0  ubuntu-6.10-server-sparc.iso

First column is the md5 string and second column is the location of the file. To check all them from
file, do this:

md5sum -c md5sum.txt


={============================================================================
*kt_linux_tool_002* tool-cut

       -b, --bytes=LIST
              select only these bytes

       -c, --characters=LIST
              select only these characters

       -d, --delimiter=DELIM
              use DELIM instead of TAB for field delimiter

       -f, --fields=LIST
              select only these fields;  also print any line that contains no
              delimiter character, unless the -s option is specified

       Use one, and only one of -b, -c or -f.  Each LIST is made up of one
       range, or many ranges separated by commas.  Selected input is written in
       the same order that it is read, and is written exactly once.  Each range
       is one of:

       N      N'th byte, character or field, counted from 1

       N-     from N'th byte, character or field, to end of line

       N-M    from N'th to M'th (included) byte, character or field

       -M     from first to M'th (included) byte, character or field

# input file
WIFI_SSID="SKY0F227"

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d=`
# this lead to WIFI_SSID='"SKY0F227"' and cannot use it as a var

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d \"`
# this fixes the problem.

<ex>
~/source/DEVARCH$ git symbolic-ref HEAD
refs/heads/topic
123456789012
~/source/DEVARCH$ git symbolic-ref HEAD | cut -b 12-
topic


={============================================================================
*kt_linux_tool_003* tool-kill, tool-killall

{kill-0}

while kill -0 "${parent_pid}" &>/dev/null;
do
   usleep 500
done

From man 2 kill

If sig is 0, then no signal is sent, but error checking is still performed; this
can be used to check for the existence of a process ID or process group ID.

<example>
#!/bin/bash
kill -0 323232 && echo "pid 323232 is present.."
kill -0 4355 && echo "pid 4355 x sess mgr is present.."

$ ./sbsh.sh 
./sbsh.sh: line 3: kill: (323232) - No such process
pid 4355 x sess mgr is present..


{kill-l}
       -l, --list [signal]
              List  signal  names.   This  option has optional argument, which
              will convert signal number to signal name, or other way round.


// from embedded

# kill -l
HUP INT QUIT ILL TRAP ABRT EMT FPE KILL BUS SEGV SYS PIPE ALRM TERM USR1 USR2
CHLD PWR WINCH URG IO STOP TSTP CONT TTIN TTOU VTALRM PROF XCPU XFSZ

// from pc

$ kill -l
 1) SIGHUP	 2) SIGINT	 3) SIGQUIT	 4) SIGILL	 5) SIGTRAP
 6) SIGABRT	 7) SIGBUS	 8) SIGFPE	 9) SIGKILL	10) SIGUSR1
11) SIGSEGV	12) SIGUSR2	13) SIGPIPE	14) SIGALRM	15) SIGTERM
16) SIGSTKFLT	17) SIGCHLD	18) SIGCONT	19) SIGSTOP	20) SIGTSTP
21) SIGTTIN	22) SIGTTOU	23) SIGURG	24) SIGXCPU	25) SIGXFSZ
26) SIGVTALRM	27) SIGPROF	28) SIGWINCH	29) SIGIO	30) SIGPWR
31) SIGSYS	34) SIGRTMIN	35) SIGRTMIN+1	36) SIGRTMIN+2	37) SIGRTMIN+3
38) SIGRTMIN+4	39) SIGRTMIN+5	40) SIGRTMIN+6	41) SIGRTMIN+7	42) SIGRTMIN+8
43) SIGRTMIN+9	44) SIGRTMIN+10	45) SIGRTMIN+11	46) SIGRTMIN+12	47) SIGRTMIN+13
48) SIGRTMIN+14	49) SIGRTMIN+15	50) SIGRTMAX-14	51) SIGRTMAX-13	52) SIGRTMAX-12
53) SIGRTMAX-11	54) SIGRTMAX-10	55) SIGRTMAX-9	56) SIGRTMAX-8	57) SIGRTMAX-7
58) SIGRTMAX-6	59) SIGRTMAX-5	60) SIGRTMAX-4	61) SIGRTMAX-3	62) SIGRTMAX-2
63) SIGRTMAX-1	64) SIGRTMAX	


{default-signal}
The signal name difference cause compatibility between machines. For example,
    the below do not work on embedded.

kill -SIGINT dbussenddaemon &>/dev/null

The default signal for kill is TERM. So uses default signal to kill remaining
processes

-INT only interrupted a process rather request terminating it.

-    kill -INT %?dbussenddaemon &>/dev/null
+    kill %?dbussenddaemon &>/dev/null


{killall}
killall - kill processes by name

killall sends a signal to all processes running any of the specified commands.
If no signal name is specified, SIGTERM is sent.


={============================================================================
*kt_linux_tool_004* dmesg

To show the boot log and can see the kernel version.


={============================================================================
*kt_linux_tool_005* tool-check-version

# -all
uname -a 

keitee@debian-keitee:~/github/kb$ uname -a
Linux debian-keitee 3.2.0-4-486 #1 Debian 3.2.63-2+deb7u2 i686 GNU/Linux

{ubuntu}
$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=12.04
DISTRIB_CODENAME=precise
DISTRIB_DESCRIPTION="Ubuntu 12.04.1 LTS"

{debian}
$ cat /etc/os-release 
PRETTY_NAME="Debian GNU/Linux 7 (wheezy)"
NAME="Debian GNU/Linux"
VERSION_ID="7"
VERSION="7 (wheezy)"
ID=debian
ANSI_COLOR="1;31"
HOME_URL="http://www.debian.org/"
SUPPORT_URL="http://www.debian.org/support/"
BUG_REPORT_URL="http://bugs.debian.org/"


={============================================================================
*kt_linux_tool_006* cp

{copy-symbolic}
To copy symbolic links as well, cp -r don't work and use

-L, --dereference
always follow symbolic links

{careful}
cp âr /xx/dir/ /dst/ 

This cause that files copied under /dst/dir/. If want to copy only files under dir then use

cp âr /xx/dir/* /dst/


={============================================================================
*kt_linux_tool_007* mkdir

{p-option}
Use to make parent directories as well.


={============================================================================
*kt_linux_tool_008* strings

To find string in the library.

keitee@linux:~/share/temp> strings sec_getba.a | grep GCC
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6


={============================================================================
*kt_linux_tool_009* sort

See the unix power tool 22.6 for more. -u remove duplicates and sort against field [4,7].

sort -u -k 4,7

http://www.gnu.org/software/coreutils/manual/html_node/sort-invocation.html

--key=pos1[,pos2]

Specify a sort field that consists of the part of the line between pos1 and pos2 (or the end of the
line, if pos2 is omitted), inclusive.  Each pos has the form 'f[.c][opts]', where f is the number of
the field to use, and c is the number of the first character from the beginning of the field. Fields
and character positions are numbered starting with 1; a character position of zero in pos2 indicates
the field's last character. If '.c' is omitted from pos1, it defaults to 1 (the beginning of the
field); if omitted from pos2, it defaults to 0 (the end of the field). opts are ordering options,
allowing individual keys to be sorted according to different rules; see below for details. Keys can
span multiple fields.  Example: To sort on the second field, use --key=2,2 (-k 2,2). See below for
more notes on keys and more examples. See also the --debug option to help determine the part of the
line being used in the sort.

To sort
NDS: ^0946684946.752246 !ERROR -aem          < M:aem_list.c F:AEM_ListGetApplication L:01808 > Can't find
-1-- ----------2------- ---3-- -4--         -5 ------6----- ---------------7-------- ---8--- 9 -10--

sort -u -k 4,10 ndsfusion.test > ndsfusion.dic 


-t separator --field-separator=separator

Use character separator as the field separator when finding the sort keys in each line. By default,
fields are separated by the empty string between a non-blank character and a blank character. By
default a blank is a space or a tab, but the LC_CTYPE locale can change this.

That is, given the input line ' foo bar', sort breaks it into fields ' foo' and ' bar'. The field
separator is not considered to be part of either the field preceding or the field following, so with
'sort -t " "' the same input line has three fields: an empty field, 'foo', and 'bar'. However,
fields that extend to the end of the line, as -k 2, or fields consisting of a range, as -k 2,3,
retain the field separators present between the endpoints of the range.  To specify ASCII NUL as the
field separator, use the two-character string '\0', e.g., 'sort -t '\0''. 


mh5a_variable.c
mh5b_variable.c

sort -t _ -k 2,2

Use this to list out member functions from sources

egrep -r 'g_pAppWindow->' . | sort -t - -k 2,3 >> in.txt

<example>
$ df -h | sort -rnk 5


={============================================================================
*kt_linux_tool_010* tool-grep

       grep [OPTIONS] PATTERN [FILE...]
       grep [OPTIONS] [-e PATTERN | -f FILE] [FILE...]

       -s, --no-messages
              Suppress error messages about nonexistent or unreadable files.

       -o, --only-matching
              Print only the matched (non-empty) parts of a matching line, with
              each such part on a separate output line.

       -n, --line-number
              Prefix each line of output with the 1-based line number within its
              input file.  (-n is specified by POSIX.)

       -R, -r, --recursive
              Read  all  files  under  each  directory, recursively; this is
              equivalent to the -d recurse option.

       -l, --files-with-matches
              Suppress normal output; instead  print  the name of each input
              file from which output would normally have been printed. The
              scanning will stop on the first  match.

To specify the target files:

grep -nr --include *.c __setup ./
grep -nr --include *.c [a-z]*_initcall ./


{when-see-binary-errors-on-text-file}
Even if that is a text file, grep say it is binary file then use -a option.

(http://www.gnu.org/software/grep/manual/html_node/Usage.html)

Why does grep report Binary file matches? If grep listed all matching lines from
a binary file, it would probably generate output that is not useful, and it
might even muck up your display. So gnu grep suppresses output from files that
appear to be binary files. To force gnu grep to output lines even from files
that appear to be binary, use the '-a' or '--binary-files=text' option. To
eliminate the Binary file matches messages, use the '-I' or
'--binary-files=without-match' option. 

To exclude binaries:

-I     
Process a binary file as if it did not contain matching data; this is equivalent to the
--binary-files=without-match option.


{q-option}
Used in the script.

-q, --quiet, --silent

Quiet; do not write anything to standard output. Exit immediately with zero
status if any match is found, even if an error was detected. Also see the -s or
--no-messages option. 

echo xx | grep -q '.gz$' && echo ture
echo xx.gz | grep -q '.gz$' && echo ture
ture

The script example:

##:zcat if this is a .gz, cat otherwise
F=cat
echo "$1" | grep -q '\.gz$' && F=zcat
$F "$1"


<line-control>

-A NUM, --after-context=NUM
Print NUM lines of trailing context after matching lines.  Places a line
containing a group separator  (--)  between  contiguous  groups  of  matches.
With  the  -o  or --only-matching option, this has no effect and a warning is
given.

-B NUM, --before-context=NUM
Print  NUM  lines  of  leading  context  before  matching  lines.   Places  a
line containing a group separator (--) between contiguous groups of matches.
With the -o or --only-matching option, this has no effect and a warning is
given.

-C NUM, -NUM, --context=NUM
Print NUM lines of output context.  Places a line containing a group separator
(--) between contiguous groups of matches.  With the -o or --only-matching
option, this  has no effect and a warning is given.

note: -C works like before and after


{case-example}
# what it's trying to do is that search through the same errors in logs which
# was uploaded in Feburary and was under translation. For example,
# translation/Zone9-Box5_Feb_18_09_38_42b3e5c9130e1d758acdc71bb9d12b2a

ls translation/ | grep Feb | while read x; do pushd -n translation/$x; pwd;\
     egrep --color -ano 'btreePageFromDbPage' .detailed_output; popd -n; done 

$ egrep --color -an 'btreePageFromDbPage' translation/*_Feb_*/.detailed_output

translation/PaulRiley_Work_Feb_18_09_42_25d1235e4ebe020a369a64d728c2ce9f/
  .detailed_output:8:parser_result=<p1>MW_Process
crash in #0 btreePageFromDbPage (pDbPage=0x2b994bcc, pgno=67, pBt=0x2bb06da4) (built by hudson)</p1>
$ 

$ egrep --color -ano 'event manager POLL time exceeded threshold' \
    translation/*_Feb_*/LOGlastrun_realtime

translation/000000002704_Feb_1_14_05_3a6f10bd9e8934088df33e9d991faa74/
  LOGlastrun_realtime:2100:event manager POLL time exceeded threshold


<busybox>
grep

    grep [-HhrilLnqvsoweFEABCz] PATTERN [FILE]...

    Search for PATTERN in each FILE or standard input

    Options:

            -H      Prefix output lines with filename where match was found
            -h      Suppress the prefixing filename on output
            -r      Recurse
            -i      Ignore case distinctions
            -l      List names of files that match
            -L      List names of files that do not match
            -n      Print line number with output lines
            -q      Quiet. Return 0 if PATTERN is found, 1 otherwise
            -v      Select non-matching lines
            -s      Suppress file open/read error messages
            -c      Only print count of matching lines
            -o      Show only the part of a line that matches PATTERN
            -m MAX  Match up to MAX times per file
            -w      Match whole words only
            -F      PATTERN is a set of newline-separated strings
            -E      PATTERN is an extended regular expression
            -e PTRN Pattern to match
            -f FILE Read pattern from file
            -A      Print NUM lines of trailing context
            -B      Print NUM lines of leading context
            -C      Print NUM lines of output context
            -z      Input is NUL terminated


={============================================================================
*kt_linux_tool_011* find

{find-mtime}
24 hours based so -mtime 3 means that 72 hours from the current time and -mtime -3 means 72 before.
To specify between 72 and 96, -mtime 3 and after 96, -mtime +3. If want to find file or dirs changed
most recently, use -mtime 0 or 1.


{find-dir}
find . -type d -name [dir]


{find-to-get-the-number-of-files}
find -type f | wc -l

find . -maxdepth 1 -type l


{find-print0}
-print0

print the full file name on the standard output, followed by a null character (instead of the new-
line character that -print uses). This allows file names that contain newlines or other types of
white space to be correctly interpreted by programs that process the find output. This option
corresponds to the -0 option of xargs.

$ find . -name CMakeLists.txt -print0
./CMakeLists.txt./Source/CMakeLists.txt./Source/cmake/gtest/CMakeLists.txt./Source/WebKit2/CMakeLists.txt./Source/WebKit2/UIProcess/efl/po_tizen/CMakeLists.txt./Source/WebKit/CMakeLists.txt./Source/WebKit/efl/DefaultTheme/CMakeLists.txt./Source/JavaScriptCore/CMakeLists.txt./Source/JavaScriptCore/shell/CMakeLists.txt./Source/WebCore/CMakeLists.txt./Source/ThirdParty/gtest/CMakeLists.txt./Source/WTF/CMakeLists.txt./Source/WTF/wtf/CMakeLists.txt./Tools/WinCELauncher/CMakeLists.txt./Tools/tizen-webview-test/CMakeLists.txt./Tools/MiniBrowser/efl/CMakeLists.txt./Tools/CMakeLists.txt./Tools/TestWebKitAPI/CMakeLists.txt./Tools/EWebLauncher/CMakeLists.txt./Tools/EWebLauncher/ControlTheme/CMakeLists.txt./Tools/DumpRenderTree/TestNetscapePlugIn/CMakeLists.txt./Tools/DumpRenderTree/efl/CMakeLists.txt./Tools/WebKitTestRunner/CMakeLists.txtkeitee.park@rockford /home/tbernard/Git/vdTizen/webkit

{Q} when useful? To use the out as a single line?
Used for xargs


{find-exec}
{} where the filename will be inserted. Add \; at the end of the command to complete the required
syntax. note: there must be a space after {}

$ find . -name CMakeLists.txt -exec egrep PROJECT {} \;

To run dirtags script for each directory:

$ find * -type d -exec dirtags {} \;


{find-oring}
You can specify a logical "or" condition using -o:

find / \( -size +50 -o -mtime -3 \) -print
find /my/project/dir -name '*.c' -o -name '*.h'
find -name *.[ch]

This is from bash  expr: expr1 -or expr2 and this means expr1 -o expr2, but not POSIX compliant.


{find-ignore}
-path pattern

File name matches shell pattern pattern.  The metacharacters do not treat `/' or `.' specially; so,
for example,

find . -path "./sr*sc"

will print an entry for a directory called `./src/misc' (if one exists).  To ignore a  whole
directory tree,  use  -prune  rather  than  checking  every file in the tree.  For example, to skip
the directory `src/emacs' and all files and directories under it, and print the names of the other
files  found,  do something like this:

find . -path ./src/emacs -prune -o -print

Note  that the pattern match test applies to the whole file name, starting from one of the start
points named on the command line.  It would only make sense to use an absolute path name here if the
relevant start point is also an absolute path.  This means that this command will never match
anything:

find bar -path /foo/bar/myfile -print

The  predicate  -path is also supported by HP-UX find and will be in a forthcoming version of the
POSIX standard.


{find-sym}
-L     
Follow symbolic links.


={============================================================================
*kt_linux_tool_012* make a empty file without touch

can use 'touch' but when busybox do not support touch, can use following to make a empty
file or to reset a file.

cat /dev/null > file


={============================================================================
*kt_linux_tool_013* tool-xargs

--null
-0     

Input items are terminated by a null character instead of by whitespace, and the quotes and
backslash are not special (every character is taken literally). Disables the end of file string,
which is treated like any other argument. Useful when input items might contain white space, quote
  marks, or  backslashes. The GNU find -print0 option produces input suitable for this mode.
note: see find-print0

-I replace-str

Replace occurrences of replace-str in the initial-arguments with names read from standard input.
Also, unquoted  blanks  do not terminate input items; instead the separator is the newline
character. Implies -x and -L 1.

$ ls | grep Nov_ | xargs -I{} find {} -name LOGlastrun_realtime -print | xargs egrep -an \ 
"(NCM_ADDRESSING_TYPE_DHCP address still in status list)"


<xargs-vs-find>
The xargs command builds and executes command lines from standard input. This has the advantage that
the command line is filled until the system limit is reached. Only then will the command to execute
be called, in the above example this would be rm. If there are more arguments, a new command line
will be used, until that one is full or until there are no more arguments. The same thing using find
-exec calls on the command to execute on the found files every time a file is found. Thus, using
xargs greatly speeds up your scripts and the performance of your machine.


<without-xarg>

$ ls -al $(find . -name "*.log")
$ tar -cjf daemon-logs.tar.bz2 $(find /opt/zinc*/var/daemons/ -name "*.log")


<busybox>

xargs

    xargs [OPTIONS] [PROG [ARGS]]

    Run PROG on every item given by standard input

    Options:

            -p      Ask user whether to run each command
            -r      Do not run command if input is empty  note:
            -0      Input is separated by NUL characters
            -t      Print the command on stderr before execution
            -e[STR] STR stops input processing
            -n N    Pass no more than N args to PROG
            -s N    Pass command line of no more than N bytes
            -x      Exit if size is exceeded


={============================================================================
*kt_linux_tool_014* ssh and putty

{aim}
To switch hosts using key base instead of using password.

{ssh-keygen}
$ ssh-keygen <enter>

$ ls -al .ssh/
total 20
drwx------  2 parkkt ccusers 4096 Dec  9 13:24 .
drwxr-xr-x 15 parkkt ccusers 4096 Dec  9 12:47 ..
-rw-------  1 parkkt ccusers 1675 Dec  9 13:24 id_rsa
-rw-r--r--  1 parkkt ccusers  411 Dec  9 13:24 id_rsa.pub

The file we need to copy to the server is named id_dsa.pub. As you can see above, the file needed
exists. You may or may not have other files in ~/.ssh as I do. If the key doesn't exist, however,
you can make one as follows:

$cp id_rsa.pub authorized_keys
$scp -p ~/.ssh/authorized_keys ukstbuild3:.ssh/
(user@homebox ~ $ scp ~/.ssh/id_dsa.pub user@'servername':.ssh/authorized_keys)
(     -p      Preserves modification times, access times, and modes from the
             original file.)

This make one-way ssh connection which means the machine you are on is added the authorized_keys of
the server so can run scp from the machine to the server:

scp remote-server:{path}/filename .
scp filename remote-server:{path}/filename

note: if there is working ssh connection, can use tab key to get file completion when use scp.
note: ssh-copy is not working and key line in authorized_keys must be one line.
note: if copy host's pub key in the target's authorized_keys, then do need to enter password when
ssh to the target from host.

<to-passwordless-login-to-box>
Therefore, if copy rsa.pub to authorized_keys on a box then make one direction open and no need to
use password from host to box.

<ssh-copy-id>
NAME
       ssh-copy-id - install your public key in a remote machine's authorized_keys

SYNOPSIS
       ssh-copy-id [-i [identity_file]] [user@]machine

DESCRIPTION
       ssh-copy-id is a script that uses ssh to log into a remote machine and append the indicated
       identity file to that machine's ~/.ssh/authorized_keys file.

       If the -i option is given then the identity file (defaults to ~/.ssh/id_rsa.pub) is used,
       regardless of whether there are any keys in your ssh-agent.  Otherwise, if this:

             ssh-add -L

       provides any output, it uses that in preference to the identity file.

       If the -i option is used, or the ssh-add produced no output, then it uses the contents of the
identity file.  Once it has one or more fingerprints (by whatever means) it uses ssh to append them
to ~/.ssh/authorized_keys on the remote machine (creating the file, and directory, if necessary.)


{to-check-match-pair}
$ ssh-keygen /?
  -y          Read private key file and print public key.

$ ssh-keygen -y -f id_rsa


{caution}
note:
If the file permissions are too open then ssh will not trust them, and will still prompt
you for your password. 

chmod 700 ~/.ssh
chmod 644 ~/.ssh/authorized_keys    # must check this as caused big grief when different
chmod 644 ~/.ssh/id_dsa_pub
chmod 644 ~/.ssh/known_hosts
chmod 600 ~/.ssh/id_dsa


{config}
When user name is different between servers, must have an entry in this file for servers to connect.

$ cat ~/.ssh/config
Host tizen
        Hostname 168.219.241.167
        IdentityFile ~/.ssh/id_rsa
        User keitee.park                 # this is username that can be different from real user.
        Port 29418

To debug ssh. note: shall use name on the command line
-v  # -vv
Verbose mode. Causes ssh to print debugging messages about its progress. This is helpful in
debugging connection, authentication, and configuration problems. Multiple -v options increase
the verbosity. The maximum is 3. 

$ ssh -vT tizen


{github-when-ssh-do-not-work}
Using SSH over the HTTPS port

Sometimes the administrator of a firewall will refuse to allow SSH connections entirely. If using
HTTPS cloning with credential caching is not an option, you can attempt to clone using an SSH
connection made over the HTTPS port. Most firewall rules should allow this, but proxy servers may
interfere. 

Testing

To test if SSH over the HTTPS port is possible, run this ssh command:

ssh -T -p 443 git@ssh.github.com
# Hi username! You've successfully authenticated, but GitHub does not provide shell access.

Make it so

If you are able to ssh to git@ssh.github.com over port 443, you can override your ssh settings to
force any connection to github.com to run though that server and port. To set this in your ssh
config, edit the file at ~/.ssh/config and add this section:

Host github.com
  Hostname ssh.github.com
  Port 443

You can test that this works by connecting to github.com:

ssh -T git@github.com
# Hi username! You've successfully authenticated, but GitHub does not provide shell access.


{putty-ssh-setup}
When use keys generated from putty.

o run puttygen to make key pairs. rsa or dsa
o get a pub key and save a pri key(ppk)
o run putty and set ssh key to use
  menu: connection: ssh: auth: private key file for auth: specify the path to a pri key.
o login to the host and add a pub key in the auth key list


{convert-rsa-key-to-putty-ppk}
To convert keys from linux machine to putty ppk and from ppk to lunux(opsnssh
    keys)

o run puttygen and menu: conversion: import key:
o save it as a pri key(ppk)


{run-command}
$ ssh root@172.20.33.192 ls -al


={============================================================================
*kt_linux_tool_015* tool-pgrep, tool-pidof

pidof -- find the process ID of a running program.

Pidof finds the process id's (pids) of the named  programs. It prints those id's
on the standard output. 

$ pidof getty
2974 2973 2972 2971 2970 2969


{pgrep}
       pgrep, pkill - look up or signal processes based on name and other
       attributes

SYNOPSIS
       pgrep [options] pattern
       pkill [options] pattern

DESCRIPTION
       pgrep looks through the currently running processes and lists the process
       IDs which match the selection criteria to stdout.  All the criteria have
       to match.  For example,

              $ pgrep -u root sshd

       will only list the processes called sshd AND owned by root.  On the other
       hand,

              $ pgrep -u root,daemon

       will list the processes owned by root OR daemon.

       pkill will send the specified signal (by default SIGTERM) to each process
       instead of listing them on stdout.

$ pgrep -h    
pgrep: invalid option -- h
BusyBox v1.21.0 (2015-06-02 11:02:10 BST) multi-call binary.

Usage: pgrep [-flnovx] [-s SID|-P PPID|PATTERN]

Display process(es) selected by regex PATTERN

	-l	Show command name too
	-f	Match against entire command line
	-n	Show the newest process only
	-o	Show the oldest process only
	-v	Negate the match
	-x	Match whole name (not substring)
	-s	Match session ID (0 for current)
	-P	Match parent process ID


$ pgrep linearsourced
1079

$ pgrep -l linearsourced
1079 /opt/zinc-trunk/bin/linearsourced


={============================================================================
*kt_linux_tool_016* tool-screen

Screen is a full-screen software program that can be used to multiplexes a
physical console between several processes (typically interactive shells). It
offers a user to open several separate terminal instances inside a one single
terminal window manager.

sudo screen -S name -a -D -R -fn -l /dev/ttyUSB0 115200,cs8

<help>
       C-a ?       (help)        Show key bindings.

<close>
to close screen use Ctrl-A, k, y. Do not use Ctrl-C as it can kill processes
running on the box.

       C-a k
       C-a C-k     (kill)        Destroy current window.


{logging}
Use -L for auto logging and will log on 'screenlog.0'. Use this file to see:

-L   tells screen to turn on automatic output logging for the windows.

screen -a -D -R -fn -l -L /dev/ttyUSB0 115200,cs8


       C-a h       (hardcopy)    Write a hardcopy of the current window to the
       file "hardcopy.n".

       C-a H       (log)         Begins/ends logging of the current window to
       the file "screenlog.n".

<monitoring>
       C-a M       (monitor)     Toggles monitoring of the current window.


={============================================================================
*kt_linux_tool_017* tool-ldd

To see library dependancy of a application. This is a script file and shows a
dynamic library dependancy. If not, shows 'not a dynamic executable' message.

The simplest approach is to pick a binary that you consider is typical (e.g.
    /bin/ls and run ldd on it. One of the listed libraries should be libc -
    check its version number.

$ ldd /bin/ls
        libc.so.6 => /lib/libc.so.6 (0x4000e000)
        /lib/ld-linux.so.2 => /lib/ld-linux.so.2 (0x40000000)


{case}
The ldd is script. Although the binary uses shared library, ldd don't understand it.

$ readelf -d nexus-inspect | grep NEED
(standard input):4: 0x00000001 (NEEDED)  Shared library: [libnexus.so]
(standard input):5: 0x00000001 (NEEDED)  Shared library: [libgcc_s.so.1]
(standard input):6: 0x00000001 (NEEDED)  Shared library: [libpthread.so.0]
(standard input):7: 0x00000001 (NEEDED)  Shared library: [libc.so.0]

$ ldd -v nexus-inspect
	not a dynamic executable

note: 
The ldd on a target is an executable but not a script and also shows details
when used on shared library. However, the host version only works on executable.


{shows-if-can-load-dependency}
This is result of target ldd version.

root# ldd /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so
checking sub-depends for 'not found' ~
checking sub-depends for '/usr/local/lib/libnexus.so'
checking sub-depends for '/opt/zinc/oss/lib/libgstbase-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgstmpegts-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgstreamer-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgobject-2.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libglib-2.0.so.0'
checking sub-depends for '/lib/libgcc_s.so.1'
checking sub-depends for '/lib/libpthread.so.0'
checking sub-depends for '/lib/libc.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgmodule-2.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libintl.so.8'
checking sub-depends for '/lib/libdl.so.0'
checking sub-depends for '/lib/libm.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libffi.so.5'
  libnexusMgr.so.0 => not found (0x00000000) ~
  libnexus.so => /usr/local/lib/libnexus.so (0x00000000)
  libgstbase-1.0.so.0 => /opt/zinc/oss/lib/libgstbase-1.0.so.0 (0x00000000)
  libgstmpegts-1.0.so.0 => /opt/zinc/oss/lib/libgstmpegts-1.0.so.0 (0x00000000)
  libgstreamer-1.0.so.0 => /opt/zinc/oss/lib/libgstreamer-1.0.so.0 (0x00000000)
  libgobject-2.0.so.0 => /opt/zinc/oss/lib/libgobject-2.0.so.0 (0x00000000)
  libglib-2.0.so.0 => /opt/zinc/oss/lib/libglib-2.0.so.0 (0x00000000)
  libgcc_s.so.1 => /lib/libgcc_s.so.1 (0x00000000)
  libpthread.so.0 => /lib/libpthread.so.0 (0x00000000)
  libc.so.0 => /lib/libc.so.0 (0x00000000)
  libgmodule-2.0.so.0 => /opt/zinc/oss/lib/libgmodule-2.0.so.0 (0x00000000)
  libintl.so.8 => /opt/zinc/oss/lib/libintl.so.8 (0x00000000)
  libdl.so.0 => /lib/libdl.so.0 (0x00000000)
  libm.so.0 => /lib/libm.so.0 (0x00000000)
  libffi.so.5 => /opt/zinc/oss/lib/libffi.so.5 (0x00000000)
  not a dynamic executable


={============================================================================
*kt_linux_tool_018* ls

{get-filename-only}
ls -1

{get-color}
For bash, copy /etc/DIR_COLORS into home as .dir_colors and edit it to change default values. Run
man dir_colors for help.


={============================================================================
*kt_linux_tool_019* strace

strace - trace system calls and signals 

-f
Trace child processes as they are created by currently traced processes as a result of the fork(2)
system call. 

-o filename
Write the trace output to the file filename rather than to stderr. Use filename.pid if -ff is used.
If the argument begins with '|' or with '!' then the rest of the argument is treated as a command
and all output is piped to it. This is convenient for piping the debugging output to a program
without affecting the redirections of executed programs. 

-ff         
If the -o filename option is in effect, each processes trace is written to filename.pid where pid is
the numeric process id of each process.  This is incompatible with -c, since no per-process counts
are kept.

note: This is important to get every thread output into a separate file (-ff option).

-p pid      
Attach to the process with the process ID pid and begin tracing. The trace may be terminated at any
time by a keyboard interrupt signal (CTRL-C).  strace will respond by detaching itself from the
traced process(es) leaving it (them) to continue running.  Multiple -p options can be used to attach
to up to 32 processes in addition to command (which is optional if at least one -p option is given).

-s strsize  
Specify the maximum string size to print (the default is 32).  Note that filenames are not
considered strings and are always printed in full.

-E var=val  Run command with var=val in its list of environment variables.

-e expr     

A qualifying expression which modifies which events to trace or how to trace them.  The format of
the expression is:

[qualifier=][!]value1[,value2]...

where qualifier is one of trace, abbrev, verbose, raw, signal, read, or write and value is a
qualifier-dependent symbol or number. The 'default' qualifier is trace. Using an exclamation mark
negates the set of values.  

For example, -e open means literally -e trace=open which in turn means trace only the open system
call. By contrast, -e trace=!open means to trace every system call except open. In addition, the
special values all and none have the obvious meanings.

Note that some shells use the exclamation point for history expansion even inside quoted arguments.
If so, you must escape the exclamation point with a back-slash.


<example>
When use to run a script:

$ strace -f -o trace sh ./compile.sh

When use to run a program:

$ strace ./appname

strace -ff -s 128 -v -o dbus-trace.log -p <dbus daemon PID>


<how-to-run-app-with-strace>
# this is the original line
LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib" LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
exec "${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"

# this is the same line with strace
exec strace -ff -o /opt/adobe/stagecraft/data/trace.log -s 128 \
-E LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib" -E LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
"${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"


={============================================================================
*kt_linux_tool_020* time and date

kit@kit-vb:~/tizencore$ time -f "%E real,%U user,%S sys" ls -Fs
-f: command not found

real	0m0.143s
user	0m0.068s
sys	0m0.040s

kit@kit-vb:~/tizencore$ /usr/bin/time -f "%E real,%U user,%S sys" ls -Fs
total 24
4 app-core/  4 appfw/  4 application/  4 app-service/  4 dlog/	4 README
0:00.00 real,0.00 user,0.00 sys
kit@kit-vb:~/tizencore$ 

Users of the bash shell need to use an explicit path in order to run the external time command and
not the shell builtin variant. On system where time is installed in /usr/bin, the first example
would become /usr/bin/time wc /etc/hosts


{date}
$ date --rfc-3339=s
2015-01-27 13:49:11+00:00

$ date
Tue Jan 27 13:49:19 UTC 2015


={============================================================================
*kt_linux_tool_021* tool-chmod

{to-set-uid}
That require root access to function properly even when invoked by a nonroot
user

%chmod +s /sample_file


={============================================================================
*kt_linux_tool_022* mknod

%mknod $name c $major $minor


={============================================================================
*kt_linux_tool_023* wc

-l, --lines
     print the newline counts

{how-to-count-lines-recursively}

find . -name '*.php' | xargs wc -l

# should be re-written using sh scripting
#!/bin/bash
echo "debug..."
find debug -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "dsm"
find dsm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "include"
find include -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "main"
find main -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

#
echo "mah"
find mah -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5dec"
find mh5dec -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5eng"
find mh5eng -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5gpi"
find mh5gpi -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mhv"
find mhv -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "pfm"
find pfm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l


={============================================================================
*kt_linux_tool_024* du and df

' to print one level
du -h --max-depth=1

-h, --human-readable
print sizes in human readable format (e.g., 1K 234M 2G)

' to show all and total
du -ach
du -sh


={============================================================================
*kt_linux_tool_025* ln

       -f, --force
              remove existing destination files

       -s, --symbolic
              make symbolic links instead of hard links

ln -sf input output. E.g., ln -sf linux-2.4.25-2.8 linux

<example>
$ ln -s /usr/src/linux .
lrwxrwxrwx  1 keitee keitee    14 Feb 18 23:50 linux -> /usr/src/linux/

<example>
To create mutiple link at one go. Assume that there are two files under /bin e.g., gdb and gdbserver
then it will create two links in /usr/local/bin/.

ln -sf /opt/zinc/oss/debugtools/bin/gdb* /usr/local/bin/


={============================================================================
*kt_linux_tool_026* tool-rsync

NAME
       rsync - a fast, versatile, remote (and local) file-copying tool

SYNOPSIS
       Local:  rsync [OPTION...] SRC... [DEST]

       Access via remote shell:
         Pull: rsync [OPTION...] [USER@]HOST:SRC... [DEST]
         Push: rsync [OPTION...] SRC... [USER@]HOST:DEST

       Access via rsync daemon:
         Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST]
               rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST]
         Push: rsync [OPTION...] SRC... [USER@]HOST::DEST
               rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST

       Usages with just one SRC arg and no DEST arg will list the source files
       instead of copying.

Rsync, which stands for "remote sync", is a remote and local file
synchronization tool. It uses an algorithm that minimizes the amount of data
copied by only moving the portions of files that have changed.


DESCRIPTION
       Rsync  is  a  fast  and extraordinarily versatile file copying tool.  It
       can copy locally, to/from another host over any remote shell, or to/from
       a remote rsync daemon.  It offers a large number of options that control
       every aspect of its behavior and permit very flexible specification of
       the set of files to be copied.  It is famous for its 'delta'-transfer
       algorithm, which reduces the amount of data sent over the network by
       sending only the  differences between the source files and the existing
       files in the destination.  Rsync is widely used for backups and mirroring
       and as an improved copy command for everyday use.

       Rsync finds files that need to be transferred using a "quick check"
       algorithm (by default) that looks for files that have changed in 'size' or
       in last-modified 'time'.  Any changes in the other preserved attributes (as
           requested by options) are made on the destination file directly when
       the quick check indicates that the file's data does not need to be
       updated.

       Some of the additional features of rsync are:

       o      support for copying links, devices, owners, groups, and
              permissions

       o      exclude and exclude-from options similar to GNU tar

       o      a CVS exclude mode for ignoring the same files that CVS would
              ignore

       o      can use any transparent remote shell, including ssh or rsh

       o      does not require super-user privileges

       o      pipelining of file transfers to minimize latency costs

       o      support for anonymous or authenticated rsync daemons (ideal for
              mirroring)


        -r, --recursive             recurse into directories

GENERAL
       Rsync copies files either to or from a remote host, or locally on the
       current host (it does 'not' support copying files between two remote
           hosts).

       There are two different ways for rsync to contact a remote system: using
       a remote-shell program as the transport (such as ssh or rsh) or
       contacting an rsync daemon directly via TCP.  
       
       The remote-shell transport is used  whenever  the  source  or destination
       path contains a single colon (:) separator after a host specification.  
       
       Contacting an rsync daemon directly happens when the source or
       destination path contains a double colon (::) separator after a host
       specification, OR when an rsync:// URL is specified (see also the "USING
       RSYNC-DAEMON FEATURES VIA A REMOTE-SHELL CONNECTION" section for an
       exception to this latter rule).

       As a special case, if a single source arg is specified without a
       destination, the files are listed in an output format similar to "ls -l".

       As expected, if neither the source or destination path specify a remote
       host, the copy occurs locally (see also the --list-only option).

       Rsync refers to the local side as the "client" and the remote side as the
       "server".  Don't confuse "server" with an rsync daemon -- a daemon is
       always a server, but a server can be either a daemon or a  remote-shell
       spawned process.

USAGE
       You use rsync in the same way you use rcp. You must specify a source and
       a destination, one of which may be remote.

       Perhaps the best way to explain the syntax is with some examples:

       -t, --times                 preserve modification times

              rsync -t *.c foo:src/

       This would transfer all files matching the pattern *.c from the current
       directory to the directory src on the machine foo. If any of the files
       already exist on the remote system then the rsync remote-update protocol
       is used to update the file by sending only the differences. See the tech
       report for details.

        -v, --verbose               increase verbosity
        -a, --archive               archive mode; 'equals' -rlptgoD (no -H,-A,-X)
            --no-OPTION             turn off an implied OPTION (e.g. --no-D)
        -z, --compress              compress file data during the transfer

              rsync -avz foo:src/bar /data/tmp

       This  would  recursively transfer all files from the directory src/bar on
       the machine foo into the /data/tmp/bar directory on the local machine.
       The files are transferred in "archive" mode, which ensures that symbolic
       links, devices, attributes, permissions, ownerships, etc. are preserved
       in the transfer.  Additionally, compression will be used to reduce the
       size of data portions of the transfer.

              rsync -avz foo:src/bar/ /data/tmp

       A 'trailing' slash on the source changes this behavior to avoid creating an
       additional directory level at the destination.  You can think of a
       trailing / on a source as meaning "copy the contents of this directory"
       as  opposed to  "copy  the directory by name", but in both cases the
       attributes of the containing directory are transferred to the containing
       directory on the destination.  In other words, each of the following
       commands copies the files in the same way, including their setting of the
       attributes of /dest/foo:

              rsync -av /src/foo /dest
              rsync -av /src/foo/ /dest/foo

       Note also that host and module references don't require a trailing slash
       to copy the contents of the default directory.  For example, both of
       these copy the remote directory's contents into "/dest":

              rsync -av host: /dest
              rsync -av host::module /dest

       You can also use rsync in local-only mode, where both the source and
       destination don't have a ':' in the name. In this case it behaves like an
       improved copy command.

       Finally, you can 'list' all the (listable) 'modules' available from a
       particular rsync daemon by leaving off the module name:

              rsync somehost.mydomain.com::

              <ex>
              ~/source/DEVARCH$ rsync zinc@humax-04535::
              Root           	Humax box

       See the following section for more details.

CONNECTING TO AN RSYNC DAEMON
       It is also possible to use rsync without a remote shell as the transport.
       In this case you will directly connect to a remote rsync daemon, 
       typically using TCP port 873.  

       (This obviously requires the daemon to be running on the remote system,
        so refer to the STARTING AN RSYNC DAEMON TO ACCEPT CONNECTIONS section
        below for information on that.)

       Using rsync in this way is the same as using it with a remote shell
       except that:

       o      you either use a double colon :: instead of a single colon to
              separate the hostname from the path, or you use an rsync:// URL.

       o      the first word of the "path" is actually a module name.

       o      the remote daemon may print a message of the day when you connect.

       o      if you specify no path name on the remote daemon then the list of
              accessible paths on the daemon will be shown.

       o      if you specify no local destination then a listing of the
              specified files on the remote daemon is provided.

       o      you must not specify the --rsh (-e) option.

       An example that copies all the files in a remote module named "src":

           rsync -av host::src /dest

       Some  modules  on the remote daemon may require authentication. 

       If so, you will receive a password prompt when you connect. You can avoid
       the password prompt by setting the environment variable RSYNC_PASSWORD to
       the password you want to use or using the --password-file option. This
       may be useful when scripting rsync.

       WARNING: On some systems environment variables are visible to all users.
       On those systems using --password-file is recommended.

STARTING AN RSYNC DAEMON TO ACCEPT CONNECTIONS
       In  order  to connect to an rsync daemon, the remote system needs to have
       a daemon already running (or it needs to have configured something like
           inetd to spawn an rsync daemon for incoming connections on a
           particular port).  
       
       For full information on how to start a daemon that will handling incoming
       socket connections, see the rsyncd.conf(5) man page -- that is the config
       file for the daemon, and it contains the full details for  how  to  run
       the daemon (including stand-alone and inetd configurations).

       If you're using one of the remote-shell transports for the transfer,
       there is no need to manually start an rsync daemon.

EXAMPLES
       Here are some examples of how I use rsync.

       To backup my wife's home directory, which consists of large MS Word files and mail folders, I use a cron job that runs

              rsync -Cavz . arvidsjaur:backup

       each night over a PPP connection to a duplicate directory on my machine "arvidsjaur".

       To synchronize my samba source trees I use the following Makefile targets:

           get:
                   rsync -avuzb --exclude '*~' samba:samba/ .
           put:
                   rsync -Cavuzb . samba:samba/
           sync: get put

       this allows me to sync with a CVS directory at the other end of the connection. I then do CVS operations on the remote machine, which saves a lot of time as the remote CVS protocol isn't very efficient.

       I mirror a directory between my "old" and "new" ftp sites with the command:

       rsync -az -e ssh --delete ~ftp/pub/samba nimbus:"~ftp/pub/tridge"

       This is launched from cron every few hours.

OPTIONS
       Rsync  accepts  both  long  (double-dash  +  word)  and  short
       (single-dash + letter) options.  The full list of the available options
       are described below.  
       
       If an option can be specified in more than one way, the choices are
       comma-separated.  Some options only have a long variant, not a short.  If
       the option takes a parameter, the parameter is only listed after the long
       variant, even though it must also be specified for the short.  When
       specify- ing  a  parameter,  you  can either use the form --option=param
       or replace the '=' with whitespace.  The parameter may need to be quoted
       in some manner for it to survive the shell's command-line parsing.  Keep
       in mind that a leading tilde (~) in a filename is substituted by your
       shell, so --option=~/foo will not change the tilde into your home
       directory (remove the '=' for that).


       -n, --dry-run
              This  makes  rsync  perform  a  trial run that doesn't make any
              changes (and produces mostly the same output as a real run).  It
              is most commonly used in combination with the -v, --verbose and/or
              -i, --itemize-changes options to see what an rsync command is
              going to do before one actually runs it.

              The output of --itemize-changes is supposed to be exactly the same
              on a dry run and a subsequent real run (barring intentional
                  trickery and system call failures); if it isn't, that's a bug.
              Other  output  should  be mostly unchanged, but may differ in some
              areas.  Notably, a dry run does not send the actual data for file
              transfers, so --progress has no effect, the "bytes sent", "bytes
              received", "literal data", and "matched data" statistics are too
              small, and the "speedup" value is equivalent to a run where no
              file transfers were needed.

       --delete
              note: delete(sync) when there are deleted files in source side.

              This tells rsync to delete extraneous files from the receiving
              side (ones that aren't on the sending side), but only for the
              directories that are being synchronized.  You must have asked
              rsync to send the whole directory (e.g. "dir" or "dir/") without
              using a wildcard for the directory's contents (e.g. "dir/*") since
              the wildcard is expanded by the shell and rsync thus gets a
              request to transfer individual files, not  the  files' parent
              directory.   Files  that  are  excluded  from  the  transfer  are
              also  excluded  from being deleted unless you use the
              --delete-excluded option or mark the rules as only matching on the
              sending side (see the include/exclude modifiers in the FILTER
                  RULES section).

              Prior to rsync 2.6.7, this option would have no effect unless
              --recursive was enabled.  Beginning with 2.6.7, deletions will
              also occur when --dirs (-d) is enabled, but only for directories
              whose  contents  are  being copied.

              This option can be dangerous if used incorrectly!  It is a very
              good idea to first try a run using the --dry-run option (-n) to
              see what files are going to be deleted.

              If  the  sending side detects any I/O errors, then the deletion of
              any files at the destination will be automatically disabled. This
              is to prevent temporary filesystem failures (such as NFS errors)
              on the sending side from causing a massive deletion of files on
              the destination.  You can override this with the --ignore-errors
              option.

              The --delete option may be 'combined' with one of the
              --delete-WHEN options without conflict, as well as
              --delete-excluded.  However, if  none  of  the  --delete-WHEN
              options  are  specified,  rsync  will  choose the --delete-during
              algorithm when talking to rsync 3.0.0 or newer, and the
              --delete-before algorithm when talking to an older rsync.  See
              also --delete-delay and --delete-after.


       --delete-excluded
              In addition to deleting the files on the receiving side that are
              'not' on the sending side, this tells rsync to also delete any
              files on the receiving side that are 'excluded' (see --exclude).
              See the FILTER  RULES  section for a way to make individual
              exclusions behave this way on the receiver, and for a way to
              protect files from --delete-excluded.  See --delete (which is
                  implied) for more details on file-deletion.


       -f, --filter=RULE
              This option allows you to add rules to selectively 'exclude'
              certain files from the list of files to be transferred. This is
              most useful in combination with a recursive transfer.

              You  may  use  as  many --filter options on the command line as
              you like to build up the list of files to exclude.  If the filter
              contains whitespace, be sure to quote it so that the shell gives
              the rule to rsync as a single argument.  The text below also
              mentions that you can use an underscore to replace the space that
              separates a rule from its arg.

              See the FILTER RULES section for detailed information on this
              option.

FILTER RULES
       The filter rules allow for flexible selection of which files to transfer
       (include) and which files to skip (exclude).  The rules either directly
       specify include/exclude  patterns  or  they  specify  a  way  to  acquire
       more include/exclude patterns (e.g. to read them from a file).

       As  the list of files/directories to transfer is built, rsync checks each
       name to be transferred against the list of include/exclude patterns in
       turn, and the first matching pattern is acted on:  if it is an exclude
       pattern, then that file is skipped; if it is an include pattern then that
       filename is not skipped; if no matching pattern is found, then the
       filename is not skipped.

       Rsync builds an ordered list of filter rules as specified on the
       command-line.  Filter rules have the following syntax:

              RULE [PATTERN_OR_FILENAME]
              RULE,MODIFIERS [PATTERN_OR_FILENAME]

       You have your choice of using either short or long RULE names, as
       described below.  If you use a short-named rule, the ',' separating the
       RULE from the MODIFIERS is optional.  The  PATTERN  or  FILENAME  that
       follows  (when present) must come after either a single space or an
       underscore (_).  
       
       Here are the available rule 'prefixes':

              exclude, - specifies an exclude pattern.
              include, + specifies an include pattern.

              merge, . specifies a merge-file to read for more rules.

              dir-merge, : specifies a per-directory merge-file.
              hide, H specifies a pattern for hiding files from the transfer.
              show, S files that match the pattern are not hidden.

              protect, P specifies a pattern for protecting files from deletion.

              risk, R files that match the pattern are not protected.
              clear, ! clears the current include/exclude list (takes no arg)

       When rules are being read from a file, empty lines are ignored, as are
       comment lines that start with a "#".

INCLUDE/EXCLUDE PATTERN RULES
       You can include and exclude files by specifying patterns using the filter
       rules.  The include/exclude rules each specify a pattern that is matched
       against  the names of the files that are going to be transferred.  These
       patterns can take several forms:

       o      if the pattern 'ends' with a / then it will only match a directory,
              not a regular file, symlink, or device.

       o      rsync chooses between doing a simple string match and wildcard
       matching by checking if the pattern contains one of these three wildcard
       characters: '*', '?', and '[' .

       o      a '*' matches any path component, but it stops at slashes.

       o      use '**' to match anything, including slashes.

       o      a '?' matches any character except a slash (/).

       o      a '[' introduces a character class, such as [a-z] or [[:alpha:]].

       o      if the pattern contains a / (not counting a trailing /) or a "**",
       then it is matched against the full pathname, including any leading
         directories. If the pattern doesn't contain a / or a "**", then it is
         matched only against the final component of the filename.  (Remember
             that the algorithm is applied recursively so "full filename" can
             actually be any portion of a path from the starting directory on
             down.)

MERGE-FILE FILTER RULES
       You can merge whole files into your filter rules by specifying either a
       merge (.) or a dir-merge (:) filter rule (as introduced in the FILTER
           RULES section above).

       There are two kinds of merged files -- single-instance ('.') and
       per-directory (':').  A single-instance merge file is read one time, and
       its rules are incorporated into the filter list in the place of  the  "."
       rule.   For per-directory  merge  files,  rsync  will scan every
       directory that it traverses for the named file, merging its contents when
       the file exists into the current list of inherited rules.  These
       per-directory rule files must be created on the sending side because it
       is the sending side that is being scanned for the available files to
       transfer.  These rule files may also need to be transferred to the
       receiving side if you want them  to  affect  what files don't get deleted
       (see PER-DIRECTORY RULES AND DELETE below).

       Some examples:

              merge /etc/rsync/default.rules
              . /etc/rsync/default.rules
              dir-merge .per-dir-filter
              dir-merge,n- .non-inherited-per-dir-excludes
              :n- .non-inherited-per-dir-excludes

       The following modifiers are accepted after a 'merge' or dir-merge rule:

       o      A - specifies that the file should consist of only exclude
       patterns, with no other rule-parsing except for in-file comments.

       <ex>
        ~/source/DEVARCH$ cat /home/kpark/source/setup-humax/rsync.filter 
        P var/applications/
        - include/
        - **.a

        # files
        - oss/bin/msggrep
        - oss/bin/envsubst
        - oss/lib/gstreamer-1.0/*.la
        - oss/lib/libmpeg*


<config>

DESCRIPTION

The rsyncd.conf file is the runtime configuration file for rsync when run with
the --daemon option. When run in this way rsync becomes a rsync server listening
on TCP port 873. Connections from rsync clients are accepted for either
anonymous or authenticated rsync sessions.

The rsyncd.conf file controls authentication, access, logging and available
modules. 

MODULE OPTIONS

After the global options you should define a number of modules, each module
exports a directory tree as a symbolic name. Modules are exported by specifying
a module name in square brackets [module] followed by the options for that
module. 

secrets file
    The "secrets file" option specifies the name of a file that contains the
    username:password pairs used for authenticating this 'module'. This file is
    only consulted if the "auth users" option is specified. The file is line
    based and contains username:password pairs separated by a single colon. Any
    line starting with a hash (#) is considered a comment and is skipped. The
    passwords can contain any characters but be warned that many operating
    systems limit the length of passwords that can be typed at the client end,
    so you may find that passwords longer than 8 characters don't work.  There
    is no default for the "secrets file" option, you must choose a name (such as
    /etc/rsyncd.secrets). The file must normally not be readable by "other"; see
    "strict modes". 


[root@HUMAX /]# cat /etc/rsyncd.conf 
log file = /var/log/rsyncd.log
pid file = /run/rsyncd.pid
lock file = /run/rsync.lock

[Root]
   path = /
   comment = Humax box
   uid = root
   gid = root
   read only = no
   list = yes
   auth users = zinc
   secrets file = /etc/rsyncd.scrt

[root@HUMAX /]# cat /etc/rsyncd.scrt 
zinc:zinc


<ex>
$ mkdir dir1 dir2
$ touch dir1/file{1..10}
$ ls dir1
file1  file10  file2  file3  file4  file5  file6  file7  file8  file9

$ rsync -r dir1/ dir2      " okay as synced
$ ls dir2
file1  file10  file2  file3  file4  file5  file6  file7  file8  file9

note: This trailing / is necessary to mean "the contents of dir1".

$ rsync -r dir1 dir2       " not okay as
$ ls dir2
dir1


The -a option is a combination flag.

It stands for "archive" and syncs recursively and preserves symbolic links, special and device
files, modification times, group, owner, and permissions. It is more commonly used than -r and is
usually what you want to use.

The -n or --dry-run options. 

The -v flag (for verbose). 

The -P flag is very helpful. 
It combines the flags --progress and --partial. The first of these gives you a progress bar for the
transfers and the second allows you to resume interrupted transfers:

The -z flag. 
If you are transferring files that have not already been compressed, like text files, you can reduce
the network transfer by adding compression with the -z option.

<update>
Update the modification time on some of the files and see that rsync intelligently re-copies only
the changed files:

<delete>
By default, rsync does not delete anything from the destination directory. We can change this
behavior with the --delete option. Before using this option, use the --dry-run option and do testing
to prevent data loss:

rsync -a --delete source destination


<example> over SSH
rsync -a ~/dir1 username@remote_host:destination_directory
rsync -a username@remote_host:/home/username/dir1 place_to_sync_on_local_machine

<example>
Want to copy all expect .git from source to destination

rsync -av --progress /home/kit/mheg-remote-git/mag_shared/ . --exclude .git

<example>
setupRsyncDaemon() {

    # Setup rsync daemon for faster non-encrypted transfer
    rsync ${privatekey:+ --rsh="ssh -i $privatekey"} \
        "$(thisScriptSrcDir)"/rsyncd.{conf,scrt} root@$stbip:/etc/
    $ssh "chmod o-rwx /etc/rsyncd.scrt && rsync --daemon --ipv4"
    export RSYNC_PASSWORD=zinc
}


={============================================================================
*kt_linux_tool_027* awk

Another popular stream editor. The basic function of awk is to search files for lines or other text
units containing one or more patterns. When a line matches one of the patterns, special actions are
performed on that line.

<data-driven>
Programs in awk are different from programs in most other languages, because awk programs are
"data-driven": you describe the data you want to work with and then what to do when you find it.
Most other languages are "procedural."

<rule>
The program consists of a series of rules. Each rule specifies one pattern to search for and one
action to perform upon finding the pattern.

<command>
print

The print command in awk outputs selected data from the input file. $0 (zero) holds the value of the
entire line.

ls -l | awk '{ print $5 $9 }'

With formatting.

awk '{ print "Size is " $5 " bytes for " $9 }'

<regex>
awk 'EXPRESSION { PROGRAM }' file(s)

For files ending in ".conf" and starting with either "a" or "x", using extended regular expressions
note: the below do not work.

ls -l | awk '/\<(a|x).*\.conf$/ { print $9 }'

To add text before output: begin message. beginning of execution before any input has been processed

awk 'BEGIN { print "Files found:\n" } /\<[a|x].*\.conf$/ { print $9 }'

To add text after output:

awk '/\<[a|x].*\.conf$/ { print $9 } END { print \ "Can I do anything else for you, mistress?" }'

<variables>
field separator

awk 'BEGIN { FS=":" } { print $1 "\t" $5 }' /etc/passwd

output field separator

> cat test
record1 data1
record2 data2

> awk '{ print $1 $2}' test
record1data1
record2data2

> awk '{ print $1, $2}' test
record1 data1
record2 data2

<example>
To make a filename from the date:

date: Fri Jul 25 05:30:12 BST 2014 -> log-Fri-Jul-25-...

script -f /home/kit/log/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`

To make a gateway from the ip address:
gateway=`echo $ip | awk 'BEGIN { FS="." } ; { print $1"."$2"."$3"."1 }'`


={============================================================================
*kt_linux_tool_028* tool-sed

A stream editor is used to perform basic transformations on text read from a
file or a pipe. The result is sent to standard output. The editor does not
modify the original input.

What distinguishes sed from other editors, such as vi and ed, is its ability to
filter text that it gets from a pipeline feed. You do not need to interact with
the editor while it is running. This feature allows use of editing commands in
scripts, greatly easing repetitive editing tasks. When facing replacement of
text in a 'large' number of files, sed is a great help.


{command}

COMMAND SYNOPSIS

       This is just a brief synopsis of sed commands to serve as a reminder to
       those who already know sed; other documentation (such as the texinfo
           document) must be consulted for fuller descriptions.

       s/regexp/replacement/

       Attempt to match regexp against the pattern space.  If successful,
       replace that portion matched with replacement.  The replacement may
       contain the special character & to refer to that  portion  of  the
       pattern  space which matched, and the special escapes \1 through \9 to
       refer to the corresponding matching sub-expressions in the regexp.

       i \
       text   
       Insert text, which has each embedded newline preceded by a backslash.

<ex>
To find and replace string in multiple files.

$ egrep -lr --include *.c mhvSessionCancel .
./mh5eng/mh5a_application.c

$ egrep -lr --include *.c mhvSessionCancel . \
  | xargs sed -i 's/mhvSessionCancel/mmhv_session_cancel/'


sed
-i[SUFFIX], --in-place[=SUFFIX]

edit files in place (makes backup if extension supplied)

-n, --quiet, --silent

suppress automatic printing of pattern space. 
note: why this? since sed print out line which are not in matches so use -n to output only matches.

-u, --unbuffered

load minimal amounts of data from the input files and flush the output buffers more often

<print-range>
Here, "p" is a command.

sed -n '2,4p' example
sed -n '3,$p' example

<busybox>
sed

    sed [-efinr] SED_CMD [FILE]...

    Options:

            -e CMD  Add CMD to sed commands to be executed
            -f FILE Add FILE contents to sed commands to be executed
            -i      Edit files in-place
            -n      Suppress automatic printing of pattern space
            -r      Use extended regex syntax

    If no -e or -f is given, the first non-option argument is taken as the sed
    command to interpret. All remaining arguments are names of input files; if
    no input files are specified, then the standard input is read. Source files
    will not be modified unless -i option is given.


<ex>

libTitaniumDeviceAuthoritySystemDbusClient.so createDbusSystemFactory

sed -i \
  -e '1i libTitaniumDeviceAuthoritySystemOff.so createOffSystemFactory' \
  -e 's/^/# DA disabled by patch-a-tron: /' [FILE]

libTitaniumDeviceAuthoritySystemOff.so createOffSystemFactory
# DA disabled by patch-a-tron: libTitaniumDeviceAuthoritySystemDbusClient.so
      createDbusSystemFactory


={============================================================================
*kt_linux_tool_029* pyserial and grabserial

http://elinux.org/Grabserial
https://github.com/tbird20d/grabserial

If no options are specified, grabserial uses serial port /dev/ttyS0, at 115200 baud with "8, None
and 1" (8N1) settings. 

alias gse="sudo grabserial -v -d "/dev/ttyUSB0" -b 115200 -w 8 -p N -s 1 \
| tee 2>&1 ~/logs/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`"

alias gse="sudo grabserial -v -d /dev/ttyUSB0 \
| tee 2>&1 ~/logs/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`"


={============================================================================
*kt_linux_tool_030* diff and patch

{diff}
<from-to-and-unified>
To compare two files:

diff [options] from-file to-file
diff -u file1 file2

-u    Use the 'unified' output format.

This outputs a description of how to transform file1 'into' file2 to stdout in unified format which
is the easiest to read. The description is called a patch.

You can compare a whole directory tree:

diff -u -r directory1 directory2

This recurses the directory structures. Whenever a file differs, the patch for that file is appended
to the output.

To save the patches to a file that you can store or send to someone else, simply redirect stdout to
a file, e.g:

diff -u file1 file2 > my_changes.patch

{unified-format}
A typical patch looks like this:

--- a/vobs/DARWIN_APPLICATIONS/DARWIN_GAUDI_ORCHID/build/applications/GAUDI_Orchid/EPG/statmgr/xtv_states/QhsmPVR.c
+++ b/vobs/DARWIN_APPLICATIONS/DARWIN_GAUDI_ORCHID/build/applications/GAUDI_Orchid/EPG/statmgr/xtv_states/QhsmPVR.c
@@ -2815,7 +2815,6 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
 
         /*set the playback speed to normal*/
         DBGMSG_M("QHsmPVR_backWard_skip - PVR_NAV_SetSpeed(PVR_NAV_PLAY)\n");
-        PVR_NAV_SetSpeed(PVR_NAV_PLAY);
 
         /*check if we are in RB mode*/
         CONSELECTCO_GetCurrentPlayRBFlag( &state);
@@ -2827,6 +2826,7 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
         {
             /*In RB mode*/
             PVR_NAV_SetPosition ( PVR_NAV_RELATIVE_POSITION_START , 1000);
+           PVR_NAV_SetSpeed(PVR_NAV_PLAY);^M
         }
         else{
             /*We are in playback mode*/
@@ -2840,6 +2840,7 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
             if (rc != PVR_NAV_RC_OK) {
                 DBGMSG_M("No previous skip point; skipping to beginning and waiting for timeout;");
                 PVR_NAV_SetPosition ( PVR_NAV_RELATIVE_POSITION_START , 0);
+               PVR_NAV_SetSpeed(PVR_NAV_PLAY);^M
                 prevPoint = -1;
             }
             else {

<header>
The first two lines describe the files to be transformed. The file indicated by "---" is transformed
into the file indicated by "+++". Let's call them the old file and the new file.

<hunks>
This header is followed by a series of "hunks". Each hunk describes a changes to make to a section
of the file. The @@ symbols indicate the 'start' of a hunk. The first set of numbers, e.g. "-2815,7"
indicates that in the old file, the section 'started' on line 2815 and 'lasted' for 7 lines. The second
set of numbers, e.g. "+2815,6" indicates that in the new file, the section starts on like 2815 and
lasts for 6 lines. (You can guess that we are going to remove a line.)

<p-option>
The text after the @@ symbol indicates the function name that the change is in. This is generated by
the -p option of diff so may not always be there. It is just to make the patch easier to read.

       -p, --show-c-function
              show which C function each change is in

<context>
Next we have three lines of context. These are just the three lines before the change. They are used
to check that the change is going to be made in the right place. For example, it is possible that
you want to apply the patch to a different version of the file it was created with. These lines help
you manually or automatically apply the change in the right place, even if the line numbers have
changed. The context is also used to detect when a patch cannot be applied because of a conflict.
More on this later.

<changes>
Next we have the change itself. Lines are either removed (indicated by a - sign) or added, indicated
by a + sign. When a line is changed, it usually appears as a removed line followed by an added line.

Finally we have three more lines of context.

So now you can read patches and even manually apply simple ones. But with complicated patches, you
would prefer to apply them automatically.


{patch}
GNU patch reads a patch file and applies the transformations it describes. Typical use:

cd directory_containing_file_to_change

patch < my_changes.patch

GNU patch reads patches from stdin. In this case we have redirected stdin to the patch file with the "<" symbol.

If you have a patch with lots of files in different directories, you might do this:

cd parent_directory

patch -p1 < my_changes.patch

<p-option>
The number after the p tells patch how 'many' directory names to strip from the filenames before
applying the patch. For example, typically the person generating the patch did something like this:

cd my_code
cd ..
cp -R my_code my_original_code

...make changes to lots of files...

diff -u -r my_original_code my_code

This means that the patch file will contain filenames like this:

--- my_original_code/directory/file.c
+++ my_code/directory/file.c

Now you probably don't have directories called "my_code" and "my_original_code", so you would cd to
"directory" and tell patch to ignore the first directory in each file path by using the "-p1"
option.

<patching-non-identical-files>
GNU patch will try to apply the changes even if you are applying them to a different version of the
file to the one the patch was originally created against. It uses the context lines to do this. As
long as enough context lines appear, and they have not moved to far from their original positions,
     the change will be applied.

So, for example, if a new function was added to the top of the file, the change to a function
further down the file would still be correctly applied. This means it can be more useful to send
someone a patch than a whole file, because it will often still work if the person you send it to is
starting with a different version of the file.

If GNU patch decides it cannot apply a change, you will see:

HUNK FAILED

along with details of which hunk it was. You can then investigate why the hunk failed - patches are
easy enough to read to do this.

<concatenating-patches>
Patches can be concatenated, so you can combine multiple patches into one:

diff -u my_code_orig/file1.c my_code/file1.c > patch1
diff -u my_code_orig/file2.c my_code/file2.c > patch2
cat patch1 patch2 > big_patch


{example}
-a     Treat all files as text and compare them line-by-line, even if they do not seem to be text.
-N
--new-file
   In directory comparison, if a file is found in only one directory, treat it as present but empty
   in the other directory.

diff -Naur darwin-spk-1.18_orig/target/device/Sky/ams-drx890/kernel-debug.config darwin-spk-1.18/target/device/Sky/ams-drx890/kernel-debug.config
--- darwin-spk-1.18_orig/target/device/Sky/ams-drx890/kernel-debug.config	2011-10-05 12:36:12.000000000 +0100
+++ darwin-spk-1.18/target/device/Sky/ams-drx890/kernel-debug.config	2012-01-18 15:23:41.000000000 +0000
@@ -1157,7 +1157,7 @@
 CONFIG_DEBUG_FS=y
 # CONFIG_WANT_EXTRA_DEBUG_INFORMATION is not set
 CONFIG_CROSSCOMPILE=y
-CONFIG_CMDLINE="mem=160M rw console=uart,mmio,0x10400b00,115200n8 ip_conntrack_tftp.ports=4085"
+CONFIG_CMDLINE="mem=166M rw console=uart,mmio,0x10400b00,115200n8 ip_conntrack_tftp.ports=4085"
 CONFIG_SYS_SUPPORTS_KGDB=y
 # CONFIG_MIPS_BRCM_SIM is not set

<patch-options> in the script

       -d dir  or  --directory=dir
          Change to the directory dir immediately, before doing anything else.

       --dry-run
          Print  the results of applying the patches without actually changing
          any files.

       -E  or  --remove-empty-files
          Remove output files that are  empty  after  the  patches  have  been
          applied.  Normally this option is unnecessary, since patch can examâ
          ine the time stamps on the header to determine whether a file should
          exist  after  patching.  However, if the input is not a context diff
          or if patch is conforming to POSIX,  patch  does  not  remove  empty
          patched  files  unless  this  option is given.  When patch removes a
          file, it also attempts to remove any empty ancestor directories.

       -f  or  --force
          Assume that the user knows exactly what he or she is doing,  and  do
          not  ask any questions.  Skip patches whose headers do not say which
          file is to be patched; patch files even though they have  the  wrong
          version  for  the Prereq: line in the patch; and assume that patches
          are not reversed even if they look like they are.  This option  does
          not suppress commentary; use -s for that.

       -s  or  --silent  or  --quiet
          Work silently, unless an error occurs.

   patch)
      # For each patch, try to derive whether it was created as -p0 or -p1.
      # p0 is typically from svn/cvs/git, while p1 will typically come from diff.
      patchlevel=

      patch -d $SPKDIR -p1 --dry-run --quiet -f < $p > /dev/null 2>&1
      p1=$?
      if [ $p1 -eq 0 ]; then
         patchlevel=-p1
      fi

      patch -d $SPKDIR -p0 --dry-run --quiet -f < $p > /dev/null 2>&1
      p0=$?
      if [ $p0 -eq 0 ]; then
         patchlevel=-p0
      fi
      if [ $p0 -eq 0 -a $p1 -eq 0 ]; then
         echo "WARNING: patch '$p' can be applied -p0 or -p1. Using -p0."
         patchlevel=-p0
      fi
      if [ -z "$patchlevel" ]; then
         echo "Unable to apply patch '$p'"
         exit 1
      fi
      echo "Applying patch '$p'..."
      patch -d $SPKDIR $patchlevel -E < $p
      if [ $? -ne 0 ]; then
         echo "Patch $p failed. Aborting."
         exit 1
      fi
      ;;


={============================================================================
*kt_linux_tool_031* zip and tar

{zip}
-d --decompress --uncompress 
-l --list

" maintain the original file
       -c --stdout --to-stdout
              Write output on standard output; keep original files unchanged.  If there are
              several input files, the output consists of a sequence of independently  com-
              pressed  members.  To  obtain better compression, concatenate all input files
              before compressing them.

gzip -c ramdisk_rootfs_img > ramdisk_rootfs_img.gz


{tar}
# maintain permissions when create a archive
% tar cvzfp xxx.tgz ./

# to extract
% tar zxvf *.tgz

-f, --file ARCHIVE
use archive file or device ARCHIVE

# '-' is used instead of filename after -f
# to extract from stdin. wget write to stdout and tar read from stdin. '-' used differently.
wget http://xxx.tar.bz2 -O - | tar -xjf -

" to extract bz file
% tar xjf *.bz2

" to create bzip2
% tar cvjfp filename
% bzip2 -d gdb.bz2

" to list
% tar tvf 

<C-option>
-C changes dir

tar -cjvf "$SPK_TARBALL_MW" -C nds-mw.

Since when run $nds-mw> tar -cjvf xx.bz . then there will be xx.bz which is empty in the archive. To
get around this, use -C to move dir and do zip but the output will be make before moving a dir.

<symlink>
When use tar on dir which is a symlink, then tar don not work. Any options?


={============================================================================
*kt_linux_tool_032* split

       split [OPTION]... [INPUT [PREFIX]]

       Output  fixed-size  pieces of INPUT to PREFIXaa, PREFIXab, ...; default size is 1000
       lines, and default PREFIX is 'x'.  

       -b, --bytes=SIZE
              put SIZE bytes per output file

$ split -b 200MB build-full-after-clean.log build-full-after-clean.

build-full-after-clean.aa
...
build-full-after-clean.ci


={============================================================================
*kt_linux_tool_033* getconf

       getconf - Query system configuration variables

       getconf [-v specification] system_var

       -a
       Displays all configuration variables for the current system
       and their values.

       -v
       Indicate the specification and version for which to obtain
       configuration variables.

       system_var
       A system configuration variable, as defined by sysconf(3) or
       confstr(3).

$ getconf PAGESIZE
4096


={============================================================================
*kt_linux_tool_034* ps

" to get gpid of the shell
kpark@wll1p04345:~/work$ ps -o pgid $$
 PGID
 7523


$ ps -p $$ -o 'pid pgid sid command'
  PID  PGID   SID COMMAND
 3697  3697  3697 bash


$ ps -ewf


={============================================================================
*kt_linux_tool_035* wget

{wget}
wget http://xxx.tar.bz2 -O - | tar -xjf -

-O file
--output-document=file
The documents will not be written to the appropriate files, but all will be concatenated together
and written to file. If - is used as file, documents will be printed to standard output, disabling
link conversion. 


{curl}
curl  is  a  tool  to  transfer data from or to a server, using one of the supported protocols
(DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP,
 SFTP, SMTP, SMTPS, TELNET and TFTP). The command is designed to work without user interaction.

curl offers a busload of useful tricks like proxy support, user authentication, FTP upload, HTTP
post, SSL connections, cookies, file transfer resume and more. As  you  will  see below, the number
of features will make your head spin!

curl is powered by libcurl for all transfer-related features. See libcurl(3) for details.

curl --silent --show-error -o "$(basename $galliumurl)" "$galliumurl"


={============================================================================
*kt_linux_tool_036* nc

netcat is a simple unix utility which reads and writes data across network connections, using
TCP or UDP protocol. It is designed to be a reliable "back-end" tool that can be used directly or
easily driven by other programs and scripts. At the same time, it is a feature-rich network
debugging and exploration tool, since it can create almost any  kind of connection you would need
and has several interesting built-in capabilities. 


In the simplest usage, "nc host port" creates a TCP connection to the given port on the given target
host. Your standard input is then sent to the host, and anything that  comes back across the
connection is sent to your standard output. This continues indefinitely, until the network side of
the connection shuts down.  Note that this behavior is different from most other applications which
shut everything down and exit after an end-of-file on the standard input.


note: on host but not the target, when run nc without -q then no prompt back when run so need to
specify -q to get prompt back.

printf "D\t${key}\n\0U\t${key}\n\0" | nc -q 1 $box_ip $box_port

-q seconds   after EOF on stdin, wait the specified number of seconds and then quit. If seconds is
negative, wait forever.


={============================================================================
*kt_linux_tool_037* port checks

Type the following command to see list well-known of TCP and UDP port numbers:

$ less /etc/services
$ grep -w 80 /etc/services 


{port-numbers}
Typically port number less than 1024 are used by well know network servers such as Apache. Under
UNIX and Linux like oses root (super user) privileges are required to open privileged ports. Almost
all clients uses a high port numbers for short term use.

The port numbers are divided into three ranges:

1. Well Known Ports: those from 0 through 1023.
2. Registered Ports: those from 1024 through 49151
3. Dynamic and/or Private Ports: those from 49152 through 65535

You can increase local port range by typing the following command (Linux specific example):
# echo 1024 65535 > /proc/sys/net/ipv4/ip_local_port_range
#
You can also increase or decrease socket timeout (Linux specific example):
# echo 2000 > /proc/sys/net/ipv4/tcp_keepalive_time

# for debian pc and embedded linux
#
root# cat /proc/sys/net/ipv4/ip_local_port_range 
32768	61000

<to-see-open-ports>

netstat [address_family_options] [--tcp|-t] [--udp|-u] [--raw|-w] [--listening|-l] [--all|-a]
[--numeric|-n] [--numeric-hosts] [--numeric-ports] [--numeric-users] [--symbolic|-N]
[--extend|-e[--extend|-e]] [--timers|-o] [--program|-p] [--verbose|-v] [--continuous|-c]


$ netstat -tulpn
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      -               
tcp        0      0 0.0.0.0:47541           0.0.0.0:*               LISTEN      -               
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:3350          0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      -               
tcp        0      0 0.0.0.0:3389            0.0.0.0:*               LISTEN      -               
tcp6       0      0 :::111                  :::*                    LISTEN      -               
tcp6       0      0 :::22                   :::*                    LISTEN      -               
tcp6       0      0 ::1:631                 :::*                    LISTEN      -               
tcp6       0      0 :::33815                :::*                    LISTEN      -               
tcp6       0      0 ::1:25                  :::*                    LISTEN      -               

To displays listening sockets (open ports)

$ netstat -nat | grep LISTEN


To list open IPv4 connections use the lsof command:

$ lsof -Pnl +M -i4
COMMAND     PID     USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME
icedove    3900     1024   58u  IPv4 1836476      0t0  TCP 172.20.33.215:38273->132.245.226.18:993 (ESTABLISHED)
skype      4588     1024   11u  IPv4   15436      0t0  UDP 127.0.0.1:35553 
skype      4588     1024   35u  IPv4 1859784      0t0  TCP 172.20.33.215:51348->157.56.192.193:443 (ESTABLISHED)
skype      4588     1024   36w  IPv4   17138      0t0  TCP *:28890 (LISTEN)
skype      4588     1024   45u  IPv4   17139      0t0  UDP *:28890 
skype      4588     1024   54u  IPv4   18119      0t0  TCP 172.20.33.215:54124->157.55.235.145:40030 (ESTABLISHED)
skype      4588     1024   92u  IPv4   15527      0t0  TCP 172.20.33.215:41335->91.190.218.55:12350 (ESTABLISHED)
iceweasel  9539     1024   62u  IPv4 1585820      0t0  TCP 172.20.33.215:36101->216.58.208.69:443 (ESTABLISHED)
iceweasel  9539     1024   65u  IPv4 1856332      0t0  TCP 172.20.33.215:44701->74.125.195.189:443 (ESTABLISHED)
iceweasel  9539     1024   74u  IPv4 1813863      0t0  TCP 172.20.33.215:42211->31.221.26.57:80 (ESTABLISHED)
iceweasel  9539     1024   77u  IPv4 1545135      0t0  TCP 172.20.33.215:44615->185.45.5.50:443 (ESTABLISHED)
iceweasel  9539     1024   87u  IPv4 1839953      0t0  TCP 172.20.33.215:56696->185.45.5.43:443 (ESTABLISHED)
hipchat.b 11542     1024   33u  IPv4   57796      0t0  TCP 172.20.33.215:36851->54.161.161.10:5222 (ESTABLISHED)
ssh       19536     1024    3r  IPv4 1852553      0t0  TCP 172.20.33.215:42930->172.20.33.192:22 (ESTABLISHED)
ssh       19607     1024    3r  IPv4 1744563      0t0  TCP 172.20.33.215:43054->172.20.33.192:22 (ESTABLISHED)


<firewall>
In other words, Apache port is open but it may be blocked by UNIX (pf) or Linux (iptables) firewall.
You also need to open port at firewall level. In this example, open tcp port 80 using Linux iptables
firewall tool:

$ /sbin/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT
$ /sbin/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 2033 -j ACCEPT
$ service iptables save


={============================================================================
*kt_linux_tool_038* minicom

1. Setup minicon with -s switch

$ minicom -s

Choose Serial port setup and specify below settings, come out of Serial port setup by pressing Esc
and then save configuration file.

Once saved, it will store the configuration in /etc/minicom/. To connect to your device, run minicom
with config file name:

$ minicom cisco or minicom -c on cisco

To Exit from minicom, Click Ctrl-A and Z then X.

alias mc='sudo minicom -C hmax.log -c on hmax'

<when-cannot-get-shell-prompt>
Without knowing either your set-top box or your cable, I would first try disabling hardware flow
control, since the set top probably doesn't implement it. Essentially your Linux client is waiting
for an "OK to send" signal that it will never receive because there's no physical wire in the set
  top to send it


={============================================================================
*kt_linux_tool_039* tool-mount: find rootfs

[root@HUMAX /]# mount
rootfs on / type rootfs (rw)
/dev/root on / type ext3 (rw,relatime,errors=continue,barrier=1,data=ordered)
proc on /proc type proc (rw,relatime)
...

For the / mount point, you are just told that it corresponds to /dev/root, which
is not the real device you are looking for.

Of course, you can look at the kernel command line and see on which initial root
filesystem Linux was instructed to boot (root parameter):

[root@HUMAX /]# cat /proc/cmdline 
root=/dev/sda1 rw  macaddr=28:32:C5:3F:59:4E bmem=360M@512M wakeup=REBOOT ...


={============================================================================
*kt_linux_tool_039* mount --bind and nfs

The bind mounts.

Since Linux 2.4.0 it is possible to remount part of the file hierarchy somewhere else. The call is

mount --bind olddir newdir

or shortoption
mount -B olddir newdir

or fstab entry is:
/olddir /newdir none bind

After this call the same contents is accessible in two places. One can also remount a single file
(on a single file). It's also possible to use the bind mount to create a mountpoint from a regular
directory, for example:

mount --bind foo foo

The bind mount call attaches only (part of) a single filesystem, not possible submounts. The entire
file hierarchy including submounts is attached a second place using

mount --rbind olddir newdir

or shortoption

mount -R olddir newdir

Note that the filesystem mount options will remain the same as those on the original mount point,
and cannot be changed by passing the -o option along with --bind/--rbind. The mount options can be
    changed by a separate remount command, for example:

mount --bind olddir newdir
mount -o remount,ro newdir

Note that behavior of the remount operation depends on the /etc/mtab file. The first command stores
the 'bind' flag to the /etc/mtab file and the second command reads  the flag  from  the  file.   If
you have a system without the /etc/mtab file or if you explicitly define source and target for the
remount command (then mount(8) does not read /etc/mtab), then you have to use bind flag (or option)
for the remount command too. For example:

mount --bind olddir newdir
mount -o remount,ro,bind olddir newdir


{nfs-mount}
mount -o nolock -t nfs 172.18.253.143:/home/NDS-UK/parkkt/fob/logs /mnt/temp/
mount -o nolock -t nfs 172.18.200.185:/mnt ./
mount -t nfs 172.18.253.143:/home/NDS-UK/parkkt/fob /mnt/nfs


{nfs}
sudo apt-get install nfs-kernel-server

You can configure the directories to be exported by adding them to the /etc/exports file. For
example:

/ubuntu  *(ro,sync,no_root_squash)
/home    *(rw,sync,no_root_squash)


note: You can replace * with one of the hostname formats. Make the hostname declaration as specific
as possible so unwanted systems cannot access the NFS mount.

To start the NFS server, you can run the following command at a terminal prompt:
sudo /etc/init.d/nfs-kernel-server stop/start


note: "-o xxx" makes a 'difference' and this was from a board, that is busybox.

http://lists.busybox.net/pipermail/busybox/2002-July/040613.html

// host side
/home/kpark/src-dev     *(rw,sync,no_root_squash,no_subtree_check)

[root@HUMAX /]# mount -t nfs 172.20.33.215:/home/kpark/src-dev /mnt/tmp
mount: mounting 172.20.33.215:/home/kpark/src-dev on /mnt/tmp failed: Connection refused

[root@HUMAX /]# mount -t nfs -o sync,nolock 172.20.33.215:/home/kpark/src-dev /mnt/tmp
[root@HUMAX /]# ls /mnt/tmp
DEVARCH/         GPATH            GRTAGS           GTAGS            gtags-target@    zinc-build-root/


{exportfs}
Normally  the  master  export  table is initialized with the contents of
/etc/exports and files under /etc/exports.d by invoking exportfs -a. However, a
system administrator can choose to add or delete exports without modifying
/etc/exports or files under /etc/exports.d by using the exportfs command.


={============================================================================
*kt_linux_tool_040* install

{case}
See a case that the build output, executable, does not match with the build time. In this example,
    there are 'build-root' and 'install-root' where build-root has all temporary files from the
    build and includes the final binary and 'install-root' has only the final binary. 

However, found that when do a build, a timestamp of install-root is older than the build one. So
wondered if the build system is broken and the build is not populated to the install-root.

Found that that's due to install command but not the build system.

/usr/bin/install -C {build-root}/nexus-inspect {install-root}/nexus-inspect

-C, --compare
compare each pair of source and destination files, and in some cases, do not modify the destination
at all

That is the install-root will not copied if two are the same.


={============================================================================
*kt_linux_tool_041* notify-send

wget URL && notify-send "Done" || notify-send "Failed"

notify-send --hint=int:transient:1 -i
/usr/share/icons/gnome/32x32/status/stock_dialog-warning.png 'Building
zb/DEVARCH-8092-TEMP: failure'


={============================================================================
*kt_linux_tool_042* tool-tail: print multiple files with filename

$ grep "" *-config
default-system-factory.plugin-config:1:libNickelSystemDbusClient.so createDbusSystemFactory
http-application%2Fdash%2Bxml.plugin-config:1:libNickelSystemGstreamer.so createGstSystemFactory
https-application%2Fdash%2Bxml.plugin-config:1:libNickelSystemGstreamer.so createGstSystemFactory

$ tail -n +1 *-config
==> default-proxy-config <==

==> default-system-factory.plugin-config <==
libNickelSystemDbusClient.so createDbusSystemFactory

==> http-application%2Fdash%2Bxml.plugin-config <==
libNickelSystemGstreamer.so createGstSystemFactory

==> https-application%2Fdash%2Bxml.plugin-config <==
libNickelSystemGstreamer.so createGstSystemFactory

       -n, --lines=K
              output the last K lines, instead of the last 10; or use -n +K to
              output lines starting with the Kth

<f-option>
       -f, --follow[={name|descriptor}]
              output appended data as the file grows; -f, --follow, and
              --follow=descriptor are equivalent


={============================================================================
*kt_linux_tool_044* mc

https://www.midnight-commander.org/

apt-get install mc


={============================================================================
*kt_linux_tool_045* graphbiz

sudo apt-get install graphviz

dot -Tjpeg gst-launch.PLAYING_PAUSED.dot -o gst-launch.PLAYING_PAUSED.jpg


={============================================================================
*kt_linux_tool_046* mktemp

Create  a  temporary  file  or  directory,  safely,  and  print its name.  TEMPLATE must contain at
least 3 consecutive `X's in last component.  If TEMPLATE is not specified, use tmp.XXXXXXXXXX, and
--tmpdir is implied.  Files are created u+rw, and directories u+rwx, minus umask restrictions.


$ mktemp
/tmp/tmp.eezueA0X5X


={============================================================================
*kt_linux_tool_047* tool-env

env - run a program in a modified environment

LD_PRELOAD=/usr/local/lib/libdirectfb.so:/usr/local/lib/libdirect.so:/usr/local/lib/libinit.so

env ${LD_PRELOAD} nickelmediad \
    --no-mediasettings --no-localmedialibrary \
    --no-outputmanager --no-servicelistbuilder \
    -b $BUS_NAME -f $cfg &


={============================================================================
*kt_linux_tool_048* term: gnome-terminal

{gnome-terminal}
I have just found that you can open new terminals using 'gnome-terminal'.  You can open multiple
windows and multiple tabs like this:

gnome-terminal --window --tab --window --tab --tab

<key-shortcuts>
New Tab        Shift+Ctrl+T
Close Tab      Shift+Ctrl+W

note: this is to open new terminal
New Window     Shift+Ctrl+N
Close Window   Shift+Ctrl+Q

Copy           Ctrl+Shift+C
Paste          Ctrl+Shift+V

Switch to Previous Tab     Ctrl+Page Up
Switch to Next Tab         Ctrl+Page Down

note: this is to 'move' a tab
Move Tab to the Left       Shift+Ctrl+Page Up
Move Tab to the Right      Shift+Ctrl+Page Down

Switch to Tab 1            Alt+1
Switch to Tab N            Alt+N

<page-up>
Shift+PgUp/PgDn/Home/End will scroll in gnome-terminal and Terminal.

<find>
Using the Search menu or a keyboard short-cut Shift+Ctrl+F


={============================================================================
*kt_linux_tool_049* term: terminator

https://launchpad.net/terminator

./setup.py install --record=install-files.txt

For more keyboard shortcuts and also the command line options, please see the
manpage "terminator". For configuration options, see the manpage
"terminator_config".

/home/keitee/.config/terminator/config

// from terminator_config

keybindings

<open-close>
new_tab
Open a new tab.  Default value: <Ctrl><Shift>T

close_term
Close the current terminal.  Default value: <Ctrl><Shift>W

close_window
Quit Terminator.  Default value: <Ctrl><Shift>Q

new_window
Open a new Terminator window as part of the existing process.  Default value: <Ctrl><Shift>I

new_terminator
Spawn a new instance of Terminator.  Default value: <Super>i


<switch-tab>
next_tab
Move to the next tab.  Default value: <Ctrl>Page_Down

prev_tab
Move to the previous tab.  Default value: <Ctrl>Page_Up


<switch-termial>
about terminals in a tab

go_up  
Move cursor focus to the terminal above.  Default value: <Alt>Up

go_down
Move cursor focus to the terminal below.  Default value: <Alt>Down

go_left
Move cursor focus to the terminal to the left.  Default value: <Alt>Left

go_right
Move cursor focus to the terminal to the right.  Default value: <Alt>Right


<split-terminal>
Q: how to change the size of split?. Use resize instead?

split_horiz
Split the current terminal horizontally.  Default value: <Ctrl><Shift>O

split_vert
Split the current terminal vertically.  Default value: <Ctrl><Shift>E


<resize-terminal>
resize_up
Move the parent dragbar upwards. value: <Ctrl>Up

resize_down
Move the parent dragbar downwards. value: <Ctrl>Down

resize_left
Move the parent dragbar left.

resize_right
Move the parent dragbar right.


<copy-paste>
copy   
Copy the currently selected text to the clipboard.  Default value: <Ctrl><Shift>C

paste  
Paste the current contents of the clipboard.  Default value: <Ctrl><Shift>V


<search>
Q: shows if there is match or not but not highlight or scroll.

search 
Search for text in the terminal scrollback history.  Default value: <Ctrl><Shift>F


<zoom>
toggle_zoom
Zoom/Unzoom the current terminal to fill the window.  Default value: <Ctrl><Shift>X


<reset>
reset_clear
Reset the terminal state and clear the terminal window.  Default value: <Ctrl><Shift>G


={============================================================================
*kt_linux_tool_050* tool-watch

$watch -n1 -d ccache -s

NAME
       watch - execute a program periodically, showing output fullscreen

SYNOPSIS
       watch [options] command

DESCRIPTION
       watch runs command repeatedly, displaying its output and errors (the
               first screenfull).  This allows you to watch the program output
       change over time.  By default, the program is run every 2 seconds.  By
       default, watch will run until interrupted.

OPTIONS
       -d, --differences [permanent] Highlight the differences between
       successive updates.  Option will read optional argument that changes
       highlight to be permanent, allowing to see what has changed at least once
       since first iteration.

       -n, --interval seconds Specify update interval.  The command will not
       allow quicker than 0.1 second interval, in which the smaller values are
       converted.


={============================================================================
*kt_linux_tool_051* font

Copy a font file in the directory /usr/share/fonts/truetype (for all users) or
~/.fonts (for a specific user). 

Remember to verify font's permissions on disk (777); if you save your fonts in

$ chmod -R 777 /usr/local/share/fonts

fc-list – lists fonts
fc-cache -fv – rebuilds cached list of fonts 

http://sourcefoundry.org/hack/


={============================================================================
*kt_linux_tool_051* tool-echo

-e     enable interpretation of backslash escapes

If -e is in effect, the following sequences are recognized:

\e     escape

echo -e "\E[1;31mThe commit subject does not match \"DEVARCH-xxxx: <subject>\"";

However, "\E" also works.


<busybox>
echo

    echo [-neE] [ARG...]

    Print the specified ARGs to stdout

    Options:

            -n      Suppress trailing newline
            -e      Interpret backslash-escaped characters (i.e., \t=tab)
            -E      Disable interpretation of backslash-escaped characters

echo -e "\E[1;31mThe commit subject does not match \"DEVARCH-xxxx: <subject>\"";


={============================================================================
*kt_linux_tool_051* tool-wireshark

$ sudo wireshark


filter the response to a matched HTTP request

https://ask.wireshark.org/questions/30972/filter-the-response-to-a-matched-http-request

you can do this:

  Filter for the request: http.request.uri contains "/test"
  http.request.uri contains TT.mpd

  Get the TCP stream number(s) of those frames (tcp.stream)
  Field name is tcp.stream and displayed as "Stream index:"

  Then filter for: tcp.stream eq xxx and frame contains "HTTP/1.1 200 OK" (or
      HTTP/1.0)


You can automate that with tshark and some scripting.

tshark -nr input.pcap -R 'http.request.uri contains "/test"' -T fields -e tcp.stream
Read the tcp streams with a script and create new filters based on them
tshark -nr input.pcap -R 'tcp.stream eq xxx and frame contains "HTTP/1.1 200 OK"'

See also my answer to a similar question


={============================================================================
*kt_linux_tool_051* tool-htop


# ============================================================================
#{
={============================================================================
*kt_linux_tool_100* package: apt-xxx to get package

To search package:
apt-cache search <program name>

To install a package:
sudo apt-get install tk8.5 

To remove a package:
sudo apt-get purge tk8.5 


{update-and-upgrade}
The apt-get install command is 'recommended' because it upgrades one or more
already installed packages without upgrading every package installed, whereas
the apt-get upgrade command installs the newest version of all currently
installed packages. In additon, apt-get update command must be executed before
an upgrade to resynchronize the package index files.


{update-error}
When see:
Update Error: Require Installation Of Untrusted Packages

Run manually on console

sudo apt-get upgrade xxx

The simplest way to get the required packages is using apt-get build-dep wine
respectively aptitude build-dep wine. 


={============================================================================
*kt_linux_tool_101* package: pkg-config

The pkg-config program is used to retrieve information about installed libraries  in the system.  It
is typically used to compile and link against one or more libraries.  Here is a typical usage
scenario in a Makefile:

program: program.c
   cc program.c $(pkg-config --cflags --libs gnomeui)

pkg-config retrieves information about packages from special metadata  files.  These files  are
named after the package, and has a .pc extension.

It will additionally look in the colon-separated (on Windows, semicolon-separated) list of
directories  specified  by the PKG_CONFIG_PATH environment variable.

       --cflags
              This  prints pre-processor and compile flags required to compile the packages
              on the command line, including flags for all their  dependencies.  Flags  are
              "compressed"  so that each identical flag appears only once. pkg-config exits
              with a nonzero code if it can't find metadata for one or more of the packages
              on the command line.

       --cflags-only-I
              This  prints the -I part of "--cflags". That is, it defines the header search
              path but doesn't specify anything else.

       --libs This option is identical to "--cflags", only it prints  the  link  flags.  As
              with  "--cflags",  duplicate  flags are merged (maintaining proper ordering),
              and flags for dependencies are included in the output.

       --list-all
              List all modules found in the pkg-config path.


={============================================================================
*kt_linux_tool_102* package: dpkg and install deb file

dpkg - package manager for Debian

List all packages installed

$ dpkg-query -l

List packages using a search pattern: It is possible to add a search pattern to list packages: 

$ dpkg-query -l 'ibus*'

$ dpkg --info skype-debian_4.2.0.11-1_i386.deb 

# -i, --install
$ sudo dpkg --install skype-debian_4.2.0.11-1_i386.deb

<search>
aptitude search ^wine

aptitude install "name"
aptitude remove "name"

<package-manager-database>
$ cat /etc/apt/sources.list
# 

# deb cdrom:[Debian GNU/Linux 7.7.0 _Wheezy_ - Official amd64 CD Binary-1 20141018-13:06]/ wheezy main

# deb cdrom:[Debian GNU/Linux 7.7.0 _Wheezy_ - Official amd64 CD Binary-1 20141018-13:06]/ wheezy main

deb http://mirrors.kernel.org/debian wheezy main contrib non-free
deb-src http://mirrors.kernel.org/debian wheezy main contrib non-free

deb http://security.debian.org/ wheezy/updates main
deb-src http://security.debian.org/ wheezy/updates main

# wheezy-updates, previously known as 'volatile'
# deb http://mirrors.kernel.org/debian wheezy-updates main
# deb-src http://mirrors.kernel.org/debian wheezy-updates main
# Devarch Packages
deb http://devarch-deb.dev.youview.co.uk:8080/job/DEBs/ws wheezy main contrib non-free
deb http://http.debian.net/debian/ wheezy-backports main contrib non-free
# deb http://ftp.uk.debian.org/debian wheezy main
deb http://mozilla.debian.net/ wheezy-backports iceweasel-release


={============================================================================
*kt_linux_tool_130*	cmake

={============================================================================
*kt_linux_tool_140*	cmake

<ref>
http://www.cmake.org/cmake/help/v2.8.9/cmake.html
http://stackoverflow.com/questions/6352123/multiple-directories-under-cmake

From:

./mheg
+-- CMakeLists.txt
+-- include
Â¦Â Â  +-- dbg.h
Â¦Â Â  +-- def.h
+-- main
Â¦Â Â  +-- main.c


To:

./mheg
+-- CMakeLists.txt
+-- include
Â¦Â Â  +-- dbg.h
Â¦Â Â  +-- def.h
+-- main
Â¦Â Â  +-- main.c
+-- mh5eng
Â¦Â Â  +-- CMakeLists.txt
Â¦Â Â  +-- sample.c
Â¦Â Â  +-- sample.h


The changes made to build:

../mheg/CMakeLists.txt

	 INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/mh5eng)

	 ADD_EXECUTABLE(${PROJECT_NAME}
		 main/main.c # uses sample.h
	 )

	 ADD_SUBDIRECTORY(mh5eng)

	 TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})

../mheg/mh5eng/CMakeLists.txt

	 SET(MH5ENG_HEADERS
		 sample.h
	 )

	 SET(MH5ENG_SOURCES
		 sample.c
	 )

	 ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES})

This create <libmh5eng.a> as a output


==============================================================================
*kt_linux_tool_141*	cmake: mix of c and cpp build

To compile the mix of c and cpp file, when try followings:

PROJECT(mhegproto C)

SET(MH5ENG_SOURCES
	xx.c
	mh5i_residentprogram_db.cpp
)

ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES} )

cmake do not compile cpp file although it is defined in the set variable. To make it built, must add
c++ build as below:

PROJECT(mhegproto C CXX)

From http://cmake.org/cmake/help/v2.8.8/cmake.html

project: Set a name for the entire project.

project(<projectname> [languageName1 languageName2 ... ] )

Sets the name of the project. Additionally this sets the variables <projectName>_BINARY_DIR and
<projectName>_SOURCE_DIR to the respective values.

Optionally you can specify which languages your project supports. Example languages are CXX (i.e.
C++), C, Fortran, etc. By default C and CXX are enabled. E.g. if you do not have a C++ compiler, you
can disable the check for it by explicitly listing the languages you want to support, e.g. C. By
using the special language "NONE" all checks for any language can be disabled. If a variable exists
called CMAKE_PROJECT_<projectName>_INCLUDE_FILE, the file pointed to by that variable will be
included as the last step of the project command.

note: by default? seems not.


==============================================================================
*kt_linux_tool_142*	cmake: cflags

To enable preprocessor, set -E as below.

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "-E ${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")


==============================================================================
*kt_linux_tool_143*	cmake: includes

Found that building a library in the subdir cannot find necessary includes.

root CMakeList.txt:

INCLUDE(FindPkgConfig)
pkg_check_modules(APPS_PKGS REQUIRED
	capi-appfw-application
	dlog
	edje
	elementary
	ecore-x
	evas
	utilX
	x11
	aul
	ail
)

ADD_EXECUTABLE(${PROJECT_NAME}
	main/main.c
	main/viewmgr.c
	main/view_main.c
)

ADD_SUBDIRECTORY(mh5eng)
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)

Here there is no problem to use headers from packages such as elementary but when add the same to
the one in /mh5eng then cannot find headers. When looked at flags used to build mh5eng, there is no
necessary -I. This is the same when add pkg_check_moudles in the mh5eng/CMakeList.txt. Thing is
build faild to update flags as expected.

The finding is that when move mh5eng then it builds without adding pkg_check_moudles in the
mh5eng/CMakeList.txt

...
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)
ADD_SUBDIRECTORY(mh5eng) [KT] moved to here

That suggests that where to put is important and not sure it is a GBS(git build system) or cmake
itself problem. 


Found that this error still happens when build cpp file and the solution is:

From:
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

To:
MESSAGE("KT >>>>>PKGS_LDFLAGS>>>>>" : ${APPS_PKGS_CFLAGS})
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")
SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${EXTRA_CFLAGS}")

Turns out that PKGS_CFLAGS will have necessary includes depending on pkg selection.


={============================================================================
*kt_linux_tool_144* cmake: link group

To solve {cyclic-dependencies} in link, can use this:

From defining each: 
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
...

To use group:
TARGET_LINK_LIBRARIES(${PROJECT_NAME} -Wl,--start-group mhdebug pfm mh5eng mh5dec mhv mah
-Wl,--end-group ${APPS_PKGS_LDFLAGS})

 On 05/19/2011 11:11 AM, Anton Sibilev wrote:
> Hello!
> I'm wondering how I can use "--start-group archives --end-group"
> linker flags with "Unix Makefiles".
> May be somebody know the right way?

You might specify these flags immediately in TARGET_LINK_LIBRARIES():

CMAKE_MINIMUM_REQUIRED(VERSION 2.8 FATAL_ERROR)
PROJECT(LINKERGROUPS C)
SET(CMAKE_VERBOSE_MAKEFILE ON)
FILE(WRITE ${CMAKE_BINARY_DIR}/f.c "void f(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g1.c "void g1(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g2.c "void g2(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/h.c "void h(void){}\n")
ADD_LIBRARY(f STATIC f.c)
ADD_LIBRARY(g1 STATIC g1.c)
ADD_LIBRARY(g2 STATIC g2.c)
ADD_LIBRARY(h STATIC h.c)
FILE(WRITE ${CMAKE_BINARY_DIR}/main.c "int main(void){return 0;}\n")
ADD_EXECUTABLE(main main.c)
TARGET_LINK_LIBRARIES(main f -Wl,--start-group g1 g2 -Wl,--end-group h)

However, do you really need these flags? Refer to the target properties
[IMPORTED_]LINK_INTERFACE_MULTIPLICITY[_<CONFIG>] and the documentation
of TARGET_LINK_LIBRARIES().

Regards,

Michael

LINK_INTERFACE_MULTIPLICITY

Repetition count for STATIC libraries with cyclic dependencies.

When linking to a STATIC library target with cyclic dependencies the linker may need to scan more
than once through the archives in the strongly connected component of the dependency graph. CMake by
default constructs the link line so that the linker will scan through the component at least twice.
This property specifies the minimum number of scans if it is larger than the default. CMake uses the
largest value specified by any target in a component.


={============================================================================
*kt_linux_tool_150* autoconf:

http://www.gnu.org/software/autoconf/autoconf.html
http://www.gnu.org/software/autoconf/manual/index.html

Autoconf is a tool for 'producing' shell scripts that automatically configure
software source code packages to adapt to many kinds of Posix-like systems. The
configuration scripts produced by Autoconf are independent of Autoconf when they
are run, so their users do not need to have Autoconf.

The configuration scripts produced by Autoconf require no manual user
intervention when run; they do not normally even need an argument specifying the
system type. Instead, they individually test for the presence of each feature
that the software package they are for might need. (Before each check, they
    print a one-line message stating what they are checking for, so the user
    doesn't get too bored while waiting for the script to finish.) As a result,
they deal well with systems that are hybrids or customized from the more common
  Posix variants. There is no need to maintain files that list the features
  supported by each release of each variant of Posix.

For each software package that Autoconf is used with, it creates a configuration
script from a template file that lists the system features that the package
needs or can use. After the shell code to recognize and respond to a system
feature has been written, Autoconf allows it to be shared by many software
packages that can use (or need) that feature. If it later turns out that the
shell code needs adjustment for some reason, it needs to be changed in only one
place; all of the configuration scripts can be regenerated automatically to take
advantage of the updated code.

Those who do not understand Autoconf are condemned to reinvent it, poorly. The
primary goal of Autoconf is making the user's life easier; making the
maintainer's life easier is only a secondary goal. Put another way, the primary
goal is not to make the generation of configure automatic for package
maintainers (although patches along that front are welcome, since package
    maintainers form the user base of Autoconf); rather, the goal is to make
configure painless, portable, and predictable for the end user of each
autoconfiscated package. And to this degree, Autoconf is highly successful at
its goal — most complaints to the Autoconf list are about difficulties in
writing Autoconf input, and not in the behavior of the resulting configure. Even
packages that don't use Autoconf will generally provide a configure script, and
the most common complaint about these alternative home-grown scripts is that
they fail to meet one or more of the GNU Coding Standards (see Configuration)
that users have come to expect from Autoconf-generated configure scripts.

Autoconf requires GNU M4 version 1.4.6 or later in order to generate the
scripts. It uses features that some versions of M4, including GNU M4 1.3, do not
have. Autoconf works better with GNU M4 version 1.4.14 or later, though this is
not required. 


3.1 Writing configure.ac

To produce a configure script for a software package, create a file called
configure.ac that contains invocations of the Autoconf macros that test the
system features your package needs or can use. Autoconf macros already exist to
check for many features; see Existing Tests, for their descriptions. For most
other features, you can use Autoconf template macros to produce custom checks;
see Writing Tests, for information about them. 
  
Previous versions of Autoconf promoted the name configure.in, which is somewhat
ambiguous (the tool needed to process this file is not described by its
    extension), and introduces a slight confusion with config.h.in and so on
(for which ‘.in’ means “to be processed by configure”). Using configure.ac is
now preferred.


3.4 Using autoconf to Create configure

To create 'configure' from configure.ac, run the autoconf program with no
arguments. autoconf processes configure.ac with the M4 macro processor, using
the Autoconf macros. If you give autoconf an argument, it reads that file
instead of configure.ac and writes the configuration script to the standard
output instead of to configure. If you give autoconf the argument -, it reads
from the standard input instead of configure.ac and writes the configuration
script to the standard output.

The Autoconf macros are defined in several files. Some of the files are
distributed with Autoconf; autoconf reads them first. Then it looks for the
optional file acsite.m4 in the directory that contains the distributed Autoconf
macro files, and for the optional file aclocal.m4 in the current directory.
Those files can contain your site's or the package's own Autoconf macro
definitions (see Writing Autoconf Macros, for more information). If a macro is
defined in more than one of the files that autoconf reads, the last definition
it reads overrides the earlier ones.

autoconf accepts the following options: 


={============================================================================
*kt_linux_tool_150* autoconf: error on LDFLAGS

configure: using LDFLAGS:
-L/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/lib
-L/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/oss/lib
-Wl,--as-needed
-Wl,-rpath-link,/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/lib
-Wl,-rpath-link,/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/oss/lib
-L/data/ws/zinc-install-root/release/humax-dtr_t4000/lib64
-L/opt/oem-staging/humax-dtr_t4000/usr/local/lib  -mcpu=cortex-a15
-mtune=cortex-a15 -mfloat-abi=softfp -mfpu=neon-vfpv4 -D__arm__
-L/opt/oem-staging/humax-dtr_t4000/usr/local/lib
-Wl,-rpath-link,/opt/oem-staging/humax-dtr_t4000/usr/local/lib

configure: LDFLAGS error: LDFLAGS may only be used to specify linker flags, not
macro definitions. Use CPPFLAGS for: -D__arm__


={============================================================================
*kt_linux_tool_150* automake:

https://www.gnu.org/software/automake/

https://www.gnu.org/software/automake/manual/automake.html
https://www.gnu.org/software/automake/manual/automake.txt


1 Introduction

Automake is a tool for automatically generating Makefile.ins 'from' files called
Makefile.am. Each Makefile.am is basically a series of make variable
definitions, with rules being thrown in occasionally. The generated Makefile.ins
are compliant with the GNU Makefile standards

Automake does constrain a project in 'certain' ways; for instance, it 'assumes'
that the project uses Autoconf (see Introduction in The Autoconf Manual), and
enforces certain restrictions on the configure.ac contents.

2.2.1 Basic Installation

make check 

causes the package's tests to be run. This step is not mandatory, but it is
often good to make sure the programs that have been built behave as they should,
before you decide to install them. Our example does not contain any tests, so
  running make check is a no-op.

make install

After everything has been built, and maybe tested, it is time to install it on
the system. That means copying the programs, libraries, header files, scripts,
and other data files from the source directory to their final destination on the
  system. The command make install will do that. 

make installcheck

A last and optional step is to run make installcheck. This command may run tests
on the installed files. make check tests the files in the source tree, while
make installcheck tests their installed copies. The tests run by the latter can
be different from those run by the former. For instance, there are tests that
cannot be run in the source tree. Conversely, some packages are set up so that
make installcheck will run the very same tests as make check, only on different
files (non-installed vs. installed). It can make a difference, for instance when
the source tree's layout is different from that of the installation. Furthermore
it may help to diagnose an incomplete installation.

Presently most packages do not have any installcheck tests because the existence
of installcheck is little known, and its usefulness is neglected. Our little toy
package is no better: make installcheck does nothing. 


={============================================================================
*kt_linux_tool_150* automake: standard targets

2.2.2 Standard Makefile Targets

So far we have come across four ways to run make in the GNU Build System: make,
make check, make install, and make installcheck. The words check, install, and
  installcheck, passed as arguments to make, are called targets. make is a
  shorthand for make all, all being the default target in the GNU Build System.

Here is a list of the most useful targets that the GNU Coding Standards specify.

make all
    Build programs, libraries, documentation, etc. (same as make). 

make install
    Install what needs to be installed, copying the files from the package’s
    tree to system-wide directories.  make install-strip

    Same as make install, then strip debugging symbols. Some users like to trade
    space for useful bug reports... 

make uninstall
    The opposite of make install: erase the installed files. (This needs to be
        run from the same build tree that was installed.) 

make clean
    Erase from the build tree the files built by make all. 

make distclean
    Additionally erase anything ./configure created. 

make check
    Run the test suite, if any. 

make installcheck
    Check the installed programs or libraries, if supported. 

make dist
    Recreate package-version.tar.gz from all the source files. 


From GNU Coding Standards

http://www.gnu.org/prep/standards/html_node/Standard-Targets.html#Standard-Targets

7.2.6 Standard Targets for Users

All GNU programs should have the following targets in their Makefiles:

‘all’

    Compile the entire program. This should be the default target. This target
    need not rebuild any documentation files; Info files should normally be
    included in the distribution, and DVI (and other documentation format) files
    should be made only when explicitly asked for.

    By default, the Make rules should compile and link with ‘-g’, so that
    executable programs have debugging symbols. Otherwise, you are essentially
    helpless in the face of a crash, and it is often far from easy to reproduce
    with a fresh build.  
    
‘install’

    Compile the program and copy the executables, libraries, and so on to the
    file names where they should reside for actual use. If there is a simple
    test to verify that a program is properly installed, this target should run
    that test.

    Do not strip executables when installing them. This helps eventual debugging
    that may be needed later, and nowadays disk space is cheap and dynamic
    loaders typically ensure debug sections are not loaded during normal
    execution. Users that need stripped binaries may invoke the install-strip
    target to do that.

    If possible, write the install target rule so that it does not modify
    anything in the directory where the program was built, provided ‘make all’
    has just been done. This is convenient for building the program under one
    user name and installing it under another.

    The commands should create all the directories in which files are to be
    installed, if they don’t already exist. This includes the directories
    specified as the values of the variables prefix and exec_prefix, as well as
    all subdirectories that are needed. One way to do this is by means of an
    installdirs target as described below.

    Use ‘-’ before any command for installing a man page, so that make will
    ignore any errors. This is in case there are systems that don’t have the
    Unix man page documentation system installed.

    The way to install Info files is to copy them into $(infodir) with
    $(INSTALL_DATA) (see Command Variables), and then run the install-info
    program if it is present. install-info is a program that edits the Info dir
    file to add or update the menu entry for the given Info file; it is part of
    the Texinfo package.

    Here is a sample rule to install an Info file that also tries to handle some
    additional situations, such as install-info not being present.

    do-install-info: foo.info installdirs
            $(NORMAL_INSTALL)
    # Prefer an info file in . to one in srcdir.
            if test -f foo.info; then d=.; \
             else d="$(srcdir)"; fi; \
            $(INSTALL_DATA) $$d/foo.info \
              "$(DESTDIR)$(infodir)/foo.info"
    # Run install-info only if it exists.
    # Use 'if' instead of just prepending '-' to the
    # line so we notice real errors from install-info.
    # Use '$(SHELL) -c' because some shells do not
    # fail gracefully when there is an unknown command.
            $(POST_INSTALL)
            if $(SHELL) -c 'install-info --version' \
               >/dev/null 2>&1; then \
              install-info --dir-file="$(DESTDIR)$(infodir)/dir" \
                           "$(DESTDIR)$(infodir)/foo.info"; \
            else true; fi

    When writing the install target, you must classify all the commands into
      three categories: normal ones, pre-installation commands and
      post-installation commands. See Install Command Categories.

‘install-html’
‘install-dvi’
‘install-pdf’
‘install-ps’

    These targets install documentation in formats other than Info; they’re
    intended to be called explicitly by the person installing the package, if
    that format is desired. GNU prefers Info files, so these must be installed
    by the install target.

    When you have many documentation files to install, we recommend that you
    avoid collisions and clutter by arranging for these targets to install in
    subdirectories of the appropriate installation directory, such as htmldir.
    As one example, if your package has multiple manuals, and you wish to
    install HTML documentation with many files (such as the “split” mode output
        by makeinfo --html), you’ll certainly want to use subdirectories, or two
    nodes with the same name in different manuals will overwrite each other.

    Please make these install-format targets invoke the commands for the format
    target, for example, by making format a dependency.

‘uninstall’

    Delete all the installed files—the copies that the ‘install’ and ‘install-*’
    targets create.

    This rule should not modify the directories where compilation is done, only
    the directories where files are installed.

    The uninstallation commands are divided into three categories, just like the
    installation commands. See Install Command Categories.  
    
‘install-strip’

    Like install, but strip the executable files while installing them. In
    simple cases, this target can use the install target in a simple way:

    install-strip:
            $(MAKE) INSTALL_PROGRAM='$(INSTALL_PROGRAM) -s' \
                    install

    But if the package installs scripts as well as real executables, the
    install-strip target can’t just refer to the install target; it has to strip
    the executables but not the scripts.

    install-strip should not strip the executables in the build directory which
    are being copied for installation. It should only strip the copies that are
    installed.

    Normally we do not recommend stripping an executable unless you are sure the
    program has no bugs. However, it can be reasonable to install a stripped
    executable for actual execution while saving the unstripped executable
    elsewhere in case there is a bug.

‘clean’

    Delete all files in the current directory that are normally created by
    building the program. Also delete files in other directories if they are
    created by this makefile. However, don’t delete the files that record the
    configuration. Also preserve files that could be made by building, but
    normally aren’t because the distribution comes with them. There is no need
    to delete parent directories that were created with ‘mkdir -p’, since they
    could have existed anyway.

    Delete .dvi files here if they are not part of the distribution.

‘distclean’

    Delete all files in the current directory (or created by this makefile) that
    are created by configuring or building the program. If you have unpacked the
    source and built the program without creating any other files, ‘make
    distclean’ should leave only the files that were in the distribution.
    However, there is no need to delete parent directories that were created
    with ‘mkdir -p’, since they could have existed anyway.

‘mostlyclean’

    Like ‘clean’, but may refrain from deleting a few files that people normally
    don’t want to recompile. For example, the ‘mostlyclean’ target for GCC does
    not delete libgcc.a, because recompiling it is rarely necessary and takes a
    lot of time.  

‘maintainer-clean’

    Delete almost everything that can be reconstructed with this Makefile. This
    typically includes everything deleted by distclean, plus more: C source
    files produced by Bison, tags tables, Info files, and so on.

    The reason we say “almost everything” is that running the command ‘make
    maintainer-clean’ should not delete configure even if configure can be
    remade using a rule in the Makefile. More generally, ‘make maintainer-clean’
    should not delete anything that needs to exist in order to run configure and
    then begin to build the program. Also, there is no need to delete parent
    directories that were created with ‘mkdir -p’, since they could have existed
    anyway. These are the only exceptions; maintainer-clean should delete
    everything else that can be rebuilt.

    The ‘maintainer-clean’ target is intended to be used by a maintainer of the
    package, not by ordinary users. You may need special tools to reconstruct
    some of the files that ‘make maintainer-clean’ deletes. Since these files
    are normally included in the distribution, we don’t take care to make them
    easy to reconstruct. If you find you need to unpack the full distribution
    again, don’t blame us.

    To help make users aware of this, the commands for the special
    maintainer-clean target should start with these two:

    @echo 'This command is intended for maintainers to use; it'
    @echo 'deletes files that may need special tools to rebuild.'

‘TAGS’

    Update a tags table for this program.

‘info’

    Generate any Info files needed. The best way to write the rules is as
    follows:

    info: foo.info

    foo.info: foo.texi chap1.texi chap2.texi
            $(MAKEINFO) $(srcdir)/foo.texi

    You must define the variable MAKEINFO in the Makefile. It should run the
    makeinfo program, which is part of the Texinfo distribution.

    Normally a GNU distribution comes with Info files, and that means the Info
    files are present in the source directory. Therefore, the Make rule for an
    info file should update it in the source directory. When users build the
    package, ordinarily Make will not update the Info files because they will
    already be up to date.

‘dvi’
‘html’
‘pdf’
‘ps’

    Generate documentation files in the given format. These targets should
    always exist, but any or all can be a no-op if the given output format
    cannot be generated. These targets should not be dependencies of the all
    target; the user must manually invoke them.

    Here’s an example rule for generating DVI files from Texinfo:

    dvi: foo.dvi

    foo.dvi: foo.texi chap1.texi chap2.texi
            $(TEXI2DVI) $(srcdir)/foo.texi

    You must define the variable TEXI2DVI in the Makefile. It should run the
    program texi2dvi, which is part of the Texinfo distribution. (texi2dvi uses
        TeX to do the real work of formatting. TeX is not distributed with
        Texinfo.) Alternatively, write only the dependencies, and allow GNU make
    to provide the command.

    Here’s another example, this one for generating HTML from Texinfo:

    html: foo.html

    foo.html: foo.texi chap1.texi chap2.texi
            $(TEXI2HTML) $(srcdir)/foo.texi

    Again, you would define the variable TEXI2HTML in the Makefile; for example,
  it might run makeinfo --no-split --html (makeinfo is part of the Texinfo
      distribution).

‘dist’

    Create a distribution tar file for this program. The tar file should be set
    up so that the file names in the tar file start with a subdirectory name
    which is the name of the package it is a distribution for. This name can
    include the version number.

    For example, the distribution tar file of GCC version 1.40 unpacks into a
    subdirectory named gcc-1.40.

    The easiest way to do this is to create a subdirectory appropriately named,
    use ln or cp to install the proper files in it, and then tar that
      subdirectory.

    Compress the tar file with gzip. For example, the actual distribution file
    for GCC version 1.40 is called gcc-1.40.tar.gz. It is ok to support other
      free compression formats as well.

    The dist target should explicitly depend on all non-source files that are in
    the distribution, to make sure they are up to date in the distribution. See
    Making Releases.

‘check’

    Perform self-tests (if any). The user must build the program before running
    the tests, but need not install the program; you should write the self-tests
    so that they work when the program is built but not installed. 

The following targets are suggested as conventional names, for programs in which
they are useful.

installcheck

    Perform installation tests (if any). The user must build and install the
    program before running the tests. You should not assume that $(bindir) is in
    the search path.  
    
installdirs

    It’s useful to add a target named ‘installdirs’ to create the directories
    where files are installed, and their parent directories. There is a script
    called mkinstalldirs which is convenient for this; you can find it in the
    Gnulib package. You can use a rule like this:

    # Make sure all installation directories (e.g. $(bindir))
    # actually exist by making them if necessary.
    installdirs: mkinstalldirs
            $(srcdir)/mkinstalldirs $(bindir) $(datadir) \
                                    $(libdir) $(infodir) \
                                    $(mandir)

    or, if you wish to support DESTDIR (strongly encouraged),

    # Make sure all installation directories (e.g. $(bindir))
    # actually exist by making them if necessary.
    installdirs: mkinstalldirs
            $(srcdir)/mkinstalldirs \
                $(DESTDIR)$(bindir) $(DESTDIR)$(datadir) \
                $(DESTDIR)$(libdir) $(DESTDIR)$(infodir) \
                $(DESTDIR)$(mandir)

    This rule should not modify the directories where compilation is done. It
    should do nothing but create installation directories. 


={============================================================================
*kt_linux_tool_150* automake: directory variable

2.2.3 Standard Directory Variables

The GNU Coding Standards also specify a hierarchy of variables to denote
installation directories. Some of these are:

Directory variable	Default value
prefix	/usr/local
  exec_prefix	${prefix}
    bindir	${exec_prefix}/bin
    libdir	${exec_prefix}/lib
    ...
  includedir	${prefix}/include
  datarootdir	${prefix}/share
    datadir	${datarootdir}
    mandir	${datarootdir}/man
    infodir	${datarootdir}/info
    docdir	${datarootdir}/doc/${PACKAGE}


2.2.6 Parallel Build Trees (a.k.a. VPATH Builds)

The GNU Build System distinguishes 'two' trees: the 'source' tree, and the
'build' tree.

The source tree is rooted in the directory containing configure. It contains all
the sources files (those that are distributed), and may be arranged using
several subdirectories.

The build tree is rooted in the directory in which 'configure' was 'run', and is
populated with all object files, programs, libraries, and other derived files
built from the sources (and hence not distributed). The build tree usually has
the same subdirectory layout as the source tree; its subdirectories are created
automatically by the build system.

If configure is executed in its own directory, the source and build trees are
combined: derived files are constructed in the same directories as their
sources. This was the case in our first installation example (see Basic
    Installation).

A common request from users is that they want to confine all derived files to a
single directory, to keep their source directories uncluttered. Here is how we
could run configure to build everything in a subdirectory called build/.

~ % tar zxf ~/amhello-1.0.tar.gz
~ % cd amhello-1.0
~/amhello-1.0 % mkdir build && cd build

~/amhello-1.0/build % ../configure    note: see run configure in different dir
...
~/amhello-1.0/build % make
...

These setups, where source and build trees are different, are often called
parallel builds or VPATH builds. The expression parallel build is misleading:
the word parallel is a reference to the way the build tree shadows the source
tree, it is not about some concurrency in the way build commands are run. For
this reason we refer to such setups using the name VPATH builds in the
following. VPATH is the name of the make feature used by the Makefiles to allow
these builds (see VPATH Search Path for All Prerequisites in The GNU Make
    Manual).

VPATH builds have other interesting uses. One is to build the same sources with
'multiple' configurations. For instance:

~ % tar zxf ~/amhello-1.0.tar.gz
~ % cd amhello-1.0
~/amhello-1.0 % mkdir debug optim && cd debug
~/amhello-1.0/debug % ../configure CFLAGS='-g -O0'
...
~/amhello-1.0/debug % make
...
~/amhello-1.0/debug % cd ../optim
~/amhello-1.0/optim % ../configure CFLAGS='-O3 -fomit-frame-pointer'
...
~/amhello-1.0/optim % make
...


2.2.8 Cross-Compilation

To cross-compile is to build on one platform a binary that will run on another
platform. When speaking of cross-compilation, it is important to distinguish
between the build platform on which the compilation is performed, and the host
platform on which the resulting executable is expected to run. The following
configure options are used to specify each of them:

--build=build

    The system on which the package is built. 

--host=host

    The system where built programs and libraries will run. 

When the --host is used, configure will search for the cross-compiling suite for
this platform. Cross-compilation tools commonly have their target architecture
as prefix of their name. For instance my cross-compiler for MinGW32 has its
binaries called i586-mingw32msvc-gcc, i586-mingw32msvc-ld, i586-mingw32msvc-as,
         etc.

Here is how we could build amhello-1.0 for i586-mingw32msvc on a GNU/Linux PC.

~/amhello-1.0 % ./configure --build i686-pc-linux-gnu --host i586-mingw32msvc
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking for i586-mingw32msvc-strip... i586-mingw32msvc-strip
checking for i586-mingw32msvc-gcc... i586-mingw32msvc-gcc
checking for C compiler default output file name... a.exe
checking whether the C compiler works... yes
checking whether we are cross compiling... yes
checking for suffix of executables... .exe
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether i586-mingw32msvc-gcc accepts -g... yes
checking for i586-mingw32msvc-gcc option to accept ANSI C...
...
~/amhello-1.0 % make
...
~/amhello-1.0 % cd src; file hello.exe
hello.exe: MS Windows PE 32-bit Intel 80386 console executable not relocatable


The --host and --build options are usually all we need for cross-compiling. The
only exception is if the package being built is itself a cross-compiler: we need
a third option to specify its target architecture.

--target=target

    When building compiler tools: the system for which the tools will create
    output. 

For instance when installing GCC, the GNU Compiler Collection, we can use
--target=target to specify that we want to build GCC as a cross-compiler for
target. Mixing --build and --target, we can actually cross-compile a
cross-compiler; such a three-way cross-compilation is known as a Canadian cross. 


2.2.10 Building Binary Packages Using DESTDIR

The GNU Build System's make install and make uninstall interface does not
exactly fit the needs of a system administrator who has to deploy and upgrade
packages on lots of hosts. In other words, the GNU Build System does not replace
a package manager.

Such package managers usually need to know which files have been installed by a
package, so a mere make install is inappropriate.

The DESTDIR variable can be used to perform a staged installation. The package
should be configured as if it was going to be installed in its final location
(e.g., --prefix /usr), but when running make install, the DESTDIR should be set
to the absolute name of a directory into which the installation will be
diverted. From this directory it is easy to review which files are being
installed where, and finally copy them to their final location by some means. 


2.2.12 Automatic Dependency Tracking

Dependency tracking is performed as a side-effect of compilation. Each time the
build system compiles a source file, it computes its list of dependencies (in C
    these are the header files included by the source being compiled). Later,
any time make is run and a dependency appears to have changed, the dependent
  files will be rebuilt.

Automake generates code for automatic dependency tracking by default, unless the
developer chooses to override it; for more information, see Dependencies.

When configure is executed, you can see it probing each compiler for the
dependency mechanism it supports (several mechanisms can be used):

~/amhello-1.0 % ./configure --prefix /usr
...
checking dependency style of gcc... gcc3
...

Because dependencies are only computed as a side-effect of the compilation, no
dependency information exists the first time a package is built. This is OK
because all the files need to be built anyway: make does not have to decide
which files need to be rebuilt. 


={============================================================================
*kt_linux_tool_150* automake: distcheck

2.2.11 Preparing Distributions

We have already mentioned make dist. This target collects all your source files
and the necessary parts of the build system to create a tarball named
package-version.tar.gz.

Another, more useful command is make distcheck. The distcheck target constructs
package-version.tar.gz just as well as dist, but it additionally ensures 'most'
of the use cases presented so far work:

-. It attempts a full compilation of the package (see Basic Installation),
unpacking the newly constructed tarball, running make, make check, make install,
as well as make installcheck, and even make dist,

-. it tests VPATH builds with read-only source tree (see VPATH Builds),

-. it makes sure make clean, make distclean, and make uninstall do not omit any
  file (see Standard Targets),

-. and it checks that DESTDIR installations work (see DESTDIR). 

All of these actions are performed in a 'temporary' directory, so that no root
privileges are required. 

Please note that the exact location and the exact structure of such a
subdirectory (where the extracted sources are placed, how the temporary build
    and install directories are named and how deeply they are nested, etc.) is
to be considered an implementation detail, which can change at any time; so do
not rely on it.

Releasing a package that fails make distcheck means that one of the scenarios we
presented will not work and some users will be disappointed. 

Therefore it is a good 'practice' to release a package only after a successful
make distcheck. This of course does not imply that the package will be flawless,
but at least it will prevent some of the embarrassing errors you may find in
  packages released by people who have never heard about distcheck (like DESTDIR
      not working because of a typo, or a distributed file being erased by make
      clean, or even VPATH builds not working).

See Creating amhello, to recreate amhello-1.0.tar.gz using make distcheck. See
Checking the Distribution, for more information about distcheck. 


14.4 Checking the Distribution

Automake also 'generates' a distcheck rule that can be of help to ensure that a
given distribution will actually work. Simplifying a bit, we can say this rule
first makes a distribution, and then, operating from it, takes the following
steps:

-. tries to do a VPATH build (see VPATH Builds), with the srcdir and all its
content made read-only;

-. runs the test suite (with make check) on this fresh build;

-. installs the package in a temporary directory (with make install), and tries
  runs the test suite on the resulting installation (with make installcheck);

-. checks that the package can be correctly uninstalled (by make uninstall) and
  cleaned (by make distclean);

-. finally, makes another tarball to ensure the distribution is self-contained. 


distcheck-hook

If the distcheck-hook rule is defined in your top-level Makefile.am, then it
will be invoked by distcheck after the new distribution has been unpacked, but
before the unpacked copy is configured and built. Your distcheck-hook can do
almost anything, though as always caution is advised. Generally this hook is
used to check for potential distribution errors not caught by the standard
mechanism. Note that distcheck-hook as well as AM_DISTCHECK_CONFIGURE_FLAGS and
DISTCHECK_CONFIGURE_FLAGS are not honored in a subpackage Makefile.am, but the
flags from AM_DISTCHECK_CONFIGURE_FLAGS and DISTCHECK_CONFIGURE_FLAGS are passed
down to the configure script of the subpackage. 


={============================================================================
*kt_linux_tool_150* automake: example

2.4 A Small Hello World

In this section we recreate the amhello-1.0 package from scratch. The first
subsection shows how to call the Autotools to instantiate the GNU Build System,
while the second explains the meaning of the configure.ac and Makefile.am files
  read by the Autotools. 


2.4.1 Creating amhello-1.0.tar.gz

Create the following files in an empty directory.

    src/main.c is the source file for the hello program. We store it in the src/
    subdirectory, because later, when the package evolves, it will ease the
    addition of a man/ directory for man pages, a data/ directory for data
    files, etc.

    <1>
    ~/amhello % cat src/main.c
    #include <config.h>
    #include <stdio.h>

    int
    main (void)
    {
      puts ("Hello World!");
      puts ("This is " PACKAGE_STRING ".");
      return 0;
    }

    <2>
    README contains some very limited documentation for our little package.

    ~/amhello % cat README
    This is a demonstration package for GNU Automake.
    Type 'info Automake' to read the Automake manual.

    <3>
    Makefile.am and src/Makefile.am contain Automake 'instructions' for these
    two directories.

    ~/amhello % cat src/Makefile.am
    bin_PROGRAMS = hello
    hello_SOURCES = main.c

    <4>
    ~/amhello % cat Makefile.am
    SUBDIRS = src
    dist_doc_DATA = README

    <5>
    Finally, configure.ac contains Autoconf instructions to create the configure
    script.

    ~/amhello % cat configure.ac
    AC_INIT([amhello], [1.0], [bug-automake@gnu.org])
    AM_INIT_AUTOMAKE([-Wall -Werror foreign])
    AC_PROG_CC
    AC_CONFIG_HEADERS([config.h])
    AC_CONFIG_FILES([
     Makefile
     src/Makefile
    ])
    AC_OUTPUT

<6>
Once you have these five files, it is time to run the Autotools to instantiate
the build system. Do this using the autoreconf command as follows:

~/amhello % autoreconf --install
configure.ac: installing './install-sh'
configure.ac: installing './missing'
configure.ac: installing './compile'
src/Makefile.am: installing './depcomp'

At this point the build system is complete. 

In addition to the three scripts mentioned in its output, you can see that
autoreconf 'created' four other files: configure, config.h.in, Makefile.in, and
src/Makefile.in. The latter three files are templates that will be adapted to
the system by configure under the names config.h, Makefile, and src/Makefile.


$ ls -alR
.:
total 36
drwxr-xr-x 3 kpark kpark  4096 Sep 24 13:47 .
drwxr-xr-x 8 kpark kpark  4096 Sep 23 17:14 ..
-rw-r--r-- 1 kpark kpark   188 Sep 24 13:47 configure.ac
-rw-r--r-- 1 kpark kpark    36 Sep 23 15:39 Makefile.am
-rw-r--r-- 1 kpark kpark    56 Sep 23 15:36 README
drwxr-xr-x 2 kpark kpark  4096 Sep 23 15:39 src

./src:
total 16
drwxr-xr-x 2 kpark kpark 4096 Sep 23 15:39 .
drwxr-xr-x 3 kpark kpark 4096 Sep 24 13:47 ..
-rw-r--r-- 1 kpark kpark  133 Sep 23 15:35 main.c
-rw-r--r-- 1 kpark kpark   45 Sep 23 15:39 Makefile.am


~/amhello % autoreconf --install

$ ls -alR
.:
total 292
drwxr-xr-x 4 kpark kpark   4096 Sep 24 13:49 .
drwxr-xr-x 8 kpark kpark   4096 Sep 23 17:14 ..
-rw-r--r-- 1 kpark kpark  34939 Sep 24 13:49 aclocal.m4
drwxr-xr-x 2 kpark kpark   4096 Sep 24 13:49 autom4te.cache
-rw-r--r-- 1 kpark kpark    625 Sep 24 13:49 config.h.in
-rwxr-xr-x 1 kpark kpark 139386 Sep 24 13:49 configure
-rw-r--r-- 1 kpark kpark    188 Sep 24 13:47 configure.ac
-rwxr-xr-x 1 kpark kpark  20899 Sep 24 13:49 depcomp
-rwxr-xr-x 1 kpark kpark  13998 Sep 24 13:49 install-sh
-rw-r--r-- 1 kpark kpark     36 Sep 23 15:39 Makefile.am
-rw-r--r-- 1 kpark kpark  17822 Sep 24 13:49 Makefile.in
-rwxr-xr-x 1 kpark kpark  10346 Sep 24 13:49 missing
-rw-r--r-- 1 kpark kpark     56 Sep 23 15:36 README
drwxr-xr-x 2 kpark kpark   4096 Sep 24 13:49 src

./autom4te.cache:
total 352
drwxr-xr-x 2 kpark kpark   4096 Sep 24 13:49 .
drwxr-xr-x 4 kpark kpark   4096 Sep 24 13:49 ..
-rw-r--r-- 1 kpark kpark 139847 Sep 24 13:49 output.0
-rw-r--r-- 1 kpark kpark 139847 Sep 24 13:49 output.1
-rw-r--r-- 1 kpark kpark   6687 Sep 24 13:49 requests
-rw-r--r-- 1 kpark kpark  34070 Sep 24 13:49 traces.0
-rw-r--r-- 1 kpark kpark  20296 Sep 24 13:49 traces.1

./src:
total 32
drwxr-xr-x 2 kpark kpark  4096 Sep 24 13:49 .
drwxr-xr-x 4 kpark kpark  4096 Sep 24 13:49 ..
-rw-r--r-- 1 kpark kpark   133 Sep 23 15:35 main.c
-rw-r--r-- 1 kpark kpark    45 Sep 23 15:39 Makefile.am
-rw-r--r-- 1 kpark kpark 14732 Sep 24 13:49 Makefile.in


Let's do this:

<7>
~/amhello % ./configure
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for gawk... no
checking for mawk... mawk
checking whether make sets $(MAKE)... yes
...

configure: creating ./config.status
config.status: creating Makefile ~
config.status: creating src/Makefile
config.status: creating config.h
config.status: executing depfiles commands


$ ls -alR
.:
total 364
drwxr-xr-x 4 kpark kpark   4096 Sep 24 13:57 .
drwxr-xr-x 8 kpark kpark   4096 Sep 23 17:14 ..
-rw-r--r-- 1 kpark kpark  34939 Sep 24 13:49 aclocal.m4
drwxr-xr-x 2 kpark kpark   4096 Sep 24 13:49 autom4te.cache
-rw-r--r-- 1 kpark kpark    774 Sep 24 13:57 config.h ~
-rw-r--r-- 1 kpark kpark    625 Sep 24 13:49 config.h.in
-rw-r--r-- 1 kpark kpark   8255 Sep 24 13:57 config.log
-rwxr-xr-x 1 kpark kpark  32599 Sep 24 13:57 config.status
-rwxr-xr-x 1 kpark kpark 139386 Sep 24 13:49 configure
-rw-r--r-- 1 kpark kpark    188 Sep 24 13:47 configure.ac
-rwxr-xr-x 1 kpark kpark  20899 Sep 24 13:49 depcomp
-rwxr-xr-x 1 kpark kpark  13998 Sep 24 13:49 install-sh
-rw-r--r-- 1 kpark kpark  17972 Sep 24 13:57 Makefile ~
-rw-r--r-- 1 kpark kpark     36 Sep 23 15:39 Makefile.am
-rw-r--r-- 1 kpark kpark  17822 Sep 24 13:49 Makefile.in
-rwxr-xr-x 1 kpark kpark  10346 Sep 24 13:49 missing
-rw-r--r-- 1 kpark kpark     56 Sep 23 15:36 README
drwxr-xr-x 3 kpark kpark   4096 Sep 24 13:57 src
-rw-r--r-- 1 kpark kpark     23 Sep 24 13:57 stamp-h1

./autom4te.cache:
total 352
drwxr-xr-x 2 kpark kpark   4096 Sep 24 13:49 .
drwxr-xr-x 4 kpark kpark   4096 Sep 24 13:57 ..
-rw-r--r-- 1 kpark kpark 139847 Sep 24 13:49 output.0
-rw-r--r-- 1 kpark kpark 139847 Sep 24 13:49 output.1
-rw-r--r-- 1 kpark kpark   6687 Sep 24 13:49 requests
-rw-r--r-- 1 kpark kpark  34070 Sep 24 13:49 traces.0
-rw-r--r-- 1 kpark kpark  20296 Sep 24 13:49 traces.1

./src:
total 52
drwxr-xr-x 3 kpark kpark  4096 Sep 24 13:57 .
drwxr-xr-x 4 kpark kpark  4096 Sep 24 13:57 ..
drwxr-xr-x 2 kpark kpark  4096 Sep 24 13:57 .deps
-rw-r--r-- 1 kpark kpark   133 Sep 23 15:35 main.c
-rw-r--r-- 1 kpark kpark 14541 Sep 24 13:57 Makefile
-rw-r--r-- 1 kpark kpark    45 Sep 23 15:39 Makefile.am
-rw-r--r-- 1 kpark kpark 14732 Sep 24 13:49 Makefile.in

./src/.deps:
total 12
drwxr-xr-x 2 kpark kpark 4096 Sep 24 13:57 .
drwxr-xr-x 3 kpark kpark 4096 Sep 24 13:57 ..
-rw-r--r-- 1 kpark kpark    8 Sep 24 13:57 main.Po

You can see Makefile, src/Makefile, and config.h being created at the end after
configure has probed the system. It is now possible to run all the targets we
wish 

<8>
~/amhello % make
…
~/amhello % src/hello
Hello World!
This is amhello 1.0.
~/amhello % make distcheck
...
=============================================
amhello-1.0 archives ready for distribution:
amhello-1.0.tar.gz
=============================================


<files-created>

Note that running autoreconf is only needed initially when the GNU Build
System does not exist. When you later change some instructions in a
Makefile.am or configure.ac, the relevant part of the build system will be
regenerated automatically when you execute make.

However, because Autoconf and Automake have separate manuals, the important
point to understand is that autoconf is in charge of 'creating' configure
'from' configure.ac, while automake is in charge of 'creating' Makefile.ins
'from' Makefile.am and configure.ac. This should at least direct you to the
right manual when seeking answers. 


={============================================================================
*kt_linux_tool_150* automake: example: explained

Let us begin with the contents of ‘configure.ac’.

     AC_INIT([amhello], [1.0], [bug-automake@gnu.org])
     AM_INIT_AUTOMAKE([-Wall -Werror foreign])
     AC_PROG_CC
     AC_CONFIG_HEADERS([config.h])
     AC_CONFIG_FILES([
      Makefile
      src/Makefile
     ])
     AC_OUTPUT

This file is read by both ‘autoconf’ (to create ‘configure’) and ‘automake’
(to create the various ‘Makefile.in’s). It contains a series of M4 macros that
will be expanded as shell code to finally form the ‘configure’ script.  

The macros prefixed with ‘AC_’ are Autoconf macros, documented in the Autoconf
manual. The macros that start with ‘AM_’ are Automake macros.

The first two lines of ‘configure.ac’ initialize Autoconf and Automake.
‘AC_INIT’ takes in as parameters the name of the package, its version number,
and a contact address for bug-reports about the package.
  
The ‘AC_PROG_CC’ line causes the ‘configure’ script to search for a C compiler
and define the variable ‘CC’ with its name. The ‘src/Makefile.in’ file
generated by Automake uses the variable ‘CC’ to build ‘hello’, so when
‘configure’ creates ‘src/Makefile’ from ‘src/Makefile.in’, it will define ‘CC’
with the value it has found. If Automake is asked to create a ‘Makefile.in’
that uses ‘CC’ but ‘configure.ac’ does not define it, it will suggest you add
a call to ‘AC_PROG_CC’.


The ‘AC_CONFIG_HEADERS([config.h])’ invocation causes the ‘configure’ script
to create a ‘config.h’ file gathering ‘#define’s defined by other macros in
‘configure.ac’. In our case, the ‘AC_INIT’ macro already defined a few of
them.  Here is an excerpt of ‘config.h’ after ‘configure’ has run:

     ...
     /* Define to the address where bug reports for this package should be sent. */
     #define PACKAGE_BUGREPORT "bug-automake@gnu.org"

     /* Define to the full name and version of this package. */
     #define PACKAGE_STRING "amhello 1.0"
     ...

As you probably noticed, ‘src/main.c’ includes ‘config.h’ so it can use
‘PACKAGE_STRING’. In a real-world project, ‘config.h’ can grow really big,
with one ‘#define’ per feature probed on the system.


The ‘AC_CONFIG_FILES’ macro declares the list of files that ‘configure’ should
create from their ‘*.in’ templates. Automake also scans this list to find the
‘Makefile.am’ files it must process. (This is important to remember: when
    adding a new directory to your project, you should add its ‘Makefile’ to
    this list, otherwise Automake will never process the new ‘Makefile.am’ you
    wrote in that directory.)

Finally, the ‘AC_OUTPUT’ line is a closing command that actually produces the
part of the script in charge of creating the files registered with
‘AC_CONFIG_HEADERS’ and ‘AC_CONFIG_FILES’.


2.4.3 ‘amhello’’s ‘Makefile.am’ Setup Explained
-----------------------------------------------

We now turn to ‘src/Makefile.am’. This file contains Automake instructions to
build and install ‘hello’.

     bin_PROGRAMS = hello
     hello_SOURCES = main.c

A ‘Makefile.am’ has the same syntax as an ordinary ‘Makefile’. When automake
processes a ‘Makefile.am’ it copies the entire file into the output
‘Makefile.in’ (that will be later 'turned' into 'Makefile' by 'configure') but
will react to certain variable definitions by generating some build rules and
other variables. Often ‘Makefile.am’s contain only a list of variable
definitions as above, but they can also contain other variable and rule
definitions that automake will pass along without interpretation.


//     bin_PROGRAMS = hello

Variables that 'end' with ‘_PROGRAMS’ are special variables that list 'programs'
that the resulting ‘Makefile’ should build. In Automake speak, this ‘_PROGRAMS’
suffix is called a "primary"; Automake recognizes other primaries such as
‘_SCRIPTS’, ‘_DATA’, ‘_LIBRARIES’, etc. corresponding to different 'types' of
files.


The ‘bin’ part of the ‘bin_PROGRAMS’ tells ‘automake’ that the resulting
programs should be 'installed' in BINDIR. Recall that the GNU Build System uses
a set of variables to denote destination directories and allow users to
customize these locations (*note Standard Directory Variables::). Any such
directory variable can be put in front of a primary (omitting the ‘dir’
    suffix) to tell ‘automake’ where to install the listed files.


//     hello_SOURCES = main.c

Programs need to be built from source files, so for each program ‘PROG’ listed
in a ‘_PROGRAMS’ variable, ‘automake’ will look for another variable named
‘PROG_SOURCES’ listing its source files. There may be more than one source file:
they will all be compiled and linked together.

Automake also knows that source files need to be distributed when creating a
tarball (unlike built programs).  So a side-effect of this ‘hello_SOURCES’
declaration is that ‘main.c’ will be part of the tarball created by ‘make dist’.


Finally here are some explanations regarding the top-level ‘Makefile.am’.

     SUBDIRS = src
     dist_doc_DATA = README

‘SUBDIRS’ is a special variable listing all directories that ‘make’ should
'recurse' into 'before' processing the current directory.  So this line is
responsible for ‘make’ building ‘src/hello’ even though we run it from the
top-level.  This line also causes ‘make install’ to install ‘src/hello’ before
installing ‘README’ (not that this order matters).

The line ‘dist_doc_DATA = README’ causes ‘README’ to be distributed and
installed in DOCDIR.  Files listed with the ‘_DATA’ primary are not
automatically part of the tarball built with ‘make dist’, so we add the ‘dist_’
prefix so they get distributed.  However, for ‘README’ it would not have been
necessary: ‘automake’ automatically distributes any ‘README’ file it encounters
(the list of other files automatically distributed is presented by ‘automake
 --help’).  The only important effect of this second line is therefore to
install ‘README’ during ‘make install’.

One thing not covered in this example is accessing the installation directory
values (*note Standard Directory Variables::) from your program code, that is,
converting them into defined macros.  For this, *note (autoconf)Defining
  Directories::.


={============================================================================
*kt_linux_tool_150* automake: 03: variables

3.1 General Operation
=====================

Automake works by reading a ‘Makefile.am’ and generating a ‘Makefile.in’.
Certain variables and rules defined in the ‘Makefile.am’ instruct Automake to
generate more specialized code; for instance, a ‘bin_PROGRAMS’ variable
definition will cause 'rules' for 'compiling' and 'linking' programs to be
generated.

Note that most GNU make extensions are not recognized by Automake.  Using such
extensions in a ‘Makefile.am’ will lead to errors or confusing behavior.

A special exception is that the GNU make append operator, ‘+=’, is supported.
This operator appends its right hand argument to the variable specified on the
left.  Automake will translate the operator into an ordinary ‘=’ operator; ‘+=’
will thus work with any make program.


Generally, Automake is not particularly smart in the parsing of unusual Makefile
constructs, so you’re advised to avoid fancy constructs or “creative” use of
whitespace.  For example, <TAB> characters cannot be used between a target name
and the following “‘:’” character, and variable assignments shouldn’t be
indented with <TAB> characters.  Also, using more complex macro in target names
can cause trouble:

     % cat Makefile.am
     $(FOO:=x): bar
     % automake
     Makefile.am:1: bad characters in variable name '$(FOO'
     Makefile.am:1: ':='-style assignments are not portable


A rule defined in ‘Makefile.am’ generally 'overrides' any such rule of a similar
name that would be automatically generated by ‘automake’.  Although this is a
supported feature, it is generally best to avoid making use of it, as sometimes
the generated rules are very particular.


Similarly, a variable defined in ‘Makefile.am’ or ‘AC_SUBST’ed from
‘configure.ac’ will 'override' any definition of the variable that ‘automake’
would ordinarily create.  This feature is more often 'useful' than the ability
to override a rule.  Be warned that many of the variables generated by
‘automake’ are considered to be for internal use only, and their names might
change in future releases.

<variable-expansion>
When examining a variable definition, Automake will recursively examine
variables referenced in the definition.  For example, if Automake is looking at
the content of ‘foo_SOURCES’ in this snippet

     xs = a.c b.c
     foo_SOURCES = c.c $(xs)

it would use the files ‘a.c’, ‘b.c’, and ‘c.c’ as the contents of ‘foo_SOURCES’.


<comments>
Automake also allows a form of comment that is _not_ copied into the output; all
lines beginning with ‘##’ (leading spaces allowed) are completely ignored by
Automake.


3.3 The Uniform Naming Scheme
=============================

Automake variables generally follow a "uniform naming scheme" that makes it easy
to decide how programs (and other derived objects) are built, and how they are
installed.  
  
At ‘make’ time, certain variables are used to determine which objects are to be
built.  The variable names are made of several pieces that are concatenated
together.

<primary>
The piece that tells ‘automake’ what is being built is commonly called the
"primary".  For instance, the primary ‘PROGRAMS’ holds a list of programs that
are to be compiled and linked.


<where-prefix>
A different set of names is used to decide where the built objects should be
'installed'.  These names are 'prefixes' to the primary, and they indicate which
standard directory should be used as the installation directory.  The standard
directory names are given in the GNU standards (*note (standards)Directory
    Variables::).  

Automake extends this list with ‘pkgdatadir’, ‘pkgincludedir’, ‘pkglibdir’, and
‘pkglibexecdir’; these are the same as the non-‘pkg’ versions, but with
‘$(PACKAGE)’ appended.  For instance, ‘pkglibdir’ is defined as
‘$(libdir)/$(PACKAGE)’.

For each primary, there is one additional variable named by prepending ‘EXTRA_’
to the primary name.  This variable is used to list objects that may or may not
be built, depending on what ‘configure’ decides.  This variable is required
because Automake must statically know the entire list of objects that may be
built in order to generate a ‘Makefile.in’ that will work in all cases.

For instance, ‘cpio’ decides at configure time which programs should be built.
Some of the programs are installed in ‘bindir’, and some are installed in
‘sbindir’:

     EXTRA_PROGRAMS = mt rmt
     bin_PROGRAMS = cpio pax
     sbin_PROGRAMS = $(MORE_PROGRAMS)

Defining a primary without a prefix as a variable, e.g., ‘PROGRAMS’, is an
error.

<bindir>
Note that the common ‘dir’ suffix is 'left' off when constructing the variable
names; thus one writes ‘bin_PROGRAMS’ and not ‘bindir_PROGRAMS’.

Not every sort of object can be installed in every directory.
Automake will flag those attempts it finds in error (but see below how
to override the check if you really need to).  Automake will also
diagnose obvious misspellings in directory names.

<non-standard-directory>
Sometimes the standard directories are not enough.  In particular it is
sometimes useful, for clarity, to install objects in a subdirectory of some
predefined directory.  To this end, Automake allows you to extend the list of
possible installation directories.  A given prefix (e.g., ‘zar’) is valid if a
variable of the same name with ‘dir’ appended is defined (e.g., ‘zardir’).

For instance, the following snippet will install ‘file.xml’ into
‘$(datadir)/xml’.

     xmldir = $(datadir)/xml
     xml_DATA = file.xml

This feature can also be used to override the sanity checks Automake performs to
diagnose suspicious directory/primary couples (in the unlikely case these checks
    are undesirable, and you really know what you’re doing).  For example,
               Automake would error out on this input:

     # Forbidden directory combinations, automake will error out on this.
     pkglib_PROGRAMS = foo
     doc_LIBRARIES = libquux.a

but it will succeed with this:

     # Work around forbidden directory combinations.  Do not use this
     # without a very good reason!
     my_execbindir = $(pkglibdir)
     my_doclibdir = $(docdir)
     my_execbin_PROGRAMS = foo
     my_doclib_LIBRARIES = libquux.a

The ‘exec’ substring of the ‘my_execbindir’ variable lets the files be installed
at the right time (*note The Two Parts of Install::).

<noinst-prefix>
The special prefix ‘noinst_’ indicates that the objects in question should be
built but not installed at all.  This is usually used for objects required to
build the rest of your package, for instance static libraries (*note A
    Library::), or helper scripts.

The special prefix ‘check_’ indicates that the objects in question should not be
built 'until' the ‘make check’ command is run.  Those objects are not installed
either.

<supported-primary>
The current primary names are:

‘PROGRAMS’, ‘LIBRARIES’, ‘LTLIBRARIES’, ‘LISP’, ‘PYTHON’, ‘JAVA’, ‘SCRIPTS’,
                ‘DATA’, ‘HEADERS’, ‘MANS’, and ‘TEXINFOS’.

Some primaries also allow additional prefixes that control other aspects of
‘automake’’s behavior.  The currently defined prefixes are ‘dist_’, ‘nodist_’,
                ‘nobase_’, and ‘notrans_’. 

These prefixes are explained later (*note Program and Library Variables::)
(*note Man Pages::).


3.5 How derived variables are named
===================================

// bin_PROGRAMS = hello
// hello_SOURCES = main.c

Sometimes a Makefile variable name is derived from some text the maintainer
supplies.  For instance, a program name listed in ‘_PROGRAMS’ is rewritten into
the name of a ‘_SOURCES’ variable.  In cases like this, Automake canonicalizes
the text, so that program names and the like do not have to follow Makefile
variable naming rules.  

<to-underscores>
All characters in the name except for letters, numbers, the strudel (@), and the
underscore are 'turned' into 'underscores' when making variable references.

For example, if your program is named ‘sniff-glue’, the derived variable name
would be ‘sniff_glue_SOURCES’, not ‘sniff-glue_SOURCES’.  Similarly the sources
for a library named ‘libmumble++.a’ should be listed in the
  ‘libmumble___a_SOURCES’ variable.

The strudel is an addition, to make the use of Autoconf substitutions in
variable names less obfuscating.


3.6 Variables reserved for the user
===================================

Some ‘Makefile’ variables are 'reserved' by the GNU Coding Standards for the use
of the 'user' - the person 'building' the package. For instance, ‘CFLAGS’ is one
such variable.

Sometimes package developers are tempted to set user variables such as ‘CFLAGS’
because it appears to make their job easier.  However, the package itself should
never set a user variable, particularly not to include switches that are
required for proper compilation of the package.  Since these variables are
documented as being for the package builder, that person rightfully expects to
be able to override any of these variables at build time.

<shadow-variable>
To get around this problem, Automake introduces an automake-specific shadow
variable for 'each' user flag variable. Shadow variables are not introduced for
variables like ‘CC’, where they would make no sense. 

The shadow variable is named by prepending ‘AM_’ to the user variable’s name.
For instance, the shadow variable for ‘YFLAGS’ is ‘AM_YFLAGS’.  The package
maintainer—that is, the author(s) of the ‘Makefile.am’ and ‘configure.ac’
files—may adjust these shadow variables however necessary.


={============================================================================
*kt_linux_tool_150* automake: 04: example packages

The second example shows how two programs can be built from the same file, using
different compilation parameters.  It contains some technical digressions that
are probably best skipped on first read.

Here is another, trickier example.  It shows how to generate two programs
(‘true’ and ‘false’) from the same source file (‘true.c’).  The difficult part
is that each compilation of ‘true.c’ requires different ‘cpp’ flags.

As it turns out, there is also a much easier way to do this same task.  Some of
the above technique is useful enough that we’ve kept the example in the manual.

However if you were to build ‘true’ and ‘false’ in real life, you would probably
use per-program compilation flags, like so:

     bin_PROGRAMS = false true

     false_SOURCES = true.c
     false_CPPFLAGS = -DEXIT_CODE=1

     true_SOURCES = true.c
     true_CPPFLAGS = -DEXIT_CODE=0

In this case Automake will cause ‘true.c’ to be compiled twice, with different
flags.  In this instance, the names of the object files would be chosen by
automake; they would be ‘false-true.o’ and ‘true-true.o’. The name of the object
files rarely matters.


={============================================================================
*kt_linux_tool_150* automake: 08: building programs and libraries

8 Building Programs and Libraries
*********************************

A large part of Automake’s functionality is dedicated to making it easy to build
programs and libraries.

In order to build a program, you need to tell Automake which sources are
part of it, and which libraries it should be linked with.

This section also covers conditional compilation of sources or programs. Most
of the comments about these also apply to libraries and libtool libraries.

     bin_PROGRAMS = hello

In this simple case, the resulting ‘Makefile.in’ will contain code to generate
a program named ‘hello’.

The variable ‘hello_SOURCES’ is used to specify which source files get built
into an executable:

     hello_SOURCES = hello.c version.c getopt.c getopt1.c getopt.h system.h

This causes each mentioned ‘.c’ file to be compiled into the corresponding
‘.o’.  Then all are linked to produce ‘hello’.

Header files listed in a ‘_SOURCES’ definition will be included in the
distribution but otherwise ignored.  In case it isn’t obvious, you should not
include the header file generated by ‘configure’ in a ‘_SOURCES’ variable; this
file should not be distributed.


8.1.2 Linking the program : LDADD, AM_LDFLAGS
-------------------------

If you need to link against libraries that are 'not' found by ‘configure’, you
can use ‘LDADD’ to do so.  This variable is used to specify 'additional' objects
or libraries to link with; it is inappropriate for specifying specific linker
'flags', you should use ‘AM_LDFLAGS’ for this purpose.


8.2 Building a library
======================

Building a library is much like building a program.  In this case, the name of
the primary is ‘LIBRARIES’.  Libraries can be installed in ‘libdir’ or
‘pkglibdir’.

For instance, to create a library named ‘libcpio.a’, but not install it, you
would write:

     noinst_LIBRARIES = libcpio.a

The sources that go into a library are determined exactly as they are for
programs, via the ‘_SOURCES’ variables.  Note that the library name is
canonicalized, so the ‘_SOURCES’ variable corresponding to ‘libcpio.a’ is
‘libcpio_a_SOURCES’, not ‘libcpio.a_SOURCES’.

Extra objects can be added to a library using the ‘LIBRARY_LIBADD’ variable.
This should be used for objects determined by ‘configure’.  Again from ‘cpio’:

     libcpio_a_LIBADD = $(LIBOBJS) $(ALLOCA)

To use a static library when building a program, add it to ‘LDADD’ for this
program.  In the following example, the program ‘cpio’ is statically linked with
the library ‘libcpio.a’.

     noinst_LIBRARIES = libcpio.a
     libcpio_a_SOURCES = ...

     bin_PROGRAMS = cpio
     cpio_SOURCES = cpio.c ...
     cpio_LDADD = libcpio.a     // note: see


8.3 Building a Shared Library
=============================

<libtool>
Building shared libraries portably is a relatively complex matter.  For this
reason, GNU Libtool was created to help build shared libraries in a
platform-independent way.

8.3.1 The Libtool Concept
-------------------------

<la-lo-suffix>

Libtool abstracts shared and static libraries into a unified concept henceforth
called "libtool libraries".  Libtool libraries are files using the ‘.la’
'suffix', and can designate a static library, a shared library, or maybe both.
Their exact nature cannot be determined until ‘./configure’ is run: not all
platforms support all kinds of libraries, and users can explicitly select which
libraries should be built.  (However the package’s maintainers can tune the
    default, *note The ‘AC_PROG_LIBTOOL’ macro: (libtool)AC_PROG_LIBTOOL.)

Because object files for shared and static libraries must be compiled
'differently', libtool is also used during compilation.  Object files built by
libtool are called "libtool objects": these are files using the ‘.lo’ suffix.
Libtool libraries are built from these libtool objects.

You should not assume anything about the structure of ‘.la’ or ‘.lo’ files and
how libtool constructs them: this is libtool’s concern, and the last thing one
wants is to learn about libtool’s guts.  However the existence of these files
matters, because they are used as targets and dependencies in ‘Makefile’s rules
when building libtool libraries.  There are situations where you may have to
refer to these, for instance when expressing dependencies for building source
files conditionally


8.3.2 Building Libtool Libraries
--------------------------------

Automake uses libtool to build libraries declared with the ‘LTLIBRARIES’
primary.  Each ‘_LTLIBRARIES’ variable is a list of libtool libraries to build.
For instance, to create a libtool library named ‘libgettext.la’, and install it
in ‘libdir’, write:

     lib_LTLIBRARIES = libgettext.la
     libgettext_la_SOURCES = gettext.c gettext.h ...
     include_HEADERS = gettext.h ...

Automake predefines the variable ‘pkglibdir’, so you can use
‘pkglib_LTLIBRARIES’ to install libraries in ‘$(libdir)/@PACKAGE@/’.

If ‘gettext.h’ is a public header file that needs to be installed in order for
people to use the library, it should be declared using a ‘_HEADERS’ variable,
       not in ‘libgettext_la_SOURCES’.  Headers listed in the latter should be
         internal headers that are not part of the public interface.


The following example builds a program named ‘hello’ that is linked with
‘libgettext.la’.

     lib_LTLIBRARIES = libgettext.la
     libgettext_la_SOURCES = gettext.c …

     bin_PROGRAMS = hello
     hello_SOURCES = hello.c …
     hello_LDADD = libgettext.la

Whether ‘hello’ is statically or dynamically linked with ‘libgettext.la’ is not
yet known: this will depend on the configuration of libtool and the capabilities
of the host.


8.3.5 Libtool Convenience Libraries
-----------------------------------

Sometimes you want to build libtool libraries that should 'not' be installed.
These are called "libtool convenience libraries" and are typically used to
encapsulate many sublibraries, later gathered into one 'big' installed library.

Unlike installed libtool libraries they do not need an ‘-rpath’ flag at link
time (actually this is the only difference).

Convenience libraries listed in ‘noinst_LTLIBRARIES’ are always built.  Those
listed in ‘check_LTLIBRARIES’ are built only upon ‘make check’.  Finally,
libraries listed in ‘EXTRA_LTLIBRARIES’ are never built explicitly: Automake
  outputs rules to build them, but if the library does not appear as a Makefile
  dependency anywhere it won’t be built (this is why ‘EXTRA_LTLIBRARIES’ is used
      for conditional compilation).

Here is a sample setup merging libtool convenience libraries from subdirectories
into one main ‘libtop.la’ library.

     # -- Top-level Makefile.am --
     SUBDIRS = sub1 sub2 ...
     lib_LTLIBRARIES = libtop.la
     libtop_la_SOURCES =
     libtop_la_LIBADD = \
       sub1/libsub1.la \
       sub2/libsub2.la \
       ...

     # -- sub1/Makefile.am --
     noinst_LTLIBRARIES = libsub1.la
     libsub1_la_SOURCES = ...

     # -- sub2/Makefile.am --
     # showing nested convenience libraries
     SUBDIRS = sub2.1 sub2.2 ...
     noinst_LTLIBRARIES = libsub2.la
     libsub2_la_SOURCES =
     libsub2_la_LIBADD = \
       sub21/libsub21.la \
       sub22/libsub22.la \
       ...

When using such setup, beware that ‘automake’ will assume ‘libtop.la’ is to be
linked with the C linker. This is because ‘libtop_la_SOURCES’ is empty, so
‘automake’ picks C as 'default' language. If ‘libtop_la_SOURCES’ was not empty,
‘automake’ would select the linker as explained in *note How the Linker is
  Chosen::.

If one of the sublibraries contains non-C source, it is important that the
appropriate linker be chosen. One way to achieve this is to pretend that there
is such a non-C file among the sources of the library, thus 'forcing' ‘automake’
to select the appropriate linker.  Here is the top-level ‘Makefile’ of our
example updated to force C++ linking.

     SUBDIRS = sub1 sub2 ...
     lib_LTLIBRARIES = libtop.la
     libtop_la_SOURCES =
     # Dummy C++ source to cause C++ linking.
     nodist_EXTRA_libtop_la_SOURCES = dummy.cxx
     libtop_la_LIBADD = \
       sub1/libsub1.la \
       sub2/libsub2.la \
       ...

‘EXTRA_*_SOURCES’ variables are used to keep track of source files that might be
compiled (this is mostly useful when doing conditional compilation using
    ‘AC_SUBST’, *note Conditional Libtool Sources::), and the ‘nodist_’ prefix
means the listed sources are not to be distributed (*note Program and Library
    Variables::).  

In effect the file ‘dummy.cxx’ does not need to exist in the source tree.  Of
course if you have some real source file to list in ‘libtop_la_SOURCES’ there is
no point in cheating with ‘nodist_EXTRA_libtop_la_SOURCES’.


<am-dummy-file>
// This file is necessary because automake assumes that a shared library with
// no source files (i.e. a shared library built from an archive library) is a C
// library, so it incorrectly links it with the C compiler, omitting libstdc++
// and causing undefined symbols
//
// TL;DR: This file is necessary because automake is rubbish.

libVanadiumWebKitVideoElement_la_SOURCES = src/Dummy.cpp

libVanadiumWebKitVideoElement_la_LIBADD = \
  libVanadiumWebKitVideoElementStatic.la \
  $(LIBADDS)


8.3.7 ‘_LIBADD’, ‘_LDFLAGS’, and ‘_LIBTOOLFLAGS’
------------------------------------------------

note: LIBRARY_LIBADD means xxx_LIBADD

As shown in previous sections, the ‘LIBRARY_LIBADD’ variable should be used to
list extra libtool objects (‘.lo’ files) or libtool libraries (‘.la’) to add to
LIBRARY.

The ‘LIBRARY_LDFLAGS’ variable is the place to list 'additional' libtool linking
'flags', such as ‘-version-info’, ‘-static’, and a lot more.  *Note Link mode:
(libtool)Link mode.

The ‘libtool’ command has 'two' kinds of options: mode-specific options and
generic options.  Mode-specific options such as the aforementioned linking flags
should be lumped with the other flags passed to the tool invoked by ‘libtool’
(hence the use of ‘LIBRARY_LDFLAGS’ for libtool linking flags).  


Generic options include ‘--tag=TAG’ and ‘--silent’ (*note Invoking ‘libtool’:
    (libtool)Invoking libtool. for more options) should appear before the mode
selection on the command line; in ‘Makefile.am’s they should be listed in the
‘LIBRARY_LIBTOOLFLAGS’ variable.


If ‘LIBRARY_LIBTOOLFLAGS’ is not defined, then the variable ‘AM_LIBTOOLFLAGS’ is
used instead.

These flags are passed to libtool after the ‘--tag=TAG’ option computed by
Automake (if any), so ‘LIBRARY_LIBTOOLFLAGS’ (or ‘AM_LIBTOOLFLAGS’) is a good
place to override or supplement the ‘--tag=TAG’ setting.

The libtool rules also use a ‘LIBTOOLFLAGS’ variable that should not be set in
‘Makefile.am’: this is a user variable (*note Flag Variables Ordering::.  It
    allows users to run ‘make LIBTOOLFLAGS=--silent’, for instance.  Note that
    the verbosity of ‘libtool’ can also be influenced by the Automake support
    for silent rules (*note Automake Silent Rules::).


={============================================================================
*kt_linux_tool_150* automake: 08: program variables

8.4 Program and Library Variables
=================================

Associated with each program is a collection of variables that can be used to
modify how that program is built.  There is a similar list of such variables for
each library.  The canonical name of the program (or library) is used as a base
for naming these variables.

In the list below, we use the name “maude” to refer to the 'program' or
'library'.  In your ‘Makefile.am’ you would replace this with the canonical name
of your program.  This list also refers to “maude” as a program, but in general
the same rules apply for both static and dynamic libraries; the documentation
below notes situations where programs and libraries differ.

‘maude_SOURCES’
     This variable, if it exists, lists all the source files that are compiled
     to build the program.  These files are added to the distribution by
     default.  When building the program, Automake will cause each source file
     to be compiled to a single ‘.o’ file (or ‘.lo’ when using libtool).
     Normally these object files are named after the source file, but other
     factors can change this.  If a file in the ‘_SOURCES’ variable has an
     unrecognized extension, Automake will do one of two things with it.  If a
     suffix rule exists for turning files with the unrecognized extension into
     ‘.o’ files, then ‘automake’ will treat this file as it will any other
     source file.  Otherwise, the file will be ignored as though it were a
     header file.

     The prefixes ‘dist_’ and ‘nodist_’ can be used to control whether files
     listed in a ‘_SOURCES’ variable are distributed.  ‘dist_’ is 'redundant',
            as sources are distributed by default, but it can be specified for
              clarity if desired.

     It is possible to have both ‘dist_’ and ‘nodist_’ variants of a given
     ‘_SOURCES’ variable at once; this lets you easily distribute some files and
     not others, for instance:

          nodist_maude_SOURCES = nodist.c
          dist_maude_SOURCES = dist-me.c

     By default the output file (on Unix systems, the ‘.o’ file) will be put
     into the current 'build' directory.  However, if the option
     ‘subdir-objects’ is in effect in the current directory then the ‘.o’ file
     will be put into the subdirectory named after the source file.  For
     instance, with ‘subdir-objects’ enabled, ‘sub/dir/file.c’ will be compiled
     to ‘sub/dir/file.o’.  Some people prefer this mode of operation.  You can
     specify ‘subdir-objects’ in ‘AUTOMAKE_OPTIONS’ (*note Options::).

‘EXTRA_maude_SOURCES’
     Automake needs to know the list of files you intend to compile
     _statically_.  For one thing, this is the only way Automake has of knowing
     what sort of language support a given ‘Makefile.in’ requires.  (1) This
     means that, for example, you can’t put a configure substitution like
     ‘@my_sources@’ into a ‘_SOURCES’ variable.  If you intend to conditionally
     compile source files and use ‘configure’ to substitute the appropriate
     object names into, e.g., ‘_LDADD’ (see below), then you should list the
     corresponding source files in the ‘EXTRA_’ variable.

     This variable also supports ‘dist_’ and ‘nodist_’ prefixes.  For instance,
‘nodist_EXTRA_maude_SOURCES’ would list extra sources that may need to be built,
but should not be distributed.

‘maude_AR’
     A static library is created by default by invoking ‘$(AR) $(ARFLAGS)’
     followed by the name of the library and then the objects being put into the
     library.  You can override this by setting the ‘_AR’ variable.  This is
     usually used with C++; some C++ compilers require a special invocation in
     order to instantiate all the templates that should go into a library.  For
     instance, the SGI C++ compiler likes this variable set like so:

     libmaude_a_AR = $(CXX) -ar -o

‘maude_LIBADD’
     Extra objects can be added to a _library_ using the ‘_LIBADD’ variable.
     For instance, this should be used for objects determined by ‘configure’
     (*note A Library::).

     In the case of libtool libraries, ‘maude_LIBADD’ can also refer to other
     libtool libraries.

‘maude_LDADD’
     Extra objects (‘*.$(OBJEXT)’) and libraries (‘*.a’, ‘*.la’) can be added to
     a _program_ by listing them in the ‘_LDADD’ variable.  For instance, this
     should be used for objects determined by ‘configure’ (*note Linking::).

     ‘_LDADD’ and ‘_LIBADD’ are 'inappropriate' for passing program-'specific'
     linker flags (except for ‘-l’, ‘-L’, ‘-dlopen’ and ‘-dlpreopen’).  Use the
     ‘_LDFLAGS’ variable for this purpose.

     For instance, if your ‘configure.ac’ uses ‘AC_PATH_XTRA’, you could link
     your program against the X libraries like so:

          maude_LDADD = $(X_PRE_LIBS) $(X_LIBS) $(X_EXTRA_LIBS)

     We recommend that you use ‘-l’ and ‘-L’ only when referring to third-party
     libraries, and give the explicit file names of any library built by your
     package.  Doing so will ensure that ‘maude_DEPENDENCIES’ (see below) is
     correctly defined by default.

‘maude_LDFLAGS’
     This variable is used to pass 'extra''flags' to the link step of a program
     or a shared library.  It overrides the ‘AM_LDFLAGS’ variable.

‘maude_LIBTOOLFLAGS’
     This variable is used to pass extra options to ‘libtool’.  It overrides the
     ‘AM_LIBTOOLFLAGS’ variable.  These options are output before ‘libtool’’s
     ‘--mode=MODE’ option, so they should not be mode-specific options (those
         belong to the compiler or linker flags).  *Note Libtool Flags::.

‘maude_DEPENDENCIES’
‘EXTRA_maude_DEPENDENCIES’
     It is also occasionally useful to have a target (program or library) depend
     on some other file that is not actually part of that target.  This can be
     done using the ‘_DEPENDENCIES’ variable.  Each target depends on the
     contents of such a variable, but no further interpretation is done.

     Since these dependencies are associated to the link rule used to
     create the programs they should normally list files used by the
     link command.  That is ‘*.$(OBJEXT)’, ‘*.a’, or ‘*.la’ files for
     programs; ‘*.lo’ and ‘*.la’ files for Libtool libraries; and
     ‘*.$(OBJEXT)’ files for static libraries.  In rare cases you may
     need to add other kinds of files such as linker scripts, but
     _listing a source file in ‘_DEPENDENCIES’ is wrong_.  If some
     source file needs to be built before all the components of a
     program are built, consider using the ‘BUILT_SOURCES’ variable
     (*note Sources::).

     If ‘_DEPENDENCIES’ is not supplied, it is computed by Automake.
     The automatically-assigned value is the contents of ‘_LDADD’ or
     ‘_LIBADD’, with most configure substitutions, ‘-l’, ‘-L’, ‘-dlopen’
     and ‘-dlpreopen’ options removed.  The configure substitutions that
     are left in are only ‘$(LIBOBJS)’ and ‘$(ALLOCA)’; these are left
     because it is known that they will not cause an invalid value for
     ‘_DEPENDENCIES’ to be generated.

     ‘_DEPENDENCIES’ is more likely used to perform conditional
     compilation using an ‘AC_SUBST’ variable that contains a list of
     objects.  *Note Conditional Sources::, and *note Conditional
     Libtool Sources::.

     The ‘EXTRA_*_DEPENDENCIES’ variable may be useful for cases where
     you merely want to augment the ‘automake’-generated ‘_DEPENDENCIES’
     variable rather than replacing it.

‘maude_LINK’
     You can override the linker on a per-program basis.  By default the
     linker is chosen according to the languages used by the program.
     For instance, a program that includes C++ source code would use the
     C++ compiler to link.  The ‘_LINK’ variable must hold the name of a
     command that can be passed all the ‘.o’ file names and libraries to
     link against as arguments.  Note that the name of the underlying
     program is _not_ passed to ‘_LINK’; typically one uses ‘$@’:

          maude_LINK = $(CCLD) -magic -o $@

     If a ‘_LINK’ variable is not supplied, it may still be generated
     and used by Automake due to the use of per-target link flags such
     as ‘_CFLAGS’, ‘_LDFLAGS’ or ‘_LIBTOOLFLAGS’, in cases where they
     apply.

‘maude_CCASFLAGS’
‘maude_CFLAGS’
‘maude_CPPFLAGS’
‘maude_CXXFLAGS’
‘maude_FFLAGS’
‘maude_GCJFLAGS’
‘maude_LFLAGS’
‘maude_OBJCFLAGS’
‘maude_OBJCXXFLAGS’
‘maude_RFLAGS’
‘maude_UPCFLAGS’
‘maude_YFLAGS’
     Automake allows you to set 'compilation' flags on a per-program (or
         per-library) basis.  A single source file can be included in several
     programs, and it will potentially be compiled with different flags for each
     program.  This works for any language directly supported by Automake.

     These "per-target compilation flags" are 
     ‘_CCASFLAGS’, ‘_CFLAGS’, ‘_CPPFLAGS’, ‘_CXXFLAGS’, ‘_FFLAGS’, ‘_GCJFLAGS’, 
     ‘_LFLAGS’, ‘_OBJCFLAGS’, ‘_OBJCXXFLAGS’, ‘_RFLAGS’, ‘_UPCFLAGS’,
     and ‘_YFLAGS’.

     When using a per-target compilation flag, Automake will choose a different
     name for the intermediate object files.  Ordinarily a file like ‘sample.c’
     will be compiled to produce ‘sample.o’.  However, if the program’s
     ‘_CFLAGS’ variable is set, then the object file will be named, for
     instance, ‘maude-sample.o’.  (See also *note Renamed Objects::).

     In compilations with per-target flags, the ordinary ‘AM_’ form of the flags
     variable is _not_ automatically included in the compilation (however, the
         user form of the variable _is_ included).

     So for instance, if you want the hypothetical ‘maude’ compilations
     to also use the value of ‘AM_CFLAGS’, you would need to write:

          maude_CFLAGS = … your flags ... $(AM_CFLAGS)

     *Note Flag Variables Ordering::, for more discussion about the interaction
     between 'user' variables, ‘AM_’ 'shadow' variables, and per-'target'
     variables.

‘maude_SHORTNAME’
     On some platforms the allowable file names are very short.  In
     order to support these systems and per-target compilation flags at
     the same time, Automake allows you to set a “short name” that will
     influence how intermediate object files are named.  For instance,
     in the following example,

          bin_PROGRAMS = maude
          maude_CPPFLAGS = -DSOMEFLAG
          maude_SHORTNAME = m
          maude_SOURCES = sample.c …

     the object file would be named ‘m-sample.o’ rather than
     ‘maude-sample.o’.

     This facility is rarely needed in practice, and we recommend
     avoiding it until you find it is required.

   ---------- Footnotes ----------

   (1) There are other, more obscure reasons for this limitation as well.


8.7 Variables used when building a program
==========================================

Occasionally it is useful to know which ‘Makefile’ variables Automake uses for
compilations, and in which 'order'; for instance, you might need to do your own
compilation in some special cases.

   Some variables are 'inherited' from Autoconf; these are 

   ‘CC’, ‘CFLAGS’, ‘CPPFLAGS’, ‘DEFS’, ‘LDFLAGS’, and ‘LIBS’.

   There are some additional variables that Automake defines on its own:

‘AM_CPPFLAGS’
     The contents of this variable are passed to every compilation that invokes
     the C 'preprocessor'; it is a list of arguments to the preprocessor.  For
     instance, ‘-I’ and ‘-D’ options should be listed here.

     Automake already provides some ‘-I’ options automatically, in a separate
     variable that is also passed to every compilation that invokes the C
     preprocessor.  In particular it generates ‘-I.’, ‘-I$(srcdir)’, and a ‘-I’
     pointing to the directory holding ‘config.h’ (if you’ve used
         ‘AC_CONFIG_HEADERS’).  You can disable the default ‘-I’ options using
     the ‘nostdinc’ option.

     When a file to be included is generated during the build and not
     part of a distribution tarball, its location is under
     ‘$(builddir)’, not under ‘$(srcdir)’.  This matters especially for
     packages that use header files placed in sub-directories and want
     to allow builds outside the source tree (*note VPATH Builds::).  In
     that case we recommend to use a pair of ‘-I’ options, such as,
     e.g., ‘-Isome/subdir -I$(srcdir)/some/subdir’ or
     ‘-I$(top_builddir)/some/subdir -I$(top_srcdir)/some/subdir’.  Note
     that the reference to the build tree should come before the
     reference to the source tree, so that accidentally leftover
     generated files in the source directory are ignored.

     ‘AM_CPPFLAGS’ is ignored in preference to a per-executable (or
     per-library) ‘_CPPFLAGS’ variable if it is defined.

// ‘INCLUDES’
//      This does the same job as ‘AM_CPPFLAGS’ (or any per-target
//      ‘_CPPFLAGS’ variable if it is used).  It is an older name for the
//      same functionality.  This variable is 'deprecated'; we suggest using
//      ‘AM_CPPFLAGS’ and per-target ‘_CPPFLAGS’ instead.

‘AM_CFLAGS’
     This is the variable the ‘Makefile.am’ author can use to pass in additional
     C compiler flags.  In some situations, this is not used, in preference to
     the per-executable (or per-library) ‘_CFLAGS’.

‘COMPILE’
     This is the command used to actually compile a C source file.  The file
     name is appended to form the complete command line.

‘AM_LDFLAGS’
     This is the variable the ‘Makefile.am’ author can use to pass in additional
     linker flags.  In some situations, this is not used, in preference to the
     per-executable (or per-library) ‘_LDFLAGS’.

‘LINK’
     This is the command used to actually link a C program.  It already includes
     ‘-o $@’ and the usual variable references (for instance, ‘CFLAGS’); it
     takes as “arguments” the names of the object files and libraries to link
     in.  This variable is not used when the linker is overridden with a
     per-target ‘_LINK’ variable or per-target flags cause Automake to define
     such a ‘_LINK’ variable.


8.9 C++ Support
===============

Automake includes full support for C++.

Any package including C++ code 'must' define the output variable ‘CXX’ in
‘configure.ac’; the simplest way to do this is to use the ‘AC_PROG_CXX’ macro
(*note Particular Program Checks: (autoconf)Particular Programs.).

A few additional variables are defined when a C++ source file is seen:

‘CXX’
     The name of the C++ compiler.

‘CXXFLAGS’
     Any flags to pass to the C++ compiler.

‘AM_CXXFLAGS’
     The maintainer’s variant of ‘CXXFLAGS’.

‘CXXCOMPILE’
     The command used to actually compile a C++ source file.  The file name is
     appended to form the complete command line.

‘CXXLINK’
     The command used to actually link a C++ program.


27.6 Flag Variables Ordering
============================

What is the difference between ‘AM_CFLAGS’, ‘CFLAGS’, and ‘mumble_CFLAGS’?

Why does ‘automake’ output ‘CPPFLAGS’ after ‘AM_CPPFLAGS’ on compile lines?
Shouldn’t it be the converse?

My ‘configure’ adds some warning flags into ‘CXXFLAGS’.  In one ‘Makefile.am’ I
would like to append a new flag, however if I put the flag into ‘AM_CXXFLAGS’ it
is prepended to the other flags, not appended.

Compile Flag Variables
----------------------

This section attempts to answer all the above questions.  We will mostly discuss
‘CPPFLAGS’ in our examples, but actually the answer holds for 'all' the compile
flags used in Automake: ‘CCASFLAGS’, ‘CFLAGS’, ‘CPPFLAGS’, ‘CXXFLAGS’,
‘FCFLAGS’, ‘FFLAGS’, ‘GCJFLAGS’, ‘LDFLAGS’, ‘LFLAGS’, ‘LIBTOOLFLAGS’,
‘OBJCFLAGS’, ‘OBJCXXFLAGS’, ‘RFLAGS’, ‘UPCFLAGS’, and ‘YFLAGS’.

‘CPPFLAGS’, ‘AM_CPPFLAGS’, and ‘mumble_CPPFLAGS’ are three variables that can be
used to pass flags to the C preprocessor (actually these variables are also used
    for other languages like C++ or preprocessed Fortran).  ‘CPPFLAGS’ is the
user variable (*note User Variables::), ‘AM_CPPFLAGS’ is the Automake variable,
and ‘mumble_CPPFLAGS’ is the variable specific to the ‘mumble’ target (we call
    this a per-target variable, *note Program and Library Variables::).

<use-order>
Automake always uses two of these variables when compiling C sources files.
When compiling an object file for the ‘mumble’ target, the first variable will
be ‘mumble_CPPFLAGS’ if it is defined, or ‘AM_CPPFLAGS’ otherwise.  The second
variable is always ‘CPPFLAGS’.

   In the following example,

     bin_PROGRAMS = foo bar
     foo_SOURCES = xyz.c
     bar_SOURCES = main.c
     foo_CPPFLAGS = -DFOO
     AM_CPPFLAGS = -DBAZ

‘xyz.o’ will be compiled with ‘$(foo_CPPFLAGS) $(CPPFLAGS)’, (because ‘xyz.o’ is
    part of the ‘foo’ target), while ‘main.o’ will be compiled with
‘$(AM_CPPFLAGS) $(CPPFLAGS)’ (because there is no per-target variable for target
    ‘bar’).

The difference between ‘mumble_CPPFLAGS’ and ‘AM_CPPFLAGS’ being clear enough,
let’s focus on ‘CPPFLAGS’.  ‘CPPFLAGS’ is a user variable, i.e., a variable that
  users are entitled to modify in order to compile the package.  This variable,
like many others, is documented at the end of the output of ‘configure --help’.

For instance, someone who needs to add ‘/home/my/usr/include’ to the C
compiler’s search path would configure a package with

     ./configure CPPFLAGS='-I /home/my/usr/include'

and this flag would be 'propagated' to the compile rules of all ‘Makefile’s.

<make-time-change>
It is also not uncommon to override a user variable at ‘make’-time.  Many
installers do this with ‘prefix’, but this can be useful with compiler flags
too.  For instance, if, while debugging a C++ project, you need to 'disable'
optimization in one specific object file, you can run something like

     rm file.o
     make CXXFLAGS=-O0 file.o
     make

The reason ‘$(CPPFLAGS)’ appears after ‘$(AM_CPPFLAGS)’ or ‘$(mumble_CPPFLAGS)’
in the compile command is that users should always have the last say.  It
probably makes more sense if you think about it while looking at the
‘CXXFLAGS=-O0’ above, which should 'supersede' any other switch from
‘AM_CXXFLAGS’ or ‘mumble_CXXFLAGS’ (and this of course replaces the previous
    value of ‘CXXFLAGS’).

You should never redefine a user variable such as ‘CPPFLAGS’ in ‘Makefile.am’.
Use ‘automake -Woverride’ to diagnose such mistakes.

Even something like

     CPPFLAGS = -DDATADIR=\"$(datadir)\" @CPPFLAGS@

is erroneous.  Although this preserves ‘configure’’s value of ‘CPPFLAGS’, the
definition of ‘DATADIR’ will disappear if a user attempts to override ‘CPPFLAGS’
from the ‘make’ command line.

     AM_CPPFLAGS = -DDATADIR=\"$(datadir)\"

is all that is needed here if no per-target flags are used.

You should not add options to these user variables within ‘configure’ either,
for the same reason.  Occasionally you need to modify these variables to perform
  a test, but you should reset their values afterwards.  In contrast, it is OK
    to modify the ‘AM_’ variables within ‘configure’ if you ‘AC_SUBST’ them, but
    it is rather rare that you need to do this, unless you really want to change
    the default definitions of the ‘AM_’ variables in all ‘Makefile’s.

What we recommend is that you define extra flags in separate variables.  For
instance, you may write an Autoconf macro that computes a set of warning options
for the C compiler, and ‘AC_SUBST’ them in ‘WARNINGCFLAGS’; you may also have an
  Autoconf macro that determines which compiler and which linker flags should be
    used to link with library ‘libfoo’, and ‘AC_SUBST’ these in ‘LIBFOOCFLAGS’
    and ‘LIBFOOLDFLAGS’.  Then, a ‘Makefile.am’ could use these variables as
    follows:

     AM_CFLAGS = $(WARNINGCFLAGS)
     bin_PROGRAMS = prog1 prog2
     prog1_SOURCES = ...
     prog2_SOURCES = ...
     prog2_CFLAGS = $(LIBFOOCFLAGS) $(AM_CFLAGS)
     prog2_LDFLAGS = $(LIBFOOLDFLAGS)

In this example both programs will be compiled with the flags substituted into
‘$(WARNINGCFLAGS)’, and ‘prog2’ will additionally be compiled with the flags
required to link with ‘libfoo’.

Note that listing ‘AM_CFLAGS’ in a per-target ‘CFLAGS’ variable is a common
idiom to ensure that ‘AM_CFLAGS’ applies to every target in a ‘Makefile.in’.

Using variables like this gives you full control over the ordering of the flags.
For instance, if there is a flag in $(WARNINGCFLAGS) that you want to negate for
a particular target, you can use something like ‘prog1_CFLAGS = $(AM_CFLAGS)
-no-flag’.  If all of these flags had been forcefully appended to ‘CFLAGS’,
  there would be no way to disable one flag.  Yet another reason to leave user
    variables to users.

Finally, we have avoided naming the variable of the example ‘LIBFOO_LDFLAGS’
(with an underscore) because that would cause Automake to think that this is
actually a per-target variable (like ‘mumble_LDFLAGS’) for some non-declared
‘LIBFOO’ target.

Other Variables
---------------

There are other variables in Automake that follow similar principles to allow
user options.  For instance, Texinfo rules (*note Texinfo::) use ‘MAKEINFOFLAGS’
and ‘AM_MAKEINFOFLAGS’.  Similarly, DejaGnu tests (*note DejaGnu Tests::) use
‘RUNTESTDEFAULTFLAGS’ and ‘AM_RUNTESTDEFAULTFLAGS’.  The tags and ctags rules
(*note Tags::) use ‘ETAGSFLAGS’, ‘AM_ETAGSFLAGS’, ‘CTAGSFLAGS’, and
‘AM_CTAGSFLAGS’.  Java rules (*note Java::) use ‘JAVACFLAGS’ and
‘AM_JAVACFLAGS’.  None of these rules support per-target flags (yet).

To some extent, even ‘AM_MAKEFLAGS’ (*note Subdirectories::) obeys this naming
scheme.  The slight difference is that ‘MAKEFLAGS’ is passed to sub-‘make’s
implicitly by ‘make’ itself.

‘ARFLAGS’ (*note A Library::) is usually defined by Automake and has neither
‘AM_’ nor per-target cousin.

Finally you should not think that the existence of a per-target variable implies
the existence of an ‘AM_’ variable or of a user variable.  For instance, the
‘mumble_LDADD’ per-target variable overrides the makefile-wide ‘LDADD’ variable
(which is not a user variable), and ‘mumble_LIBADD’ exists only as a per-target
variable.  *Note Program and Library Variables::.


={============================================================================
*kt_linux_tool_150* automake: 08: program variables: case issue

In short LDFLAGS is added before the object files on the command line and LDADD
is added afterwards.

<fail-case>
nexus_inspect_LDADD = \
   @NEXUS_LIBS@            note: this has -Wl,--as-needed

nexus_inspect_LDFLAGS = \
   -lpthread \
   -linit \
   -rdynamic

-Wl,--as-needed -L/zinc-install-root/release/huawei-bcm7409/lib
...
-lnexusMgr -ldirectfb -lfusion -ldirect -linit -lnexus -lz -lpthread

<okay-case>
nexus_inspect_LDFLAGS = @NEXUS_LIBS@

So -lxxx and then --as-needed

-lnexusMgr -ldirectfb -lfusion -ldirect -linit -lnexus -lz -lpthread
...
-Wl,--as-needed -L/zinc-install-root/release/huawei-bcm7409/lib

So not sure how automake combines all but it makes different order. This caues
the problem because shared libraries after as-needed may not linked at the time
of linking and cause undefined symbol when making executable binary.


={============================================================================
*kt_linux_tool_150* automake: 09: scripts

9 Other Derived Objects
***********************

Automake can handle derived objects that are not C programs.  Sometimes the
support for actually building such objects must be explicitly supplied, but
Automake will still automatically handle installation and distribution.

9.1 Executable Scripts
======================

<when-copied>
It is possible to define and install programs that are scripts.  Such
programs are listed using the ‘SCRIPTS’ primary name.  When the script
is distributed in its final, installable form, the ‘Makefile’ usually
looks as follows:

     # Install my_script in $(bindir) and distribute it.
     dist_bin_SCRIPTS = my_script

Scripts are not distributed by default; as we have just seen, those that should
be distributed can be specified using a ‘dist_’ prefix as with other primaries.

Scripts can be installed in ‘bindir’, ‘sbindir’, ‘libexecdir’, ‘pkglibexecdir’,
or ‘pkgdatadir’.

Scripts that need not be installed can be listed in ‘noinst_SCRIPTS’, and among
them, those which are needed only by ‘make check’ should go in ‘check_SCRIPTS’.

<when-to-be-built>
When a script needs to be built, the ‘Makefile.am’ should include the
appropriate rules.  For instance the ‘automake’ program itself is a Perl script
that is generated from ‘automake.in’.  Here is how this is handled:

     bin_SCRIPTS = automake
     CLEANFILES = $(bin_SCRIPTS)
     EXTRA_DIST = automake.in

     do_subst = sed -e 's,[@]datadir[@],$(datadir),g' \
                 -e 's,[@]PERL[@],$(PERL),g' \
                 -e 's,[@]PACKAGE[@],$(PACKAGE),g' \
                 -e 's,[@]VERSION[@],$(VERSION),g' \
                 …

     automake: automake.in Makefile
             $(do_subst) < $(srcdir)/automake.in > automake
             chmod +x automake

Such scripts for which a build rule has been supplied need to be deleted
explicitly using ‘CLEANFILES’ (*note Clean::), and their sources have to be
distributed, usually with ‘EXTRA_DIST’ (*note Basics of Distribution::).

Another common way to build scripts is to process them from ‘configure’ with
‘AC_CONFIG_FILES’.  In this situation Automake knows which files should be
cleaned and distributed, and what the rebuild rules should look like.

For instance if ‘configure.ac’ contains

     AC_CONFIG_FILES([src/my_script], [chmod +x src/my_script])

to build ‘src/my_script’ from ‘src/my_script.in’, then a ‘src/Makefile.am’ to
install this script in ‘$(bindir)’ can be as simple as

     bin_SCRIPTS = my_script
     CLEANFILES = $(bin_SCRIPTS)

There is no need for ‘EXTRA_DIST’ or any build rule: Automake infers them from
‘AC_CONFIG_FILES’ (*note Requirements::).  ‘CLEANFILES’ is still useful, because
by default Automake will clean targets of ‘AC_CONFIG_FILES’ in ‘distclean’, not
‘clean’.

Although this looks simpler, building scripts this way has one drawback:
directory variables such as ‘$(datadir)’ are not fully expanded and may refer to
other directory variables.


={============================================================================
*kt_linux_tool_150* automake: 15: test and make check

15 Support for test suites
**************************

Automake can generate code to handle two kinds of test suites.  One is based on
integration with the ‘dejagnu’ framework.  The other (and most used) form is
based on the use of generic test scripts, and its 'activation' is triggered by
the definition of the special ‘TESTS’ variable.  This second form allows for
various degrees of sophistication and customization; in particular, it allows
for concurrent execution of test scripts, use of established test protocols such
  as TAP, and definition of custom test drivers and test runners.

In either case, the testsuite is 'invoked' via ‘make check’.


15.2.1 Scripts-based Testsuites
-------------------------------

If the 'special' variable ‘TESTS’ is defined, its value is taken to be a list of
'programs' or 'scripts' to 'run' in order to do the testing.  Under the
appropriate circumstances, it’s possible for ‘TESTS’ to list also data files to
be passed to one or more test scripts defined by different means (the so-called
    “log compilers”, *note Parallel Test Harness::).

Test scripts can be executed serially or concurrently.  Automake supports both
these kinds of test execution, with the 'parallel' test harness being the
'default'.  The concurrent test harness relies on the concurrence capabilities
(if any) offered by the underlying ‘make’ implementation, and can thus only be
as good as those are.

By default, only the exit statuses of the test scripts are considered when
determining the testsuite outcome.  But Automake allows also the use of more
complex test protocols, either standard (*note Using the TAP test protocol::) or
custom (*note Custom Test Drivers::). In the rest of this section we are going
to concentrate mostly on protocol-less tests, since we cover test protocols in a
later section (again, *note Custom Test Drivers::).

<return-value>
When no test protocol is in use, an exit status of 0 from a test script will
denote a success, an exit status of 77 a skipped test, an exit status of 99 an
hard error, and any other exit status will denote a failure.

You may define the variable ‘XFAIL_TESTS’ to a list of tests (usually a subset
    of ‘TESTS’) that are expected to fail; this will effectively reverse the
result of those tests (with the provision that skips and hard errors remain
    untouched).  You may also instruct the testsuite harness to treat hard
errors like simple failures, by defining the ‘DISABLE_HARD_ERRORS’ make variable
to a nonempty value.

Note however that, for tests based on more complex test protocols, the exact
effects of ‘XFAIL_TESTS’ and ‘DISABLE_HARD_ERRORS’ might change, or they might
even have no effect at all (for example, in tests using TAP, there is not way to
    disable hard errors, and the ‘DISABLE_HARD_ERRORS’ variable has no effect on
    them).

The result of each test case run by the scripts in ‘TESTS’ will be 'printed' on
standard output, along with the test name.  For test protocols that allow more
test cases per test script (such as TAP), a number, identifier and/or brief
description specific for the single test case is expected to be printed in
addition to the name of the test script.  The possible results (whose meanings
    should be clear from the previous *note Generalities about Testing::) are
‘PASS’, ‘FAIL’, ‘SKIP’, ‘XFAIL’, ‘XPASS’ and ‘ERROR’.  Here is an example of
output from an hypothetical testsuite that uses both plain and TAP tests:

     PASS: foo.sh
     PASS: zardoz.tap 1 - Daemon started
     PASS: zardoz.tap 2 - Daemon responding
     SKIP: zardoz.tap 3 - Daemon uses /proc # SKIP /proc is not mounted
     PASS: zardoz.tap 4 - Daemon stopped
     SKIP: bar.sh
     PASS: mu.tap 1
     XFAIL: mu.tap 2 # TODO frobnication not yet implemented

A testsuite summary (expected to report at least the number of run, skipped and
    failed tests) will be printed at the end of the testsuite run.

If the standard output is connected to a capable terminal, then the test results
and the summary are colored appropriately.  The developer and the user can
disable colored output by setting the ‘make’ variable ‘AM_COLOR_TESTS=no’; the
user can in addition force colored output even without a connecting terminal
with ‘AM_COLOR_TESTS=always’.  It’s also worth noting that some ‘make’
implementations, when used in parallel mode, have slightly different semantics
(*note (autoconf)Parallel make::), which can break the automatic detection of a
connection to a capable terminal.  If this is the case, the user will have to
resort to the use of ‘AM_COLOR_TESTS=always’ in order to have the testsuite
output colorized.


Test programs that need data files should look for them in ‘srcdir’ (which is
    both a make variable and an environment variable made available to the
    tests), so that they work when building in a separate directory (*note Build
      Directories: (autoconf)Build Directories.), and in particular for the
    ‘distcheck’ rule (*note Checking the Distribution::).

The ‘AM_TESTS_ENVIRONMENT’ and ‘TESTS_ENVIRONMENT’ variables can be used to run
initialization code and set environment variables for the test scripts.  The
former variable is developer-reserved, and can be defined in the ‘Makefile.am’,
       while the latter is reserved for the user, which can employ it to extend
         or override the settings in the former; for this to work portably,
            however, the contents of a non-empty ‘AM_TESTS_ENVIRONMENT’ _must_
              be terminated by a semicolon.

The ‘AM_TESTS_FD_REDIRECT’ variable can be used to define file descriptor
redirections for the test scripts.  One might think that ‘AM_TESTS_ENVIRONMENT’
could be used for this purpose, but experience has shown that doing so portably
is practically impossible.  The main hurdle is constituted by Korn shells, which
usually set the close-on-exec flag on file descriptors opened with the ‘exec’
builtin, thus rendering an idiom like ‘AM_TESTS_ENVIRONMENT = exec 9>&2;’
ineffectual.  This issue also affects some Bourne shells, such as the HP-UX’s
‘/bin/sh’,

     AM_TESTS_ENVIRONMENT = \
     ## Some environment initializations are kept in a separate shell
     ## file 'tests-env.sh', which can make it easier to also run tests
     ## from the command line.
       . $(srcdir)/tests-env.sh; \
     ## On Solaris, prefer more POSIX-compliant versions of the standard
     ## tools by default.
       if test -d /usr/xpg4/bin; then \
         PATH=/usr/xpg4/bin:$$PATH; export PATH; \
       fi;
     ## With this, the test scripts will be able to print diagnostic
     ## messages to the original standard error stream, even if the test
     ## driver redirects the stderr of the test scripts to a log file
     ## before executing them.
     AM_TESTS_FD_REDIRECT = 9>&2

Note however that ‘AM_TESTS_ENVIRONMENT’ is, for historical and implementation
reasons, _not_ supported by the serial harness (*note Serial Test Harness::).

Automake ensures that each file listed in ‘TESTS’ is built 'before' it is run;
     you can list both source and derived programs (or scripts) in ‘TESTS’; the
       generated rule will look both in ‘srcdir’ and ‘.’.  For instance, you
       might want to run a C program as a test.  To do this you would list its
       name in ‘TESTS’ and also in ‘check_PROGRAMS’, and then specify it as you
       would any other program.

Programs listed in ‘check_PROGRAMS’ (and ‘check_LIBRARIES’,
    ‘check_LTLIBRARIES’...)  are only built 'during' ‘make check’, not during
‘make all’.  You should list there any program needed by your tests that does
'not' need to be built by ‘make all’.  Note that ‘check_PROGRAMS’ are _not_
automatically added to ‘TESTS’ because ‘check_PROGRAMS’ usually lists programs
used by the tests, not the tests themselves.  Of course you can set ‘TESTS =
$(check_PROGRAMS)’ if all your programs are test cases.


15.2.3 Parallel Test Harness
----------------------------

By default, Automake generated a parallel (concurrent) test harness.  It
features automatic collection of the test scripts output in ‘.log’ files,
         concurrent execution of tests with ‘make -j’, specification of
           inter-test dependencies, lazy reruns of tests that have not completed
           in a prior run, and hard errors for exceptional failures.

The parallel test harness operates by defining a set of ‘make’ rules that run
the test scripts listed in ‘TESTS’, and, for each such script, save its output
in a corresponding ‘.log’ file and its results (and other “metadata”, *note API
    for Custom Test Drivers::) in a corresponding ‘.trs’ (as in Test ReSults)
file.  The ‘.log’ file will contain all the output emitted by the test on its
standard output and its standard error.  The ‘.trs’ file will contain, among the
other things, the results of the test cases run by the script.

The parallel test harness will also create a summary log file, ‘TEST_SUITE_LOG’,
which defaults to ‘test-suite.log’ and requires a ‘.log’ suffix.  This file
  depends upon all the ‘.log’ and ‘.trs’ files created for the test scripts
  listed in ‘TESTS’.

As with the serial harness above, by default one status line is printed per
completed test, and a short summary after the suite has completed.  However,
standard output and standard error of the test are redirected to a per-test log
  file, so that parallel execution does not produce intermingled output.  The
  output from failed tests is collected in the ‘test-suite.log’ file.  If the
  variable ‘VERBOSE’ is set, this file is output after the summary.

Each couple of ‘.log’ and ‘.trs’ files is created when the corresponding test
has completed.  The set of log files is listed in the read-only variable
‘TEST_LOGS’, and defaults to ‘TESTS’, with the executable extension if any
(*note EXEEXT::), as well as any suffix listed in ‘TEST_EXTENSIONS’ removed, and
‘.log’ appended.  Results are undefined if a test file name ends in several
concatenated suffixes.  ‘TEST_EXTENSIONS’ defaults to ‘.test’; it can be
overridden by the user, in which case any extension listed in it must be
constituted by a dot, followed by a non-digit alphabetic character, followed by
any number of alphabetic characters.  For example, ‘.sh’, ‘.T’ and ‘.t1’ are
valid extensions, while ‘.x-y’, ‘.6c’ and ‘.t.1’ are not.

It is important to note that, due to current limitations (unlikely to be
    lifted), configure substitutions in the definition of ‘TESTS’ can only work
if they will expand to a list of tests that have a suffix listed in
  ‘TEST_EXTENSIONS’.

For tests that match an extension ‘.EXT’ listed in ‘TEST_EXTENSIONS’, you can
provide a custom “test runner” using the variable ‘EXT_LOG_COMPILER’ (note the
    upper-case extension) and pass options in ‘AM_EXT_LOG_FLAGS’ and allow the
user to pass options in ‘EXT_LOG_FLAGS’.  It will cause all tests with this
extension to be called with this runner.  For all tests without a registered
extension, the variables ‘LOG_COMPILER’, ‘AM_LOG_FLAGS’, and ‘LOG_FLAGS’ may be
used.  For example,

     TESTS = foo.pl bar.py baz
     TEST_EXTENSIONS = .pl .py
     PL_LOG_COMPILER = $(PERL)
     AM_PL_LOG_FLAGS = -w
     PY_LOG_COMPILER = $(PYTHON)
     AM_PY_LOG_FLAGS = -v
     LOG_COMPILER = ./wrapper-script
     AM_LOG_FLAGS = -d

will invoke ‘$(PERL) -w foo.pl’, ‘$(PYTHON) -v bar.py’, and ‘./wrapper-script -d
baz’ to produce ‘foo.log’, ‘bar.log’, and ‘baz.log’, respectively.  The
‘foo.trs’, ‘bar.trs’ and ‘baz.trs’ files will be automatically produced as a
side-effect.

It’s important to note that, differently from what we’ve seen for the serial
test harness (*note Serial Test Harness::), the ‘AM_TESTS_ENVIRONMENT’ and
‘TESTS_ENVIRONMENT’ variables _cannot_ be use to define a custom test runner;
     the ‘LOG_COMPILER’ and ‘LOG_FLAGS’ (or their extension-specific
         counterparts) should be used instead:

     ## This is WRONG!
     AM_TESTS_ENVIRONMENT = PERL5LIB='$(srcdir)/lib' $(PERL) -Mstrict -w

     ## Do this instead.
     AM_TESTS_ENVIRONMENT = PERL5LIB='$(srcdir)/lib'; export PERL5LIB;
     LOG_COMPILER = $(PERL)
     AM_LOG_FLAGS = -Mstrict -w

By default, the test suite harness will run all tests, but there are several
ways to 'limit' the set of tests that are run:

• You can set the ‘TESTS’ variable.  For example, you can use a command like
this to run only a subset of the tests:

          env TESTS="foo.test bar.test" make -e check

Note however that the command above will unconditionally overwrite the
‘test-suite.log’ file, thus clobbering the recorded results of any previous
testsuite run.  This might be undesirable for packages whose testsuite takes
long time to execute.  Luckily, this problem can easily be avoided by overriding
also ‘TEST_SUITE_LOG’ at runtime; for example,

          env TEST_SUITE_LOG=partial.log TESTS="..." make -e check

will write the result of the partial testsuite runs to the ‘partial.log’,
without touching ‘test-suite.log’.

• You can set the ‘TEST_LOGS’ variable.  By default, this variable is computed
at ‘make’ run time from the value of ‘TESTS’ as described above.  For example,
you can use the following:

          set x subset*.log; shift
          env TEST_LOGS="foo.log $*" make -e check

The comments made above about ‘TEST_SUITE_LOG’ overriding applies here too.

• By default, the test harness removes all old per-test ‘.log’ and ‘.trs’ files
before it starts running tests to regenerate them.  The variable ‘RECHECK_LOGS’
contains the set of ‘.log’ (and, by implication, ‘.trs’) files which are
removed.  ‘RECHECK_LOGS’ defaults to ‘TEST_LOGS’, which means all tests need to
be rechecked.  By overriding this variable, you can choose which tests need to
be reconsidered.  For example, you can lazily rerun only those tests which are
outdated, i.e., older than their prerequisite test files, by setting this
variable to the empty value:

          env RECHECK_LOGS= make -e check

• You can ensure that all tests are rerun which have failed or passed
unexpectedly, by running ‘make recheck’ in the test directory.  This convenience
target will set ‘RECHECK_LOGS’ appropriately before invoking the main test
harness.

In order to guarantee an ordering between tests even with ‘make -jN’,
dependencies between the corresponding ‘.log’ files may be specified through
  usual ‘make’ dependencies.  For example, the following snippet lets the test
  named ‘foo-execute.test’ depend upon completion of the test
  ‘foo-compile.test’:

     TESTS = foo-compile.test foo-execute.test
     foo-execute.log: foo-compile.log

Please note that this ordering ignores the _results_ of required tests, thus the
test ‘foo-execute.test’ is run even if the test ‘foo-compile.test’ failed or was
skipped beforehand.  Further, please note that specifying such dependencies
currently works only for tests that end in one of the suffixes listed in
‘TEST_EXTENSIONS’.

Tests without such specified dependencies may be run concurrently with parallel
‘make -jN’, so be sure they are prepared for concurrent execution.

The combination of lazy test execution and correct dependencies
between tests and their sources may be exploited for efficient unit
testing during development.  To further speed up the edit-compile-test
cycle, it may even be useful to specify compiled programs in
‘EXTRA_PROGRAMS’ instead of with ‘check_PROGRAMS’, as the former allows
intertwined compilation and test execution (but note that
‘EXTRA_PROGRAMS’ are not cleaned automatically, *note Uniform::).

The variables ‘TESTS’ and ‘XFAIL_TESTS’ may contain conditional parts as well as
configure substitutions.  In the latter case, however, certain restrictions
apply: substituted test names must end with a nonempty test suffix like ‘.test’,
so that one of the inference rules generated by ‘automake’ can apply.  For
  literal test names, ‘automake’ can generate per-target rules to avoid this
  limitation.

Please note that it is currently not possible to use ‘$(srcdir)/’ or
‘$(top_srcdir)/’ in the ‘TESTS’ variable.  This technical limitation is
necessary to avoid generating test logs in the source tree and has the
unfortunate consequence that it is not possible to specify distributed tests
that are themselves generated by means of explicit rules, in a way that is
portable to all ‘make’ implementations (*note (autoconf)Make Target Lookup::,
    the semantics of FreeBSD and OpenBSD ‘make’ conflict with this).  In case of
doubt you may want to require to use GNU ‘make’, or work around the issue with
inference rules to generate the tests.



={============================================================================
*kt_linux_tool_160* ccache

The ccache tool tries to speed up a build by taking object files from cache when
the preprocessed source didn't change. In addition, it can often avoid
preprocessing, as a cache lookup on the source file path and compiler options
returns a list of include files and their modification times. If those weren't
updated, that counts as a 'direct hit'. Otherwise it falls back to preprocessing
the source and trying to look-up its hash in the cache. If successful, that
counts as a 'preprocessed hit'.

<problem>
The main barrier preventing cache sharing across git branches has to do with the fact we create
separate build slaves for each git branch, which results in different source and binary directories
being seen by the compiler. More specifically:

The compiler sets the __FILE__ macro to be the absolute path to the source file. The value of that
macro leaks to preprocessed source through the use of assert().

We pass preprocessor definitions like -DMACRO__prefix="...", -DMACRO__builddir="...",
   -DMACRO__top_srcdir="...", which can also leak to preprocessed source.

note: After all, like to have speed-up when switching between branches since the most are the same
between them.

branch 01                                    branch 02                     
 source dirs, build root dirs                 source dirs, build root dirs

             -> maps                             <- maps to virtual
                        _virtual_

So it is not to save space but to make ccache see _virtual_ when switching branches so that have
more cache hit.

To address this issue, the new zb-virtual-slave tool was developed. It's meant to wrap around other
build tools, such as zb-build-with-progress, zb-deploy, zb-shell, etc. and leading them to believe
we are always building a branch called _virtual_. This way, the compiler will see identical source
and binary locations, enabling cache hits across different git branches. zb-virtual-slave uses the
map-dir-and-exec tool internally, which you will have to build and install yourself.

note: need to install map-dir-and-exec package

CCACHE_DIR

The CCACHE_DIR environment variable specifies where ccache will keep its cached
compiler output. The default is $HOME/.ccache.

You can monitor the current usage of ccache by executing

CCACHE_DIR=.. $watch -n1 -d ccache -s


={============================================================================
*kt_linux_tool_161* icecream

icecream is a better version of distcc - a tool for distributing build jobs
across multiple machines.

It has a central scheduler (running `icecc-scheduler`) which runs on some server
and one or more nodes (machines running `iceccd`). The scheduler distributes
jobs depending on the load and speed of nodes - always picking the fastest and
least-loaded (preferring localhost when load is low).

One nice thing is it lets you specify what toolchain you are using and icecream
will distribute your toolchain for you so the remote machine can build
compatible object files.

 
note: do all as a root

Building icecream
-----------------
To build icecream the following worked for me, putting all files under
/opt/icecream


darren@djg-ubuntu:/data/software/icecream/build$ cat /data/documents/recipes/icecream.sh

#!/bin/bash
 
# Get source
git clone https://github.com/icecc/icecream.git
cd icecream
 
# Get dependencies
sudo apt-get install -y libcap-ng-dev liblzo2-dev docbook2x

or

sudo apt-get install -y --force-yes libcap-ng-dev liblzo2-dev docbook2x
 
# Build it
mkdir build
cd build
../autogen.sh
../configure --with-pic --prefix=/opt/icecream
make -j8
sudo make install

note: should run build on /opt/icecream but not /opt/icecream/build since fails
to search headers when building.


Preparing the Toolchain
-----------------------
toolchainTarball=$(/opt/icecream/bin/icecc --build-native | grep creating | cut -d' ' -f2)
sudo mkdir -p /opt/icecream/etc/
sudo cp -a $toolchainTarball /opt/icecream/etc/gcc-native.tar.gz


Preparing the Scheduler Node
-----------------------
Just pick a fast server for this. The scheduler doesn't appear to use much in
the way of resources. Then run: 

/opt/icecream/sbin/icecc-scheduler -d


Preparing Client Nodes
-----------------------

Add the icecc user (`sudo useradd icecc`).
Run the iceccd daemon (`sudo /opt/icecream/sbin/iceccd -d`). Note that it drops
privileges after chrooting.

If you're on Debian, this should work for you:

sudo apt-get install liblzo2-d libcap-ng-dev
curl devnfs2/~darren.garvey/icecream-debian.tar.bz2 | sudo tar -C / -xjf - && sudo useradd icecc && sudo /opt/icecream/sbin/iceccd -d

note: For debian whiz: sudo apt-get install liblzo2-2 libcap-ng-dev


={============================================================================
*kt_linux_tool_200* bin-:

{gcc-binutil}
https://sourceware.org/binutils/
http://sourceware.org/binutils/docs-2.20/
http://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html

GCC provides a large number of built-in functions other than the ones mentioned
above. Some of these are for internal use in the processing of exceptions or
variable-length argument lists and are not documented here because they may
change from time to time; we do not recommend general use of these functions. 


={============================================================================
*kt_linux_tool_201* bin-readelf

Program Header in ELF

Executable and shared object files statically represent programs. To execute
such programs, the system uses the files to create dynamic program
representations, or process images. A process image has segments that hold its
text, data, stack, and so on. The major sections in this part discuss the
following.

The primary data structure, a program header table, locates segment images
within the file and contains other information necessary to create the memory
image for the program. Each entry describing a segment or other information the
system needs to prepare the program for execution. An object file segment
contains one or more sections.

Program headers are meaningful only for executable and shared object files.

<all-headers>
-e : headers
-e --headers           Equivalent to: -h -l -S
  -h --file-header       Display the ELF file header
  -l --program-headers   Display the program headers
     --segments          An alias for --program-headers
  -S --section-headers   Display the sections' header
     --sections          An alias for --section-headers

# mipsel-linux-uclibc-readelf -e vmlinux
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           MIPS R3000
  Version:                           0x1
  Entry point address:               0x802de000
  Start of program headers:          52 (bytes into file)
  Start of section headers:          3158368 (bytes into file)
  Flags:                             0x50001001, noreorder, o32, mips32
  Size of this header:               52 (bytes)
  Size of program headers:           32 (bytes)
  Number of program headers:         1
  Size of section headers:           40 (bytes)
  Number of section headers:         21
  Section header string table index: 18

Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .text             PROGBITS        80001000 001000 26f89c 00  AX  0   0 2048
  [ 2] __ex_table        PROGBITS        802708a0 2708a0 001a68 00   A  0   0  4
  [ 3] .rodata           PROGBITS        80272310 272310 02b8e0 00   A  0   0 16
  [ 4] .pci_fixup        PROGBITS        8029dbf0 29dbf0 0002e0 00   A  0   0  4
  [ 5] __ksymtab         PROGBITS        8029ded0 29ded0 0039a0 00   A  0   0  4
  [ 6] __ksymtab_gpl     PROGBITS        802a1870 2a1870 000678 00   A  0   0  4
  [ 7] __ksymtab_strings PROGBITS        802a1ee8 2a1ee8 009080 00   A  0   0  4
  [ 8] __param           PROGBITS        802aaf68 2aaf68 0001f4 00   A  0   0  4
  [ 9] .data             PROGBITS        802ac000 2ac000 02fa10 00  WA  0   0 8192
  [10] .data.cacheline_a PROGBITS        802dc000 2dc000 0010c0 00  WA  0   0 32
  [11] .init.text        PROGBITS        802de000 2de000 021bbc 00  AX  0   0  4
  [12] .init.data        PROGBITS        802ffbc0 2ffbc0 0025e4 00  WA  0   0  8
  [13] .init.setup       PROGBITS        803021b0 3021b0 0001d4 00  WA  0   0  4
  [14] .initcall.init    PROGBITS        80302384 302384 000168 00  WA  0   0  4
  [15] .con_initcall.ini PROGBITS        803024ec 3024ec 000004 00  WA  0   0  4
  [16] .init.ramfs       PROGBITS        80303000 303000 000085 00   A  0   0  1
  [17] .bss              NOBITS          80304000 303085 024020 00  WA  0   0 4096
  [18] .shstrtab         STRTAB          00000000 303085 0000d8 00      0   0  1
  [19] .symtab           SYMTAB          00000000 3034a8 03a800 10     20 10448  4
  [20] .strtab           STRTAB          00000000 33dca8 044b17 00      0   0  1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings)
  I (info), L (link order), G (group), x (unknown)
  O (extra OS processing required) o (OS specific), p (processor specific)

Program Headers:
  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
  LOAD           0x001000 0x80001000 0x80001000 0x302085 0x327020 RWE 0x2000

 Section to Segment mapping:
  Segment Sections...
   00     .text __ex_table .rodata .pci_fixup __ksymtab __ksymtab_gpl __ksymtab_strings __param
   .data .data.cacheline_aligned .init.text .init.data .init.setup .initcall.init .con_initcall.init
   .init.ramfs .bss


# mipsel-linux-uclibc-readelf -e xx.a
File: /home/NDS-UK/parkkt/platforms/mstar/libverifier.a(pairglbo.o)
ELF Header:
  Magic:   7f 45 4c 46 01 02 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, big endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              REL (Relocatable file)
  Machine:                           MIPS R3000
  Version:                           0x1
  Entry point address:               0x0
  Start of program headers:          0 (bytes into file)
  Start of section headers:          5304 (bytes into file)
  Flags:                             0x70001005, noreorder, cpic, o32, mips32r2
  Size of this header:               52 (bytes)
  Size of program headers:           0 (bytes)
  Number of program headers:         0
  Size of section headers:           40 (bytes)
  Number of section headers:         14
  Section header string table index: 11

Section Headers:

There are no program headers in this file.

File: /home/NDS-UK/parkkt/platforms/mstar/libverifier.a(p_posix.o)
ELF Header:
  Magic:   7f 45 4c 46 01 02 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, big endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              REL (Relocatable file)
  Machine:                           MIPS R3000
  Version:                           0x1
  Entry point address:               0x0
  Start of program headers:          0 (bytes into file)
  Start of section headers:          5180 (bytes into file)
  Flags:                             0x70001005, noreorder, cpic, o32, mips32r2
  Size of this header:               52 (bytes)
  Size of program headers:           0 (bytes)
  Number of program headers:         0
  Size of section headers:           40 (bytes)
  Number of section headers:         15
  Section header string table index: 12

Section Headers:


<speficy-section>
mipsel-linux-uclibc-readelf -x 13 vmlinux

<sections>
The .text section contains the executable program code. 

The .rodata section contains constant data in your program. 

The .data section generally contains initialized global data used by the C library prologue code and
can contain large initialized data items from your application.  The .sdata section is used for
smaller initialized global data items and exists only on some architectures. Some processor
architectures can make use of optimized data access when the attributes of the memory area are
known. The .sdata and .sbss sections enable these optimizations. 

The .bss and .sbss sections contain uninitialized (global) data in your program. These sections
occupy no space in the program image their memory space is allocated and initialized to zero on
program startup by C library prologue code.


{example-analysis}
/*
** This is sample program to see how elfs is made and allocated
**
** ktpark
*/

/* bss */
int kt_bss_vars[100];

/* data */
int kt_data_vars[100]={0x01};

/* constant */
char* kt_const = "this is constant string array.\n";

int main( int argc, char** argv )
{
  int kt_local_bss_vars[100];
  int kt_local_data_vars[100]={0xFF};
  char* kt_local_const = "this is local constant string array.\n";
  int i;

  printf("\n\n this is sample program to see elf.\n\n");

  for(i=0; i <= 10;i++)
  {
    sleep(1000);
  }

  printf("\nend of program.\n");
}
--

# free (before)
              total         used         free       shared      buffers
  Mem:       116472        18052        98420            0         8192
 Swap:            0            0            0
Total:       116472        18052        98420

# free (after)
              total         used         free       shared      buffers
  Mem:       116472        18068        98404            0         8192
 Swap:            0            0            0
Total:       116472        18068        98404

16K used.

<read-sections>
-S
--sections
--section-headers

Displays the information contained in the file's section headers, if it has any.


$ readelf -S a.out 
There are 26 section headers, starting at offset 0xfac:

Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .interp           PROGBITS        00400114 000114 000014 00   A  0   0  1
  [ 2] .reginfo          MIPS_REGINFO    00400128 000128 000018 18   A  0   0  4
  [ 3] .dynamic          DYNAMIC         00400140 000140 0000d8 08   A  6   0  4
  [ 4] .hash             HASH            00400218 000218 0000a4 04   A  5   0  4
  [ 5] .dynsym           DYNSYM          004002bc 0002bc 000160 10   A  6   1  4
  [ 6] .dynstr           STRTAB          0040041c 00041c 0000eb 00   A  0   0  1
  [ 7] .init             PROGBITS        00400508 000508 00008c 00  AX  0   0  4
  [ 8] .text             PROGBITS        004005a0 0005a0 000320 00  AX  0   0 16 (800)
  [ 9] .MIPS.stubs       PROGBITS        004008c0 0008c0 000050 00  AX  0   0  4
  [10] .fini             PROGBITS        00400910 000910 000050 00  AX  0   0  4
  [11] .rodata           PROGBITS        00400960 000960 000220 00   A  0   0 16 (544)
  [12] .eh_frame         PROGBITS        00400b80 000b80 000004 00   A  0   0  4
  [13] .ctors            PROGBITS        00410b84 000b84 000008 00  WA  0   0  4
  [14] .dtors            PROGBITS        00410b8c 000b8c 000008 00  WA  0   0  4
  [15] .jcr              PROGBITS        00410b94 000b94 000004 00  WA  0   0  4
  [16] .data             PROGBITS        00410ba0 000ba0 0001d0 00  WA  0   0 16 (464)
  [17] .rld_map          PROGBITS        00410d70 000d70 000004 00  WA  0   0  4
  [18] .got              PROGBITS        00410d80 000d80 00004c 04 WAp  0   0 16
  [19] .bss              NOBITS          00410dd0 000dcc 0001b0 00  WA  0   0 16 (432)
  [20] .comment          PROGBITS        00000000 000dcc 00005a 00      0   0  1
  [21] .mdebug.abi32     PROGBITS        0000005a 000e26 000000 00      0   0  1
  [22] .pdr              PROGBITS        00000000 000e28 0000c0 00      0   0  4
  [23] .shstrtab         STRTAB          00000000 000ee8 0000c3 00      0   0  1
  [24] .symtab           SYMTAB          00000000 0013bc 0004b0 10     25  46  4
  [25] .strtab           STRTAB          00000000 00186c 000254 00      0   0  1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings)
  I (info), L (link order), G (group), x (unknown)
O (extra OS processing required) o (OS specific), p (processor specific)

<read-rodata>
$ readelf -x 11 a.out (rodata)

Hex dump of section '.rodata':
  0x00400960 746e6174 736e6f63 20736920 73696874 this is constant
  0x00400970 000a2e79 61727261 20676e69 72747320  string array...
  0x00400980 00000000 00000000 00000000 000000ff ................
  0x00400990 00000000 00000000 00000000 00000000 ................
  0x004009a0 00000000 00000000 00000000 00000000 ................
  0x004009b0 00000000 00000000 00000000 00000000 ................
  0x004009c0 00000000 00000000 00000000 00000000 ................
  0x004009d0 00000000 00000000 00000000 00000000 ................
  0x004009e0 00000000 00000000 00000000 00000000 ................
  0x004009f0 00000000 00000000 00000000 00000000 ................
  0x00400a00 00000000 00000000 00000000 00000000 ................
  0x00400a10 00000000 00000000 00000000 00000000 ................
  0x00400a20 00000000 00000000 00000000 00000000 ................
  0x00400a30 00000000 00000000 00000000 00000000 ................
  0x00400a40 00000000 00000000 00000000 00000000 ................
  0x00400a50 00000000 00000000 00000000 00000000 ................
  0x00400a60 00000000 00000000 00000000 00000000 ................
  0x00400a70 00000000 00000000 00000000 00000000 ................
  0x00400a80 00000000 00000000 00000000 00000000 ................
  0x00400a90 00000000 00000000 00000000 00000000 ................
  0x00400aa0 00000000 00000000 00000000 00000000 ................
  0x00400ab0 00000000 00000000 00000000 00000000 ................
  0x00400ac0 00000000 00000000 00000000 00000000 ................
  0x00400ad0 00000000 00000000 00000000 00000000 ................
  0x00400ae0 00000000 00000000 00000000 00000000 ................
  0x00400af0 00000000 00000000 00000000 00000000 ................
  0x00400b00 00000000 00000000 00000000 00000000 ................
  0x00400b10 6f63206c 61636f6c 20736920 73696874 this is local co
  0x00400b20 72612067 6e697274 7320746e 6174736e nstant string ar
  0x00400b30 20736968 74200a0a 0000000a 2e796172 ray....... this 
  0x00400b40 6172676f 72702065 6c706d61 73207369 is sample progra
  0x00400b50 000a0a2e 666c6520 65657320 6f74206d m to see elf....
  0x00400b60 2e6d6172 676f7270 20666f20 646e650a .end of program.
  0x00400b70 00000000 00000000 00000000 0000000a ................


{example-analysis}
#include <stdio.h>

int bss_var; /* Uninitialized global variable */
int data_var = 1; /* Initialized global variable */

int main(int argc, char **argv)
{
  void *stack_var; /* Local variable on the stack */
  stack_var = (void *)main; /* Don't let the compiler optimize it out */

  printf("Hello, World! Main is executing at %p\n", stack_var);
  printf("This address (%p) is in our stack frame\n", &stack_var);

  /* bss section contains uninitialized data */
  printf("This address (%p) is in our bss section\n", &bss_var);

  /* data section contains initializated data */
  printf("This address (%p) is in our data section\n", &data_var);

  return 0;
}

root@amcc:~# ./hello
Hello, World! Main is executing at 0x10000418
This address (0x7ff8ebb0) is in our stack frame
This address (0x10010a1c) is in our bss section
This address (0x10010a18) is in our data section


{section-and-nm-map}
(from the readelf of kernel)
Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .init             PROGBITS      40018000 008000 01c000 00 WAX  0   0 32
  [ 2] .text             PROGBITS      40034000 024000 2af998 00  AX  0   0 32
  [ 3] .pci_fixup        PROGBITS        402e4000 2d4000 000490 00   A  0   0  4
  [ 4] __ksymtab         PROGBITS        402e4490 2d4490 004020 00   A  0   0  4
  [ 5] __ksymtab_gpl     PROGBITS        402e84b0 2d84b0 000f40 00   A  0   0  4
  [ 6] __ksymtab_gpl_fut PROGBITS        402e93f0 2d93f0 000018 00   A  0   0  4
  [ 7] __ksymtab_strings PROGBITS        402e9408 2d9408 00b4d8 00   A  0   0  4
  [ 8] __param           PROGBITS        402f5000 2e5000 0004b0 00   A  0   0  4
  [ 9] .data             PROGBITS        402f8000 2e8000 05c210 00  WA  0   0 32
  [10] .bss              NOBITS          40354220 344210 020438 00  WA  0   0 32
  [11] .comment          PROGBITS        00000000 344210 002e68 00      0   0  1
  [12] .ARM.attributes   ARM_ATTRIBUTES  00000000 347078 000010 00      0   0  1
  [13] .debug_abbrev     PROGBITS        00000000 347088 0b55ee 00      0   0  1
  [14] .debug_info       PROGBITS        00000000 3fc676 16124fa 00      0   0  1
  [15] .debug_line       PROGBITS        00000000 1a0eb70 17d764 00      0   0  1
  [16] .debug_pubnames   PROGBITS        00000000 1b8c2d4 0210e5 00      0   0  1
  [17] .debug_str        PROGBITS        00000000 1bad3b9 09173a 01  MS  0   0  1
  [18] .debug_aranges    PROGBITS        00000000 1c3eaf3 006408 00      0   0  1
  [19] .debug_frame      PROGBITS        00000000 1c44efc 05b4b0 00      0   0  4
  [20] .debug_loc        PROGBITS        00000000 1ca03ac 23ee57 00      0   0  1
  [21] .debug_ranges     PROGBITS        00000000 1edf203 069400 00      0   0  1
  [22] .shstrtab         STRTAB          00000000 1f48603 000113 00      0   0  1
  [23] .symtab           SYMTAB          00000000 1f48b00 07f8a0 10     24 26436  4
  [24] .strtab           STRTAB          00000000 1fc83a0 05e3c9 00      0   0  1
  
(from the map)
0000000040034000 T _text
00000000402f54b0 A _etext
00000000402f54b0-0000000040034000=0x2C14B0(2,888,880)

Why these are different in size??

<dynamic>
  -d --dynamic           Display the dynamic section (if present)


={============================================================================
*kt_linux_tool_202* binutil: check if it is built with debug symbols

note:
file command do not show difference between nodebug and debug binary.

{readelf}

<on-nodebug-build>

note: see no debug sections

$ readelf -S nickelmediad 
There are 37 section headers, starting at offset 0xe4bfc:

$ readelf -S
Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .interp           PROGBITS        00400194 000194 000014 00   A  0   0  1
  [ 2] .reginfo          MIPS_REGINFO    004001a8 0001a8 000018 18   A  0   0  4
  [ 3] .dynamic          DYNAMIC         004001c0 0001c0 000188 08   A  6   0  4
  [ 4] .hash             HASH            00400348 000348 000998 04   A  5   0  4
  [ 5] .dynsym           DYNSYM          00400ce0 000ce0 0015d0 10   A  6   1  4
  [ 6] .dynstr           STRTAB          004022b0 0022b0 003d20 00   A  0   0  1
  [ 7] .gnu.version      VERSYM          00405fd0 005fd0 0002ba 02   A  5   0  2
  [ 8] .gnu.version_r    VERNEED         0040628c 00628c 000080 00   A  6   2  4
  [ 9] .rel.dyn          REL             0040630c 00630c 000238 08   A  5   0  4
  [10] .rel.plt          REL             00406bfc 006bfc 0003b0 08   A  5  12  4
  [11] .init             PROGBITS        00406fac 006fac 000048 00  AX  0   0  4
  [12] .plt              PROGBITS        00407000 007000 000784 00  AX  0   0 32
  [13] .text             PROGBITS        00407790 007790 0aa1f0 00  AX  0   0 16
  [14] .MIPS.stubs       PROGBITS        004b1980 0b1980 000670 00  AX  0   0  4
  [15] .fini             PROGBITS        004b1ff0 0b1ff0 000038 00  AX  0   0  4
  [16] .rodata           PROGBITS        004b2030 0b2030 009a78 00   A  0   0 16
  [17] .eh_frame_hdr     PROGBITS        004bbaa8 0bbaa8 001afc 00   A  0   0  4
  [18] .eh_frame         PROGBITS        004cdbc0 0bdbc0 009068 00  WA  0   0  4
  [19] .gcc_except_table PROGBITS        004d6c28 0c6c28 011914 00  WA  0   0  4
  [20] .ctors            PROGBITS        004e853c 0d853c 000024 00  WA  0   0  4
  [21] .dtors            PROGBITS        004e8560 0d8560 000008 00  WA  0   0  4
  [22] .jcr              PROGBITS        004e8568 0d8568 000004 00  WA  0   0  4
  [23] .data.rel.ro      PROGBITS        004e8570 0d8570 001a8c 00  WA  0   0  8
  [24] .data             PROGBITS        004ea000 0da000 0003d0 00  WA  0   0 16
  [25] .rld_map          PROGBITS        004ea3d0 0da3d0 000004 00  WA  0   0  4
  [26] .got.plt          PROGBITS        004ea3d4 0da3d4 0001e0 00  WA  0   0  4
  [27] .got              PROGBITS        004ea5c0 0da5c0 00079c 04 WAp  0   0 16
  [28] .sdata            PROGBITS        004ead5c 0dad5c 000004 00 WAp  0   0  4
  [29] .bss              NOBITS          004ead60 0dad60 001270 00  WA  0   0 16
  [30] .comment          PROGBITS        00000000 0dad60 00018c 00      0   0  1
  [31] .gnu.attributes   LOOS+ffffff5    00000000 0daeec 000010 00      0   0  1
  [32] .mdebug.abi32     PROGBITS        00000090 0daefc 000000 00      0   0  1
  [33] .pdr              PROGBITS        00000000 0daefc 009bc0 00      0   0  4
  [34] .shstrtab         STRTAB          00000000 0e4abc 00013e 00      0   0  1
  [35] .symtab           SYMTAB          00000000 0e51c4 009b20 10     36 2134  4
  [36] .strtab           STRTAB          00000000 0eece4 0457fd 00      0   0  1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings)
  I (info), L (link order), G (group), T (TLS), E (exclude), x (unknown)
  O (extra OS processing required) o (OS specific), p (processor specific)

<on-debug-build>
There are 46 section headers, starting at offset 0x6e857c:

Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .interp           PROGBITS        00400194 000194 000014 00   A  0   0  1
  [ 2] .reginfo          MIPS_REGINFO    004001a8 0001a8 000018 18   A  0   0  4
  [ 3] .dynamic          DYNAMIC         004001c0 0001c0 000188 08   A  6   0  4
  [ 4] .hash             HASH            00400348 000348 001814 04   A  5   0  4
  [ 5] .dynsym           DYNSYM          00401b5c 001b5c 003fa0 10   A  6   1  4
  [ 6] .dynstr           STRTAB          00405afc 005afc 0121b8 00   A  0   0  1
  [ 7] .gnu.version      VERSYM          00417cb4 017cb4 0007f4 02   A  5   0  2
  [ 8] .gnu.version_r    VERNEED         004184a8 0184a8 000060 00   A  6   2  4
  [ 9] .rel.dyn          REL             00418508 018508 000328 08   A  5   0  4
  [10] .rel.plt          REL             00419000 019000 0003f8 08   A  5  12  4
  [11] .init             PROGBITS        004193f8 0193f8 000048 00  AX  0   0  4
  [12] .plt              PROGBITS        00419440 019440 000814 00  AX  0   0 32
  [13] .text             PROGBITS        00419c60 019c60 187120 00  AX  0   0 16
  [14] .MIPS.stubs       PROGBITS        005a0d80 1a0d80 000620 00  AX  0   0  4
  [15] .fini             PROGBITS        005a13a0 1a13a0 000038 00  AX  0   0  4
  [16] .rodata           PROGBITS        005a13e0 1a13e0 017438 00   A  0   0 16
  [17] .eh_frame_hdr     PROGBITS        005b8818 1b8818 009114 00   A  0   0  4
  [18] .eh_frame         PROGBITS        005d1bb0 1c1bb0 02c034 00  WA  0   0  4
  [19] .gcc_except_table PROGBITS        005fdbe4 1edbe4 0147bc 00  WA  0   0  4
  [20] .ctors            PROGBITS        006123a0 2023a0 000024 00  WA  0   0  4
  [21] .dtors            PROGBITS        006123c4 2023c4 000008 00  WA  0   0  4
  [22] .jcr              PROGBITS        006123cc 2023cc 000004 00  WA  0   0  4
  [23] .data.rel.ro      PROGBITS        006123d0 2023d0 001c2c 00  WA  0   0  8
  [24] .data             PROGBITS        00614000 204000 0003d0 00  WA  0   0 16
  [25] .rld_map          PROGBITS        006143d0 2043d0 000004 00  WA  0   0  4
  [26] .got.plt          PROGBITS        006143d4 2043d4 000204 00  WA  0   0  4
  [27] .got              PROGBITS        006145e0 2045e0 002580 04 WAp  0   0 16
  [28] .sdata            PROGBITS        00616b60 206b60 000004 00 WAp  0   0  4
  [29] .bss              NOBITS          00616b70 206b64 001270 00  WA  0   0 16
  [30] .comment          PROGBITS        00000000 206b64 00018c 00      0   0  1
  [31] .debug_aranges    MIPS_DWARF      00000000 206cf0 008908 00      0   0  1       // {
  [32] .debug_pubnames   MIPS_DWARF      00000000 20f5f8 0c6fea 00      0   0  1
  [33] .debug_info       MIPS_DWARF      00000000 2d65e2 1a07f7 00      0   0  1
  [34] .debug_abbrev     MIPS_DWARF      00000000 476dd9 006741 00      0   0  1
  [35] .debug_line       MIPS_DWARF      00000000 47d51a 039d68 00      0   0  1
  [36] .debug_frame      MIPS_DWARF      00000000 4b7284 03f620 00      0   0  4
  [37] .debug_str        MIPS_DWARF      00000000 4f68a4 14d3fc 01  MS  0   0  1
  [38] .debug_loc        MIPS_DWARF      00000000 643ca0 05326e 00      0   0  1
  [39] .debug_ranges     MIPS_DWARF      00000000 696f0e 013188 00      0   0  1       // }
  [40] .gnu.attributes   LOOS+ffffff5    00000000 6aa096 000010 00      0   0  1
  [41] .mdebug.abi32     PROGBITS        00000090 6aa0a6 000000 00      0   0  1
  [42] .pdr              PROGBITS        00000000 6aa0a8 03e320 00      0   0  4
  [43] .shstrtab         STRTAB          00000000 6e83c8 0001b4 00      0   0  1
  [44] .symtab           SYMTAB          00000000 6e8cac 020680 10     45 7279  4
  [45] .strtab           STRTAB          00000000 70932c 10ffac 00      0   0  1


{readelf-dump}
Also this command line shows no output on nodebug version but shows all symbols on debug version.

$ readelf --debug-dump=decodedline nickelmediad 

readelf -w

-w[lLiaprmfFsoRt]
--debug-dump[=rawline,=decodedline,=info,=abbrev,=pubnames,=aranges,=macro,=frames,=frames-interp,=str,=loc,=Ranges,=pubtypes,=trace_info,=trace_abbrev,=trace_aranges,=gdb_index]

   Displays the contents of the debug sections in the file, if any are
   present.  If one of the optional letters or words follows the switch then
   only data found in those specific sections will be dumped.

   Note that there is no single letter option to display the content of trace
   sections or .gdb_index.

   Note: the =decodedline option will display the interpreted contents of a
   .debug_line section whereas the =rawline option dumps the contents in a raw
   format.

   Note: the =frames-interp option will display the interpreted contents of a
   .debug_frame section whereas the =frames option dumps the contents in a raw
   format.

   Note: the output from the =info option can also be affected by the options
   --dwarf-depth and --dwarf-start.


{use-gdb}
Used gdb on host.

for nodebug version.

# gdb /bin/nickelmediad
...
Reading symbols from /opt/zinc-trunk/bin/nickelmediad...(no debugging symbols found)...done. ~
(gdb) 

for debug version.

# gdb /bin/nickelmediad
...
Reading symbols from /home/kpark/builds/Nickel.System.DBusServer/.libs/nickelmediad...done.
(gdb)


={============================================================================
*kt_linux_tool_202* binutil: call trace of debug and release build

1. Seems that core file are the same since the size is the same and show the
same gdb result when use with both debug and release executable.

-rw-------   1 kpark kpark 397312 Sep  1 14:11 7842.core
-rw-------   1 kpark kpark 397312 Sep  1 14:15 7960.core
-rwxr-xr-x   1 kpark kpark  27974 Sep  1 13:40 a-g.out*
-rwxr-xr-x   1 kpark kpark  10350 Sep  1 14:15 a-r.out*

<debug> with -g option

$ gdb a-g.out 7960.core
...
Reading symbols from /home/kpark/work/a-g.out...done.

warning: core file may not match specified executable file.
[New LWP 7960]

warning: Can't read pathname for load map: Input/output error.
Core was generated by `./a.out'.
Program terminated with signal 11, Segmentation fault.
#0  0x0000000000400a1a in main () at tdebug.cpp:16
16	    cout << "anode's next is " << anode.next->val << endl;;

(gdb) l
11	
12	int main()
13	{
14	    node anode = { NULL, 0 };
15	
16	    cout << "anode's next is " << anode.next->val << endl;;
17	
18	    cout << "anode {" << anode.next << ", " << anode.val << "}" << endl;
19	}

(gdb) bt
#0  0x0000000000400a1a in main () at tdebug.cpp:16
(gdb) 


<release>
$ gdb a.out 7960.core
...
Reading symbols from /home/kpark/work/a.out...(no debugging symbols found)...done.
Illegal process-id: 7960.core.
[New LWP 7960]

warning: Can't read pathname for load map: Input/output error.
Core was generated by `./a.out'.
Program terminated with signal 11, Segmentation fault.
#0  0x0000000000400a1a in main ()

(gdb) l
No symbol table is loaded.  Use the "file" command.

(gdb) bt
#0  0x0000000000400a1a in main ()

2. readelf output above are true for debug and release build.


={============================================================================
*kt_linux_tool_202* bin-nm

{nm-find-symbol}

%nm *.a

%/opt/toolchains/bin/mipsel-linux-uclibc-nm
%nm libicammulti.a | grep vendor

00000bc0 t _GLOBAL__I_vendor_init
00000980 T vendor_cleanup
00000000 T vendor_init
000002c8 T vendor_setup
000000bc r _ZZ12vendor_setupE12__FUNCTION__
00000088 r _ZZ12vendor_setupE19__PRETTY_FUNCTION__


$ nm -A /usr/lib/lib*.so 2> /dev/null | grep ' crypt$'
/usr/lib/libcrypt.so:00007080 W crypt

<help>
% man nm

nm -C -U to get a demangled list of all undefined references

-A
-o
--print-file-name
Precede each symbol by the name of the input file (or archive member) in which
it was found, rather than identifying the input file once only, before all of
its symbols.

-D
--dynamic
Display the dynamic symbols rather than the normal symbols. This is only
meaningful for dynamic objects, such as certain types of shared libraries.

note: this shows only dynamic symbols and shows a smaller set than running it
without option.

Uppercase means global and lowercase means local. 

B        is bss
T/t      is text and means symbols defined in this object file. 
D        data
A        is this address is absolute and is not subject to modification by an
         additional link stage
U        is undefined meaning extern

"W"
"w" The symbol is a weak symbol that has not been specifically tagged as a weak
object symbol. When a weak defined symbol is linked with a normal defined
symbol, the normal defined symbol is used with no error. When a weak undefined
symbol is linked and the symbol is not defined, the value of the symbol is
determined in a system-specific manner without error. On some systems, uppercase
indicates that a default value has been specified.

note:
Cannot use nm for a stripped object

nm: ./local/lib/libWasabi.so: no symbols


={============================================================================
*kt_linux_tool_203* binutil: objdump

gcc -S
objdump -d ELF > out.txt

note: difference between gcc -S and objdump? objdump do on objects after complilation.

objdump
-d, --disassemble        Display assembler contents of executable sections
-S, --source             Intermix source code with disassembly


={============================================================================
*kt_linux_tool_204* binutil: addr2line

Usage: ./mipsel-linux-uclibc-addr2line [option(s)] [addr(s)]

Convert addresses into line number/file name pairs. If no addresses are specified on the command
line, they will be read from stdin The options are:

  @<file>                Read options from <file>
  -b --target=<bfdname>  Set the binary file format
  -e --exe=<executable>  Set the input file name (default is a.out)
  -i --inlines           Unwind inlined functions
  -j --section=<name>    Read section-relative offsets instead of addresses
  -s --basenames         Strip directory names
  -f --functions         Show function names
  -C --demangle[=style]  Demangle function names
  -h --help              Display this information
  -v --version           Display the program's version

./mipsel-linux-uclibc-addr2line: supported targets: elf32-tradlittlemips elf32-tradbigmips
ecoff-littlemips ecoff-bigmips elf32-ntradlittlemips elf64-tradlittlemips elf32-ntradbigmips
elf64-tradbigmips elf64-little elf64-big elf32-little elf32-big srec symbolsrec tekhex binary ihex
Report bugs to URL:http://www.sourceware.org/bugzilla/

If the file name or function name can not be determined, addr2line will print two question marks in
their place. If the line number can not be determined, addr2line will print 0


={============================================================================
*kt_linux_tool_205* binutil: ld

https://sourceware.org/binutils/docs/ld/index.html#Top
This file documents the gnu linker ld (GNU Binutils) version 2.25. 

<debian>
keitee@debian-keitee:~/work/xxx$ which ld
/usr/bin/ld
keitee@debian-keitee:~/work/xxx$ ld --version
GNU ld (GNU Binutils for Debian) 2.22
Copyright 2011 Free Software Foundation, Inc.


={============================================================================
*kt_linux_tool_250* bin-busybox

http://www.busybox.net/source.html
http://www.busybox.net/downloads/BusyBox.html


# ============================================================================
#{ gdb
={============================================================================
*kt_linux_tool_300* gdb: term and links

{inferior}
GDB represents the state of each program execution with an object called an 'inferior'. 


{target}
info files shows your active targets.

gdb program

Same as using "-se file" 
Read symbol table from file 'file' and use it as the executable file.


{reference}
http://www.gnu.org/software/gdb/
https://sourceware.org/gdb/download/onlinedocs/

http://visualgdb.com/gdbreference/commands/sharedlibrary 


={============================================================================
*kt_linux_tool_300* gdb: sample session

1 A Sample gdb Session

You can use this manual at your leisure to read all about gdb. However, a
handful of commands are enough to get started using the debugger. This chapter
illustrates those commands.

One of the preliminary versions of gnu m4 (a generic macro processor) exhibits
the following bug: sometimes, when we change its quote strings from the default,
the commands used to capture one macro definition within another stop working.
  In the following short m4 session, we define a macro foo which expands to
  0000; we then use the m4 built-in defn to define bar as the same thing.
  However, when we change the open quote string to <QUOTE> and the close quote
  string to <UNQUOTE>, the same procedure fails to define a new synonym baz:

     $ cd gnu/m4
     $ ./m4
     define(foo,0000)
     
     foo
     0000
     define(bar,defn(`foo'))
     
     bar
     0000
     changequote(<QUOTE>,<UNQUOTE>)
     
     define(baz,defn(<QUOTE>foo<UNQUOTE>))
     baz
     Ctrl-d
     m4: End of input: 0: fatal error: EOF in string

Let us use gdb to try to see what is going on.

     $ gdb m4
     
     
     (gdb)

gdb reads only enough symbol data to know where to find the rest when needed; as
a result, the first prompt comes up very quickly. We now tell gdb to use a
narrower display width than usual, so that examples fit in this manual.

     (gdb) set width 70

We need to see how the m4 built-in changequote works. Having looked at the
source, we know the relevant subroutine is m4_changequote, so we set a
breakpoint there with the gdb break command.

     (gdb) break m4_changequote
     Breakpoint 1 at 0x62f4: file builtin.c, line 879.

Using the run command, we start m4 running under gdb control; as long as control
does not reach the m4_changequote subroutine, the program runs as usual:

     (gdb) run
     Starting program: /work/Editorial/gdb/gnu/m4/m4
     define(foo,0000)
     
     foo
     0000


<gdb-call-function>

To trigger the breakpoint, we 'call' changequote. gdb suspends execution of m4,
displaying information about the context where it stops.

     changequote(<QUOTE>,<UNQUOTE>)
     
     Breakpoint 1, m4_changequote (argc=3, argv=0x33c70)
         at builtin.c:879
     879         if (bad_argc(TOKEN_DATA_TEXT(argv[0]),argc,1,3))

Now we use the command n (next) to advance execution to the next line of the
current function.

     (gdb) n
     882         set_quotes((argc >= 2) ? TOKEN_DATA_TEXT(argv[1])\
      : nil,

set_quotes looks like a promising subroutine. We can go into it by using the
command s (step) instead of next. step goes to the next line to be executed in
any subroutine, so it steps into set_quotes.

     (gdb) s
     set_quotes (lq=0x34c78 "<QUOTE>", rq=0x34c88 "<UNQUOTE>")
         at input.c:530
     530         if (lquote != def_lquote)

The display that shows the subroutine where m4 is now suspended (and its
  arguments) is called a stack frame display. It shows a summary of the stack.
We can use the backtrace command (which can also be spelled bt), to see where we
are in the stack as a whole: the backtrace command displays a stack frame for
each active subroutine.

     (gdb) bt
     #0  set_quotes (lq=0x34c78 "<QUOTE>", rq=0x34c88 "<UNQUOTE>")
         at input.c:530
     #1  0x6344 in m4_changequote (argc=3, argv=0x33c70)
         at builtin.c:882
     #2  0x8174 in expand_macro (sym=0x33320) at macro.c:242
     #3  0x7a88 in expand_token (obs=0x0, t=209696, td=0xf7fffa30)
         at macro.c:71
     #4  0x79dc in expand_input () at macro.c:40
     #5  0x2930 in main (argc=0, argv=0xf7fffb20) at m4.c:195

We step through a few more lines to see what happens. The first two times, we
can use ‘s’; the next two times we use n to avoid falling into the xstrdup
subroutine.

     (gdb) s
     0x3b5c  532         if (rquote != def_rquote)
     (gdb) s
     0x3b80  535         lquote = (lq == nil || *lq == '\0') ?  \
     def_lquote : xstrdup(lq);
     (gdb) n
     536         rquote = (rq == nil || *rq == '\0') ? def_rquote\
      : xstrdup(rq);
     (gdb) n
     538         len_lquote = strlen(rquote);

note: looks odd since "len_lquote (left) = strlen(rquote) (right)

The last line displayed looks a little odd; we can examine the variables lquote
  and rquote to see if they are in fact the new left and right quotes we
  specified. We use the command p (print) to see their values.

     (gdb) p lquote
     $1 = 0x35d40 "<QUOTE>"
     (gdb) p rquote
     $2 = 0x35d50 "<UNQUOTE>"

lquote and rquote are indeed the new left and right quotes. To look at some
context, we can display ten lines of source surrounding the current line with
the l (list) command.

     (gdb) l
     533             xfree(rquote);
     534
     535         lquote = (lq == nil || *lq == '\0') ? def_lquote\
      : xstrdup (lq);
     536         rquote = (rq == nil || *rq == '\0') ? def_rquote\
      : xstrdup (rq);
     537
     538         len_lquote = strlen(rquote);
     539         len_rquote = strlen(lquote);
     540     }
     541
     542     void

Let us step past the two lines that set len_lquote and len_rquote, and then
examine the values of those variables.

     (gdb) n
     539         len_rquote = strlen(lquote);
     (gdb) n
     540     }
     (gdb) p len_lquote
     $3 = 9
     (gdb) p len_rquote
     $4 = 7

That certainly looks 'wrong', assuming len_lquote and len_rquote are meant to be
the lengths of lquote and rquote respectively. We can 'set' them to better
values using the p command, since it can print the value of any expression—and
that expression can include subroutine calls and assignments.

<gdb-set-var>

     (gdb) p len_lquote=strlen(lquote)
     $5 = 7
     (gdb) p len_rquote=strlen(rquote)
     $6 = 9

Is that enough to fix the problem of using the new quotes with the m4 built-in
defn? We can allow m4 to continue executing with the c (continue) command, and
then try the example that caused trouble initially:

     (gdb) c
     Continuing.
     
     define(baz,defn(<QUOTE>foo<UNQUOTE>))
     
     baz
     0000

Success! The new quotes now work just as well as the default ones. The problem
seems to have been just the two typos defining the wrong lengths. We allow m4
exit by giving it an EOF as input:

     Ctrl-d
     Program exited normally.

The message ‘Program exited normally.’ is from gdb; it indicates m4 has finished
executing. We can end our gdb session with the gdb quit command.

     (gdb) quit


={============================================================================
*kt_linux_tool_301* gdb: start

2 Getting In and Out of gdb

2.1 Invoking gdb

The most usual way to start gdb is with one argument, specifying an executable
program:

     gdb program

// note
// Since it reads any arguments other than options as see the first argument as
// equivalent to the '-se' option and the second argument as equivalent to the
// '-c'/'-p' option followed by that argument:
//
// gdb program is the same as gdb -se program
//
// -se file
// Read symbol table from file 'file' and use it as the executable file.

You can also start with both an executable program and a core file specified:

     gdb program core

You can, instead, specify a process ID as a second argument, if you want to
debug a running process:

     gdb program 1234

would attach gdb to process 1234 (unless you also have a file named 1234; gdb
    does check for a core file first).


<gdb-args-options>

Taking advantage of the second command-line argument requires a fairly complete
operating system; when you use gdb as a remote debugger attached to a bare
board, there may not be any notion of “process”, and there is often no way to
get a core dump. gdb will warn you if it is unable to attach or to read core
dumps.

You can optionally have gdb pass any arguments after the executable file to the
inferior using --args. This option 'stops' option processing.

     gdb --args gcc -O2 -c foo.c

This will cause gdb to debug gcc, and to set gcc's command-line arguments (see
    Arguments) to '-O2 -c foo.c'.

// note
// gdb --args /opt/zinc-trunk/oss/bin/gst-launch-1.0 $@


You can run gdb without printing the front material, which describes gdb's
non-warranty, by specifying --silent (or -q/--quiet):

     gdb --silent

You can further control how gdb starts up by using command-line options. gdb
itself can remind you of the options available.

Type

     gdb -help

to display all available options and briefly describe their use (‘gdb -h’ is a
    shorter equivalent).

All options and command line arguments you give are processed in sequential
order. The order makes a difference when the ‘-x’ option is used.


={============================================================================
*kt_linux_tool_301* gdb: start: options

<gdb-eval-option>

-eval-command command
-ex command

Execute a 'single' gdb command. This option may be used multiple times to call
multiple commands. It may also be interleaved with '-command' as required.

gdb -eval-command=run --args /opt/zinc-trunk/oss/bin/gst-launch-1.0 $@


<gdb-command-file>

-command file
-x file
    Execute commands from file file. The contents of this file is evaluated
    exactly as the 'source' command would. See 23.1.3 Command Files

-init-command file
-ix file
    Execute commands from file file 'before' loading the inferior (but after
        loading gdbinit files). See Startup.

-init-eval-command command
-iex command
    Execute a single gdb command 'before' loading the inferior (but after
        loading gdbinit files). See Startup.


<gdb-set-search>

-directory directory
-d directory
    Add directory to the path to search for source and script files. 


={============================================================================
*kt_linux_tool_301* gdb: start: repeatedly run a test in GDB, until it fails

HOWTO repeatedly run a test in GDB, until it fails

Those of you grappling with intermittent failures may find this useful. I hit a
test that was intermittently aborting in x86 debug build, due to use of a
"singular iterator". It was particularly awkward to reproduce in gdb, so I
wanted to script it such that gdb would only prompt in the event of failure, and
otherwise quit and allow the next iteration.

Here's what I ended-up with:

while gdb --eval-command=start --eval-command="b abort" \
  --eval-command=continue --eval-command=quit \
    --args ./dbusoutputmanagertest; do echo "OK"; done


={============================================================================
*kt_linux_tool_301* gdb: start: what do during startup

2.1.3 What gdb Does During Startup

Here's the description of what gdb does during session startup:

    Sets up the command interpreter as specified by the command line (see
        interpreter).  
    
    Reads the system-wide init file (if --with-system-gdbinit was used when
        building gdb; see System-wide configuration and settings) and executes
    all the commands in that file.

    Reads the init file (if any) in your home directory and executes all the
    commands in that file.

    Executes commands and command files specified by the ‘-iex’ and ‘-ix’
    options in their specified order. note: Usually you should use the '-ex' and
    '-x' options instead, but this way you can apply settings before gdb init
    files get executed and before inferior gets loaded.  
    
    Processes command line options and operands.

    Reads and executes the commands from init file (if any) in the current
    working directory as long as ‘set auto-load local-gdbinit’ is set to ‘on’
    (see Init File in the Current Directory). 
    
    note: This is only done if the current directory is different from your home
    directory. Thus, you can have more than one init file, one generic in your
    home directory, and another, specific to the program you are debugging, in
    the directory where you invoke gdb.  
  
    If the command line specified a program to debug, or a process to attach to,
or a core file, gdb loads any auto-loaded scripts provided for the program or
  for its loaded shared libraries. See Auto-loading.

    Executes commands and command files specified by the ‘-ex’ and ‘-x’ options
      in their specified order.

    note: command-history
    Reads the command history recorded in the history file. See Command History,
          for more details about the command history and the files where gdb
            records it. 

Init files use the same syntax as command files and are processed by gdb in the
same way. The init file in your home directory can set options (such as 'set
    complaints') that affect subsequent processing of command line options and
operands. Init files are not executed if you use the ‘-nx’ option (see Choosing
    Modes).

<gdb-list-init>
To display the list of init files loaded by gdb at startup, you can use gdb
--help.

The gdb init files are normally called .gdbinit. 

Q: use HOME varaible?


={============================================================================
*kt_linux_tool_301* gdb: log

2.4 Logging Output

You may want to save the output of gdb commands to a file. There are several
commands to control gdb's logging.

set logging on
    Enable logging.

set logging off
    Disable logging.

set logging file file
    Change the name of the current logfile. The default logfile is gdb.txt.

set logging overwrite [on|off]
    By default, gdb will append to the logfile. Set overwrite if you want set
    logging on to 'overwrite' the logfile instead.

set logging redirect [on|off]
    By default, gdb output will go to both the terminal and the logfile. Set
    redirect if you want output to go 'only' to the log file.

show logging
    Show the current values of the logging settings. 


={============================================================================
*kt_linux_tool_301* gdb: shell command

2.3 Shell Commands

If you need to execute occasional shell commands during your debugging session,
there is no need to leave or suspend gdb; you can just use the shell command. 

shell command-string
!command-string

Invoke a standard shell to execute command-string. Note that no space is needed
between ! and command-string.


={============================================================================
*kt_linux_tool_301* gdb-cpp:

15.4.1.7 gdb Features for C++

Some gdb commands are particularly useful with C++, and some are designed
specifically for use with C++. Here is a summary:

breakpoint menus
    When you want a breakpoint in a function whose name is 'overloaded', gdb has
    the capability to display a menu of possible breakpoint locations to help
    you specify which function definition you want. See Ambiguous Expressions.


rbreak regex
    Setting breakpoints using regular expressions is helpful for setting
    breakpoints on overloaded functions that are not members of any special
    classes. See Setting Breakpoints.

catch throw
catch rethrow
catch catch
    Debug C++ exception handling using these commands. See Setting Catchpoints.


ptype typename
    Print inheritance relationships as well as other information for type
    typename. See Examining the Symbol Table.

info vtbl expression.
    The info vtbl command can be used to display the virtual method tables of
    the object computed by expression. This shows one entry per virtual table.
    there may be multiple virtual tables when multiple inheritance is in use.

demangle name
    Demangle name. See Symbols, for a more complete description of the demangle
    command.


set print demangle
show print demangle
set print asm-demangle
show print asm-demangle
    Control whether C++ symbols display in their source form, both when
    displaying code as C++ source and when displaying disassemblies. See Print
    Settings.

set print object
show print object
    Choose whether to print derived (actual) or declared types of objects. See
    Print Settings.

set print vtbl
show print vtbl
    Control the format for printing virtual function tables. See Print Settings.
    (The vtbl commands do not work on programs compiled with the HP ANSI C++
     compiler (aCC).)


set overload-resolution on
    Enable overload resolution for C++ expression evaluation. The default is on.
    For overloaded functions, gdb evaluates the arguments and searches for a
    function whose signature matches the argument types, using the standard C++
    conversion rules (see C++ Expressions, for details). If it cannot find a
    match, it emits a message.

set overload-resolution off
    Disable overload resolution for C++ expression evaluation. For overloaded
    functions that are not class member functions, gdb chooses the first
    function of the specified name that it finds in the symbol table, whether or
    not its arguments are of the correct type. For overloaded functions that are
    class member functions, gdb searches for a function whose signature exactly
    matches the argument types.


show overload-resolution
    Show the current setting of overload resolution.

Overloaded symbol names
    You can specify a particular definition of an overloaded symbol, using the
    same notation that is used to declare such symbols in C++: type
    symbol(types) rather than just symbol. You can also use the gdb command-line
    word completion facilities to list the available choices, or to finish the
    type list for you. See Command Completion, for details on how to do this. 


={============================================================================
*kt_linux_tool_301* gdb-cpp: completion

Press the TAB key whenever you want gdb to fill out the rest of a word; command,
function name or a field in a structure,

Sometimes the string you need, while logically a "word", may contain parentheses
  or other characters that gdb normally excludes from its notion of a word. To
  permit word completion to work in this situation, you may enclose words in ’
  (single quote marks) in gdb commands.

The most likely situation is C++ function overloading:

(gdb) b ’bubble( <tab>
bubble(double,double) bubble(int,int)
(gdb) b ’bubble(


={============================================================================
*kt_linux_tool_302* gdb: general and help

3 gdb Commands

{test-abbreviation}
You can test abbreviations by using them as arguments to the help command.

(gdb) help s
Step program until it reaches a different source line.
Argument N means do this N times (or till program stops for another reason).


{return}
A blank line as input to gdb (typing just RET) means to 'repeat' the previous
command. Certain commands (for example, run) will not repeat this way.

The list and x commands, when you repeat them with RET, construct new arguments
rather than repeating exactly as typed. This permits easy scanning of source or
memory.


{comment}
Any text from a # to the end of the line is a comment; it does nothing. This is
useful mainly in command files


{interrupt}
An interrupt (often Ctrl-c) does not exit from gdb, but rather terminates the
action of any gdb command that is in progress and returns to gdb command prompt.
It is safe to type the interrupt character at any time because gdb does not
allow it to take effect until a time when it is safe.

What if the program is running but you forgot to set breakpoints? You can hit
CTRL-C and that'll stop the program wherever it happens to be and return you to
a "(gdb)" prompt. At that point, you could set up a proper breakpoint somewhere
and continue to that breakpoint.


{help}
help class

Using one of the general help classes as an argument, you can get a list of the
individual commands in that class.

help command

With a command name as help argument, gdb displays a short paragraph on how to
use that command.

apropos args

The apropos command searches through all of the gdb commands, and their
documentation, for the regular expression specified in args. It prints out all
matches found.


={============================================================================
*kt_linux_tool_303* gdb: about status

To inquire about the state of your program, or the state of gdb itself.

{info}
This command (abbreviated i) is for describing the state of your program. For example, you can show
the arguments passed to a function with "info args", list the registers currently in use with "info
registers", or list the breakpoints you have set with "info breakpoints". You can get a complete list
of the info sub-commands with help info.

info args
info registers
info breakpoints


{show}
In contrast to info, show is for describing the state of gdb itself. You can change most of the
things you can show, by using the related command set; for example, you can control what number
system is used for displays with set radix, or simply inquire which is currently in use with show
radix. To display all the settable parameters and their current values, you can use show with no
arguments; you may also use info set. Both commands produce the same display.


={============================================================================
*kt_linux_tool_304* gdb: debugging info

4 Running Programs Under gdb

{debugging-information}
When you run a program under gdb, you must first generate debugging information when you compile it.

Programs that are to be shipped to your customers are compiled with optimizations, using the '-O'
compiler option. However, some compilers are unable to handle the '-g' and '-O' options together.
Using those compilers, you cannot generate optimized executables containing debugging information.


<gcc-case>
gcc, the gnu C/C++ compiler, supports '-g' with or without '-O', making it possible to debug
optimized code. We recommend that you always use '-g' whenever you compile a program. For more
information, see Chapter 11 [Optimized Code], page 149.


<macro-expansion>
gdb knows about preprocessor macros and can show you their expansion (see Chapter 12 Macros, page
        153). Most compilers do not include information about preprocessor macros in the debugging
information if you specify the '-g' alone. Version 3.1 and later of gcc provides macro information
if you are using the DWARF debugging format, and specify the option '-g3'. See Section "Options for
Debugging Your Program or GCC" in Using the gnu Compiler Collection (GCC), for more information on
gcc options affecting debug information.


={============================================================================
*kt_linux_tool_305* gdb: run

4.2 Starting your Program

{run}
run
r 

Use the run command to start your program under gdb.

If you are running your program in an execution environment that supports processes, run 'creates'
an 'inferior' process and makes that process run your program.


{information-to-inferior}
The execution of a program is affected by certain information it receives from its 'superior'. gdb
provides ways to specify this information, which you must do 'before' starting your program.

*. arguments.

Specify the arguments to give your program as the arguments of the run command. If a shell is
available on your target, the shell is used to pass the arguments, so that you may use normal
conventions (such as wildcard expansion or variable substitution) in describing the arguments. 

*. environment.

Your program normally inherits its environment from gdb, but you can use the gdb commands "set
environment" and "unset environment" to change parts of the environment that 'affect' your program. 

*. working directory.

Your program inherits its working directory from gdb. You can set the gdb working directory with the
cd command in gdb. 

*. standard input and output.

Your program normally uses the 'same' device for standard input and standard output as gdb is using.
You can redirect input and output in the run command line, or you can use the tty command to set a
different device for your program. See Section 4.6 [Your Program's Input and Output], page 32.

note: Warning: While input and output redirection work, you cannot use pipes to pass the output of
the program you are debugging to another program; if you attempt this, gdb is likely to wind up
debugging the wrong program.


<reload-symbol>
If the modification time of your symbol file has changed since the last time gdb read its symbols,
   gdb discards its symbol table, and reads it again. When it does this, gdb tries to retain your
   current breakpoints.


{start}

start 

The name of the main procedure can vary from language to language. With C or C++, the main procedure
name is always main, but other languages such as Ada do not require a specific name for their main
procedure. 

The debugger provides a convenient way to start the execution of the program and to 'stop' at the
beginning of the main procedure, depending on the language used. The start command does the
equivalent of setting a temporary breakpoint at the beginning of the main procedure and then
invoking the run command.

Specify the arguments to give to your program as arguments to the 'start' command.

note: Sams as with run, but stop as at the main.


{arguments}
The arguments to your program can be specified by the arguments of the run command. They are passed
to a 'shell', which expands wildcard characters and performs redirection of I/O, and thence to your
program. 

note: gdb runs your program via a shell

run with no arguments uses the same arguments used by the previous run, or those set by the set args
command.

set args 

Specify the arguments to be used the next time your program is run. If set args has no arguments,
        run executes your program with no arguments. Once you have run your program with arguments,
        using set args before the next run is the only way to run it again without arguments.

show args 

Show the arguments to give your program when it is started.


note: don't need to care about \"..." as below.

(gdb) set args -b Zinc.MediaProxy 
(gdb) show args
Argument list to give program being debugged when it is started is "-b Zinc.MediaProxy".
(gdb) 


{environment}
Usually you set up environment variables with the shell and they are inherited by all the other
programs you run. When debugging, it can be useful to try running your program with a modified
environment without having to start gdb over again.

show paths

Display the list of search paths for executables (the PATH environment variable).

show environment [varname]

Print the value of environment variable varname to be given to your program when it starts. If you
do not supply varname, print the names and values of 'all' environment variables to be given to your
program. You can abbreviate environment as env.

set environment varname [=value]

Set environment variable varname to value. The value changes for your program (and the shell gdb
        uses to launch it), not for gdb itself. The value parameter is optional; if it is
eliminated, the variable is set to a null value.

unset environment varname

Remove variable varname from the environment to be passed to your program.


{standard-input-and-output}
By default, the program you run under gdb does input and output to the same terminal that gdb uses.
gdb switches the terminal to its own terminal modes to interact with you, but it records the
terminal modes your program was using and switches back to them when you continue running your
program.

run > outfile

starts your program, diverting its output to the file 'outfile'.

Another way to specify where your program should do input and output is with the tty command.

An explicit redirection in run overrides the tty command’s effect on the input/output device, but
'not' its effect on the controlling terminal.

When you use the tty command or redirect input in the run command, only the input for your program
is affected. The input for gdb still comes from your terminal. tty is an alias for set inferior-tty.

set inferior-tty /dev/ttyb

Set the tty for the program being debugged to /dev/ttyb.

show inferior-tty

Show the current tty for the program being debugged.


={============================================================================
*kt_linux_tool_306* gdb-attach: debug already running process

4.7 Debugging an Already-running Process

attach process-id

This command attaches to a running process—one that was started outside gdb.

info files 

shows your active targets.

note: You must also have permission to send the process a signal.

The first thing gdb does after arranging to debug the specified process is to
'stop' it. You can examine and modify an attached process with all the gdb
commands that are ordinarily available when you start processes with run. You
can insert breakpoints; you can step and continue; you can modify storage. If
you would rather the process continue running, you may use the continue
'command' after attaching gdb to the process.

detach 

When you have finished debugging the attached process, you can use the detach
command to release it from gdb control. Detaching the process continues its
execution. After the detach command, that process and gdb become completely
independent once more, and you are ready to attach another process or start one
with run.

$ gdb gst-launch-1.0 11476


={============================================================================
*kt_linux_tool_307* gdb: debug with multiple threads

note: These facilities are not yet available on every gdb configuration where the operating system
supports threads. If your gdb does not support threads, these commands have no effect. For example,
a system without thread support shows no output from 'info threads', and always rejects the thread
    command, like this:

(gdb) info threads
(gdb) thread 1
Thread ID 1 not known. Use the "info threads" command to
see the IDs of currently known threads.


<current-thread>
One thread in particular is always the focus of debugging. This thread is called the current thread.
Debugging commands show program information from the perspective of the current thread.


For debugging purposes, gdb associates its own thread number

info threads [id...]

Display a summary of all threads currently in your program. Optional argument id... is one or more
thread ids separated by spaces, and means to print information only about the specified thread or
threads. gdb displays for each thread (in this order):

1. the thread number assigned by gdb
2. the target system's thread identifier (systag)
3. the thread's name, if one is known. A thread can either be named by the user (see thread name,
        below), or, in some cases, by the program itself.
4. the current stack frame summary for that thread

An asterisk ‘*’ to the left of the gdb thread number indicates the current thread.

For example,

(gdb) info threads
  Id Target Id        Frame
  3 process 35 thread 27 0x34e5 in sigpause ()
  2 process 35 thread 23 0x34e5 in sigpause ()
* 1 process 35 thread 13 main (argc=1, argv=0x7ffffff8)
at threadtest.c:68


thread threadno

Make thread number threadno the current thread.

thread apply [threadno | all] command

The thread apply command allows you to apply the named command to one or more threads. Specify the
numbers of the threads that you want affected with the command argument threadno. It can be a single
thread number, one of the numbers shown in the first field of the 'info threads' display; or it
could be a range of thread numbers, as in 2-4. To apply a command to all threads, type thread apply
all command.


={============================================================================
*kt_linux_tool_308* gdb: call trace

When see call traces using bt, it does not show:

1. function calls which is completed along the call tree.
2. function calls which do "sleep()" call in an atempt to see if call traces
up to this point when attaching gdb.

Q: how to put some code to provide a point to use gdb?


={============================================================================
*kt_linux_tool_309* gdb: tip: get a call trace promatically

In running gst-launch, see the call traces made automatically as shown.

Caught SIGSEGV
#0  0x7753cd1c in __lll_lock_wait () from /lib/libpthread.so.0
#1  0x775446fc in pthread_mutex_lock () from /lib/libpthread.so.0
#2  0x772c4fcc in g_mutex_lock () from /opt/zinc-trunk/oss/lib/libglib-2.0.so.0
#3  0x75b6f2ac in gst_nexus_mgr_set_rate (mgr=0x48e0b0, settings=0x47ff18, res=0x0, rate=1) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexusmgr.c:1097
#4  0x75b74aa0 in gst_nexus_sink_play_locked (sink=0x47fd18) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexussink.c:1531
#5  gst_nexus_sink_change_state_locked (transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING, sink=0x47fd18) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexussink.c:1354
#6  gst_nexus_sink_change_state (element=<optimized out>, transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexussink.c:1326
#7  0x77464ed8 in gst_element_change_state (element=0x47fd18, transition=<optimized out>) at gstelement.c:2602
#8  0x77465aac in gst_element_set_state_func (element=0x47fd18, state=<optimized out>) at gstelement.c:2558
#9  0x7743a954 in gst_bin_element_set_state (next=GST_STATE_PLAYING, current=GST_STATE_PAUSED, start_time=0, base_time=592515640814054, element=0x47fd18, bin=0x489078) at gstbin.c:2328
#10 gst_bin_change_state_func (element=0x489078, transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING) at gstbin.c:2665
#11 0x77490e24 in gst_pipeline_change_state (element=0x489078, transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING) at gstpipeline.c:474
#12 0x77464ed8 in gst_element_change_state (element=0x489078, transition=<optimized out>) at gstelement.c:2602
#13 0x77465aac in gst_element_set_state_func (element=0x489078, state=<optimized out>) at gstelement.c:2558
#14 0x00404c5c in main (argc=28, argv=0x7f9c2c94) at gst-launch.c:1095
Spinning.  Please run 'gdb gst-launch-1.0 15054' to continue debugging, Ctrl-C to quit, or Ctrl-\ to dump core.


gstreamer-1.5.0/tools/gst-launch.c

static void
fault_handler_sighandler (int signum)
{
  fault_restore ();

  /* printf is used instead of g_print(), since it's less likely to
   * deadlock */
  switch (signum) {
    case SIGSEGV:
      fprintf (stderr, "Caught SIGSEGV\n");
      break;
    case SIGQUIT:
      if (!quiet)
        printf ("Caught SIGQUIT\n");
      break;
    default:
      fprintf (stderr, "signo:  %d\n", signum);
      break;
  }

  fault_spin ();
}

static void
fault_spin (void)
{
  int spinning = TRUE;

  glib_on_error_halt = FALSE;
  g_on_error_stack_trace ("gst-launch-" GST_API_VERSION);

  wait (NULL);

  /* FIXME how do we know if we were run by libtool? */
  fprintf (stderr,
      "Spinning.  Please run 'gdb gst-launch-" GST_API_VERSION " %d' to "
      "continue debugging, Ctrl-C to quit, or Ctrl-\\ to dump core.\n",
      (gint) getpid ());
  while (spinning)
    g_usleep (1000000);
}

static void
fault_restore (void)
{
  struct sigaction action;

  memset (&action, 0, sizeof (action));
  action.sa_handler = SIG_DFL;

  sigaction (SIGSEGV, &action, NULL);
  sigaction (SIGQUIT, &action, NULL);
}

static void
fault_setup (void)
{
  struct sigaction action;

  memset (&action, 0, sizeof (action));
  action.sa_handler = fault_handler_sighandler;

  sigaction (SIGSEGV, &action, NULL);
  sigaction (SIGQUIT, &action, NULL);
}

int
main (int argc, char *argv[])
{
  ...

#ifdef G_OS_UNIX
    fault_setup ();
#endif

}


={============================================================================
*kt_linux_tool_320* gdb: examine sources and source path

9 Examining Source Files

gdb can print parts of your program's source, since the debugging information recorded in the
program tells gdb what source files were used to build it.

To print lines from a source file, use the list command (abbreviated l). By default, ten lines are
printed.

list linenum
Print lines centered around line number linenum in the current source file.

list function
Print lines centered around the beginning of function function.

list 
Print more lines. If the last lines printed were printed with a list command, this prints lines
following the last lines printed; however, if the last line printed was a solitary line printed as
part of displaying a stack frame (see Chapter 8 [Examining the Stack], page 91), this prints lines
centered around that line.


{specifying-location}

filename:linenum

Specifies the line linenum in the source file filename. If filename is a relative file name, then it
will match any source file name with the same 'trailing' components. For example, if filename is
'gcc/expr.c', then it will match source file name of '/build/trunk/gcc/expr.c', but not
/build/trunk/libcpp/expr.c or /build/trunk/gcc/x-expr.c.

filename:function

Specifies the line that begins the body of the function function in the file filename.  You only
need the file name with a function name to avoid ambiguity when there are identically named
functions in different source files.

*address 

Specifies the program address 'address'. For line-oriented commands, such as list and edit, this
specifies a source line that contains address. 

For break and other breakpoint oriented commands, this can be used to set breakpoints in parts of
your program which do not have debugging information or source files.

Here address may be any expression valid in the current working language (see Chapter 15
        [Languages], page 183) that specifies a code address. In addition, as a convenience, gdb
extends the semantics of expressions used in locations to cover the situations that frequently
happen during debugging. 

Here are the various forms of address:

expression
Any expression valid in the current working language.

funcaddr 
An address of a function or procedure derived from its name. In C, C++, Java, Objective-C, Fortran,
   minimal, and assembly, this is simply the function's name function (and actually a special case
           of a valid expression). This form specifies the address of the function's first
   instruction, 'before' the stack frame and arguments have been set up.

'filename'::funcaddr
Like funcaddr above, but also specifies the name of the source file explicitly. This is useful if
the name of the function does not specify the function unambiguously, e.g., overloads


{specifying-source}

9.5 Specifying Source Directories

Executable programs sometimes do not record the directories of the source files from which they were
compiled, just the names. Even when they do, the directories could be moved between the compilation
and your debugging session. gdb has a list of directories to search for source files; this is called
the source path.

Each time gdb wants a source file, it tries all the directories in the list, in the 'order' they are
present in the list, until it finds a file with the desired name.

For example, suppose an executable references the file /usr/src/foo-1.0/lib/foo.c, and our source
path is /mnt/cross. The file is first looked up literally; if this fails,
     /mnt/cross/usr/src/foo-1.0/lib/foo.c is tried; if this fails, /mnt/cross/foo.c is opened; if
     this fails, an error message is printed.

Plain file names, relative file names with leading directories, file names containing dots, etc. are
all treated as described above.

Note that the executable search path is not used to locate the source files.


{substitution}
A substitution rule specifies how to rewrite source directories stored in the program's debug
information in case the sources were moved to a different directory between compilation and
debugging.

gdb does a simple string replacement of from with to at the start of the directory part of the
source file name, and uses that result instead of the original file name to look up the sources.

a rule is applied only if the from part of the directory name 'ends' at a directory separator and
only at the 'beginning' of the directory name,

‘/mnt/cross’ will be applied to ‘/usr/source/foo-1.0’ but not to ‘/usr/sourceware/foo-2.0’ and not
be applied to ‘/root/usr/source/baz.c’ either.

In many cases, you can achieve the same result using the directory command. However, set
substitute-path can be more efficient in the case where the sources are organized in a complex tree
with multiple subdirectories.

set substitute-path from to

Define a source path substitution rule, and add it at the end of the current list of existing
substitution rules. If a rule with the same from was already defined, then the old rule is also
deleted.

For example, if the file ‘/foo/bar/baz.c’ was moved to ‘/mnt/cross/baz.c’, then the command

(gdb) set substitute-path /usr/src /mnt/cross

will tell gdb to replace ‘/usr/src’ with ‘/mnt/cross’, which will allow gdb to find the file ‘baz.c’
even though it was moved.

show substitute-path [path]

If a path is specified, then print the source path substitution rule which would rewrite that path,
   if any. If no path is specified, then print all existing source path substitution rules.


<to-reset>
If your source path is cluttered with directories that are no longer of interest, gdb may sometimes
cause confusion by finding the wrong versions of source. You can correct the situation as follows:

1. Use directory with no argument to reset the source path to its default value.
2. Use directory with suitable arguments to reinstall the directories you want in the source path.
You can add all the directories in one command.


<todo>
You can configure a default source path substitution rule by configuring gdb with the
‘--with-relocated-sources=dir’ option. The dir should be the name of a directory under
gdb’s configured prefix (set with ‘--prefix’ or ‘--exec-prefix’), and directory names in
debug information under dir will be adjusted automatically if the installed gdb is moved
to a new location. This is useful if gdb, libraries or executables with debug information
and corresponding source code are being moved together.


={============================================================================
*kt_linux_tool_321* gdb: sources and machine code

You can use the command info line to map source lines to program addresses (and vice versa), and the
command disassemble to display a range of addresses as machine instructions.

info line linespec

Print the starting and ending addresses of the compiled code for source line linespec. You can
specify source lines in any of the ways.

For example, we can use info line to discover the location of the object code for the first line of
function m4_changequote:

(gdb) info line m4_changequote
Line 895 of "builtin.c" starts at pc 0x634c and ends at 0x6350.

We can also inquire (using *addr as the form for linespec) what source line covers a particular
address:

(gdb) info line *0x63ff
Line 926 of "builtin.c" starts at pc 0x63e4 and ends at 0x6404.


note: changes pc?

After info line, the default address for the x command is 'changed' to the starting address of the
line, so that 'x/i' is sufficient to begin examining the machine code


disassemble
disassemble /m
disassemble /r

This specialized command dumps a range of memory as machine instructions. It can also print mixed
source+disassembly by specifying the /m modifier and print the raw instructions in hex as well as in
symbolic form by specifying the /r. 

The default memory range is the function surrounding the "program counter" of the selected frame.  A
single argument to this command is a program counter value; gdb dumps the function surrounding this
value. 
    
When two arguments are given, they should be separated by a comma, possibly surrounded by
whitespace. The arguments specify a range of addresses to dump, in one of two forms: 

start,end 

the addresses from start (inclusive) to end (exclusive)

The argument(s) can be any expression yielding a numeric value, such as 0x32c4, &main+10 or $pc-8.

<address>
Addresses 'cannot' be specified as a linespec (see Section 9.2 [Specify Location]). So, for example,
if you want to disassemble function bar in file ‘foo.c’, you must type ‘disassemble ’foo.c’::bar’
    and not ‘disassemble foo.c:bar’.


set disassemble-next-line
show disassemble-next-line

Control whether or not gdb will disassemble the next source line or instruction when execution
stops. If ON, gdb will display disassembly of the next source line when execution of the program
being debugged stops. This is in addition to displaying the source line itself, which gdb always
does if possible. 

If the next source line cannot be displayed for some reason (e.g., if gdb cannot find the source
        file, or there's no line info in the debug info), gdb will display disassembly of the next
instruction instead of showing the next source line. If AUTO, gdb will display disassembly of next
instruction only if the source line cannot be displayed. This setting causes gdb to display some
feedback when you step through a function with no line info or whose source file is unavailable.
The default is OFF, which means never display the disassembly of the next line or instruction.


<shared-libraries>
For programs that were dynamically linked and use shared libraries, instructions that call functions
or branch to locations in the shared libraries might show a seemingly bogus location; it's actually
a location of the relocation table. On some architectures, gdb might be able to resolve these to
actual function names.

note: might? how about linux?


={============================================================================
*kt_linux_tool_322* gdb: examine symbols

16 Examining the Symbol Table

To allow gdb to recognize ‘foo.c’ as a single symbol, enclose it in single quotes; for example,

p ’foo.c’::x


{symbol-and-address}

info address <symbol>

Print the address of a symbol. Describe where the data for symbol is stored. For a register
variable, this says which register it is kept in. For a non-register local variable, this prints the
stack-frame offset at which the variable is always stored.

info symbol <addr>

Print the name of a symbol which is stored at the address addr. If no symbol is stored exactly at
addr, gdb prints the nearest symbol and an offset from it:

(gdb) info symbol 0x54320
_initialize_vx + 396 in section .text

This is the 'opposite' of the info address command. You can use it to find out the name of a
variable or a function given its address.

<shared-library>
For dynamically linked executables, the name of executable or shared library containing the symbol
is also printed:

(gdb) info symbol 0x400225
_start + 5 in section .text of /tmp/a.out
(gdb) info symbol 0x2aaaac2811cf
__read_nocancel + 6 in section .text of /usr/lib64/libc.so.6


{demangle}
demangle [-l language] [--] name
Demangle name. If language is provided it is the name of the language to
demangle name in. Otherwise name is demangled in the current language.
The ‘--’ option specifies the end of options, and is useful when name begins
with a dash.
The parameter demangle-style specifies how to interpret the kind of mangling
used. See Section 10.8 [Print Settings], page 121.


{whatis}

whatis[/flags] [arg]

Print the data 'type' of arg, which can be either an expression or a name of a data type. With no
argument, print the data type of $, the last value in the value history. If arg is an expression
(see Section 10.1 [Expressions], page 109), it is not actually evaluated, and any side-effecting
operations (such as assignments or function calls) inside it do not take place. 

If arg is a variable or an expression, whatis prints its literal type as it is used in the source
code. If the type was defined using a typedef, whatis will not print the data type underlying the
typedef. If the type of the variable or the expression is a compound data type, such as struct or
class, whatis never prints their fields or methods. It just prints the struct/class name a.k.a.  its
tag. 

If you want to see the members of such a compound data type, use ptype. If arg is a type name that
was defined using typedef, whatis unrolls only one level of that typedef. Unrolling means that
whatis will show the underlying type used in the typedef declaration of arg. However, if that
underlying type is also a typedef, whatis will not unroll it.

(gdb) whatis ppkSectionData
type = const uint8_t **

flags can be used to modify how the type is displayed. Available flags are:

r 
Display in "raw" form. Normally, gdb substitutes template parameters and typedefs defined in a class
when printing the class’ members. The /r flag disables this.

m 
Do not print methods defined in the class. M Print methods defined in the class. This is the
'default', but the flag exists in case you change the default with set print type methods.

t 
Do not print typedefs defined in the class. Note that this controls whether the typedef definition
itself is printed, not whether typedef names are substituted when printing other types.

T 
Print typedefs defined in the class. This is the default, but the flag exists in case you change the
default with set print type typedefs.


{ptype}
ptype[/flags] [arg]

ptype accepts the same arguments as whatis, but prints a detailed description of the type, instead
of just the name of the type.

Contrary to whatis, ptype 'always' unrolls any typedefs in its argument declaration, whether the
argument is a variable, expression, or a data type.


typedef double real_t;
struct complex { real_t real; double imag; };

typedef struct complex complex_t;

complex_t var;
real_t *real_pointer_var;

(gdb) whatis var
type = complex_t

(gdb) ptype var
type = struct complex {
    real_t real;
    double imag;
}

(gdb) whatis complex_t
type = struct complex

(gdb) whatis struct complex
type = struct complex

(gdb) ptype struct complex
type = struct complex {
    real_t real;
    double imag;
}

(gdb) whatis real_pointer_var
type = real_t *

(gdb) ptype real_pointer_var
type = double *


{info-scope}
info scope location

List all the variables local to a particular scope. This command accepts a location argument; a
function name, a source line, or an address preceded by a ‘*’, and prints all the variables local to
the scope defined by that location.

For example:

(gdb) info scope command line handler
Scope for command_line_handler:
Symbol rl is an argument at stack/frame offset 8, length 4.
Symbol linebuffer is in static storage at address 0x150a18, length 4.
Symbol linelength is in static storage at address 0x150a1c, length 4.
Symbol p is a local variable in register $esi, length 4.
Symbol p1 is a local variable in register $ebx, length 4.
Symbol nline is a local variable in register $edx, length 4.
Symbol repeat is a local variable at frame offset -8, length 4.

This command is especially useful for determining what data to collect during a trace experiment


{info-sources}
info source

Show information about the current source file

info sources

Print the names of all source files in your program for which organized into two lists: files whose
symbols have already been read, and files whose symbols 'will' be read when needed.


{info-variables}
info variables

Print the names and data types of all variables that are defined outside of functions i.e. excluding
local variables.


={============================================================================
*kt_linux_tool_323* gdb: examine symbols loading

note:
not supported in 7.2

set print symbol-loading
set print symbol-loading full
set print symbol-loading brief
set print symbol-loading off
show print symbol-loading

The set print symbol-loading command allows you to control the printing of messages when gdb loads
symbol information. By default a message is printed for the executable and one for each shared
library, and normally this is what you want. 
    
However, when debugging apps with large numbers of shared libraries these messages can be annoying.
When set to 'brief' a message is printed for each executable, and when gdb loads a collection of
shared libraries at once it will only print one message regardless of the number of shared
libraries.


={============================================================================
*kt_linux_tool_324* gdb: specify files

{file-command}
You may want to specify executable and core dump file names. 

<why-needed>
Occasionally it is necessary to change to a different file during a gdb session. Or you may run gdb
and forget to specify a file you want to use. 

Or you are debugging a remote target via gdbserver. In these situations the gdb commands to specify
new files are useful.

file filename

Use filename as the program to be debugged. Use the file command to get 'both' symbol table and
program to run from the same file. 

file 

file with no argument makes gdb discard any information it has on both executable file and the
symbol table.

note: any real case when useful to load seperately symbol file or executable?

core-file [filename]
core 

Specify the whereabouts of a core dump file to be used as the "contents of memory". core files
contain only some parts of the address space of the process that generated them; gdb can access the
executable file itself for other parts.  

core-file 

with no argument specifies that no core file is to be used.


{info-files}
info files
info target

info files and info target are synonymous; both print the current target, including the names of the
executable and core dump files currently in use by gdb, and the files from which symbols were
loaded. The command "help target" lists all possible targets rather than current ones.


={============================================================================
*kt_linux_tool_325* gdb-slib: difference between with -g and without -g

note no -g on shared

  $ gcc -fpic -shared -o foo.so foo.c
  $ gcc -o main main.c ./foo.so -g
  $ gdb main

note: pending

  (gdb) b foo
  Function "foo" not defined.
  Make breakpoint pending on future shared library load? (y or [n]) y
  Breakpoint 1 (foo) pending.

  (gdb) r
  Starting program: /home/bhushan/RD/Shared_Lib_Debug/main
  Reading symbols from shared object read from target memory...done.
  Loaded system supplied DSO at 0x470000
  Breakpoint 2 at 0xdb7493

note: pending resolved

  Pending breakpoint "foo" resolved
  Breakpoint 2, 0x00db7493 in foo () from ./foo.so

  (gdb) s
  Single stepping until exit from function foo,
  which has no line number information.
  main () at main.c:7
  7 printf("inside main i = %d\n", i);

  (gdb) s
  inside main i = 4
  8 return 0;


NOW if you build the shared libarary using -g option

  $ gcc -fpic -shared -o foo.so foo.c -g
  $ gcc -o main main.c ./foo.so -g
  $ gdb main

  (gdb) b foo

  Function "foo" not defined.
  Make breakpoint pending on future shared library load? (y or [n]) y
  Breakpoint 1 (foo) pending.

  (gdb) r
  Starting program: /home/bhushan/RD/Shared_Lib_Debug/main
  Reading symbols from shared object read from target memory...done.
  Loaded system supplied DSO at 0x470000
  Breakpoint 2 at 0x1c5493: file foo.c, line 5.
  Pending breakpoint "foo" resolved

  Breakpoint 2, foo () at foo.c:5
  5 return 2*2;

  (gdb) s
  7 }

  (gdb) s
  main () at main.c:7
  7 printf("inside main i = %d\n", i);

  (gdb) s
  inside main i = 4
  8 return 0;

  (gdb)
  U can see the differences in bold lines.


={============================================================================
*kt_linux_tool_325* gdb-slib: load, search

18 gdb Files

Normally, GDB will load the shared library symbols automatically. You can
control this behavior using set auto-solib-add command.

set auto-solib-add mode

If mode is on, symbols from all shared object libraries will be loaded
'automatically' when the inferior begins execution, you attach to an
independently started inferior, or when the dynamic linker informs gdb that a
new library has been loaded. If mode is off, symbols must be loaded manually,
using the sharedlibrary command. The 'default' value is on.

<selective-load>
If your program uses lots of shared libraries with debug info that takes large
amounts of memory, you can 'decrease' the gdb memory footprint by preventing it
from automatically loading the symbols from shared libraries. To that end, type
set auto-solib-add off before running the inferior, then load each library whose
debug symbols you do need with sharedlibrary regexp, where regexp is a regular
expression that matches the libraries whose symbols you want to be loaded.

show auto-solib-add

Display the current autoloading mode.


{info-share-and-share}
info share regex
info sharedlibrary regex

'print' the names of the shared libraries which are currently loaded that match
regex. If regex is omitted then print 'all' shared libraries that are loaded.

sharedlibrary regex
share regex

'load' shared object library symbols for files matching a Unix regular
expression.  As with files loaded automatically, it only loads shared libraries
required by your program for a core file or 'after' typing run. If regex is
omitted 'all' shared libraries required by your program are loaded.

note: 
Can specify library name which are required by a program but 'not' a filename
with a path to force a loading of it. That means cannot load a libaray until a
program needs it.

nosharedlibrary

Unload all shared object library symbols. This discards all symbols that have
been loaded from all shared libraries. Symbols from shared libraries that were
loaded by explicit user requests are 'not' discarded.


{support-shared-library-remote} from 18 gdb Files
Shared libraries are also supported in many cross or remote debugging
configurations. gdb needs to have access to the target's libraries; this can be
accomplished either by providing copies of the libraries "on the host system",
             or by asking gdb to automatically retrieve the libraries from the
             target. 

If copies of the target libraries are provided, they need to be the same as the
target libraries, although the copies on the 'target' 'can' be 'stripped' as
long as the copies on the host are not.

For remote debugging, you need to tell gdb where the target libraries are, so
that it can load the correct copies. otherwise, it may try to load the host's
libraries. gdb has two variables to specify the search directories for target
libraries.


<sysroot>
set sysroot path

Use path as the system root 'for' the program being debugged. Any absolute
shared library paths will be 'prefixed' with path; many runtime loaders store
the absolute paths to the shared library in the target program's memory. 

If you use set sysroot to find shared libraries, they need to be laid out in the
'same' way that they are on the target, with e.g. a ‘/lib’ and ‘/usr/lib’
hierarchy under path.

The set solib-absolute-prefix command is an 'alias' for set sysroot.

show sysroot

Display the current shared library prefix.

note: said that this command forces to reload shared libraries.


<solib-search-path>
set solib-search-path path

If this variable is set, path is a colon-separated list of directories to search
for shared libraries. solib-search-path is used after sysroot fails to locate
the library, or if the path to the library is relative instead of absolute. 

note:
If you want to use solib-search-path instead of 'sysroot', be sure to set
sysroot to a nonexistent directory to prevent gdb from finding your host's
libraries.

sysroot is preferred; setting it to a nonexistent directory may interfere with
automatic loading of shared library symbols.

show solib-search-path

Display the current shared library search path.


{when-need-manual}
However, in some cases (e.g. when debugging with gdbserver and having
    incompatible symbols or using old Android toolchains) GDB will not load the
symbols automatically. In this case you can use the info sharedlibrary command
to list the loaded shared libraries and the sharedlibrary command to force the
symbols to be loaded. 

<check-solib-search-path>
If GDB does not automatically load debugging symbols for your library when
debugging with gdbserver, please check the search path using the set
solib-search-path command.

={============================================================================
*kt_linux_tool_325* gdb-slib: ex

In this example we will disable shared library loading using the set
auto-solib-add command, then run the application, list the source files and load
the symbols manually:

  (gdb) set auto-solib-add off
  (gdb) break main
  Breakpoint 1 at 0x80484ed: file main.cpp, line 7.
  (gdb) run
  Starting program: /home/testuser/libtest/testApp

  Breakpoint 1, main () at main.cpp:7
  7 printf("In main()\n");

  (gdb) info sources
  Source files for which symbols have been read in:

  /home/testuser/libtest/main.cpp

  Source files for which symbols will be read in on demand:

  (gdb) info sharedlibrary
  From To Syms Read Shared Object Library
  0xb7fde820 0xb7ff6b9f No /lib/ld-linux.so.2
  0xb7fd83a0 0xb7fd84c8 No /home/testuser/libtest/libTest.so
  0xb7e30f10 0xb7f655cc No /lib/i386-linux-gnu/libc.so.6

  (gdb) sharedlibrary libTest
  Reading symbols from /home/testuser/libtest/libTest.so...done.
  Loaded symbols for /home/testuser/libtest/libTest.so

  (gdb) info sources
  Source files for which symbols have been read in:

  /home/testuser/libtest/main.cpp

  Source files for which symbols will be read in on demand:

  /home/testuser/libtest/lib.cpp    // note. see added source file

  (gdb) break lib.cpp:5
  Breakpoint 2 at 0xb7fd846e: file lib.cpp, line 5.

  (gdb) continue
  Continuing.
  In main()

  Breakpoint 2, func () at lib.cpp:5
  5 printf("In func()\n");


={============================================================================
*kt_linux_tool_326* gdb: debugging with strace and gdb

Getting a core dump from the sandboxed application

First of all, bad things happen and sometimes we need to find out what went wrong. Because the
sandbox is intended to be robust, we do not want to leave any back-door to get into with the
debugger or tracing tools. Obviously back-door hooks are allowed for debug builds but sometimes
getting a debug build is inconvenient.

In this short article I want to focus on a rapid approach to get to the point as quickly as
possible. Please bear in mind that this is more a bunch of advise rather than a copy-and-paste
approach. A bit of warning: if you get frustrated very early with questions like "where do I get
this" or "where do I get that", then have a break with a cup of tea, ask people around etc. If
you're still frustrated after that then probably this article is not for you.

note:
The first thing to try is to get the core dump and then possibly use strace and/or GDB. By GDB I
mean here a native build of GDB rather than gdbserver as debugging with the latter one gets really
awkward with a "heavy" Stagecraft process.

Note that these tools take control over the application process and affect process relations, i.e.
they get in the middle of the application process and its parent. This has implications wherever
process relations are important, e.g. no remote control input will be directed to the application
process and the application process will not be allowed to write into the screen buffer.

Getting the core dump

This is the easiest thing to do and requires the least effort. Obviously you need a seg-fault. So if
you don't have a seg-fault handy, then this is not for you. You need to get a grip and carry on.

You need to apply all below before starting the babysitterd (C13) (uimanagerd before C13).

The instructions below must be applied in the same console which the babysitterd is started from.
Note that it won't work when starting the babysitterd over the D-BUS as the D-BUS daemon will not be
aware of the new settings until restarted. In this situation start the babysitterd from the command
line.

# tell the kernel where to store the core file (it's going to be from within the sandbox, so bear
# with me)
echo /opt/adobe/stagecraft/data/core > /proc/sys/kernel/core_pattern

# tell the kernel we don't mind ending with a large core file (use your imagination to get the size
# you like)
ulimit -c 10000000000

# in case of a crash in any *setuid* executable, additionally you need to do the following:
echo 1 > /proc/sys/fs/suid_dumpable
chmod +r <setuid executable>

# start the babysitterd, launch the UIME and app and make it crash

Note that currently the only location where the sandboxed application can write to, is its private
/tmp which is not persistent. The other one is its private "copy" of /opt/adobe/stagecraft/data
tree. Obviously the choice it to create the core file in /opt/adobe/stagecraft/data.

Where to look for:

private writeable area mapped as /opt/adobe/stagecraft/data:
$PREFIX/var/applications/data/air/<app CRID sha256>/stagecraft-data/

private log files:
$PREFIX/var/applications/data/air/<app CRID sha256>/log/


* Attaching to the application process

Unless the investigation is related to the application startup, it is better and easier to attach to
the process. 

Strace

# get the stagecraft process(es) PID(s)
/opt/zinc/oss/bin/busybox pgrep stagecraft

# pick the interesting process PID and strace it
strace -ff -o /tmp/stagecraft.log -s 128 -p <PID>

Please note that some of the Stagecraft threads are quite "intensive" loops producing a lot of
useless strace output. This is important to get every thread output into a separate file (-ff
option).  

GDB

As a prerequisite, you need to get MIPSEL build of GDB and a corresponding version of thread
libraries.

# override the default thread libraries
mount --bind <MY_PATH_TO_GDB_PTHREAD>/usr/lib /usr/lib
mount --bind <MY_PATH_TO_GDB_PTHREAD>/lib/libthread_db-0.9.29.so /lib/libthread_db-0.9.29.so
mount --bind <MY_PATH_TO_GDB_PTHREAD>/lib/libpthread-0.9.29.so /lib/libpthread.so.0

# add <MY_PATH_TO_GDB_PTHREAD> to $PREFIX/lib/sandbox/application.conf somewhere under [directories]

# launch the app

# get the stagecraft process(es) PID(s)
/opt/zinc/oss/bin/busybox pgrep stagecraft

# pick the interesting process PID and attach the debugger to it
<MY_PATH_TO_GDB>/gdb -p <PID> /opt/stagecraft-2.0/bin/stagecraft


* Starting the application process with a diagnostic tool

Strace

The easiest way to use strace is to edit $PREFIX/bin/runStagecraft2.sh somewhere at the end:

# this is the original line
LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib:/opt/zinc-trunk/lib:/usr/local/lib" \
LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
exec "${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"

# this is the same line with strace
exec strace -ff -o /opt/adobe/stagecraft/data/trace.log -s 128 \
-E LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib:/opt/zinc-trunk/lib:/usr/local/lib" \
-E LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
"${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"

You will find strace log files here: $PREFIX/var/applications/data/air/<app CRID
sha256>/stagecraft-data/.  

GDB

The prerequisites are the same as in "Attaching to the application process". You can use
GDB_STAGECRAFT variable in $PREFIX/bin/runStagecraft2.sh. As setting it in the environment would
require re-starting UIME, I prefer to edit runStagecraft2.sh instead:

+GDB_STAGECRAFT=1
 if [ -z "${GDB_STAGECRAFT}" ]; then
 	LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib:/opt/zinc-trunk/lib:/usr/local/lib" \
 	LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
 	exec "${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"
 else
-	GDB="gdb"
+	GDB="<MY_PATH_TO_GDB>/gdb"
 	ORG_LD_LIBRARY_PATH="${LD_LIBRARY_PATH}"
 	LD_LIBRARY_PATH="/usr/lib:/usr/local/lib:/opt/zinc-trunk/oss/lib" exec ${GDB} \

The other important thing here is to start the babysitterd in foreground as at some point you will
want GDB interactive session. You will also need to add <MY_PATH_TO_GDB> to
$PREFIX/lib/sandbox/application.conf somewhere under [directories].

Remember about permissions


={============================================================================
*kt_linux_tool_327* gdb: debugging remote

20 Debugging Remote Programs

Some remote targets allow gdb to access program files over the same connection used to communicate
with gdb. With such a target, if the remote program is unstripped, the only command you need is
target remote. 
    
Otherwise, start up gdb using the name of the local unstripped copy of your program as the first
argument, or use the file command.


{host-side}
The target remote command establishes a connection to the target. Its arguments indicate which
medium to use:

target remote tcp:host:port

Debug using a TCP connection to port on host. The host may be either a host name or a numeric IP
address; port must be a decimal number. For example, to connect to port 2828 on a terminal server
named manyfarms:

target remote manyfarms:2828

detach

When you have finished debugging the remote program, you can use the detach command to release it
from gdb control. Detaching from the target normally resumes its execution, but the results will
depend on your particular remote stub. After the detach command, gdb is free to connect to another
target.

disconnect

The disconnect command behaves like detach, except that the target is gener- ally not resumed. It
will wait for gdb (this instance or another one) to connect and continue debugging. After the
disconnect command, gdb is again free to connect to another target.

note:
First make sure you have the necessary symbol files. Load symbols for your application using the
file command before you connect. Use set sysroot to locate target libraries unless your gdb was
compiled with the correct sysroot using --with-sysroot.

The symbol file and target libraries must exactly match the executable and libraries on the target,
    with one exception: the files on the host system should not be stripped, even if the files on
    the target system are. Mismatched or missing files will lead to confusing results during
    debugging. On gnu/Linux targets, mismatched or missing files may also prevent gdbserver from
    debugging multi-threaded programs.

note:
The remote targets are always running. If you get an error message like this one below then use
continue to run your program. You may need load first.

The "remote" target does not support "run".  Try "help target" or "continue".  

note:
"Cannot access memory at address 0x0" warning,
This happens when run gdb client in case use to debug a applicaiton using a shared library. Thought
that gdb is not working but gdb works as usual.


{once-connected}
Once the connection has been established, you can use all the usual commands to examine and change
data. The remote program is already running; you can use step and continue, and you do not need to
use run.


{target-side}
<why-gdbserver>
gdbserver is sometimes useful nevertheless, because it is a much 'smaller' program than gdb itself.
It is also easier to port than all of gdb, so you may be able to get started more quickly on a new
system by using gdbserver. Finally, if you develop code for real-time systems, you may find that the
tradeoffs involved in real-time operation make it more convenient to do as much development work as
possible on another system, for example by cross-compiling. You can use gdbserver to make a similar
choice for debugging.

<no-symbol>
Run gdbserver on the target system. You need a copy of the program you want to debug, including any
libraries it requires. gdbserver does 'not' need your program's symbol table, so you can strip the
program if necessary to save space. gdb on the host system does all the symbol handling.

<server-run>
target> gdbserver comm program [ args ... ]

target> gdbserver host:2345 emacs foo.txt

The host:2345 argument means that we are expecting to see a TCP connection from host to local TCP
port 2345. (note Currently, the host part is ignored.) You can choose any number you want for the
port number as long as it does not conflict with any existing TCP ports on the target system. This
same port number must be used in the host GDBs target remote command.

<attach>
target> gdbserver --attach comm pid

$ gdbserver --attach 172.20.33.215:12345 1368


={============================================================================
*kt_linux_tool_328* gdb: debugging remote real example

{scenario-one} 
Use remote when debug starting from main. OK. 

$ gdbserver 172.20.33.215:12345 /opt/zinc-trunk/bin/nickelmediad -b Zinc.MediaProxy

// console logs comes here

<host>
$ ./mips-linux-uclibc-gdb 

(gdb) file /home/kpark/builds/yv-media-next/humax.1000/zinc-install-root/debug/humax-dtr_t1000/opt/zinc-trunk/bin/nickelmediad

(gdb) target remote 172.20.32.34:12345

(gdb) set substitute-path /home/kpark/builds/_virtual_/humax.1000 /home/kpark/src-dev

(gdb) set sysroot /home/kpark/builds/yv-media-next/humax.1000/zinc-install-root/debug/humax-dtr_t1000/opt/zinc-trunk/lib

(gdb) b MediaDaemon.cpp:187
Breakpoint 1 at 0x41d238: file
/home/kpark/builds/_virtual_/humax.1000/DEVARCH/Nickel/Nickel.System.DBusServer/src/MediaDaemon.cpp,
    line 187.
(gdb) c
Continuing.
Error while mapping shared library sections:
/usr/local/lib/libdirectfb.so: No such file or directory.
...
/lib/ld-uClibc.so.0: No such file or directory.

Breakpoint 1, parseArgs (this=0x7f839f28, argc=3, argv=0x7f83a184) at /home/kpark/builds/_virtual_/humax.1000/DEVARCH/Nickel/Nickel.System.DBusServer/src/MediaDaemon.cpp:189
189		std::string defaultConfigFilePath = NS_ZINC::PackageDataFinder().find("media-daemon.plugin-config");
(gdb) list


{scenario-two} 
Use remote when debug stating from main. NOT OK. However, when code has a call to load shared
library using dl_open then gdb do not picks up the library and hence do not hit a break point in
that shared library. Tried sysroot and solib-search-path. 


{scenario-three} 
To solve the above, use local approach. Copy sources from host to target and set substitute. This
works since can trace routine calling code to load shared library.

$ gdb --args /opt/zinc-bin/bin/nickelmediad -b Zinc.MediaProxy 
(gdb) set substitute-path /home/kpark/builds/_virtual_/humax.1000/DEVARCH/Nickel /root
(gdb) b ProxySystemFactory.cpp:82
(gdb) run


{scenario-four}
Still cannot use remote since still use dl_open depending on selection via dbus. So must use local
approach.

$ dbussenddaemon &
$ sleep 1
$ /opt/zinc-trunk/bin/nickelmediad -b Zinc.MediaProxy > /var/tmp/x.log 2>&1 &

// to check PID and if necessary, kill the previous one.
$ pgrep -l nickelmediad

$ tail /var/tmp/x.log 
$ gdb -p 1846 /opt/zinc-trunk/bin/nickelmediad 


// do not need to run file since it's attached

(gdb) set substitute-path /home/kpark/builds/_virtual_/humax.1000/DEVARCH/Nickel /root
(gdb) b ProxyMediaRouter.cpp:999

// From another ssh session, run dbus commands:
dbus-send --session --print-reply --type=method_call --dest='Zinc.MediaProxy' $MR \
Zinc.Media.MediaRouter.setSource string:http://dash.bidi.int.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd int32:0

// then gdb hits a break point.


{scenario-five}
This is a case which have no direct loading of shared library and hence remote approach is used.

<on-target>
$ gdbserver 172.20.33.215:12345 /opt/zinc-trunk/bin/nickelmediad -b Zinc.MediaProxy

<on-host>

// to see the default setting
(gdb)  show auto-solib-add             
Autoloading of shared library symbols is on.

// shall load file first before setting a breakpoint
(gdb) file /home/kit/tizen/tv-viewer/tv-viewer
Reading symbols from /home/kit/tizen/tv-viewer/tv-viewer...done.

(gdb) target remote 106.1.11.219:2345
Remote debugging using 106.1.11.219:2345
warning: Unable to find dynamic linker breakpoint function.
GDB will be unable to debug shared library initialisers
and track explicitly loaded dynamic code.
0xb63da7c0 in ?? ()

// to set a breakpoint in the gdbint; otherwise, gdb set no since no input from the user
(gdb) set breakpoint pending on

// this is a warning at this moment
(gdb) b main
Cannot access memory at address 0x0
Breakpoint 1 at 0x29842716: file /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1/src/core/AppMain.cpp, line 33.

// no shared library sicne the application do not started yet.
(gdb) i sharedlibrary
No shared libraries loaded at this time.

// see warnings on shared library now.
// set solib-search-path before and seems to have only one path as set solib-search-path /home/kit/mheg-port-ug
(gdb) c
Continuing.
warning: `/usr/lib/libicui18n.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicuuc.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicudata.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libfribidi.so.0': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicule.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: Could not load shared library symbols for 184 libraries, e.g. /usr/lib/libsys-assert.so.
Use the "info sharedlibrary" command to see the complete listing.
Do you need "set solib-search-path" or "set sysroot"?

Breakpoint 1, main (argc=-1237321556, argv=0xb63f4a30)
    at /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1/src/core/AppMain.cpp:33
33           _ERR("%s","Failed to create app!");

(gdb)
Continuing.

// see that even after running, not loaded full libraries. want to debug libug-mhegUG-efl.so
(gdb) i sharedlibrary
From        To          Syms Read   Shared Object Library
                        No          /usr/lib/libsys-assert.so
                        ...
                        No          /usr/lib/libsqlite3.so.0
0xb55efb24  0xb572307c  Yes (*)     /usr/lib/libicui18n.so.48
0xb55154e4  0xb55dfe7c  Yes (*)     /usr/lib/libicuuc.so.48
0xb43e4258  0xb43e4350  Yes (*)     /usr/lib/libicudata.so.48
                        No          /usr/lib/libavoc.so
                        ...
(*): Shared library is missing debugging information.

(gdb) c
Continuing.

(gdb) CTRL-C
Program received signal SIGINT, Interrupt.
0xb3db296c in ?? ()
warning: Could not load shared library symbols for 21 libraries, e.g. /usr/lib/ecore/immodules/libisf-imf-module.so.
Use the "info sharedlibrary" command to see the complete listing.
Do you need "set solib-search-path" or "set sysroot"?

// see that the wanted library is loaded
(gdb) i sharedlibrary
From        To          Syms Read   Shared Object Library
                        ...
0xab8d96c0  0xab9a2d78  No          /home/kit/mheg-port-ug/libug-mhegUG-efl.so
                        ..
(*): Shared library is missing debugging information.

// note:
// HOWEVER, the problem is that the library is loaded but 'not' the symblos and files. Can check to
// see if that is the case by running "i sources" to see files.

// note to fource to load again and check with i sources
(gdb) sharedlibrary mheg
Reading symbols from /home/kit/mheg-port-ug/libug-mhegUG-efl.so...done.
Loaded symbols for /home/kit/mheg-port-ug/libug-mhegUG-efl.so
Cannot access memory at address 0xb

// here can see the result of this substitude command:
// set substitute-path /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1 /home/kit/tizen/tv-viewer
(gdb) i sources
Source files for which symbols have been read in:

/home/kit/tizen/tv-viewer/src/core/AppMain.cpp, /usr/include/c++/4.5.3/new,
...

Source files for which symbols will be read in on demand:
...
/home/kit/mheg-port-ug/mh5eng/mh5e_token.c, /home/kit/mheg-port-ug/mh5eng/mh5b_program.c,

// now set a breakpoint in the loaded library
(gdb) b _key_pressed(char const*)
Cannot access memory at address 0xb
Breakpoint 2 at 0xab8d9afa: file /home/abuild/rpmbuild/BUILD/ug-mheg-0.2/main/Main.cpp, line 144.

(gdb) c

# hit breakpoint

(gdb) list

// note that if do not run this command fast enough, it seems that gdb fails to run a session. means
// not hitting a breakpoint.

(gdb) c
Continuing.


={============================================================================
*kt_linux_tool_300*  gdb: core dump setting

{setting}
# default
-sh-3.2# cat /proc/sys/kernel/core_pattern 
core

<core-pattern>
# set core dump location and format
echo '/tmp/%p.COR' >/proc/sys/kernel/core_pattern

the following pattern elements in the core_pattern file:

%p: pid
%: '%' is dropped
%%: output one '%'
%u: uid
%g: gid
%s: signal number
%t: UNIX time of dump
%h: hostname
%e: executable filename
%: both are dropped

# configure it forever

The changes done before are only applicable until the next reboot. In order to make the change in
all future reboots, you will need to add the following in /etc/sysctl.conf

# own core file pattern...
kernel.core_pattern=/tmp/cores/core.%e.%p.%h.%t

sysctl.conf is the file controlling every configuration under /proc/sys

Just wanted to say that there is no need to edit the file manually. simply run the sysctl command,
which does the stuff


<run-command-in-core-pattern>

[root@HUMAX sandbox]# cat /proc/sys/kernel/core_pattern 
|/bin/bash /usr/local/bin/core-dump /mnt/hd1/ %p

#!/bin/bash

##
# This is meant to be used to:
# - save the core dump piped to STDIN,
# - capture process memory map (/proc/pid/maps)
# - process the core dump (if gdb is installed)
#   to produce a text file with the backtrace
#   already demangled and info about threads.
# See core (5) man page for more information.
#
# This script is meant to be executed by the Kernel if
# /proc/sys/kernel/core_pattern references it.
#
# Usage:
#         ulimit -c unlimited
#         echo "|/bin/bash /path/to/core-dump /mnt/hd1/ %p" > /proc/sys/kernel/core_pattern
#
# Author: hubert.lacote@youview.com
#         kris@youview.com
##

gdb_path=/mnt/hd1/opt/zinc/oss/debugtools/bin/gdb

die() {
    echo "[Error]: $1" >&2
    exit 1
}

##
# Produce a text file from the core dump with the backtrace
# already demangled and info about threads... 
##
process_core_dump() {
    local core_dump="$1"
    local core_maps="$2"
    local output_file="$3"

    local first_maps_line="$(head -1 $core_maps)"
    # Assume the executable is under /opt
    local executable="/opt/${first_maps_line#*/opt/}"

    [[ -e "$executable" ]] || die "Failed to extract executable from '$first_maps_line'." 2> $output_file

    $gdb_path --batch \
              --eval-command="info threads" \
              --eval-command "bt" \
              --eval-command "thread apply all bt" \
              --eval-command 'info sharedlibrary' \
              --eval-command 'info registers' \
              --eval-command 'info locals' \
              --eval-command 'info args' \
              --eval-command 'x/100a $sp' \
              --eval-command 'x/50i $pc' \
              "$executable" "$core_dump" &> "$output_file"
}


[[ $# -eq 2 ]] || { echo "Error: script is not meant to be run manually." >&2; exit 1; }

core_output_path=$1
pid=$2

cat /proc/$pid/maps > "${core_output_path}/core-maps.$pid"
dd of="${core_output_path}/core.$pid" bs=1M

[[ -x "$gdb_path" ]] &&
    process_core_dump \
        "${core_output_path}/core.$pid" \
        "${core_output_path}/core-maps.$pid" \
        "${core_output_path}/core-text.$pid"


note:
This means that when use command in the pattern then dump core on stdin.


<conditions-to-check>
0. configured for cross platform

1. write permissions in the directory 

2. ulimit -c unlimited
this is shell command to set resource limit for a core.
-c     The maximum size of core files created 

3. compilation of the image: should be with debug symbols (not release version), and statically
compiled. In cygwin compilations, static link is enabled by default. In linux compilations, you
should modify platform.mk in your view.

If instead of the callstack you see only "??", it usually means the application wasn't
compiled with debug symbols (release version) or it was not compiled using static linkage

4. debug build
can use gdb without -g but need to map virtual address.

mkdir -p /tmp/cores
chmod a+rwx /tmp/cores
echo "/tmp/cores/core.%e.%p.%h.%t" > /proc/sys/kernel/core_pattern
 
<force-core>
ulimit -c unlimited

# SIGSEGV 	11 	Core 	Invalid memory reference 
# -s signal Specify the signal to send.  The signal may be given as a signal name or number.
 
kill -s SIGSEGV $$
kill -s 11 113

export P=`ps -a | grep APP_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;
export P=`ps -a | grep MW_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;

SIG_KERNEL_COREDUMP_MASK (.../kernel/signal.c) defines sianals to create a core.

SIGSEGV(segmentation fault)

#define SIG_KERNEL_COREDUMP_MASK (\
        M(SIGQUIT)   |  M(SIGILL)    |  M(SIGTRAP)   |  M(SIGABRT)   | \
        M(SIGFPE)    |  M(SIGSEGV)   |  M(SIGBUS)    |  M(SIGSYS)    | \
        M(SIGXCPU)   |  M(SIGXFSZ)   |  M_SIGEMT                     )

<ulimit>
Do not use "ulimit" bash command to check if ulimit is unlimited since "If no
option is given, then -f is assumed." from bash manual.

kpark@wll1p04345:~/work$ ulimit
unlimited
kpark@wll1p04345:~/work$ ulimit -a
core file size          (blocks, -c) 0             note:
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128235
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128235
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited


kpark@wll1p04345:~/work$ ulimit -c unlimited
kpark@wll1p04345:~/work$ ulimit
unlimited
kpark@wll1p04345:~/work$ ulimit -a 
core file size          (blocks, -c) unlimited     note:
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128235
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128235
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited



={============================================================================
*kt_linux_tool_301*  gdb: core dump analysis {frame-command}

export LD_LIBRARY_PATH=/home/NDS-UK/parkkt/bins
mips-gdb
set solib-absolute-prefix /junk
set solib-search-path /home/NDS-UK/parkkt/com.nds.darwin.debugsupport/debug_libs/uClibc-nptl-0.9.29-20070423
file APP_Process 
core 272.COR
thread apply all bt full
bt

(gdb) f n

<gdb-frame-command>
frame n 

Select frame number n. Recall that frame zero is the innermost (currently executing) frame, frame
one is the frame that called the innermost one, and so on. The highest-numbered frame is the one for
main.


# 01
#

(gdb) bt
#0 memset () at libc/string/mips/memset.S:132

(gdb) i r
	zero     at       v0         v1       a0         a1       a2       a3
R0	00000000 fffffffc [00000000] ffffffff [00000008] 00000000 00000004 00022000
	t0 t1 t2 t3 t4 t5 t6 t7
R8	00000004 00000000 ffffffff 000000c2 00000000 00000004 00a52630 00000000
	s0 s1 s2 s3 s4 s5 s6 s7
R16 00020000 00000000 00022004 2c57b174 00000004 2c70d4d0 2d9dcd30 00be6f70
	t8 t9 k0 k1 gp sp s8 ra
R24 00befcc8 2ab25160 00000000 00000000 00bf7e60 2d9dc858 2d9dcbd0 00993fdc
	sr       lo       hi       bad        cause    pc
	00008413 00000000 00000000 [00000000] 0080000c [2ab251b4]
	fsr fir
	00001004 00000000

# void *memset(void *s, int c, size_t n);

(gdb) disassemble $pc
Dump of assembler code for function memset:
0x2ab25160 <memset+0>: slti t1,a2,8
0x2ab25164 <memset+4>: bnez t1,0x2ab251d4 <memset+116>
-> 0x2ab25168 <memset+8>: move v0,a0
0x2ab2516c <memset+12>: beqz a1,0x2ab25184 <memset+36>
0x2ab25170 <memset+16>: andi a1,a1,0xff
0x2ab25174 <memset+20>: sll t0,a1,0x8
0x2ab25178 <memset+24>: or a1,a1,t0
0x2ab2517c <memset+28>: sll t0,a1,0x10
0x2ab25180 <memset+32>: or a1,a1,t0
0x2ab25184 <memset+36>: negu t0,a0 # negu d, s; d = -s;
0x2ab25188 <memset+40>: andi t0,t0,0x3
0x2ab2518c <memset+44>: beqz t0,0x2ab2519c <memset+60>
0x2ab25190 <memset+48>: subu a2,a2,t0
0x2ab25194 <memset+52>: swl a1,0(a0) # swl t, addr; Store word left/right
0x2ab25198 <memset+56>: addu a0,a0,t0
0x2ab2519c <memset+60>: andi t0,a2,0x7
0x2ab251a0 <memset+64>: beq t0,a2,0x2ab251c0 <memset+96>
0x2ab251a4 <memset+68>: subu a3,a2,t0
0x2ab251a8 <memset+72>: addu a3,a3,a0
0x2ab251ac <memset+76>: move a2,t0
-> 0x2ab251b0 <memset+80>: addiu a0,a0,8
-> 0x2ab251b4 <memset+84>: sw a1,-8(a0)
0x2ab251b8 <memset+88>: bne a0,a3,0x2ab251b0 <memset+80>

this shows that a0, first arg, was null. hence SEGFLT

# 02
#

(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc38, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x007f09c8 in MHWMemAllocStatic (pool=0x131bc38, size=64) at mem_static.c:63
#2  0x007e6854 in MEMMAN_API_AllocStaticP (pool=0x131bc38, size=41) at memman_st.c:350
#3  0x00419328 in DIAG_JAVA_GetJavaString (env=0x2d7056c0, l_java_string=0x2c4b3358, l_byte_array=0x2d705698, 
    l_len=0x2d70569c) at ../src/natdiag.c:63
#4  0x004197fc in DIAG_JAVA_GetJavaString (env=0x131bc38) at ../src/natdiag.c:341
#5  0x0041a0f8 in Java_com_nds_fusion_diagimpl_DiagImpl_natLogInfo (THIS=<value optimized out>, jClass=0x40, jInt=1, 
    jString=0x0) at sunnatdiag.c:177

[New process 124]
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
271     mem_blockpool.c: No such file or directory.
        in mem_blockpool.c

(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x72617469 in ?? ()
Backtrace stopped: previous frame inner to this frame (corrupt stack?)

(gdb) i r
          zero       at       v0       v1       a0       a1       a2       a3
 R0   00000000 00000005 00000004 00000000 0131bc40 00000040 00000001 00000000 
            t0       t1       t2       t3       t4       t5       t6       t7
 R8   0000000c 00000001 ffffffff ffffffff 00000000 00000000 00d7f220 00000200 
            s0       s1       s2       s3       s4       s5       s6       s7
 R16  00000000 0131bc40 0131bc40 00000040 00000001 00000000 00000001 2d705b00 
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00f0c360 00b62d98 00000000 00000000 00f14570 2d7055a0 2d7057d8 007f09c8 
            sr       lo       hi      bad    cause       pc
      00008413 000efdcb 00000005 00000018 00800008 007eb73c 
           fsr      fir
      00001004 00000000 

(gdb) disassemble $pc
# source
# uint32_t MHWMemCheckBank(MEMMAN_API_MemoryPoolHandle * mpool, uint32_t size, # uint32_t mem_nb_bank)
# {
#     MemWholeMemory *bank = mpool->s_WholeMem;
# }
#
Dump of assembler code for function MHWMemCheckBank:
0x007eb6b8 <MHWMemCheckBank+0>: addiu   sp,sp,-48
0x007eb6bc <MHWMemCheckBank+4>: sw      s4,40(sp)
0x007eb6c0 <MHWMemCheckBank+8>: sw      s3,36(sp)
0x007eb6c4 <MHWMemCheckBank+12>:        sw      s2,32(sp)
0x007eb6c8 <MHWMemCheckBank+16>:        sw      ra,44(sp)
0x007eb6cc <MHWMemCheckBank+20>:        sw      s1,28(sp)
0x007eb6d0 <MHWMemCheckBank+24>:        sw      s0,24(sp)
0x007eb6d4 <MHWMemCheckBank+28>:        lw      s0,40(a0)
0x007eb6d8 <MHWMemCheckBank+32>:        lw      v1,36(a0)
0x007eb6dc <MHWMemCheckBank+36>:        lui     v0,0x4
0x007eb6e0 <MHWMemCheckBank+40>:        lw      a3,108(a0)
0x007eb6e4 <MHWMemCheckBank+44>:        and     v1,v1,v0
0x007eb6e8 <MHWMemCheckBank+48>:        addiu   t0,s0,12
0x007eb6ec <MHWMemCheckBank+52>:        addiu   v0,s0,4
0x007eb6f0 <MHWMemCheckBank+56>:        move    s2,a0
0x007eb6f4 <MHWMemCheckBank+60>:        movn    t0,v0,v1
0x007eb6f8 <MHWMemCheckBank+64>:        move    s3,a1
0x007eb6fc <MHWMemCheckBank+68>:        beqz    a3,0x7eb73c <MHWMemCheckBank+132>
0x007eb700 <MHWMemCheckBank+72>:        move    s4,a2
0x007eb704 <MHWMemCheckBank+76>:        lw      a1,0(t0)
0x007eb708 <MHWMemCheckBank+80>:        lw      v1,24(s0)
0x007eb70c <MHWMemCheckBank+84>:        lw      v0,104(a0)
0x007eb710 <MHWMemCheckBank+88>:        li      a2,100
0x007eb714 <MHWMemCheckBank+92>:        mul     v1,v1,a2
0x007eb718 <MHWMemCheckBank+96>:        mul     v0,a1,v0
0x007eb71c <MHWMemCheckBank+100>:       sltu    v0,v1,v0
0x007eb720 <MHWMemCheckBank+104>:       beqz    v0,0x7eb73c <MHWMemCheckBank+132>
0x007eb724 <MHWMemCheckBank+108>:       nop
0x007eb728 <MHWMemCheckBank+112>:       divu    zero,v1,a1
0x007eb72c <MHWMemCheckBank+116>:       teq     a1,zero,0x7
0x007eb730 <MHWMemCheckBank+120>:       mflo    a1
0x007eb734 <MHWMemCheckBank+124>:       jal     0x7efa14 <MEMMAN_SHL_Notify>
0x007eb738 <MHWMemCheckBank+128>:       subu    a1,a2,a1
-> 0x007eb73c <MHWMemCheckBank+132>:       lw      v0,24(s0)
...
---Type <return> to continue, or q <return> to quit---

(gdb) info locals
ind_bank = <value optimized out>
bank = (MemWholeMemory *) 0x0		#
pool_max = (uint32_t *) 0xc

(gdb) x/16wx 0x0131bc40 	# value of a0
0x131bc40:      0x0131f408      0x2ab97ca0      0x00000000      0x00000000
0x131bc50:      0x00000000      0x00000000      0x00000000      0x00000000
0x131bc60:      0xffffffff      0x00000000      0x00000000      0x41445054
0x131bc70:      0x5f444941      0x47000000      0x00000000      0x00000000

this shows that structure passed on a0 has some NULLs and means that this pool was already
destoried. when see destory func, it sets poolHandle->s_WholeMem=NULL;


={============================================================================
*kt_linux_tool_302* gdb: kernel crash analysis {frame-command}

# 01
#

The crash dump When a crash in a kernel module happens, you should see output like the
following on the serial port or in the dmesg buffer (just run the dmesg command to see
it). 

<4>Unhandled kernel unaligned access[#1]:
<4>Cpu 0
<4>$ 0   : 00000000 10008400 {f7ffdfdf} 80070000	# {v0}
<4>$ 4   : c06e27c0 000ee208 8123a000 898d2680
<4>$ 8   : 00000000 7edbffff ffdeff7f fffb7fff
<4>$12   : fdf7fed7 000ee247 00000001 00000001
<4>$16   : 898d2680 00000000 00000001 00000001
<4>$20   : 898d2680 c06e2800 898d2680 00000001
<4>$24   : 00000001 c0504bcc                  
<4>$28   : 89cd2000 89cd3830 000ee208 c050cbe4
<4>Hi    : 00000128
<4>Lo    : 003e5708
<4>epc   : c050cc54 XHddLowIO+0x124/0x3d8 [xtvfs]     Tainted: P    # {pc} 
<4>ra    : c050cbe4 XHddLowIO+0xb4/0x3d8 [xtvfs]
<4>Status: 10008403    KERNEL EXL IE 
<4>Cause : 00800010
<4>BadVA : f7ffe073
<4>PrId  : 0002a044
<4>Modules linked in: xtvfs mhxnvramfs callisto_periph callisto_tuner callisto
<4>Process mount (pid: 404, threadinfo=89cd2000, task=89a249e8)
<4>Stack : 00000001 000ee08d 00000000 c04e523c c05394c4 00000000 00000000 f7ffdfdf
<4>        c06e2800 003e5708 00000001 000ee208 00000000 c04e523c c05394c4 00000000
<4>        c053950c c050cf50 00808000 c050d21c c06e2800 8567bc00 00000000 003e5708
<4>        c04d7f78 c04d7f58 ff7fffdf 000edf25 00000001 00000001 c067e200 c04d804c
<4>        c05394c4 89cd3950 00000000 c04e523c 000ee208 c06e2800 00000001 00000000
<4>        ...
<4>Call Trace:
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]
<4>[<c050cf50>] XHddLowRead+0x1c/0x28 [xtvfs]
<4>[<c04d7f78>] root_dir_devio_read+0x58/0x12c [xtvfs]
<4>[<c04d809c>] root_dir_devio_lock_read+0x50/0x84 [xtvfs]
<4>[<c04d8430>] RootDirCpyClusterReadBuffer+0xec/0x180 [xtvfs]
<4>[<c04d90cc>] RootDirCpyCheckCreate+0xf0/0x1214 [xtvfs]
<4>[<c04da344>] RootDirCpyInit+0x154/0x200 [xtvfs]
<4>[<c04d2944>] pc_ppart_init+0x10bc/0x12a8 [xtvfs]
<4>[<c04fb178>] XTVFS_CheckPpartInit+0x38/0x32c [xtvfs]
<4>[<c04fc684>] InitPpart+0x238/0x540 [xtvfs]
<4>[<c04fca28>] XTVFS_Mount+0x9c/0x490 [xtvfs]
<4>[<c050c8c8>] xtvfs_read_super+0x1e0/0x370 [xtvfs]
<4>[<c050bb80>] xtvfs_fill_super+0x18/0x48 [xtvfs]
<4>[<8007a6b0>] get_sb_bdev+0x114/0x194
<4>[<c050bb5c>] xtvfs_get_sb+0x2c/0x38 [xtvfs]
<4>[<80079f2c>] vfs_kern_mount+0x68/0xc4
<4>[<80079fe4>] do_kern_mount+0x4c/0x7c
<4>[<80094f10>] do_mount+0x5a8/0x614
<4>[<80095010>] sys_mount+0x94/0xec
<4>[<8000e5f0>] stack_done+0x20/0x3c
<4>
<4>
<4>Code: 00008821  8fa2001c  3c038007 <8c440094> 24631824  0060f809  8c46000c  ae020000  27de0001


Unhandled kernel unaligned access An unaligned access is a type of crash. Unlike a
segmentation fault, where a process tries to read memory that is not in its memory map,
and unaligned access is an attempt to read or write an address that is not on a word
boundry. On 32 bit CPUs this means an address not divisble by 4. Often this will generate
an exception and some software will handle the access by reading adjacent words and
piecing things together. But in our case the exception is unhandled. 


$0, $4, etc

This output shows the value of the registers. We are on a MIPS CPU and many of the
registers have defined uses. This
document(http://msdn.microsoft.com/en-us/library/aa448706.aspx) describes them. For
example, $4 to $7 are used to store the first 4 words of function arguments when calling a
function. In assembler these registers are referred to as a0 to a3. You can't know this in
advance, you have to read the MIPS documentation to find it out.  epc c050cc54
XHddLowIO+0x124/0x3d8 [xtvfs] This is the {exception-program-counter}. It shows the address
that the exception occurred at, and that this is 0x124 bytes into the function XHddLowIO
in the module xtvfs.ko. 


Getting the disassembly

Given a kernel module like xtvfs.ko, it is possible to see the disassembled code using the
objdump -D command. Since we have a mips module, we use the cross-compiler from the
Clearcase Fusion view, so our command will look something like: 

mips-linux-uclibc-objdump -D xtvfs.ko

We can then look for the function where we crashed, which is XHddLowIO from the epc trace
above. It starts like this: 

00045b30 <XHddLowIO>:
   45b30:       27bdffb8        addiu   sp,sp,-72
   45b34:       3c020002        lui     v0,0x2
   45b38:       afb50034        sw      s5,52(sp)
   45b3c:       345500d0        ori     s5,v0,0xd0
   45b40:       3c020000        lui     v0,0x0
   45b44:       afb7003c        sw      s7,60(sp)

From the call trace we can calculate the address offset in use. Recall: 
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]

So c050cc54 - 0x124 - 45b30 = offset = c04c7000 (the start of loadded in memory for this
xtvfs.ko). The crash happened at c050cc54 which will appear as c050cc54 - c04c7000 = 45c54
in the disassembly. That code looks like this: 

or 45b30+0x124 = 45c54


   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)
   45c50:       3c030000        lui     v1,0x0
 {45c54}:       8c440094        lw      a0,148(v0)
   45c58:       24630000        addiu   v1,v1,0
   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

So we have crashed executing an lw instruction. 


Using the relocation table

Given a kernel module like xtvfs.ko, it is possible to see the offsets of functions (the
relocation table) using the objdump -r command. Since we have a mips module, we use the
cross-compiler from the Clearcase Fusion view, so our command will look something like: 

mips-linux-uclibc-objdump -r xtvfs.ko > xtvfs_relocations


The output near to our crash address of 45c54 is a table like this: 

 00045c20 R_MIPS_26         .text
 00045c2c R_MIPS_26         .text
 00045c44 R_MIPS_26         .text
 00045c50 R_MIPS_HI16       __getblk
 00045c58 R_MIPS_LO16       __getblk
 00045c78 R_MIPS_HI16       printk
 00045c80 R_MIPS_LO16       printk


Because we are using load time
relocation(http://eli.thegreenplace.net/2011/08/25/load-time-relocation-of-shared-libraries/)
of shared libraries, this table tells the operating system how to replace addresses in the
code. The first column is the address in the code, the second column is the type of
relocation to do, and the third column is the address to relocate. So the code at address
45c50 and 45c58 will get overwritten with the address of __getblk. That makes the
disassembly of the code near our crash look like this: 

   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)
   45c50:       3c030000        lui     v1,__getblk
   45c54:       8c440094        lw      a0,148(v0)
   45c58:       24630000        addiu   v1,v1,0
   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

Understanding __getblk

At 45c5c there is a jump instruction to v1 which has been loaded with the address of
__getblk. But we crashed immediately before that.

	So it seems we crashed while preparing to call __getblk. 
	
So it would help to understand this function. We can look it up in the kernel code: 
http://lxr.free-electrons.com/source/fs/buffer.c?v=2.6.30;a=mips#L1363

1362 /*
1363  * __getblk will locate (and, if necessary, create) the buffer_head
1364  * which corresponds to the passed block_device, block and size. The
1365  * returned buffer has its reference count incremented.
1366  *
1367  * __getblk() cannot fail - it just keeps trying.  If you pass it an
1368  * illegal block number, __getblk() will happily return a buffer_head
1369  * which represents the non-existent block.  Very weird.
1370  *
1371  * __getblk() will lock up the machine if grow_dev_page's try_to_free_buffers()
1372  * attempt is failing.  FIXME, perhaps?
1373  */
1374 struct buffer_head *
1375 __getblk(struct block_device *bdev, sector_t block, unsigned size)
1376 {
1377         struct buffer_head *bh = __find_get_block(bdev, block, size);
1378 
1379         might_sleep();
1380         if (bh == NULL)
1381                 bh = __getblk_slow(bdev, block, size);
1382         return bh;
1383 }
1384 EXPORT_SYMBOL(__getblk);

We should also notice that it can be called via inline function sb_blk : 

287 static inline struct buffer_head *
288 sb_getblk(struct super_block *sb, sector_t block)
289 {
290         return __getblk(sb->s_bdev, block, sb->s_blocksize);
291 }


Understanding where in our C code the crash happened

Now we can tell exactly where in our C code the crash happened. We know we were in the
function XHddLowIO from the call trace and now we know we were calling __getblk or
sb_getblk. In XHddLowIO in the XTVFS code we can see: 


/* allocate sector buffers */
for(i = 0; i < cnt; i++){
    bh_array[i] = sb_getblk(sb,  block++);
    if(!bh_array[i]){ /* no sufficient buffers */
        printk("\n BH_ArrayXHddLowIO: bh = 0, i = %d !!!!!", i);
        if(0 == i){ /* no at all */
            return X_ERROR;
        }
        /* use what we have */
        cnt = i;
    }

So it is likely that the crash happened very close to this sb_getblk call. 


Understanding the lw instruction

Recall the instruction that crashed: 

   45c54:       8c440094        lw      a0,148(v0)

What does that notation mean? We can look up information about the MIPS instructions:
Description: A word is loaded into a register from the specified address.  Operation: $t =
MEM[$s + offset]; advance_pc (4); Syntax: lw $t, offset($s) The whole instruction means:
load a0 with the address in v0 + 148. 


Understanding the MIPS registers

In MIPS, registers tend to have special functions, such as return addresses or function
parameters. We can read about this online. 

a0, the register we are writing to, is the first of the "function argument registers that
hold the first four words of integer type arguments." So a0 is the first argument to the
function we are calling.  v0 is a "function result register" and is also called $2. So we
know its value from the original trace: 

<4>$ 0   : 00000000 10008400 f7ffdfdf 80070000

It is f7ffdfdf. Which is an *odd number*. Since we are trying to read from this address
and do arithmetic (add 148) with it, this would explain why we get an unaligned access. 


Putting it all together

We are executing this line of C: 

bh_array[i] = sb_getblk(sb,  block++);

Because sb_getblk is an inline function, it has been expanded by the compiler into:
__getblk(sb->s_bdev, block, sb->s_blocksize); So our crashing instruction is adding 148
because 148 is the offset of s_bdev withing the sb struct. We hav verify this by looking
at struct super_block in the code.  However, sb has somehow become and odd number, and
THAT is our bug. 


={============================================================================
*kt_linux_tool_303* gdb: .gdbinit

{example-one}
can define user func that have commands to run

$ more .gdbinit
set history save on
set history filename ./.gdb_history
set output-radix 16

define connect
    handle SIG32 nostop noprint pass
    handle SIG33 nostop noprint pass
#    b CTL_SimpleZapperTestStep
#    b CTL_ChannelZapping_FullStbTearDown
#               b readSectionFilterDataAndWriteToFile
    b sectionFilterTask
                b CTL_SectionFilter_Engine.c:2126
                b CTL_SectionFilter_Engine.c:2146
    directory components/FOSH/FUSIONOS_XTV_TESTS/xtv_test/src/
                i b
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define init_task
   set $t=&init_task
   printf "task name \"%s\", pid %05d \n", $t->comm, t->pid
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define find_task
  # Addresses greater than _end: kernel data...
  # ...user passed in an address
  if ((unsigned)$arg0 > (unsigned)&_end)
    set $t=(struct task_struct *)$arg0
  else
    # User entered a numeric PID
    # Walk the task list to find it
    set $t=&init_task
    if (init_task.pid != (unsigned)$arg0)
      find_next_task $t
      while (&init_task!=$t && $t->pid != (unsigned)$arg0)
        find_next_task $t
      end
      if ($t == &init_task)
        printf "Couldn't find task; using init_task\n"
      end
    end
  end
  printf "Task \"%s\":\n", $t->comm
end


Reads and executes the commands from init file (if any) in the current working directory. This is
only done if the current directory is different from your home directory. Thus, you can have more
than one init file, one generic in your home directory, and another, specific to the program you are
debugging, in the directory where you invoke gdb.


{gdbinit-for-c++}
#
#   STL GDB evaluators/views/utilities - 1.03
#
#   The new GDB commands:
#       are entirely non instrumental
#       do not depend on any "inline"(s) - e.g. size(), [], etc
#       are extremely tolerant to debugger settings
#
#   This file should be "included" in .gdbinit as following:
#   source stl-views.gdb or just paste it into your .gdbinit file
#
#   The following STL containers are currently supported:
#
#       std::vector<T> -- via pvector command
#       std::list<T> -- via plist or plist_member command
#       std::map<T,T> -- via pmap or pmap_member command
#       std::multimap<T,T> -- via pmap or pmap_member command
#       std::set<T> -- via pset command
#       std::multiset<T> -- via pset command
#       std::deque<T> -- via pdequeue command
#       std::stack<T> -- via pstack command
#       std::queue<T> -- via pqueue command
#       std::priority_queue<T> -- via ppqueue command
#       std::bitset<n> -- via pbitset command
#       std::string -- via pstring command
#       std::widestring -- via pwstring command
#
#   The end of this file contains (optional) C++ beautifiers
#   Make sure your debugger supports $argc
#
#   Simple GDB Macros writen by Dan Marinescu (H-PhD) - License GPL
#   Inspired by intial work of Tom Malnar,
#     Tony Novac (PhD) / Cornell / Stanford,
#     Gilad Mishne (PhD) and Many Many Others.
#   Contact: dan_c_marinescu@yahoo.com (Subject: STL)
#
#   Modified to work with g++ 4.3 by Anders Elton
#   Also added _member functions, that instead of printing the entire class in map, prints a member.

# support for pending breakpoints - you can now set a breakpoint into a shared library before the it was loaded.
set breakpoint pending on

#
# std::vector<>
#

define pvector
    if $argc == 0
        help pvector
    else
        set $size = $arg0._M_impl._M_finish - $arg0._M_impl._M_start
        set $capacity = $arg0._M_impl._M_end_of_storage - $arg0._M_impl._M_start
        set $size_max = $size - 1
    end
    if $argc == 1
        set $i = 0
        while $i < $size
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
    end
    if $argc == 2
        set $idx = $arg1
        if $idx < 0 || $idx > $size_max
            printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
        else
            printf "elem[%u]: ", $idx
            p *($arg0._M_impl._M_start + $idx)
        end
    end
    if $argc == 3
      set $start_idx = $arg1
      set $stop_idx = $arg2
      if $start_idx > $stop_idx
        set $tmp_idx = $start_idx
        set $start_idx = $stop_idx
        set $stop_idx = $tmp_idx
      end
      if $start_idx < 0 || $stop_idx < 0 || $start_idx > $size_max || $stop_idx > $size_max
        printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
      else
        set $i = $start_idx
        while $i <= $stop_idx
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
      end
    end
    if $argc > 0
        printf "Vector size = %u\n", $size
        printf "Vector capacity = %u\n", $capacity
        printf "Element "
        whatis $arg0._M_impl._M_start
    end
end

document pvector
    Prints std::vector<T> information.
    Syntax: pvector <vector> <idx1> <idx2>
    Note: idx, idx1 and idx2 must be in acceptable range [0..<vector>.size()-1].
    Examples:
    pvector v - Prints vector content, size, capacity and T typedef
    pvector v 0 - Prints element[idx] from vector
    pvector v 1 2 - Prints elements in range [idx1..idx2] from vector
end

#
# std::list<>
#

define plist
    if $argc == 0
        help plist
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 2
                printf "elem[%u]: ", $size
                p *($arg1*)($current + 1)
            end
            if $argc == 3
                if $size == $arg2
                    printf "elem[%u]: ", $size
                    p *($arg1*)($current + 1)
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist <variable_name> <element_type> to see the elements in the list.\n"
        end
    end
end

document plist
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist l - prints list size and definition
    plist l int - prints all elements and list size
    plist l int 2 - prints the third element in the list (if exists) and list size
end

define plist_member
    if $argc == 0
        help plist_member
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 3
                printf "elem[%u]: ", $size
                p (*($arg1*)($current + 1)).$arg2
            end
            if $argc == 4
                if $size == $arg3
                    printf "elem[%u]: ", $size
                    p (*($arg1*)($current + 1)).$arg2
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist_member <variable_name> <element_type> <member> to see the elements in the list.\n"
        end
    end
end

document plist_member
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist_member l int member - prints all elements and list size
    plist_member l int member 2 - prints the third element in the list (if exists) and list size
end


#
# std::map and std::multimap
#

define pmap
    if $argc == 0
        help pmap
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 3
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p *($arg1*)$value
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p *($arg2*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 4
            set $idx = $arg3
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p *($arg1*)$value
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p *($arg2*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        if $argc == 5
            set $idx1 = $arg3
            set $idx2 = $arg4
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                set $valueLeft = *($arg1*)$value
                set $valueRight = *($arg2*)($value + sizeof($arg1))
                if $valueLeft == $idx1 && $valueRight == $idx2
                    printf "elem[%u].left: ", $i
                    p $valueLeft
                    printf "elem[%u].right: ", $i
                    p $valueRight
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap m - prints map size and definition
    pmap m int int - prints all elements and map size
    pmap m int int 20 - prints the element(s) with left-value = 20 (if any) and map size
    pmap m int int 20 200 - prints the element(s) with left-value = 20 and right-value = 200 (if any) and map size
end


define pmap_member
    if $argc == 0
        help pmap_member
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 5
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p (*($arg1*)$value).$arg2
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p (*($arg3*)$value).$arg4
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 6
            set $idx = $arg5
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p (*($arg1*)$value).$arg2
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p (*($arg3*)$value).$arg4
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap_member
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap_member m class1 member1 class2 member2 - prints class1.member1 : class2.member2
    pmap_member m class1 member1 class2 member2 lvalue - prints class1.member1 : class2.member2 where class1 == lvalue
end


#
# std::set and std::multiset
#

define pset
    if $argc == 0
        help pset
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Set "
            whatis $tree
            printf "Use pset <variable_name> <element_type> to see the elements in the set.\n"
        end
        if $argc == 2
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u]: ", $i
                p *($arg1*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 3
            set $idx = $arg2
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u]: ", $i
                    p *($arg1*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Set size = %u\n", $tree_size
    end
end

document pset
    Prints std::set<T> or std::multiset<T> information. Works for std::multiset as well.
    Syntax: pset <set> <T> <val>: Prints set size, if T defined all elements or just element(s) having val
    Examples:
    pset s - prints set size and definition
    pset s int - prints all elements and the size of s
    pset s int 20 - prints the element(s) with value = 20 (if any) and the size of s
end



#
# std::dequeue
#

define pdequeue
    if $argc == 0
        help pdequeue
    else
        set $size = 0
        set $start_cur = $arg0._M_impl._M_start._M_cur
        set $start_last = $arg0._M_impl._M_start._M_last
        set $start_stop = $start_last
        while $start_cur != $start_stop
            p *$start_cur
            set $start_cur++
            set $size++
        end
        set $finish_first = $arg0._M_impl._M_finish._M_first
        set $finish_cur = $arg0._M_impl._M_finish._M_cur
        set $finish_last = $arg0._M_impl._M_finish._M_last
        if $finish_cur < $finish_last
            set $finish_stop = $finish_cur
        else
            set $finish_stop = $finish_last
        end
        while $finish_first != $finish_stop
            p *$finish_first
            set $finish_first++
            set $size++
        end
        printf "Dequeue size = %u\n", $size
    end
end

document pdequeue
    Prints std::dequeue<T> information.
    Syntax: pdequeue <dequeue>: Prints dequeue size, if T defined all elements
    Deque elements are listed "left to right" (left-most stands for front and right-most stands for back)
    Example:
    pdequeue d - prints all elements and size of d
end



#
# std::stack
#

define pstack
    if $argc == 0
        help pstack
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = $size - 1
        while $i >= 0
            p *($start_cur + $i)
            set $i--
        end
        printf "Stack size = %u\n", $size
    end
end

document pstack
    Prints std::stack<T> information.
    Syntax: pstack <stack>: Prints all elements and size of the stack
    Stack elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    pstack s - prints all elements and the size of s
end



#
# std::queue
#

define pqueue
    if $argc == 0
        help pqueue
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = 0
        while $i < $size
            p *($start_cur + $i)
            set $i++
        end
        printf "Queue size = %u\n", $size
    end
end

document pqueue
    Prints std::queue<T> information.
    Syntax: pqueue <queue>: Prints all elements and the size of the queue
    Queue elements are listed "top to bottom" (top-most element is the first to come on pop)
    Example:
    pqueue q - prints all elements and the size of q
end



#
# std::priority_queue
#

define ppqueue
    if $argc == 0
        help ppqueue
    else
        set $size = $arg0.c._M_impl._M_finish - $arg0.c._M_impl._M_start
        set $capacity = $arg0.c._M_impl._M_end_of_storage - $arg0.c._M_impl._M_start
        set $i = $size - 1
        while $i >= 0
            p *($arg0.c._M_impl._M_start + $i)
            set $i--
        end
        printf "Priority queue size = %u\n", $size
        printf "Priority queue capacity = %u\n", $capacity
    end
end

document ppqueue
    Prints std::priority_queue<T> information.
    Syntax: ppqueue <priority_queue>: Prints all elements, size and capacity of the priority_queue
    Priority_queue elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    ppqueue pq - prints all elements, size and capacity of pq
end



#
# std::bitset
#

define pbitset
    if $argc == 0
        help pbitset
    else
        p /t $arg0._M_w
    end
end

document pbitset
    Prints std::bitset<n> information.
    Syntax: pbitset <bitset>: Prints all bits in bitset
    Example:
    pbitset b - prints all bits in b
end



#
# std::string
#

define pstring
    if $argc == 0
        help pstring
    else
        printf "String \t\t\t= \"%s\"\n", $arg0._M_data()
        printf "String size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "String capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "String ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pstring
    Prints std::string information.
    Syntax: pstring <string>
    Example:
    pstring s - Prints content, size/length, capacity and ref-count of string s
end

#
# std::wstring
#

define pwstring
    if $argc == 0
        help pwstring
    else
        call printf("WString \t\t= \"%ls\"\n", $arg0._M_data())
        printf "WString size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "WString capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "WString ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pwstring
    Prints std::wstring information.
    Syntax: pwstring <wstring>
    Example:
    pwstring s - Prints content, size/length, capacity and ref-count of wstring s
end

#
# C++ related beautifiers (optional)
#

set print pretty on
set print object on
set print static-members on
set print vtbl on
set print demangle on
set demangle-style gnu-v3
set print sevenbit-strings off

set history filename ~/.gdb_history
set history save

# finally stop the silly "A debugging session is active." - question ... just quit both.
set confirm off


={============================================================================
*kt_linux_tool_307* gdb: commands for stepping

{gdb-next-gdb-step}
n (next) to advance execution to the next line of the current function.

s (step) instead of next. step goes to the next line to be executed in any subroutine.
It shows a summary of the stack. can use the backtrace command (which can also be spelled bt), to
see where we are in the stack as a whole: the backtrace command displays a stack frame for each
active subroutine.

note. Warning: If you use the step command while control is within a function that was compiled
without debugging information, execution proceeds until control reaches a function that does have
debugging information. Likewise, it will not step into a function which is compiled without
debugging information. To step through functions without debugging information, use the stepi
command, described below.

To step for a single assembly instruction, use the (gdb) stepi

Also, the step command 'only' enters a function if there is line number information for the
function. Otherwise it acts like the next command.

{gdb-return-gdb-finish}
To resume execution at a different place, you can use return (see Section 17.4 [Returning from a
Function], page 221) to go back to the calling function; or jump (see Section 17.2 [Continuing
at a Different Address], page 220) to go to an arbitrary location in your program.

The finish Continue running until just after function in the selected stack frame returns.  Print
the returned value (if any). This command can be abbreviated as fin.  Contrast this with the return
command (see Section 17.4 [Returning from a Function], page 221). Q: this is only difference?

{gdb-print}
p (print) to see their values.

(gdb) p len lquote
$3 = 9
(gdb) p len rquote
$4 = 7

{gdb-set}
can 'set' them to better values using the p command, since it can print the value of any
expressionâand that expression can include subroutine calls and assignments. set new value and
continue to see if it fixes the bug.

(gdb) p len lquote=strlen(lquote)
$5 = 7
(gdb) p len rquote=strlen(rquote)
$6 = 9
(gdb) c
Continuing.


{gdb-advance}
To continue to a specific location, use the 
(gdb) advance funcname


{?}
(gdb) run -v all
(gdb) sectionFilterTable[sfIdx]
(gdb) directory components/FOSH/FUSIONOS_XTV_TESTS/xtv_test/src/
(gdb) directory components/FOSH/FUSIONOS_TEST_SHARED/DSL/src/ (SDS_SectionFilter.c)


={============================================================================
*kt_linux_tool_309* gdb-break

5.1 Breakpoints, Watchpoints, and Catchpoints

A breakpoint makes your program stop whenever a certain point in the program is
reached. For each breakpoint, you can add conditions to control in finer detail
whether your program stops. You can set breakpoints with the break command and
its variants (see Setting Breakpoints), to specify the place where your program
should stop 'by' line number, function name or exact address in the program. 

On some systems, you can set breakpoints in shared libraries before the
executable is run. 

A watchpoint is a special breakpoint that stops your program when the value of
an expression changes. The expression may be a value of a variable, or it could
involve values of one or more variables combined by operators, such as 'a + b'.
This is sometimes called 'data' breakpoints. You must use a different command to
set watchpoints, but aside from that, you can manage a watchpoint like any other
breakpoint: you enable, disable, and delete both breakpoints and watchpoints
using the same commands.

You can arrange to have values from your program displayed automatically
whenever gdb stops at a breakpoint. See Automatic Display.

A catchpoint is another special breakpoint that stops your program when a
certain kind of 'event' occurs, such as the throwing of a C++ exception or the
loading of a library. As with watchpoints, you use a different command to set a
catchpoint, but aside from that, you can manage a catchpoint like any other
breakpoint. (To stop when your program receives a signal, use the handle
    command; see Signals.)

gdb assigns a number to each breakpoint, watchpoint, or catchpoint when you
create it; these numbers are successive integers starting with one. In many of
the commands for controlling various features of breakpoints you use the
breakpoint number to say which breakpoint you want to change. Each breakpoint
may be enabled or disabled; if disabled, it has no effect on your program until
you enable it again.

Some gdb commands accept a range of breakpoints on which to operate. A
breakpoint range is either a single breakpoint number, like '5', or two such
numbers, in increasing order, separated by a hyphen, like '5-7'. When a
breakpoint range is given to a command, all breakpoints in that range are
operated on. 


={============================================================================
*kt_linux_tool_309* gdb-break: specifying a location

5.1.1 Setting Breakpoints

Breakpoints are set with the break command (abbreviated b).

break location
    Set a breakpoint at the given location, which can specify a function name, a
    line number, or an address of an instruction. The breakpoint will stop your
    program just before it executes any of the code in the specified location.

    When using source languages that permit overloading of symbols, such as C++,
    a function name may refer to more than one possible place to break. See
      Ambiguous Expressions, for a discussion of that situation.

    It is also possible to insert a breakpoint that will stop the program only
    if a specific 'thread' (see Thread-Specific Breakpoints).


9.2 Specifying a Location

Several gdb commands accept arguments that specify a location of your program's
code. Since gdb is a source-level debugger, a location usually specifies some
line in the source code; for that reason, locations are also known as linespecs.

9.2.1 Linespec Locations

A linespec is a colon-separated list of source location parameters such as file
name, function name, etc. Here are all the different ways of specifying a
linespec:

linenum
    Specifies the line number linenum of the current source file.
-offset
+offset
    Specifies the line offset lines before or after the current line. For the
    list command, the current line is the last one printed; for the breakpoint
    commands, this is the line at which execution stopped in the currently
    selected stack frame (see Frames, for a description of stack frames.) When
    used as the second of the two linespecs in a list command, this specifies
    the line offset lines up or down from the first linespec.

filename:linenum
    Specifies the line linenum in the source file filename. If filename is a
    relative file name, then it will match any source file name with the same
    trailing components. For example, if filename is ‘gcc/expr.c’, then it will
    match source file name of /build/trunk/gcc/expr.c, but not
    /build/trunk/libcpp/expr.c or /build/trunk/gcc/x-expr.c.

function
    Specifies the line that begins the body of the function function. For
    example, in C, this is the line with the open brace.

function:label
    Specifies the line where label appears in function.

filename:function
    Specifies the line that begins the body of the function in the file
    filename. You only need the file name with a function name to avoid
    ambiguity when there are identically named functions in different source
    files.

label
    Specifies the line at which the label named label appears in the function
    corresponding to the currently selected stack frame. If there is no current
    selected stack frame (for instance, if the inferior is not running), then
    gdb will not search for a label. 


9.2.2 Explicit Locations

Explicit locations allow the user to directly specify the source location's
parameters using option-value pairs.

Explicit locations are useful when several functions, labels, or file names have
the 'same' name (base name for files) in the program's sources. In these cases,
    explicit locations point to the source line you meant more accurately and
      unambiguously. Also, using explicit locations might be faster in large
      programs.

For example, the linespec 'foo:bar' may refer to a function bar defined in the
file named foo or the label bar in a function named foo. gdb must search either
the file system or the symbol table to know.

The list of valid explicit location options is summarized in the following
table:

-source filename
    The value specifies the source file name. To differentiate between files
    with the same base name, prepend as many directories as is necessary to
    uniquely identify the desired file, e.g., foo/bar/baz.c. Otherwise gdb will
    use the first file it finds with the given base name. This option requires
    the use of either -function or -line.

-function function
    The value specifies the name of a function. Operations on function locations
    unmodified by other options (such as -label or -line) refer to the line that
    begins the body of the function. In C, for example, this is the line with
    the open brace.

-label label
    The value specifies the name of a label. When the function name is not
    specified, the label is searched in the function of the currently selected
    stack frame.

-line number
    The value specifies a line offset for the location. The offset may either be
    absolute (-line 3) or relative (-line +3), depending on the command. When
    specified without any other options, the line offset is relative to the
    current line. 

Explicit location options may be abbreviated by omitting any non-unique trailing
characters from the option name, e.g., break -s main.c -li 3. 


9.2.3 Address Locations


={============================================================================
*kt_linux_tool_309* gdb-break: specifying a location in cpp


  (gdb) break lib.cpp:5
  Breakpoint 2 at 0xb7fd846e: file lib.cpp, line 5.


={============================================================================
*kt_linux_tool_309* gdb-break: set

5.1.1 Setting Breakpoints

tbreak args
    Set a breakpoint enabled only for 'one' stop. The args are the same as for
    the break command, and the breakpoint is set in the same way, but the
    breakpoint is automatically deleted after the first time your program stops
    there. See Disabling Breakpoints. 


break ... if cond
    Set a breakpoint with condition cond; evaluate the expression cond each time
    the breakpoint is reached, and stop only if the value is nonzero—that is, if
    cond evaluates as true. ‘...’ stands for one of the possible arguments
    described above (or no argument) specifying where to break. See Break
    Conditions, for more information on breakpoint conditions. 


<ex>
gpointer
g_object_ref (gpointer _object)
{
  ...
}

break g_object_ref if _object == 0xcafebabe
break g_object_unref if _object == 0xcafebabe


gdb normally implements breakpoints by replacing the program code at the
breakpoint address with a special instruction, which, when executed, given
control to the debugger.  By default, the program code is so modified only when
the program is resumed. As soon as the program stops, gdb restores the original
instructions. This behaviour guards against leaving breakpoints inserted in the
target should gdb abrubptly disconnect. However, with slow remote targets,
inserting and removing breakpoint can reduce the performance. This behavior can
  be controlled with the following commands::

set breakpoint always-inserted off
    All breakpoints, including newly added by the user, are inserted in the
    target only when the target is resumed. All breakpoints are removed from the
    target when it stops. This is the 'default' mode.

set breakpoint always-inserted on
    Causes all breakpoints to be inserted in the target at all times. If the
    user adds a new breakpoint, or changes an existing breakpoint, the
    breakpoints in the target are updated immediately. A breakpoint is removed
    from the target only when breakpoint itself is deleted. 


gdb handles conditional breakpoints by evaluating these conditions when a
breakpoint breaks. If the condition is true, then the process being debugged
stops, otherwise the process is resumed.

If the target supports evaluating conditions on its end, gdb may download the
breakpoint, together with its conditions, to it.

This feature can be controlled via the following commands:

set breakpoint condition-evaluation auto
    This is the default mode. If the target supports evaluating breakpoint
    conditions on its end, gdb will download breakpoint conditions to the target
    (limitations mentioned previously apply). If the target does not support
    breakpoint condition evaluation, then gdb will fallback to evaluating all
    these conditions on the host's side. 


={============================================================================
*kt_linux_tool_309* gdb-break: conditional ex


There are times when simply breaking on every call to a function isn't what you
want. Sometimes you want to be more 'selective'. For example, with the problem
I'm debugging right now, I'm interested in when close() is called on a
particular file descriptor. I don't want to break on every call to close(),
since it is called often.

GDB supports conditional breaking, but it wasn't immediately clear to me how I
should:

     Access the function parameters - I'm interested in the first parameter to
     the function (but I do 'not' know the parameter name).

    Construct the conditional expression part of the break command.

Reading the Function Parameters

I found a way to do this, although it is machine specific (intel in this case),
and requires knowledge of the machine stack layout. I found a reasonable
  reference for Intel here:
  http://www.tenouk.com/Bufferoverflowc/Bufferoverflow3.html

You don't need to understand every last detail. It's enough to remember that the
parameters can be found at a 2-word positive offset from $ebp. So in the 32-bit
case, the first parameter can be found at $ebp+8.

Say I break on this function:

void foo(int i, int j) {
    // stuff
}

And I want to inspect the first and second parameters (i & j). I can get the
value of the first parameter like this:

x/d $ebp+8

Similarly, I can get the second parameter like this:

x/d $ebp+12

since sizeof(int) == 4 in this case.

Conditional break

Armed with this knowledge, let's say I want to set a breakpoint on foo, but only
break when the value of the first parameter is 12345. How do I construct the
expression? Well, it turns out that the conditional syntax is just like a C 'if'
statement. So I do:

b foo if (*(int*)($ebp+8)==12345)

And if I want to put a constraint on the value of the second parameter:

b foo if (*(int*)($ebp+8)==12345 && *(int*)($ebp+12)==5678)


={============================================================================
*kt_linux_tool_309* gdb-break: info, get you quickly where

info breakpoints [n...]
info break [n...]

Print a table of all breakpoints, watchpoints, and catchpoints set and not
deleted. Optional argument n means print information only about the specified
breakpoint(s) (or watchpoint(s) or catchpoint(s)). For each breakpoint,
following columns are printed:


note: tip

info break displays a count of the number of times the breakpoint has been hit.
This is especially useful in conjunction with the ignore command. You can ignore
a large number of breakpoint hits, look at the breakpoint info to see how many
times the breakpoint was hit, and then run again, ignoring one less than that
number. This will get you 'quickly' to the last hit of that breakpoint.


note: on cpp

It is possible that a breakpoint corresponds to several locations in your
program. Examples of this situation are:

    Multiple functions in the program may have the same name.

    For a C++ constructor, the gcc compiler generates several instances of the
    function body, used in different cases.

    For a C++ template function, a given line in the function can correspond to
    any number of instantiations.

    For an inlined function, a given source line can correspond to several
    places where that function is inlined. 


In all those cases, gdb will insert a breakpoint at all the relevant locations.

A breakpoint with multiple 'locations' is displayed in the breakpoint table
using several rows—one header row, followed by one row for each breakpoint
location.  The header row has ‘<MULTIPLE>’ in the address column. The rows for
individual locations contain the actual addresses for locations, and show the
functions to which those locations belong. The number column for a location is
of the form breakpoint-number.'location'-number.

For example:

     Num     Type           Disp Enb  Address    What
     1       breakpoint     keep y    <MULTIPLE>
             stop only if i==1
             breakpoint already hit 1 time
     1.1                         y    0x080486a2 in void foo<int>() at t.cc:8
     1.2                         y    0x080486ca in void foo<double>() at t.cc:8

Each location can be individually enabled or disabled by passing
breakpoint-number.location-number as argument to the enable and disable
commands. Note that you 'cannot' delete the individual locations from the list,
you can only delete the entire list of locations that belong to their parent
  breakpoint (with the delete num command, where num is the number of the parent
      breakpoint, 1 in the above example). Disabling or enabling the parent
  breakpoint (see Disabling) affects all of the locations that belong to that
  breakpoint. 


={============================================================================
*kt_linux_tool_309* gdb-break: set in shared

5.1.1 Setting Breakpoints

<pending-breakpoint>

Shared libraries can be loaded and unloaded explicitly, and possibly repeatedly,
as the program is executed.  To support this use case, gdb updates breakpoint
  locations 'whenever' any shared library is loaded or unloaded.  Typically, you
  would set a breakpoint in a shared library at the 'beginning' of your
  debugging session, when the library is not loaded, and when the symbols from
  the library are not available. When you try to set breakpoint, gdb will ask
  you if you want to set a so called 'pending' breakpoint-breakpoint whose
  address is not yet resolved.  After the program is run, whenever a new shared
  library is loaded, gdb reevaluates all the breakpoints. When a newly loaded
  shared library contains the symbol or line referred to by some pending
  breakpoint, that breakpoint is resolved and becomes an ordinary breakpoint.
  When a library is unloaded, all breakpoints that refer to its symbols or
  source lines become pending again.

This logic works for breakpoints with multiple locations, too. For example, if
you have a breakpoint in a C++ template function, and a newly loaded shared
library has an instantiation of that template, a new location is added to the
list of locations for the breakpoint.

Except for having unresolved address, pending breakpoints do not differ from
regular breakpoints.  You can set conditions or commands, enable and disable
them and perform other breakpoint operations.

set breakpoint pending auto

This is the 'default' behavior. When gdb cannot find the breakpoint location, it
queries you whether a pending breakpoint should be created.

set breakpoint pending on

This indicates that an unrecognized breakpoint location should automatically
result in a pending
breakpoint being created.


={============================================================================
*kt_linux_tool_309* gdb-break: set watch

5.1.2 Setting Watchpoints

You can use a watchpoint to stop execution whenever the 'value' of an
'expression' changes, 'without' having to predict a particular place where this
may happen. (This is sometimes called a data breakpoint.) The expression may be
as simple as the value of a single variable, or as complex as many variables
combined by operators. Examples include:

    A reference to the value of a single variable.

    An address cast to an appropriate data type. For example, ‘*(int
        *)0x12345678’ will watch a 4-byte region at the specified address
    (assuming an int occupies 4 bytes).

    An arbitrarily complex expression, such as ‘a*b + c/d’. The expression can
    use any operators valid in the program's native language (see Languages). 

You can set a watchpoint on an expression even if the expression can not be
evaluated 'yet'. For instance, you can set a watchpoint on ‘*global_ptr’ before
‘global_ptr’ is initialized. gdb will stop when your program sets ‘global_ptr’
and the expression produces a valid value. If the expression becomes valid in
some other way than changing a variable (e.g. if the memory pointed to by
    ‘*global_ptr’ becomes readable as the result of a malloc call), gdb may not
stop until the next time the expression changes.

Depending on your system, watchpoints may be implemented in software or
hardware. gdb does 'software' watchpointing by single-stepping your program and
testing the variable's value each time, which is hundreds of times slower than
normal execution. (But this may still be worth it, to catch errors where you
    have no clue what part of your program is the culprit.)

watch [-l|-location] expr [thread threadnum] [mask maskvalue]

    Set a watchpoint for an expression. gdb will break when the expression expr
    is written into by the program and its value 'changes'. The simplest (and
        the most popular) use of this command is to watch the value of a
    'single' variable:

              (gdb) watch foo

    If the command includes a [thread threadnum] argument, gdb breaks only when
    the thread identified by threadnum changes the value of expr. If any other
    threads change the value of expr, gdb will not break. Note that watchpoints
    restricted to a single thread in this way only work with Hardware
    Watchpoints.

    Ordinarily a watchpoint respects the scope of variables in expr (see below).
    The -location argument tells gdb to instead watch the memory referred to by
    expr. In this case, gdb will evaluate expr, take the address of the result,
and watch the memory at that address. The type of the result is used to
  determine the size of the watched memory. If the expression's result does not
  have an address, then gdb will print an error.

    The [mask maskvalue] argument allows creation of 'masked' watchpoints, if the
    current architecture supports this feature (e.g., PowerPC Embedded
        architecture, see PowerPC Embedded.) A masked watchpoint specifies a
    mask in addition to an address to watch. The mask specifies that some 'bits'
    of an address (the bits which are reset in the mask) should be ignored when
    matching the address accessed by the inferior against the watchpoint
    address. Thus, a masked watchpoint watches many addresses
    simultaneously—those addresses whose unmasked bits are identical to the
    unmasked bits in the watchpoint address. The mask argument implies
    -location. Examples:

              (gdb) watch foo mask 0xffff00ff
              (gdb) watch *0xdeadbeef mask 0xffffff00


rwatch [-l|-location] expr [thread threadnum] [mask maskvalue]
    Set a watchpoint that will break when the value of expr is read by the
    program.


awatch [-l|-location] expr [thread threadnum] [mask maskvalue]
    Set a watchpoint that will break when expr is either read from or written
    into by the program.


info watchpoints [n...]
    This command prints a list of watchpoints, using the same format as info
    break (see Set Breaks). 

If you watch for a change in a numerically entered address you need to
'dereference' it, as the address itself is just a constant number which will
never change. gdb refuses to create a watchpoint that watches a never-changing
value:

     (gdb) watch 0x600850
     Cannot watch constant value 0x600850.
     (gdb) watch *(int *) 0x600850
     Watchpoint 1: *(int *) 6293584

gdb sets a hardware watchpoint if 'possible'. Hardware watchpoints execute very
quickly, and the debugger reports a change in value at the exact instruction
where the change occurs. If gdb cannot set a hardware watchpoint, it sets a
software watchpoint, which executes more slowly and reports the change in value
at the next statement, not the instruction, after the change occurs.

Currently, the awatch and rwatch commands can only set hardware watchpoints,
because accesses to data that don't change the value of the watched expression
  cannot be detected without examining every instruction as it is being
  executed, and gdb does not do that currently. If gdb finds that it is unable
  to set a hardware breakpoint with the awatch or rwatch command, it will print
  a message like this:

     Expression cannot be implemented with read/access watchpoint.

Sometimes, gdb cannot set a hardware watchpoint because the data type of the
watched expression is wider than what a hardware watchpoint on the target
machine can handle. For example, some systems can only watch regions that are up
to 4 bytes wide; on such systems you cannot set hardware watchpoints for an
expression that yields a double-precision floating-point number (which is
    typically 8 bytes wide). As a work-around, it might be possible to break the
large region into a series of smaller ones and watch them with separate
watchpoints.

If you set too many hardware watchpoints, gdb might be unable to insert all of
them when you resume the execution of your program. Since the precise number of
active watchpoints is unknown until such time as the program is about to be
resumed, gdb might not be able to warn you about this when you set the
watchpoints, and the warning will be printed only when the program is resumed:

     Hardware watchpoint num: Could not insert watchpoint

If this happens, delete or disable some of the watchpoints.

Watching complex expressions that reference many variables can also exhaust the
resources available for hardware-assisted watchpoints. That's because gdb needs
to watch every variable in the expression with separately allocated resources.

If you call a function interactively using print or call, any watchpoints you
have set will be inactive until gdb reaches another kind of breakpoint or the
call completes.

gdb automatically deletes watchpoints that watch local (automatic) variables, or
expressions that involve such variables, when they go out of scope, that is,
when the execution leaves the block in which these variables were defined. In
  particular, when the program being debugged terminates, all local variables go
  out of scope, and so only watchpoints that watch global variables remain set.
  If you rerun the program, you will need to set all such watchpoints again. One
  way of doing that would be to set a code breakpoint at the entry to the main
  function and when it breaks, set all the watchpoints.

In multi-threaded programs, watchpoints will detect changes to the watched
expression 'from' every thread.

note:

Warning: In multi-threaded programs, software watchpoints have only limited
usefulness. If gdb creates a software watchpoint, it can only watch the value of
an expression in a single thread. If you are confident that the expression can
only change due to the current thread's activity (and if you are also confident
    that no other thread can become current), then you can use software
watchpoints as usual. However, gdb may not notice when a non-current thread's
activity changes the expression. Hardware watchpoints, in contrast, watch an
expression in all threads. 


={============================================================================
*kt_linux_tool_309* gdb-break: set catch

5.1.3 Setting Catchpoints

You can use catchpoints to cause the debugger to stop for certain kinds of
program events, such as C++ 'exceptions' or the loading of a shared library. Use
the catch command to set a catchpoint.

catch event
    Stop when event occurs. The event can be any of the following:

    throw [regexp]
    rethrow [regexp]
    catch [regexp]
        The throwing, re-throwing, or catching of a C++ exception.

        If regexp is given, then only exceptions whose type matches the regular
        expression will be caught.

        The convenience variable $_exception is available at an
        exception-related catchpoint, on some systems. This holds the exception
        being thrown.

        There are currently some limitations to C++ exception handling in gdb:

            note:
            The support for these commands is system-dependent. Currently, only
            systems using the ‘gnu-v3’ C++ ABI (see ABI) are supported.

            The regular expression feature and the $_exception convenience
            variable rely on the presence of some SDT probes in libstdc++. If
            these probes are not present, then these features cannot be used.
            These probes were first available in the GCC 4.8 release, but
            whether or not they are available in your GCC also depends on how it
            was built.
            
            The $_exception convenience variable is only valid at the
            instruction at which an exception-related catchpoint is set.

            When an exception-related catchpoint is hit, gdb stops at a location
            in the system library which implements runtime exception support for
            C++, usually libstdc++. You can use up (see Selection) to get to
            your code.

            If you call a function interactively, gdb normally returns control
            to you when the function has finished executing. If the call raises
            an exception, however, the call may bypass the mechanism that
            returns control to you and cause your program either to abort or to
            simply continue running until it hits a breakpoint, catches a signal
            that gdb is listening for, or exits. This is the case even if you
            set a catchpoint for the exception; catchpoints on exceptions are
            disabled within interactive calls. See Calling, for information on
            controlling this with set unwind-on-terminating-exception.

            You cannot raise an exception interactively.

            You cannot install an exception handler interactively. 

    exec
        A call to exec. This is currently only available for HP-UX and gnu/Linux.

    syscall
    syscall [name | number] ...
        A call to or return from a system call, a.k.a. syscall. A syscall is a
        mechanism for application programs to request a service from the
        operating system (OS) or one of the OS system services. gdb can catch
        some or all of the syscalls issued by the debuggee, and show the related
        information for each syscall. If no argument is specified, calls to and
        returns from all system calls will be caught.

        name can be any system call name that is valid for the underlying OS.
        you can use the gdb command-line completion facilities to list the
        available choices.

        You may also specify the system call numerically. A syscall's number is
        the value passed to the OS's syscall dispatcher to identify the
        requested service. When you specify the syscall by its name, gdb uses
        its database of syscalls to convert the name into the corresponding
        numeric code, but using the number directly may be useful if gdb's
        database does not have the complete list of syscalls on your system
        (e.g., because gdb lags behind the OS upgrades).

        The example below illustrates how this command works if you don't
        provide arguments to it:

                       (gdb) catch syscall
                       Catchpoint 1 (syscall)
                       (gdb) r
                       Starting program: /tmp/catch-syscall
                       
                       Catchpoint 1 (call to syscall 'close'), \
                       	   0xffffe424 in __kernel_vsyscall ()
                       (gdb) c
                       Continuing.
                       
                       Catchpoint 1 (returned from syscall 'close'), \
                       	0xffffe424 in __kernel_vsyscall ()
                       (gdb)

        Here is an example of catching a system call by name:

                       (gdb) catch syscall chroot
                       Catchpoint 1 (syscall 'chroot' [61])
                       (gdb) r
                       Starting program: /tmp/catch-syscall
                       
                       Catchpoint 1 (call to syscall 'chroot'), \
                       		   0xffffe424 in __kernel_vsyscall ()
                       (gdb) c
                       Continuing.
                       
                       Catchpoint 1 (returned from syscall 'chroot'), \
                       	0xffffe424 in __kernel_vsyscall ()
                       (gdb)

        An example of specifying a system call numerically. In the case below,
        the syscall number has a corresponding entry in the XML file, so gdb
          finds its name and prints it:

                       (gdb) catch syscall 252
                       Catchpoint 1 (syscall(s) 'exit_group')
                       (gdb) r
                       Starting program: /tmp/catch-syscall
                       
                       Catchpoint 1 (call to syscall 'exit_group'), \
                       		   0xffffe424 in __kernel_vsyscall ()
                       (gdb) c
                       Continuing.
                       
                       Program exited normally.
                       (gdb)

        // skipped

    fork
        A call to fork. This is currently only available for HP-UX and gnu/Linux.

    vfork
        A call to vfork. This is currently only available for HP-UX and gnu/Linux.

    load [regexp]
    unload [regexp]
        The loading or unloading of a shared library. If regexp is given, then
        the catchpoint will stop only if the regular expression matches one of
        the affected libraries.

    signal [signal... | ‘all’]
        The delivery of a signal.

        With no arguments, this catchpoint will catch any signal that is not
        used internally by gdb, specifically, all signals except ‘SIGTRAP’ and
        ‘SIGINT’.

        With the argument ‘all’, all signals, including those used by gdb, will
        be caught. This argument cannot be used with other signal names.

        Otherwise, the arguments are a list of signal names as given to handle
        (see Signals). Only signals specified in this list will be caught.

        One reason that catch signal can be more useful than handle is that you
        can attach commands and conditions to the catchpoint.

        When a signal is caught by a catchpoint, the signal's stop and print
        settings, as specified by handle, are ignored. However, whether the
        signal is still delivered to the inferior depends on the pass setting;
        this can be changed in the catchpoint's commands. 


tcatch event
    Set a catchpoint that is enabled only for 'one' stop. The catchpoint is
    automatically deleted after the first time the event is caught. 

Use the info break command to list the current catchpoints. 


={============================================================================
*kt_linux_tool_310* gdb-break: unset

5.1.4 Deleting Breakpoints

It is often necessary to eliminate a breakpoint, watchpoint, or catchpoint once
it has done its job and you no longer want your program to stop there. This is
called deleting the breakpoint. A breakpoint that has been deleted no longer
exists; it is forgotten.

With the clear command you can delete breakpoints according to where they are in
your program. With the delete command you can delete individual breakpoints,
watchpoints, or catchpoints by specifying their breakpoint numbers.

clear
    Delete any breakpoints at the next instruction to be executed in the
    selected stack frame (see Selecting a Frame). When the innermost frame is
    selected, this is a good way to delete a breakpoint where your program just
    stopped.  clear location Delete any breakpoints set at the specified
    location. See Specify Location, for the various forms of location; the most
    useful ones are listed below:

    clear function
    clear filename:function
        Delete any breakpoints set at entry to the named function.
    clear linenum
    clear filename:linenum
        Delete any breakpoints set at or within the code of the specified linenum of the specified filename. 


delete [breakpoints] [range...]
    Delete the breakpoints, watchpoints, or catchpoints of the breakpoint ranges
    specified as arguments. If 'no' argument is specified, delete 'all'
    breakpoints (gdb asks confirmation, unless you have set confirm off). You
    can abbreviate this command as d. 


={============================================================================
*kt_linux_tool_310* gdb-break: disable

5.1.5 Disabling breakpoints

This makes the breakpoint inoperative as if it had been deleted, but remembers
the information on the breakpoint so that you can enable it again later. 

A breakpoint, watchpoint, or catchpoint can have any of 'four' different
'states' of enablement:


    Enabled. The breakpoint stops your program. A breakpoint set with the break
    command starts out in this state.

    Disabled. The breakpoint has no effect on your program.

    Enabled 'once'. The breakpoint stops your program, but then becomes disabled.

    Enabled for a 'count'. The breakpoint stops your program for the next N times,
then becomes disabled.

    Enabled for 'deletion'. The breakpoint stops your program, but immediately
    after it does so it is deleted permanently. A breakpoint set with the tbreak
    command starts out in this state. 


You can use the following commands to enable or disable breakpoints, watchpoints, and catchpoints:

disable [breakpoints] [range...]
    Disable the specified breakpoints—or all breakpoints, if none are listed. A
    disabled breakpoint has no effect but is not forgotten. All options such as
    ignore-counts, conditions and commands are remembered in case the breakpoint
    is enabled again later. You may abbreviate disable as dis.


enable [breakpoints] [range...]
    Enable the specified breakpoints (or all defined breakpoints). They become
    effective once again in stopping your program.

enable [breakpoints] once range...
    Enable the specified breakpoints temporarily. gdb disables any of these
    breakpoints immediately after stopping your program.

enable [breakpoints] count count range...
    Enable the specified breakpoints temporarily. gdb records count with each of
    the specified breakpoints, and decrements a breakpoint's count when it is
    hit. When any count reaches 0, gdb disables that breakpoint. If a breakpoint
    has an ignore count (see Break Conditions), that will be decremented to 0
    before count is affected.

enable [breakpoints] delete range...
    Enable the specified breakpoints to work once, then die. gdb deletes any of
    these breakpoints as soon as your program stops there. Breakpoints set by
    the tbreak command start out in this state. 

Except for a breakpoint set with tbreak, breakpoints that you set are initially
enabled; subsequently, they become disabled or enabled only when you use one of
the commands above. (The command until can set and delete a breakpoint of its
    own, but it does not change the state of your other breakpoints; see
    Continuing and Stepping.) 


={============================================================================
*kt_linux_tool_310* gdb-break: run command on break

5.1.7 Breakpoint Command Lists

You can give any breakpoint (or watchpoint or catchpoint) a series of commands
to 'execute' when your program stops due to that breakpoint. For example, you
might want to print the values of certain expressions, or enable other
breakpoints.

For example, here is how you could use breakpoint commands to print the value of
x at entry to foo whenever x is positive.

  break foo if x>0
  commands
  silent
  printf "x is %d\n",x
  cont
  end

One application for breakpoint commands is to compensate for one bug so you can
test for another. Put a breakpoint just after the erroneous line of code, give
it a condition to detect the case in which something erroneous has been done,
and give it commands to assign correct values to any variables that need them.

End with the continue command so that your program does not stop, and start with
the silent command so that no output is produced. Here is an example:

  break 403
  commands
  silent
  set x = y + 4           // note <gdb-set>
  cont
  end


={============================================================================
*kt_linux_tool_310* gdb-break: save

5.1.9 How to save breakpoints to a file

To save breakpoint definitions to a file use the save breakpoints command. 

save breakpoints [filename]



={============================================================================
*kt_linux_tool_310* gdb-dprintf

The dynamic printf command dprintf 'combines' a breakpoint with formatted
printing of your program's data to give you the effect of inserting printf calls
into your program on-the-fly, 'without' having to recompile it.

<to-direct-to-programs-output>
In its most basic form, the output goes to the GDB console. However, you can set
the variable dprintf-style for alternate handling. For instance, you can ask to
format the output by calling 'your' program's printf function. This has the
advantage that the characters go to the program's output device, so they can
recorded in redirects to files and so forth.

As an example, if you wanted dprintf output to go to a logfile that is a
standard I/O stream assigned to the variable mylog, you could do the following:

(gdb) set dprintf-style call
(gdb) set dprintf-function fprintf
(gdb) set dprintf-channel mylog
(gdb) dprintf 25,"at line 25, glob=%d\n",glob
Dprintf 1 at 0x123456: file main.c, line 25.
(gdb) info break
1 dprintf keep y 0x00123456 in main at main.c:25
call (void) fprintf (mylog,"at line 25, glob=%d\n",glob)
continue
(gdb)

note that the info break displays the dynamic printf commands as normal
breakpoint commands; you can thus easily see the effect of the variable
settings.

dprintf location,template,expression[,expression...]

Whenever execution reaches location, print the values of one or more expressions
under the control of the string template. To print several values, separate them
with commas.

set dprintf-style style

Set the dprintf output to be handled in one of several different styles
enumerated below. A change of style affects all existing dynamic printfs
immediately. (If you need individual control over the print commands, simply
    define normal breakpoints with explicitly-supplied command lists.) gdb
Handle the output using the gdb printf command.

'call' Handle the output by calling a function in your program (normally
    printf).  agent Have the remote debugging agent (such as gdbserver) handle
the output itself.  This style is only available for agents that support running
commands on the target.

set dprintf-function function

Set the function to call if the dprintf style is call. By default its value is
printf. You may set it to any expression. that gdb can evaluate to a function,
as per the call command.

set dprintf-channel channel

Set a channel for dprintf. If set to a non-empty value, gdb will evaluate it as
an expression and pass the result as a <first-argument> to the dprintf-function,
in the manner of fprintf and similar functions. Otherwise, the dprintf format
  string will be the first argument, in the manner of printf.


={============================================================================
*kt_linux_tool_311* gdb: examining running


{gdb-print}
(gdb) printf "%d\n", i
40
(gdb) printf "%08X\n", i
00000028

print i
Print the value of variable i.

print *p
Print the contents of memory pointed to by p, where p is a pointer variable.

(gdb) whatis pszSection
type = uint32_t *
(gdb) print *pszSection

print x.field
Check the different members of a structure.

print x
Check all the members of a structure, assuming x is a structure.

print y-field
y is a pointer to a structure.

print array[i]
Print the i'th element of array.

print array
Print all the elements of array.

(gdb) print /x block1->magic
$5 = 0xabeaa5b3

(gdb) print /x block1 
$9 = 0x1150f4c

(gdb) print /x *block1
$8 = {magic = 0xabeaa5b3, size = 0x28, line = 0x0, owner = 0x0, header = 0x21, data = {free = {previous = 0x8, next = 0x115106c}, 
    userData = {0x0}}}

# db_contexts_array is a global var
(gdb) p db_contexts_array
(gdb) p db_contexts_array[-1]


<gdb-disp>
to inspect over the course of the run

(gdb) [disp]lay var
(gdb) [undisp] disp_num
(gdb) info disp 	" to list disps



={============================================================================
*kt_linux_tool_400* gdb: remote debugging

<arguments>
20.3.1.4 Other Command-Line Arguments for gdbserver

--debug

Instruct gdbserver to display extra status information about the debugging process. This option is
intended for gdbserver development and for bug reports to the developers.

--remote-debug

Instruct gdbserver to display remote protocol debug output. This option is intended for gdbserver
development and for bug reports to the developers.

<symbols>
First make sure you have the necessary symbol files. Load symbols for your application using the
file command before you connect. Use set sysroot to locate target libraries (unless your GDB was
compiled with the correct sysroot using --with-sysroot). Q: <sysroot>?

The symbol file and target libraries must exactly match the executable and libraries on the target
with one exception: the files on the host system should not be stripped, even if the files on the
target system are. Mismatched or missing files will lead to confusing results during debugging. On
GNU/Linux targets, mismatched or missing files may also prevent gdbserver from debugging
multi-threaded programs. 


{gdb-client}
target remote host:port

monitor cmd

This command allows you to send arbitrary commands directly to the remote monitor. Since GDB doesn't
care about the commands it sends like this, this command is the way to extend GDBâyou can add new
commands that only the external monitor will understand and implement. 
	

20.3.3 Monitor Commands for gdbserver

During a GDB session using gdbserver, you can use the monitor command to send special requests to
gdbserver. Here are the available commands.

monitor help
List the available monitor commands.

monitor set debug 0
monitor set debug 1
Disable or enable general debugging messages.

monitor set remote-debug 0
monitor set remote-debug 1
Disable or enable specific debugging messages associated with the remote protocol (see Remote
Protocol).  

monitor set debug-format option1[,option2,...]

Specify additional text to add to debugging messages. Possible options are:

none Turn off all extra information in debugging output. 

all Turn on all extra information in debugging output. 

timestamps Include a timestamp in each line of debugging output. 

Options are processed in order. Thus, for example, if none appears last then no additional
information is added to debugging output. 

monitor exit

Tell gdbserver to exit immediately. This command should be followed by disconnect to close the
debugging session. gdbserver will detach from any attached processes and kill any processes it
created. Use monitor exit to terminate gdbserver at the end of a multi-process mode debug session.


={============================================================================
*kt_linux_tool_300* gdb: frontend tool: cgdb

It is how to change cgdb to use cross tool gdb.

http://www.programdevelop.com/4527764/

The GDB code CGDB calls in the path: /VARIOUS/util/src/fork_util.c by function invoke_debugger in

int invoke_debugger( 
            const char *path,  
            int argc, char *argv[],  
            int *in, int *out,  
            int choice, char *filename)  
{ 
    pid_t pid;     
    //GDBGDB?arm-linux-gdb 
    const char * const GDB               = "arm-linux-gdb"; 
}

./configure --prefix=/usr/local/ --program-suffix=arm-linux
make
sudo make install

kit@kit-vb:~/mheg-port$ ls /usr/local/bin/cgdb*
/usr/local/bin/cgdb  /usr/local/bin/cgdbarm-linux
kit@kit-vb:~/mheg-port$

http://cgdb.github.io/


{config}
/.cgdb/cgdbrc
==
:set winspilt=top_big
:set arrowstyle=long
:map <F6> :continue<CR>
:map <F7> :finish<CR>
:map <F8> :step<CR>
:map <F9> :next<CR>
==


={============================================================================
*kt_linux_tool_251* gdb-fend: eclipse tip

Debugging with Eclipse and gdb

I have recently tried to debug some elements with Eclipse. It's quite
straightforward. 

Steps to start debugging:

1. Right click on a project to be debugged and choose "Debug As" -> "Debug
Configurations...".

It should create new debug configuration under C/C++ Application branch named
same as project itself. If it is not created then it is possible to create it
with "New launch configuration" (button in upper left corner).

2. Select executable

In the Main tab it is required to provide a path to executable file which will
be launched ("Browse...") so need to know which executable we're going to debug
(e. g. some test).

3. Set environment

In most cases D-BUS demon is required. In Environment tab need to add variable
DBUS_SESSION_BUS_ADDRESS set to proper value (assuming that D-BUS daemon is
    started, e. g.
    unix:path=/tmp/dbus-7a0FdfCMQ8,guid=02115674e4f1464d822c1e700000230a).

4. Start debugging

Eclipse automatically stops on the first line in main() so there's no need to
worry about first breakpoint. Depending on settings in Eclipse it might ask for
switching to Debug perspective.

It is also possible to attach to the running process. In case of test
application running under Stagecraft some hack is required: need to hold
application, attach to it and resume it. It's a matter of experience for
choosing proper point of suspension. Using Lead project as an example I added
following line at the beginning of LeadAirPlugin::registerBindings() :

for ( bool bDbg = true; bDbg; ) {}

Rebuild:

make Lead.AIR.Client.API-clean
make Lead.AIR.Client.API

Run application:

$PREFIX/bin/runStagecraft2.sh --forceembeddedvectorfonts
/home/zinc/Zinc/source/Canvas/elements.trunk/Lead/Lead.ClientApiTests/as3/getTunedChannel/bin/YoureWatching.swf

Now need to attach to stagecraft application as follows:

1. Right click on particular project and choose  Debug As" -> "Debug
Configurations...".

In C/C++ Attach to Application branch create new configuration (name it whatever
    you want).

2. Select stagecraft executable: /opt/stagecraft-2.0/bin/stagecraft

3. Start debugging

New window with a list of processes will occur. Just start typing 'stagecraft'
and select proper process (it should be only one if you're not running any other
    Stagecraft applications) and choose "OK". In debug perspective take a look
into stack and find registerBindings() in one of threads (possibly the last
    one). Double-click it and the hack-line should appear in the source code
editor. In the upper right corner of Debug perspective there's a set of tabbed
windows. Select Variables then select 'bDbg' variable by clicking on its 'Value'
column and change it to false. Now you can step through the code.

This method is very simple but requires rebuilding project whenever holding
application is required or not. A better approach might be inserting this sort
of code somewhere:

for ( bool bDbg = getenv("GDB_HOLD_STAGECRAFT"); bDbg; ) {}  

This hack works also with optimized code ('bDbg' variable is set conditionally
    so compiler cannot optimize any code afterwards). It requires looking into
  assembly and finding out where the program execution should continue after the
  hack-loop. Example below is based on a simple few-liner application I've made
  for this purpose (test.cpp, I've used "KRK_DBG" environment variable):

#include <cstdlib>
#include <iostream>
 
int main() {
    for ( bool bDbg = getenv("KRK_DBG"); bDbg; ) {}
 
    std::cout << "haha" << std::endl;
    return 0;
}
 

And the session:

> g++ -O3 -o test test.cpp
> export KRK_DBG=1
> ./test &
[1] 1258

> gdb -p 1258
... some GDB introduction stuff ...

(gdb) set disassembly-flavor intel

(gdb) x/2i $pc
0x8048815 <main+245>:	lea    esi,[esi+0x0]
0x8048818 <main+248>:	jmp    0x8048815 <main+245>

(gdb) x/20i main
0x8048720 <main>:	lea    ecx,[esp+0x4]
0x8048724 <main+4>:	and    esp,0xfffffff0
0x8048727 <main+7>:	push   DWORD PTR [ecx-0x4]
0x804872a <main+10>:	push   ebp
0x804872b <main+11>:	mov    ebp,esp
0x804872d <main+13>:	push   edi
0x804872e <main+14>:	push   esi
0x804872f <main+15>:	push   ebx
0x8048730 <main+16>:	push   ecx
0x8048731 <main+17>:	sub    esp,0x118
0x8048737 <main+23>:	mov    DWORD PTR [esp],0x80488e4
0x804873e <main+30>:	call   0x80485a8 <getenv@plt>
0x8048743 <main+35>:	test   eax,eax
0x8048745 <main+37>:	jne    0x8048815 <main+245>
0x804874b <main+43>:	mov    DWORD PTR [esp+0x8],0x4
0x8048753 <main+51>:	mov    DWORD PTR [esp+0x4],0x80488ec
0x804875b <main+59>:	mov    DWORD PTR [esp],0x8049b20
0x8048762 <main+66>:	call   0x8048618 <_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_i@plt>
0x8048767 <main+71>:	mov    eax,ds:0x8049b20
0x804876c <main+76>:	mov    eax,DWORD PTR [eax-0xc]
(gdb) set $pc=0x804874b
(gdb) cont
Continuing.
haha

Program exited normally.
(gdb)


Stepping through chain of calls.

Assume this part of code:
class A {
   void someMethod() const {
      // (3)
 
   }
 
} ;
 
const A& getA() {
   static A a;
   return a;
} // (2)
 
void f() {
   getA().someMethod(); // (1)
   // (4)
 
}

Stepping in (F5) at point (1) will follow getA() body which might be stepped
over (F6) but it is important to step in (F5) at point (2) to get into
A::someMethod() at point (3). It's confusing but stepping over (F6) at point (2)
will bring the debugger into point (4).

Some observations:

    Keeping 'Variables' tab opened causes gdb to ignore breakpoints or crashes
    gdb session in some situations.

    Sometimes breakpoints are not re-installed (i. e. after restarting debug
        session).

    Stepping through chain of calls is not intuitive (see the example above).

All of these might be caused rather by MI bugs (gdb interface).

It is also possible to debug one or more processes simultaneously in Eclipse.
There is no problem to debug full conversation of Flash application (stagecraft
    -> Flash -> C++ bindings actually) with service by attaching to stagecraft
and particular daemon.

Obviously it is also possible to do it with pure GDB (command line) and requires
starting another instance of GDB what Eclipse as a matter of fact does.


# ============================================================================
#{
={============================================================================
|kt_linux_perf_001| perf-bootchart

http://www.bootchart.org/

Bootchart 2.0

I couldn't get bootchart2 to work, (no netlink taskstat interface).  However,
bootchart-0.9 works with only small tweaks. (See attached tarball.)


={============================================================================
|kt_linux_perf_001| perf-timeline

Using Frederico's Timeline Tools

These tools can be used to dig into the reasons for the time taken.

The tools used are strace and frederico's timeline tools from
http://people.gnome.org/~federico/news-2006-03.html#timeline-tools

Download requirements:

sudo yum install python-cairo
git clone git://gitorious.org/performance-scripts/mainline.git performance-scripts


# ============================================================================
#{
={============================================================================
*kt_linux_core_001* check shared libraries that process uses

cat /proc/NNNN/maps | awk '{print $6}' | grep '\.so' | sort | uniq

or

can use ldd.


={============================================================================
*kt_linux_core_002* stdin, stderr, and stdout

On program startup, the integer file descriptors associated with the streams stdin, stdout, and
stderr are 0, 1, and 2, respectively.


={============================================================================
*kt_linux_core_003* check libc version and path

Use ldd to find out the path and when run libc, it shows version.

keitee@debian-keitee:~/github/kb$ ldd ~/bin/vim | grep libc
   libc.so.6 => /lib/i386-linux-gnu/i686/cmov/libc.so.6 (0xb6915000)

$ /lib/i386-linux-gnu/i686/cmov/libc.so.6
GNU C Library (Debian EGLIBC 2.13-38+deb7u6) stable release version 2.13, by Roland McGrath et al.
Copyright (C) 2011 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.
There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE.
Compiled by GNU CC version 4.4.7.
Compiled on a Linux 3.2.60 system on 2014-10-16.
Available extensions:
	crypt add-on version 2.1 by Michael Glad and others
	GNU Libidn by Simon Josefsson
	Native POSIX Threads Library by Ulrich Drepper et al
	BIND-8.2.3-T5B
libc ABIs: UNIQUE IFUNC
For bug reporting instructions, please see:
<http://www.debian.org/Bugs/>.

Can use macro at compile time or use a call at runtime to check libc version:

#include <gnu/libc-version.h>
const char *gnu_get_libc_version(void);


={============================================================================
*kt_linux_core_004* errors and errno

The syscalls(2) manual page lists the Linux system calls.

<system-call-is-expensive>
As an example of the overhead of making a system call, consider the getppid() system call, which
simply returns the process ID of the parent of the calling process. On one of the author's x86-32
systems running Linux 2.6.25, 10 million calls to getppid() required approximately 2.2 seconds to
complete. This amounts to around 0.3 microseconds per call. By comparison, on the same system, 10
million calls to a C function that simply returns an integer required 0.11 seconds, or around
one-twentieth of the time required for calls to getppid(). Of course, most system calls have
significantly more overhead than getppid().

See LPI 3.1 for more.

<form>
Usually, an error is indicated by a return of -1. Thus, a system call can be checked with code such
as the following:

fd = open(pathname, flags, mode); /* system call to open a file */

if (fd == -1) {
   /* Code to handle the error */
}


{errno}
When a system call fails, it sets the 'global' integer variable errno to a positive value that
identifies the specific error. Including the <errno.h> header file provides a declaration of errno,
as well as a set of constants for the various error numbers. All of these symbolic names
commence with E.

man errno

EINVAL          Invalid argument (POSIX.1)

<when-to-check>
Successful system calls and library functions 'never' reset errno to 0, so this variable may have a
nonzero value as a consequence of an error from a previous call. Furthermore, SUSv3 permits a
successful function call to set errno to a nonzero value (although few functions do this).

Therefore, when checking for an error, we should always first check if the function return value
indicates an error, and only then examine errno to determine the cause of the error.

<few-exception>
A few system calls (e.g., getpriority()) can legitimately return -1 on success. To determine whether
an error occurs in such calls, we set errno to 0 before the call, and then check it afterward. If
the call returns -1 and errno is nonzero, an error occurred.

<categories>
For our purposes, library functions can be divided into the following categories:

1. Some library functions return error information in exactly the same way as system calls: a -1
return value, with errno indicating the specific error. 

An example of such a function is remove(), which removes a file (using the unlink() system call) or
a directory (using the rmdir() system call). Errors from these functions can be diagnosed in the
same way as errors from system calls.

2. Some library functions return a value 'other' than -1 on error, but nevertheless set errno to
indicate the specific error condition. For example, fopen() returns a NULL pointer on error, and the
setting of errno depends on which underlying system call failed. The perror() and strerror()
  functions can be used to diagnose these errors.

3. Other library functions don't use errno at all. The method for determining the existence and
cause of errors depends on the particular function and is documented in the function's manual page.
For these functions, it is a mistake to use errno, perror(), or strerror() to diagnose errors.

note: no 'uniform' way of error reporting when use system and library calls.

<print-errnos>
to print an error message based on the errno value. The perror() and strerror() library functions
are provided for this purpose.

#include <stdio.h>
void perror(const char *msg);

The perror() function prints the string pointed to by its msg argument, 'followed' by a message
corresponding to the current value of errno.

#include <string.h>
char *strerror(int errnum);


={============================================================================
*kt_linux_core_005* error handling codes from LPI

errMsg()
Prints a message on standard error. Its argument list is the same as for printf(), except that a
terminating newline character is automatically appended to the output string. The errMsg() function
prints the error text corresponding to the current value of errno-this consists of the error name,
such as EPERM, plus the error description as returned by strerror()-followed by the formatted output
  specified in the argument list.

errExit() 
Operates like errMsg(), but also terminates the program, either by calling exit() or, if the
environment variable EF_DUMPCORE is defined with a nonempty string value, by calling abort() to
produce a core dump file.

err_exit() 
is similar to errExit(), but differs in two respects:

@ It doesn't flush standard output before printing the error message.
@ It terminates the process by calling _exit() instead of exit(). This causes the process to
terminate without flushing stdio buffers or invoking exit handlers.

The details of these differences in the operation of err_exit() will become clearer where we
describe the differences between _exit() and exit(), and consider the treatment of stdio buffers and
exit handlers in a child created by fork(). 

For now, we simply note that err_exit() is especially 'useful' if we write a library function that
creates a child process that needs to terminate because of an error. This termination should occur
without flushing the child's copy of the parent's (i.e., the calling process's) stdio buffers and
without invoking exit handlers established by the parent.

<thread-errno>
errExitEN() 
is the same as errExit(), except that instead of printing the error text corresponding to the
current value of errno, it prints the text corresponding to the error number (thus, the EN suffix)
given in the argument errnum.

Mainly, we use errExitEN() in programs that employ the POSIX threads API. Since:

The traditional method of returning status from system calls and some library functions is to return
0 on success and -1 on error, with errno being set to indicate the error. The functions in the
pthreads API do things differently. All pthreads functions return 0 on success or a positive value
on failure. The failure value is one of the same values that can be placed in errno by traditional
UNIX system calls. 

We could diagnose errors from the POSIX threads functions using code such as the following:

errno = pthread_create(&thread, NULL, func, &arg);
if (errno != 0)
   errExit("pthread_create");

However, this approach is 'inefficient' because errno is defined in threaded programs as a macro that
expands into a 'function' call that returns a modifiable lvalue. 

errExitEN() function allows us to write a more efficient equivalent of the above code:

int s;
s = pthread_create(&thread, NULL, func, &arg);
if (s != 0)
   errExitEN(s, "pthread_create");

note: errExitEN do 'not' use errno which is a function call and user provides it instead. 

<lib/error_functions.h>

/*************************************************************************\
*                  Copyright (C) Michael Kerrisk, 2014.                   *
*                                                                         *
* This program is free software. You may use, modify, and redistribute it *
* under the terms of the GNU Lesser General Public License as published   *
* by the Free Software Foundation, either version 3 or (at your option)   *
* any later version. This program is distributed without any warranty.    *
* See the files COPYING.lgpl-v3 and COPYING.gpl-v3 for details.           *
\*************************************************************************/

/* Listing 3-2 */

/* error_functions.h

   Header file for error_functions.c.
*/
#ifndef ERROR_FUNCTIONS_H
#define ERROR_FUNCTIONS_H

/* Error diagnostic routines */

void errMsg(const char *format, ...);

#ifdef __GNUC__

    /* This macro stops 'gcc -Wall' complaining that "control reaches
       end of non-void function" if we use the following functions to
       terminate main() or some other non-void function. */

#define NORETURN __attribute__ ((__noreturn__))
#else
#define NORETURN
#endif

void errExit(const char *format, ...) NORETURN ;

void err_exit(const char *format, ...) NORETURN ;

void errExitEN(int errnum, const char *format, ...) NORETURN ;

void fatal(const char *format, ...) NORETURN ;

void usageErr(const char *format, ...) NORETURN ;

void cmdLineErr(const char *format, ...) NORETURN ;

#endif

<lib/error_functions.c>

/*************************************************************************\
*                  Copyright (C) Michael Kerrisk, 2014.                   *
*                                                                         *
* This program is free software. You may use, modify, and redistribute it *
* under the terms of the GNU Lesser General Public License as published   *
* by the Free Software Foundation, either version 3 or (at your option)   *
* any later version. This program is distributed without any warranty.    *
* See the files COPYING.lgpl-v3 and COPYING.gpl-v3 for details.           *
\*************************************************************************/

/* Listing 3-3 */

/* error_functions.c

   Some standard error handling routines used by various programs.
*/
#include <stdarg.h>
#include "error_functions.h"
#include "tlpi_hdr.h"
#include "ename.c.inc"          /* Defines ename and MAX_ENAME */

#ifdef __GNUC__                 /* Prevent 'gcc -Wall' complaining  */
__attribute__ ((__noreturn__))  /* if we call this function as last */
#endif                          /* statement in a non-void function */
static void
terminate(Boolean useExit3)
{
    char *s;

    /* Dump core if EF_DUMPCORE environment variable is defined and
       is a nonempty string; otherwise call exit(3) or _exit(2),
       depending on the value of 'useExit3'. */

    s = getenv("EF_DUMPCORE");

    if (s != NULL && *s != '\0')
        abort();
    else if (useExit3)
        exit(EXIT_FAILURE);
    else
        _exit(EXIT_FAILURE);
}

/* Diagnose 'errno' error by:

      * outputting a string containing the error name (if available
        in 'ename' array) corresponding to the value in 'err', along
        with the corresponding error message from strerror(), and

      * outputting the caller-supplied error message specified in
        'format' and 'ap'. */

static void
outputError(Boolean useErr, int err, Boolean flushStdout,
        const char *format, va_list ap)
{
#define BUF_SIZE 500
    char buf[BUF_SIZE], userMsg[BUF_SIZE], errText[BUF_SIZE];

    vsnprintf(userMsg, BUF_SIZE, format, ap);

    if (useErr)
        snprintf(errText, BUF_SIZE, " [%s %s]",
                (err > 0 && err <= MAX_ENAME) ?
                ename[err] : "?UNKNOWN?", strerror(err));
    else
        snprintf(errText, BUF_SIZE, ":");

    snprintf(buf, BUF_SIZE, "ERROR%s %s\n", errText, userMsg);

    if (flushStdout)
        fflush(stdout);       /* Flush any pending stdout */
    fputs(buf, stderr);
    fflush(stderr);           /* In case stderr is not line-buffered */
}

/* Display error message including 'errno' diagnostic, and
   return to caller */

void
errMsg(const char *format, ...)
{
    va_list argList;
    int savedErrno;

    savedErrno = errno;       /* In case we change it here */

    va_start(argList, format);
    outputError(TRUE, errno, TRUE, format, argList);
    va_end(argList);

    errno = savedErrno;
}

/* Display error message including 'errno' diagnostic, and
   terminate the process */

void
errExit(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errno, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Display error message including 'errno' diagnostic, and
   terminate the process by calling _exit().

   The relationship between this function and errExit() is analogous
   to that between _exit(2) and exit(3): unlike errExit(), this
   function does not flush stdout and calls _exit(2) to terminate the
   process (rather than exit(3), which would cause exit handlers to be
   invoked).

   These differences make this function especially useful in a library
   function that creates a child process that must then terminate
   because of an error: the child must terminate without flushing
   stdio buffers that were partially filled by the caller and without
   invoking exit handlers that were established by the caller. */

void
err_exit(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errno, FALSE, format, argList);
    va_end(argList);

    terminate(FALSE);
}

/* The following function does the same as errExit(), but expects
   the error number in 'errnum' */

void
errExitEN(int errnum, const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errnum, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Print an error message (without an 'errno' diagnostic) */

void
fatal(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(FALSE, 0, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Print a command usage error message and terminate the process */

void
usageErr(const char *format, ...)
{
    va_list argList;

    fflush(stdout);           /* Flush any pending stdout */

    fprintf(stderr, "Usage: ");
    va_start(argList, format);
    vfprintf(stderr, format, argList);
    va_end(argList);

    fflush(stderr);           /* In case stderr is not line-buffered */
    exit(EXIT_FAILURE);
}

/* Diagnose an error in command-line arguments and
   terminate the process */

void
cmdLineErr(const char *format, ...)
{
    va_list argList;

    fflush(stdout);           /* Flush any pending stdout */

    fprintf(stderr, "Command-line usage error: ");
    va_start(argList, format);
    vfprintf(stderr, format, argList);
    va_end(argList);

    fflush(stderr);           /* In case stderr is not line-buffered */
    exit(EXIT_FAILURE);
}


={============================================================================
*kt_linux_core_006* process credential

{real-and-effective}
The real user ID and group ID identify the user and group to which the process belongs. in the
/etc/passwd file

The effective user ID and group ID are used to 'determine' the permissions granted to a process when
it tries to perform various operations such as system calls. 

For example, these identifiers determine the permissions granted to a process when it accesses
resources such as files and System V interprocess communication (IPC) objects, which themselves have
associated user and group IDs determining to whom they belong. the effective user ID is also used by
the kernel to determine whether one process can send a signal to another.

A process whose effective user ID is 0 (the user ID of root) has all of the privileges of the
superuser. Such a process is referred to as a privileged process. Certain system calls can be
executed only by privileged processes.

<can-be-different>
Normally, the effective user and group IDs have the same values as the corresponding real IDs, but
there are two ways in which the effective IDs can assume different values. One way is through the
use of system calls that we discuss in Section 9.7. 

The second way is through the execution of set-user-ID and set-group-ID programs.


{set-uid-gid}
In addition, an executable file has two special permission bits: the set-user-ID and set-group-ID
bits. In fact, every file has these two permission bits, but it is their use with executable files
that interests us here.

# chmod u+s prog  # Turn on set-user-ID permission bit
# chmod g+s prog  # Turn on set-group-ID permission bit

# ls -l prog
-rwsr-sr-x 1 root root 302585 Jun 26 15:05 prog


{why}
When a set-user-ID program is run, the kernel 'sets' the effective user ID of the process to be the
same as the user ID of the executable 'file'. Running a set-group-ID program has an analogous effect
for the effective group ID of the process. Changing the effective user or group ID in this way gives
a process (in other words, the user executing the program) privileges it would not normally have.

For example, if an executable file is owned by root (superuser) and has the set-user-ID permission
bit enabled, then the process gains superuser privileges when that program is run.

<example>
Examples of commonly used set-user-ID programs on Linux include: passwd(1), which changes a userâs
password; mount(8) and umount(8), which mount and unmount file systems; and su(1), which allows a
user to run a shell under a different user ID.

$ ls -al /bin/mount
-rwsr-xr-x 1 root root 88744 Dec  9  2012 /bin/mount


={============================================================================
*kt_linux_core_007* /dev/null

/dev/null is a virtual device that always discards the data written to it. When we want to eliminate
the standard output or error of a shell command, we can redirect it to this file. Reads from this
device always return end-of-file.


={============================================================================
*kt_linux_core_008* score: file ownership and permissions

Each file has an associated user ID and group ID that define the owner of the
file and the group to which it belongs. The ownership of a file is used to
determine the access rights available to users of the file.

For the purpose of accessing a file, the system divides users into three
categories: the owner of the file, users, group, and the rest of the world
(other). Three permission bits may be set for each of these categories of user.

The read permission allows the contents of the file to be read; write permission
allows modification of the contents of the file; and execute permission allows
execution of the file, which is either a program or a script to be processed by
some interpreter.

<search-permission>
These permissions may also be set on directories, although their meanings are
slightly different: read permission allows the contents of (i.e., the filenames
    in) the directory to be listed; write permission allows the contents of the
directory to be changed (i.e., filenames can be added, removed, and changed);
and execute (sometimes called search) permission allows access to files within
  the directory (subject to the permissions on the files themselves).

note: when there is no excute permission on a directory, cannot 'cd' in. this
causes a failure when do build and turns out that some dirs don't have execute
on dirs for a group and make system cannot cd in.


<mode-letter>

From chmod:

The letters rwxXst select file mode bits for the affected users: read (r), write
(w), execute (or search for directories) (x), execute/search only if the file is
a directory or already has execute permission  for  some  user (X),  set  user
or group ID on execution (s), restricted deletion flag or sticky bit (t).
Instead of one or more of these letters, you can specify exactly one of the
letters ugo: the permissions granted to the user who owns the file (u), the
permissions granted to other users who are members of the file's group (g), and
the permissions granted to users that are in neither of the two preceding
categories (o).


drwxrws--T    2 root     keys          4096 Oct 29 10:19 ./
drwxr-xr-x    3 root     root          4096 Oct 29 10:19 ../
-rw-rw-r-T    1 root     keys         63514 Oct 29 10:19 db0.storage


<sticky-bit>
http://www.linuxnix.com/sticky-bit-set-linux/

What is Sticky Bit?

Sticky Bit is mainly used on folders in order to avoid deletion of a folder and
its content by other users though they having write permissions on the folder
contents. If Sticky bit is enabled on a folder, the folder contents are deleted
by only owner who created them and the root user. No one else can delete other
users data in this folder(Where sticky bit is set). This is a security measure
to avoid deletion of critical folders and their content(sub-folders and files),
   though other users have full permissions.

Q: I am seeing "T" ie Capital s in the file permissions, what's that?

After setting Sticky Bit to a file/folder, if you see 'T' in the file permission
area that indicates the file/folder does not have executable permissions for
'all' users on that particular file/folder.


# ============================================================================
#{ SIGNAL
={============================================================================
*kt_linux_core_050* signal

{signal-is-notification}
A signal is a notification to a process that an event has occurred. Sometimes described as software
interrupts and are analogous to hardware interrupts in that they interrupt the normal flow of
execution of a program.

{signal-and-kernel}
One process can (if it has suitable permissions) send a signal to another process. Can be employed
as a synchronization technique, or even as a primitive form of interprocess communication (IPC). It
is also possible for a process to send a signal to itself. However, the usual source of many signals
sent to a process is the kernel.

{signal-symbolic-names}
Each signal is defined as a unique (small) integer, starting sequentially from 1. These integers are
defined in <signal.h> with symbolic names of the form SIGxxxx. Since the actual numbers used for
each signal vary across implementations, it is these symbolic names that are always used in
programs. note: $kill -l to see signals.

{pending}
A signal is said to be generated by some event. Once generated, a signal is later delivered to a
process, which then takes some action in response to the signal. Between the time it is generated
and the time it is delivered, a signal is said to be pending. Why pending since do not know when it
is next scheduled to run, or immediately if the process is already running

{signal-mask-per-process} {signal-block}
Sometimes, however, we need to ensure that a segment of code is 'not' interrupted by the delivery of a
signal. To do this, we can add a signal to the process's signal mask-a set of signals whose delivery
is currently blocked. If a signal is generated while it is blocked, it remains pending until it is
later unblocked (removed from the signal mask).

If a process receives a signal that it is currently blocking, that signal is added to the processâs
set of pending signals.

That delivery of a signal is blocked during the execution of its handler (unless we specify the
SA_NODEFER flag to sigaction()). If the signal is (again) generated while the handler is
executing, then it is marked as pending and later delivered when the handler returns.

<signal-is-queued-or-not>
Standard signals can't be queued; delivered only once. The set of pending signals is only a mask; it
indicates whether or not a signal has occurred, but not how many times it has occurred. In other
words, if the same signal is generated multiple times while it is blocked, then it is recorded in
the set of pending signals, and later delivered, just once. One of the differences between standard
and realtime signals is that realtime signals are queued.

{signal-handler}
Instead of accepting the default for a particular signal, a program can change the action that
occurs when the signal is delivered. Can be used to ignore signals or to change the default. To
change a default is usually referred to as installing or establishing a signal handler. When a
signal handler is invoked in response to the delivery of a signal, we say that the signal has been
handled or, synonymously, caught.

<not-for-all-signals>
It isn't possible to set the disposition of a signal to terminate or dump core unless one of these
is the default disposition of the signal. The nearest we can get to this is to install a handler for
the signal that then calls either exit() or abort().  The abort() function generates a SIGABRT
signal for the process, which causes it to dump core and terminate.


{signal-reentrant} {async-signal-safe-function}
Because a signal handler may asynchronously interrupt the execution of a program at any point in
time, the main program and the signal handler in effect form two independent (although not
concurrent) threads of execution within the same process.

A function is said to be reentrant if it can safely be simultaneously executed by multiple threads
of execution in the same process. In this context, "safe" means that the function achieves its
expected result, regardless of the state of execution of any other thread of execution.

A function may be nonreentrant if it updates global or static data structures. A function that
employs only local variables is guaranteed to be reentrant because of race-condition. This book
shows an example using crypt() in both main and signal handler. This corrupts internal buffer which
is statically allocated and crypt uses when calls it with differnt parameter.

{Q} In short, it is reentrant if it do not cause race-condition. Even if it uses only local
variables, it can still cause race-condition. Doesn't it?

Such possibilities are in fact rife within the standard C library. For example, we already noted in
Section 7.1.3 that malloc() and free() maintain a linked list of freed memory blocks available for
reallocation from the heap. If a call to malloc() in the main program is interrupted by a signal
handler that also calls malloc(), then this linked list can be corrupted. For this reason, the
malloc() family of functions, and other library functions that use them, are nonreentrant.

Even if we are not using nonreentrant library functions, reentrancy issues can still be relevant. If
a signal handler updates programmer-defined global data structures that are also updated within the
main program, then we can say that the signal handler is nonreentrant with respect to the main
program.

If a function is nonreentrant, then its manual page will normally provide an explicit or implicit
indication of this fact. In particular, watch out for statements that the function uses or returns
information in statically allocated variables.

{async-signal-safe-function}
An async-signal-safe function is one that the implementation guarantees to be safe when called from
a signal handler. A function is async-signal-safe either because it is reentrant or because it is
not interruptible by a signal handler.

Table 21-1: Functions required to be async-signal-safe by POSIX.1-1990, SUSv2, and SUSv3

Real-world applications should avoid calling non-async-signal-safe functions from signal handlers.


{signal-and-proc}
The Linux-specific /proc/PID/status file contains various bit-mask fields that can be inspected to
determine a processâs treatment of signals. The bit masks are displayed as hexadecimal numbers, with
the least significant bit representing signal 1, the next bit to the left representing signal 2, and
so on. 

These fields are SigPnd (per-thread pending signals), ShdPnd (process-wide pending signals since
Linux 2.6), SigBlk (blocked signals), SigIgn (ignored signals), and SigCgt (caught signals).
(The difference between the SigPnd and ShdPnd fields will become clear when we describe the handling
 of signals in multithreaded processes in Section 33.2.) The same information can also be obtained
using various options to the ps(1) command.

note: signals for thread and process.


{standard-and-realtime-signal}
Signals fall into two broad categories; standard and realtime. On Linux, the standard signals are
numbered from 1 to 31. We describe the standard signals in this chapter. The other set of signals
consists of the realtime signals.

<why-reliable-signal>
In early implementations, signals could be lost (i.e., not delivered to the target process) in
certain circumstances. Furthermore, although facilities were provided to block delivery of signals
while critical code was executed, in some circumstances, blocking was not reliable. These problems
were remedied in 4.2BSD, which provided so-called reliable signals.

However, the Linux signal(7) manual page lists more than 31 signal names. The excess names can be
accounted for in a variety of ways. Some of the names are simply synonyms for other names, and are
defined for source compatibility with other UNIX implementations. Other names are defined but
unused.


{signals}
<SIGHUP> see process group for more.
When a terminal disconnect (hangup) occurs, this signal is sent to the 'controlling' process of the
terminal. A second use of SIGHUP is with daemons (e.g., init, httpd, and inetd). Many daemons are
designed to respond to the receipt of SIGHUP by reinitializing themselves and rereading their
configuration files. The system administrator triggers these actions by manually sending SIGHUP to
the daemon, either by using an explicit kill command or by executing a program or script that does
the same.

<SIGABRT>
The abort() function (Section 21.2.2) generates a SIGABRT signal for the process, which causes it to
dump core and terminate.

<SIGINT>
When the user types the terminal interrupt character (usually Control-C), the
terminal driver sends this signal to the foreground process group. The default
action for this signal is to terminate the process.

<SIGPIPE> broken-pipe
This signal is generated when a process tries to write to a pipe, a FIFO, or a socket for which
there is no corresponding reader process. This normally occurs because the reading process has
closed its file descriptor for the IPC channel. See Section 44.2 for further details.

<SIGSEGV>
This very popular signal is generated when a program makes an invalid memory
reference. A memory reference may be invalid because the referenced page doesn't
exist e.g., it lies in an unmapped area somewhere between the heap and the
stack, the process tried to update a location in read-only memory e.g., the
program text segment or a region of mapped memory marked read-only, or the
process tried to access a part of kernel memory while running in user mode
(Section 2.1). In C, these events often result from dereferencing a pointer
containing a bad address (e.g., an uninitialized pointer) or passing an invalid
argument in a function call. The name of this signal derives from the term
segmentation violation.

<SIGKILL>
This is the sure kill signal. It can't be blocked, ignored, or caught by a
handler, and thus always terminates a process.

<SIGTERM>
This is the standard signal used for terminating a process and is the 'default'
signal sent by the 'kill' and 'killall' commands. 

<difference-between-sigkill-and-sigterm>
Users sometimes explicitly send the SIGKILL signal to a process using kill -KILL or kill -9.
However, this is generally a mistake. A well-designed application will have a handler for SIGTERM
that causes the application to exit 'gracefully', cleaning up temporary files and releasing other
resources beforehand. Killing a process with SIGKILL bypasses the SIGTERM handler. Thus, we should
always first attempt to terminate a process using SIGTERM, and reserve SIGKILL as a last resort for
killing runaway processes that don't respond to SIGTERM. 

note: this means that if use sure kill, then can cause resource locked up and leaking.

note: what if there is no handler of SIGTERM? will be terminated by kernel anyway? when tried to
kill a simple application which do sleep and loop, it is terminated when use kill, SIGTERM.

<SIGURG>
This signal is sent to a process to indicate the presence of out-of-band (also known as urgent) data
on a socket

<SIGCHLD>
This signal is sent (by the kernel) to a parent process when one of its children terminates: either
by calling exit() or as a result of being killed by a signal. It may also be sent to a process when
one of its children is stopped or resumed by a signal. By default, SIGCHLD is ignored.

<SIGUSR1>
This signal and SIGUSR2 are available for programmer-defined purposes. The kernel never generates
these signals for a process. Processes may use these signals to notify one another of events or to
synchronize with each other. In early UNIX implementations, these were the only two signals that
could be freely used in applications. In fact, processes can send one another any signal, but this
has the potential for confusion if the kernel also generates one of the signals for a process.
Modern UNIX implementations provide a large set of realtime signals that are also available for
programmer-defined purposes (Section 22.8).


={============================================================================
*kt_linux_core_051* signal: example: use signal as synchronization

#include <signal.h>
#include "curr_time.h" /* Declaration of currTime() */
#include "tlpi_hdr.h"

#define SYNC_SIG SIGUSR1 /* Synchronization signal */

static void /* Signal handler - does nothing but return */
handler(int sig)
{
}

int
main(int argc, char *argv[])
{
  pid_t childPid;
  sigset_t blockMask, origMask, emptyMask;
  struct sigaction sa;

  setbuf(stdout, NULL); /* Disable buffering of stdout */

  // The sigemptyset() function initializes a signal set to contain no members. The sigfillset()
  // function initializes a set to contain all signals (including all realtime signals). After
  // initialization, individual signals can be added to a set using sigaddset() and removed using
  // sigdelset().

  sigemptyset(&blockMask);
  sigaddset(&blockMask, SYNC_SIG); /* Block signal */

  // We can use sigprocmask() to change the process signal mask, to retrieve the existing mask, or
  // both. The how argument determines the changes that sigprocmask() makes to the signal mask:

  if (sigprocmask(SIG_BLOCK, &blockMask, &origMask) == -1)
    errExit("sigprocmask");

  sigemptyset(&sa.sa_mask);
  sa.sa_flags = SA_RESTART;
  sa.sa_handler = handler;

  if (sigaction(SYNC_SIG, &sa, NULL) == -1)
    errExit("sigaction");

  switch (childPid = fork()) {
    case -1:
      errExit("fork");

    case 0: /* Child */

      /* Child does some required action here... */
      printf("[%s %ld] Child started - doing some work\n", currTime("%T"), (long) getpid());
      sleep(2); /* Simulate time spent doing some work */

      /* And then signals parent that it's done */
      printf("[%s %ld] Child about to signal parent\n", currTime("%T"), (long) getpid());

      if (kill(getppid(), SYNC_SIG) == -1)
        errExit("kill");

      /* Now child can do other things... */
      _exit(EXIT_SUCCESS);

    default: /* Parent */

      /* Parent may do some work here, and then waits for child to complete the required action */
      printf("[%s %ld] Parent about to wait for signal\n", currTime("%T"), (long) getpid());

      sigemptyset(&emptyMask);
      if (sigsuspend(&emptyMask) == -1 && errno != EINTR)
        errExit("sigsuspend");

      printf("[%s %ld] Parent got signal\n", currTime("%T"), (long) getpid());

      /* If required, return signal mask to its original state */
      if (sigprocmask(SIG_SETMASK, &origMask, NULL) == -1)
        errExit("sigprocmask");

      /* Parent carries on to do other things... */
      exit(EXIT_SUCCESS);
  }
}


={============================================================================
*kt_linux_core_052* signal: kill: checking for the existence of a process

Can send a signal to another process using the kill() system call, which is the analog of the kill
shell command.

#include <signal.h>

int kill(pid_t pid, int sig); Returns 0 on success, or -1 on error

If no process matches the specified pid, kill() fails and sets errno to ESRCH ("No such process"). A
process needs appropriate permissions to be able send a signal to another process.

If pid equals 0, the signal is sent to every process in the same process group as the calling
process, including the calling process itself. 

<signal-to-process-group>
If pid is less than -1, the signal is sent to all of the processes in the process group whose ID
equals the absolute value of pid. Sending a signal to all of the processes in a process group finds
particular use in shell job control.

<check-existence>
The kill() system call can serve another purpose. If the sig argument is specified as 0, the
so-called null signal, then no signal is sent. Instead, kill() merely performs error checking to see
if the process 'can' be signaled. 

Read another way, this means we can use the null signal to test if a process with a specific process
  ID exists. 

1. If sending a null signal fails with the error ESRCH, then we know the process doesn't exist. 
  
2. If the call fails with the error EPERM (meaning the process exists, but we don't have permission
    to send a signal to it) or 

3. succeeds (meaning we do have permission to send a signal to the process), then we know that the
process exists.

So this code do sleep while a process of pid is alive.

while (0 == kill(parent_pid, 0))
{
    sleep(1);
}


<but>
Verifying the existence of a particular process ID doesn't guarantee that a 'particular' program is
still running. Because the kernel recycles process IDs as processes are born and die, the same
process ID may, over time, refer to a different process.


={============================================================================
*kt_linux_core_100* process

LPI 6.

{process}
A process is an abstract entity, defined by the kernel, to which system resources are allocated in
order to execute a program.


{process-id}
PID is integer type and the Linux limits PIDs to 32767. Once it has reached 32,767, the process ID
counter is reset to 300, rather than 1. This is done because many low-numbered process IDs are in
permanent use by system processes and daemons, and thus time would be wasted searching for an unused
process ID in this range. 

In Linux 2.4 and earlier, the process ID limit of 32,767 is defined by the kernel constant PID_MAX.
With Linux 2.6, things change. While the default upper limit for process IDs remains 32,767, this
limit is adjustable via the value in the Linux-specific /proc/sys/kernel/pid_max file (which is one
greater than the maximum process ID). On 32-bit platforms, the maximum value for this file is
32,768, but on 64-bit platforms, it can be adjusted to any value up to 222 (approximately 4
million), making it possible to accommodate very large numbers of processes.


={============================================================================
*kt_linux_core_101* process creation

LPI 24.

{process-creation}
The wait(&status) system call has two purposes. First, if a child of this process has not yet
terminated by calling exit(), then wait() suspends execution of the process until one of its
children has terminated. Second, the termination status of the child is returned in the status
argument of wait().

<fork-and-exec>
The UNIX approach is usually simpler and more elegant. Separating these two steps makes the APIs
simpler and allows a program a great degree of flexibility in the actions it performs between the
two steps. Moreover, it is often useful to perform a fork() without a following exec().

Parent process running program A
      |
      A
      |
      child PID = fork(void);                   Child process running program A. 0 = fork(void);
      |                                               |
Parent may perform other atcions here                 A  
      |                                               | 
      wait(&status); // optional                Child may perform further actions here
                                                      |
                                                      execev( B, ... ); // optional
                                                      |
                                                      B
                                                      |
                                                Execution of program B
                                                      |
                                                      exit(status);
                                                      1. Child status passed to parent and kernel
                                                      restarts parent.
                                                      2. Delivers SIGCHLD optionally.

<pid-of-child>
The PID of parent remains the same but child is not. Then when child pid is assigned and what pid 0
check in the code is? 

The following idiom is sometimes employed when calling fork():

pid_t childPid; /* Used in parent after successful fork() to record PID of child */

switch (childPid = fork()) {
  case -1: /* fork() failed */
    /* Handle error */
  case 0: /* Child of successful fork() comes here */
    /* Perform actions specific to child */
  default: /* Parent comes here after successful fork() */
    /* Perform actions specific to parent */
}

note that child pid is assigned immediately after fork and has the same name in ps. So pid == 0
check is to distinguish execution path in the same text.

$ pid (26337): before fork:  
pid (26337): in parent before sleep:  
pid (26338): in child before sleep:  

$ ps   
  PID TTY          TIME CMD
26131 pts/10   00:00:00 bash
26337 pts/10   00:00:00 a.out
26338 pts/10   00:00:00 a.out
26339 pts/10   00:00:00 ps


<sharing-between-parent-and-child>
The child's stack, data, and heap segments are initially exact duplicates of the corresponding parts
the parent's memory. The child receives duplicates of all of the parent's file descriptors.

<race-condition-after-fork>
After a fork(), it is indeterminate which of the two is next scheduled to use CPU. If we need to
guarantee a particular order, we must use some kind of synchronization technique.

<case-use-signal>
Avoiding Race Conditions by Synchronizing with Signals. Although, in practice, such coordination is
more likely to be done using semaphores, file locks, or message passing. See *kt_linux_core_401* for
code example.


={============================================================================
*kt_linux_core_102* process: termination

LPI 25.

{two-ways}
A process may terminate in two general ways. One of these is abnormal termination, caused by the
delivery of a signal whose default action is to terminate the process (with or without a core dump).
Alternatively, a process can terminate normally, using the _exit() system call.

#include <unistd.h>
void _exit(int status);


{status-vaule}
By convention, a termination status of 0 indicates that a process completed successfully, and a
nonzero status value indicates that the process terminated unsuccessfully. There are no fixed rules
about how nonzero status values are to be interpreted; SUSv3 specifies two constants, EXIT_SUCCESS
(0) and EXIT_FAILURE (1)

Performing an explicit return n is generally equivalent to calling exit(n), since the run-time
function that invokes main() uses the return value from main() in a call to exit().

Performing a return without specifying a value, or falling off the end of the main() function, also
results in the caller of main() invoking exit(), but with results that vary depending on the version
of the C standard supported and the compilation options employed:


{exit-library-call}
Programs generally don't call _exit() directly, but instead call the exit() library function,
which performs various actions before calling _exit().

#include <stdlib.h>
void exit(int status);

The following actions are performed by exit():

1. Exit handlers (functions registered with atexit() and on_exit()) are called, in reverse order of
their registration (Section 25.3).

2. The stdio stream buffers are flushed.

3. The _exit() system call is invoked, using the value supplied in status.

Unlike _exit(), which is UNIX-specific, exit() is defined as part of the standard C library; that
is, it is available with every C implementation.


{actions-on-termination}
During both normal and abnormal termination of a process, the following actions
occur:

1. Open file descriptors, directory streams (Section 18.8), message catalog descriptors
(see the catopen(3) and catgets(3) manual pages), and conversion descriptors
(see the iconv_open(3) manual page) are closed.

2. As a consequence of closing file descriptors, any file locks (Chapter 55) held by
this process are released.

<3>. Any attached System V shared memory segments are detached, and the shm_nattch counter
corresponding to each segment is decremented by one.

4. For each System V semaphore for which a semadj value has been set by the process,
that semadj value is added to the semaphore value. (Refer to Section 47.8.)

<5>. If this is the controlling process for a controlling terminal, then the SIGHUP signal is sent to
each process in the controlling terminal's foreground process group, and the terminal is
disassociated from the session.

6. Any POSIX named semaphores that are open in the calling process are closed as though sem_close()
  were called.

7. Any POSIX message queues that are open in the calling process are closed as though mq_close()
  were called.

8. If, as a consequence of this process exiting, a process group becomes orphaned and there are any
stopped processes in that group, then all processes in the group are sent a SIGHUP signal followed
by a SIGCONT signal. We consider this point further in Section 34.7.4.

9. Any memory locks established by this process using mlock() or mlockall() (Section 50.2) are
removed.

10. Any memory mappings established by this process using mmap() are unmapped.


{exit-handler}


{stdio-buffers-and-exit}

#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

int main(int argc, char** argv)
{
  printf("Hello world.\n");
  write( STDOUT_FILENO, "Ciao\n", 5);

  // setbuf( stdout, NULL );

  if( -1 == fork() )
  {
    fprintf( stderr, "fork() failed\n" );
    exit(EXIT_FAILURE);
  }

  exit(EXIT_SUCCESS);
}

$ gcc sample-two.c 
$ ./a.out 
Hello world.
Ciao

$ ./a.out > a
$ cat a
Ciao
Hello world.
Hello world.

<problem> Why different output order and duplicates?

1. duplicates.
Recall that the stdio buffers are maintained in a process's user-space memory (refer to 13.2).
Therefore, these buffers are duplicated in the child by fork(). When standard output is directed to
a terminal, it is line-buffered by default, with the result that the newline-terminated string
written by printf() appears 'immediately'. 

However, when standard output is directed to a file, it is block-buffered by default. Thus the
string written by printf() is still in the parent's stdio buffer at the time of the fork(), and this
string is duplicated in the child. When the parent and the child later call exit(), they both flush
their copies of the stdio buffers, resulting in duplicate output.

To remedy this, can use setbuf() and fflush() to flush the stdio buffer prior to a fork() call.
However, still have different order.

$ ./a.out 
Hello world.
Ciao

$ ./a.out > a
$ cat a
Ciao
Hello world.

2. the output order.
The write() transfers data directly to a kernel buffer, and this buffer is not duplicated during a
fork(). note: write is a system call.

The output of write() appears before that from printf() because the output of write() is immediately
transferred to the kernel buffer cache, while the output from printf() is transferred only when the
stdio buffers are flushed by the call to exit().

note: In general, care is required when mixing stdio functions and system calls to perform I/O on
the same file, as described in Section 13.7.


={============================================================================
*kt_linux_core_103* process: monitor child process

{monitoring-child-process}
From #26 in ref-LPI. 
<wait-system-call>
The wait() system call waits for one of the children of the calling process to terminate and returns
the termination status of that child in the buffer pointed to by status.

#include <sys/wait.h>
pid_t wait(int *status);

If no (previously unwaited-for) child of the calling process has yet terminated, the call blocks
until one of the children terminates. If a child has already terminated by the time of the call,
wait() returns immediately.

As its function result, wait() returns the process ID of the child that has terminated.

On error, wait() returns -1. One possible error is that the calling process has no previously
unwaited-for children, which is indicated by the errno value ECHILD. This means that we can use the
following loop to wait for all children of the calling process to terminate:

while ((childPid = wait(NULL)) != -1)
  continue;
if (errno != ECHILD) /* An unexpected error... */
  errExit("wait");

<waitpid-system-call>
The wait() system call has a number of limitations, which waitpid() was designed to address. See the
reference for more details.

#include <sys/wait.h>
pid_t waitpid(pid_t pid, int *status, int options);


={============================================================================
*kt_linux_core_104* process: zombie

{zombie-process} orphan process, init process 
2014.02 from google phone interview. The lifetimes of parent and child processes are usually not the
same-either the parent outlives the child or vice versa. This raises two questions: 

1. Who becomes the parent of an orphaned child? Each process has a parent-the process that created
it. If a child process becomes orphaned because its "birth" parent terminates, then the child is
adopted by the 'init' process, and subsequent calls to getppid() in the child return 1. See Section
26.2. The process 1, init, the ancestor of all processes. The init process adopts the child and
automatically performs a wait(), thus removing the zombie process from the system.

2. What happens to a child that terminates before its parent has had a chance to perform a wait()?
The point here is that, although the child has finished its work, the parent should still be
permitted to perform a wait() at some later time to determine how the child terminated. The kernel
deals with this situation by 'turning' the child into a <zombie>. This means that most of the
resources held by the child are released back to the system to be reused by other processes. The
only part of the process that remains is an entry in the kernel's process table recording; among
other things the child's process ID, termination status, and resource usage statistics. Section
36.1.

note: after all, kernel make a process a zombie when not able to call wait() on it.

A zombie process can't be killed by a signal, not even the (silver bullet) SIGKILL. This ensures
that the parent can always eventually perform a wait(). When the parent does perform a wait(), the
kernel removes the zombie, since the last remaining information about the child is no longer
required.

<why-zombie-can-be-a-problem> note: second case when zombie is created
If a parent creates a child, but fails to perform a wait(), then an entry for the zombie child will
be maintained indefinitely in the kernel's process table. If a large number of such zombie children
are created, they will eventually fill the kernel process table, 'preventing' the creation of new
processes. Since the zombies can't be killed by a signal, the only way to remove them from the
system is to kill their parent (or wait for it to exit), at which time the zombies are adopted and
waited on by init, and consequently removed from the system.

$ ./make_zombie
Parent PID=1013
Child (PID=1014) exiting
  1013 pts/4 00:00:00 make_zombie Output from ps(1)
  1014 pts/4 00:00:00 make_zombie <defunct>
After sending SIGKILL to make_zombie (PID=1014):
  1013 pts/4 00:00:00 make_zombie Output from ps(1)
  1014 pts/4 00:00:00 make_zombie <defunct>

In the above output, we see that ps(1) displays the string <defunct> to indicate a process in the
zombie state.

A common way of reaping dead child processes is to establish a handler for the SIGCHLD signal.

<key> The zombie is created in two cases: child terminates early before parent calls wait and parent
fail to wait for child. The second case is a real problem. The problem is that kernel's process
table fills up and prevent creatring new processes. The zombies will be adopted by init process
eventually and can be removed by killing init process.


={============================================================================
*kt_linux_core_105* process: memory layout

LPI 6. This is a layout in virtual memory.

<process-virtual-memory-address> figure 6.1
The ** areas represent invalid ranges in the process's virtual address space; that is, areas for
which page tables have 'not' been created.

High           +----------------+
                  ** kernel. mapped into process virtual memory but not accessible to program.
               +----------------+                  0xC0000000
                  argv, environ
               +----------------+
                  stack (grows down)
top of stack   +----------------+
                  ** unallocated
               +----------------+
                  heap (grows up)
               +----------------+ < &end
                  bss
               +----------------+ < &edata
                  inited data
               +----------------+ < &etext
                  text
               +----------------+
                  **
Low            +----------------+                  0x00000000

note: what's the kernel in this digram?

/proc/kallsyms provides addresses of kernel symbols in this region. /proc/ksyms in kernel 2.4 and
earlier.

<text-segment> 
The text segment contains the machine-language instructions of the program run by the process. The
text segment is made 'read'-only so that a process doesn't accidentally modify its own instructions
via a bad pointer value. Since many processes may be running the same program, the text segment is
made 'sharable' so that a single copy of the program code can be mapped into the virtual address space
of all of the processes.

<initialized-data-segment>
The initialized data segment contains 'global' and 'static' variables that are explicitly
initialized. The values of these variables are read from the executable file when the program is
loaded into memory.

<uninitialized-data-segment> <bss>
The uninitialized data segment contains 'global' and 'static' variables that are 'not' explicitly
initialized. Before starting the program, the system initializes all memory in this segment to 0.
For historical reasons, this is often called the bss segment, a name derived from an old assembler
mnemonic for "block started by symbol." The main reason for placing global and static variables that
are initialized into a separate segment from those that are uninitialized is that, when a program is
stored on disk, it is not necessary to allocate space for the uninitialized data. Instead, the
executable merely needs to record the location and size required for the uninitialized data segment,
and this space is allocated by the program loader at 'runtime'.

<stack>
The stack is a dynamically growing and shrinking segment containing stack frames. One stack frame is
allocated for each currently called function. A frame stores the function's local variables
(so-called automatic variables), arguments, and return value. Stack frames are discussed in more
detail in Section 6.5.

<heap>
The heap is an area from which memory (for variables) can be dynamically allocated at run time. The
top end of the heap is called the program break.

<segment-and-section>
Sometimes, the term section is used instead of segment, since section is more consistent with the
terminology used in the now ubiquitous ELF specification for executable file formats.

<etext>
Three global symbols: etext, edata, and end. These symbols can be used from within a program to
obtain the addresses of the next byte past, respectively, the end of the program text, the end of
the initialized data segment, and the end of the uninitialized data segment.

<size-command>
The size(1) command displays the size of the text, initialized data, and uninitialized data (bss)
segments of a binary executable.


={============================================================================
*kt_linux_core_106* process: virtual memory

{virtual-memory} locality-of-reference
The aim of this virtual memory is to make efficient use of both the CPU and RAM (physical memory) by
exploiting a property that is typical of most programs: locality of reference. Most programs
demonstrate two kinds of locality:

1. 'spatial' locality is the tendency of a program to reference memory addresses that are 'near' those
that were recently accessed (because of 'sequential' processing of instructions, and, sometimes,
sequential processing of data structures).

2. 'temporal' locality is the tendency of a program to access the 'same' memory addresses in the near
future that it accessed in the recent past (because of loops).

The upshot of locality of reference is that it is possible to execute a program while maintaining
only 'part' of its address space in RAM.

<paging>
A virtual memory scheme splits the memory used by each program into small, fixed-size units called
pages. Correspondingly, RAM is divided into a series of page frames of the same size. At any one
time, only some of the pages of a program need to be resident in physical memory page frames; these
pages form the so-called resident set. Copies of the unused pages of a program are maintained in the
'swap' area-a reserved area of disk space used to supplement the computer's RAM-and loaded into
physical memory only as required. 

<page-fault>
When a process references a page that is 'not' currently 'resident' in physical memory, a page
'fault' occurs, at which point the kernel suspends execution of the process while the page is loaded
from disk into memory.

The kernel maintains a page table for 'each'-process (Figure 6-2). The page table describes the
location of each page in the process's virtual address space (the set of all virtual memory pages
available to the process). 

process virtual address space    page table        physical memory(RAM) page frames 
low   page 0                           4                    0
      page 1                           2                    1
      page 2                           7                    2
high  page 3                           0                    3

Each entry in the page table either 'indicates' the location of a virtual page in RAM or indicates
that it currently resides on disk.

Not all address ranges in the process's virtual address space require page-table entries. Typically,
large ranges of the potential virtual address space are unused, so that it isn't necessary to
maintain corresponding page-table entries. If a process tries to access an address for which
there is 'no' corresponding page-table entry, it receives a SIGSEGV signal.


{valid-virtual-addresses}
A process's range of 'valid' virtual addresses can change over its lifetime, as the kernel allocates
and deallocates pages (and page-table entries) for the process.

o as the stack grows downward beyond limits previously reached;

o when memory is allocated or deallocated on the heap, by raising the program break using brk(),
  sbrk(), or the malloc family of functions

o when System V shared memory regions are attached using shmat() and detached using shmdt()

o when memory mappings are created using mmap() and unmapped using munmap()


{advantages}
o Processes are isolated from one another and from the kernel, so that one process can't read or
modify the memory of another process or the kernel. This is accomplished by having the page-table
entries for each process point to distinct sets of physical pages in RAM (or in the swap area).

o Where appropriate, two or more processes can share memory. The kernel makes this possible by
having page-table entries in different processes refer to the same pages of RAM. Memory sharing
occurs in two common circumstances:

- Multiple processes executing the same program can share a single (readonly) copy of the program
code. This type of sharing is performed implicitly when multiple programs execute the same program
file (or load the same shared library).

- Processes can use the shmget() and mmap() system calls to explicitly request sharing of memory
regions with other processes. This is done for the purpose of interprocess communication.

o The implementation of memory protection schemes is facilitated; that is, pagetable entries can be
marked to indicate that the contents of the corresponding page are readable, writable, executable,
       or some combination of these protections. Where multiple processes share pages of RAM, it is
       possible to specify that each process has different protections on the memory; for example,
       one process might have read-only access to a page, while another has read-write access.  
       
o Programmers, and tools such as the compiler and linker, don't need to be concerned with the
physical layout of the program in RAM.  
       
o Because only a part of a program needs to reside in memory, the program loads and runs faster.
Furthermore, the memory footprint (i.e., virtual size) of a process can exceed the capacity of RAM.

o One final advantage of virtual memory management is that since each process uses less RAM, more
processes can simultaneously be held in RAM. This typically leads to better CPU utilization, since
it increases the likelihood that, at any moment in time, there is at least one process that the CPU
can execute.


{kernel-stack}
Sometimes, the term "user stack" is used to distinguish the stack we describe here from the "kernel
stack". The kernel stack is a "per-process" memory region maintained in kernel memory that is used
as the stack for execution of the functions called internally during the execution of a system call.
The kernel can't employ the user stack for this purpose since it resides in unprotected user memory.


{stack-frame}
Each (user) stack frame contains the following information:

o Function arguments and local variables: In C these are referred to as automatic variables, since
they are automatically created when a function is called. These variables also automatically
disappear when the function returns (since the stack frame disappears), and this forms the primary
semantic distinction between automatic and static (and global) variables: the latter have a
permanent existence independent of the execution of functions.

o Call linkage information: Each function uses certain CPU registers, such as the program counter,
  which points to the next machine-language instruction to be executed. Each time one function calls
  another, a copy of these registers is saved in the called function’s stack frame so that when the
  function returns, the appropriate register values can be restored for the calling function.

Referring to Listing 6-1, during the execution of the function square(), the stack will contain
frames as:

+----------------+
 frames for c run-time startup functions  note:
+----------------+
 frame for main
+----------------+
 frame for doCalc()
+----------------+
 frame for square() 
+----------------+      <- sp. grows down


={============================================================================
*kt_linux_core_107* process: environment list

{in-process}
As shown in Figure 6-1, the argv and environ arrays, as well as the 'strings' they initially point
to, 'reside' in a single contiguous area of memory just above the process stack.


{command-arguments}
The command-line arguments of any process can be read via the Linux-specific /proc/PID/cmdline file,
    with each argument being terminated by a null byte. A program can access its own command-line
    arguments via /proc/self/cmdline.


{name-value-pair}
Each process has an associated array of strings called the environment list, or simply the
environment. Each of these strings is a definition of the form name=value.


{copied}
When a new process is created, it inherits a copy of its parent's environment. This is a primitive
    but frequently used form of interprocess communication; the environment provides a way to
    transfer information from a parent process to its child(ren). Since the child gets a copy of its
    parent's environment at the time it is created, this transfer of information is one-way and
once-only. (since copied)


{shell-export-command}
A value can be added to the environment using the export command. The above commands permanently add
a value to the shell's environment, and this environment is then inherited by all child processes
that the shell creates. At any point, an environment variable can be removed with the unset command

$ export SHELL=/bin/bash

In the Bourne shell and its descendants, the following syntax can be used to add values to the
environment used to execute a single program, 'without' affecting the parent shell (and subsequent
        commands):

$ NAME=value program

This adds a definition to the environment of just the child process executing the named program. If
desired, multiple assignments (delimited by white space) can precede the program name.


{proc-pid-environ}
The environment list of any process can be examined via the Linux-specific /proc/PID/environ file,
    with each NAME=value pair being terminated by a null byte.


{get-set-env-call}
The getenv() function retrieves individual values from the 'process' environment.

#include <stdlib.h>

char *getenv(const char *name);

Returns pointer to (value) string, or NULL if no such variable

The setenv() function is an alternative to putenv() for adding a variable to the environment.

int setenv(const char *name, const char *value, int overwrite);
int unsetenv(const char *name);

Returns 0 on success, or -1 on error

int putenv(char *string);

Returns 0 on success, or nonzero on error

The string argument is a pointer to a string of the form name=value. After the putenv() call, this
string is part of the environment. In other words, rather than duplicating the string pointed to by
string, one of the elements of environ will be set to point to the same location as string.

The setenv() function 'creates' a new environment variable by allocating a memory buffer for a
string of the form name=value, and copying the strings pointed to by name and value into that
buffer. note: this is why setenv() is preferable to putenv() since putenv() do not allocate.

The setenv() function doesn't change the environment if the variable identified by name already
exists and overwrite has the value 0. If overwrite is nonzero, the environment is always changed.

The unsetenv() function removes the variable identified by name from the environment.
note: from man page, The unsetenv() function 'deletes' the variable name from the environment. If
name does not exist in the environment, then the function succeeds, and the environment is
unchanged.

On occasion, it is useful to erase the entire environment, and then rebuild it with selected values.
For example, we might do this in order to execute set-user-ID programs in a secure manner (Section
        38.8). We can erase the environment by assigning NULL to environ:

environ = NULL;

This is exactly the step performed by the clearenv() library function.

#define _BSD_SOURCE /* Or: #define _SVID_SOURCE */
#include <stdlib.h>

int clearenv(void)

Returns 0 on success, or a nonzero on error

<memory-leaks>
In some circumstances, the use of setenv() and clearenv() can lead to memory leaks in a program. We
noted above that setenv() allocates a memory buffer that is then made part of the environment. When
we call clearenv(), it doesn't free this buffer (it can't, since it doesn't know of the buffer's
        existence). A program that repeatedly employed these two functions would steadily leak
memory. 

In practice, this is unlikely to be a problem, because a program typically calls clearenv() just
once on startup, in order to remove all entries from the environment that it inherited from its
predecessor (i.e., the program that called exec() to start this program).


$ ./modify_env "GREET=Guten Tag" SHELL=/bin/bash BYE=Ciao
GREET=Guten Tag
SHELL=/bin/bash

$ ./modify_env SHELL=/bin/sh BYE=byebye
SHELL=/bin/sh
GREET=Hello world


#define _GNU_SOURCE /* To get various declarations from <stdlib.h> */
#include <stdlib.h>
#include "tlpi_hdr.h"

extern char **environ;

int main(int argc, char *argv[])
{
    int j;
    char **ep;

    // note: do not 'free' actually
    clearenv(); /* Erase entire environment */

    for (j = 1; j < argc; j++)
        if (putenv(argv[j]) != 0)
            errExit("putenv: %s", argv[j]);

    // note: when no GREET in arguments, this will create one but no call to free it?
    if (setenv("GREET", "Hello world", 0) == -1)
        errExit("setenv");

    // note: when BYE is from argv, how dose this free it if unsetenv() do delete it?
    unsetenv("BYE");

    for (ep = environ; *ep != NULL; ep++)
        puts(*ep);

    exit(EXIT_SUCCESS);
}


<inter-process-program>
Sometimes, it is useful for a process to modify its environment. One reason is to make a change that
is visible in all child processes subsequently created by that process.  Another possibility is that
we want to set a variable that is visible to a new program to be loaded into the memory of this
process (“execed”). In this sense, the environment is not just a form of interprocess communication,
        but also a method of interprogram communication.


={============================================================================
*kt_linux_core_107* process: group

LPI 34.

{why-group-and-session}
A process group is a collection of 'related' processes, and a session is a collection of 'related'
process groups. Process groups and sessions are abstractions defined to 'support' two uses:

1. shell job control, which allows interactive users to run commands in the foreground or in the
background. note: So need to know fine details when write a shell.

2. send a signal to a group.


{process-group} also known as job
A process group is a set of one or more processes sharing the same process group identifier (PGID).
A process group ID is a number of the same type (pid_t) as a process ID. A process group has a
process group 'leader', which is the process that 'creates' the group and whose process ID becomes
the process group ID of the group. A new process inherits its parent's process group ID.

note: the group leader has PID == PGID.

A process group has a lifetime, which is the period of time beginning when the leader creates the
group and ending when the last member process leaves the group.


{process-session}
A session is a collection of process groups. A process's session membership is determined by its
session identifier (SID), which, like the process group ID, is a number of type pid_t. A session
'leader' is the process that 'creates' a new session and whose process ID becomes the session ID. A
new process inherits its parent's session ID.

note: the session leader has PID == PGID == SID. That is group and session leader.


{controlling-process-and-terminal}
All of the processes in a session 'share' a single controlling terminal. The controlling terminal is
established when the 'session' leader first 'opens' a terminal device, /dev/tty. A terminal may be
the controlling terminal of at most 'one' session. Opening the controlling terminal also causes the
session leader to become the 'controlling' process for the terminal.

<forground-process-group>
At any point in time, one of the process groups in a session is the foreground process group for the
terminal, and the others are background process groups. 'only' processes in the foreground process
group can read input from the controlling terminal. note: process'es'

<signal-forground-group>
When the user types one of the signal-generating terminal characters on the controlling terminal, a
signal is sent to 'all' members of the 'foreground' process group. These characters are the
interrupt character (usually Control-C), which generates SIGINT; the quit character (usually
    Control-\), which generates SIGQUIT; and the suspend character (usually Control-Z), which
    generates SIGTSTP.


{use-other-case} 
Process groups occasionally find uses in areas other than job control, since they have two useful
properties: a parent process can wait on any of its children in a particular process group (Section
  26.1.2), and a signal can be sent to all of the members of a process group.

note: Use kill system call and see <signal-to-process-group> for detail.


{use-job-control} login-shell
Why session and group? Sessions and process groups were defined to support 'shell' job control

For an interactive login, the controlling terminal is the one on which the user logs in. The login
shell becomes the session leader and the controlling process for the terminal, and is also made the
'sole' member of its own process group. 

'each' job(a simple command or pipeline of commands) started from the shell results in the creation
of one or more processes, and the shell places all of these processes in a 'new' process group.

$ echo $$                              " Display the PID of the shell
400
$ find / 2> /dev/null | wc -l &        " Creates 2 processes in background group
[1] 659
$ sort < longlist | uniq -c            " Creates 2 processes in foreground group

<---------------------------- session 400 ----------------------------------->
bash (session/group leader)      find (group leader)        sort(group leader)
   PID  = 400                       PID  = 658                 PID  = 660
   PPID = 399                       PPID = <400>               PPID = <400>
   PGID = 400                       PGID = 658                 PGID = 660
   SID  = 400                       SID  = 400                 SID  = 400

                                 wc                         uniq
                                    PID  = 659                 PID  = 661
                                    PPID = <400>               PPID = <400>
                                    PGID = 658                 PGID = 660
                                    SID  = 400                 SID  = 400

<-------------------------->    <-------------------->    <------------------>
process group 400                process group 658          process group 660
'controlling' process
background process group         background group           forground group

<controlling-terminal>
Forground PGID    = 660
Controlling SID   = 400


{group}
<calls>
The setpgid() system call changes the process group of the process whose process ID is pid to the
value specified in pgid. Put simply, change pgid of a process with pid.

#include <unistd.h>
pid_t getpgrp(void);
Always successfully returns process group ID of calling process

int setpgid(pid_t pid, pid_t pgid);
Returns 0 on success, or -1 on error

If pid is specified as 0, the 'calling' process's process group ID is changed. If pgid is specified
as 0, then the process group ID of the process specified by pid is made the same as its process ID.
Thus, the following setpgid() calls are equivalent:

setpgid(0, 0);
setpgid(getpid(), 0);
setpgid(getpid(), getpid());

note: Put simply, this call is to change pgid of a process with the pid. However, when pid is 0, it
is for the calling process.

If the pid and pgid arguments specify the 'same' process, then a 'new' process group is created, and
the specified process is made the 'leader' of the new group. The typical callers of setpgid() and
setsid() are programs such as the shell 

If the two arguments specify different values, then setpgid() is being used to move a process
between process groups.

<group-creation>
The setpgid(0,0) is a way to create a new group and a group leader once a process is created such as
fork as shown example below.

However, from LPI 34-1 code:

childPid = fork();
switch (childPid) 
{
  case -1: /* fork() failed */
  /* Handle error */

  case 0: /* Child */
    if (setpgid(0, pipelinePgid) == -1)
    /* Handle error */
    /* Child carries on to exec the required program */

   default: /* Parent */
      if (setpgid(childPid, pipelinePgid) == -1 && errno != EACCES)
        /* Handle error */
        /* Parent carries on to do other things */
}

and 

Each job(a simple command or pipeline of commands) started from the shell results in the creation of
one or more processes, and the shell places all of these processes in a 'new' process group.

note: Q? 'not' clear when and how a group is created? The <example> seems to make more sense.

note: The above code is to show "How a job-control shell sets the process group ID of a child
process" because the scheduling of the parent and child is indeterminate after a fork(). 

Therefore, job-control shells are programmed so that the parent and the child process both call
setpgid() to change the childâs process group ID to the same value immediately after a fork(), and
the parent ignores any occurrence of the EACCES error on the setpgid() call.

So seems that the group creation is done before doing this.

<restrictions>
1. The pid argument may specify only the calling process or one of its children. Violation of this
rule results in the error ESRCH.

2. A process may 'not' change the process group ID of one of its children 'after' that child has
performed an exec(). Violation of this rule results in the error EACCES. The rationale for this
constraint is that it could confuse a program if its process group ID were changed after it had
commenced.

This restriction affects the programming of job-control shells, which have the following
requirements:

<sent-by-shell>
All of the processes in a job (i.e., a command or a pipeline) must be placed in a single process
group. This step permits the 'shell' to use killpg() (or, kill() with a negative pid argument) to
simultaneously send job-control signals to 'all' of the members of the process group. Naturally this
step must be carried out before any job-control signals are sent.

killpg - send signal to a process group

#include <signal.h>
int killpg(int pgrp, int sig);

<example>
The thing is that when a shell runs this line, creates a child process to run this application and
in this application, it make a new background group and make it itself a group leader.

note: In the first time, scripts runs "else" and set PPID with the pid that runs this script

#!/bin/bash

# to clean up A/V resources when a process dies, especially on a crash.
#
this_script=$0
prefix=@prefix@
parent_pid=${NEXUS_INSPECT_PARENT_PID:-}

if [ -n "${parent_pid}" ];
then
        echo "Going to watch pid: ${parent_pid}"
        while kill -0 "${parent_pid}" &>/dev/null;
        do
                usleep 500
        done
        echo "Pid ${parent_pid} has died. Going to cleanup."
        ${prefix}/bin/nexus-inspect -r -p "${parent_pid}"
else
        [ -x "${prefix}/bin/nexus-inspect" ] && \
            NEXUS_INSPECT_PARENT_PID=$$ \
               ${prefix}/bin/setpgid-and-exec bash "${this_script}" &
        exec "${@}"
fi


/*
 * The purpose of this simple program is to launch a program in a new process
 * group and have it become the group leader.
 *
 * This is useful in situations where the launched program fork()s and we want
 * to be sure that when we kill it, all of it's children are also killed.
 */

note: Q? does this mean that if kill group leader then all children will be killed automatically? 

#include <unistd.h>
#include <error.h>
#include <stdio.h>

int main(int argc, char* argv[]) {

  if (argc < 2) {
    fprintf(stderr, "Usage: %s <program> [ <arg> ... ]", argv[0]);
    exit(1);
  }

  /* Create new group and become group leader */
  if (setpgid(0, 0)) {
    perror("setpgid failed!");
    exit(2);
  }

  /* Execute the command */
  if (execvp(argv[1], (char*const*)(argv+1)) != 0) {
    perror("exec failed!");
    exit(3);
  }
}


{session}
<calls>
If the calling process is 'not' a process group leader, setsid() creates a 'new' session.

#include <unistd.h>
pid_t setsid(void);

Returns session ID of new session, or (pid_t) -1 on error

The setsid() system call creates a new session as follows:

1. The calling process becomes the leader of a new session, and is made the leader of a new process
group within that session. The calling processâs process group ID and session ID are set to the same
value as its process ID. note: this is a login shell example.

2. The calling process has 'no' controlling terminal. Any previously existing connection to a
controlling terminal is broken. note: Upon creation, a session has no controlling terminal

<restriction>
If the calling process is a process group leader, setsid() fails with the error EPERM. The simplest
way of ensuring that this doesn't happen is to perform a fork() and have the parent exit while the
child carries on to call setsid(). Since the child inherits its parent's process group ID and
receives its own unique process ID, it can't be a process group leader.

The restriction that group leader cannot call setsid() is necessary because, without it, the process
group leader would be able to place itself in another (new) session,

<example>
Shows the use of setsid() to create a new session. To check that it no longer has a controlling
terminal, this program attempts to open the special file /dev/ tty

$ ps -p $$ -o 'pid pgid sid command'      # $$ is PID of shell
PID PGID SID COMMAND
12243 12243 12243                         # bash PID, PGID, and SID of shell

$ ./t_setsid
$ PID=12352, PGID=12352, SID=12352
ERROR [ENXIO Device not configured] open /dev/tty

As can be seen from the output, the process successfully places itself in a new process group within
a new session. Has no controlling terminal, the open() call fails.

int
main(int argc, char *argv[])
{
  if (fork() != 0) /* Exit if parent, or on error */
    _exit(EXIT_SUCCESS);

  if (setsid() == -1)
    errExit("setsid");

  printf("PID=%ld, PGID=%ld, SID=%ld\n", (long) getpid(),
      (long) getpgrp(), (long) getsid(0));

  if (open("/dev/tty", O_RDWR) == -1)
    errExit("open /dev/tty");

  exit(EXIT_SUCCESS);
}

note: From this example, shell is not a group leader and this shows how a shell creates a new
session.


{SIGHUP-signal}
SIGHUP
When a terminal disconnect (hangup) occurs, this signal is sent to the controlling process of the
terminal. A second use of SIGHUP is with daemons (e.g., init, httpd, and inetd). Many daemons are
designed to respond to the receipt of SIGHUP by reinitializing themselves and rereading their
configuration files. The system administrator triggers these actions by manually sending SIGHUP to
the daemon, either by using an explicit kill command or by executing a program or script that does
the same.

The default action of SIGHUP is to terminate a process (by kernel). The delivery of SIGHUP to the
controlling process can set off a kind of chain reaction, resulting in the delivery of SIGHUP to
many other processes. 

This may occur in two ways:

First, in a login session, the shell is normally the controlling process for the terminal.  Most
shells are programmed so that, when run interactively, they establish a handler for SIGHUP. This
handler terminates the shell, but beforehand sends a SIGHUP signal to each of the process groups
(both foreground and background) created by the shell. 

note: sends a SIGHUP signal to only groups that the shell created. The below example shows that.

note: NOT understand this in 34.6.1 since if not handle SIGHUP then kernel will terminate all
anyway. So why install a hander and a shell send a signal? 

<because> Since only a foreground process can read input from the controlling terminal and receive
terminal generated signals. 

Second, if delivery of SIGHUP results in termination of a controlling process, then the kernel also
sends SIGHUP to all of the members of the 'foreground' process group of the controlling terminal.

note: SIGHUP is to terminate by default. So both terminates all children.

<example>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <signal.h>

static void
handler(int sig)
{
}

int main(int argc, char *argv[])
{
  pid_t childPid;
  struct sigaction sa;

  setbuf(stdout, NULL); /* Make stdout unbuffered */

  sigemptyset(&sa.sa_mask);
  sa.sa_flags = 0;
  sa.sa_handler = handler;

  if (sigaction(SIGHUP, &sa, NULL) == -1)
  {
    fprintf(stderr, "error: sigaction\n");
    exit(1);
  }

  childPid = fork();
  if (childPid == -1)
  {
    fprintf(stderr, "error: fork\n");
    exit(1);
  }

  if (childPid == 0 && argc > 1)
    if (setpgid(0, 0) == -1) /* Move to new process group */
    {
      fprintf(stderr, "error: setpgid\n");
      exit(1);
    }

  printf("PID=%ld; PPID=%ld; PGID=%ld; SID=%ld\n", (long) getpid(),
      (long) getppid(), (long) getpgrp(), (long) getsid(0));

  /* An unhandled SIGALRM ensures this process will die if nothing else terminates it */
  alarm(60);

  for(;;) {
    /* Wait for signals */
    pause();
    printf("%ld: caught SIGHUP\n", (long) getpid());
  }
}

The output on debian linux:

kpark@wll1p04345:~/work$ ps -o pgid $$
 PGID
 7523

kpark@wll1p04345:~/work$ echo $$
7523

kpark@wll1p04345:~/work$ ./a.out > samegroup.log 2>&1 &
[1] 16098
kpark@wll1p04345:~/work$ ./a.out x > diffgroup.log 2>&1  
[1]+  Alarm clock             ./a.out > samegroup.log 2>&1
Alarm clock

kpark@wll1p04345:~/work$ cat samegroup.log 
PID=16098; PPID=7523; PGID=16098; SID=7523         " shell, 7523 created a new process and group
PID=16099; PPID=16098; PGID=16098; SID=7523        " child

kpark@wll1p04345:~/work$ cat diffgroup.log         
PID=16231; PPID=7523; PGID=16231; SID=7523         " shell created a new process and group.
PID=16232; PPID=16231; PGID=16232; SID=7523        " child created a new group as well.

note: Unlike example output in 34.6.1, there are no output like:

$ cat samegroup.log
PID=5612; PPID=5611; PGID=5611; SID=5533  "child. This example shows that child runs first.
PID=5611; PPID=5533; PGID=5611; SID=5533  "parent
5611: caught SIGHUP
5612: caught SIGHUP

WHY NOT? The problem is the above application is not a shell and bash doc says:

The shell exits by default upon receipt of a SIGHUP. Before exiting, an interactive shell resends
the SIGHUP to all jobs, running or stopped. Stopped jobs are sent SIGCONT to ensure that they
receive the SIGHUP.

So when runs above commands, exit the shell by pressing C-D or close terminal. The output is:

kpark@wll1p04345:~/work$ cat samegroup.log 
PID=12104; PPID=7523; PGID=12104; SID=7523
PID=12105; PPID=12104; PGID=12104; SID=7523
12104: caught SIGHUP
12105: caught SIGHUP

kpark@wll1p04345:~/work$ cat diffgroup.log 
PID=15299; PPID=7523; PGID=15299; SID=7523
PID=15300; PPID=15299; PGID=15300; SID=7523
15299: caught SIGHUP
15299: caught SIGHUP


{session-leader-is-controlling-process}
As a consequence of establishing the connection to (i.e., opening) the controlling terminal, the
'session' 'leader' becomes the 'controlling' process for the terminal.

<disconnect>
The principal significance of being the controlling process is that the 'kernel' sends 'this'
process a SIGHUP signal if a terminal 'disconnect' occurs. note: say only SIGHUP.

<termination>
As for the termination of the controlling process, the following steps occur: The kernel sends a
SIGHUP signal (and a SIGCONT signal) to all members of the foreground process group, to inform them
of the loss of the controlling terminal.


{io-on-controlling-terminal}
The controlling terminal is inherited by the child of a fork() and preserved across an exec(). When
a session leader opens a controlling terminal, it becomes controlling process.

<terminal-driver>
To support job control, the terminal driver maintains a record of the foreground process group for
the controlling terminal. The terminal driver delivers job-control signals to the foreground job
when certain characters are typed. These signals either terminate or stop the foreground job.

<read>
The notion of the terminal's foreground job is also used to arbitrate terminal I/O requests. Only
processes in the foreground job may read from the controlling terminal. Background jobs are
prevented from reading by delivery of the SIGTTIN signal, whose default action is to stop the job. 

note: process'es'

SIGTTIN
When running under a job-control shell, the terminal driver sends this signal to a background
process 'group' when it attempts to read() from the terminal. This signal stops 'a' process by
default.

note: group but not process? when one of process in background group then get this signal.

<write>
If the terminal TOSTOP is set, then background jobs are also prevented from writing to the
controlling terminal by delivery of a SIGTTOU signal, whose default action is to stop the job.

SIGTTOU
This signal serves an analogous purpose to SIGTTIN, but for terminal output by background jobs. When
running under a job-control shell, if the TOSTOP (terminal output stop) option has been enabled for
the terminal (perhaps via the command stty tostop), the terminal driver sends SIGTTOU to a
background process group when it attempts to write() to the terminal (see Section 34.7.1).  This
signal stops a process by default.


={============================================================================
*kt_linux_core_108* process: daemon

LPI 37.

{characteritics}
A daemon is a process with the following characteristics:

1. It runs in the background and has 'no' controlling terminal. The lack of a controlling terminal
ensures that the kernel 'never' automatically generates any job-control or terminal-related signals
(such as SIGINT, SIGTSTP, and SIGHUP) for a daemon.

2. Many standard daemons run as privileged processes (i.e., effective user ID of 0),

3. It is a 'convention' (not universally observed) that daemons have names ending with the letter d.
On Linux, certain daemons are run as kernel threads. The code of such daemons is part of the kernel,
   and they are typically created during system startup. When listed using ps(1), the names of these
   daemons are surrounded by square brackets ([]). One example of a kernel thread is pdflush, which
   periodically flushes dirty pages (e.g., pages from the buffer cache) to disk.


{steps-to-create-daemon}

1. Perform a fork(), after which the parent exits and the child continues. As a consequence, the
daemon becomes a child of the init process. note: zombie.

This is done for two reasons:

@ Assuming the daemon was started from the command line, the parent's termination is noticed by the
shell, which then displays another shell prompt and leaves the child to 'continue' in the
background.

@ To guarantee not to be a process group leader, since it inherited its process group ID from its
parent and obtained its own unique process ID, which differs from the inherited process group ID.

2. The child process calls setsid() to start a new session and free itself of any association with a
controlling terminal.

note: From step 1, it is guaranteed not to be a group leader and this is necessary since only a
process which is not a group leader can create a new session and this has no controlling terminal.
See process: group: session.

3. If the daemon never opens any terminal devices thereafter, then we don't need to worry about the
daemon reacquiring a controlling terminal.

4.5. See 37.2

6. Close all open file descriptors that the daemon has inherited from its parent. A daemon may need
to keep certain inherited file descriptors open, so this step is optional, or open to variation.

This is done for a variety of reasons. Since the daemon has lost its controlling terminal and is
running in the background, it makes 'no' sense for the daemon to keep file descriptors 0, 1, and 2
open if these refer to the terminal. 

Furthermore, we can't 'unmount' any file systems on which the long-lived daemon holds files open.
And, as usual, we should close unused open file descriptors because file descriptors are a finite
resource.

7. After having closed file descriptors 0, 1, and 2, a daemon normally opens /dev/null and uses
dup2() (or similar) to make all those descriptors refer to this device.

note: this is why cannot see output when use sandbox?

This is done for two reasons:

@ It ensures that if the daemon calls library functions that perform I/O on these descriptors, those
functions won't unexpectedly fail. note: due to step 6.

@ It prevents the possibility that the daemon later opens a file using descriptor 1 or 2, which is
then written to-and thus corrupted-by a library function that expects to treat these descriptors as
standard output and standard error.

TODO: Once figure out the daemon implementation and compare it with listing 37.1.


{reinit-daemon}
The fact that many daemons should run continuously presents a couple of programming hurdles:

@ Typically, a daemon reads operational parameters from an associated configuration file on startup.
Sometimes, it is desirable to be able to change these parameters "on the fly," 'without' needing to
stop and restart the daemon.

@ Some daemons produce log files. If the daemon never closes the log file, then it may grow
endlessly, eventually clogging the file system. noted that even if we remove the last name of a
file, the file continues to exist as long as any process has it open. What we need is a way of
telling the daemon to close its log file and open a new file, so that we can rotate log files as
required.

The solution to both of these problems is to have the daemon establish a handler for SIGHUP, and
perform the required steps upon receipt of this signal.

Why is it possible? Since a daemon has no controlling terminal, the kernel never generates this
signal for a daemon. Therefore, daemons can use SIGHUP for the purpose described here.


{syslog}
note: no syslogd on a embedded system.

<why>
When writing a daemon, one problem we encounter is how to display error messages. Since a daemon
runs in the background, we can't display messages on an associated terminal, as we would typically
do with other programs. 

One possible alternative is to write messages to an application-specific log file, as is done in the
program in Listing 37-3. The main problem with this approach is that it is difficult for a system
administrator to manage multiple application log files and monitor them all for error messages. 

The syslog facility was devised to address this problem which provides a single, centralized logging
facility that can be used to log messages by 'all' applications on the system.

<daemon-and-conf>
The System Log daemon, syslogd, accepts log messages from two different sources: /dev/log, which
holds locally produced messages, and remote.

The syslogd daemon examines the facility and level of each message, and then passes it along to any
of several possible destinations according to the associated configuration file, /etc/syslog.conf.
Possible destinations include a terminal or virtual console, a disk file, etc.

<klogd>
An alternative source of the messages placed on /dev/log is the Kernel Log daemon, klogd, which
collects kernel log messages (produced by the kernel using its printk() function). These messages
are collected using either of two equivalent Linux-specific interfaces-the /proc/kmsg file and the
syslog(2) 'system' call-and then placed on /dev/log using the syslog(3) 'library' function.

note: Figure 37-1. printf uses syslog 2 and /proc/kmsg to log and klogd uses syslog 3 to log in to
/dev/log.

<calls>
#include <syslog.h>

void openlog(const char *ident, int log_options, int facility);
void syslog(int priority, const char *format, ...);

<facility-and-level>
The priority argument is created by ORing together a facility value and a level value. The facility
indicates the general category of the application logging the message, and is specified as one of
the values listed in Table 37-1. If omitted, the facility defaults to the value specified in a
previous openlog() call, or to LOG_USER if that call was omitted.

       LOG_AUTH       security/authorization messages
       LOG_AUTHPRIV   security/authorization messages (private)
       ...

The level value indicates the severity of the message, and is specified as one of the values in
Table 37-2. All of the level values listed in this table appear in SUSv3.

       LOG_EMERG      system is unusable
       LOG_ALERT      action must be taken immediately
       LOG_CRIT       critical conditions
       LOG_ERR        error conditions
       LOG_WARNING    warning conditions
       LOG_NOTICE     normal, but significant, condition
       LOG_INFO       informational message
       LOG_DEBUG      debug-level message

One difference from printf() is that the format string doesn't need to include a terminating newline
character. Also, the format string may include the 2-character sequence %m, which is replaced by the
error string correspond- ing to the current value of errno (i.e., the equivalent of
    strerror(errno)).


={============================================================================
*kt_linux_core_110* process: execution, exec call

LPI. 27

{execve-system-call}
Various library functions, all with names beginning with exec, are layered on
top of the execve() system call. Each of these functions provides a different
interface to the same functionality. The loading of a new program by any of
these calls is commonly referred to as an exec operation, or simply by the
notation exec().

#include <unistd.h>
int execve(const char *pathname, char *const argv[], char *const envp[]);

'never' returns on success; returns -1 on error

<attribute-that-remains>
After an execve(), the process ID of the process remains the same, because the
same process continues to exist.

<never-return>
Never need to check the return value from execve(); it will always be -1 if get
since the very fact that it returned informs us that an error occurred,

<ex>
note: See how envs are passed to.

$ ./t_execve ./envargs 
argv[0] = envargs 
argv[1] = hello world 
argv[2] = goodbye 
env: GREET=salut 
env: BYE=adieu 

// t_execve.c

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

int main(int argc, char** argv)
{
  char *argVec[10];
  char *envVec[] = { "GREET=salut", "BYE=adieu", NULL };

  if( argc != 2 || strcmp( argv[1], "--help" ) == 0 )
    printf("%s pathname\n", argv[0] );

  // get basename
  argVec[0] = strrchr( argv[1], '/' );
  if( argVec[0] != NULL )
    argVec[0]++;
  else
    argVec[0] = argv[1];

  // note: ternimate with NULL and argVec[] is passed as a whole to execed
  // application.

  argVec[1] = "hello world";
  argVec[2] = "goodbye";
  argVec[3] = NULL;

  execve( argv[1], argVec, envVec );

  printf("if we get here, someting wrong.\n" );
  exit(EXIT_FAILURE);
}

// envarg.c

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

extern char **environ;

int main(int argc, char** argv)
{
  int j;
  char **ep;

  for( j = 0; j < argc; j++ )
    printf("argv[%d] = %s \n", j, argv[j] );

  for( ep = environ; *ep != NULL; ep++ )
    printf("env: %s \n", *ep );

  exit(EXIT_SUCCESS);
}


{exec-library}
The library functions perfors an exec() and all are layered on top of execve().

#include <unistd.h>

int execle(const char *pathname, const char *arg, ...  
    /* , (char *) NULL, char *const envp[] */ );
int execlp(const char *filename, const char *arg, ...  
    /* , (char *) NULL */);
int execvp(const char *filename, char *const argv[]);
int execv(const char *pathname, char *const argv[]);
int execl(const char *pathname, const char *arg, ...  /* , (char *) NULL */);

None of the above returns on success; all return -1 on error

<path-filename>
execvp() allow the program to be specified using just a filename. The filename
is sought in the list of directories specified in the PATH environment variable.
The functions contain the letter p for PATH to indicate this.

The PATH variable is 'not' used if the filename contains a slash (/), in which
case it is treated as a relative or absolute pathname.

The PATH value for a login shell is set by system-wide and user-specific shell
startup scripts.  Since a child process inherits a copy of its parent's
environment variables, each process that the shell creates to execute a command
inherits a copy of the shell's PATH.

<env>
The execve() and execle() functions allow the programmer to 'explicitly' specify
the environment for the new program using envp, a NULL-terminated array of
pointers to character strings. The names of these functions end with the letter
e (for environment) to indicate this fact. All of the other exec() functions use
the caller's existing environment (i.e., the contents of environ) as the
environment for the new program.

<ex>
This is to two things: parent is to run actual application given and child is to
check if a parent is alive in a loop. If parent is died, do some action.

1. shell runs this line in a script. 
note: this cmd comes from outside to specify the application to run.

   exec $exec_wrapper "$cmd" args...
   
2. $exec_wrapper is an application, exec-then-cleanup-app, which do two things: 
   @ ppid from env is null and set ppid of shell. 
   @ call fork

   @ child sets env with ppid and 'exec' wrapper again. since env is set, child
   runs in a loop while checking if parent is died. If so, 'exec' cleanup
   application with ppid. 

   note: ppid comes from env and argv as well. argv's one is not used.

   @ parent 'exec' $cmd to run the given application such as a browser.

shell: exec(wrapper);

  // 'ppid' is null and set ppid with shell's pid
  wrapper: ppid = getpid();    

  wrapper: fork()
    -> child: setenv(PPID);
       child: exec(wrapper);

       // run 'wrapper' code again but with 'ppid' set this time before exec and
       // this is <inter-program> communication using envronment variable.

       // will have different pid from parent which comes from fork() but not
       // exec.

       wrapper: do watch
       wrapper: if parent is died, exec(cleanup);

    -> parent: exec(application);
       // has the same pid which is the shell's pid and run application code.


note: see how exec used to replace "program code(text)" to run. the shell,
  parent, replace itself with given application and the chain of exec in the
  child.

exec "$prefix/bin/exec-then-cleanup-app" "$prefix/bin/w3cEngine" \
$enable_webkit_remote_debugging_as_needed \
-cache "$app_data_dir/client-cache" \
-cache-size "$cacheSize" \
-jar "$app_data_dir/cookies.sqlite" \
-url "$url" \
-src "$sources"

// exec-then-cleanup-app

#include <unistd.h>
#include <signal.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <assert.h>
#include <sys/stat.h>
#include <sys/types.h>

void usage(const char* exec_name)
{
  fprintf(stderr, "Usage: %s cmd [args]\n\n"
                  "Run `cmd` passing it `args` and run inspect"
                  " when that completes\n", basename(exec_name));
}

int main(int argc, char** argv)
{
  pid_t pid;
  int opt = 0;

  // note: getenv returns NULL if an env is not set.

  char* parent_pid_str = getenv("NEXUS_INSPECT_PARENT_PID");
  char* cleanup_exe = PKG_BIN_DIR "/nexus-inspect";

  if (NULL == parent_pid_str && argc < 2)
  {
    usage(argv[0]);
    return 1;
  }

  if (NULL == parent_pid_str)
  {
    // Even if pids were unsigned 64-bit numbers, there could be a maximum
    // of 20 characters in it. We add another for the null terminator.
    char parent_pid[21];
    int ret = snprintf(parent_pid, sizeof(parent_pid), "%i", getpid());
    if (ret >= 20)
    {
      // If you're on a system with pids that are larger than unsigned
      // 64-bit integers, you should seriously consider why you're still
      // using nexus-inspect.
      fprintf(stderr, "Pids are longer than expected (ie %d characters)."
          " What's going on?", ret);
      return 2;
    }

    pid = fork();
    assert(!(pid < 0));
    if (0 == pid)
    {
      // Child.

      // Only launch the watcher if the cleanup app exists.
      struct stat sb;
      if (-1 == stat(cleanup_exe, &sb))
      {
        fprintf(stderr, "Cleanup executable (%s) doesn't exist, so not" 
            " spawning watcher\n", cleanup_exe);
        return 0;
      }

      // Create new group and become group leader.
      if (setpgid(0, 0)) {
        perror("setpgid failed!");
        return 3;
      }

      // "$prefix/bin/exec-then-cleanup-app" "-p" "$parent_pid"
      // args: /opt/zinc-trunk/bin/exec-then-cleanup-app -p 1479 

      // note: ppid via args is not used.

      const char* args[4] = { argv[0], "-p", parent_pid, NULL };

      // We set the pid in the environment. Doing it via environment rather than
      // arguments simplifies the code because we don't have to do any argument
      // parsing - note that we do accept arguments and options, but they are
      // *all* passed on to the exec'd process (see the parent branch, below).

      setenv("NEXUS_INSPECT_PARENT_PID", parent_pid, 1);
      execv(argv[0], (char*const*)args);
    }
    else
    {
      // Parent.
      if (execvp(argv[1], (char*const*)(argv+1)) != 0) {
        perror("exec failed!");
        return 4;;
      }
    }
  }
  else
  {
    // note: even if got ppid in number, have to use ppid string to pass over
    // cleanup_exe

    int parent_pid=atoi(parent_pid_str);
    fprintf(stderr, "Going to watch pid: %d\n", parent_pid);
    while (0 == kill(parent_pid, 0))
    {
      sleep(1);
    }
    fprintf(stderr, "Pid %d has died. Going to cleanup.\n", parent_pid);

    char* args[5] = { cleanup_exe, "-r", "-p", parent_pid_str, NULL };
    return execv(cleanup_exe, args);
  }

  return 0;
}


{pass-arguments}
To run "strace -e open /bin/ls" to see open system calls when ls runs, but was
not straightforward. Why?

./a.out /usr/bin/strace

#include <stdio.h>
#include <stdlib.h>

int main(int argc, char* argv[] )
{
    if( argc != 2 || strcmp( argv[1], "--help" ) == 0 )
    {
        printf("%s pathname\n", argv[0] );
        exit(EXIT_FAILURE);
    }

    // const char* args[] = {"ls", "-al", ".", NULL };
    // note: works fine

    // const char* args[] = {"strace", "-e open", "/bin/ls", NULL };
    // note: error when run "strace: invalid system call ` open'"

    const char* args[] = {"strace", "-e", "open", "/bin/ls", NULL };
    // note: works fine

    if( execvp( argv[1], args ) != 0 )
    {
        // if (execvp("/bin/strace", (char*const*)(args)) != 0) {
        perror("exec failed!");
        return 4;;
    }

    exit(EXIT_FAILURE);
}


={============================================================================
*kt_linux_core_150* thread vs process. LPI 29

A process is an instance of an executing program and a thread is an instance of an executing a task.
In other words, a process is processor abstract, an abstract entity defined by kernel and with
allocated resources in order to execute a program.  

UNIX programs have a single thread of execution: the CPU processes instructions for a single logical
flow of execution through the program. In a multithreaded program, there are multiple, independent,
concurrent logical flows of execution within the same process.

<key> In short, a process is an abstract(virtualisation) of a processor and a thread is an abstract
of single flow of execution.

<on-linux>
Two virtualisation in Linux: processer and memory.

Linux don't distinguish a process(task) with a thread. The thread is just a special process: 

1. Linux kernel scheduler schedules based on 'thread'.
2. Linux kernel has a double-linked list which has <thread_info> struct element which has
thread_struct*.
3. Thread is special since it shares resource (open files, pending signals, internal kernel data,
process state, address space, text and data section) with others threads.

In other words, process is a single thread that don't share resources.

This is clear when see how to create a thread:

clone( CLONE_VM| CLONE_FS| CLONE_FILES| CLONE_SIGHAND, 0);

to create a process:

clone(SIGCHLD, 0);

Two usual steps to create a process:

fork();  // copy a child from a parent and actually use clone() call
exec*()  // load a new program text.

{thread-vs-process-model}
PM or TM?
<PM>
1. It is difficult to share information between processes. Since the parent and child don't share
memory other than the read-only text segment, we must use IPC.

2. Process creation with fork() is relatively expensive although 'copy-on-write' is used. The need
to duplicate various process attributes such as page tables and file descriptor tables means that a
fork() call is still time-consuming.

<TM>
1. Sharing information between threads is easy and fast since it is just a matter of copying data
into shared (global or heap) variables. However, needs syncronization.

2. Thread creation is faster because many of the attributes that must be duplicated in a child
created by fork() are instead shared between threads such as page table.

<TM-disadvantages>
1. more efforts to ensure thread-safe
2. buggy thread can damage all of the threads in the process. less protection.
3. each thread is competing for use of the finite virtual address space of a host process.
4. desirable to avoid the use of signals in multi-threaded since requires careful designs.
5. should run the same program.
6. more threads means more memory and context switching.


{shared-and-not-shared-between-threads}
From CH29 in {ref-LPI} and {ref-UNP}

<shared-attributes>
The attributes that are shared; in other words, global attributes to a process:

the same global memory, process code and most data;
process ID and parent process ID;
process group ID and session ID;
controlling terminal
process credentials (user and group IDs);

open file descriptors;
signal dispositions;
file system-related information: umask, current working directory, and root directory;
resource limits;

<not-shared-attributes> thread specific
thread ID (Section 29.5);
signal mask;
thread-specific data (Section 31.3);
alternate signal stack (sigaltstack());
the errno variable;
realtime 'scheduling' policy and priority (Sections 35.2 and 35.3);
capabilities (Linux-specific, described in Chapter 39); and
'stack' (local variables and function call linkage information).
'registers' including PC and SP


{multithreaded-vs-singlethreaded}
The mutiltithreaded means that a single process has multi threads, 'lightweight'-process. The
singlethreaed means that a single process has a single thread. MT has less IPC but prone to error
because shares resources; less protection. ST has more IPC but more protection. 

note: tradeoff between IPC and protection.




{user-and-kernel-stack}
The kernel stack is a per-process memory region maintained in kernel memory that is used as the
stack for execution of the functions called internally during the execution of a system call.

Each (user) stack frame contains the following information:

1. Function arguments and local variables
2. Some registers. RA.

note: needs more about difference?


={============================================================================
*kt_linux_core_151* pthread and source

POSIX.1 threads approved in 1995 is a POSIX standard for threads. The standard defines an API for
creating and manipulating threads. The two main Linux threading implementations - LinuxThreads and
Native POSIX Threads Library (NPTL) - deviate from the standard.


{pthread-apis}
http://pubs.opengroup.org/onlinepubs/7990989799/xsh/pthread.h.html
http://pubs.opengroup.org/onlinepubs/7908799/xsh/pthread.h.html

Linux threading models compared: LinuxThreads and NPTL
http://www-128.ibm.com/developerworks/linux/library/l-threading.html?ca=dgr-lnxw07LinuxThreadsAndNPTL

From 2.6, NPTL(New Posix Threading Library) that supports futex(fast user space mutex) and is part
of glibc.


{source}
# from uclibc source

uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\pthread.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\rwlock.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\pthread.h
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\bits\pthreadtypes.h
(_pthread_rwlock_t)


={============================================================================
*kt_linux_core_152* pthread errno. LPI 29.2

{thread-errno}
The traditional method of returning status from system calls and some library functions is to return
0 on success and -1 on error, with errno being set to indicate the error. 

The functions in the pthreads API do things differently. 

<why>
The errno is a global integer variable. However, this doesn't suffice for threaded programs. If a
thread made a call that returned an error in a global errno variable, then this would confuse other
threads that might also be making calls and checking errno. In other words, race conditions would
result.

<how>
Each thread has its own errno value. On Linux, a thread-specific errno is achieved in a similar
manner to most other UNIX: errno is defined as a 'macro' that expands into a function call returning
a modifiable lvalue that is distinct for each thread. Since the lvalue is modifiable, it is still
possible to write assignment statements of the form errno = value in threaded programs.

To summarize, the errno mechanism has been adapted for threads in a manner that leaves error
reporting unchanged from the traditional UNIX API.

note: after all, errno macro returns a per-thread errno.
note: Nowadays, a program is required to declare errno by including <errno.h>, which enables the
implementation of a per-thread errno.

<value>
All pthreads functions return 0 on success or a positive value on 'failure'. The failure value is
one of the same values that can be placed in errno by traditional UNIX system calls. 


{LPI-approach}
errExitEN() 
is the same as errExit(), except that instead of printing the error text corresponding to the
current value of errno, it prints the text corresponding to the error number (thus, the EN suffix)
given in the argument errnum.

Mainly, we use errExitEN() in programs that employ the POSIX threads API. Since:

The traditional method of returning status from system calls and some library functions is to return
0 on success and -1 on error, with errno being set to indicate the error. The functions in the
pthreads API do things differently. All pthreads functions return 0 on success or a positive value
on failure. The failure value is one of the same values that can be placed in errno by traditional
UNIX system calls. 

We could diagnose errors from the POSIX threads functions using code such as the following:

errno = pthread_create(&thread, NULL, func, &arg);
if (errno != 0)
   errExit("pthread_create");

However, this approach is 'inefficient' because errno is defined in threaded programs as a macro that
expands into a 'function' call that returns a modifiable lvalue. 

errExitEN() function allows us to write a more efficient equivalent of the above code:

int s;
s = pthread_create(&thread, NULL, func, &arg);
if (s != 0)
   errExitEN(s, "pthread_create");

note: errExitEN do 'not' use errno which is a function call and user provides it instead. 

TODO: needs to clarify further. 
<one>
From {ref-UNP}. Why is this? Because pthread funcs do not set <errno-var> and return errno instead.
This means that set manually errno before calling err_sys() for example. This util funcs handles
this:

To avoid cluttering the code with braces, use {comma-operator} to combine assignment and the call:

int n;
if(( n = pthread_mutex_lock( &ndone_mutex )) != 0 )
   errno = n, err_sys("pthread_mutex_lock error");

Or

void Pthread_mutex_lock(pthread_mutex_t* mptr)
{
  int n;

  if(( n = pthread_mutex_lock( mptr )) == 0 )
    return;

  errno = n;
  err_sys("pthread_mutex_lock error");
} 

From Appdix C in {ref-UNP}. The reason for our own error funcs is to handle error case with a single
line. See {pthread-util-func} for the use.

if( error condition )
   err_sys( printf format with any number of args );

instead of

if( error condition ) {
  char buff[200];
  snprintf( buff, sizeof(buff), printf format with any number of args );
  perror(buff);
  exit(1);
}


={============================================================================
*kt_linux_core_153* pthread compile

On Linux, programs that use the Pthreads API must be compiled with the cc -pthread option. The
effects of this option include the following:

1. The _REENTRANT preprocessor macro is defined. This causes the declarations of a few reentrant
functions to be exposed.

2. The program is linked with the libpthread library (the equivalent of -lpthread).

gcc -pthread sample.c


={============================================================================
*kt_linux_core_154* pthread create and exit

<pthread-create>
#include <pthread.h>

// return 0 if okay, positive Exxx on error which is different from most system calls. 
//
// If need multiple arguments to the function, must package them into a structure and then pass the
// address of this as the single argument to the start function.
//
// The tid argument points to a buffer of type pthread_t into which the unique identifier for
// this thread is 'copied' before pthread_create() returns. This identifier can be used in later
// Pthreads calls to refer to the thread. The arg should be in global or heap.
//
// If attr is NULL, uses default values.
//
// EAGAIN : when cannot create a new thread because exceeded the limit on the number of threads

int pthread_create( pthread_t *tid, const pthread_attr_t *attr, void *(*func)(void*), void *arg);


<pthread-exit>
// The execution of a thread terminates in one of the following ways: 
//
// o The thread's start function performs a return specifying a return value for the thread. This is
// equivalent to pthread_exit()  
// 
// o The thread calls pthread_exit() which can be called in any func called by start func.
//
// o The thread is canceled using pthread_cancel()
//
// o note: Any of the threads calls exit(), or the main thread performs a return (in the main() function),
// which causes 'all' threads in the process to terminate immediately.
// 
// @retval
//
// Specifies a return value that can be obtained in 'another' thread by calling pthread_join(). The
// value pointed to by retval should 'not' be located on the thread's stack, since the contents of
// that stack become undefined on thread termination. The same statement applies to the value given
// to a return statement in the thread's start function.
//
// <continue-after-main-thread-exit>
// note: If the main thread calls pthread_exit() instead of calling exit() or performing a return,
// then the other threads continue to execute.

void pthread_exit(void *retval);


={============================================================================
*kt_linux_core_155* pthread join and zombie thread. LPI 29

<pthread-join> 
// return 0 if okay, positive Exxx on error. 
//
// To 'wait' for a given thread to terminate. If that thread has already terminated, pthread_join()
// returns 'immediately'.
//
// If retval is a non-NULL pointer, then it receives a copy of the terminated thread's return value
// - that is, the value that was specified when the thread performed a return or called
// pthread_exit().

int pthread_join( pthread_t thread, void **retval );

<join-restriction>
Calling pthread_join() for a thread ID that has been previously joined can lead to 'unpredictable'
behavior; for example, it might instead join with a thread created later that happened to reuse the
same thread ID.

<pthread-detach>
int pthread_detach(pthread_t tid);

Thread is either 'joinable'(the 'default') or 'detached'. 

To make the specified thread detached and often used to detach itself or can create detached thread
when create it by using attr setting.

pthread_detach( pthread_self() );

By default, a thread is joinable, meaning that when it terminates, another thread can obtain its
  return status using pthread_join(). Sometimes, we don't care about the thread's return status; we
  simply want the system to automatically clean up and remove the thread when it terminates. In this
  case, can mark the thread as detached, by making a call to pthread_detach().

Detaching a thread does 'not' make it 'immune' to a call to exit() in another thread or a return in
the main thread. In such an event, all threads in the process are immediately terminated,
    'regardless' of whether they are joinable or detached. 
    
To put things another way, pthread_detach() simply controls what happens after a thread terminates,
   not how or when it terminates.

note: After all, detaching means cannot use join() to get retrun value but not like zombie process
since still related to a process.


{zombie-thread}
If a thread is not detached, then we must join with it using pthread_join(). If not, then, when the
thread terminates, it produces the thread 'equivalent of a zombie process. Aside from wasting system
resources, if enough thread zombies accumulate, we won't be able to create additional threads.

{what-join-do}
The task that pthread_join() performs for threads is similar to that performed
by waitpid() for processes. However, there are some notable differences:

1. Threads are 'peers'. Any thread in a process can use pthread_join() to join with any other thread
in the process. For example, if thread A creates thread B, which creates thread C, then it is
possible for thread A to join with thread C, or vice versa. This differs from the 'hierarchical'
relationship between processes.

When a parent process creates a child using fork(), it is the only process that can wait() on that
child. There is no such relationship between the thread that calls pthread_create() and the
resulting new thread.

2. There is no way of saying "join with any thread" (for processes, we can do this using the call
    waitpid(-1, &status, options)); nor is there a way to do a nonblocking join (analogous to the
    waitpid() WNOHANG flag). There are ways to achieve similar functionality using condition
    variables; we show an example in Section 30.2.4.


={============================================================================
*kt_linux_core_156* pthread thread id. LPI 29

{TID}
Each thread within a process is uniquely identified by a thread ID. This ID is returned to the
caller of pthread_create(), and a thread can obtain its own ID using pthread_self().

int pthread_equeal( pthread_t tid, pthread_t tid );

Returns nonzero value if t1 and t2 are equal, otherwise 0

pthread_t pthread_self(void);

<why-equal-function-needed> opaque-data
All pthread data type should be treated 'opaque' data. On Linux, pthread_t happens to be defined as
an unsigned long, but on other implementations, it could be a pointer or a structure.

In NPTL, pthread_t is actually a pointer that has been cast to unsigned long.

Two problems:

1. Therefore, we can't 'portably' use code such as the following to display a thread ID. Though it
does work on many implementations, including Linux, and is sometimes useful for debugging purposes:

pthread_t thr;
printf("Thread ID = %ld\n", (long) thr);

note: Q. really work?

// return tid of calling thread. note: pthread_t is structure and need more to print out and also
// implementation dependant. 

2. In the Linux threading implementations, thread IDs are unique across processes. However, this is
'not' necessarily the case on other implementations


{posix-TID-and-kernel-TID}
POSIX thread IDs are 'not' the same as the thread IDs returned by the Linux specific gettid() system
call. POSIX thread IDs are assigned and maintained by the threading implementation. The thread ID
returned by gettid() is a number (similar to a process ID) that is assigned by the kernel. 

Although each POSIX thread has a unique kernel thread ID in the Linux NPTL implementation, an
application generally doesn't need to know about the kernel IDs and won't be portable if it depends
on knowing them.


={============================================================================
*kt_linux_core_157* pthread thread id. LPI 29

/*  status must not point to an object that is local to the calling thread.
 *
 *  terminate two other ways:
 *
 *  o thread function returns. return value is the exit status of the thread. {Q} kernal handle and
 *  manage it?
 *
 *  o main thread function returns or any thread call exit/_exit, the process terminates
 *  immediately.
 */
void pthread_exit(void *status);

// A thread may be canceled by any other thread in the same process. For example, if multiple
// threads are started to work on a given task (say finding a record in a database) adn the first
// thread completes the task then cancels the other threds.
//
// To handle the possibility of being canceled, can install(push) and remove(pop) cleanup handlers.
// These handlers are called:
// a) when the thread is canceled by pthread_cancel
// b) when the thread voluntarily terminates (either by calling pthread_exit or returning from its
// thread)

int   pthread_cancel(pthread_t);
void  pthread_cleanup_push(void*), void *);
void  pthread_cleanup_pop(int);


{pthread-attribute} 
To override the default and normally take the detault using the attr arg as a NULL. Attributes are a
way to specify behavior that is different from the default. When a thread is created with
pthread_create or when a synchronization variable is initialized, an attribute object can be
specified. However the default atributes are usually sufficient for most applications. 

Note: Attributes are specified [only] at thread creation time; they cannot be altered while the thread
is being used. {Q} really?

Thus three functions are usually called in tandem when setting attribute

o Thread attibute intialisation 
pthread_attr_init() create a default pthread_attr_t tattr. The function pthread_attr_init() is used
to initialize object attributes to their default values. The storage is allocated by the thread
system during execution. note: once the thread has been creted, the attribute object is no longer
needed, and so is destoryed.

o Thread attribute value change (unless defaults appropriate) 
A variety of pthread_attr_*() functions are available to set individual attribute values for the
pthread_attr_t tattr structure. (see below).  

o Thread creation 
A call to pthread_create() with approriate attribute values set in a pthread_attr_t structure. 
 
<code>

The following code fragment should make this point clearer: 

#include <pthread.h> 

pthread_attr_t tattr; // note: can be a local var
pthread_t tid;
void *start_routine;
void arg
int ret;

ret = pthread_attr_init(&tattr);
ret = pthread_attr_*(&tattr,SOME_ATRIBUTE_VALUE_PARAMETER);
ret = pthread_create(&tid, &tattr, start_routine, arg);
ret = pthread_attr_destroy(&tattr);

In order to save space, code examples mainly focus on the attribute setting functions and the
intializing and creation functions are ommitted. These must of course be present in all actual code
fragtments. 

An attribute object is opaque, and cannot be directly modified by assignments. A set of functions is
provided to initialize, configure, and destroy each object type. Once an attribute is initialized
and configured, it has process-wide scope. The suggested method for using attributes is to configure
all required state specifications at one time in the early stages of program execution. The
appropriate attribute object can then be referred to as needed. Using attribute objects has two
primary advantages: 

First, it adds to code portability. Even though supported attributes might vary between
implementations, you need not modify function calls that create thread entities because the
attribute object is hidden from the interface. If the target port supports attributes that are not
found in the current port, provision must be made to manage the new attributes. This is an easy
porting task though, because attribute objects need only be initialized once in a well-defined
location. 

Second, state specification in an application is simplified. As an example, consider that several
sets of threads might exist within a process, each providing a separate service, and each with its
own state requirements. At some point in the early stages of the application, a thread attribute
object can be initialized for each set. All future thread creations will then refer to the attribute
object initialized for that type of thread. The initialization phase is simple and localized, and
any future modifications can be made quickly and reliably. Attribute objects require attention at
process exit time. When the object is initialized, memory is allocated for it. This memory must be
returned to the system. The pthreads standard provides function calls to destroy attribute objects. 


{pthread-example-in-FOSH}
/* Assert throughout that the POSIX calls worked. If not, HFL cannot be guaranteed to work. */
resPOSIX = pthread_attr_init(&threadAttrs);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* The stacksize attribute defines the minimum stack size (in bytes) allocated for the created
 * threads stack.
 *
 * int pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize);
 */
resPOSIX = pthread_attr_setstack(&threadAttrs, ptThreadInfo->pvStack,(size_t)ptThreadInfo->szStack);
/* If this assert is triggered, it might be because of not aligning address to a boundary of 8. */
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * To set the other scheduling policy: 
 */
resPOSIX = pthread_attr_setschedpolicy(&threadAttrs, SCHED_RR);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * The example to change prio:
 *
 * sched_param param;
 * param.sched_priority = 30;
 * ret = pthread_attr_setschedparam (&tattr, &param);
 * 
 * used to set/inquire a current thread's priority of scheduling.
 * 
 * int pthread_attr_setschedparam(pthread_attr_t *attr, const struct sched_param *param);
 * int pthread_attr_getschedparam(const pthread_attr_t *attr, struct sched_param *param);
 *
 * {Q} {pthread-schedule-at-runtime} is it possible as attr is only at creation?
 */
resPOSIX = pthread_attr_setschedparam(&threadAttrs, &tSchedParam);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);

resPOSIX = pthread_attr_setscope(&threadAttrs, ContentionScope);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* The pthread_attr_setinheritsched() function sets the scheduling parameter inheritance state
 * attribute in the specified attribute object. The thread's scheduling parameter inheritance state
 * determines whether scheduling parameters are explicitly specified in this attribute object, or if
 * scheduling attributes should be inherited from the creating thread. Valid settings for
 * inheritsched include:
 * 
 * PTHREAD_INHERIT_SCHED Scheduling parameters for the newly created thread are the same as those of
 * the creating thread.
 * 
 * PTHREAD_EXPLICIT_SCHED Scheduling parameters for the newly created thread are specified in the
 * thread attribute object.
 */
resPOSIX = pthread_attr_setinheritsched(&threadAttrs, PTHREAD_EXPLICIT_SCHED);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);

resPOSIX = pthread_create(&(ptThreadInfo->idThread), &threadAttrs, pfThreadMain, pvParam);


{pthread-stack} from CH33 in {ref-LPI}.
Each thread has its own stack whose size is fixed when the thread is created. On Linux/x86-32, for
all threads other than the main thread, the default size of the per-thread stack is 2 MB. The main
thread has a much larger space for stack growth. Occasionally, it is useful to change the size of a
thread's stack. 

The pthread_attr_setstacksize() function sets a thread attribute (Section 29.8) that determines the
size of the stack in threads created using the thread attributes object. The related
pthread_attr_setstack() function can be used to control both the size and the location of the stack,
but setting the location of a stack can decrease application portability.

One reason to change the size of per-thread stacks is to allow for larger stacks for threads that
allocate large automatic variables or make nested function calls of great depth (perhaps because of
recursion).

Alternatively, an application may want to reduce the size of per-thread stacks to allow for a
greater number of threads within a process.

The minimum stack that can be employed on a particular architecture can be determined by calling
sysconf(_SC_THREAD_STACK_MIN). For the NPTL implementation on Linux/x86-32, this call returns the
value 16,384.


{nptl}
Linux threading models compared: LinuxThreads and NPTL
http://www-128.ibm.com/developerworks/linux/library/l-threading.html?ca=dgr-lnxw07LinuxThreadsAndNPTL

NPTL(new posix threading library) comes from kernel 2.6 and supports Futex(fast user space mutex).
It is part of glibc.

<how-to-check-nptl-version>
$ getconf GNU_LIBPTHREAD_VERSION
NPTL 2.15

<pid-nptl>
note: Q. In Linux, every thread has a PID and can see when use ps command but in NPTL, thread
group has one PID. is it true?


={============================================================================
*kt_linux_core_102* how to run three threads sequencially

From Cracking the coding interview, p425, 16.5:

Suppose we have following code:

public class Foo {
  public Foo() {...}
  public void first() {...}
  public void second() {...}
  public void third() {...}
}

The same instance of Foo will be passed to three different threads. ThreadA will call first, ThreadB
will call second, and ThreadC will call third. Design a mechanism to ensure that first is called
before second and second is called before third.

Using lock?

public class FooBad {
  public FooBad() {
    lock1 = new ReentrantLock();
    lock2 = new ReentrantLock();

    // locks all in a ctor
    lock1.lock(); lock2.lock(); 
  }
}

// already got lock1
public void first()
{
  ...
  lock1.unlock(); // finished first
}

// already got lock2
public void second()
{
  lock1.lock();   // wait first to finish
  lock1.unlock();
  ...
  lock2.unlock(); // finished second
}

public void third()
{
  lock2.lock();   // wait second to finish
  lock2.unlock();
  ...
}

This WON'T work in JAVA since a lock in JAVA is owned by the same thread which locked in.

http://docs.oracle.com/javase/6/docs/api/java/util/concurrent/locks/ReentrantLock.html
A ReentrantLock is owned by the thread last successfully locking, but not yet unlocking it. A thread
invoking lock will return, successfully acquiring the lock, when the lock is not owned by another
thread. The method will return immediately if the current thread already owns the lock. 

note: There's no such limitation in Linux and it's possible in the same thread group and if threads
in different group use lock, can use semaphore or lock on the shared memory.

The mutex has ownership as well and see {mutex-ownership}.


={============================================================================
*kt_linux_core_103* priority and schedule 

{priority-on-linux}

high           low
0     99 100   139

0  -99  : realtime priority. static and can set when create a process
100-139 : user priority. can use nice command which uses with value from -20 to +19. The default
nice value is 0. note: does it mean 120 is default? 100 is highest?


{realtime-and-latency}
The aim to distribute fairly CPU resource to all process on a system is not realtime approach.

<latency-components>

interrupt      ISR         ISR signals       user process
event          runs        user process      runs
 |             |              |                 |
---------------------------------------------------------> time
   interrupt      interrupt      scheduling
   latency        processing     latency

<no-hard-realtime>
No support in kernel. To use hard realtime, install patch from 
http://people.redhat.com/~mingo/realtime-preempt


{schedule}
Before Linux 2, kernel didn't support preemption which means that no other process can run in kernel
mode when one user process is already in the kernel mode until that is bloked or finishes its work.

<scheduler>
O(1) scheduler from Linux 2.5 and supports constant scheduling decision regardless of the number of
process.

<schedule-policy>
(from ~/include/linux/sched.h)
/*
 * Scheduling policies
 */

// normal user process. fairness
#define SCHED_NORMAL 0     

// realtime and run the highest priority. On the same priority, the first runs until it's blocked.
// So it is realtime without time slice.
#define SCHED_FIFO   1     

// realtime and run the highest priority. On the same priority, the first runs but in the time
// sliced. So it is realtime with time slice.
#define SCHED_RR     2

<default-policy>
SCHED_NORMAL(OTHER) is default.

To change the policy after a boot:
For example, if scheduling is modified before insmod-ing callisto BCM drivers, tasks inherit changed
scheduling. In that case last two task mods can be dropped. The list is quite aggressive as changing
policy of kthread affects all new kernel threads. Fine tuning would obviously have to be done along
with BCM.

(about ways to change from NORMAL to RR)
http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0182.html
http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0330.html


<code>
/* from sched.h.
#define MAX_USER_RT_PRIO   100
#define MAX_RT_PRIO        MAX_USER_RT_PRIO
*/

#define MY_RT_PRIORITY MAX_USER_RT_PRIO /* Highest possible */

int main(int argc, char **argv)
{
  ...
  int rc, old_scheduler_policy;
  struct sched_param my_params;
  ...

  /* Passing zero specifies caller's (our) policy */
  old_scheduler_policy = sched_getscheduler(0);
  my_params.sched_priority = MY_RT_PRIORITY;

  /* Passing zero specifies callers (our) pid */
  rc = sched_setscheduler(0, SCHED_RR, &my_params);
  if ( rc == -1 )
    handle_error();
  ...
}

(sched.c)
/**
 * sys_sched_get_priority_max - return maximum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the maximum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_max(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = MAX_USER_RT_PRIO-1;
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
                break;
        }
        return ret;
}

/**
 * sys_sched_get_priority_min - return minimum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the minimum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_min(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = 1;
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
        }
        return ret;
}


{pthread-schedule}
The POSIX draft standard specifies scheduling policy attributes of SCHED_FIFO (first-in-first-out),
SCHED_RR (round-robin), or SCHED_OTHER (an implementation-defined method). SCHED_FIFO and
SCHED_RR are optional in POSIX, and only are supported for real time bound threads


={============================================================================
*kt_linux_core_200* file io

{descriptors} LPI 4
All system calls for performing I/O refer to open files using a file descriptor, a (usually small)
    nonnegative integer. File descriptors are used to refer to 'all' types of open files, including
    pipes, FIFOs, sockets, terminals, devices, and regular files. 
    

<universality-of-io>
One of the distinguishing features of the UNIX I/O model is the concept of universality of I/O. This
means that the same four system calls - open(), read(), write(), and close() - are used to perform
I/O on all types of files, including devices such as terminals. 

$ ./copy test test.old           Copy a regular file
$ ./copy a.txt /dev/tty          Copy a regular file to this terminal
$ ./copy /dev/tty b.txt          Copy input from this terminal to a regular file
$ ./copy /dev/pts/16 /dev/tty    Copy input from another terminal

Universality of I/O is achieved by ensuring that each file system and device driver implements the
'same' set of I/O system calls. Because details specific to the file system or device are handled
within the kernel, we can generally ignore device-specific factors when writing application
programs. When access to specific features of a file system or device is required, a program can use
the catchall ioctl() system call (Section 4.8), which provides an interface to features that fall
outside the universal I/O model.


{descriptors-and-process}
Each process has its own set of file descriptors.


{four-key-calls}
o fd = open(pathname, flags, mode) 

opens the file identified by pathname, returning a file descriptor used to refer to the open file in
subsequent calls. If the file doesn't exist, open() may create it, depending on the settings of the
flags bit-mask argument. The flags argument also specifies whether the file is to be opened for
reading, writing, or both. The mode argument specifies the permissions to be placed on the file if
it is created by this call. If the open() call is not being used to create a file, this argument is
ignored and can be omitted.

o numread = read(fd, buffer, count) 

reads at 'most' count bytes from the open file referred to by fd and stores them in buffer. The
read() call returns the number of bytes 'actually' read. If no further bytes could be read (i.e.,
        end-of-file was encountered), read() returns 0.  

o numwritten = write(fd, buffer, count)

writes up to count bytes from buffer to the open file referred to by fd. The write() call returns
the number of bytes actually written, which may be less than count.  

o status = close(fd)

is called after all I/O has been completed, in order to release the file descriptor fd and its
associated kernel resources.

<code>
/* Transfer data until we encounter end of input or an error */
while ((numRead = read(inputFd, buf, BUF_SIZE)) > 0)
   if (write(outputFd, buf, numRead) != numRead)
      fatal("couldn't write whole buffer");

if (numRead == -1)
   errExit("read");


{open-call}
#include <sys/stat.h>
#include <fcntl.h>

int open(const char *pathname, int flags, ... /* mode_t mode */);

Returns file descriptor on success, or –1 on error

/* Open existing file for reading */

fd = open("startup", O_RDONLY);
if (fd == -1)
errExit("open");

/* Open new or existing file for reading and writing, truncating to zero bytes; file permissions
 * read+write for owner, nothing for all others */

fd = open("myfile", O_RDWR | O_CREAT | O_TRUNC, S_IRUSR | S_IWUSR);
if (fd == -1)
errExit("open");

/* Open new or existing file for writing; writes should always append to end of file */

fd = open("w.log", O_WRONLY | O_CREAT | O_TRUNC | O_APPEND, S_IRUSR | S_IWUSR);


<open-flags>
Table 4-3 are divided into the following groups:

o File access mode flags 
These are the O_RDONLY, O_WRONLY, and O_RDWR flags described earlier. They can be retrieved using
the fcntl() F_GETFL operation (Section 5.3).

o File creation flags 
These are the flags shown in the second part of Table 4-3. They control various aspects of the
behavior of the open() call, as well as options for subsequent I/O operations. These flags can't be
retrieved or changed.

o Open file status flags
These are the remaining flags in Table 4-3. They can be retrieved and modified using the fcntl()
    F_GETFL and F_SETFL operations (Section 5.3). These flags are sometimes simply called the file
    status flags.


<proc-fdinfo>
Since kernel 2.6.22, the Linux-specific files in the directory /proc/PID/fdinfo can be read to
obtain information about the file descriptors of any process on the system. There is one file in
this directory for each of the process's open file descriptors, with a name that matches the number
of the descriptor. The pos field in this file shows the current file offset (Section 4.7). The flags
field is an octal number that shows the file access mode flags and open file status flags.  (To
        decode this number, we need to look at the numeric values of these flags in the C library
        header files.)

# cat /proc/847/fdinfo/71
pos:	0
flags:	0200


O_CLOEXEC (since Linux 2.6.23)

Enable the close-on-exec flag (FD_CLOEXEC) for the new file descriptor. We describe the FD_CLOEXEC
flag in Section 27.4. Using the O_CLOEXEC flag allows a program to avoid additional fcntl() F_SETFD
and F_SETFD operations to set the close-on-exec flag. It is also necessary in multithreaded programs
to avoid the race conditions that could occur using the latter technique. These races can occur when
one thread opens a file descriptor and then tries to mark it close-on-exec at the same time as
another thread does a fork() and then an exec() of an arbitrary program. (Suppose that the second
        thread manages to both fork() and exec() between the time the first thread opens the file
        descriptor and uses fcntl() to set the close-on-exec flag.) Such races could result in open
file descriptors being unintentionally passed to unsafe programs. (We say more about race conditions
        in Section 5.1.)

O_CREAT

If the file doesn't already exist, it is created as a new, empty file. This flag is effective even
if the file is being opened only for reading. If we specify O_CREAT, then we must supply a mode
argument in the open() call; otherwise, the permissions of the new file will be set to some random
value from the stack.


O_EXCL

This flag is used in conjunction with O_CREAT to indicate that if the file already 'exists', it should
not be opened; instead, open() should fail, with errno set to EEXIST. In other words, this flag
allows the caller to 'ensure' that it is the process creating the file. 


<open-error>
If an error occurs while trying to open the file, open() returns -1, and errno identifies the cause
of the error. 


{read-call}
#include <unistd.h>

ssize_t read(int fd, void *buffer, size_t count);

Returns number of bytes read, 0 on EOF, or -1 on error

The count argument specifies the maximum number of bytes to read. The size_t data type is an
unsigned integer type. The buffer argument supplies the address of the memory buffer into which the
input data is to be placed. This buffer must be at least count bytes long.

A successful call to read() returns the number of bytes actually read, or 0 if EOF is encountered.
On error, the usual -1 is returned. The ssize_t data type is a signed integer type used to hold a
byte count or a -1 error indication.  

A call to read() may read 'less' than the requested number of bytes. For a regular file, the
probable reason for this is that we were close to the end of the file.


<not-null-terminated>
The output from read() is strange because read() doesn't place a terminating null byte at the end of
the string that printf() is being asked to print. A moment's reflection leads us to realize that
this must be so, since read() can be used to read 'any' sequence of bytes from a file. 

In some cases, this input might be text, but in other cases, the input might be binary integers or C
structures in binary form. There is no way for read() to tell the difference, and so it can't attend
to the C convention of null terminating character strings. If a terminating null byte is required at
the end of the input buffer, we must put it there explicitly:


{write-call}
#include <unistd.h>

ssize_t write(int fd, void *buffer, size_t count);

Returns number of bytes written, or -1 on error


{ioctl-call}
The ioctl() system call is a general-purpose mechanism for performing file and device operations
that fall outside the universal I/O model described earlier in this chapter.

#include <sys/ioctl.h>

int ioctl(int fd, int request, ... /* argp */);

Value returned on success depends on request, or -1 on error

The fd argument is an open file descriptor for the device or file upon which the control operation
specified by 'request' is to be performed. Device-specific header files define constants that can be
passed in the request argument.


={============================================================================
*kt_linux_core_201* file io: futher details

LPI 5.

{automicity}
All system calls are executed atomically. By this, we mean that the kernel guarantees that all of
the steps in a system call are completed as a single operation, without being interrupted by another
process or thread.

<race-condition>
Atomicity is essential to the successful completion of some operations. In particular, it allows us
to avoid race conditions (sometimes known as race hazards). A race condition is a situation where
the result produced by two processes (or threads) operating on shared resources depends in an
unexpected way on the relative order in which the processes gain access to the CPU(s).


{two-cases-when-needs-automicity}
<in-creation>
Creating a file exclusively

Using a single open() call that specifies the O_CREAT and O_EXCL flags prevents this possibility by
guaranteeing that the check and creation steps are carried out as a single atomic uninterruptible
operation.

See 5.1 for the problem description.

<in-appending>
When we have multiple processes appending data to the same file (e.g., a global log file).

if (lseek(fd, 0, SEEK_END) == -1)
   errExit("lseek");

if (write(fd, buf, len) != len)
   fatal("Partial/failed write");

Again, this is a race condition because the results depend on the order of scheduling of the two
processes. Avoiding this problem requires that the seek to the next byte past the end of the file
and the write operation happen atomically. This is what opening a file with the O_APPEND flag
guarantees.


{file-control-operations}
#include <fcntl.h>

int fcntl(int fd, int cmd, ...);

Return on success depends on cmd, or -1 on error

Using fcntl() to modify open file status flags is particularly useful in the following cases:

o The file was not opened by the calling program, so that it had no control over the flags used in
the open() call. e.g., the file may be one of the three standard descriptors that are opened before
the program is started.

o The file descriptor was obtained from a system call other than open(). Examples of such system
calls are pipe(), which creates a pipe and returns two file descriptors referring to either end of
the pipe, and socket(), which creates a socket and returns a file descriptor referring to the
socket.


{file-descriptors-and-open-files}
It is possible-and useful-to have multiple descriptors referring to the same open file. Three data
structures maintained by the 'kernel':

o the per-process open file descriptor table
o the system-wide table of open file descriptions
o the file system i-node table


<open-file-descriptor-table>
o a set of flags controlling the operation of the file descriptor (there is just one such flag, the
        close-on-exec flag, which we describe in Section 27.4); and

o a reference to the open file description.


<open-file-description-table>
An open file description stores all information relating to an open file, including:

o the current file offset (as updated by read() and write(), or explicitly modified using lseek());
o status flags specified when opening the file (i.e., the flags argument to open());
o the file access mode (read-only, write-only, or read-write, as specified in open());
o settings relating to signal-driven I/O (Section 63.3); and
o a reference to the i-node object for this file.

<inode-table>
Each file system has a table of i-nodes for all files residing in the file system. The i-node for
each file includes the following information:

o file type (e.g., regular file, socket, or FIFO) and permissions;
o a pointer to a list of locks held on this file; and
o various properties of the file, including its size and timestamps relating to different types of
file operations.

From figure 5-2.

<when-the-same-process-has-multiple-descriptors-for-the-same-file>
In process A, descriptors 1 and 20 both refer to the same open file description (labeled 23). This
situation may arise as a result of a call to dup(), dup2(), or fcntl() (see Section 5.5).

<when-different-process-has-descriptor-for-the-same-file>
Descriptor 2 of process A and descriptor 2 of process B refer to a single open file description
(73). This scenario could occur after a call to fork() (i.e., process A is the parent of process B,
        or vice versa), or if one process passed an open descriptor to another process using a UNIX
domain socket (Section 61.13.3).

<when-different-process-has-the-same-inode>
Finally, we see that descriptor 0 of process A and descriptor 3 of process B refer to different open
file descriptions, but that these descriptions refer to the same i-node table entry (1976)-in other
words, to the same file. This occurs because each process independently called open() for the same
file. A similar situation could occur if a single process opened the same file twice.


{duplicating-descriptors}
The case example of duplicating descriptors. The following informs the shell that we wish to have
standard error (2) redirected to the same place to which standard output (1) is being sent.

$ ./myscript > results.log 2>&1

The shell achieves the redirection of standard error by duplicating file descriptor 2 so that it
refers to the same open file description as file descriptor 1.

Note that it is not sufficient for the shell simply to open the results.log file twice: once on
descriptor 1 and once on descriptor 2. As shown above, this makes two descriptors and descriptions. 

One reason for this is that the two file descriptors would not share a file offset pointer, and
hence could end up overwriting each other's output. (Since uses different description)

Another reason is that the file may not be a disk file. Consider the following command, which sends standard error down the
same pipe as standard output: (Same problem of overwriting?)

$ ./myscript 2>&1 | less


<dup-call>
The dup() call takes oldfd, an open file descriptor, and returns a new descriptor that refers to the
same open file 'description'. The new descriptor is guaranteed to be the lowest unused file
descriptor.

#include <unistd.h>

int dup(int oldfd);
int dup2(int oldfd, int newfd);

Returns (new) file descriptor on success, or -1 on error

The dup2() system call makes a duplicate of the file descriptor given in oldfd using the descriptor
number 'supplied' in newfd. If the file descriptor specified in newfd is already open, dup2()
'closes' it first. Any error that occurs during this close is silently ignored; safer programming
practice is to explicitly close() newfd if it is open before the call to dup2().

We could simplify the preceding calls to close() and dup() to the following:

dup2(1, 2);


{on-threads}
All of the threads in a process share the same file descriptor table. This means that the file
offset for each open file is global to all threads. Using pread() or pwrite(), multiple threads can
simultaneously perform I/O on the same file descriptor without being affected by changes made to the
file offset by other threads.


={============================================================================
*kt_linux_core_202* file io: non-blocking

LPI 5.9

The O_NONBLOCK flag when opening a file serves two purposes:

o If the file can't be opened immediately, then open() returns an error instead of blocking. One
case where open() can block is with FIFOs (Section 44.7).

o After a successful open(), 'subsequent' I/O operations are also nonblocking. If an I/O system call
can't complete immediately, then either a partial data transfer is performed or the system call
fails with one of the errors EAGAIN or EWOULDBLOCK. Which error is returned depends on the system
call. On Linux, as on many UNIX implementations, these two error constants are synonymous.

<nonblocking-pipe>
Nonblocking mode can be used with devices (e.g., terminals and pseudoterminals), pipes, FIFOs, and
sockets. Because file descriptors for pipes and sockets are not obtained using open(), we must
enable this flag using the fcntl() F_SETFL operation.

note: This is why use pipe2-call.

O_NONBLOCK is generally ignored for regular files, because the kernel buffer cache ensures that I/O
on regular files does not block, as described in Section 13.1. However, O_NONBLOCK does have an
effect for regular files when mandatory file locking is employed (Section 55.4).


={============================================================================
*kt_linux_core_203* file io: /dev/fd

For each process, the kernel provides the special virtual directory /dev/fd. This directory contains
filenames of the form /dev/fd/n, where n is a number corresponding to one of the open file
descriptors for the process.

note: which process?

/dev/fd is actually a symbolic link to the Linux-specific /proc/self/fd directory. The latter
directory is a special case of the Linux-specific /proc/PID/fd directories, each of which contains
symbolic links corresponding to all of the files held open by a process.

$ ll /dev/fd/
total 0
dr-x------ 2 keitee keitee  0 May 10 22:22 ./
dr-xr-xr-x 8 keitee keitee  0 May 10 22:22 ../
lrwx------ 1 keitee keitee 64 May 10 22:22 0 -> /dev/pts/1
lrwx------ 1 keitee keitee 64 May 10 22:22 1 -> /dev/pts/1
lrwx------ 1 keitee keitee 64 May 10 22:22 2 -> /dev/pts/1
lr-x------ 1 keitee keitee 64 May 10 22:22 3 -> /proc/4122/fd/


The files in the /dev/fd directory are rarely used within programs. To use standard input or output
(as appropriate) for this filename argument.

Using /dev/fd allows to use standard input, output, and error as filename arguments to any program
requiring them. Thus, we can write the previous shell command as follows:

$ ls | diff /dev/fd/0 oldfilelist

As a convenience, the names /dev/stdin, /dev/stdout, and /dev/stderr are provided as symbolic links
to, respectively, /dev/fd/0, /dev/fd/1, and /dev/fd/2.


={============================================================================
*kt_linux_core_204* file attribute

LPI 15.

{calls}
The stat(), lstat(), and fstat() system calls retrieve information about a file, mostly drawn from
the file i-node.

man 2 stat

#include <sys/stat.h>
int stat(const char *pathname, struct stat *statbuf);
int lstat(const char *pathname, struct stat *statbuf);
int fstat(int fd, struct stat *statbuf);

All return 0 on success, or -1 on error

stat() returns information about a named file;

lstat() is similar to stat(), except that if the named file is a symbolic link, information about
the link itself is returned, rather than the file to which the link points; and

fstat() returns information about a file referred to by an open file descriptor.

<permission>
The stat() and lstat() system calls don't require permissions on the file itself. However, execute
(search) permission is required on 'all' of the parent directories specified in pathname. The fstat()
  system call always succeeds, if provided with a valid file descriptor. 

<use-to-check>
Shows that can use stat() to check if the file exist or not.

struct stat sb;
if (-1 == stat(cleanup_exe, &sb))
{
  fprintf(stderr, "executable (%s) doesn't exist\n", cleanup_exe);
  return 0;
}


={============================================================================
*kt_linux_core_200* ipc

CH43, Fig 43-1 in {ref-LPI} which says that the 'general' term IPC is often used to describe them all;
communication, signal, and synchronization.

communication - data transfer - byte stream  - pipe
                                             - fifo
                                             - stream socket

                              - message      - sys v message q
                                             - posix message q
                                             - datagram socket

                              - pseudoterminal

               - shared memory   - sys v shm
                                 - posix shm
                                 - memory mapping  - anonymous mapping
                                                   - mapped file

signal   - standard signal
         - realtime signal

synchronization   - semaphore - sys v
                  - posix     - named
                              - unnamed
                  - file lock - record lock
                              - file lock

                  - mutex
                  - condition variable

Signals: Although signals are intended primarily for other purposes, they can be used as a
synchronization technique in certain circumstances. More rarely, signals can be used as a
communication technique: the signal number itself is a form of information, and realtime signals can
be accompanied by associated data (an integer or a pointer).


{nonnetworked-ipc}
From {ref-UNP}. nonnetworked-ipc means that ipc for local and newtworked-ipc means that for remote
such as socket.


{categories-of-ipc}
From {ref-UNP}:

three-ways-to-share-between-processes

(1) Process Process       (2) Process Process          (3) Process <- shm -> Process
      |        |                 |       |      
Kernel                          shared info in kernel
      |        |
Filesystem


<persistence> which is lifetime of an objects
Define the persistence of any type of IPC as how long an object of that type remains in existence.

Process-persistence exists until last process with IPC open closes the object and kernel one exists
until kernel reboots or IPC objects is explicitly deleted.

process-persistence: pipe, fifo, mutex, condition-var, read-write-lock, ...
kernel-presistence : shm, named-semaphore, ...

Be careful when defining the presistence of ipc because it is not always as it seems: the data
within a pipe is maintained within the kernel, but pipes have process-persistence because after the
last process that has the pipe open for reading closes the pipe, the kernel discard all data and
remove the pipe.

From {ref-LPI}:

<data-transfer> 
1. requires two data transfers between user and kenel memory
2. synchronization between the reader and writer processes is automatic by kernel. if a reader
attempts to fetch data from data-transfer facility that has no data, then read will bock until some
write data to it.  
3. available to one which done read operation since read consumes data.

<byte-stream>
read and write is independent meaning read may read an arbitrary bytes. This models "file as a
sequence of bytes".

An application can also impose a message-oriented model on a byte-stream facility, by using
delimiter characters, fixed-length messages, or message headers that encode the length of the total
message message: each read reads a whole message. not possible to read part of a message and to read
multiple messages.


<shared-memory>
1. don't require system calls or data transfer between user and kenel. Hence shared memory
provide very fast communication. 
2. However it can be offset by the need to sync and semaphore is the usual method used with shared
memory.
3. avaible to all of the processes that share that memory

<file-desc-based>
facility using file descriptors like pipe, fifo, and sockets

The primary benefit of these techniques is that they allow an application to simultaneously monitor
multiple file descriptors to see whether I/O is possible on any of them.


{namespace}
Use name or identifier so that one process can create ipc object and other processes can specify
that same ipc object.

type                       name used to identify   handle used to refer to object
------------------------------------------------------------------------------------
pipe                       no name                 file descriptor
fifo                       pathname                ditto

UNIX domain socket         pathname                ditto
Internet domain socket     IP and port             ditto

posix message q            posix ipc pathname      mqd_t
posix named semaphore      ditto                   sem_t* (sem pointer)
posix unnamed semaphore    no name                 sem_t*
posix shared memory        posix ipc pathname      file descriptor 

anonymous mapping          no name                 none
memory mapped file         pathname                file descriptor 


{accessibility-and-persistence}
type                       accessibility                 persistence
------------------------------------------------------------------------------------
pipe                       only by related processes     process
fifo                       permission mask               ditto

UNIX domain socket         permission mask               ditto
Internet domain socket     by any processe               ditto

posix message q            permission mask               kernel
posix named semaphore      permission mask               kernel
posix unnamed semaphore    permission of underlying mem  depends
posix shared memory        permission mask               kernel

anonymous mapping          only by releated              process 
memory mapped file         permission mask               file system 

unix and network domain:

Of all of the IPC methods shown in Figure 43-1, only sockets permit processes to communicate over a
network. Sockets are generally used in one of two domains: the UNIX domain, which allows
communication between processes on the same system, and the Internet domain, which allows
communication between processes on different hosts connected via a TCP/IP network. Often, only minor
changes are required to convert a program that uses UNIX domain sockets into one that uses Internet
domain sockets, so an application that is built using UNIX domain sockets can be made
network-capable with relatively little effort.

<portability>
However, the POSIX IPC facilities (message queues, semaphores, and shared memory) are not quite as
widely available as their System V IPC counterparts, especially on older UNIX systems. An
implementation of POSIX message queues and 'full' support for POSIX semaphores have appeared on Linux
only in the 2.6.x kernel series. Therefore, from a portability point of view, System V IPC may be
preferable to POSIX IPC.

As of 06 Jan 2014, the latest stable kernel release is 3.12.6

note: Here, 'related' means related via fork(). In order for two processes to access the object, one
of them must create the object and then call fork(). As a consequence of the fork(), the child
process inherits a handle referring to the object, allowing both processes to share the object.

<performance>
In some circumstances, different IPC facilities may show notable differences in performance.
However, in later chapters, we generally refrain from making performance comparisons, for the
following reasons:

1) The performance of an IPC facility may not be a significant factor in the overall performance of
an application, and it may not be the only factor in determining the choice of an IPC facility.

2) The relative performance of the various IPC facilities may vary across UNIX implementations or
between different versions of the Linux kernel.

3) Most importantly, the performance of an IPC facility will vary depending on the precise manner
and environment in which it is used. Relevant factors include the size of the data units exchanged
in each IPC operation, the amount of unread data that may be outstanding on the IPC facility,
whether or not a process context switch is required for each unit of data exchanged, and other
load on the system.

If IPC performance is crucial, there is no substitute for application-specific benchmarks run under
an environment that matches the target system. To this end, it may be worth writing an 'abstract'
software layer that hides details of the IPC facility from the application and then testing
performance when different IPC facilities are substituted underneath the abstract layer.


={============================================================================
*kt_linux_core_201* ipc: system v

{interfaces}
A more significant reason for discussing the System V IPC mechanisms together is that their
programming interfaces share a number of common characteristics, so that many of the same concepts
apply to all of these mechanisms.


{key-and-identifier}
System V IPC keys are integer values represented using the data type key_t which is analogous to a
filename and get calls which is analogous to the open() system call used for files. The get calls
'translate' a key into the corresponding integer IPC identifier which is analogous to a file
descriptor.

<identifier>
There is, however, an important semantic difference. Whereas a file descriptor is a process
attribute, an IPC identifier is a property of the object itself and is visible 'system'-wide.

All processes accessing the same object use the same identifier. This means that if we know an IPC
object already exists, we can skip the get call, provided we have some other means of knowing the
identifier of the object.

<flag>
We specify the permissions to be placed on the new object as part of the final (flags) argument to
the get call, using the 'same' bit-mask constants as are used for files.

<key>
So, how do we provide a unique key? (LPI, 45.2)

One of three methods. Specify the IPC_PRIVATE constant as the key value to the get call when
creating the IPC object, which 'always' results in the creation of a 'new' IPC object that is
guaranteed to have a 'unique' key.


{ctl-calls}
A few are generic to all IPC mechanisms. An example of a generic control operation is IPC_RMID,
  which is used to delete an object.

<when-deleted>
For message queues and semaphores, deletion of the IPC object is immediate, and any information
contained within the object is destroyed, regardless of whether any other process is still using the
object.

For files, if we remove the last link to a file, then the file is actually removed only after all
open file descriptors referring to it have been closed. As with files, Deletion of shared memory
objects occurs differently.


{persistence}
System V IPC objects have 'kernel' persistence. Once created, an object continues
to exist until it is explicitly deleted or the system is shut down. 

Two disadvantages: 
1. system-imposed limit. If we fail to remove unused objects, we may eventually encounter
application errors as a result of reaching these limits.

2. When deleting a message queue or semaphore object, a multiprocess application may not be able to
easily determine which will be the last process requiring access to the object, and thus when the
object can be safely deleted. Not for shm.


{associated-data}
The kernel maintains an associated data structure for 'each' instance of a System V IPC object. 

The associated data structure for an IPC object is initialized when the object is created via the
appropriate get system call. Once the object has been 'created', a program can obtain a copy of this
data structure using the appropriate ctl system call, by specifying an operation type of IPC_STAT.

<ipc_perm>
As well as data specific to the type of IPC object, the associated data structure for all three IPC
mechanisms includes a substructure, ipc_perm, that holds information used to determine permissions
granted on the object

EACCES.

See LPI 45.3 for more about permission checks.

<bypass-permission-check>
The second user could bypass this check by specifying 0 for the second argument of the msgget()
  call, in which case an error would occur only when the program attempted an operation requiring
  write permission on the IPC object

msgget(key, 0);


{ipcs-command}
The ipcs command lists the System V IPC objects that currently exist on the system. The ipcrm
command is used to remove System IPC objects.

ipcs - provide information on ipc facilities

       ipcs [-asmq] [-tclup]

       Resources may be specified as follows:
       -m     shared memory segments
       -q     message queues
       -s     semaphore arrays
       -a     all (this is the default)

       The output format may be specified as follows:
       -t     time
       -p     pid
       -c     creator
       -l     limits
       -u     summary

$ ipcs -m -l

The status flags indicate whether the region has been locked into RAM to prevent swapping (Section
    48.7) and whether the region has been marked to be destroyed when all processes have detached
it.

<limitation>
On Linux, ipcs(1) displays information 'only' about IPC objects for which we have read permission,
   regardless of whether we own the objects.

<non-portable-means>
Linux provides two nonstandard methods of obtaining a list of all IPC objects on the system:

1. files within the /proc/sysvipc directory that list all IPC objects

/proc/sysvipc/msg
/proc/sysvipc/sem
/proc/sysvipc/shm
key         shmid    perms    size  cpid  lpid nattch uid   gid  cuid  cgid      atime      dtime      ctime
305450176   163844   666      24    1063  1395 4      504   504   504   504 1422517874 1422517866         33

note: see cpid and size

/proc/sysvipc
Subdirectory containing the pseudo-files msg, sem and shm. These files list the System V
Interprocess Communication (IPC) objects (respectively: message queues, semaphores, and shared
    memory) that currently exist on the system, providing similar information to that available via
ipcs(1).  These files have headers and are formatted (one IPC object per line) for easy
understanding. <svipc>(7) provides further background on the information shown by these files.

2. the use of Linux-specific ctl calls.

Unlike the ipcs command, these files always show all objects of the corresponding type, regardless
of whether read permission is available on the objects.


={============================================================================
*kt_linux_core_202* ipc: system v: shm

{segment}
Shared memory allows two or more processes to share the same region (usually referred to as a
segment) of physical memory. 


{good-and-bad}
Since a shared memory segment becomes part of a process's user-space memory, no kernel intervention
is required for IPC. note: means the fastest IPC.

On the other hand, the fact that IPC using shared memory is not mediated by the kernel means that,
   typically, some method of synchronization is required so that processes don't simultaneously
   access the shared memory. e.g., two processes performing simultaneous updates, or one process
   fetching data from the shared memory while another process is in the middle of updating it.


{interfaces}
<shmget>
To create a new shared memory segment or obtain the identifier of an existing segment. note: The
contents of a newly created shared memory segment are initialized to 0.

#include <sys/shm.h>

int shmget(key_t key, size_t size, int shmflg);

Returns shared memory segment 'identifier' on success, or -1 on error

<creator-and-size>
The 'shmflg' argument performs the same task as for the other IPC get calls, specifying the
permissions to be placed on a new shared memory segment or 'checked' against an existing segment. In
addition, zero or more of the fol- lowing flags can be ORed (|) in shmflg to control the operation
of shmget():

The 'kernel' allocates shared memory in multiples of the system page size, so size is effectively
rounded up to the next multiple of the system 'page' size. If we are using shmget() to obtain the
identifier of an existing segment, then size has 'no' effect on the segment, but it 'must' be less
than or equal to the size of the segment.

to-creat:

IPC_CREAT
If no segment with the specified key exists, create a new segment.

<when-get-errors>
The real case which got EINVAL when called shmget. The below is from man page.

EINVAL 

A new segment was to be created and size < SHMMIN or size > SHMMAX, or no new segment was to be
created, a segment with given key existed, but size is 'greater' than the size of that segment.

note: The problem was to ask shm which is 'greater' than the size of the segment. Interestingly, the
real size was 24 but /proc/PID/maps shows 4K since the pagesize is 4K. So careful to see mapping
info.

<shmat>
#include <sys/shm.h>

void *shmat(int shmid, const void *shmaddr, int shmflg);

Returns address at which shared memory is attached on success, or (void *) -1 on error

The shmaddr argument and the setting of the SHM_RND bit in the shmflg bit-mask argument control
how the segment is attached: See 48.3 for full options.

If shmaddr is NULL, then the segment is attached at a suitable address selected by the kernel. This
is the 'preferred' method of attaching a segment since specifying a non-NULL value for shmaddr is
not recommended, for the following reasons:

It reduces the portability of an application and the particular address will be already in use.

As its function result, shmat() returns the address at which the shared memory segment is attached.
Typically, we assign the return value from shmat() to a pointer to some programmer-defined
structure, in order to impose that structure on the segment. 

If SHM_RDONLY is not specified, the memory can be both read and modified.

note: Seen shmat(, , 0), that is shmflg is 0. what is it? See <bypass-permission-check>

<shmdt>
When a process no longer needs to access a shared memory segment, it can call shmdt() to detach the
segment from its virtual address space.

#include <sys/shm.h>

int shmdt(const void *shmaddr);

Returns 0 on success, or -1 on error

Detaching a shared memory segment is not the same as deleting it. Deletion is performed using the
shmctl() IPC_RMID operation

<child-and-exec>
A child created by fork() inherits its parent's attached shared memory segments. Thus, shared memory
provides an easy method of IPC between parent and child. During an exec(), all attached shared
memory segments are detached. Shared memory segments are also 'automatically' detached on process
'termination'.

<shmctl> 
#include <sys/shm.h>

int shmctl(int shmid, int cmd, struct shmid_ds *buf);

Returns 0 on success, or -1 on error

IPC_RMID <deletion>
Mark the shared memory segment and its associated shmid_ds data structure for deletion. If no
processes currently have the segment attached, deletion is immediate; otherwise, the segment is
removed only 'after' all processes have detached from it (i.e., when the value of the shm_nattch
    field in the shmid_ds data structure falls to 0). This is much closer to the situation with file
deletion.

shmctl(id, IPC_RMID, NULL);   note: buf must be NULL.

Only one process needs to perform this step.

IPC_STAT
Place a copy of the shmid_ds data structure associated with this shared memory segment in the buffer
pointed to by buf.

IPC_SET
Update selected fields of the shmid_ds data structure associated with this shared memory segment
using values in the buffer pointed to by buf.


{associated-data}
See LPI 48.8 for full details.

struct shmid_ds {
  struct ipc_perm shm_perm;   /* Ownership and permissions */
  size_t shm_segsz;           /* Size of segment in bytes */
  time_t shm_atime;           /* Time of last shmat() */
  time_t shm_dtime;           /* Time of last shmdt() */
  time_t shm_ctime;           /* Time of last change */
  pid_t shm_cpid;             /* PID of creator */
  pid_t shm_lpid;             /* PID of last shmat() / shmdt() */
  shmatt_t shm_nattch;        /* Number of currently attached processes */
};

shm_cpid
This field is set to the process ID of the process that created the segment using shmget().

shm_lpid
This field is set to 0 when the shared memory segment is created, and then set to the process ID of
the calling process on each successful shmat() or shmdt().

shm_nattch
This field counts the number of processes that currently have the segment attached. It is
initialized to 0 when the segment is 'created', and then incremented by each successful shmat() and
decremented by each successful shmdt(). 


{shm-in-virtual-memory}
From LPI 48.5. To check shm details from /proc/PID/maps.

The shared memory segments are attached starting at the virtual address 0x40000000 between heap and
stack. Mapped mappings and shared libraries are also placed in this area. The address 0x40000000 is
defined as the kernel constant TASK_UNMAPPED_BASE. Than can be changed.

$ ./svshm_create -p 102400       # size
9633796                          # shm id
$ ./svshm_create -p 3276800
9666565

$ ./svshm_attach 9633796:0 9666565:0
SHMLBA = 4096 (0x1000), PID = 9903
1: 9633796:0 ==> 0xb7f0d000      # attached at a address chosen by kernel
2: 9666565:0 ==> 0xb7bed000

$ cat /proc/9903/maps
...
b7bed000-b7f0d000 rw-s 00000000 00:09 9666565 /SYSV00000000 (deleted)
b7f0d000-b7f26000 rw-s 00000000 00:09 9633796 /SYSV00000000 (deleted)
...

<2> Two lines for the attached System V shared memory segments.

<columns>
1. A pair of hyphen-separated numbers indicating the virtual address range at which the memory
segment is mapped. The second of these numbers is the address of the next byte after the end of the
segment.

note: From this, can get the size. b7f0d000-b7bed000=0x320000. 3,276,800

2. Protection and flags for this memory segment. The first three letters indicate the protection of
the segment: read (r), write (w), and execute (x). A hyphen (-) in place of any of these letters
indicates that the corresponding protection is disabled. The final letter indicates the mapping flag
for the memory segment; it is either private (p) or shared (s). For an explanation of these flags,
    see the description of the MAP_PRIVATE and MAP_SHARED flags in Section 49.2. 
    
note: A System V shared memory segment is always marked shared.

3. The hexadecimal offset (in bytes) of the segment within the corresponding mapped file. The
meanings of this and the following two columns will become clearer when we describe the mmap()
system call in Chapter 49. 

note: For a System V shared memory segment, the offset is always 0.

4. The device number (major and minor IDs) of the device on which the corresponding mapped file is
located.

5. The i-node number of the mapped file, or, for System V shared memory segments, the 'identifier'
for the segment.

6. The filename or other identifying tag associated with this memory segment. For a System V shared
memory segment, this consists of the string SYSV concatenated with the shmget() key of the segment
(expressed in hexadecimal). In this example, SYSV is followed by zeros because we created the
segments using the key IPC_PRIVATE (which has the value 0). The string (deleted) that appears after
the SYSV field for a System V shared memory segment is an artifact of the implementation of shared
memory segments. Such segments are created as mapped files in an invisible tmpfs file system
(Section 14.10), and then later unlinked. Shared anonymous memory mappings are implemented in the
same manner. (We describe mapped files and shared anonymous memory mappings in Chapter 49.)

<example> 
Linux (none) 2.6.31-3.2 #2 SMP Wed Dec 10 02:53:42 EST 2014 mips GNU/Linux

cat /proc/sysvipc/shm

key         shmid    perms    size  cpid  lpid nattch uid   gid  cuid  cgid      atime      dtime      ctime
305450176   163844   666      24    1066  1470 4      504   504   504   504 1423064150 1423064135         33

root# ls -al /proc/1066/
lrwxrwxrwx    1 root     root             0 Jan  1 00:12 exe -> /opt/cds/bin/huaweidaemon

root# cat /proc/1066/maps | grep SYS      note: why twice?
2aab3000-2aab4000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)
2aab5000-2aab6000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)

root# cat /proc/1470/maps | grep SYS
2aab2000-2aab3000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)

The diff is 0x1000. 4096.

root# cat /proc/1523/smaps | grep -A 20 2fc37
2fc37000-2fc38000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)
Size:                  4 kB
Rss:                   4 kB
Pss:                   1 kB
Shared_Clean:          4 kB
Shared_Dirty:          0 kB
Private_Clean:         0 kB
Private_Dirty:         0 kB
Referenced:            4 kB
Swap:                  0 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB            <pagesize>


{shm-limits}
On Linux, some of the limits can be viewed or changed via files in the /proc file system. See LPI
48.9 for more details. 

Below is a list of the Linux shared memory limits. The system call affected by the limit and the
error that results if the limit is reached are noted in parentheses.

SHMMNI
This is a system-wide limit on the number of shared memory 'identifiers' (in other words, shared
    memory segments) that can be created. (shmget(), ENOSPC)

SHMMIN
This is the minimum size (in bytes) of a shared memory segment. This limit is defined with the value
1 (this can't be changed). However, the effective limit is the system page size. (shmget(), <EINVAL>)

SHMMAX
This is the maximum size (in bytes) of a shared memory segment. The practical upper limit for SHMMAX
depends on available RAM and swap space. (shmget(), <EINVAL>)

SHMALL
This is a system-wide limit on the total number of pages of shared memory. Most other UNIX
implementations don't provide this limit. The practical upper limit for SHMALL depends on available
RAM and swap space. (shmget(), ENOSPC)

<debian-linux>
keitee@debian-keitee:/proc/sys/kernel$ uname -a
Linux debian-keitee 3.2.0-4-486 #1 Debian 3.2.63-2+deb7u2 i686 GNU/Linux
keitee@debian-keitee:/proc/sys/kernel$ ls -al shm*
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmall
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmmax
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmmni
-rw-r--r-- 1 root root 0 Jan 28 21:25 shm_rmid_forced
keitee@debian-keitee:/proc/sys/kernel$ cat shmall
2097152
keitee@debian-keitee:/proc/sys/kernel$ cat shmmax
33554432
keitee@debian-keitee:/proc/sys/kernel$ cat shmmni
4096
keitee@debian-keitee:/proc/sys/kernel$ cat shm_rmid_forced 
0

<source> ucLinux case
/include/linux/shm.h

SHMMAX: shm segment max size in byte,     131,072 (typical value),      33,554,432 (ucLinux)
SHMMIN: shm segment min size in byte,     1 (typical value),            1 (ucLinux)
SHMMNI: shm segment max num in system,    100 (typical value),          4,096 (ucLinux)
SHMSEG: shm segment max size in process,  6 (typical value),            4,096 (ucLinux)


{locking}
A shared memory segment can be locked into RAM, so that it is never swapped out. This provides a
performance benefit, since, once each page of the segment is memory-resident, an application is
guaranteed never to be delayed by a page fault when it accesses the page.

<not-all-support> TODO:
These operations are not specified by SUSv3, and they are not provided on all UNIX implementations.
In versions of Linux before 2.6.10, only privileged (CAP_IPC_LOCK) processes can lock a shared
memory segment into memory. Since Linux 2.6.10, an unprivileged process can lock and unlock a shared
memory segment if its effective user ID matches either the owner or the creator user ID of the
segment and (in the case of SHM_LOCK) the process has a sufficiently high RLIMIT_MEMLOCK resource
limit. See Section 50.2 for details.


={============================================================================
*kt_linux_core_203* ipc: server consideration

From LPI 45.4.

The consideration when IPC server is terminated prematurely.

Suppose a client engages in an extended dialogue with a server, with multiple IPC operations being
performed by each process (e.g., multiple messages exchanged, a sequence of semaphore operations, or
    multiple updates to shared memory). 

What happens if the server process 'crashes' or is deliberately halted and then restarted? 

At this point, it would make no sense to blindly reuse the existing IPC object created by the
previous server process, since the new server process has no knowledge of the historical information
associated with the current state of the IPC object. (For example, there may be a secondary request
    within a message queue that was sent by a client in response to an earlier message from the old
    server process.)

In such a scenario, the only option for the server may be to 'abandon' all existing
clients, 'delete' the IPC objects created by the previous server process, and create new
instances of the IPC objects. 

A newly started server handles the possibility that a previous instance of the server terminated
prematurely by first trying to create an IPC object by specifying both the IPC_CREAT and the
IPC_EXCL flags within the get call. If the get call fails because an object with the specified key
already exists, then the server assumes the object was created by an old server process; it
therefore uses the IPC_RMID ctl operation to delete the object, and once more performs a get call to
create the object. (This may be combined with other steps to ensure that another server process is
    not currently running, such as those described in Section 55.6.)

<example>
// return false if process is in slave mode(client)
// return true if process is in master mode(server)

// if no shm created already, then return true to create one.
// if shm is there, but no one use. so try to delete it and return true.
// if shm is there, but some use it, then return false.

// if no one use it then crash might happened so start it all over again.

static bool do_platform_init( size_t shMemSz )
{
  int    shmid, shmflag;
  struct shmid_ds shmds;

  shmflag = 0666;
  shmid = shmget( SHAREDMEM_KEY, shMemSz, shmflag );

  if ( shmid >= 0 )
  {
    if ( shmctl( shmid, IPC_STAT, &shmds ) < 0 )
    {
      perror( "shmctl - IPC_STAT failed" );
    }
    else
    {
      fprintf( stderr, "%s(): shmds.shm_nattch=%d\n",
               __FUNCTION__, (unsigned)shmds.shm_nattch );
      if ( 0 == shmds.shm_nattch && shmctl( shmid, IPC_RMID, (struct shmid_ds *)NULL ) < 0 )
      {
         fprintf( stderr, "shmctl - destroy failed\n" );
      }
    }
    return (0 == (unsigned)shmds.shm_nattch);
  }
  else
  {
    perror("shmget");
  }
  return true;
}


={============================================================================
*kt_linux_core_250* ipc: posix

One of the POSIX.1b developers' aims was to devise a set of IPC mechanisms that did not suffer the
deficiencies of the System V IPC facilities. These IPC mechanisms are collectively referred to as
POSIX IPC.

note: what deficiencies?


={============================================================================
*kt_linux_core_300* ipc: socket: LPI 56

Sockets are a method of IPC that allow data to be exchanged between applications, either on the same
host (computer) or on different hosts connected by a network.

A socket is created using the socket() system call, which returns a file descriptor used to refer to
the socket in subsequent system calls:

fd = socket(domain, type, protocol);

For all applications described in this book, protocol is 'always' specified as 0.


{communication-domain}
domain determines:

1. the method of identifying a socket (i.e., the format of a socket “address”)

2. the range of communication (i.e., either between applications on the same host or between
    applications on different hosts connected via a network).


OS supports at least the followings: AF stands for address family

Table 56-1: Socket domains

Domain   Communication     Communication           Address format          Address structure
         performed         between applications

AF_UNIX  within kernel     on same host            pathname                sockaddr_un

AF_INET  via IPv4          on hosts connected      32-bit IPv4 address +   sockaddr_in
                           via an IPv4 network     16-bit port number

AF_INET6 via IPv6          on hosts connected      128-bit IPv6 address +  sockaddr_in6
                           via an IPv6 network     16-bit port number


{type}
Every sockets implementation provides at least two types of sockets: stream and datagram.

Table 56-2: Socket types and their properties

Property                         Stream Socket     Datagram Socket 
Reliable delivery?               Y                 N
Message boundaries preserved?    N                 Y
Connection-oriented?             Y                 N

<stream-socket>
Stream sockets (SOCK_STREAM) provide a reliable, bidirectional, byte-stream communication channel.

These socket types are supported in both the UNIX(on same host) and the Internet domains(on
network).

1. Reliable means that we are guaranteed that either the transmitted data will arrive intact at the
receiving application, exactly as it was transmitted by the sender (assuming that neither the
    network link nor the receiver crashes), or that we'll receive notification of a probable failure
in transmission.

2. Bidirectional means that data may be transmitted in either direction between two sockets.

3. Byte-stream means that, as with pipes, there is no concept of message boundaries

Stream sockets operate in connected pairs so described as connection-'oriented'. A stream socket can
be connected to 'only' one peer.

<datagram-socket>
Datagram sockets (SOCK_DGRAM) allow data to be exchanged in the form of 'messages' called datagrams.

With datagram sockets, message boundaries are preserved, but data transmission is not reliable.
Messages may arrive out of order, be duplicated, or not arrive at all.

Datagram sockets are an example of the more generic concept of a 'connectionless' socket since
unlike a stream socket, a datagram socket doesn't need to be connected to another socket in order to
be used.

note:
In the Internet domain, datagram sockets employ the User Datagram Protocol (UDP), and stream sockets
(usually) employ the Transmission Control Protocol (TCP).  So often just use the terms UDP socket
and TCP socket, respectively.


{socket-io}
By default, these system calls 'block' if the I/O operation can't be completed immediately.
Nonblocking I/O is also possible, by using the fcntl() F_SETFL operation (Section 5.3) to enable the
O_NONBLOCK open file status flag.


{stream-example}

Passive socket (server)

  socket()

  bind()

  listen()

  accept()                                      Active socket(client)
      : blocks until client connects
   ...                                             socket()

      : resumes                  <-                connect()

+-------------------------------------------------------------+
| read()                         <-                write()    |
|                       : (possibly multiple) data            |
|                          transfers in either direction      |
| write()                        ->                read()     |
+-------------------------------------------------------------+
  close()                                          close()


<well-known-address>
Typically, we bind a server's socket to a well-known address - that is, a fixed address that is
known in advance to client applications that need to communicate with that server.


{syscalls}
#include <sys/socket.h>

<socket>
int socket(int domain, int type, int protocol);

Creates a new socket. Returns file descriptor on success, or -1 on error.
note: returns fd on success.

<bind>
int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

Binds a socket to an address. Returns 0 on success, or -1 on error

The sockfd argument is a file descriptor obtained from a previous call to socket(). 

<accept>
int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);

Returns file descriptor on success, or -1 on error

The accept() system call accepts an incoming connection on the listening stream socket referred to
by the file descriptor sockfd. If there are no pending connections when accept() is called, the call
blocks until a connection request arrives.

note: listening vs connection socket

The key point to understand about accept() is that it creates a 'new' socket, and it is this new
socket that is connected to the peer socket that performed the connect(). A file descriptor for the
connected socket is returned as the function result of the accept() call. The listening socket
(sockfd) remains open, and can be used to accept 'further' connections. A typical server application
creates one listening socket, binds it to a well-known address, and then handles all client requests
by accepting connections via that socket.

note: this is why accept() requires addr argument

The remaining arguments to accept() 'return' the address of the peer socket. The addr argument
points to a structure that is used to return the socket address. The type of this argument depends
on the socket domain

note: <value-result> argument is in-out argument which is also mentioned in man page.

The addrlen argument is a value-result argument. It points to an integer that, prior to the call,
    must be initialized to the size of the buffer pointed to by addr, 'so' that the kernel 'knows'
    how much space is available to return the socket address. Upon return from accept(), this
    integer is set to 'indicate' the number of bytes of data actually copied into the buffer.

If we are not interested in the address of the peer socket, then addr and addrlen should be
specified as NULL and 0, respectively. If desired, we can retrieve the peer’s address later using
the getpeername() system call, as described in Section 61.5.


{generic-address-structure}
The addr argument is a pointer to a structure specifying the address to which this socket is to be
bound. The type of structure passed in this argument 'depends' on the socket domain. The addrlen
argument specifies the size of the address structure.

For each socket domain, a 'different' structure type is defined to store a socket address. However,
    because system calls such as bind() are 'generic' to 'all' socket 'domains', they must be able
    to accept address structures of any type.

How?

In order to permit this, the sockets API defines a 'generic' address structure, struct sockaddr. The
only purpose for this type is to cast the various domain-specific address structures to a single
type for use as arguments in the socket system calls.

This structure serves as a template for all of the domain-specific address structures.

The value in the family field is 'sufficient' to determine the size and format of the address stored
in the remainder of the structure.

<sockaddr>
/* Structure describing a generic socket address.  */
struct sockaddr
{
  __SOCKADDR_COMMON (sa_);    /* Common data: address family and length.  */
  // note: that is sa_family_t sa_family
  char sa_data[14];           /* Address data.  */
};

#define  __SOCKADDR_COMMON(sa_prefix) \
  sa_family_t sa_prefix##family

<sockaddr_un>
/* Structure describing the address of an AF_LOCAL (aka AF_UNIX) socket.  */
struct sockaddr_un
{
  __SOCKADDR_COMMON (sun_);
  char sun_path[108];   /* Path name.  */          note: bigger size!
};

<sockaddr_in>
/* Structure describing an Internet (IP) socket address. */
#define __SOCK_SIZE__   16    /* sizeof(struct sockaddr) */
struct sockaddr_in {
  __kernel_sa_family_t  sin_family;    /* Address family */
  __be16                sin_port;      /* Port number    */    // 16 bit
  struct in_addr        sin_addr;      /* Internet address */  // 32 bit

  /* Pad to size of `struct sockaddr'. */
  unsigned char   __pad[__SOCK_SIZE__ - sizeof(short int) -
    sizeof(unsigned short int) - sizeof(struct in_addr)];
};

<sockaddr_in6>
struct sockaddr_in6 {
  unsigned short int    sin6_family;    /* AF_INET6 */
  __be16                sin6_port;      /* Transport layer port # */
  __be32                sin6_flowinfo;  /* IPv6 flow information */
  struct in6_addr       sin6_addr;      /* IPv6 address */
  __u32                 sin6_scope_id;  /* scope id (new in RFC2553) */
};


{code-example}

{
  const char *SOCKNAME = "/tmp/mysock";

  int sfd;
  struct sockaddr_un addr;

  sfd = socket(AF_UNIX, SOCK_STREAM, 0); /* Create socket */
  if (sfd == -1)
    errExit("socket");

  memset(&addr, 0, sizeof(struct sockaddr_un)); /* Clear structure */

  addr.sun_family = AF_UNIX; /* UNIX domain address */

  strncpy(addr.sun_path, SOCKNAME, sizeof(addr.sun_path) - 1);

  if (bind(sfd, (struct sockaddr *) &addr, sizeof(struct sockaddr_un)) == -1)
    errExit("bind");
}

// net/socket.c
//
/*
 * We move the socket address to kernel space before we call
 * the protocol layer (having also checked the address is ok).
 */
SYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)
{
  sock = sockfd_lookup_light(fd, &err, &fput_needed);
  if (sock) {
    err = move_addr_to_kernel(umyaddr, addrlen, &address);
    if (err >= 0) {
      err = security_socket_bind(sock,
          (struct sockaddr *)&address,
          addrlen);
      if (!err)
        err = sock->ops->bind(sock,
            (struct sockaddr *)
            &address, addrlen);
    }
    fput_light(sock->file, fput_needed);
  }
  return err;
}


// net/ipv4/af_inet.c
int inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
{
  // note: only used to check size
  if (addr_len < sizeof(struct sockaddr_in))
    goto out;
}


{
  struct sockaddr_storage claddr;

  addrlen = sizeof(struct sockaddr_storage);
  cfd = accept(lfd, (struct sockaddr *) &claddr, &addrlen);
  if (cfd == -1) {
    errMsg("accept");
    continue;
  }
}

/*
 * For accept, we attempt to create a new socket, set up the link
 * with the client, wake up the client, then return the new
 * connected fd. We collect the address of the connector in kernel
 * space and 'move' it to user at the very end. This is unclean because
 * we open the socket then return an error.
 *
 * 1003.1g adds the ability to recvmsg() to query connection pending
 * status to recvmsg. We need to add that support in a way thats
 * clean when we restucture accept also.
 */
SYSCALL_DEFINE4(accept4, int, fd, struct sockaddr __user *, upeer_sockaddr, int __user *, upeer_addrlen, int, flags)
{
  err = move_addr_to_user(&address, len, upeer_sockaddr, upeer_addrlen);
}


={============================================================================
*kt_linux_core_101*  ipc: pipe and fifo

{pipe}
The pipe is 'unnamed' fifo and is an early form that can be used 'related'-processes such as parent
and child. In other words, created using fork() call. Linux supports uni-directional pipe or
half-duplex so need 'two' pipes for read and write.


<connect-two-process>

$ ls | wc -l

ls (stdout, fd 1)    ->             pipe: unidirectional byte stream    ->    (stdin, fd 0) wc
                  write end                                             read end
                  of pipe                                               of pipe


<byte-stream-and-sequential> LPI 44.1
When we say that a pipe is a byte stream, we mean that there is no concept of 'messages' or message
boundaries when using a pipe. 

The process reading from a pipe can read 'blocks' of data of any size, regardless of the size of
blocks written by the writing process. Furthermore, the data passes through the pipe 'sequentially'
- bytes are read from a pipe in exactly the order they were written. It is not possible to randomly
access the data in a pipe using lseek().


<block-on-read>
Attempts to read from a pipe that is currently empty 'block' until at least one byte has been
written to the pipe. If the write end of a pipe is closed, then a process reading from the pipe will
see end-of-file (i.e., read() returns 0) once it has read all remaining data in the pipe.


<limited-capacity> <block-on-write> LPI 44
A pipe is simply a 'buffer' maintained in kernel memory. This buffer has a maximum capacity. Once a
pipe is full, further writes to the pipe 'block' until the reader removes some data from the pipe.


<use-large-buffer-size>
SUSv3 makes no requirement about the capacity of a pipe. In Linux kernels before 2.6.11, the pipe
capacity is the same as the system page size (e.g., 4096 bytes on x86-32); since Linux 2.6.11, the
pipe capacity is 65,536 bytes. Other UNIX implementations have different pipe capacities.

This means that pipe is kernel resource meaning that there is copy between kernel and process. In
theory, there is no reason why a pipe couldn't operate with smaller capacities, even with a
single-byte buffer. The reason for employing large buffer sizes is efficiency: each time a writer
fills the pipe, the kernel must perform a context switch to allow the reader to be scheduled so that
it can empty some data from the pipe. Employing a larger buffer size means that fewer context
switches are required.


<create-pipe-using-pipe-call>
#include <unistd.h>

int pipe(int filedes[2]);        Returns 0 on success, or -1 on error


<use-read-and-write>
As with any file descriptor, we can use the read() and write() system calls to perform I/O on the
pipe.


<set-up-pipe>
Parent gets two fds from a pipe() call which are fd[0] for read and fd[1] for write. Create a child
via fork() and the child process inherits copies of its parent's file descriptors. Close unused fds
to create a single channel between parent and child. note: 0 for read and 1 for write which are
fixed.

            parent                 ->                    parent           
                                             
fd[1] write       fd[0] read                 fd[1] write
                                             
   [         pipe       ]                       [         pipe        ]
                                             
            child                                        child
                                             
fd[0] write       fd[0] read                                fd[0] read


int filedes[2];

if (pipe(filedes) == -1) /* Create the pipe */
   errExit("pipe");

switch (fork()) { /* Create a child process */
 case -1:
   errExit("fork");

 case 0: /* Child */
   if (close(filedes[1]) == -1) // close unused 'write' end
     errExit("close");
   /* Child now reads from pipe */
   break;

 default: /* Parent */
   if (close(filedes[0]) == -1) // close unused 'read' end
     errExit("close");
   /* Parent now writes to pipe */
   break;
}


<needs-of-closing-for-reading-process>
If the reading process doesn't close the write end of the pipe, then, after the other process closes
its write descriptor, the reader won't see end-of-file, even after it has read all data from the
pipe. Instead, a read() would block waiting for data, because the kernel knows that there is still
at least one write descriptor open for the pipe. That this descriptor is held open by the reading
process itself is irrelevant; in theory, that process could still write to the pipe, even if it is
blocked trying to read. For example, the read() might be interrupted by a signal handler that writes
data to the pipe. (This is a realistic scenario, as we’ll see in Section 63.5.2.)


<broken-pipe>
When a process tries to write to a pipe for which no process has an open read descriptor, the kernel
sends the SIGPIPE signal to the writing process. By default, this signal kills a process. A process
can instead arrange to catch or ignore this signal, in which case the write() on the pipe fails with
the error EPIPE (broken pipe). Receiving the SIGPIPE signal or getting the EPIPE error is a useful
indication about the status of the pipe, and this is why unused read descriptors for the pipe should
be closed.


<needs-of-closing-for-writing-process>
If the writing process doesn't close the read end of the pipe, then, even after the other process
closes the read end of the pipe, the writing process will still be able to write to the pipe.
Eventually, the writing process will fill the pipe, and a further attempt to write will block
indefinitely.


<wary-of-deadlock>
If employing two pipes for bi-directional buffer, then we need to be wary of deadlocks that may
occur if both processes block while trying to read from empty pipes OR while trying to write to
pipes that are already full.


<pipe2-call>
Starting with kernel 2.6.27, Linux supports a new, nonstandard system call, pipe2(). This system
call performs the same task as pipe(), but supports an additional argument, flags, that can be used
to modify the behavior of the system call. Two flags are supported. The O_CLOEXEC flag causes the
kernel to enable the close-on-exec flag (FD_CLOEXEC) for the two new file descriptors. This flag is
useful for the same reasons as the open() O_CLOEXEC flag described in Section 4.3.1. The O_NONBLOCK
flag causes the kernel to mark both underlying open file descriptions as nonblocking, so that future
I/O operations will be nonblocking. This saves additional calls to fcntl() to achieve the same
result.


<pipe-for-unrelated-process>
There is an exception that pipes can be used to communicate only between related processes. Passing
a file descriptor over a UNIX domain socket (a technique that described in Section 61.13.3) makes it
possible to pass a file descriptor for a pipe to an unrelated process.


<second> to-create-pipe
popen() call which simplfies pipe creation, fork, reading/writing setting. However, need to set
problem to fork in the command line.

The C standard I/O library popen(3) makes it easy for the application programmer to open a pipe to
an external process.

#include <stdio.h>
FILE *popen(const char *command, const char *mode);
int pclose(FILE *stream);

The argument command must be a command that is acceptable to the UNIX shell. The second argument
mode must be the C string "r" for reading or "w" for writing.  No other combination, such as "w+",
is acceptable. 

(reading example)
FILE *p;
char cmd[1000];
/* argv[2] is fname */
sprintf(cmd,"grep 'Time has been updated to (Year:Month' %s | head -1",argv[2]);
p=popen(cmd,"r");
fgets(tmp,sizeof(tmp),p);
pclose(p);

After all, the reason that can use pipe between parent/child is that fds are shared.


={============================================================================
*kt_linux_core_102* ipc: way to check pipe fd and setup

[root@HUMAX 856]# ll /proc/846/fd
...
lr-x------    1 root     root          64 May  6 14:35 68 -> socket:[22102]=
lrwx------    1 root     root          64 May  6 14:45 69 -> socket:[22104]=
l-wx------    1 root     root          64 May  6 14:35 7 -> /var/opt-zinc-var/log/copper.log
lrwx------    1 root     root          64 May  6 14:35 70 -> /var/tmp/zmp-log
lrwx------    1 root     root          64 May  6 14:45 71 -> /var/tmp/zmp-log
lr-x------    1 root     root          64 May  6 14:45 72 -> pipe:[22129]| // note:
lrwx------    1 root     root          64 May  6 14:35 8 -> /dev/fusion0
lrwx------    1 root     root          64 May  6 14:35 9 -> /dev/nexus_proxy

[root@HUMAX 856]# lsof | grep 22129
856	/opt/zinc/bin/bronzemediad.oem	pipe:[22129]
1067	/run/youview/jail/daemons/linearsourced-S3kWYA/opt/zinc-trunk/bin/linearsourced	pipe:[22129]

This shows 'two' ends of pipe.


={============================================================================
*kt_linux_core_103* ipc: fifo

{fifo}
To solve this, fifo was introduced and is called 'named'-pipe since has path name. Means that it
is created in the filesystem as a file. Use usual read and write call. Fifo is either read-only or
write-only.

1. create a fifo using mkfifo call.
2. open a fifo for read or write using open call

<fifo-create>
#include <sys/stat.h>
int mkfifo(const char *pathname, mode_t mode);

<fifo-sync>
Therefore, by default, opening a FIFO for reading (the open() O_RDONLY flag) blocks until another
process opens the FIFO for writing (the open() O_WRONLY flag). Conversely, opening the FIFO for
writing blocks until another process opens the FIFO for reading.  In other words, opening a FIFO
synchronizes the reading and writing processes.

<note-from-nds-fusion-ipc>
FIFOs have certain natural limitations, they are unidirectional, and fragment large message sizes
with no built in support for reassembling the fragments. Furthermore FIFOs are limited in number due
to system resources. The key are 'fragment' and 'limit'-in-number.

Due to the limited number of FIFOs it was determined that there would be one
control FIFO per server (to establish communication from clients) and one pair of unidirectional
FIFOs for every pair of server and client instances (note that there is one instance of a client in
every process that uses that client). This is illustrated below.

server                  client a
- control pipe          -> and <-

                        client b
                        -> and <-

note: KT. do not match with this pic?

To cope with fragmentation and the fact that there may be several interfaces being used on a single
client instance (and multiple components using that client instance in a single process) a protocol
was designed as described below. This protocol is used in both FIFO and TCP/IP IPC communication.


{summary}
When writes, if there is no reading process, broken-pipe. When reads, if there
is no writing process, blocked.


={============================================================================
*kt_linux_core_104* ipc: nonblocking read() and write() on pipes and fifo

LPI 44.10

Summarizes the operation of read() for pipes and FIFOs, and includes the effect of the O_NONBLOCK
flag.

<on-read>
The only difference between blocking and nonblocking reads occurs when no data is present and the
write end is open. In this case, a normal read() blocks, while a nonblocking read() fails with the
error EAGAIN.

Table 44-2: Semantics of 'read'ing n bytes from a pipe or FIFO containing p bytes

O_NONBLOCK  | Data bytes available in pipe or FIFO (p)
enabled?    | p = 0, write end open    | p = 0, write end closed  | p < n        | p >= n
----------------------------------------------------------------------------------------------- 
No          | block                    | return 0 (EOF)           | read p bytes | read n bytes
Yes         | fail (EAGAIN)            | return 0 (EOF)           | read p bytes | read n bytes

note: read(n) gets n or p


<on-write>
The impact of the O_NONBLOCK flag when writing to a pipe or FIFO is made complex by interactions
with the PIPE_BUF limit. The write() behavior is summarized in Table 44-3.

Table 44-3: Semantics of 'write'ing n bytes from a pipe or FIFO

O_NONBLOCK  | read end open
enabled?    | n <= PIFE_BUF
----------------------------------------------------------------------------------------------- 
No          | Atomically write n bytes; may block until sufficient data is read for write() to
            | be performed
Yes         | If sufficient space is available to immediately write n bytes, then write()
            | succeeds atomically; otherwise, it fails (EAGAIN)

note: if there is no space to write, then either blocks or EAGAIN.

O_NONBLOCK  | read end open
enabled?    | n > PIPE_BUF
----------------------------------------------------------------------------------------------- 
No          | Write n bytes; may block until sufficient data read for write() to complete; 
            | data may be interleaved with writes by other processes
Yes         | If there is sufficient space to immediately write some bytes, then write between 1 
            | and n bytes (which may be interleaved with data written by other processes);
            | otherwise, write() fails (EAGAIN)

            | read end closed
              SIGPIPE + EPIPE

The O_NONBLOCK flag causes a write() on a pipe or FIFO to fail (with the error EAGAIN) in any case
where data can't be transferred immediately. 

This means that if we are writing up to PIPE_BUF bytes, then the write() will fail if there is not
sufficient space in the pipe or FIFO, because the kernel can't complete the operation immediately
and can't perform a partial write, since that would break the requirement that writes of up to
PIPE_BUF bytes are 'atomic'.

When writing more than PIPE_BUF bytes at a time, a write is 'not' required to be atomic. For this
reason, write() transfers as many bytes as possible (a partial write) to fill up the pipe or FIFO.
In this case, the return value from write() is the number of bytes actually transferred, and the
'caller' 'must' retry later in order to write the remaining bytes. 

However, if the pipe or FIFO is full, so that not even one byte can be transferred, then write()
fails with the error EAGAIN.

Although blocks of data of any size can be written to a pipe, only writes that do not exceed
PIPE_BUF bytes are guaranteed to be atomic. 


={============================================================================
*kt_linux_core_105* ipc: which one to use

When performing interprocess synchronization, our choice of facility is typically determined by the
functional requirements. When coordinating access to a file, file record locking is usually the best
choice. Semaphores are often the better choice for coordinating access to other types of shared
resource.

{semaphores-versus-pthreads-mutexes}

<1>
Unlike mutex (this mean condition?), semaphore does not get lost when there is no waiting one.

<2>
POSIX semaphores and Pthreads mutexes can both be used to synchronize the actions of threads within
the same process, and their performance is [similar]. 

However, mutexes are usually preferable, because the [ownership] property of mutexes enforces good
structuring of code; only the thread that locks a mutex can unlock it. By contrast, one thread can
increment a semaphore that was decremented by another thread. This flexibility can lead to poorly
structured synchronization designs. For this reason, semaphores are sometimes referred to as the
"gotos" of concurrent programming.

There is one circumstance in which mutexes can't be used in a multithreaded application and
semaphores may therefore be preferable. Because it is async-signalsafe. See
{async-signal-safe-function}. The sem_post() function can be used from within a signal handler to
synchronize with another thread. This is not possible with mutexes, because the Pthreads functions
for operating on mutexes are not asyncsignal-safe. 

However, because it is usually preferable to deal with asynchronous signals by accepting them using
sigwaitinfo() (or similar), rather than using signal handlers (see Section 33.2.4), this advantage
of semaphores over mutexes is seldom required.

<3>
From 30.1.3. Peformance of mutex in ref-LPI. The problem with file locks and semaphores is that they
always require a system call for the lock and unlock operations, and each system call has a small
but appreciable, cost (Section 3.1). By contrast, mutexes are implemented using atomic
machine-language operations; performed on memory locations visible to all threads and require system
calls only in case of lock contention.


On Linux, mutexes are implemented using futexes (an acronym derived from fast user space mutexes),
   and lock contentions are dealt with using the futex() system call. We donât describe futexes in
   this book (they are not intended for direct use in user-space applications), but details can be
   found in [Drepper, 2004 (a)], which also describes how mutexes are implemented using futexes.
   [Franke et al., 2002] is a (now outdated) paper written by the developers of futexes, which
   describes the early futex implementation and looks at the performance gains derived from futexes.


# ============================================================================
#{
={============================================================================
*kt_linux_core_200* ipc: sync: semaphore

{what-is-semaphore}
note: This can be used for process or thread but it is expensive for thread.

System V semaphores are not used to transfer data between processes. Instead, they allow processes
to synchronize their actions. A semaphore is a 'kernel'-maintained 'integer' whose value is never
permitted to fall below 0. A process can decrease or increase the value of a semaphore.  If an
attempt is made to decrease the value of the semaphore below 0, then the kernel blocks the operation
until the semaphore's value increases to a level that permits the operation to be performed. 

The meaning of a semaphore is determined by the application. A process decrements a semaphore from
say, 1 to 0 in order to reserve exclusive access to some 'shared' resource, and after completing work
on the resource, increments the semaphore so that the shared resource is released for use by some
other process. The use of a binary semaphore-a semaphore whose value is limited to 0 or 1-is common.

However, an application that deals with multiple instances of a shared resource would employ a
semaphore whose maximum value equals the 'number' of shared resources. Linux provides both System V
semaphores and POSIX semaphores, which have essentially similar functionality.

1. Setting the semaphore to an absolute value;
2. Adding a number to the current value of the semaphore;            // give
3. Subtracting a number from the current value of the semaphore; and // take
4. Waiting for the semaphore value to be equal to 0.

The last two of these operations may cause the calling process to block. A semaphore has no meaning
in and of itself. Its meaning is determined only by the associations given to it by the processes
using the semaphore.

However, System V semaphores are rendered unusually complex by the fact that they are allocated in
groups called semaphore sets. So move to POSIX semaphore.


{posix-semaphore}
SUSv3 specifies two types of POSIX semaphores:

1. Named semaphores: This type of semaphore has a name. By calling sem_open() with the same name
'unrelated' processes can access the same semaphore.

2. Unnamed semaphores: This type of semaphore doesn't have a name; instead, it resides at an
agreed-upon location in 'memory'. Unnamed semaphores can be shared between processes or between a
group of 'threads'. When shared between processes, the semaphore must reside in a region of (System V,
POSIX, or mmap()) shared memory. When shared between threads, the semaphore may reside in an
area of memory shared by the threads (e.g., on the heap or in a global variable).

POSIX semaphores operate in a manner similar to System V semaphores; that is, a POSIX semaphore is
an integer whose value is not permitted to fall below 0. If a process attempts to decrease the value
of a semaphore below 0(take), then, depending on the function used, the call either blocks or fails
with an error indicating that the operation was not currently possible.

Some systems don't provide a full implementation of POSIX semaphores. A typical restriction is that
only unnamed thread-shared semaphores are supported. That was the situation on Linux 2.4; 

<linux-2-6-support>
With Linux 2.6 and a glibc that provides NPTL, a full implementation of POSIX semaphores is
available.

<named-semaphore>
To work with a named semaphore, we employ the following functions:

1. The sem_open() function opens or creates a semaphore, initializes the semaphore if it is created
by the call, and returns a handle for use in later calls.

#include <fcntl.h> /* Defines O_* constants */
#include <sys/stat.h> /* Defines mode constants */
#include <semaphore.h>

sem_t *sem_open(const char *name, int oflag, ...  /* mode_t mode, unsigned int value */ );

Returns pointer to semaphore on success, or SEM_FAILED on error

2. The sem_post(sem) and sem_wait(sem) functions respectively increment and decrement a semaphore's
value. give and take

3. The sem_getvalue() function retrieves a semaphore's current value.

4. The sem_close() function removes the calling processâs association with a semaphore that it
previously opened.

5. The sem_unlink() function removes a semaphore name and marks the semaphore for deletion when all
processes have closed it.

<named-semaphore-on-linux>
SUSv3 doesn't specify how named semaphores are to be implemented. On Linux, they are created as
small POSIX shared memory objects with names of the form sem.name, in a dedicated tmpfs file system
(Section 14.10) mounted under the directory /dev/shm. This file system has 'kernel'-persistence-the
semaphore objects that it contains will persist, even if no process currently has them open, but
they will be lost if the system is shut down.

<unnamed-semaphore>
The semaphore is made available to the processes or threads that use it by placing it in an area of
memory that they share. Operations on unnamed semaphores use the same functions; sem_wait(),
sem_post(), sem_getvalue(), and so on that are used to operate on named semaphores.

In addition, two further functions are required:

The sem_init() function initializes a semaphore and informs the system of whether the semaphore will
be shared between processes or between the threads of a single process.

The sem_destroy(sem) function destroys a semaphore. These functions should not be used with named
semaphores.

<when-useful-to-use-unnamed>
1. A semaphore that is shared between 'threads' doesn't need a name. Making an unnamed semaphore a
shared (global or heap) variable automatically makes it accessible to all threads.

2. A semaphore that is being shared between 'related' processes doesn't need a name. If a parent
process allocates an unnamed semaphore in a region of shared memory (e.g., a shared anonymous
mapping), then a child automatically inherits the mapping and thus the semaphore as part of the
operation of fork().

3. If we are building a dynamic data structure (e.g., a binary tree), each of whose items requires
an associated semaphore, then the simplest approach is to allocate an unnamed semaphore within each
item. Opening a named semaphore for each item would require us to design a convention for generating
a (unique) semaphore name for each item and to manage those names (e.g., unlinking them when they
are no longer required). note: real example? useful?


={============================================================================
*kt_linux_core_201* sync: mutex

LPI-30

The term critical section is used to refer to a section of 'code' that accesses
a shared resource and whose execution should be 'atomic'; that is, its execution
should not be interrupted by another thread that simultaneously accesses the
same shared resource.

In the below problem, two threads shares the same code. It is about critial
region but what is really protected is the 'data' being manipulated within the
critical region.


Problem
-------------------------------
The kind of problems that can occur when shared resources are not accessed
atomically.

This program creates two threads, each of which executes the same function. The
function executes a loop that repeatedly increments a global variable, glob, by
copying glob into the local variable loc.


Since loc is an automatic variable allocated on the per-thread stack, each
thread has its 'own' copy of this variable.

The vagaries of the kernel's CPU scheduling decisions:
The scheduler time slice for thread 1 expires, and thread 2 commences execution.

In complex programs, this 'nondeterministic' behavior means that such errors may
occur only rarely, be hard to reproduce, and therefore be difficult to find.

static int glob = 0;

staic void threadFunc()
{
  int loc, j;

  for(j=0; j < loops; j++)
  {
    loc = glob;
    loc++;
    glob = loc;
  }
}


Solution?
-------------------------------
It might seem that we could eliminate the problem by replacing the three statements
inside the for loop with a single statement. Looks atomic?

staic void threadFunc()
{
  int loc, j;

  for(j=0; j < loops; j++)
  {
    glob++; /* or: ++glob; */
  }
}

However, on many hardware architectures (e.g., RISC architectures), the compiler
would still need to convert this single statement into machine code whose steps
are equivalent to the three statements inside the loop. In other words, despite
its simple appearance, even a C increment operator may 'not' be atomic.


Mutex
-------------------------------
More generally, mutexes can be used to ensure atomic access to any shared
resource, but protecting shared variables is the most common use.


Mutex Ownership
-------------------------------
A mutex has two states: locked and unlocked. At any moment, *at-most* one thread
may hold the lock on a mutex. Attempting to lock a mutex that is already locked
either blocks or fails with an error depending on the method used to place the
lock.

When a thread locks a mutex, it becomes the 'owner' of that mutex. 'only' the
mutex owner can unlock the mutex. Because of this ownership property, the terms
'acquire' and 'release' are sometimes used synonymously for lock and unlock.


Mutex protocol
-------------------------------
Each thread employs the following protocol for accessing a resource:
1. lock the mutex for the shared resource;
2. access the shared resource; and
3. unlock the mutex.


Cooperative lock
-------------------------------
This means that mutex locking is advisory, rather than mandatory; nothing can
prevent one thread from manipulating the data without first obtaining the mutex.
For example, threads not participating a mutex circle can.


Statically Allocated Mutexes
-------------------------------
A mutex can either be allocated as a static variable or be created dynamically
at run time. Before it can be used, a mutex must always be initialized.

pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;


Dynamically Initializing a Mutex
-------------------------------
See LPI-30.1.5, 6


Locking and Unlocking a Mutex
-------------------------------
After initialization, a mutex is unlocked.

#include <pthread.h>

int pthread_mutex_lock(pthread_mutex_t *mutex);
int pthread_mutex_unlock(pthread_mutex_t *mutex);

Both return 0 on success, or a positive error number on error

If the mutex is currently locked by another thread, then pthread_mutex_lock()
  blocks 'until' the mutex is unlocked, at which point it locks the mutex and
  'returns'.

<indeterminate>
If more than one other thread is waiting to acquire the mutex unlocked by a call
to pthread_mutex_unlock(), it is 'indeterminate' which thread will succeed in
acquiring it.


Mutex Types
-------------------------------
o May not lock the same mutex twice. If do, two may happen for 'default'
type of mutex: mutex-'deadlock' or EDEADLK for error check type. On Linux,
     deadlock by default from {ref-LPI}. Why deadlock?  Because blocked trying
     to lock a mutex that it already owns.

o Unlocking a mutex that is not owned(locked) or that is locked by another
thread 'undefined' result. note: ownership matters when unlock and semaphore do
not have ownership.

Precisely what happens in each of these cases depends on the type of the mutex.


PTHREAD_MUTEX_NORMAL

(Self-)deadlock detection is not provided for this type of mutex. If a thread
tries to lock a mutex that it has already locked, then 'deadlock' results.
Unlocking a mutex that is not locked or that is locked by another thread
produces 'undefined' results. 

On Linux, both of these operations 'succeed' for this mutex type and this is
PTHREAD_MUTEX_DEFAULT


PTHREAD_MUTEX_ERRORCHECK

All three of the above scenarios cause the relevant Pthreads function to return
an error than blocking or deadlock but 'slower' than a normal so debugging
purpose.


PTHREAD_MUTEX_RECURSIVE

A recursive mutex maintains the concept of a *lock-count* When a thread first
acquires the mutex, the lock count is set to 1. Each subsequent lock operation
by the same thread increments the lock count, and each unlock operation
decrements the count. The mutex is released (i.e., made available for other
    threads to acquire) only when the lock count falls to 0. 

When is it useful?

Still have ownership notion but there is no mutex deadlock or undefined result
as NOMAL type has.


Locking Variants
-------------------------------
Two variants of the pthread_mutex_lock() function which are much less frequently
used:

The pthread_mutex_trylock() function is the same as pthread_mutex_lock(), except
that if the mutex is currently locked, pthread_mutex_trylock() fails, returning
the error EBUSY.

The pthread_mutex_timedlock() function is the same as pthread_mutex_lock(),
    except that the caller can specify an additional argument, abstime, that
    places a limit on the time that the thread will sleep while waiting to
    acquire the mutex. If the time interval specified by its abstime argument
    expires without the caller becoming the owner of the mutex,
    pthread_mutex_timedlock() returns the error ETIMEDOUT.

note: Why less used?

In most well-designed applications, a thread should hold a mutex for only a
short time, so that other threads are not prevented from executing in parallel. 

A thread that uses pthread_mutex_trylock() to periodically poll the mutex to see
if it can be locked risks being starved of access to the mutex while other
queued threads are successively granted to the mutex via pthread_mutex_lock().


Performance of Mutexes
-------------------------------
This is relatively cheap and the performance impact of using a mutex is not
significant in most applications.

The problem with file locks and semaphores is that they always require a system
call for the lock and unlock operations, and each system call has a small, but
appreciable, cost (Section 3.1). By contrast, mutexes are implemented using
atomic machine-language operations (performed on memory locations visible to all
    threads) and require system calls only in case of lock contention.


On Linux, mutexes are implemented using futexes (derived from fast user space
    mutexes), and lock contentions are dealt with using the futex() system call.

We don't describe futexes in this book (they are not intended for direct use in
    user-space applications), but details can be found in [Drepper, 2004 (a)],
   which also describes how mutexes are implemented using futexes. [Franke et
   al., 2002] is a (now outdated) paper written by the developers of futexes,
   which describes the early futex implementation and looks at the performance
   gains derived from futexes.


={============================================================================
*kt_linux_core_202* sync: deadlock

Deadlocks. LPI-30.1.4
-------------------------------
A deadlock is a situation where more than one thread is locking the same set of
mutexes and will remain blocked indefinitely. Waiting for something never
happens. Unrequited love?

Thread A                            Thread B
1. pthread_mutex_lock(mutex1);      1. pthread_mutex_lock(mutex2);
2. pthread_mutex_lock(mutex2);      2. pthread_mutex_lock(mutex1);
blocks                              blocks


Deadlock Examples
-------------------------------
<1> from pipe
If employing this bidirectional communication using two pipes, then we need to
be wary of deadlocks that may occur if both processes block while trying to read
from empty pipes or while trying to write to pipes that are already full.

<2> from fifo.
use-different-fifo-for-read-and-write

Process X                              Process Y
1. Open FIFO A for reading(blocks)     1. Open FIFO A for reading(blocks)
2. Open FIFO B for writing             2. Open FIFO B for writing

When open a fifo to read which is not opend to write but there is no writing
process then reading process is blocked. Therefore, if parent and child opens
different fifos to read, deadlock happens.

use-same-fifo-for-read-and-write-but-the-same-order

Process X                              Process Y
1. Open FIFO A for reading(blocks)     1. Open FIFO B for reading(blocks)
2. Open FIFO B for writing             2. Open FIFO A for writing

The two processes shown in Figure 44-8 are deadlocked. Each process is blocked
waiting to open a FIFO for reading. This blocking would not happen if each
process could perform its second step (opening the other FIFO for writing). So
change order or use non-blocking call.

<3>
Message queues have a limited capacity. This has the potential to cause a couple of problems. One of
these is that multiple simultaneous clients could fill the message queue, resulting in a deadlock
situation, where no new client requests can be submitted and the server is blocked from writing any
responses.

<4> from mutex-deadlock
Locking. If try to lock what is already locked by self, two may happen for
'default' type of mutex: mutex-deadlock or EDEADLK. On Linux, deadlock by
default from {ref-LPI}. Why deadlock? Because blocked trying to lock a mutex
that it already owns. This could happen when exception is raised between lock
and unlock pair.

<5> from lock-free-queue
Used the timed_wait() instead of the simpler wait() to solve a possible deadlock
when Produce() is called between line A and line B in Listing One. Then wait()
    will miss the notify_one() call and have to wait for the next produced
    element to wake up. 'if' this element never comes (no more produced elements
            or if the Produce() call actually waits for Consume() to return),
    there's a deadlock.

<6> mutex-deadlock

Thead A             Thread B
lock(mutex1)        lock(mutex2)
lock(mutex1)        lock(mutex2)


Deadlock Avoid
-------------------------------

o Mutex hierarchy or order

The simplest way to avoid such deadlocks is to define a mutex hierarchy. When
threads can lock the same set of mutexes, they should always lock them in the
same order. Less flexible.

o Try and back off
An alternative strategy that is 'less' frequently used is "try, and then back
off." See above on why try_lock is less used. If try_lock fails, release 'all'
and try again later. This is less efficient than hierarchy approach but can be
more 'flexible' since no need rigid hierarchy.


={============================================================================
*kt_linux_core_202* sync: why condition variable using cons and prod problem

UNP-XX

Consumer and Producer: 01
-------------------------------
Multiple producer(writing) and one consumer(reading). Once writing finishs,
consumer get started. No sync between producer and consumer and only sync for
  producers. So use mutex. 

No sync for reading? NO for this example since reaading starts after writing.
However, need to sync reading if want to have right result while writing
happens.  

<ex>

#include <stdio.h>
#include <pthread.h>
#include <sys/errno.h>

#define MAXNITEMS     1000000
#define MAXNTHREADS   100

#define min(a,b) ((a) < (b) ? (a) : (b))
#define max(a,b) ((a) > (b) ? (a) : (b))

void 
Pthread_create
(pthread_t* tid, const pthread_attr_t* attr, void *(*func)(void*), void*arg)
{
  int n;

  if(( n = pthread_create( tid, attr, func, arg )) == 0 )
    return;

  errno = n;
  fprintf( stderr, "pthread_create error(%d)", n );
}

// shared by all threads
int nitems;

struct {
  pthread_mutex_t mutex;
  int buff[MAXNITEMS];
  int nput;                     // next index to write
  int nval;                     // next val to write
} shared = { PTHREAD_MUTEX_INITIALIZER };

void *produce(void *), *consume(void *);

int main( int argc, char** argv )
{
  int i, nthreads, count[MAXNTHREADS]={0};
  pthread_t tid_produce[MAXNTHREADS]={0}, tid_consume;

  if( argc != 3 )
  {
    fprintf( stderr, "usuage: prodcons2 <#items> <#threads>\n");
    exit(1);
  }

  nitems = min( atoi( argv[1] ), MAXNITEMS );
  nthreads = min( atoi( argv[2] ), MAXNTHREADS );

  // start all producer threads
  for( i=0; i < nthreads; ++i )
  {
    count[i] = 0;
    Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
  }

  // wait for all the producer threads
  for( i=0; i < nthreads; ++i ) 
  {
    pthread_join( tid_produce[i], NULL );
    printf("tid[%d] count[%d] = %d\n", tid_produce[i], i, count[i] );
  }

  // start, then wait for the consumer thread
  Pthread_create(&tid_consume, NULL, consume, NULL );
  pthread_join(tid_consume, NULL );

  exit(0);
}

void* produce(void* arg)
{
  printf("run tid[%d] \n", pthread_self());

  for(;;) 
  {
    pthread_mutex_lock( &shared.mutex );

    // nitems is the max num of shared.buff. When buff is full, we are done.
    if( shared.nput >= nitems )
    {
      printf("done tid[%d] \n", pthread_self());
      pthread_mutex_unlock(&shared.mutex);
      return NULL;
    }

    shared.buff[ shared.nput ] = shared.nval;
    shared.nput++;
    shared.nval++;

    pthread_mutex_unlock( &shared.mutex );

    // inc(count[i]) is the num of items that this thread wrote. Why not in
    // the critical section? Because each thread has its own
    *((int*)arg) += 1;
  }
}

void* consume(void* arg)
{
  int i;

  // see use of global vars; nitems and shared.buff
  for( i=0; i < nitems; i++ )
  {
    if( shared.buff[i] != i )
      printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
  }

  printf("consume done\n" );

  return NULL;
}


$ ./a.out 1000000 5
run tid[-1252185280] 
run tid[-1243792576] 
run tid[-1235399872] 
run tid[-1227007168] 
run tid[-1218614464] 
done tid[-1252185280] 
done tid[-1243792576] 
done tid[-1227007168] 
done tid[-1218614464] 
tid[-1218614464] count[0] = 198088
tid[-1227007168] count[1] = 220562
done tid[-1235399872] 
tid[-1235399872] count[2] = 254805
tid[-1243792576] count[3] = 162778
tid[-1252185280] count[4] = 163767
consume done


Consumer and Producer: 02
-------------------------------
Now consumer runs at the same time and runs when there is data to read. All is
accessing the same data and are in the 'same' mutex group. Problem is that
when consumer has a lock and wakes up, consumer do polling or spinning to
check data.

note:
The consumer and producer has the equal opportunity to run since it is
'indeterminate' which thread will succeed in acquiring a lock.


<ex>
Uses shared mutex between producers and consumer. If items to read are ready
then returns. Otherwise, do loops. The changes from the previous is:

int main()
{
  // ...

  // start all producer threads
  for( i=0; i < nthreads; ++i )
  {
    count[i] = 0;
    Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
  }

  // do not waits for producers to finish off and starts consumer right away
  Pthread_create(&tid_consume, NULL, consume, NULL );

  // ...
}

void consume_wait(int i)
{
  for(;;) 
  {
    pthread_mutex_lock(&shared.mutex);

    if( i < shared.nput ) {
      pthread_mutex_unlock(&shared.mutex);

      // item is ready
      return; 
    }

    // item is not ready so continue waiting
    pthread_mutex_unlock(&shared.mutex);
  }
}

void* consume(void* arg)
{
  // reading index
  int i;

  // see use of global vars; nitems and shared.buff
  for( i=0; i < nitems; i++ )
  { >
    // polling until item i is ready
    consume_wait(i);

    if( shared.buff[i] != i )
      printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
  }

  printf("con: done\n" );

  return NULL;
}


Consumer and Producer: 03
-------------------------------
For producers, do not need to wait since when have a lock meaning that it is
time for it to do write what it need to do is to write blindly. However, for
consumers, it is not the same as producers since it have to chceck if items are
available to read to to its work.

That is why it has polling to check but The polling is a waste of cpu time. How
to solve? A mutex is for locking and a cond-var is for waiting. The mutex
provides mutual exclusion for accessing the shared variable, while the condition
variable is used to signal changes in the variable's state.

note:
-. The second example has reading and writing in the same mutex group and all
competes with others. So reading and writing cannot run at the same time.

-. This example uses two mutex group: one for writing and one for reading. This
allows two things:

1. More chances to run reading and writing at the same time. Still possible to
write when condition is signaled.

2. Consumer wakes up and competes with producers only when there are items to
process. 

After all, codition variable is to reduce such waste of polling and blocking.

<ex>

// note: previously
// 
// // shared by all threads
// int nitems;
// 
// struct {
//   pthread_mutex_t mutex;
//   int buff[MAXNITEMS];
//   int nput;                     // next index to write
//   int nval;                     // next val to write
// } shared = { PTHREAD_MUTEX_INITIALIZER };

int nitems;
int buff[MAXNITEMS];

struct {
  pthread_mutex_t mutex;
  int nput;                     // next index to write
  int nval;                     // next val to write
} put = { PTHREAD_MUTEX_INITIALIZER };

struct {
  pthread_mutex_t mutex;
  pthread_cond_t cond;

  // note:
  int ready;

} nready = { PTHREAD_MUTEX_INITIALIZER, PTHREAD_COND_INITIALIZER };

void* produce(void* arg)
{
  printf("pro: run tid[%d] \n", pthread_self());

  for(;;) 
  {
    pthread_mutex_lock( &put.mutex );

    // buff is full, we are done.
    if( put.nput >= nitems )
    {
      printf("pro: no more. done tid[%d] \n", pthread_self());
      pthread_mutex_unlock(&put.mutex);
      return NULL;
    }

    buff[ put.nput ] = put.nval;
    put.nput++;
    put.nval++;

    pthread_mutex_unlock( &put.mutex );

    // note: changed to use cond-var 
    // {
    pthread_mutex_lock( &nready.mutex );

    // "ready" is 0 and means there is no items to read. Now there is one item
    // produced and signal cond-var. So "ready" is the number of items
    // avaialbe to read.

    if( nready.ready == 0 )
      pthread_cond_signal( &nready.cond );

    nready.ready++;

    pthread_mutex_unlock( &nready.mutex );
    // }

    // inc(count[i]) is the num of items that this thread wrote. Why not in
    // the critical section? Because each thread has its own
    *((int*)arg) += 1;
  }
}

void* consume(void* arg)
{
  int i;

  // see use of global vars; nitems and shared.buff
  for( i=0; i < nitems; i++ )
  { >
    // receieve cond-var
    pthread_mutex_lock( &nready.mutex );

    // Always test the condition again when wakes up because
    // *spurious-wakeups* can occur. Unlock "nready.ready" and wait. 

    while( nready.ready == 0 ) 
      pthread_cond_wait( &nready.cond, &nready.mutex);

    // When returns from pthread_cond_wait, "nready.ready" is locked
    // automatically and there is a item to read.
    
    nready.ready--;

    pthread_mutex_unlock( &nready.mutex );

    if( buff[i] != i )
      printf("con: err: buff[%d] = %d\n", i, buff[i] );
  }

  printf("con: done\n" );

  return NULL;
}


Q: Is it possible to implement this using two mutex than using a mutex and a
cond-var?

Q: If change producer's handling of ready like this, is it better?

{
    // note: changed to use cond-var 
    // {
    pthread_mutex_lock( &nready.mutex );

    // "ready" is 0 and means there is no items to read. Now there is one item
    // produced and signal cond-var. So "ready" is the number of items
    // avaialbe to read.

    if( nready.ready == 0 )
    {
      nready.ready++;
      pthread_cond_signal( &nready.cond );
    }

    pthread_mutex_unlock( &nready.mutex );
    // }
}

Q: Better if consumer reads all when has a lock?


={============================================================================
*kt_linux_core_202* sync-cond: condition variable 

<cond-wait>
// This is blocking on condition and ALWAYS has an associated mutex. Why?
// because both producer and consumer accesses to the shared state variable
// which is linked to that condition so should be synced. In other words, there
// is a natural association of a muext with a condition varaible.
//
// pthread_cond_wait( &nready.cond, &nready.mutex );
//
// Do three things: unlock a mutex, block a calling thread until signaled, and
// relock mutex when signaled. 
//
// A condition variable holds no state information. It is simply a mechanism for
// communicating information about the application's state.

int pthread_cond_wait( pthread_cond_t *cptr, pthread_mutex_t* mptr);

<cond-can-be-lost>
// Guaranteede that at least one of the blocked thread is woken up. If no thread
// is waiting, the signal is lost since condition variable holds no state
// information.

int pthread_cond_signal( pthread_cond_t *cptr );
int pthread_cond_broadcast( pthread_cond_t *cptr );
int pthread_cond_timedwait( ... );

The {ref-LPI} said that _signal is for the case where all waiting threads do the
  same task since it do not care which should woken up and _broadcast is for
  case where waiting threads do different task.  Real examples?


{the-order-of-call}

{ref-LPI} uses different call order for producer as below because {ref-UNP} said
that use _signal before unlock can cause {mutex-deadlock} or {lock-conflict}.
Therefore, POSIX recommends the followings and {ref-LPI} said this may yieid
better performance in 30.2.2:

<approach-one>
pthread_mutex_lock( &nready.mutex );
if( nready.ready == 0 )
   pthread_cond_signal( &nready.cond );

nready.ready++;

pthread_mutex_unlock( &nready.mutex );

<approach-two>
pthread_mutex_lock( &nready.mutex );

nready.ready++;

pthread_mutex_unlock( &nready.mutex );

if( nready.ready == 0 )
 pthread_cond_signal( &nready.cond );


{check-on-predicate}

Each condition variable has an associated predicate involving one or more shared
variables. In this example, nready.ready == 0 is the predicate.

This demonstrates a general design principle: a pthread_cond_wait() call must be
governed by a while loop rather than an if statement. This is so because, on
return from pthread_cond_wait(), there are no guarantees about the state of the
predicate; therefore, we should immediately recheck the predicate and resume
sleeping if it is not in the desired state. [KT] When calls cond_wait, the
associated lock is released so no gurantee that this predicate is the same when
wakes up.

So use [while-loop] always to test the condition again when wakes up because
spurious wakeups can occur for following reasons:

while( nready.ready == 0 )
	pthread_cond_wait( &nready.cond, &nready.mutex );

-. Other threads may be woken up first. Perhaps several threads were waiting to
acquire the mutex associated with the condition variable. Even if the thread
that signaled the mutex set the predicate to the desired state, it is still
possible that another thread might acquire the mutex first and change the state
of the associated shared variable(s), and thus the state of the predicate.

-. Designing for "loose" predicates may be simpler. Sometimes, it is easier to
design applications based on condition variables that indicate possibility
rather than certainty. In other words, signaling a condition variable would mean
"there may be something" for the signaled thread to do rather than "there is
something" to do. Using this approach, the condition variable can be signaled
based on approximations of the predicate's state, and the signaled thread can
ascertain if there really is something to do by rechecking the predicate.

-. Spurious wake-ups can occur. On some implementations, a thread waiting on a
condition variable may be woken up even though no other thread actually signaled
the condition variable. Such spurious wake-ups are a (rare) consequence of the
techniques required for efficient implementation on some multiprocessor systems,
           and are explicitly permitted by SUSv3.


={============================================================================
*kt_linux_core_202* sync-cond: example

typedef struct
{
  GstNexusMgr* mgr;

  GstNexusCallbackId callback_id;

  int reason;

  /* The members below are specific to callback_id. */
  int playpump_num;
  EXUS_Callback delegate_callback;
  void* delegate_context;
} GstNexusMgrCallbackInfo;


typedef struct
{
  GstNexusMgrCallbackInfo audio_buffer_available;
  GstNexusMgrCallbackInfo video_buffer_available;
  GstNexusMgrCallbackInfo audio_first_pts;
  GstNexusMgrCallbackInfo video_first_pts;
  GstNexusMgrCallbackInfo audio_error;
  GstNexusMgrCallbackInfo video_error;
} GstNexusMgrCallbackContexts;


// callback from hardware
static void gst_nexus_mgr_generic_1st_level_cb(void* context, int reason)
{
  /* ctx points to a member of mgr->callback_contexts, which we can read
   * but not write from this thread.
   */
  const GstNexusMgrCallbackInfo* ctx = context;

  GstNexusMgrCallbackInfo* posted_cb = g_malloc(sizeof(GstNexusMgrCallbackInfo));
  memcpy(posted_cb, ctx, sizeof(GstNexusMgrCallbackInfo));
  posted_cb->reason = reason;

  gst_nexus_mgr_enqueue_callback(mgr, posted_cb);

  /* mgr->callback_cond is associated with mgr->resource_lock rather than
   * with mgr->pending_callbacks_mutex. We are not allowed to lock
   * mgr->resource_lock in 1st level callbacks. Fortunately, signalling
   * a condition variable doesn't require locking the corresponding mutex.
   */
  g_cond_signal(&mgr->callback_cond);
}


note: is it going to be a double lock on the same mutex?

static gpointer mgr_callback_thread(gpointer context)
{
  GstNexusMgr* mgr = context;

  g_mutex_lock(&mgr->resource_lock);

  while(!mgr->callback_thread_exiting) {

    // deque cb item
    GstNexusMgrCallbackInfo* cb = mgr_dequeue_callback(mgr);

    // no item in the queue to process. wait and release a lock
    if (!cb){
      g_cond_wait(&mgr->callback_cond, &mgr->resource_lock);
      continue;
    }

    // use cb item
    mgr_2nd_level_cb_table[cb->callback_id](cb);
    g_free(cb);
  }

  g_mutex_unlock(&mgr->resource_lock);

  return NULL;
}


={============================================================================
*kt_linux_core_202* sync-cond: example use single lock for api and callback

This uses a single mutex, managerMutex, to guard access to APIs and callbacks.

void XX::API1(...)
{
    boost::unique_lock<boost::mutex> statusLock(managerMutex);

    if(activePlayer)
    {
        // Suspend player and wait for StatusEvent saying MR is stopped
        activePlayer->suspend();
        managerCondition.wait(statusLock);
    }

    mediaRouter->addListener(dispatcher, shared_from_this());
}

void XX::API2(...)
{
    const boost::unique_lock<boost::mutex> statusLock(managerMutex);
    ...
}

void XX::CALLBACK(const EventValue::Enum statusEvent)
{
    if(statusEvent == StatusEventValue::stopped)
    {
        boost::unique_lock<boost::mutex> statusLock(managerMutex);
        managerCondition.notify_one();
    }
}


={============================================================================
*kt_linux_core_202* sync: between processes

As with semaphore, mutex and cond-var uses 'global' structure and if these are
shared in a shared memory, these can be used between processes. If not, it is
for threads in a process.

In {ref-LPI} p881, process-shared mutexes and condition variables. They are
'not' available on all UNIX systems, and so are not commonly employed for
process synchronization.

note: As said, if share mutex structure in shared memory, then what is
process-shared mutex?



{implicit-sync}
This is synchronization handled by kernel not by applicaion. Such as pipe and
message-q. For example,

grep pattern chapters.* | wc -l

Writing by producer and reading by consumer are handled by kernel. Does it mean
that grep and wc runs at the same time? not one after one.


==============================================================================
*kt_linux_core_220*	sync: read-write lock

From {ref-UNP} but no such a thing in {ref-LPI}, so may be old way but surely in posix but may be
different to this since this lock is before posix standard. See {ref-UNP} note.

To distinguish between obtaining the read-write lock for reading and for writing. The rules:

1. Any number of threads can hold a given read-write lock for reading as long as no threads holds
the the lock for writing.

2. A read-write lock can be allocated for writing only if no thread hold the lock for reading or
writing.

Stated another way, any threads can have read access to a data as long as no thread is modifying
that. A data can be modified only if no other thread is reading or modifying the data.

In application, the data is read more often than the data is modified, and these can benefit from
using read-write locks instead of mutex locks. 

can provide more concurrency than a plain mutex lock when the data is read more than it is written
and known as shared-exclusive locking since shared lock for reading and exclusive lock for writing.
Multiple readers and one writer problem or readers-writer locks.


{apis}

// to get read lock. blocks the calling if there are writers
int   pthread_rwlock_rdlock(pthread_rwlock_t *);

// to get write lock. blocks the calling if there are readers or writers
int   pthread_rwlock_wrlock(pthread_rwlock_t *);

int   pthread_rwlock_unlock(pthread_rwlock_t *);
int   pthread_rwlock_tryrdlock(pthread_rwlock_t *);
int   pthread_rwlock_trywrlock(pthread_rwlock_t *);

int   pthread_rwlock_init(pthread_rwlock_t *, const pthread_rwlockattr_t *);
int   pthread_rwlock_destroy(pthread_rwlock_t *);
int   pthread_rwlockattr_destroy(pthread_rwlockattr_t *);
int   pthread_rwlockattr_init(pthread_rwlockattr_t *);

// to share the lock between different processes
int   pthread_rwlockattr_setpshared(pthread_rwlockattr_t *, int); pthread_t
int   pthread_rwlockattr_getpshared(const pthread_rwlockattr_t *, int *);


{example-implementation}
\unpv22e.tar\unpv22e\my_rwlock_cancel\

Can be implemented using mutexes and condition variables. This is an implementation which gives
preference to waiting writers but there are other alternatives.

typedef struct {
	pthread_mutex_t 	rw_mutex;			// lock on this struct
	pthread_cond_t 	rw_condreaders;	// for reader threads waiting
	pthread_cond_t 	rw_condwriters;	// for writer threads waiting

	// [KT]
	// when struct is inited, set to RW_MAGIC and used by all functions to check that the caller is
	// passing a pointer to an initialized lock and set to 0 when the lock is destroyed.
	int 					rw_magic;
	int 					rw_nwaitreaders;
	int 					rw_nwaitwriters;

	// the current status of the read-write lock. only one of these can exist at a time: -1 indicates
	// a write lock, 0 is lock available, and an value greater than 0 menas that many read locks are
	// held.
	int 					rw_refcount; 
} pthread_rwlock_t;


#define RW_MAGIC 0x19283746

/* init and destroy */
int
pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		result;

	if (attr != NULL)
		return(EINVAL);		/* not supported */

	if ( (result = pthread_mutex_init(&rw->rw_mutex, NULL)) != 0)
		goto err1;
	if ( (result = pthread_cond_init(&rw->rw_condreaders, NULL)) != 0)
		goto err2;
	if ( (result = pthread_cond_init(&rw->rw_condwriters, NULL)) != 0)
		goto err3;
	rw->rw_nwaitreaders = 0;
	rw->rw_nwaitwriters = 0;
	rw->rw_refcount = 0;
	rw->rw_magic = RW_MAGIC;

	return(0);

err3:
	pthread_cond_destroy(&rw->rw_condreaders);
err2:
	pthread_mutex_destroy(&rw->rw_mutex);
err1:
	return(result);			/* an errno value */
}

void
Pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		n;

	if ( (n = pthread_rwlock_init(rw, attr)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_init error");
}

int
pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);
	if (rw->rw_refcount != 0 ||
		rw->rw_nwaitreaders != 0 || rw->rw_nwaitwriters != 0)
		return(EBUSY);

	pthread_mutex_destroy(&rw->rw_mutex);
	pthread_cond_destroy(&rw->rw_condreaders);
	pthread_cond_destroy(&rw->rw_condwriters);
	rw->rw_magic = 0;

	return(0);
}

void
Pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	int		n;

	if ( (n = pthread_rwlock_destroy(rw)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_destroy error");
}


// rdlock
// A problem exists in this function: if the calling thread blocks in the call to pthread_cond_wait
// and the thread is then canceled, the thread terminates while it holds the mutex lock, and the
// counter rw_nwaitreaders is wrong. The same problem exists in our implentation of
// pthred_rwlock_wrlock. Can correct these problem in {}

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// 4give preference to waiting writers. cannot get a read lock if a) rw_refcount < 0 (meaning
	// there is a writer holding the lock and b) if threads are waiting to get a write lock.
	// [KT] if. case that there are writers
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}

	// [KT] else. case that there are no writers
	if (result == 0)
		rw->rw_refcount++;		/* another reader has a read lock */

	pthread_mutex_unlock(&rw->rw_mutex);
	return (result);
}

/* tryrdlock */
int
pthread_rwlock_tryrdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0)
		result = EBUSY;			/* held by a writer or waiting writers */
	else
		rw->rw_refcount++;		/* increment count of reader locks */

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


/* wrlock */
int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// [KT] if there are other readers or writers
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}

	// [KT] else there are no readers and writers
	if (result == 0)
		rw->rw_refcount = -1;

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


// unlock. used by both reader and writer
int
pthread_rwlock_unlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount > 0)
		rw->rw_refcount--;			// releasing a reader
	else if (rw->rw_refcount == -1)
		rw->rw_refcount = 0;			// releasing a writer
	else
		err_dump("rw_refcount = %d", rw->rw_refcount); // cannot be since it is to unlock

	// 4give preference to waiting writers over waiting readers
	//
	// {Q} The ref-UNP says: notice that we do not grant any additional read locks as soon as a
	// writer is waiting; otherwise, a stream of continual read requests could block a waiting writer
	// forever. For this reason, we need two separate if tests and cannot write
	//
	// if( rw->rw_nwaitwriters > 0 && rw->rw_refcount == 0 )
	// ...
	// 
	// could also omit the test of rw->rw_refcount, but that can result in calls to
	// pthread_cond_signal when read locks are still allocated, which is less efficient.
	//
	if (rw->rw_nwaitwriters > 0) {
		if (rw->rw_refcount == 0)
			result = pthread_cond_signal(&rw->rw_condwriters);		// signal single writer
	} else if (rw->rw_nwaitreaders > 0)
		result = pthread_cond_broadcast(&rw->rw_condreaders);		// signal all readers

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


{example}

#include	"unpipc.h"
#include	"pthread_rwlock.h"

pthread_rwlock_t	rwlock = PTHREAD_RWLOCK_INITIALIZER;

void	 *thread1(void *), *thread2(void *);
pthread_t	tid1, tid2;

int
main(int argc, char **argv)
{
	void	*status;
	Pthread_rwlock_init(&rwlock, NULL);

	Set_concurrency(2);
	Pthread_create(&tid1, NULL, thread1, NULL);
	sleep(1);		/* let thread1() get the lock */
	Pthread_create(&tid2, NULL, thread2, NULL);

	Pthread_join(tid2, &status);
	if (status != PTHREAD_CANCELED)
		printf("thread2 status = %p\n", status);

	Pthread_join(tid1, &status);
	if (status != NULL)
		printf("thread1 status = %p\n", status);

	printf("rw_refcount = %d, rw_nwaitreaders = %d, rw_nwaitwriters = %d\n",
		   rwlock.rw_refcount, rwlock.rw_nwaitreaders,
		   rwlock.rw_nwaitwriters);
	Pthread_rwlock_destroy(&rwlock);
	/* 4returns EBUSY error if cancelled thread does not cleanup */

	exit(0);
}

void *
thread1(void *arg)
{
	Pthread_rwlock_rdlock(&rwlock);
	printf("thread1() got a read lock\n");
	sleep(3);		/* let thread2 block in pthread_rwlock_wrlock() */
	pthread_cancel(tid2);
	sleep(3);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

void *
thread2(void *arg)
{
	printf("thread2() trying to obtain a write lock\n");
	Pthread_rwlock_wrlock(&rwlock);

	// followings are never get executed since it gets canceled.
	printf("thread2() got a write lock\n");
	sleep(1);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

When run this, the program is hung. The occurred steps are:

1. the second trys to get write lock and blocks on pthread_cond_wait.
2. the first returns from slepp(3) and cancel the second.

3. when the second is canceled while it is blocked in a condition variable wait, the mutex is
reacquired before calling the first cleanup hander (even if not installed any handers, but the mutex
is still reacquired before the thread is canceled.) Therefore, when the secondis canceled, it holds
the mutex lock for the read-write lock.

4. the first calls pthread_rwlock_unlock, but it blocks forever in its call to pthread_mutex_lock
because the mutex is still locked by the first thread that was canceled. [KT] this means cancel
terminates a thread immediately and do not continue the rest in pthread_rwlock_wrlock. Hence still
locked. 

If remove the call to pthread_rwlock_unlock in the thread1 func, the main will print:

rw_refcount = 1, rw_nwaitreaders = 0, rw_nwaitwriters = 1
pthread_rwlock_destroy error: Device busy

The correctio is simple and this is addition to pthread_rwlock_rdlock:

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		pthread_cleanup_push( rwlock_cancelrdwait, (void*)rw); ~
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelrdwait */
static void
rwlock_cancelrdwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitreaders--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelrdwait */ 

int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		pthread_cleanup_push(rwlock_cancelwrwait, (void *) rw); ~
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelwrwait */
static void
rwlock_cancelwrwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitwriters--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelwrwait */


==============================================================================
*kt_linux_core_230*	sync: file-lock

File locks: File locks are a synchronization method explicitly designed to coordinate the actions of
multiple processes operating [on-the-same-file]. They can also be used to coordinate access to other
shared resources. File locks come in two flavors: read (shared) locks and write (exclusive) locks.
Any number of processes can hold a read lock on the same file (or region of a file). However, when
one process holds a write lock on a file (or file region), other processes are prevented from
holding either read or write locks on that file (or file region). Linux provides file-locking
facilities via the flock() and fcntl() system calls. The flock() system call provides a simple
locking mechanism, allowing processes to place a shared or an exclusive lock on an entire file.
Because of its limited functionality, flock() locking facility is rarely used nowadays. The fcntl()
  system call provides record locking, allowing processes to place multiple read and write locks on
  different regions of the same file.



==============================================================================
*kt_linux_core_240* 	sync: common problems when use threads

the big three of threading problems are deadlock, races and starvation.

The simplest deadlock condition is when there are two threads and thread A can't progress until
thread B finishes, while thread B can't progress until thread A finishes. This is usually because
both need the same two resources to progress, A has one and B has the other. Various symmetry
breaking algorithms can prevent this in the two thread or larger circle cases.

Races happen when one thread changes the state of some resource when another thread is not expecting
it (such as changing the contents of a memory location when another thread is part way through
reading, or writing to that memory). Locking methods are the key here. (Some lock free methods
and containers are also good choices for this. As are atomic operations, or transaction
based operations.)

Starvation happens when a thread needs a resource to proceed, but can't get it. The resource is
constantly tied up by other threads and the one that needs it can't get in. The scheduling algorithm
is the problem when this happens. Look at algorithms that assure access.


==============================================================================
*kt_linux_core_250*	sync: reentrant and thread-safe

{thread-safe}
A function is said to be thread-safe if it can safely be invoked by multiple threads at the same
time; put conversely, if a function is not thread-safe, then we canât call it from one thread while
it is being executed in another thread. The same example in {why-need-sync}.

<solution-one> serialization
There are various methods of rendering a function thread-safe. One way is to associate a mutex with
the function or perhaps with all of the functions in a library, if they all share the same global
variables, lock that mutex when the function is called, and unlock it when the mutex returns. This
approach has the virtue of simplicity. On the other hand, it means that only one thread at a time
can execute the function-we say that access to the function is [serialized]. 

The downside is that if the threads spend a significant amount of time executing this function, then
this serialization results in a [reduce-or-loss-of-concurrency], because the threads of a program
can no longer execute in parallel.

<solution-two> critial-section
A more sophisticated solution is to associate the mutex with a shared variable. We then determine
which parts of the function are critical sections that access the shared variable, and acquire and
release the mutex only during the execution of these critical sections. This allows multiple threads
to execute the function at the same time and to operate in parallel, except when more than one
thread needs to execute a critical section.


{non-thread-safe-functions}
To facilitate the development of threaded applications, all of the functions specified in SUSv3 are
required to be implemented in a thread-safe manner, except those listed in Table 31-1.

In ref-LPI, Table 31-1: Functions that SUSv3 does not require to be thread-safe


{reentrant-functions}
Although the use of critical sections to implement thread safety is a significant improvement over
the use of per-function mutexes, it is still somewhat inefficient because there is a cost to locking
and unlocking a mutex. A reentrant function achieves thread safety without the use of mutexes.

<how>
It does this by avoiding the use of global and static variables. Any information that must be
returned to the caller, or maintained between calls to the function, is stored in buffers allocated
by the caller. {Q} Although use a buffer from a caller, still seems to be a problem.

However, not all functions can be made reentrant. The usual reasons are the following:

o By their nature, some functions must access global data structures. The functions in the malloc
library provide a good example. These functions maintain a global linked list of free blocks on the
heap. The functions of the malloc library are made thread-safe through the use of mutexes.

o Some functions (defined before the invention of threads) have an interface that by definition is
nonreentrant, because they return pointers to storage statically allocated by the function, or they
employ static storage to maintain information between successive calls to the same (or a related)
function.

See {signal-reentrant} for more about reentrant.

[KT] In sum, reentrant is bigger notion than thread-safe.

<r_prefix>
For several of the functions that have nonreentrant interfaces, SUSv3 specifies reentrant
equivalents with names ending with the suffix _r. These functions require the caller to allocate a
buffer whose address is then passed to the function and used to return the result. This allows the
calling thread to use a local (stack) variable for the function result buffer.

For example, glibc provides crypt_r(), gethostbyname_r(), getservbyname_r(), getutent_r(),
getutid_r(), getutline_r(), and ptsname_r(). 

However, a portable application can't rely on these functions being present on other
implementations.


{code-reentrant-function}
The most efficient way of making a function thread-safe is to make it [reentrant]. All new library
functions should be implemented in this way. However, for an existing nonreentrant library function
(one that was perhaps designed before the use of threads became common), this approach usually
requires changing the functionâs interface, which means modifying all of the programs that use the
function. 

<why-thread-specific-data>
Thread-specific data is a technique for making an existing function thread-safe without
changing its interface. A function that uses thread-specific data may be slightly less efficient
than a reentrant function, but allows us to leave the programs that call the function unchanged.

<strerror-manpage>
STRERROR(3)               Linux Programmer's Manual              STRERROR(3)

NAME
       strerror, strerror_r - return string describing error number

SYNOPSIS

       #include <string.h>

       char *strerror(int errnum);

       int strerror_r(int errnum, char *buf, size_t buflen);
                   /* XSI-compliant */

       char *strerror_r(int errnum, char *buf, size_t buflen);
                   /* GNU-specific */

   Feature Test Macro Requirements for glibc (see feature_test_macros(7)):

       The XSI-compliant version of strerror_r() is provided if:
       (_POSIX_C_SOURCE >= 200112L || _XOPEN_SOURCE >= 600) && ! _GNU_SOURCE
       Otherwise, the GNU-specific version is provided.

DESCRIPTION

The strerror() function returns a pointer to a string that describes the error code passed in the
argument errnum, possibly using the LC_MESSAGES part of the current locale to select the appropriate
language. (For example, if errnum is EINVAL, the returned description will "Invalid argument".)
This string must not be modified by the application, but may be modified by a subsequent call to
strerror(). No library function, including perror(3), will modify this string.

The strerror_r() function is similar to strerror(), but is thread safe. This function is available
in two versions: an XSI-compliant version specified in POSIX.1-2001 (available since glibc 2.3.4,
but not POSIX-compliant until glibc 2.13), and a GNU-specific version (available since glibc
2.0). The XSI-compliant version is provided with the feature test macros settings shown in
the SYNOPSIS; otherwise the GNU-specific version is provided. If no feature test macros are
explicitly defined, then (since glibc 2.4) _POSIX_SOURCE is defined by default with the value
200112L, so that the XSI- compliant version of strerror_r() is provided by default.

The XSI-compliant strerror_r() is preferred for portable applications. It returns the error string
in the user-supplied buffer buf of length buflen.

The GNU-specific strerror_r() returns a pointer to a string containing the error message. This may
be either a pointer to a string that the function stores in buf, or a pointer to some (immutable)
static string (in which case buf is unused). If the function stores a string in buf, then at most
buflen bytes are stored (the string may be truncated if buflen is too small and errnum is
unknown). The string always includes a terminating null byte ('\0').

RETURN VALUE

The strerror() and the GNU-specific strerror_r() functions return the appropriate error description
string, or an "Unknown error nnn" message if the error number is unknown.

POSIX.1-2001 and POSIX.1-2008 require that a successful call to strerror() shall leave errno
unchanged, and note that, since no function return value is reserved to indicate an error, an
application that wishes to check for errors should initialize errno to zero before the call, and
then check errno after the call.

The XSI-compliant strerror_r() function returns 0 on success. On error, a (positive) error number is
returned (since glibc 2.13), or -1 is returned and errno is set to indicate the error (glibc
versions before 2.13).

ERRORS

EINVAL The value of errnum is not a valid error number.
ERANGE Insufficient storage was supplied to contain the error description string.

ATTRIBUTES

Multithreading (see pthreads(7))
The strerror() function is not thread-safe.
The strerror_r() function is thread-safe.

CONFORMING TO

strerror() is specified by POSIX.1-2001, C89, C99.  strerror_r() is specified by POSIX.1-2001.

The GNU-specific strerror_r() function is a nonstandard extension.

POSIX.1-2001 permits strerror() to set errno if the call encounters an error, but does not specify
what value should be returned as the function result in the event of an error. On some systems,
strerror() returns NULL if the error number is unknown.  On other systems, strerror() returns a
string something like "Error nnn occurred" and sets errno to EINVAL if the error number is
unknown. C99 and POSIX.1-2008 require the return value to be non-NULL.

SEE ALSO
err(3), errno(3), error(3), perror(3), strsignal(3)

COLOPHON

This page is part of release 3.61 of the Linux man-pages project.  A description of the project, and
information about reporting bugs, can be found at http://www.kernel.org/doc/man-pages/.

2013-06-21                      STRERROR(3)

<POSIX-1-2001>
Beginning in 1999, the IEEE, The Open Group, and the ISO/IEC Joint Technical Committee 1
collaborated in the Austin Common Standards Revision Group with the aim of revising and
consolidating the POSIX standards and the Single UNIX Specification. This resulted in the
ratification of POSIX 1003.1-2001, sometimes just called POSIX.1-2001, in December 2001
(subsequently approved as an ISO standard, ISO/IEC 9945:2002).

<thread-specific-data>
1. The function creates a key, which is the means of differentiating the thread-specific data item
used by this function from the thread-specific data items used by other functions. The key is
created by calling the pthread_key_create() function. Creating a key needs to be done only once,
when the first thread calls the function. For this purpose, pthread_once() is employed.
Creating a key doesnât allocate any blocks of thread-specific data.

2. The call to pthread_key_create() serves a second purpose: it allows the caller to specify the
address of the programmer-defined destructor function that is used to deallocate each of the storage
blocks allocated for this key (see the next step). When a thread that has thread-specific data
terminates, the Pthreads API automatically invokes the destructor, passing it a pointer to the data
block for this thread.

<typical-thread-specific-data-implementation>
This is a typical implementation (NPTL is typical). In this implementation, the pthread_key_t value
returned by pthread_key_create() is simply an index into the global array, which we label
pthread_keys

                  -----------------
pthread_keys[0]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
pthread_keys[1]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
pthread_keys[2]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
                  ...

int pthread_setspecific(pthread_key_t key, const void *value);

So key is allocated for each client which is a library function, strerror, in this case and assumes
that this is keys[1]. The data structure for value or buffer for each thread is:

All correspond to                   thread A
pthread_keys[1]    value of key1    tsd[0] | pointer |
                   for thread A  -> tsd[1] | pointer |  -> TSD buffer for strerror in thread A
                                    tsd[2] | pointer |                                         

                                    thread B
                   value of key1    tsd[0] | pointer |
                   for thread B  -> tsd[1] | pointer |  -> TSD buffer for strerror in thread B
                                    tsd[2] | pointer |                                        

                   ...

The TDS buffer is allocated in strerror for each thread and when each thread calls 

void *pthread_getspecific(pthread_key_t key);

The pthread library know the thread and use the key to pick up the entry and the buffer pointer.
When a thread is first created, all of its thread-specific data pointers are initialized to NULL.
This means that when our library function is called by a thread for the first time, it must begin by
using pthread_getspecific() to check whether the thread already has an associated value for key.

[KT] This means that when pthread_key_create is called, it allocates a entry in keys array and also
TSD array for each thread even if some thread is not using strerror.

<strerror-non-thread-safe-version>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#define MAX_ERROR_LEN 256 /* Maximum length of string
                             returned by strerror() */
static char buf[MAX_ERROR_LEN]; /* Statically allocated return buffer */
  char *
strerror(int err)
{
  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1);
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }
  return buf;
}

<user-code>
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#include "tlpi_hdr.h"

static void *
threadFunc(void *arg)
{
  char *str;
  printf("Other thread about to call strerror()\n");
  str = strerror(EPERM);
  printf("Other thread: str (%p) = %s\n", str, str);
  return NULL;
}

int
main(int argc, char *argv[])
{
  pthread_t t;
  int s;
  char *str;
  str = strerror(EINVAL);
  printf("Main thread has called strerror()\n");

  s = pthread_create(&t, NULL, threadFunc, NULL);
  if (s != 0)
    errExitEN(s, "pthread_create");

  s = pthread_join(t, NULL);
  if (s != 0)
    errExitEN(s, "pthread_join");

  printf("Main thread: str (%p) = %s\n", str, str);

  exit(EXIT_SUCCESS);
}

<strerror-thread-safe-version>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#include "tlpi_hdr.h"

static pthread_once_t once = PTHREAD_ONCE_INIT;
static pthread_key_t strerrorKey;

#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread
                             buffer returned by strerror() */

static void /* Free thread-specific data buffer */
q destructor(void *buf)
{
  free(buf);
}

static void /* One-time key creation function */
createKey(void) <2>
{
  int s;
  /* Allocate a unique thread-specific data key and save the address
     of the destructor for thread-specific data buffers */
  s = pthread_key_create(&strerrorKey, destructor); <1>
  if (s != 0)
    errExitEN(s, "pthread_key_create");
}

char *
strerror(int err)
{
  int s;
  char *buf;

  /* Make first caller allocate key for thread-specific data */
  s = pthread_once(&once, createKey); <2>
  if (s != 0)
    errExitEN(s, "pthread_once");

  buf = pthread_getspecific(strerrorKey); <3>
  if (buf == NULL) { /* If first call from this thread, allocate
                        buffer for thread, and save its location */
    buf = malloc(MAX_ERROR_LEN);
    if (buf == NULL)
      errExit("malloc");

    s = pthread_setspecific(strerrorKey, buf); <4>
    if (s != 0)
      errExitEN(s, "pthread_setspecific");
  }

  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); <4>
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }

  return buf;
}

<limitation>
SUSv3 requires that an implementation support at least 128 (_POSIX_THREAD_KEYS_MAX) keys. An
application can determine how many keys an implementation actually supports either via the
definition of PTHREAD_KEYS_MAX (defined in <limits.h>) or by calling sysconf(_SC_THREAD_KEYS_MAX).
Linux supports up to 1024 keys.

Even 128 keys should be more than sufficient for most applications. This is because each library
function should employ only a small number of keysâoften just one. If a function requires multiple
thread-specific data values, these can usually be placed in a single structure that has just one
associated thread-specific data key.


{thread-local-storage}
Like thread-specific data, thread-local storage provides persistent per-thread storage. This
feature is [nonstandard], but it is provided in the same or a similar form on many other UNIX
implementations.

Thread-local storage requires support from the kernel (provided in Linux 2.6), the Pthreads
implementation (provided in NPTL), and the C compiler (provided on x86-32 with gcc 3.3 and later).

The main advantage of thread-local storage is that it is much simpler to use than thread-specific
data. To create a thread-local variable, we simply include the __thread specifier in the declaration
of a global or static variable:

static __thread buf[MAX_ERROR_LEN];

Each thread has its own copy of the variables declared with this specifier. The variables
in a threadâs thread-local storage persist until the thread terminates, at which
time the storage is automatically deallocated.

The following points about the declaration and use of thread-local variables:

o The __thread keyword must immediately follow the static or extern keyword, if either of these is
specified in the variableâs declaration.

o The declaration of a thread-local variable can include an initializer, in the same manner as a
normal global or static variable declaration.

o The C address (&) operator can be used to obtain the address of a thread-local variable.

<revised-strerror-using-tls>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread
                             buffer returned by strerror() */
static __thread char buf[MAX_ERROR_LEN];
/* Thread-local return buffer */
char *
strerror(int err)
{
  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1);
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }
  return buf;
}


==============================================================================
*kt_linux_core_260*	sync: atomic operations

{atomic-operations}

For full articles:
http://www.informit.com/guides/content.aspx?g=cplusplus&seqNum=469

Atomicity

An atomic operation is a sequence of one or more machine instructions that are executed
sequentially, without interruption. By default, any sequence of two or more machine instructions
isn't atomic since the operating system may suspend the execution of the current sequence of
operations in favor of another task. If you want to ensure that a sequence of operations is atomic
you must use some form of locking or other types of synchronization. 

Without that, the only guarantee you have is that a single machine instruction is always atomic. the
CPU will not interrupt a single instruction in the middle. [KT] Not entirely true. 

We can conclude from that minimal guarantee that if you can prove that your compiler translates a
certain C++ statement into a single machine instruction, that C++ statement is naturally atomic
meaning, the programmer doesn't have to use explicit locking to enforce the atomic execution of that
statement.  

Which C++ Statements are Naturally Atomic?

Obviously, there are very few universal rules of thumb because each hardware architecture might
translate the same C++ statement differently. Many textbooks tell you that the unary ++ and --
operators, when applied to integral types and pointers, are guaranteed to be atomic. Historically,
when Dennis Ritchie and Brian Kernighan designed C, they added these operators to the language
because they wanted to take advantage of the fast INC (increment) assembly directive that many
machines supported. However, there is no guarantee in the C or C++ standards that these operators
shall be atomic. Ritchie and Kernighan were more concerned about speed rather than atomicity.

You shouldn't make assumptions about the atomicity of C++ statements without examining the output of
your compiler. In some cases, you might discover that what appears to be a single C++ statement is
in fact translated into a long and complex set of machine instructions. 


Epilog

The multithreading support of C++0x consists of a thread class as well as a standard atomics library
that guarantees the atomicity of logical and arithmetic operations. I will introduce the C++0x
atomics library in a separate column.


From C++11:

Data-dependency ordering: atomics and memory model 	N2664 	GCC 4.4
(memory_order_consume)


From StackOverflow:

The increment-memory machine instruction on an X86 is atomic only if you use it with a LOCK prefix.

x++ in C and C++ doesn't have atomic behavior. If you do unlocked increments, due to races in which
processor is reading and writing X, if two separate processors attempt an increment, you can end up
with just one increment or both being seen (the second processor may have read the initial value,
incremented it, and written it back after the first writes its results back).

I believe that C++11 offers atomic increments, and most vendor compilers have an idiomatic way to
cause an atomic increment of certain built-in integer types (typically int and long); see your
compiler reference manual.

If you want to increment a "large value" (say, a multiprecision integer), you need to do so with
using some standard locking mechanism such as a semaphore.

Note that you need to worry about atomic reads, too. On the x86, reading a 32 or 64 bit value
happens to be atomic if it is 64-bit word aligned. That won't be true of a "large value"; again
you'll need some standard lock.

{rmw-operations}
The RMW(read-modify-write) is:

<from-wikipedia>
A class of atomic operations such as test-and-set, fetch-and-add, and compare-and-swap which both
read a memory location and write a new value into it simultaneously, either with a completely new
value or some function of the previous value. These operations prevent race conditions in
multi-threaded applications. Typically they are used to implement mutexes or semaphores. These
atomic operations are also heavily used in non-blocking synchronization.

{atomic-non-atomic-operations}
http://preshing.com/20130618/atomic-vs-non-atomic-operations/

Much has already been written about atomic operations on the web, usually with a focus on atomic
read-modify-write (RMW) operations. However, those arenât the only kinds of atomic operations. There
are also atomic loads and stores, which are equally important. In this post, Iâll compare atomic
loads and stores to their non-atomic counterparts at both the processor level and the C/C++ language
level. Along the way, weâll clarify the C++11 concept of a âdata raceâ.

Automic operations: automic loads and stores + automic read-modify-write operations

An operation acting on shared memory is atomic if it completes in a single step relative to other
threads. When an atomic store is performed on a shared variable, no other thread can observe the
modification half-complete. When an atomic load is performed on a shared variable, it reads the
entire value as it appeared at a single moment in time. Non-atomic loads and stores do not make
those guarantees.

Without those guarantees, lock-free programming would be impossible, since you could never let
different threads manipulate a shared variable at the same time. We can formulate it as a rule:

Any time two threads operate on a shared variable concurrently, and one of those operations performs
a write, both threads must use atomic operations.

If you violate this rule, and either thread uses a non-atomic operation, youâll have what the C++11
standard refers to as a [data-race] (not to be confused with Javaâs concept of a data race, which is
different, or the more general race condition). [Q] what is the general race condition?

The C++11 standard doesnât tell you why data races are bad; only that if you have one, âundefined
behaviorâ will result (section 1.10.21). The real reason why such data races are bad is actually
quite simple: They result in [torn-reads] and [torn-writes].

A memory operation can be non-atomic because it uses multiple CPU instructions, non-atomic even when
using a single CPU instruction, or non-atomic because youâre writing portable code and you simply
canât make the assumption. Letâs look at a few examples.


<Non-Atomic Due to Multiple CPU Instructions>

Suppose you have a 64-bit global variable, initially zero.

uint64_t sharedValue = 0;

At some point, you assign a 64-bit value to this variable.

void storeValue()
{
    sharedValue = 0x100000002;
}

When you compile this function for 32-bit x86 using GCC, it generates the following machine code.

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	DWORD PTR sharedValue, 2
        mov	DWORD PTR sharedValue+4, 1
        ret
        ...

As you can see, the compiler implemented the 64-bit assignment using two separate machine
instructions. The first instruction sets the lower 32 bits to 0x00000002, and the second sets the
upper 32 bits to 0x00000001. Clearly, this assignment operation is not atomic. If sharedValue is
accessed concurrently by different threads, several things can now go wrong:

- If a thread calling storeValue is preempted between the two machine instructions, it will leave
the value of 0x0000000000000002 in memory â a torn write. At this point, if another thread reads
sharedValue, it will receive this completely bogus value which nobody intended to store.

- Even worse, if a thread is preempted between the two instructions, and another thread modifies
sharedValue before the first thread resumes, it will result in a permanently torn write: the upper
32 bits from one thread, the lower 32 bits from another.

- On multicore devices, it isnât even necessary to preempt one of the threads to have a torn write.
When a thread calls storeValue, any thread executing on a different core could read sharedValue at a
moment when only half the change is visible.

Reading concurrently from sharedValue brings its own set of problems:

uint64_t loadValue()
{
    return sharedValue;
}

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	eax, DWORD PTR sharedValue
        mov	edx, DWORD PTR sharedValue+4
        ret
        ...

Here too, the compiler has implemented the load operation using two machine instructions: The first
reads the lower 32 bits into eax, and the second reads the upper 32 bits into edx. In this case, if
a concurrent store to sharedValue becomes visible between the two instructions, it will result in a
torn read â even if the concurrent store was atomic.

These problems are not just theoretical. Mintomicâs test suite includes a test case called
test_load_store_64_fail, in which one thread stores a bunch of 64-bit values to a single variable
using a plain assignment operator, while another thread repeatedly performs a plain load from the
same variable, validating each result. On a multicore x86, this test fails consistently, as
expected.


<Non-Atomic in a single CPU Instructions>

A memory operation can be non-atomic even when performed by a single CPU instruction. For example,
  the ARMv7 instruction set includes the strd instruction, which stores the contents of two 32-bit
  source registers to a single 64-bit value in memory.

strd r0, r1, [r2]

On some ARMv7 processors, this instruction is not atomic. When the processor sees this instruction,
	it actually performs [two-separate-32-bit-stores] under the hood (section A3.5.3). Once again,
	another thread running on a separate core has the possibility of observing a torn write.
	Interestingly, a torn write is even possible on a single-core device: A system interrupt â say,
	for a scheduled thread context switch â can actually occur between the two internal 32-bit
	stores! In this case, when the thread resumes from the interrupt, it will restart the strd
	instruction all over again.

As another example, itâs well-known that on x86, a 32-bit mov instruction is atomic if the memory
operand is naturally aligned, but non-atomic otherwise. In other words, atomicity is [only]
guaranteed when the 32-bit integer is located at an address which is an exact multiple of 4.


<All C/C++ Operations Are Presumed Non-Atomic>

In C and C++, every operation is presumed non-atomic unless otherwise specified by the compiler or
hardware vendor â even plain 32-bit integer assignment.

uint32_t foo = 0;

void storeFoo()
{
    foo = 0x80286;
}

The language standards have nothing to say about atomicity in this case. Maybe integer assignment is
atomic, maybe it isnât. Since non-atomic operations donât make any guarantees, plain integer
assignment in C is non-atomic by definition.

In practice, we usually know more about our target platforms than that. For example, itâs common
knowledge that on all modern x86, x64, Itanium, SPARC, ARM and PowerPC processors, plain 32-bit
integer assignment is atomic [as-long-as] the target variable is naturally aligned. You can verify it
by consulting your processor manual and/or compiler documentation. In the games industry, I can tell
you that a lot of 32-bit integer assignments rely on this particular guarantee.

Nonetheless, when writing truly portable C and C++, thereâs a long-standing tradition of pretending
that we donât know anything more than what the language standards tell us. Portable C and C++ is
designed to run on every possible computing device past, present and imaginary. Personally, I like
to imagine a machine where memory can only be changed by mixing it up first:

On such a machine, you definitely wouldnât want to perform a concurrent read at the same time as a
plain assignment; you could end up reading a completely random value.

<CPP11>
In C++11, there is finally a way to perform truly portable atomic loads and stores: the C++11 atomic
library. Atomic loads and stores performed using the C++11 atomic library would even work on the
imaginary computer above - even if it means the C++11 atomic library must secretly [lock] a mutex to
make each operation atomic. Thereâs also the Mintomic library which I released last month, which
doesnât support as many platforms, but works on several older compilers, is hand-optimized and is
guaranteed to be lock-free.


Relaxed Atomic Operations

Letâs return to the original sharedValue example from earlier in this post. Weâll rewrite it using
Mintomic so that all operations are performed atomically on every platform Mintomic supports. First,
we must declare sharedValue as one of Mintomicâs atomic data types.

#include <mintomic/mintomic.h>

mint_atomic64_t sharedValue = { 0 };

The mint_atomic64_t type guarantees correct memory alignment for atomic access on each platform.
This is important because, for example, the GCC 4.2 compiler for ARM bundled with Xcode 3.2.5
doesnât guarantee that plain uint64_t will be 8-byte aligned.

In storeValue, instead of performing a plain, non-atomic assignment, we must call
mint_store_64_relaxed.

void storeValue()
{
    mint_store_64_relaxed(&sharedValue, 0x100000002);
}

Similarly, in loadValue, we call mint_load_64_relaxed.

uint64_t loadValue()
{
    return mint_load_64_relaxed(&sharedValue);
}

Using C++11âs terminology, these functions are now data race-free. When executing concurrently,
		there is absolutely no possibility of a torn read or write, whether the code runs on
		ARMv6/ARMv7 (Thumb or ARM mode), x86, x64 or PowerPC. If youâre curious how
		mint_load_64_relaxed and mint_store_64_relaxed actually work, both functions expand to an
		inline cmpxchg8b instruction on x86; for other platforms, consult Mintomicâs implementation.

Hereâs the exact same thing written in C++11 instead:

#include <atomic>

std::atomic<uint64_t> sharedValue(0);

void storeValue()
{
    sharedValue.store(0x100000002, std::memory_order_relaxed);
}

uint64_t loadValue()
{
    return sharedValue.load(std::memory_order_relaxed);
}

Youâll notice that both the Mintomic and C++11 examples use relaxed atomics, as evidenced by the
_relaxed suffix on various identifiers. The _relaxed suffix is a reminder that, just as with plain
loads and stores, no guarantees are made about memory ordering.

The only difference between a relaxed atomic load (or store) and a non-atomic load (or store) is
that relaxed atomics guarantee atomicity. No other difference is guaranteed.

In particular, it is still legal for the memory effects of a relaxed atomic operation to be
reordered with respect to instructions which follow or precede it in program order, either due to
compiler reordering or memory reordering on the processor itself. The compiler could even perform
optimizations on redundant relaxed atomic operations, just as with non-atomic operations. In all
cases, the operation remains atomic.

When manipulating shared memory concurrently, I think itâs good practice to always use Mintomic or
C++11 atomic library functions, even in cases where you know that a plain load or store would
already be atomic on your target platform. An atomic library function serves as a reminder that
elsewhere, the variable is the target of concurrent data access.

Hopefully, itâs now a bit more clear why the Worldâs Simplest Lock-Free Hash Table uses Mintomic
library functions to manipulate shared memory concurrently from different threads.


{lock-free-programming}
http://preshing.com/20120612/an-introduction-to-lock-free-programming/

An Introduction to Lock-Free Programming

Lock-free programming is a challenge, not just because of the complexity of the task itself, but
because of how difficult it can be to penetrate the subject in the first place.

I was fortunate in that my first introduction to lock-free (also known as lockless) programming was
Bruce Dawsonâs excellent and comprehensive white paper, Lockless Programming Considerations. And
like many, Iâve had the occasion to put Bruceâs advice into practice developing and debugging
lock-free code on platforms such as the Xbox 360.

Since then, a lot of good material has been written, ranging from abstract theory and proofs of
correctness to practical examples and hardware details. Iâll leave a list of references in the
footnotes. At times, the information in one source may appear orthogonal to other sources: For
instance, some material assumes sequential consistency, and thus sidesteps the memory ordering
issues which typically plague lock-free C/C++ code. The new C++11 atomic library standard throws
another wrench into the works, challenging the way many of us express lock-free algorithms.

In this post, Iâd like to re-introduce lock-free programming, first by defining it, then by
distilling most of the information down to a few key concepts. Iâll show how those concepts relate
to one another using flowcharts, then weâll dip our toes into the details a little bit. At a
minimum, any programmer who dives into lock-free programming should already understand how to write
correct multithreaded code using mutexes, and other high-level synchronization objects such as
semaphores and events.  

What Is It?

People often describe lock-free programming as programming without mutexes, which are also referred
to as [locks]. Thatâs true, but itâs only [part] of the story. The generally accepted definition, based
on academic literature, is a bit more broad. At its essence, lock-free is a property used to
describe some code, without saying too much about how that code was actually written.

Basically, if some part of your program satisfies the following conditions, then that part can
rightfully be considered lock-free. Conversely, if a given part of your code doesnât satisfy these
conditions, then that part is not lock-free.

<definition>
(This was a flow chart)

Are you programming with multiple threads? or interrupt, signal handlers, etc?
-> Yes

Do the threads access shared memeory?
-> Yes

Can the threads block each other? ie. is there some way to schedule the threads which would
'lock-up' indefinitely?
-> No

It is lock-free programming.

In this sense, the lock in lock-free [does not refer directly to mutexes], but rather to the
possibility of âlocking upâ the entire application in some way, whether itâs deadlock, livelock â or
even due to hypothetical thread scheduling decisions made by your worst enemy. That last point
sounds funny, but itâs key. Shared mutexes are ruled out trivially, because as soon as one thread
obtains the mutex, your worst enemy could simply never schedule that thread again. Of course, real
operating systems donât work that way - weâre merely defining terms.

Hereâs a simple example of an operation which contains no mutexes, but is still not lock-free.
Initially, X = 0. As an exercise for the reader, consider how two threads could be scheduled in a
way such that neither thread exits the loop.

while (X == 0)
{
    X = 1 - X;
}

Nobody expects a large application to be entirely lock-free. Typically, we identify a [specific-set]
of lock-free operations out of the whole codebase. For example, in a lock-free queue, there might be
a handful of lock-free operations such as push, pop, perhaps isEmpty, and so on.

Herlihy & Shavit, authors of The Art of Multiprocessor Programming, tend to express such operations
as class methods, and offer the following succinct definition of lock-free (see slide 150): 
	
"In an infinite execution, infinitely often some method call finishes." In other words, as long as
the program is able to keep calling those lock-free operations, the number of completed calls keeps
increasing, no matter what. It is algorithmically impossible for the system to lock up during those
operations. {Q} Didn't get that.

<why-use>
One important consequence of lock-free programming is that if you suspend a single thread, it will
never prevent other threads from making progress, as a group, through their own lock-free
operations. This hints at the value of lock-free programming when writing interrupt handlers and
real-time systems, where certain tasks must complete within a certain time limit, no matter what
state the rest of the program is in.

A final precision: Operations that are designed to block do not disqualify the algorithm. For
example, a queueâs pop operation may intentionally block when the queue is empty. The remaining
codepaths can still be considered lock-free.

<TODO> there is more in this page.


==============================================================================
*kt_linux_core_261*	sync: ref: locks aren't slow; lock contention is

<contention-and-frequency>
For example, this post measures the performance of a lock under heavy conditions: each thread must
hold the lock to do any work (high contention), and the lock is held for an extremely short interval
of time (high frequency)

But donât disregard locks yet. One good example of a place where locks perform admirably, in real
software, is when protecting the memory allocator. Doug Leaâs Malloc is a popular memory allocator
in video game development, but itâs single threaded, so we need to protect it using a lock. During
gameplay, itâs not uncommon to see multiple threads hammering the memory allocator, say around 15000
times per second. While loading, this figure can climb to 100000 times per second or more. Itâs not
a big problem, though. As youâll see, locks handle the workload like a champ.
{Q} what's this memory allocator?

Lock Contention Benchmark

In this test, we spawn a thread which generates random numbers, using a custom Mersenne Twister
implementation. For example, suppose we want to acquire the lock 15000 times per second, and keep it
held 50% of the time. (whole working time) 

This is code example to show 50% lock:

QueryPerformanceCounter(&start);
for (;;)
{
  // Do some work without holding the lock
  workunits = (int) (random.poissonInterval(averageUnlockedCount) + 0.5f);
  for (int i = 1; i < workunits; i++)
    random.integer();       // Do one work unit
  workDone += workunits;

  QueryPerformanceCounter(&end);
  elapsedTime = (end.QuadPart - start.QuadPart) * ooFreq;
  if (elapsedTime >= timeLimit)
    break;

  // Do some work while holding the lock
  EnterCriticalSection(&criticalSection);
  workunits = (int) (random.poissonInterval(averageLockedCount) + 0.5f);
  for (int i = 1; i < workunits; i++)
    random.integer();       // Do one work unit
  workDone += workunits;
  LeaveCriticalSection(&criticalSection);

  QueryPerformanceCounter(&end);
  elapsedTime = (end.QuadPart - start.QuadPart) * ooFreq;
  if (elapsedTime >= timeLimit)
    break;
}

Now suppose we launch two such threads, each running on a different core. Each thread will hold the
lock during 50% of the time when it can perform work, but if one thread tries to acquire the lock
while the other thread is holding it, it will be forced to wait. This is known as lock contention.

(Tested on dual core) When we run the above scenario, we find that each thread spends roughly 25% of
its time waiting, and 75% of its time doing actual work. Together, both threads achieve a net
performance of 1.5x compared to the single-threaded case.

I ran several variations of the test on a 2.66 GHz quad-core Xeon, from 1 thread, 2 threads, all the
way up to 4 threads, each running on its own core. I also varied the duration of the lock, from the
trivial case where the the lock is never held, all the way up to the maximum where each thread must
hold the lock for 100% of its workload. In all cases, the lock frequency remained constant â threads
acquired the lock 15000 times for each second of work performed.

<KT>
The graph shows that go up to 4x when 0% lock duration and down below 1x when 100% lock duration. 0%
means that there is no sharing between threads hence no lock is needed.

The results were interesting. For short lock durations, up to say 10%, the system achieved very high
parallelism. Not perfect parallelism, but close. Locks are fast!

To put the results in perspective, I analyzed the memory allocator lock in a multithreaded game
engine using this profiler. During gameplay, with 15000 locks per second coming from 3 threads, the
lock duration was in the neighborhood of just 2%. Thatâs well within the comfort zone on the left
side of the diagram.

These results also show that once the lock duration passes 90%, thereâs no point using multiple
threads anymore. A single thread performs better. Most surprising is the way the performance of 4
threads drops off a cliff around the 60% mark! This looked like an anomaly, so I re-ran the tests
several additional times, even trying a different testing order. The same behavior happened
consistently. My best hypothesis is that the experiment hits some kind of snag in the Windows
scheduler, but I didnât investigate further.

Lock Frequency Benchmark

Even a lightweight mutex has overhead. As my next post shows, a pair of lock/unlock operations on a
Windows Critical Section takes about 23.5 ns on the CPU used in these tests. Therefore, 15000 locks
per second is low enough that lock overhead does not significantly impact the results. But what
happens as we turn up the dial on lock frequency?

The algorithm offers very fine control over the amount of work performed between one lock and the
next, so I performed a new batch of tests using smaller amounts: from a very fine-grained 10 ns
between locks, all the way up to 31 Î¼s, which corresponds to roughly 32000 acquires per second. Each
test used exactly two threads:

As you might expect, for very high lock frequencies, the overhead of the lock itself begins to dwarf
the actual work being done. Several benchmarks youâll find online, including the one linked earlier
fall into the bottom-right corner of this chart. At such frequencies, youâre talking about some
seriously short lock times â on the scale of a few CPU instructions. The good news is that, when the
work between locks is that simple, a lock-free implementation is more likely to be feasible.

At the same time, the results show that locking up to 320000 times per second (3.1 Î¼s between
    successive locks) is not unreasonable. In game development, the memory allocator may flirt with
this frequency during load times. You can still achieve more than 1.5x parallelism if the lock
duration is short.

Weâve now seen a wide spectrum of lock performance: cases where it performs great, and cases where
the application slows to a crawl. Iâve argued that the lock around the memory allocator in a game
engine will often achieve excellent performance. Given this example from the real world, it cannot
be said that all locks are slow. Admittedly, itâs very easy to abuse locks, but one shouldnât live
in too much fear â any resulting bottlenecks will show up during careful profiling. When you
consider how reliable locks are, and the relative ease of understanding them (compared to lock-free
    techniques), locks are actually pretty awesome sometimes.

The goal of this post was to give locks a little respect where deserved - corrections are welcome. I
also realize that locks are used in a wide variety of industries and applications, and it may not
always be so easy to strike a good balance in lock performance. If youâve found that to be the case
in your own experience, I would love to hear from you in the comments.

{DN}
Then the bottomlien is that try to minimize the lock contention rather than try to find out which
one is fast. Of course, should use better one but the lock contention is far more important.


==============================================================================
*kt_linux_core_262*	sync: ref: always use a lightweight mutex {mutex-vs-semaphore}

http://preshing.com/20111124/always-use-a-lightweight-mutex/
Always Use a Lightweight Mutex

Have started wondering since seen this article which says that critical section is faster than mutex
in windows. This critical section is called as lightweight mutex which is equivalent to pthread
mutex in Linux. Does it mean that pthread mutex is faster than posix semaphore? Tried the same
approach that this article had and it seems not. Like ref-LPI, there is no big difference.

This is the result ran on the real Linux server machine which has multiple processors and as can
see, semaphore is slightly slower than using a mutex but not significant. This seems different from
what this article said.

<snippet>
Now, suppose you have a thread which acquires a Critical Section 100000 times per second, and there
are no other threads competing for the lock. Based on the above figures, you can expect to pay
between 0.2% and 0.6% in lock overhead. Not too bad! At lower frequencies, the overhead becomes
negligible.

<KT> Here shows the result in the graphic which is 58.7ns on Core 2 Duo and 23.5ns on Xeon for
windows critical section. Think that Core 2 means dual core and Xeon is single so means that there
are lock contention for Core 2 and not for Xeon. That's why he mean that there is 0.6% overhead for
locking. Then it's more about experimenting of mutl-core.

Naturally, Ubuntu 11.10 provides a lock implementation using the POSIX Threads API as well. Itâs
another lightweight mutex, based on a Linux-specific construct known as a futex. A pair of
pthread_mutex_lock/pthread_mutex_unlock calls takes about 66 ns on my Core 2 Duo. You can even share
this implementation between processes, but I didnât test that.

In my previous post, I argued against the misconception that locks are slow and provided some data
to support the argument. At this point, it should be clear that if you arenât using a lightweight
mutex, the entire argument goes out the window. Iâm fairly sure that the existence of heavy lock
implementations has only added to this misconception over the years.

Some of you old-timers may point out ancient platforms where a heavy lock was the only
implementation available, or when a semaphore had to be used for the job. But it seems all modern
platforms offer a lightweight mutex. And even if they didnât, you could write your own lightweight
mutex at the application level, even sharing it between processes, provided youâre willing to live
with certain caveats. Youâll find one example in my followup post, Roll Your Own Lightweight Mutex.


{when-with-no-threads}
keitee.park@magnum ~
$ ./sem
sem run
56:645
56:702   [57]
keitee.park@magnum ~
$ ./mtx
mtx run
3:710
3:773    [63]

keitee.park@magnum ~
$ ./sem
sem run
5:663
5:719    [56]
keitee.park@magnum ~
$ ./mtx
mtx run
6:894
6:957    [63]

keitee.park@magnum ~
$ ./sem
sem run
8:103
8:160    [57]
keitee.park@magnum ~
$ ./mtx
mtx run
9:294
9:356    [62]

<for-semaphore-example>
#include <stdio.h>
#include <time.h>
#include <fcntl.h>
#include <semaphore.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

  unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

int main()
{
  int s = 0, loop = 0;
  int flags, opt;
  mode_t perms;
  unsigned int value;
  sem_t *sem;

  flags = O_CREAT | O_EXCL;
  perms = S_IRUSR | S_IWUSR;

  fprintf( stderr, "sem run\n");

  // will create /dev/shm/sem.demo
  sem = sem_open("/demo", flags, perms, 1 );
  if(sem == SEM_FAILED)
    fprintf( stderr, "fail on sem_open");

  get_time_ms();

  for(loop = 0; loop < MAXLOOP; loop++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "fail on sem_wait");

    s = sem_post(sem);
    if (s != 0)
    fprintf(stderr, "fail on sem_post");
  }

  get_time_ms();

  s = sem_unlink("/demo");
  if(s != 0)
    fprintf(stderr, "fail on sem_unlink\n" );

}

<for-mutex-example>
#include <stdio.h>
#include <time.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

  unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;

int main()
{
  int s = 0, loop = 0;

  fprintf( stderr, "mtx run\n");

  get_time_ms();

  for(loop = 0; loop < MAXLOOP; loop++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "pthread_mutex_unlock");
  }

  get_time_ms();
}


{when-with-two-threads}
keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
14:946
15:370      [424]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
24:498
24:829      [331]
main: this is the end

keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
33:705
34:207      [502]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
35:993
36:460      [467]
main: this is the end

keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
2:673
3:125       [452]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
4:792
5:224       [432]
main: this is the end

<for-semaphore-example>
#include <stdio.h>
#include <time.h>
#include <fcntl.h>
#include <semaphore.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

	unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static sem_t *sem;
static unsigned int loopA;
static unsigned int loopB;

static void* threadFuncA( void *arg )
{
  int s = 0;

  for(loopA = 0; loopA < MAXLOOP; loopA++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "TA: fail on sem_wait\n");

    s = sem_post(sem);
    if (s != 0)
    fprintf( stderr, "TA: fail on sem_post\n");
  }
}

static void* threadFuncB( void *arg )
{
  int s = 0;

  for(loopB = 0; loopB < MAXLOOP; loopB++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "TB: fail on sem_wait\n");

    s = sem_post(sem);
    if (s != 0)
    fprintf( stderr, "TB: fail on sem_post\n");
  }
}

int main()
{
  int flags, opt;
  mode_t perms;
  pthread_t tA, tB;
  int s;

  flags = O_CREAT | O_EXCL;
  perms = S_IRUSR | S_IWUSR;

  // will create /dev/shm/sem.demo
  sem = sem_open("/demo", flags, perms, 1 );
  if(sem == SEM_FAILED)
    fprintf( stderr, "main: fail on sem_open\n");

  fprintf( stderr, "main: this is the second sem run\n");
  get_time_ms();

  s = pthread_create( &tA, NULL, threadFuncA, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(A)\n");

  s = pthread_create( &tB, NULL, threadFuncB, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(B)\n");

  s = pthread_join( tA, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(A)\n");

  s = pthread_join( tB, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(B)\n");

  get_time_ms();
  fprintf( stderr, "main: this is the end\n");

  s = sem_unlink("/demo");
  if(s != 0)
    fprintf(stderr, "main: fail on sem_unlink\n" );
}

<for-mutex-example>
#include <stdio.h>
#include <time.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

	unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;
static unsigned int loopA;
static unsigned int loopB;

static void* threadFuncA( void *arg )
{
  int s = 0;

  for(loopA = 0; loopA < MAXLOOP; loopA++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "TA: pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "TA: pthread_mutex_unlock");
  }
}

static void* threadFuncB( void *arg )
{
  int s = 0;

  for(loopB = 0; loopB < MAXLOOP; loopB++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "TB: pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "TB: pthread_mutex_unlock");
  }
}

int main()
{
  pthread_t tA, tB;
  int s;

  fprintf( stderr, "main: this is the second mtx run\n");
  get_time_ms();

  s = pthread_create( &tA, NULL, threadFuncA, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(A)\n");

  s = pthread_create( &tB, NULL, threadFuncB, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(B)\n");

  s = pthread_join( tA, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(A)\n");

  s = pthread_join( tB, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(B)\n");

  get_time_ms();
  fprintf( stderr, "main: this is the end\n");
}


={============================================================================
*kt_linux_core_263*	sync: ref: lock-free code: a false sense of security

{one}
http://www.drdobbs.com/cpp/lock-free-code-a-false-sense-of-security/210600279

Lock-Free Code: A False Sense of Security By Herb Sutter, September 08, 2008

Writing lock-free code can confound anyoneâeven expert programmers, as Herb shows this month.

Given that lock-based synchronization has serious problems [1], it can be tempting to think
lock-free code must be the answer. Sometimes that is true. In particular, it's useful to have
libraries provide hash tables and other handy types whose implementations are internally
synchronized using lock-free techniques, such as Java's ConcurrentHashMap, so that we can use those
types safely from multiple threads without external synchronization and without having to understand
the subtle lock-free implementation details.
{Q} Java's ConcurrentHashMap

<two-drawbacks-for-lock-free-code>
But replacing locks wholesale by writing your own lock-free code is not the answer. Lock-free code
has two major drawbacks. First, it's not broadly useful for solving typical problems-lots of basic
data structures, even doubly linked lists, still have no known lock-free implementations. Coming up
with a new or improved lock-free data structure will still earn you at least a published paper in a
refereed journal, and sometimes a degree.

Second, it's hard even for experts. It's easy to write lock-free code that appears to work, but it's
very difficult to write lock-free code that is correct and performs well. Even good magazines and
refereed journals have published a substantial amount of lock-free code that was actually broken in
subtle ways and needed correction.

To illustrate, let's dissect some peer-reviewed lock-free code that was published here in DDJ just
two months ago [2]. The author, Petru Marginean, has graciously allowed me to dissect it here so
that we can see what's wrong and why, what lessons we should learn, and how to write the code
correctly. That someone as knowledgable as Petru, who has published many good and solid articles,
can get this stuff wrong should be warning enough that lock-free coding requires great care.
<KT> See Lock-Free Queues in below.

A Limited Lock-Free Queue

<limitation-or-assumption-of-q>
Marginean's goal was to write a limited lock-free queue that can be used safely without internal or
external locking. To simplify the problem, the article imposed some significant restrictions,
including that the queue must only be used from two threads with specific roles: one Producer thread
that inserts into the queue, and one Consumer thread that removes items from the queue.

Marginean uses a nice technique that is designed to prevent conflicts between the writer and reader:

o The producer and consumer always work in separate parts of the underlying list, so that their work
won't conflict. At any given time, the first "unconsumed" item is the one after the one iHead refers
to, and the last (most recently added) "unconsumed" item is the one before the one iTail refers to.

o The consumer increments iHead to tell the producer that it has consumed another item in the queue.

o The producer increments iTail to tell the consumer that another item is now available in the
queue. <o> Only the producer thread ever actually modifies the queue.<o> That means the producer is
responsible, not only for adding into the queue, but also for removing consumed items. To maintain
separation between the producer and consumer and prevent them from doing work in adjacent nodes, the
producer won't clean up the most recently consumed item (the one referred to by iHead).

<KT> The consumer only changes iterator and do not modify list itself.

Q     ...   <- begin()
      ...
      ...
      [ ]   <- head, -> consumer, dummy. uses tail to check empty, publish head and consume.
      [ ]   <- first unconsumed item
      ...
      [ ]   <- last unconsumed item
      [ ]   <- tail, <- producer, end(). add, publish tail and uses head to trim unused. 
               end() and push_back()

The idea is reasonable; only the implementation is fatally flawed. Here's the original code, written
in C++ and using an STL doubly linked list<T> as the underlying data structure. I've reformatted the
code slightly for presentation, and added a few comments for readability: 

// Original code from [1] (broken without external locking)
//
template <typename T>
struct LockFreeQueue {
  private:
    std::list<T> list;
    typename std::list<T>::iterator iHead, iTail;

  public:
    LockFreeQueue() {
      list.push_back(T());        // add dummy separator
      iHead = list.begin();
      iTail = list.end();
    }

    // Produce is called on the producer thread only:

    void Produce(const T& t) {
      list.push_back(t);               // add the new item
      iTail = list.end();              // <publish> it
      list.erase(list.begin(), iHead); // trim unused nodes
    }

    // Consume is called on the consumer thread only:

    bool Consume(T& t) {
      typename std::list<T>::iterator iNext = iHead;
      ++iNext;
      if (iNext != iTail) {         // if queue is nonempty
        iHead = iNext;              // <publish> that we took an item
        t = *iHead;                 // copy it back to the caller
        return true;                // and report success
      }
      return false;                 // else report queue was empty
    }
};

<lock-free-variable> <two-key-property>
The fundamental reason that the code is broken is that it has race conditions on both would-be
lock-free variables, iHead and iTail. To avoid a race, a lock-free variable must have two key
properties that we need to watch for and guarantee: atomicity and ordering. These variables are
neither.

Atomicity

First, reads and writes of a lock-free variable must be atomic. For this reason, lock-free variables
are typically no larger than the machine's native word size, and are usually pointers (C++), object
references (Java, .NET), or integers. Trying to use an ordinary list<T>::iterator variable as a
lock-free shared variable isn't a good idea and can't reliably meet the atomicity requirement, as we
will see.

Let's consider the races on iHead and iTail in these lines from Produce and Consume:

void Produce(const T& t) {
  ...
  iTail = list.end();
  list.erase(list.begin(), iHead);
}
 
bool Consume(T& t) {
  ...
  if (iNext != iTail) {
    iHead = iNext;
  ...   
  }
}

If reads and writes of iHead and iTail are not atomic, then Produce could read a partly updated (and
therefore corrupt) iHead and try to dereference it, and Consume could read a corrupt iTail and
fall off the end of the queue. Marginean does note this requirement:

"Reading/writing list<T>::iterator is atomic on the machine upon which you run the application." [2]

Alas, atomicity is necessary but not sufficient (see next section), and not supported by
list<T>::iterator. First, in practice, many list<T>::iterator implementations I examined are [larger]
than the native machine/pointer size, which means that they can't be read or written with atomic
loads and stores on most architectures. Second, in practice, even if they were of an appropriate
size, you'd have to add other decorations to the variable to ensure atomicity, for example to
require that the variable be properly [aligned] in memory.

Finally, the code isn't valid ISO C++. The 1998 C++ Standard said nothing about concurrency, and so
provided no such guarantees at all. The upcoming second C++ standard that is now being finalized
C++0x, does include a memory model and thread support, and explicitly forbids it. In brief, C++0x
says that the answer to questions such as, "What do I need to do to use a list<T> mylist
thread-safely?" is "Same as any other object"âif you know that an object like mylist is shared, you
must externally synchronize access to it, including via iterators, by protecting all such uses with
locks, else you've written a race [3]. Note: Using C++0x's std::atomic<> is not an option for
list<T>::iterator, because atomic<T> requires T to be a bit-copyable type, and STL types and their
iterators aren't guaranteed to be that.

Ordering Problems in Produce

Second, reads and writes of a lock-free variable must occur in an expected order, which is nearly
always the exact order they appear in the program source code. But compilers, processors, and caches
love to optimize reads and writes, and will helpfully reorder, invent, and remove memory reads and
writes unless you prevent it from happening. 

<to-prevent-reordering>
The right prevention happens implicitly when you use mutex locks or ordered atomic variables; {Q}
why mutex prevent this? 

C++0x std::atomic, Java/.NET volatile; you can also do it explicitly, but with considerably more
effort, using ordered API calls e.g., Win32 InterlockedExchange or memory fences/barriers e.g.,
Linux mb.  Trying to write lock-free code without using any of these tools can't possibly work.

Consider again this code from Produce, and ignore that the assignment iTail isn't atomic as we look
for other problems:

list.push_back(t);  // A: add the new item
iTail = list.end(); // B: publish it

This is a classic publication race because lines A and B can be (partly or entirely) reordered. For
example, let's say that some of the writes to the T object's members are delayed until after the
write to iTail, which publishes that the new object is available; then the consumer thread can see a
partly assigned T object.

What is the minimum necessary fix? We might be tempted to write a memory barrier between the two
lines:

// Is this change enough?
list.push_back(t);  // A: add the new item
mb();               // full fence
iTail = list.end(); // B: publish it

Before reading on, think about it and see if you're convinced that this is (or isn't) right.

Have you thought about it? As a starter, here's one issue: Although list.end is probably unlikely to
perform writes, it's possible that it might, and those are side effects that need to be complete
before we publish iTail. 

The general issue is that you can't make assumptions about the side effects of library functions you
call, and you have to make sure they're fully performed before you publish the new state. 

So a slightly improved version might try to store the result of list.end into a local unshared
variable and assign it after the barrier:

// Better, but is it enough?
list.push_back(t);
tmp = list.end();
mb();                // full fence
iTail = tmp;

Unfortunately, this still isn't enough. Besides the fact that assigning to iTail isn't atomic and
that we still have a race on iTail in general, compilers and processors can also invent writes to
iTail that break this code. Let's consider write invention in the context of another problem area:
Consume.

Ordering Problems in Consume

Here's another reordering problem, this time from Consume:

++iNext;
if (iNext != iTail) {
  iHead = iNext;        // C
  t = *iHead;           // D

Note that Consume updates iHead to advertise that it has consumed another item before it actually
  reads the item's value. Is that a problem? We might think it's innocuous, because the producer
  always leaves the iHead item alone to stay at least one item away from the part of the list the
  consumer is using.

It turns out this code is broken regardless of which order we write lines C and D, because the
compiler or processor or cache can reorder either version in unfortunate ways. Consider what happens
if the consumer thread performs a consecutive two calls to Consume: The memory reads and writes
performed by those two calls could be reordered so that iHead is incremented twice before we copy
the two list nodes' values, and then we have a problem because the producer may try to remove
nodes the consumer is still using. [KT] Think that this is a problem besides odering and lock-free
variable.

Note: This doesn't mean the compiler or processor transformations are broken; they're not. Rather
the code is racy and has insufficient synchronization, and so it breaks the memory model guarantees
and makes such transformations possible and visible.

Reordering isn't the only issue. Another problem is that compilers and processors can invent writes,
so they could inject a transient value:

// Problematic compiler/processor transformation
if (iNext != iTail) {
  iHead = 0xDEADBEEF;
  iHead = iNext;
  t = *iHead;

Clearly, that would break the producer thread, which would read a bad value for iHead. More likely,
the compiler or processor might speculate that most of the time iNext != iTail:

// Another problematic transformation
//
__temp = iHead;
iHead = iNext;  // speculatively set to iNext

if (iNext == iTail) {   // note: inverted test!
  iHead = __temp;   // undo if we guessed wrong
} else {
  t = *iHead;

<invariant>
But now iHead could equal iTail, which breaks the essential invariant that iHead must never equal
iTail, on which the whole design depends.

Can we solve these problems by writing line D before C, then separating them with a full fence? Not
entirely: That will prevent most of the aforementioned optimizations, but it will not eliminate all
of the problematic invented writes. More is needed.

Next Steps

These are a sample of the concurrency problems in the original code. Marginean showed a good
algorithm, but the implementation is broken because it uses an inappropriate type and performs
insufficient synchronization/ordering. Fixing the code will require a rewrite, because we need to
change the data structure and the code to let us use proper ordered atomic lock-free variables. But
how? Next month, we'll consider a fixed version. Stay tuned.

Notes

[1] H. Sutter, "The Trouble With Locks," C/C++ Users Journal, March 2005.
(www.ddj.com/cpp/184401930).

[2] P. Marginean, "Lock-Free Queues," Dr. Dobb's Journal, July 2008. (www.ddj.com/208801974).

[3] B. Dawes, et al., "Thread-Safety in the Standard Library," ISO/IEC JTC1/SC22/WG21 N2669, June
2008. (www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2669.htm).


{two}
http://www.drdobbs.com/parallel/lock-free-queues/208801974?pgno=1
Lock-Free Queues By Petru Marginean, July 01, 2008

<note-begin>
This article as written assumes a sequentially consistent model. In particular, the code relies on
specific order of instructions in both Consumer and Producer methods. However, without inserting
proper memory barrier instructions, these instructions can be reordered with unpredictable results
(see, for example, the classic Double-Checked Locking problem).

Another issue is using the standard std::list<T>. While the article mentions that it is the
developer responsibility to check that the reading/writing std::list<T>::iterator is atomic, this
turns out to be too restrictive. While gcc/MSVC++2003 has 4-byte iterators, the MSVC++2005 has
8-byte iterators in Release Mode and 12-byte iterators in the Debug Mode.

The solution to prevent this is to use memory barriers/volatile variables. 
The downloadable code featured at the top of this article has fixed that issue. ~

Many thanks to Herb Sutter who signaled the issue and helped me fix the code. --P.M.
<note-end>

Queues can be useful in a variety of systems involving data-stream processing. Typically, you have a
data source producing dataârequests coming to a web server, market feeds, or digital telephony
packetsâat a variable pace, and you need to process the data as fast as possible so there are no
losses. To do this, you can push data into a queue using one thread and process it using a different
threadâa good utilization of resources on multicore processors. One thread inserts data into the
queue, and the other reads/deletes elements from the queue. Your main requirement is that a
high-rate data burst does not last longer than the system's ability to accumulate data while the
consumer thread handles it. [KT] This means no overlow? 

The queue you use has to be threadsafe to prevent race conditions when inserting/removing data from
multiple threads. For obvious reasons, it is necessary that the queue mutual exclusion mechanism add
as little overhead as possible.

In this article, I present a lock-free queue (the source code for the lockfreequeue class is
available online; see www.ddj.com/code/) in which one thread can write to the queue and another
read from itâat the same time without any locking. /0807/lockfreequeue/

To do this, the code implements these requirements:

<conditions>
o There is a single writer (Producer) and single reader (Consumer). When you have multiple producers
and consumers, you can still use this queue with some external locking. You cannot have multiple
producers writing at the same time (or multiple consumers consuming the data simultaneously), but
you can have one producer and one consumer (2x threads) accessing the queue at the same time
(Responsibility: developer).

o When inserting/erasing to/from an std::list<T>, the iterators for the existing elements must
remain valid (Responsibility: library implementor).

o <Only> one thread modifies the queue; the producer thread both adds/erases elements in the queue
(Responsibility: library implementor).

o Beside the underlying std::list<T> used as the container, the lock-free queue class also holds two
iterators pointing to the not-yet-consumed range of elements; each is modified by one thread and
read by the other (Responsibility: library implementor).

o Reading/writing list<T>::iterator is <atomic> on the machine upon which you run the application. If
they are not on your implementation of STL, you should check whether the raw pointer's operations
are atomic. You could easily replace the iterators to be mentioned shortly with raw pointers in the
code (Responsibility: machine). <KT> ?

<KT> Here big condition is that one producer and consumer model, only producer modify a list, and
updating iterators between threads are atomic.

Because I use Standard C++, the code is portable under the aforementioned "machine" assumption: 

template <typename T>
struct LockFreeQueue
{
    LockFreeQueue();
    void Produce(const T& t);
    bool Consume(T& t);

  private:
    typedef std::list<T> TList;
    TList list;
    typename TList::iterator iHead, iTail;   <KT> why not 'TList::iterator iHead'?
};

Considering how simple this code is, you might wonder how can it be threadsafe. The magic is due to
design, not implementation. Take a look at the implementation of the Produce() and Consume()
methods. The Produce() method looks like this: 

void Produce(const T& t)
{
  list.push_back(t);
  iTail = list.end();
  list.erase(list.begin(), iHead);
}

To understand how this works, mentally separate the data from LockFreeQueue<T> into two groups:

o The list and the iTail iterator, modified by the Produce() method (Producer thread).
o The iHead iterator, modified by the Consume() method (Consumer thread). 

<o>
Produce() is the [only] method that changes the list (adding new elements and erasing the consumed
elements), and it is essential that [only] one thread ever calls Produce()âit's the Producer
thread! The iterator (iTail) (only manipulated by the Producer thread) changes it only after a new
element is added to the list. This way, when the Consumer thread is reading the iTail element, the
new added element is ready to be used. The Consume() method tries to read all the elements between
iHead and iTail (excluding both ends). 
<o>

bool Consume(T& t)
{
  typename TList::iterator iNext = iHead;
  ++iNext;
  if (iNext != iTail)
  {
    iHead = iNext;
    t = *iHead;
    return true;
  }
  return false;
}

This method reads the elements, but doesn't remove them from the list. Nor does it access the list
directly, but through the iterators. They are guaranteed to be valid after std::list<T> is modified,
so no matter what the Producer thread does to the list, you are safe to use them.

The std::list<T> maintains an element (pointed to by iHead) that is considered already read. For
this algorithm to work even when the queue was just created, I add an empty T() element in the
constructor of the LockFreeQueue<T> (see Figure 1): 

LockFreeQueue()
{
  list.push_back(T());
  iHead = list.begin();
  iTail = list.end();
}

<discussion-when-queue-is-empty>
Consume() may fail to read an element (and return false). Unlike traditional lock-based queues, this
queue works fast when the queue is not empty, but needs an external locking or polling method to
wait for data. Sometimes you want to wait if there is no element available in the queue, and avoid
returning false. A naive approach to waiting is: 

T Consume()
{
  T tmp;
  while (!Consume(tmp))
    ;
  return tmp;
}

This Consume() method will likely heat up one of your CPUs red-hot to 100-percent use if there are
no elements in the queue. Nevertheless, this should have good performance when the queue is not
empty. However, if you think of it, a queue that's almost never empty is a sign of systemic trouble:
It means the consumer is unable to keep pace with the producer, and sooner or later, the system is
doomed to die of memory exhaustion. Call this approach NAIVE_POLLING. 

A friendlier Consume() function does some pooling and calls some sort of sleep() or yield() function
available on your system: 

T Consume(int wait_time = 1/*milliseconds*/)
{
  T tmp;
  while (!Consume(tmp))
  {
    Sleep(wait_time/*milliseconds*/);
  }
  return tmp;
}

The DoSleep() can be implemented using nanosleep() (POSIX) or Sleep() (Windows), or even better,
using boost::thread::sleep(), which abstracts away system-dependent nomenclature. Call this
approach SLEEP. Instead of simple polling, you can use more advanced techniques to signal the
Consumer thread that a new element is available. I illustrate this in Listing One using a
boost::condition variable.

#include <boost/thread.hpp>
#include <boost/thread/condition.hpp>
#include <boost/thread/xtime.hpp>
    
template <typename T>
struct WaitFreeQueue
{
    void Produce(const T& t)
    {
        queue.Produce(t);
        cond.notify_one();
    }
    bool Consume(T& t)
    {
        return queue.Consume(t);
    }
    T Consume(int wait_time = 1/*milliseconds*/)
    {
        T tmp;
        if (Consume(tmp))
            return tmp;

        // the queue is empty, try again (possible waiting...)
        boost::mutex::scoped_lock lock(mtx);
        while (!Consume(tmp))          // line A
        {
            boost::xtime t;
            boost::xtime_get(&t, boost::TIME_UTC);
            AddMilliseconds(t, wait_time);
            cond.timed_wait(lock, t);  // line B
        }
        return tmp;
    }
private:
    LockFreeQueue<T> queue;
    boost::condition cond;
    boost::mutex mtx;
};

I used the timed_wait() instead of the simpler wait() to solve a possible deadlock when Produce() is
called between line A and line B in Listing One. Then wait() will miss the notify_one() call and
have to wait for the next produced element to wake up. <If> this element never comes (no more produced
elements or if the Produce() call actually waits for Consume() to return), there's a deadlock.
Call this approach TIME_WAIT.

The lock is still wait-free as long as there are elements in the queue. In this case, the Consumer()
thread does no waiting and reads data as fast as possible (even with the Producer() that is
inserting new elements). Only when the queue is exhausted does locking occur. 
<KT> claims that it is lock-free as long as queue is not empty.

The Ping-Pong Test

To compare the three approaches (NAIVE_POLLING, SLEEP, and TIME_WAIT), I implemented a test called
"Ping-Pong" that is similar to the game of table tennis (the source code is available online). In
Figure 2, there are two identical queues between the threads T1 and T2. 

      queue 1
T1 ------------> T2
   <------------
      queue 2

You first load one of the queues with a number of "balls," then ask each thread to read from one
queue and write to the other. The result is a controlled infinite loop. By limiting the game to a
fixed number of reads/writes ("shots"), you get an understanding of how the queue behaves when
varying the waiting/sleep time and strategy and the number of "balls." The faster the game, the
better the performance. You should also check CPU usage to see how much of it is used for real work.

o "No ball" means "do nothing" (like two players waiting for the other to start). This gives you an
idea of how good the queues are when there is no dataâhow nervous the players are. Ideally, CPU
usage should be zero.
 
o "One ball" is like the real ping-pong game: Each player shoots and waits for the other to shoot.

o "Two (or more) balls" means both players could shoot at the same time, modulo collision and
waiting issues.


In a wait-free system, the more balls in the game, the better the performance gain compared to the
classic locking strategy. This is because wait-free is an optimistic concurrency control method
(works best when there is no contention), while classical lock-based concurrency control is
pessimistic (assumes contention happens and preemptively inserts locking).

Ready to play? Here is the Ping-Pong test command line:

$> ./pingpong [strategy] [timeout] [balls] [shots]

When you run the program, the tests show the results in the table shown in Figure 3:

o The best combination is the timed_wait() with a small wait time (1ms in the test for TIMED_WAIT).
It has a very fast response time and almost 0 percent CPU usage when the queue is empty.

o Even when the sleep time is 0 (usleep(0)), the worst seems to be the sleep() method, especially
when the queue is likely to be empty. (The number of shots in this case is 100-times smaller than
the other cases because of the long duration of the game.)

o The NO_WAIT strategy is [fast] but behaves worst when there are no balls (100-percent CPU usage to
do nothing). It has the best performance when there is a single ball.

Figure 4 presents a table with the results for a classic approach (see SafeQueue). These results
show that this queue is, on average, more than four-times slower than the LockFreeQueue. The
slowdown comes from the synchronization between threads. Both Produce() and Consume() have to wait
for each other to finish. CPU usage is almost 100 percent for this test (similar to the NO_WAIT
strategy, but not even close to its performance).

Final Considerations

The single-threaded code below shows the value of the list.size() when Producing/ Consuming
elements: 

LockFreeQueue<int> q;   // list.size() == 1
q.Produce(1);           // list.size() == 2
int i;
q.Consume(i);           // list.size() == still 2!;
                        // Consume() doesn't modify the list
q.Produce(i);           // list.size() == 2 again;

The size of the queue is 1 if Produce() was never called and greater than 1 if any element was
produced.

No matter how many times Consume() is called, the list's size will stay constant. It is Produce()
that is increasing the size (by 1); and if there were consumed elements, it will also delete them
from the queue. In a way, Produce() acts as a simple garbage collector. <o> The whole thread safety
comes from the fact that specific data is modified from single threads only. The synchronization
between threads is done using iterators (or pointers, whichever has atomic read/write operation on
your machine). <o> Also consider this code: 

usleep(1000);    // sleep 1 microsecond

On the face of it, this line of code makes a thread sleep for 1 microsecond, and
then continue. In reality, 1 microsecond is just a lower bound to the duration
of the call.

The man page for usleep() says, "The usleep() function suspends execution of the
calling process for (at least) usec microseconds. The sleep may be lengthened
slightly by any system activity or by the time spent processing the call or by
the granularity of system timers," or if you use the nanosleep() function.
"Therefore, nanosleep() always pauses for at least the specified time; however,
    it can take up to 10 ms longer than specified until the process becomes
    runnable again."

So if the process is not scheduled under a real-time policy, there's no
guarantee when your thread will be running again. I've done some tests and (to
        my surprise) there are situations when code such as: 

cond.timed_wait(lock, x);    // x = e.g. 1 millisecond

will actually wait for more than 1 second.


{three}
http://www.drdobbs.com/cpp/the-trouble-with-locks/184401930
The Trouble with Locks

By Herb Sutter, March 01, 2005

References

[1] Sutter, H. "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software," Dr.
Dobb's Journal, March 2005. Available online at http://www.gotw.ca/publications/concurrency-ddj.htm.

[2] Sutter, H. "The Concurrency Revolution," C/C++ Users Journal, February 2005. This is an
abbreviated version of [1].

[3] Alexandrescu, A. "Lock-Free Data Structures," C/C++ Users Journal, October 2004.

[4] Alexandrescu, A. and M. Michael. "Lock-Free Data Structures with Hazard Pointers," C/C++ Users
Journal, December 2004.

[5] http://blogs.msdn.com/cbrumme. 

Lock-based programming may not be the best approach to building large concurrent programs.

In my most recent articles [1, 2], I presented reasons why concurrency (for example, multithreading)
will be the next revolution in the way we develop software â a sea change of the same order as the
object-oriented revolution. I also stated that "the vast majority of programmers today don't grok
concurrency, just as the vast majority of programmers 15 years ago didn't yet grok objects."

In this column, I'd like to consider just one question that several people wrote to ask, namely: "Is
concurrency really that hard?" In particular, a few readers felt that lock-based programming is well
understood; it is, after all, the status quo mainstream solution to concurrency control.

So, "is concurrency really that hard?" My short answer is this:

o Lock-based programming, our status quo, is difficult for experts to get right. Worse, it is also
fundamentally flawed for building large programs. This article focuses exclusively on lock-based
programming just because there's so much to say in even a 50,000-foot overview that I ran out of
room.

o Lock-free programming is difficult for gurus to get right. I'll save this for another time, but if
you're interested, you should check out Andrei Alexandrescu's recent articles for a sampling of the
issues in lock-free programming and hazard pointers [3, 4]. (Aside: Yes, I'm implying Andrei is a
guru. I hope he doesn't mind my outing him in public like this. I don't think it was much of a
secret.) (More aside: The hazard pointer work shows in particular why, if you're writing
lock-free data structures, you really really want garbage collection. You can do it yourself
without garbage collection, but it's like working with knives that are sharp on both edges and
don't have handles. But that's another article. Specifically, it's Andrei's other article.)

Unfortunately, today's reality is that only thoughtful experts can write explicitly concurrent
programs that are correct and efficient. This is because today's programming models for concurrency
are subtle, intricate, and fraught with pitfalls that easily (and frequently) result in unforeseen
races (i.e., program corruption) deadlocks (i.e., program lockup) and performance cliffs (e.g.,
priority inversion, convoying, and sometimes complete loss of parallelism and/or even worse
performance than a single-threaded program). And even when a correct and efficient concurrent
program is written, it takes great care to maintain â it's usually brittle and difficult to maintain
correctly because current programming models set a very high bar of expertise required to reason
reliably about the operation of concurrent programs, so that apparently innocuous changes to a
working concurrent program can (and commonly do, in practice) render it entirely or intermittently
nonworking in unintended and unexpected ways. Because getting it right and keeping it right is so
difficult, at many major software companies there is a veritable priesthood of gurus who write and
maintain the core concurrent code.

Some people think I'm overstating this, so let me amplify. In this article, I'll focus on just the
narrow question of how to write a lock-based program correctly, meaning that it works (avoids data
corruption) and doesn't hang (avoids deadlock and livelock). That's pretty much the minimum
requirement to write a program that runs at all.

Question: Is the following code thread-safe? If it is, why is it safe? If it isn't always, under
what conditions is it thread-safe?

T Add( T& a, T& b ) {
  T result;
  // ... read a and b and set result to
  // proper values ...
  return result;
}

There are a lot of possibilities here. Let's consider some of the major ones.

Lock-Based Solutions?

Assume that reading a T object isn't an atomic operation. Then, if a and/or b are accessible from
another thread, we have a classic race condition: While we are reading the values of a and/or b,
some other thread might be changing those objects, resulting in blowing up the program; if you're
lucky, say, by causing the object to follow an internal pointer some other thread just deleted; or
reading corrupt values.

How would you solve that? Please stop and think about it before reading on...

Ready? Okay: Now please stop a little longer and think about your solution some more, and consider
whether it might have any further holes, before reading on...

Now that you've thought about it deeply, let's consider some alternatives.

Today's typical lock-based approach is to acquire locks so that uses of a and b on one thread won't
interleave. Typically, this is done by acquiring a lock on an explicit synchronization object [a
mutex, for instance] that covers both objects, or by acquiring locks on an implicit mutex associated
with the objects themselves. To acquire a lock that covers both objects, Add has to know what that
lock is, either because a and b and their lock are globals [but then why pass a and b as
parameters?] or because the caller acquires the lock outside of Add [which is usually preferable].
To acquire a lock on each object individually, we could write:

T SharedAdd( T& a, T& b ) {
  T result;
  lock locka( a );  // lock is a helper
  // whose constructor acquires a lock
  lock lockb( b );  // and whose
  // destructor releases the lock
  // ... read a and b and set result to
  //  proper values ...
  return result;
} // release the locks 


{four}
http://www.drdobbs.com/parallel/writing-lock-free-code-a-corrected-queue/210604448?pgno=1
Writing Lock-Free Code: A Corrected Queue

By Herb Sutter, September 29, 2008

Notes

[1] H. Sutter. âLock-Free Code: A False Sense of Securityâ (DDJ, September 2008).
(www.ddj.com/cpp/210600279).

[2] P. Marginean. "Lock-Free Queues" (DDJ, July 2008). (www.ddj.com/208801974).

[3] This is just like a canonical exception safety patternâdo all the work off to the side, then
commit to accept the new state using nonthrowing operations only. "Think in transactions" applies
everywhere, and should be ubiquitous in the way we write our code.

[4] Compare-and-swap (CAS) is the most widely available fundamental lock-free operation and so I'll
focus on it here. However, some systems instead provide the equivalently powerful
load-linked/store-conditional (LL/SC) instead.


As we saw last month [1], lock-free coding is hard even for experts. There, I dissected a published
lock-free queue implementation [2] and examined why the code was quite broken. This month, let's see
how to do it right.

Lock-Free Fundamentals

When writing lock-free code, always keep these essentials well in mind:

Key concepts. Think in transactions. Know who owns what data. Key tool. The ordered atomic variable. 

When writing a lock-free data structure, "to think in transactions" means to make sure that each
operation on the data structure is atomic, all-or-nothing with respect to other concurrent
operations on that same data. The typical coding pattern to use is to do work off to the side, then
"publish" each change to the shared data with a single atomic write or compare-and-swap. [3] Be sure
that concurrent writers don't interfere with each other or with concurrent readers, and pay special
attention to any operations that delete or remove data that a concurrent operation might still be
using.

Be highly aware of who owns what data at any given time; mistakes mean races where two threads think
they can proceed with conflicting work. You know who owns a given piece of shared data right now by
looking at the value of the ordered atomic variable that says who it is. To hand off ownership of
some data to another thread, do it at the end of a transaction with a single atomic operation that
means "now it's your's."

An ordered atomic variable is a "lock-free-safe" variable with the following properties that make it
safe to read and write across threads without any explicit locking:

Atomicity. Each individual read and write is guaranteed to be atomic with respect to all other reads
and writes of that variable. The variables typically fit into the machine's native word size, and so
are usually pointers (C++), object references (Java, .NET), or integers. Order. Each read and write
is guaranteed to be executed in source code order. Compilers, CPUs, and caches will respect it and
not try to optimize these operations the way they routinely distort reads and writes of ordinary
variables. Compare-and-swap (CAS) [4]. There is a special operation you can call using a syntax like
variable.compare_exchange( expectedValue, newValue ) that does the following as an atomic operation:
If variable currently has the value expectedValue, it sets the value to newValue and returns true;
else returns false. A common use is if(variable.compare_exchange(x,y)), which you should get in the
habit of reading as, "if I'm the one who gets to change variable from x to y." 

Ordered atomic variables are spelled in different ways on popular platforms and environments. For
example:

volatile in C#/.NET, as in volatile int. volatile or * Atomic* in Java, as in volatile int,
AtomicInteger. atomic<T> in C++0x, the forthcoming ISO C++ Standard, as in atomic<int>. 

In the code that follows, I'm going to highlight the key reads and writes of such a variable; these
variables should leap out of the screen at you, and you should get used to being very aware of every
time you touch one.

If you don't yet have ordered atomic variables yet on your language and platform, you can emulate
them by using ordinary but aligned variables whose reads and writes are guaranteed to be naturally
atomic, and enforce ordering by using either platform-specific ordered API calls (such as Win32's
InterlockedCompareExchange for compare-and-swap) or platform-specific explicit memory
fences/barriers (for example, Linux mb).


={============================================================================
*kt_linux_core_264*	conc: ref: the free lunch is over 
http://www.gotw.ca/publications/concurrency-ddj.htm

The Free Lunch Is Over
A Fundamental Turn Toward Concurrency in Software

By Herb Sutter

The biggest sea change in software development since the OO revolution is knocking at the door, and
its name is Concurrency.

This article appeared in Dr. Dobb's Journal, 30(3), March 2005. A much briefer version under the
title "The Concurrency Revolution" appeared in C/C++ Users Journal, 23(2), February 2005.

Update note: The CPU trends graph last updated August 2009 to include current data and show the
trend continues as predicted. The rest of this article including all text is still original as first
posted here in December 2004.

Your free lunch will soon be over. What can you do about it? What are you doing about it?

The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have
run out of room with most of their traditional approaches to boosting CPU performance. Instead of
driving clock speeds and straight-line instruction throughput ever higher, they are instead turning
en masse to hyperthreading and multicore architectures. Both of these features are already available
on chips today; in particular, multicore is available on current PowerPC and Sparc IV processors,
   and is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004 In-Stat/MDR Fall
   Processor Forum was multicore devices, as many companies showed new or updated multicore
   processors. Looking back, itâs not much of a stretch to call 2004 the year of multicore.

And that puts us at a fundamental turning point in software development, at least for the next few
years and for applications targeting general-purpose desktop computers and low-end servers (which
    happens to account for the vast bulk of the dollar value of software sold today). In this
article, Iâll describe the changing face of hardware, why it suddenly does matter to software, and
how specifically the concurrency revolution matters to you and is going to change the way you will
likely be writing software in the future.

Arguably, the free lunch has already been over for a year or two, only weâre just now noticing.  

<The Free Performance Lunch>

Thereâs an interesting phenomenon thatâs known as âAndy giveth, and Bill taketh away.â No matter how
fast processors get, software consistently finds new ways to eat up the extra speed. Make a CPU ten
times as fast, and software will usually find ten times as much to do (or, in some cases, will feel
    at liberty to do it ten times less efficiently). Most classes of applications have enjoyed free
and regular performance gains for several decades, even without releasing new versions or doing
anything special, because the CPU manufacturers (primarily) and memory and disk manufacturers
(secondarily) have reliably enabled ever-newer and ever-faster mainstream systems. Clock speed isnât
the only measure of performance, or even necessarily a good one, but itâs an instructive one: Weâre
used to seeing 500MHz CPUs give way to 1GHz CPUs give way to 2GHz CPUs, and so on. Today weâre in
the 3GHz range on mainstream computers.

The key question is: When will it end? After all, Mooreâs Law predicts exponential growth, and
clearly exponential growth canât continue forever before we reach hard physical limits; light isnât
getting any faster. The growth must eventually slow down and even end. (Caveat: Yes, Mooreâs Law
    applies principally to transistor densities, but the same kind of exponential growth has
    occurred in related areas such as clock speeds. Thereâs even faster growth in other spaces, most
    notably the data storage explosion, but that important trend belongs in a different article.)

If youâre a software developer, chances are that you have already been riding the âfree lunchâ wave
of desktop computer performance. Is your applicationâs performance borderline for some local
operations? âNot to worry,â the conventional (if suspect) wisdom goes; âtomorrowâs processors will
have even more throughput, and anyway todayâs applications are increasingly throttled by factors
other than CPU throughput and memory speed (e.g., theyâre often I/O-bound, network-bound,
    database-bound).â Right?

Right enough, in the past. But dead wrong for the foreseeable future.

The good news is that processors are going to continue to become more powerful. The bad news is
that, at least in the short term, the growth will come mostly in directions that do not take most
current applications along for their customary free ride.

Over the past 30 years, CPU designers have achieved performance gains in three main areas, the first
two of which focus on straight-line execution flow:

clock speed
execution optimization
cache

Increasing clock speed is about getting more cycles. Running the CPU faster more or less directly
means doing the same work faster.

Optimizing execution flow is about doing more work per cycle. Todayâs CPUs sport some more powerful
instructions, and they perform optimizations that range from the pedestrian to the exotic, including
pipelining, branch prediction, executing multiple instructions in the same clock cycle(s), and even
reordering the instruction stream for out-of-order execution. These techniques are all designed to
make the instructions flow better and/or execute faster, and to squeeze the most work out of each
clock cycle by reducing latency and maximizing the work accomplished per clock cycle.

Chip designers are under so much pressure to deliver ever-faster CPUs that theyâll risk changing the
meaning of your program, and possibly break it, in order to make it run faster

Brief aside on instruction reordering and memory models: Note that some of what I just called
âoptimizationsâ are actually far more than optimizations, in that they can change the meaning of
programs and cause visible effects that can break reasonable programmer expectations. This is
significant. CPU designers are generally sane and well-adjusted folks who normally wouldnât hurt a
fly, and wouldnât think of hurting your codeâ¦ normally. But in recent years they have been willing
to pursue aggressive optimizations just to wring yet more speed out of each cycle, even knowing full
well that these aggressive rearrangements could endanger the semantics of your code. Is this Mr.
Hyde making an appearance? Not at all. That willingness is simply a clear indicator of the extreme
pressure the chip designers face to deliver ever-faster CPUs; theyâre under so much pressure that
theyâll risk changing the meaning of your program, and possibly break it, in order to make it run
faster. Two noteworthy examples in this respect are write reordering and read reordering: Allowing a
processor to reorder write operations has consequences that are so surprising, and break so many
programmer expectations, that the feature generally has to be turned off because itâs too difficult
for programmers to reason correctly about the meaning of their programs in the presence of arbitrary
write reordering. Reordering read operations can also yield surprising visible effects, but that is
more commonly left enabled anyway because it isnât quite as hard on programmers, and the demands for
performance cause designers of operating systems and operating environments to compromise and choose
models that place a greater burden on programmers because that is viewed as a lesser evil than
giving up the optimization opportunities.

Finally, increasing the size of on-chip cache is about staying away from RAM. Main memory continues
to be so much slower than the CPU that it makes sense to put the data closer to the processorâand
you canât get much closer than being right on the die. On-die cache sizes have soared, and today
most major chip vendors will sell you CPUs that have 2MB and more of on-board L2 cache. (Of these
    three major historical approaches to boosting CPU performance, increasing cache is the only one
    that will continue in the near term. Iâll talk a little more about the importance of cache later
    on.)

Okay. So what does this mean?

A fundamentally important thing to recognize about this list is that all of these areas are
concurrency-agnostic. Speedups in any of these areas will directly lead to speedups in sequential
(nonparallel, single-threaded, single-process) applications, as well as applications that do make
use of concurrency. Thatâs important, because the vast majority of todayâs applications are
single-threaded, for good reasons that Iâll get into further below.

Of course, compilers have had to keep up; sometimes you need to recompile your application, and
target a specific minimum level of CPU, in order to benefit from new instructions (e.g., MMX, SSE)
  and some new CPU features and characteristics. But, by and large, even old applications have
  always run significantly fasterâeven without being recompiled to take advantage of all the new
  instructions and features offered by the latest CPUs.

That world was a nice place to be. Unfortunately, it has already disappeared.

<Obstacles, and Why You Donât Have 10GHz Today>

CPU performance growth as we have known it hit a wall two years ago. Most people have only recently
started to notice.

You can get similar graphs for other chips, but Iâm going to use Intel data here. Figure 1 graphs
the history of Intel chip introductions by clock speed and number of transistors. The number of
transistors continues to climb, at least for now. Clock speed, however, is a different story.

Around the beginning of 2003, youâll note a disturbing sharp turn in the previous trend toward
ever-faster CPU clock speeds. Iâve added lines to show the limit trends in maximum clock speed;
instead of continuing on the previous path, as indicated by the thin dotted line, there is a sharp
flattening. It has become harder and harder to exploit higher clock speeds due to not just one but
several physical issues, notably heat (too much of it and too hard to dissipate), power consumption
(too high), and current leakage problems.

Quick: Whatâs the clock speed on the CPU(s) in your current workstation? Are you running at 10GHz?
On Intel chips, we reached 2GHz a long time ago (August 2001), and according to CPU trends before
2003, now in early 2005 we should have the first 10GHz Pentium-family chips. A quick look around
shows that, well, actually, we donât. Whatâs more, such chips are not even on the horizonâwe have no
good idea at all about when we might see them appear.

Well, then, what about 4GHz? Weâre at 3.4GHz alreadyâsurely 4GHz canât be far away? Alas, even 4GHz
seems to be remote indeed. In mid-2004, as you probably know, Intel first delayed its planned
introduction of a 4GHz chip until 2005, and then in fall 2004 it officially abandoned its 4GHz plans
entirely. As of this writing, Intel is planning to ramp up a little further to 3.73GHz in early 2005
(already included in Figure 1 as the upper-right-most dot), but the clock race really is over, at
least for now; Intelâs and most processor vendorsâ future lies elsewhere as chip companies
aggressively pursue the same new multicore directions.

Weâll probably see 4GHz CPUs in our mainstream desktop machines someday, but it wonât be in 2005.
Sure, Intel has samples of their chips running at even higher speeds in the labâbut only by heroic
efforts, such as attaching hideously impractical quantities of cooling equipment. You wonât have
that kind of cooling hardware in your office any day soon, let alone on your lap while computing on
the plane.

<Myths and Realities: 2 x 3GHz < 6 GHz>

So a dual-core CPU that combines two 3GHz cores practically offers 6GHz of processing power. Right?

Wrong. Even having two threads running on two physical processors doesnât mean getting two times the
performance. Similarly, most multi-threaded applications wonât run twice as fast on a dual-core box.
They should run faster than on a single-core CPU; the performance gain just isnât linear, thatâs
all.

Why not? First, there is coordination overhead between the cores to ensure cache coherency (a
    consistent view of cache, and of main memory) and to perform other handshaking. Today, a two- or
four-processor machine isnât really two or four times as fast as a single CPU even for
multi-threaded applications. The problem remains essentially the same even when the CPUs in question
sit on the same die.

Second, unless the two cores are running different processes, or different threads of a single
process that are well-written to run independently and almost never wait for each other, they wonât
be well utilized. (Despite this, I will speculate that todayâs single-threaded applications as
    actually used in the field could actually see a performance boost for most users by going to a
    dual-core chip, not because the extra core is actually doing anything useful, but because it is
    running the adware and spyware that infest many usersâ systems and are otherwise slowing down
    the single CPU that user has today. I leave it up to you to decide whether adding a CPU to run
    your spyware is the best solution to that problem.)

If youâre running a single-threaded application, then the application can only make use of one core.
There should be some speedup as the operating system and the application can run on separate cores,
      but typically the OS isnât going to be maxing out the CPU anyway so one of the cores will be
      mostly idle. (Again, the spyware can share the OSâs core most of the time.) 
  

<TANSTAAFL: Mooreâs Law and the Next Generation(s)>

âThere ainât no such thing as a free lunch.â âR. A. Heinlein, The Moon Is a Harsh Mistress

Does this mean Mooreâs Law is over? Interestingly, the answer in general seems to be no. Of course,
     like all exponential progressions, Mooreâs Law must end someday, but it does not seem to be in
     danger for a few more years yet. Despite the wall that chip engineers have hit in juicing up
     raw clock cycles, transistor counts continue to explode and it seems CPUs will continue to
     follow Mooreâs Law-like throughput gains for some years to come.  The key difference, which is
     the heart of this article, is that the performance gains are going to be accomplished in
     fundamentally different ways for at least the next couple of processor generations. And most
     current applications will no longer benefit from the free ride without significant redesign.

For the near-term future, meaning for the next few years, the performance gains in new chips will be
fueled by three main approaches, only one of which is the same as in the past. The near-term future
performance growth drivers are:

hyperthreading
multicore
cache

Hyperthreading is about running two or more threads in parallel inside a single CPU. Hyperthreaded
CPUs are already available today, and they do allow some instructions to run in parallel. A limiting
factor, however, is that although a hyper-threaded CPU has some extra hardware including extra
registers, it still has just one cache, one integer math unit, one FPU, and in general just one each
of most basic CPU features. Hyperthreading is sometimes cited as offering a 5% to 15% performance
boost for reasonably well-written multi-threaded applications, or even as much as 40% under ideal
conditions for carefully written multi-threaded applications. Thatâs good, but itâs hardly double,
           and it doesnât help single-threaded applications.

Multicore is about running two or more actual CPUs on one chip. Some chips, including Sparc and
PowerPC, have multicore versions available already. The initial Intel and AMD designs, both due in
2005, vary in their level of integration but are functionally similar. AMDâs seems to have some
initial performance design advantages, such as better integration of support functions on the same
die, whereas Intelâs initial entry basically just glues together two Xeons on a single die. The
performance gains should initially be about the same as having a true dual-CPU system (only the
    system will be cheaper because the motherboard doesnât have to have two sockets and associated
    âglueâ chippery), which means something less than double the speed even in the ideal case, and
just like today it will boost reasonably well-written multi-threaded applications. Not
single-threaded ones.

Finally, on-die cache sizes can be expected to continue to grow, at least in the near term. Of these
three areas, only this one will broadly benefit most existing applications. The continuing growth in
on-die cache sizes is an incredibly important and highly applicable benefit for many applications,
  simply because space is speed. Accessing main memory is expensive, and you really donât want to
  touch RAM if you can help it. On todayâs systems, a cache miss that goes out to main memory often
  costs 10 to 50 times as much getting the information from the cache; this, incidentally, continues
  to surprise people because we all think of memory as fast, and it is fast compared to disks and
  networks, but not compared to on-board cache which runs at faster speeds. If an applicationâs
  working set fits into cache, weâre golden, and if it doesnât, weâre not. That is why increased
  cache sizes will save some existing applications and breathe life into them for a few more years
  without requiring significant redesign: As existing applications manipulate more and more data,
  and as they are incrementally updated to include more code for new features, performance-sensitive
  operations need to continue to fit into cache. As the Depression-era old-timers will be quick to
  remind you, âCache is king.â

(Aside: Hereâs an anecdote to demonstrate âspace is speedâ that recently hit my compiler team. The
 compiler uses the same source base for the 32-bit and 64-bit compilers; the code is just compiled
 as either a 32-bit process or a 64-bit one. The 64-bit compiler gained a great deal of baseline
 performance by running on a 64-bit CPU, principally because the 64-bit CPU had many more registers
 to work with and had other code performance features. All well and good. But what about data? Going
 to 64 bits didnât change the size of most of the data in memory, except that of course pointers in
 particular were now twice the size they were before. As it happens, our compiler uses pointers much
 more heavily in its internal data structures than most other kinds of applications ever would.
 Because pointers were now 8 bytes instead of 4 bytes, a pure data size increase, we saw a
 significant increase in the 64-bit compilerâs working set. That bigger working set caused a
 performance penalty that almost exactly offset the code execution performance increase weâd gained
 from going to the faster processor with more registers. As of this writing, the 64-bit compiler
 runs at the same speed as the 32-bit compiler, even though the source base is the same for both and
 the 64-bit processor offers better raw processing throughput. Space is speed.)

But cache is it. Hyperthreading and multicore CPUs will have nearly no impact on most current
applications.

So what does this change in the hardware mean for the way we write software? By now youâve probably
noticed the basic answer, so letâs consider it and its consequences.

<What This Means For Software: The Next Revolution>

In the 1990s, we learned to grok objects. The revolution in mainstream software development from
structured programming to object-oriented programming was the greatest such change in the past 20
years, and arguably in the past 30 years. There have been other changes, including the most recent
(and genuinely interesting) naissance of web services, but nothing that most of us have seen during
our careers has been as fundamental and as far-reaching a change in the way we write software as the
object revolution.

Until now.

Starting today, the performance lunch isnât free any more. Sure, there will continue to be generally
applicable performance gains that everyone can pick up, thanks mainly to cache size improvements.
But if you want your application to benefit from the continued exponential throughput advances in
new processors, it will need to be a well-written concurrent (usually multithreaded) application.
And thatâs easier said than done, because not all problems are inherently parallelizable and because
concurrent programming is hard.

I can hear the howls of protest: âConcurrency? Thatâs not news! People are already writing
concurrent applications.â Thatâs true. Of a small fraction of developers.

Remember that people have been doing object-oriented programming since at least the days of Simula
in the late 1960s. But OO didnât become a revolution, and dominant in the mainstream, until the
1990s. Why then? The reason the revolution happened was primarily that our industry was driven by
requirements to write larger and larger systems that solved larger and larger problems and exploited
the greater and greater CPU and storage resources that were becoming available. OOPâs strengths in
abstraction and dependency management made it a necessity for achieving large-scale software
development that is economical, reliable, and repeatable.

Concurrency is the next major revolution in how we write software

Similarly, weâve been doing concurrent programming since those same dark ages, writing coroutines
and monitors and similar jazzy stuff. And for the past decade or so weâve witnessed incrementally
more and more programmers writing concurrent (multi-threaded, multi-process) systems. But an actual
revolution marked by a major turning point toward concurrency has been slow to materialize. Today
the vast majority of applications are single-threaded, and for good reasons that Iâll summarize in
the next section.

By the way, on the matter of hype: People have always been quick to announce âthe next software
development revolution,â usually about their own brand-new technology. Donât believe it. New
technologies are often genuinely interesting and sometimes beneficial, but the biggest revolutions
in the way we write software generally come from technologies that have already been around for some
years and have already experienced gradual growth before they transition to explosive growth. This
is necessary: You can only base a software development revolution on a technology thatâs mature
enough to build on (including having solid vendor and tool support), and it generally takes any new
software technology at least seven years before itâs solid enough to be broadly usable without
performance cliffs and other gotchas. As a result, true software development revolutions like OO
happen around technologies that have already been undergoing refinement for years, often decades.
Even in Hollywood, most genuine âovernight successesâ have really been performing for many years
before their big break.

Concurrency is the next major revolution in how we write software. Different experts still have
different opinions on whether it will be bigger than OO, but that kind of conversation is best left
to pundits. For technologists, the interesting thing is that concurrency is of the same order as OO
both in the (expected) scale of the revolution and in the complexity and learning curve of the
technology.

<Benefits and Costs of Concurrency>

There are two major reasons for which concurrency, especially multithreading, is already used in
mainstream software. The first is to logically separate naturally independent control flows; for
example, in a database replication server I designed it was natural to put each replication session
on its own thread, because each session worked completely independently of any others that might be
active (as long as they werenât working on the same database row). The second and less common reason
to write concurrent code in the past has been for performance, either to scalably take advantage of
multiple physical CPUs or to easily take advantage of latency in other parts of the application; in
my database replication server, this factor applied as well and the separate threads were able to
scale well on multiple CPUs as our server handled more and more concurrent replication sessions with
many other servers.

There are, however, real costs to concurrency. Some of the obvious costs are actually relatively
unimportant. For example, yes, locks can be expensive to acquire, but when used judiciously and
properly you gain much more from the concurrent execution than you lose on the synchronization, if
you can find a sensible way to parallelize the operation and minimize or eliminate shared state.

Perhaps the second-greatest cost of concurrency is that not all applications are amenable to
parallelization. Iâll say more about this later on.

Probably the greatest cost of concurrency is that concurrency really is hard: The programming model,
         meaning the model in the programmerâs head that he needs to reason reliably about his
         program, is much harder than it is for sequential control flow.

Everybody who learns concurrency thinks they understand it, ends up finding mysterious races they
thought werenât possible, and discovers that they didnât actually understand it yet after all. As
the developer learns to reason about concurrency, they find that usually those races can be caught
by reasonable in-house testing, and they reach a new plateau of knowledge and comfort. What usually
doesnât get caught in testing, however, except in shops that understand why and how to do real
stress testing, is those latent concurrency bugs that surface only on true multiprocessor systems,
       where the threads arenât just being switched around on a single processor but where they
       really do execute truly simultaneously and thus expose new classes of errors. This is the
       next jolt for people who thought that surely now they know how to write concurrent code: Iâve
       come across many teams whose application worked fine even under heavy and extended stress
       testing, and ran perfectly at many customer sites, until the day that a customer actually had
       a real multiprocessor machine and then deeply mysterious races and corruptions started to
       manifest intermittently. In the context of todayâs CPU landscape, then, redesigning your
       application to run multithreaded on a multicore machine is a little like learning to swim by
       jumping into the deep endâgoing straight to the least forgiving, truly parallel environment
       that is most likely to expose the things you got wrong. Even when you have a team that can
       reliably write safe concurrent code, there are other pitfalls; for example, concurrent code
       that is completely safe but isnât any faster than it was on a single-core machine, typically
       because the threads arenât independent enough and share a dependency on a single resource
       which re-serializes the programâs execution. This stuff gets pretty subtle.

The vast majority of programmers today donât grok concurrency, just as the vast majority of
programmers 15 years ago didnât yet grok objects

Just as it is a leap for a structured programmer to learn OO (whatâs an object? whatâs a virtual
    function? how should I use inheritance? and beyond the âwhatsâ and âhows,â why are the correct
    design practices actually correct?), itâs a leap of about the same magnitude for a sequential
programmer to learn concurrency (whatâs a race? whatâs a deadlock? how can it come up, and how do I
    avoid it? what constructs actually serialize the program that I thought was parallel? how is the
    message queue my friend? and beyond the âwhatsâ and âhows,â why are the correct design practices
    actually correct?).

The vast majority of programmers today donât grok concurrency, just as the vast majority of
programmers 15 years ago didnât yet grok objects. But the concurrent programming model is learnable,
            particularly if we stick to message- and lock-based programming, and once grokked it
            isnât that much harder than OO and hopefully can become just as natural. Just be ready
            and allow for the investment in training and time, for you and for your team.

(I deliberately limit the above to message- and lock-based concurrent programming models. There is
 also lock-free programming, supported most directly at the language level in Java 5 and in at least
 one popular C++ compiler. But concurrent lock-free programming is known to be very much harder for
 programmers to understand and reason about than even concurrent lock-based programming. Most of the
 time, only systems and library writers should have to understand lock-free programming, although
 virtually everybody should be able to take advantage of the lock-free systems and libraries those
 people produce. Frankly, even lock-based programming is hazardous.)

<What It Means For Us>

Okay, back to what it means for us.

1. The clear primary consequence weâve already covered is that applications will increasingly need
to be concurrent if they want to fully exploit CPU throughput gains that have now started becoming
available and will continue to materialize over the next several years. For example, Intel is
talking about someday producing 100-core chips; a single-threaded application can exploit at most
1/100 of such a chipâs potential throughput. âOh, performance doesnât matter so much, computers just
keep getting fasterâ has always been a naÃ¯ve statement to be viewed with suspicion, and for the near
future it will almost always be simply wrong.

Applications will increasingly need to be concurrent if they want to fully exploit continuing
exponential CPU throughput gains

Efficiency and performance optimization will get more, not less, important

Now, not all applications (or, more precisely, important operations of an application) are amenable
to parallelization. True, some problems, such as compilation, are almost ideally parallelizable. But
others arenât; the usual counterexample here is that just because it takes one woman nine months to
produce a baby doesnât imply that nine women could produce one baby in one month. Youâve probably
come across that analogy before. But did you notice the problem with leaving the analogy at that?
Hereâs the trick question to ask the next person who uses it on you: Can you conclude from this that
the Human Baby Problem is inherently not amenable to parallelization? Usually people relating this
analogy err in quickly concluding that it demonstrates an inherently nonparallel problem, but thatâs
actually not necessarily correct at all. It is indeed an inherently nonparallel problem if the goal
is to produce one child. It is actually an ideally parallelizable problem if the goal is to produce
many children! Knowing the real goals can make all the difference. This basic goal-oriented
principle is something to keep in mind when considering whether and how to parallelize your
software.

2. Perhaps a less obvious consequence is that applications are likely to become increasingly
CPU-bound. Of course, not every application operation will be CPU-bound, and even those that will be
affected wonât become CPU-bound overnight if they arenât already, but we seem to have reached the
end of the âapplications are increasingly I/O-bound or network-bound or database-boundâ trend,
    because performance in those areas is still improving rapidly (gigabit WiFi, anyone?) while
    traditional CPU performance-enhancing techniques have maxed out. Consider: Weâre stopping in the
    3GHz range for now. Therefore single-threaded programs are likely not to get much faster any
    more for now except for benefits from further cache size growth (which is the main good news).
    Other gains are likely to be incremental and much smaller than weâve been used to seeing in the
    past, for example as chip designers find new ways to keep pipelines full and avoid stalls, which
    are areas where the low-hanging fruit has already been harvested. The demand for new application
    features is unlikely to abate, and even more so the demand to handle vastly growing quantities
    of application data is unlikely to stop accelerating. As we continue to demand that programs do
    more, they will increasingly often find that they run out of CPU to do it unless they can code
    for concurrency.

There are two ways to deal with this sea change toward concurrency. One is to redesign your
applications for concurrency, as above. The other is to be frugal, by writing code that is more
efficient and less wasteful. This leads to the third interesting consequence:

3. Efficiency and performance optimization will get more, not less, important. Those languages that
already lend themselves to heavy optimization will find new life; those that donât will need to find
ways to compete and become more efficient and optimizable. Expect long-term increased demand for
performance-oriented languages and systems.

4. Finally, programming languages and systems will increasingly be forced to deal well with
concurrency. The Java language has included support for concurrency since its beginning, although
mistakes were made that later had to be corrected over several releases in order to do concurrent
programming more correctly and efficiently. The C++ language has long been used to write heavy-duty
multithreaded systems well, but it has no standardized support for concurrency at all (the ISO C++
    standard doesnât even mention threads, and does so intentionally), and so typically the
concurrency is of necessity accomplished by using nonportable platform-specific concurrency features
and libraries. (Itâs also often incomplete; for example, static variables must be initialized only
    once, which typically requires that the compiler wrap them with a lock, but many C++
    implementations do not generate the lock.) Finally, there are a few concurrency standards,
    including pthreads and OpenMP, and some of these support implicit as well as explicit
    parallelization. Having the compiler look at your single-threaded program and automatically
    figure out how to parallelize it implicitly is fine and dandy, but those automatic
    transformation tools are limited and donât yield nearly the gains of explicit concurrency
    control that you code yourself. The mainstream state of the art revolves around lock-based
    programming, which is subtle and hazardous. We desperately need a higher-level programming model
    for concurrency than languages offer today; I'll have more to say about that soon.

<Conclusion>

If you havenât done so already, now is the time to take a hard look at the design of your
application, determine what operations are CPU-sensitive now or are likely to become so soon, and
identify how those places could benefit from concurrency. Now is also the time for you and your team
to grok concurrent programmingâs requirements, pitfalls, styles, and idioms.

A few rare classes of applications are naturally parallelizable, but most arenât. Even when you know
exactly where youâre CPU-bound, you may well find it difficult to figure out how to parallelize
those operations; all the most reason to start thinking about it now. Implicitly parallelizing
compilers can help a little, but donât expect much; they canât do nearly as good a job of
parallelizing your sequential program as you could do by turning it into an explicitly parallel and
threaded version.

Thanks to continued cache growth and probably a few more incremental straight-line control flow
optimizations, the free lunch will continue a little while longer; but starting today the buffet
will only be serving that one entrÃ©e and that one dessert. The filet mignon of throughput gains is
still on the menu, but now it costs extraâextra development effort, extra code complexity, and extra
testing effort. The good news is that for many classes of applications the extra effort will be
worthwhile, because concurrency will let them fully exploit the continuing exponential gains in
processor throughput.


={============================================================================
*kt_linux_core_265* sync: case: subtle race

This is to make sure that callback is called exactly once when there is an event
which calls code below. Thing is that if not remove it, it will keep adding
callbacks. So if there is callabck which is already in a queue, then remove the
new. The assumption was that the same events happens more but need to call
callback once asynchronously.

{
  // this add a callback to the glib main thread which is different from the
  // current thread.
  guint task_handle = g_idle_add( video_changed_message_callback, sink);

  // this set sink->task_handle atomic variable with new tast_handle when it
  // is 0 and return TRUE when it succeed.
  if (0 != g_atomic_int_compare_and_exchange(&sink->task_handle, 0,
        task_handle))
  {
    // There is already a task waiting to run, so we can cancel this one.
    g_source_remove(temporary_change_task_handle);
  }
}

The first problem is the wrong interpretation of the return value of
g_atomic_int_compare_and_exchange(). It doesn't return the old value of the
atomic variable. It returns TRUE if exchange took place and FALSE otherwise.
Because of that, we were proceeding to g_source_remove() when the atomic
variable went from zero to a non-zero value and it returns TRUE, which is wrong. 

This cause a race:

1. If the main thread managed to execute the newly added idle callback before
the background thread reached g_source_remove(), we were getting the following
error from g_source_remove():

GLib-CRITICAL **: Source ID 3 was not found when attempting to remove it

2. Otherwise, the idle callback wasn't executed at all!

3. When tried out, saw that both g_source_remove and callback works okay.

All in all, this is racy.

Even after changing the code above to:

{
  guint task_handle = g_idle_add( video_changed_message, sink);
  if (!g_atomic_int_compare_and_exchange(&sink->task_handle, 0,
        task_handle))
  {
    // There is already a task waiting to run, so we can cancel this one.
    g_source_remove(task_handle);
  }
}


There are still a subtle race condition. Consider the following sequence of
events:

1. One instance of this particular idle callback is queued. The atomic variable
   becomes non-zero.

2. Another instance is queued. Because the atomic variable is non-zero, we go
   inside the if.

3. Another thread (the one running GLib main loop) executes both of the queued
   callbacks.

4. The thread that entered the "if" finally reached g_source_remove(), which now
   acts on a non-existing source.

It turns out we don't have to execute the callback on the main thread, as all it
does is posting a message on GstBus, which does asynchronous delivery anyway. So
remove callback and do what it does in the current function.

Q: What if still needs callback approach?


={============================================================================
*kt_linux_core_266* sync: case: sync with no lock

  obj1                    obj2

    \                   /

  // callback called by external thread
  callback_func( object* obj, ... )
  {
    // access and set obj but no lock on objs
    deq_from_q_of_callbaks();
    run_callback();
  }

  ^
  |

  callback_to_hw()
  {
    enq_to_q_of_callbacks();
  }
  
  thread1             thread2

This callback is registered to two threads with different obj pointer. When gets
  called, picks up the obj and use it. But why no locks?

Since callback thread use queue, it's serialized and no need to have lock on
objects.


# ============================================================================
#{
==============================================================================
*kt_linux_core_290*	ref: concurrency in C++

{what-is-concurrency}
There is genuine or hardware concurrency or illusion of concurrency. What's new is that the
increased computing power of these machine comes not from running a single task faster but from
running multiple tasks in parallel.

<parallelism>
The task parallelism is to divide a single task into parts and run each in parallel, thus reducing
the total runtime. It can be complex since there may be many dependencies.

The data parallelism is that each thread performs the same operation on different parts of the data.
Good scalability; many hands make light work. There's a different focus in which more data can be
processed in the same amount of time.


{when-use-concurrency}
The use of concurrency is like any other optimisation strategy. Therefore it is only worth doing for
those performance critical parts of the application where there is the potential for mesurable gain.

{two-approaches-to-concurrency}
<use-multiple-process>
The downsides are 
o communication between processes.
o inherent overhead such as time to launch and resource in OS

The upsides are:
o safer code than threads
o can extend over a network

<use-multiple-thread>
The low overhead with launching and communicating between threads. So C++11 do not provide any
support for process and only support for threads.


{C++11}
o Allow writing portable multithreaded code without relying on platform-specific extensions.
o There is [abstraction penalty] compared to using the underlying low-level facilities directly.

<code-example>
#include <iostream>
#include <thread>

void hello()
{
  std::cout << "hello con world\n";
}

int main(int argc, char** argv)
{
  std::thread t(hello);
  t.join();
  // std::cout << "end of main" << std::endl;
}

If build and run like below:
kt@kt-ub-vb:~/work$ g++ -g -std=c++0x use-thread.cpp 
kt@kt-ub-vb:~/work$ ./a.out 
terminate called after throwing an instance of 'std::system_error'
  what():  Operation not permitted
Aborted (core dumped)

If build and run with pthread option, then works fine. This is known problem in g++.           
kt@kt-ub-vb:~/work$ g++ -g -std=c++0x -pthread use-thread.cpp 


==============================================================================
*kt_linux_core_291*	ref: concurrency in C++

{std-thread-basic}
<launch>
The std::thread works with any [callable] type; function object and lambda.

void do_some_work();
std::thread my_thread(do_some_work);

or

class background_task
{
  public:
    void operator() () const
    {
      do_something();
      do_something_else();
    }
};

background_task f;
std::thread my_thread(f);

Why callable? In this case, the supplied function object is copied into the storage belonging to the
created thread and invoked from there.

<join-and-detach>
If don't decide whether to join or to detach it before std::thread object is destroyed then program
is terminated since the std::thread destructor calls std::terminate(). Ture even in the presence of
exceptions.

The join() cleans up any storage associated with the thread so std::thread object isn't associated
with any thread. Once this, joinable() will return false.
{Q} what will happen when call join() twice on the same?

When detach a thread, make sure that the data accessed by the thread is vaild until the thread has
finished with it. For example, creat a thread within a function with thread function hold pointers
or reference to local varaibles. Bad idea and avoid this. 

void oops()
{
  int some_local_state = 0;
  T my_func(some_local_state);   // T(int&)
  std::thread my_thread(my_func);
  my_thread.detach();
}

<join-raii>
To avoid program being terminated when an exception is thrown, accidental lifetime problems, how?
Can use try and catch but verbose and easy to get it wrong. 

If you wan to do particular action for all possible exit path, whether normal or exceptional, use
raii.

class thread_guard
{
  std::thread& t;

  public:
  explicit thread_guard( std::thread& t_ ): t(t_) {}
  ~thread_guard()
  {
    if( t.joinable() )
    {
      t.join();
    }
  }

  thread_guard( thread_guard const& )=delete;
  thread_guard& operator=( thread_guard const&)=delete;
};

void oops()
{
  int some_local_state = 0;
  T my_func(some_local_state);         // T(int&)
  std::thread my_thread(my_func);
  thread_guard g(t);                   // <DN>

  do_something_in_current_thread();    // main thread continue to run
}


==============================================================================
*kt_linux_core_292*	ref: concurrency in C++

# ============================================================================
#{
==============================================================================
*kt_linux_core_300*	case: own semaphore and mutex class using pthred cond var

POSIX semaphore are system calls which means expensive. Is it possible to implement semaphore without it?

{class-semaphore}

This is for linux. When count is 0, waits and there is no upper limit. Also see that use one mutex
with many condition variables for semaphores.


{util-class}

Just to provide util funcs to all instances since these are static. Also SetPriority is not used.

class CThreadSelf
{
private:
	CThreadSelf(void) {}

public:
	// Returns the ID of the current executing thread.
	'static' int Id(void)
	{ return (int)pthread_self(); }

	'static' bool SetPriority(int priority);
	{
#if defined _LINUX

		  switch (priority)
		  {
			 case CThread::PRIORITY_HIGH: // [note] class type member
				 setpriority(PRIO_PROCESS, pthread_self(), -10); // [note] man setpriority
		       // param.sched_priority = 60;
				 break;
			 case CThread::PRIORITY_NORMAL:
				 setpriority(PRIO_PROCESS, pthread_self(), 0);
		       // param.sched_priority = 50;
				 break;
			 case CThread::PRIORITY_LOW:
		       // param.sched_priority = 40;
				 setpriority(PRIO_PROCESS, pthread_self(), 10);
				 break;
			 default:
				 return false;
		  }

#elif defined _WIN32
	}
};


{semaphore} [KT] the case uses containment(composition) to have implementation.

Use init count but no max count. 0 means to wait and other values means it is okay to get. Used as a
class memeber.

{Q} why need this? code says it calls sched_yield whenever sem count reaches 16.

#define	CONFIG_MAXIMUM_YIELD_COUNTER 16
static unsigned char semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;

pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER; [KT] this is global

struct PSemaphore
{
	pthread_cond_t  cond;
	int             count;
};

class Semaphore
{
	 private:
	 	PSemaphore* m_id;

	 public:
		Semaphore() { m_id = NULL; }
		virtual ~Semaphore() { assert( FlagCreate() == false); }

		bool Create(int count) // initial count
		{
			 pthread_mutex_lock(&mtx);

			 assert( FlagCreate() == false );

			 pthread_cond_t cond = PTHREAD_COND_INITIALIZER;

			 m_id = 'new' PSemaphore;	// new and m_id is not null
			 assert( m_id != NULL );
			 
			 m_id->cond = cond;  // [KT] is it okay as it is local variable?
			 m_id->count = count;

			 pthread_mutex_unlock(&mtx);

			 return m_id != NULL;
		}

		// return true when created
		bool FlagCreate() { return m_id != NULL; }

		virtual void Destory(void)
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  pthread_cond_destroy(&m_id->cond);
			  delete m_id;

			  m_id = NULL;

			  pthread_mutex_unlock(&mtx);
		}

		void Take()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  while (m_id->count <= 0)
			  {
				  pthread_cond_wait(&m_id->cond, &mtx);
			  }

			  m_id->count--;

			  pthread_mutex_unlock(&mtx);
		}

		void Give()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  m_id->count++;

			  pthread_cond_signal(&m_id->cond);

			  pthread_mutex_unlock(&mtx);

			  if (!semCounter--) {
				  sched_yield();
				  semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;
			  }
		}

		void Try(unsigned long msec = 0)
		{
			  if (msec == (unsigned long) INFINITY)
			  {
				  Take();

				  return true;
			  }

			  pthread_mutex_lock(&TimeMutex);
			  ASSERT(FlagCreate() == true);

			  struct timeval  now;
			  struct timespec timeout;
			  int             ret = 0;
			  bool            tf;


			  if (msec == 0)
			  {
				  if (m_id->count <= 0)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }
			  else
			  {
				  while ((m_id->count <= 0) && (ret != ETIMEDOUT))
				  {
					  gettimeofday(&now, NULL);
					  timeout.tv_sec  = now.tv_sec + msec / 1000;
					  timeout.tv_nsec = now.tv_usec + msec % 1000 * 1000;

					  while (timeout.tv_nsec > 1000000)
					  {
						  timeout.tv_nsec -= 1000000;
						  timeout.tv_sec++;
					  }

					  timeout.tv_nsec *= 1000;

					  ret = pthread_cond_timedwait(&m_id->cond, &TimeMutex, &timeout);
				  }

				  if (ret == ETIMEDOUT)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }

			  pthread_mutex_unlock(&TimeMutex);

			  return tf;
		}
};


{use-of-semaphore-one}

To make sure that an user can set prio once a thread is created.

class CThread
{
   PCSemaphore m_pidSync;

   Create()
   {
      m_pidSync.Create(0);
   }

   bool PCThread::SetPriority(int priority)
   {
      ASSERT(FlagCreate() == true);

      if (m_pid == -1)
      {
         m_pidSync.Take();
      }
      ...
   }

   inline void CThreadRun(CThread* thread)
   {
      thread->m_sync[0].Take();

      thread->m_pid = pthread_self();
      thread->m_pidSync.Give();

      thread->t_Main(); [KT] while loop on event get()

      thread->m_sync[1].Give();

      return;
   }
};


{mutex} 

CDerivedA: CMutex
 - thread   - Semaphore : uses global mutex

CDerivedB: CMutex
 - thread   - Semaphore : uses global mutex

Using sync always happens in the same thread. The case uses inheritance to have implementation. This
is based on the fact that mutex is a binary semaphore. Created with 1. Used to give the derived
class the lock/unlock feature to control interface access. That is, sync feature to class objects.
If need other use of sem, then have sem as a member.

class CMutex
{
	 private:
		  PCSemaphore 	m_sem; // [note] use of semaphore
		  int				m_tid;
		  int				m_count;

	 public:
	 	  // [note] no ctor?
	 	  virtual ~Mutex() { assert( FlagCreate() == false); }

		  bool Create(void)
		  {
				assert(FlagCreate() == false);

				if (m_sem.Create(1) == false) // [KT] create a sem
				{
					return false;
				}

				m_threadId = 0;

				return true;
		  }

		  virtual void Destroy(void)
		  {
				ASSERT(FlagCreate() == true);
				m_sem.Destroy();
		  }

		  bool FlagCreate(void) { return m_sem.FlagCreate(); }

		  void Lock(void)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;
				  return;
				}

				m_sem.Take();

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

		  }

		  bool Unlock(void);
		  {
				assert(FlagCreate() == true);

				if (m_threadId != CThreadSelf::Id())
				{
					return false;
				}

				m_count--;

				if (m_count > 0)
				{
				  return true;
				}

				m_threadId = 0;

				m_sem.Give();

				return true;
		  }

		  bool Try(unsigned long msec = 0)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;

				  return true;
				}

				if (m_sem.Try(msec) == false)
				{
				  return false;
				}

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

				return true;

		  }
};	

{cqueue}

struct PTEvent
{
	friend class PCQueue;
	friend class PCTask;

private:
	void* sync;

public:
	class PCHandler* receiver; //!< The Pointer to the handler that receives the event
	unsigned long    type;     //!< The event type

	//! Parameters of an event
	/*!
	 * The union of event parameters are 8-bytes long. That is, it can carry
	 * 2 long integers, 4 short integers, or 8 characters.
	 */
	union
	{
		long  l[2];
		short s[4];
		char  c[8];
	} param;
};

class CQueue : public CMutex
{
	private:
      PCSemaphore m_sem; // [KT] used to say that events are avaiable
		PTEvent     m_event[CONFIG_QUEUE_SIZE];

	bool CQueue::Create(void)
	{
		ASSERT(FlagCreate() == false);

		if (CMutex::Create() == false)
		{ return false; }

		if (m_sem.Create(0) == false)
		{
			CMutex::Destroy();
			return false;
		}

		m_in   = 0;		// [KT] this is {queue-contiguous-implementation} in *kt_dev*
		m_out  = 0;		// in(tail), out(head), size(count)
		m_size = 0;

		return true;
	}

	bool CQueue::Put(PTEvent* event, bool sync=false, bool priority=false)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		PCSemaphore sem;

		if (sync == false)
		{
			event->sync = NULL;
		}
		else
		{
			if (sem.Create(0) == false)
			{
				return false;
			}

			event->sync = &sem;
		}

		Lock();	// CMutex::Lock();

		int size;

		size = (priority == false) ? CONFIG_EMEGENCY_QUEUE_SIZE(16) : 0;
		size += m_size;

		if (size >= CONFIG_QUEUE_SIZE(256) )
		{
			Unlock();

			if (sync == true)
			{
				sem.Destroy();
			}

			PCDebug::Print("ERROR: Event queue full");

			return false;
		}

		if (priority == true)
		{
			m_out          = (m_out + CONFIG_QUEUE_SIZE - 1) % CONFIG_QUEUE_SIZE;
			m_event[m_out] = *event;
			m_size++;
		}
		// [KT] copy in event and inc count
		else
		{
			m_event[m_in] = *event;
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
			m_size++;
		}

		m_sem.Give();

      // [KT] As see how Get() uses m_sem, use m_sem to see if event are avaiable like a count or
      // length and then if there are call Lock() to lock access to this objects. May have some
      // performance gain from this.
		
      Unlock();	

		if (sync == true)
		{
			sem.Take();
			sem.Destroy();
		}

		return true;
	}

	bool PCQueue::Get(PTEvent* event, unsigned long msec = INFINITY)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		while (true)
		{
			if (m_sem.Try(msec) == false)
			{
				return false;
			}

			Lock();

			if (m_size == 0)
			{
				Unlock();

				continue;
			}

			*event = m_event[m_out];
			m_out  = (m_out + 1) % CONFIG_QUEUE_SIZE;
			m_size--;

			Unlock();

			return true;
		}
	}

};


{when-seek-one-specific}

{Q} This suggest that there are events for all(broadcast) and for specific ones. Move all event
between [head+1 ... tail-1] after tail. why? priority?

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver)
{ }

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver, unsigned long type)
{
	ASSERT(FlagCreate() == true);

	bool done = 0;

	Lock();

	unsigned long size = m_size;

	while (size-- != 0)
	{
		if (done == false && m_event[m_out].receiver == receiver && m_event[m_out].type == type)
		{
			*event = m_event[m_out];
			m_size--;
			done = true;
		}
		else
		{
			m_event[m_in] = m_event[m_out];
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
		}

		m_out = (m_out + 1) % CONFIG_QUEUE_SIZE;
	}

	if (done)
	{
		m_sem.Try();
	}

	Unlock();

	return done;
}

This q implementation uses q per thread. The different approach is to have q that threads share.
See *kt_linux_core_014* for msg q between threads


==============================================================================
*kt_linux_core_301*  case: use of mutex and thread class

This is case study using Mutex, Queue and Thread classes.

{CThread}

'inline' void CThreadRun(CThread* thread)
{
	thread->m_sync[0].Take();  // [KT] wait until signaled

#ifdef	_LINUX
	thread->m_pid = pthread_self();
	thread->m_pidSync.Give();
#endif

	thread->t_Main();

	thread->m_sync[1].Give();  // [KT] no use

	return;
}

'static' void* _Process(void* param)
{
	CThreadRun((CThread*)param);

	return NULL;
}

class CThread
{
private:

	int         m_id;
	int			m_pid;
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];

	friend void CThreadRun(CThread* thread); // [KT] inline friend

protected:

	virtual void t_Main(void) = 0;
	/*!< This function is the thread main virtual function.  As soon as a thread starts, this
	 * function is called.  When this function returns, the thread is terminated.
	 *
	 * You have to define this function when you define a new class that inherits CThread class.
	 */

public:

	//! Configuration constants
	enum PTConfigType
	{
		CONFIG_STACK_SIZE = 4096
	};

	enum PTPriorityType
	{
		PRIORITY_HIGH,
		PRIORITY_NORMAL,
		PRIORITY_LOW,
	};

	virtual ~CThread(void) { ASSERT(FlagCreate() == false); }

	bool Create(const char* name, unsigned long stackSize = CONFIG_STACK_SIZE);
	 {
		 ASSERT(FlagCreate() == false);

		 if (m_sync[0].Create(0) == false)
		 {
			 return false;
		 }

		 if (m_sync[1].Create(0) == false)
		 {
			 m_sync[0].Destroy();
			 return false;
		 }

#elif defined _LINUX

		 pthread_attr_t attr;
		 pthread_attr_init(&attr);
		 pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
		 pthread_attr_setstacksize(&attr, stackSize);
		 //struct sched_param schedParam;
		 //schedParam.sched_priority = 50;
		 //pthread_attr_getschedparam(&attr, &schedParam);
		 //pthread_attr_setschedpolicy(&attr, SCHED_RR);

		 m_pid = -1;
		 m_pidSync.Create(0);

		 if (pthread_create((pthread_t*)&m_id, &attr, _Process, this) != 0)
		 {
			 pthread_attr_destroy(&attr);
			 
			 ASSERT(!"CThread not created");

			 m_sync[0].Destroy();
			 m_sync[1].Destroy();

			 return false;
		 }

		 pthread_attr_destroy(&attr); 
		 
		 m_sync[0].Give();   // [KT] now a thread can run
		 return true;
	 }

	//! Check if the instance was created
	bool FlagCreate(void) { return m_sync[0].FlagCreate(); }
	//! Destroy the instance
	virtual void Destroy(void);

	//! Returns the ID of the CThread.
	int Id(void)
	{
		 ASSERT(FlagCreate() == true);
		 return m_id;
	}

	//! Set the priority value for the thread
	bool SetPriority(int priority);
};

The class hiarachy is:

class CThread
{
private:
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];
};

class CQueue : public CMutex

class CTask: private CThread, public CQueue, public CHandler
{
   private:
      void t_Main(void);

   protected:
      virtual bool t_Create(void);
};

class CSIEngineBase : public PCTask

class CSIVoiceEngine : public CSIEngineBase

class CSIEngineManager


<run>

CThread:
	CThreadRun: 
      thread->t_Main();	[pure-virtual] [main-start]

CTask:
              	t_Main()                 [main-end] {template-method} in kt_dev_txt
					{
				   	t_Create();                      [virtual] [create-start]
                 	while (ExecuteEvent() == true)   [thread-loop]
						//	virtual bool ExecuteEvent
						//	{
						//		 PTEvent event;
						//		 Get(&event, m_msec); [copy-in-event]
						//		 event.receiver->OnEvent(&event);
						//	}

					  	t_Destory();
					}

CSIEngineBase:
                                                 
CSIVoiceEngine:
                                                  t_Create() [create-end]
																     SendSelfEvent( OWN_EVENT_TYPE );

<event>

CHandler:
	protected:
	virtual bool t_OnEvent(const PTEvent* event) = 0;

	public:
	inline bool OnEvent
	{
      t_OnEvent() [pure-virtual] [event-start]
	}

CTask: public CHandler
	No OnEvent which means use CHandler one

	protectd:
	bool PCTask::t_OnEvent(const PTEvent* event)
	{
		has default basic event handling
	}

	static Send( PTEvent* event, bool sync = false, bool priority = false);
	  [to-other-task]
	  if ((sync == false) || (event->receiver->Task()->Id() != PCThreadSelf::Id()))
	  {
		 return event->receiver->Task()->Put(event, sync, priority);
		 {
			  In CQueue::Put, use copy-ctor of event structure to copy it into receiver's taks
			  m_event[].

			  m_event[i] *event;
		 }
	  }
	  [self] ends up with a func call
	  event->receiver->OnEvent(event);
		   
CSIEngineBase:
   public:
      bool SendSelfEvent(int nEventType)
      {
         PTEvent evt;
         evt.receiver = this; [event-to-self]
         evt.type = nEventType;
         Send(&evt);   // CTask::
      }

  protected:
      virtual bool t_OnEvent(const PTEvent* event)
      { return true; }

		virtual bool t_OnEvent( PTEvent* ) [event-end]
      {
		   PCTask::t_OnEvent(event); {hook-operation} in {template-method}
		   t_ProcessEvent(event); [virtual] [start]
      }

CSIVoiceEngine:
   SendSelfEvent( OWN_EVENT_TYPE );  // CSIEngineBase::

   virtual bool t_ProcessEvent(event);           [end]
   {
	   handle OWN_EVENT_TYPE;
   }


<create>

CThread:
   Create:
	  pthread_create

CTask:
	public:
   Create()
	  PCQueue::Create();
	  PCHandler::Create(this);
	  CThread::Create(stackSize);

CSIEngineBase: public CTask
	public:
   Create(const char* name)
      return CTask::Create(name);					[create]

CSIVoiceEngine: public CSIEngineBase
   no Create:

// {design-note}
// This can be a application manager which creates all applications and call Create() on them. This
// is platform wide and each application can override t_Create and init their own thing without
// knowing Create() calls made from outside.

class CSIEngineManager:
{
	 private:
	 CSIEngineBase* m_pVoiceEngine;

	 static CSIEngineManager* CSIEngineManager::GetInstance(void) {factory-method}
	 {
		 if(m_EngineManagerInstance == NULL)
		 {
			 m_EngineManagerInstance = 'new' CSIEngineManager;
		 }

		 return m_EngineManagerInstance;
	 }

	 CSIEngineManager::GetEngineInstance
	 {
		  m_pVoiceEngine =  'new' CSIVoiceEngine;
		  m_pVoiceEngine->Create("SIVoiceEngine"); // [create] and use inherited implementation
	 }
}

From review, t_ prefix means a primitive operation and to be overridden and all works are using the
most derived class object. Therefore, dreived version will be used for virtuals as shown
{template-method}


==============================================================================
*kt_linux_core_302*  case: analysis of 200 and 201 case

{how-this-work}

CUserClass instance:
                                       
CUserClass {
   // Eash CTask has a Q and thread
   CTask { CQueue, CHandler : CQueue { CMutex }
      CThread
         : pthread( staic _Process )
      }
}
  
static _Process(this) will run the derived class CTask t_Main which has a message loop. So this
_Process is a template code for all threads. 

Get(&event); in which all threads use the same code for thread routine

CUserB instance:
                                       
CUserB {
   CTask {
      CThread
         : pthread( staic _Process )
      }
}

Mutex class via inheritance:
Mutex #01     Mutex #02    Mutex #03     Mutex #04    Mutex #04  
(Semaphore)   (Semaphore)  (Semaphore)   (Semaphore)  (Semaphore)

Sem member in a class:
Sepmphore #01  Sepmphore #02 ...


Regarding q, each task has a q and other task can call put to insert a mesg to receiver's q.

{Q} Eash has its own pthread cond in Semaphore but all use a single global mutex for signaling. How
about performance? Is it better solution?


==============================================================================
*kt_linux_core_303*  case: msg q between threads

This uses stl q and sems to read, write and count(length) lock:


/** Maximum length of message queue name */
#define MQ_NAME_LENGTH 5

/** Queue Magic identifier Corresponds to ASCII QuEu*/
#define MQ_MAGIC 0x51754575

typedef struct PFMMessageQueueInfo_t_
{
    char                name[ MQ_NAME_LENGTH ];
    PCSemaphore         *readsem;
    PCMutex             *writeLock;
    PCMutex             *readLock;
    uint32_t            magic;
    std::queue<SPfmMessage>* container;
} PFMMessageQueueInfo_t;


extern "C"
{
///////////////////////////////////////////////////
// PFM Queue Create
///////////////////////////////////////////////////
HPfmQueue
pfmQueueCreate(const char* name, uint32_t max_size)
{
    PFMMessageQueueInfo_t *qptr;

    // Allocate queue controll structure
    qptr = (PFMMessageQueueInfo_t *)pfmMalloc( sizeof(PFMMessageQueueInfo_t) );
    if(!qptr)
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc\n");
        return PFM_NULL_HANDLE;
    }

    // create storage container
    qptr->container = new std::queue<SPfmMessage>();
    if (qptr->container == NULL)
    {
        fprintf(stderr,
            "pfmQueueCreate failed to alloc storage\n");
        pfmFree(qptr);
        return PFM_NULL_HANDLE;
    }

    // Create & initialize Read mutex for read serialization
    qptr->readLock = new PCMutex;
    if( qptr->readLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc read mutex\n");

        delete qptr->container;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    qptr->readLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init read mutex\n");

        delete qptr->container;
        delete qptr->readLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create & initialize Write mutex for write serialization
    qptr->writeLock = new PCMutex;
    if( qptr->writeLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }
    qptr->writeLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create and initialize read semaphore
    qptr->readsem = new PCSemaphore;
    if( qptr->readsem == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc semaphore\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    //Initialize read semaphore so it will "block" on try.
    qptr->readsem->Create(0);
    if( !qptr->readsem->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc sem\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        delete qptr->readsem;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Copy semaphore name
    if( !name )
    {
        char nameTmp[] = "SEM";
        PCString::Copy( qptr->name,nameTmp,MQ_NAME_LENGTH);
    }
    else
    {
        PCString::Copy( qptr->name,name,MQ_NAME_LENGTH);
    }

    qptr->name[ (MQ_NAME_LENGTH-1) ] = (char)NULL;
    qptr->magic = MQ_MAGIC;

    return (HPfmQueue)qptr;
}

///////////////////////////////////////////////////
// PFM Queue Destroy
///////////////////////////////////////////////////
pfmerr_t
pfmQueueDestroy(HPfmQueue h)
{
    // Validate input
    if( h == PFM_NULL_HANDLE)
    {
        pfmAssert( h != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)h;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Mark queue as invalid.
    qptr->magic = 0;

    // Release Reader (if exists) & Grab read/write locks.
    // This shall prevent "use while destruct" scenario.
    // Mutexes can be destroyed while locked.
    qptr->readsem->Give();
    qptr->readLock->Lock();
    qptr->writeLock->Lock();

    if( qptr->readsem->FlagCreate() )
    {
        qptr->readsem->Destroy();
    }
    if( qptr->readLock->FlagCreate() )
    {
        qptr->readLock->Destroy();
    }
    if( qptr->writeLock->FlagCreate() )
    {
        qptr->writeLock->Destroy();
    }

    delete qptr->readsem;
    delete qptr->readLock;
    delete qptr->writeLock;

    delete qptr->container;

    pfmFree( qptr );

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Receive
///////////////////////////////////////////////////
pfmerr_t
pfmQueueReceive(HPfmQueue q, uint32_t timeout_ms, SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    unsigned long readLockTickStart;
    unsigned long readLockTickEnd;
    unsigned long tickDiff;
    uint32_t    waitTime;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Convert PFM infinite timeout to Shadwo's infinite timeout
    switch( timeout_ms )
    {
        case PFM_WAIT_NONE:
            waitTime = 0;
            break;
        case PFM_WAIT_FOREVER:
            waitTime = INFINITY;
            break;
        default:
            waitTime = timeout_ms;
            break;
    }

    // Do thread serialized reading
    readLockTickStart = PCTime::Tick();
    if( !qptr->readLock->Try(waitTime) )
    {
        return ERR_TIMEDOUT;
    }

    // If waiting for data, do the tick calculation to
    // potentially reduce wait value
    if( waitTime )
    {
        readLockTickEnd = PCTime::Tick();
        tickDiff = pfmTickDiff( readLockTickStart, readLockTickEnd );

        // Check if we have time to hang on a samephore
        if( tickDiff > waitTime )
        {
            // We have waited longer on a mutex, so wait as short as possible
            waitTime = 0;
        }
        else
        {
            waitTime -= tickDiff;
        }
    }

    // Consume semaphore as it is signalled after every insertion
    qptr->readsem->Try(waitTime);

    // Check if there's data in the queue. This should only happen
    // on wait with timeout. Should not occour in infinite timeout
    // scenario
    if (qptr->container->empty())
    {
        pfmAssert(waitTime != (uint32_t)INFINITY );
        qptr->readLock->Unlock();
        return ERR_TIMEDOUT;
    }

    // read message
    *msg = qptr->container->front();
    qptr->container->pop();

    // This should be the last possible place where
    // a dying queue could be trapped (unlock fails).
    if( !qptr->readLock->Unlock() )
    {
        return ERR_SYS;
    }

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Send
///////////////////////////////////////////////////
pfmerr_t
pfmQueueSend(HPfmQueue q, const SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    pfmerr_t res;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Grab an read Lock
    qptr->writeLock->Lock();

    qptr->container->push(*msg);

    // Inform readers that there's data avaliable.
    qptr->readsem->Give();

    //Unlock queue
    if( !qptr->writeLock->Unlock() )
    {
        // This should handle a dying queue
        res = ERR_SYS;
    }
    else
    {
        res = ERR_OK;
    }

    return res;
}


={============================================================================
*kt_linux_core_400* uclibc

http://www.uclibc.org/
http://git.uclibc.org/uClibc/


={============================================================================
*kt_linux_core_400* uclibc: config ld_debug

http://git.uclibc.org/uClibc/plain/extra/Configs/Config.in

config SUPPORT_LD_DEBUG
	bool "Build the shared library loader with debugging support"
	depends on HAVE_SHARED
	help
	  Answer Y here to enable all the extra code needed to debug the uClibc
	  native shared library loader.  The level of debugging noise that is
	  generated depends on the LD_DEBUG environment variable...  Just set
	  LD_DEBUG to something like: 'LD_DEBUG=token1,token2,..  prog' to
	  debug your application.  Diagnostic messages will then be printed to
	  the stderr.

	  For now these debugging tokens are available:
	    detail        provide more information for some options
	    move          display copy processing
	    symbols       display symbol table processing
	    reloc         display relocation processing; detail shows the
	                  relocation patch
	    nofixups      never fixes up jump relocations
	    bindings      displays the resolve processing (function calls);
	                  detail shows the relocation patch
	    all           Enable everything!

	  The additional environment variable:
	    LD_DEBUG_OUTPUT=file
	  redirects the diagnostics to an output file created using
	  the specified name and the process id as a suffix.

	  An excellent start is simply:
	    $ LD_DEBUG=binding,move,symbols,reloc,detail ./appname
	  or to log everything to a file named 'logfile', try this
	    $ LD_DEBUG=all LD_DEBUG_OUTPUT=logfile ./appname

	  If you are doing development and want to debug uClibc's shared library
	  loader, answer Y.  Mere mortals answer N.


={============================================================================
*kt_linux_core_400* uclibc: fail to find symbol

Q: HOW to check if linker finds a symbols? 

<1>
Another unfortunate consequence of this situation, together with poor
implementation of uClibc dynamic linker [2], is that some libraries fail to
load. This can happen in a scenario when a C program loads a C library which
loads another C library that depends on some C++ libraries. uClibc dynamic
linker in some of such cases doesn't resolve symbols in the right order,
especially for global C++ objects with constructors!

[2] It's been proved many times that uClibc dynamic linker implementation
available on devices is broken as it doesn't resolve symbols properly when their
dependencies are complex enough. Possibly upstream implementation works better
but the one available from Broadcom is a mix of a very old version and some
random backported patches.

This libgstcencdec.so is a C library which depends on another C library:
libdrm.so. The latter one depends on several C++ libraries, with one being
SystemAPI.so. Because SystemAPI.so now depends on dbus-c++-1, the dynamic linker
tries to load it. But due some unfortunate library and symbol ordering, it gets
it wrong which results in a SEGFAULT while scanning for plug-ins:

Program received signal SIGSEGV, Segmentation fault.

[Switching to Thread 0x77ff4000 (LWP 1548)]
0x00000000 in ?? ()
(gdb) bt
#0 0x00000000 in ?? ()
#1 0x7661d180 in global constructors keyed to eventloop_integration.cpp () 
  from /opt/zinc-trunk/lib/libdbus-c++-1.so.0

#2 0x7661d558 in __do_global_ctors_aux () from 
  /opt/zinc-trunk/lib/libdbus-c++-1.so.0


<2>
During development, works okay but suddenly failed to start during code
management.

After all, found that there is one function which thought was deleted but was in
the code to be called. However, there is no function definition since it was
removed but only calls to that function remained.

How could that possible without linking error? Since it was so file and run nm:

U        is undefined meaning extern

This was undefined and supposed to be linked when loaded. So when run
application, failed to find symbol and crashes. 

root        1.1M Sep 18 15:16 core.gst-plugin-scan.8785.HUMAX.1442585817
root        1.1M Sep 18 15:16 core.gst-plugin-scan.8786.HUMAX.1442585817
root      171.9M Sep 18 15:17 core.multiqueue1:src.8775.HUMAX.1442585819


<3>
http://stackoverflow.com/questions/1617286/easy-check-for-unresolved-symbols-in-shared-libraries

Q: To check if there are undefined symbol at compile time than dynamic loading
time?

A: (not checked yet)

Check out the linker option -z defs / --no-undefined. When creating a shared
object, it will cause the link to fail if there are unresolved symbols.

If you are using gcc to invoke the linker, you'll use the compiler -Wl option to
pass the option to the linker:

gcc -shared ... -Wl,-z,defs

As an example, consider the following file:

#include <stdio.h>

void forgot_to_define(FILE *fp);

void doit(const char *filename)
{
    FILE *fp = fopen(filename, "r");
    if (fp != NULL)
    {
        forgot_to_define(fp);
        fclose(fp);
    }
}

Now, if you build that into a shared object, it will succeed:

> gcc -shared -fPIC -o libsilly.so silly.c && echo succeeded || echo failed
succeeded

But if you add -z defs, the link will fail and tell you about your missing symbol:

> 
gcc -shared -fPIC -o libsilly.so silly.c -Wl,-z,defs && echo succeeded || echo failed
/tmp/cccIwwbn.o: In function `doit':
silly.c:(.text+0x2c): undefined reference to `forgot_to_define'
collect2: ld returned 1 exit status
failed


={============================================================================
*kt_linux_core_400* slib: shared library

LPI 41.

{static-library}
<create-static> ar-command
The archive also records various attributes of each of the component object
files, including file permissions, numeric user and group IDs, and last
modification time.

r (replace): 
  
Insert an object file into the archive, replacing any previous object file of
the same name. This is the standard method for creating and updating an archive.
Thus, we might build an archive with the following commands:

$ cc -g -c mod1.c mod2.c mod3.c
$ ar r libdemo.a mod1.o mod2.o mod3.o

t (table of contents): 
  
Display a table of contents of the archive. By default, this lists just the
names of the object files in the archive. By adding the v (verbose) modifier, we
additionally see all of the other attributes recorded in the archive for each
object file, as in the following example:

$ ar tv libdemo.a
rw-r--r-- 1000/100 1001016 Nov 15 12:26 2009 mod1.o
rw-r--r-- 1000/100 406668 Nov 15 12:21 2009 mod2.o
rw-r--r-- 1000/100 46672 Nov 15 12:21 2009 mod3.o

<link-static>
Couple of ways in linking:

1. The first is to name the static library as part of the link command, as in
the following:

$ cc -g -c prog.c
$ cc -g -o prog prog.o libdemo.a

note: 'linktime' search
2. can place the library in one of the 'standard' directories searched by the
linker such as /usr/lib, and then specify the library name; the filename of the
library without the lib prefix and .a suffix using the -l option:

$ cc -g -o prog prog.o -ldemo

3. If the library resides in a directory not normally searched by the linker, we
can specify that the linker should search this additional directory using the -L
option:

$ cc -g -o prog prog.o -Lmylibdir -ldemo

Although a static library may contain many object modules, the linker includes
'only' those modules that the program 'requires'.


{downside-of-static}
1. Duplicates in disk and ram spce.

2. If a change is required perhaps a security or bug fix to an object module in
a static library, then all executables using that module must be relinked in
order to incorporate the change. This disadvantage is further compounded by the
fact that the system administrator needs to be aware of which applications were
linked against the library.


{what-is-shared}
Although the code of a shared library is shared among multiple processes, its
variables are not.  Each process that uses the library has its own copies of the
global and static variables that are defined within the library.


{further-advantages}
o Because overall program size is smaller, in some cases, programs can be loaded
into memory and started more 'quickly'. This point holds true only for large
shared libraries that are already in use by another program.

o Such changes can be carried out even while running programs are using an
existing version of the shared library.


{cost-of-shared}
o Shared libraries are more 'complex' than static libraries, both at the
conceptual level, and at the practical level of creating shared libraries and
building the programs that use them.

o Shared libraries 'must' be compiled to use position-independent code, which
has a performance 'overhead' on most architectures because it requires the use
of an extra register 

o Symbol relocation must be performed at run time. During symbol relocation,
  references to each symbol (a variable or function) in a shared library need to
  be modified to correspond to the actual run-time location at which the symbol
  is placed in virtual memory. Take a little more time to execute.


{position-independent-code}
These changes allow the code to be located at any virtual address at run time.
This is necessary for shared libraries, since there is no way of knowing at link
time where the shared library code will be located in memory.

In order to determine whether an existing object file has been compiled with the
-fPIC option, can check for the presence of the name _GLOBAL_OFFSET_TABLE_ in
the object file's symbol table, using either of the following commands:

$ nm mod1.o | grep _GLOBAL_OFFSET_TABLE_
$ readelf -s mod1.o | grep _GLOBAL_OFFSET_TABLE_         // -s, --syms|--symbols

note: as shown above, this not necessarily means 'shared' object. It's only tell
-fPIC used.

<pic-vs-relocatable>
This is talking about position independent code, which is 'not' the same as
relocatable code.  Relocatable code is code whose address may be assigned at
'linktime'. Position independent code is code whose address may be assigned at
'runtime'.

'all' object files are relocatable. For most targets, only object files compiled
with -fpic or -fPIC or -pie are position independent.

For more about relocatable code: http://www.airs.com/blog/archives/41


{create-shared}
$ gcc -g -c -fPIC -Wall mod1.c mod2.c mod3.c
$ gcc -g -shared -o libfoo.so mod1.o mod2.o mod3.o

or 

$ gcc -g -fPIC -Wall mod1.c mod2.c mod3.c -shared -o libfoo.so

Unlike static libraries, it is not possible to add or remove individual object
modules from a previously built shared library. As with normal executables, the
object files within a shared library no longer maintain distinct identities.


{shared-vs-static} do-not-use-single-line

$ cat main.c 
#nclude <stdio.h>

extern void foo(void);

int main(void)
{
  foo();
  return 0;
}

$ cat foo.c 
#include <stdio.h>

void foo(void)
{
  printf("foo: this is foo...\n");
}

$ ls -alR 
drwxr-xr-x 2 kpark kpark 4096 Feb  5 09:34 one
drwxr-xr-x 2 kpark kpark 4096 Feb  5 09:37 two
drwxr-xr-x 2 kpark kpark 4096 Feb  5 09:46 thr

<used-seperated-step> recommended
./one:
-rw-r--r-- 1 kpark kpark   73 Feb  5 09:32 foo.c
-rw-r--r-- 1 kpark kpark 1528 Feb  5 09:33 foo.o
-rwxr-xr-x 1 kpark kpark 6407 Feb  5 09:33 libfoo.so
-rw-r--r-- 1 kpark kpark   82 Feb  5 09:32 main.c
-rwxr-xr-x 1 kpark kpark 7120 Feb  5 09:34 one

note: -shared
Produce a shared object which can then be linked with other objects to form an
executable. Not all systems support this option. For predictable results, you
must also specify the same set of options used for compilation (-fpic, -fPIC, or
    model suboptions) when you specify this linker option

$ gcc -c -fpic foo.c 
$ gcc -shared -o libfoo.so foo.o
$ gcc -o one main.c libfoo.so 

$ ./one 
./one: error while loading shared libraries: libfoo.so: cannot open shared
  object file: No such file or directory

$ readelf -d one | grep NEEDED
:4: 0x0000000000000001 (NEEDED)             Shared library: [libfoo.so]
:5: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ file libfoo.so 
libfoo.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), 
  dynamically linked,
  BuildID[sha1]=0x7c5aa43368b2a98af48a1558d7e2c5a010d238b0, not stripped

$ readelf -s libfoo.so | grep _OFFSET_TA
(standard input):64:    43: 0000000000200978     0 OBJECT  LOCAL  DEFAULT  \
                   ABS _GLOBAL_OFFSET_TABLE_

note: no SONAME is involved here and use a realname since so do not have SONAME.

$ nm one | grep foo
(standard input):32:                 U foo


<used-single-step>
./two:
-rw-r--r-- 1 kpark kpark   73 Feb  5 09:32 foo.c
-rw-r--r-- 1 kpark kpark 1528 Feb  5 09:37 libfoo.so
-rw-r--r-- 1 kpark kpark   82 Feb  5 09:32 main.c
-rwxr-xr-x 1 kpark kpark 6872 Feb  5 09:37 two

$ gcc -c -fpic foo.c -shared -o libfoo.so
$ gcc -o two main.c libfoo.so

$ ./two 
foo: this is foo...

$ readelf -d two | grep NEEDED
4: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ readelf -s libfoo.so | grep _OFFSET_TA
(standard input):14:    10: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  \
                   UND _GLOBAL_OFFSET_TABLE_

$ file libfoo.so 
libfoo.so: ELF 64-bit LSB 'relocatable', x86-64, version 1 (SYSV), not stripped

note: what does it mean? same as static link? looks like "libfoo.so" is not
shared object.

$ nm two | grep foo
(standard input):32:000000000040051c T foo


<used-static-build>
./thr:
-rw-r--r-- 1 kpark kpark   73 Feb  5 09:45 foo.c
-rw-r--r-- 1 kpark kpark 1480 Feb  5 09:46 foo.o
-rw-r--r-- 1 kpark kpark   82 Feb  5 09:45 main.c
-rwxr-xr-x 1 kpark kpark 6872 Feb  5 09:46 thr

$ gcc -c foo.c -o foo.o 
$ gcc -o thr main.c foo.o 

$ ./thr 
foo: this is foo...

$ file foo.o 
foo.o: ELF 64-bit LSB 'relocatable', x86-64, version 1 (SYSV), not stripped

$ readelf -s foo.o | grep _OFFSET_TA
$ readelf -d thr | grep NEEDED
4: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ nm thr | grep foo
(standard input):32:000000000040051c T foo


={============================================================================
*kt_linux_core_400* slib: dynamic linker

{dynamic-linker}
note: From GCC doc, the dynamic loader is not part of GCC; it is part of the
operating system.

Two steps must occur that are not required for programs that use static
libraries:

$ gcc -c -fpic foo.c 
$ gcc -shared -o libfoo.so foo.o
$ gcc -o one main.c libfoo.so          <embedding-the-name-of-lib>

1. At 'linktime'. Since the executable file 'no' longer contains copies of the
object files that it requires, must have some mechanism for identifying the
shared library that it needs at runtime.

This is done by 'embedding' the name of the shared library inside the executable
during the link phase. DT_NEEDED tag in ELF.

$ readelf -d one | grep NEED
0x00000001 (NEEDED)  Shared library: [libfoo.so]    // note: "libfoo"
0x00000001 (NEEDED)  Shared library: [libc.so.6]

note: In order to embed the shared library in a executable that use so, requires
'so' file in linking.


2. At 'runtime', there must be some 'mechanism' for 'resolving' the embedded
library name-that is, for finding the shared library file corresponding to the
name specified in the executable file-and then loading the library into memory,
     if it is not already present.


{dynamic-linking}
The dynamic linking, which is the task of resolving the embedded library name at
runtime. This task is performed by the dynamic linker (also called the dynamic
    linking loader or the runtime linker). The dynamic linker is 'itself' a
'shared' library, named /lib/ld-linux.so.2, which is employed by every ELF
executable that uses shared libraries.

<host>
$ ls -l /lib/ld-linux.so.2 
lrwxrwxrwx 1 root root 25 Oct 17 00:50 /lib/ld-linux.so.2 
  -> i386-linux-gnu/ld-2.13.so

<target>
# ls -al /lib/ld-uClibc*
root     root         31760 Jul 31 11:10 /lib/ld-uClibc-0.9.32.1.so*
root     root            21 Aug  5 08:50 /lib/ld-uClibc.so 
  -> ld-uClibc-0.9.32.1.so*
root     root            21 Aug  5 08:50 /lib/ld-uClibc.so.0 
  -> ld-uClibc-0.9.32.1.so*

note:
Every program-including those that use shared libraries-goes through a
static-linking phase. At run time, a program that employs shared libraries
additionally undergoes dynamic linking.


{ld-library-path}
Some of these rules specify a set of 'standard' directories in which shared
libraries normally reside. To inform the dynamic linker that a shared library
resides in a nonstandard directory

If LD_LIBRARY_PATH is defined, then the dynamic linker searches for the shared
library in the directories it lists before looking in the standard library
directories.

note: Creates an environment variable definition 'within' the process executing
prog. Since if you don't export the changes to an environment variable, they
won't be inherited by the child processes.  The loader and our test program
didn't inherit the changes we made.

$ LD_LIBRARY_PATH=. ./one 
foo: this is foo...

$ ./one 
./one: error while loading shared libraries: libfoo.so: cannot open shared 
  object file: No such file or directory

$ export LD_LIBRARY_PATH=.
$ ./one 
foo: this is foo...


{soname-and-realname}
In the above example, libfoo is realname. As with a realname, soname is used in
linking and is embedded in the executable, and used by the linker at runtime.

Why soname? The purpose of the soname is to provide a level of 'indirection'
that permits an executable to use, at runtime, a version of the shared library
that is different from but compatible with the library against which it was
linked.

$ gcc -c -fpic foo.c

$ gcc -shared -Wl,-soname,libbar.so -o libfoo.so foo.o      
note: shall no space between -soname

$ gcc -shared -Wl,-soname -Wl,libbar.so -o libfoo.so foo.o  

$ readelf -d libfoo.so | grep SONAME              note: DT_SONAME in ELF
(standard input):5: 0x000000000000000e (SONAME)   Library soname: [libbar.so]

$ gcc -o one main.c libfoo.so

$ readelf -d one | grep NEED 
:4: 0x0000000000000001 (NEEDED)             Shared library: [libbar.so]
:5: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ ./one 
./one: error while loading shared libraries: libbar.so: cannot open shared 
  object file: No such file or directory

$ LD_LIBRARY_PATH=. ./one
./one: error while loading shared libraries: libbar.so: cannot open shared 
  object file: No such file or directory

note: Why fails this time? So set soname in so file itself and executable embeds
it instead of realname. Now linker ld looks for libbar instead but not libfoo.

<indirection>
When using a soname, one further step is required: we must create a symbolic
link from the soname to the real name of the library. This symbolic link must be
created in one of the directories searched by the dynamic linker. Thus, we could
run our program as follows:

$ ln -s libfoo.so libbar.so 
$ ll
lrwxrwxrwx 1 kpark kpark    9 Feb  5 11:22 libbar.so -> libfoo.so*
$ ./one 
foo: this is foo...


{versioning}
<requirement>
The same calling interface and are semantically equivalent (they achieve
    identical results). Such differing but compatible versions are referred to
as 'minor' versions of a shared library.

Occasionally, however, it is necessary to create a new 'major' version of a
library-one that is incompatible with a previous version. At the same time, it
must still be possible to continue running programs that require the older
version of the library.

<realname>
libdemo.so.1.0.1
libdemo.so.1.0.2        Minor version, compatible with version 1.0.1
libdemo.so.2.0.0        New major version, incompatible with version 1.*

<soname> minor-independent link
The soname of the shared library includes the same 'major' version identifier as
its corresponding real library name, but excludes the minor version identifier.
Thus, the soname has the form libname.so.major-id.

Usually, the soname is created as a relative symbolic link in the directory that
contains the real name. The following are some examples of sonames, along with
the real names to which they might be symbolically linked:

libdemo.so.1   -> libdemo.so.1.0.2
libdemo.so.2   -> libdemo.so.2.0.0

Normally, the soname corresponding to each major library version points to the
most recent 'minor' version within the major version. This setup allows for the
correct versioning semantics during the runtime operation of shared libraries. 

Because the static-linking phase embeds a copy of the soname in the executable
which has major, and the soname symbolic link may subsequently be modified to
point to a newer (minor) version of the shared library, it is possible to ensure
that an executable loads the most up-to-date minor version of the library at
runtime.

Furthermore, since different major versions of a library have different sonames,
  they can happily coexist and be accessed by the programs that require them.

note: Due to this indirection, possible to meet versioning requirement by
changing a sym link. After all, soname is to have 'indirection' to minor
version.

<linkername> version-independent link
The linker name, which is used when 'linking' an executable against the shared
library. The linker name is a symbolic link containing just the library name and
thus has the form libname.so. The linker name allows us to construct
version-independent link commands that automatically operate with the correct
version of the shared library.

libdemo.so ->  libdemo.so.2


real name(minor)   <-   soname(major)         <-   linker name         
libdemo.so.1.0.1        libdemo.so.1               libdemo.so
(regular file)          (symbolic link)            (symbolic link)
lib name.so.maj.min     libname.so.maj
Object code for         
library modules


$ gcc -g -c -fPIC -Wall mod1.c mod2.c mod3.c

created a realname and soname

$ gcc -g -shared -Wl,-soname,libdemo.so.1 -o libdemo.so.1.0.1 mod1.o mod2.o mod3.o

$ ln -s libdemo.so.1.0.1 libdemo.so.1
$ ln -s libdemo.so.1 libdemo.so

$ ls -l libdemo.so* | awk '{print $1, $9, $10, $11}'
lrwxrwxrwx libdemo.so -> libdemo.so.1
lrwxrwxrwx libdemo.so.1 -> libdemo.so.1.0.1
-rwxr-xr-x libdemo.so.1.0.1

$ gcc -g -Wall -o prog prog.c -L. -ldemo        
$ gcc -g -Wall -o prog prog.c libdemo.so        
// as with exmple $ gcc -o one main.c libfoo.so

note: -ldemo or libdemo.so is linkername and use is in static linking stage.

$ LD_LIBRARY_PATH=. ./prog
Called mod1-x1
Called mod2-x2

So three cases:

o the case when linkername is realname. Means that no soname is used. 
o the case when linkername is soname. This is one-level indirection. 
o the case when linkername is different from soname. This is two-level
indirection.

note: Even when linkername is different from soname, executable 'always' embeds
'soname' and the linkername is a name to use linking stage that must be exist
before doing linking. Checked with the above one example. 

Using two names and two indirections enable us to change major and minor libs to
use. 

When major changes, changes linkname link: need to link again 

libdemo.so -> libdemo.so.1          =>    libdemo.so -> libdemo.so.2

When minor changes, change soname link: no need to link.

libdemo.so.1 -> libdemo.so.1.0.1    =>    libdemo.so.1 -> libdemo.so.1.0.1

note: The above is convention but not mandatory so can have followings:

17 Dec 29 13:08 liblog4c.so -> liblog4c.so.3.1.0*
17 Dec 29 13:08 liblog4c.so.3 -> liblog4c.so.3.1.0*


={============================================================================
*kt_linux_core_401* slib: --as-needed flag and link error

<manual>
https://sourceware.org/binutils/docs/ld/Options.html

--as-needed 
--no-as-needed

This option affects ELF DT_NEEDED tags for 'dynamic' libraries mentioned on the
command line 'after' the --as-needed option. 

<default>
Normally the linker will add a DT_NEEDED tag for each dynamic library mentioned
on the command line, 'regardless' of whether the library is actually needed or
not.

The --as-needed causes a DT_NEEDED tag to 'only' be emitted for a library that
"at that point in the link" satisfies a non-weak undefined symbol reference from
a regular object file or, if the library is not found in the DT_NEEDED lists of
other needed libraries, a non-weak undefined symbol reference from another
needed dynamic library. 

Object files or libraries appearing on the command line after the library in
question do not affect whether the library is seen as needed. This is similar to
the rules for extraction of object files from archives. --no-as-needed restores
the 'default' behaviour. 

<example>
$ gcc -c -fpic foo.c 
$ gcc -shared -o libfoo.so foo.o
$ gcc -o one main.c libfoo.so          <embedding-the-name-of-lib>

or 

$ gcc -L. -lfoo -o one main.c

$ nm libfoo.so | grep foo
(standard input):24:00000550 T foo

$ nm one | grep foo
(standard input):33:         U foo

$ readelf -d one | grep NEEDED
(standard input):4: 0x00000001 (NEEDED)   Shared library: [libfoo.so]
(standard input):5: 0x00000001 (NEEDED)   Shared library: [libc.so.6]


However, when tried:

$ gcc -Wl,--as-needed -L. -lfoo -o one main.c
/tmp/ccUshwwe.o: In function `main':
main.c:(.text+0x7): undefined reference to `foo'
collect2: error: ld returned 1 exit status

The followings are okay.

$ gcc -L. -lfoo -Wl,--as-needed -o one main.c
$ gcc -Wl,--as-needed -o one main.c libfoo.so

note: WHY this fails to link even if main really uses foo()?


<case>
There is an application which is said that it uses one shared library and this
library again uses the other shard library. 

The assumption is: application <- a.so <- b.so.

The problem happens when simply changes application source file to use functions
in a.so and statred to see "undefined symbols" of b.so from the linker. But
those symbols are not used in that application at all and the call that
applicaion uses don't have any reference to b.so as well. However other APIs of
a.so does have reference to b.so. Why is this?

When looked at application and library dependancies for okay case, there was no
reference to a.so library which has undefined symbols defined. Why was it okay
before?

Two problems:

1. Wrong assumption. For okay case, there was actually no reference between
application and a.so. That's why cannot see dependancies.

2. When build the new source, this code introduce a call to a.so and this call
has calls to the b.so. Now application need to know both a.so and b.so. Since
the application is the 'final' in the linking process and now see undefined
symbols which used in a.so but in the b.so. The problem is that b.so is dropped
to link of application due to as-needed option.

application          libMgr                  libFb
reference to Mgr  -> no reference to Fb
                     reference to Fb      ->


When fails:

-Wl,--as-needed -lMgr -lFb ...

/.../usr/local/lib/libMgr.so: undefined reference to `DirectFBSetOption' 
/.../usr/local/lib/libMgr.so: undefined reference to `DirectFBCreate' 
/.../usr/local/lib/libMgr.so: undefined reference to `DirectFBInit' 
  collect2: ld returned 1 exit status

When works:

-o application application.o
-lMgr -lFb ...
-Wl,--as-needed ...

So works fine when move --as-needed after necessary library.


<ex>

// main.c
#include <stdio.h>

extern void foo(void);

int main(void)
{
  // foo(5);
  foo();
  return 0;
}

// foo.c
#include <stdio.h>

extern void boo();

void foo()
{
  printf("foo: this is foo...\n");
  boo();
}

// boo.c
#include <stdio.h>

void boo()
{
  printf("boo: this is boo...\n");
}

$ gcc -c -fpic boo.c 
$ gcc -c -fpic foo.c
$ gcc -shared -o libfoo.so foo.o
$ gcc -shared -o libboo.so boo.o
$ gcc -o one main.c libfoo.so libboo.so

$ readelf -d one | ag NEEDED
 0x00000001 (NEEDED)                     Shared library: [libfoo.so]
 0x00000001 (NEEDED)                     Shared library: [libboo.so]
 0x00000001 (NEEDED)                     Shared library: [libc.so.6]

$ LD_LIBRARY_PATH=. ./one
foo: this is foo...
boo: this is boo...

$ gcc -o one main.c -Wl,--as-needed libfoo.so libboo.so
// same as $ gcc -o one main.c libfoo.so libboo.so

$ gcc -Wl,--as-needed libfoo.so libboo.so -o one main.c 
$ gcc -Wl,--as-needed -L. -lfoo -lboo -o one main.c 
/tmp/cc3ZjAgB.o: In function `main':
main.c:(.text+0x7): undefined reference to `foo'
collect2: error: ld returned 1 exit status

$ gcc -Wl,--as-needed -lfoo -lboo -o one main.c
/usr/bin/ld: cannot find -lfoo
/usr/bin/ld: cannot find -lboo
collect2: error: ld returned 1 exit status


<useful>
http://wiki.gentoo.org/wiki/Project:Quality_Assurance/As-needed#What_is_--as-needed.3F

What is --as-needed?

The --as-needed flag is passed to the GNU linker (GNU ld ). The flag tells the
linker to link in the produced binary only the libraries containing symbols
'actually' used by the binary itself. This binary can be either a final
executable or another library.

In theory, when linking something, only the needed libraries are passed to the
command line used to invoke the linker. But to workaround systems with broken
linkers or not using ELF format, many libraries declare some "dependencies" that
get pulled in while linking. A simple example can be found by looking at the
libraries declared as dependencies by gtk+ 2.0 :

libraries needed to link to gtk+ 2.0

$ pkg-config gtk+-2.0 --libs
-lgtk-x11-2.0 -lgdk-x11-2.0 -latk-1.0 -lgdk_pixbuf-2.0 -lm -lpangocairo-1.0 
-lpango-1.0 -lcairo -lgobject-2.0 -lgmodule-2.0 -ldl -lglib-2.0

If the application is just using functions from gtk+ 2.0, a simple link line
with -lgtk-x11-2.0 should make it build fine, but looking at which libraries are
needed and which are not from a package point of view is often an impossible
task. 

How can --as-needed be useful?

<improve-startup-time>
The use of the --as-needed flag allows the linker to avoid linking extra
libraries in a binary. This not only improves 'startup' times (as the loader does
    not have to load all the libraries for every step) but might avoid the full
initialization of things like KDE's KIO for a binary if it's not using the KIO
framework. 

More importantly, the use of --as-needed 'avoids' adding dependencies to a
binary that are prerequisites of one of its direct or indirect dependencies.
This is important because when a library changes SONAME after an ABI change, all
the binaries directly linking to it have to be 'rebuilt'. 

By linking only the libraries that are actually needed, the breakage due to an
ABI change is reduced. It is particularly useful when the ABI breakage happens
in a library used by some other high level library (like cairo , which is used
    directly by gtk+-2.0 , and gets linked indirectly in applications using the
    latter), as it prevents the rebuild of the final binaries and thus of the
packages carrying them.

It is also useful to check whether the dependencies stated by the documentation
are actually used by a package: it's not impossible that a package checks in a
configure script for some library, and then links to it, but without using it at
all because the code using it was removed or refactored or has not been written. 

<final-linking> only for executable but not library
Failure in final linking, undefined symbols

This is the most common error that happens while using --as-needed. It happens
during the final linking stage of an executable note: libraries don't create
problems, because they are allowed to have undefined symbols. The executable
linking stage dies because of an undefined symbol that is present in one of the
libraries fed to the command line. However, the library is not used by the
executable itself, thus it gets 'removed' by --as-needed.

This usually means that a library was not linked to another library, but was
using it, and then 'relying' on the final executable to link them together. This
behavior is also an extra encumbrance on developers using that library because
they have to check for the requirements.

The fix to this kind of problem is usually simple: just find which library
provides the symbols and which one is requiring them (the error message from the
    linker should contain the name of the latter). Then make sure that when the
library is linked from the source files it's also linked to the first. 


={============================================================================
*kt_linux_core_402* slib: search and resolve

{search-rules}
note: >
LD_LIBRARY_PATH is broken and should 'not' be used if at all possible since
LD_LIBRARY_PATH is great for quick tests and for systems on which you don't have
admin privileges. However, as a downside, exporting the LD_LIBRARY_PATH variable
means it may cause problems with other programs if you don't reset it to its
previous state when you're done.


The dynamic linker searches for the shared library using the following rules:

1. If the executable has any directories listed in its DT_RPATH run-time library
path list (rpath) and the executable does not contain a DT_RUNPATH list, then
these directories are searched (in the order that they were supplied when
    linking the program).

2. If the LD_LIBRARY_PATH environment variable is defined, then each of the
colon-separated directories listed in its value is searched in turn. 

<security>
If the executable is a set-user-ID or set-group-ID program, then LD_LIBRARY_PATH
is ignored. This is a security measure to prevent users from tricking the
dynamic linker into loading a private version of a library with the same name as
a library required by the executable.

3. If the executable has any directories listed in its DT_RUNPATH run-time
library path list, then these directories are searched (in the order that they
    were supplied when linking the program).

4. The file /etc/ld.so.cache is checked to see if it contains an entry for the
library.

5. The directories /lib and /usr/lib are searched (in that order).

<rpath>
There is a third way: during the static editing phase, we can insert into the
executable a list of directories that should be searched at run time for shared
libraries. This is useful if we have libraries that reside in fixed locations
that are not among the standard locations searched by the dynamic linker. To do
this, we employ the -rpath linker option when creating an executable:

$ gcc -g -Wall -Wl,-rpath,/home/mtk/pdir -o prog prog.c libdemo.so

note: From ld linker docs:

2. Any directories specified by -rpath options. The difference between -rpath
and -rpath-link is that directories specified by -rpath options are included in
the executable and used at runtime, whereas the -rpath-link option is only
effective at link time. Searching -rpath in this way is only supported by native
linkers and cross linkers which have been configured with the --with-sysroot
option.


{runtime-resolution}
Suppose that a global symbol (i.e., a 'function' or variable) is defined in
multiple locations, such as in an executable and in a shared library, or in
'multiple' shared libraries. How is a reference to that symbol resolved?

         prog                                   libfoo.so
-----------------------------          -----------------------------
xyz() {                                xyz() {               
  printf("main-xyz\n");                  printf("foo-xyz\n");
}                                      }
                                       
main() {                               func() {
  func();    -->                         xyz();                
}                                      }       

$ gcc -g -c -fPIC -Wall -c foo.c
$ gcc -g -shared -o libfoo.so foo.o
$ gcc -g -o prog prog.c libfoo.so
$ LD_LIBRARY_PATH=. ./prog
main-xyz

See that the definition of xyz() in the main program overrides the one in the
shared library.

The following semantics apply:

o A definition of a global symbol in the 'main' program 'overrides' a definition
in a library.

<resolve-order>
o If a global symbol is defined in 'multiple' libraries, then a reference to
that symbol is bound to the 'first' definition found by scanning libraries in
the left-to-right 'order' in which they were listed on the static 'link' command
line.

<problem>
Although these semantics make the transition from static to shared libraries
relatively straightforward, they can cause some problems. The most significant
problem is that these semantics conflict with the model of a shared library as
implementing a 'self'-contained subsystem. 

By default, a shared library can 'not' guarantee that a reference to one of its
own global symbols will actually be bound to the library's definition of that
symbol.


={============================================================================
*kt_linux_core_403* slib: case problem in name resolve. crash

03/07/2014. At samsung. 
When moves application which was a process and uses static link to the shared
library application to be used by other processes. The codes which works well
before starts to fail since crash happens when try to create a thread using
custom thread library. 

Problem of linking? Problem of the thread library when used in shared library
application? Somehow linker picks up the wrong libaray since a debugger shows
odd address when thread creation call is made and causes a crash? 

Tried various directions and spent many days. Eventually, found out that
PCThread::Create() is a problem and works fine when changes parameter orders
which is different from the call the application believes to uses. 

The problem was that the process loads a lot of shared library and one of those
has the same PCThread class in it but different signature. When our library make
a call, it picks it up from the other shared library in which has different
signature so crashes. Sovled when wraps PCThread class with a namespace.

note: So this was a problem of symbols between shared libraries.

<Q> so when say 'symbol' in runtime resolution, does it mean 'name' only or the
whole signature?

# foo.c

#include <stdio.h>

void foo()           <1>
void foo(int x)      <2>
{
  printf("foo: this is foo... %d \n", x );
}

# main.c

#include <stdio.h>

// extern void foo(void);
void foo()
{
  printf("main: this is foo...\n");
}

int main(void)
{
  foo();       <1>
  foo(5);      <2>
  return 0;
}

$ gcc -c -fpic foo.c
$ gcc -shared -o libfoo.so foo.o
$ gcc -o prog main.c libfoo.so 
$ LD_LIBRARY_PATH=. ./prog
main: this is foo...

$ gcc -c -fpic foo.c
$ gcc -shared -o libfoo.so foo.o
$ gcc -o prog main.c libfoo.so 
$ LD_LIBRARY_PATH=. ./prog
main: this is foo...

note: Unlike the case-example, this is a problem between main and shared library.

<Q> So always picks up the main version and suggest that only use 'name' in C.
Given the real-case above, C++ may be different?

<Q> If multiple shared library are used in a process, all constitute a single
lookup space?

NO. As noted above, symbols in main overrides others.


<solution> to the case-example

1. As the case-example, can use namespace which effectively make a different
symbol.

2. As shown here, can use linker option, -Bsymbolic to ensure that the
invocation of the same symbol in the shared library actually called the version
of the function defined 'within' the library.

$ gcc -g -shared -Wl,-Bsymbolic -o libfoo.so foo.o

note: this means that main version gets called for the above example but would
solve the case-example since it force to pick up the one in the same library.


={============================================================================
*kt_linux_core_404* slib: case problem in name resolve. pthread stub 

{description}
2015.05. YouView.
When uses a wrapper process which in short exec application given, hangs on
application launch and appears hang on this wrapper when looks at top and call
stack from gdb. This wrapper has other works but not relavent in this problem.

The odd thing is that it only happens on the different hardware platform with
the same wrapper source.

shell exec -> wrapper -> application      // fail and hang
shell exec -> application                 // okay

#0 0x2ab64fb0 in pthread_cond_init () from /lib/libc.so.0
#1 0x2ae12d50 in global constructors keyed to FutureContextBase.cpp () 
  from /opt/zinc-trunk/lib/libZincCommon.so.0
#2 0x2ae35ed4 in __do_global_ctors_aux () 
  from /opt/zinc-trunk/lib/libZincCommon.so.0
#3 0x2adcc59c in ?? () from /opt/zinc-trunk/lib/libZincCommon.so.0


Since known that pthread stub from glibc was used from call stack and found that
works okay when specify pthread library in LD_PRELOAD explicitly, looked at
library dependancy and found difference between two platforms:

for not working platform
: wrapper                    
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]

: libxx which is preloaded before application
 0x00000001 (NEEDED)                     Shared library: [libTitaniumUtils.so.0]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]

: application
 ...
 0x00000001 (NEEDED)                     Shared library: [libdirect-1.4.so.15]
 0x00000001 (NEEDED)                     Shared library: [libpthread.so.0]
 0x00000001 (NEEDED)                     Shared library: [libdl.so.0]
 ...

for working platforms

: libxx which is preloaded before application
 ...
 0x00000001 (NEEDED)                     Shared library: [libstdc++.so.6]
 0x00000001 (NEEDED)                     Shared library: [libm.so.0]
 0x00000001 (NEEDED)                     Shared library: [libpthread.so.0]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]


So "libxx" has libpthread for working case and thought this explains the problem
since the stub was used for not working case. While having problem to mess with
as-needed in the build system and libtool, eventually turned out that the
problem happens regardless of pthread so in libxx.


<ld-bug>
The vendor investigation is:

o dynamic loader for working platform is more powerful. for not working,
  ld-0.9.29.so, for working, ld-0.9.32.1.so

o libpthread in working platform is changed to meet the require of dynamic
loader.

When libpthread.so is not included in LD_PRELOAD and on working platform, 

__dl_runtime_resolve will finish the relocation, for example before invoking
pthread_cond_init, the symbol still belongs to libc, but we really invoking
pthread_cont_init, the pthread_cond_init funciton will be relocated to
libpthread.  this is implemented by __dl_runtime_resolve, of course
__dl_runtime_resolve is invoked by dynamic loader.

If open the log of dynamic loader, we can see this:

resolve function: pthread_cond_init
patched 0x77a313e0 ==> 0x77cbfa60 @ 0x77a6cef8 (reloacated)

However, we can't see this log for not working platform.


<conclusion>
This is a problem of ld which fails to relocate symbols. This is vendor's
conclusion but the same condition such as the same ld version and others works
okay on the other vendor's platform. Believe that it's uclibc bug and it is
fixed in the latest and the other vendor patched the old version.

int __pthread_cond_init (pthread_cond_t *cond, const pthread_condattr_t *cond_attr) 
{ 
if (__libc_pthread_functions.ptr___pthread_cond_init == ((void *)0)) 
	return 0; 

// {
// added by me to test if 
if (__libc_pthread_functions.ptr___pthread_cond_init == __pthread_cond_init)
	return 0;
// }

return __libc_pthread_functions.ptr___pthread_cond_init (cond, cond_attr); 
}


<glibc-stub>
http://stackoverflow.com/questions/21092601/is-pthread-in-glibc-so-implemented-by-weak-symbol-to-provide-pthread-stub-functi

I know there is pthread.so to provide the functions similar with pthread in
glibc.so somebody said pthread in glibc provide stub only and will be replace
when explicit linking to lpthread.o my question is how to support it? using
weak symbol or other tech?


Yes, glibc uses a stub implementation of various pthread functions, so that
single threaded programs do not have to waste cycles doing things like locking
and unlocking mutexes, and yet do not have to link to a different C library
(like what is done in the Microsoft world, for instance).

For instance, according to POSIX, every time you call fputc(ch, stream), there
is mutex lock and unlock. If you don't want that, you call fputc_unlocked. But
when you do that, you're using a POSIX extension related to threading; it's not
an appropriate workaround for programs that don't use POSIX or don't use the
threading API.

The overriding of the stub pthread functions with the real ones (in the dynamic
    glibc) is not based on weak symbols. The shared library mechanism makes it
possible to override non-weak definitions.


note:
Weak symbols are a mechanism which allows for symbol overriding under 'static'
linking.

If you want a source for the above statement, here it is:

"Note that a definition in a DSO being weak has no effects. Weak definitions
only play a role in static linking." [Ulrich Drepper, "How To Write Shared
Libraries"].  http://www.akkadia.org/drepper/dsohowto.pdf

If you run nm on the static glibc on your system (if you have one), libc.a, you
will note that functions like pthread_mutex_lock are marked weak. In the dynamic
version, libc.so.<whatetever>, the functions are not marked weak.

Note: you should use nm -D or nm --dynamic to look at the symbols in a shared
library. nm will not produce anything on a shared library that is stripped. If
it does, you're looking at the debug symbols, not the dynamic symbols.


<from-embedded-libc>
root# ls -al /lib/libc.so.0
lrwxrwxrwx    1 root     root            19 Jan  1  1970 
  /lib/libc.so.0 -> libuClibc-0.9.29.so

$ nm -D libc.so.0 | ag pthread
000574b0 T __libc_pthread_init
00056d1c T __pthread_attr_init_2_1
00056f58 T __pthread_cond_broadcast
00056f84 T __pthread_cond_destroy
00056fb0 T __pthread_cond_init
00056fdc T __pthread_cond_signal
00057034 T __pthread_cond_timedwait
00057008 T __pthread_cond_wait
0005730c T __pthread_exit
         w __pthread_initialize_minimal
         w __pthread_mutex_init
         w __pthread_mutex_lock
         w __pthread_mutex_trylock
         w __pthread_mutex_unlock
         w _pthread_cleanup_pop_restore
         w _pthread_cleanup_push_defer
00056cf0 T pthread_attr_destroy
00056d48 T pthread_attr_getdetachstate
00056da0 T pthread_attr_getinheritsched
00056df8 T pthread_attr_getschedparam
00056e50 T pthread_attr_getschedpolicy
00056ea8 T pthread_attr_getscope
00056d1c W pthread_attr_init
00056d74 T pthread_attr_setdetachstate
00056dcc T pthread_attr_setinheritsched
00056e24 T pthread_attr_setschedparam
00056e7c T pthread_attr_setschedpolicy
00056ed4 T pthread_attr_setscope
00056f58 W pthread_cond_broadcast
00056f84 W pthread_cond_destroy
00056fb0 W pthread_cond_init
00056fdc W pthread_cond_signal
00057034 W pthread_cond_timedwait
00057008 W pthread_cond_wait
00056f00 T pthread_condattr_destroy
00056f2c T pthread_condattr_init
00057060 T pthread_equal
0005708c T pthread_getschedparam
000570e4 T pthread_mutex_destroy
00057110 T pthread_mutex_init
00057168 T pthread_mutex_lock
0005713c T pthread_mutex_trylock
00057194 T pthread_mutex_unlock
000571ec T pthread_mutexattr_destroy
000571c0 T pthread_mutexattr_init
00057218 T pthread_mutexattr_settype
00057244 T pthread_self
00057270 T pthread_setcancelstate
0005729c T pthread_setcanceltype
000570b8 T pthread_setschedparam


={============================================================================
*kt_linux_core_405* slib: dl, ABI, visibility

When an executable starts, the dynamic linker loads 'all' of the shared
libraries in the program's dynamic dependency list. Sometimes, however, it can
be useful to load libraries at a 'later' time. For example, a plug-in is loaded
only when it is needed. This functionality is provided by an API to the dynamic
linker.


{version-check}
The dynamic linker is 'itself' a 'shared' library, named /lib/ld-linux.so.2,
    which is employed by every ELF executable that uses shared libraries.

# for host:
$ ls -l /lib/ld-linux.so.2 
25 Oct 17 00:50 /lib/ld-linux.so.2 -> i386-linux-gnu/ld-2.13.so

# for-embedded:
# ls -al /lib/ld-linux.so.2 
24 Jan  1  1970 /lib/ld-linux.so.2 -> /lib/ld-uClibc-0.9.29.so


{dlopen-apis} from LPI 42.1
<dlopen>
#include <dlfcn.h>

void *dlopen(const char *libfilename, int flags);

Returns library handle on success, or NULL on error

The dlopen() opens a shared library, returning a handle used by sub-sequent
calls. Loads the shared library named in libfilename into the calling process's
virtual address space and increments the count of open references to the
library.

If the shared library specified by libfilename contains 'dependencies' on other
shared libraries, dlopen() also automatically loads those libraries. This
procedure occurs recursively if necessary. We refer to the set of such loaded
libraries as this library's dependency tree.

RTLD_LAZY

Undefined function symbols in the library should be resolved only as the code is
executed. If a piece of code requiring a particular symbol is not executed, that
symbol is 'never' resolved. Lazy resolution is performed 'only' for function
references; references to variables are always resolved immediately. Specifying
the RTLD_LAZY flag provides behavior that corresponds to the normal operation of
the dynamic linker when loading the shared libraries identified in an
executable's dynamic dependency list.

It is also possible to include further values in flags. The following flags are
specified in SUSv3:

RTLD_GLOBAL

Symbols in this library and its dependency tree are made available for resolving
references in other libraries loaded by this process and also for lookups via
dlsym().

<dlsym>
#include <dlfcn.h>

void *dlsym(void *handle, char *symbol);

Returns address of symbol, or NULL if symbol is not found

searches a library for a symbol (a string containing the name of a function or
    variable) and returns its address.

The value of a symbol returned by dlsym() may be NULL, which is
indistinguishable from the "symbol not found" return. In order to differentiate
the two possibilities, we must call dlerror() beforehand (to make sure that any
    previously held error string is cleared) and then if, after the call to
dlsym(), dlerror() returns a non-NULL value, we know that an error occurred.

The dlclose() function closes a library previously opened by dlopen().

The dlerror() function returns an error-message string, and is used after a
fail- ure return from one of the preceding functions.

<to-link>
To build programs that use the dlopen API on Linux, we must specify the -ldl
option, in order to link against the libdl library.


{access-symbols-in-main}
Sometimes, it is desirable instead to have x() in shared library invoke an
implementation of y() in the main program. In order to do this, we must make the
(global-scope) symbols in the main program available to the dynamic linker, by
linking the program using the --export-dynamic linker option:

$ gcc -Wl,--export-dynamic main.c (plus further options and arguments)

Equivalently, we can write the following:

$ gcc -export-dynamic main.c

Using either of these options allows a dynamically loaded library to access
global symbols in the main program.


{visibility}
A well-designed shared library should make visible only those symbols (functions
    and variables) that form part of its specified application binary interface
(ABI). The reasons for this are as follows:

o If the shared library designer accidentally exports unspecified interfaces,
  then authors of applications that use the library may choose to employ these
  interfaces. This creates a 'compatibility' problem for future upgrades of the
  shared library.

The library developer expects to be able to change or remove any interfaces
'other' than those in the documented ABI, while the library user expects to
continue using the same interfaces (with the same semantics) that they currently
employ.

o During run-time symbol resolution, any symbols that are exported by a shared
library might interpose definitions that are provided in other shared libraries.

note: this is a problem as <resolve-real-problem>?

o Exporting unnecessary symbols increases the size of the dynamic symbol table
that must be loaded at runtime. note: also means 'faster' loading time?

All of these problems can be minimized or avoided altogether if the library designer ensures that
only the symbols required by the library's specified ABI are exported. 

The following techniques can be used to control the export of symbols:

In a C program, we can use the static keyword to make a symbol private to a source-code module, thus
rendering it unavailable for binding by other object files.

note:
As well as making a symbol private to a source-code module, the static keyword also has a converse
effect. If a symbol is marked as static, then all references to the symbol in the same source file
will be bound to that definition of the symbol. Consequently, these references won't be subject to
run-time interposition by definitions from other shared libraries in the manner described in Section
41.12. This effect of the static keyword is similar to the -Bsymbolic linker option described in
Section 41.12, with the difference that the static keyword affects a 'single' symbol within a single
source file.

@ The GNU C complier, gcc, provides a compiler-specific attribute declaration that performs a
similar task to the static keyword:

void __attribute__ ((visibility("hidden")))
func(void) {
   /* Code */
}

note: see *kt_dev_gcc_110* gcc: attributes

Whereas the static keyword limits the visibility of a symbol to a single source code file, the
hidden attribute makes the symbol available across all source code files that compose the shared
library, but prevents it from being visible outside the library.

As with the static keyword, the hidden attribute also has the converse effect of preventing symbol
interposition at runtime.

note: there are more ways in LPI 42.3.


={============================================================================
*kt_linux_core_406* shared library: preload, debug and monitor ld

{ld-preload}
For testing purposes, it can sometimes be useful to 'selectively' override functions and other symbols
that would normally be found by the dynamic linker using the search rules. 

To do this, we can define the environment variable LD_PRELOAD as a string consisting of
space-separated or colon-separated names of shared libraries that should be loaded 'before' any
other shared libraries. 

Since these libraries are loaded first, any functions they define will automatically be used if
required by the executable, thus overriding any other functions of the same 'name' that the dynamic
linker would otherwise have searched for.

$ ./prog
Called mod1-x1 DEMO
Called mod2-x2 DEMO

$ LD_PRELOAD=libalt.so ./prog
Called mod1-x1 ALT                  # x1 in libalt.so and changed this only, selectively.
Called mod2-x2 DEMO

<process-and-system-wide>
The LD_PRELOAD environment variable controls preloading on a <per-process> basis. Alternatively, the
file /etc/ld.so.preload, which lists libraries separated by white space, can be used to perform the
same task on a system-wide basis. Libraries specified by LD_PRELOAD are loaded before those
specified in /etc/ld.so.preload. 

<security>
For security reasons, set-user-ID and set-group-ID programs ignore LD_PRELOAD.

<tip>
You can use the following command to test if the driver (the driver itself just dlopen() this lib)
    is happy to load:

LD_PRELOAD=/opt/zinc/lib/libyouviewrcu.so /bin/true


{ld-debug}
Sometimes, it is useful to monitor the operation of the dynamic linker in order to know, for
example, where it is searching for libraries. We can use the LD_DEBUG environment variable to do
this. By setting this variable to one (or more) of a set of standard keywords, we can obtain various
kinds of tracing information from the dynamic linker.

If we assign the value help to LD_DEBUG, the dynamic linker displays help information about
LD_DEBUG, and the specified command is not executed:

note: can be any command other 
$ LD_DEBUG=help ls  
Valid options for the LD_DEBUG environment variable are:

  libs        display library search paths
  reloc       display relocation processing
  files       display progress for input file
  symbols     display symbol table processing
  bindings    display information about symbol binding
  versions    display version dependencies
  all         all previous options combined
  statistics  display relocation statistics
  unused      determined unused DSOs
  help        display this help message and exit

To direct the debugging output into a file instead of standard output
a filename can be specified using the LD_DEBUG_OUTPUT environment variable.


$ LD_DEBUG=libs ls
     20177:	find library=libselinux.so.1 [0]; searching
     20177:	 search path=./tls/x86_64:./tls:./x86_64:.		(LD_LIBRARY_PATH)
     20177:	  trying file=./tls/x86_64/libselinux.so.1
     20177:	  trying file=./tls/libselinux.so.1
     20177:	  trying file=./x86_64/libselinux.so.1
     20177:	  trying file=./libselinux.so.1
     20177:	 search cache=/etc/ld.so.cache
     20177:	  trying file=/lib/x86_64-linux-gnu/libselinux.so.1
     20177:	
     20177:	find library=librt.so.1 [0]; searching
     20177:	 search path=./tls/x86_64:./tls:./x86_64:.		(LD_LIBRARY_PATH)
     20177:	  trying file=./tls/x86_64/librt.so.1
     20177:	  trying file=./tls/librt.so.1
     20177:	  trying file=./x86_64/librt.so.1
     20177:	  trying file=./librt.so.1
     ...
     20177:	
install-sh  ltmain.sh  Makefile.am  Makefile.in  missing  src $ 

note: ls command was executed. works on host but not on a target. use strace instead.

The PID value displayed at the start of each line and this is useful if we are monitoring several
processes (e.g., parent and child).

If desired, we can assign multiple options to LD_DEBUG by separating them with commas (no spaces
    should appear). 

<symbol-option>
The output of the symbols option (which traces symbol resolution by the dynamic linker) is
particularly voluminous.

LD_DEBUG is effective both for libraries implicitly loaded by the dynamic linker and for libraries
dynamically loaded by dlopen().

For security reasons, LD_DEBUG is (since glibc 2.2.5) ignored in set-user-ID and set- group-ID
programs.


={============================================================================
*kt_linux_core_407* shared library: further information

Further information

Various information related to static and shared libraries can be found in the ar(1), gcc(1), ld(1),
ldconfig(8), ld.so(8), dlopen(3), and objdump(1) manual pages and in the info documentation for ld
  and readelf. [Drepper, 2004 (b)] covers many of the finer details of writing shared libraries on
  Linux. Further useful information can also be found in David Wheeler's Program Library HOWTO,
which is online at the LDP web site, http://www.tldp.org/. 

The GNU shared library scheme has many similarities to that implemented in Solaris, and therefore it
is worth reading Sun¿s Linker and Libraries Guide (available at http://docs.sun.com/) for further
information and examples. [Levine, 2000] provides an introduction to the operation of static and
dynamic linkers.

Information about GNU Libtool, a tool that shields the programmer from the implementation-specific
details of building shared libraries, can be found online at http://www.gnu.org/software/libtool and
in [Vaughan et al., 2000].

The document Executable and Linking Format, from the Tools Interface Standards committee, provides
details on ELF. This document can be found online at http://refspecs.freestandards.org/elf/elf.pdf.
[Lu, 1995] also provides a lot of useful detail on ELF.


={============================================================================
*kt_linux_core_408* shared library: md5sum

{can-use-on-library-to-check-integrity}
The idea is that can use md5sum on a library to confirm that it uses the exact same compile and link
options. The assumption is that if both party has the same build configuration, the library made
from the same source 'must' have the same md5 checksum. If that is the case, can use the md5
checksum as a quick way to check if both party has and uses the same build configuration.

Is it true?

https://gcc.gnu.org/ml/gcc-help/2010-01/msg00082.html
beaugy.a@free.fr wrote:

    Hi all,
    So, to put it in a nutshell, all my generated objects file are
    identical on dev1 and dev2 and object files contained in my
    convenience libraries are all identical. The only difference
    remaining, before I generate my binary, resides in the generated
    convenience libraries which are not identical, but their contents
    are. So AFAK, this slight difference shall not make the difference. So
    "why does gcc output (MD5 checksum) differs when I build a binary
    using the project object files (*.o) or the project convenience
    libraries (*.a)?" and "what can I do to fix that?".

Your *.o files are proceeded in a different order when on the command line and on the .a archive,
     putting symbols on different addresses, so obviously different binaries are produced.

Even if you do:
gcc 1.o 2.o
And:
gcc 2.o 1.o

you get binaries with different md5.

<example>
This is example from "*kt_dev_gcc_103* gcc link and ld"

$ cat simplefunc.c
int func(int i) {
    return i + 21;
}

$ cat simplemain.c
int func(int);

int main(int argc, const char* argv[])
{
    return func(argc);
}

$ gcc -c simplefunc.c
$ gcc -c simplemain.c
$ gcc simplefunc.o simplemain.o
$ ./a.out ; echo $?
22

:~/work$ nm simplefunc.o
00000000 T func

:~/work$ nm simplemain.o 
         U func
00000000 T main

$ ar r libsimplefunc.a simplefunc.o    // ar rs to skip ranlib command.
$ ranlib libsimplefunc.a
$ gcc simplemain.o -L. -lsimplefunc
$ ./a.out ; echo $?
22

Now we have *.o and *.a files and run the same build and ar command without changes in source. Just
make o and a files.

<o-files-are-the-same>
keitee@debian-keitee:~/work$ ll simplefunc.*
-rw-r--r-- 1 keitee keitee 848 Jan 20 22:15 simplefunc.o
-rw-r--r-- 1 keitee keitee 848 Jan 20 22:06 simplefunc.o.old
keitee@debian-keitee:~/work$ diff simplefunc.o simplefunc.o.old 
keitee@debian-keitee:~/work$ 

<a-files-are-different>
keitee@debian-keitee:~/work$ ll libsimplefunc.a*
-rw-r--r-- 1 keitee keitee 990 Jan 20 22:14 libsimplefunc.a
-rw-r--r-- 1 keitee keitee 990 Jan 20 22:14 libsimplefunc.a.old
keitee@debian-keitee:~/work$ diff libsimplefunc.a libsimplefunc.a.old 
Binary files libsimplefunc.a and libsimplefunc.a.old differ
keitee@debian-keitee:~/work$ 

The md5 checksum result shows the same.

keitee@debian-keitee:~/work$ md5sum simplefunc.o*
3acb728c611c4c936320d64c1b360633  simplefunc.o
3acb728c611c4c936320d64c1b360633  simplefunc.o.old

keitee@debian-keitee:~/work$ md5sum libsimplefunc.a*
87663beba78feef15106bf156a8f6ef3  libsimplefunc.a
02a369c6a4476eea407bec53beaa7b95  libsimplefunc.a.old

So even when make a library from the same object at 'different' time, it will create different
library file.

<Q> However, when do the md5sum on library files created from project build at different time, the
libraries are the same on diff and md5. WHY?

Tried to make a shared library with the same code:

gcc -fpic -shared -o simple.so simplefunc.c
ar rs libsimple.so simple.so

Again, simple.so files are the same but libsimple.so are differ. This means that when run ar, will
have different output files. Then how were the libraries the same for the above case?

<A> The answer is that do not use ar when make a shared library and then will the same md5sum.

-rw-r--r-- 1 kpark kpark 1528 Feb  5 10:02 foo.o
-rw-r--r-- 1 kpark kpark 1528 Feb  5 09:33 foo.o.old
-rwxr-xr-x 1 kpark kpark 6407 Feb  5 10:03 libfoo.so*
-rwxr-xr-x 1 kpark kpark 6407 Feb  5 09:33 libfoo.so.old*

$ diff foo.o foo.o.old 
$ diff libfoo.so libfoo.so.old 
$ md5sum foo.o*
1f87ad103b677a3090707fee9daaea33  foo.o
1f87ad103b677a3090707fee9daaea33  foo.o.old

$ md5sum libfoo.so*
e64fd5c673979f09360178f938e6e1b7  libfoo.so
e64fd5c673979f09360178f938e6e1b7  libfoo.so.old


={============================================================================
*kt_linux_core_409* shared library: inspect dynamic sections

$ readelf -d libgstnexus.so.from.huawei.box 

Dynamic section at offset 0x16c contains 35 entries:
  Tag        Type                         Name/Value
 0x00000001 (NEEDED)                     Shared library: [libnexusMgr.so]
 0x00000001 (NEEDED)                     Shared library: [libnexus.so]
 0x00000001 (NEEDED)                     Shared library: [libgstbase-1.0.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgstmpegts-1.0.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgstreamer-1.0.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgobject-2.0.so.0]
 0x00000001 (NEEDED)                     Shared library: [libglib-2.0.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libpthread.so.0]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
 0x0000000e (SONAME)                     Library soname: [libgstnexus.so.0]
 0x0000000f (RPATH)                      Library rpath: [/usr/local/lib]
 0x0000001d (RUNPATH)                    Library runpath: [/usr/local/lib]
 ...


$ objdump -p libgstnexus.so.from.huawei.box 

libgstnexus.so.from.huawei.box:     file format elf32-little

...

Dynamic Section:
  NEEDED               libnexusMgr.so
  NEEDED               libnexus.so
  NEEDED               libgstbase-1.0.so.0
  NEEDED               libgstmpegts-1.0.so.0
  NEEDED               libgstreamer-1.0.so.0
  NEEDED               libgobject-2.0.so.0
  NEEDED               libglib-2.0.so.0
  NEEDED               libgcc_s.so.1
  NEEDED               libpthread.so.0
  NEEDED               libc.so.0
  SONAME               libgstnexus.so.0
  RPATH                /usr/local/lib
  RUNPATH              /usr/local/lib

...


note: this is ldd on a target which is executable but ldd on host pc is scripts and do not show
details like this.

root# ldd libgstnexus.so
checking sub-depends for '/usr/local/lib/libnexusMgr.so'
checking sub-depends for '/usr/local/lib/libnexus.so'
checking sub-depends for '/opt/zinc/oss/lib/libgstbase-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgstmpegts-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgstreamer-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgobject-2.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libglib-2.0.so.0'
checking sub-depends for '/lib/libgcc_s.so.1'
checking sub-depends for '/lib/libpthread.so.0'
checking sub-depends for '/lib/libc.so.0'
checking sub-depends for '/lib/libstdc++.so.6'
checking sub-depends for '/lib/libm.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgmodule-2.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libintl.so.8'
checking sub-depends for '/lib/libdl.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libffi.so.5'
	libnexusMgr.so => /usr/local/lib/libnexusMgr.so (0x00000000)
	libnexus.so => /usr/local/lib/libnexus.so (0x00000000)
	libgstbase-1.0.so.0 => /opt/zinc/oss/lib/libgstbase-1.0.so.0 (0x00000000)
	libgstmpegts-1.0.so.0 => /opt/zinc/oss/lib/libgstmpegts-1.0.so.0 (0x00000000)
	libgstreamer-1.0.so.0 => /opt/zinc/oss/lib/libgstreamer-1.0.so.0 (0x00000000)
	libgobject-2.0.so.0 => /opt/zinc/oss/lib/libgobject-2.0.so.0 (0x00000000)
	libglib-2.0.so.0 => /opt/zinc/oss/lib/libglib-2.0.so.0 (0x00000000)
	libgcc_s.so.1 => /lib/libgcc_s.so.1 (0x00000000)
	libpthread.so.0 => /lib/libpthread.so.0 (0x00000000)
	libc.so.0 => /lib/libc.so.0 (0x00000000)
	libstdc++.so.6 => /lib/libstdc++.so.6 (0x00000000)
	libm.so.0 => /lib/libm.so.0 (0x00000000)
	libgmodule-2.0.so.0 => /opt/zinc/oss/lib/libgmodule-2.0.so.0 (0x00000000)
	libintl.so.8 => /opt/zinc/oss/lib/libintl.so.8 (0x00000000)
	libdl.so.0 => /lib/libdl.so.0 (0x00000000)
	libffi.so.5 => /opt/zinc/oss/lib/libffi.so.5 (0x00000000)
	not a dynamic executable
root# 


={============================================================================
*kt_linux_core_410* shared library: check libraries that  process uses

cat /proc/NNNN/maps | awk '{print $6}' | grep '\.so' | sort | uniq

or

can use ldd.


={============================================================================
*kt_linux_core_411* slib: case problem in open failure

/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so:     
  file format elf32-tradlittlemips

Dynamic Section:
  NEEDED               libnexusMgr.so.0 ~
  NEEDED               libnexus.so
  NEEDED               libgstbase-1.0.so.0
  NEEDED               libgstmpegts-1.0.so.0
  NEEDED               libgstreamer-1.0.so.0
  NEEDED               libgobject-2.0.so.0
  NEEDED               libglib-2.0.so.0
  NEEDED               libgcc_s.so.1
  NEEDED               libpthread.so.0
  NEEDED               libc.so.0
  SONAME               libgstnexus.so.0
  RPATH                /usr/local/lib
  RUNPATH              /usr/local/lib

From strace log:

open("/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so", O_RDONLY) = 60

// see how search path works

open("/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/opt/zinc-trunk/oss/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/opt/zinc-trunk/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/usr/local/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)

open("/opt/zinc-trunk/oss/lib/gstreamer-1.0/libnexusMgr.so.0", O_RDONLY) = -1
ENOENT (No such file or directory)

open("/opt/zinc-trunk/devel/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/opt/zinc-trunk/tests/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/usr/local/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/usr/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)

// this is error on strerr

write(2, "\n(w3cEngine:1641): GStreamer-WARN"..., 134
(w3cEngine:1641): GStreamer-WARNING **: 
  Failed to load plugin '/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so': 
  File not found
) = 134


note: Even though the real issue is that there is no Mgr.so which is the next
so, the error suggest that failed to open the starting so.


={============================================================================
*kt_linux_core_411* slib: points to enhance performance

Again, reiterate pros and cons:

{downside-of-static}
1. Duplicates in disk and ram spce.

2. If a change is required perhaps a security or bug fix to an object module in
a static library, then all executables using that module must be relinked in
order to incorporate the change. This disadvantage is further compounded by the
fact that the system administrator needs to be aware of which applications were
linked against the library.


{what-is-shared}
Although the code of a shared library is shared among multiple processes, its
variables are not. Each process that uses the library has its own copies of the
global and static variables that are defined within the library.


{further-advantages}
o Because overall program size is smaller, in some cases, programs can be loaded
into memory and started more 'quickly'. This point holds true only for large
shared libraries that are already in use by another program.

o Such changes can be carried out even while running programs are using an
existing version of the shared library.


{cost-of-shared}
o Shared libraries are more 'complex' than static libraries, both at the
conceptual level, and at the practical level of creating shared libraries and
building the programs that use them.

o Shared libraries 'must' be compiled to use position-independent code, which
has a performance 'overhead' on most architectures because it requires the use
of an extra register 

o Symbol relocation must be performed at run time. During symbol relocation,
  references to each symbol (a variable or function) in a shared library need to
  be modified to correspond to the actual run-time location at which the symbol
  is placed in virtual memory. Take a little more time to execute.


{points-to-think}

From YV real cases:

1. Reordering LD_LIBRARY_PATH

Quote {
From the visualisation, its fairly obvious the there are many (failing) repeated
attempts to find common libraries in /opt/zinc/lib /opt/zinc/oss/lib, before
finding them in there actual location in /lib

In the "before" and "after" visualisations, you can see the significant (0.5)
improvement for reordering of LD_LIBRARY_PATH.
Quote }

To see what is the best LD_LIBRARY_PATH ordering, use strace to show all the
calls to open that succeed and all the calls that failed during the application
startup. 

It seems that reordering the LD_LIBRARY_PATH brings an improvement of 0.2s. The
precision of the profiling has to be taken into account: 0.2s is not much bigger
than the precision of 0.1s.


2. Reduce the code size

Moreover, #Including <zinc-common/logger.h> causes object code bloat. A symbol
for a boost::shared_ptr<LoggerCache> is emitted for every translation unit that
includes the header. This means that the symbol is duplicated over 800 times in
our stack, this means that the stack size is bigger and therefore that it will
take more time to load the shared libraries.

It seems that removing the logger brings an improvement of 0.25s. The precision
of the profiling has to be taken into account: 0.25s is not much bigger than the
precision of 0.10s.


3. Removing unnecessary library dependencies

In order to reduce the number of dependencies, John mentioned a linker option
that allows to keep the dependencies that are really needed: --as-needed.
Normally, it would be as easy as adding this option to the global LDFLAGS and
recompiling the stack: export LDFLAGS="-Wl,--as-needed ${LDFLAGS}"

dynamic libraries mentioned on the command line after the --as-needed option
Unfortunately, the --as-needed option will only apply to the dynamic libraries
that appears after it.  And unfortunately, autotools reorders the parameters and
the --as-needed ends up at the end of the compilation parameters.

Fortunately, we are not the only one using autotools and there are some patches
for fixing that: See

Why –as–needed doesn't work as expected for your libraries on your autotools
project

http://sigquit.wordpress.com/2011/02/16/why-asneeded-doesnt-work-as-expected-for-your-libraries-on-your-autotools-project/

for more details and for links to patches.

Another issue is that this might require some Makefile changes for projects that
forget to list some dependencies and that happened to get them by transitive
dependencies.

We can finally see that this change brings an improvement of 0.32s. We have to
keep in mind that in this case, the box is not busy at all. Therefore, the gain
of 0.32s could be much higher in production when the performance are IO bound.


4. Reducing exported symbols

By applying the flag -fvisibility=hidden

This causes an improvement of 0.4s with sandboxing off and a bit less with
sandboxing on (no idea why?). This is a considerable improvement.

note: This figure comes from a situation when FLASH application loads lots of
libraries from the mw stack.

5. Removing script

As seen previously, a lot of time is spent in Python. It also seems to be a
reasonable idea to remove the python scripts from the launch chain as they did
not have a real justification.


={============================================================================
*kt_linux_core_412* slib: ld.conf and ld.cache

/etc/ld.so.cache

/etc/ld.so.cache is required to run D-BUS daemon as non-root. This is due to
the fact that LD_LIBRARY_PATH is ignored when it calls setuid
yv-daemon-sandbox (this is intended behaviour). The dynamic linker does not
resolve shared object dependencies properly in this situation. Using RPATH
could help with that but it is generally not recommended and it is broken in
uClibc.

In order to create ld.so.cache you may create /etc/ld.so.conf with the
following content:

/opt/zinc/oss/lib
/opt/zinc/lib
/usr/local/lib
/opt/stagecraft-2.0/bin

and run ldconfig. You can get ldconfig as part of uClibc build or download
MIPSEL rootfs from the uClibc site and extract it from there. Note that you
should generate /etc/ld.so.cache with every build as the list of the shared
libraries may differ.


={============================================================================
*kt_linux_core_412* slib: as-needed and _GLOBAL_OFFSET_TABLE_

The case is that shared so has liked with another shared library as:

mipsel-linux-gcc -shared sqlite3.o -Wl,--as-needed 
  -Wl,-soname -Wl,libsqlite3.so.0 
  -o .libs/libsqlite3.so

note: no -shared or -fPIC used to make the second shared so.

mipsel-linux-gcc -Wl,--as-needed 
-o .libs/sqlite3 shell.o ./.libs/libsqlite3.so -ldl -lpthread

$ readelf -d sqlite3

 0x00000001 (NEEDED)                     Shared library: [libsqlite3.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]

The problem is that on certain target, for the same package, the link to make
the second so fails with link error:

./.libs/libsqlite3.so:(.got+0x0): multiple definition of `_GLOBAL_OFFSET_TABLE_'
collect2: ld returned 1 exit status
make: *** [xxx] Error 1


Q1: Founds that links works okay without as-needed. Why?
Q2: Why the same commands does work on the other target?

A: The reason for that is that 'ld' seems to have a bug:
                       
https://sourceware.org/ml/binutils/2013-02/msg00159.html

Re: binutils 2.19.92 linker broke with --as-needed flag

On Mon, Feb 04, 2013 at 05:58:37PM -0800, Vincent Wen wrote:
> The linker broke when as-need flag is added.
> 
>  /bin/sh ../libtool --tag=CC --mode=link mipsel-linux-gcc -I../include -g
> -O2 -Wl,--as-needed -o test-example test.o ../lib/libTestGcc.la
> libtool: link: mipsel-linux-gcc -I../include -g -O2 -Wl,--as-needed -o
> .libs/test-example test.o ../lib/.libs/libTestGcc.so -Wl,-rpath
> -Wl,/usr/local/lib
> ../lib/.libs/libTestGcc.so:(.got+0x0): multiple definition of
> `_GLOBAL_OFFSET_TABLE_'

I think mips was broken before --as-needed.  mips seems to want a
dynamic _GLOBAL_OFFSET_TABLE_ symbol in shared libs, presumably for
use by ld.so.  However, a global symbol will be resolved by ld.so
according to the ELF rules which will result in the symbol being
resolved to the first definition seen in a breadth first search of the
application and its shared libraries.  That means the value in the
first shared lib.  So the value seen in other shared libs is wrong.
_GLOBAL_OFFSET_TABLE_ must resolve locally.

So _GLOBAL_OFFSET_TABLE_ must at least be STV_PROTECTED, and could be
STV_HIDDEN as you do in your patch.

> --- a/bfd/elfxx-mips.c    2013-02-01 03:26:00.000000000 -0800
> +++ b/bfd/elfxx-mips.c    2013-02-01 03:26:16.000000000 -0800
> @@ -4681,6 +4681,7 @@
>    h->non_elf = 0;
>    h->def_regular = 1;
>    h->type = STT_OBJECT;
> +  h->other = STV_HIDDEN;
>    elf_hash_table (info)->hgot = h;
> 
>    if (info->shared

The trouble with this is that making it STV_HIDDEN results in no
dynamic _GLOBAL_OFFSET_TABLE_ symbol at all, due to the following code
in bfd_elf_link_record_dynamic_symbol.

      /* XXX: The ABI draft says the linker must turn hidden and
	 internal symbols into STB_LOCAL symbols when producing the
	 DSO. However, if ld.so honors st_other in the dynamic table,
	 this would not be necessary.  */
      switch (ELF_ST_VISIBILITY (h->other))
	{
	case STV_INTERNAL:
	case STV_HIDDEN:
	  if (h->root.type != bfd_link_hash_undefined
	      && h->root.type != bfd_link_hash_undefweak)
	    {
	      h->forced_local = 1;
	      if (!elf_hash_table (info)->is_relocatable_executable)
		return TRUE;
	    }

Now that code is also wrong, I think.  The "return TRUE" should never
happen.

-- 
Alan Modra
Australia Development Lab, IBM


Found that:

GNU ld (GNU Binutils for Debian) 2.22   : Link fails on mips
GNU ld (GNU Binutils) 2.19.1            : Likk okay on mips


={============================================================================
*kt_linux_core_500* sandbox

There are occasional situations where static libraries may be appropriate. If the program is to be
run in an environment (perhaps a chroot jail, for example) where shared libraries are unavailable.


={============================================================================
*kt_linux_core_500* file io

#include <sys/stat.h>
#include <fcntl.h>
#include <sys/wait.h>
#include "tlpi_hdr.h"

int
main(int argc, char *argv[])
{
  int fd, flags;
  char template[] = "/tmp/testXXXXXX";

  setbuf(stdout, NULL);    // {KT} Disable buffering of stdout

  fd = mkstemp(template);  // opens a temporary file
  if (fd == -1)
    errExit("mkstemp");

  printf("File offset before fork(): %lld\n", (long long) lseek(fd, 0, SEEK_CUR));

  flags = fcntl(fd, F_GETFL);    // get file flags
  if (flags == -1)
    errExit("fcntl - F_GETFL");
  
  printf("O_APPEND flag before fork() is: %s\n", (flags & O_APPEND) ? "on" : "off");

  switch (fork()) {
    case -1:
      errExit("fork");

    case 0: /* Child: change file offset and status flags */
      if (lseek(fd, 1000, SEEK_SET) == -1)
        errExit("lseek");

      flags = fcntl(fd, F_GETFL); /* Fetch current flags */
      if (flags == -1)
        errExit("fcntl - F_GETFL");

      flags |= O_APPEND; /* Turn O_APPEND on */
      if (fcntl(fd, F_SETFL, flags) == -1)
        errExit("fcntl - F_SETFL");
      _exit(EXIT_SUCCESS);

    default: /* Parent: can see file changes made by child */
      if (wait(NULL) == -1)
        errExit("wait"); /* Wait for child exit */

      printf("Child has exited\n");
      printf("File offset in parent: %lld\n", (long long) lseek(fd, 0, SEEK_CUR));

      flags = fcntl(fd, F_GETFL);
      if (flags == -1)
        errExit("fcntl - F_GETFL");

      printf("O_APPEND flag in parent is: %s\n", (flags & O_APPEND) ? "on" : "off");
      exit(EXIT_SUCCESS);
  }
}

For an explanation of why we cast the return value from lseek() to long long in Listing 24-2, see
Section 5.10.


={============================================================================
*kt_linux_core_600* time

LPI 10.

{two-kinds}
Within a program, we may be interested in two kinds of time

o Real time: This is the time as measured either from some standard point (calendar time) or from
some fixed point (typically the start) in the life of a process (elapsed or wall clock time).
Obtaining the calendar time is useful to programs that, for example, timestamp database records or
files. Measuring elapsed time is useful for a program that takes periodic actions or makes regular
measurements from some external input device.

o Process time: This is the amount of CPU time used by a process. Measuring process time is useful
for checking or optimizing the performance of a program or algorithm.


{calender-time}
Regardless of geographic location, UNIX systems represent time internally as a measure of seconds
since the Epoch; that is, since midnight on the morning of 1 January 1970, Universal Coordinated
Time (UTC, previously known as Greenwich Mean Time, or GMT). This is approximately the date when the
UNIX system came into being. Calendar time is stored in variables of type time_t, an integer type
specified by SUSv3.

note: UTC == GMT


{time-t-type}
On 32-bit Linux systems, time_t, which is a signed integer, can represent dates in the range 13
December 1901 20:45:52 to 19 January 2038 03:14:07. SUSv3 leaves the meaning of negative time_t
values unspecified. 

Thus, many current 32-bit UNIX systems face a theoretical Year 2038 problem, which they may
encounter before 2038, if they do calculations based on dates in the future.  This problem will be
significantly alleviated by the fact that by 2038, probably all UNIX systems will have long become
64-bit and beyond. However, 32-bit embedded systems, which typically have a much longer lifespan
than desktop hardware, may still be afflicted by the problem. Furthermore, the problem will remain
for any legacy data and applications that maintain time in a 32-bit time_t format.

note: Appears that 32bit also has 64 bits time_t? This is taken from debian 32 bits in VM.

Linux debian-keitee 3.2.0-4-486 #1 Debian 3.2.63-2+deb7u2 i686 GNU/Linux

__STD_TYPE __TIME_T_TYPE __time_t;	/* Seconds since the Epoch.  */

__TIME_T_TYPE      50 i386-linux-gnu/bits/typesizes.h #define __TIME_T_TYPE	__SLONGWORD_TYPE

__SLONGWORD_TYPE  103 i386-linux-gnu/bits/types.h #define __SLONGWORD_TYPE	long int


{gettimeofday-time}
The gettimeofday() system call returns the 'calendar' time in the buffer pointed to by tv.

#include <sys/time.h>

int gettimeofday(struct timeval *tv, struct timezone *tz);

Returns 0 on success, or -1 on error

struct timeval {
    time_t tv_sec;         /* Seconds since 00:00:00, 1 Jan 1970 UTC */
    suseconds_t tv_usec;   /* Additional microseconds (long int) */
};

The tz argument to gettimeofday() is a historical artifact. This argument is now obsolete and should
always be specified as NULL.


<time-call>
The time() system call returns the number of seconds since the Epoch. i.e., the same value that
gettimeofday() returns in the tv_sec field of its tv argument.

#include <time.h>

time_t time(time_t *timep);

Returns number of seconds since the Epoch,or (time_t) -1 on error

Since time() returns the same value in two ways, we often simply use the following call without
error checking:

t = time(NULL);

note:
The existence of time() as a system call is historical and now redundant; it could be implemented as
a library function that calls gettimeofday().


={============================================================================
*kt_linux_core_601* time: conversion

Shows the functions used to convert between time_t values and other time formats, including
printable representations. These functions shield us from the complexity brought to such conversions
by timezones, daylight saving time (DST) regimes, and localization issues.

* affected by TZ env variable
+ affected by locale 

Kernel   -> time()               time_t               -> ctime()*    fixed-format string
         <- stime()                                                  Tue Feb 1 21:39:46 2011\n\0

         -> gettimeofday()       struct timeval
         <- settimeofday()

         -> gmtime()             struct tm            -> asctime()
         -> localtime()*         (broken down time)
         <- mktime()*            struct tm            -> strftime()*+   user-formatted,
                                                      <- strptime()+    localized string


{to-printable-form}
The ctime() function provides a simple method of converting a time_t value into printable form. The
ctime() function automatically accounts for local timezone and DST settings when performing the
conversion.

The returned string is statically allocated; future calls to ctime() will overwrite it.

#include <time.h>

char *ctime(const time_t *timep);

Returns pointer to statically allocated string terminated by newline and \0 on success, or NULL on
error

<statically-allocated>
SUSv3 states that calls to any of the functions ctime(), gmtime(), localtime(), or asctime() may
overwrite the statically allocated structure that is returned by any of the other functions. In
other words, these functions may share single copies of the returned character array and tm
structure, and this is done in some versions of glibc. If we need to maintain the returned
information across multiple calls to these functions, we must save local copies.


{to-broken-down-from}
The gmtime() and localtime() functions convert a time_t value into a so-called brokendown time. The
broken-down time is placed in a statically allocated structure whose address is returned as the
function result.

#include <time.h>

struct tm *gmtime(const time_t *timep);
struct tm *localtime(const time_t *timep);

Both return a pointer to a statically allocated broken-down time structure on success, or NULL on
error

Unlike gmtime(), localtime() takes into account timezone and DST settings to return a broken-down
time corresponding to the system's local time.

struct tm {
    int tm_sec;      /* Seconds (0-60) */
    int tm_min;      /* Minutes (0-59) */
    int tm_hour;     /* Hours (0-23) */
    int tm_mday;     /* Day of the month (1-31) */
    int tm_mon;      /* Month (0-11) */
    int tm_year;     /* Year since 1900 */   note: 1900 but not 1970
    int tm_wday;     /* Day of the week (Sunday = 0)*/
    int tm_yday;     /* Day in the year (0-365; 1 Jan = 0)*/
    int tm_isdst;    /* Daylight saving time flag
                     > 0: DST is in effect;
                     = 0: DST is not effect;
                     < 0: DST information not available */
};


{between-broken-down-and-printable-form}
functions that convert a broken-down time to printable form, and vice versa.

<no-control-format>
#include <time.h>

char *asctime(const struct tm *timeptr);

Returns pointer to statically allocated string terminated by newline and \0 on success, or NULL on
error

By contrast with ctime(), local timezone settings have no effect on asctime(), since it is
converting a broken-down time that is typically either already localized via localtime() or in UTC
as returned by gmtime(). As with ctime(), we have no control over the format of the string produced
by asctime().


<to-control-format>
The strftime() function provides us with more precise control when converting a broken-down time
into printable form. Given a broken-down time pointed to by timeptr, strftime() places a
corresponding null-terminated, date-plus-time string in the buffer pointed to by outstr.

The string returned in outstr is formatted according to the specification in format.

#include <time.h>

size_t strftime(char *outstr, size_t maxsize, const char *format, const struct tm *timeptr);

Returns number of bytes placed in outstr (excluding terminating null byte) on success, or 0 on error


<example>
#include <time.h>
#include "curr_time.h" /* Declares function defined here */

#define BUF_SIZE 1000

// Return a string containing the current time formatted according to the specification in 'format'
// (see strftime(3) for specifiers). If 'format' is NULL, we use "%c" as a specifier (which gives
// the date and time as for ctime(3), but without the trailing newline). Returns NULL on error.
//
// %c Date and time                       Tue Feb 1 21:39:46 2011
// %T Time (same as %H:%M:%S)             21:39:46

char * currTime(const char *format)
{
  static char buf[BUF_SIZE]; /* Nonreentrant */
  time_t t;
  size_t s;
  struct tm *tm;
  t = time(NULL);
  tm = localtime(&t);
  if (tm == NULL)
    return NULL;
  s = strftime(buf, BUF_SIZE, (format != NULL) ? format : "%c", tm);
  return (s == 0) ? NULL : buf;
}

fprintf( stdout, "time: %s\n", currTime(NULL));
fprintf( stdout, "time: %s\n", currTime("%T"));

$ ./a.out 
time: Fri May  8 00:55:29 2015
time: 00:55:29


={============================================================================
*kt_linux_core_602* time: resolution, jiffies

10-9  1 nanosecond   ns    one billionth of one second
10-6  1 microsecond  us    one millionth of one second
10-3  1 millisecond  ms    one thousandth of one second

The accuracy of various time-related system calls described in this book is
limited to the resolution of the system software clock, which measures time in
units called jiffies.

The size of a jiffy is defined by the constant HZ within the kernel source code.
This is the unit in which the kernel allocates the CPU to processes under the
roundrobin time-sharing scheduling algorithm.

Because CPU speeds have greatly increased since Linux was first implemented, in
kernel 2.6.0, the rate of the software clock was raised to 1000 hertz on Linux/
x86-32. The advantages of a higher software clock rate are that timers can
operate with greater accuracy and time measurements can be made with greater
precision.

However, it isn't desirable to set the clock rate to arbitrarily high values,
because each clock interrupt consumes a small amount of CPU time, which is time
    that the CPU can't spend executing processes.

Debate among kernel developers eventually resulted in the software clock rate
becoming a configurable kernel option (under Processor type and features, Timer
        frequency). Since kernel 2.6.13, the clock rate can be set to 100, 250
(the default), or 1000 HZ, giving jiffy values of 10, 4, and 1 milliseconds,
respectively.


={============================================================================
*kt_linux_core_603* time: posix clock, realtime

LPI 23.5

POSIX clocks (originally defined in POSIX.1b) provide an API for accessing clocks that measure time
with nanosecond precision. Nanosecond time values are represented using the same timespec structure

The main system calls in the POSIX clocks API are clock_gettime(), which retrieves the current value
of a clock; clock_getres(), which returns the resolution of a clock; and clock_settime(), which
updates a clock.

note:
On Linux, programs using this API must be compiled with the -lrt option, in order to link against
the librt (realtime) library.


{clock-gettime}
The time value is returned in the timespec structure pointed to by tp.

Although the timespec structure affords nanosecond precision, the granularity of the time value
returned by clock_gettime() may be coarser than this. The clock_getres() system call returns a
pointer to a timespec structure containing the resolution of the clock specified in clockid.

#define _POSIX_C_SOURCE 199309
#include <time.h>

int clock_gettime(clockid_t clockid, struct timespec *tp);
int clock_getres(clockid_t clockid, struct timespec *res);

Both return 0 on success, or -1 on error

<clockid>
The clockid_t data type is a type specified by SUSv3 for representing a clock identifier and
supports for POSIX.1b timers, as defined in include/linux/time.h, are:

CLOCK_REALTIME                Settable system-wide real-time clock

The CLOCK_REALTIME clock is a system-wide clock that measures wall-clock time. By contrast with the
CLOCK_MONOTONIC clock, the setting of this clock can be changed.

CLOCK_MONOTONIC               Nonsettable monotonic clock

SUSv3 specifies that the CLOCK_MONOTONIC clock measures time since some “unspecified point in the
past” that doesn’t change after system startup. This clock is useful for applications that must not
be affected by discontinuous changes to the system clock (e.g., a manual change to the system time).
On Linux, this clock measures the time since system startup.

CLOCK_PROCESS_CPUTIME_ID      Per-process CPU-time clock (since Linux 2.6.12)
CLOCK_THREAD_CPUTIME_ID       Per-thread CPU-time clock (since Linux 2.6.12)

The CLOCK_PROCESS_CPUTIME_ID clock measures the user and system CPU time consumed by the calling
process. The CLOCK_THREAD_CPUTIME_ID clock performs the analogous task for an individual thread
within a process.

note:
but only CLOCK_REALTIME is mandatory and widely supported on UNIX implementations.

<linux>
Linux 2.6.28 adds a new clock type, CLOCK_MONOTONIC_RAW, to those listed in Table 23-1. This is a
nonsettable clock that is similar to CLOCK_MONOTONIC, but it gives access to a pure hardware-based
time that is unaffected by NTP adjustments.  This nonstandard clock is intended for use in
specialized clocksynchronization applications.

Linux 2.6.32 adds two more new clocks to those listed in Table 23-1: CLOCK_REALTIME_COARSE and
CLOCK_MONOTIC_COARSE. These clocks are similar to CLOCK_REALTIME and CLOCK_MONOTONIC, but intended
for applications that want to obtain lower-resolution timestamps at minimal cost. These nonstandard
clocks don't cause any access to the hardware clock (which can be expensive for some hardware clock
        sources), and the resolution of the returned value is the jiffy (Section 10.6).


={============================================================================
*kt_linux_core_604* time: ms timestamp

If clock has 1024 HZ resolution, then

1/1024 = 0.000.9765625 (the result is 976562.5 nanoseconds.)
1/1000 = 0.001.
1/2048 = 0.000.48828125

The unit is wider or shorter depending on a resolution.

struct timespec {
  time_t   tv_sec;        /* seconds */
  long     tv_nsec;       /* 'nano'seconds */
};

The CLOCK_REALTIME clock measures the amount of time that has elapsed since epoch. note: since uses
time_t. The tv_nsec field specifies a nanoseconds value. It must be a number in the range 0 to
999,999,999.


To get nanos in total

(tv_sec * 10+9) + tv_nsec nanoseconds

and the following gets time in ms.

1 ms = 1 sec * 10+3        // multiply
1 ms = 1 nsec * 10-6       // devide

So this is to get time in ms and us.

static uint32_t get_time_ms()
{
    struct timespec ts = {0, 0};
    clock_gettime(CLOCK_REALTIME, &ts);
    return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000));
}

static uint32_t time_get_us()
{
    struct	 timespec ts;
    xclock_gettime(CLOCK_REALTIME, &ts);
    return (uint32_t)((ts.tv_sec * 1000000) + (ts.tv_nsec / 10000));
}


<timpstamp-in-ms-example>
1. Since start from 0, this makes timestamp in HH:MM:SS:MS since started but not shows days.
2. Do math as below. So if wants to have timestamps with years and days then use other way.

hour = (thi)/3600000; 				# hour in ms
msec = (thi - (hour*3600000));	# ms remains 
minute = msec / 60000; 				# mins 
msec = msec - (minute * 60000);	# ms remains
sec = msec / 1000; 					# secs
msec = msec - (sec * 1000); 		# ms remains

/*
 * time in MIPS
 */
#include <stdio.h>
#include <unistd.h>
#include <linux/unistd.h>
#include <errno.h> 
#include <stdlib.h>
#include <string.h>
#include <time.h>

typedef unsigned int uint32_t;

// note: starts from 0
static uint32_t tstart = 0;

#if __mips__ /* optimization for mips */
static inline void xclock_gettime(unsigned int which_clock, struct timeval * tv);

#define _syscall_clock_gettimeX(type,name,atype,a,btype,b) \
type x##name(atype a, btype b) \
{ \
    register unsigned long __a0 asm("$4") = (unsigned long) a; \
    register unsigned long __a1 asm("$5") = (unsigned long) b; \
    register unsigned long __a3 asm("$7"); \
    unsigned long __v0; \
    \
    __asm__ volatile ( \
            ".set\tnoreorder\n\t" \
            "li\t$2, %4\t\t\t# " #name "\n\t" \
            "syscall\n\t" \
            "move\t%0, $2\n\t" \
            ".set\treorder" \
            : "=&r" (__v0), "=r" (__a3) \
            : "r" (__a0), "r" (__a1), "i" (__NR_##name) \
            : "$2", "$8", "$9", "$10", "$11", "$12", "$13", "$14", "$15", "$24", \
            "memory"); \
}

_syscall_clock_gettimeX(void, clock_gettime, unsigned int, which_clock, struct timeval *, tv);

#endif

static uint32_t time_get(void)
{
    struct timespec ts;
#if __mips__
    xclock_gettime(CLOCK_REALTIME, &ts);
    printf("call xclock_getttime: ts.tv_sec = %ld, ts.tv_nsec = %ld\n", ts.tv_sec, ts.tv_nsec);
#else
    clock_gettime(CLOCK_REALTIME, &ts);
#endif
    return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000)) - tstart;
}

void main(int argc, char* agrv[]) 
{
    int hour = 0, minute = 0, sec = 0, msec = 0;
    uint32_t tlo =0, thi = 0;

    tstart = time_get();

    thi = time_get();
    hour = (thi)/3600000;
    msec = (thi - (hour*3600000));
    minute = msec / 60000;
    msec = msec - (minute * 60000);
    sec = msec / 1000;
    msec = msec - (sec * 1000);
    printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

    sleep(2);

    thi = time_get();
    hour = (thi)/3600000;
    msec = (thi - (hour*3600000));
    minute = msec / 60000;
    msec = msec - (minute * 60000);
    sec = msec / 1000;
    msec = msec - (sec * 1000);
    printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

    return;
}

$ ./a.out 
call xclock_getttime: ts.tv_sec = 946693753, ts.tv_nsec = 932727000
call xclock_getttime: ts.tv_sec = 946693753, ts.tv_nsec = 932829000
thi:   0, time: 000:00:00.000

call xclock_getttime: ts.tv_sec = 946693755, ts.tv_nsec = 933026000
thi:2001, time: 000:00:02.001


#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <time.h>

typedef unsigned int uint32_t;

static uint32_t tstart = 0;

static time_get(void)
{
    struct timespec ts;
    clock_gettime(CLOCK_REALTIME, &ts);
    return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000)) - tstart;
}

static char* time_stamp(void)
{
    uint32_t tdiff = time_get();

    static char buf[100];
    int hour = 0, minute = 0, sec = 0, msec = 0;
    hour = (tdiff)/3600000;
    msec = (tdiff - (hour*3600000));
    minute = msec / 60000;
    msec = msec - (minute * 60000);
    sec = msec / 1000;
    msec = msec - (sec * 1000);
    sprintf( buf, "%.2d:%.2d:%.2d.%.3d", hour, minute, sec, msec);

    return buf;
}

int main(int argc, char** argv)
{
    tstart = time_get();

    fprintf( stdout, "time: %s\n", time_stamp() );

    int i, j;
    for(i = 0; i < 1000000000; i++)
    {
        j = 30;
        j = j++ % 10;
    }

    fprintf( stdout, "time: %s\n", time_stamp() );

    exit(EXIT_SUCCESS);
}


$ ./a.out 
time: 00:00:00.000
time: 00:00:04.648


={============================================================================
*kt_linux_core_605* time: us timestamp

#ifndef PERFCOUNTER_H
#define PERFCOUNTER_H

#include <string>

class PerfCounter
{
public:
    PerfCounter();
    ~PerfCounter();

    void snap(const char* name);

    std::string dump();

private:
    class DataRec;

    DataRec* head;
    DataRec* lastRec;
};

#endif


#include "PerfCounter.h"

#include <time.h>
#include <sstream>

#include <cstdint>

class PerfCounter::DataRec
{
public:
    DataRec() : next(NULL) {}

    struct timespec ts;
    const char* name;

    DataRec* next;
};

PerfCounter::PerfCounter()
{
    head = new DataRec;
    lastRec = head;
    head->name = "Start";
    clock_gettime(CLOCK_MONOTONIC,&(head->ts));
}

PerfCounter::~PerfCounter()
{
    //delete our records
    DataRec* toDel = head;
    while(toDel)
    {
        head = toDel->next;
        delete toDel;
        toDel = head;
    }
}

void PerfCounter::snap(const char* name)
{
    lastRec->next = new DataRec;
    lastRec = lastRec->next;
    lastRec->name = name;
    clock_gettime(CLOCK_MONOTONIC,&(lastRec->ts));
}

std::string PerfCounter::dump()
{
    snap("end");
    std::stringstream ss;

    DataRec* current = head;

    while(current && current->next)
    {
        //first calculate the number of us difference in the seconds part
        int64_t usecDiff = (current->next->ts.tv_sec - current->ts.tv_sec)*1000000;
        //now the nsec part
        usecDiff += (current->next->ts.tv_nsec - current->ts.tv_nsec)/1000;
        ss << current->name << " -> " << current->next->name << " took " << usecDiff << "us" << std::endl;
        current = current->next;
    }
    return ss.str();
}


#include "PerfCounter.h"

#include <iostream>
#include <sstream>
#include <boost/lexical_cast.hpp>

int main(int argc,char** argv)
{
    PerfCounter counter;
    
    for(int i=0;i<10000;++i)
    {
        int out;
        sscanf("42","%d",&out);
    }
    counter.snap("scanf int");

    for(int i=0;i<10000;++i)
    {
        int out;
        std::stringstream ss("42");
        ss >> out;
    }
    counter.snap("stringstream int");

    for(int i=0;i<10000;++i)
    {
        int out = boost::lexical_cast<int>("42");
    }
    counter.snap("boost::lexical_cast<int>");
    std::cout << counter.dump() << std::endl;
}


={============================================================================
*kt_linux_core_606* time: sleep

{usleep}

usleep - suspend execution for microsecond intervals

#include <unistd.h>

int usleep(useconds_t usec);

note: copied from other article.

Also consider this code: 

usleep(1000);    // sleep 1 microsecond

On the face of it, this line of code makes a thread sleep for 1 microsecond
and then continue. In reality, 1 microsecond is just a lower bound to the
duration of the call.

The man page for usleep() says, "The usleep() function suspends execution of
the calling process for (at least) usec microseconds. The sleep may be
lengthened slightly by any system activity or by the time spent processing the
call or by the granularity of system timers," or if you use the nanosleep()
function. "Therefore, nanosleep() always pauses for at least the specified
time; however, it can take up to 10 ms longer than specified until the process
becomes runnable again."

So if the process is not scheduled under a real-time policy, there's no
guarantee when your thread will be running again. I've done some tests and (to
        my surprise) there are situations when code such as: 

cond.timed_wait(lock, x);    // x = e.g. 1 millisecond

will actually wait for more than 1 second.


{msleep}

void msleep(size_t milliseconds)
{
    usleep(milliseconds * 1000);
}


={============================================================================
*kt_linux_core_700* dbus

{dbus}
http://www.freedesktop.org/wiki/Software/dbus/

What is D-Bus?

D-Bus is a message bus system, a simple way for applications to talk to one
another. In addition to interprocess communication, D-Bus helps coordinate
process lifecycle; it makes it simple and reliable to code a "single instance"
application or daemon, and to launch applications and daemons on demand when
their services are needed.

D-Bus is an Inter-Process Communication (IPC) and Remote Procedure Calling (RPC)
    mechanism specifically designed for efficient and easy-to-use communication
    between processes running on the 'same' machine.


{tutorial}
from http://dbus.freedesktop.org/doc/dbus-tutorial.html

D-Bus is a system for interprocess communication (IPC). Architecturally, it has
several layers:

<libdbus> low-level binding?
A library, 'libdbus', that allows two applications to connect to each other and
exchange messages.

libdbus only supports one-to-one connections, just like a raw network socket.
However, rather than sending byte streams over the connection, you send
'messages'. Messages have a header identifying the kind of message, and a body
containing a data payload. libdbus also abstracts the exact transport used
(sockets vs. whatever else), and handles details such as authentication.

<bus-deamon>
A 'message' bus daemon executable, built on libdbus, that multiple applications
can connect to. The daemon can route messages from one application to zero or
more other applications.

The message bus daemon forms the hub of a wheel. Each spoke of the wheel is a
one-to-one connection to an application using libdbus. 

An application sends a message to the bus daemon over its spoke, and the bus
daemon forwards the message to other connected applications as appropriate.
Think of the daemon as a 'router'. 

<binding> high-level binding
'wrapper' libraries or 'bindings' based on particular application frameworks.
For example libdbus-glib and libdbus-qt. There are also bindings to languages
such as Python. These wrapper libraries are the API most people should use, as
they simplify the details of D-Bus programming.  libdbus is intended to be a
low-level backend for the higher level bindings. Much of the libdbus API is only
useful for binding implementation. 

<session-and-system>
D-Bus applications. D-Bus is designed for 'two' specific 'cases':

The bus daemon has 'multiple' instances on a typical computer. The first
instance is a machine-global singleton, that is, a system daemon similar to
sendmail or Apache. This instance has heavy security restrictions on what
messages it will accept, and is used for systemwide communication. The other
instances are created one per user login session. These instances allow
applications in the user's session to communicate with one another.

The systemwide and per-user daemons are separate. Normal within-session IPC does
not involve the systemwide message bus process and vice versa. 

1. session-bus. Communication between desktop applications in the 'same' desktop
'session'; to allow integration of the desktop session as a whole, and address
issues of process lifecycle when do desktop components start and stop running.

2. system-bus. Communication between the desktop session and the operating
system, where the operating system would typically include the kernel and any
system daemons or processes. 

For the within-desktop-session use case, the GNOME and KDE desktops have
significant previous experience with different IPC solutions such as CORBA and
DCOP. D-Bus is built 'on' that experience and carefully tailored to meet the
needs of these desktop projects in particular. D-Bus may or may not be
appropriate for other applications; the FAQ has some comparisons to 'other' IPC
systems.


{concept}

bus daemon process

 DbusConnection      ->          message dispatcher         <-        DbusConnection
 Instance            <-          if(message is signal)      ->        Instance
   |                               broadcast
   |                             else
   |                               find destination
   |                               named by message
   |
   |                             (destination table)
   |                 <-          Connection 1
   |                             Connection 2               ->
   |                             "The Window Manager"
   | bidirectional               ...
   | message system
   |
application process 1                                                 application process 2

 DbusConnection
 Instance

 incoming                   outgoing call
 | locate object via       /|\
 | object path              | marshal method call
 |                          | to message
 | bindings marshal         |
 | to method call           |
\|/                         |

 C/C++ object              Bindings proxy
 instance                  instance

note: incoming from address to method

<address>
Applications using D-Bus are either servers or clients. A server listens for
incoming connections; a client connects to a server. Once the connection is
established, it is a symmetric flow of messages; the client-server distinction
only matters when setting up the connection.

If you're using the bus daemon, as you probably are, your application will be a
client of the bus daemon. That is, the bus daemon listens for connections and
your application initiates a connection to the bus daemon.

A D-Bus address specifies where a server will listen, and where a client will
connect. For example the address unix:path=/tmp/abcdef specifies that the server
will listen on a UNIX domain socket at the path /tmp/abcdef and the client will
connect to that socket. An address can also specify TCP/IP sockets, or any other
'transport' defined in future iterations of the D-Bus specification. 

<name> for application
When each application connects to the bus daemon, the daemon immediately
'assigns' it a 'name' called the unique connection name. A unique name begins
with a ':' (colon) character. These names are never reused during the lifetime
of the bus daemon - that is, you know a given name will always refer to the same
application.

An example of a unique name might be :34-907. The numbers after the colon have
no meaning other than their 'uniqueness'.

When a name is mapped to a particular application's connection, that application
is said to own that name.

Applications may ask to own 'additional' well-known names. For example, you
could write a specification to define a name called com.mycompany.TextEditor.
Your definition could specify that to own this name, an application should have
an object at the path /com/mycompany/TextFileManager supporting the interface
org.freedesktop.FileHandler.

Applications could then send messages to this bus name, object, and interface to
execute method calls.

You could think of the unique names as IP addresses, and the well-known names as
domain names. So com.mycompany.TextEditor might map to something like :34-907
just as mycompany.com maps to something like 192.168.0.5. 

note: can monitor if application is live or not

Names have a 'second' important 'use', other than routing messages. They are
used to track 'lifecycle'. When an application exits (or crashes), its
connection to the message bus will be closed by the operating system kernel. The
message bus then sends out 'notification' messages telling remaining
applications that the application's names have lost their owner. By tracking
these notifications, your application can reliably monitor the lifetime of other
applications. 


<object-path>
Your programming framework probably defines what an "object" is like; usually
with a base class. For example: java.lang.Object, GObject, QObject, python's
base Object, or whatever. Let's call this a native object. 

The low-level D-Bus protocol, and corresponding libdbus API, does not care about
native objects.

However, it provides a concept called an object path. The idea of an object path
is that higher-level bindings can 'name' native object instances, and allow
remote applications to refer to them. 

The object path looks like a filesystem path, for example an object could be
named /org/kde/kspread/sheets/3/cells/4/5. Human-readable paths are nice, but
you are free to create an object named /com/mycompany/c5yo817y0c1y1c5b if it
makes sense for your application.

Namespacing object paths is smart, by starting them with the components of a
domain name you own (e.g. /org/kde). This keeps different code modules in the
same process from stepping on one another's toes. 


<interface>
Each object supports one or more interfaces. Think of an interface as a named
group of methods and signals, just as it is in GLib or Qt or Java. Interfaces
define the type of an object instance.

DBus identifies interfaces with a simple 'namespaced' string, something like
org.freedesktop.Introspectable. Most bindings will map these interface names
directly to the appropriate programming language construct, for example to Java
interfaces or C++ pure virtual classes. 


<method-and-signal>
Each object has members; the two 'kinds' of member are methods and signals.
Methods are operations that can be invoked on an object, with optional input
(aka arguments or "in parameters") and output (aka return values or "out
        parameters"). Signals are broadcasts from the object to any interested
observers of the object; signals may contain a data payload.

Both methods and signals are referred to by name, such as "Frobate" or
"OnClicked".

note: outgoing

<proxies>
A proxy object is a convenient 'native' object created to 'represent' a 'remote' object in another
process. The low-level DBus API involves manually creating a method call message, sending it, then
manually receiving and processing the method reply message. Higher-level bindings provide proxies as
an alternative. Proxies look like a normal native object; but when you invoke a method on the proxy
object, the binding converts it into a DBus method call message, waits for the reply message,
    unpacks the return value, and returns it from the native method.

Programming with proxies might look like this:

Proxy proxy = new Proxy(getBusConnection(), "/remote/object/path");
Object returnValue = proxy.MethodName(arg1, arg2);
        

{big-picture} to specify call
Pulling all these concepts together, to specify a particular method call on a particular object
instance, a number of nested components have to be named:

Address -> [Bus Name] -> Path -> Interface -> Method

dbus-send --session --print-reply --type=method_call \
    --dest='Zinc.MediaProxy' \
    /Zinc/Media/MediaRouters/1 \
    Zinc.Media.MediaRouter.setSource \
    string:http://dash.bidi.int.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd int32:0

const char* const MEDIA_ROUTER_FACTORY = "/Zinc/Media/MediaRouterFactory";

dbus.expose(ObjectPath::MEDIA_ROUTER_FACTORY, factoryMediaRouter.createMediaRouterFactory(),
            boost::make_shared<NonInheritingAdaptorFactory<MediaRouter> >
            (
             NS_ZINC_DBUS_BINDING::RefCountedAdaptorFactory<MediaRouter>(*bnm), conn, 
             "/Zinc/Media/MediaRouters/")
            );

dbus-send --session --print-reply --type=method_call \
    --dest='Zinc.Media' \              // bus name
    /Zinc/Media/DefaultMediaRouter  \  // OBJECT_PATH
    Zinc.Media.MediaRouter.stop     \  // INTERFACE.MEMBER [CONTENTS...]

const char* const DEFAULT_MEDIA_ROUTER = "/Zinc/Media/DefaultMediaRouter";

dbus.expose(ObjectPath::DEFAULT_MEDIA_ROUTER, factoryMediaRouterDefault.createDefaultMediaRouter());


{dbus-messages}
There are 4 message 'types':

1. Method call messages ask to invoke a method on an object.  you send a method call message, and
receive either a method return message or an error message in reply. 

2. Method return messages return the results of invoking a method.

3. Error messages return an exception caused by invoking a method.

4. Signal messages are notifications that a given signal has been emitted that
an 'event' has occurred. You could also think of these as "event" messages. 

A method call maps very simply to messages: you send a method call message, and
receive either a method return message or an error message in reply.

Each message has a header, including fields, and a body, including arguments.
You can think of the header as the routing information for the message, and the
body as the payload. Header fields might include the sender bus name,
     destination bus name, method or signal name, and so forth. One of the
     header fields is a type signature describing the values found in the body.
     For example, the letter "i" means "32-bit integer" so the signature "ii"
     means the payload has two 32-bit integers. 


{calling-a-method} Behind the Scenes
A method call in DBus consists of two messages; a method call message sent from
process A to process B, and a matching method reply message sent from process B
to process A. Both the call and the reply messages are routed through the bus
daemon. The caller includes a different serial number in each call message, and
the reply message includes this number to allow the caller to match replies to
calls.

The call message will contain any arguments to the method. The reply message may
indicate an error, or may contain data returned by the method.

A method invocation in DBus happens as follows:

o The language binding may provide a proxy, such that invoking a method on an
in-process object invokes a method on a remote object in another process. If so,
    the application calls a method on the proxy, and the proxy constructs a
    method call message to send to the remote process.

o For more low-level APIs, the application may construct a method call message
itself, without using a proxy.

o In either case, the method call message contains: a bus name belonging to the
remote process; the name of the method; the arguments to the method; an object
path inside the remote process; and optionally the name of the interface that
specifies the method.

o The method call message is sent to the bus daemon.

o The bus daemon looks at the destination bus name. If a process owns that name,
  the bus daemon forwards the method call to that process. Otherwise, the bus
  daemon creates an error message and sends it back as the reply to the method
  call message.

o The receiving process unpacks the method call message. In a simple low-level
API situation, it may immediately run the method and send a method reply message
to the bus daemon. When using a high-level binding API, the binding might
examine the object path, interface, and method name, and convert the method call
message into an invocation of a method on a native object sucn as
java.lang.Object, QObject, etc., then convert the return value from the native
method into a method , reply message.

o The bus daemon receives the method reply message and sends it to the process
that made the method call.

o The process that made the method call looks at the method reply and makes use
of any return values included in the reply. The reply may also indicate that an
error occurred. When using a binding, the method reply message may be converted
into the return value of of a proxy method, or into an exception. 

The bus daemon 'never' reorders messages. That is, if you send two method call
messages to the same recipient, they will be received in the order they were
sent. The recipient is not required to reply to the calls in order, however; for
example, it may process each method call in a separate thread, and return reply
messages in an undefined order depending on when the threads complete. Method
calls have a unique serial number used by the method caller to match reply
messages to call messages. 


{emitting-a-signal} Behind the Scenes
A signal in DBus consists of a single message, sent by one process to any number
of other processes.  That is, a signal is a unidirectional broadcast. The signal
may contain arguments (a data payload), but because it is a broadcast, it never
has a "return value." Contrast this with a method call where the method call
message has a matching method reply message.

The emitter (aka sender) of a signal has no knowledge of the signal recipients.
Recipients register with the bus daemon to receive signals based on "match
rules" - these rules would typically include the sender and the signal name. The
bus daemon sends each signal only to recipients who have expressed interest in
that signal.

A signal in DBus happens as follows:

o A signal message is created and sent to the bus daemon. When using the
low-level API this may be done manually, with certain bindings it may be done
for you by the binding when a native object emits a native signal or event.

o The signal message contains the name of the interface that specifies the
signal; the name of the signal; the bus name of the process sending the signal;
and any arguments

o Any process on the message bus can register "match rules" indicating which
signals it is interested in. The bus has a list of registered match rules.

o The bus daemon examines the signal and determines which processes are
interested in it. It sends the signal message to these processes.

o Each process receiving the signal decides what to do with it; if using a
binding, the binding may choose to emit a native signal on a proxy object. If
using the low-level API, the process may just look at the signal sender and name
and decide what to do based on that. 


={============================================================================
*kt_linux_core_701* dbus introspection

{introspection}
D-Bus objects may support the interface org.freedesktop.DBus.Introspectable. This interface has one
method "Introspect" which takes no arguments and returns an XML string. The XML string describes the
interfaces, methods, and signals of the object. See the D-Bus specification for more details on this
introspection format. 


{signature-strings}
D-Bus uses a string-based type encoding mechanism called Signatures to describe the number and types
of arguments requried by methods and signals. Signatures are used for interface
declaration/documentation, data marshalling, and validity checking. Their string encoding uses a
simple, though expressive, format and a basic understanding of it is required for effective D-Bus
use. The table below lists the fundamental types and their encoding characters.


Character 	Code Data Type
y           8-bit unsigned integer
b           boolean value
n           16-bit signed integer
q           16-bit unsigned integer
i           32-bit signed integer
u           32-bit unsigned integer
x           64-bit signed integer
t           64-bit unsigned integer
d           double-precision floating point (IEEE 754)
s           UTF-8 string (no embedded nul characters)
o           D-Bus Object Path string
g           D-Bus Signature string
a           Array
(           Structure start
)           Structure end
v           Variant type (described below)
{           Dictionary/Map begin
}           Dictionary/Map end
h           Unix file descriptor

<example> xml has member and type defs as well
[17-02-2015 10:57:31.574183] signal sender=:1.0 -> dest=(null destination) serial=96 
path=/org/freedesktop/NetworkManager/Devices/0; interface=org.freedesktop.NetworkManager.Device; 
member=StateChanged

<node name="/" xmlns:tp="http://telepathy.freedesktop.org/wiki/DbusSpec#extensions-v0">
  <interface name="org.freedesktop.NetworkManager.Device">
    ...
    <signal name="StateChanged">
      <arg name="new_state" type="u" tp:type="NM_DEVICE_STATE">
        <tp:docstring>
          The new state of the device.
        </tp:docstring>
      </arg>
      <arg name="old_state" type="u" tp:type="NM_DEVICE_STATE">
        <tp:docstring>
          The previous state of the device.
        </tp:docstring>
      </arg>
      <arg name="reason" type="u" tp:type="NM_DEVICE_STATE_REASON">
        <tp:docstring>
          A reason for the state transition.
        </tp:docstring>
      </arg>
    </signal>

    <tp:enum name="NM_DEVICE_STATE" type="u">
      <tp:enumvalue suffix="UNKNOWN" value="0">
        <tp:docstring>
          The device is in an unknown state.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="UNMANAGED" value="1">
        <tp:docstring>
          The device is not managed by NetworkManager.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="UNAVAILABLE" value="2">
        <tp:docstring>
          The device cannot be used (carrier off, rfkill, etc).
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="DISCONNECTED" value="3">
        <tp:docstring>
          The device is not connected.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="PREPARE" value="4">
        <tp:docstring>
          The device is preparing to connect.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="CONFIG" value="5">
        <tp:docstring>
          The device is being configured.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="NEED_AUTH" value="6">
        <tp:docstring>
          The device is awaiting secrets necessary to continue connection.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="IP_CONFIG" value="7">
        <tp:docstring>
          The IP settings of the device are being requested and configured.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="ACTIVATED" value="8">
        <tp:docstring>
          The device is active.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="FAILED" value="9">
        <tp:docstring>
          The device is in a failure state following an attempt to activate it.
        </tp:docstring>
      </tp:enumvalue>
    </tp:enum>
    ...

<example>

<yv:member type="a{ss}" name="shortTitle">

->

std::map< std::string, std::string > shortTitle;


<introspect>
dbus-send  --session --type=method_call --print-reply --dest='org.freedesktop.DBus' / org.freedesktop.DBus.Introspectable.Introspect

method return sender=org.freedesktop.DBus -> dest=:1.255 reply_serial=2
   string "<!DOCTYPE node PUBLIC "-//freedesktop//DTD D-BUS Object Introspection 1.0//EN"
"http://www.freedesktop.org/standards/dbus/1.0/introspect.dtd">
<node>
  <interface name="org.freedesktop.DBus"> ~
    <method name="Hello">
      <arg direction="out" type="s"/>
    </method>
    ...
    <method name="ListNames"> ~
      <arg direction="out" type="as"/>
    </method>
    ...
  </interface>
  <interface name="org.freedesktop.DBus.Introspectable"> ~
    <method name="Introspect">
      <arg direction="out" type="s"/>
    </method>
  </interface>
</node>
"


={============================================================================
*kt_linux_core_702* dbus: dbus-send tool

dbus-send, distributed with D-Bus, allows you to invoke methods on services from
the command line.

http://dbus.freedesktop.org/doc/dbus-send.1.html

dbus-send — Send a message to a message bus

Synopsis

dbus-send [ --system | --session | --address=ADDRESS ] [--dest=NAME] 
   [ --print-reply [=literal]] [--reply-timeout=MSEC] [--type=TYPE] 
   OBJECT_PATH INTERFACE.MEMBER [CONTENTS...]

DESCRIPTION

The dbus-send command is used to send a message to a D-Bus message bus. See
http://www.freedesktop.org/software/dbus/ for more information about the big
picture.

note: system vs session

There are two well-known message buses: the systemwide message bus (installed on
    many systems as the "messagebus" service) and the per-user-login-session
message bus (started each time a user logs in). 

The --system and --session options direct dbus-send to send messages to the
system or session buses respectively. note: If neither is specified, dbus-send
sends to the session bus.

Nearly all uses of dbus-send must provide the --dest argument which is the name
of a connection on the bus to send the message to. If --dest is omitted, no
destination is set.

note: message is either method or signal.

The object path and the name of the message to send must always be specified.
Following arguments, if any, are the message contents (message arguments). These
are given as type-specified values and may include containers (arrays, dicts,
        and variants) as described below.

<contents>   ::= <item> | <container> [ <item> | <container>...]
<item>       ::= <type>:<value>
<container>  ::= <array> | <dict> | <variant>
<array>      ::= array:<type>:<value>[,<value>...]
<dict>       ::= dict:<type>:<type>:<key>,<value>[,<key>,<value>...]
<variant>    ::= variant:<type>:<value>
<type>       ::= string | int16 | uint 16 | int32 | uint32 | int64 | uint64 
                        | double | byte | boolean | objpath

D-Bus supports more types than these, but dbus-send currently does not. Also,
dbus-send does not permit empty containers or nested containers (e.g. arrays of
        variants).

Here is an example invocation:

  dbus-send --dest=org.freedesktop.ExampleName               \
            /org/freedesktop/sample/object/name              \
            org.freedesktop.ExampleInterface.ExampleMethod   \
            int32:47 string:'hello world' double:65.32       \
            array:string:"1st item","next item","last item"  \
            dict:string:int32:"one",1,"two",2,"three",3      \
            variant:int32:-8                                 \
            objpath:/org/freedesktop/sample/object/name

  dbus-send --session --type=method_call --print-reply --dest=Zinc.MediaProxy2
  /Zinc/Media/MediaRouters/0 Zinc.Media.MediaRouter.getSourceInformation

Note that the interface is separated from a method or signal name by a dot,
though in the actual protocol the interface and the interface member are
    separate fields.  

OPTIONS

The following options are supported:

--dest=NAME
    Specify the name of the connection to receive the message.

--print-reply
    'block' for a reply to the message sent, and print any reply received in a
    human-readable form. It also means the message type (--type=) is
    method_call.

--print-reply=literal
    Block for a reply to the message sent, and print the body of the reply. If
    the reply is an object path or a string, it is printed literally, with no
    punctuation, escape characters etc.

--reply-timeout=MSEC
    Wait for a reply for up to MSEC milliseconds. The default is
    implementation-defined, typically 25 seconds.

--system
    Send to the system message bus.

--session
    Send to the session message bus. (This is the default.)

--address=ADDRESS
    Send to ADDRESS.

--type=TYPE
    Specify method_call or signal (defaults to "signal").


={============================================================================
*kt_linux_core_703* dbus: lsdbus

Get the list of activateable bus names
dbus-send  --session --type=method_call --print-reply --dest='org.freedesktop.DBus' \
/ org.freedesktop.DBus.ListActivatableNames

to find the owners of all dbus connections
dbus-send  --session --type=method_call --print-reply --dest='org.freedesktop.DBus' \
/ org.freedesktop.DBus.ListNames

note: org.freedesktop.Dbus means the dbus itself and ListNmaes to get the list of names on the bus

method return sender=org.freedesktop.DBus -> dest=:1.81 reply_serial=2
   array [
      string "org.freedesktop.DBus"
      string ":1.7"
      string ":1.8"
      string "Zinc.BabySitter"
      string "Zinc.Application"
      string "org.freedesktop.NetworkManager"
      string "Zinc.Crb"
      string "Zinc.OEMSystemTime"
      string ":1.81"
      string "Zinc.ApplicationPackages"
      string ":1.41"
      string "Zinc.UsageCollection"
      string ":1.42"
      string ":1.65"
      string ":1.21"
      string ":1.43"
      string ":1.66"
      string ":1.44"
      string ":1.67"
      string ":1.45"
      string "org.freedesktop.NetworkManagerSystemSettings"
      string ":1.46"
      string ":1.69"
      string "Zinc.Media"
      string ":1.47"
      string ":1.48"
      string "Zinc.OEMSystem"
      string ":1.29"
      string "Zinc.ContentAcquisition"
      string "Zinc.Broker"
      string "org.freedesktop.Avahi"
      string "Zinc.DeviceSoftware"
      string "Zinc.RemoteBooking"
      string "Zinc.LinearSource"
      string "Zinc.DeviceManager"
      string "Zinc.Tuner"
      string ":1.70"
      string ":1.71"
      string "Zinc.OEMSystemManager"
      string ":1.72"
      string ":1.73"
      string ":1.75"
      string "Zinc.MetadataProxy"
      string ":1.53"
      string ":1.76"
      string "Zinc.Reminders"
      string ":1.34"
      string "Zinc.Metadata"
      string "Zinc.System"
      string ":1.0"
      string ":1.58"
      string "Zinc.RemoteDiagnostics"
      string ":1.4"
      string ":1.5"
      string ":1.18"
      string ":1.19"
   ]


# lsdbus
793   :1.0                     /opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
793   org.freedesktop.NetworkManager/opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
793   org.freedesktop.NetworkManagerSystemSettings/opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
831   :1.4                     /opt/zinc/bin/litaniumsystemmanagerd            
831   :1.5                     /opt/zinc/bin/litaniumsystemmanagerd            
831   Zinc.OEMSystemManager    /opt/zinc/bin/litaniumsystemmanagerd            
831   Zinc.System              /opt/zinc/bin/litaniumsystemmanagerd            
...


#cat /opt/zinc/devel/bin/lsdbus
#!/bin/sh -e

# Print a list of all dbus connections and the processes that own them.
# Taken from the wiki page:
# https://wiki.youview.co.uk/display/canvas/How+to+Introspect+DBus+from+the+Command+Line

# The format of the output is:
# pid | bus name | command

function ListBusNames {
    dbus-send --session --type=method_call --print-reply \
              --dest=org.freedesktop.DBus / \
              org.freedesktop.DBus.ListNames
}

function GetProcessID {
    dbus-send --session --type=method_call --print-reply \
              --dest=org.freedesktop.DBus / \
              org.freedesktop.DBus.GetConnectionUnixProcessID string:$1 2>/dev/null \
        | xargs -n1 | tail -1
}

for i in $(ListBusNames | grep string | cut -d'"' -f2)
do
    DCNAME=$i
    DCPID=$(GetProcessID $DCNAME)
    if [ -n "$DCPID" ]
    then
        DCPCMD=$(cat /proc/$DCPID/cmdline 2>/dev/null | xargs -0 echo) && \
               printf "%-6s%-25s%-40s%-8s%s\n" \
                  "$DCPID" "$DCNAME" "$DCPCMD"
    fi
done | sort -n


={============================================================================
*kt_linux_core_704* dbus: dbus-monitor tool

http://dbus.freedesktop.org/doc/dbus-monitor.1.html

Distributed with D-Bus, prints out traffic on the bus. You can filter the output by passing match
rules as arguments. 

dbus-monitor [ --system | --session | --address ADDRESS ] 
  [ --profile | --monitor ] [ watch expressions ]

<session>
There are two well-known message buses: the systemwide message bus; installed on
many systems as the "messagebus" service and the per-user-login-session message
bus; started each time a user logs in.  The --system and --session options
direct dbus-monitor to monitor the system or session buses respectively. 

note:
If neither is specified, dbus-monitor monitors the session bus.

<monitor>
dbus-monitor has two different output modes, the 'classic'-style monitoring mode
and profiling mode.  The profiling format is a compact format with a single line
per message and microsecond-resolution timing information. The --profile and
--monitor options select the profiling and monitoring output format
respectively. 

note: If neither is specified, dbus-monitor uses the monitoring output format.


<profile>
--profile
    Use the profiling output format.

[root@HUMAX /]# /opt/zinc-trunk/oss/bin/dbus-monitor --profile 
  "interface=Zinc.Media.MediaRouter,member=getSourceInformation" "type=method_call" "type=method_return"

sig	1442323921	119827	2	/org/freedesktop/DBus	org.freedesktop.DBus	NameAcquired
mc	1442323921	124985	3	:1.167	/org/freedesktop/DBus	org.freedesktop.DBus	AddMatch
mc	1442323921	128350	4	:1.167	/org/freedesktop/DBus	org.freedesktop.DBus	AddMatch
mc	1442323923	728265	1587	:1.72	/Zinc/Tuner/LinearPlaybackControl	Zinc.Tuner.LinearPlaybackControl	getSourceInformation
mc	1442323923	731820	1424	:1.80	/Zinc/Media/DefaultMediaRouter	Zinc.Media.MediaRouter	getSourceInformation
mr	1442323923	734371	772	1424	:1.80
mr	1442323923	735621	1425	1587	:1.72
mc	1442323927	846109	1588	:1.72	/Zinc/Broker/UnifiedEventRepository	Zinc.Broker.UnifiedEventRepository	getScheduleEvents
mc	1442323927	854912	3206	:1.22	/Zinc/Metadata/EventRepository	Zinc.Metadata.EventRepository	getScheduleEvents
mr	1442323927	868006	1366	3206	:1.22
mr	1442323927	884479	3207	1588	:1.72
mc	1442323927	890274	1589	:1.72	/Zinc/Broker/UnifiedEventRepository	Zinc.Broker.UnifiedEventRepository	getScheduleEvents
mc	1442323927	895714	3208	:1.22	/Zinc/Metadata/EventRepository	Zinc.Metadata.EventRepository	getScheduleEvents
mr	1442323927	901114	1367	3208	:1.22
mr	1442323927	906785	3209	1589	:1.72
mc	1442323927	910330	1590	:1.72	/Zinc/Broker/UnifiedEventRepository	Zinc.Broker.UnifiedEventRepository	getScheduleEvents


<watch-expr>
In order to get dbus-monitor to see the messages you are interested in, you
should specify a set of watch expressions as you would expect to be passed to
the dbus_bus_add_match function.

# display only method calls, returns and errors. No signals at all will be
# displayed but it does have the benefit of you not getting the PositionChange
# signals cluttering your display.

dbus-monitor "type=method_call" "type=method_return" "type=error"

dbus-monitor profile 
  "interface=Zinc.Application.ApplicationManager,member=launchApplication" \ 
  "interface=Zinc.Application.ApplicationManager,member=ApplicationLifecycleEvent"

dbus-monitor > /mnt/hd1/mylogs.log &
dbus-monitor | tee /mnt/hd1/mylogs.log &


# To launch Dbus-Monitor on your STB, and inspect MediaRouter activity, run the
# following command:

dbus-monitor "interface=Zinc.Media.MediaRouter,member=getSourceInformation"
  "type=method_call" "type=method_return"

<log>
[07-07-2015 07:45:04.848504] 
method call sender=:1.246 -> dest=Zinc.Media serial=2
path=/Zinc/Media/DefaultMediaRouter; interface=Zinc.Media.MediaRouter;
member=stop

[07-07-2015 07:45:04.864666] 
method return sender=:1.6 -> dest=:1.246 reply_serial=2

<>

// 1.247 becomes dbussenddaemon

[07-07-2015 07:45:05.164966] 
signal sender=org.freedesktop.DBus -> dest=(null destination) 
  serial=413 path=/org/freedesktop/DBus;
  interface=org.freedesktop.DBus; member=NameOwnerChanged string
  "Zinc.DBusSendDaemon" string "" string ":1.247"

[07-07-2015 07:45:05.165242] 
method call sender=:1.247 -> dest=org.freedesktop.DBus 
  serial=2 path=/org/freedesktop/DBus; 
  interface=org.freedesktop.DBus; member=RequestName
  string "Zinc.DBusSendDaemon"
  uint32 0

// dbussenddaemon fowards a call

[07-07-2015 07:45:07.926145] 
method call sender=:1.249 -> dest=Zinc.DBusSendDaemon serial=2
path=/Zinc/Media/MediaRouterFactory; interface=Zinc.Media.MediaRouterFactory;
  member=createMediaRouter string "Zinc.MediaProxy"

[07-07-2015 07:45:07.935001] 
method call sender=:1.247 -> dest=Zinc.MediaProxy serial=3
path=/Zinc/Media/MediaRouterFactory; interface=Zinc.Media.MediaRouterFactory;
  member=createMediaRouter


// fowards a reply

[07-07-2015 07:45:07.945512] 
method return sender=:1.231 -> dest=:1.247 reply_serial=3 object path
"/Zinc/Media/MediaRouters/1"

[07-07-2015 07:45:07.948415] 
method return sender=:1.247 -> dest=:1.249 reply_serial=2 object path
"/Zinc/Media/MediaRouters/1"


={============================================================================
*kt_linux_core_710* dbus: kdbus

{kdbus}
https://github.com/gregkh/presentation-kdbus
https://github.com/gregkh/kdbus


={============================================================================
*kt_linux_core_800* proc: /proc/mounts

The proc(5) manual page.

from stackoverflow:
The definitive list of mounted filesystems in in /proc/mounts.

If you have any form of containers on your system, /proc/mounts only lists the filesystems that are
in your present container. For example, in a chroot, /proc/mounts lists only the filesystems whose
mount point is within the chroot. (There are ways to escape the chroot, mind.)

There's also a list of mounted filesystems in /etc/mtab. This list is maintained by the mount and
umount commands. That means that if you don't use these commands (which is pretty rare), your action
(mount or unmount) won't be recorded. In practice, it's mostly in a chroot that you'll find
/etc/mtab files that differ wildly from the state of the system (also mounts performed in the chroot
    will be reflected in the chroot's /etc/mtab but not in the main /etc/mtab). 

Actions performed while /etc/mtab is on a read-only filesystem are also not recorded there. The
reason why you'd sometimes want to consult /etc/mtab in preference to or in addition to /proc/mounts
is that because it has access to the mount command line, it's sometimes able to present information
in a way that's easier to understand; for example you see mount options as requested (whereas
    /proc/mounts lists the mount and kernel defaults as well), and bind mounts appear as such in
/etc/mtab.


={============================================================================
*kt_linux_core_801* proc: /proc/PID/status

The parent of any process can be found by looking at the Ppid field provided in the Linux-specific
/proc/PID/status file.

Uid:  1024  1024  1024  1024
Gid:  1025  1025  1025  1025

The credentials of any process can be found by examining the Uid, Gid, and Groups lines provided in
the Linux-specific /proc/PID/status file. The Uid and Gid lines list the identifiers in the order
real, effective, saved set, and file system.

FDSize:	32
Groups:	
VmPeak:	    2292 kB
VmSize:	    2288 kB
VmLck:	       0 kB
VmPin:	       0 kB
VmHWM:	     752 kB
VmRSS:	     752 kB
VmData:	     164 kB
VmStk:	     136 kB
VmExe:	      32 kB
VmLib:	    1904 kB
VmPTE:	      16 kB
VmSwap:	       0 kB
Threads:	1

<signals>
SigQ: 0/3941
SigPnd: 0000000000000000
ShdPnd: 0000000000000000
SigBlk: 0000000000000000
SigIgn: fffffffe57f0d8fc
SigCgt: 00000000280b2603

CapInh:	0000000000000000
CapPrm:	ffffffffffffffff
CapEff:	ffffffffffffffff
CapBnd:	ffffffffffffffff
Cpus_allowed:	1
Cpus_allowed_list:	0
Mems_allowed:	1
Mems_allowed_list:	0
voluntary_ctxt_switches:	310
nonvoluntary_ctxt_switches:	127


={============================================================================
*kt_linux_core_802* proc: /proc/PID/maps

Using the Linux-specific /proc/PID/maps file, we can see the location of the shared memory segments
and shared libraries mapped by a program.

From LPI 48.5.

$ cat /proc/9903/maps
08048000-0804a000 r-xp 00000000 08:05 5526989 /home/mtk/svshm_attach          <1>
0804a000-0804b000 r--p 00001000 08:05 5526989 /home/mtk/svshm_attach
0804b000-0804c000 rw-p 00002000 08:05 5526989 /home/mtk/svshm_attach
b7bed000-b7f0d000 rw-s 00000000 00:09 9666565 /SYSV00000000 (deleted)         <2>
b7f0d000-b7f26000 rw-s 00000000 00:09 9633796 /SYSV00000000 (deleted)
b7f26000-b7f27000 rw-p b7f26000 00:00 0
b7f27000-b8064000 r-xp 00000000 08:06 122031 /lib/libc-2.8.so                 <3>
b8064000-b8066000 r--p 0013d000 08:06 122031 /lib/libc-2.8.so
b8066000-b8067000 rw-p 0013f000 08:06 122031 /lib/libc-2.8.so
b8067000-b806b000 rw-p b8067000 00:00 0
b8082000-b8083000 rw-p b8082000 00:00 0
b8083000-b809e000 r-xp 00000000 08:06 122125 /lib/ld-2.8.so                   <4>
b809e000-b809f000 r--p 0001a000 08:06 122125 /lib/ld-2.8.so
b809f000-b80a0000 rw-p 0001b000 08:06 122125 /lib/ld-2.8.so
bfd8a000-bfda0000 rw-p bffea000 00:00 0 [stack]                               <5>
ffffe000-fffff000 r-xp 00000000 00:00 0 [vdso]                                <6>

<1> Three lines for the main program, shm_attach. These correspond to the text and data segments of
the program. The second of these lines is for a readonly page holding the string constants used by
the program.

<2> Two lines for the attached System V shared memory segments.

<3> Lines corresponding to the segments for two shared libraries. One of these is the standard C
library (libc-version.so). 

<4> The other is the dynamic linker (ld-version.so).

<5> A line labeled [stack]. This corresponds to the process stack.

<columns>
1. A pair of hyphen-separated numbers indicating the virtual address range at which the memory
segment is mapped. The second of these numbers is the address of the next byte after the end of the
segment.

note: From this, can get the size. b7f0d000-b7bed000=0x320000. 3,276,800

2. Protection and flags for this memory segment. The first three letters indicate the protection of
the segment: read (r), write (w), and execute (x). A hyphen (-) in place of any of these letters
indicates that the corresponding protection is disabled. The final letter indicates the mapping flag
for the memory segment; it is either private (p) or shared (s). For an explanation of these flags,
    see the description of the MAP_PRIVATE and MAP_SHARED flags in Section 49.2. 
    
note: A System V shared memory segment is always marked shared.

3. The hexadecimal offset (in bytes) of the segment within the corresponding mapped file. The
meanings of this and the following two columns will become clearer when we describe the mmap()
system call in Chapter 49. 

note: For a System V shared memory segment, the offset is always 0.

4. The device number (major and minor IDs) of the device on which the corresponding mapped file is
located.

5. The i-node number of the mapped file, or, for System V shared memory segments, the 'identifier'
for the segment.

6. The filename or other identifying tag associated with this memory segment. For a System V shared
memory segment, this consists of the string SYSV concatenated with the shmget() key of the segment
(expressed in hexadecimal). In this example, SYSV is followed by zeros because we created the
segments using the key IPC_PRIVATE (which has the value 0). The string (deleted) that appears after
the SYSV field for a System V shared memory segment is an artifact of the implementation of shared
memory segments. Such segments are created as mapped files in an invisible tmpfs file system
(Section 14.10), and then later unlinked. Shared anonymous memory mappings are implemented in the
same manner. (We describe mapped files and shared anonymous memory mappings in Chapter 49.)

<smaps>
Starting with kernel 2.6.14, Linux also provides the /proc/PID/smaps file, which exposes more
information about the memory consumption of each of a process's mappings. For further details, see
the proc(5) manual page.

The snippet from proc man and see man page for more.

/proc/[pid]/smaps (since Linux 2.6.14)

This file shows memory consumption for each of the process's mappings. (The pmap(1) command displays
    similar information, in a form that may be easier for parsing.) For each mapping there is a
series of lines such as the following:

00400000-0048a000 r-xp 00000000 fd:03 960637       /bin/bash
Size:                552 kB
Rss:                 460 kB
Pss:                 100 kB
Shared_Clean:        452 kB
Shared_Dirty:          0 kB
Private_Clean:         8 kB
Private_Dirty:         0 kB
Referenced:          460 kB
Anonymous:             0 kB
AnonHugePages:         0 kB
Swap:                  0 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB
Locked:                0 kB


={============================================================================
*kt_linux_core_803* proc: /proc/PID/exe

The Linux-specific /proc/PID/exe file is a symbolic link containing the absolute pathname of the
executable file being run by the corresponding process.

$ ls -1l /proc/3510/exe
lrwxrwxrwx 1 keitee keitee 0 Feb  2 22:21 /proc/3510/exe -> /home/keitee/bin/vim

note: to check the path name of the process.


={============================================================================
*kt_linux_core_804* proc: /proc/PID/fd. which file does process open?

/proc/[pid]/fd

This is a subdirectory containing one entry for each file which the process has open, named by its
file descriptor, and which is a symbolic link to the actual file. Thus, 0 is standard input, 1
standard output, 2 standard error, etc.

In a multithreaded process, the contents of this directory are not available if the main thread has
already terminated (typically by calling pthread_exit(3)).

root       793  0.7  1.0  25528  4176 ?        Sl   16:04   0:00 /opt/zinc/oss/sbin/NetworkManager 
   --no-daemon --log-level=INFO

# ll /proc/793/fd
fd/     fdinfo/ 

# ll /proc/793/fd/
dr-x------    2 root     root           0 Feb 16 16:06 ./
dr-xr-xr-x    6 root     root           0 Jan  1  2000 ../
lr-x------    1 root     root          64 Feb 16 16:06 0 -> /dev/null
l-wx------    1 root     root          64 Feb 16 16:06 1 -> /var/tmp/NetworkManager.log
lrwx------    1 root     root          64 Feb 16 16:06 10 -> /dev/nexus_astm
lrwx------    1 root     root          64 Feb 16 16:06 11 -> /dev/nexus_display
lrwx------    1 root     root          64 Feb 16 16:06 12 -> /dev/nexus_graphics2d
lrwx------    1 root     root          64 Feb 16 16:06 13 -> /dev/nexus_surface
lrwx------    1 root     root          64 Feb 16 16:06 14 -> /dev/nexus_audio
lrwx------    1 root     root          64 Feb 16 16:06 15 -> /dev/nexus_video_decoder
lrwx------    1 root     root          64 Feb 16 16:06 16 -> /dev/nexus_transport
lrwx------    1 root     root          64 Feb 16 16:06 17 -> /dev/nexus_dma
lrwx------    1 root     root          64 Feb 16 16:06 18 -> /dev/nexus_security
lrwx------    1 root     root          64 Feb 16 16:06 19 -> /dev/nexus_spi
l-wx------    1 root     root          64 Feb 16 16:06 2 -> /var/tmp/NetworkManager.log
lrwx------    1 root     root          64 Feb 16 16:06 20 -> /dev/nexus_frontend
lrwx------    1 root     root          64 Feb 16 16:06 21 -> /dev/nexus_keypad
lrwx------    1 root     root          64 Feb 16 16:06 22 -> /dev/nexus_rfm
lrwx------    1 root     root          64 Feb 16 16:06 23 -> /dev/nexus_uhf_input
lrwx------    1 root     root          64 Feb 16 16:06 24 -> /dev/nexus_input_capture
lrwx------    1 root     root          64 Feb 16 16:06 25 -> /dev/nexus_ir_blaster
lrwx------    1 root     root          64 Feb 16 16:06 26 -> /dev/nexus_ir_input
lrwx------    1 root     root          64 Feb 16 16:06 27 -> /dev/nexus_led
lrwx------    1 root     root          64 Feb 16 16:06 28 -> /dev/nexus_gpio
lrwx------    1 root     root          64 Feb 16 16:06 29 -> /dev/nexus_i2c
lr-x------    1 root     root          64 Feb 16 16:06 3 -> pipe:[548]|
lrwx------    1 root     root          64 Feb 16 16:06 30 -> /dev/nexus_pwm
...


={============================================================================
*kt_linux_core_813* proc: /proc/sys/kernel, system resource limits

Various files under the /proc/sys/kernel directory can be used to view and modify these limits.

On Linux, the ipcs -l command can be used to list the limits on each of the IPC mechanisms. 


={============================================================================
*kt_linux_core_814* proc: /proc/stat

root# cat /proc/stat 
cpu  33343 0 15876 89567 4707 788 3710 0 0
cpu0 15661 0 8726 40043 3559 788 3698 0 0
cpu1 17681 0 7149 49524 1147 0 11 0 0
intr 3547815 0 125304 0 2 128777 0 1798 0 3886 2 7 0 0 71484 0 2 42852 0 1 5264 435754 0 5346 0 0 0 0 0 0 0 0 34415 47045 0 0 0 0 0 0 0 0 0 27337 0 0 0 0 0 7031 0 0 0 0 0 0 0 0 0 16826 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2181 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 65562 107793 0 0 0 0 0 2419144 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
ctxt 4296231
btime 1429628483
processes 1602
procs_running 4
procs_blocked 0
softirq 2964653 827836 1298645 1927 47517 27361 5147 32904 1321 721995

cpu  3357 0 4313 1362393
The amount of time, measured in units of USER_HZ (1/100ths of a second on most architectures, use sysconf(_SC_CLK_TCK) to obtain the right value), 

1. that  the  system spent  in  user  mode, 
2. user mode with low priority (nice), 
3. system mode, and 
4. the idle task, respectively.  

The last value should be USER_HZ times the second entry in the uptime pseudo-file.

In Linux 2.6 this line includes three additional columns: 

5. iowait - time waiting for I/O  to  complete  (since  2.5.41);  
6. irq  -  time  servicing  interrupts  (since 2.6.0-test4); 
7. softirq - time servicing softirqs (since 2.6.0-test4).

Since Linux 2.6.11, there is an eighth column, 
      
8. steal - stolen time, which is the time spent in other operating systems when running in a
    virtualized environment

Since Linux 2.6.24, there is a ninth column, 

9. guest, which is the time spent running a virtual CPU for guest operating systems under the
    control of the Linux kernel.


<example>
/**
 * Abstraction of proc/stats file format to hold the data and to
 *   allow calculation of the CPU utilisation based on it.
 */
struct ProcStats
{
    long user;   // Time spent executing user applications (user mode).
    long nice;   // Time spent executing user applications with low priority (nice).
    long system; // Time spent executing system calls (system mode).
    long idle;   // Idle time.
    long iowait; // Time waiting for I/O operations to complete.
    long irq;    // Time spent servicing interrupts.
    long softirq;// Time spent servicing soft-interrupts.

    ProcStats operator-(const ProcStats& other)
    {
        ProcStats res;
        res.user    = user   - other.user;
        res.nice    = nice   - other.nice;
        res.system  = system - other.system;
        res.idle    = idle   - other.idle;
        res.iowait  = iowait - other.iowait;
        res.irq     = irq    - other.irq;
        res.softirq = softirq- other.softirq;
        return res;
    }

    /**
     * Returns calculated total cpu utilisation in percentage.
     */
    double cpuUtilisation() const
    {
        double cpu_total = user + nice + system + irq + softirq;
        double cpu_idle  = idle + iowait;
        return cpu_total / (cpu_total + cpu_idle) * 100; // in percentage
    }
};

inline std::istream& operator>>(std::istream& in, ProcStats& st)
{
    std::string cpu;
    if (in >> cpu)
    {
        in >> st.user >> st.nice >> st.system >> st.idle;
        in >> st.iowait >> st.irq >> st.softirq;
    }
    return in;
}


/**
 * Objects of this type can read (repeatedly) cpu stats file and compare
 * it against values read previously.
 */
class CpuProcStatsReader
{
public:
    CpuProcStatsReader() :
        stat_filename("/proc/stat"),
        now_first(false)
    {
    }

    /**
     * Reads current value of /proc/stats and returns ProcStats with differences
     * with values read during last call to get().
     */
    ProcStats get()
    {
        std::string cpu;
        std::ifstream f(stat_filename.c_str());
        ProcStats& st = curr();
        f >> st;
        now_first = !now_first;
        return now_first ? stats.second - stats.first : stats.first - stats.second;
    }

    /**
     * Helper function (useful rather for testing) to setup a location of
     *   /proc/stat file.
     */
    void setupProcStatLocation(const std::string& filename)
    {
        stat_filename = filename;
    }

private:
    /**
     * Internal method to get the least recently used stats that is to be used/updated.
     */
    ProcStats& curr()
    {
        return now_first ? stats.first : stats.second;
    }

    std::pair<ProcStats, ProcStats> stats;
    std::string stat_filename;
    bool now_first;
};


={============================================================================
*kt_linux_core_814* proc: /proc/PID/cmdline, get process name

{
  printf( "Nexus resources are owned by PID %d\n", sharedMem->resource_pid);
  char name[512];
  sprintf(name, "cat /proc/%d/cmdline", sharedMem->resource_pid);
  system(name);
}


# ============================================================================
#{ SYSCALL
={============================================================================
*kt_linux_sysc_001* syscall list

What syscalls are valid depends on the OS. On GNU and Unix systems, you can find
the full list of valid syscall names on /usr/include/asm/unistd.h.


={============================================================================
*kt_linux_sysc_001* pause

pause() causes the calling process (or thread) to sleep until a signal is delivered that either
terminates the process or causes the invocation of a signal-catching function.


={============================================================================
*kt_linux_sysc_002* alarm

Since the final for loop of the program loops forever, this program uses alarm() to establish a
timer to deliver SIGALRM. The arrival of an unhandled SIGALRM signal guarantees process termination,
if the process is not other- wise terminated.

unsigned int alarm(unsigned int seconds);

alarm() arranges for a SIGALRM signal to be delivered to the calling process in seconds seconds.

int main(int argc, char *argv[])
{
  /* An unhandled SIGALRM ensures this process will die if nothing else terminates it */
  alarm(60);

  for(;;) {
    /* Wait for signals */
    pause();
    printf("%ld: caught SIGHUP\n", (long) getpid());
  }
}


={============================================================================
*kt_linux_sysc_003* getenv, setenv

<getenv>
#include <stdlib.h>

char *getenv(const char *name);

RETURN VALUE
The getenv() function returns a pointer to the value in the environment, or NULL
if there is no match.

note:
"no match" means when there is no env variable set so when "SAMPLE_ENV=" and it
has null but is set, getenv returns "\0" null string but not null pointer. So
need to use strcmp to check if env variable is set or unset since check on NULL
means that env not defined but not set. 


<setenv>
int setenv(const char *name, const char *value, int overwrite);

The setenv() function adds the variable name to the environment with the value
  value, if name does not already exist. If name does exist in the environment,
  then its value is changed to value 'if' overwrite is 'nonzero'; if overwrite
    is zero, then the value of name is not changed. This function makes copies
    of the strings pointed to by name and value (by contrast with putenv(3)).


# ============================================================================
#{ LINUX SYS ADMIN
={============================================================================
*kt_linux_sysa_001* sys: cannot execute a file in cdrom, fstab

When tried to run a shell script on a cdrom, got "permission denied" even if it has excutable and
run it as a root. The problem was it was mounted as a read-only. The solution is to edit /etc/fstab
to mount it with exec option as:

# /etc/fstab: static file system information.
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
/dev/sr0        /media/cdrom0   udf,iso9660 user,noauto,exec   0       0

the filesystem is mounted with the noexec option, so the execute permission bits on all files are
ignored, and you cannot directly execute any program residing on this filesystem. Note that the
noexec mount option is implied by the user option in /etc/fstab. ... If you use user and want to
have executable files, use user,exec.


# ============================================================================
#{
={============================================================================
*kt_linux_sete_001* ubuntu: virtualbox

{graphic-issue}
When see a problem with NVIDIA, need to install a driver for ubuntu manually:

http://askubuntu.com/questions/141606/how-to-fix-the-system-is-running-in-low-graphics-mode-error

In short, get console and do:

sudo apt-get install nvidia-current

Also, install the guest addition:

Click on Install Guest Additions from the Devices menu and all will be done automatically.


{sharing-between-os}
http://www.virtualbox.org/manual/ch04.html#sharedfolders

# set sharing folder on the host using virtual box menu and reboot
#
select "Shared folders" from the "Devices" menu, or click on the folder icon on the status bar in
the bottom right corner.

# shared folder is
#
/media/sf_myfiles 

# add group permission when see access error when reading shared folder
#
Access to auto-mounted shared folders is only granted to the user group vboxsf, which is created by
the VirtualBox Guest Additions installer. Hence guest users have to be member of that group to have
read/write access or to have read-only access in case the folder is not mapped writable.

sudo usermod -a -G vboxsf {username}


{sharing-using-samba}
<1> Have samba installation, setting, and user. This is simple and general and see as an example.
http://www.sitepoint.com/ubuntu-12-04-lts-precise-pangolin-file-sharing-with-samba/

sudo apt-get install samba samba-common system-config-samba winbind

change winbind setting

For samba server settings: Workgroup. (this is domain) This field should be the same value as that
used by your Windows Workgroupi.e if your WIndows Users are members of the 'Home' workgroup, type
'Home' in this field.  

For samba users: use windows user name.


<2> By default, VB uses Host-only networking and this means host(windows) cannot see guest. To
enable for host to see guest, change to 'bridged network' as shown in:

6.4. Bridged networking
http://www.virtualbox.org/manual/ch06.html

So change it in setting menu of VB and restart VM. Get IP and can access guest from host windows.

note: must have two network adaptors: NAT for internet and bridged for samba between host and guest.
But cannot ssh to other host. seems firewall problem and when back to host only network, ssh works
but not sharing as host only net gives private net ip.


==============================================================================
*kt_linux_sete_002*

==============================================================================
*kt_linux_sete_003*	ubuntu: samba

sudo service smbd start
sudo service smbd stop
sudo service smbd restart


To see what are shared:

$ smbclient -L //106.1.8.6/
Enter keitee.park's password:
Domain=[SERILOCAL] OS=[Unix] Server=[Samba 3.6.3]

        Sharename       Type      Comment
        ---------       ----      -------
        IPC$            IPC       IPC Service (rockford server (Samba, Ubuntu))
        dsk1            Disk      Main Disk
Domain=[SERILOCAL] OS=[Unix] Server=[Samba 3.6.3]

        Server               Comment
        ---------            -------

        Workgroup            Master
        ---------            -------
        WORKGROUP




==============================================================================
*kt_linux_sete_005*	ubuntu: check running services

sudo service --status-all


={============================================================================
*kt_linux_sete_006*	ubuntu: connect from windows remote desktop

sudo apt-get install xrdp

Then run windows remote desktop and connect using ip or hostname. That's it.

<q-and-a>
I use Ubuntu on my desktop. When I am away from my desktop, I would like to
access the session using my Windows 7 laptop. Currently, I am using xrdp to
connect, but it starts up a remote session. Is there any way to just use the
same desktop session? I want to be able to pick up where I left off on the
desktop.

http://askubuntu.com/questions/235905/use-xrdp-to-connect-to-desktop-session

// original when installs

[xrdp1]
name=sesman-Xvnc
lib=libvnc.so
username=ask
password=ask
ip=127.0.0.1
port=-1

// this works as said in the above link but not for dual 

[xrdp1]
name=Active Local Login
lib=libvnc.so
username=
password=ask
ip=127.0.0.1
port=5900


={============================================================================
*kt_linux_sete_007*	ubuntu: cpuinfo

cat /proc/cpuinfo 


={============================================================================
*kt_linux_sete_008* ubuntu: change default application setting

Change a map between file type and default application in:

/usr/share/applications/defaults.list -> /etc/gnome/defaults.list


={============================================================================
*kt_linux_sete_100* gnome: windows

{activate-button}
To access your windows and applications, click the Activities button, or just
move your mouse pointer to the top-left hot corner. You can also press the Super
key on your keyboard. You can see your windows and applications in the overview.
You can also just start typing to search your applications, files, folders and
the web. note: this is window key.


{tile-windows}
Super+Left     " to left
Super+Right    " to right
Super+Up       " to max
Supaer+Down    " back to original size

{lock-screen}
Super-L


{keyboard-shortcuts}
https://help.gnome.org/users/gnome-help/stable/shell-keyboard-shortcuts.html.en

To change key maps:

System Setting -> Keyboard -> Shortcuts


={============================================================================
*kt_linux_sete_101* gnome: workspace

https://help.gnome.org/users/gnome-help/stable/shell-workspaces.html.en

To add a workspace, drag and drop a window from an existing workspace onto the
empty workspace in the workspace selector. This workspace now contains the
window you have dropped, and a new empty workspace will appear below it.

default keymap:

Cult-Alt-Up       move to workspace above
Cult-Alt-Down     move to workspace below


={============================================================================
*kt_linux_sete_101* gnome: change wallpapers

http://fabhax.com/technology/change-wallpapers-in-gnome-3.4/

={============================================================================
*kt_linux_sete_102* win xserver

{win-xserver}
I stumbled across " VcXsrv Windows X Server" - that seems to be the well maintained and if you want
you can compiled yourself (not that you need to).

http://sourceforge.net/projects/vcxsrv/?source=directory

If you chose to use it, you need to run VcXsrc without OpenGL support but it works - see highlighted
text below ...

"C:\Program Files\VcXsrv\vcxsrv.exe" :0 -ac -terminate -lesspointer -multiwindow -clipboard -nowgl

Note: this server is better since it seems to have more fonts and shows gvim properly than xming
which is old xserver. It replace xming and seems to use the rest since use xlanuch. 'One window' do
not works but 'multiple windows' works.

http://alexcooper.co.uk/blog/2013/09/using-vcxsrv-putty-remote-x-windows/


={============================================================================
*kt_linux_sete_103* firefox key shortcuts

Ctrl+D                     Add Bookmark
Ctrl+B or Ctrl+I           Bookmarks

Backspace or Alt+<-        Back
Shft+Backspace or Alt+->   Forward

Ctrl+W or Ctrl+F4          Close Tab
Ctrl+T                     New Tab
Ctrl+Tab or Ctrl+PageDown  Next Tab

F5 or Ctrl+R               Reload


={============================================================================
*kt_linux_sete_104* solarized color for ls and gnome

http://www.webupd8.org/2011/04/solarized-must-have-color-paletter-for.html

{for-ls}
wget --no-check-certificate https://raw.github.com/seebi/dircolors-solarized/master/dircolors.ansi-dark
mv dircolors.ansi-dark .dircolors
eval `dircolors ~/.dircolors`

wget --no-check-certificate https://raw.github.com/seebi/dircolors-solarized/master/dircolors.ansi-light
mv dircolors.ansi-light .dircolors
eval `dircolors ~/.dircolors`


{gonme}
git clone https://github.com/sigurdga/gnome-terminal-colors-solarized.git
cd gnome-terminal-colors-solarized

" can set it to light or dark using the following commands:

./set_dark.sh
./set_light.sh


{guake}
git clone https://github.com/coolwanglu/guake-colors-solarized.git
cd guake-colors-solarized

./set_dark.sh
./set_light.sh


={============================================================================
*kt_linux_sete_105* web server

{nginx}

http://wiki.nginx.org/Main

$ sudo apt-get install nginx

Nginx has become one of the most important web servers over the last couple of
years. There's a reason for that. Instead of using the standard threaded- or
process-oriented architecture, it uses a scalable, event-driven (asynchronous)
  architecture. 

So not only is it incredibly light weight, it's highly scalable and memory usage
is far better suited for limited resource deployments. Nginx also handles simple
load balancing, fault tolerance, auto-indexing, virtual servers (both name- and
    IP-based), mod_rewrite, access control, and much more. Nginx can also serve
as a reverse proxy and an IMAP/POP3 proxy server.

Surprisingly, Nginx powers a few very high-profile sites, such as: Netflix,
  Hulu, Pinterest, Wordpress.com, and AirBnB.

Who is Nginx right for? The nice thing about this particular light weight HTTPD
daemon is that it doesn't perform like a lightweight server. Not only does it
run with minimal resources, it offers plenty of optional modules and addons. You
can find pre-built packages for Linux and BSD for easy installation. So if you
need a powerhouse server, in a lighter weight package, Nginx is the server for
you.

Nginx comes in at a 10 MB installation (versus the Apache 30 MB installation)
  and can give you up to a 35 percent performance increase (versus Apache).

<config>

/etc/nginx/sites-enabled/default 

server {
  #listen   80; ## listen for ipv4; this line is default and implied
  #listen   [::]:80 default_server ipv6only=on; ## listen for ipv6

  root /usr/share/nginx/www;
  index index.html index.htm;
  ...
}


<commands>

Use the following command:

# /etc/init.d/nginx restart
# /etc/init.d/nginx reload

# service nginx restart
# service nginx reload

However, recommend way is as follows. This should work with any Linux
distributions or Unix like operating systems:

# nginx -s reload
# /path/to/full/nginx -s reload


{proxy-pass}
http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass

Take a look at nginx's HttpProxyModule, which is where proxy_pass comes from.
The proxy_pass docs say:

This directive sets the address of the proxied server and the URI to which
location will be mapped.

So when you tell Nginx to proxy_pass, you're saying "Pass this request on to
this proxy URL".


<okay>
This is a configuration for local web server and want want to have is when there
are requests to the local server, it would redirect those to the actual server.

wget http://wll1p04345.dev.youview.co.uk/e/pseudolive/bbb/client_manifest.mpd;

To:

wget http://dash.bidi.int.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd;


location / {
# First attempt to serve request as file, then
# as directory, then fall back to displaying a 404.
# try_files $uri $uri/ /index.html;
# Uncomment to enable naxsi on this location
# include /etc/nginx/naxsi.rules
  proxy_pass http://dash.bidi.int.bbc.co.uk;
}

<not-okay>

location / {
# First attempt to serve request as file, then
# as directory, then fall back to displaying a 404.
try_files $uri $uri/ /index.html;

# Uncomment to enable naxsi on this location
# include /etc/nginx/naxsi.rules
  proxy_pass http://dash.bidi.int.bbc.co.uk;
}


={============================================================================
*kt_linux_sete_105* set: update adobe flash plugin

update-flashplugin-nonfree --install


={============================================================================
*kt_linux_sete_200* which to install?

{pdf-viewer}
Okular


# ============================================================================
#{
={============================================================================
*kt_linux_ref_001* references

{ref-UNP}
Richard Stevens. Unix Network Programming Vol 2 Addision Wesley. 2nd Ed.
http://www.kohala.com/start/unpv22e/unpv22e.html

{ref-LPI}
The Linux Programming Interface: A Linux and UNIX System Programming Handbook 
Michael Kerrisk (Author) 

For sources, http://www.man7.org/tlpi/


-------------------------------------------------------------------------------
Copyright: see |ktkb|  vim:tw=100:ts=3:ft=help:norl:

