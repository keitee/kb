*kt_linux*                                                           tw=100

/^[#=]{ 

aWed 18 Dec 2013 09:47:23 GMT

KT KB. Linux

|kt_linux_bash_000| bash: reference
|kt_linux_bash_001| bash: history and reverse search
|kt_linux_bash_002| bash: variable <substitution> <expansion>
|kt_linux_bash_003| bash: if and test command primaries <string-comparison> <[[>
|kt_linux_bash_004| bash: globbing (wildcard)
|kt_linux_bash_005| bash: quoting <grep>
|kt_linux_bash_006| bash: args <shift-operator> <while>
|kt_linux_bash_007| bash: set var using other commands <dirname> <readlink> <if>
|kt_linux_bash_008| bash: for
|kt_linux_bash_009| bash: select
|kt_linux_bash_010| bash: case
|kt_linux_bash_011| bash: while <getopts>
|kt_linux_bash_012| bash: break and continue
|kt_linux_bash_013| bash: read
|kt_linux_bash_014| bash: function
|kt_linux_bash_015| bash: basename
|kt_linux_bash_016| bash: here document
|kt_linux_bash_017| bash: redirection
|kt_linux_bash_018| bash: ':' command
|kt_linux_bash_019| bash: subshell and multiple command <execution-env>
|kt_linux_bash_020| bash: list options
|kt_linux_bash_021| bash: test exit status
|kt_linux_bash_022| bash: && and ||
|kt_linux_bash_023| bash: debug
|kt_linux_bash_024| bash: builtin: exec
|kt_linux_bash_025| bash: builtin: set
|kt_linux_bash_026| bash: which one when there are script and exe binary in the same dir?
|kt_linux_bash_027| bash: test on file using which command
|kt_linux_bash_028| bash: bash: builtin: programmable completion
|kt_linux_bash_029| bash: variable arithmetic
|kt_linux_bash_030| bash: interactive shell or not

|kt_linux_bash_100| bash: config: colour prompt
|kt_linux_bash_101| bash: config: how to know shell nesting level

|kt_linux_bash_200| bash: code: debug and printf
|kt_linux_bash_201| bash: code: array from 0 and return 1 for success like c
|kt_linux_bash_202| bash: code: recursion test

|kt_linux_bash_300| bash: faq: var# 

|kt_linux_tool_001| md5sum
|kt_linux_tool_002| cut
|kt_linux_tool_003| kill and killall
|kt_linux_tool_004| dmesg
|kt_linux_tool_005| uname, check linux version
|kt_linux_tool_006| cp
|kt_linux_tool_007| mkdir
|kt_linux_tool_008| strings
|kt_linux_tool_009| sort
|kt_linux_tool_010| grep and egrep
|kt_linux_tool_011| find
|kt_linux_tool_012| make a empty file without touch
|kt_linux_tool_013| xargs
|kt_linux_tool_014| ssh and putty {scp}
|kt_linux_tool_015| pidof
|kt_linux_tool_016| screen
|kt_linux_tool_017| ldd
|kt_linux_tool_018| ls
|kt_linux_tool_019| strace
|kt_linux_tool_020| time and date
|kt_linux_tool_021| chmod
|kt_linux_tool_022| mknod
|kt_linux_tool_023| wc
|kt_linux_tool_024| du
|kt_linux_tool_025| ln
|kt_linux_tool_026| rsync
|kt_linux_tool_027| awk
|kt_linux_tool_028| sed
|kt_linux_tool_029| pyserial and grabserial
|kt_linux_tool_030| diff and patch
|kt_linux_tool_031| tar
|kt_linux_tool_032| split
|kt_linux_tool_033| getconf
|kt_linux_tool_034| ps
|kt_linux_tool_035| wget, curl
|kt_linux_tool_036| nc
|kt_linux_tool_037| port checks
|kt_linux_tool_038| minicom

|kt_linux_tool_100| package: apt-xxx to get package
|kt_linux_tool_101| package: pkg-config
|kt_linux_tool_102| package: dpkg and install deb file

|kt_linux_tool_140| cmake
|kt_linux_tool_141| cmake: mix of c and cpp build
|kt_linux_tool_142| cmake: cflags
|kt_linux_tool_143| cmake: includes {message-keyword}
|kt_linux_tool_144| cmake: link group

|kt_linux_tool_150| automake: LDADD vs LDFLAGS

|kt_linux_tool_200| binutil:
|kt_linux_tool_201| binutil: readelf
|kt_linux_tool_202| binutil: nm
|kt_linux_tool_203| binutil: objdump
|kt_linux_tool_204| binutil: addr2line
|kt_linux_tool_205| binutil: ld


#{ GDB
|kt_linux_tool_200| gdb: core dump setting
|kt_linux_tool_201| gdb: core dump analysis {frame-command}
|kt_linux_tool_202| gdb: kernel crash analysis
|kt_linux_tool_203| gdb: .gdbinit
|kt_linux_tool_204| gdb: source and docs
|kt_linux_tool_205| gdb: debugging info, run and quit
|kt_linux_tool_206| gdb: commands
|kt_linux_tool_207| gdb: commands for stepping
|kt_linux_tool_208| gdb: multi thread
|kt_linux_tool_209| gdb: breakpoints
|kt_linux_tool_210| gdb: breakpoints: advanced
|kt_linux_tool_211| gdb: examine running
|kt_linux_tool_212| gdb: examine sources
|kt_linux_tool_213| gdb: symbols and files
|kt_linux_tool_214| gdb: shared library debugging

|kt_linux_tool_250| gdb: remote debugging

|kt_linux_tool_300| gdb: frontend tool: cgdb


|kt_linux_core_001| check shared libraries that process uses 
|kt_linux_core_002| stdin, stdout, and stderr
|kt_linux_core_003| check libc version and path
|kt_linux_core_004| system call errors and errno
|kt_linux_core_005| error handling codes from LPI
|kt_linux_core_006| process credential
|kt_linux_core_007| /dev/null
|kt_linux_core_008| file ownership and permissions

#{ SIGNAL
|kt_linux_core_050| signal
|kt_linux_core_051| signal: example: use signal as synchronization
|kt_linux_core_052| signal: kill: checking for the existence of a process

#{ PROCESS
|kt_linux_core_100| thread vs. process
|kt_linux_core_101| process: creation
|kt_linux_core_102| process: termination
|kt_linux_core_103| process: monitor child process
|kt_linux_core_104| process: zombie
|kt_linux_core_105| process: memory layout
|kt_linux_core_106| process: virtual memory
|kt_linux_core_107| process: group
|kt_linux_core_108| process: deamon
|kt_linux_core_109| process: execution <exec-wrapper-example>

|kt_linux_core_150| pthread {nptl}
|kt_linux_core_102| how to run three threads sequencially
|kt_linux_core_103| priority and schedule

|kt_linux_core_200| file io
|kt_linux_core_201| system resource limits

|kt_linux_core_200| ipc
|kt_linux_core_201| ipc: system v <ipcs-command>
|kt_linux_core_202| ipc: system v: shm
|kt_linux_core_203| ipc: server consideration

|kt_linux_core_250| ipc: posix
|kt_linux_core_251| ipc: posix: shm

|kt_linux_core_101| ipc: pipe and fifo
|kt_linux_core_103| ipc: which one to use {semaphores-versus-pthreads-mutexes}

|kt_linux_core_200| ipc: sync: semaphore 
|kt_linux_core_201| ipc: sync: pthread mutex and cond-var {race-condition} {deadlock-condition} {mutex-ownership} {mutex-types} 
|kt_linux_core_220| sync: read-write lock

|kt_linux_core_230|  sync: file-lock

|kt_linux_core_240|  sync: common problems when use threads {race-condition}

|kt_linux_core_250|  sync: reentrant and thread-safe {thread-specific-data} {thread-local-storage}
|kt_linux_core_260|  sync: atomic operations {lock-free-programming}
|kt_linux_core_261|  sync: ref: locks aren't slow; lock contention is
|kt_linux_core_262|  sync: ref: always use a lightweight mutex {mutex-vs-semaphore}
|kt_linux_core_263|  conc: ref: lock-free code: a false sense of security
|kt_linux_core_264|  conc: ref: the free lunch is over 

|kt_linux_core_290|  ref: concurrency in C++. ch01
|kt_linux_core_291|  ref: concurrency in C++. ch02 {std::thread}
|kt_linux_core_293|  ref: concurrency in C++. ch03

|kt_linux_core_300|  case: own semaphore and mutex class using pthread cond-var {cqueue}
|kt_linux_core_301|  case: use of mutex and thread class
|kt_linux_core_302|  case: analysis of 200 and 201 case
|kt_linux_core_303|  case: msg q between threads


|kt_linux_core_500|  file io
|kt_linux_core_600|  time
|kt_linux_core_601|  time: curTime
|kt_linux_core_602|  time: ms and us

|kt_linux_core_700| dbus
|kt_linux_core_800| proc: /proc/mounts
|kt_linux_core_801| proc: /proc/PID/status
|kt_linux_core_802| proc: /proc/PID/maps
|kt_linux_core_803| proc: /proc/PID/exe
|kt_linux_core_804| proc: /proc/PID/fd
|kt_linux_core_813| proc: /proc/sys/kernel, system resource limits

#{ SYSCALL
|kt_linux_sysc_001| pause
|kt_linux_sysc_002| alarm
|kt_linux_sysc_003| getenv, setenv

#{ SYSADMIN
|kt_linux_sysa_001| sys: cannot execute a file in cdrom, fstab

#{ SYSENV
|kt_linux_sete_001| ubuntu: virtualbox
|kt_linux_sete_002| ubuntu: workspace
|kt_linux_sete_003| ubuntu: samba
|kt_linux_sete_004| ubuntu: nfs
|kt_linux_sete_005| ubuntu: check running services
|kt_linux_sete_006| ubuntu: connect from windows remote desktop
|kt_linux_sete_007| ubuntu: cpuinfo
|kt_linux_sete_008| ubuntu: change default application setting

|kt_linux_sete_100| gnome: windows
|kt_linux_sete_101| gnome: gnome-terminal
|kt_linux_sete_102| win xserver
|kt_linux_sete_103| firefox key shortcuts

|kt_linux_sete_200| which to install?

|kt_linux_refe_001|  references


# ============================================================================
#{
={============================================================================
*kt_linux_bash_000* bash: reference


http://www.gnu.org/software/bash/

Advanced Bash-Scripting Guide
https://wiki.kldp.org/HOWTO/html/Adv-Bash-Scr-HOWTO/
http://www.tldp.org/LDP/abs/html/


={============================================================================
*kt_linux_bash_001* bash: history

{erase-duplicates}
# eliminate the continuous repeated entry across the whole history

export HISTCONTROL=erasedups

<history-per-terminal>
# in .bash_profile
echo $SSH_TTY
export HISTFILE=/home/NDS-UK/parkkt/.bash_history_$(echo $SSH_TTY | cut -f 4 -d'/')
export HISTFILESIZE=10000
export HISTSIZE=10000 

note: better to have TTY number in the prompt as well.


# run multiple commands from the history

fc [-e ename] [-lnr] [first] [last]
fc -s [pat=rep] [cmd]

Fix Command. In  the  first form, a range of commands from first to last is selected
from the history list.  First and last may be specified as a string (to locate  the  last
command  beginning  with  that string)  or  as  a  number (an index into the history list,
where a negative number is used as an offset from the current command number).

This is not quite what you want as it will launch an editor first, but that is probably a
good thing since it gives you a chance to double check that you have the correct commands
and even edit them using all the capabilities of your favorite editor. Once you save you
changes and exit the editor, the commands will be run.

e.g.

$ fc 100 120


{reverse-search-history}
http://www.gnu.org/software/bash/manual/html_node/Commands-For-History.html

<C-r> to search backward starting at the current line and moving 'up' through the history as
necessary. This is an incremental search.

Once you've found the command you have several options:

1. Press enter to run it
2. <C-r> to cycle through other commands that are filterd out with the letters you've typed.
3. <C-g> to quit the search and back to the command line 'empty'-handed
4. Press ESC to take one and edit the command.


<edit-command-line>
To set vi mode for editing command line:

8.3.1 Readline Init File Syntax

There are only a few basic constructs allowed in the Readline init file. Blank lines are ignored.
Lines beginning with a `#' are comments. Lines beginning with a `$' indicate conditional constructs
(see section 8.3.2 Conditional Init Constructs). Other lines denote variable settings and key
bindings.

Variable Settings
You can modify the run-time behavior of Readline by altering the values of variables in Readline
using the set command within the init file. The syntax is simple:

    set variable value

Here, for example, is how to change from the default Emacs-like key binding to use vi line editing
commands:

    set editing-mode vi

To set binding to up/down key to history search:

# ~/.inputrc
"\e[A": history-search-backward
"\e[B": history-search-forward

or equivalently,

# ~/.bashrc
bind '"\e[A": history-search-backward'
bind '"\e[B": history-search-forward'

Normally, Up and Down are bound to the Readline functions previous-history and next-history
respectively. I prefer to bind PgUp/PgDn to these functions, instead of displacing the normal
operation of Up/Down.

# ~/.inputrc
"\e[5~": history-search-backward
"\e[6~": history-search-forward

After you modify ~/.inputrc, restart your shell or use Ctrl+X, Ctrl+R to tell it to re-read
~/.inputrc. By the way, if you're looking for relevant documentation: Bash uses The GNU Readline
Library for the shell prompt and history.


={============================================================================
*kt_linux_bash_002* bash: variable

{global-and-local}
Global variables or 'environment' variables are available in all shells. The env or printenv
commands can be used to display environment variables.

Local variables are only available in the current shell. Using the set built-in command without any
options will display a list of all variables (including environment variables) and functions.


{set-variable}
Putting spaces around the equal sign will cause errors.

VARNAME="value"

A variable created like the ones in the example above is only available to the current shell. It is
a local variable: child processes of the current shell will not be aware of this variable. In order
to pass variables to a subshell, we need to export them using the export built-in command.

export VARNAME="value"

note: parent can export variables to child but not vice versa.

<set-variable-for-child>
In the Bourne shell and its descendants (e.g., bash and the Korn shell), the following syntax can be
used to add values to the environment used to execute a single program, without affecting the parent
shell (and subsequent commands):

$ NAME=value program

This adds a definition to the environment of 'just' the child process executing the named program.
If desired, multiple assignments (delimited by white space) can precede the program name.

<reserved-variables>
PATH        A colon-separated list of directories in which the shell looks for commands.
PS1         The primary prompt string. The default value is "'\s-\v\$ '".
PS2         The secondary prompt string. The default value is "'> '".


<special-variable>
$$          Expands to the process ID of the shell.
$?          The exit status

{constant}
The readonly built-in marks each specified variable as unchangeable. When tried to set, displays
error but execution continues.

readonly CONST=100
echo "my const var is $CONST.."
CONST=200
echo "my const var is $CONST.."

bash: CONST: readonly variable


{array}
<set>
FRUIT[0]=apple
FRUIT[1]=banana
FRUIT[2]=orange

Or,

FRUIT=(apple plum blackberry)

<use>
echo "idx 0 = ${FRUIT[0]}"    # idx 0 = apple. just a string
echo "idx 1 = ${FRUIT[1]}"
echo "idx 2 = ${FRUIT[2]}"

echo "all = ${FRUIT[*]}"      # to reference all
echo "all = ${FRUIT[@]}"      # same as above. note: this is to array'fy' from a long string.

<example>
farm_hosts=(web03 web04 web05 web06 web07)

for i in ${farm_hosts[@]}; do
  su $login -c "scp $httpd_conf_new ${i}:${httpd_conf_path}"
  su $login -c "ssh $i sudo /usr/local/apache/bin/apachectl graceful"
done

<example> for use of array and here document
A cron job that fills an array with the possible candidates, uses date +%W to find the week of the
year, and does a modulo operation to find the correct index. The lucky person gets notified by
e-mail.


#!/bin/bash
# This is get-tester-address.sh
#
# First, we test whether bash supports arrays. (Support for arrays was only added recently.)
#
whotest[0]='test' || (echo 'Failure: arrays not supported in this version of bash.' && exit 2)
#
# Our list of candidates. (Feel free to add or
# remove candidates.)
#
wholist=(
  'Bob Smith <bob@example.com>'
  'Jane L. Williams <jane@example.com>'
  'Eric S. Raymond <esr@example.com>'
  'Larry Wall <wall@example.com>'
  'Linus Torvalds <linus@example.com>'
)
#
# Count the number of possible testers.
# (Loop until we find an empty string.)
#
count=0
while [ "x${wholist[count]}" != "x" ]
do
   count=$(( $count + 1 ))
done
#
# Now we calculate whose turn it is.
#
week=`date '+%W'`             # The week of the year (0..53).
week=${week#0}                # Remove possible leading zero.
let "index = $week % $count"  # week modulo count = the lucky person
email=${wholist[index]}       # Get the lucky person's e-mail address.
echo $email                   # Output the person's e-mail address.

This script is then used in other scripts, such as this one, which uses a here document:

email=`get-tester-address.sh` # Find who to e-mail.
hostname=`hostname` # This machine's name.

# Send e-mail to the right person.

mail $email -s '[Demo Testing]' <<EOF
  The lucky tester this week is: $email
  Reminder: the list of demos is here:
  http://web.example.com:8080/DemoSites
  (This e-mail was generated by $0 on ${hostname}.)
EOF


{unset}
The unset built-in is used to destroy arrays or member variables of an array:


{parameter-expansion}
3.5.3 Shell Parameter Expansion

The '$' character introduces parameter expansion, command substitution, or arithmetic expansion. The
parameter name or symbol to be expanded may be enclosed in 'braces', which are 'optional' but serve to
protect the variable to be expanded from characters immediately following it which could be
interpreted as part of the name. 

${parameter:-word} 
If parameter is null or unset, word is substituted for parameter. The value of parameter does not
change. In other words, if unset, word is used and if set, parameter used.

This use 'default' to make sure that the prompt is always set correctly.
PS1=${HOST:-localhost}"$ " ; export PS1 ;

This from often used in conditional tests and this set 80 when it is not defined before. That is
default. note: why not use {parameter:=word} form?

# [ -z STRING ] True of the length of "STRING" is zero.

[ -z "${COLUMNS:-}" ] && COLUMNS=80

It is a shorter notation for

if [ -z "${COLUMNS:-}" ]; then
   COLUMNS=80
fi

note: In this form, the value of parameter does not change. Here TEST is not defined.

$ echo $TEST
$ echo ${TEST:-test}
test
$ echo $TEST
$


${parameter:=word} 
If parameter is null or unset, parameter is set to the value of word.

$ echo $TEST
$ echo ${TEST:=test}
test
$ echo $TEST
test
$

note: Q? this works as it has default value when parameter is not set, null. what is it?
box_ip=${2-localhost}
box_port=${3-2033}


${parameter:+word}
If parameter is null or unset, nothing is substituted, otherwise the expansion of word is
substituted.


<substring-expansion>

${parameter:offset}
${parameter:offset:length}

The LENGTH parameter defines how many characters to keep, starting from the first character after
the offset point. If LENGTH is omitted, the remainder of the variable content is taken

${VAR:OFFSET:LENGTH}

$ STRING="thisisaverylongname"
$ echo ${STRING:4}
isaverylongname
$ echo ${STRING:6:5}
avery


{expansion-on-arg}
If parameter is @, the result is length positional parameters 'beginning' at offset. A negative
offset is taken relative to one greater than the greatest positional parameter, so an offset of -1
evaluates to the last positional parameter. It is an expansion error if length evaluates to a number
less than zero.

The following examples illustrate substring expansion using positional parameters:

$ set -- 1 2 3 4 5 6 7 8 9 0 a b c d e f g h
$ echo ${@:7}
7 8 9 0 a b c d e f g h
$ echo ${@:7:0}

$ echo ${@:7:2}
7 8

$ echo ${@:7:-2}
bash: -2: substring expression < 0

$ echo ${@: -7:2}
b c

note: this shows all args
$ echo ${@}
1 2 3 4 5 6 7 8 9 0 a b c d e f g h

$ echo ${@:0}
./bash 1 2 3 4 5 6 7 8 9 0 a b c d e f g h

$ echo ${@:0:2}
./bash 1


={============================================================================
*kt_linux_bash_003* bash: if and test command primaries

if TEST-COMMANDS; then CONSEQUENT-COMMANDS; fi

The TEST-COMMAND list is executed, and if its return status is zero, the CONSEQUENT-COMMANDS list is
  executed. The return status is the exit status of the last command executed, or zero if no
  condition tested true.

<primaries-on-conditional>
Primary        Meaning
[ -a FILE ]    True if FILE exists.
[ -x FILE ]    True if FILE exists and is excutable.
[ -f FILE ]    True if FILE exists and is a regular file.

[ -z string ]  True if the length of string is zero.
[ -n string ]  True if the length of string is non-zero.

note: can use on env variable.

if [ -z "${ZB_CFG}" ]; then
   # when not set
else
   # when set
fi


if [ -n "${ZB_FORCE_BRANCH}" ]; then
   # when set
else
   # when not set
fi

<string-comparison>
[ STRING1 == STRING2 ] 
True if the strings are equal. note: "=" may be used instead of "==" for strict POSIX compliance.

[ STRING1 != STRING2 ] True if the strings are not equal.

Regular expressions may also be used in comparisons:

gender="female"
if [[ "$gender" == f* ]]


{test-expr}
<single>
test
[

    test expr

Evaluate a conditional expression expr and return a status of 0 (true) or 1 (false). Each operator
and operand must be a separate argument. Expressions are composed of the primaries described below
in Bash Conditional Expressions. test does not accept any options, nor does it accept and ignore an
argument of -- as signifying the end of options.

When the [ form is used, the last argument to the command must be a ].

Expressions may be combined using the following operators, listed in decreasing order of precedence.
The evaluation depends on the number of arguments; see below. Operator precedence is used when there
are five or more arguments.

! expr 
True if expr is false.

( expr )
Returns the value of expr. This may be used to override the normal precedence of operators.

expr1 -a expr2
True if both expr1 and expr2 are true.

expr1 -o expr2
True if 'either' expr1 or expr2 is true. 

The test and [ builtins evaluate conditional expressions using a set of rules based on the number of
arguments. 

<double>
http://www.gnu.org/software/bash/manual/bash.html#Conditional-Constructs

note: Use [] whenever you want your script to be 'portable' across shells. Use [[]] if you want
conditional expressions not supported by [] and don't need to be portable.

[[ is bash's improvement to the [ command. It has several enhancements that make it a better choice
if you write scripts that target bash. My favorites are:

1. It is a syntactical feature of the shell, so it has some special behavior that [ doesn't have.
You no longer have to quote variables like mad because [[ handles empty strings and strings with
whitespace more intuitively. For example, with [ you have to write

    if [ -f "$FILE" ]

to correctly handle empty strings or file names with spaces in them. With [[ the quotes are
unnecessary:

    if [[ -f $FILE ]]

2. Because it is a syntactical feature, it lets you use && and || operators for boolean tests and < and
> for string comparisons. [ cannot do this because it is a regular command and &&, ||, <, and > are
not passed to regular commands as command-line arguments.

3. It has a wonderful =~ operator for doing regular expression matches. With [ you might write

    if [ "$ANSWER" = y -o "$ANSWER" = yes ]

    With [[ you can write this as

    if [[ $ANSWER =~ ^y(es)?$ ]]

It even lets you access the captured groups which it stores in BASH_REMATCH. For instance,
   ${BASH_REMATCH[1]} would be "es" if you typed a full "yes" above.

4. You get pattern matching aka globbing for free. Maybe you're less strict about how to type yes.
Maybe you're okay if the user types y-anything. Got you covered:

    if [[ $ANSWER = y* ]]

Keep in mind that it is a bash extension, so if you are writing sh-compatible scripts then you need
to stick with [. Make sure you have the #!/bin/bash shebang line for your script if you use double
brackets.


={============================================================================
*kt_linux_bash_004* bash: globbing (wildcard)

*              Matches zero or more occurrences of any character
?              Matches one occurrence of any character
[characters]   Matches one occurrence of any of the given characters
[!characters]  Negative match. e.g. $ ls [!a]* 


={============================================================================
*kt_linux_bash_005* bash: quoting

<backslash>
$ echo You owe \$1250
You owe $1250

<single>
Quote all inside the single quote.

$ echo '<-$1250.**>; (update?) [y|n]'
<-$1250.**>; (update?) [y|n]

<double> <use>
Do not quote all (see below) so that can use 'variables' inside the quoted string.

$ for parameter substitution.

All other \ characters are literal (not special).

`  for command substitution.
\$ to enable literal dollar signs.
\Â´ to enable literal backquotes.
\" to enable embedded double quotes.
\\ to enable embedded backslashes.

$ echo '$USER owes <-$1250.**>; [ as of (`date +%m/%d`) ]'
$USER owes <-$1250.**>; [ as of (`date +%m/%d`) ]

$ echo "$USER owes <-$1250.**>; [ as of (`date +%m/%d`) ]"
parkkt owes <-250.**>; [ as of (01/26) ]

<quote-parms-to-other-command> <use>
To prevent globbing by shell before passing params to grep command.

$ grep '[0-9][0-9]*$' report2 report7


={============================================================================
*kt_linux_bash_006* bash: args

The args are special 'predefined' variables. 

<$0>
This variable is commonly used to determine the behavior of scripts that can be invoked with more
than one name.

$ ln -s mytar listtar
$ ln -s mytar maketar

$ listtar or maketar

#!/bin/sh
case $0 in
   *listtar) TARGS="-tvf $1" ;;
   *maketar) TARGS="-cvf $1.tar $1" ;;
esac
tar $TARGS

<not-set-when-not-given>
If not given in the command line, not set.

#!/bin/bash
echo $1 $2 $3 ' -> echo $1 $2 $3'

$./runsh.sh 1 2
1 2

<$@> <$*>
args=("$@");   # to get 'all' args and set it to array.
echo ${args[0]} ${args[1]} ${args[2]}

echo $@        # to print all args in a one go

<$#>
the number of args passed in the command line. exclued 0th.


{shift-operator}
See that use S1 all the way and $# decreases

#!/bin/bash
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"


$ ./sbash.sh 1 2 3 4 5 6 
this is 1 1, num 6
this is 1 2, num 5
this is 1 3, num 4
this is 1 4, num 3
this is 1 5, num 2

So can use 'loop' until there is no more args to process:

while [ "$1" != "" ]; do
   case $1 in
      -h | -help | --help)
         echo "help.."
         exit
         ;;
      release)
         echo "release..."
         exit
         ;;
      *)
         echo "else..."
         exit
         ;;
   esac
   shift
done


={============================================================================
*kt_linux_bash_007* bash: set var using other commands

Two ways to set var by running other commands.

<one>
DIR=$(dirname $0)

<two>
if [[ "$dir" != /* ]]
then
   echo "if"
   CURDIR=`pwd`                       # back quote
   DIR=$(readlink -f $CURDIR/$DIR)
   echo "DIR:$DIR"
fi

if [[ "$dir" != /* ]]; then         # /* ?
   echo "if"
   CURDIR=`pwd`                       # back quote
   DIR=$(readlink -f $CURDIR/$DIR)
   echo "DIR:$DIR"
fi

note: here dirname and readlink are commands.

dirname - strip last component from file name. eg. /usr/bin/ -> /usr
readlink - print value of a symbolic link or canonical file name


={============================================================================
*kt_linux_bash_008* bash: for

<form>
for name in word1 word2 ... wordN
do
   list
done

<example> in command line
for f in $(ls); do echo "var is: $f"; done

note: It is important to remember that the then and fi are considered to be separated statements in
the shell. Therefore, when issued on the command line, they are separated by a semi-colon.

for f in $( ls /var/ ); do
   echo $f
done

note: 'no' need to specify index and to shift.

<example>
for FILE in $HOME/.bash*
do
   cp $FILE ${HOME}/public_html
   chmod a+r ${HOME}/public_html/${FILE}
done


={============================================================================
*kt_linux_bash_009* bash: select

<form>
select name in word1 word2 ... wordN
do
   list
done

<example>
select COM in comp1 comp2 comp3 all none
do
   echo "is in do"
done

$ ./sample.sh  
1) comp1
2) comp2
3) comp3
4) all
5) none
#?

note: Automatically make numbers and prompts. COM var will have whatever value entered. Also this
example do infinite loop until press C-c since no break statement in do.

<example>
select COM in comp1 comp2 comp3 all none
do
  case $COM in
      comp1 | comp2 | comp3) echo "sel is $COM" ;;
      all) echo "sel is $COM" ;;
      none)
          echo "sel is $COM"
          break
           ;;
       *) echo "ERROR. sel is $REPLY" ;;
   esac
done

You can change the prompt displayed by the select loop by altering the variable PS3. If PS3 is not
set, the default prompt, #?, is displayed. Otherwise the value of PS3 is used as the prompt to
display. For example, the commands

$ PS3="Please make a selection => " ; export PS3

note: All loops has 'do .. done' block and careful the place of 'do'.


={============================================================================
*kt_linux_bash_010* bash: case

Can use either in the loop or it alone.

<form>
case $variable in
 $condition1) or pattern1)
 command...
 ;;

 $condition2 )
 command...
 ;;
esac


<example>
case $PLATFORM in 
  sam_bcm )
    PLATFORM_DIR=SAM_BCM_MIPS4K_LNUX_DARWIN_03
    FUSIONOS=FUSIONOS_3
    ENDIAN_TYPE=BigEndian
    ;;    
  pace_bcm )
    PLATFORM_DIR=PACE_BCM_MIPS4K_LNUX_DARWIN_02
    FUSIONOS=FUSIONOS_3
    ENDIAN_TYPE=BigEndian
    ;;
  ams_bcm )
    PLATFORM_DIR=AMS_BCM_MIPS4K_LNUX_DARWIN_01
    FUSIONOS=FUSIONOS_2
    ENDIAN_TYPE=BigEndian
    ;;
  tmm_bcm )
    PLATFORM_DIR=TMM_BCM_MIPS4K_LNUX_DARWIN_02
    FUSIONOS=FUSIONOS_3
    ENDIAN_TYPE=BigEndian
    ;;
  * )
    echo "USAGE ERROR"
    usage
    exit
esac


<example> note the use of pattern
case "$TERM" in
   *term)
   TERM=xterm ;;
   network | dialup | unknown | vt[0-9][0-9][0-9])
   TERM=vt100 ;;
esac


<example> can get an input from a user
read case;
case $case in
    1) echo "You selected bash";;
    2) echo "You selected perl";;
    3) echo "You selected phyton";;
    4) echo "You selected c++";;
    5) exit
esac


={============================================================================
*kt_linux_bash_011* bash: while

<form>
while command
do
   list
done

<example>

note: can use true only in the script but not command line.

while [ true ] ; do ls -l; echo Wait 1 sec ; sleep 1; done

while [ "$1" != "" ]; do
  case $1 in
    -h | -help | --help)
      usage
      exit
      ;;
    -p | --platform | -b | --box)
      shift
      PLATFORM=$1
      ;;
    --project)
      shift
      PROJECT=$1
      ;;
    release)
      RELEASE="true"
      ;;
    *)
      echo "USAGE ERROR"
      usage
      exit
      ;;
  esac
  shift
done


{getopts} shell built-in

getopts optstring name [args]

getopts is used by shell scripts to parse positional parameters. 'optstring' contains the option
characters to be recognized; if a character is followed by a colon, the option is expected to have
an argument, which should be separated from it by whitespace. 

<option>
Each time it is invoked, getopts 'places' the next option in the shell variable 'name', initializing
name if it does not exist, and the 'index' of the next argument to be processed into the variable
'OPTIND'. OPTIND is initialized to 1 each time the shell or a shell script is invoked. 

<arg>
When an option requires an argument, getopts places that 'argument' into the variable OPTARG. 

<return>
When the end of options is encountered, getopts exits with a return value greater than zero. note:
false. OPTIND is set to the index of the first 'non'-option argument, and name is set to '?'.


={============================================================================
*kt_linux_bash_012* bash: break and continue


while :     # this cause infinite loop.
do
   read CMD
   case $CMD in
      [qQ]|[qQ][uU][iI][tT]) break ;;
   *) process $CMD ;;
   esac
done

<continue>
If one of the filenames in $FILES is not a file, this loop skips it, rather than exiting.

for FILE in $FILES ;
do
   if [ ! -f "$FILE" ] ; then
      echo "ERROR: $FILE is not a file."
      continue
   fi
   # process the file
done


={============================================================================
*kt_linux_bash_013* bash: read

The read command in bash is a magic builtin. It reads a line from stdin and assigns its contents to
shell variables. It also has a return code when EOF is reached, allowing a clean exit from a loop. 

A common use of input redirection in conjunction with the read command is the reading of a file one
line at a time using the while loop.

<example> really works!
#!/bin/bash
cat list | while read f; do
  echo "line $f"
done


<example> get token from a input line
Read three tokens and echo the second.

$ grep 'sda' DF.LOG | grep 'FAT'
/dev/sda1              63   976751999   488375968   1 FAT12
$ echo "/dev/sda1              63   976751999   488375968   1 FAT12" | while read a b c; do echo $b; done
63

In the script:

A=`grep 'sda' $F | grep 'FAT' | while read a b c;do echo $b;done`


<example>
# what it's trying to do is that search through the same errors in logs which was uploaded in
# Feburary and was under translation. For example,
# translation/Zone9-Box5_Feb_18_09_38_42b3e5c9130e1d758acdc71bb9d12b2a

ls translation/ \
| grep Feb | while read x; do pushd -n translation/$x; pwd; egrep --color -ano 'btreePageFromDbPage' .detailed_output; popd -n; done 

note: WHY?

pushd -n   

Suppresses the normal change of directory when adding directories to the stack, so that only the
stack is manipulated. 


{read-and-sub-process}

<example> This cause the creation of sub process?
while read LINE
do
   ... # manipulate file here
done < file


<example>
#!/bin/sh
if [ -f "$1" ] ; then
  i=0
  while read LINE
  do
    i=`echo "$i + 1" | bc`
    echo "inner $i..."
  done < "$1"
  echo $i
fi

echo $i

$ ./sbash.sh list 
inner 1...
inner 2...
inner 3...
inner 4...
4
4

note: in the original text, it got 0 than 4 since there is a sub-process created. However, when run
it on PC linux, got 4 as shown. Is it difference between PC and embedded shell? Seems not since the
below example seems to be run on build server.


<example>
scan_for()
{
  while read line
  do
      echo "$line"
  done
}

A=`mycat $1 | egrep -n "(VRM_JOB_START:|XTVFS_WriteEx)" | scan_for`


This example cannot get output inside a function. So make a changes to:

scan_for()
{
  cat x.tmp | while read line
  do
      echo "$line"
  done
}

A=`mycat $1 | egrep -n "(VRM_JOB_START:|XTVFS_WriteEx)" > x.tmp`
scan_for()

This enables us to use echo to get output but causes the other problem.

scan_for()
{
  STARTED=0
  MATCHED=0

  # export STARTED MATCHED LASTLINE

  cat x.tmp | while read line
  do
    case $line in
      *jtVRM=3*)
          if [ $STARTED -ne 1 ]
          then
             STARTED=1;
             STARTLINE=`echo $line`;
          fi
          ;;
       *XTVFS_WriteEx*E_FSSERVER_STATUS_INVALID_ALIGNMENT*)
          if [ $STARTED -eq 1 ]
          then
             MATCHED=1;
             LASTLINE=`echo $line`;

             # note: can get output of variables as expected
             echo "started = $STARTED, MATCHED=$MATCHED";

          break;
          fi
          ;;
    esac
  done

  # note: however, the output of variables are '0'
  echo "started = $STARTLINE";
  echo "matched = $LASTLINE";

  if [[ $STARTED -eq 1 && $MATCHED -eq 1 ]]
  then
      echo "$LASTLINE";    # note: here
  fi
}

[parkkt@ukstbuild02 SI-4063]$ ./PicassoShutdownFail job.nds
started = 1, MATCHED=1
started = 0
matched = 0
started = 0
matched = 0

The reason is that "That's a tricky one, because the "while read line do done" spawns a subshell."
So use "echo variable" inside the loop to make it available after the loop. But still not able to
see the output inside the loop.

A=`mycat $1 | egrep -n "(VRM_JOB_START:|XTVFS_WriteEx)" | scan_for`
echo "A = $A";

So A will have all lines which are ecohed from the subshell 


[parkkt@ukstbuild02 SI-4063]$ ./PicassoShutdownFail job.nds
--
A = started = 1, MATCHED=1

started = 2:1801:NDS: ^[08:14:11]0946714451.604390 !MIL -VRMS < p:000000b7 t:2b933100 T:VRM_SRV
M:job_man_srv.c F:JobManSrvStart L:03884 > VRM_JOB_START: jtVRM=3, jhVRM=0x3000385

matched = 290:243541:NDS: ^[09:11:44]0946717904.242499 !FATAL -MSM_MS < p:000000b7 t:0144ce00
T:FS_WT_THREAD1 M:fs_server.c F:PerformWrite L:01943 > XTVFS_WriteEx failed,returned value:
E_FSSERVER_STATUS_INVALID_ALIGNMENT 290:243541:NDS: ^[09:11:44]0946717904.242499 !FATAL -MSM_MS <
p:000000b7 t:0144ce00 T:FS_WT_THREAD1 M:fs_server.c F:PerformWrite L:01943 > XTVFS_WriteEx
failed,returned value: E_FSSERVER_STATUS_INVALID_ALIGNMENT

started = 1, MATCHED=1 started = 2 Time:[08:14:11]0946714451,  VRM_JOB_START: jtVRM=3 Picasso not
shutting down properly - maybe Jira SI-3770

[parkkt@ukstbuild02 SI-4063]$


={============================================================================
*kt_linux_bash_014* bash: function

note: must use bash and see how to pass args to function

#!/bin/bash
# BASH FUNCTIONS CAN BE DECLARED IN ANY ORDER

function function_B {
  echo "{ fb"
  echo Function B.
  echo "} fb"
}

function function_A {
  echo "{ fa"
  echo $1
  echo "} fa"
}

function function_D {
  echo "{ fd"
  echo Function D.
  echo "} fd"
}

# function function_C {
function_C () {
  echo "{ fc"
  echo $1
  echo "} fc"
}

# FUNCTION CALLS

# Pass parameter to function A
function_A "Function A via pass"
function_B

# Pass parameter to function C
function_C "Function C via pass"
function_D 

$ ./sbash.sh list 
{ fa
Function A via pass
} fa
{ fb
Function B.
} fb
{ fc
Function C via pass
} fc
{ fd
Function D.
} fd

{syntax}

function function_C {
  echo "{ fc"
  echo $1
  echo "} fc"
}

function_C () {
  echo "{ fc"
  echo $1
  echo "} fc"
}

note: this causes an error as:

function_C {
  echo "{ fc"
  echo $1
  echo "} fc"
}

./sample.sh: line 23: function_C: command not found
{ fc

} fc
./sample.sh: line 27: syntax error near unexpected token `}'
./sample.sh: line 27: `}'


={============================================================================
*kt_linux_bash_015* bash: basename

$ dirname /home/kt/kb
/home/kt
$ basename /home/kt/kb
kb

# DESCRIPTION
# Print NAME with any leading directory components removed. If 'specified', also remove a trailing
# SUFFIX.

$ basename cfgversion.out  
cfgversion.out
$ basename cfgversion.out .out
cfgversion


={============================================================================
*kt_linux_bash_016* bash: here document

The general form for a here document is

command << delimiter
document
delimiter

Here the shell interprets the << operator as an instruction to read 'input' until it finds a line
containing the specified delimiter. All the input lines up to the line containing the delimiter are
then fed into the standard 'input' of the command.

The delimiter must be a single word that does not contain spaces or tabs. For example, to print a
quick list of URLs, you could use the following here document:

lpr << MYURLS
http://www.csua.berkeley.edu/~ranga/
http://www.cisco.com/
http://www.marathon.org/story/
http://www.gnu.org/
MYURLS

For example, you can use the following command to create a file with the short list of URLs given
previously:

cat > urls << MYURLS
http://www.csua.berkeley.edu/~ranga/
http://www.cisco.com/
http://www.marathon.org/story/
http://www.gnu.org/
MYURLS


={============================================================================
*kt_linux_bash_017* bash: redirection

<form>
command n> file
command n>> file

<example>
One of the most common uses of file descriptors is to redirect STDOUT and STDERR to separate files.
The basic syntax is

command 1> file1 2> file2

Often the STDOUT file descriptor, 1, is not written, so a shorter form of the basic syntax is

command > file1 2> file2

Redirecting STDOUT and STDERR to the 'same' file

list > file 2>&1

n>&m. Here n and m are file descriptors (integers). If you let n=2 and m=1, you see that STDERR is
redirected to STDOUT. By redirecting STDOUT to a file, you also redirect STDERR.

<short>
list &> file

Bash allows for both standard output and standard error to be redirected to the file. This is the
equivalent of > FILE 2>&1. note: cannot use tee in this case.

<example> the all are the same
prog1 > /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log 2>&1
prog1 2>&1 | tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log
prog1 |& tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log


={============================================================================
*kt_linux_bash_018* bash: ':' command

<example> can use as 'no-op'
Can be used as a no-op, which is a command that does nothing and thus can be safely inserted
anywhere a command is needed for purely syntactical reasons:

if [ -x $CMD ]
then :
else
   echo Error: $CMD is not executable >&2
fi

In this example, assume you are not quite ready to write the code to follow the then statement. The
shell flags a syntax error if you leave that code out completely, so you insert the : command as a
temporary noop command that can be replaced by the desired code later.


<example>
Because the : always returns a successful result, it is sometimes used to create an infinite loop:

while :
do
  echo "Enter some input: \c"
  read INPUT
  [ "$INPUT" = stop ] && break
Done

Because the : always returns a successful or true result, the while loop will continue forever or
until a break is executed within the loop. Sometimes you might find that "while true" used in place
of while : but using the : is more efficient because it is a shell built-in command, whereas true is
a command that must be read from a disk file, if you are in the Bourne shell.


={============================================================================
*kt_linux_bash_019* bash: subshell and multiple commands

3.7.3 Command Execution Environment

The shell has an execution environment, which consists of the following:

open files inherited by the shell at invocation, as modified by redirections supplied to the exec builtin

the current working directory as set by cd, pushd, or popd, or inherited by the shell at invocation
the file creation mode mask as set by umask or inherited from the shell's parent

current traps set by trap

shell parameters that are set by variable assignment or with set or inherited from the shell's
parent in the environment

shell functions defined during execution or inherited from the shell's parent in the environment
options enabled at invocation (either by default or with command-line arguments) or by set options
enabled by shopt (see The Shopt Builtin)

shell aliases defined with alias (see Aliases)

various process IDs, including those of background jobs (see Lists), the value of $$, and the value of $PPID 

<separate>
When a simple command other than a builtin or shell function is to be executed, it is invoked in a
'separate' execution environment that consists of the following. Unless otherwise noted, the values
are inherited from the shell.

- the shell's open files, plus any modifications and additions specified by redirections to the command

- the current working directory
- the file creation mode mask

- note: shell variables and functions marked for export, along with variables exported for the
command, passed in the environment (see Environment)

A command invoked in this separate environment cannot affect the shell's execution environment.

Command substitution, commands grouped with parentheses, and asynchronous commands are invoked in a
subshell environment that is a duplicate of the shell environment, except that traps caught by the
shell are reset to the values that the shell inherited from its parent at invocation. 

Builtin commands that are invoked as part of a pipeline are also executed in a 'subshell' environment.
Changes made to the subshell environment cannot affect the shell's execution environment.

Subshells spawned to execute command substitutions inherit the value of the -e option from the
parent shell. When not in POSIX mode, Bash clears the -e option in such subshells.


3.2.2 Pipelines

A pipeline is a sequence of simple commands separated by one of the control operators | or |&.

The format for a pipeline is

[time [-p]] [!] command1 [ | or |& command2 ] ...

The output of each command in the pipeline is connected via a pipe to the input of the next command.
That is, each command reads the previous command's output. This connection is performed before any
redirections specified by the command.

If |& is used, command1's standard error, in addition to its standard output, is connected to
command2's standard input through the pipe; it is shorthand for 2>&1 |. This implicit redirection of
the standard error to the standard output is performed after any redirections specified by the
command.

If the pipeline is 'not' executed asynchronously (see Lists), the shell 'waits' for all commands in
the pipeline to complete.

'each' command in a pipeline is executed in its own subshell (see Command Execution Environment). The
exit status of a pipeline is the exit status of the last command in the pipeline, unless the
pipefail option is enabled (see The Set Builtin). 


3.2.3 Lists of Commands

A list is a sequence of one or more pipelines separated by one of the operators ;, &, &&, or ||, and
optionally terminated by one of ;, &, or a newline. 

Commands separated by a ; are executed 'sequentially'; the shell waits for each command to terminate
in turn. The return status is the exit status of the last command executed. 

If a command is terminated by the control operator &, the shell executes the command asynchronously
in a subshell. This is known as executing the command in the background.


3.2.4.3 Grouping Commands

Bash provides two ways to group a list of commands to be executed as a unit. When commands are
grouped, redirections may be applied to the entire command list. For example, the output of all the
commands in the list may be redirected to a single stream.

()

( list )

Placing a list of commands between parentheses causes a 'subshell' environment to be created (see
    Command Execution Environment), and each of the commands in list to be executed 'in' that
subshell. Since the list is executed in a subshell, variable assignments do not remain in effect
after the subshell completes.

<example>
$ global strlen                 # strlen() is not found
$ (cd /usr/src/lib; gtags)      # library source
$ (cd /usr/src/sys; gtags)      # kernel source


{}

{ list; }

Placing a list of commands between curly braces causes the list to be executed in the 'current'
shell context. No subshell is created. The semicolon (or newline) following list is required. 

In addition to the creation of a subshell, there is a subtle difference between these two constructs
due to historical reasons. The braces are reserved words, so they must be separated from the list by
blanks or other shell metacharacters. The parentheses are operators, and are recognized as separate
tokens by the shell even if they are not separated from the list by whitespace.

The exit status of both of these constructs is the exit status of list. 

note: must be spaces between {}.

$ { date;time; }
$ { date;time; } > mylog


={============================================================================
*kt_linux_bash_020* bash: list options

keitee@debian-keitee:~/github/kb$ set -o
allexport      	off
braceexpand    	on
emacs          	on
errexit        	off
errtrace       	off
functrace      	off
hashall        	on
histexpand     	on
history        	on
ignoreeof      	off
interactive-comments	on
keyword        	off
monitor        	on
noclobber      	off
noexec         	off
noglob         	off
nolog          	off
notify         	off
nounset        	off
onecmd         	off
physical       	off
pipefail       	off
posix          	off
privileged     	off
verbose        	off
vi             	off
xtrace         	off

{shopt}

shopt [-pqsu] [-o] [optname ...]

Toggle  the values of variables controlling optional shell behavior.  With no options, or with the
-p option, a 'list' of all settable options is  displayed, with  an indication of whether or not
each is set.  The -p option causes output to be displayed in a form that may be reused  as  input.
Other  options have the following meanings:

-s     Enable (set) each optname.


<direxpand>
shopt -s direxpand

direxpand

If set, bash replaces directory names with the results of word expansion when performing filename
completion.  This changes the  contents of  the  readline  editing buffer.  If not set, bash
attempts to preserve what the user typed.


$ shopt
autocd         	off
cdable_vars    	off
cdspell        	off
checkhash      	off
checkjobs      	off
checkwinsize   	on
cmdhist        	on
compat31       	off
compat32       	off
compat40       	off
compat41       	off
direxpand      	on
dirspell       	off
dotglob        	off
execfail       	off
expand_aliases 	on
extdebug       	off
extglob        	on
extquote       	on
failglob       	off
force_fignore  	on
globstar       	off
gnu_errfmt     	off
histappend     	on
histreedit     	off
histverify     	off
hostcomplete   	off
huponexit      	off
interactive_comments	on
lastpipe       	off
lithist        	off
login_shell    	off
mailwarn       	off
no_empty_cmd_completion	off
nocaseglob     	off
nocasematch    	off
nullglob       	off
progcomp       	on
promptvars     	on
restricted_shell	off
shift_verbose  	off
sourcepath     	on
xpg_echo       	off


={============================================================================
*kt_linux_bash_021* bash: test exit status

The $? expands to the exit status of the 'most' recently executed foreground pipeline. Can be used
in test construct.

The following example demonstrates that TEST COMMANDS might be 'any' UNIX command that returns an
exit status, and that if again returns an exit status of zero:

<example>
note: The grep return 0 when found and 1 when not found. The test in shell see that 0 is true and 1
is false.

kpark@wll1p04345:~$ if grep KB readme; then echo "found"; fi
readme:1:this is KB for keitee.
found

kpark@wll1p04345:~$ if ! grep KB readme; then echo "found"; fi
readme:1:this is KB for keitee.
kpark@wll1p04345:~$ 

kpark@wll1p04345:~$ if true; then echo "found"; fi
found

<example>
while kill -0 "${parent_pid}" &>/dev/null;
do
   usleep 500
done

<example>

#include <stdio.h>
#include <stdlib.h>

int main(int argc, char** argv)
{
  printf(" this is a main function.\n");

  if( 2 == argc )
    exit(EXIT_FAILURE);

  exit(EXIT_SUCCESS);
}

$ ./a.out && echo "return success"
 this is a main function.
return success
$ 

$ ./a.out xx && echo "return success"
 this is a main function.
$ 


={============================================================================
*kt_linux_bash_022* bash: && and ||

if [ "$(whoami)" != 'root' ]; then
   echo "You have no permission to run $0 as non-root user."
   exit 1;
fi

With Bash, you can shorten this type of construct. The compact equivalent of the above test is as
follows:

[ "$(whoami)" != 'root' ] && ( echo you are using a non-privileged account; exit 1 )

Similar to the "&&" expression which indicates what to do if the test proves true, "||" specifies
what to do if the test is false.

<and-and-or>
3.2.3 Lists of Commands

An AND list has the form

command1 && command2

command2 is executed if, and only if, command1 returns an exit status of zero.

An OR list has the form

command1 || command2

command2 is executed if, and only if, command1 returns a non-zero(false) exit status.

The return status of AND and OR lists is the exit status of the last command executed in the list. 

<example>
1. if loginscript is set(defined), run -f. 
   if file exist then test return true so no fail() call.
   if file not exist then test return false so run fail() call.
2. if loginscript is not set, no run -f and test return true(0) so no fail() call.

[[ -z "$loginscript" || -f "$loginscript" ]] ||
    fail "Loginscript '$loginscript' doesn't exist."

note: progress to next cond when only previous one is true. [[ cond || cond ]] || exec

<example>
To see if meet at least one of options.

    [[ -n "$release" || -n "$zincdir" || -n "$jenkinshost" || -n "$jenkinsurl" ||
       -n "$galliumurl" || -n "$localgallium" || -n "$platformconfigurl" ||
       "$virtualinputdriver" -ne 0 || -n "$irdriver" || "$enabledbustcp" -ne 0 ||
       "$directfbsrc" -ne 0 || "$killzincdaemons" -ne 0 || -n "$toolnames" ||
       -n "$networkmanager" || -n "$loginscript" || -n "$startupscript" ||
       "$forcefirsttime" -ne 0 || "$needsreboot" -ne 0 || "$vidmemcapture" -ne 0 ]] ||
    {
        fail "You must specify at least one of -[ZzjugcdeEtsknlbTfrv]."
    }

<example>

function x {
  ...

  [[ 
    "$cacheOptionCorrectAtExpectedPosition" = 1 &&
    "$cacheValueCorrectAtExpectedPosition" = 1 &&
    "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
    "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
    "$jarOptionCorrectAtExpectedPosition" = "1" && 
    "$jarValueCorrectAtExpectedPosition" = "1" && 
    "$urlOptionCorrectAtExpectedPosition" = "1" && 
    "$urlValueCorrectAtExpectedPosition" = "1" 
  ]] 
}

From the debug output, shows only 4 since stops as soon as see flase. This may be confusing when
debugging.

+ [[ 1 = 1 ]]
+ [[ 1 = 1 ]]
+ [[ 1 = 1 ]]
+ [[ 0 = 1 ]]


={============================================================================
*kt_linux_bash_023* bash: debug

-x    " to start up the subshell in debug mode

#!/bin/bash -ex


={============================================================================
*kt_linux_bash_024* bash: builtin: exec

exec

exec [-cl] [-a name] [command [arguments]]

Two uses:

1. If command is supplied, it replaces the shell 'without' creating a new process. If command cannot
be executed for some reason, a non-interactive shell exits, unless the execfail shell option is
enabled. In that case, it returns failure. An interactive shell returns failure if the file cannot
be executed. 

Replace the shell with a given program (executing it, not as new process)

<example>
As you can see, the subshell is replaced by echo.

user@host:~$ PS1="supershell$ "
supershell$ bash
user@host:~$ PS1="subshell$ "
subshell$ exec echo hello
hello
supershell$ 

2. If no command is specified, redirections may be used to affect the current shell environment. If
there are no redirection errors, the return status is zero; otherwise the return status is non-zero.

Set redirections for the program to execute or for the current shell. If only redirections are
given, the redirections affect the current shell without executing any program. 


={============================================================================
*kt_linux_bash_025* bash: builtin: set

4.3.1 The Set Builtin

This builtin is so complicated that it deserves its own section. set allows you to change the values
of shell options and set the positional parameters, or to display the names and values of shell
variables.

set

    set [--abefhkmnptuvxBCEHPT] [-o option-name] [argument ...]
    set [+abefhkmnptuvxBCEHPT] [+o option-name] [argument ...]

If no options or arguments are supplied, set displays the names and values of all shell variables
and functions, sorted according to the current locale, in a format that may be reused as input for
setting or resetting the currently-set variables. Read-only variables cannot be reset. In POSIX
mode, only shell variables are listed.

When options are supplied, they set or unset shell attributes. Options, if specified, have the
following meanings: 

-- 

If no arguments follow this option, then the positional parameters are unset. Otherwise, the
positional parameters are set to the arguments, even if some of them begin with a -.


={============================================================================
*kt_linux_bash_026* bash: which one when there are script and exe binary in the same dir?

The executable binary gets executed.


={============================================================================
*kt_linux_bash_027* bash: test on file using which command

if which ssh-copy-id >/dev/null;
then
    ssh-copy-id -i $privatekey root@$stbip || true  # ssh-copy-id always returns failure
fi


={============================================================================
*kt_linux_bash_028* bash: builtin: programmable completion

https://www.gnu.org/software/bash/manual/html_node/Programmable-Completion-Builtins.html

Three builtin commands are available to manipulate the programmable completion facilities: one to
specify how the arguments to a particular command are to be completed, and two to modify the
completion as it is happening.

compgen

compgen [option] [word]

Generate possible completion matches for word according to the options, which may be any option
accepted by the complete builtin with the exception of -p and -r, and write the matches to the
standard output. 

The matches will be generated in the same way as if the programmable completion code had generated
them directly from a completion specification with the same flags. If word is specified, only those
completions matching word will be displayed.

The return value is true unless an invalid option is supplied, or no matches were generated.

note: compgen is the same as complete command but limited. use to 'generate' matches.

complete

complete [-abcdefgjksuv] [-o comp-option] [-DE] [-A action] [-G globpat] [-W wordlist]
    [-F function] [-C command] [-X filterpat]
    [-P prefix] [-S suffix] name [name ¿]
    complete -pr [-DE] [name ...

-A action
The action may be one of the following to generate a list of possible completions: 

function    Names of shell functions.

<example>
#!/bin/bash
# BASH FUNCTIONS CAN BE DECLARED IN ANY ORDER

function func_B {
  echo "{ fb"
  echo Function B.
  echo "} fb"
}

function func_A {
  echo "{ fa"
  echo $1
  echo "} fa"
}

function func_D {
  echo "{ fd"
  echo Function D.
  echo "} fd"
}

# function function_C {
function_C () {
  echo "{ fc"
  echo $1
  echo "} fc"
}

## foreach test
for test_fun in $(compgen -A function func_); do

  echo -e -n "\n$test_fun ... "

done


func_D ... kpark@wll1p04345:$ ./sample.sh 

func_A ... 
func_B ... 
func_D ... kpark@wll1p04345:$ 


={============================================================================
*kt_linux_bash_029* bash: variable arithmetic

The both makes the same result.

COUNT=10

while [ $COUNT -gt 0 ]; do
   echo Value of count is: $COUNT
   let COUNT=COUNT-1
done

COUNT=10

while [ $COUNT -gt 0 ]; do
   echo Value of count is: $COUNT
   COUNT=$(( $COUNT-1 ))
done


={============================================================================
*kt_linux_bash_030* bash: interactive shell or not


={============================================================================
*kt_linux_bash_100* bash: config: colour prompt

Uncomment the line that says force_color_prompt=yes then logout and back in in the .bashrc.


={============================================================================
*kt_linux_bash_101* bash: config: how to know shell nesting level

SHLVL  Incremented by one each time an instance of bash is started.
$ echo $SHLVL


={============================================================================
*kt_linux_bash_102* bash: config: prompt

keitee@debian-keitee:~/github/kb$ echo $PS1
\[\e]0;\u@\h: \w\a\]${debian_chroot:+($debian_chroot)}\u@\h:\w\$

PS1="\[\e]0;\u@\h: \w\a\]${debian_chroot:+($debian_chroot)}\u@\h:\w\:$SHLVL$"

keitee@debian-keitee:~/github/kb\:2$


={============================================================================
*kt_linux_bash_200* bash: code: debug and printf

debug() {
    # Note: echo -e doesn't work on OS X's default bash (3.2).
    printf '\n\033[0;32m%s\n' "$*"
    tput sgr0
}


={============================================================================
*kt_linux_bash_201* bash: code: array from 0 and return 1 for success like c

Use array from index 0 and return 1 for success. Unlike a shell convention which use 0 for success.

// return 1 (okay) when found a string in a array at the expected position. otherwise, return 0 (fail).

function paramsContainStringAtPosition () {
    local stringToSearchFor="$1"
    local positionToExpectStringAt="$2"
    local stringToSearch="${@:3}"
    local stringToSearchAsArray=($stringToSearch)
    local currentParamPos=0;
    local currentSearchString
        
    for currentSearchString in "${stringToSearchAsArray[@]}"
        do
            if [ "$currentSearchString" == "$stringToSearchFor" ]; then
                if [ "$positionToExpectStringAt" == "$currentParamPos" ]; then
                    return 1;
                fi
            fi
            currentParamPos=$(( $currentParamPos+1 ))
        done
    return 0
}

function x {

   ... 
   paramsContainStringAtPosition "$expected_url" 7 "$@"
   urlValueCorrectAtExpectedPosition=$?

   // note: here convert c style return to shell return by comararing "return from function = 1"
   
   [[
     "$cacheOptionCorrectAtExpectedPosition" = 1 &&
     "$cacheValueCorrectAtExpectedPosition" = 1 &&
     "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
     "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
     "$jarOptionCorrectAtExpectedPosition" = "1" && 
     "$jarValueCorrectAtExpectedPosition" = "1" && 
     "$urlOptionCorrectAtExpectedPosition" = "1" && 
     "$urlValueCorrectAtExpectedPosition" = "1" 
   ]] 
}


={============================================================================
*kt_linux_bash_202* bash: code: recursion test

The same script file is used to define tests and routines to check result using exec trick and
communication between parent process that runs the the script and the subshell that runs each test
case.

test.sh

test_xxx () {
  set local vars

  run.sh args...        
  " run.sh to exec the subprocess with the given cmd so the subprocess will run the cmd.

}

test_xxx_handler() {

}

if [ -z "$test_handler" ]; then
   for each test case

   " create a subprocess
   " run a test case that runs the other script and exec call eventually.
   (
    set env vars to pass to subprocess.

    " note: use env var to pass data to a subprocess
    " note: use a single line to run

    cmd = test.sh \                       " set cmd as the same script so 
    test_handler="${test_xxx}_handler" \
    test_xxx
   )
   done

else

  xxx_handler     " that checks the results

fi

So the parent process runs the test.sh and drives the whole test. For each test, set env vars and
create a subprocess that exec and run the given cmd. By having cmd as the same test.sh and the
test_handler env var set, subprocess runs the same script and env var set. So run the different path
and run check call in this case. So looks like a recursion or callback.

<code>

test_that_generates_url() {

    local app_dir="$scratch_dir/app"
    createApp "$app_dir"

    runBrowser.sh --app "$app_dir" --data "$default_app_data_dir" \
        --app-launch-parameters \
        "launch_context.ui.youview.com" "portal" \
        "some.test.param" "some.test.param.value" \
        "test.param.spaces" "param value with spaces"
}


that_generates_url_handler() {

    echo "that_generates_url_handler() - Got args: '$@'"

    local expected_url="http://youview.tv/test-player?\
launch_context.ui.youview.com=portal&some.test.param=some.test.param.value&\
test.param.spaces=param%20value%20with%20spaces"

    paramsContainStringAtPosition "-cache" 0 "$@"
    cacheOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "/tmp/client-cache" 1 "$@"
    cacheValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-cache-size" 2 "$@"
    cacheSizeOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "0" 3 "$@"
    cacheSizeValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-jar" 4 "$@"
    jarOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "$default_cookie_jar_location" 5 "$@"
    jarValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-url" 6 "$@"
    urlOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "$expected_url" 7 "$@"
    urlValueCorrectAtExpectedPosition=$?

    [[ 
      "$cacheOptionCorrectAtExpectedPosition" = 1 &&
      "$cacheValueCorrectAtExpectedPosition" = 1 &&
      "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
      "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
      "$jarOptionCorrectAtExpectedPosition" = "1" && 
      "$jarValueCorrectAtExpectedPosition" = "1" && 
      "$urlOptionCorrectAtExpectedPosition" = "1" && 
      "$urlValueCorrectAtExpectedPosition" = "1" 
    ]] 
}


# In test configuration runBrowser.sh will not actually launch the browser but
# will instead call back into this test script recursively.  We then check the
# environment in the test to see if the browser would have been launched with
# the appropriate environment and parameters.  This is done with pairs of
# functions, one called test_xxx and the other xxx_handler.  test_x functions
# are tests and the xxx_handler functions are what will be run when we
# runBrowser.sh calls us back.
#
# The recursion and the callback are communicated through the environment
# variable $test_handler.
if [ -z "$test_handler" ]; then

        failures=0
        successes=0

        ## foreach test
        for test_fun in $(compgen -A function test_); do

            echo -e -n "\n$test_fun ... "

            setup
            (

                RUN_BROWSER_CMD="$this_script" \
                RUN_BROWSER_DATA_DIR="${datadir:-}" \
                GST_INSPECT_CMD="touch $scratch_dir/gst-inspect" \
                test_handler="${test_fun#test_*}_handler" \
                $test_fun

            ) >> $logfile 2>&1

            status=$?
            if [ $status -eq 0 ]; then
                echo "OK"
                ((++successes))
            else
                echo "FAILURE"
                ((++failures))
            fi

            [[ "$1" == '-v' || $status -ne 0 ]] && { cat $logfile; echo; }
            teardown &> /dev/null

        done

        printf "$invoked_as complete. %i PASSES %i FAILURES\n" \
            $successes $failures

        exit $failures

else

        $test_handler "$@"

fi


={============================================================================
*kt_linux_bash_300* bash: faq: var#

The both shows the same result. What is this "${test_func#test*}" for?

test_handler="${test_fun#test*}_handler"
test_handler_two="${test_fun}_handler"
echo ">> $test_handler, $test_handler_two"


# ============================================================================
#{
={============================================================================
*kt_linux_tool_001* md5sum

md5sum - compute and check MD5 message digest


={============================================================================
*kt_linux_tool_002* cut

# input file
WIFI_SSID="SKY0F227"

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d=`
# this lead to WIFI_SSID='"SKY0F227"' and cannot use it as a var

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d \"`
# this fixes the problem.


={============================================================================
*kt_linux_tool_003* kill and killall

{kill-0}

while kill -0 "${parent_pid}" &>/dev/null;
do
   usleep 500
done

From man 2 kill
If sig is 0, then no signal is sent, but error checking is still performed; this can be used to
check for the existence of a process ID or process group ID.

<example>
#!/bin/bash
kill -0 323232 && echo "pid 323232 is present.."
kill -0 4355 && echo "pid 4355 x sess mgr is present.."

$ ./sbsh.sh 
./sbsh.sh: line 3: kill: (323232) - No such process
pid 4355 x sess mgr is present..


{kill-l}
       -l, --list [signal]
              List  signal  names.   This  option has optional argument, which
              will convert signal number to signal name, or other way round.


{killall}
killall - kill processes by name


={============================================================================
*kt_linux_tool_004* dmesg

To show the boot log and can see the kernel version.


={============================================================================
*kt_linux_tool_005* uname

# -all
uname -a 

keitee@debian-keitee:~/github/kb$ uname -a
Linux debian-keitee 3.2.0-4-486 #1 Debian 3.2.63-2+deb7u2 i686 GNU/Linux

{ubuntu}
$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=12.04
DISTRIB_CODENAME=precise
DISTRIB_DESCRIPTION="Ubuntu 12.04.1 LTS"


={============================================================================
*kt_linux_tool_006* cp

{copy-symbolic}
To copy symbolic links as well, cp -r don't work and use

-L, --dereference
always follow symbolic links

{careful}
cp âr /xx/dir/ /dst/ 

This cause that files copied under /dst/dir/. If want to copy only files under dir then use

cp âr /xx/dir/* /dst/


={============================================================================
*kt_linux_tool_007* mkdir

{p-option}
Use to make parent directories as well.


={============================================================================
*kt_linux_tool_008* strings

To find string in the library.

keitee@linux:~/share/temp> strings sec_getba.a | grep GCC
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6


={============================================================================
*kt_linux_tool_009* sort

See the unix power tool 22.6 for more. -u remove duplicates and sort against field [4,7].

sort -u -k 4,7

http://www.gnu.org/software/coreutils/manual/html_node/sort-invocation.html

--key=pos1[,pos2]

Specify a sort field that consists of the part of the line between pos1 and pos2 (or the end of the
line, if pos2 is omitted), inclusive.  Each pos has the form 'f[.c][opts]', where f is the number of
the field to use, and c is the number of the first character from the beginning of the field. Fields
and character positions are numbered starting with 1; a character position of zero in pos2 indicates
the field's last character. If '.c' is omitted from pos1, it defaults to 1 (the beginning of the
field); if omitted from pos2, it defaults to 0 (the end of the field). opts are ordering options,
allowing individual keys to be sorted according to different rules; see below for details. Keys can
span multiple fields.  Example: To sort on the second field, use --key=2,2 (-k 2,2). See below for
more notes on keys and more examples. See also the --debug option to help determine the part of the
line being used in the sort.

To sort
NDS: ^0946684946.752246 !ERROR -aem          < M:aem_list.c F:AEM_ListGetApplication L:01808 > Can't find
-1-- ----------2------- ---3-- -4--         -5 ------6----- ---------------7-------- ---8--- 9 -10--

sort -u -k 4,10 ndsfusion.test > ndsfusion.dic 


-t separator --field-separator=separator

Use character separator as the field separator when finding the sort keys in each line. By default,
fields are separated by the empty string between a non-blank character and a blank character. By
default a blank is a space or a tab, but the LC_CTYPE locale can change this.

That is, given the input line ' foo bar', sort breaks it into fields ' foo' and ' bar'. The field
separator is not considered to be part of either the field preceding or the field following, so with
'sort -t " "' the same input line has three fields: an empty field, 'foo', and 'bar'. However,
fields that extend to the end of the line, as -k 2, or fields consisting of a range, as -k 2,3,
retain the field separators present between the endpoints of the range.  To specify ASCII NUL as the
field separator, use the two-character string '\0', e.g., 'sort -t '\0''. 


mh5a_variable.c
mh5b_variable.c

sort -t _ -k 2,2

Use this to list out member functions from sources

egrep -r 'g_pAppWindow->' . | sort -t - -k 2,3 >> in.txt

<example>
$ df -h | sort -rnk 5


={============================================================================
*kt_linux_tool_010* grep and egrep

To find files which has the given string:

grep -nro LINUX ./

       -s, --no-messages
              Suppress error messages about nonexistent or unreadable files.   Portability  note:
       -o, --only-matching
              Print only the matched (non-empty) parts of a matching line, with each such part on
              a separate output line.
       -n, --line-number
              Prefix each line of output with the 1-based line number within its input file.  (-n
              is specified by POSIX.)
       -R, -r, --recursive
              Read  all  files  under  each  directory, recursively; this is equivalent to the -d
              recurse option.

To specify the target files:

grep -nr --include *.c __setup ./
grep -nr --include *.c [a-z]*_initcall ./


{when-see-binary-errors-on-text-file}
Even if that is a text file, grep say it is binary file then use -a option.

(http://www.gnu.org/software/grep/manual/html_node/Usage.html)

Why does grep report Binary file matches? If grep listed all matching lines from a binary file, it
would probably generate output that is not useful, and it might even muck up your display. So gnu
grep suppresses output from files that appear to be binary files. To force gnu grep to output lines
even from files that appear to be binary, use the '-a' or '--binary-files=text' option. To eliminate
the Binary file matches messages, use the '-I' or '--binary-files=without-match' option. 

To exclude binaries:

-I     
Process a binary file as if it did not contain matching data; this is equivalent to the
--binary-files=without-match option.


{q-option}
Used in the script.

-q, --quiet, --silent

Quiet; do not write anything to standard output. Exit immediately with zero status if any match is
found, even if an error was detected. Also see the -s or --no-messages option. 

echo xx | grep -q '.gz$' && echo ture
echo xx.gz | grep -q '.gz$' && echo ture
ture

The script example:

##:zcat if this is a .gz, cat otherwise
F=cat
echo "$1" | grep -q '\.gz$' && F=zcat
$F "$1"


{line-control}
-A NUM, --after-context=NUM
Print NUM lines of trailing context after matching lines.  Places a line containing a group
separator  (--)  between  contiguous  groups  of  matches.   With  the  -o  or --only-matching
option, this has no effect and a warning is given.

-B NUM, --before-context=NUM
Print  NUM  lines  of  leading  context  before  matching  lines.   Places  a  line containing a
group separator (--) between contiguous groups of matches.  With the -o or --only-matching option,
      this has no effect and a warning is given.

-C NUM, -NUM, --context=NUM
Print NUM lines of output context.  Places a line containing a group separator (--) between
contiguous groups of matches.  With the -o or --only-matching option, this  has no effect and a
warning is given.

note: -C works like before and after


{case-example}
# what it's trying to do is that search through the same errors in logs which was uploaded in
# Feburary and was under translation. For example,
# translation/Zone9-Box5_Feb_18_09_38_42b3e5c9130e1d758acdc71bb9d12b2a
#

ls translation/ | grep Feb | while read x; do pushd -n translation/$x; pwd; egrep --color -ano 'btreePageFromDbPage' .detailed_output; popd -n; done 

$ egrep --color -an 'btreePageFromDbPage' translation/*_Feb_*/.detailed_output

translation/PaulRiley_Work_Feb_18_09_42_25d1235e4ebe020a369a64d728c2ce9f/.detailed_output:8:parser_result=<p1>MW_Process
crash in #0 btreePageFromDbPage (pDbPage=0x2b994bcc, pgno=67, pBt=0x2bb06da4) (built by hudson)</p1>
$ 

$ egrep --color -ano 'event manager POLL time exceeded threshold' \
    translation/*_Feb_*/LOGlastrun_realtime

translation/000000002704_Feb_1_14_05_3a6f10bd9e8934088df33e9d991faa74/LOGlastrun_realtime:2100:event
manager POLL time exceeded threshold


={============================================================================
*kt_linux_tool_011* find

{find-mtime}
24 hours based so -mtime 3 means that 72 hours from the current time and -mtime -3 means 72 before.
To specify between 72 and 96, -mtime 3 and after 96, -mtime +3. If want to find file or dirs changed
most recently, use -mtime 0 or 1.


{find-to-get-the-number-of-files}
%find -type f | wc -l


{find-print0}
-print0

print the full file name on the standard output, followed by a null character (instead of the new-
line character that -print uses). This allows file names that contain newlines or other types of
white space to be correctly interpreted by programs that process the find output. This option
corresponds to the -0 option of xargs.

$ find . -name CMakeLists.txt -print0
./CMakeLists.txt./Source/CMakeLists.txt./Source/cmake/gtest/CMakeLists.txt./Source/WebKit2/CMakeLists.txt./Source/WebKit2/UIProcess/efl/po_tizen/CMakeLists.txt./Source/WebKit/CMakeLists.txt./Source/WebKit/efl/DefaultTheme/CMakeLists.txt./Source/JavaScriptCore/CMakeLists.txt./Source/JavaScriptCore/shell/CMakeLists.txt./Source/WebCore/CMakeLists.txt./Source/ThirdParty/gtest/CMakeLists.txt./Source/WTF/CMakeLists.txt./Source/WTF/wtf/CMakeLists.txt./Tools/WinCELauncher/CMakeLists.txt./Tools/tizen-webview-test/CMakeLists.txt./Tools/MiniBrowser/efl/CMakeLists.txt./Tools/CMakeLists.txt./Tools/TestWebKitAPI/CMakeLists.txt./Tools/EWebLauncher/CMakeLists.txt./Tools/EWebLauncher/ControlTheme/CMakeLists.txt./Tools/DumpRenderTree/TestNetscapePlugIn/CMakeLists.txt./Tools/DumpRenderTree/efl/CMakeLists.txt./Tools/WebKitTestRunner/CMakeLists.txtkeitee.park@rockford /home/tbernard/Git/vdTizen/webkit

{Q} when useful? To use the out as a single line?
Used for xargs


{find-exec}
{} where the filename will be inserted. Add \; at the end of the command to complete the required
syntax. note: there must be a space after {}

$ find . -name CMakeLists.txt -exec egrep PROJECT {} \;

To run dirtags script for each directory:

$ find * -type d -exec dirtags {} \;


{find-oring}
You can specify a logical "or" condition using -o:

find / \( -size +50 -o -mtime -3 \) -print
find /my/project/dir -name '*.c' -o -name '*.h'
find -name *.[ch]

This is from bash  expr: expr1 -or expr2 and this means expr1 -o expr2, but not POSIX compliant.


{find-ignore}
-path pattern

File name matches shell pattern pattern.  The metacharacters do not treat `/' or `.' specially; so,
for example,

find . -path "./sr*sc"

will print an entry for a directory called `./src/misc' (if one exists).  To ignore a  whole
directory tree,  use  -prune  rather  than  checking  every file in the tree.  For example, to skip
the directory `src/emacs' and all files and directories under it, and print the names of the other
files  found,  do something like this:

find . -path ./src/emacs -prune -o -print

Note  that the pattern match test applies to the whole file name, starting from one of the start
points named on the command line.  It would only make sense to use an absolute path name here if the
relevant start point is also an absolute path.  This means that this command will never match
anything:

find bar -path /foo/bar/myfile -print

The  predicate  -path is also supported by HP-UX find and will be in a forthcoming version of the
POSIX standard.


{find-sym}
-L     
Follow symbolic links.


={============================================================================
*kt_linux_tool_012* make a empty file without touch

can use 'touch' but when busybox do not support touch, can use following to make a empty
file or to reset a file.

cat /dev/null > file


={============================================================================
*kt_linux_tool_013* xargs

--null
-0     

Input items are terminated by a null character instead of by whitespace, and the quotes and
backslash are not special (every character is taken literally). Disables the end of file string,
which is treated like any other argument. Useful when input items might contain white space, quote
  marks, or  backslashes. The GNU find -print0 option produces input suitable for this mode.
note: see find-print0

-I replace-str

Replace occurrences of replace-str in the initial-arguments with names read from standard input.
Also, unquoted  blanks  do not terminate input items; instead the separator is the newline
character. Implies -x and -L 1.

$ ls | grep Nov_ | xargs -I{} find {} -name LOGlastrun_realtime -print | xargs egrep -an \ 
"(NCM_ADDRESSING_TYPE_DHCP address still in status list)"


<xargs-vs-find>
The xargs command builds and executes command lines from standard input. This has the advantage that
the command line is filled until the system limit is reached. Only then will the command to execute
be called, in the above example this would be rm. If there are more arguments, a new command line
will be used, until that one is full or until there are no more arguments. The same thing using find
-exec calls on the command to execute on the found files every time a file is found. Thus, using
xargs greatly speeds up your scripts and the performance of your machine.


={============================================================================
*kt_linux_tool_014* ssh and putty

{aim}
To switch hosts using key base instead of using password.

{ssh-keygen}
$ ssh-keygen <enter>

$ ls -al .ssh/
total 20
drwx------  2 parkkt ccusers 4096 Dec  9 13:24 .
drwxr-xr-x 15 parkkt ccusers 4096 Dec  9 12:47 ..
-rw-------  1 parkkt ccusers 1675 Dec  9 13:24 id_rsa
-rw-r--r--  1 parkkt ccusers  411 Dec  9 13:24 id_rsa.pub

The file we need to copy to the server is named id_dsa.pub. As you can see above, the file needed
exists. You may or may not have other files in ~/.ssh as I do. If the key doesn't exist, however,
you can make one as follows:

$cp id_rsa.pub authorized_keys
$scp -p ~/.ssh/authorized_keys ukstbuild3:.ssh/
(user@homebox ~ $ scp ~/.ssh/id_dsa.pub user@'servername':.ssh/authorized_keys)
(     -p      Preserves modification times, access times, and modes from the
             original file.)

This make one-way ssh connection which means the machine you are on is added the authorized_keys of
the server so can run scp from the machine to the server:

scp remote-server:{path}/filename .
scp filename remote-server:{path}/filename

note: if there is working ssh connection, can use tab key to get file completion when use scp.
note: ssh-copy is not working and key line in authorized_keys must be one line.
note: if copy host's pub key in the target's authorized_keys, then do need to enter password when
ssh to the target from host.

<to-passwordless-login-to-box>
Therefore, if copy rsa.pub to authorized_keys on a box then make one direction open and no need to
use password from host to box.

<ssh-copy-id>
NAME
       ssh-copy-id - install your public key in a remote machine's authorized_keys

SYNOPSIS
       ssh-copy-id [-i [identity_file]] [user@]machine

DESCRIPTION
       ssh-copy-id is a script that uses ssh to log into a remote machine and append the indicated identity file to that machine's ~/.ssh/authorized_keys file.

       If the -i option is given then the identity file (defaults to ~/.ssh/id_rsa.pub) is used, regardless of whether there are any keys in your ssh-agent.  Otherwise, if this:

             ssh-add -L

       provides any output, it uses that in preference to the identity file.

       If the -i option is used, or the ssh-add produced no output, then it uses the contents of the identity file.  Once it has one or more fingerprints (by whatever means) it uses ssh
       to append them to ~/.ssh/authorized_keys on the remote machine (creating the file, and directory, if necessary.)


{to-check-match-pair}
$ ssh-keygen /?
  -y          Read private key file and print public key.

$ ssh-keygen -y -f id_rsa


{caution}
note:
If the file permissions are too open then ssh will not trust them, and will still prompt
you for your password. 

chmod 700 ~/.ssh
chmod 644 ~/.ssh/authorized_keys    # must check this as caused big grief when different
chmod 644 ~/.ssh/id_dsa_pub
chmod 644 ~/.ssh/known_hosts
chmod 600 ~/.ssh/id_dsa


{config}
When user name is different between servers, must have an entry in this file for servers to connect.

$ cat ~/.ssh/config
Host tizen
        Hostname 168.219.241.167
        IdentityFile ~/.ssh/id_rsa
        User keitee.park                 # this is username that can be different from real user.
        Port 29418

To debug ssh. note: shall use name on the command line
-v  # -vv
Verbose mode. Causes ssh to print debugging messages about its progress. This is helpful in
debugging connection, authentication, and configuration problems. Multiple -v options increase
the verbosity. The maximum is 3. 

$ ssh -vT tizen


{github-when-ssh-do-not-work}
Using SSH over the HTTPS port

Sometimes the administrator of a firewall will refuse to allow SSH connections entirely. If using
HTTPS cloning with credential caching is not an option, you can attempt to clone using an SSH
connection made over the HTTPS port. Most firewall rules should allow this, but proxy servers may
interfere. 

Testing

To test if SSH over the HTTPS port is possible, run this ssh command:

ssh -T -p 443 git@ssh.github.com
# Hi username! You've successfully authenticated, but GitHub does not provide shell access.

Make it so

If you are able to ssh to git@ssh.github.com over port 443, you can override your ssh settings to
force any connection to github.com to run though that server and port. To set this in your ssh
config, edit the file at ~/.ssh/config and add this section:

Host github.com
  Hostname ssh.github.com
  Port 443

You can test that this works by connecting to github.com:

ssh -T git@github.com
# Hi username! You've successfully authenticated, but GitHub does not provide shell access.


{putty-ssh-setup}
When use keys generated from putty.

o run puttygen to make key pairs. rsa or dsa
o get a pub key and save a pri key(ppk)
o run putty and set ssh key to use
  menu: connection: ssh: auth: private key file for auth: specify the path to a pri key.
o login to the host and add a pub key in the auth key list


{convert-rsa-key-to-putty-ppk}
To convert keys from linux machine to putty ppk and from ppk to lunux(opsnssh keys)

o run puttygen and menu: conversion: import key:
o save it as a pri key(ppk)


={============================================================================
*kt_linux_tool_015* pidof

pidof -- find the process ID of a running program.

Pidof finds the process id's (pids) of the named  programs. It prints those id's on the standard
output. 

$ pidof getty
2974 2973 2972 2971 2970 2969


={============================================================================
*kt_linux_tool_016* screen

Screen is a full-screen software program that can be used to multiplexes a physical console between
several processes (typically interactive shells). It offers a user to open several separate terminal
instances inside a one single terminal window manager.


screen -a -D -R -fn -l /dev/ttyUSB0 115200,cs8

to close screen use Ctrl-A, k, y. Do not use Ctrl-C as it can kill processes running on the box.

{logging}
Use -L for auto logging and will log on 'screenlog.0'. Use this file to see:

-L   tells screen to turn on automatic output logging for the windows.

screen -a -D -R -fn -l -L /dev/ttyUSB0 115200,cs8


={============================================================================
*kt_linux_tool_017* ldd

To see library dependancy of a application. This is a script file and shows a dynamic library
dependancy. If not, shows 'not a dynamic executable' message.

The simplest approach is to pick a binary that you consider is typical (e.g. /bin/ls and run ldd on
    it. One of the listed libraries should be libc - check its version number.

$ ldd /bin/ls
        libc.so.6 => /lib/libc.so.6 (0x4000e000)
        /lib/ld-linux.so.2 => /lib/ld-linux.so.2 (0x40000000)


{case}
The ldd is script. Although the binary uses shared library, ldd don't understand it.

$ readelf -d nexus-inspect | grep NEED
(standard input):4: 0x00000001 (NEEDED)                     Shared library: [libnexus.so]
(standard input):5: 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
(standard input):6: 0x00000001 (NEEDED)                     Shared library: [libpthread.so.0]
(standard input):7: 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
$ ldd -v nexus-inspect
	not a dynamic executable


={============================================================================
*kt_linux_tool_018* ls

{get-filename-only}
ls -1

{get-color}
For bash, copy /etc/DIR_COLORS into home as .dir_colors and edit it to change default values. Run
man dir_colors for help.


={============================================================================
*kt_linux_tool_019* strace

strace - trace system calls and signals 

-f
Trace child processes as they are created by currently traced processes as a result of the fork(2)
system call. 

-o filename
Write the trace output to the file filename rather than to stderr. Use filename.pid if -ff is used.
If the argument begins with '|' or with '!' then the rest of the argument is treated as a command
and all output is piped to it. This is convenient for piping the debugging output to a program
without affecting the redirections of executed programs. 

<example>
When use to run a script:

$ strace -f -o trace sh ./compile.sh

When use to run a program:

$ strace ./appname


={============================================================================
*kt_linux_tool_020* time and date

kit@kit-vb:~/tizencore$ time -f "%E real,%U user,%S sys" ls -Fs
-f: command not found

real	0m0.143s
user	0m0.068s
sys	0m0.040s

kit@kit-vb:~/tizencore$ /usr/bin/time -f "%E real,%U user,%S sys" ls -Fs
total 24
4 app-core/  4 appfw/  4 application/  4 app-service/  4 dlog/	4 README
0:00.00 real,0.00 user,0.00 sys
kit@kit-vb:~/tizencore$ 

Users of the bash shell need to use an explicit path in order to run the external time command and
not the shell builtin variant. On system where time is installed in /usr/bin, the first example
would become /usr/bin/time wc /etc/hosts


{date}
$ date --rfc-3339=s
2015-01-27 13:49:11+00:00

$ date
Tue Jan 27 13:49:19 UTC 2015


={============================================================================
*kt_linux_tool_021* chmod

{to-set-uid}
That require root access to function properly even when invoked by a nonroot user

%chmod +s /sample_file


={============================================================================
*kt_linux_tool_022* mknod

%mknod $name c $major $minor


={============================================================================
*kt_linux_tool_023* wc

-l, --lines
     print the newline counts

{how-to-count-lines-recursively}

find . -name '*.php' | xargs wc -l

# should be re-written using sh scripting
#!/bin/bash
echo "debug..."
find debug -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "dsm"
find dsm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "include"
find include -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "main"
find main -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

#
echo "mah"
find mah -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5dec"
find mh5dec -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5eng"
find mh5eng -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5gpi"
find mh5gpi -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mhv"
find mhv -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "pfm"
find pfm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l


={============================================================================
*kt_linux_tool_024* du

' to print one level
du -h --max-depth=1

-h, --human-readable
print sizes in human readable format (e.g., 1K 234M 2G)

' to show all and total
du -ach
du -sh


={============================================================================
*kt_linux_tool_025* ln

       -f, --force
              remove existing destination files

       -s, --symbolic
              make symbolic links instead of hard links

ln -sf input output. E.g., ln -sf linux-2.4.25-2.8 linux

<example>
$ ln -s /usr/src/linux .
lrwxrwxrwx  1 keitee keitee    14 Feb 18 23:50 linux -> /usr/src/linux/


={============================================================================
*kt_linux_tool_026* rsync

Rsync, which stands for "remote sync", is a remote and local file synchronization tool. It uses an
algorithm that minimizes the amount of data copied by only moving the portions of files that have
changed.

<example>
$ mkdir dir1 dir2
$ touch dir1/file{1..10}
$ ls dir1
file1  file10  file2  file3  file4  file5  file6  file7  file8  file9

$ rsync -r dir1/ dir2      " okay as synced
$ ls dir2
file1  file10  file2  file3  file4  file5  file6  file7  file8  file9

note: This trailing / is necessary to mean "the contents of dir1".

$ rsync -r dir1 dir2       " not okay as
$ ls dir2
dir1

The -a option is a combination flag.

It stands for "archive" and syncs recursively and preserves symbolic links, special and device
files, modification times, group, owner, and permissions. It is more commonly used than -r and is
usually what you want to use.

The -n or --dry-run options. 

The -v flag (for verbose). 

The -P flag is very helpful. 
It combines the flags --progress and --partial. The first of these gives you a progress bar for the
transfers and the second allows you to resume interrupted transfers:

The -z flag. 
If you are transferring files that have not already been compressed, like text files, you can reduce
the network transfer by adding compression with the -z option.

<update>
Update the modification time on some of the files and see that rsync intelligently re-copies only
the changed files:

<delete>
By default, rsync does not delete anything from the destination directory. We can change this
behavior with the --delete option. Before using this option, use the --dry-run option and do testing
to prevent data loss:

rsync -a --delete source destination


<example> over SSH
rsync -a ~/dir1 username@remote_host:destination_directory
rsync -a username@remote_host:/home/username/dir1 place_to_sync_on_local_machine

<example>
Want to copy all expect .git from source to destination

rsync -av --progress /home/kit/mheg-remote-git/mag_shared/ . --exclude .git

<example>
setupRsyncDaemon() {

    # Setup rsync daemon for faster non-encrypted transfer
    rsync ${privatekey:+ --rsh="ssh -i $privatekey"} \
        "$(thisScriptSrcDir)"/rsyncd.{conf,scrt} root@$stbip:/etc/
    $ssh "chmod o-rwx /etc/rsyncd.scrt && rsync --daemon --ipv4"
    export RSYNC_PASSWORD=zinc
}


={============================================================================
*kt_linux_tool_027* awk

Another popular stream editor. The basic function of awk is to search files for lines or other text
units containing one or more patterns. When a line matches one of the patterns, special actions are
performed on that line.

<data-driven>
Programs in awk are different from programs in most other languages, because awk programs are
"data-driven": you describe the data you want to work with and then what to do when you find it.
Most other languages are "procedural."

<rule>
The program consists of a series of rules. Each rule specifies one pattern to search for and one
action to perform upon finding the pattern.

<command>
print

The print command in awk outputs selected data from the input file. $0 (zero) holds the value of the
entire line.

ls -l | awk '{ print $5 $9 }'

With formatting.

awk '{ print "Size is " $5 " bytes for " $9 }'

<regex>
awk 'EXPRESSION { PROGRAM }' file(s)

For files ending in ".conf" and starting with either "a" or "x", using extended regular expressions
note: the below do not work.

ls -l | awk '/\<(a|x).*\.conf$/ { print $9 }'

To add text before output: begin message. beginning of execution before any input has been processed

awk 'BEGIN { print "Files found:\n" } /\<[a|x].*\.conf$/ { print $9 }'

To add text after output:

awk '/\<[a|x].*\.conf$/ { print $9 } END { print \ "Can I do anything else for you, mistress?" }'

<variables>
field separator

awk 'BEGIN { FS=":" } { print $1 "\t" $5 }' /etc/passwd

output field separator

> cat test
record1 data1
record2 data2

> awk '{ print $1 $2}' test
record1data1
record2data2

> awk '{ print $1, $2}' test
record1 data1
record2 data2

<example>
To make a filename from the date:

date: Fri Jul 25 05:30:12 BST 2014 -> log-Fri-Jul-25-...

script -f /home/kit/log/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`

To make a gateway from the ip address:
gateway=`echo $ip | awk 'BEGIN { FS="." } ; { print $1"."$2"."$3"."1 }'`


={============================================================================
*kt_linux_tool_028* sed

A 's'tream 'ed'itor is used to perform basic transformations on text read from a file or a pipe. The
result is sent to standard output. The editor does not modify the original input.

What distinguishes sed from other editors, such as vi and ed, is its ability to filter text that it
gets from a pipeline feed. You do not need to interact with the editor while it is running. This
feature allows use of editing commands in scripts, greatly easing repetitive editing tasks. When
facing replacement of text in a 'large' number of files, sed is a great help.

<command>
s     search and replace text

<example>
To find and replace string in multiple files.

$ egrep -lr --include *.c mhvSessionCancel .
./mh5eng/mh5a_application.c

$ egrep -lr --include *.c mhvSessionCancel . | xargs sed -i 's/mhvSessionCancel/mmhv_session_cancel/'

grep
-l, --files-with-matches

Suppress normal output; instead  print  the name of each input file from which output would normally
have been printed. The scanning will stop on the first  match.

sed
-i[SUFFIX], --in-place[=SUFFIX]

edit files in place (makes backup if extension supplied)

-n, --quiet, --silent

suppress automatic printing of pattern space. 
note: why this? since sed print out line which are not in matches so use -n to output only matches.

-u, --unbuffered

load minimal amounts of data from the input files and flush the output buffers more often

<range>
sed -n '2,4d' example
sed -n '3,$p' example


={============================================================================
*kt_linux_tool_029* pyserial and grabserial

http://elinux.org/Grabserial
https://github.com/tbird20d/grabserial

If no options are specified, grabserial uses serial port /dev/ttyS0, at 115200 baud with "8, None
and 1" (8N1) settings. 

alias gse="sudo grabserial -v -d "/dev/ttyUSB0" -b 115200 -w 8 -p N -s 1 \
| tee 2>&1 ~/logs/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`"

alias gse="sudo grabserial -v -d /dev/ttyUSB0 \
| tee 2>&1 ~/logs/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`"


={============================================================================
*kt_linux_tool_030* diff and patch

{diff}
<from-to-and-unified>
To compare two files:

diff [options] from-file to-file
diff -u file1 file2

-u    Use the 'unified' output format.

This outputs a description of how to transform file1 'into' file2 to stdout in unified format which
is the easiest to read. The description is called a patch.

You can compare a whole directory tree:

diff -u -r directory1 directory2

This recurses the directory structures. Whenever a file differs, the patch for that file is appended
to the output.

To save the patches to a file that you can store or send to someone else, simply redirect stdout to
a file, e.g:

diff -u file1 file2 > my_changes.patch

{unified-format}
A typical patch looks like this:

--- a/vobs/DARWIN_APPLICATIONS/DARWIN_GAUDI_ORCHID/build/applications/GAUDI_Orchid/EPG/statmgr/xtv_states/QhsmPVR.c
+++ b/vobs/DARWIN_APPLICATIONS/DARWIN_GAUDI_ORCHID/build/applications/GAUDI_Orchid/EPG/statmgr/xtv_states/QhsmPVR.c
@@ -2815,7 +2815,6 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
 
         /*set the playback speed to normal*/
         DBGMSG_M("QHsmPVR_backWard_skip - PVR_NAV_SetSpeed(PVR_NAV_PLAY)\n");
-        PVR_NAV_SetSpeed(PVR_NAV_PLAY);
 
         /*check if we are in RB mode*/
         CONSELECTCO_GetCurrentPlayRBFlag( &state);
@@ -2827,6 +2826,7 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
         {
             /*In RB mode*/
             PVR_NAV_SetPosition ( PVR_NAV_RELATIVE_POSITION_START , 1000);
+           PVR_NAV_SetSpeed(PVR_NAV_PLAY);^M
         }
         else{
             /*We are in playback mode*/
@@ -2840,6 +2840,7 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
             if (rc != PVR_NAV_RC_OK) {
                 DBGMSG_M("No previous skip point; skipping to beginning and waiting for timeout;");
                 PVR_NAV_SetPosition ( PVR_NAV_RELATIVE_POSITION_START , 0);
+               PVR_NAV_SetSpeed(PVR_NAV_PLAY);^M
                 prevPoint = -1;
             }
             else {

<header>
The first two lines describe the files to be transformed. The file indicated by "---" is transformed
into the file indicated by "+++". Let's call them the old file and the new file.

<hunks>
This header is followed by a series of "hunks". Each hunk describes a changes to make to a section
of the file. The @@ symbols indicate the 'start' of a hunk. The first set of numbers, e.g. "-2815,7"
indicates that in the old file, the section 'started' on line 2815 and 'lasted' for 7 lines. The second
set of numbers, e.g. "+2815,6" indicates that in the new file, the section starts on like 2815 and
lasts for 6 lines. (You can guess that we are going to remove a line.)

<p-option>
The text after the @@ symbol indicates the function name that the change is in. This is generated by
the -p option of diff so may not always be there. It is just to make the patch easier to read.

       -p, --show-c-function
              show which C function each change is in

<context>
Next we have three lines of context. These are just the three lines before the change. They are used
to check that the change is going to be made in the right place. For example, it is possible that
you want to apply the patch to a different version of the file it was created with. These lines help
you manually or automatically apply the change in the right place, even if the line numbers have
changed. The context is also used to detect when a patch cannot be applied because of a conflict.
More on this later.

<changes>
Next we have the change itself. Lines are either removed (indicated by a - sign) or added, indicated
by a + sign. When a line is changed, it usually appears as a removed line followed by an added line.

Finally we have three more lines of context.

So now you can read patches and even manually apply simple ones. But with complicated patches, you
would prefer to apply them automatically.


{patch}
GNU patch reads a patch file and applies the transformations it describes. Typical use:

cd directory_containing_file_to_change

patch < my_changes.patch

GNU patch reads patches from stdin. In this case we have redirected stdin to the patch file with the "<" symbol.

If you have a patch with lots of files in different directories, you might do this:

cd parent_directory

patch -p1 < my_changes.patch

<p-option>
The number after the p tells patch how 'many' directory names to strip from the filenames before
applying the patch. For example, typically the person generating the patch did something like this:

cd my_code
cd ..
cp -R my_code my_original_code

...make changes to lots of files...

diff -u -r my_original_code my_code

This means that the patch file will contain filenames like this:

--- my_original_code/directory/file.c
+++ my_code/directory/file.c

Now you probably don't have directories called "my_code" and "my_original_code", so you would cd to
"directory" and tell patch to ignore the first directory in each file path by using the "-p1"
option.

<patching-non-identical-files>
GNU patch will try to apply the changes even if you are applying them to a different version of the
file to the one the patch was originally created against. It uses the context lines to do this. As
long as enough context lines appear, and they have not moved to far from their original positions,
     the change will be applied.

So, for example, if a new function was added to the top of the file, the change to a function
further down the file would still be correctly applied. This means it can be more useful to send
someone a patch than a whole file, because it will often still work if the person you send it to is
starting with a different version of the file.

If GNU patch decides it cannot apply a change, you will see:

HUNK FAILED

along with details of which hunk it was. You can then investigate why the hunk failed - patches are
easy enough to read to do this.

<concatenating-patches>
Patches can be concatenated, so you can combine multiple patches into one:

diff -u my_code_orig/file1.c my_code/file1.c > patch1
diff -u my_code_orig/file2.c my_code/file2.c > patch2
cat patch1 patch2 > big_patch


{example}
-a     Treat all files as text and compare them line-by-line, even if they do not seem to be text.
-N
--new-file
   In directory comparison, if a file is found in only one directory, treat it as present but empty
   in the other directory.

diff -Naur darwin-spk-1.18_orig/target/device/Sky/ams-drx890/kernel-debug.config darwin-spk-1.18/target/device/Sky/ams-drx890/kernel-debug.config
--- darwin-spk-1.18_orig/target/device/Sky/ams-drx890/kernel-debug.config	2011-10-05 12:36:12.000000000 +0100
+++ darwin-spk-1.18/target/device/Sky/ams-drx890/kernel-debug.config	2012-01-18 15:23:41.000000000 +0000
@@ -1157,7 +1157,7 @@
 CONFIG_DEBUG_FS=y
 # CONFIG_WANT_EXTRA_DEBUG_INFORMATION is not set
 CONFIG_CROSSCOMPILE=y
-CONFIG_CMDLINE="mem=160M rw console=uart,mmio,0x10400b00,115200n8 ip_conntrack_tftp.ports=4085"
+CONFIG_CMDLINE="mem=166M rw console=uart,mmio,0x10400b00,115200n8 ip_conntrack_tftp.ports=4085"
 CONFIG_SYS_SUPPORTS_KGDB=y
 # CONFIG_MIPS_BRCM_SIM is not set

<patch-options> in the script

       -d dir  or  --directory=dir
          Change to the directory dir immediately, before doing anything else.

       --dry-run
          Print  the results of applying the patches without actually changing
          any files.

       -E  or  --remove-empty-files
          Remove output files that are  empty  after  the  patches  have  been
          applied.  Normally this option is unnecessary, since patch can examâ
          ine the time stamps on the header to determine whether a file should
          exist  after  patching.  However, if the input is not a context diff
          or if patch is conforming to POSIX,  patch  does  not  remove  empty
          patched  files  unless  this  option is given.  When patch removes a
          file, it also attempts to remove any empty ancestor directories.

       -f  or  --force
          Assume that the user knows exactly what he or she is doing,  and  do
          not  ask any questions.  Skip patches whose headers do not say which
          file is to be patched; patch files even though they have  the  wrong
          version  for  the Prereq: line in the patch; and assume that patches
          are not reversed even if they look like they are.  This option  does
          not suppress commentary; use -s for that.

       -s  or  --silent  or  --quiet
          Work silently, unless an error occurs.

   patch)
      # For each patch, try to derive whether it was created as -p0 or -p1.
      # p0 is typically from svn/cvs/git, while p1 will typically come from diff.
      patchlevel=

      patch -d $SPKDIR -p1 --dry-run --quiet -f < $p > /dev/null 2>&1
      p1=$?
      if [ $p1 -eq 0 ]; then
         patchlevel=-p1
      fi

      patch -d $SPKDIR -p0 --dry-run --quiet -f < $p > /dev/null 2>&1
      p0=$?
      if [ $p0 -eq 0 ]; then
         patchlevel=-p0
      fi
      if [ $p0 -eq 0 -a $p1 -eq 0 ]; then
         echo "WARNING: patch '$p' can be applied -p0 or -p1. Using -p0."
         patchlevel=-p0
      fi
      if [ -z "$patchlevel" ]; then
         echo "Unable to apply patch '$p'"
         exit 1
      fi
      echo "Applying patch '$p'..."
      patch -d $SPKDIR $patchlevel -E < $p
      if [ $? -ne 0 ]; then
         echo "Patch $p failed. Aborting."
         exit 1
      fi
      ;;


={============================================================================
*kt_linux_tool_031* zip and tar

{zip}
-d --decompress --uncompress 
-l --list

" maintain the original file
       -c --stdout --to-stdout
              Write output on standard output; keep original files unchanged.  If there are
              several input files, the output consists of a sequence of independently  com-
              pressed  members.  To  obtain better compression, concatenate all input files
              before compressing them.

gzip -c ramdisk_rootfs_img > ramdisk_rootfs_img.gz


{tar}
# maintain permissions when create a archive
% tar cvzfp xxx.tgz ./

# to extract
% tar zxvf *.tgz

-f, --file ARCHIVE
use archive file or device ARCHIVE

# '-' is used instead of filename after -f
# to extract from stdin. wget write to stdout and tar read from stdin. '-' used differently.
wget http://xxx.tar.bz2 -O - | tar -xjf -

" to extract bz file
% tar xjf *.bz2

" to create bzip2
% tar cvjfp filename
% bzip2 -d gdb.bz2

" to list
% tar tvf 

<C-option>
-C changes dir

tar -cjvf "$SPK_TARBALL_MW" -C nds-mw.

Since when run $nds-mw> tar -cjvf xx.bz . then there will be xx.bz which is empty in the archive. To
get around this, use -C to move dir and do zip but the output will be make before moving a dir.

<symlink>
When use tar on dir which is a symlink, then tar don not work. Any options?


={============================================================================
*kt_linux_tool_032* split

       split [OPTION]... [INPUT [PREFIX]]

       Output  fixed-size  pieces of INPUT to PREFIXaa, PREFIXab, ...; default size is 1000
       lines, and default PREFIX is 'x'.  

       -b, --bytes=SIZE
              put SIZE bytes per output file

$ split -b 200MB build-full-after-clean.log build-full-after-clean.

build-full-after-clean.aa
...
build-full-after-clean.ci


={============================================================================
*kt_linux_tool_033* getconf

       getconf - Query system configuration variables

       getconf [-v specification] system_var

       -a
       Displays all configuration variables for the current system
       and their values.

       -v
       Indicate the specification and version for which to obtain
       configuration variables.

       system_var
       A system configuration variable, as defined by sysconf(3) or
       confstr(3).

$ getconf PAGESIZE
4096


={============================================================================
*kt_linux_tool_034* ps

" to get gpid of the shell
kpark@wll1p04345:~/work$ ps -o pgid $$
 PGID
 7523


$ ps -p $$ -o 'pid pgid sid command'
  PID  PGID   SID COMMAND
 3697  3697  3697 bash


$ ps -ewf


={============================================================================
*kt_linux_tool_035* wget

{wget}
wget http://xxx.tar.bz2 -O - | tar -xjf -

-O file
--output-document=file
The documents will not be written to the appropriate files, but all will be concatenated together
and written to file. If - is used as file, documents will be printed to standard output, disabling
link conversion. 


{curl}
curl  is  a  tool  to  transfer data from or to a server, using one of the supported protocols
(DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP,
 SFTP, SMTP, SMTPS, TELNET and TFTP). The command is designed to work without user interaction.

curl offers a busload of useful tricks like proxy support, user authentication, FTP upload, HTTP
post, SSL connections, cookies, file transfer resume and more. As  you  will  see below, the number
of features will make your head spin!

curl is powered by libcurl for all transfer-related features. See libcurl(3) for details.

curl --silent --show-error -o "$(basename $galliumurl)" "$galliumurl"


={============================================================================
*kt_linux_tool_036* nc

netcat is a simple unix utility which reads and writes data across network connections, using
TCP or UDP protocol. It is designed to be a reliable "back-end" tool that can be used directly or
easily driven by other programs and scripts. At the same time, it is a feature-rich network
debugging and exploration tool, since it can create almost any  kind of connection you would need
and has several interesting built-in capabilities. 


In the simplest usage, "nc host port" creates a TCP connection to the given port on the given target
host. Your standard input is then sent to the host, and anything that  comes back across the
connection is sent to your standard output. This continues indefinitely, until the network side of
the connection shuts down.  Note that this behavior is different from most other applications which
shut everything down and exit after an end-of-file on the standard input.


note: on host but not the target, when run nc without -q then no prompt back when run so need to
specify -q to get prompt back.

printf "D\t${key}\n\0U\t${key}\n\0" | nc -q 1 $box_ip $box_port

-q seconds   after EOF on stdin, wait the specified number of seconds and then quit. If seconds is
negative, wait forever.


={============================================================================
*kt_linux_tool_037* port checks

Type the following command to see list well-known of TCP and UDP port numbers:

$ less /etc/services
$ grep -w 80 /etc/services 


{port-numbers}
Typically port number less than 1024 are used by well know network servers such as Apache. Under
UNIX and Linux like oses root (super user) privileges are required to open privileged ports. Almost
all clients uses a high port numbers for short term use.

The port numbers are divided into three ranges:

1. Well Known Ports: those from 0 through 1023.
2. Registered Ports: those from 1024 through 49151
3. Dynamic and/or Private Ports: those from 49152 through 65535

You can increase local port range by typing the following command (Linux specific example):
# echo 1024 65535 > /proc/sys/net/ipv4/ip_local_port_range
#
You can also increase or decrease socket timeout (Linux specific example):
# echo 2000 > /proc/sys/net/ipv4/tcp_keepalive_time

# for debian pc and embedded linux
#
root# cat /proc/sys/net/ipv4/ip_local_port_range 
32768	61000

<to-see-open-ports>

netstat [address_family_options] [--tcp|-t] [--udp|-u] [--raw|-w] [--listening|-l] [--all|-a]
[--numeric|-n] [--numeric-hosts] [--numeric-ports] [--numeric-users] [--symbolic|-N]
[--extend|-e[--extend|-e]] [--timers|-o] [--program|-p] [--verbose|-v] [--continuous|-c]

$ netstat -tulpn

To list open IPv4 connections use the lsof command:

$ lsof -Pnl +M -i4
COMMAND     PID     USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME
icedove    3900     1024   58u  IPv4 1836476      0t0  TCP 172.20.33.215:38273->132.245.226.18:993 (ESTABLISHED)
skype      4588     1024   11u  IPv4   15436      0t0  UDP 127.0.0.1:35553 
skype      4588     1024   35u  IPv4 1859784      0t0  TCP 172.20.33.215:51348->157.56.192.193:443 (ESTABLISHED)
skype      4588     1024   36w  IPv4   17138      0t0  TCP *:28890 (LISTEN)
skype      4588     1024   45u  IPv4   17139      0t0  UDP *:28890 
skype      4588     1024   54u  IPv4   18119      0t0  TCP 172.20.33.215:54124->157.55.235.145:40030 (ESTABLISHED)
skype      4588     1024   92u  IPv4   15527      0t0  TCP 172.20.33.215:41335->91.190.218.55:12350 (ESTABLISHED)
iceweasel  9539     1024   62u  IPv4 1585820      0t0  TCP 172.20.33.215:36101->216.58.208.69:443 (ESTABLISHED)
iceweasel  9539     1024   65u  IPv4 1856332      0t0  TCP 172.20.33.215:44701->74.125.195.189:443 (ESTABLISHED)
iceweasel  9539     1024   74u  IPv4 1813863      0t0  TCP 172.20.33.215:42211->31.221.26.57:80 (ESTABLISHED)
iceweasel  9539     1024   77u  IPv4 1545135      0t0  TCP 172.20.33.215:44615->185.45.5.50:443 (ESTABLISHED)
iceweasel  9539     1024   87u  IPv4 1839953      0t0  TCP 172.20.33.215:56696->185.45.5.43:443 (ESTABLISHED)
hipchat.b 11542     1024   33u  IPv4   57796      0t0  TCP 172.20.33.215:36851->54.161.161.10:5222 (ESTABLISHED)
ssh       19536     1024    3r  IPv4 1852553      0t0  TCP 172.20.33.215:42930->172.20.33.192:22 (ESTABLISHED)
ssh       19607     1024    3r  IPv4 1744563      0t0  TCP 172.20.33.215:43054->172.20.33.192:22 (ESTABLISHED)

To displays listening sockets (open ports)

$ netstat -nat | grep LISTEN

<firewall>
In other words, Apache port is open but it may be blocked by UNIX (pf) or Linux (iptables) firewall.
You also need to open port at firewall level. In this example, open tcp port 80 using Linux iptables
firewall tool:

$ /sbin/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT
$ service iptables save


={============================================================================
*kt_linux_tool_038* minicom

1. Setup minicon with -s switch

$ minicom -s

Choose Serial port setup and specify below settings, come out of Serial port setup by pressing Esc
and then save configuration file.

Once saved, it will store the configuration in /etc/minicom/. To connect to your device, run minicom
with config file name:

$ minicom cisco or minicom -c on cisco

To Exit from minicom, Click Ctrl, A, Z then X.

<when-cannot-get-shell-prompt>
Without knowing either your set-top box or your cable, I would first try disabling hardware flow
control, since the set top probably doesn't implement it. Essentially your Linux client is waiting
for an "OK to send" signal that it will never receive because there's no physical wire in the set
  top to send it


# ============================================================================
#{
={============================================================================
*kt_linux_tool_100* package: apt-xxx to get package

To search package:
apt-cache search <program name>

To install a package:
sudo apt-get install tk8.5 

To remove a package:
sudo apt-get purge tk8.5 


{update-and-upgrade}
note: The apt-get install command is recommended because it upgrades one or more already installed
packages without upgrading every package installed, whereas the apt-get upgrade command installs the
newest version of all currently installed packages. In additon, apt-get update command must be
executed before an upgrade to resynchronize the package index files.


{update-error}
When see:
Update Error: Require Installation Of Untrusted Packages

Run manually on console

sudo apt-get upgrade xxx

The simplest way to get the required packages is using apt-get build-dep wine respectively aptitude
build-dep wine. 


={============================================================================
*kt_linux_tool_101* package: pkg-config

The pkg-config program is used to retrieve information about installed libraries  in the system.  It
is typically used to compile and link against one or more libraries.  Here is a typical usage
scenario in a Makefile:

program: program.c
   cc program.c $(pkg-config --cflags --libs gnomeui)

pkg-config retrieves information about packages from special metadata  files.  These files  are
named after the package, and has a .pc extension.

It will additionally look in the colon-separated (on Windows, semicolon-separated) list of
directories  specified  by the PKG_CONFIG_PATH environment variable.

       --cflags
              This  prints pre-processor and compile flags required to compile the packages
              on the command line, including flags for all their  dependencies.  Flags  are
              "compressed"  so that each identical flag appears only once. pkg-config exits
              with a nonzero code if it can't find metadata for one or more of the packages
              on the command line.

       --cflags-only-I
              This  prints the -I part of "--cflags". That is, it defines the header search
              path but doesn't specify anything else.

       --libs This option is identical to "--cflags", only it prints  the  link  flags.  As
              with  "--cflags",  duplicate  flags are merged (maintaining proper ordering),
              and flags for dependencies are included in the output.

       --list-all
              List all modules found in the pkg-config path.


={============================================================================
*kt_linux_tool_102* package: dpkg and install deb file

dpkg - package manager for Debian

List all packages installed

$ dpkg-query -l

List packages using a search pattern: It is possible to add a search pattern to list packages: 

$ dpkg-query -l 'ibus*'

$ dpkg --info skype-debian_4.2.0.11-1_i386.deb 

# -i, --install
$ sudo dpkg --install skype-debian_4.2.0.11-1_i386.deb

<search>
aptitude search ^wine

aptitude remove "name"


={============================================================================
*kt_linux_tool_140*	cmake

<ref>
http://www.cmake.org/cmake/help/v2.8.9/cmake.html
http://stackoverflow.com/questions/6352123/multiple-directories-under-cmake

From:

./mheg
+-- CMakeLists.txt
+-- include
Â¦Â Â  +-- dbg.h
Â¦Â Â  +-- def.h
+-- main
Â¦Â Â  +-- main.c


To:

./mheg
+-- CMakeLists.txt
+-- include
Â¦Â Â  +-- dbg.h
Â¦Â Â  +-- def.h
+-- main
Â¦Â Â  +-- main.c
+-- mh5eng
Â¦Â Â  +-- CMakeLists.txt
Â¦Â Â  +-- sample.c
Â¦Â Â  +-- sample.h


The changes made to build:

../mheg/CMakeLists.txt

	 INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/mh5eng)

	 ADD_EXECUTABLE(${PROJECT_NAME}
		 main/main.c # uses sample.h
	 )

	 ADD_SUBDIRECTORY(mh5eng)

	 TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})

../mheg/mh5eng/CMakeLists.txt

	 SET(MH5ENG_HEADERS
		 sample.h
	 )

	 SET(MH5ENG_SOURCES
		 sample.c
	 )

	 ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES})

This create <libmh5eng.a> as a output


==============================================================================
*kt_linux_tool_141*	cmake: mix of c and cpp build

To compile the mix of c and cpp file, when try followings:

PROJECT(mhegproto C)

SET(MH5ENG_SOURCES
	xx.c
	mh5i_residentprogram_db.cpp
)

ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES} )

cmake do not compile cpp file although it is defined in the set variable. To make it built, must add
c++ build as below:

PROJECT(mhegproto C CXX)

From http://cmake.org/cmake/help/v2.8.8/cmake.html

project: Set a name for the entire project.

project(<projectname> [languageName1 languageName2 ... ] )

Sets the name of the project. Additionally this sets the variables <projectName>_BINARY_DIR and
<projectName>_SOURCE_DIR to the respective values.

Optionally you can specify which languages your project supports. Example languages are CXX (i.e.
C++), C, Fortran, etc. By default C and CXX are enabled. E.g. if you do not have a C++ compiler, you
can disable the check for it by explicitly listing the languages you want to support, e.g. C. By
using the special language "NONE" all checks for any language can be disabled. If a variable exists
called CMAKE_PROJECT_<projectName>_INCLUDE_FILE, the file pointed to by that variable will be
included as the last step of the project command.

note: by default? seems not.


==============================================================================
*kt_linux_tool_142*	cmake: cflags

To enable preprocessor, set -E as below.

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "-E ${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")


==============================================================================
*kt_linux_tool_143*	cmake: includes

Found that building a library in the subdir cannot find necessary includes.

root CMakeList.txt:

INCLUDE(FindPkgConfig)
pkg_check_modules(APPS_PKGS REQUIRED
	capi-appfw-application
	dlog
	edje
	elementary
	ecore-x
	evas
	utilX
	x11
	aul
	ail
)

ADD_EXECUTABLE(${PROJECT_NAME}
	main/main.c
	main/viewmgr.c
	main/view_main.c
)

ADD_SUBDIRECTORY(mh5eng)
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)

Here there is no problem to use headers from packages such as elementary but when add the same to
the one in /mh5eng then cannot find headers. When looked at flags used to build mh5eng, there is no
necessary -I. This is the same when add pkg_check_moudles in the mh5eng/CMakeList.txt. Thing is
build faild to update flags as expected.

The finding is that when move mh5eng then it builds without adding pkg_check_moudles in the
mh5eng/CMakeList.txt

...
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)
ADD_SUBDIRECTORY(mh5eng) [KT] moved to here

That suggests that where to put is important and not sure it is a GBS(git build system) or cmake
itself problem. 


Found that this error still happens when build cpp file and the solution is:

From:
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

To:
MESSAGE("KT >>>>>PKGS_LDFLAGS>>>>>" : ${APPS_PKGS_CFLAGS})
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")
SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${EXTRA_CFLAGS}")

Turns out that PKGS_CFLAGS will have necessary includes depending on pkg selection.


={============================================================================
*kt_linux_tool_144* cmake: link group

To solve {cyclic-dependencies} in link, can use this:

From defining each: 
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
...

To use group:
TARGET_LINK_LIBRARIES(${PROJECT_NAME} -Wl,--start-group mhdebug pfm mh5eng mh5dec mhv mah
-Wl,--end-group ${APPS_PKGS_LDFLAGS})

 On 05/19/2011 11:11 AM, Anton Sibilev wrote:
> Hello!
> I'm wondering how I can use "--start-group archives --end-group"
> linker flags with "Unix Makefiles".
> May be somebody know the right way?

You might specify these flags immediately in TARGET_LINK_LIBRARIES():

CMAKE_MINIMUM_REQUIRED(VERSION 2.8 FATAL_ERROR)
PROJECT(LINKERGROUPS C)
SET(CMAKE_VERBOSE_MAKEFILE ON)
FILE(WRITE ${CMAKE_BINARY_DIR}/f.c "void f(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g1.c "void g1(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g2.c "void g2(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/h.c "void h(void){}\n")
ADD_LIBRARY(f STATIC f.c)
ADD_LIBRARY(g1 STATIC g1.c)
ADD_LIBRARY(g2 STATIC g2.c)
ADD_LIBRARY(h STATIC h.c)
FILE(WRITE ${CMAKE_BINARY_DIR}/main.c "int main(void){return 0;}\n")
ADD_EXECUTABLE(main main.c)
TARGET_LINK_LIBRARIES(main f -Wl,--start-group g1 g2 -Wl,--end-group h)

However, do you really need these flags? Refer to the target properties
[IMPORTED_]LINK_INTERFACE_MULTIPLICITY[_<CONFIG>] and the documentation
of TARGET_LINK_LIBRARIES().

Regards,

Michael

LINK_INTERFACE_MULTIPLICITY

Repetition count for STATIC libraries with cyclic dependencies.

When linking to a STATIC library target with cyclic dependencies the linker may need to scan more
than once through the archives in the strongly connected component of the dependency graph. CMake by
default constructs the link line so that the linker will scan through the component at least twice.
This property specifies the minimum number of scans if it is larger than the default. CMake uses the
largest value specified by any target in a component.


={============================================================================
*kt_linux_tool_150* automake: LDADD vs LDFLAGS

In short LDFLAGS is added before the object files on the command line and LDADD is added afterwards.

<fail-case>
nexus_inspect_LDADD = \
   @NEXUS_LIBS@            note: this has -Wl,--as-needed

nexus_inspect_LDFLAGS = \
   -lpthread \
   -linit \
   -rdynamic

-Wl,--as-needed -L/data/builds/DEVARCH-8092/huawei.370/zinc-install-root/release/huawei-bcm7409/lib
...
-lnexusMgr -ldirectfb -lfusion -ldirect -linit -lnexus -lz -lpthread

<okay-case>
nexus_inspect_LDFLAGS = @NEXUS_LIBS@

So -lxxx and then --as-needed

-lnexusMgr -ldirectfb -lfusion -ldirect -linit -lnexus -lz -lpthread
...
-Wl,--as-needed -L/data/builds/DEVARCH-8092/huawei.370/zinc-install-root/release/huawei-bcm7409/lib

So not sure how automake combines all but it makes different order. This caues the problem because
shared libraries after as-needed may not linked at the time of linking and cause undefined symbol
when making executable binary.


={============================================================================
*kt_linux_tool_200* binutil:

{gcc-binutil}
http://sourceware.org/binutils/docs-2.20/
http://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html

GCC provides a large number of built-in functions other than the ones mentioned above. Some of these
are for internal use in the processing of exceptions or variable-length argument lists and are not
documented here because they may change from time to time; we do not recommend general use of these
functions. 


={============================================================================
*kt_linux_tool_201* binutil: readelf

Program Header in ELF

Executable and shared object files statically represent programs. To execute such programs, the
system uses the files to create dynamic program representations, or process images. A process image
has segments that hold its text, data, stack, and so on. The major sections in this part discuss the
following.

The primary data structure, a program header table, locates segment images within the file and
contains other information necessary to create the memory image for the program. Each entry
describing a segment or other information the system needs to prepare the program for execution. An
object file segment contains one or more sections.

Program headers are meaningful only for executable and shared object files.

<all-headers>
-e : headers
-e --headers           Equivalent to: -h -l -S
  -h --file-header       Display the ELF file header
  -l --program-headers   Display the program headers
     --segments          An alias for --program-headers
  -S --section-headers   Display the sections' header
     --sections          An alias for --section-headers

# mipsel-linux-uclibc-readelf -e vmlinux
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           MIPS R3000
  Version:                           0x1
  Entry point address:               0x802de000
  Start of program headers:          52 (bytes into file)
  Start of section headers:          3158368 (bytes into file)
  Flags:                             0x50001001, noreorder, o32, mips32
  Size of this header:               52 (bytes)
  Size of program headers:           32 (bytes)
  Number of program headers:         1
  Size of section headers:           40 (bytes)
  Number of section headers:         21
  Section header string table index: 18

Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .text             PROGBITS        80001000 001000 26f89c 00  AX  0   0 2048
  [ 2] __ex_table        PROGBITS        802708a0 2708a0 001a68 00   A  0   0  4
  [ 3] .rodata           PROGBITS        80272310 272310 02b8e0 00   A  0   0 16
  [ 4] .pci_fixup        PROGBITS        8029dbf0 29dbf0 0002e0 00   A  0   0  4
  [ 5] __ksymtab         PROGBITS        8029ded0 29ded0 0039a0 00   A  0   0  4
  [ 6] __ksymtab_gpl     PROGBITS        802a1870 2a1870 000678 00   A  0   0  4
  [ 7] __ksymtab_strings PROGBITS        802a1ee8 2a1ee8 009080 00   A  0   0  4
  [ 8] __param           PROGBITS        802aaf68 2aaf68 0001f4 00   A  0   0  4
  [ 9] .data             PROGBITS        802ac000 2ac000 02fa10 00  WA  0   0 8192
  [10] .data.cacheline_a PROGBITS        802dc000 2dc000 0010c0 00  WA  0   0 32
  [11] .init.text        PROGBITS        802de000 2de000 021bbc 00  AX  0   0  4
  [12] .init.data        PROGBITS        802ffbc0 2ffbc0 0025e4 00  WA  0   0  8
  [13] .init.setup       PROGBITS        803021b0 3021b0 0001d4 00  WA  0   0  4
  [14] .initcall.init    PROGBITS        80302384 302384 000168 00  WA  0   0  4
  [15] .con_initcall.ini PROGBITS        803024ec 3024ec 000004 00  WA  0   0  4
  [16] .init.ramfs       PROGBITS        80303000 303000 000085 00   A  0   0  1
  [17] .bss              NOBITS          80304000 303085 024020 00  WA  0   0 4096
  [18] .shstrtab         STRTAB          00000000 303085 0000d8 00      0   0  1
  [19] .symtab           SYMTAB          00000000 3034a8 03a800 10     20 10448  4
  [20] .strtab           STRTAB          00000000 33dca8 044b17 00      0   0  1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings)
  I (info), L (link order), G (group), x (unknown)
  O (extra OS processing required) o (OS specific), p (processor specific)

Program Headers:
  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
  LOAD           0x001000 0x80001000 0x80001000 0x302085 0x327020 RWE 0x2000

 Section to Segment mapping:
  Segment Sections...
   00     .text __ex_table .rodata .pci_fixup __ksymtab __ksymtab_gpl __ksymtab_strings __param
   .data .data.cacheline_aligned .init.text .init.data .init.setup .initcall.init .con_initcall.init
   .init.ramfs .bss


# mipsel-linux-uclibc-readelf -e xx.a
File: /home/NDS-UK/parkkt/platforms/mstar/libverifier.a(pairglbo.o)
ELF Header:
  Magic:   7f 45 4c 46 01 02 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, big endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              REL (Relocatable file)
  Machine:                           MIPS R3000
  Version:                           0x1
  Entry point address:               0x0
  Start of program headers:          0 (bytes into file)
  Start of section headers:          5304 (bytes into file)
  Flags:                             0x70001005, noreorder, cpic, o32, mips32r2
  Size of this header:               52 (bytes)
  Size of program headers:           0 (bytes)
  Number of program headers:         0
  Size of section headers:           40 (bytes)
  Number of section headers:         14
  Section header string table index: 11

Section Headers:

There are no program headers in this file.

File: /home/NDS-UK/parkkt/platforms/mstar/libverifier.a(p_posix.o)
ELF Header:
  Magic:   7f 45 4c 46 01 02 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, big endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              REL (Relocatable file)
  Machine:                           MIPS R3000
  Version:                           0x1
  Entry point address:               0x0
  Start of program headers:          0 (bytes into file)
  Start of section headers:          5180 (bytes into file)
  Flags:                             0x70001005, noreorder, cpic, o32, mips32r2
  Size of this header:               52 (bytes)
  Size of program headers:           0 (bytes)
  Number of program headers:         0
  Size of section headers:           40 (bytes)
  Number of section headers:         15
  Section header string table index: 12

Section Headers:


<speficy-section>
mipsel-linux-uclibc-readelf -x 13 vmlinux

<sections>
The .text section contains the executable program code. 

The .rodata section contains constant data in your program. 

The .data section generally contains initialized global data used by the C library prologue code and
can contain large initialized data items from your application.  The .sdata section is used for
smaller initialized global data items and exists only on some architectures. Some processor
architectures can make use of optimized data access when the attributes of the memory area are
known. The .sdata and .sbss sections enable these optimizations. 

The .bss and .sbss sections contain uninitialized (global) data in your program. These sections
occupy no space in the program image their memory space is allocated and initialized to zero on
program startup by C library prologue code.


{example-analysis}
/*
** This is sample program to see how elfs is made and allocated
**
** ktpark
*/

/* bss */
int kt_bss_vars[100];

/* data */
int kt_data_vars[100]={0x01};

/* constant */
char* kt_const = "this is constant string array.\n";

int main( int argc, char** argv )
{
  int kt_local_bss_vars[100];
  int kt_local_data_vars[100]={0xFF};
  char* kt_local_const = "this is local constant string array.\n";
  int i;

  printf("\n\n this is sample program to see elf.\n\n");

  for(i=0; i <= 10;i++)
  {
    sleep(1000);
  }

  printf("\nend of program.\n");
}
--

# free (before)
              total         used         free       shared      buffers
  Mem:       116472        18052        98420            0         8192
 Swap:            0            0            0
Total:       116472        18052        98420

# free (after)
              total         used         free       shared      buffers
  Mem:       116472        18068        98404            0         8192
 Swap:            0            0            0
Total:       116472        18068        98404

16K used.

<read-sections>
-S
--sections
--section-headers

Displays the information contained in the file's section headers, if it has any.


$ readelf -S a.out 
There are 26 section headers, starting at offset 0xfac:

Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .interp           PROGBITS        00400114 000114 000014 00   A  0   0  1
  [ 2] .reginfo          MIPS_REGINFO    00400128 000128 000018 18   A  0   0  4
  [ 3] .dynamic          DYNAMIC         00400140 000140 0000d8 08   A  6   0  4
  [ 4] .hash             HASH            00400218 000218 0000a4 04   A  5   0  4
  [ 5] .dynsym           DYNSYM          004002bc 0002bc 000160 10   A  6   1  4
  [ 6] .dynstr           STRTAB          0040041c 00041c 0000eb 00   A  0   0  1
  [ 7] .init             PROGBITS        00400508 000508 00008c 00  AX  0   0  4
  [ 8] .text             PROGBITS        004005a0 0005a0 000320 00  AX  0   0 16 (800)
  [ 9] .MIPS.stubs       PROGBITS        004008c0 0008c0 000050 00  AX  0   0  4
  [10] .fini             PROGBITS        00400910 000910 000050 00  AX  0   0  4
  [11] .rodata           PROGBITS        00400960 000960 000220 00   A  0   0 16 (544)
  [12] .eh_frame         PROGBITS        00400b80 000b80 000004 00   A  0   0  4
  [13] .ctors            PROGBITS        00410b84 000b84 000008 00  WA  0   0  4
  [14] .dtors            PROGBITS        00410b8c 000b8c 000008 00  WA  0   0  4
  [15] .jcr              PROGBITS        00410b94 000b94 000004 00  WA  0   0  4
  [16] .data             PROGBITS        00410ba0 000ba0 0001d0 00  WA  0   0 16 (464)
  [17] .rld_map          PROGBITS        00410d70 000d70 000004 00  WA  0   0  4
  [18] .got              PROGBITS        00410d80 000d80 00004c 04 WAp  0   0 16
  [19] .bss              NOBITS          00410dd0 000dcc 0001b0 00  WA  0   0 16 (432)
  [20] .comment          PROGBITS        00000000 000dcc 00005a 00      0   0  1
  [21] .mdebug.abi32     PROGBITS        0000005a 000e26 000000 00      0   0  1
  [22] .pdr              PROGBITS        00000000 000e28 0000c0 00      0   0  4
  [23] .shstrtab         STRTAB          00000000 000ee8 0000c3 00      0   0  1
  [24] .symtab           SYMTAB          00000000 0013bc 0004b0 10     25  46  4
  [25] .strtab           STRTAB          00000000 00186c 000254 00      0   0  1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings)
  I (info), L (link order), G (group), x (unknown)
O (extra OS processing required) o (OS specific), p (processor specific)

<read-rodata>
$ readelf -x 11 a.out (rodata)

Hex dump of section '.rodata':
  0x00400960 746e6174 736e6f63 20736920 73696874 this is constant
  0x00400970 000a2e79 61727261 20676e69 72747320  string array...
  0x00400980 00000000 00000000 00000000 000000ff ................
  0x00400990 00000000 00000000 00000000 00000000 ................
  0x004009a0 00000000 00000000 00000000 00000000 ................
  0x004009b0 00000000 00000000 00000000 00000000 ................
  0x004009c0 00000000 00000000 00000000 00000000 ................
  0x004009d0 00000000 00000000 00000000 00000000 ................
  0x004009e0 00000000 00000000 00000000 00000000 ................
  0x004009f0 00000000 00000000 00000000 00000000 ................
  0x00400a00 00000000 00000000 00000000 00000000 ................
  0x00400a10 00000000 00000000 00000000 00000000 ................
  0x00400a20 00000000 00000000 00000000 00000000 ................
  0x00400a30 00000000 00000000 00000000 00000000 ................
  0x00400a40 00000000 00000000 00000000 00000000 ................
  0x00400a50 00000000 00000000 00000000 00000000 ................
  0x00400a60 00000000 00000000 00000000 00000000 ................
  0x00400a70 00000000 00000000 00000000 00000000 ................
  0x00400a80 00000000 00000000 00000000 00000000 ................
  0x00400a90 00000000 00000000 00000000 00000000 ................
  0x00400aa0 00000000 00000000 00000000 00000000 ................
  0x00400ab0 00000000 00000000 00000000 00000000 ................
  0x00400ac0 00000000 00000000 00000000 00000000 ................
  0x00400ad0 00000000 00000000 00000000 00000000 ................
  0x00400ae0 00000000 00000000 00000000 00000000 ................
  0x00400af0 00000000 00000000 00000000 00000000 ................
  0x00400b00 00000000 00000000 00000000 00000000 ................
  0x00400b10 6f63206c 61636f6c 20736920 73696874 this is local co
  0x00400b20 72612067 6e697274 7320746e 6174736e nstant string ar
  0x00400b30 20736968 74200a0a 0000000a 2e796172 ray....... this 
  0x00400b40 6172676f 72702065 6c706d61 73207369 is sample progra
  0x00400b50 000a0a2e 666c6520 65657320 6f74206d m to see elf....
  0x00400b60 2e6d6172 676f7270 20666f20 646e650a .end of program.
  0x00400b70 00000000 00000000 00000000 0000000a ................


{example-analysis}
#include <stdio.h>

int bss_var; /* Uninitialized global variable */
int data_var = 1; /* Initialized global variable */

int main(int argc, char **argv)
{
  void *stack_var; /* Local variable on the stack */
  stack_var = (void *)main; /* Don't let the compiler optimize it out */

  printf("Hello, World! Main is executing at %p\n", stack_var);
  printf("This address (%p) is in our stack frame\n", &stack_var);

  /* bss section contains uninitialized data */
  printf("This address (%p) is in our bss section\n", &bss_var);

  /* data section contains initializated data */
  printf("This address (%p) is in our data section\n", &data_var);

  return 0;
}

root@amcc:~# ./hello
Hello, World! Main is executing at 0x10000418
This address (0x7ff8ebb0) is in our stack frame
This address (0x10010a1c) is in our bss section
This address (0x10010a18) is in our data section


{section-and-nm-map}
(from the readelf of kernel)
Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .init             PROGBITS      40018000 008000 01c000 00 WAX  0   0 32
  [ 2] .text             PROGBITS      40034000 024000 2af998 00  AX  0   0 32
  [ 3] .pci_fixup        PROGBITS        402e4000 2d4000 000490 00   A  0   0  4
  [ 4] __ksymtab         PROGBITS        402e4490 2d4490 004020 00   A  0   0  4
  [ 5] __ksymtab_gpl     PROGBITS        402e84b0 2d84b0 000f40 00   A  0   0  4
  [ 6] __ksymtab_gpl_fut PROGBITS        402e93f0 2d93f0 000018 00   A  0   0  4
  [ 7] __ksymtab_strings PROGBITS        402e9408 2d9408 00b4d8 00   A  0   0  4
  [ 8] __param           PROGBITS        402f5000 2e5000 0004b0 00   A  0   0  4
  [ 9] .data             PROGBITS        402f8000 2e8000 05c210 00  WA  0   0 32
  [10] .bss              NOBITS          40354220 344210 020438 00  WA  0   0 32
  [11] .comment          PROGBITS        00000000 344210 002e68 00      0   0  1
  [12] .ARM.attributes   ARM_ATTRIBUTES  00000000 347078 000010 00      0   0  1
  [13] .debug_abbrev     PROGBITS        00000000 347088 0b55ee 00      0   0  1
  [14] .debug_info       PROGBITS        00000000 3fc676 16124fa 00      0   0  1
  [15] .debug_line       PROGBITS        00000000 1a0eb70 17d764 00      0   0  1
  [16] .debug_pubnames   PROGBITS        00000000 1b8c2d4 0210e5 00      0   0  1
  [17] .debug_str        PROGBITS        00000000 1bad3b9 09173a 01  MS  0   0  1
  [18] .debug_aranges    PROGBITS        00000000 1c3eaf3 006408 00      0   0  1
  [19] .debug_frame      PROGBITS        00000000 1c44efc 05b4b0 00      0   0  4
  [20] .debug_loc        PROGBITS        00000000 1ca03ac 23ee57 00      0   0  1
  [21] .debug_ranges     PROGBITS        00000000 1edf203 069400 00      0   0  1
  [22] .shstrtab         STRTAB          00000000 1f48603 000113 00      0   0  1
  [23] .symtab           SYMTAB          00000000 1f48b00 07f8a0 10     24 26436  4
  [24] .strtab           STRTAB          00000000 1fc83a0 05e3c9 00      0   0  1
  
(from the map)
0000000040034000 T _text
00000000402f54b0 A _etext
00000000402f54b0-0000000040034000=0x2C14B0(2,888,880)

Why these are different in size??


={============================================================================
*kt_linux_tool_202* binutil: nm

{nm-find-symbol}

%nm *.a

%/opt/toolchains/bin/mipsel-linux-uclibc-nm
%nm libicammulti.a | grep vendor

00000bc0 t _GLOBAL__I_vendor_init
00000980 T vendor_cleanup
00000000 T vendor_init
000002c8 T vendor_setup
000000bc r _ZZ12vendor_setupE12__FUNCTION__
00000088 r _ZZ12vendor_setupE19__PRETTY_FUNCTION__


$ nm -A /usr/lib/lib*.so 2> /dev/null | grep ' crypt$'
/usr/lib/libcrypt.so:00007080 W crypt

<help>
% man nm

-A
-o
--print-file-name
Precede each symbol by the name of the input file (or archive member) in which it was found, rather
than identifying the input file once only, before all of its symbols.


Uppercase means global and lowercase means local. 

B        is bss
T/t      is text and means symbols defined in this object file. 
D        data
A        is this address is absolute and is not subject to modification by an additional link stage
U        is undefined meaning extern

Cannot use nm for a stripped object


={============================================================================
*kt_linux_tool_203* binutil: objdump

gcc -S
objdump -d ELF > out.txt

note: difference between gcc -S and objdump? objdump do on objects after complilation.

objdump
-d, --disassemble        Display assembler contents of executable sections
-S, --source             Intermix source code with disassembly


={============================================================================
*kt_linux_tool_204* binutil: addr2line

Usage: ./mipsel-linux-uclibc-addr2line [option(s)] [addr(s)]

Convert addresses into line number/file name pairs. If no addresses are specified on the command
line, they will be read from stdin The options are:

  @<file>                Read options from <file>
  -b --target=<bfdname>  Set the binary file format
  -e --exe=<executable>  Set the input file name (default is a.out)
  -i --inlines           Unwind inlined functions
  -j --section=<name>    Read section-relative offsets instead of addresses
  -s --basenames         Strip directory names
  -f --functions         Show function names
  -C --demangle[=style]  Demangle function names
  -h --help              Display this information
  -v --version           Display the program's version

./mipsel-linux-uclibc-addr2line: supported targets: elf32-tradlittlemips elf32-tradbigmips
ecoff-littlemips ecoff-bigmips elf32-ntradlittlemips elf64-tradlittlemips elf32-ntradbigmips
elf64-tradbigmips elf64-little elf64-big elf32-little elf32-big srec symbolsrec tekhex binary ihex
Report bugs to URL:http://www.sourceware.org/bugzilla/

If the file name or function name can not be determined, addr2line will print two question marks in
their place. If the line number can not be determined, addr2line will print 0


={============================================================================
*kt_linux_tool_205* binutil: ld

https://sourceware.org/binutils/docs/ld/index.html#Top
This file documents the gnu linker ld (GNU Binutils) version 2.25. 

<debian>
keitee@debian-keitee:~/work/xxx$ which ld
/usr/bin/ld
keitee@debian-keitee:~/work/xxx$ ld --version
GNU ld (GNU Binutils for Debian) 2.22
Copyright 2011 Free Software Foundation, Inc.


# ============================================================================
#{
={============================================================================
*kt_linux_tool_200*  gdb: core dump setting

{setting}
# default
-sh-3.2# cat /proc/sys/kernel/core_pattern 
core

<core-pattern>
# set core dump location and format
echo '/tmp/%p.COR' >/proc/sys/kernel/core_pattern

the following pattern elements in the core_pattern file:

%p: pid
%: '%' is dropped
%%: output one '%'
%u: uid
%g: gid
%s: signal number
%t: UNIX time of dump
%h: hostname
%e: executable filename
%: both are dropped

# configure it forever

The changes done before are only applicable until the next reboot. In order to make the change in
all future reboots, you will need to add the following in /etc/sysctl.conf

# own core file pattern...
kernel.core_pattern=/tmp/cores/core.%e.%p.%h.%t

sysctl.conf is the file controlling every configuration under /proc/sys

Just wanted to say that there is no need to edit the file manually. simply run the sysctl command,
which does the stuff

<conditions-to-check>
0. configured for cross platform

1. write permissions in the directory 

2. ulimit -c unlimited
this is shell command to set resource limit for a core.
-c     The maximum size of core files created 

3. compilation of the image: should be with debug symbols (not release version), and statically
compiled. In cygwin compilations, static link is enabled by default. In linux compilations, you
should modify platform.mk in your view.

If instead of the callstack you see only "??", it usually means the application wasn't
compiled with debug symbols (release version) or it was not compiled using static linkage

4. debug build
can use gdb without -g but need to map virtual address.

mkdir -p /tmp/cores
chmod a+rwx /tmp/cores
echo "/tmp/cores/core.%e.%p.%h.%t" > /proc/sys/kernel/core_pattern
 
<force-core>
ulimit -c unlimited

# SIGSEGV 	11 	Core 	Invalid memory reference 
# -s signal Specify the signal to send.  The signal may be given as a signal name or number.
 
kill -s SIGSEGV $$
kill -s 11 113

export P=`ps -a | grep APP_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;
export P=`ps -a | grep MW_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;

SIG_KERNEL_COREDUMP_MASK (.../kernel/signal.c) defines sianals to create a core.

SIGSEGV(segmentation fault)

#define SIG_KERNEL_COREDUMP_MASK (\
        M(SIGQUIT)   |  M(SIGILL)    |  M(SIGTRAP)   |  M(SIGABRT)   | \
        M(SIGFPE)    |  M(SIGSEGV)   |  M(SIGBUS)    |  M(SIGSYS)    | \
        M(SIGXCPU)   |  M(SIGXFSZ)   |  M_SIGEMT                     )


={============================================================================
*kt_linux_tool_201*  gdb: core dump analysis {frame-command}

export LD_LIBRARY_PATH=/home/NDS-UK/parkkt/bins
mips-gdb
set solib-absolute-prefix /junk
set solib-search-path /home/NDS-UK/parkkt/com.nds.darwin.debugsupport/debug_libs/uClibc-nptl-0.9.29-20070423
file APP_Process 
core 272.COR
thread apply all bt full
bt

(gdb) f n

<gdb-frame-command>
frame n 

Select frame number n. Recall that frame zero is the innermost (currently executing) frame, frame
one is the frame that called the innermost one, and so on. The highest-numbered frame is the one for
main.


# 01
#

(gdb) bt
#0 memset () at libc/string/mips/memset.S:132

(gdb) i r
	zero     at       v0         v1       a0         a1       a2       a3
R0	00000000 fffffffc [00000000] ffffffff [00000008] 00000000 00000004 00022000
	t0 t1 t2 t3 t4 t5 t6 t7
R8	00000004 00000000 ffffffff 000000c2 00000000 00000004 00a52630 00000000
	s0 s1 s2 s3 s4 s5 s6 s7
R16 00020000 00000000 00022004 2c57b174 00000004 2c70d4d0 2d9dcd30 00be6f70
	t8 t9 k0 k1 gp sp s8 ra
R24 00befcc8 2ab25160 00000000 00000000 00bf7e60 2d9dc858 2d9dcbd0 00993fdc
	sr       lo       hi       bad        cause    pc
	00008413 00000000 00000000 [00000000] 0080000c [2ab251b4]
	fsr fir
	00001004 00000000

# void *memset(void *s, int c, size_t n);

(gdb) disassemble $pc
Dump of assembler code for function memset:
0x2ab25160 <memset+0>: slti t1,a2,8
0x2ab25164 <memset+4>: bnez t1,0x2ab251d4 <memset+116>
-> 0x2ab25168 <memset+8>: move v0,a0
0x2ab2516c <memset+12>: beqz a1,0x2ab25184 <memset+36>
0x2ab25170 <memset+16>: andi a1,a1,0xff
0x2ab25174 <memset+20>: sll t0,a1,0x8
0x2ab25178 <memset+24>: or a1,a1,t0
0x2ab2517c <memset+28>: sll t0,a1,0x10
0x2ab25180 <memset+32>: or a1,a1,t0
0x2ab25184 <memset+36>: negu t0,a0 # negu d, s; d = -s;
0x2ab25188 <memset+40>: andi t0,t0,0x3
0x2ab2518c <memset+44>: beqz t0,0x2ab2519c <memset+60>
0x2ab25190 <memset+48>: subu a2,a2,t0
0x2ab25194 <memset+52>: swl a1,0(a0) # swl t, addr; Store word left/right
0x2ab25198 <memset+56>: addu a0,a0,t0
0x2ab2519c <memset+60>: andi t0,a2,0x7
0x2ab251a0 <memset+64>: beq t0,a2,0x2ab251c0 <memset+96>
0x2ab251a4 <memset+68>: subu a3,a2,t0
0x2ab251a8 <memset+72>: addu a3,a3,a0
0x2ab251ac <memset+76>: move a2,t0
-> 0x2ab251b0 <memset+80>: addiu a0,a0,8
-> 0x2ab251b4 <memset+84>: sw a1,-8(a0)
0x2ab251b8 <memset+88>: bne a0,a3,0x2ab251b0 <memset+80>

this shows that a0, first arg, was null. hence SEGFLT

# 02
#

(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc38, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x007f09c8 in MHWMemAllocStatic (pool=0x131bc38, size=64) at mem_static.c:63
#2  0x007e6854 in MEMMAN_API_AllocStaticP (pool=0x131bc38, size=41) at memman_st.c:350
#3  0x00419328 in DIAG_JAVA_GetJavaString (env=0x2d7056c0, l_java_string=0x2c4b3358, l_byte_array=0x2d705698, 
    l_len=0x2d70569c) at ../src/natdiag.c:63
#4  0x004197fc in DIAG_JAVA_GetJavaString (env=0x131bc38) at ../src/natdiag.c:341
#5  0x0041a0f8 in Java_com_nds_fusion_diagimpl_DiagImpl_natLogInfo (THIS=<value optimized out>, jClass=0x40, jInt=1, 
    jString=0x0) at sunnatdiag.c:177

[New process 124]
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
271     mem_blockpool.c: No such file or directory.
        in mem_blockpool.c

(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x72617469 in ?? ()
Backtrace stopped: previous frame inner to this frame (corrupt stack?)

(gdb) i r
          zero       at       v0       v1       a0       a1       a2       a3
 R0   00000000 00000005 00000004 00000000 0131bc40 00000040 00000001 00000000 
            t0       t1       t2       t3       t4       t5       t6       t7
 R8   0000000c 00000001 ffffffff ffffffff 00000000 00000000 00d7f220 00000200 
            s0       s1       s2       s3       s4       s5       s6       s7
 R16  00000000 0131bc40 0131bc40 00000040 00000001 00000000 00000001 2d705b00 
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00f0c360 00b62d98 00000000 00000000 00f14570 2d7055a0 2d7057d8 007f09c8 
            sr       lo       hi      bad    cause       pc
      00008413 000efdcb 00000005 00000018 00800008 007eb73c 
           fsr      fir
      00001004 00000000 

(gdb) disassemble $pc
# source
# uint32_t MHWMemCheckBank(MEMMAN_API_MemoryPoolHandle * mpool, uint32_t size, # uint32_t mem_nb_bank)
# {
#     MemWholeMemory *bank = mpool->s_WholeMem;
# }
#
Dump of assembler code for function MHWMemCheckBank:
0x007eb6b8 <MHWMemCheckBank+0>: addiu   sp,sp,-48
0x007eb6bc <MHWMemCheckBank+4>: sw      s4,40(sp)
0x007eb6c0 <MHWMemCheckBank+8>: sw      s3,36(sp)
0x007eb6c4 <MHWMemCheckBank+12>:        sw      s2,32(sp)
0x007eb6c8 <MHWMemCheckBank+16>:        sw      ra,44(sp)
0x007eb6cc <MHWMemCheckBank+20>:        sw      s1,28(sp)
0x007eb6d0 <MHWMemCheckBank+24>:        sw      s0,24(sp)
0x007eb6d4 <MHWMemCheckBank+28>:        lw      s0,40(a0)
0x007eb6d8 <MHWMemCheckBank+32>:        lw      v1,36(a0)
0x007eb6dc <MHWMemCheckBank+36>:        lui     v0,0x4
0x007eb6e0 <MHWMemCheckBank+40>:        lw      a3,108(a0)
0x007eb6e4 <MHWMemCheckBank+44>:        and     v1,v1,v0
0x007eb6e8 <MHWMemCheckBank+48>:        addiu   t0,s0,12
0x007eb6ec <MHWMemCheckBank+52>:        addiu   v0,s0,4
0x007eb6f0 <MHWMemCheckBank+56>:        move    s2,a0
0x007eb6f4 <MHWMemCheckBank+60>:        movn    t0,v0,v1
0x007eb6f8 <MHWMemCheckBank+64>:        move    s3,a1
0x007eb6fc <MHWMemCheckBank+68>:        beqz    a3,0x7eb73c <MHWMemCheckBank+132>
0x007eb700 <MHWMemCheckBank+72>:        move    s4,a2
0x007eb704 <MHWMemCheckBank+76>:        lw      a1,0(t0)
0x007eb708 <MHWMemCheckBank+80>:        lw      v1,24(s0)
0x007eb70c <MHWMemCheckBank+84>:        lw      v0,104(a0)
0x007eb710 <MHWMemCheckBank+88>:        li      a2,100
0x007eb714 <MHWMemCheckBank+92>:        mul     v1,v1,a2
0x007eb718 <MHWMemCheckBank+96>:        mul     v0,a1,v0
0x007eb71c <MHWMemCheckBank+100>:       sltu    v0,v1,v0
0x007eb720 <MHWMemCheckBank+104>:       beqz    v0,0x7eb73c <MHWMemCheckBank+132>
0x007eb724 <MHWMemCheckBank+108>:       nop
0x007eb728 <MHWMemCheckBank+112>:       divu    zero,v1,a1
0x007eb72c <MHWMemCheckBank+116>:       teq     a1,zero,0x7
0x007eb730 <MHWMemCheckBank+120>:       mflo    a1
0x007eb734 <MHWMemCheckBank+124>:       jal     0x7efa14 <MEMMAN_SHL_Notify>
0x007eb738 <MHWMemCheckBank+128>:       subu    a1,a2,a1
-> 0x007eb73c <MHWMemCheckBank+132>:       lw      v0,24(s0)
...
---Type <return> to continue, or q <return> to quit---

(gdb) info locals
ind_bank = <value optimized out>
bank = (MemWholeMemory *) 0x0		#
pool_max = (uint32_t *) 0xc

(gdb) x/16wx 0x0131bc40 	# value of a0
0x131bc40:      0x0131f408      0x2ab97ca0      0x00000000      0x00000000
0x131bc50:      0x00000000      0x00000000      0x00000000      0x00000000
0x131bc60:      0xffffffff      0x00000000      0x00000000      0x41445054
0x131bc70:      0x5f444941      0x47000000      0x00000000      0x00000000

this shows that structure passed on a0 has some NULLs and means that this pool was already
destoried. when see destory func, it sets poolHandle->s_WholeMem=NULL;


={============================================================================
*kt_linux_tool_202* gdb: kernel crash analysis {frame-command}

# 01
#

The crash dump When a crash in a kernel module happens, you should see output like the
following on the serial port or in the dmesg buffer (just run the dmesg command to see
it). 

<4>Unhandled kernel unaligned access[#1]:
<4>Cpu 0
<4>$ 0   : 00000000 10008400 {f7ffdfdf} 80070000	# {v0}
<4>$ 4   : c06e27c0 000ee208 8123a000 898d2680
<4>$ 8   : 00000000 7edbffff ffdeff7f fffb7fff
<4>$12   : fdf7fed7 000ee247 00000001 00000001
<4>$16   : 898d2680 00000000 00000001 00000001
<4>$20   : 898d2680 c06e2800 898d2680 00000001
<4>$24   : 00000001 c0504bcc                  
<4>$28   : 89cd2000 89cd3830 000ee208 c050cbe4
<4>Hi    : 00000128
<4>Lo    : 003e5708
<4>epc   : c050cc54 XHddLowIO+0x124/0x3d8 [xtvfs]     Tainted: P    # {pc} 
<4>ra    : c050cbe4 XHddLowIO+0xb4/0x3d8 [xtvfs]
<4>Status: 10008403    KERNEL EXL IE 
<4>Cause : 00800010
<4>BadVA : f7ffe073
<4>PrId  : 0002a044
<4>Modules linked in: xtvfs mhxnvramfs callisto_periph callisto_tuner callisto
<4>Process mount (pid: 404, threadinfo=89cd2000, task=89a249e8)
<4>Stack : 00000001 000ee08d 00000000 c04e523c c05394c4 00000000 00000000 f7ffdfdf
<4>        c06e2800 003e5708 00000001 000ee208 00000000 c04e523c c05394c4 00000000
<4>        c053950c c050cf50 00808000 c050d21c c06e2800 8567bc00 00000000 003e5708
<4>        c04d7f78 c04d7f58 ff7fffdf 000edf25 00000001 00000001 c067e200 c04d804c
<4>        c05394c4 89cd3950 00000000 c04e523c 000ee208 c06e2800 00000001 00000000
<4>        ...
<4>Call Trace:
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]
<4>[<c050cf50>] XHddLowRead+0x1c/0x28 [xtvfs]
<4>[<c04d7f78>] root_dir_devio_read+0x58/0x12c [xtvfs]
<4>[<c04d809c>] root_dir_devio_lock_read+0x50/0x84 [xtvfs]
<4>[<c04d8430>] RootDirCpyClusterReadBuffer+0xec/0x180 [xtvfs]
<4>[<c04d90cc>] RootDirCpyCheckCreate+0xf0/0x1214 [xtvfs]
<4>[<c04da344>] RootDirCpyInit+0x154/0x200 [xtvfs]
<4>[<c04d2944>] pc_ppart_init+0x10bc/0x12a8 [xtvfs]
<4>[<c04fb178>] XTVFS_CheckPpartInit+0x38/0x32c [xtvfs]
<4>[<c04fc684>] InitPpart+0x238/0x540 [xtvfs]
<4>[<c04fca28>] XTVFS_Mount+0x9c/0x490 [xtvfs]
<4>[<c050c8c8>] xtvfs_read_super+0x1e0/0x370 [xtvfs]
<4>[<c050bb80>] xtvfs_fill_super+0x18/0x48 [xtvfs]
<4>[<8007a6b0>] get_sb_bdev+0x114/0x194
<4>[<c050bb5c>] xtvfs_get_sb+0x2c/0x38 [xtvfs]
<4>[<80079f2c>] vfs_kern_mount+0x68/0xc4
<4>[<80079fe4>] do_kern_mount+0x4c/0x7c
<4>[<80094f10>] do_mount+0x5a8/0x614
<4>[<80095010>] sys_mount+0x94/0xec
<4>[<8000e5f0>] stack_done+0x20/0x3c
<4>
<4>
<4>Code: 00008821  8fa2001c  3c038007 <8c440094> 24631824  0060f809  8c46000c  ae020000  27de0001


Unhandled kernel unaligned access An unaligned access is a type of crash. Unlike a
segmentation fault, where a process tries to read memory that is not in its memory map,
and unaligned access is an attempt to read or write an address that is not on a word
boundry. On 32 bit CPUs this means an address not divisble by 4. Often this will generate
an exception and some software will handle the access by reading adjacent words and
piecing things together. But in our case the exception is unhandled. 


$0, $4, etc

This output shows the value of the registers. We are on a MIPS CPU and many of the
registers have defined uses. This
document(http://msdn.microsoft.com/en-us/library/aa448706.aspx) describes them. For
example, $4 to $7 are used to store the first 4 words of function arguments when calling a
function. In assembler these registers are referred to as a0 to a3. You can't know this in
advance, you have to read the MIPS documentation to find it out.  epc c050cc54
XHddLowIO+0x124/0x3d8 [xtvfs] This is the {exception-program-counter}. It shows the address
that the exception occurred at, and that this is 0x124 bytes into the function XHddLowIO
in the module xtvfs.ko. 


Getting the disassembly

Given a kernel module like xtvfs.ko, it is possible to see the disassembled code using the
objdump -D command. Since we have a mips module, we use the cross-compiler from the
Clearcase Fusion view, so our command will look something like: 

mips-linux-uclibc-objdump -D xtvfs.ko

We can then look for the function where we crashed, which is XHddLowIO from the epc trace
above. It starts like this: 

00045b30 <XHddLowIO>:
   45b30:       27bdffb8        addiu   sp,sp,-72
   45b34:       3c020002        lui     v0,0x2
   45b38:       afb50034        sw      s5,52(sp)
   45b3c:       345500d0        ori     s5,v0,0xd0
   45b40:       3c020000        lui     v0,0x0
   45b44:       afb7003c        sw      s7,60(sp)

From the call trace we can calculate the address offset in use. Recall: 
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]

So c050cc54 - 0x124 - 45b30 = offset = c04c7000 (the start of loadded in memory for this
xtvfs.ko). The crash happened at c050cc54 which will appear as c050cc54 - c04c7000 = 45c54
in the disassembly. That code looks like this: 

or 45b30+0x124 = 45c54


   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)
   45c50:       3c030000        lui     v1,0x0
 {45c54}:       8c440094        lw      a0,148(v0)
   45c58:       24630000        addiu   v1,v1,0
   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

So we have crashed executing an lw instruction. 


Using the relocation table

Given a kernel module like xtvfs.ko, it is possible to see the offsets of functions (the
relocation table) using the objdump -r command. Since we have a mips module, we use the
cross-compiler from the Clearcase Fusion view, so our command will look something like: 

mips-linux-uclibc-objdump -r xtvfs.ko > xtvfs_relocations


The output near to our crash address of 45c54 is a table like this: 

 00045c20 R_MIPS_26         .text
 00045c2c R_MIPS_26         .text
 00045c44 R_MIPS_26         .text
 00045c50 R_MIPS_HI16       __getblk
 00045c58 R_MIPS_LO16       __getblk
 00045c78 R_MIPS_HI16       printk
 00045c80 R_MIPS_LO16       printk


Because we are using load time
relocation(http://eli.thegreenplace.net/2011/08/25/load-time-relocation-of-shared-libraries/)
of shared libraries, this table tells the operating system how to replace addresses in the
code. The first column is the address in the code, the second column is the type of
relocation to do, and the third column is the address to relocate. So the code at address
45c50 and 45c58 will get overwritten with the address of __getblk. That makes the
disassembly of the code near our crash look like this: 

   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)
   45c50:       3c030000        lui     v1,__getblk
   45c54:       8c440094        lw      a0,148(v0)
   45c58:       24630000        addiu   v1,v1,0
   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

Understanding __getblk

At 45c5c there is a jump instruction to v1 which has been loaded with the address of
__getblk. But we crashed immediately before that.

	So it seems we crashed while preparing to call __getblk. 
	
So it would help to understand this function. We can look it up in the kernel code: 
http://lxr.free-electrons.com/source/fs/buffer.c?v=2.6.30;a=mips#L1363

1362 /*
1363  * __getblk will locate (and, if necessary, create) the buffer_head
1364  * which corresponds to the passed block_device, block and size. The
1365  * returned buffer has its reference count incremented.
1366  *
1367  * __getblk() cannot fail - it just keeps trying.  If you pass it an
1368  * illegal block number, __getblk() will happily return a buffer_head
1369  * which represents the non-existent block.  Very weird.
1370  *
1371  * __getblk() will lock up the machine if grow_dev_page's try_to_free_buffers()
1372  * attempt is failing.  FIXME, perhaps?
1373  */
1374 struct buffer_head *
1375 __getblk(struct block_device *bdev, sector_t block, unsigned size)
1376 {
1377         struct buffer_head *bh = __find_get_block(bdev, block, size);
1378 
1379         might_sleep();
1380         if (bh == NULL)
1381                 bh = __getblk_slow(bdev, block, size);
1382         return bh;
1383 }
1384 EXPORT_SYMBOL(__getblk);

We should also notice that it can be called via inline function sb_blk : 

287 static inline struct buffer_head *
288 sb_getblk(struct super_block *sb, sector_t block)
289 {
290         return __getblk(sb->s_bdev, block, sb->s_blocksize);
291 }


Understanding where in our C code the crash happened

Now we can tell exactly where in our C code the crash happened. We know we were in the
function XHddLowIO from the call trace and now we know we were calling __getblk or
sb_getblk. In XHddLowIO in the XTVFS code we can see: 


/* allocate sector buffers */
for(i = 0; i < cnt; i++){
    bh_array[i] = sb_getblk(sb,  block++);
    if(!bh_array[i]){ /* no sufficient buffers */
        printk("\n BH_ArrayXHddLowIO: bh = 0, i = %d !!!!!", i);
        if(0 == i){ /* no at all */
            return X_ERROR;
        }
        /* use what we have */
        cnt = i;
    }

So it is likely that the crash happened very close to this sb_getblk call. 


Understanding the lw instruction

Recall the instruction that crashed: 

   45c54:       8c440094        lw      a0,148(v0)

What does that notation mean? We can look up information about the MIPS instructions:
Description: A word is loaded into a register from the specified address.  Operation: $t =
MEM[$s + offset]; advance_pc (4); Syntax: lw $t, offset($s) The whole instruction means:
load a0 with the address in v0 + 148. 


Understanding the MIPS registers

In MIPS, registers tend to have special functions, such as return addresses or function
parameters. We can read about this online. 

a0, the register we are writing to, is the first of the "function argument registers that
hold the first four words of integer type arguments." So a0 is the first argument to the
function we are calling.  v0 is a "function result register" and is also called $2. So we
know its value from the original trace: 

<4>$ 0   : 00000000 10008400 f7ffdfdf 80070000

It is f7ffdfdf. Which is an *odd number*. Since we are trying to read from this address
and do arithmetic (add 148) with it, this would explain why we get an unaligned access. 


Putting it all together

We are executing this line of C: 

bh_array[i] = sb_getblk(sb,  block++);

Because sb_getblk is an inline function, it has been expanded by the compiler into:
__getblk(sb->s_bdev, block, sb->s_blocksize); So our crashing instruction is adding 148
because 148 is the offset of s_bdev withing the sb struct. We hav verify this by looking
at struct super_block in the code.  However, sb has somehow become and odd number, and
THAT is our bug. 


={============================================================================
*kt_linux_tool_203* gdb: .gdbinit

{example-one}
can define user func that have commands to run

$ more .gdbinit
set history save on
set history filename ./.gdb_history
set output-radix 16

define connect
    handle SIG32 nostop noprint pass
    handle SIG33 nostop noprint pass
#    b CTL_SimpleZapperTestStep
#    b CTL_ChannelZapping_FullStbTearDown
#               b readSectionFilterDataAndWriteToFile
    b sectionFilterTask
                b CTL_SectionFilter_Engine.c:2126
                b CTL_SectionFilter_Engine.c:2146
    directory components/FOSH/FUSIONOS_XTV_TESTS/xtv_test/src/
                i b
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define init_task
   set $t=&init_task
   printf "task name \"%s\", pid %05d \n", $t->comm, t->pid
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define find_task
  # Addresses greater than _end: kernel data...
  # ...user passed in an address
  if ((unsigned)$arg0 > (unsigned)&_end)
    set $t=(struct task_struct *)$arg0
  else
    # User entered a numeric PID
    # Walk the task list to find it
    set $t=&init_task
    if (init_task.pid != (unsigned)$arg0)
      find_next_task $t
      while (&init_task!=$t && $t->pid != (unsigned)$arg0)
        find_next_task $t
      end
      if ($t == &init_task)
        printf "Couldn't find task; using init_task\n"
      end
    end
  end
  printf "Task \"%s\":\n", $t->comm
end


Reads and executes the commands from init file (if any) in the current working directory. This is
only done if the current directory is different from your home directory. Thus, you can have more
than one init file, one generic in your home directory, and another, specific to the program you are
debugging, in the directory where you invoke gdb.


{gdbinit-for-c++}
#
#   STL GDB evaluators/views/utilities - 1.03
#
#   The new GDB commands:
#       are entirely non instrumental
#       do not depend on any "inline"(s) - e.g. size(), [], etc
#       are extremely tolerant to debugger settings
#
#   This file should be "included" in .gdbinit as following:
#   source stl-views.gdb or just paste it into your .gdbinit file
#
#   The following STL containers are currently supported:
#
#       std::vector<T> -- via pvector command
#       std::list<T> -- via plist or plist_member command
#       std::map<T,T> -- via pmap or pmap_member command
#       std::multimap<T,T> -- via pmap or pmap_member command
#       std::set<T> -- via pset command
#       std::multiset<T> -- via pset command
#       std::deque<T> -- via pdequeue command
#       std::stack<T> -- via pstack command
#       std::queue<T> -- via pqueue command
#       std::priority_queue<T> -- via ppqueue command
#       std::bitset<n> -- via pbitset command
#       std::string -- via pstring command
#       std::widestring -- via pwstring command
#
#   The end of this file contains (optional) C++ beautifiers
#   Make sure your debugger supports $argc
#
#   Simple GDB Macros writen by Dan Marinescu (H-PhD) - License GPL
#   Inspired by intial work of Tom Malnar,
#     Tony Novac (PhD) / Cornell / Stanford,
#     Gilad Mishne (PhD) and Many Many Others.
#   Contact: dan_c_marinescu@yahoo.com (Subject: STL)
#
#   Modified to work with g++ 4.3 by Anders Elton
#   Also added _member functions, that instead of printing the entire class in map, prints a member.

# support for pending breakpoints - you can now set a breakpoint into a shared library before the it was loaded.
set breakpoint pending on

#
# std::vector<>
#

define pvector
    if $argc == 0
        help pvector
    else
        set $size = $arg0._M_impl._M_finish - $arg0._M_impl._M_start
        set $capacity = $arg0._M_impl._M_end_of_storage - $arg0._M_impl._M_start
        set $size_max = $size - 1
    end
    if $argc == 1
        set $i = 0
        while $i < $size
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
    end
    if $argc == 2
        set $idx = $arg1
        if $idx < 0 || $idx > $size_max
            printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
        else
            printf "elem[%u]: ", $idx
            p *($arg0._M_impl._M_start + $idx)
        end
    end
    if $argc == 3
      set $start_idx = $arg1
      set $stop_idx = $arg2
      if $start_idx > $stop_idx
        set $tmp_idx = $start_idx
        set $start_idx = $stop_idx
        set $stop_idx = $tmp_idx
      end
      if $start_idx < 0 || $stop_idx < 0 || $start_idx > $size_max || $stop_idx > $size_max
        printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
      else
        set $i = $start_idx
        while $i <= $stop_idx
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
      end
    end
    if $argc > 0
        printf "Vector size = %u\n", $size
        printf "Vector capacity = %u\n", $capacity
        printf "Element "
        whatis $arg0._M_impl._M_start
    end
end

document pvector
    Prints std::vector<T> information.
    Syntax: pvector <vector> <idx1> <idx2>
    Note: idx, idx1 and idx2 must be in acceptable range [0..<vector>.size()-1].
    Examples:
    pvector v - Prints vector content, size, capacity and T typedef
    pvector v 0 - Prints element[idx] from vector
    pvector v 1 2 - Prints elements in range [idx1..idx2] from vector
end

#
# std::list<>
#

define plist
    if $argc == 0
        help plist
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 2
                printf "elem[%u]: ", $size
                p *($arg1*)($current + 1)
            end
            if $argc == 3
                if $size == $arg2
                    printf "elem[%u]: ", $size
                    p *($arg1*)($current + 1)
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist <variable_name> <element_type> to see the elements in the list.\n"
        end
    end
end

document plist
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist l - prints list size and definition
    plist l int - prints all elements and list size
    plist l int 2 - prints the third element in the list (if exists) and list size
end

define plist_member
    if $argc == 0
        help plist_member
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 3
                printf "elem[%u]: ", $size
                p (*($arg1*)($current + 1)).$arg2
            end
            if $argc == 4
                if $size == $arg3
                    printf "elem[%u]: ", $size
                    p (*($arg1*)($current + 1)).$arg2
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist_member <variable_name> <element_type> <member> to see the elements in the list.\n"
        end
    end
end

document plist_member
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist_member l int member - prints all elements and list size
    plist_member l int member 2 - prints the third element in the list (if exists) and list size
end


#
# std::map and std::multimap
#

define pmap
    if $argc == 0
        help pmap
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 3
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p *($arg1*)$value
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p *($arg2*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 4
            set $idx = $arg3
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p *($arg1*)$value
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p *($arg2*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        if $argc == 5
            set $idx1 = $arg3
            set $idx2 = $arg4
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                set $valueLeft = *($arg1*)$value
                set $valueRight = *($arg2*)($value + sizeof($arg1))
                if $valueLeft == $idx1 && $valueRight == $idx2
                    printf "elem[%u].left: ", $i
                    p $valueLeft
                    printf "elem[%u].right: ", $i
                    p $valueRight
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap m - prints map size and definition
    pmap m int int - prints all elements and map size
    pmap m int int 20 - prints the element(s) with left-value = 20 (if any) and map size
    pmap m int int 20 200 - prints the element(s) with left-value = 20 and right-value = 200 (if any) and map size
end


define pmap_member
    if $argc == 0
        help pmap_member
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 5
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p (*($arg1*)$value).$arg2
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p (*($arg3*)$value).$arg4
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 6
            set $idx = $arg5
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p (*($arg1*)$value).$arg2
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p (*($arg3*)$value).$arg4
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap_member
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap_member m class1 member1 class2 member2 - prints class1.member1 : class2.member2
    pmap_member m class1 member1 class2 member2 lvalue - prints class1.member1 : class2.member2 where class1 == lvalue
end


#
# std::set and std::multiset
#

define pset
    if $argc == 0
        help pset
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Set "
            whatis $tree
            printf "Use pset <variable_name> <element_type> to see the elements in the set.\n"
        end
        if $argc == 2
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u]: ", $i
                p *($arg1*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 3
            set $idx = $arg2
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u]: ", $i
                    p *($arg1*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Set size = %u\n", $tree_size
    end
end

document pset
    Prints std::set<T> or std::multiset<T> information. Works for std::multiset as well.
    Syntax: pset <set> <T> <val>: Prints set size, if T defined all elements or just element(s) having val
    Examples:
    pset s - prints set size and definition
    pset s int - prints all elements and the size of s
    pset s int 20 - prints the element(s) with value = 20 (if any) and the size of s
end



#
# std::dequeue
#

define pdequeue
    if $argc == 0
        help pdequeue
    else
        set $size = 0
        set $start_cur = $arg0._M_impl._M_start._M_cur
        set $start_last = $arg0._M_impl._M_start._M_last
        set $start_stop = $start_last
        while $start_cur != $start_stop
            p *$start_cur
            set $start_cur++
            set $size++
        end
        set $finish_first = $arg0._M_impl._M_finish._M_first
        set $finish_cur = $arg0._M_impl._M_finish._M_cur
        set $finish_last = $arg0._M_impl._M_finish._M_last
        if $finish_cur < $finish_last
            set $finish_stop = $finish_cur
        else
            set $finish_stop = $finish_last
        end
        while $finish_first != $finish_stop
            p *$finish_first
            set $finish_first++
            set $size++
        end
        printf "Dequeue size = %u\n", $size
    end
end

document pdequeue
    Prints std::dequeue<T> information.
    Syntax: pdequeue <dequeue>: Prints dequeue size, if T defined all elements
    Deque elements are listed "left to right" (left-most stands for front and right-most stands for back)
    Example:
    pdequeue d - prints all elements and size of d
end



#
# std::stack
#

define pstack
    if $argc == 0
        help pstack
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = $size - 1
        while $i >= 0
            p *($start_cur + $i)
            set $i--
        end
        printf "Stack size = %u\n", $size
    end
end

document pstack
    Prints std::stack<T> information.
    Syntax: pstack <stack>: Prints all elements and size of the stack
    Stack elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    pstack s - prints all elements and the size of s
end



#
# std::queue
#

define pqueue
    if $argc == 0
        help pqueue
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = 0
        while $i < $size
            p *($start_cur + $i)
            set $i++
        end
        printf "Queue size = %u\n", $size
    end
end

document pqueue
    Prints std::queue<T> information.
    Syntax: pqueue <queue>: Prints all elements and the size of the queue
    Queue elements are listed "top to bottom" (top-most element is the first to come on pop)
    Example:
    pqueue q - prints all elements and the size of q
end



#
# std::priority_queue
#

define ppqueue
    if $argc == 0
        help ppqueue
    else
        set $size = $arg0.c._M_impl._M_finish - $arg0.c._M_impl._M_start
        set $capacity = $arg0.c._M_impl._M_end_of_storage - $arg0.c._M_impl._M_start
        set $i = $size - 1
        while $i >= 0
            p *($arg0.c._M_impl._M_start + $i)
            set $i--
        end
        printf "Priority queue size = %u\n", $size
        printf "Priority queue capacity = %u\n", $capacity
    end
end

document ppqueue
    Prints std::priority_queue<T> information.
    Syntax: ppqueue <priority_queue>: Prints all elements, size and capacity of the priority_queue
    Priority_queue elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    ppqueue pq - prints all elements, size and capacity of pq
end



#
# std::bitset
#

define pbitset
    if $argc == 0
        help pbitset
    else
        p /t $arg0._M_w
    end
end

document pbitset
    Prints std::bitset<n> information.
    Syntax: pbitset <bitset>: Prints all bits in bitset
    Example:
    pbitset b - prints all bits in b
end



#
# std::string
#

define pstring
    if $argc == 0
        help pstring
    else
        printf "String \t\t\t= \"%s\"\n", $arg0._M_data()
        printf "String size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "String capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "String ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pstring
    Prints std::string information.
    Syntax: pstring <string>
    Example:
    pstring s - Prints content, size/length, capacity and ref-count of string s
end

#
# std::wstring
#

define pwstring
    if $argc == 0
        help pwstring
    else
        call printf("WString \t\t= \"%ls\"\n", $arg0._M_data())
        printf "WString size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "WString capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "WString ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pwstring
    Prints std::wstring information.
    Syntax: pwstring <wstring>
    Example:
    pwstring s - Prints content, size/length, capacity and ref-count of wstring s
end

#
# C++ related beautifiers (optional)
#

set print pretty on
set print object on
set print static-members on
set print vtbl on
set print demangle on
set demangle-style gnu-v3
set print sevenbit-strings off

set history filename ~/.gdb_history
set history save

# finally stop the silly "A debugging session is active." - question ... just quit both.
set confirm off

={============================================================================
*kt_linux_tool_204* gdb: source and docs

{gdb-site}
https://sourceware.org/gdb/

{gdb-doc}
# pdf is also available
https://sourceware.org/gdb/download/onlinedocs/gdb/index

# GDB Command Referencehtml
http://visualgdb.com/gdbreference/commands/sharedlibrary 


={============================================================================
*kt_linux_tool_205* gdb: debugging info, run and quit

{gdb-gcc} # debugging information
Programs that are to be shipped to your customers are compiled with optimizations, using the â-Oâ
compiler option. However, some compilers are unable to handle the â-gâ and â-Oâ options together.
Using those compilers, you cannot generate optimized executables containing debugging information.

<gbd-debug-and-optimization>
gcc, the gnu C/C++ compiler, supports â-gâ with or without â-Oâ, making it possible to debug
optimized code. We recommend that you always use â-gâ whenever you compile a program. You may think
your program is correct, but there is no sense in pushing your luck. For more information, see
Chapter 11 [Optimized Code], page 149.

In the gdb backtrace the function arguments will be displayed from the stack if they are available.
Otherwise, gdb displays the values of the argument registers and hopes for the best. The arguments
in the backtrace are mostly correct because gdb can often pull them from the stack. When compiled
-O0 functions always save their argument registers to the stack, so it is only optimized code which
can make debugging difficult in this way. See mips-tdep.c from gdb source

<gdb-macro>
gdb knows about preprocessor macros and can show you their expansion (see Chapter 12 [Macros], page
153). Most compilers do not include information about preprocessor macros in the debugging
information if you specify the â-gâ flag alone. Version 3.1 and later of gcc provides macro
information if you are using the DWARF debugging format, and specify the option â-g3â.  See Section
âOptions for Debugging Your Program or GCCâ in Using the gnu Compiler Collection (GCC), for more
information on gcc options affecting debug information.

<qdb-quit>
type quit or Ctrl-d to exit.

<gdb-interrupt>
An interrupt (often Ctrl-c) does not exit from gdb, but rather terminates the action of any gdb
command that is in progress and returns to gdb command level. It is safe to type the interrupt
character at any time because gdb does not allow it to take effect until a time when it is safe.

What if the program is running but you forgot to set breakpoints? You can hit CTRL-C and that'll
stop the program wherever it happens to be and return you to a "(gdb)" prompt. At that point, you
could set up a proper breakpoint somewhere and continue to that breakpoint.

<return-key>
One final shortcut is that just hitting RETURN will repeat the last command entered; this will save
you typing next over and over again.


={============================================================================
*kt_linux_tool_206* gdb: commands

{gdb-command-completion}
If you just want to see the list of alternatives in the first place, you can press M-? rather than
pressing TAB twice. M-? means META ?. You can type this either by holding down a key designated as
the META shift on your keyboard (if there is one) while typing ?, or as ESC followed by ?.

<gdb-command-completion-for-cpp-overloading>
Sometimes the string you need, while logically a âwordâ, may contain parentheses or other characters
that gdb normally excludes from its notion of a word. To permit word completion to work in this
situation, you may enclose words in â (single quote marks) in gdb commands.

The most likely situation where you might need this is in typing the name of a C++ function. This is
because C++ allows function overloading (multiple definitions of the same function, distinguished by
argument type). For example, when you want to set a breakpoint you may need to distinguish
whether you mean the version of name that takes an int parameter, name(int), or the version that
takes a float parameter, name(float). To use the word-completion facilities in this situation, type
a single quote â at the beginning of the function name. This alerts gdb that it may need to consider
more information than usual when you press TAB or M-? to request word completion: (gdb) b âbubble(
M-?  bubble(double,double) bubble(int,int) (gdb) b âbubble(

<gdb-command-completion-for-structure>
When completing in an expression which looks up a field in a structure, gdb also tries to limit
completions to the field names available in the type of the left-hand-side:

(gdb) p gdb_stdout.M-?
magic to_fputs to_rewind
to_data to_isatty to_write
to_delete to_put to_write_async_safe
to_flush to_read

{gdb-info}
info This command (abbreviated i) is for describing the state of your program. For example, you can
show the arguments passed to a function with info args, list the registers currently in use with
info registers, or list the breakpoints you have set with info breakpoints. You can get a complete
list of the info sub-commands with help info.

info program

Display information about the status of your program: whether it is running or not, what process it
is, and why it stopped.

{gdb-show}
show configuration
Display detailed information about the way gdb was configured when it was built. This displays the
optional arguments passed to the âconfigureâ script and also configuration parameters detected
automatically by configure. When reporting a gdb bug (see Chapter 31 [GDB Bugs], page 483), it is
important to include this information in your report.


={============================================================================
*kt_linux_tool_207* gdb: commands for stepping

{gdb-next-gdb-step}
n (next) to advance execution to the next line of the current function.

s (step) instead of next. step goes to the next line to be executed in any subroutine.
It shows a summary of the stack. can use the backtrace command (which can also be spelled bt), to
see where we are in the stack as a whole: the backtrace command displays a stack frame for each
active subroutine.

note. Warning: If you use the step command while control is within a function that was compiled
without debugging information, execution proceeds until control reaches a function that does have
debugging information. Likewise, it will not step into a function which is compiled without
debugging information. To step through functions without debugging information, use the stepi
command, described below.

To step for a single assembly instruction, use the (gdb) stepi

Also, the step command 'only' enters a function if there is line number information for the
function. Otherwise it acts like the next command.

{gdb-return-gdb-finish}
To resume execution at a different place, you can use return (see Section 17.4 [Returning from a
Function], page 221) to go back to the calling function; or jump (see Section 17.2 [Continuing
at a Different Address], page 220) to go to an arbitrary location in your program.

The finish Continue running until just after function in the selected stack frame returns.  Print
the returned value (if any). This command can be abbreviated as fin.  Contrast this with the return
command (see Section 17.4 [Returning from a Function], page 221). Q: this is only difference?

{gdb-print}
p (print) to see their values.

(gdb) p len lquote
$3 = 9
(gdb) p len rquote
$4 = 7

{gdb-set}
can 'set' them to better values using the p command, since it can print the value of any
expressionâand that expression can include subroutine calls and assignments. set new value and
continue to see if it fixes the bug.

(gdb) p len lquote=strlen(lquote)
$5 = 7
(gdb) p len rquote=strlen(rquote)
$6 = 9
(gdb) c
Continuing.


{gdb-advance}
To continue to a specific location, use the 
(gdb) advance funcname


{?}
(gdb) run -v all
(gdb) sectionFilterTable[sfIdx]
(gdb) directory components/FOSH/FUSIONOS_XTV_TESTS/xtv_test/src/
(gdb) directory components/FOSH/FUSIONOS_TEST_SHARED/DSL/src/ (SDS_SectionFilter.c)


={============================================================================
*kt_linux_tool_208* gdb: multi thread

{gdb-thread}
4.10 Debugging Programs with Multiple Threads

The precise semantics of threads differ from one operating system to another, but in general the
threads of a single program are akin to multiple processesâexcept that they share one address space
(that is, they can all examine and modify the same variables). On the other hand, each thread has
its own registers and execution stack, and perhaps private memory.

gdb provides these facilities for debugging multi-thread programs:

* automatic notification of new threads
* âthread threadnoâ, a command to switch among threads
* âinfo threadsâ, a command to inquire about existing threads
* âthread apply [threadno] [all] argsâ, a command to apply a command to a list of threads
* thread-specific breakpoints

<current-thread>
The gdb thread debugging facility allows you to observe all threads while your program runsâbut
whenever gdb takes control, one thread in particular is always the focus of debugging. This thread
is called the current thread. Debugging commands show program information from the perspective of
the current thread.

(gdb) info threads
Id Target Id Frame
3 process 35 thread 27 0x34e5 in sigpause ()
2 process 35 thread 23 0x34e5 in sigpause ()
* 1 process 35 thread 13 main (argc=1, argv=0x7ffffff8)

{gdb-thread-apply}
thread apply [threadno | all] command

The thread apply command allows you to apply the named command to one or more threads. Specify the
numbers of the threads that you want affected with the command argument threadno. It can be a single
thread number, one of the numbers shown in the first field of the âinfo threadsâ display; or it
could be a range of thread numbers, as in 2-4. To apply a command to all threads, type thread apply
all command.


={============================================================================
*kt_linux_tool_209* gdb: breakpoints

{gdb-breakpoint}
Breakpoints are set with the break command (abbreviated b).

gdb normally implements breakpoints by replacing the program code at the breakpoint address with a
special instruction, which, when executed, given control to the debugger.  By default, the program
code is so modified only when the program is resumed. As soon as the program stops, gdb restores the
original instructions. This behaviour guards against leaving breakpoints inserted in the target
should gdb abrubptly disconnect. However, with slow remote targets, inserting and removing
breakpoint can reduce the performance. This behavior can be controlled with the following commands::

<shared-and-multiple>
On some systems, you can set breakpoints in shared libraries before the executable is run. There is
a minor limitation on HP-UX systems: you must wait until the executable is run in order to set
breakpoints in shared library routines that are not called directly by the program (for example,
routines that are arguments in a pthread_create call).

Itâs quite common to have a breakpoint inside a shared library. Shared libraries can be loaded and
unloaded explicitly, and possibly repeatedly, as the program is executed.  To support this use case,
gdb updates breakpoint locations whenever any shared library is loaded or unloaded.
Typically, you would set a breakpoint in a shared library at the beginning of your
debugging session, when the library is not loaded, and when the symbols from the library
are not available. When you try to set breakpoint, gdb will ask you if you want to set a so
called pending breakpointâbreakpoint whose address is not yet resolved.  After the program
is run, whenever a new shared library is loaded, gdb reevaluates all the breakpoints. When
a newly loaded shared library contains the symbol or line referred to by some pending
breakpoint, that breakpoint is resolved and becomes an ordinary breakpoint.  When a library
is unloaded, all breakpoints that refer to its symbols or source lines become pending
again.

This logic works for breakpoints with multiple locations, too. For example, if you have a breakpoint
in a C++ template function, and a newly loaded shared library has an instantiation of that template,
a new location is added to the list of locations for the breakpoint.

Except for having unresolved address, pending breakpoints do not differ from regular breakpoints.
You can set conditions or commands, enable and disable them and perform other breakpoint operations.

set breakpoint pending auto

This is the default behavior. When gdb cannot find the breakpoint location, it queries you whether a
pending breakpoint should be created.

set breakpoint pending on

This indicates that an unrecognized breakpoint location should automatically result in a pending
breakpoint being created.

A breakpoint with multiple locations is displayed in the breakpoint table using several rowsâone
header row, followed by one row for each breakpoint location. The header row has â<MULTIPLE>â in the
address column. The rows for individual locations contain the actual addresses for locations, and
show the functions to which those locations belong. The number column for a location is of the form
breakpoint-number.location-number.

For example:
Num   Type        Disp     Enb      Address     What
1     breakpoint  keep     y        <MULTIPLE>
      stop only if i==1
      breakpoint already hit 1 time
1.1                        y 0x080486a2 in void foo<int>() at t.cc:8
1.2                        y 0x080486ca in void foo<double>() at t.cc:8

Each location can be individually enabled or disabled by passing breakpointnumber.  location-number
as argument to the enable and disable commands.

<watchpoint>
A watchpoint is a special breakpoint that stops your program when the value of an expression
changes. The expression may be a value of a variable, or it could involve values of one or more
variables combined by operators, such as âa + bâ. This is sometimes called data breakpoints. You
must use a different command to set watchpoints (see Section 5.1.2 [Setting Watchpoints], page 50),
but aside from that, you can manage a watchpoint like any other breakpoint: you enable,
disable, and delete both breakpoints and watchpoints using the same commands.

<catchpoint>
A catchpoint is another special breakpoint that stops your program when a certain kind of event
occurs, such as the throwing of a C++ exception or the loading of a library. As with watchpoints,
you use a different command to set a catchpoint (see Section 5.1.3 [Setting Catchpoints], page
53), but aside from that, you can manage a catchpoint like any other breakpoint. (To stop when
your program receives a signal, use the handle command; see Section 5.4 [Signals], page 70.)


{gdb-break-location}
Set a breakpoint at the given location, which can specify a function name, a line number, or an
address of an instruction. (See Section 9.2 [Specify Location], page 98, for a list of all the
possible ways to specify a location.) The breakpoint will stop your program just before it
executes any of the code in the specified location.

<overloading-and-thread>
When using source languages that permit overloading of symbols, such as C++, a function name may
refer to more than one possible place to break. See Section 10.2 [Ambiguous Expressions], page 110,
for a discussion of that situation.  It is also possible to insert a breakpoint that will stop
the program only if a specific thread (see Section 5.5.4 [Thread-Specific Breakpoints], page
76) or a specific task (see Section 15.4.9.6 [Ada Tasks], page 204) hits that breakpoint.

{gdb-break-cond}
break ... if cond

Set a breakpoint with condition cond; evaluate the expression cond each time the breakpoint is
reached, and stop only if the value is nonzeroâthat is, if cond evaluates as true. â...â stands for
one of the possible arguments described above (or no argument) specifying where to break. See
Section 5.1.6 [Break Conditions], page 59, for more information on breakpoint conditions.

{gdb-break-tbreak}
tbreak args

Set a breakpoint enabled only for 'one' stop. args are the same as for the break command, and the
breakpoint is set in the same way, but the breakpoint is auto matically deleted after the first time
your program stops there. See Section 5.1.5 [Disabling Breakpoints], page 58.

<gdb-break-info>
info breakpoints [n...]
info break [n...]

Print a table of all breakpoints, watchpoints, and catchpoints set and not deleted. Optional
argument n means print information only about the specified breakpoint(s) (or watchpoint(s) or
catchpoint(s)). For each breakpoint, following columns are printed:

It is possible that a breakpoint corresponds to several locations in your program. Examples
of this situation are:

1) Multiple functions in the program may have the same name.

2) For a C++ constructor, the gcc compiler generates several instances of the function body, used in
different cases.

3) For a C++ template function, a given line in the function can correspond to any number of
instantiations.

4) For an inlined function, a given source line can correspond to several places where that function
is inlined.

In all those cases, gdb will insert a breakpoint at all the relevant locations.


{break-disabling}
5.1.5 Disabling breakpoints

This makes the breakpoint inoperative as if it had been deleted, but remembers the information on
the breakpoint so that you can enable it again later. 

Optionally specifying one or more breakpoint numbers as arguments. Use info break or info watch to
print a list of breakpoints, watchpoints, and catchpoints if you do not know which numbers to use.

A breakpoint, watchpoint, or catchpoint can have any of four different 'states' of enablement:

- Enabled. The breakpoint stops your program. A breakpoint set with the break command starts out in
this state. You may abbreviate disable as dis. 
- Disabled. The breakpoint has no effect on your program.
- Enabled once. The breakpoint stops your program, but then becomes disabled.
- Enabled for deletion. The breakpoint stops your program, but immediately after it does so it is
deleted permanently. A breakpoint set with the tbreak command starts out in this state. 

disable [breakpoints] [range...]
Disable the specified breakpoints-or 'all' breakpoints, if none are listed. A disabled breakpoint
has no effect but is not forgotten. All options such as ignore-counts, conditions and commands are
remembered in case the breakpoint is enabled again later. You may abbreviate disable as dis.

enable [breakpoints] [range...]
Enable the specified breakpoints (or 'all' defined breakpoints). They become effective once again in
stopping your program.


={============================================================================
*kt_linux_tool_210* gdb: breakpoints: advanced

{break-command-list}
5.1.7 Breakpoint Command Lists
You can give any breakpoint (or watchpoint or catchpoint) a series of commands to 'execute' when
your program stops due to that breakpoint. For example, you might want to print the values of
certain expressions, or enable other breakpoints.

For example, here is how you could use breakpoint commands to print the value of x at entry to foo
whenever x is positive.

break foo if x>0
commands
silent
printf "x is %d\n",x
cont
end

One application for breakpoint commands is to compensate for one bug so you can test for another.
Put a breakpoint just after the erroneous line of code, give it a condition to detect the case in
which something erroneous has been done, and give it commands to assign correct values to any
variables that need them. End with the continue command so that your program does not stop, and
start with the silent command so that no output is produced. Here is an example:

break 403
commands
silent
set x = y + 4           // note <gdb-set>
cont
end

{dynamic-printf}
The dynamic printf command dprintf combines a breakpoint with formatted printing of your programâs
data to give you the effect of inserting printf calls into your program on-the-fly, without having
to recompile it.

In its most basic form, the output goes to the GDB console. However, you can set the variable
dprintf-style for alternate handling. For instance, you can ask to format the output by calling your
programâs printf function. This has the advantage that the characters go to the programâs output
device, so they can recorded in redirects to files and so forth.

As an example, if you wanted dprintf output to go to a logfile that is a standard I/O stream
assigned to the variable mylog, you could do the following:

(gdb) set dprintf-style call
(gdb) set dprintf-function fprintf
(gdb) set dprintf-channel mylog
(gdb) dprintf 25,"at line 25, glob=%d\n",glob
Dprintf 1 at 0x123456: file main.c, line 25.
(gdb) info break
1 dprintf keep y 0x00123456 in main at main.c:25
call (void) fprintf (mylog,"at line 25, glob=%d\n",glob)
continue
(gdb)

note that the info break displays the dynamic printf commands as normal breakpoint commands; you can
thus easily see the effect of the variable settings.

dprintf location,template,expression[,expression...]

Whenever execution reaches location, print the values of one or more expressions under the control
of the string template. To print several values, separate them with commas.

set dprintf-style style

Set the dprintf output to be handled in one of several different styles enumerated below. A change
of style affects all existing dynamic printfs immediately. (If you need individual control over the
print commands, simply define normal breakpoints with explicitly-supplied command lists.) gdb
Handle the output using the gdb printf command.

'call' Handle the output by calling a function in your program (normally printf).  agent Have the
remote debugging agent (such as gdbserver) handle the output itself.  This style is only available
for agents that support running commands on the target.

set dprintf-function function

Set the function to call if the dprintf style is call. By default its value is printf. You may set
it to any expression. that gdb can evaluate to a function, as per the call command.

set dprintf-channel channel

Set a âchannelâ for dprintf. If set to a non-empty value, gdb will evaluate it as an expression and
pass the result as a <first-argument> to the dprintf-function, in the manner of fprintf and similar
functions. Otherwise, the dprintf format string will be the first argument, in the manner of printf.

{break-save}
5.1.9 How to save breakpoints to a file

To save breakpoint definitions to a file use the save breakpoints command.  save breakpoints
[filename]


={============================================================================
*kt_linux_tool_211* gdb: examining running

{whatis}
16 Examining the Symbol Table

whatis[/flags] [arg]

Print the data type of arg, which can be either an expression or a name of a data type. With no
argument, print the data type of $, the last value in the value history. If arg is an expression
(see Section 10.1 [Expressions], page 109), it is not actually evaluated, and any side-effecting
operations (such as assignments or function calls) inside it do not take place. 

If arg is a variable or an expression, whatis prints its literal type as it is used in the source
code. If the type was defined using a typedef, whatis will not print the data type underlying the
typedef. If the type of the variable or the expression is a compound data type, such as struct or
class, whatis never prints their fields or methods. It just prints the struct/class name (a.k.a.
    its tag). If you want to see the members of such a compound data type, use ptype.  If arg is a
type name that was defined using typedef, whatis unrolls only one level of that typedef. Unrolling
means that whatis will show the underlying

(gdb) whatis ppkSectionData
type = const uint8_t **


{gdb-print}
(gdb) printf "%d\n", i
40
(gdb) printf "%08X\n", i
00000028

print i
Print the value of variable i.

print *p
Print the contents of memory pointed to by p, where p is a pointer variable.

(gdb) whatis pszSection
type = uint32_t *
(gdb) print *pszSection

print x.field
Check the different members of a structure.

print x
Check all the members of a structure, assuming x is a structure.

print y-field
y is a pointer to a structure.

print array[i]
Print the i'th element of array.

print array
Print all the elements of array.

(gdb) print /x block1->magic
$5 = 0xabeaa5b3

(gdb) print /x block1 
$9 = 0x1150f4c

(gdb) print /x *block1
$8 = {magic = 0xabeaa5b3, size = 0x28, line = 0x0, owner = 0x0, header = 0x21, data = {free = {previous = 0x8, next = 0x115106c}, 
    userData = {0x0}}}

# db_contexts_array is a global var
(gdb) p db_contexts_array
(gdb) p db_contexts_array[-1]


<gdb-disp>
to inspect over the course of the run

(gdb) [disp]lay var
(gdb) [undisp] disp_num
(gdb) info disp 	" to list disps


={============================================================================
*kt_linux_tool_212* gdb: examine sources

<directory>
9.5 Specifying Source Directories

<list>
9 Examining Source Files

list 

Print more lines. If the last lines printed were printed with a list command, this prints lines
following the last lines printed; however, if the last line printed was a solitary line printed as
part of displaying a stack frame (see Chapter 8 [Examining the Stack], page 89), this prints lines
centered around that line.


={============================================================================
*kt_linux_tool_213* gdb: symbols and files

<info-sources>
16 Examining the Symbol Table

info sources

Print the names of all source files in your program for which there is debugging information,
organized into two lists: files whose symbols have already been read, and files whose symbols will
be read when needed.

<file> <core>
You may want to specify executable and core dump file names. The usual way to do this is at start-up
time, using the arguments to gdbâs start-up commands (see Chapter 2 [Getting In and Out of gdb],
page 11). 

Occasionally it is necessary to change to a different file during a gdb session. Or you may run gdb
and forget to specify a file you want to use. Or you are debugging a remote target via gdbserver
(see Section 20.3 [Using the gdbserver Program], page 247). In these situations the gdb commands to
specify new files are useful.

file filename

Use filename as the program to be debugged.

core-file [filename]
core 

Specify the whereabouts of a core dump file to be used as the âcontents of memoryâ. Traditionally,
core files contain only some parts of the address space of the process that generated them; gdb can
access the executable file itself for other parts.  core-file with no argument specifies that no
core file is to be used.


<info-files>
info files
info target

info files and info target are synonymous; both print the current target (see Chapter 19 [Specifying
a Debugging Target], page 241), including the names of the executable and core dump files
currently in use by gdb, and the files from which symbols were loaded. The command help target lists
all possible targets rather than current ones.


<sysroot> <solib-absolute-prefix>
18 gdb Files

Shared libraries are also supported in many cross or remote debugging configurations.  gdb needs to
have access to the targetâs libraries; this can be accomplished either by providing copies of the
libraries on the host system, or by asking gdb to automatically retrieve the libraries from the
target. If copies of the target libraries are provided, they need to be the same as the target
libraries, although the copies on the target can be stripped as long as the copies on the host are
not.

<note-this>
For remote debugging, you need to tell gdb where the target libraries are, so that it can load the
correct copies-'otherwise', it may try to load the 'host'âs libraries. gdb has two variables to
specify the search directories for target libraries.

set sysroot 'path'

Use path as the system root for the program being debugged. Any absolute shared library paths will
be prefixed with 'path'; many runtime loaders store the absolute paths to the shared library in the
target programâs memory. If you use set sysroot to find shared libraries, they need to be laid out
in the same way that they are on the target, with e.g. a â/libâ and â/usr/libâ hierarchy under
'path'.

If path starts with the sequence âremote:â, gdb will retrieve the target libraries from the remote
system. This is only supported when using a remote target that supports the remote get command (see
Section 20.2 [Sending files to a remote system], page 247). The part of path following the
initial âremote:â (if present) is used as system root prefix on the remote file system.1

The set solib-absolute-prefix command is an alias for set sysroot.

You can set the default system root by using the configure-time â--with-sysrootâ option. If the
system root is inside gdbâs configured binary prefix (set with â--prefixâ or â--exec-prefixâ), then
the default system root will be updated automatically if the installed gdb is moved to a new
location.

show sysroot
Display the current shared library prefix.

set solib-search-path path
If this variable is set, path is a colon-separated list of directories to search for shared
libraries. âsolib-search-pathâ is used after âsysrootâ fails to locate the library, or if the path
to the library is relative instead of absolute. 

If you want to use âsolib-search-pathâ instead of âsysrootâ, be sure to set âsysrootâ to a
'nonexistent' directory to prevent gdb from finding your hostâs libraries. âsysrootâ is preferred;
setting it to a nonexistent directory may interfere with automatic loading of shared library
symbols.

show solib-search-path
Display the current shared library search path.


={============================================================================
*kt_linux_tool_214* gdb: shared library debugging

{how-to-debug-shared-lib} from online

HOW TO DEBUG shared library using GDB

[bhushan@Shared_Lib_Debug]$ gcc -fpic -shared -o foo.so foo.c              // note no -g

[bhushan@Shared_Lib_Debug]$ gcc -o main main.c ./foo.so -g

[bhushan@Shared_Lib_Debug]$ gdb main
GNU gdb Red Hat Linux (6.3.0.0-1.21rh)
Copyright 2004 Free Software Foundation, Inc.
GDB is free software, covered by the GNU General Public License, and you are
welcome to change it and/or distribute copies of it under certain conditions.
Type "show copying" to see the conditions.
There is absolutely no warranty for GDB. Type "show warranty" for details.
This GDB was configured as "i386-redhat-linux-gnu"...Using host libthread_db library "/lib/libthread_db.so.1".

(gdb) b foo
Function "foo" not defined.
Make breakpoint pending on future shared library load? (y or [n]) y     // note
Breakpoint 1 (foo) pending.

(gdb) r

Starting program: /home/bhushan/RD/Shared_Lib_Debug/main
Reading symbols from shared object read from target memory...done.
Loaded system supplied DSO at 0x470000
Breakpoint 2 at 0xdb7493

Pending breakpoint "foo" resolved                                       // note
Breakpoint 2, 0x00db7493 in foo () from ./foo.so

(gdb) s

Single stepping until exit from function foo,
which has no line number information.
main () at main.c:7
7 printf("inside main i = %d\n", i);
(gdb) s
inside main i = 4
8 return 0;

NOW if you build the shared libarary using -g option
[bhushan@Shared_Lib_Debug]$ gcc -fpic -shared -o foo.so foo.c -g

[bhushan@Shared_Lib_Debug]$ gcc -o main main.c ./foo.so -g

[bhushan@Shared_Lib_Debug]$ gdb main
GNU gdb Red Hat Linux (6.3.0.0-1.21rh)
Copyright 2004 Free Software Foundation, Inc.
GDB is free software, covered by the GNU General Public License, and you are
welcome to change it and/or distribute copies of it under certain conditions.
Type "show copying" to see the conditions.
There is absolutely no warranty for GDB. Type "show warranty" for details.
This GDB was configured as "i386-redhat-linux-gnu"...Using host libthread_db library "/lib/libthread_db.so.1".


(gdb) b foo

Function "foo" not defined.

Make breakpoint pending on future shared library load? (y or [n]) y

Breakpoint 1 (foo) pending.

(gdb) r

Starting program: /home/bhushan/RD/Shared_Lib_Debug/main
Reading symbols from shared object read from target memory...done.
Loaded system supplied DSO at 0x470000
Breakpoint 2 at 0x1c5493: file foo.c, line 5.
Pending breakpoint "foo" resolved

Breakpoint 2, foo () at foo.c:5
5 return 2*2;

(gdb) s

7 }

(gdb) s

main () at main.c:7
7 printf("inside main i = %d\n", i);

(gdb) s

inside main i = 4

8 return 0;

(gdb)

U can see the differences in bold lines.

<sharedlibrary-command>
http://visualgdb.com/gdbreference/commands/sharedlibrary # GDB Command Reference
sharedlibrary command

Forces GDB to load symbols for the specified shared libraries or all loaded shared libraries.

Syntax
sharedlibrary
sharedlibrary [Library Name]
sharedlibrary [Regular Expression]
share [...]

Parameters

Library Name 
Specifies the library to load debugging symbols for.

Regular Expression 
When specified, GDB will load the symbols for all currently loaded libraries matching the specified expression.

Remarks

Normally, GDB will load the shared library symbols automatically. You can control this behavior
using set auto-solib-add command. <auto-solib-add>

<further-reference>
18 gdb Files

To control the automatic loading of shared library symbols, use the commands: 

set auto-solib-add mode

If mode is on, symbols from all shared object libraries will be loaded automatically when the
inferior begins execution, you attach to an independently started inferior, or when the dynamic
linker informs gdb that a new library has been loaded. If mode is off, symbols must be loaded
manually, using the sharedlibrary command. The 'default' value is on.

If your program uses lots of shared libraries with debug info that takes large amounts of memory,
you can decrease the gdb memory footprint by preventing it from automatically loading the symbols
from shared libraries. 

To that end, type set auto-solib-add off before running the inferior, then load each library whose
debug symbols you do need with sharedlibrary regexp, where regexp is a regular expression that
matches the libraries whose symbols you want to be loaded. To explicitly load shared library
symbols, use the 'sharedlibrary' command:

show auto-solib-add

Display the current autoloading mode.

<info-sharedlibrary>
info share regex
info sharedlibrary regex
Print the names of the shared libraries which are currently loaded that match regex. If regex is
omitted then print all shared libraries that are loaded.

sharedlibrary regex
share regex
Load shared object library symbols for files matching a Unix regular expression.  As with files
loaded automatically, it only loads shared libraries required by your program for a core file or
'after' typing run. If regex is omitted all shared libraries required by your program are loaded.

note. can specify library name which are required by a program but not a filename with a path to
force a loading of it. That means cannot load a libaray until a program needs it.

<continue-article>
However, in some cases (e.g. when debugging with gdbserver and having incompatible symbols or using
old Android toolchains) GDB will not load the symbols automatically. In this case you can use
the info sharedlibrary command to list the loaded shared libraries and the sharedlibrary command to
force the symbols to be loaded. 

If GDB does not automatically load debugging symbols for your library when debugging with gdbserver,
please check the search path using the set solib-search-path command.

Examples

In this example we will disable shared library loading using the set auto-solib-add command, then
run the application, list the source files and load the symbols manually:

(gdb) set auto-solib-add off
(gdb) break main
Breakpoint 1 at 0x80484ed: file main.cpp, line 7.
(gdb) run
Starting program: /home/testuser/libtest/testApp

Breakpoint 1, main () at main.cpp:7
7 printf("In main()\n");

(gdb) info sources      // <info-sources>
Source files for which symbols have been read in:

/home/testuser/libtest/main.cpp

Source files for which symbols will be read in on demand:

(gdb) info sharedlibrary
From To Syms Read Shared Object Library
0xb7fde820 0xb7ff6b9f No /lib/ld-linux.so.2
0xb7fd83a0 0xb7fd84c8 No /home/testuser/libtest/libTest.so
0xb7e30f10 0xb7f655cc No /lib/i386-linux-gnu/libc.so.6

(gdb) sharedlibrary libTest
Reading symbols from /home/testuser/libtest/libTest.so...done.
Loaded symbols for /home/testuser/libtest/libTest.so

(gdb) info sources
Source files for which symbols have been read in:

/home/testuser/libtest/main.cpp

Source files for which symbols will be read in on demand:

/home/testuser/libtest/lib.cpp                  // note. see added source file

(gdb) break lib.cpp:5
Breakpoint 2 at 0xb7fd846e: file lib.cpp, line 5.

(gdb) continue
Continuing.
In main()

Breakpoint 2, func () at lib.cpp:5
5 printf("In func()\n");


{real-examples}
# to see the default setting
(gdb)  show auto-solib-add
Autoloading of shared library symbols is on.

# note that shall load file first before setting a breakpoint
(gdb) file /home/kit/tizen/tv-viewer/tv-viewer
Reading symbols from /home/kit/tizen/tv-viewer/tv-viewer...done.

(gdb) target remote 106.1.11.219:2345
Remote debugging using 106.1.11.219:2345
warning: Unable to find dynamic linker breakpoint function.
GDB will be unable to debug shared library initialisers
and track explicitly loaded dynamic code.
0xb63da7c0 in ?? ()

# to set a breakpoint in the gdbint; otherwise, gdb set no since no input from the user
(gdb) set breakpoint pending on
(gdb)

# note that this is a warning at this moment
(gdb) b main
Cannot access memory at address 0x0
Breakpoint 1 at 0x29842716: file /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1/src/core/AppMain.cpp, line 33.

# no shared library sicne the application do not started yet.
(gdb) i sharedlibrary
No shared libraries loaded at this time.

# note that see warnings on shared library now.
# set solib-search-path before and seems to have only one path as set solib-search-path /home/kit/mheg-port-ug
(gdb) c
Continuing.
warning: `/usr/lib/libicui18n.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicuuc.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicudata.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libfribidi.so.0': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicule.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: Could not load shared library symbols for 184 libraries, e.g. /usr/lib/libsys-assert.so.
Use the "info sharedlibrary" command to see the complete listing.
Do you need "set solib-search-path" or "set sysroot"?

Breakpoint 1, main (argc=-1237321556, argv=0xb63f4a30)
    at /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1/src/core/AppMain.cpp:33
33           _ERR("%s","Failed to create app!");

(gdb)
Continuing.

# note that even after running, not loaded full libraries. want to debug libug-mhegUG-efl.so
(gdb) i sharedlibrary
From        To          Syms Read   Shared Object Library
                        No          /usr/lib/libsys-assert.so
                        ...
                        No          /usr/lib/libsqlite3.so.0
0xb55efb24  0xb572307c  Yes (*)     /usr/lib/libicui18n.so.48
0xb55154e4  0xb55dfe7c  Yes (*)     /usr/lib/libicuuc.so.48
0xb43e4258  0xb43e4350  Yes (*)     /usr/lib/libicudata.so.48
                        No          /usr/lib/libavoc.so
                        ...
(*): Shared library is missing debugging information.

(gdb) c
Continuing.

(gdb) CTRL-C
Program received signal SIGINT, Interrupt.
0xb3db296c in ?? ()
warning: Could not load shared library symbols for 21 libraries, e.g. /usr/lib/ecore/immodules/libisf-imf-module.so.
Use the "info sharedlibrary" command to see the complete listing.
Do you need "set solib-search-path" or "set sysroot"?

# note that the wanted library is not loaded
(gdb) i sharedlibrary
From        To          Syms Read   Shared Object Library
                        ...
0xab8d96c0  0xab9a2d78  No          /home/kit/mheg-port-ug/libug-mhegUG-efl.so
                        ..
(*): Shared library is missing debugging information.

# HOWEVER, the problem is that the library is loaded but not the symblos and files. can check to see
# if that is the case by running "i sources" to see files.

# note to fource to load again and check with i sources
(gdb) sharedlibrary mheg
Reading symbols from /home/kit/mheg-port-ug/libug-mhegUG-efl.so...done.
Loaded symbols for /home/kit/mheg-port-ug/libug-mhegUG-efl.so
Cannot access memory at address 0xb

# here can see the result of this command:
# set substitute-path /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1 /home/kit/tizen/tv-viewer
(gdb) i sources
Source files for which symbols have been read in:

/home/kit/tizen/tv-viewer/src/core/AppMain.cpp, /usr/include/c++/4.5.3/new,
...

Source files for which symbols will be read in on demand:
...
/home/kit/mheg-port-ug/mh5eng/mh5e_token.c, /home/kit/mheg-port-ug/mh5eng/mh5b_program.c,

# note that now set a breakpoint
(gdb) b _key_pressed(char const*)
Cannot access memory at address 0xb
Breakpoint 2 at 0xab8d9afa: file /home/abuild/rpmbuild/BUILD/ug-mheg-0.2/main/Main.cpp, line 144.

(gdb) c

# hit breakpoint

(gdb) list

# note that if do not run this command fast enough, it seems that gdb fails to run a session. means
# not hitting a breakpoint.

(gdb) c
Continuing.

Program received signal SIGINT, Interrupt.
0xb3db296c in ?? ()
(gdb) c
Continuing.
Cannot access memory at address 0xb
[New Thread 9633]

Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 9633]
0xb4124ab0 in ?? ()
(gdb) c
Continuing.
^C
^CInterrupted while waiting for the program.
Give up (and stop debugging it)? (y or n) n


={============================================================================
*kt_linux_tool_250* gdb: remote debugging

{cannot-access-memory-warning} "Cannot access memory at address 0Ã0" warning,
This happens when run gdb client in case use to debug a applicaiton using a shared library. Thought
that gdb is not working but seems not as below. 

<how-to-setup-eclipse>
http://hertaville.com/2013/01/11/remote-debugging/
Neil, Yes the remote path has to existâ¦I will amend to tutorial & explicitly make note of this. As
for the "Cannot access memory at address 0Ã0" warning, I also get it (see Fig15). I have no problems
however with stepping and using breakpoints. I do not have any serious speed issues when
simulating a simple program similar to the one in the Tutorial. I also had no problems when
debugging the GPIO examples posted on this site. Having said that I didnât simulate a program
that calls external libraries such as libjpeg. Perhaps that is why debugging is slow in your
case ?

{gdb-term}
GDB represents the state of each program execution with an object called an 'inferior'. 

{gdb-server}
To use a TCP connection, you could say:

target> gdbserver host:2345 emacs foo.txt

This says pretty much the same thing as the last example, except that we are going to communicate
with the host GDB via TCP. The host:2345 argument means that we are expecting to see a TCP
connection from host to local TCP port 2345. (note Currently, the host part is ignored.) You can
choose any number you want for the port number as long as it does not conflict with any existing TCP
ports on the target system. This same port number must be used in the host GDBs target remote
command, which will be described shortly. Note that if you chose a port number that conflicts with
another service, gdbserver will print an error message and exit. 

# does make a difference?
sudo gdbserver 10.42.0.1:12345 HelloRPiWorld 

<arguments>
20.3.1.4 Other Command-Line Arguments for gdbserver

--debug

Instruct gdbserver to display extra status information about the debugging process. This option is
intended for gdbserver development and for bug reports to the developers.

--remote-debug

Instruct gdbserver to display remote protocol debug output. This option is intended for gdbserver
development and for bug reports to the developers.

<symbols>
First make sure you have the necessary symbol files. Load symbols for your application using the
file command before you connect. Use set sysroot to locate target libraries (unless your GDB was
compiled with the correct sysroot using --with-sysroot). Q: <sysroot>?

The symbol file and target libraries must exactly match the executable and libraries on the target
with one exception: the files on the host system should not be stripped, even if the files on the
target system are. Mismatched or missing files will lead to confusing results during debugging. On
GNU/Linux targets, mismatched or missing files may also prevent gdbserver from debugging
multi-threaded programs. 


{gdb-client}
target remote host:port

monitor cmd

This command allows you to send arbitrary commands directly to the remote monitor. Since GDB doesnât
care about the commands it sends like this, this command is the way to extend GDBâyou can add new
commands that only the external monitor will understand and implement. 
	

20.3.3 Monitor Commands for gdbserver

During a GDB session using gdbserver, you can use the monitor command to send special requests to
gdbserver. Here are the available commands.

monitor help
List the available monitor commands.

monitor set debug 0
monitor set debug 1
Disable or enable general debugging messages.

monitor set remote-debug 0
monitor set remote-debug 1
Disable or enable specific debugging messages associated with the remote protocol (see Remote
Protocol).  

monitor set debug-format option1[,option2,...]

Specify additional text to add to debugging messages. Possible options are:

none Turn off all extra information in debugging output. 

all Turn on all extra information in debugging output. 

timestamps Include a timestamp in each line of debugging output. 

Options are processed in order. Thus, for example, if none appears last then no additional
information is added to debugging output. 

monitor exit

Tell gdbserver to exit immediately. This command should be followed by disconnect to close the
debugging session. gdbserver will detach from any attached processes and kill any processes it
created. Use monitor exit to terminate gdbserver at the end of a multi-process mode debug session.


={============================================================================
*kt_linux_tool_251* gdb: local debugging

={============================================================================
*kt_linux_tool_300* gdb: frontend tool: cgdb

It is how to change cgdb to use cross tool gdb.

http://www.programdevelop.com/4527764/

The GDB code CGDB calls in the path: /VARIOUS/util/src/fork_util.c by function invoke_debugger in

int invoke_debugger( 
            const char *path,  
            int argc, char *argv[],  
            int *in, int *out,  
            int choice, char *filename)  
{ 
    pid_t pid;     
    //GDBGDB?arm-linux-gdb 
    const char * const GDB               = "arm-linux-gdb"; 
}

./configure --prefix=/usr/local/ --program-suffix=arm-linux
make
sudo make install

kit@kit-vb:~/mheg-port$ ls /usr/local/bin/cgdb*
/usr/local/bin/cgdb  /usr/local/bin/cgdbarm-linux
kit@kit-vb:~/mheg-port$

http://cgdb.github.io/


{config}
/.cgdb/cgdbrc
==
:set winspilt=top_big
:set arrowstyle=long
:map <F6> :continue<CR>
:map <F7> :finish<CR>
:map <F8> :step<CR>
:map <F9> :next<CR>
==


# ============================================================================
#{
={============================================================================
*kt_linux_core_001* check shared libraries that process uses

cat /proc/NNNN/maps | awk '{print $6}' | grep '\.so' | sort | uniq

or

can use ldd.


={============================================================================
*kt_linux_core_002* stdin, stderr, and stdout

On program startup, the integer file descriptors associated with the streams stdin, stdout, and
stderr are 0, 1, and 2, respectively.


={============================================================================
*kt_linux_core_003* check libc version and path

Use ldd to find out the path and when run libc, it shows version.

keitee@debian-keitee:~/github/kb$ ldd ~/bin/vim | grep libc
   libc.so.6 => /lib/i386-linux-gnu/i686/cmov/libc.so.6 (0xb6915000)

$ /lib/i386-linux-gnu/i686/cmov/libc.so.6
GNU C Library (Debian EGLIBC 2.13-38+deb7u6) stable release version 2.13, by Roland McGrath et al.
Copyright (C) 2011 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.
There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE.
Compiled by GNU CC version 4.4.7.
Compiled on a Linux 3.2.60 system on 2014-10-16.
Available extensions:
	crypt add-on version 2.1 by Michael Glad and others
	GNU Libidn by Simon Josefsson
	Native POSIX Threads Library by Ulrich Drepper et al
	BIND-8.2.3-T5B
libc ABIs: UNIQUE IFUNC
For bug reporting instructions, please see:
<http://www.debian.org/Bugs/>.

Can use macro at compile time or use a call at runtime to check libc version:

#include <gnu/libc-version.h>
const char *gnu_get_libc_version(void);


={============================================================================
*kt_linux_core_004* errors and errno

The syscalls(2) manual page lists the Linux system calls.

<system-call-is-expensive>
As an example of the overhead of making a system call, consider the getppid() system call, which
simply returns the process ID of the parent of the calling process. On one of the author's x86-32
systems running Linux 2.6.25, 10 million calls to getppid() required approximately 2.2 seconds to
complete. This amounts to around 0.3 microseconds per call. By comparison, on the same system, 10
million calls to a C function that simply returns an integer required 0.11 seconds, or around
one-twentieth of the time required for calls to getppid(). Of course, most system calls have
significantly more overhead than getppid().

See LPI 3.1 for more.

<form>
Usually, an error is indicated by a return of -1. Thus, a system call can be checked with code such
as the following:

fd = open(pathname, flags, mode); /* system call to open a file */

if (fd == -1) {
   /* Code to handle the error */
}


{errno}
When a system call fails, it sets the 'global' integer variable errno to a positive value that
identifies the specific error. Including the <errno.h> header file provides a declaration of errno,
as well as a set of constants for the various error numbers. All of these symbolic names
commence with E.

man errno

EINVAL          Invalid argument (POSIX.1)

<when-to-check>
Successful system calls and library functions 'never' reset errno to 0, so this variable may have a
nonzero value as a consequence of an error from a previous call. Furthermore, SUSv3 permits a
successful function call to set errno to a nonzero value (although few functions do this).

Therefore, when checking for an error, we should always first check if the function return value
indicates an error, and only then examine errno to determine the cause of the error.

<few-exception>
A few system calls (e.g., getpriority()) can legitimately return -1 on success. To determine whether
an error occurs in such calls, we set errno to 0 before the call, and then check it afterward. If
the call returns -1 and errno is nonzero, an error occurred.

<categories>
For our purposes, library functions can be divided into the following categories:

1. Some library functions return error information in exactly the same way as system calls: a -1
return value, with errno indicating the specific error. 

An example of such a function is remove(), which removes a file (using the unlink() system call) or
a directory (using the rmdir() system call). Errors from these functions can be diagnosed in the
same way as errors from system calls.

2. Some library functions return a value 'other' than -1 on error, but nevertheless set errno to
indicate the specific error condition. For example, fopen() returns a NULL pointer on error, and the
setting of errno depends on which underlying system call failed. The perror() and strerror()
  functions can be used to diagnose these errors.

3. Other library functions don't use errno at all. The method for determining the existence and
cause of errors depends on the particular function and is documented in the function's manual page.
For these functions, it is a mistake to use errno, perror(), or strerror() to diagnose errors.

note: no 'uniform' way of error reporting when use system and library calls.

<print-errnos>
to print an error message based on the errno value. The perror() and strerror() library functions
are provided for this purpose.

#include <stdio.h>
void perror(const char *msg);

The perror() function prints the string pointed to by its msg argument, 'followed' by a message
corresponding to the current value of errno.

#include <string.h>
char *strerror(int errnum);


={============================================================================
*kt_linux_core_005* error handling codes from LPI

errMsg()
Prints a message on standard error. Its argument list is the same as for printf(), except that a
terminating newline character is automatically appended to the output string. The errMsg() function
prints the error text corresponding to the current value of errno-this consists of the error name,
such as EPERM, plus the error description as returned by strerror()-followed by the formatted output
  specified in the argument list.

errExit() 
Operates like errMsg(), but also terminates the program, either by calling exit() or, if the
environment variable EF_DUMPCORE is defined with a nonempty string value, by calling abort() to
produce a core dump file.

err_exit() 
is similar to errExit(), but differs in two respects:

@ It doesn't flush standard output before printing the error message.
@ It terminates the process by calling _exit() instead of exit(). This causes the process to
terminate without flushing stdio buffers or invoking exit handlers.

The details of these differences in the operation of err_exit() will become clearer where we
describe the differences between _exit() and exit(), and consider the treatment of stdio buffers and
exit handlers in a child created by fork(). 

For now, we simply note that err_exit() is especially 'useful' if we write a library function that
creates a child process that needs to terminate because of an error. This termination should occur
without flushing the child's copy of the parent's (i.e., the calling process's) stdio buffers and
without invoking exit handlers established by the parent.

errExitEN() 
is the same as errExit(), except that instead of printing the error text corresponding to the
current value of errno, it prints the text corresponding to the error number (thus, the EN suffix)
given in the argument errnum.

Mainly, we use errExitEN() in programs that employ the POSIX threads API. Since:

<difference>
The traditional method of returning status from system calls and some library functions is to return
0 on success and -1 on error, with errno being set to indicate the error. The functions in the
pthreads API do things differently. All pthreads functions return 0 on success or a positive value
on failure. The failure value is one of the same values that can be placed in errno by traditional
UNIX system calls. 

We could diagnose errors from the POSIX threads functions using code such as the following:

errno = pthread_create(&thread, NULL, func, &arg);
if (errno != 0)
   errExit("pthread_create");

However, this approach is inefficient because errno is defined in threaded programs as a macro that
expands into a 'function' call that returns a modifiable lvalue. 

errExitEN() function allows us to write a more efficient equivalent of the above code:

int s;
s = pthread_create(&thread, NULL, func, &arg);
if (s != 0)
   errExitEN(s, "pthread_create");

note: errExitEN do not use errno and user provide it instead. this also suggest that errno is
managed per thread.

<lib/error_functions.h>

/*************************************************************************\
*                  Copyright (C) Michael Kerrisk, 2014.                   *
*                                                                         *
* This program is free software. You may use, modify, and redistribute it *
* under the terms of the GNU Lesser General Public License as published   *
* by the Free Software Foundation, either version 3 or (at your option)   *
* any later version. This program is distributed without any warranty.    *
* See the files COPYING.lgpl-v3 and COPYING.gpl-v3 for details.           *
\*************************************************************************/

/* Listing 3-2 */

/* error_functions.h

   Header file for error_functions.c.
*/
#ifndef ERROR_FUNCTIONS_H
#define ERROR_FUNCTIONS_H

/* Error diagnostic routines */

void errMsg(const char *format, ...);

#ifdef __GNUC__

    /* This macro stops 'gcc -Wall' complaining that "control reaches
       end of non-void function" if we use the following functions to
       terminate main() or some other non-void function. */

#define NORETURN __attribute__ ((__noreturn__))
#else
#define NORETURN
#endif

void errExit(const char *format, ...) NORETURN ;

void err_exit(const char *format, ...) NORETURN ;

void errExitEN(int errnum, const char *format, ...) NORETURN ;

void fatal(const char *format, ...) NORETURN ;

void usageErr(const char *format, ...) NORETURN ;

void cmdLineErr(const char *format, ...) NORETURN ;

#endif

<lib/error_functions.c>

/*************************************************************************\
*                  Copyright (C) Michael Kerrisk, 2014.                   *
*                                                                         *
* This program is free software. You may use, modify, and redistribute it *
* under the terms of the GNU Lesser General Public License as published   *
* by the Free Software Foundation, either version 3 or (at your option)   *
* any later version. This program is distributed without any warranty.    *
* See the files COPYING.lgpl-v3 and COPYING.gpl-v3 for details.           *
\*************************************************************************/

/* Listing 3-3 */

/* error_functions.c

   Some standard error handling routines used by various programs.
*/
#include <stdarg.h>
#include "error_functions.h"
#include "tlpi_hdr.h"
#include "ename.c.inc"          /* Defines ename and MAX_ENAME */

#ifdef __GNUC__                 /* Prevent 'gcc -Wall' complaining  */
__attribute__ ((__noreturn__))  /* if we call this function as last */
#endif                          /* statement in a non-void function */
static void
terminate(Boolean useExit3)
{
    char *s;

    /* Dump core if EF_DUMPCORE environment variable is defined and
       is a nonempty string; otherwise call exit(3) or _exit(2),
       depending on the value of 'useExit3'. */

    s = getenv("EF_DUMPCORE");

    if (s != NULL && *s != '\0')
        abort();
    else if (useExit3)
        exit(EXIT_FAILURE);
    else
        _exit(EXIT_FAILURE);
}

/* Diagnose 'errno' error by:

      * outputting a string containing the error name (if available
        in 'ename' array) corresponding to the value in 'err', along
        with the corresponding error message from strerror(), and

      * outputting the caller-supplied error message specified in
        'format' and 'ap'. */

static void
outputError(Boolean useErr, int err, Boolean flushStdout,
        const char *format, va_list ap)
{
#define BUF_SIZE 500
    char buf[BUF_SIZE], userMsg[BUF_SIZE], errText[BUF_SIZE];

    vsnprintf(userMsg, BUF_SIZE, format, ap);

    if (useErr)
        snprintf(errText, BUF_SIZE, " [%s %s]",
                (err > 0 && err <= MAX_ENAME) ?
                ename[err] : "?UNKNOWN?", strerror(err));
    else
        snprintf(errText, BUF_SIZE, ":");

    snprintf(buf, BUF_SIZE, "ERROR%s %s\n", errText, userMsg);

    if (flushStdout)
        fflush(stdout);       /* Flush any pending stdout */
    fputs(buf, stderr);
    fflush(stderr);           /* In case stderr is not line-buffered */
}

/* Display error message including 'errno' diagnostic, and
   return to caller */

void
errMsg(const char *format, ...)
{
    va_list argList;
    int savedErrno;

    savedErrno = errno;       /* In case we change it here */

    va_start(argList, format);
    outputError(TRUE, errno, TRUE, format, argList);
    va_end(argList);

    errno = savedErrno;
}

/* Display error message including 'errno' diagnostic, and
   terminate the process */

void
errExit(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errno, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Display error message including 'errno' diagnostic, and
   terminate the process by calling _exit().

   The relationship between this function and errExit() is analogous
   to that between _exit(2) and exit(3): unlike errExit(), this
   function does not flush stdout and calls _exit(2) to terminate the
   process (rather than exit(3), which would cause exit handlers to be
   invoked).

   These differences make this function especially useful in a library
   function that creates a child process that must then terminate
   because of an error: the child must terminate without flushing
   stdio buffers that were partially filled by the caller and without
   invoking exit handlers that were established by the caller. */

void
err_exit(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errno, FALSE, format, argList);
    va_end(argList);

    terminate(FALSE);
}

/* The following function does the same as errExit(), but expects
   the error number in 'errnum' */

void
errExitEN(int errnum, const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errnum, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Print an error message (without an 'errno' diagnostic) */

void
fatal(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(FALSE, 0, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Print a command usage error message and terminate the process */

void
usageErr(const char *format, ...)
{
    va_list argList;

    fflush(stdout);           /* Flush any pending stdout */

    fprintf(stderr, "Usage: ");
    va_start(argList, format);
    vfprintf(stderr, format, argList);
    va_end(argList);

    fflush(stderr);           /* In case stderr is not line-buffered */
    exit(EXIT_FAILURE);
}

/* Diagnose an error in command-line arguments and
   terminate the process */

void
cmdLineErr(const char *format, ...)
{
    va_list argList;

    fflush(stdout);           /* Flush any pending stdout */

    fprintf(stderr, "Command-line usage error: ");
    va_start(argList, format);
    vfprintf(stderr, format, argList);
    va_end(argList);

    fflush(stderr);           /* In case stderr is not line-buffered */
    exit(EXIT_FAILURE);
}


={============================================================================
*kt_linux_core_006* process credential

{real-and-effective}
The real user ID and group ID identify the user and group to which the process belongs. in the
/etc/passwd file

The effective user ID and group ID are used to 'determine' the permissions granted to a process when
it tries to perform various operations such as system calls. 

For example, these identifiers determine the permissions granted to a process when it accesses
resources such as files and System V interprocess communication (IPC) objects, which themselves have
associated user and group IDs determining to whom they belong. the effective user ID is also used by
the kernel to determine whether one process can send a signal to another.

A process whose effective user ID is 0 (the user ID of root) has all of the privileges of the
superuser. Such a process is referred to as a privileged process. Certain system calls can be
executed only by privileged processes.

<can-be-different>
Normally, the effective user and group IDs have the same values as the corresponding real IDs, but
there are two ways in which the effective IDs can assume different values. One way is through the
use of system calls that we discuss in Section 9.7. 

The second way is through the execution of set-user-ID and set-group-ID programs.


{set-uid-gid}
In addition, an executable file has two special permission bits: the set-user-ID and set-group-ID
bits. In fact, every file has these two permission bits, but it is their use with executable files
that interests us here.

# chmod u+s prog  # Turn on set-user-ID permission bit
# chmod g+s prog  # Turn on set-group-ID permission bit

# ls -l prog
-rwsr-sr-x 1 root root 302585 Jun 26 15:05 prog


{why}
When a set-user-ID program is run, the kernel 'sets' the effective user ID of the process to be the
same as the user ID of the executable 'file'. Running a set-group-ID program has an analogous effect
for the effective group ID of the process. Changing the effective user or group ID in this way gives
a process (in other words, the user executing the program) privileges it would not normally have.

For example, if an executable file is owned by root (superuser) and has the set-user-ID permission
bit enabled, then the process gains superuser privileges when that program is run.

<example>
Examples of commonly used set-user-ID programs on Linux include: passwd(1), which changes a userâs
password; mount(8) and umount(8), which mount and unmount file systems; and su(1), which allows a
user to run a shell under a different user ID.

$ ls -al /bin/mount
-rwsr-xr-x 1 root root 88744 Dec  9  2012 /bin/mount


={============================================================================
*kt_linux_core_007* /dev/null

/dev/null is a virtual device that always discards the data written to it. When we want to eliminate
the standard output or error of a shell command, we can redirect it to this file. Reads from this
device always return end-of-file.


={============================================================================
*kt_linux_core_008* file ownership and permissions

Each file has an associated user ID and group ID that define the owner of the file
and the group to which it belongs. The ownership of a file is used to determine the
access rights available to users of the file.

For the purpose of accessing a file, the system divides users into three categories: the owner of
the file, users, group, and the rest of the world (other). Three permission bits may be set for each
of these categories of user.

The read permission allows the contents of the file to be read; write permission allows modification
of the contents of the file; and execute permission allows execution of the file, which is either a
program or a script to be processed by some interpreter.

<search-permission>
These permissions may also be set on directories, although their meanings are slightly different:
read permission allows the contents of (i.e., the filenames in) the directory to be listed; write
permission allows the contents of the directory to be changed (i.e., filenames can be added,
    removed, and changed); and execute (sometimes called search) permission allows access to files
within the directory (subject to the permissions on the files themselves).

note: when there is no excute permission on a directory, cannot 'cd' in. this causes a failure when
do build and turns out that some dirs don't have execute on dirs for a group and make system cannot
  cd in.


# ============================================================================
#{ SIGNAL
={============================================================================
*kt_linux_core_050* signal

{signal-is-notification}
A signal is a notification to a process that an event has occurred. Sometimes described as software
interrupts and are analogous to hardware interrupts in that they interrupt the normal flow of
execution of a program.

{signal-and-kernel}
One process can (if it has suitable permissions) send a signal to another process. Can be employed
as a synchronization technique, or even as a primitive form of interprocess communication (IPC). It
is also possible for a process to send a signal to itself. However, the usual source of many signals
sent to a process is the kernel.

{signal-symbolic-names}
Each signal is defined as a unique (small) integer, starting sequentially from 1. These integers are
defined in <signal.h> with symbolic names of the form SIGxxxx. Since the actual numbers used for
each signal vary across implementations, it is these symbolic names that are always used in
programs. note: $kill -l to see signals.

{pending}
A signal is said to be generated by some event. Once generated, a signal is later delivered to a
process, which then takes some action in response to the signal. Between the time it is generated
and the time it is delivered, a signal is said to be pending. Why pending since do not know when it
is next scheduled to run, or immediately if the process is already running

{signal-mask-per-process} {signal-block}
Sometimes, however, we need to ensure that a segment of code is 'not' interrupted by the delivery of a
signal. To do this, we can add a signal to the process's signal mask-a set of signals whose delivery
is currently blocked. If a signal is generated while it is blocked, it remains pending until it is
later unblocked (removed from the signal mask).

If a process receives a signal that it is currently blocking, that signal is added to the processâs
set of pending signals.

That delivery of a signal is blocked during the execution of its handler (unless we specify the
SA_NODEFER flag to sigaction()). If the signal is (again) generated while the handler is
executing, then it is marked as pending and later delivered when the handler returns.

<signal-is-queued-or-not>
Standard signals can't be queued; delivered only once. The set of pending signals is only a mask; it
indicates whether or not a signal has occurred, but not how many times it has occurred. In other
words, if the same signal is generated multiple times while it is blocked, then it is recorded in
the set of pending signals, and later delivered, just once. One of the differences between standard
and realtime signals is that realtime signals are queued.

{signal-handler}
Instead of accepting the default for a particular signal, a program can change the action that
occurs when the signal is delivered. Can be used to ignore signals or to change the default. To
change a default is usually referred to as installing or establishing a signal handler. When a
signal handler is invoked in response to the delivery of a signal, we say that the signal has been
handled or, synonymously, caught.

<not-for-all-signals>
It isn't possible to set the disposition of a signal to terminate or dump core unless one of these
is the default disposition of the signal. The nearest we can get to this is to install a handler for
the signal that then calls either exit() or abort().  The abort() function generates a SIGABRT
signal for the process, which causes it to dump core and terminate.


{signal-reentrant} {async-signal-safe-function}
Because a signal handler may asynchronously interrupt the execution of a program at any point in
time, the main program and the signal handler in effect form two independent (although not
concurrent) threads of execution within the same process.

A function is said to be reentrant if it can safely be simultaneously executed by multiple threads
of execution in the same process. In this context, "safe" means that the function achieves its
expected result, regardless of the state of execution of any other thread of execution.

A function may be nonreentrant if it updates global or static data structures. A function that
employs only local variables is guaranteed to be reentrant because of race-condition. This book
shows an example using crypt() in both main and signal handler. This corrupts internal buffer which
is statically allocated and crypt uses when calls it with differnt parameter.

{Q} In short, it is reentrant if it do not cause race-condition. Even if it uses only local
variables, it can still cause race-condition. Doesn't it?

Such possibilities are in fact rife within the standard C library. For example, we already noted in
Section 7.1.3 that malloc() and free() maintain a linked list of freed memory blocks available for
reallocation from the heap. If a call to malloc() in the main program is interrupted by a signal
handler that also calls malloc(), then this linked list can be corrupted. For this reason, the
malloc() family of functions, and other library functions that use them, are nonreentrant.

Even if we are not using nonreentrant library functions, reentrancy issues can still be relevant. If
a signal handler updates programmer-defined global data structures that are also updated within the
main program, then we can say that the signal handler is nonreentrant with respect to the main
program.

If a function is nonreentrant, then its manual page will normally provide an explicit or implicit
indication of this fact. In particular, watch out for statements that the function uses or returns
information in statically allocated variables.

{async-signal-safe-function}
An async-signal-safe function is one that the implementation guarantees to be safe when called from
a signal handler. A function is async-signal-safe either because it is reentrant or because it is
not interruptible by a signal handler.

Table 21-1: Functions required to be async-signal-safe by POSIX.1-1990, SUSv2, and SUSv3

Real-world applications should avoid calling non-async-signal-safe functions from signal handlers.


{signal-and-proc}
The Linux-specific /proc/PID/status file contains various bit-mask fields that can be inspected to
determine a processâs treatment of signals. The bit masks are displayed as hexadecimal numbers, with
the least significant bit representing signal 1, the next bit to the left representing signal 2, and
so on. 

These fields are SigPnd (per-thread pending signals), ShdPnd (process-wide pending signals since
Linux 2.6), SigBlk (blocked signals), SigIgn (ignored signals), and SigCgt (caught signals).
(The difference between the SigPnd and ShdPnd fields will become clear when we describe the handling
 of signals in multithreaded processes in Section 33.2.) The same information can also be obtained
using various options to the ps(1) command.

note: signals for thread and process.


{standard-and-realtime-signal}
Signals fall into two broad categories; standard and realtime. On Linux, the standard signals are
numbered from 1 to 31. We describe the standard signals in this chapter. The other set of signals
consists of the realtime signals.

<why-reliable-signal>
In early implementations, signals could be lost (i.e., not delivered to the target process) in
certain circumstances. Furthermore, although facilities were provided to block delivery of signals
while critical code was executed, in some circumstances, blocking was not reliable. These problems
were remedied in 4.2BSD, which provided so-called reliable signals.

However, the Linux signal(7) manual page lists more than 31 signal names. The excess names can be
accounted for in a variety of ways. Some of the names are simply synonyms for other names, and are
defined for source compatibility with other UNIX implementations. Other names are defined but
unused.


{signals}
<SIGHUP> see process group for more.
When a terminal disconnect (hangup) occurs, this signal is sent to the 'controlling' process of the
terminal. A second use of SIGHUP is with daemons (e.g., init, httpd, and inetd). Many daemons are
designed to respond to the receipt of SIGHUP by reinitializing themselves and rereading their
configuration files. The system administrator triggers these actions by manually sending SIGHUP to
the daemon, either by using an explicit kill command or by executing a program or script that does
the same.

<SIGABRT>
The abort() function (Section 21.2.2) generates a SIGABRT signal for the process, which causes it to
dump core and terminate.

<SIGINT>
When the user types the terminal interrupt character (usually Control-C), the terminal driver sends
this signal to the foreground process group. The default action for this signal is to terminate the
process.

<SIGPIPE> broken-pipe
This signal is generated when a process tries to write to a pipe, a FIFO, or a socket for which
there is no corresponding reader process. This normally occurs because the reading process has
closed its file descriptor for the IPC channel. See Section 44.2 for further details.

<SIGSEGV>
This very popular signal is generated when a program makes an invalid memory reference. A memory
reference may be invalid because the referenced page doesnât exist e.g., it lies in an unmapped area
somewhere between the heap and the stack, the process tried to update a location in read-only memory
e.g., the program text segment or a region of mapped memory marked read-only, or the process tried
to access a part of kernel memory while running in user mode (Section 2.1). In C, these events often
result from dereferencing a pointer containing a bad address (e.g., an uninitialized pointer) or
passing an invalid argument in a function call. The name of this signal derives from the term
segmentation violation.

<SIGKILL>
This is the sure kill signal. It canât be blocked, ignored, or caught by a handler, and thus always
terminates a process.

<SIGTERM>
This is the standard signal used for terminating a process and is the 'default' signal sent by the
'kill' and 'killall' commands. 

<difference-between-sigkill-and-sigterm>
Users sometimes explicitly send the SIGKILL signal to a process using kill -KILL or kill -9.
However, this is generally a mistake. A well-designed application will have a handler for SIGTERM
that causes the application to exit 'gracefully', cleaning up temporary files and releasing other
resources beforehand. Killing a process with SIGKILL bypasses the SIGTERM handler. Thus, we should
always first attempt to terminate a process using SIGTERM, and reserve SIGKILL as a last resort for
killing runaway processes that don't respond to SIGTERM. 

note: this means that if use sure kill, then can cause resource locked up and leaking.

note: what if there is no handler of SIGTERM? will be terminated by kernel anyway? when tried to
kill a simple application which do sleep and loop, it is terminated when use kill, SIGTERM.

<SIGURG>
This signal is sent to a process to indicate the presence of out-of-band (also known as urgent) data
on a socket

<SIGCHLD>
This signal is sent (by the kernel) to a parent process when one of its children terminates: either
by calling exit() or as a result of being killed by a signal. It may also be sent to a process when
one of its children is stopped or resumed by a signal. By default, SIGCHLD is ignored.

<SIGUSR1>
This signal and SIGUSR2 are available for programmer-defined purposes. The kernel never generates
these signals for a process. Processes may use these signals to notify one another of events or to
synchronize with each other. In early UNIX implementations, these were the only two signals that
could be freely used in applications. In fact, processes can send one another any signal, but this
has the potential for confusion if the kernel also generates one of the signals for a process.
Modern UNIX implementations provide a large set of realtime signals that are also available for
programmer-defined purposes (Section 22.8).


={============================================================================
*kt_linux_core_051* signal: example: use signal as synchronization

#include <signal.h>
#include "curr_time.h" /* Declaration of currTime() */
#include "tlpi_hdr.h"

#define SYNC_SIG SIGUSR1 /* Synchronization signal */

static void /* Signal handler - does nothing but return */
handler(int sig)
{
}

int
main(int argc, char *argv[])
{
  pid_t childPid;
  sigset_t blockMask, origMask, emptyMask;
  struct sigaction sa;

  setbuf(stdout, NULL); /* Disable buffering of stdout */

  // The sigemptyset() function initializes a signal set to contain no members. The sigfillset()
  // function initializes a set to contain all signals (including all realtime signals). After
  // initialization, individual signals can be added to a set using sigaddset() and removed using
  // sigdelset().

  sigemptyset(&blockMask);
  sigaddset(&blockMask, SYNC_SIG); /* Block signal */

  // We can use sigprocmask() to change the process signal mask, to retrieve the existing mask, or
  // both. The how argument determines the changes that sigprocmask() makes to the signal mask:

  if (sigprocmask(SIG_BLOCK, &blockMask, &origMask) == -1)
    errExit("sigprocmask");

  sigemptyset(&sa.sa_mask);
  sa.sa_flags = SA_RESTART;
  sa.sa_handler = handler;

  if (sigaction(SYNC_SIG, &sa, NULL) == -1)
    errExit("sigaction");

  switch (childPid = fork()) {
    case -1:
      errExit("fork");

    case 0: /* Child */

      /* Child does some required action here... */
      printf("[%s %ld] Child started - doing some work\n", currTime("%T"), (long) getpid());
      sleep(2); /* Simulate time spent doing some work */

      /* And then signals parent that it's done */
      printf("[%s %ld] Child about to signal parent\n", currTime("%T"), (long) getpid());

      if (kill(getppid(), SYNC_SIG) == -1)
        errExit("kill");

      /* Now child can do other things... */
      _exit(EXIT_SUCCESS);

    default: /* Parent */

      /* Parent may do some work here, and then waits for child to complete the required action */
      printf("[%s %ld] Parent about to wait for signal\n", currTime("%T"), (long) getpid());

      sigemptyset(&emptyMask);
      if (sigsuspend(&emptyMask) == -1 && errno != EINTR)
        errExit("sigsuspend");

      printf("[%s %ld] Parent got signal\n", currTime("%T"), (long) getpid());

      /* If required, return signal mask to its original state */
      if (sigprocmask(SIG_SETMASK, &origMask, NULL) == -1)
        errExit("sigprocmask");

      /* Parent carries on to do other things... */
      exit(EXIT_SUCCESS);
  }
}


={============================================================================
*kt_linux_core_052* signal: kill: checking for the existence of a process

Can send a signal to another process using the kill() system call, which is the analog of the kill
shell command.

#include <signal.h>

int kill(pid_t pid, int sig); Returns 0 on success, or -1 on error

If no process matches the specified pid, kill() fails and sets errno to ESRCH ("No such process"). A
process needs appropriate permissions to be able send a signal to another process.

If pid equals 0, the signal is sent to every process in the same process group as the calling
process, including the calling process itself. 

<signal-to-process-group>
If pid is less than -1, the signal is sent to all of the processes in the process group whose ID
equals the absolute value of pid. Sending a signal to all of the processes in a process group finds
particular use in shell job control.

<check-existence>
The kill() system call can serve another purpose. If the sig argument is specified as 0, the
so-called null signal, then no signal is sent. Instead, kill() merely performs error checking to see
if the process 'can' be signaled. 

Read another way, this means we can use the null signal to test if a process with a specific process
  ID exists. 

1. If sending a null signal fails with the error ESRCH, then we know the process doesn't exist. 
  
2. If the call fails with the error EPERM (meaning the process exists, but we don't have permission
    to send a signal to it) or 

3. succeeds (meaning we do have permission to send a signal to the process), then we know that the
process exists.

So this code do sleep while a process of pid is alive.

while (0 == kill(parent_pid, 0))
{
    sleep(1);
}


<but>
Verifying the existence of a particular process ID doesn't guarantee that a 'particular' program is
still running. Because the kernel recycles process IDs as processes are born and die, the same
process ID may, over time, refer to a different process.


={============================================================================
*kt_linux_core_100* thread vs process

A process is an instance of an executing program and a thread is an instance of an executing a task.
In other words, a process is processor abstract, an abstract entity defined by kernel and with
allocated resources in order to execute a program.  

UNIX programs have a single thread of execution: the CPU processes instructions for a single logical
flow of execution through the program. In a multithreaded program, there are multiple, independent,
concurrent logical flows of execution within the same process.

<key> In short, a process is an abstract(virtualisation) of a processor and a thread is an abstract
of single flow of execution.

<on-linux>
Two virtualisation in Linux: processer and memory.

Linux don't distinguish a process(task) with a thread. The thread is just a special process: 

1. Linux kernel scheduler schedules based on 'thread'.
2. Linux kernel has a double-linked list which has <thread_info> struct element which has
thread_struct*.
3. Thread is special since it shares resource (open files, pending signals, internal kernel data,
process state, address space, text and data section) with others threads.

In other words, process is a single thread that don't share resources.

This is clear when see how to create a thread:

clone( CLONE_VM| CLONE_FS| CLONE_FILES| CLONE_SIGHAND, 0);

to create a process:

clone(SIGCHLD, 0);

Two usual steps to create a process:

fork();  // copy a child from a parent and actually use clone() call
exec*()  // load a new program text.


{shared-and-not-shared-between-threads}
From CH29 in {ref-LPI} and {ref-UNP}

<shared>
The attributes that are shared; in other words, global attributes to a process:

the same global memory, process code and most data;
process ID and parent process ID;
process group ID and session ID;
process credentials (user and group IDs);

open file descriptors;
signal dispositions;
file system-related information: umask, current working directory, and root directory;
resource limits;

<not-shared>
The attributes for each thread:

thread ID (Section 29.5);
signal mask;
thread-specific data (Section 31.3);
alternate signal stack (sigaltstack());
the errno variable;
realtime 'scheduling' policy and priority (Sections 35.2 and 35.3);
capabilities (Linux-specific, described in Chapter 39); and
'stack' (local variables and function call linkage information).
'registers' including PC and SP


{multithreaded-vs-singlethreaded}
The mutiltithreaded means that a single process has multi threads, 'lightweight'-process. The
singlethreaed means that a single process has a single thread. MT has less IPC but prone to error
because shares resources; less protection. ST has more IPC but more protection. 

note: tradeoff between IPC and protection.


{why-thread}
From {ref-LPI}. Problems with fork:

1) The fork call is expensive because memory is copied from the parent to the child. Although
<copy-on-write> is used to avoid this, but still expensive.

2) Need IPC after fork between parent and child.

Thread help with these problems as it is 'lightweight'-process in terms of 'creation' cost:

Thread creation is faster because many of the attributes that must be duplicated in a child created
by fork() are instead shared between threads.

Sharing information between threads is easy and fast. It is just a matter of copying data into
shared (global or heap) variables.

However, has disadvantages:

1) more efforts to ensure thread-safe
2) buggy thread can damage all of the threads in the process. less protection.
3) each thread is competing for use of the finite virtual address space of a host process.
4) usually desirable to avoid the use of signals in multi-threaded programs since requires careful
designs.
5) should run the same program.
6) more threads, more memory and context switching.


{process}
<process-id>
From {ref-LPI} PID is integer type and the Linux limits PIDs to 32767. Once it has reached 32,767, 
the process ID counter is reset to 300, rather than 1. This is done because many low-numbered
process IDs are in permanent use by system processes and daemons, and thus time would be wasted
searching for an unused process ID in this range. 

In Linux 2.4 and earlier, the process ID limit of 32,767 is defined by the kernel constant PID_MAX.
With Linux 2.6, things change. While the default upper limit for process IDs remains 32,767, this
limit is adjustable via the value in the Linux-specific /proc/sys/kernel/pid_max file (which is one
greater than the maximum process ID). On 32-bit platforms, the maximum value for this file is
32,768, but on 64-bit platforms, it can be adjusted to any value up to 222 (approximately 4
million), making it possible to accommodate very large numbers of processes.


{user-and-kernel-stack}
The kernel stack is a per-process memory region maintained in kernel memory that is used as the
stack for execution of the functions called internally during the execution of a system call.

Each (user) stack frame contains the following information:

1. Function arguments and local variables
2. Some registers. RA.

note: needs more about difference?


={============================================================================
*kt_linux_core_101* process creation

LPI 24.

{process-creation}
The wait(&status) system call has two purposes. First, if a child of this process has not yet
terminated by calling exit(), then wait() suspends execution of the process until one of its
children has terminated. Second, the termination status of the child is returned in the status
argument of wait().

<fork-and-exec>
The UNIX approach is usually simpler and more elegant. Separating these two steps makes the APIs
simpler and allows a program a great degree of flexibility in the actions it performs between the
two steps. Moreover, it is often useful to perform a fork() without a following exec().

Parent process running program A
      |
      A
      |
      child PID = fork(void);                   Child process running program A. 0 = fork(void);
      |                                               |
Parent may perform other atcions here                 A  
      |                                               | 
      wait(&status); // optional                Child may perform further actions here
                                                      |
                                                      execev( B, ... ); // optional
                                                      |
                                                      B
                                                      |
                                                Execution of program B
                                                      |
                                                      exit(status);
                                                      1. Child status passed to parent and kernel
                                                      restarts parent.
                                                      2. Delivers SIGCHLD optionally.

The following idiom is sometimes employed when calling fork():

pid_t childPid; /* Used in parent after successful fork() to record PID of child */

switch (childPid = fork()) {
  case -1: /* fork() failed */
    /* Handle error */
  case 0: /* Child of successful fork() comes here */
    /* Perform actions specific to child */
  default: /* Parent comes here after successful fork() */
    /* Perform actions specific to parent */
}

<sharing-between-parent-and-child>
The child's stack, data, and heap segments are initially exact duplicates of the corresponding parts
the parent's memory. The child receives duplicates of all of the parent's file descriptors.

<race-condition-after-fork>
After a fork(), it is indeterminate which of the two is next scheduled to use CPU. If we need to
guarantee a particular order, we must use some kind of synchronization technique.

<case-use-signal>
Avoiding Race Conditions by Synchronizing with Signals. Although, in practice, such coordination is
more likely to be done using semaphores, file locks, or message passing. See *kt_linux_core_401* for
code example.


={============================================================================
*kt_linux_core_102* process: termination

LPI 25.

{two-ways}
A process may terminate in two general ways. One of these is abnormal termination, caused by the
delivery of a signal whose default action is to terminate the process (with or without a core dump).
Alternatively, a process can terminate normally, using the _exit() system call.

#include <unistd.h>
void _exit(int status);


{status-vaule}
By convention, a termination status of 0 indicates that a process completed successfully, and a
nonzero status value indicates that the process terminated unsuccessfully. There are no fixed rules
about how nonzero status values are to be interpreted; SUSv3 specifies two constants, EXIT_SUCCESS
(0) and EXIT_FAILURE (1)

Performing an explicit return n is generally equivalent to calling exit(n), since the run-time
function that invokes main() uses the return value from main() in a call to exit().

Performing a return without specifying a value, or falling off the end of the main() function, also
results in the caller of main() invoking exit(), but with results that vary depending on the version
of the C standard supported and the compilation options employed:


{exit-library-call}
Programs generally don't call _exit() directly, but instead call the exit() library function,
which performs various actions before calling _exit().

#include <stdlib.h>
void exit(int status);

The following actions are performed by exit():

1. Exit handlers (functions registered with atexit() and on_exit()) are called, in reverse order of
their registration (Section 25.3).

2. The stdio stream buffers are flushed.

3. The _exit() system call is invoked, using the value supplied in status.

Unlike _exit(), which is UNIX-specific, exit() is defined as part of the standard C library; that
is, it is available with every C implementation.


{actions-on-termination}
During both normal and abnormal termination of a process, the following actions
occur:

1. Open file descriptors, directory streams (Section 18.8), message catalog descriptors
(see the catopen(3) and catgets(3) manual pages), and conversion descriptors
(see the iconv_open(3) manual page) are closed.

2. As a consequence of closing file descriptors, any file locks (Chapter 55) held by
this process are released.

<3>. Any attached System V shared memory segments are detached, and the shm_nattch counter
corresponding to each segment is decremented by one.

4. For each System V semaphore for which a semadj value has been set by the process,
that semadj value is added to the semaphore value. (Refer to Section 47.8.)

<5>. If this is the controlling process for a controlling terminal, then the SIGHUP signal is sent to
each process in the controlling terminal's foreground process group, and the terminal is
disassociated from the session.

6. Any POSIX named semaphores that are open in the calling process are closed as though sem_close()
  were called.

7. Any POSIX message queues that are open in the calling process are closed as though mq_close()
  were called.

8. If, as a consequence of this process exiting, a process group becomes orphaned and there are any
stopped processes in that group, then all processes in the group are sent a SIGHUP signal followed
by a SIGCONT signal. We consider this point further in Section 34.7.4.

9. Any memory locks established by this process using mlock() or mlockall() (Section 50.2) are
removed.

10. Any memory mappings established by this process using mmap() are unmapped.


{exit-handler}


{stdio-buffers-and-exit}

#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

int main(int argc, char** argv)
{
  printf("Hello world.\n");
  write( STDOUT_FILENO, "Ciao\n", 5);

  // setbuf( stdout, NULL );

  if( -1 == fork() )
  {
    fprintf( stderr, "fork() failed\n" );
    exit(EXIT_FAILURE);
  }

  exit(EXIT_SUCCESS);
}

$ gcc sample-two.c 
$ ./a.out 
Hello world.
Ciao

$ ./a.out > a
$ cat a
Ciao
Hello world.
Hello world.

<problem> Why different output order and duplicates?

1. duplicates.
Recall that the stdio buffers are maintained in a process's user-space memory (refer to 13.2).
Therefore, these buffers are duplicated in the child by fork(). When standard output is directed to
a terminal, it is line-buffered by default, with the result that the newline-terminated string
written by printf() appears 'immediately'. 

However, when standard output is directed to a file, it is block-buffered by default. Thus the
string written by printf() is still in the parent's stdio buffer at the time of the fork(), and this
string is duplicated in the child. When the parent and the child later call exit(), they both flush
their copies of the stdio buffers, resulting in duplicate output.

To remedy this, can use setbuf() and fflush() to flush the stdio buffer prior to a fork() call.
However, still have different order.

$ ./a.out 
Hello world.
Ciao

$ ./a.out > a
$ cat a
Ciao
Hello world.

2. the output order.
The write() transfers data directly to a kernel buffer, and this buffer is not duplicated during a
fork(). note: write is a system call.

The output of write() appears before that from printf() because the output of write() is immediately
transferred to the kernel buffer cache, while the output from printf() is transferred only when the
stdio buffers are flushed by the call to exit().

note: In general, care is required when mixing stdio functions and system calls to perform I/O on
the same file, as described in Section 13.7.


={============================================================================
*kt_linux_core_103* process: monitor child process

{monitoring-child-process}
From #26 in ref-LPI. 
<wait-system-call>
The wait() system call waits for one of the children of the calling process to terminate and returns
the termination status of that child in the buffer pointed to by status.

#include <sys/wait.h>
pid_t wait(int *status);

If no (previously unwaited-for) child of the calling process has yet terminated, the call blocks
until one of the children terminates. If a child has already terminated by the time of the call,
wait() returns immediately.

As its function result, wait() returns the process ID of the child that has terminated.

On error, wait() returns -1. One possible error is that the calling process has no previously
unwaited-for children, which is indicated by the errno value ECHILD. This means that we can use the
following loop to wait for all children of the calling process to terminate:

while ((childPid = wait(NULL)) != -1)
  continue;
if (errno != ECHILD) /* An unexpected error... */
  errExit("wait");

<waitpid-system-call>
The wait() system call has a number of limitations, which waitpid() was designed to address. See the
reference for more details.

#include <sys/wait.h>
pid_t waitpid(pid_t pid, int *status, int options);


={============================================================================
*kt_linux_core_104* process: zombie

{zombie-process} orphan process, init process 
2014.02 from google phone interview. The lifetimes of parent and child processes are usually not the
same-either the parent outlives the child or vice versa. This raises two questions: 

1. Who becomes the parent of an orphaned child? Each process has a parent-the process that created
it. If a child process becomes orphaned because its "birth" parent terminates, then the child is
adopted by the 'init' process, and subsequent calls to getppid() in the child return 1. See Section
26.2. The process 1, init, the ancestor of all processes. The init process adopts the child and
automatically performs a wait(), thus removing the zombie process from the system.

2. What happens to a child that terminates before its parent has had a chance to perform a wait()?
The point here is that, although the child has finished its work, the parent should still be
permitted to perform a wait() at some later time to determine how the child terminated. The kernel
deals with this situation by 'turning' the child into a <zombie>. This means that most of the
resources held by the child are released back to the system to be reused by other processes. The
only part of the process that remains is an entry in the kernel's process table recording; among
other things the child's process ID, termination status, and resource usage statistics. Section
36.1.

note: after all, kernel make a process a zombie when not able to call wait() on it.

A zombie process can't be killed by a signal, not even the (silver bullet) SIGKILL. This ensures
that the parent can always eventually perform a wait(). When the parent does perform a wait(), the
kernel removes the zombie, since the last remaining information about the child is no longer
required.

<why-zombie-can-be-a-problem> note: second case when zombie is created
If a parent creates a child, but fails to perform a wait(), then an entry for the zombie child will
be maintained indefinitely in the kernel's process table. If a large number of such zombie children
are created, they will eventually fill the kernel process table, 'preventing' the creation of new
processes. Since the zombies can't be killed by a signal, the only way to remove them from the
system is to kill their parent (or wait for it to exit), at which time the zombies are adopted and
waited on by init, and consequently removed from the system.

$ ./make_zombie
Parent PID=1013
Child (PID=1014) exiting
  1013 pts/4 00:00:00 make_zombie Output from ps(1)
  1014 pts/4 00:00:00 make_zombie <defunct>
After sending SIGKILL to make_zombie (PID=1014):
  1013 pts/4 00:00:00 make_zombie Output from ps(1)
  1014 pts/4 00:00:00 make_zombie <defunct>

In the above output, we see that ps(1) displays the string <defunct> to indicate a process in the
zombie state.

A common way of reaping dead child processes is to establish a handler for the SIGCHLD signal.

<key> The zombie is created in two cases: child terminates early before parent calls wait and parent
fail to wait for child. The second case is a real problem. The problem is that kernel's process
table fills up and prevent creatring new processes. The zombies will be adopted by init process
eventually and can be removed by killing init process.


={============================================================================
*kt_linux_core_105* process: memory layout

LPI 6. This is a layout in virtual memory.

<text-segment> 
The text segment contains the machine-language instructions of the program run by the process. The
text segment is made 'read'-only so that a process doesn't accidentally modify its own instructions
via a bad pointer value. Since many processes may be running the same program, the text segment is
made 'sharable' so that a single copy of the program code can be mapped into the virtual address space
of all of the processes.

<initialized-data-segment>
The initialized data segment contains 'global' and 'static' variables that are explicitly
initialized. The values of these variables are read from the executable file when the program is
loaded into memory.

<uninitialized-data-segment> <bss>
The uninitialized data segment contains 'global' and 'static' variables that are 'not' explicitly
initialized. Before starting the program, the system initializes all memory in this segment to 0.
For historical reasons, this is often called the bss segment, a name derived from an old assembler
mnemonic for "block started by symbol." The main reason for placing global and static variables that
are initialized into a separate segment from those that are uninitialized is that, when a program is
stored on disk, it is not necessary to allocate space for the uninitialized data. Instead, the
executable merely needs to record the location and size required for the uninitialized data segment,
and this space is allocated by the program loader at 'runtime'.

<stack>
The stack is a dynamically growing and shrinking segment containing stack frames. One stack frame is
allocated for each currently called function. A frame stores the function's local variables
(so-called automatic variables), arguments, and return value. Stack frames are discussed in more
detail in Section 6.5.

<heap>
The heap is an area from which memory (for variables) can be dynamically allocated at run time. The
top end of the heap is called the program break.

<segment-and-section>
Sometimes, the term section is used instead of segment, since section is more consistent with the
terminology used in the now ubiquitous ELF specification for executable file formats.

<etext>
Three global symbols: etext, edata, and end. These symbols can be used from within a program to
obtain the addresses of the next byte past, respectively, the end of the program text, the end of
the initialized data segment, and the end of the uninitialized data segment.


={============================================================================
*kt_linux_core_106* process: virtual memory

{virtual-memory} locality-of-reference
The aim of this virtual memory is to make efficient use of both the CPU and RAM (physical memory) by
exploiting a property that is typical of most programs: locality of reference. Most programs
demonstrate two kinds of locality:

1. 'spatial' locality is the tendency of a program to reference memory addresses that are 'near' those
that were recently accessed (because of 'sequential' processing of instructions, and, sometimes,
sequential processing of data structures).

2. 'temporal' locality is the tendency of a program to access the 'same' memory addresses in the near
future that it accessed in the recent past (because of loops).

The upshot of locality of reference is that it is possible to execute a program while maintaining
only 'part' of its address space in RAM.

<paging>
A virtual memory scheme splits the memory used by each program into small, fixed-size units called
pages. Correspondingly, RAM is divided into a series of page frames of the same size. At any one
time, only some of the pages of a program need to be resident in physical memory page frames; these
pages form the so-called resident set. Copies of the unused pages of a program are maintained in the
'swap' area-a reserved area of disk space used to supplement the computer's RAM-and loaded into
physical memory only as required. 

<page-fault>
When a process references a page that is 'not' currently 'resident' in physical memory, a page
'fault' occurs, at which point the kernel suspends execution of the process while the page is loaded
from disk into memory.

The kernel maintains a page table for 'each'-process (Figure 6-2). The page table describes the
location of each page in the process's virtual address space (the set of all virtual memory pages
available to the process). 

process virtual address space    page table        physical memory(RAM) page frames 
low   page 0                           4                    0
      page 1                           2                    1
      page 2                           7                    2
high  page 3                           0                    3

Each entry in the page table either 'indicates' the location of a virtual page in RAM or indicates
that it currently resides on disk.

Not all address ranges in the process's virtual address space require page-table entries. Typically,
large ranges of the potential virtual address space are unused, so that it isn't necessary to
maintain corresponding page-table entries. If a process tries to access an address for which
there is 'no' corresponding page-table entry, it receives a SIGSEGV signal.

<process-virtual-memory-address>
The ** areas represent invalid ranges in the process's virtual address space; that is, areas for
which page tables have not been created.

High           +----------------+
                  ** kernel. mapped into process virtual memory but not accessible to program.
               +----------------+                  0xC0000000
                  argv, environ
               +----------------+
                  stack (grows down)
top of stack   +----------------+
                  ** unallocated
               +----------------+
                  heap (grows up)
               +----------------+ < &end
                  bss
               +----------------+ < &edata
                  inited data
               +----------------+ < &etext
                  text
               +----------------+

Low            +----------------+                  0x00000000

note: what's the kernel in this digram?


={============================================================================
*kt_linux_core_107* process: group

LPI 34.

{why-group-and-session}
A process group is a collection of 'related' processes, and a session is a collection of 'related'
process groups. Process groups and sessions are abstractions defined to 'support' two uses:

1. shell job control, which allows interactive users to run commands in the foreground or in the
background. note: So need to know fine details when write a shell.

2. send a signal to a group.


{process-group} also known as job
A process group is a set of one or more processes sharing the same process group identifier (PGID).
A process group ID is a number of the same type (pid_t) as a process ID. A process group has a
process group 'leader', which is the process that 'creates' the group and whose process ID becomes
the process group ID of the group. A new process inherits its parent's process group ID.

note: the group leader has PID == PGID.

A process group has a lifetime, which is the period of time beginning when the leader creates the
group and ending when the last member process leaves the group.


{process-session}
A session is a collection of process groups. A process's session membership is determined by its
session identifier (SID), which, like the process group ID, is a number of type pid_t. A session
'leader' is the process that 'creates' a new session and whose process ID becomes the session ID. A
new process inherits its parent's session ID.

note: the session leader has PID == PGID == SID. That is group and session leader.


{controlling-process-and-terminal}
All of the processes in a session 'share' a single controlling terminal. The controlling terminal is
established when the 'session' leader first 'opens' a terminal device, /dev/tty. A terminal may be
the controlling terminal of at most 'one' session. Opening the controlling terminal also causes the
session leader to become the 'controlling' process for the terminal.

<forground-process-group>
At any point in time, one of the process groups in a session is the foreground process group for the
terminal, and the others are background process groups. 'only' processes in the foreground process
group can read input from the controlling terminal. note: process'es'

<signal-forground-group>
When the user types one of the signal-generating terminal characters on the controlling terminal, a
signal is sent to 'all' members of the 'foreground' process group. These characters are the
interrupt character (usually Control-C), which generates SIGINT; the quit character (usually
    Control-\), which generates SIGQUIT; and the suspend character (usually Control-Z), which
    generates SIGTSTP.


{use-other-case} 
Process groups occasionally find uses in areas other than job control, since they have two useful
properties: a parent process can wait on any of its children in a particular process group (Section
  26.1.2), and a signal can be sent to all of the members of a process group.

note: Use kill system call and see <signal-to-process-group> for detail.


{use-job-control} login-shell
Why session and group? Sessions and process groups were defined to support 'shell' job control

For an interactive login, the controlling terminal is the one on which the user logs in. The login
shell becomes the session leader and the controlling process for the terminal, and is also made the
'sole' member of its own process group. 

'each' job(a simple command or pipeline of commands) started from the shell results in the creation
of one or more processes, and the shell places all of these processes in a 'new' process group.

$ echo $$                              " Display the PID of the shell
400
$ find / 2> /dev/null | wc -l &        " Creates 2 processes in background group
[1] 659
$ sort < longlist | uniq -c            " Creates 2 processes in foreground group

<---------------------------- session 400 ----------------------------------->
bash (session/group leader)      find (group leader)        sort(group leader)
   PID  = 400                       PID  = 658                 PID  = 660
   PPID = 399                       PPID = <400>               PPID = <400>
   PGID = 400                       PGID = 658                 PGID = 660
   SID  = 400                       SID  = 400                 SID  = 400

                                 wc                         uniq
                                    PID  = 659                 PID  = 661
                                    PPID = <400>               PPID = <400>
                                    PGID = 658                 PGID = 660
                                    SID  = 400                 SID  = 400

<-------------------------->    <-------------------->    <------------------>
process group 400                process group 658          process group 660
'controlling' process
background process group         background group           forground group

<controlling-terminal>
Forground PGID    = 660
Controlling SID   = 400


{group}
<calls>
The setpgid() system call changes the process group of the process whose process ID is pid to the
value specified in pgid. Put simply, change pgid of a process with pid.

#include <unistd.h>
pid_t getpgrp(void);
Always successfully returns process group ID of calling process

int setpgid(pid_t pid, pid_t pgid);
Returns 0 on success, or -1 on error

If pid is specified as 0, the 'calling' process's process group ID is changed. If pgid is specified
as 0, then the process group ID of the process specified by pid is made the same as its process ID.
Thus, the following setpgid() calls are equivalent:

setpgid(0, 0);
setpgid(getpid(), 0);
setpgid(getpid(), getpid());

note: Put simply, this call is to change pgid of a process with the pid. However, when pid is 0, it
is for the calling process.

If the pid and pgid arguments specify the 'same' process, then a 'new' process group is created, and
the specified process is made the 'leader' of the new group. The typical callers of setpgid() and
setsid() are programs such as the shell 

If the two arguments specify different values, then setpgid() is being used to move a process
between process groups.

<group-creation>
The setpgid(0,0) is a way to create a new group and a group leader once a process is created such as
fork as shown example below.

However, from LPI 34-1 code:

childPid = fork();
switch (childPid) 
{
  case -1: /* fork() failed */
  /* Handle error */

  case 0: /* Child */
    if (setpgid(0, pipelinePgid) == -1)
    /* Handle error */
    /* Child carries on to exec the required program */

   default: /* Parent */
      if (setpgid(childPid, pipelinePgid) == -1 && errno != EACCES)
        /* Handle error */
        /* Parent carries on to do other things */
}

and 

Each job(a simple command or pipeline of commands) started from the shell results in the creation of
one or more processes, and the shell places all of these processes in a 'new' process group.

note: Q? 'not' clear when and how a group is created? The <example> seems to make more sense.

note: The above code is to show "How a job-control shell sets the process group ID of a child
process" because the scheduling of the parent and child is indeterminate after a fork(). 

Therefore, job-control shells are programmed so that the parent and the child process both call
setpgid() to change the childâs process group ID to the same value immediately after a fork(), and
the parent ignores any occurrence of the EACCES error on the setpgid() call.

So seems that the group creation is done before doing this.

<restrictions>
1. The pid argument may specify only the calling process or one of its children. Violation of this
rule results in the error ESRCH.

2. A process may 'not' change the process group ID of one of its children 'after' that child has
performed an exec(). Violation of this rule results in the error EACCES. The rationale for this
constraint is that it could confuse a program if its process group ID were changed after it had
commenced.

This restriction affects the programming of job-control shells, which have the following
requirements:

<sent-by-shell>
All of the processes in a job (i.e., a command or a pipeline) must be placed in a single process
group. This step permits the 'shell' to use killpg() (or, kill() with a negative pid argument) to
simultaneously send job-control signals to 'all' of the members of the process group. Naturally this
step must be carried out before any job-control signals are sent.

killpg - send signal to a process group

#include <signal.h>
int killpg(int pgrp, int sig);

<example>
The thing is that when a shell runs this line, creates a child process to run this application and
in this application, it make a new background group and make it itself a group leader.

note: In the first time, scripts runs "else" and set PPID with the pid that runs this script

#!/bin/bash

# to clean up A/V resources when a process dies, especially on a crash.
#
this_script=$0
prefix=@prefix@
parent_pid=${NEXUS_INSPECT_PARENT_PID:-}

if [ -n "${parent_pid}" ];
then
        echo "Going to watch pid: ${parent_pid}"
        while kill -0 "${parent_pid}" &>/dev/null;
        do
                usleep 500
        done
        echo "Pid ${parent_pid} has died. Going to cleanup."
        ${prefix}/bin/nexus-inspect -r -p "${parent_pid}"
else
        [ -x "${prefix}/bin/nexus-inspect" ] && \
            NEXUS_INSPECT_PARENT_PID=$$ \
               ${prefix}/bin/setpgid-and-exec bash "${this_script}" &
        exec "${@}"
fi


/*
 * The purpose of this simple program is to launch a program in a new process
 * group and have it become the group leader.
 *
 * This is useful in situations where the launched program fork()s and we want
 * to be sure that when we kill it, all of it's children are also killed.
 */

note: Q? does this mean that if kill group leader then all children will be killed automatically? 

#include <unistd.h>
#include <error.h>
#include <stdio.h>

int main(int argc, char* argv[]) {

  if (argc < 2) {
    fprintf(stderr, "Usage: %s <program> [ <arg> ... ]", argv[0]);
    exit(1);
  }

  /* Create new group and become group leader */
  if (setpgid(0, 0)) {
    perror("setpgid failed!");
    exit(2);
  }

  /* Execute the command */
  if (execvp(argv[1], (char*const*)(argv+1)) != 0) {
    perror("exec failed!");
    exit(3);
  }
}


{session}
<calls>
If the calling process is 'not' a process group leader, setsid() creates a 'new' session.

#include <unistd.h>
pid_t setsid(void);

Returns session ID of new session, or (pid_t) -1 on error

The setsid() system call creates a new session as follows:

1. The calling process becomes the leader of a new session, and is made the leader of a new process
group within that session. The calling processâs process group ID and session ID are set to the same
value as its process ID. note: this is a login shell example.

2. The calling process has 'no' controlling terminal. Any previously existing connection to a
controlling terminal is broken. note: Upon creation, a session has no controlling terminal

<restriction>
If the calling process is a process group leader, setsid() fails with the error EPERM. The simplest
way of ensuring that this doesn't happen is to perform a fork() and have the parent exit while the
child carries on to call setsid(). Since the child inherits its parent's process group ID and
receives its own unique process ID, it can't be a process group leader.

The restriction that group leader cannot call setsid() is necessary because, without it, the process
group leader would be able to place itself in another (new) session,

<example>
Shows the use of setsid() to create a new session. To check that it no longer has a controlling
terminal, this program attempts to open the special file /dev/ tty

$ ps -p $$ -o 'pid pgid sid command'      # $$ is PID of shell
PID PGID SID COMMAND
12243 12243 12243                         # bash PID, PGID, and SID of shell

$ ./t_setsid
$ PID=12352, PGID=12352, SID=12352
ERROR [ENXIO Device not configured] open /dev/tty

As can be seen from the output, the process successfully places itself in a new process group within
a new session. Has no controlling terminal, the open() call fails.

int
main(int argc, char *argv[])
{
  if (fork() != 0) /* Exit if parent, or on error */
    _exit(EXIT_SUCCESS);

  if (setsid() == -1)
    errExit("setsid");

  printf("PID=%ld, PGID=%ld, SID=%ld\n", (long) getpid(),
      (long) getpgrp(), (long) getsid(0));

  if (open("/dev/tty", O_RDWR) == -1)
    errExit("open /dev/tty");

  exit(EXIT_SUCCESS);
}

note: From this example, shell is not a group leader and this shows how a shell creates a new
session.


{SIGHUP-signal}
SIGHUP
When a terminal disconnect (hangup) occurs, this signal is sent to the controlling process of the
terminal. A second use of SIGHUP is with daemons (e.g., init, httpd, and inetd). Many daemons are
designed to respond to the receipt of SIGHUP by reinitializing themselves and rereading their
configuration files. The system administrator triggers these actions by manually sending SIGHUP to
the daemon, either by using an explicit kill command or by executing a program or script that does
the same.

The default action of SIGHUP is to terminate a process (by kernel). The delivery of SIGHUP to the
controlling process can set off a kind of chain reaction, resulting in the delivery of SIGHUP to
many other processes. 

This may occur in two ways:

First, in a login session, the shell is normally the controlling process for the terminal.  Most
shells are programmed so that, when run interactively, they establish a handler for SIGHUP. This
handler terminates the shell, but beforehand sends a SIGHUP signal to each of the process groups
(both foreground and background) created by the shell. 

note: sends a SIGHUP signal to only groups that the shell created. The below example shows that.

note: NOT understand this in 34.6.1 since if not handle SIGHUP then kernel will terminate all
anyway. So why install a hander and a shell send a signal? 

<because> Since only a foreground process can read input from the controlling terminal and receive
terminal generated signals. 

Second, if delivery of SIGHUP results in termination of a controlling process, then the kernel also
sends SIGHUP to all of the members of the 'foreground' process group of the controlling terminal.

note: SIGHUP is to terminate by default. So both terminates all children.

<example>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <signal.h>

static void
handler(int sig)
{
}

int main(int argc, char *argv[])
{
  pid_t childPid;
  struct sigaction sa;

  setbuf(stdout, NULL); /* Make stdout unbuffered */

  sigemptyset(&sa.sa_mask);
  sa.sa_flags = 0;
  sa.sa_handler = handler;

  if (sigaction(SIGHUP, &sa, NULL) == -1)
  {
    fprintf(stderr, "error: sigaction\n");
    exit(1);
  }

  childPid = fork();
  if (childPid == -1)
  {
    fprintf(stderr, "error: fork\n");
    exit(1);
  }

  if (childPid == 0 && argc > 1)
    if (setpgid(0, 0) == -1) /* Move to new process group */
    {
      fprintf(stderr, "error: setpgid\n");
      exit(1);
    }

  printf("PID=%ld; PPID=%ld; PGID=%ld; SID=%ld\n", (long) getpid(),
      (long) getppid(), (long) getpgrp(), (long) getsid(0));

  /* An unhandled SIGALRM ensures this process will die if nothing else terminates it */
  alarm(60);

  for(;;) {
    /* Wait for signals */
    pause();
    printf("%ld: caught SIGHUP\n", (long) getpid());
  }
}

The output on debian linux:

kpark@wll1p04345:~/work$ ps -o pgid $$
 PGID
 7523

kpark@wll1p04345:~/work$ echo $$
7523

kpark@wll1p04345:~/work$ ./a.out > samegroup.log 2>&1 &
[1] 16098
kpark@wll1p04345:~/work$ ./a.out x > diffgroup.log 2>&1  
[1]+  Alarm clock             ./a.out > samegroup.log 2>&1
Alarm clock

kpark@wll1p04345:~/work$ cat samegroup.log 
PID=16098; PPID=7523; PGID=16098; SID=7523         " shell, 7523 created a new process and group
PID=16099; PPID=16098; PGID=16098; SID=7523        " child

kpark@wll1p04345:~/work$ cat diffgroup.log         
PID=16231; PPID=7523; PGID=16231; SID=7523         " shell created a new process and group.
PID=16232; PPID=16231; PGID=16232; SID=7523        " child created a new group as well.

note: Unlike example output in 34.6.1, there are no output like:

$ cat samegroup.log
PID=5612; PPID=5611; PGID=5611; SID=5533  "child. This example shows that child runs first.
PID=5611; PPID=5533; PGID=5611; SID=5533  "parent
5611: caught SIGHUP
5612: caught SIGHUP

WHY NOT? The problem is the above application is not a shell and bash doc says:

The shell exits by default upon receipt of a SIGHUP. Before exiting, an interactive shell resends
the SIGHUP to all jobs, running or stopped. Stopped jobs are sent SIGCONT to ensure that they
receive the SIGHUP.

So when runs above commands, exit the shell by pressing C-D or close terminal. The output is:

kpark@wll1p04345:~/work$ cat samegroup.log 
PID=12104; PPID=7523; PGID=12104; SID=7523
PID=12105; PPID=12104; PGID=12104; SID=7523
12104: caught SIGHUP
12105: caught SIGHUP

kpark@wll1p04345:~/work$ cat diffgroup.log 
PID=15299; PPID=7523; PGID=15299; SID=7523
PID=15300; PPID=15299; PGID=15300; SID=7523
15299: caught SIGHUP
15299: caught SIGHUP


{session-leader-is-controlling-process}
As a consequence of establishing the connection to (i.e., opening) the controlling terminal, the
'session' 'leader' becomes the 'controlling' process for the terminal.

<disconnect>
The principal significance of being the controlling process is that the 'kernel' sends 'this'
process a SIGHUP signal if a terminal 'disconnect' occurs. note: say only SIGHUP.

<termination>
As for the termination of the controlling process, the following steps occur: The kernel sends a
SIGHUP signal (and a SIGCONT signal) to all members of the foreground process group, to inform them
of the loss of the controlling terminal.


{io-on-controlling-terminal}
The controlling terminal is inherited by the child of a fork() and preserved across an exec(). When
a session leader opens a controlling terminal, it becomes controlling process.

<terminal-driver>
To support job control, the terminal driver maintains a record of the foreground process group for
the controlling terminal. The terminal driver delivers job-control signals to the foreground job
when certain characters are typed. These signals either terminate or stop the foreground job.

<read>
The notion of the terminal's foreground job is also used to arbitrate terminal I/O requests. Only
processes in the foreground job may read from the controlling terminal. Background jobs are
prevented from reading by delivery of the SIGTTIN signal, whose default action is to stop the job. 

note: process'es'

SIGTTIN
When running under a job-control shell, the terminal driver sends this signal to a background
process 'group' when it attempts to read() from the terminal. This signal stops 'a' process by
default.

note: group but not process? when one of process in background group then get this signal.

<write>
If the terminal TOSTOP is set, then background jobs are also prevented from writing to the
controlling terminal by delivery of a SIGTTOU signal, whose default action is to stop the job.

SIGTTOU
This signal serves an analogous purpose to SIGTTIN, but for terminal output by background jobs. When
running under a job-control shell, if the TOSTOP (terminal output stop) option has been enabled for
the terminal (perhaps via the command stty tostop), the terminal driver sends SIGTTOU to a
background process group when it attempts to write() to the terminal (see Section 34.7.1).  This
signal stops a process by default.


={============================================================================
*kt_linux_core_108* process: daemon

LPI 37.

{characteritics}
A daemon is a process with the following characteristics:

1. It runs in the background and has 'no' controlling terminal. The lack of a controlling terminal
ensures that the kernel 'never' automatically generates any job-control or terminal-related signals
(such as SIGINT, SIGTSTP, and SIGHUP) for a daemon.

2. Many standard daemons run as privileged processes (i.e., effective user ID of 0),

3. It is a 'convention' (not universally observed) that daemons have names ending with the letter d.
On Linux, certain daemons are run as kernel threads. The code of such daemons is part of the kernel,
   and they are typically created during system startup. When listed using ps(1), the names of these
   daemons are surrounded by square brackets ([]). One example of a kernel thread is pdflush, which
   periodically flushes dirty pages (e.g., pages from the buffer cache) to disk.


{steps-to-create-daemon}

1. Perform a fork(), after which the parent exits and the child continues. As a consequence, the
daemon becomes a child of the init process. note: zombie.

This is done for two reasons:

@ Assuming the daemon was started from the command line, the parent's termination is noticed by the
shell, which then displays another shell prompt and leaves the child to 'continue' in the
background.

@ To guarantee not to be a process group leader, since it inherited its process group ID from its
parent and obtained its own unique process ID, which differs from the inherited process group ID.

2. The child process calls setsid() to start a new session and free itself of any association with a
controlling terminal.

note: From step 1, it is guaranteed not to be a group leader and this is necessary since only a
process which is not a group leader can create a new session and this has no controlling terminal.
See process: group: session.

3. If the daemon never opens any terminal devices thereafter, then we don't need to worry about the
daemon reacquiring a controlling terminal.

4.5. See 37.2

6. Close all open file descriptors that the daemon has inherited from its parent. A daemon may need
to keep certain inherited file descriptors open, so this step is optional, or open to variation.

This is done for a variety of reasons. Since the daemon has lost its controlling terminal and is
running in the background, it makes 'no' sense for the daemon to keep file descriptors 0, 1, and 2
open if these refer to the terminal. 

Furthermore, we can't 'unmount' any file systems on which the long-lived daemon holds files open.
And, as usual, we should close unused open file descriptors because file descriptors are a finite
resource.

7. After having closed file descriptors 0, 1, and 2, a daemon normally opens /dev/null and uses
dup2() (or similar) to make all those descriptors refer to this device.

note: this is why cannot see output when use sandbox?

This is done for two reasons:

@ It ensures that if the daemon calls library functions that perform I/O on these descriptors, those
functions won't unexpectedly fail. note: due to step 6.

@ It prevents the possibility that the daemon later opens a file using descriptor 1 or 2, which is
then written to-and thus corrupted-by a library function that expects to treat these descriptors as
standard output and standard error.

TODO: Once figure out the daemon implementation and compare it with listing 37.1.


{reinit-daemon}
The fact that many daemons should run continuously presents a couple of programming hurdles:

@ Typically, a daemon reads operational parameters from an associated configuration file on startup.
Sometimes, it is desirable to be able to change these parameters "on the fly," 'without' needing to
stop and restart the daemon.

@ Some daemons produce log files. If the daemon never closes the log file, then it may grow
endlessly, eventually clogging the file system. noted that even if we remove the last name of a
file, the file continues to exist as long as any process has it open. What we need is a way of
telling the daemon to close its log file and open a new file, so that we can rotate log files as
required.

The solution to both of these problems is to have the daemon establish a handler for SIGHUP, and
perform the required steps upon receipt of this signal.

Why is it possible? Since a daemon has no controlling terminal, the kernel never generates this
signal for a daemon. Therefore, daemons can use SIGHUP for the purpose described here.


{syslog}
note: no syslogd on a embedded system.

<why>
When writing a daemon, one problem we encounter is how to display error messages. Since a daemon
runs in the background, we can't display messages on an associated terminal, as we would typically
do with other programs. 

One possible alternative is to write messages to an application-specific log file, as is done in the
program in Listing 37-3. The main problem with this approach is that it is difficult for a system
administrator to manage multiple application log files and monitor them all for error messages. 

The syslog facility was devised to address this problem which provides a single, centralized logging
facility that can be used to log messages by 'all' applications on the system.

<daemon-and-conf>
The System Log daemon, syslogd, accepts log messages from two different sources: /dev/log, which
holds locally produced messages, and remote.

The syslogd daemon examines the facility and level of each message, and then passes it along to any
of several possible destinations according to the associated configuration file, /etc/syslog.conf.
Possible destinations include a terminal or virtual console, a disk file, etc.

<klogd>
An alternative source of the messages placed on /dev/log is the Kernel Log daemon, klogd, which
collects kernel log messages (produced by the kernel using its printk() function). These messages
are collected using either of two equivalent Linux-specific interfaces-the /proc/kmsg file and the
syslog(2) 'system' call-and then placed on /dev/log using the syslog(3) 'library' function.

note: Figure 37-1. printf uses syslog 2 and /proc/kmsg to log and klogd uses syslog 3 to log in to
/dev/log.

<calls>
#include <syslog.h>

void openlog(const char *ident, int log_options, int facility);
void syslog(int priority, const char *format, ...);

<facility-and-level>
The priority argument is created by ORing together a facility value and a level value. The facility
indicates the general category of the application logging the message, and is specified as one of
the values listed in Table 37-1. If omitted, the facility defaults to the value specified in a
previous openlog() call, or to LOG_USER if that call was omitted.

       LOG_AUTH       security/authorization messages
       LOG_AUTHPRIV   security/authorization messages (private)
       ...

The level value indicates the severity of the message, and is specified as one of the values in
Table 37-2. All of the level values listed in this table appear in SUSv3.

       LOG_EMERG      system is unusable
       LOG_ALERT      action must be taken immediately
       LOG_CRIT       critical conditions
       LOG_ERR        error conditions
       LOG_WARNING    warning conditions
       LOG_NOTICE     normal, but significant, condition
       LOG_INFO       informational message
       LOG_DEBUG      debug-level message

One difference from printf() is that the format string doesn't need to include a terminating newline
character. Also, the format string may include the 2-character sequence %m, which is replaced by the
error string correspond- ing to the current value of errno (i.e., the equivalent of
    strerror(errno)).


={============================================================================
*kt_linux_core_109* process: execution

LPI. 27

{execve-system-call}
Various library functions, all with names beginning with exec, are layered on top of the execve()
  system call. Each of these functions provides a different interface to the same functionality. The
  loading of a new program by any of these calls is commonly referred to as an exec operation, or
  simply by the notation exec().

#include <unistd.h>
int execve(const char *pathname, char *const argv[], char *const envp[]);

'never' returns on success; returns -1 on error

<attribute-that-remains>
After an execve(), the process ID of the process remains the same, because the same process
continues to exist.

<never-return>
Never need to check the return value from execve(); it will always be -1 if get since the very fact
that it returned informs us that an error occurred,

<example>
note: See how envs are passed to.

$ ./t_execve ./envargs 
argv[0] = envargs 
argv[1] = hello world 
argv[2] = goodbye 
env: GREET=salut 
env: BYE=adieu 

// t_execve.c

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

int main(int argc, char** argv)
{
  char *argVec[10];
  char *envVec[] = { "GREET=salut", "BYE=adieu", NULL };

  if( argc != 2 || strcmp( argv[1], "--help" ) == 0 )
    printf("%s pathname\n", argv[0] );

  // get basename
  argVec[0] = strrchr( argv[1], '/' );
  if( argVec[0] != NULL )
    argVec[0]++;
  else
    argVec[0] = argv[1];

  argVec[1] = "hello world";
  argVec[2] = "goodbye";
  argVec[3] = NULL;

  execve( argv[1], argVec, envVec );

  printf("if we get here, someting wrong.\n" );
  exit(EXIT_FAILURE);
}

// envarg.c

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

extern char **environ;

int main(int argc, char** argv)
{
  int j;
  char **ep;

  for( j = 0; j < argc; j++ )
    printf("argv[%d] = %s \n", j, argv[j] );

  for( ep = environ; *ep != NULL; ep++ )
    printf("env: %s \n", *ep );

  exit(EXIT_SUCCESS);
}


{exec-library}
The library functions perfors an exec() and all are layered on top of execve().

#include <unistd.h>

int execle(const char *pathname, const char *arg, ...  /* , (char *) NULL, char *const envp[] */ );
int execlp(const char *filename, const char *arg, ...  /* , (char *) NULL */);
int execvp(const char *filename, char *const argv[]);
int execv(const char *pathname, char *const argv[]);
int execl(const char *pathname, const char *arg, ...  /* , (char *) NULL */);

None of the above returns on success; all return -1 on error

<path-filename>
execvp() allow the program to be specified using just a filename. The filename is sought in the list
of directories specified in the PATH environment variable. The functions contain the letter p for
PATH to indicate this.

The PATH variable is 'not' used if the filename contains a slash (/), in which case it is treated as a
relative or absolute pathname.

The PATH value for a login shell is set by system-wide and user-specific shell startup scripts.
Since a child process inherits a copy of its parent's environment variables, each process that the
shell creates to execute a command inherits a copy of the shell's PATH.

<env>
The execve() and execle() functions allow the programmer to 'explicitly' specify the environment for
the new program using envp, a NULL-terminated array of pointers to character strings. The names of
these functions end with the letter e (for environment) to indicate this fact. All of the other
exec() functions use the caller's existing environment (i.e., the contents of environ) as the
environment for the new program.

<example>
This is to two things: parent is to run actual application given and child is to check if a parent
is alive in a loop. If parent is died, do some action.

1. shell runs this line in a script. 
note: this cmd comes from outside to specify the application to run.

   exec $exec_wrapper "$cmd" args...
   
2. $exec_wrapper is an application, exec-then-cleanup-app, which do two things: 
   @ call fork
   @ call 'exec' to exec the given arg from $cmd and thst is parent becomes the actual application such as a browser.
   @ child is alive in a loop while checking if parent is died. If so, do some action.

   same pid starting from the shell which runs a script
<------------------------------------------------->
$cmd    run script      exec           fork: exec
      <------------> <------------> <------------->
                     shell    wrapper_app    application
                  
                                       diff pid
                                    <------------->
                                       fork:
                                    <------------->
                                             child

4. After fork, child will set ppid via env and exec exec-then-cleanup-app. This time will do loop.
5. After fork, parent will exec application with args.

note: see how exec used to replace application to run. the shell exec replace itself with given
application.

// exec-then-cleanup-app

#include <unistd.h>
#include <signal.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <assert.h>
#include <sys/stat.h>
#include <sys/types.h>

void usage(const char* exec_name)
{
  fprintf(stderr, "Usage: %s cmd [args]\n\n"
                  "Run `cmd` passing it `args` and run inspect"
                  " when that completes\n", basename(exec_name));
}

int main(int argc, char** argv)
{
  pid_t pid;
  int opt = 0;

  // note: getenv returns NULL if an env is not set.
  char* parent_pid_str = getenv("NEXUS_INSPECT_PARENT_PID");
  char* cleanup_exe = PKG_BIN_DIR "/nexus-inspect";

  if (NULL == parent_pid_str && argc < 2)
  {
    usage(argv[0]);
    return 1;
  }

  if (NULL == parent_pid_str)
  {
    // Even if pids were unsigned 64-bit numbers, there could be a maximum
    // of 20 characters in it. We add another for the null terminator.
    char parent_pid[21];
    int ret = snprintf(parent_pid, sizeof(parent_pid), "%i", getpid());
    if (ret >= 20)
    {
      // If you're on a system with pids that are larger than unsigned
      // 64-bit integers, you should seriously consider why you're still
      // using nexus-inspect.
      fprintf(stderr, "Pids are longer than expected (ie %d characters)."
          " What's going on?", ret);
      return 2;
    }

    pid = fork();
    assert(!(pid < 0));
    if (0 == pid)
    {
      // Child.

      // Only launch the watcher if the cleanup app exists.
      struct stat sb;
      if (-1 == stat(cleanup_exe, &sb))
      {
        fprintf(stderr, "Cleanup executable (%s) doesn't exist, so not" " spawning watcher\n", cleanup_exe);
        return 0;
      }

      // Create new group and become group leader.
      if (setpgid(0, 0)) {
        perror("setpgid failed!");
        return 3;
      }

      const char* args[4] = { argv[0], "-p", parent_pid, NULL };

      // We set the pid in the environment. Doing it via environment rather than arguments
      // simplifies the code because we don't have to do any argument parsing - note that we do
      // accept arguments and options, but they are *all* passed on to the exec'd process (see the
      // parent branch, below).

      setenv("NEXUS_INSPECT_PARENT_PID", parent_pid, 1);
      execv(argv[0], (char*const*)args);
    }
    else
    {
      // Parent.
      if (execvp(argv[1], (char*const*)(argv+1)) != 0) {
        perror("exec failed!");
        return 4;;
      }
    }
  }
  else
  {
    int parent_pid=atoi(parent_pid_str);
    fprintf(stderr, "Going to watch pid: %d\n", parent_pid);
    while (0 == kill(parent_pid, 0))
    {
      sleep(1);
    }
    fprintf(stderr, "Pid %d has died. Going to cleanup.\n", parent_pid);

    char* args[5] = { cleanup_exe, "-r", "-p", parent_pid_str, NULL };
    return execv(cleanup_exe, args);
  }

  return 0;
}


={============================================================================
*kt_linux_core_150* pthread

POSIX.1 threads approved in 1995 is a POSIX standard for threads. The standard defines an API for
creating and manipulating threads. So there is one posix standard but there are two implementation
like NPTL.


{pthread-apis}
http://pubs.opengroup.org/onlinepubs/7990989799/xsh/pthread.h.html
http://pubs.opengroup.org/onlinepubs/7908799/xsh/pthread.h.html

Linux threading models compared: LinuxThreads and NPTL
http://www-128.ibm.com/developerworks/linux/library/l-threading.html?ca=dgr-lnxw07LinuxThreadsAndNPTL

From 2.6, NPTL(New Posix Threading Library) that supports futex(fast user space mutex) and is part
of glibc.


{source}
# from uclibc source

uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\pthread.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\rwlock.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\pthread.h
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\bits\pthreadtypes.h
(_pthread_rwlock_t)


{thread-errno}
The traditional method of returning status from system calls and some library functions is to return
0 on success and -1 on error, with errno being set to indicate the error. The functions in the
pthreads API do things differently. All pthreads functions return 0 on success or a positive value
on failure. The failure value is one of the same values that can be placed in errno by traditional
UNIX system calls. Two approaches:

<one>
From {ref-UNP}. Why is this? Because pthread funcs do not set <errno-var> and return errno instead.
This means that set manually errno before calling err_sys() for example. This util funcs handles
this:

To avoid cluttering the code with braces, use {comma-operator} to combine assignment and the call:

int n;
if(( n = pthread_mutex_lock( &ndone_mutex )) != 0 )
   errno = n, err_sys("pthread_mutex_lock error");

Or

void Pthread_mutex_lock(pthread_mutex_t* mptr)
{
  int n;

  if(( n = pthread_mutex_lock( mptr )) == 0 )
    return;

  errno = n;
  err_sys("pthread_mutex_lock error");
} 

From Appdix C in {ref-UNP}. The reason for our own error funcs is to handle error case with a single
line. See {pthread-util-func} for the use.

if( error condition )
   err_sys( printf format with any number of args );

instead of

if( error condition ) {
  char buff[200];
  snprintf( buff, sizeof(buff), printf format with any number of args );
  perror(buff);
  exit(1);
}

<two>
See "error handling codes from LPI".


{pthread.h}
#include <pthread.h>

// return 0 if okay, positive Exxx on error which is different from most system calls. 
//
// If need multiple arguments to the function, must package them into a structure and then pass the
// address of this as the single argument to the start function.
//
// The tid argument points to a buffer of type pthread_t into which the unique identifier for
// this thread is copied before pthread_create() returns. This identifier can be used in later
// Pthreads calls to refer to the thread. The arg should be in global or heap.
//
// EAGAIN : when cannot create a new thread because exceeded the limit on the number of threads
//
int pthread_create( pthread_t *tid, const pthread_attr_t *attr, void *(*func)(void*), void *arg);

<join> <pthread-error-use>
// return 0 if okay, positive Exxx on error which is different from most system calls. See
// {errno-use}
//
// To wait for a given thread to terminate but no way to wait for any other threads. If that thread
// has already terminated, pthread_join() returns immediately.
//
// If status is not null, the return value from the thread(a pointer to some object) is stored.
//
// <circumbent-join-restriction>
// We noted earlier that pthread_join() can be used to join with only a specific thread. It
// provides no mechanism for joining with any terminated thread. We now show how a condition
// variable can be used to circumvent this restriction. The ref-LPI 30.2.4 shows code.

int pthread_join( pthread_t tid, void **status);


/* The execution of a thread terminates in one of the following ways: 
 *
 * o The thread's start function performs a return specifying a return value for the thread. This is
 * equivalent to pthread_exit()  
 * 
 * o The thread calls pthread_exit() which can be called in any func called by start func.
 *
 * o The thread is canceled using pthread_cancel()
 *
 * o note: Any of the threads calls exit(), or the main thread performs a return (in the main() function),
 * which causes 'all' threads in the process to terminate immediately.
 * 
 * {return-value}
 *
 * The retval argument specifies the return value for the thread. The value pointed to by retval
 * should not be located on the thread's stack, since the contents of that stack become undefined on
 * thread termination. (For example, that region of the process's virtual memory might be
 * immediately reused by the stack for a new thread.) The same statement applies to the value given
 * to a return statement in the thread's start function.
 *
 * {main-thread-exit}
 *
 * note: If the main thread calls pthread_exit() instead of calling exit() or performing a return, then
 * the other threads continue to execute.
 */
void pthread_exit(void *retval);


// {threadid}
// return tid of calling thread. note: pthread_t is structure and need more to print out and also
// implementation dependant. In NPTL, pthread_t is a pointer so treat it as opaque data and hence
// need pthread_equal(). 
//
// if (pthread_equal(tid, pthread_self()) 
//    printf("tid matches self\n");
//
// In 29.5 of {ref-LPI}, POSIX thread IDs are not the same as the thread IDs returned by the Linux
// specific gettid() system call. POSIX thread IDs are assigned and maintained by the threading
// implementation. The thread ID returned by gettid() is a number (similar to a process ID) that is
// assigned by the kernel.  Although each POSIX thread has a unique kernel thread ID in the Linux
// NPTL threading implementation, an application generally doesn't need to know about the kernel IDs
// (and won't be portable if it depends on knowing them).
//
//  return 0 if they are equal
//
int pthread_equeal( pthread_t tid, pthread_t tid );
pthread_t pthread_self(void);


/*  Thread is either 'joinable'(the default) or 'detached'. The detached thread is like a daemon
 *  process: when it terminates, all its resources are released, and cannot wait for it to
 *  terminate. This means cannot get return state. {Q} does it mean joinable thread possibly leaks
 *  resources when not joined? If a joinable thread termintes without pthread_join, it became a
 *  'zombie' process. Leak resources and may not able to create additional threads. 
 *
 *  {ref-UNP} Detaching a thread doesn't make it immune to a call to exit() in another thread or a
 *  return in the main thread. In such an event, all threads in the process are immediately
 *  terminated, regardless of whether they are joinable or detached. To put things another way,
 *  pthread_detach() simply controls what happens after a thread terminates, not how or when it
 *  terminates.
 *
 *  To make the specified thread detached and often used to detach itself. Can create detached
 *  thread when create it by using attr setting.
 *
 *  pthread_detach( pthread_self() );
 */
int pthread_detach(pthread_t tid);


/*  status must not point to an object that is local to the calling thread.
 *
 *  terminate two other ways:
 *
 *  o thread function returns. return value is the exit status of the thread. {Q} kernal handle and
 *  manage it?
 *
 *  o main thread function returns or any thread call exit/_exit, the process terminates
 *  immediately.
 */
void pthread_exit(void *status);

// A thread may be canceled by any other thread in the same process. For example, if multiple
// threads are started to work on a given task (say finding a record in a database) adn the first
// thread completes the task then cancels the other threds.
//
// To handle the possibility of being canceled, can install(push) and remove(pop) cleanup handlers.
// These handlers are called:
// a) when the thread is canceled by pthread_cancel
// b) when the thread voluntarily terminates (either by calling pthread_exit or returning from its
// thread)

int   pthread_cancel(pthread_t);
void  pthread_cleanup_push(void*), void *);
void  pthread_cleanup_pop(int);


{pthread-attribute} 
To override the default and normally take the detault using the attr arg as a NULL. Attributes are a
way to specify behavior that is different from the default. When a thread is created with
pthread_create or when a synchronization variable is initialized, an attribute object can be
specified. However the default atributes are usually sufficient for most applications. 

Note: Attributes are specified [only] at thread creation time; they cannot be altered while the thread
is being used. {Q} really?

Thus three functions are usually called in tandem when setting attribute

o Thread attibute intialisation 
pthread_attr_init() create a default pthread_attr_t tattr. The function pthread_attr_init() is used
to initialize object attributes to their default values. The storage is allocated by the thread
system during execution. note: once the thread has been creted, the attribute object is no longer
needed, and so is destoryed.

o Thread attribute value change (unless defaults appropriate) 
A variety of pthread_attr_*() functions are available to set individual attribute values for the
pthread_attr_t tattr structure. (see below).  

o Thread creation 
A call to pthread_create() with approriate attribute values set in a pthread_attr_t structure. 
 
<code>

The following code fragment should make this point clearer: 

#include <pthread.h> 

pthread_attr_t tattr; // note: can be a local var
pthread_t tid;
void *start_routine;
void arg
int ret;

ret = pthread_attr_init(&tattr);
ret = pthread_attr_*(&tattr,SOME_ATRIBUTE_VALUE_PARAMETER);
ret = pthread_create(&tid, &tattr, start_routine, arg);
ret = pthread_attr_destroy(&tattr);

In order to save space, code examples mainly focus on the attribute setting functions and the
intializing and creation functions are ommitted. These must of course be present in all actual code
fragtments. 

An attribute object is opaque, and cannot be directly modified by assignments. A set of functions is
provided to initialize, configure, and destroy each object type. Once an attribute is initialized
and configured, it has process-wide scope. The suggested method for using attributes is to configure
all required state specifications at one time in the early stages of program execution. The
appropriate attribute object can then be referred to as needed. Using attribute objects has two
primary advantages: 

First, it adds to code portability. Even though supported attributes might vary between
implementations, you need not modify function calls that create thread entities because the
attribute object is hidden from the interface. If the target port supports attributes that are not
found in the current port, provision must be made to manage the new attributes. This is an easy
porting task though, because attribute objects need only be initialized once in a well-defined
location. 

Second, state specification in an application is simplified. As an example, consider that several
sets of threads might exist within a process, each providing a separate service, and each with its
own state requirements. At some point in the early stages of the application, a thread attribute
object can be initialized for each set. All future thread creations will then refer to the attribute
object initialized for that type of thread. The initialization phase is simple and localized, and
any future modifications can be made quickly and reliably. Attribute objects require attention at
process exit time. When the object is initialized, memory is allocated for it. This memory must be
returned to the system. The pthreads standard provides function calls to destroy attribute objects. 


{pthread-example-in-FOSH}
/* Assert throughout that the POSIX calls worked. If not, HFL cannot be guaranteed to work. */
resPOSIX = pthread_attr_init(&threadAttrs);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* The stacksize attribute defines the minimum stack size (in bytes) allocated for the created
 * threads stack.
 *
 * int pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize);
 */
resPOSIX = pthread_attr_setstack(&threadAttrs, ptThreadInfo->pvStack,(size_t)ptThreadInfo->szStack);
/* If this assert is triggered, it might be because of not aligning address to a boundary of 8. */
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * To set the other scheduling policy: 
 */
resPOSIX = pthread_attr_setschedpolicy(&threadAttrs, SCHED_RR);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * The example to change prio:
 *
 * sched_param param;
 * param.sched_priority = 30;
 * ret = pthread_attr_setschedparam (&tattr, &param);
 * 
 * used to set/inquire a current thread's priority of scheduling.
 * 
 * int pthread_attr_setschedparam(pthread_attr_t *attr, const struct sched_param *param);
 * int pthread_attr_getschedparam(const pthread_attr_t *attr, struct sched_param *param);
 *
 * {Q} {pthread-schedule-at-runtime} is it possible as attr is only at creation?
 */
resPOSIX = pthread_attr_setschedparam(&threadAttrs, &tSchedParam);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);

resPOSIX = pthread_attr_setscope(&threadAttrs, ContentionScope);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* The pthread_attr_setinheritsched() function sets the scheduling parameter inheritance state
 * attribute in the specified attribute object. The thread's scheduling parameter inheritance state
 * determines whether scheduling parameters are explicitly specified in this attribute object, or if
 * scheduling attributes should be inherited from the creating thread. Valid settings for
 * inheritsched include:
 * 
 * PTHREAD_INHERIT_SCHED Scheduling parameters for the newly created thread are the same as those of
 * the creating thread.
 * 
 * PTHREAD_EXPLICIT_SCHED Scheduling parameters for the newly created thread are specified in the
 * thread attribute object.
 */
resPOSIX = pthread_attr_setinheritsched(&threadAttrs, PTHREAD_EXPLICIT_SCHED);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);

resPOSIX = pthread_create(&(ptThreadInfo->idThread), &threadAttrs, pfThreadMain, pvParam);


{pthread-stack} from CH33 in {ref-LPI}.
Each thread has its own stack whose size is fixed when the thread is created. On Linux/x86-32, for
all threads other than the main thread, the default size of the per-thread stack is 2 MB. The main
thread has a much larger space for stack growth. Occasionally, it is useful to change the size of a
thread's stack. 

The pthread_attr_setstacksize() function sets a thread attribute (Section 29.8) that determines the
size of the stack in threads created using the thread attributes object. The related
pthread_attr_setstack() function can be used to control both the size and the location of the stack,
but setting the location of a stack can decrease application portability.

One reason to change the size of per-thread stacks is to allow for larger stacks for threads that
allocate large automatic variables or make nested function calls of great depth (perhaps because of
recursion).

Alternatively, an application may want to reduce the size of per-thread stacks to allow for a
greater number of threads within a process.

The minimum stack that can be employed on a particular architecture can be determined by calling
sysconf(_SC_THREAD_STACK_MIN). For the NPTL implementation on Linux/x86-32, this call returns the
value 16,384.


{nptl}
Linux threading models compared: LinuxThreads and NPTL
http://www-128.ibm.com/developerworks/linux/library/l-threading.html?ca=dgr-lnxw07LinuxThreadsAndNPTL

NPTL(new posix threading library) comes from kernel 2.6 and supports Futex(fast user space mutex).
It is part of glibc.

<how-to-check-nptl-version>
$ getconf GNU_LIBPTHREAD_VERSION
NPTL 2.15

<pid-nptl>
note: Q. In Linux, every thread has a PID and can see when use ps command but in NPTL, thread
group has one PID. is it true?


{pthread-compile-link}
It is on GCC 4.6.3 and not -lpthread. This is said when run 'man' on pthread funcs. On Linux,
option. The effects of this option include the following:

o The _REENTRANT preprocessor macro is defined. This causes the declarations of a few reentrant
functions to be exposed.

o The program is linked with the libpthread library (the equivalent of -lpthread).

gcc -pthread sample.c


={============================================================================
*kt_linux_core_102* how to run three threads sequencially

From Cracking the coding interview, p425, 16.5:

Suppose we have following code:

public class Foo {
  public Foo() {...}
  public void first() {...}
  public void second() {...}
  public void third() {...}
}

The same instance of Foo will be passed to three different threads. ThreadA will call first, ThreadB
will call second, and ThreadC will call third. Design a mechanism to ensure that first is called
before second and second is called before third.

Using lock?

public class FooBad {
  public FooBad() {
    lock1 = new ReentrantLock();
    lock2 = new ReentrantLock();

    // locks all in a ctor
    lock1.lock(); lock2.lock(); 
  }
}

// already got lock1
public void first()
{
  ...
  lock1.unlock(); // finished first
}

// already got lock2
public void second()
{
  lock1.lock();   // wait first to finish
  lock1.unlock();
  ...
  lock2.unlock(); // finished second
}

public void third()
{
  lock2.lock();   // wait second to finish
  lock2.unlock();
  ...
}

This WON'T work in JAVA since a lock in JAVA is owned by the same thread which locked in.

http://docs.oracle.com/javase/6/docs/api/java/util/concurrent/locks/ReentrantLock.html
A ReentrantLock is owned by the thread last successfully locking, but not yet unlocking it. A thread
invoking lock will return, successfully acquiring the lock, when the lock is not owned by another
thread. The method will return immediately if the current thread already owns the lock. 

note: There's no such limitation in Linux and it's possible in the same thread group and if threads
in different group use lock, can use semaphore or lock on the shared memory.

The mutex has ownership as well and see {mutex-ownership}.


={============================================================================
*kt_linux_core_103* priority and schedule 

{priority-on-linux}

high           low
0     99 100   139

0  -99  : realtime priority. static and can set when create a process
100-139 : user priority. can use nice command which uses with value from -20 to +19. The default
nice value is 0. note: does it mean 120 is default? 100 is highest?


{realtime-and-latency}
The aim to distribute fairly CPU resource to all process on a system is not realtime approach.

<latency-components>

interrupt      ISR         ISR signals       user process
event          runs        user process      runs
 |             |              |                 |
---------------------------------------------------------> time
   interrupt      interrupt      scheduling
   latency        processing     latency

<no-hard-realtime>
No support in kernel. To use hard realtime, install patch from 
http://people.redhat.com/~mingo/realtime-preempt


{schedule}
Before Linux 2, kernel didn't support preemption which means that no other process can run in kernel
mode when one user process is already in the kernel mode until that is bloked or finishes its work.

<scheduler>
O(1) scheduler from Linux 2.5 and supports constant scheduling decision regardless of the number of
process.

<schedule-policy>
(from ~/include/linux/sched.h)
/*
 * Scheduling policies
 */

// normal user process. fairness
#define SCHED_NORMAL 0     

// realtime and run the highest priority. On the same priority, the first runs until it's blocked.
// So it is realtime without time slice.
#define SCHED_FIFO   1     

// realtime and run the highest priority. On the same priority, the first runs but in the time
// sliced. So it is realtime with time slice.
#define SCHED_RR     2

<default-policy>
SCHED_NORMAL(OTHER) is default.

To change the policy after a boot:
For example, if scheduling is modified before insmod-ing callisto BCM drivers, tasks inherit changed
scheduling. In that case last two task mods can be dropped. The list is quite aggressive as changing
policy of kthread affects all new kernel threads. Fine tuning would obviously have to be done along
with BCM.

(about ways to change from NORMAL to RR)
http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0182.html
http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0330.html


<code>
/* from sched.h.
#define MAX_USER_RT_PRIO   100
#define MAX_RT_PRIO        MAX_USER_RT_PRIO
*/

#define MY_RT_PRIORITY MAX_USER_RT_PRIO /* Highest possible */

int main(int argc, char **argv)
{
  ...
  int rc, old_scheduler_policy;
  struct sched_param my_params;
  ...

  /* Passing zero specifies caller's (our) policy */
  old_scheduler_policy = sched_getscheduler(0);
  my_params.sched_priority = MY_RT_PRIORITY;

  /* Passing zero specifies callers (our) pid */
  rc = sched_setscheduler(0, SCHED_RR, &my_params);
  if ( rc == -1 )
    handle_error();
  ...
}

(sched.c)
/**
 * sys_sched_get_priority_max - return maximum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the maximum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_max(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = MAX_USER_RT_PRIO-1;
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
                break;
        }
        return ret;
}

/**
 * sys_sched_get_priority_min - return minimum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the minimum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_min(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = 1;
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
        }
        return ret;
}


{pthread-schedule}
The POSIX draft standard specifies scheduling policy attributes of SCHED_FIFO (first-in-first-out),
SCHED_RR (round-robin), or SCHED_OTHER (an implementation-defined method). SCHED_FIFO and
SCHED_RR are optional in POSIX, and only are supported for real time bound threads


# ============================================================================
#{ FILE
={============================================================================
*kt_linux_core_200* file io

{open-flag} LPI, 4.3.1
O_CREAT

If the file doesn't already exist, it is created as a new, empty file. This flag is effective even
if the file is being opened only for reading. If we specify O_CREAT, then we must supply a mode
argument in the open() call; otherwise, the permissions of the new file will be set to some random
value from the stack.

O_EXCL

This flag is used in conjunction with O_CREAT to indicate that if the file already exists, it should
not be opened; instead, open() should fail, with errno set to EEXIST. In other words, this flag
allows the caller to 'ensure' that it is the process creating the file. 


={============================================================================
*kt_linux_core_201* file attribute

LPI 15.

{calls}
The stat(), lstat(), and fstat() system calls retrieve information about a file, mostly drawn from
the file i-node.

man 2 stat

#include <sys/stat.h>
int stat(const char *pathname, struct stat *statbuf);
int lstat(const char *pathname, struct stat *statbuf);
int fstat(int fd, struct stat *statbuf);

All return 0 on success, or -1 on error

stat() returns information about a named file;

lstat() is similar to stat(), except that if the named file is a symbolic link, information about
the link itself is returned, rather than the file to which the link points; and

fstat() returns information about a file referred to by an open file descriptor.

<permission>
The stat() and lstat() system calls don't require permissions on the file itself. However, execute
(search) permission is required on 'all' of the parent directories specified in pathname. The fstat()
  system call always succeeds, if provided with a valid file descriptor. 

<use-to-check>
Shows that can use stat() to check if the file exist or not.

struct stat sb;
if (-1 == stat(cleanup_exe, &sb))
{
  fprintf(stderr, "executable (%s) doesn't exist\n", cleanup_exe);
  return 0;
}


={============================================================================
*kt_linux_core_202* 


={============================================================================
*kt_linux_core_200* ipc

CH43, Fig 43-1 in {ref-LPI} which says that the 'general' term IPC is often used to describe them all;
communication, signal, and synchronization.

communication - data transfer - byte stream  - pipe
                                             - fifo
                                             - stream socket

                              - message      - sys v message q
                                             - posix message q
                                             - datagram socket

                              - pseudoterminal

               - shared memory   - sys v shm
                                 - posix shm
                                 - memory mapping  - anonymous mapping
                                                   - mapped file

signal   - standard signal
         - realtime signal

synchronization   - semaphore - sys v
                  - posix     - named
                              - unnamed
                  - file lock - record lock
                              - file lock

                  - mutex
                  - condition variable

Signals: Although signals are intended primarily for other purposes, they can be used as a
synchronization technique in certain circumstances. More rarely, signals can be used as a
communication technique: the signal number itself is a form of information, and realtime signals can
be accompanied by associated data (an integer or a pointer).


{nonnetworked-ipc}
From {ref-UNP}. nonnetworked-ipc means that ipc for local and newtworked-ipc means that for remote
such as socket.


{categories-of-ipc}
From {ref-UNP}:

three-ways-to-share-between-processes

(1) Process Process       (2) Process Process          (3) Process <- shm -> Process
      |        |                 |       |      
Kernel                          shared info in kernel
      |        |
Filesystem


<persistence> which is lifetime of an objects
Define the persistence of any type of IPC as how long an object of that type remains in existence.

Process-persistence exists until last process with IPC open closes the object and kernel one exists
until kernel reboots or IPC objects is explicitly deleted.

process-persistence: pipe, fifo, mutex, condition-var, read-write-lock, ...
kernel-presistence : shm, named-semaphore, ...

Be careful when defining the presistence of ipc because it is not always as it seems: the data
within a pipe is maintained within the kernel, but pipes have process-persistence because after the
last process that has the pipe open for reading closes the pipe, the kernel discard all data and
remove the pipe.

From {ref-LPI}:

<data-transfer> 
1. requires two data transfers between user and kenel memory
2. synchronization between the reader and writer processes is automatic by kernel. if a reader
attempts to fetch data from data-transfer facility that has no data, then read will bock until some
write data to it.  
3. available to one which done read operation since read consumes data.

<byte-stream>
read and write is independent meaning read may read an arbitrary bytes. This models "file as a
sequence of bytes".

An application can also impose a message-oriented model on a byte-stream facility, by using
delimiter characters, fixed-length messages, or message headers that encode the length of the total
message message: each read reads a whole message. not possible to read part of a message and to read
multiple messages.


<shared-memory>
1. don't require system calls or data transfer between user and kenel. Hence shared memory
provide very fast communication. 
2. However it can be offset by the need to sync and semaphore is the usual method used with shared
memory.
3. avaible to all of the processes that share that memory

<file-desc-based>
facility using file descriptors like pipe, fifo, and sockets

The primary benefit of these techniques is that they allow an application to simultaneously monitor
multiple file descriptors to see whether I/O is possible on any of them.


{namespace}
Use name or identifier so that one process can create ipc object and other processes can specify
that same ipc object.

type                       name used to identify   handle used to refer to object
------------------------------------------------------------------------------------
pipe                       no name                 file descriptor
fifo                       pathname                ditto

UNIX domain socket         pathname                ditto
Internet domain socket     IP and port             ditto

posix message q            posix ipc pathname      mqd_t
posix named semaphore      ditto                   sem_t* (sem pointer)
posix unnamed semaphore    no name                 sem_t*
posix shared memory        posix ipc pathname      file descriptor 

anonymous mapping          no name                 none
memory mapped file         pathname                file descriptor 


{accessibility-and-persistence}
type                       accessibility                 persistence
------------------------------------------------------------------------------------
pipe                       only by related processes     process
fifo                       permission mask               ditto

UNIX domain socket         permission mask               ditto
Internet domain socket     by any processe               ditto

posix message q            permission mask               kernel
posix named semaphore      permission mask               kernel
posix unnamed semaphore    permission of underlying mem  depends
posix shared memory        permission mask               kernel

anonymous mapping          only by releated              process 
memory mapped file         permission mask               file system 

unix and network domain:

Of all of the IPC methods shown in Figure 43-1, only sockets permit processes to communicate over a
network. Sockets are generally used in one of two domains: the UNIX domain, which allows
communication between processes on the same system, and the Internet domain, which allows
communication between processes on different hosts connected via a TCP/IP network. Often, only minor
changes are required to convert a program that uses UNIX domain sockets into one that uses Internet
domain sockets, so an application that is built using UNIX domain sockets can be made
network-capable with relatively little effort.

<portability>
However, the POSIX IPC facilities (message queues, semaphores, and shared memory) are not quite as
widely available as their System V IPC counterparts, especially on older UNIX systems. An
implementation of POSIX message queues and 'full' support for POSIX semaphores have appeared on Linux
only in the 2.6.x kernel series. Therefore, from a portability point of view, System V IPC may be
preferable to POSIX IPC.

As of 06 Jan 2014, the latest stable kernel release is 3.12.6

note: Here, 'related' means related via fork(). In order for two processes to access the object, one
of them must create the object and then call fork(). As a consequence of the fork(), the child
process inherits a handle referring to the object, allowing both processes to share the object.

<performance>
In some circumstances, different IPC facilities may show notable differences in performance.
However, in later chapters, we generally refrain from making performance comparisons, for the
following reasons:

1) The performance of an IPC facility may not be a significant factor in the overall performance of
an application, and it may not be the only factor in determining the choice of an IPC facility.

2) The relative performance of the various IPC facilities may vary across UNIX implementations or
between different versions of the Linux kernel.

3) Most importantly, the performance of an IPC facility will vary depending on the precise manner
and environment in which it is used. Relevant factors include the size of the data units exchanged
in each IPC operation, the amount of unread data that may be outstanding on the IPC facility,
whether or not a process context switch is required for each unit of data exchanged, and other
load on the system.

If IPC performance is crucial, there is no substitute for application-specific benchmarks run under
an environment that matches the target system. To this end, it may be worth writing an 'abstract'
software layer that hides details of the IPC facility from the application and then testing
performance when different IPC facilities are substituted underneath the abstract layer.


={============================================================================
*kt_linux_core_201* ipc: system v

{interfaces}
A more significant reason for discussing the System V IPC mechanisms together is that their
programming interfaces share a number of common characteristics, so that many of the same concepts
apply to all of these mechanisms.


{key-and-identifier}
System V IPC keys are integer values represented using the data type key_t which is analogous to a
filename and get calls which is analogous to the open() system call used for files. The get calls
'translate' a key into the corresponding integer IPC identifier which is analogous to a file
descriptor.

<identifier>
There is, however, an important semantic difference. Whereas a file descriptor is a process
attribute, an IPC identifier is a property of the object itself and is visible 'system'-wide.

All processes accessing the same object use the same identifier. This means that if we know an IPC
object already exists, we can skip the get call, provided we have some other means of knowing the
identifier of the object.

<flag>
We specify the permissions to be placed on the new object as part of the final (flags) argument to
the get call, using the 'same' bit-mask constants as are used for files.

<key>
So, how do we provide a unique key? (LPI, 45.2)

One of three methods. Specify the IPC_PRIVATE constant as the key value to the get call when
creating the IPC object, which 'always' results in the creation of a 'new' IPC object that is
guaranteed to have a 'unique' key.


{ctl-calls}
A few are generic to all IPC mechanisms. An example of a generic control operation is IPC_RMID,
  which is used to delete an object.

<when-deleted>
For message queues and semaphores, deletion of the IPC object is immediate, and any information
contained within the object is destroyed, regardless of whether any other process is still using the
object.

For files, if we remove the last link to a file, then the file is actually removed only after all
open file descriptors referring to it have been closed. As with files, Deletion of shared memory
objects occurs differently.


{persistence}
System V IPC objects have 'kernel' persistence. Once created, an object continues
to exist until it is explicitly deleted or the system is shut down. 

Two disadvantages: 
1. system-imposed limit. If we fail to remove unused objects, we may eventually encounter
application errors as a result of reaching these limits.

2. When deleting a message queue or semaphore object, a multiprocess application may not be able to
easily determine which will be the last process requiring access to the object, and thus when the
object can be safely deleted. Not for shm.


{associated-data}
The kernel maintains an associated data structure for 'each' instance of a System V IPC object. 

The associated data structure for an IPC object is initialized when the object is created via the
appropriate get system call. Once the object has been 'created', a program can obtain a copy of this
data structure using the appropriate ctl system call, by specifying an operation type of IPC_STAT.

<ipc_perm>
As well as data specific to the type of IPC object, the associated data structure for all three IPC
mechanisms includes a substructure, ipc_perm, that holds information used to determine permissions
granted on the object

EACCES.

See LPI 45.3 for more about permission checks.

<bypass-permission-check>
The second user could bypass this check by specifying 0 for the second argument of the msgget()
  call, in which case an error would occur only when the program attempted an operation requiring
  write permission on the IPC object

msgget(key, 0);


{ipcs-command}
The ipcs command lists the System V IPC objects that currently exist on the system. The ipcrm
command is used to remove System IPC objects.

ipcs - provide information on ipc facilities

       ipcs [-asmq] [-tclup]

       Resources may be specified as follows:
       -m     shared memory segments
       -q     message queues
       -s     semaphore arrays
       -a     all (this is the default)

       The output format may be specified as follows:
       -t     time
       -p     pid
       -c     creator
       -l     limits
       -u     summary

$ ipcs -m -l

The status flags indicate whether the region has been locked into RAM to prevent swapping (Section
    48.7) and whether the region has been marked to be destroyed when all processes have detached
it.

<limitation>
On Linux, ipcs(1) displays information 'only' about IPC objects for which we have read permission,
   regardless of whether we own the objects.

<non-portable-means>
Linux provides two nonstandard methods of obtaining a list of all IPC objects on the system:

1. files within the /proc/sysvipc directory that list all IPC objects

/proc/sysvipc/msg
/proc/sysvipc/sem
/proc/sysvipc/shm
key         shmid    perms    size  cpid  lpid nattch uid   gid  cuid  cgid      atime      dtime      ctime
305450176   163844   666      24    1063  1395 4      504   504   504   504 1422517874 1422517866         33

note: see cpid and size

/proc/sysvipc
Subdirectory containing the pseudo-files msg, sem and shm. These files list the System V
Interprocess Communication (IPC) objects (respectively: message queues, semaphores, and shared
    memory) that currently exist on the system, providing similar information to that available via
ipcs(1).  These files have headers and are formatted (one IPC object per line) for easy
understanding. <svipc>(7) provides further background on the information shown by these files.

2. the use of Linux-specific ctl calls.

Unlike the ipcs command, these files always show all objects of the corresponding type, regardless
of whether read permission is available on the objects.


={============================================================================
*kt_linux_core_202* ipc: system v: shm

{segment}
Shared memory allows two or more processes to share the same region (usually referred to as a
segment) of physical memory. 


{good-and-bad}
Since a shared memory segment becomes part of a process's user-space memory, no kernel intervention
is required for IPC. note: means the fastest IPC.

On the other hand, the fact that IPC using shared memory is not mediated by the kernel means that,
   typically, some method of synchronization is required so that processes don't simultaneously
   access the shared memory. e.g., two processes performing simultaneous updates, or one process
   fetching data from the shared memory while another process is in the middle of updating it.


{interfaces}
<shmget>
To create a new shared memory segment or obtain the identifier of an existing segment. note: The
contents of a newly created shared memory segment are initialized to 0.

#include <sys/shm.h>

int shmget(key_t key, size_t size, int shmflg);

Returns shared memory segment 'identifier' on success, or -1 on error

<creator-and-size>
The 'kernel' allocates shared memory in multiples of the system page size, so size is effectively
rounded up to the next multiple of the system 'page' size. If we are using shmget() to obtain the
identifier of an existing segment, then size has 'no' effect on the segment, but it 'must' be less
than or equal to the size of the segment.

to-creat:

IPC_CREAT
If no segment with the specified key exists, create a new segment.

<when-get-errors>
The real case which got EINVAL when called shmget. The below is from man page.

EINVAL 

A new segment was to be created and size < SHMMIN or size > SHMMAX, or no new segment was to be
created, a segment with given key existed, but size is 'greater' than the size of that segment.

note: The problem was to ask shm which is 'greater' than the size of the segment. Interestingly, the
real size was 24 but /proc/PID/maps shows 4K since the pagesize is 4K. So careful to see mapping
info.

<shmat>
#include <sys/shm.h>

void *shmat(int shmid, const void *shmaddr, int shmflg);

Returns address at which shared memory is attached on success, or (void *) -1 on error

The shmaddr argument and the setting of the SHM_RND bit in the shmflg bit-mask argument control
how the segment is attached: See 48.3 for full options.

If shmaddr is NULL, then the segment is attached at a suitable address selected by the kernel. This
is the 'preferred' method of attaching a segment since specifying a non-NULL value for shmaddr is
not recommended, for the following reasons:

It reduces the portability of an application and the particular address will be already in use.

As its function result, shmat() returns the address at which the shared memory segment is attached.
Typically, we assign the return value from shmat() to a pointer to some programmer-defined
structure, in order to impose that structure on the segment. 

If SHM_RDONLY is not specified, the memory can be both read and modified.

note: Seen shmat(, , 0), that is shmflg is 0. what is it? See <bypass-permission-check>

<shmdt>
When a process no longer needs to access a shared memory segment, it can call shmdt() to detach the
segment from its virtual address space.

#include <sys/shm.h>

int shmdt(const void *shmaddr);

Returns 0 on success, or -1 on error

Detaching a shared memory segment is not the same as deleting it. Deletion is performed using the
shmctl() IPC_RMID operation

<child-and-exec>
A child created by fork() inherits its parent's attached shared memory segments. Thus, shared memory
provides an easy method of IPC between parent and child. During an exec(), all attached shared
memory segments are detached. Shared memory segments are also 'automatically' detached on process
'termination'.

<shmctl> 
#include <sys/shm.h>

int shmctl(int shmid, int cmd, struct shmid_ds *buf);

Returns 0 on success, or -1 on error

IPC_RMID <deletion>
Mark the shared memory segment and its associated shmid_ds data structure for deletion. If no
processes currently have the segment attached, deletion is immediate; otherwise, the segment is
removed only 'after' all processes have detached from it (i.e., when the value of the shm_nattch
    field in the shmid_ds data structure falls to 0). This is much closer to the situation with file
deletion.

shmctl(id, IPC_RMID, NULL);   note: buf must be NULL.

Only one process needs to perform this step.

IPC_STAT
Place a copy of the shmid_ds data structure associated with this shared memory segment in the buffer
pointed to by buf.

IPC_SET
Update selected fields of the shmid_ds data structure associated with this shared memory segment
using values in the buffer pointed to by buf.


{associated-data}
See LPI 48.8 for full details.

struct shmid_ds {
  struct ipc_perm shm_perm;   /* Ownership and permissions */
  size_t shm_segsz;           /* Size of segment in bytes */
  time_t shm_atime;           /* Time of last shmat() */
  time_t shm_dtime;           /* Time of last shmdt() */
  time_t shm_ctime;           /* Time of last change */
  pid_t shm_cpid;             /* PID of creator */
  pid_t shm_lpid;             /* PID of last shmat() / shmdt() */
  shmatt_t shm_nattch;        /* Number of currently attached processes */
};

shm_cpid
This field is set to the process ID of the process that created the segment using shmget().

shm_lpid
This field is set to 0 when the shared memory segment is created, and then set to the process ID of
the calling process on each successful shmat() or shmdt().

shm_nattch
This field counts the number of processes that currently have the segment attached. It is
initialized to 0 when the segment is 'created', and then incremented by each successful shmat() and
decremented by each successful shmdt(). 


{shm-in-virtual-memory}
From LPI 48.5. To check shm details from /proc/PID/maps.

The shared memory segments are attached starting at the virtual address 0x40000000 between heap and
stack. Mapped mappings and shared libraries are also placed in this area. The address 0x40000000 is
defined as the kernel constant TASK_UNMAPPED_BASE. Than can be changed.

$ ./svshm_create -p 102400       # size
9633796                          # shm id
$ ./svshm_create -p 3276800
9666565

$ ./svshm_attach 9633796:0 9666565:0
SHMLBA = 4096 (0x1000), PID = 9903
1: 9633796:0 ==> 0xb7f0d000      # attached at a address chosen by kernel
2: 9666565:0 ==> 0xb7bed000

$ cat /proc/9903/maps
...
b7bed000-b7f0d000 rw-s 00000000 00:09 9666565 /SYSV00000000 (deleted)
b7f0d000-b7f26000 rw-s 00000000 00:09 9633796 /SYSV00000000 (deleted)
...

<2> Two lines for the attached System V shared memory segments.

<columns>
1. A pair of hyphen-separated numbers indicating the virtual address range at which the memory
segment is mapped. The second of these numbers is the address of the next byte after the end of the
segment.

note: From this, can get the size. b7f0d000-b7bed000=0x320000. 3,276,800

2. Protection and flags for this memory segment. The first three letters indicate the protection of
the segment: read (r), write (w), and execute (x). A hyphen (-) in place of any of these letters
indicates that the corresponding protection is disabled. The final letter indicates the mapping flag
for the memory segment; it is either private (p) or shared (s). For an explanation of these flags,
    see the description of the MAP_PRIVATE and MAP_SHARED flags in Section 49.2. 
    
note: A System V shared memory segment is always marked shared.

3. The hexadecimal offset (in bytes) of the segment within the corresponding mapped file. The
meanings of this and the following two columns will become clearer when we describe the mmap()
system call in Chapter 49. 

note: For a System V shared memory segment, the offset is always 0.

4. The device number (major and minor IDs) of the device on which the corresponding mapped file is
located.

5. The i-node number of the mapped file, or, for System V shared memory segments, the 'identifier'
for the segment.

6. The filename or other identifying tag associated with this memory segment. For a System V shared
memory segment, this consists of the string SYSV concatenated with the shmget() key of the segment
(expressed in hexadecimal). In this example, SYSV is followed by zeros because we created the
segments using the key IPC_PRIVATE (which has the value 0). The string (deleted) that appears after
the SYSV field for a System V shared memory segment is an artifact of the implementation of shared
memory segments. Such segments are created as mapped files in an invisible tmpfs file system
(Section 14.10), and then later unlinked. Shared anonymous memory mappings are implemented in the
same manner. (We describe mapped files and shared anonymous memory mappings in Chapter 49.)

<example> 
Linux (none) 2.6.31-3.2 #2 SMP Wed Dec 10 02:53:42 EST 2014 mips GNU/Linux

cat /proc/sysvipc/shm

key         shmid    perms    size  cpid  lpid nattch uid   gid  cuid  cgid      atime      dtime      ctime
305450176   163844   666      24    1066  1470 4      504   504   504   504 1423064150 1423064135         33

root# ls -al /proc/1066/
lrwxrwxrwx    1 root     root             0 Jan  1 00:12 exe -> /opt/cds/bin/huaweidaemon

root# cat /proc/1066/maps | grep SYS      note: why twice?
2aab3000-2aab4000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)
2aab5000-2aab6000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)

root# cat /proc/1470/maps | grep SYS
2aab2000-2aab3000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)

The diff is 0x1000. 4096.

root# cat /proc/1523/smaps | grep -A 20 2fc37
2fc37000-2fc38000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)
Size:                  4 kB
Rss:                   4 kB
Pss:                   1 kB
Shared_Clean:          4 kB
Shared_Dirty:          0 kB
Private_Clean:         0 kB
Private_Dirty:         0 kB
Referenced:            4 kB
Swap:                  0 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB            <pagesize>


{shm-limits}
On Linux, some of the limits can be viewed or changed via files in the /proc file system. See LPI
48.9 for more details. 

Below is a list of the Linux shared memory limits. The system call affected by the limit and the
error that results if the limit is reached are noted in parentheses.

SHMMNI
This is a system-wide limit on the number of shared memory 'identifiers' (in other words, shared
    memory segments) that can be created. (shmget(), ENOSPC)

SHMMIN
This is the minimum size (in bytes) of a shared memory segment. This limit is defined with the value
1 (this can't be changed). However, the effective limit is the system page size. (shmget(), <EINVAL>)

SHMMAX
This is the maximum size (in bytes) of a shared memory segment. The practical upper limit for SHMMAX
depends on available RAM and swap space. (shmget(), <EINVAL>)

SHMALL
This is a system-wide limit on the total number of pages of shared memory. Most other UNIX
implementations don't provide this limit. The practical upper limit for SHMALL depends on available
RAM and swap space. (shmget(), ENOSPC)

<debian-linux>
keitee@debian-keitee:/proc/sys/kernel$ uname -a
Linux debian-keitee 3.2.0-4-486 #1 Debian 3.2.63-2+deb7u2 i686 GNU/Linux
keitee@debian-keitee:/proc/sys/kernel$ ls -al shm*
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmall
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmmax
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmmni
-rw-r--r-- 1 root root 0 Jan 28 21:25 shm_rmid_forced
keitee@debian-keitee:/proc/sys/kernel$ cat shmall
2097152
keitee@debian-keitee:/proc/sys/kernel$ cat shmmax
33554432
keitee@debian-keitee:/proc/sys/kernel$ cat shmmni
4096
keitee@debian-keitee:/proc/sys/kernel$ cat shm_rmid_forced 
0

<source> ucLinux case
/include/linux/shm.h

SHMMAX: shm segment max size in byte,     131,072 (typical value),      33,554,432 (ucLinux)
SHMMIN: shm segment min size in byte,     1 (typical value),            1 (ucLinux)
SHMMNI: shm segment max num in system,    100 (typical value),          4,096 (ucLinux)
SHMSEG: shm segment max size in process,  6 (typical value),            4,096 (ucLinux)


{locking}
A shared memory segment can be locked into RAM, so that it is never swapped out. This provides a
performance benefit, since, once each page of the segment is memory-resident, an application is
guaranteed never to be delayed by a page fault when it accesses the page.

<not-all-support> TODO:
These operations are not specified by SUSv3, and they are not provided on all UNIX implementations.
In versions of Linux before 2.6.10, only privileged (CAP_IPC_LOCK) processes can lock a shared
memory segment into memory. Since Linux 2.6.10, an unprivileged process can lock and unlock a shared
memory segment if its effective user ID matches either the owner or the creator user ID of the
segment and (in the case of SHM_LOCK) the process has a sufficiently high RLIMIT_MEMLOCK resource
limit. See Section 50.2 for details.


={============================================================================
*kt_linux_core_203* ipc: server consideration

From LPI 45.4.

The consideration when IPC server is terminated prematurely.

Suppose a client engages in an extended dialogue with a server, with multiple IPC operations being
performed by each process (e.g., multiple messages exchanged, a sequence of semaphore operations, or
    multiple updates to shared memory). 

What happens if the server process 'crashes' or is deliberately halted and then restarted? 

At this point, it would make no sense to blindly reuse the existing IPC object created by the
previous server process, since the new server process has no knowledge of the historical information
associated with the current state of the IPC object. (For example, there may be a secondary request
    within a message queue that was sent by a client in response to an earlier message from the old
    server process.)

In such a scenario, the only option for the server may be to 'abandon' all existing
clients, 'delete' the IPC objects created by the previous server process, and create new
instances of the IPC objects. 

A newly started server handles the possibility that a previous instance of the server terminated
prematurely by first trying to create an IPC object by specifying both the IPC_CREAT and the
IPC_EXCL flags within the get call. If the get call fails because an object with the specified key
already exists, then the server assumes the object was created by an old server process; it
therefore uses the IPC_RMID ctl operation to delete the object, and once more performs a get call to
create the object. (This may be combined with other steps to ensure that another server process is
    not currently running, such as those described in Section 55.6.)

<example>
// return false if process is in slave mode(client)
// return true if process is in master mode(server)

// if no shm created already, then return true to create one.
// if shm is there, but no one use. so try to delete it and return true.
// if shm is there, but some use it, then return false.

// if no one use it then crash might happened so start it all over again.

static bool do_platform_init( size_t shMemSz )
{
  int    shmid, shmflag;
  struct shmid_ds shmds;

  shmflag = 0666;
  shmid = shmget( SHAREDMEM_KEY, shMemSz, shmflag );

  if ( shmid >= 0 )
  {
    if ( shmctl( shmid, IPC_STAT, &shmds ) < 0 )
    {
      perror( "shmctl - IPC_STAT failed" );
    }
    else
    {
      fprintf( stderr, "%s(): shmds.shm_nattch=%d\n",
               __FUNCTION__, (unsigned)shmds.shm_nattch );
      if ( 0 == shmds.shm_nattch && shmctl( shmid, IPC_RMID, (struct shmid_ds *)NULL ) < 0 )
      {
         fprintf( stderr, "shmctl - destroy failed\n" );
      }
    }
    return (0 == (unsigned)shmds.shm_nattch);
  }
  else
  {
    perror("shmget");
  }
  return true;
}


={============================================================================
*kt_linux_core_250*  ipc: posix

One of the POSIX.1b developers' aims was to devise a set of IPC mechanisms that did not suffer the
deficiencies of the System V IPC facilities. These IPC mechanisms are collectively referred to as
POSIX IPC.


={============================================================================
*kt_linux_core_101*  ipc: pipe and fifo

{pipe}
The pipe is 'unnamed' fifo and is an early form that can be used 'related'-processes such as parent
and child. In other words, created using fork() call. Linux supports uni-directional pipe or
half-duplex so need 'two' pipes for read and write.

<limited-capacity> from LPI 44
A pipe is simply a buffer maintained in kernel memory. This buffer has a maximum capacity. Once a
pipe is full, further writes to the pipe block until the reader removes some data from the pipe.
This means that pipe is kernel resource meaning that there is copy between kernel and process. Also
there is <no-open-call>.

<first> to-create-pipe
Get two fds from a pipe() call. fd[0] for read and fd[1] for write. Create a child via fork() and
close unused fds to create a single channel between parent and child. note: 0 for read and 1 for
write which are fixed.

parent                        child
fd[1] write   pipe ->         fd[0] read
fd[0] read                    fd[1] write

int filedes[2];

if (pipe(filedes) == -1) /* Create the pipe */
   errExit("pipe");

switch (fork()) { /* Create a child process */
 case -1:
   errExit("fork");

 case 0: /* Child */
   if (close(filedes[1]) == -1) // close unused 'write' end
     errExit("close");
   /* Child now reads from pipe */
   break;

 default: /* Parent */
   if (close(filedes[0]) == -1) // close unused 'read' end
     errExit("close");
   /* Parent now writes to pipe */
   break;
}


<second> to-create-pipe
popen() call which simplfies pipe creation, fork, reading/writing setting. However, need to set
problem to fork in the command line.

The C standard I/O library popen(3) makes it easy for the application programmer to open a pipe to
an external process.

#include <stdio.h>
FILE *popen(const char *command, const char *mode);
int pclose(FILE *stream);

The argument command must be a command that is acceptable to the UNIX shell. The second argument
mode must be the C string "r" for reading or "w" for writing.  No other combination, such as "w+",
is acceptable. 

(reading example)
FILE *p;
char cmd[1000];
/* argv[2] is fname */
sprintf(cmd,"grep 'Time has been updated to (Year:Month' %s | head -1",argv[2]);
p=popen(cmd,"r");
fgets(tmp,sizeof(tmp),p);
pclose(p);

After all, the reason that can use pipe between parent/child is that fds are shared.

<broken-pipe>
When 'write' to fifo that is not opened to read. Menas there is no reading process or reading process
is killed because reading fd of pipe or file gets closed. SIGPIPE. Instead, there is no data to
read, empty, reading thread or process is blocked.


{fifo}
To solve this, fifo was introduced and is called 'named'-pipe since has path name. Means that it
is created in the filesystem as a file. Use usual read and write call. Fifo is either read-only or
write-only.

1. create a fifo using mkfifo call.
2. open a fifo for read or write using open call

<fifo-create>
#include <sys/stat.h>
int mkfifo(const char *pathname, mode_t mode);

<fifo-sync>
Therefore, by default, opening a FIFO for reading (the open() O_RDONLY flag) blocks until another
process opens the FIFO for writing (the open() O_WRONLY flag). Conversely, opening the FIFO for
writing blocks until another process opens the FIFO for reading.  In other words, opening a FIFO
synchronizes the reading and writing processes.

<note-from-nds-fusion-ipc>
FIFOs have certain natural limitations, they are unidirectional, and fragment large message sizes
with no built in support for reassembling the fragments. Furthermore FIFOs are limited in number due
to system resources. The key are 'fragment' and 'limit'-in-number.

Due to the limited number of FIFOs it was determined that there would be one
control FIFO per server (to establish communication from clients) and one pair of unidirectional
FIFOs for every pair of server and client instances (note that there is one instance of a client in
every process that uses that client). This is illustrated below.

server                  client a
- control pipe          -> and <-

                        client b
                        -> and <-

note: KT. do not match with this pic?

To cope with fragmentation and the fact that there may be several interfaces being used on a single
client instance (and multiple components using that client instance in a single process) a protocol
was designed as described below. This protocol is used in both FIFO and TCP/IP IPC communication.


{summary}
When writes, if there is no reading process, broken-pipe. When reads, if there
is no writing process, blocked.


={============================================================================
*kt_linux_core_103* ipc: which one to use

When performing interprocess synchronization, our choice of facility is typically determined by the
functional requirements. When coordinating access to a file, file record locking is usually the best
choice. Semaphores are often the better choice for coordinating access to other types of shared
resource.

{semaphores-versus-pthreads-mutexes}

<1>
Unlike mutex (this mean condition?), semaphore does not get lost when there is no waiting one.

<2>
POSIX semaphores and Pthreads mutexes can both be used to synchronize the actions of threads within
the same process, and their performance is [similar]. 

However, mutexes are usually preferable, because the [ownership] property of mutexes enforces good
structuring of code; only the thread that locks a mutex can unlock it. By contrast, one thread can
increment a semaphore that was decremented by another thread. This flexibility can lead to poorly
structured synchronization designs. For this reason, semaphores are sometimes referred to as the
"gotos" of concurrent programming.

There is one circumstance in which mutexes can't be used in a multithreaded application and
semaphores may therefore be preferable. Because it is async-signalsafe. See
{async-signal-safe-function}. The sem_post() function can be used from within a signal handler to
synchronize with another thread. This is not possible with mutexes, because the Pthreads functions
for operating on mutexes are not asyncsignal-safe. 

However, because it is usually preferable to deal with asynchronous signals by accepting them using
sigwaitinfo() (or similar), rather than using signal handlers (see Section 33.2.4), this advantage
of semaphores over mutexes is seldom required.

<3>
From 30.1.3. Peformance of mutex in ref-LPI. The problem with file locks and semaphores is that they
always require a system call for the lock and unlock operations, and each system call has a small
but appreciable, cost (Section 3.1). By contrast, mutexes are implemented using atomic
machine-language operations; performed on memory locations visible to all threads and require system
calls only in case of lock contention.


On Linux, mutexes are implemented using futexes (an acronym derived from fast user space mutexes),
   and lock contentions are dealt with using the futex() system call. We donât describe futexes in
   this book (they are not intended for direct use in user-space applications), but details can be
   found in [Drepper, 2004 (a)], which also describes how mutexes are implemented using futexes.
   [Franke et al., 2002] is a (now outdated) paper written by the developers of futexes, which
   describes the early futex implementation and looks at the performance gains derived from futexes.


# ============================================================================
#{
={============================================================================
*kt_linux_core_200* ipc: sync: semaphore

{what-is-semaphore}
note: This can be used for process or thread but it is expensive for thread.

System V semaphores are not used to transfer data between processes. Instead, they allow processes
to synchronize their actions. A semaphore is a 'kernel'-maintained 'integer' whose value is never
permitted to fall below 0. A process can decrease or increase the value of a semaphore.  If an
attempt is made to decrease the value of the semaphore below 0, then the kernel blocks the operation
until the semaphore's value increases to a level that permits the operation to be performed. 

The meaning of a semaphore is determined by the application. A process decrements a semaphore from
say, 1 to 0 in order to reserve exclusive access to some 'shared' resource, and after completing work
on the resource, increments the semaphore so that the shared resource is released for use by some
other process. The use of a binary semaphore-a semaphore whose value is limited to 0 or 1-is common.

However, an application that deals with multiple instances of a shared resource would employ a
semaphore whose maximum value equals the 'number' of shared resources. Linux provides both System V
semaphores and POSIX semaphores, which have essentially similar functionality.

1. Setting the semaphore to an absolute value;
2. Adding a number to the current value of the semaphore;            // give
3. Subtracting a number from the current value of the semaphore; and // take
4. Waiting for the semaphore value to be equal to 0.

The last two of these operations may cause the calling process to block. A semaphore has no meaning
in and of itself. Its meaning is determined only by the associations given to it by the processes
using the semaphore.

However, System V semaphores are rendered unusually complex by the fact that they are allocated in
groups called semaphore sets. So move to POSIX semaphore.


{posix-semaphore}
SUSv3 specifies two types of POSIX semaphores:

1. Named semaphores: This type of semaphore has a name. By calling sem_open() with the same name
'unrelated' processes can access the same semaphore.

2. Unnamed semaphores: This type of semaphore doesn't have a name; instead, it resides at an
agreed-upon location in 'memory'. Unnamed semaphores can be shared between processes or between a
group of 'threads'. When shared between processes, the semaphore must reside in a region of (System V,
POSIX, or mmap()) shared memory. When shared between threads, the semaphore may reside in an
area of memory shared by the threads (e.g., on the heap or in a global variable).

POSIX semaphores operate in a manner similar to System V semaphores; that is, a POSIX semaphore is
an integer whose value is not permitted to fall below 0. If a process attempts to decrease the value
of a semaphore below 0(take), then, depending on the function used, the call either blocks or fails
with an error indicating that the operation was not currently possible.

Some systems don't provide a full implementation of POSIX semaphores. A typical restriction is that
only unnamed thread-shared semaphores are supported. That was the situation on Linux 2.4; 

<linux-2-6-support>
With Linux 2.6 and a glibc that provides NPTL, a full implementation of POSIX semaphores is
available.

<named-semaphore>
To work with a named semaphore, we employ the following functions:

1. The sem_open() function opens or creates a semaphore, initializes the semaphore if it is created
by the call, and returns a handle for use in later calls.

#include <fcntl.h> /* Defines O_* constants */
#include <sys/stat.h> /* Defines mode constants */
#include <semaphore.h>

sem_t *sem_open(const char *name, int oflag, ...  /* mode_t mode, unsigned int value */ );

Returns pointer to semaphore on success, or SEM_FAILED on error

2. The sem_post(sem) and sem_wait(sem) functions respectively increment and decrement a semaphore's
value. give and take

3. The sem_getvalue() function retrieves a semaphore's current value.

4. The sem_close() function removes the calling processâs association with a semaphore that it
previously opened.

5. The sem_unlink() function removes a semaphore name and marks the semaphore for deletion when all
processes have closed it.

<named-semaphore-on-linux>
SUSv3 doesn't specify how named semaphores are to be implemented. On Linux, they are created as
small POSIX shared memory objects with names of the form sem.name, in a dedicated tmpfs file system
(Section 14.10) mounted under the directory /dev/shm. This file system has 'kernel'-persistence-the
semaphore objects that it contains will persist, even if no process currently has them open, but
they will be lost if the system is shut down.

<unnamed-semaphore>
The semaphore is made available to the processes or threads that use it by placing it in an area of
memory that they share. Operations on unnamed semaphores use the same functions; sem_wait(),
sem_post(), sem_getvalue(), and so on that are used to operate on named semaphores.

In addition, two further functions are required:

The sem_init() function initializes a semaphore and informs the system of whether the semaphore will
be shared between processes or between the threads of a single process.

The sem_destroy(sem) function destroys a semaphore. These functions should not be used with named
semaphores.

<when-useful-to-use-unnamed>
1. A semaphore that is shared between 'threads' doesn't need a name. Making an unnamed semaphore a
shared (global or heap) variable automatically makes it accessible to all threads.

2. A semaphore that is being shared between 'related' processes doesn't need a name. If a parent
process allocates an unnamed semaphore in a region of shared memory (e.g., a shared anonymous
mapping), then a child automatically inherits the mapping and thus the semaphore as part of the
operation of fork().

3. If we are building a dynamic data structure (e.g., a binary tree), each of whose items requires
an associated semaphore, then the simplest approach is to allocate an unnamed semaphore within each
item. Opening a named semaphore for each item would require us to design a convention for generating
a (unique) semaphore name for each item and to manage those names (e.g., unlinking them when they
are no longer required). note: real example? useful?


={============================================================================
*kt_linux_core_201* ipc: sync: pthread mutex and cond var

From #07 in {ref-UNP} and #30 in {ref-LPI}

This is to sync for data or critial region. Although talking of critial region, what is really
protected is the 'data' being manipulated within the critical region. This is posix.1 standard.

As with semaphore, mutex and cond-var uses 'global' structure and if these are shared in a shared
memory, these can be used between processes. If not, it is for threads in a process.

More generally, mutexes can be used to ensure atomic access to any shared resource, but protecting
shared variables is the most common use.

In {ref-LPI} p881, process-shared mutexes and condition variables. They are 'not' available on all
UNIX systems, and so are not commonly employed for process synchronization.

note: As said, if share mutex structure in shared memory, then what is process-shared mutex?


{why-need-sync}
Because of 'race' condition. Like {ref-LPI}, when run two threads without sync, task-switching
happens based on time-slot given and yield 'non''determistic' result.

threadFunc()
{
  for(j=0; j < loops; j++)
  {
    local = global;
    local++;
    global = local;
  }
}

If we change this like below, do not need to sync because inc looks atomic?

threadFunc()
{
  for(j=0; j < loops; j++)
  {
    global++;
  }
}

NO because this single inc operator may result in 'multiple' machine code which are equivalent to the
before. See *kt_linux_core_107* for more about atomic and race-condition.


{ownership} mutex-deadlock
A mutex has two states: locked and unlocked. At any moment, at most one thread may hold the lock on
a mutex. Attempting to lock a mutex that is already locked either blocks or fails with an error
depending on the method used to place the lock.

When a thread locks a mutex, it becomes the 'owner' of that mutex. Only the mutex owner can 'unlock' the
mutex. This property 'improves' the structure of code that uses mutexes and also allows for some
optimizations in the implementation of mutexes.

1. Locking. If try to lock what is already locked by self, two may happen for 'default' type of
mutex: mutex-'deadlock' or EDEADLK for error check type. On Linux, deadlock by default from
{ref-LPI}. Why deadlock?  Because blocked trying to lock a mutex that it already owns.

2. Unlocking. Unlocking a mutex that is not locked or that is locked by another thread 'undefined'
result.

note: ownership matters when unlock and semaphore do not have ownership.


{cooperative-lock}
This means that mutex locking is advisory, rather than mandatory; nothing can prevent one thread
from manipulating the data without first obtaining the mutex. For example, threads not participating
a mutex circle can.

Each thread employs the following protocol for accessing a resource:
1. lock the mutex for the shared resource;
2. access the shared resource; and
3. unlock the mutex.


{implicit-sync}
This is synchronization handled by kernel not by applicaion. Such as pipe and message-q. For
example,

grep pattern chapters.* | wc -l

Writing by producer and reading by consumer are handled by kernel. Does it mean that grep and wc
runs at the same time? not one after one.


{mutex-apis} {explicit-sync}
Unlike semaphore and file locks, mutex only requires system call for lock contention since on Linux,
mutexes are implemented using futexes (an acronym derived from fast user space mutexes), and lock
contentions are dealt with using the futex() system call. So less expensive. from 30.1.3 in
{ref-LPI}

If it is 'statically' allocated for default attributes, should use:

pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;

If it is dymamically allocated such as malloc or shared-mem, should use. When? mutex can be part of
structure or object on the heap.

1. The mutex was dynamically allocated on the heap. For example, suppose that we create a
dynamically allocated linked list of structures, and each structure in the list includes a
pthread_mutex_t field that holds a mutex that is used to protect access to that structure.

2. The mutex is an automatic variable allocated on the stack.

3. We want to initialize a statically allocated mutex with attributes other than the defaults.

int pthread_mutex_init(pthread_mutex_t* mutex, const pthread_mutexattr_t* attr);
int pthread_mutex_destory(pthread_mutex_t* mutex);


This is blocking call.

int pthread_mutex_lock( pthread_mutex_t *mptr); 
int pthread_mutex_unlock( pthread_mutex_t *mptr);


This is non-blocking and returns EBUSY if the mutex is already locked. If the time interval
specified by its abstime argument expires without the caller becoming the owner of the mutex
pthread_mutex_timedlock() returns the error ETIMEDOUT.

These are less used because need to poll and risks being starved. Well designed one can avoid this.
In most well-designed applications, a thread should hold a mutex for only a short time, so that
other threads are not prevented from executing in parallel.

int pthread_mutex_trylock( pthread_mutex_t *mptr);
int pthread_mutex_timedlock( pthread_mutex_t *mptr);


{deadlock-condition}
A deadlock is a situation waiting for something never happens or where two or more process are
blocked because each is waiting on the other process(es) to complete some action. Unrequited love?

<1> from pipe
If employing this bidirectional communication using two pipes, then we need to be wary of deadlocks
that may occur if both processes block while trying to read from empty pipes or while trying to
write to pipes that are already full.

<2> from fifo.
use-different-fifo-for-read-and-write

Process X                              Process Y
1. Open FIFO A for reading(blocks)     1. Open FIFO A for reading(blocks)
2. Open FIFO B for writing             2. Open FIFO B for writing

When open a fifo to read which is not opend to write but there is no writing process then reading
process is blocked. Therefore, if parent and child opens different fifos to read, deadlock happens.

use-same-fifo-for-read-and-write-but-the-same-order

Process X                              Process Y
1. Open FIFO A for reading(blocks)     1. Open FIFO B for reading(blocks)
2. Open FIFO B for writing             2. Open FIFO A for writing

The two processes shown in Figure 44-8 are deadlocked. Each process is blocked waiting to open a
FIFO for reading. This blocking would not happen if each process could perform its second step
(opening the other FIFO for writing). So change order or use non-blocking call.

<3>
Message queues have a limited capacity. This has the potential to cause a couple of problems. One of
these is that multiple simultaneous clients could fill the message queue, resulting in a deadlock
situation, where no new client requests can be submitted and the server is blocked from writing any
responses.

<4> from mutex-deadlock
Locking. If try to lock what is already locked by self, two may happen for 'default' type of mutex:
mutex-deadlock or EDEADLK. On Linux, deadlock by default from {ref-LPI}. Why deadlock? Because
blocked trying to lock a mutex that it already owns. This could happen when exception is raised
between lock and unlock pair.

<5> from lock-free-queue
Used the timed_wait() instead of the simpler wait() to solve a possible deadlock when Produce() is
called between line A and line B in Listing One. Then wait() will miss the notify_one() call and
have to wait for the next produced element to wake up. 'if' this element never comes (no more
produced elements or if the Produce() call actually waits for Consume() to return), there's a
deadlock.

<6> from {ref-LPI}, deadlock happens that is the same as mutex-deadlock

Thead A             Thread B
lock(mutex1)        lock(mutex2)
lock(mutex2)        lock(mutex1)

Both tries to lock the mutex that the other thread has already locked. Both threads will remain
blocked indefinitely.


{to-avoid-deadklock}
1. mutex-hierarchy-or-order
Less flexible. Use locks in the same set of mutexs in the same order.

2. try-and-back-off
Flexible. If try_lock fails, release all and try again later. This is less efficient than hierarchy
approach but can be more 'flexible' since no need rigid hierarchy.


{mutex-types} from {ref-LPI} 30.1.7:
PTHREAD_MUTEX_NORMAL
// no deadlock detection. On Linux, this is PTHREAD_MUTEX_DEFAULT

PTHREAD_MUTEX_ERRORCHECK
// reports errors when fails than blocking or deadlock but 'slower' than a normal so debugging
// purpose.

PTHREAD_MUTEX_RECURSIVE
// A recursive mutex maintains the concept of a lock-'count'. When a thread first acquires the mutex,
// the lock count is set to 1. Each subsequent lock operation by the same thread increments the lock
// count, and each unlock operation decrements the count. The mutex is released (i.e., made
// available for other threads to acquire) only when the lock count falls to 0. 
//
// When is it useful?
//
// Unlocking an unlocked mutex fails, as does unlocking a mutex that is currently locked by another
// thread. 
//
// note: Still have ownership notion but there is no mutex deadlock or undefined result as NOMAL
// type has.


{consumer-producer-example-one}
From {ref-UNP}. Multiple producer(writing) and one consumer(reading). Once writing finishs, consumer
get started. No need to sync between producer and consumer. Only sync between producers. So use
mutex and no cond var. 

No need to have a sync for reading? NO. Need to have reading in sync if want to have right result.
For this example, reaading starts after writing. Hence no need.

<code-example>

#include <stdio.h>
#include <pthread.h>
#include <sys/errno.h>

#define MAXNITEMS 	1000000
#define MAXNTHREADS	100

#define	min(a,b)	((a) < (b) ? (a) : (b))
#define	max(a,b)	((a) > (b) ? (a) : (b))

//
void Pthread_create(pthread_t* tid, const pthread_attr_t* attr, void *(*func)(void*), void*arg)
{
	int n;

	if(( n = pthread_create( tid, attr, func, arg )) == 0 )
		return;
	
	errno = n;
	fprintf( stderr, "pthread_create error(%d)", n );
}

// shared by all threads
int nitems;

struct {
	pthread_mutex_t mutex;
	int				buff[MAXNITEMS];
	int				nput;				// next index to write
	int				nval;				// next val to write
} shared = { PTHREAD_MUTEX_INITIALIZER };

void *produce(void *), *consume(void *);

int main( int argc, char** argv )
{
	int i, nthreads, count[MAXNTHREADS]={0};
	pthread_t tid_produce[MAXNTHREADS]={0}, tid_consume;

	if( argc != 3 )
	{
		fprintf( stderr, "usuage: prodcons2 <#items> <#threads>\n");
		exit(1);
	}

	nitems = min( atoi( argv[1] ), MAXNITEMS );
	nthreads = min( atoi( argv[2] ), MAXNTHREADS );

	// start all producer threads
	for( i=0; i < nthreads; ++i )
	{
		count[i] = 0;
		Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
	}

	// wait for all the producer threads
	for( i=0; i < nthreads; ++i ) 
	{
		pthread_join( tid_produce[i], NULL );
		printf("tid[%d] count[%d] = %d\n", tid_produce[i], i, count[i] );
	}

	// start, then wait for the consumer thread
	Pthread_create(&tid_consume, NULL, consume, NULL );
	pthread_join(tid_consume, NULL );

	exit(0);
}

void* produce(void* arg)
{
	printf("run tid[%d] \n", pthread_self());

	for(;;) 
	{
		pthread_mutex_lock( &shared.mutex );
		if( shared.nput >= nitems )  // buff is full, we are done.
		{
			printf("done tid[%d] \n", pthread_self());
			pthread_mutex_unlock(&shared.mutex);
			return NULL;
		}

		shared.buff[ shared.nput ] = shared.nval;
		shared.nput++;
		shared.nval++;

		pthread_mutex_unlock( &shared.mutex );

		// inc of the count is not part of the CR because each thread has its own
		*((int*)arg) += 1;
	}
}

void* consume(void* arg)
{
	int i;

	// see use of global vars; nitems and shared.buff
	for( i=0; i < nitems; i++ )
	{
		if( shared.buff[i] != i )
			printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
	}

	printf("consume done\n" );

	return NULL;
}


kit@kit-VirtualBox:~/work$ ./a.out 1000000 5
run tid[-1252185280] 
run tid[-1243792576] 
run tid[-1235399872] 
run tid[-1227007168] 
run tid[-1218614464] 
done tid[-1252185280] 
done tid[-1243792576] 
done tid[-1227007168] 
done tid[-1218614464] 
tid[-1218614464] count[0] = 198088
tid[-1227007168] count[1] = 220562
done tid[-1235399872] 
tid[-1235399872] count[2] = 254805
tid[-1243792576] count[3] = 162778
tid[-1252185280] count[4] = 163767
consume done


{consumer-producer-example-two} 

Now consumer runs at the same time and runs when there is data to read. All is accessing the same
data and are in the same mutex group. Problem is that consumer do busy loop to check data, <polling>
or <spinning> 

<code-example>

The changes from the previous is:

int main()
{
	...

	// start all producer threads
	for( i=0; i < nthreads; ++i )
	{
		count[i] = 0;
		Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
	}

	// moved here. start, then wait for the consumer thread
	Pthread_create(&tid_consume, NULL, consume, NULL );

	...
}

Uses shared mutex between producers and consumer. If items to read are ready then call returns.
Otherwise, do loops. 

void consume_wait(int i)
{
  for(;;) {
    pthread_mutex_lock(&shared.mutex);
    if( i < shared.nput ) {
      pthread_mutex_unlock(&shared.mutex);
      return; /* item is ready */
    }
    pthread_mutex_unlock(&shared.mutex);
  }
}

void* consume(void* arg)
{
	int i;

	// see use of global vars; nitems and shared.buff
	for( i=0; i < nitems; i++ )
	{ >
		// polling until item is ready
		consume_wait(i);
<
		if( shared.buff[i] != i )
			printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
	}

	printf("con: done\n" );

	return NULL;
}


{consumer-producer-example-three} {cond-var}

The polling is a waste of cpu time. How to solve? A mutex is for locking and a cond-var is for
waiting. The mutex provides mutual exclusion for accessing the shared variable, while the condition
variable is used to signal changes in the variableâs state.

<cond-wait>
// This is blocking on condition and ALWAYS has an associated mutex. Why? because both producer and
// consumer accesses to the shared state variable which is linked to that condition so should be
// synced. In other words, there is a natural association of a muext with a condition varaible.
//
// pthread_cond_wait( &nready.cond, &nready.mutex );
//
// Do three things: unlock a mutex, block a calling thread until signaled, and relock mutex when
// signaled. 
//
// A condition variable holds no state information. It is simply a mechanism for communicating
// information about the application's state.

int pthread_cond_wait( pthread_cond_t *cptr, pthread_mutex_t* mptr);

<cond-can-be-lost>
// Guaranteede that at least one of the blocked thread is woken up. If no thread is waiting, the
// signal is lost since condition variable holds no state information.

int pthread_cond_signal( pthread_cond_t *cptr );
int pthread_cond_broadcast( pthread_cond_t *cptr );
int pthread_cond_timedwait( ... );

The {ref-LPI} said that _signal is for the case where all waiting threads do the same task since it
do not care which should woken up and _broadcast is for case where waiting threads do different task.
Real examples?

<code-example>

{Q} Is it possible to implement this using two mutex than using a muext and a cond-var?

//
int nitems;
int buff[MAXNITEMS];

// shared by all threads
struct {
	pthread_mutex_t mutex;
	int				nput;				// next index to write
	int				nval;				// next val to write
} put = { PTHREAD_MUTEX_INITIALIZER };

struct {
	pthread_mutex_t mutex;
	pthread_cond_t cond;
	int ready;						// [KT] this is state information linked to condition.
} nready = { PTHREAD_MUTEX_INITIALIZER, PTHREAD_COND_INITIALIZER };

void* produce(void* arg)
{
	printf("pro: run tid[%d] \n", pthread_self());

	for(;;) 
	{
		pthread_mutex_lock( &put.mutex );
		if( put.nput >= nitems )  // buff is full, we are done.
		{
			printf("pro: no more. done tid[%d] \n", pthread_self());
			pthread_mutex_unlock(&put.mutex);
			return NULL;
		}

		buff[ put.nput ] = put.nval;
		put.nput++;
		put.nval++;

		pthread_mutex_unlock( &put.mutex );
>
		// { signal cond-var saying some has been written
		pthread_mutex_lock( &nready.mutex );
		if( nready.ready == 0 )
			pthread_cond_signal( &nready.cond );

		nready.ready++;

		pthread_mutex_unlock( &nready.mutex );
		// }
		
		// inc of the count is not part of the CR because each thread has its own
		*((int*)arg) += 1;
	}
}

void* consume(void* arg)
{
	int i;

	// see use of global vars; nitems and shared.buff
	for( i=0; i < nitems; i++ )
	{ >
		// receieve cond-var
		pthread_mutex_lock( &nready.mutex );

		// Always test the condition again when wakes up because [spurious-wakeups] can occur.
		// Unlock and wait. When returns from pthread_cond_wait, lock in again.
		while( nready.ready == 0 )
			pthread_cond_wait( &nready.cond, &nready.mutex );

		nready.ready--;

		pthread_mutex_unlock( &nready.mutex );
<
		if( buff[i] != i )
			printf("con: err: buff[%d] = %d\n", i, buff[i] );
	}

	printf("con: done\n" );

	return NULL;
}


{the-order-of-call}

{ref-LPI} uses different call order for producer as below because {ref-UNP} said that use _signal
before unlock can cause {mutex-deadlock} or {lock-conflict}. Therefore, POSIX recommends the
followings and {ref-LPI} said this may yieid better performance in 30.2.2:

<approach-one>
pthread_mutex_lock( &nready.mutex );
if( nready.ready == 0 )
   pthread_cond_signal( &nready.cond );

nready.ready++;

pthread_mutex_unlock( &nready.mutex );

<approach-two>
pthread_mutex_lock( &nready.mutex );

nready.ready++;

pthread_mutex_unlock( &nready.mutex );

if( nready.ready == 0 )
 pthread_cond_signal( &nready.cond );


{check-on-predicate}

Each condition variable has an associated predicate involving one or more shared variables. In this
example, nready.ready == 0 is the predicate.

This demonstrates a general design principle: a pthread_cond_wait() call must be governed by a while
loop rather than an if statement. This is so because, on return from pthread_cond_wait(), there are
no guarantees about the state of the predicate; therefore, we should immediately recheck the
predicate and resume sleeping if it is not in the desired state. [KT] When calls cond_wait, the
associated lock is released so no gurantee that this predicate is the same when wakes up.

So use [while-loop] always to test the condition again when wakes up because spurious wakeups can
occur for following reasons:

while( nready.ready == 0 )
	pthread_cond_wait( &nready.cond, &nready.mutex );

1> Other threads may be woken up first. Perhaps several threads were waiting to acquire the mutex
associated with the condition variable. Even if the thread that signaled the mutex set the predicate
to the desired state, it is still possible that another thread might acquire the mutex first and
change the state of the associated shared variable(s), and thus the state of the predicate.

2> Designing for "loose" predicates may be simpler. Sometimes, it is easier to design applications
based on condition variables that indicate possibility rather than certainty. In other words,
signaling a condition variable would mean "there may be something" for the signaled thread to do
rather than "there is something" to do. Using this approach, the condition variable can be signaled
based on approximations of the predicateâs state, and the signaled thread can ascertain if there
really is something to do by rechecking the predicate.

3> Spurious wake-ups can occur. On some implementations, a thread waiting on a condition variable
may be woken up even though no other thread actually signaled the condition variable. Such spurious
wake-ups are a (rare) consequence of the techniques required for efficient implementation on some
multiprocessor systems, and are explicitly permitted by SUSv3.


==============================================================================
*kt_linux_core_220*	sync: read-write lock

From {ref-UNP} but no such a thing in {ref-LPI}, so may be old way but surely in posix but may be
different to this since this lock is before posix standard. See {ref-UNP} note.

To distinguish between obtaining the read-write lock for reading and for writing. The rules:

1. Any number of threads can hold a given read-write lock for reading as long as no threads holds
the the lock for writing.

2. A read-write lock can be allocated for writing only if no thread hold the lock for reading or
writing.

Stated another way, any threads can have read access to a data as long as no thread is modifying
that. A data can be modified only if no other thread is reading or modifying the data.

In application, the data is read more often than the data is modified, and these can benefit from
using read-write locks instead of mutex locks. 

can provide more concurrency than a plain mutex lock when the data is read more than it is written
and known as shared-exclusive locking since shared lock for reading and exclusive lock for writing.
Multiple readers and one writer problem or readers-writer locks.


{apis}

// to get read lock. blocks the calling if there are writers
int   pthread_rwlock_rdlock(pthread_rwlock_t *);

// to get write lock. blocks the calling if there are readers or writers
int   pthread_rwlock_wrlock(pthread_rwlock_t *);

int   pthread_rwlock_unlock(pthread_rwlock_t *);
int   pthread_rwlock_tryrdlock(pthread_rwlock_t *);
int   pthread_rwlock_trywrlock(pthread_rwlock_t *);

int   pthread_rwlock_init(pthread_rwlock_t *, const pthread_rwlockattr_t *);
int   pthread_rwlock_destroy(pthread_rwlock_t *);
int   pthread_rwlockattr_destroy(pthread_rwlockattr_t *);
int   pthread_rwlockattr_init(pthread_rwlockattr_t *);

// to share the lock between different processes
int   pthread_rwlockattr_setpshared(pthread_rwlockattr_t *, int); pthread_t
int   pthread_rwlockattr_getpshared(const pthread_rwlockattr_t *, int *);


{example-implementation}
\unpv22e.tar\unpv22e\my_rwlock_cancel\

Can be implemented using mutexes and condition variables. This is an implementation which gives
preference to waiting writers but there are other alternatives.

typedef struct {
	pthread_mutex_t 	rw_mutex;			// lock on this struct
	pthread_cond_t 	rw_condreaders;	// for reader threads waiting
	pthread_cond_t 	rw_condwriters;	// for writer threads waiting

	// [KT]
	// when struct is inited, set to RW_MAGIC and used by all functions to check that the caller is
	// passing a pointer to an initialized lock and set to 0 when the lock is destroyed.
	int 					rw_magic;
	int 					rw_nwaitreaders;
	int 					rw_nwaitwriters;

	// the current status of the read-write lock. only one of these can exist at a time: -1 indicates
	// a write lock, 0 is lock available, and an value greater than 0 menas that many read locks are
	// held.
	int 					rw_refcount; 
} pthread_rwlock_t;


#define RW_MAGIC 0x19283746

/* init and destroy */
int
pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		result;

	if (attr != NULL)
		return(EINVAL);		/* not supported */

	if ( (result = pthread_mutex_init(&rw->rw_mutex, NULL)) != 0)
		goto err1;
	if ( (result = pthread_cond_init(&rw->rw_condreaders, NULL)) != 0)
		goto err2;
	if ( (result = pthread_cond_init(&rw->rw_condwriters, NULL)) != 0)
		goto err3;
	rw->rw_nwaitreaders = 0;
	rw->rw_nwaitwriters = 0;
	rw->rw_refcount = 0;
	rw->rw_magic = RW_MAGIC;

	return(0);

err3:
	pthread_cond_destroy(&rw->rw_condreaders);
err2:
	pthread_mutex_destroy(&rw->rw_mutex);
err1:
	return(result);			/* an errno value */
}

void
Pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		n;

	if ( (n = pthread_rwlock_init(rw, attr)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_init error");
}

int
pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);
	if (rw->rw_refcount != 0 ||
		rw->rw_nwaitreaders != 0 || rw->rw_nwaitwriters != 0)
		return(EBUSY);

	pthread_mutex_destroy(&rw->rw_mutex);
	pthread_cond_destroy(&rw->rw_condreaders);
	pthread_cond_destroy(&rw->rw_condwriters);
	rw->rw_magic = 0;

	return(0);
}

void
Pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	int		n;

	if ( (n = pthread_rwlock_destroy(rw)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_destroy error");
}


// rdlock
// A problem exists in this function: if the calling thread blocks in the call to pthread_cond_wait
// and the thread is then canceled, the thread terminates while it holds the mutex lock, and the
// counter rw_nwaitreaders is wrong. The same problem exists in our implentation of
// pthred_rwlock_wrlock. Can correct these problem in {}

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// 4give preference to waiting writers. cannot get a read lock if a) rw_refcount < 0 (meaning
	// there is a writer holding the lock and b) if threads are waiting to get a write lock.
	// [KT] if. case that there are writers
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}

	// [KT] else. case that there are no writers
	if (result == 0)
		rw->rw_refcount++;		/* another reader has a read lock */

	pthread_mutex_unlock(&rw->rw_mutex);
	return (result);
}

/* tryrdlock */
int
pthread_rwlock_tryrdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0)
		result = EBUSY;			/* held by a writer or waiting writers */
	else
		rw->rw_refcount++;		/* increment count of reader locks */

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


/* wrlock */
int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// [KT] if there are other readers or writers
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}

	// [KT] else there are no readers and writers
	if (result == 0)
		rw->rw_refcount = -1;

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


// unlock. used by both reader and writer
int
pthread_rwlock_unlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount > 0)
		rw->rw_refcount--;			// releasing a reader
	else if (rw->rw_refcount == -1)
		rw->rw_refcount = 0;			// releasing a writer
	else
		err_dump("rw_refcount = %d", rw->rw_refcount); // cannot be since it is to unlock

	// 4give preference to waiting writers over waiting readers
	//
	// {Q} The ref-UNP says: notice that we do not grant any additional read locks as soon as a
	// writer is waiting; otherwise, a stream of continual read requests could block a waiting writer
	// forever. For this reason, we need two separate if tests and cannot write
	//
	// if( rw->rw_nwaitwriters > 0 && rw->rw_refcount == 0 )
	// ...
	// 
	// could also omit the test of rw->rw_refcount, but that can result in calls to
	// pthread_cond_signal when read locks are still allocated, which is less efficient.
	//
	if (rw->rw_nwaitwriters > 0) {
		if (rw->rw_refcount == 0)
			result = pthread_cond_signal(&rw->rw_condwriters);		// signal single writer
	} else if (rw->rw_nwaitreaders > 0)
		result = pthread_cond_broadcast(&rw->rw_condreaders);		// signal all readers

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


{example}

#include	"unpipc.h"
#include	"pthread_rwlock.h"

pthread_rwlock_t	rwlock = PTHREAD_RWLOCK_INITIALIZER;

void	 *thread1(void *), *thread2(void *);
pthread_t	tid1, tid2;

int
main(int argc, char **argv)
{
	void	*status;
	Pthread_rwlock_init(&rwlock, NULL);

	Set_concurrency(2);
	Pthread_create(&tid1, NULL, thread1, NULL);
	sleep(1);		/* let thread1() get the lock */
	Pthread_create(&tid2, NULL, thread2, NULL);

	Pthread_join(tid2, &status);
	if (status != PTHREAD_CANCELED)
		printf("thread2 status = %p\n", status);

	Pthread_join(tid1, &status);
	if (status != NULL)
		printf("thread1 status = %p\n", status);

	printf("rw_refcount = %d, rw_nwaitreaders = %d, rw_nwaitwriters = %d\n",
		   rwlock.rw_refcount, rwlock.rw_nwaitreaders,
		   rwlock.rw_nwaitwriters);
	Pthread_rwlock_destroy(&rwlock);
	/* 4returns EBUSY error if cancelled thread does not cleanup */

	exit(0);
}

void *
thread1(void *arg)
{
	Pthread_rwlock_rdlock(&rwlock);
	printf("thread1() got a read lock\n");
	sleep(3);		/* let thread2 block in pthread_rwlock_wrlock() */
	pthread_cancel(tid2);
	sleep(3);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

void *
thread2(void *arg)
{
	printf("thread2() trying to obtain a write lock\n");
	Pthread_rwlock_wrlock(&rwlock);

	// followings are never get executed since it gets canceled.
	printf("thread2() got a write lock\n");
	sleep(1);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

When run this, the program is hung. The occurred steps are:

1. the second trys to get write lock and blocks on pthread_cond_wait.
2. the first returns from slepp(3) and cancel the second.

3. when the second is canceled while it is blocked in a condition variable wait, the mutex is
reacquired before calling the first cleanup hander (even if not installed any handers, but the mutex
is still reacquired before the thread is canceled.) Therefore, when the secondis canceled, it holds
the mutex lock for the read-write lock.

4. the first calls pthread_rwlock_unlock, but it blocks forever in its call to pthread_mutex_lock
because the mutex is still locked by the first thread that was canceled. [KT] this means cancel
terminates a thread immediately and do not continue the rest in pthread_rwlock_wrlock. Hence still
locked. 

If remove the call to pthread_rwlock_unlock in the thread1 func, the main will print:

rw_refcount = 1, rw_nwaitreaders = 0, rw_nwaitwriters = 1
pthread_rwlock_destroy error: Device busy

The correctio is simple and this is addition to pthread_rwlock_rdlock:

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		pthread_cleanup_push( rwlock_cancelrdwait, (void*)rw); ~
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelrdwait */
static void
rwlock_cancelrdwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitreaders--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelrdwait */ 

int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		pthread_cleanup_push(rwlock_cancelwrwait, (void *) rw); ~
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelwrwait */
static void
rwlock_cancelwrwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitwriters--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelwrwait */


==============================================================================
*kt_linux_core_230*	sync: file-lock

File locks: File locks are a synchronization method explicitly designed to coordinate the actions of
multiple processes operating [on-the-same-file]. They can also be used to coordinate access to other
shared resources. File locks come in two flavors: read (shared) locks and write (exclusive) locks.
Any number of processes can hold a read lock on the same file (or region of a file). However, when
one process holds a write lock on a file (or file region), other processes are prevented from
holding either read or write locks on that file (or file region). Linux provides file-locking
facilities via the flock() and fcntl() system calls. The flock() system call provides a simple
locking mechanism, allowing processes to place a shared or an exclusive lock on an entire file.
Because of its limited functionality, flock() locking facility is rarely used nowadays. The fcntl()
  system call provides record locking, allowing processes to place multiple read and write locks on
  different regions of the same file.



==============================================================================
*kt_linux_core_240* 	sync: common problems when use threads

the big three of threading problems are deadlock, races and starvation.

The simplest deadlock condition is when there are two threads and thread A can't progress until
thread B finishes, while thread B can't progress until thread A finishes. This is usually because
both need the same two resources to progress, A has one and B has the other. Various symmetry
breaking algorithms can prevent this in the two thread or larger circle cases.

Races happen when one thread changes the state of some resource when another thread is not expecting
it (such as changing the contents of a memory location when another thread is part way through
reading, or writing to that memory). Locking methods are the key here. (Some lock free methods
and containers are also good choices for this. As are atomic operations, or transaction
based operations.)

Starvation happens when a thread needs a resource to proceed, but can't get it. The resource is
constantly tied up by other threads and the one that needs it can't get in. The scheduling algorithm
is the problem when this happens. Look at algorithms that assure access.


==============================================================================
*kt_linux_core_250*	sync: reentrant and thread-safe

{thread-safe}
A function is said to be thread-safe if it can safely be invoked by multiple threads at the same
time; put conversely, if a function is not thread-safe, then we canât call it from one thread while
it is being executed in another thread. The same example in {why-need-sync}.

<solution-one> serialization
There are various methods of rendering a function thread-safe. One way is to associate a mutex with
the function or perhaps with all of the functions in a library, if they all share the same global
variables, lock that mutex when the function is called, and unlock it when the mutex returns. This
approach has the virtue of simplicity. On the other hand, it means that only one thread at a time
can execute the function-we say that access to the function is [serialized]. 

The downside is that if the threads spend a significant amount of time executing this function, then
this serialization results in a [reduce-or-loss-of-concurrency], because the threads of a program
can no longer execute in parallel.

<solution-two> critial-section
A more sophisticated solution is to associate the mutex with a shared variable. We then determine
which parts of the function are critical sections that access the shared variable, and acquire and
release the mutex only during the execution of these critical sections. This allows multiple threads
to execute the function at the same time and to operate in parallel, except when more than one
thread needs to execute a critical section.


{non-thread-safe-functions}
To facilitate the development of threaded applications, all of the functions specified in SUSv3 are
required to be implemented in a thread-safe manner, except those listed in Table 31-1.

In ref-LPI, Table 31-1: Functions that SUSv3 does not require to be thread-safe


{reentrant-functions}
Although the use of critical sections to implement thread safety is a significant improvement over
the use of per-function mutexes, it is still somewhat inefficient because there is a cost to locking
and unlocking a mutex. A reentrant function achieves thread safety without the use of mutexes.

<how>
It does this by avoiding the use of global and static variables. Any information that must be
returned to the caller, or maintained between calls to the function, is stored in buffers allocated
by the caller. {Q} Although use a buffer from a caller, still seems to be a problem.

However, not all functions can be made reentrant. The usual reasons are the following:

o By their nature, some functions must access global data structures. The functions in the malloc
library provide a good example. These functions maintain a global linked list of free blocks on the
heap. The functions of the malloc library are made thread-safe through the use of mutexes.

o Some functions (defined before the invention of threads) have an interface that by definition is
nonreentrant, because they return pointers to storage statically allocated by the function, or they
employ static storage to maintain information between successive calls to the same (or a related)
function.

See {signal-reentrant} for more about reentrant.

[KT] In sum, reentrant is bigger notion than thread-safe.

<r_prefix>
For several of the functions that have nonreentrant interfaces, SUSv3 specifies reentrant
equivalents with names ending with the suffix _r. These functions require the caller to allocate a
buffer whose address is then passed to the function and used to return the result. This allows the
calling thread to use a local (stack) variable for the function result buffer.

For example, glibc provides crypt_r(), gethostbyname_r(), getservbyname_r(), getutent_r(),
getutid_r(), getutline_r(), and ptsname_r(). 

However, a portable application can't rely on these functions being present on other
implementations.


{code-reentrant-function}
The most efficient way of making a function thread-safe is to make it [reentrant]. All new library
functions should be implemented in this way. However, for an existing nonreentrant library function
(one that was perhaps designed before the use of threads became common), this approach usually
requires changing the functionâs interface, which means modifying all of the programs that use the
function. 

<why-thread-specific-data>
Thread-specific data is a technique for making an existing function thread-safe without
changing its interface. A function that uses thread-specific data may be slightly less efficient
than a reentrant function, but allows us to leave the programs that call the function unchanged.

<strerror-manpage>
STRERROR(3)               Linux Programmer's Manual              STRERROR(3)

NAME
       strerror, strerror_r - return string describing error number

SYNOPSIS

       #include <string.h>

       char *strerror(int errnum);

       int strerror_r(int errnum, char *buf, size_t buflen);
                   /* XSI-compliant */

       char *strerror_r(int errnum, char *buf, size_t buflen);
                   /* GNU-specific */

   Feature Test Macro Requirements for glibc (see feature_test_macros(7)):

       The XSI-compliant version of strerror_r() is provided if:
       (_POSIX_C_SOURCE >= 200112L || _XOPEN_SOURCE >= 600) && ! _GNU_SOURCE
       Otherwise, the GNU-specific version is provided.

DESCRIPTION

The strerror() function returns a pointer to a string that describes the error code passed in the
argument errnum, possibly using the LC_MESSAGES part of the current locale to select the appropriate
language. (For example, if errnum is EINVAL, the returned description will "Invalid argument".)
This string must not be modified by the application, but may be modified by a subsequent call to
strerror(). No library function, including perror(3), will modify this string.

The strerror_r() function is similar to strerror(), but is thread safe. This function is available
in two versions: an XSI-compliant version specified in POSIX.1-2001 (available since glibc 2.3.4,
but not POSIX-compliant until glibc 2.13), and a GNU-specific version (available since glibc
2.0). The XSI-compliant version is provided with the feature test macros settings shown in
the SYNOPSIS; otherwise the GNU-specific version is provided. If no feature test macros are
explicitly defined, then (since glibc 2.4) _POSIX_SOURCE is defined by default with the value
200112L, so that the XSI- compliant version of strerror_r() is provided by default.

The XSI-compliant strerror_r() is preferred for portable applications. It returns the error string
in the user-supplied buffer buf of length buflen.

The GNU-specific strerror_r() returns a pointer to a string containing the error message. This may
be either a pointer to a string that the function stores in buf, or a pointer to some (immutable)
static string (in which case buf is unused). If the function stores a string in buf, then at most
buflen bytes are stored (the string may be truncated if buflen is too small and errnum is
unknown). The string always includes a terminating null byte ('\0').

RETURN VALUE

The strerror() and the GNU-specific strerror_r() functions return the appropriate error description
string, or an "Unknown error nnn" message if the error number is unknown.

POSIX.1-2001 and POSIX.1-2008 require that a successful call to strerror() shall leave errno
unchanged, and note that, since no function return value is reserved to indicate an error, an
application that wishes to check for errors should initialize errno to zero before the call, and
then check errno after the call.

The XSI-compliant strerror_r() function returns 0 on success. On error, a (positive) error number is
returned (since glibc 2.13), or -1 is returned and errno is set to indicate the error (glibc
versions before 2.13).

ERRORS

EINVAL The value of errnum is not a valid error number.
ERANGE Insufficient storage was supplied to contain the error description string.

ATTRIBUTES

Multithreading (see pthreads(7))
The strerror() function is not thread-safe.
The strerror_r() function is thread-safe.

CONFORMING TO

strerror() is specified by POSIX.1-2001, C89, C99.  strerror_r() is specified by POSIX.1-2001.

The GNU-specific strerror_r() function is a nonstandard extension.

POSIX.1-2001 permits strerror() to set errno if the call encounters an error, but does not specify
what value should be returned as the function result in the event of an error. On some systems,
strerror() returns NULL if the error number is unknown.  On other systems, strerror() returns a
string something like "Error nnn occurred" and sets errno to EINVAL if the error number is
unknown. C99 and POSIX.1-2008 require the return value to be non-NULL.

SEE ALSO
err(3), errno(3), error(3), perror(3), strsignal(3)

COLOPHON

This page is part of release 3.61 of the Linux man-pages project.  A description of the project, and
information about reporting bugs, can be found at http://www.kernel.org/doc/man-pages/.

2013-06-21                      STRERROR(3)

<POSIX-1-2001>
Beginning in 1999, the IEEE, The Open Group, and the ISO/IEC Joint Technical Committee 1
collaborated in the Austin Common Standards Revision Group with the aim of revising and
consolidating the POSIX standards and the Single UNIX Specification. This resulted in the
ratification of POSIX 1003.1-2001, sometimes just called POSIX.1-2001, in December 2001
(subsequently approved as an ISO standard, ISO/IEC 9945:2002).

<thread-specific-data>
1. The function creates a key, which is the means of differentiating the thread-specific data item
used by this function from the thread-specific data items used by other functions. The key is
created by calling the pthread_key_create() function. Creating a key needs to be done only once,
when the first thread calls the function. For this purpose, pthread_once() is employed.
Creating a key doesnât allocate any blocks of thread-specific data.

2. The call to pthread_key_create() serves a second purpose: it allows the caller to specify the
address of the programmer-defined destructor function that is used to deallocate each of the storage
blocks allocated for this key (see the next step). When a thread that has thread-specific data
terminates, the Pthreads API automatically invokes the destructor, passing it a pointer to the data
block for this thread.

<typical-thread-specific-data-implementation>
This is a typical implementation (NPTL is typical). In this implementation, the pthread_key_t value
returned by pthread_key_create() is simply an index into the global array, which we label
pthread_keys

                  -----------------
pthread_keys[0]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
pthread_keys[1]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
pthread_keys[2]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
                  ...

int pthread_setspecific(pthread_key_t key, const void *value);

So key is allocated for each client which is a library function, strerror, in this case and assumes
that this is keys[1]. The data structure for value or buffer for each thread is:

All correspond to                   thread A
pthread_keys[1]    value of key1    tsd[0] | pointer |
                   for thread A  -> tsd[1] | pointer |  -> TSD buffer for strerror in thread A
                                    tsd[2] | pointer |                                         

                                    thread B
                   value of key1    tsd[0] | pointer |
                   for thread B  -> tsd[1] | pointer |  -> TSD buffer for strerror in thread B
                                    tsd[2] | pointer |                                        

                   ...

The TDS buffer is allocated in strerror for each thread and when each thread calls 

void *pthread_getspecific(pthread_key_t key);

The pthread library know the thread and use the key to pick up the entry and the buffer pointer.
When a thread is first created, all of its thread-specific data pointers are initialized to NULL.
This means that when our library function is called by a thread for the first time, it must begin by
using pthread_getspecific() to check whether the thread already has an associated value for key.

[KT] This means that when pthread_key_create is called, it allocates a entry in keys array and also
TSD array for each thread even if some thread is not using strerror.

<strerror-non-thread-safe-version>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#define MAX_ERROR_LEN 256 /* Maximum length of string
                             returned by strerror() */
static char buf[MAX_ERROR_LEN]; /* Statically allocated return buffer */
  char *
strerror(int err)
{
  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1);
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }
  return buf;
}

<user-code>
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#include "tlpi_hdr.h"

static void *
threadFunc(void *arg)
{
  char *str;
  printf("Other thread about to call strerror()\n");
  str = strerror(EPERM);
  printf("Other thread: str (%p) = %s\n", str, str);
  return NULL;
}

int
main(int argc, char *argv[])
{
  pthread_t t;
  int s;
  char *str;
  str = strerror(EINVAL);
  printf("Main thread has called strerror()\n");

  s = pthread_create(&t, NULL, threadFunc, NULL);
  if (s != 0)
    errExitEN(s, "pthread_create");

  s = pthread_join(t, NULL);
  if (s != 0)
    errExitEN(s, "pthread_join");

  printf("Main thread: str (%p) = %s\n", str, str);

  exit(EXIT_SUCCESS);
}

<strerror-thread-safe-version>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#include "tlpi_hdr.h"

static pthread_once_t once = PTHREAD_ONCE_INIT;
static pthread_key_t strerrorKey;

#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread
                             buffer returned by strerror() */

static void /* Free thread-specific data buffer */
q destructor(void *buf)
{
  free(buf);
}

static void /* One-time key creation function */
createKey(void) <2>
{
  int s;
  /* Allocate a unique thread-specific data key and save the address
     of the destructor for thread-specific data buffers */
  s = pthread_key_create(&strerrorKey, destructor); <1>
  if (s != 0)
    errExitEN(s, "pthread_key_create");
}

char *
strerror(int err)
{
  int s;
  char *buf;

  /* Make first caller allocate key for thread-specific data */
  s = pthread_once(&once, createKey); <2>
  if (s != 0)
    errExitEN(s, "pthread_once");

  buf = pthread_getspecific(strerrorKey); <3>
  if (buf == NULL) { /* If first call from this thread, allocate
                        buffer for thread, and save its location */
    buf = malloc(MAX_ERROR_LEN);
    if (buf == NULL)
      errExit("malloc");

    s = pthread_setspecific(strerrorKey, buf); <4>
    if (s != 0)
      errExitEN(s, "pthread_setspecific");
  }

  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); <4>
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }

  return buf;
}

<limitation>
SUSv3 requires that an implementation support at least 128 (_POSIX_THREAD_KEYS_MAX) keys. An
application can determine how many keys an implementation actually supports either via the
definition of PTHREAD_KEYS_MAX (defined in <limits.h>) or by calling sysconf(_SC_THREAD_KEYS_MAX).
Linux supports up to 1024 keys.

Even 128 keys should be more than sufficient for most applications. This is because each library
function should employ only a small number of keysâoften just one. If a function requires multiple
thread-specific data values, these can usually be placed in a single structure that has just one
associated thread-specific data key.


{thread-local-storage}
Like thread-specific data, thread-local storage provides persistent per-thread storage. This
feature is [nonstandard], but it is provided in the same or a similar form on many other UNIX
implementations.

Thread-local storage requires support from the kernel (provided in Linux 2.6), the Pthreads
implementation (provided in NPTL), and the C compiler (provided on x86-32 with gcc 3.3 and later).

The main advantage of thread-local storage is that it is much simpler to use than thread-specific
data. To create a thread-local variable, we simply include the __thread specifier in the declaration
of a global or static variable:

static __thread buf[MAX_ERROR_LEN];

Each thread has its own copy of the variables declared with this specifier. The variables
in a threadâs thread-local storage persist until the thread terminates, at which
time the storage is automatically deallocated.

The following points about the declaration and use of thread-local variables:

o The __thread keyword must immediately follow the static or extern keyword, if either of these is
specified in the variableâs declaration.

o The declaration of a thread-local variable can include an initializer, in the same manner as a
normal global or static variable declaration.

o The C address (&) operator can be used to obtain the address of a thread-local variable.

<revised-strerror-using-tls>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread
                             buffer returned by strerror() */
static __thread char buf[MAX_ERROR_LEN];
/* Thread-local return buffer */
char *
strerror(int err)
{
  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1);
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }
  return buf;
}


==============================================================================
*kt_linux_core_260*	sync: atomic operations

{atomic-operations}

For full articles:
http://www.informit.com/guides/content.aspx?g=cplusplus&seqNum=469

Atomicity

An atomic operation is a sequence of one or more machine instructions that are executed
sequentially, without interruption. By default, any sequence of two or more machine instructions
isn't atomic since the operating system may suspend the execution of the current sequence of
operations in favor of another task. If you want to ensure that a sequence of operations is atomic
you must use some form of locking or other types of synchronization. 

Without that, the only guarantee you have is that a single machine instruction is always atomic. the
CPU will not interrupt a single instruction in the middle. [KT] Not entirely true. 

We can conclude from that minimal guarantee that if you can prove that your compiler translates a
certain C++ statement into a single machine instruction, that C++ statement is naturally atomic
meaning, the programmer doesn't have to use explicit locking to enforce the atomic execution of that
statement.  

Which C++ Statements are Naturally Atomic?

Obviously, there are very few universal rules of thumb because each hardware architecture might
translate the same C++ statement differently. Many textbooks tell you that the unary ++ and --
operators, when applied to integral types and pointers, are guaranteed to be atomic. Historically,
when Dennis Ritchie and Brian Kernighan designed C, they added these operators to the language
because they wanted to take advantage of the fast INC (increment) assembly directive that many
machines supported. However, there is no guarantee in the C or C++ standards that these operators
shall be atomic. Ritchie and Kernighan were more concerned about speed rather than atomicity.

You shouldn't make assumptions about the atomicity of C++ statements without examining the output of
your compiler. In some cases, you might discover that what appears to be a single C++ statement is
in fact translated into a long and complex set of machine instructions. 


Epilog

The multithreading support of C++0x consists of a thread class as well as a standard atomics library
that guarantees the atomicity of logical and arithmetic operations. I will introduce the C++0x
atomics library in a separate column.


From C++11:

Data-dependency ordering: atomics and memory model 	N2664 	GCC 4.4
(memory_order_consume)


From StackOverflow:

The increment-memory machine instruction on an X86 is atomic only if you use it with a LOCK prefix.

x++ in C and C++ doesn't have atomic behavior. If you do unlocked increments, due to races in which
processor is reading and writing X, if two separate processors attempt an increment, you can end up
with just one increment or both being seen (the second processor may have read the initial value,
incremented it, and written it back after the first writes its results back).

I believe that C++11 offers atomic increments, and most vendor compilers have an idiomatic way to
cause an atomic increment of certain built-in integer types (typically int and long); see your
compiler reference manual.

If you want to increment a "large value" (say, a multiprecision integer), you need to do so with
using some standard locking mechanism such as a semaphore.

Note that you need to worry about atomic reads, too. On the x86, reading a 32 or 64 bit value
happens to be atomic if it is 64-bit word aligned. That won't be true of a "large value"; again
you'll need some standard lock.

{rmw-operations}
The RMW(read-modify-write) is:

<from-wikipedia>
A class of atomic operations such as test-and-set, fetch-and-add, and compare-and-swap which both
read a memory location and write a new value into it simultaneously, either with a completely new
value or some function of the previous value. These operations prevent race conditions in
multi-threaded applications. Typically they are used to implement mutexes or semaphores. These
atomic operations are also heavily used in non-blocking synchronization.

{atomic-non-atomic-operations}
http://preshing.com/20130618/atomic-vs-non-atomic-operations/

Much has already been written about atomic operations on the web, usually with a focus on atomic
read-modify-write (RMW) operations. However, those arenât the only kinds of atomic operations. There
are also atomic loads and stores, which are equally important. In this post, Iâll compare atomic
loads and stores to their non-atomic counterparts at both the processor level and the C/C++ language
level. Along the way, weâll clarify the C++11 concept of a âdata raceâ.

Automic operations: automic loads and stores + automic read-modify-write operations

An operation acting on shared memory is atomic if it completes in a single step relative to other
threads. When an atomic store is performed on a shared variable, no other thread can observe the
modification half-complete. When an atomic load is performed on a shared variable, it reads the
entire value as it appeared at a single moment in time. Non-atomic loads and stores do not make
those guarantees.

Without those guarantees, lock-free programming would be impossible, since you could never let
different threads manipulate a shared variable at the same time. We can formulate it as a rule:

Any time two threads operate on a shared variable concurrently, and one of those operations performs
a write, both threads must use atomic operations.

If you violate this rule, and either thread uses a non-atomic operation, youâll have what the C++11
standard refers to as a [data-race] (not to be confused with Javaâs concept of a data race, which is
different, or the more general race condition). [Q] what is the general race condition?

The C++11 standard doesnât tell you why data races are bad; only that if you have one, âundefined
behaviorâ will result (section 1.10.21). The real reason why such data races are bad is actually
quite simple: They result in [torn-reads] and [torn-writes].

A memory operation can be non-atomic because it uses multiple CPU instructions, non-atomic even when
using a single CPU instruction, or non-atomic because youâre writing portable code and you simply
canât make the assumption. Letâs look at a few examples.


<Non-Atomic Due to Multiple CPU Instructions>

Suppose you have a 64-bit global variable, initially zero.

uint64_t sharedValue = 0;

At some point, you assign a 64-bit value to this variable.

void storeValue()
{
    sharedValue = 0x100000002;
}

When you compile this function for 32-bit x86 using GCC, it generates the following machine code.

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	DWORD PTR sharedValue, 2
        mov	DWORD PTR sharedValue+4, 1
        ret
        ...

As you can see, the compiler implemented the 64-bit assignment using two separate machine
instructions. The first instruction sets the lower 32 bits to 0x00000002, and the second sets the
upper 32 bits to 0x00000001. Clearly, this assignment operation is not atomic. If sharedValue is
accessed concurrently by different threads, several things can now go wrong:

- If a thread calling storeValue is preempted between the two machine instructions, it will leave
the value of 0x0000000000000002 in memory â a torn write. At this point, if another thread reads
sharedValue, it will receive this completely bogus value which nobody intended to store.

- Even worse, if a thread is preempted between the two instructions, and another thread modifies
sharedValue before the first thread resumes, it will result in a permanently torn write: the upper
32 bits from one thread, the lower 32 bits from another.

- On multicore devices, it isnât even necessary to preempt one of the threads to have a torn write.
When a thread calls storeValue, any thread executing on a different core could read sharedValue at a
moment when only half the change is visible.

Reading concurrently from sharedValue brings its own set of problems:

uint64_t loadValue()
{
    return sharedValue;
}

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	eax, DWORD PTR sharedValue
        mov	edx, DWORD PTR sharedValue+4
        ret
        ...

Here too, the compiler has implemented the load operation using two machine instructions: The first
reads the lower 32 bits into eax, and the second reads the upper 32 bits into edx. In this case, if
a concurrent store to sharedValue becomes visible between the two instructions, it will result in a
torn read â even if the concurrent store was atomic.

These problems are not just theoretical. Mintomicâs test suite includes a test case called
test_load_store_64_fail, in which one thread stores a bunch of 64-bit values to a single variable
using a plain assignment operator, while another thread repeatedly performs a plain load from the
same variable, validating each result. On a multicore x86, this test fails consistently, as
expected.


<Non-Atomic in a single CPU Instructions>

A memory operation can be non-atomic even when performed by a single CPU instruction. For example,
  the ARMv7 instruction set includes the strd instruction, which stores the contents of two 32-bit
  source registers to a single 64-bit value in memory.

strd r0, r1, [r2]

On some ARMv7 processors, this instruction is not atomic. When the processor sees this instruction,
	it actually performs [two-separate-32-bit-stores] under the hood (section A3.5.3). Once again,
	another thread running on a separate core has the possibility of observing a torn write.
	Interestingly, a torn write is even possible on a single-core device: A system interrupt â say,
	for a scheduled thread context switch â can actually occur between the two internal 32-bit
	stores! In this case, when the thread resumes from the interrupt, it will restart the strd
	instruction all over again.

As another example, itâs well-known that on x86, a 32-bit mov instruction is atomic if the memory
operand is naturally aligned, but non-atomic otherwise. In other words, atomicity is [only]
guaranteed when the 32-bit integer is located at an address which is an exact multiple of 4.


<All C/C++ Operations Are Presumed Non-Atomic>

In C and C++, every operation is presumed non-atomic unless otherwise specified by the compiler or
hardware vendor â even plain 32-bit integer assignment.

uint32_t foo = 0;

void storeFoo()
{
    foo = 0x80286;
}

The language standards have nothing to say about atomicity in this case. Maybe integer assignment is
atomic, maybe it isnât. Since non-atomic operations donât make any guarantees, plain integer
assignment in C is non-atomic by definition.

In practice, we usually know more about our target platforms than that. For example, itâs common
knowledge that on all modern x86, x64, Itanium, SPARC, ARM and PowerPC processors, plain 32-bit
integer assignment is atomic [as-long-as] the target variable is naturally aligned. You can verify it
by consulting your processor manual and/or compiler documentation. In the games industry, I can tell
you that a lot of 32-bit integer assignments rely on this particular guarantee.

Nonetheless, when writing truly portable C and C++, thereâs a long-standing tradition of pretending
that we donât know anything more than what the language standards tell us. Portable C and C++ is
designed to run on every possible computing device past, present and imaginary. Personally, I like
to imagine a machine where memory can only be changed by mixing it up first:

On such a machine, you definitely wouldnât want to perform a concurrent read at the same time as a
plain assignment; you could end up reading a completely random value.

<CPP11>
In C++11, there is finally a way to perform truly portable atomic loads and stores: the C++11 atomic
library. Atomic loads and stores performed using the C++11 atomic library would even work on the
imaginary computer above - even if it means the C++11 atomic library must secretly [lock] a mutex to
make each operation atomic. Thereâs also the Mintomic library which I released last month, which
doesnât support as many platforms, but works on several older compilers, is hand-optimized and is
guaranteed to be lock-free.


Relaxed Atomic Operations

Letâs return to the original sharedValue example from earlier in this post. Weâll rewrite it using
Mintomic so that all operations are performed atomically on every platform Mintomic supports. First,
we must declare sharedValue as one of Mintomicâs atomic data types.

#include <mintomic/mintomic.h>

mint_atomic64_t sharedValue = { 0 };

The mint_atomic64_t type guarantees correct memory alignment for atomic access on each platform.
This is important because, for example, the GCC 4.2 compiler for ARM bundled with Xcode 3.2.5
doesnât guarantee that plain uint64_t will be 8-byte aligned.

In storeValue, instead of performing a plain, non-atomic assignment, we must call
mint_store_64_relaxed.

void storeValue()
{
    mint_store_64_relaxed(&sharedValue, 0x100000002);
}

Similarly, in loadValue, we call mint_load_64_relaxed.

uint64_t loadValue()
{
    return mint_load_64_relaxed(&sharedValue);
}

Using C++11âs terminology, these functions are now data race-free. When executing concurrently,
		there is absolutely no possibility of a torn read or write, whether the code runs on
		ARMv6/ARMv7 (Thumb or ARM mode), x86, x64 or PowerPC. If youâre curious how
		mint_load_64_relaxed and mint_store_64_relaxed actually work, both functions expand to an
		inline cmpxchg8b instruction on x86; for other platforms, consult Mintomicâs implementation.

Hereâs the exact same thing written in C++11 instead:

#include <atomic>

std::atomic<uint64_t> sharedValue(0);

void storeValue()
{
    sharedValue.store(0x100000002, std::memory_order_relaxed);
}

uint64_t loadValue()
{
    return sharedValue.load(std::memory_order_relaxed);
}

Youâll notice that both the Mintomic and C++11 examples use relaxed atomics, as evidenced by the
_relaxed suffix on various identifiers. The _relaxed suffix is a reminder that, just as with plain
loads and stores, no guarantees are made about memory ordering.

The only difference between a relaxed atomic load (or store) and a non-atomic load (or store) is
that relaxed atomics guarantee atomicity. No other difference is guaranteed.

In particular, it is still legal for the memory effects of a relaxed atomic operation to be
reordered with respect to instructions which follow or precede it in program order, either due to
compiler reordering or memory reordering on the processor itself. The compiler could even perform
optimizations on redundant relaxed atomic operations, just as with non-atomic operations. In all
cases, the operation remains atomic.

When manipulating shared memory concurrently, I think itâs good practice to always use Mintomic or
C++11 atomic library functions, even in cases where you know that a plain load or store would
already be atomic on your target platform. An atomic library function serves as a reminder that
elsewhere, the variable is the target of concurrent data access.

Hopefully, itâs now a bit more clear why the Worldâs Simplest Lock-Free Hash Table uses Mintomic
library functions to manipulate shared memory concurrently from different threads.


{lock-free-programming}
http://preshing.com/20120612/an-introduction-to-lock-free-programming/

An Introduction to Lock-Free Programming

Lock-free programming is a challenge, not just because of the complexity of the task itself, but
because of how difficult it can be to penetrate the subject in the first place.

I was fortunate in that my first introduction to lock-free (also known as lockless) programming was
Bruce Dawsonâs excellent and comprehensive white paper, Lockless Programming Considerations. And
like many, Iâve had the occasion to put Bruceâs advice into practice developing and debugging
lock-free code on platforms such as the Xbox 360.

Since then, a lot of good material has been written, ranging from abstract theory and proofs of
correctness to practical examples and hardware details. Iâll leave a list of references in the
footnotes. At times, the information in one source may appear orthogonal to other sources: For
instance, some material assumes sequential consistency, and thus sidesteps the memory ordering
issues which typically plague lock-free C/C++ code. The new C++11 atomic library standard throws
another wrench into the works, challenging the way many of us express lock-free algorithms.

In this post, Iâd like to re-introduce lock-free programming, first by defining it, then by
distilling most of the information down to a few key concepts. Iâll show how those concepts relate
to one another using flowcharts, then weâll dip our toes into the details a little bit. At a
minimum, any programmer who dives into lock-free programming should already understand how to write
correct multithreaded code using mutexes, and other high-level synchronization objects such as
semaphores and events.  

What Is It?

People often describe lock-free programming as programming without mutexes, which are also referred
to as [locks]. Thatâs true, but itâs only [part] of the story. The generally accepted definition, based
on academic literature, is a bit more broad. At its essence, lock-free is a property used to
describe some code, without saying too much about how that code was actually written.

Basically, if some part of your program satisfies the following conditions, then that part can
rightfully be considered lock-free. Conversely, if a given part of your code doesnât satisfy these
conditions, then that part is not lock-free.

<definition>
(This was a flow chart)

Are you programming with multiple threads? or interrupt, signal handlers, etc?
-> Yes

Do the threads access shared memeory?
-> Yes

Can the threads block each other? ie. is there some way to schedule the threads which would
'lock-up' indefinitely?
-> No

It is lock-free programming.

In this sense, the lock in lock-free [does not refer directly to mutexes], but rather to the
possibility of âlocking upâ the entire application in some way, whether itâs deadlock, livelock â or
even due to hypothetical thread scheduling decisions made by your worst enemy. That last point
sounds funny, but itâs key. Shared mutexes are ruled out trivially, because as soon as one thread
obtains the mutex, your worst enemy could simply never schedule that thread again. Of course, real
operating systems donât work that way - weâre merely defining terms.

Hereâs a simple example of an operation which contains no mutexes, but is still not lock-free.
Initially, X = 0. As an exercise for the reader, consider how two threads could be scheduled in a
way such that neither thread exits the loop.

while (X == 0)
{
    X = 1 - X;
}

Nobody expects a large application to be entirely lock-free. Typically, we identify a [specific-set]
of lock-free operations out of the whole codebase. For example, in a lock-free queue, there might be
a handful of lock-free operations such as push, pop, perhaps isEmpty, and so on.

Herlihy & Shavit, authors of The Art of Multiprocessor Programming, tend to express such operations
as class methods, and offer the following succinct definition of lock-free (see slide 150): 
	
"In an infinite execution, infinitely often some method call finishes." In other words, as long as
the program is able to keep calling those lock-free operations, the number of completed calls keeps
increasing, no matter what. It is algorithmically impossible for the system to lock up during those
operations. {Q} Didn't get that.

<why-use>
One important consequence of lock-free programming is that if you suspend a single thread, it will
never prevent other threads from making progress, as a group, through their own lock-free
operations. This hints at the value of lock-free programming when writing interrupt handlers and
real-time systems, where certain tasks must complete within a certain time limit, no matter what
state the rest of the program is in.

A final precision: Operations that are designed to block do not disqualify the algorithm. For
example, a queueâs pop operation may intentionally block when the queue is empty. The remaining
codepaths can still be considered lock-free.

<TODO> there is more in this page.


==============================================================================
*kt_linux_core_261*	sync: ref: locks aren't slow; lock contention is

<contention-and-frequency>
For example, this post measures the performance of a lock under heavy conditions: each thread must
hold the lock to do any work (high contention), and the lock is held for an extremely short interval
of time (high frequency)

But donât disregard locks yet. One good example of a place where locks perform admirably, in real
software, is when protecting the memory allocator. Doug Leaâs Malloc is a popular memory allocator
in video game development, but itâs single threaded, so we need to protect it using a lock. During
gameplay, itâs not uncommon to see multiple threads hammering the memory allocator, say around 15000
times per second. While loading, this figure can climb to 100000 times per second or more. Itâs not
a big problem, though. As youâll see, locks handle the workload like a champ.
{Q} what's this memory allocator?

Lock Contention Benchmark

In this test, we spawn a thread which generates random numbers, using a custom Mersenne Twister
implementation. For example, suppose we want to acquire the lock 15000 times per second, and keep it
held 50% of the time. (whole working time) 

This is code example to show 50% lock:

QueryPerformanceCounter(&start);
for (;;)
{
  // Do some work without holding the lock
  workunits = (int) (random.poissonInterval(averageUnlockedCount) + 0.5f);
  for (int i = 1; i < workunits; i++)
    random.integer();       // Do one work unit
  workDone += workunits;

  QueryPerformanceCounter(&end);
  elapsedTime = (end.QuadPart - start.QuadPart) * ooFreq;
  if (elapsedTime >= timeLimit)
    break;

  // Do some work while holding the lock
  EnterCriticalSection(&criticalSection);
  workunits = (int) (random.poissonInterval(averageLockedCount) + 0.5f);
  for (int i = 1; i < workunits; i++)
    random.integer();       // Do one work unit
  workDone += workunits;
  LeaveCriticalSection(&criticalSection);

  QueryPerformanceCounter(&end);
  elapsedTime = (end.QuadPart - start.QuadPart) * ooFreq;
  if (elapsedTime >= timeLimit)
    break;
}

Now suppose we launch two such threads, each running on a different core. Each thread will hold the
lock during 50% of the time when it can perform work, but if one thread tries to acquire the lock
while the other thread is holding it, it will be forced to wait. This is known as lock contention.

(Tested on dual core) When we run the above scenario, we find that each thread spends roughly 25% of
its time waiting, and 75% of its time doing actual work. Together, both threads achieve a net
performance of 1.5x compared to the single-threaded case.

I ran several variations of the test on a 2.66 GHz quad-core Xeon, from 1 thread, 2 threads, all the
way up to 4 threads, each running on its own core. I also varied the duration of the lock, from the
trivial case where the the lock is never held, all the way up to the maximum where each thread must
hold the lock for 100% of its workload. In all cases, the lock frequency remained constant â threads
acquired the lock 15000 times for each second of work performed.

<KT>
The graph shows that go up to 4x when 0% lock duration and down below 1x when 100% lock duration. 0%
means that there is no sharing between threads hence no lock is needed.

The results were interesting. For short lock durations, up to say 10%, the system achieved very high
parallelism. Not perfect parallelism, but close. Locks are fast!

To put the results in perspective, I analyzed the memory allocator lock in a multithreaded game
engine using this profiler. During gameplay, with 15000 locks per second coming from 3 threads, the
lock duration was in the neighborhood of just 2%. Thatâs well within the comfort zone on the left
side of the diagram.

These results also show that once the lock duration passes 90%, thereâs no point using multiple
threads anymore. A single thread performs better. Most surprising is the way the performance of 4
threads drops off a cliff around the 60% mark! This looked like an anomaly, so I re-ran the tests
several additional times, even trying a different testing order. The same behavior happened
consistently. My best hypothesis is that the experiment hits some kind of snag in the Windows
scheduler, but I didnât investigate further.

Lock Frequency Benchmark

Even a lightweight mutex has overhead. As my next post shows, a pair of lock/unlock operations on a
Windows Critical Section takes about 23.5 ns on the CPU used in these tests. Therefore, 15000 locks
per second is low enough that lock overhead does not significantly impact the results. But what
happens as we turn up the dial on lock frequency?

The algorithm offers very fine control over the amount of work performed between one lock and the
next, so I performed a new batch of tests using smaller amounts: from a very fine-grained 10 ns
between locks, all the way up to 31 Î¼s, which corresponds to roughly 32000 acquires per second. Each
test used exactly two threads:

As you might expect, for very high lock frequencies, the overhead of the lock itself begins to dwarf
the actual work being done. Several benchmarks youâll find online, including the one linked earlier
fall into the bottom-right corner of this chart. At such frequencies, youâre talking about some
seriously short lock times â on the scale of a few CPU instructions. The good news is that, when the
work between locks is that simple, a lock-free implementation is more likely to be feasible.

At the same time, the results show that locking up to 320000 times per second (3.1 Î¼s between
    successive locks) is not unreasonable. In game development, the memory allocator may flirt with
this frequency during load times. You can still achieve more than 1.5x parallelism if the lock
duration is short.

Weâve now seen a wide spectrum of lock performance: cases where it performs great, and cases where
the application slows to a crawl. Iâve argued that the lock around the memory allocator in a game
engine will often achieve excellent performance. Given this example from the real world, it cannot
be said that all locks are slow. Admittedly, itâs very easy to abuse locks, but one shouldnât live
in too much fear â any resulting bottlenecks will show up during careful profiling. When you
consider how reliable locks are, and the relative ease of understanding them (compared to lock-free
    techniques), locks are actually pretty awesome sometimes.

The goal of this post was to give locks a little respect where deserved - corrections are welcome. I
also realize that locks are used in a wide variety of industries and applications, and it may not
always be so easy to strike a good balance in lock performance. If youâve found that to be the case
in your own experience, I would love to hear from you in the comments.

{DN}
Then the bottomlien is that try to minimize the lock contention rather than try to find out which
one is fast. Of course, should use better one but the lock contention is far more important.


==============================================================================
*kt_linux_core_262*	sync: ref: always use a lightweight mutex {mutex-vs-semaphore}

http://preshing.com/20111124/always-use-a-lightweight-mutex/
Always Use a Lightweight Mutex

Have started wondering since seen this article which says that critical section is faster than mutex
in windows. This critical section is called as lightweight mutex which is equivalent to pthread
mutex in Linux. Does it mean that pthread mutex is faster than posix semaphore? Tried the same
approach that this article had and it seems not. Like ref-LPI, there is no big difference.

This is the result ran on the real Linux server machine which has multiple processors and as can
see, semaphore is slightly slower than using a mutex but not significant. This seems different from
what this article said.

<snippet>
Now, suppose you have a thread which acquires a Critical Section 100000 times per second, and there
are no other threads competing for the lock. Based on the above figures, you can expect to pay
between 0.2% and 0.6% in lock overhead. Not too bad! At lower frequencies, the overhead becomes
negligible.

<KT> Here shows the result in the graphic which is 58.7ns on Core 2 Duo and 23.5ns on Xeon for
windows critical section. Think that Core 2 means dual core and Xeon is single so means that there
are lock contention for Core 2 and not for Xeon. That's why he mean that there is 0.6% overhead for
locking. Then it's more about experimenting of mutl-core.

Naturally, Ubuntu 11.10 provides a lock implementation using the POSIX Threads API as well. Itâs
another lightweight mutex, based on a Linux-specific construct known as a futex. A pair of
pthread_mutex_lock/pthread_mutex_unlock calls takes about 66 ns on my Core 2 Duo. You can even share
this implementation between processes, but I didnât test that.

In my previous post, I argued against the misconception that locks are slow and provided some data
to support the argument. At this point, it should be clear that if you arenât using a lightweight
mutex, the entire argument goes out the window. Iâm fairly sure that the existence of heavy lock
implementations has only added to this misconception over the years.

Some of you old-timers may point out ancient platforms where a heavy lock was the only
implementation available, or when a semaphore had to be used for the job. But it seems all modern
platforms offer a lightweight mutex. And even if they didnât, you could write your own lightweight
mutex at the application level, even sharing it between processes, provided youâre willing to live
with certain caveats. Youâll find one example in my followup post, Roll Your Own Lightweight Mutex.


{when-with-no-threads}
keitee.park@magnum ~
$ ./sem
sem run
56:645
56:702   [57]
keitee.park@magnum ~
$ ./mtx
mtx run
3:710
3:773    [63]

keitee.park@magnum ~
$ ./sem
sem run
5:663
5:719    [56]
keitee.park@magnum ~
$ ./mtx
mtx run
6:894
6:957    [63]

keitee.park@magnum ~
$ ./sem
sem run
8:103
8:160    [57]
keitee.park@magnum ~
$ ./mtx
mtx run
9:294
9:356    [62]

<for-semaphore-example>
#include <stdio.h>
#include <time.h>
#include <fcntl.h>
#include <semaphore.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

  unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

int main()
{
  int s = 0, loop = 0;
  int flags, opt;
  mode_t perms;
  unsigned int value;
  sem_t *sem;

  flags = O_CREAT | O_EXCL;
  perms = S_IRUSR | S_IWUSR;

  fprintf( stderr, "sem run\n");

  // will create /dev/shm/sem.demo
  sem = sem_open("/demo", flags, perms, 1 );
  if(sem == SEM_FAILED)
    fprintf( stderr, "fail on sem_open");

  get_time_ms();

  for(loop = 0; loop < MAXLOOP; loop++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "fail on sem_wait");

    s = sem_post(sem);
    if (s != 0)
    fprintf(stderr, "fail on sem_post");
  }

  get_time_ms();

  s = sem_unlink("/demo");
  if(s != 0)
    fprintf(stderr, "fail on sem_unlink\n" );

}

<for-mutex-example>
#include <stdio.h>
#include <time.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

  unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;

int main()
{
  int s = 0, loop = 0;

  fprintf( stderr, "mtx run\n");

  get_time_ms();

  for(loop = 0; loop < MAXLOOP; loop++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "pthread_mutex_unlock");
  }

  get_time_ms();
}


{when-with-two-threads}
keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
14:946
15:370      [424]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
24:498
24:829      [331]
main: this is the end

keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
33:705
34:207      [502]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
35:993
36:460      [467]
main: this is the end

keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
2:673
3:125       [452]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
4:792
5:224       [432]
main: this is the end

<for-semaphore-example>
#include <stdio.h>
#include <time.h>
#include <fcntl.h>
#include <semaphore.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

	unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static sem_t *sem;
static unsigned int loopA;
static unsigned int loopB;

static void* threadFuncA( void *arg )
{
  int s = 0;

  for(loopA = 0; loopA < MAXLOOP; loopA++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "TA: fail on sem_wait\n");

    s = sem_post(sem);
    if (s != 0)
    fprintf( stderr, "TA: fail on sem_post\n");
  }
}

static void* threadFuncB( void *arg )
{
  int s = 0;

  for(loopB = 0; loopB < MAXLOOP; loopB++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "TB: fail on sem_wait\n");

    s = sem_post(sem);
    if (s != 0)
    fprintf( stderr, "TB: fail on sem_post\n");
  }
}

int main()
{
  int flags, opt;
  mode_t perms;
  pthread_t tA, tB;
  int s;

  flags = O_CREAT | O_EXCL;
  perms = S_IRUSR | S_IWUSR;

  // will create /dev/shm/sem.demo
  sem = sem_open("/demo", flags, perms, 1 );
  if(sem == SEM_FAILED)
    fprintf( stderr, "main: fail on sem_open\n");

  fprintf( stderr, "main: this is the second sem run\n");
  get_time_ms();

  s = pthread_create( &tA, NULL, threadFuncA, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(A)\n");

  s = pthread_create( &tB, NULL, threadFuncB, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(B)\n");

  s = pthread_join( tA, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(A)\n");

  s = pthread_join( tB, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(B)\n");

  get_time_ms();
  fprintf( stderr, "main: this is the end\n");

  s = sem_unlink("/demo");
  if(s != 0)
    fprintf(stderr, "main: fail on sem_unlink\n" );
}

<for-mutex-example>
#include <stdio.h>
#include <time.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

	unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;
static unsigned int loopA;
static unsigned int loopB;

static void* threadFuncA( void *arg )
{
  int s = 0;

  for(loopA = 0; loopA < MAXLOOP; loopA++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "TA: pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "TA: pthread_mutex_unlock");
  }
}

static void* threadFuncB( void *arg )
{
  int s = 0;

  for(loopB = 0; loopB < MAXLOOP; loopB++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "TB: pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "TB: pthread_mutex_unlock");
  }
}

int main()
{
  pthread_t tA, tB;
  int s;

  fprintf( stderr, "main: this is the second mtx run\n");
  get_time_ms();

  s = pthread_create( &tA, NULL, threadFuncA, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(A)\n");

  s = pthread_create( &tB, NULL, threadFuncB, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(B)\n");

  s = pthread_join( tA, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(A)\n");

  s = pthread_join( tB, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(B)\n");

  get_time_ms();
  fprintf( stderr, "main: this is the end\n");
}


==============================================================================
*kt_linux_core_263*	sync: ref: lock-free code: a false sense of security

{one}
http://www.drdobbs.com/cpp/lock-free-code-a-false-sense-of-security/210600279

Lock-Free Code: A False Sense of Security By Herb Sutter, September 08, 2008

Writing lock-free code can confound anyoneâeven expert programmers, as Herb shows this month.

Given that lock-based synchronization has serious problems [1], it can be tempting to think
lock-free code must be the answer. Sometimes that is true. In particular, it's useful to have
libraries provide hash tables and other handy types whose implementations are internally
synchronized using lock-free techniques, such as Java's ConcurrentHashMap, so that we can use those
types safely from multiple threads without external synchronization and without having to understand
the subtle lock-free implementation details.
{Q} Java's ConcurrentHashMap

<two-drawbacks-for-lock-free-code>
But replacing locks wholesale by writing your own lock-free code is not the answer. Lock-free code
has two major drawbacks. First, it's not broadly useful for solving typical problems-lots of basic
data structures, even doubly linked lists, still have no known lock-free implementations. Coming up
with a new or improved lock-free data structure will still earn you at least a published paper in a
refereed journal, and sometimes a degree.

Second, it's hard even for experts. It's easy to write lock-free code that appears to work, but it's
very difficult to write lock-free code that is correct and performs well. Even good magazines and
refereed journals have published a substantial amount of lock-free code that was actually broken in
subtle ways and needed correction.

To illustrate, let's dissect some peer-reviewed lock-free code that was published here in DDJ just
two months ago [2]. The author, Petru Marginean, has graciously allowed me to dissect it here so
that we can see what's wrong and why, what lessons we should learn, and how to write the code
correctly. That someone as knowledgable as Petru, who has published many good and solid articles,
can get this stuff wrong should be warning enough that lock-free coding requires great care.
<KT> See Lock-Free Queues in below.

A Limited Lock-Free Queue

<limitation-or-assumption-of-q>
Marginean's goal was to write a limited lock-free queue that can be used safely without internal or
external locking. To simplify the problem, the article imposed some significant restrictions,
including that the queue must only be used from two threads with specific roles: one Producer thread
that inserts into the queue, and one Consumer thread that removes items from the queue.

Marginean uses a nice technique that is designed to prevent conflicts between the writer and reader:

o The producer and consumer always work in separate parts of the underlying list, so that their work
won't conflict. At any given time, the first "unconsumed" item is the one after the one iHead refers
to, and the last (most recently added) "unconsumed" item is the one before the one iTail refers to.

o The consumer increments iHead to tell the producer that it has consumed another item in the queue.

o The producer increments iTail to tell the consumer that another item is now available in the
queue. <o> Only the producer thread ever actually modifies the queue.<o> That means the producer is
responsible, not only for adding into the queue, but also for removing consumed items. To maintain
separation between the producer and consumer and prevent them from doing work in adjacent nodes, the
producer won't clean up the most recently consumed item (the one referred to by iHead).

<KT> The consumer only changes iterator and do not modify list itself.

Q     ...   <- begin()
      ...
      ...
      [ ]   <- head, -> consumer, dummy. uses tail to check empty, publish head and consume.
      [ ]   <- first unconsumed item
      ...
      [ ]   <- last unconsumed item
      [ ]   <- tail, <- producer, end(). add, publish tail and uses head to trim unused. 
               end() and push_back()

The idea is reasonable; only the implementation is fatally flawed. Here's the original code, written
in C++ and using an STL doubly linked list<T> as the underlying data structure. I've reformatted the
code slightly for presentation, and added a few comments for readability: 

// Original code from [1] (broken without external locking)
//
template <typename T>
struct LockFreeQueue {
  private:
    std::list<T> list;
    typename std::list<T>::iterator iHead, iTail;

  public:
    LockFreeQueue() {
      list.push_back(T());        // add dummy separator
      iHead = list.begin();
      iTail = list.end();
    }

    // Produce is called on the producer thread only:

    void Produce(const T& t) {
      list.push_back(t);               // add the new item
      iTail = list.end();              // <publish> it
      list.erase(list.begin(), iHead); // trim unused nodes
    }

    // Consume is called on the consumer thread only:

    bool Consume(T& t) {
      typename std::list<T>::iterator iNext = iHead;
      ++iNext;
      if (iNext != iTail) {         // if queue is nonempty
        iHead = iNext;              // <publish> that we took an item
        t = *iHead;                 // copy it back to the caller
        return true;                // and report success
      }
      return false;                 // else report queue was empty
    }
};

<lock-free-variable> <two-key-property>
The fundamental reason that the code is broken is that it has race conditions on both would-be
lock-free variables, iHead and iTail. To avoid a race, a lock-free variable must have two key
properties that we need to watch for and guarantee: atomicity and ordering. These variables are
neither.

Atomicity

First, reads and writes of a lock-free variable must be atomic. For this reason, lock-free variables
are typically no larger than the machine's native word size, and are usually pointers (C++), object
references (Java, .NET), or integers. Trying to use an ordinary list<T>::iterator variable as a
lock-free shared variable isn't a good idea and can't reliably meet the atomicity requirement, as we
will see.

Let's consider the races on iHead and iTail in these lines from Produce and Consume:

void Produce(const T& t) {
  ...
  iTail = list.end();
  list.erase(list.begin(), iHead);
}
 
bool Consume(T& t) {
  ...
  if (iNext != iTail) {
    iHead = iNext;
  ...   
  }
}

If reads and writes of iHead and iTail are not atomic, then Produce could read a partly updated (and
therefore corrupt) iHead and try to dereference it, and Consume could read a corrupt iTail and
fall off the end of the queue. Marginean does note this requirement:

"Reading/writing list<T>::iterator is atomic on the machine upon which you run the application." [2]

Alas, atomicity is necessary but not sufficient (see next section), and not supported by
list<T>::iterator. First, in practice, many list<T>::iterator implementations I examined are [larger]
than the native machine/pointer size, which means that they can't be read or written with atomic
loads and stores on most architectures. Second, in practice, even if they were of an appropriate
size, you'd have to add other decorations to the variable to ensure atomicity, for example to
require that the variable be properly [aligned] in memory.

Finally, the code isn't valid ISO C++. The 1998 C++ Standard said nothing about concurrency, and so
provided no such guarantees at all. The upcoming second C++ standard that is now being finalized
C++0x, does include a memory model and thread support, and explicitly forbids it. In brief, C++0x
says that the answer to questions such as, "What do I need to do to use a list<T> mylist
thread-safely?" is "Same as any other object"âif you know that an object like mylist is shared, you
must externally synchronize access to it, including via iterators, by protecting all such uses with
locks, else you've written a race [3]. Note: Using C++0x's std::atomic<> is not an option for
list<T>::iterator, because atomic<T> requires T to be a bit-copyable type, and STL types and their
iterators aren't guaranteed to be that.

Ordering Problems in Produce

Second, reads and writes of a lock-free variable must occur in an expected order, which is nearly
always the exact order they appear in the program source code. But compilers, processors, and caches
love to optimize reads and writes, and will helpfully reorder, invent, and remove memory reads and
writes unless you prevent it from happening. 

<to-prevent-reordering>
The right prevention happens implicitly when you use mutex locks or ordered atomic variables; {Q}
why mutex prevent this? 

C++0x std::atomic, Java/.NET volatile; you can also do it explicitly, but with considerably more
effort, using ordered API calls e.g., Win32 InterlockedExchange or memory fences/barriers e.g.,
Linux mb.  Trying to write lock-free code without using any of these tools can't possibly work.

Consider again this code from Produce, and ignore that the assignment iTail isn't atomic as we look
for other problems:

list.push_back(t);  // A: add the new item
iTail = list.end(); // B: publish it

This is a classic publication race because lines A and B can be (partly or entirely) reordered. For
example, let's say that some of the writes to the T object's members are delayed until after the
write to iTail, which publishes that the new object is available; then the consumer thread can see a
partly assigned T object.

What is the minimum necessary fix? We might be tempted to write a memory barrier between the two
lines:

// Is this change enough?
list.push_back(t);  // A: add the new item
mb();               // full fence
iTail = list.end(); // B: publish it

Before reading on, think about it and see if you're convinced that this is (or isn't) right.

Have you thought about it? As a starter, here's one issue: Although list.end is probably unlikely to
perform writes, it's possible that it might, and those are side effects that need to be complete
before we publish iTail. 

The general issue is that you can't make assumptions about the side effects of library functions you
call, and you have to make sure they're fully performed before you publish the new state. 

So a slightly improved version might try to store the result of list.end into a local unshared
variable and assign it after the barrier:

// Better, but is it enough?
list.push_back(t);
tmp = list.end();
mb();                // full fence
iTail = tmp;

Unfortunately, this still isn't enough. Besides the fact that assigning to iTail isn't atomic and
that we still have a race on iTail in general, compilers and processors can also invent writes to
iTail that break this code. Let's consider write invention in the context of another problem area:
Consume.

Ordering Problems in Consume

Here's another reordering problem, this time from Consume:

++iNext;
if (iNext != iTail) {
  iHead = iNext;        // C
  t = *iHead;           // D

Note that Consume updates iHead to advertise that it has consumed another item before it actually
  reads the item's value. Is that a problem? We might think it's innocuous, because the producer
  always leaves the iHead item alone to stay at least one item away from the part of the list the
  consumer is using.

It turns out this code is broken regardless of which order we write lines C and D, because the
compiler or processor or cache can reorder either version in unfortunate ways. Consider what happens
if the consumer thread performs a consecutive two calls to Consume: The memory reads and writes
performed by those two calls could be reordered so that iHead is incremented twice before we copy
the two list nodes' values, and then we have a problem because the producer may try to remove
nodes the consumer is still using. [KT] Think that this is a problem besides odering and lock-free
variable.

Note: This doesn't mean the compiler or processor transformations are broken; they're not. Rather
the code is racy and has insufficient synchronization, and so it breaks the memory model guarantees
and makes such transformations possible and visible.

Reordering isn't the only issue. Another problem is that compilers and processors can invent writes,
so they could inject a transient value:

// Problematic compiler/processor transformation
if (iNext != iTail) {
  iHead = 0xDEADBEEF;
  iHead = iNext;
  t = *iHead;

Clearly, that would break the producer thread, which would read a bad value for iHead. More likely,
the compiler or processor might speculate that most of the time iNext != iTail:

// Another problematic transformation
//
__temp = iHead;
iHead = iNext;  // speculatively set to iNext

if (iNext == iTail) {   // note: inverted test!
  iHead = __temp;   // undo if we guessed wrong
} else {
  t = *iHead;

<invariant>
But now iHead could equal iTail, which breaks the essential invariant that iHead must never equal
iTail, on which the whole design depends.

Can we solve these problems by writing line D before C, then separating them with a full fence? Not
entirely: That will prevent most of the aforementioned optimizations, but it will not eliminate all
of the problematic invented writes. More is needed.

Next Steps

These are a sample of the concurrency problems in the original code. Marginean showed a good
algorithm, but the implementation is broken because it uses an inappropriate type and performs
insufficient synchronization/ordering. Fixing the code will require a rewrite, because we need to
change the data structure and the code to let us use proper ordered atomic lock-free variables. But
how? Next month, we'll consider a fixed version. Stay tuned.

Notes

[1] H. Sutter, "The Trouble With Locks," C/C++ Users Journal, March 2005.
(www.ddj.com/cpp/184401930).

[2] P. Marginean, "Lock-Free Queues," Dr. Dobb's Journal, July 2008. (www.ddj.com/208801974).

[3] B. Dawes, et al., "Thread-Safety in the Standard Library," ISO/IEC JTC1/SC22/WG21 N2669, June
2008. (www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2669.htm).


{two}
http://www.drdobbs.com/parallel/lock-free-queues/208801974?pgno=1
Lock-Free Queues By Petru Marginean, July 01, 2008

<note-begin>
This article as written assumes a sequentially consistent model. In particular, the code relies on
specific order of instructions in both Consumer and Producer methods. However, without inserting
proper memory barrier instructions, these instructions can be reordered with unpredictable results
(see, for example, the classic Double-Checked Locking problem).

Another issue is using the standard std::list<T>. While the article mentions that it is the
developer responsibility to check that the reading/writing std::list<T>::iterator is atomic, this
turns out to be too restrictive. While gcc/MSVC++2003 has 4-byte iterators, the MSVC++2005 has
8-byte iterators in Release Mode and 12-byte iterators in the Debug Mode.

The solution to prevent this is to use memory barriers/volatile variables. 
The downloadable code featured at the top of this article has fixed that issue. ~

Many thanks to Herb Sutter who signaled the issue and helped me fix the code. --P.M.
<note-end>

Queues can be useful in a variety of systems involving data-stream processing. Typically, you have a
data source producing dataârequests coming to a web server, market feeds, or digital telephony
packetsâat a variable pace, and you need to process the data as fast as possible so there are no
losses. To do this, you can push data into a queue using one thread and process it using a different
threadâa good utilization of resources on multicore processors. One thread inserts data into the
queue, and the other reads/deletes elements from the queue. Your main requirement is that a
high-rate data burst does not last longer than the system's ability to accumulate data while the
consumer thread handles it. [KT] This means no overlow? 

The queue you use has to be threadsafe to prevent race conditions when inserting/removing data from
multiple threads. For obvious reasons, it is necessary that the queue mutual exclusion mechanism add
as little overhead as possible.

In this article, I present a lock-free queue (the source code for the lockfreequeue class is
available online; see www.ddj.com/code/) in which one thread can write to the queue and another
read from itâat the same time without any locking. /0807/lockfreequeue/

To do this, the code implements these requirements:

<conditions>
o There is a single writer (Producer) and single reader (Consumer). When you have multiple producers
and consumers, you can still use this queue with some external locking. You cannot have multiple
producers writing at the same time (or multiple consumers consuming the data simultaneously), but
you can have one producer and one consumer (2x threads) accessing the queue at the same time
(Responsibility: developer).

o When inserting/erasing to/from an std::list<T>, the iterators for the existing elements must
remain valid (Responsibility: library implementor).

o <Only> one thread modifies the queue; the producer thread both adds/erases elements in the queue
(Responsibility: library implementor).

o Beside the underlying std::list<T> used as the container, the lock-free queue class also holds two
iterators pointing to the not-yet-consumed range of elements; each is modified by one thread and
read by the other (Responsibility: library implementor).

o Reading/writing list<T>::iterator is <atomic> on the machine upon which you run the application. If
they are not on your implementation of STL, you should check whether the raw pointer's operations
are atomic. You could easily replace the iterators to be mentioned shortly with raw pointers in the
code (Responsibility: machine). <KT> ?

<KT> Here big condition is that one producer and consumer model, only producer modify a list, and
updating iterators between threads are atomic.

Because I use Standard C++, the code is portable under the aforementioned "machine" assumption: 

template <typename T>
struct LockFreeQueue
{
    LockFreeQueue();
    void Produce(const T& t);
    bool Consume(T& t);

  private:
    typedef std::list<T> TList;
    TList list;
    typename TList::iterator iHead, iTail;   <KT> why not 'TList::iterator iHead'?
};

Considering how simple this code is, you might wonder how can it be threadsafe. The magic is due to
design, not implementation. Take a look at the implementation of the Produce() and Consume()
methods. The Produce() method looks like this: 

void Produce(const T& t)
{
  list.push_back(t);
  iTail = list.end();
  list.erase(list.begin(), iHead);
}

To understand how this works, mentally separate the data from LockFreeQueue<T> into two groups:

o The list and the iTail iterator, modified by the Produce() method (Producer thread).
o The iHead iterator, modified by the Consume() method (Consumer thread). 

<o>
Produce() is the [only] method that changes the list (adding new elements and erasing the consumed
elements), and it is essential that [only] one thread ever calls Produce()âit's the Producer
thread! The iterator (iTail) (only manipulated by the Producer thread) changes it only after a new
element is added to the list. This way, when the Consumer thread is reading the iTail element, the
new added element is ready to be used. The Consume() method tries to read all the elements between
iHead and iTail (excluding both ends). 
<o>

bool Consume(T& t)
{
  typename TList::iterator iNext = iHead;
  ++iNext;
  if (iNext != iTail)
  {
    iHead = iNext;
    t = *iHead;
    return true;
  }
  return false;
}

This method reads the elements, but doesn't remove them from the list. Nor does it access the list
directly, but through the iterators. They are guaranteed to be valid after std::list<T> is modified,
so no matter what the Producer thread does to the list, you are safe to use them.

The std::list<T> maintains an element (pointed to by iHead) that is considered already read. For
this algorithm to work even when the queue was just created, I add an empty T() element in the
constructor of the LockFreeQueue<T> (see Figure 1): 

LockFreeQueue()
{
  list.push_back(T());
  iHead = list.begin();
  iTail = list.end();
}

<discussion-when-queue-is-empty>
Consume() may fail to read an element (and return false). Unlike traditional lock-based queues, this
queue works fast when the queue is not empty, but needs an external locking or polling method to
wait for data. Sometimes you want to wait if there is no element available in the queue, and avoid
returning false. A naive approach to waiting is: 

T Consume()
{
  T tmp;
  while (!Consume(tmp))
    ;
  return tmp;
}

This Consume() method will likely heat up one of your CPUs red-hot to 100-percent use if there are
no elements in the queue. Nevertheless, this should have good performance when the queue is not
empty. However, if you think of it, a queue that's almost never empty is a sign of systemic trouble:
It means the consumer is unable to keep pace with the producer, and sooner or later, the system is
doomed to die of memory exhaustion. Call this approach NAIVE_POLLING. 

A friendlier Consume() function does some pooling and calls some sort of sleep() or yield() function
available on your system: 

T Consume(int wait_time = 1/*milliseconds*/)
{
  T tmp;
  while (!Consume(tmp))
  {
    Sleep(wait_time/*milliseconds*/);
  }
  return tmp;
}

The DoSleep() can be implemented using nanosleep() (POSIX) or Sleep() (Windows), or even better,
using boost::thread::sleep(), which abstracts away system-dependent nomenclature. Call this
approach SLEEP. Instead of simple polling, you can use more advanced techniques to signal the
Consumer thread that a new element is available. I illustrate this in Listing One using a
boost::condition variable.

#include <boost/thread.hpp>
#include <boost/thread/condition.hpp>
#include <boost/thread/xtime.hpp>
    
template <typename T>
struct WaitFreeQueue
{
    void Produce(const T& t)
    {
        queue.Produce(t);
        cond.notify_one();
    }
    bool Consume(T& t)
    {
        return queue.Consume(t);
    }
    T Consume(int wait_time = 1/*milliseconds*/)
    {
        T tmp;
        if (Consume(tmp))
            return tmp;

        // the queue is empty, try again (possible waiting...)
        boost::mutex::scoped_lock lock(mtx);
        while (!Consume(tmp))          // line A
        {
            boost::xtime t;
            boost::xtime_get(&t, boost::TIME_UTC);
            AddMilliseconds(t, wait_time);
            cond.timed_wait(lock, t);  // line B
        }
        return tmp;
    }
private:
    LockFreeQueue<T> queue;
    boost::condition cond;
    boost::mutex mtx;
};

I used the timed_wait() instead of the simpler wait() to solve a possible deadlock when Produce() is
called between line A and line B in Listing One. Then wait() will miss the notify_one() call and
have to wait for the next produced element to wake up. <If> this element never comes (no more produced
elements or if the Produce() call actually waits for Consume() to return), there's a deadlock.
Call this approach TIME_WAIT.

The lock is still wait-free as long as there are elements in the queue. In this case, the Consumer()
thread does no waiting and reads data as fast as possible (even with the Producer() that is
inserting new elements). Only when the queue is exhausted does locking occur. 
<KT> claims that it is lock-free as long as queue is not empty.

The Ping-Pong Test

To compare the three approaches (NAIVE_POLLING, SLEEP, and TIME_WAIT), I implemented a test called
"Ping-Pong" that is similar to the game of table tennis (the source code is available online). In
Figure 2, there are two identical queues between the threads T1 and T2. 

      queue 1
T1 ------------> T2
   <------------
      queue 2

You first load one of the queues with a number of "balls," then ask each thread to read from one
queue and write to the other. The result is a controlled infinite loop. By limiting the game to a
fixed number of reads/writes ("shots"), you get an understanding of how the queue behaves when
varying the waiting/sleep time and strategy and the number of "balls." The faster the game, the
better the performance. You should also check CPU usage to see how much of it is used for real work.

o "No ball" means "do nothing" (like two players waiting for the other to start). This gives you an
idea of how good the queues are when there is no dataâhow nervous the players are. Ideally, CPU
usage should be zero.
 
o "One ball" is like the real ping-pong game: Each player shoots and waits for the other to shoot.

o "Two (or more) balls" means both players could shoot at the same time, modulo collision and
waiting issues.


In a wait-free system, the more balls in the game, the better the performance gain compared to the
classic locking strategy. This is because wait-free is an optimistic concurrency control method
(works best when there is no contention), while classical lock-based concurrency control is
pessimistic (assumes contention happens and preemptively inserts locking).

Ready to play? Here is the Ping-Pong test command line:

$> ./pingpong [strategy] [timeout] [balls] [shots]

When you run the program, the tests show the results in the table shown in Figure 3:

o The best combination is the timed_wait() with a small wait time (1ms in the test for TIMED_WAIT).
It has a very fast response time and almost 0 percent CPU usage when the queue is empty.

o Even when the sleep time is 0 (usleep(0)), the worst seems to be the sleep() method, especially
when the queue is likely to be empty. (The number of shots in this case is 100-times smaller than
the other cases because of the long duration of the game.)

o The NO_WAIT strategy is [fast] but behaves worst when there are no balls (100-percent CPU usage to
do nothing). It has the best performance when there is a single ball.

Figure 4 presents a table with the results for a classic approach (see SafeQueue). These results
show that this queue is, on average, more than four-times slower than the LockFreeQueue. The
slowdown comes from the synchronization between threads. Both Produce() and Consume() have to wait
for each other to finish. CPU usage is almost 100 percent for this test (similar to the NO_WAIT
strategy, but not even close to its performance).

Final Considerations

The single-threaded code below shows the value of the list.size() when Producing/ Consuming
elements: 

LockFreeQueue<int> q;   // list.size() == 1
q.Produce(1);           // list.size() == 2
int i;
q.Consume(i);           // list.size() == still 2!;
                        // Consume() doesn't modify the list
q.Produce(i);           // list.size() == 2 again;

The size of the queue is 1 if Produce() was never called and greater than 1 if any element was
produced.

No matter how many times Consume() is called, the list's size will stay constant. It is Produce()
that is increasing the size (by 1); and if there were consumed elements, it will also delete them
from the queue. In a way, Produce() acts as a simple garbage collector. <o> The whole thread safety
comes from the fact that specific data is modified from single threads only. The synchronization
between threads is done using iterators (or pointers, whichever has atomic read/write operation on
your machine). <o> Also consider this code: 

usleep(1000);    // sleep 1 microsecond

On the face of it, this line of code makes a thread sleep for 1 microsecond, and then continue. In
reality, 1 microsecond is just a lower bound to the duration of the call.

The man page for usleep() says, "The usleep() function suspends execution of the calling process for
(at least) usec microseconds. The sleep may be lengthened slightly by any system activity or by the
time spent processing the call or by the granularity of system timers," or if you use the
nanosleep() function. "Therefore, nanosleep() always pauses for at least the specified time;
however, it can take up to 10 ms longer than specified until the process becomes runnable again."

So if the process is not scheduled under a real-time policy, there's no guarantee when your thread
will be running again. I've done some tests and (to my surprise) there are situations when code such
as: 

cond.timed_wait(lock, x);    // x = e.g. 1 millisecond

will actually wait for more than 1 second.


{three}
http://www.drdobbs.com/cpp/the-trouble-with-locks/184401930
The Trouble with Locks

By Herb Sutter, March 01, 2005

References

[1] Sutter, H. "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software," Dr.
Dobb's Journal, March 2005. Available online at http://www.gotw.ca/publications/concurrency-ddj.htm.

[2] Sutter, H. "The Concurrency Revolution," C/C++ Users Journal, February 2005. This is an
abbreviated version of [1].

[3] Alexandrescu, A. "Lock-Free Data Structures," C/C++ Users Journal, October 2004.

[4] Alexandrescu, A. and M. Michael. "Lock-Free Data Structures with Hazard Pointers," C/C++ Users
Journal, December 2004.

[5] http://blogs.msdn.com/cbrumme. 

Lock-based programming may not be the best approach to building large concurrent programs.

In my most recent articles [1, 2], I presented reasons why concurrency (for example, multithreading)
will be the next revolution in the way we develop software â a sea change of the same order as the
object-oriented revolution. I also stated that "the vast majority of programmers today don't grok
concurrency, just as the vast majority of programmers 15 years ago didn't yet grok objects."

In this column, I'd like to consider just one question that several people wrote to ask, namely: "Is
concurrency really that hard?" In particular, a few readers felt that lock-based programming is well
understood; it is, after all, the status quo mainstream solution to concurrency control.

So, "is concurrency really that hard?" My short answer is this:

o Lock-based programming, our status quo, is difficult for experts to get right. Worse, it is also
fundamentally flawed for building large programs. This article focuses exclusively on lock-based
programming just because there's so much to say in even a 50,000-foot overview that I ran out of
room.

o Lock-free programming is difficult for gurus to get right. I'll save this for another time, but if
you're interested, you should check out Andrei Alexandrescu's recent articles for a sampling of the
issues in lock-free programming and hazard pointers [3, 4]. (Aside: Yes, I'm implying Andrei is a
guru. I hope he doesn't mind my outing him in public like this. I don't think it was much of a
secret.) (More aside: The hazard pointer work shows in particular why, if you're writing
lock-free data structures, you really really want garbage collection. You can do it yourself
without garbage collection, but it's like working with knives that are sharp on both edges and
don't have handles. But that's another article. Specifically, it's Andrei's other article.)

Unfortunately, today's reality is that only thoughtful experts can write explicitly concurrent
programs that are correct and efficient. This is because today's programming models for concurrency
are subtle, intricate, and fraught with pitfalls that easily (and frequently) result in unforeseen
races (i.e., program corruption) deadlocks (i.e., program lockup) and performance cliffs (e.g.,
priority inversion, convoying, and sometimes complete loss of parallelism and/or even worse
performance than a single-threaded program). And even when a correct and efficient concurrent
program is written, it takes great care to maintain â it's usually brittle and difficult to maintain
correctly because current programming models set a very high bar of expertise required to reason
reliably about the operation of concurrent programs, so that apparently innocuous changes to a
working concurrent program can (and commonly do, in practice) render it entirely or intermittently
nonworking in unintended and unexpected ways. Because getting it right and keeping it right is so
difficult, at many major software companies there is a veritable priesthood of gurus who write and
maintain the core concurrent code.

Some people think I'm overstating this, so let me amplify. In this article, I'll focus on just the
narrow question of how to write a lock-based program correctly, meaning that it works (avoids data
corruption) and doesn't hang (avoids deadlock and livelock). That's pretty much the minimum
requirement to write a program that runs at all.

Question: Is the following code thread-safe? If it is, why is it safe? If it isn't always, under
what conditions is it thread-safe?

T Add( T& a, T& b ) {
  T result;
  // ... read a and b and set result to
  // proper values ...
  return result;
}

There are a lot of possibilities here. Let's consider some of the major ones.

Lock-Based Solutions?

Assume that reading a T object isn't an atomic operation. Then, if a and/or b are accessible from
another thread, we have a classic race condition: While we are reading the values of a and/or b,
some other thread might be changing those objects, resulting in blowing up the program; if you're
lucky, say, by causing the object to follow an internal pointer some other thread just deleted; or
reading corrupt values.

How would you solve that? Please stop and think about it before reading on...

Ready? Okay: Now please stop a little longer and think about your solution some more, and consider
whether it might have any further holes, before reading on...

Now that you've thought about it deeply, let's consider some alternatives.

Today's typical lock-based approach is to acquire locks so that uses of a and b on one thread won't
interleave. Typically, this is done by acquiring a lock on an explicit synchronization object [a
mutex, for instance] that covers both objects, or by acquiring locks on an implicit mutex associated
with the objects themselves. To acquire a lock that covers both objects, Add has to know what that
lock is, either because a and b and their lock are globals [but then why pass a and b as
parameters?] or because the caller acquires the lock outside of Add [which is usually preferable].
To acquire a lock on each object individually, we could write:

T SharedAdd( T& a, T& b ) {
  T result;
  lock locka( a );  // lock is a helper
  // whose constructor acquires a lock
  lock lockb( b );  // and whose
  // destructor releases the lock
  // ... read a and b and set result to
  //  proper values ...
  return result;
} // release the locks 


{four}
http://www.drdobbs.com/parallel/writing-lock-free-code-a-corrected-queue/210604448?pgno=1
Writing Lock-Free Code: A Corrected Queue

By Herb Sutter, September 29, 2008

Notes

[1] H. Sutter. âLock-Free Code: A False Sense of Securityâ (DDJ, September 2008).
(www.ddj.com/cpp/210600279).

[2] P. Marginean. "Lock-Free Queues" (DDJ, July 2008). (www.ddj.com/208801974).

[3] This is just like a canonical exception safety patternâdo all the work off to the side, then
commit to accept the new state using nonthrowing operations only. "Think in transactions" applies
everywhere, and should be ubiquitous in the way we write our code.

[4] Compare-and-swap (CAS) is the most widely available fundamental lock-free operation and so I'll
focus on it here. However, some systems instead provide the equivalently powerful
load-linked/store-conditional (LL/SC) instead.


As we saw last month [1], lock-free coding is hard even for experts. There, I dissected a published
lock-free queue implementation [2] and examined why the code was quite broken. This month, let's see
how to do it right.

Lock-Free Fundamentals

When writing lock-free code, always keep these essentials well in mind:

Key concepts. Think in transactions. Know who owns what data. Key tool. The ordered atomic variable. 

When writing a lock-free data structure, "to think in transactions" means to make sure that each
operation on the data structure is atomic, all-or-nothing with respect to other concurrent
operations on that same data. The typical coding pattern to use is to do work off to the side, then
"publish" each change to the shared data with a single atomic write or compare-and-swap. [3] Be sure
that concurrent writers don't interfere with each other or with concurrent readers, and pay special
attention to any operations that delete or remove data that a concurrent operation might still be
using.

Be highly aware of who owns what data at any given time; mistakes mean races where two threads think
they can proceed with conflicting work. You know who owns a given piece of shared data right now by
looking at the value of the ordered atomic variable that says who it is. To hand off ownership of
some data to another thread, do it at the end of a transaction with a single atomic operation that
means "now it's your's."

An ordered atomic variable is a "lock-free-safe" variable with the following properties that make it
safe to read and write across threads without any explicit locking:

Atomicity. Each individual read and write is guaranteed to be atomic with respect to all other reads
and writes of that variable. The variables typically fit into the machine's native word size, and so
are usually pointers (C++), object references (Java, .NET), or integers. Order. Each read and write
is guaranteed to be executed in source code order. Compilers, CPUs, and caches will respect it and
not try to optimize these operations the way they routinely distort reads and writes of ordinary
variables. Compare-and-swap (CAS) [4]. There is a special operation you can call using a syntax like
variable.compare_exchange( expectedValue, newValue ) that does the following as an atomic operation:
If variable currently has the value expectedValue, it sets the value to newValue and returns true;
else returns false. A common use is if(variable.compare_exchange(x,y)), which you should get in the
habit of reading as, "if I'm the one who gets to change variable from x to y." 

Ordered atomic variables are spelled in different ways on popular platforms and environments. For
example:

volatile in C#/.NET, as in volatile int. volatile or * Atomic* in Java, as in volatile int,
AtomicInteger. atomic<T> in C++0x, the forthcoming ISO C++ Standard, as in atomic<int>. 

In the code that follows, I'm going to highlight the key reads and writes of such a variable; these
variables should leap out of the screen at you, and you should get used to being very aware of every
time you touch one.

If you don't yet have ordered atomic variables yet on your language and platform, you can emulate
them by using ordinary but aligned variables whose reads and writes are guaranteed to be naturally
atomic, and enforce ordering by using either platform-specific ordered API calls (such as Win32's
InterlockedCompareExchange for compare-and-swap) or platform-specific explicit memory
fences/barriers (for example, Linux mb).


==============================================================================
*kt_linux_core_264*	conc: ref: the free lunch is over 
http://www.gotw.ca/publications/concurrency-ddj.htm

The Free Lunch Is Over
A Fundamental Turn Toward Concurrency in Software

By Herb Sutter

The biggest sea change in software development since the OO revolution is knocking at the door, and
its name is Concurrency.

This article appeared in Dr. Dobb's Journal, 30(3), March 2005. A much briefer version under the
title "The Concurrency Revolution" appeared in C/C++ Users Journal, 23(2), February 2005.

Update note: The CPU trends graph last updated August 2009 to include current data and show the
trend continues as predicted. The rest of this article including all text is still original as first
posted here in December 2004.

Your free lunch will soon be over. What can you do about it? What are you doing about it?

The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have
run out of room with most of their traditional approaches to boosting CPU performance. Instead of
driving clock speeds and straight-line instruction throughput ever higher, they are instead turning
en masse to hyperthreading and multicore architectures. Both of these features are already available
on chips today; in particular, multicore is available on current PowerPC and Sparc IV processors,
   and is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004 In-Stat/MDR Fall
   Processor Forum was multicore devices, as many companies showed new or updated multicore
   processors. Looking back, itâs not much of a stretch to call 2004 the year of multicore.

And that puts us at a fundamental turning point in software development, at least for the next few
years and for applications targeting general-purpose desktop computers and low-end servers (which
    happens to account for the vast bulk of the dollar value of software sold today). In this
article, Iâll describe the changing face of hardware, why it suddenly does matter to software, and
how specifically the concurrency revolution matters to you and is going to change the way you will
likely be writing software in the future.

Arguably, the free lunch has already been over for a year or two, only weâre just now noticing.  

<The Free Performance Lunch>

Thereâs an interesting phenomenon thatâs known as âAndy giveth, and Bill taketh away.â No matter how
fast processors get, software consistently finds new ways to eat up the extra speed. Make a CPU ten
times as fast, and software will usually find ten times as much to do (or, in some cases, will feel
    at liberty to do it ten times less efficiently). Most classes of applications have enjoyed free
and regular performance gains for several decades, even without releasing new versions or doing
anything special, because the CPU manufacturers (primarily) and memory and disk manufacturers
(secondarily) have reliably enabled ever-newer and ever-faster mainstream systems. Clock speed isnât
the only measure of performance, or even necessarily a good one, but itâs an instructive one: Weâre
used to seeing 500MHz CPUs give way to 1GHz CPUs give way to 2GHz CPUs, and so on. Today weâre in
the 3GHz range on mainstream computers.

The key question is: When will it end? After all, Mooreâs Law predicts exponential growth, and
clearly exponential growth canât continue forever before we reach hard physical limits; light isnât
getting any faster. The growth must eventually slow down and even end. (Caveat: Yes, Mooreâs Law
    applies principally to transistor densities, but the same kind of exponential growth has
    occurred in related areas such as clock speeds. Thereâs even faster growth in other spaces, most
    notably the data storage explosion, but that important trend belongs in a different article.)

If youâre a software developer, chances are that you have already been riding the âfree lunchâ wave
of desktop computer performance. Is your applicationâs performance borderline for some local
operations? âNot to worry,â the conventional (if suspect) wisdom goes; âtomorrowâs processors will
have even more throughput, and anyway todayâs applications are increasingly throttled by factors
other than CPU throughput and memory speed (e.g., theyâre often I/O-bound, network-bound,
    database-bound).â Right?

Right enough, in the past. But dead wrong for the foreseeable future.

The good news is that processors are going to continue to become more powerful. The bad news is
that, at least in the short term, the growth will come mostly in directions that do not take most
current applications along for their customary free ride.

Over the past 30 years, CPU designers have achieved performance gains in three main areas, the first
two of which focus on straight-line execution flow:

clock speed
execution optimization
cache

Increasing clock speed is about getting more cycles. Running the CPU faster more or less directly
means doing the same work faster.

Optimizing execution flow is about doing more work per cycle. Todayâs CPUs sport some more powerful
instructions, and they perform optimizations that range from the pedestrian to the exotic, including
pipelining, branch prediction, executing multiple instructions in the same clock cycle(s), and even
reordering the instruction stream for out-of-order execution. These techniques are all designed to
make the instructions flow better and/or execute faster, and to squeeze the most work out of each
clock cycle by reducing latency and maximizing the work accomplished per clock cycle.

Chip designers are under so much pressure to deliver ever-faster CPUs that theyâll risk changing the
meaning of your program, and possibly break it, in order to make it run faster

Brief aside on instruction reordering and memory models: Note that some of what I just called
âoptimizationsâ are actually far more than optimizations, in that they can change the meaning of
programs and cause visible effects that can break reasonable programmer expectations. This is
significant. CPU designers are generally sane and well-adjusted folks who normally wouldnât hurt a
fly, and wouldnât think of hurting your codeâ¦ normally. But in recent years they have been willing
to pursue aggressive optimizations just to wring yet more speed out of each cycle, even knowing full
well that these aggressive rearrangements could endanger the semantics of your code. Is this Mr.
Hyde making an appearance? Not at all. That willingness is simply a clear indicator of the extreme
pressure the chip designers face to deliver ever-faster CPUs; theyâre under so much pressure that
theyâll risk changing the meaning of your program, and possibly break it, in order to make it run
faster. Two noteworthy examples in this respect are write reordering and read reordering: Allowing a
processor to reorder write operations has consequences that are so surprising, and break so many
programmer expectations, that the feature generally has to be turned off because itâs too difficult
for programmers to reason correctly about the meaning of their programs in the presence of arbitrary
write reordering. Reordering read operations can also yield surprising visible effects, but that is
more commonly left enabled anyway because it isnât quite as hard on programmers, and the demands for
performance cause designers of operating systems and operating environments to compromise and choose
models that place a greater burden on programmers because that is viewed as a lesser evil than
giving up the optimization opportunities.

Finally, increasing the size of on-chip cache is about staying away from RAM. Main memory continues
to be so much slower than the CPU that it makes sense to put the data closer to the processorâand
you canât get much closer than being right on the die. On-die cache sizes have soared, and today
most major chip vendors will sell you CPUs that have 2MB and more of on-board L2 cache. (Of these
    three major historical approaches to boosting CPU performance, increasing cache is the only one
    that will continue in the near term. Iâll talk a little more about the importance of cache later
    on.)

Okay. So what does this mean?

A fundamentally important thing to recognize about this list is that all of these areas are
concurrency-agnostic. Speedups in any of these areas will directly lead to speedups in sequential
(nonparallel, single-threaded, single-process) applications, as well as applications that do make
use of concurrency. Thatâs important, because the vast majority of todayâs applications are
single-threaded, for good reasons that Iâll get into further below.

Of course, compilers have had to keep up; sometimes you need to recompile your application, and
target a specific minimum level of CPU, in order to benefit from new instructions (e.g., MMX, SSE)
  and some new CPU features and characteristics. But, by and large, even old applications have
  always run significantly fasterâeven without being recompiled to take advantage of all the new
  instructions and features offered by the latest CPUs.

That world was a nice place to be. Unfortunately, it has already disappeared.

<Obstacles, and Why You Donât Have 10GHz Today>

CPU performance growth as we have known it hit a wall two years ago. Most people have only recently
started to notice.

You can get similar graphs for other chips, but Iâm going to use Intel data here. Figure 1 graphs
the history of Intel chip introductions by clock speed and number of transistors. The number of
transistors continues to climb, at least for now. Clock speed, however, is a different story.

Around the beginning of 2003, youâll note a disturbing sharp turn in the previous trend toward
ever-faster CPU clock speeds. Iâve added lines to show the limit trends in maximum clock speed;
instead of continuing on the previous path, as indicated by the thin dotted line, there is a sharp
flattening. It has become harder and harder to exploit higher clock speeds due to not just one but
several physical issues, notably heat (too much of it and too hard to dissipate), power consumption
(too high), and current leakage problems.

Quick: Whatâs the clock speed on the CPU(s) in your current workstation? Are you running at 10GHz?
On Intel chips, we reached 2GHz a long time ago (August 2001), and according to CPU trends before
2003, now in early 2005 we should have the first 10GHz Pentium-family chips. A quick look around
shows that, well, actually, we donât. Whatâs more, such chips are not even on the horizonâwe have no
good idea at all about when we might see them appear.

Well, then, what about 4GHz? Weâre at 3.4GHz alreadyâsurely 4GHz canât be far away? Alas, even 4GHz
seems to be remote indeed. In mid-2004, as you probably know, Intel first delayed its planned
introduction of a 4GHz chip until 2005, and then in fall 2004 it officially abandoned its 4GHz plans
entirely. As of this writing, Intel is planning to ramp up a little further to 3.73GHz in early 2005
(already included in Figure 1 as the upper-right-most dot), but the clock race really is over, at
least for now; Intelâs and most processor vendorsâ future lies elsewhere as chip companies
aggressively pursue the same new multicore directions.

Weâll probably see 4GHz CPUs in our mainstream desktop machines someday, but it wonât be in 2005.
Sure, Intel has samples of their chips running at even higher speeds in the labâbut only by heroic
efforts, such as attaching hideously impractical quantities of cooling equipment. You wonât have
that kind of cooling hardware in your office any day soon, let alone on your lap while computing on
the plane.

<Myths and Realities: 2 x 3GHz < 6 GHz>

So a dual-core CPU that combines two 3GHz cores practically offers 6GHz of processing power. Right?

Wrong. Even having two threads running on two physical processors doesnât mean getting two times the
performance. Similarly, most multi-threaded applications wonât run twice as fast on a dual-core box.
They should run faster than on a single-core CPU; the performance gain just isnât linear, thatâs
all.

Why not? First, there is coordination overhead between the cores to ensure cache coherency (a
    consistent view of cache, and of main memory) and to perform other handshaking. Today, a two- or
four-processor machine isnât really two or four times as fast as a single CPU even for
multi-threaded applications. The problem remains essentially the same even when the CPUs in question
sit on the same die.

Second, unless the two cores are running different processes, or different threads of a single
process that are well-written to run independently and almost never wait for each other, they wonât
be well utilized. (Despite this, I will speculate that todayâs single-threaded applications as
    actually used in the field could actually see a performance boost for most users by going to a
    dual-core chip, not because the extra core is actually doing anything useful, but because it is
    running the adware and spyware that infest many usersâ systems and are otherwise slowing down
    the single CPU that user has today. I leave it up to you to decide whether adding a CPU to run
    your spyware is the best solution to that problem.)

If youâre running a single-threaded application, then the application can only make use of one core.
There should be some speedup as the operating system and the application can run on separate cores,
      but typically the OS isnât going to be maxing out the CPU anyway so one of the cores will be
      mostly idle. (Again, the spyware can share the OSâs core most of the time.) 
  

<TANSTAAFL: Mooreâs Law and the Next Generation(s)>

âThere ainât no such thing as a free lunch.â âR. A. Heinlein, The Moon Is a Harsh Mistress

Does this mean Mooreâs Law is over? Interestingly, the answer in general seems to be no. Of course,
     like all exponential progressions, Mooreâs Law must end someday, but it does not seem to be in
     danger for a few more years yet. Despite the wall that chip engineers have hit in juicing up
     raw clock cycles, transistor counts continue to explode and it seems CPUs will continue to
     follow Mooreâs Law-like throughput gains for some years to come.  The key difference, which is
     the heart of this article, is that the performance gains are going to be accomplished in
     fundamentally different ways for at least the next couple of processor generations. And most
     current applications will no longer benefit from the free ride without significant redesign.

For the near-term future, meaning for the next few years, the performance gains in new chips will be
fueled by three main approaches, only one of which is the same as in the past. The near-term future
performance growth drivers are:

hyperthreading
multicore
cache

Hyperthreading is about running two or more threads in parallel inside a single CPU. Hyperthreaded
CPUs are already available today, and they do allow some instructions to run in parallel. A limiting
factor, however, is that although a hyper-threaded CPU has some extra hardware including extra
registers, it still has just one cache, one integer math unit, one FPU, and in general just one each
of most basic CPU features. Hyperthreading is sometimes cited as offering a 5% to 15% performance
boost for reasonably well-written multi-threaded applications, or even as much as 40% under ideal
conditions for carefully written multi-threaded applications. Thatâs good, but itâs hardly double,
           and it doesnât help single-threaded applications.

Multicore is about running two or more actual CPUs on one chip. Some chips, including Sparc and
PowerPC, have multicore versions available already. The initial Intel and AMD designs, both due in
2005, vary in their level of integration but are functionally similar. AMDâs seems to have some
initial performance design advantages, such as better integration of support functions on the same
die, whereas Intelâs initial entry basically just glues together two Xeons on a single die. The
performance gains should initially be about the same as having a true dual-CPU system (only the
    system will be cheaper because the motherboard doesnât have to have two sockets and associated
    âglueâ chippery), which means something less than double the speed even in the ideal case, and
just like today it will boost reasonably well-written multi-threaded applications. Not
single-threaded ones.

Finally, on-die cache sizes can be expected to continue to grow, at least in the near term. Of these
three areas, only this one will broadly benefit most existing applications. The continuing growth in
on-die cache sizes is an incredibly important and highly applicable benefit for many applications,
  simply because space is speed. Accessing main memory is expensive, and you really donât want to
  touch RAM if you can help it. On todayâs systems, a cache miss that goes out to main memory often
  costs 10 to 50 times as much getting the information from the cache; this, incidentally, continues
  to surprise people because we all think of memory as fast, and it is fast compared to disks and
  networks, but not compared to on-board cache which runs at faster speeds. If an applicationâs
  working set fits into cache, weâre golden, and if it doesnât, weâre not. That is why increased
  cache sizes will save some existing applications and breathe life into them for a few more years
  without requiring significant redesign: As existing applications manipulate more and more data,
  and as they are incrementally updated to include more code for new features, performance-sensitive
  operations need to continue to fit into cache. As the Depression-era old-timers will be quick to
  remind you, âCache is king.â

(Aside: Hereâs an anecdote to demonstrate âspace is speedâ that recently hit my compiler team. The
 compiler uses the same source base for the 32-bit and 64-bit compilers; the code is just compiled
 as either a 32-bit process or a 64-bit one. The 64-bit compiler gained a great deal of baseline
 performance by running on a 64-bit CPU, principally because the 64-bit CPU had many more registers
 to work with and had other code performance features. All well and good. But what about data? Going
 to 64 bits didnât change the size of most of the data in memory, except that of course pointers in
 particular were now twice the size they were before. As it happens, our compiler uses pointers much
 more heavily in its internal data structures than most other kinds of applications ever would.
 Because pointers were now 8 bytes instead of 4 bytes, a pure data size increase, we saw a
 significant increase in the 64-bit compilerâs working set. That bigger working set caused a
 performance penalty that almost exactly offset the code execution performance increase weâd gained
 from going to the faster processor with more registers. As of this writing, the 64-bit compiler
 runs at the same speed as the 32-bit compiler, even though the source base is the same for both and
 the 64-bit processor offers better raw processing throughput. Space is speed.)

But cache is it. Hyperthreading and multicore CPUs will have nearly no impact on most current
applications.

So what does this change in the hardware mean for the way we write software? By now youâve probably
noticed the basic answer, so letâs consider it and its consequences.

<What This Means For Software: The Next Revolution>

In the 1990s, we learned to grok objects. The revolution in mainstream software development from
structured programming to object-oriented programming was the greatest such change in the past 20
years, and arguably in the past 30 years. There have been other changes, including the most recent
(and genuinely interesting) naissance of web services, but nothing that most of us have seen during
our careers has been as fundamental and as far-reaching a change in the way we write software as the
object revolution.

Until now.

Starting today, the performance lunch isnât free any more. Sure, there will continue to be generally
applicable performance gains that everyone can pick up, thanks mainly to cache size improvements.
But if you want your application to benefit from the continued exponential throughput advances in
new processors, it will need to be a well-written concurrent (usually multithreaded) application.
And thatâs easier said than done, because not all problems are inherently parallelizable and because
concurrent programming is hard.

I can hear the howls of protest: âConcurrency? Thatâs not news! People are already writing
concurrent applications.â Thatâs true. Of a small fraction of developers.

Remember that people have been doing object-oriented programming since at least the days of Simula
in the late 1960s. But OO didnât become a revolution, and dominant in the mainstream, until the
1990s. Why then? The reason the revolution happened was primarily that our industry was driven by
requirements to write larger and larger systems that solved larger and larger problems and exploited
the greater and greater CPU and storage resources that were becoming available. OOPâs strengths in
abstraction and dependency management made it a necessity for achieving large-scale software
development that is economical, reliable, and repeatable.

Concurrency is the next major revolution in how we write software

Similarly, weâve been doing concurrent programming since those same dark ages, writing coroutines
and monitors and similar jazzy stuff. And for the past decade or so weâve witnessed incrementally
more and more programmers writing concurrent (multi-threaded, multi-process) systems. But an actual
revolution marked by a major turning point toward concurrency has been slow to materialize. Today
the vast majority of applications are single-threaded, and for good reasons that Iâll summarize in
the next section.

By the way, on the matter of hype: People have always been quick to announce âthe next software
development revolution,â usually about their own brand-new technology. Donât believe it. New
technologies are often genuinely interesting and sometimes beneficial, but the biggest revolutions
in the way we write software generally come from technologies that have already been around for some
years and have already experienced gradual growth before they transition to explosive growth. This
is necessary: You can only base a software development revolution on a technology thatâs mature
enough to build on (including having solid vendor and tool support), and it generally takes any new
software technology at least seven years before itâs solid enough to be broadly usable without
performance cliffs and other gotchas. As a result, true software development revolutions like OO
happen around technologies that have already been undergoing refinement for years, often decades.
Even in Hollywood, most genuine âovernight successesâ have really been performing for many years
before their big break.

Concurrency is the next major revolution in how we write software. Different experts still have
different opinions on whether it will be bigger than OO, but that kind of conversation is best left
to pundits. For technologists, the interesting thing is that concurrency is of the same order as OO
both in the (expected) scale of the revolution and in the complexity and learning curve of the
technology.

<Benefits and Costs of Concurrency>

There are two major reasons for which concurrency, especially multithreading, is already used in
mainstream software. The first is to logically separate naturally independent control flows; for
example, in a database replication server I designed it was natural to put each replication session
on its own thread, because each session worked completely independently of any others that might be
active (as long as they werenât working on the same database row). The second and less common reason
to write concurrent code in the past has been for performance, either to scalably take advantage of
multiple physical CPUs or to easily take advantage of latency in other parts of the application; in
my database replication server, this factor applied as well and the separate threads were able to
scale well on multiple CPUs as our server handled more and more concurrent replication sessions with
many other servers.

There are, however, real costs to concurrency. Some of the obvious costs are actually relatively
unimportant. For example, yes, locks can be expensive to acquire, but when used judiciously and
properly you gain much more from the concurrent execution than you lose on the synchronization, if
you can find a sensible way to parallelize the operation and minimize or eliminate shared state.

Perhaps the second-greatest cost of concurrency is that not all applications are amenable to
parallelization. Iâll say more about this later on.

Probably the greatest cost of concurrency is that concurrency really is hard: The programming model,
         meaning the model in the programmerâs head that he needs to reason reliably about his
         program, is much harder than it is for sequential control flow.

Everybody who learns concurrency thinks they understand it, ends up finding mysterious races they
thought werenât possible, and discovers that they didnât actually understand it yet after all. As
the developer learns to reason about concurrency, they find that usually those races can be caught
by reasonable in-house testing, and they reach a new plateau of knowledge and comfort. What usually
doesnât get caught in testing, however, except in shops that understand why and how to do real
stress testing, is those latent concurrency bugs that surface only on true multiprocessor systems,
       where the threads arenât just being switched around on a single processor but where they
       really do execute truly simultaneously and thus expose new classes of errors. This is the
       next jolt for people who thought that surely now they know how to write concurrent code: Iâve
       come across many teams whose application worked fine even under heavy and extended stress
       testing, and ran perfectly at many customer sites, until the day that a customer actually had
       a real multiprocessor machine and then deeply mysterious races and corruptions started to
       manifest intermittently. In the context of todayâs CPU landscape, then, redesigning your
       application to run multithreaded on a multicore machine is a little like learning to swim by
       jumping into the deep endâgoing straight to the least forgiving, truly parallel environment
       that is most likely to expose the things you got wrong. Even when you have a team that can
       reliably write safe concurrent code, there are other pitfalls; for example, concurrent code
       that is completely safe but isnât any faster than it was on a single-core machine, typically
       because the threads arenât independent enough and share a dependency on a single resource
       which re-serializes the programâs execution. This stuff gets pretty subtle.

The vast majority of programmers today donât grok concurrency, just as the vast majority of
programmers 15 years ago didnât yet grok objects

Just as it is a leap for a structured programmer to learn OO (whatâs an object? whatâs a virtual
    function? how should I use inheritance? and beyond the âwhatsâ and âhows,â why are the correct
    design practices actually correct?), itâs a leap of about the same magnitude for a sequential
programmer to learn concurrency (whatâs a race? whatâs a deadlock? how can it come up, and how do I
    avoid it? what constructs actually serialize the program that I thought was parallel? how is the
    message queue my friend? and beyond the âwhatsâ and âhows,â why are the correct design practices
    actually correct?).

The vast majority of programmers today donât grok concurrency, just as the vast majority of
programmers 15 years ago didnât yet grok objects. But the concurrent programming model is learnable,
            particularly if we stick to message- and lock-based programming, and once grokked it
            isnât that much harder than OO and hopefully can become just as natural. Just be ready
            and allow for the investment in training and time, for you and for your team.

(I deliberately limit the above to message- and lock-based concurrent programming models. There is
 also lock-free programming, supported most directly at the language level in Java 5 and in at least
 one popular C++ compiler. But concurrent lock-free programming is known to be very much harder for
 programmers to understand and reason about than even concurrent lock-based programming. Most of the
 time, only systems and library writers should have to understand lock-free programming, although
 virtually everybody should be able to take advantage of the lock-free systems and libraries those
 people produce. Frankly, even lock-based programming is hazardous.)

<What It Means For Us>

Okay, back to what it means for us.

1. The clear primary consequence weâve already covered is that applications will increasingly need
to be concurrent if they want to fully exploit CPU throughput gains that have now started becoming
available and will continue to materialize over the next several years. For example, Intel is
talking about someday producing 100-core chips; a single-threaded application can exploit at most
1/100 of such a chipâs potential throughput. âOh, performance doesnât matter so much, computers just
keep getting fasterâ has always been a naÃ¯ve statement to be viewed with suspicion, and for the near
future it will almost always be simply wrong.

Applications will increasingly need to be concurrent if they want to fully exploit continuing
exponential CPU throughput gains

Efficiency and performance optimization will get more, not less, important

Now, not all applications (or, more precisely, important operations of an application) are amenable
to parallelization. True, some problems, such as compilation, are almost ideally parallelizable. But
others arenât; the usual counterexample here is that just because it takes one woman nine months to
produce a baby doesnât imply that nine women could produce one baby in one month. Youâve probably
come across that analogy before. But did you notice the problem with leaving the analogy at that?
Hereâs the trick question to ask the next person who uses it on you: Can you conclude from this that
the Human Baby Problem is inherently not amenable to parallelization? Usually people relating this
analogy err in quickly concluding that it demonstrates an inherently nonparallel problem, but thatâs
actually not necessarily correct at all. It is indeed an inherently nonparallel problem if the goal
is to produce one child. It is actually an ideally parallelizable problem if the goal is to produce
many children! Knowing the real goals can make all the difference. This basic goal-oriented
principle is something to keep in mind when considering whether and how to parallelize your
software.

2. Perhaps a less obvious consequence is that applications are likely to become increasingly
CPU-bound. Of course, not every application operation will be CPU-bound, and even those that will be
affected wonât become CPU-bound overnight if they arenât already, but we seem to have reached the
end of the âapplications are increasingly I/O-bound or network-bound or database-boundâ trend,
    because performance in those areas is still improving rapidly (gigabit WiFi, anyone?) while
    traditional CPU performance-enhancing techniques have maxed out. Consider: Weâre stopping in the
    3GHz range for now. Therefore single-threaded programs are likely not to get much faster any
    more for now except for benefits from further cache size growth (which is the main good news).
    Other gains are likely to be incremental and much smaller than weâve been used to seeing in the
    past, for example as chip designers find new ways to keep pipelines full and avoid stalls, which
    are areas where the low-hanging fruit has already been harvested. The demand for new application
    features is unlikely to abate, and even more so the demand to handle vastly growing quantities
    of application data is unlikely to stop accelerating. As we continue to demand that programs do
    more, they will increasingly often find that they run out of CPU to do it unless they can code
    for concurrency.

There are two ways to deal with this sea change toward concurrency. One is to redesign your
applications for concurrency, as above. The other is to be frugal, by writing code that is more
efficient and less wasteful. This leads to the third interesting consequence:

3. Efficiency and performance optimization will get more, not less, important. Those languages that
already lend themselves to heavy optimization will find new life; those that donât will need to find
ways to compete and become more efficient and optimizable. Expect long-term increased demand for
performance-oriented languages and systems.

4. Finally, programming languages and systems will increasingly be forced to deal well with
concurrency. The Java language has included support for concurrency since its beginning, although
mistakes were made that later had to be corrected over several releases in order to do concurrent
programming more correctly and efficiently. The C++ language has long been used to write heavy-duty
multithreaded systems well, but it has no standardized support for concurrency at all (the ISO C++
    standard doesnât even mention threads, and does so intentionally), and so typically the
concurrency is of necessity accomplished by using nonportable platform-specific concurrency features
and libraries. (Itâs also often incomplete; for example, static variables must be initialized only
    once, which typically requires that the compiler wrap them with a lock, but many C++
    implementations do not generate the lock.) Finally, there are a few concurrency standards,
    including pthreads and OpenMP, and some of these support implicit as well as explicit
    parallelization. Having the compiler look at your single-threaded program and automatically
    figure out how to parallelize it implicitly is fine and dandy, but those automatic
    transformation tools are limited and donât yield nearly the gains of explicit concurrency
    control that you code yourself. The mainstream state of the art revolves around lock-based
    programming, which is subtle and hazardous. We desperately need a higher-level programming model
    for concurrency than languages offer today; I'll have more to say about that soon.

<Conclusion>

If you havenât done so already, now is the time to take a hard look at the design of your
application, determine what operations are CPU-sensitive now or are likely to become so soon, and
identify how those places could benefit from concurrency. Now is also the time for you and your team
to grok concurrent programmingâs requirements, pitfalls, styles, and idioms.

A few rare classes of applications are naturally parallelizable, but most arenât. Even when you know
exactly where youâre CPU-bound, you may well find it difficult to figure out how to parallelize
those operations; all the most reason to start thinking about it now. Implicitly parallelizing
compilers can help a little, but donât expect much; they canât do nearly as good a job of
parallelizing your sequential program as you could do by turning it into an explicitly parallel and
threaded version.

Thanks to continued cache growth and probably a few more incremental straight-line control flow
optimizations, the free lunch will continue a little while longer; but starting today the buffet
will only be serving that one entrÃ©e and that one dessert. The filet mignon of throughput gains is
still on the menu, but now it costs extraâextra development effort, extra code complexity, and extra
testing effort. The good news is that for many classes of applications the extra effort will be
worthwhile, because concurrency will let them fully exploit the continuing exponential gains in
processor throughput.


# ============================================================================
#{
==============================================================================
*kt_linux_core_290*	ref: concurrency in C++

{what-is-concurrency}
There is genuine or hardware concurrency or illusion of concurrency. What's new is that the
increased computing power of these machine comes not from running a single task faster but from
running multiple tasks in parallel.

<parallelism>
The task parallelism is to divide a single task into parts and run each in parallel, thus reducing
the total runtime. It can be complex since there may be many dependencies.

The data parallelism is that each thread performs the same operation on different parts of the data.
Good scalability; many hands make light work. There's a different focus in which more data can be
processed in the same amount of time.


{when-use-concurrency}
The use of concurrency is like any other optimisation strategy. Therefore it is only worth doing for
those performance critical parts of the application where there is the potential for mesurable gain.

{two-approaches-to-concurrency}
<use-multiple-process>
The downsides are 
o communication between processes.
o inherent overhead such as time to launch and resource in OS

The upsides are:
o safer code than threads
o can extend over a network

<use-multiple-thread>
The low overhead with launching and communicating between threads. So C++11 do not provide any
support for process and only support for threads.


{C++11}
o Allow writing portable multithreaded code without relying on platform-specific extensions.
o There is [abstraction penalty] compared to using the underlying low-level facilities directly.

<code-example>
#include <iostream>
#include <thread>

void hello()
{
  std::cout << "hello con world\n";
}

int main(int argc, char** argv)
{
  std::thread t(hello);
  t.join();
  // std::cout << "end of main" << std::endl;
}

If build and run like below:
kt@kt-ub-vb:~/work$ g++ -g -std=c++0x use-thread.cpp 
kt@kt-ub-vb:~/work$ ./a.out 
terminate called after throwing an instance of 'std::system_error'
  what():  Operation not permitted
Aborted (core dumped)

If build and run with pthread option, then works fine. This is known problem in g++.           
kt@kt-ub-vb:~/work$ g++ -g -std=c++0x -pthread use-thread.cpp 


==============================================================================
*kt_linux_core_291*	ref: concurrency in C++

{std-thread-basic}
<launch>
The std::thread works with any [callable] type; function object and lambda.

void do_some_work();
std::thread my_thread(do_some_work);

or

class background_task
{
  public:
    void operator() () const
    {
      do_something();
      do_something_else();
    }
};

background_task f;
std::thread my_thread(f);

Why callable? In this case, the supplied function object is copied into the storage belonging to the
created thread and invoked from there.

<join-and-detach>
If don't decide whether to join or to detach it before std::thread object is destroyed then program
is terminated since the std::thread destructor calls std::terminate(). Ture even in the presence of
exceptions.

The join() cleans up any storage associated with the thread so std::thread object isn't associated
with any thread. Once this, joinable() will return false.
{Q} what will happen when call join() twice on the same?

When detach a thread, make sure that the data accessed by the thread is vaild until the thread has
finished with it. For example, creat a thread within a function with thread function hold pointers
or reference to local varaibles. Bad idea and avoid this. 

void oops()
{
  int some_local_state = 0;
  T my_func(some_local_state);   // T(int&)
  std::thread my_thread(my_func);
  my_thread.detach();
}

<join-raii>
To avoid program being terminated when an exception is thrown, accidental lifetime problems, how?
Can use try and catch but verbose and easy to get it wrong. 

If you wan to do particular action for all possible exit path, whether normal or exceptional, use
raii.

class thread_guard
{
  std::thread& t;

  public:
  explicit thread_guard( std::thread& t_ ): t(t_) {}
  ~thread_guard()
  {
    if( t.joinable() )
    {
      t.join();
    }
  }

  thread_guard( thread_guard const& )=delete;
  thread_guard& operator=( thread_guard const&)=delete;
};

void oops()
{
  int some_local_state = 0;
  T my_func(some_local_state);         // T(int&)
  std::thread my_thread(my_func);
  thread_guard g(t);                   // <DN>

  do_something_in_current_thread();    // main thread continue to run
}


==============================================================================
*kt_linux_core_292*	ref: concurrency in C++

# ============================================================================
#{
==============================================================================
*kt_linux_core_300*	case: own semaphore and mutex class using pthred cond var

POSIX semaphore are system calls which means expensive. Is it possible to implement semaphore without it?

{class-semaphore}

This is for linux. When count is 0, waits and there is no upper limit. Also see that use one mutex
with many condition variables for semaphores.


{util-class}

Just to provide util funcs to all instances since these are static. Also SetPriority is not used.

class CThreadSelf
{
private:
	CThreadSelf(void) {}

public:
	// Returns the ID of the current executing thread.
	'static' int Id(void)
	{ return (int)pthread_self(); }

	'static' bool SetPriority(int priority);
	{
#if defined _LINUX

		  switch (priority)
		  {
			 case CThread::PRIORITY_HIGH: // [note] class type member
				 setpriority(PRIO_PROCESS, pthread_self(), -10); // [note] man setpriority
		       // param.sched_priority = 60;
				 break;
			 case CThread::PRIORITY_NORMAL:
				 setpriority(PRIO_PROCESS, pthread_self(), 0);
		       // param.sched_priority = 50;
				 break;
			 case CThread::PRIORITY_LOW:
		       // param.sched_priority = 40;
				 setpriority(PRIO_PROCESS, pthread_self(), 10);
				 break;
			 default:
				 return false;
		  }

#elif defined _WIN32
	}
};


{semaphore} [KT] the case uses containment(composition) to have implementation.

Use init count but no max count. 0 means to wait and other values means it is okay to get. Used as a
class memeber.

{Q} why need this? code says it calls sched_yield whenever sem count reaches 16.

#define	CONFIG_MAXIMUM_YIELD_COUNTER 16
static unsigned char semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;

pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER; [KT] this is global

struct PSemaphore
{
	pthread_cond_t  cond;
	int             count;
};

class Semaphore
{
	 private:
	 	PSemaphore* m_id;

	 public:
		Semaphore() { m_id = NULL; }
		virtual ~Semaphore() { assert( FlagCreate() == false); }

		bool Create(int count) // initial count
		{
			 pthread_mutex_lock(&mtx);

			 assert( FlagCreate() == false );

			 pthread_cond_t cond = PTHREAD_COND_INITIALIZER;

			 m_id = 'new' PSemaphore;	// new and m_id is not null
			 assert( m_id != NULL );
			 
			 m_id->cond = cond;  // [KT] is it okay as it is local variable?
			 m_id->count = count;

			 pthread_mutex_unlock(&mtx);

			 return m_id != NULL;
		}

		// return true when created
		bool FlagCreate() { return m_id != NULL; }

		virtual void Destory(void)
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  pthread_cond_destroy(&m_id->cond);
			  delete m_id;

			  m_id = NULL;

			  pthread_mutex_unlock(&mtx);
		}

		void Take()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  while (m_id->count <= 0)
			  {
				  pthread_cond_wait(&m_id->cond, &mtx);
			  }

			  m_id->count--;

			  pthread_mutex_unlock(&mtx);
		}

		void Give()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  m_id->count++;

			  pthread_cond_signal(&m_id->cond);

			  pthread_mutex_unlock(&mtx);

			  if (!semCounter--) {
				  sched_yield();
				  semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;
			  }
		}

		void Try(unsigned long msec = 0)
		{
			  if (msec == (unsigned long) INFINITY)
			  {
				  Take();

				  return true;
			  }

			  pthread_mutex_lock(&TimeMutex);
			  ASSERT(FlagCreate() == true);

			  struct timeval  now;
			  struct timespec timeout;
			  int             ret = 0;
			  bool            tf;


			  if (msec == 0)
			  {
				  if (m_id->count <= 0)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }
			  else
			  {
				  while ((m_id->count <= 0) && (ret != ETIMEDOUT))
				  {
					  gettimeofday(&now, NULL);
					  timeout.tv_sec  = now.tv_sec + msec / 1000;
					  timeout.tv_nsec = now.tv_usec + msec % 1000 * 1000;

					  while (timeout.tv_nsec > 1000000)
					  {
						  timeout.tv_nsec -= 1000000;
						  timeout.tv_sec++;
					  }

					  timeout.tv_nsec *= 1000;

					  ret = pthread_cond_timedwait(&m_id->cond, &TimeMutex, &timeout);
				  }

				  if (ret == ETIMEDOUT)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }

			  pthread_mutex_unlock(&TimeMutex);

			  return tf;
		}
};


{use-of-semaphore-one}

To make sure that an user can set prio once a thread is created.

class CThread
{
   PCSemaphore m_pidSync;

   Create()
   {
      m_pidSync.Create(0);
   }

   bool PCThread::SetPriority(int priority)
   {
      ASSERT(FlagCreate() == true);

      if (m_pid == -1)
      {
         m_pidSync.Take();
      }
      ...
   }

   inline void CThreadRun(CThread* thread)
   {
      thread->m_sync[0].Take();

      thread->m_pid = pthread_self();
      thread->m_pidSync.Give();

      thread->t_Main(); [KT] while loop on event get()

      thread->m_sync[1].Give();

      return;
   }
};


{mutex} 

CDerivedA: CMutex
 - thread   - Semaphore : uses global mutex

CDerivedB: CMutex
 - thread   - Semaphore : uses global mutex

Using sync always happens in the same thread. The case uses inheritance to have implementation. This
is based on the fact that mutex is a binary semaphore. Created with 1. Used to give the derived
class the lock/unlock feature to control interface access. That is, sync feature to class objects.
If need other use of sem, then have sem as a member.

class CMutex
{
	 private:
		  PCSemaphore 	m_sem; // [note] use of semaphore
		  int				m_tid;
		  int				m_count;

	 public:
	 	  // [note] no ctor?
	 	  virtual ~Mutex() { assert( FlagCreate() == false); }

		  bool Create(void)
		  {
				assert(FlagCreate() == false);

				if (m_sem.Create(1) == false) // [KT] create a sem
				{
					return false;
				}

				m_threadId = 0;

				return true;
		  }

		  virtual void Destroy(void)
		  {
				ASSERT(FlagCreate() == true);
				m_sem.Destroy();
		  }

		  bool FlagCreate(void) { return m_sem.FlagCreate(); }

		  void Lock(void)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;
				  return;
				}

				m_sem.Take();

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

		  }

		  bool Unlock(void);
		  {
				assert(FlagCreate() == true);

				if (m_threadId != CThreadSelf::Id())
				{
					return false;
				}

				m_count--;

				if (m_count > 0)
				{
				  return true;
				}

				m_threadId = 0;

				m_sem.Give();

				return true;
		  }

		  bool Try(unsigned long msec = 0)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;

				  return true;
				}

				if (m_sem.Try(msec) == false)
				{
				  return false;
				}

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

				return true;

		  }
};	

{cqueue}

struct PTEvent
{
	friend class PCQueue;
	friend class PCTask;

private:
	void* sync;

public:
	class PCHandler* receiver; //!< The Pointer to the handler that receives the event
	unsigned long    type;     //!< The event type

	//! Parameters of an event
	/*!
	 * The union of event parameters are 8-bytes long. That is, it can carry
	 * 2 long integers, 4 short integers, or 8 characters.
	 */
	union
	{
		long  l[2];
		short s[4];
		char  c[8];
	} param;
};

class CQueue : public CMutex
{
	private:
      PCSemaphore m_sem; // [KT] used to say that events are avaiable
		PTEvent     m_event[CONFIG_QUEUE_SIZE];

	bool CQueue::Create(void)
	{
		ASSERT(FlagCreate() == false);

		if (CMutex::Create() == false)
		{ return false; }

		if (m_sem.Create(0) == false)
		{
			CMutex::Destroy();
			return false;
		}

		m_in   = 0;		// [KT] this is {queue-contiguous-implementation} in *kt_dev*
		m_out  = 0;		// in(tail), out(head), size(count)
		m_size = 0;

		return true;
	}

	bool CQueue::Put(PTEvent* event, bool sync=false, bool priority=false)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		PCSemaphore sem;

		if (sync == false)
		{
			event->sync = NULL;
		}
		else
		{
			if (sem.Create(0) == false)
			{
				return false;
			}

			event->sync = &sem;
		}

		Lock();	// CMutex::Lock();

		int size;

		size = (priority == false) ? CONFIG_EMEGENCY_QUEUE_SIZE(16) : 0;
		size += m_size;

		if (size >= CONFIG_QUEUE_SIZE(256) )
		{
			Unlock();

			if (sync == true)
			{
				sem.Destroy();
			}

			PCDebug::Print("ERROR: Event queue full");

			return false;
		}

		if (priority == true)
		{
			m_out          = (m_out + CONFIG_QUEUE_SIZE - 1) % CONFIG_QUEUE_SIZE;
			m_event[m_out] = *event;
			m_size++;
		}
		// [KT] copy in event and inc count
		else
		{
			m_event[m_in] = *event;
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
			m_size++;
		}

		m_sem.Give();

      // [KT] As see how Get() uses m_sem, use m_sem to see if event are avaiable like a count or
      // length and then if there are call Lock() to lock access to this objects. May have some
      // performance gain from this.
		
      Unlock();	

		if (sync == true)
		{
			sem.Take();
			sem.Destroy();
		}

		return true;
	}

	bool PCQueue::Get(PTEvent* event, unsigned long msec = INFINITY)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		while (true)
		{
			if (m_sem.Try(msec) == false)
			{
				return false;
			}

			Lock();

			if (m_size == 0)
			{
				Unlock();

				continue;
			}

			*event = m_event[m_out];
			m_out  = (m_out + 1) % CONFIG_QUEUE_SIZE;
			m_size--;

			Unlock();

			return true;
		}
	}

};


{when-seek-one-specific}

{Q} This suggest that there are events for all(broadcast) and for specific ones. Move all event
between [head+1 ... tail-1] after tail. why? priority?

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver)
{ }

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver, unsigned long type)
{
	ASSERT(FlagCreate() == true);

	bool done = 0;

	Lock();

	unsigned long size = m_size;

	while (size-- != 0)
	{
		if (done == false && m_event[m_out].receiver == receiver && m_event[m_out].type == type)
		{
			*event = m_event[m_out];
			m_size--;
			done = true;
		}
		else
		{
			m_event[m_in] = m_event[m_out];
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
		}

		m_out = (m_out + 1) % CONFIG_QUEUE_SIZE;
	}

	if (done)
	{
		m_sem.Try();
	}

	Unlock();

	return done;
}

This q implementation uses q per thread. The different approach is to have q that threads share.
See *kt_linux_core_014* for msg q between threads


==============================================================================
*kt_linux_core_301*  case: use of mutex and thread class

This is case study using Mutex, Queue and Thread classes.

{CThread}

'inline' void CThreadRun(CThread* thread)
{
	thread->m_sync[0].Take();  // [KT] wait until signaled

#ifdef	_LINUX
	thread->m_pid = pthread_self();
	thread->m_pidSync.Give();
#endif

	thread->t_Main();

	thread->m_sync[1].Give();  // [KT] no use

	return;
}

'static' void* _Process(void* param)
{
	CThreadRun((CThread*)param);

	return NULL;
}

class CThread
{
private:

	int         m_id;
	int			m_pid;
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];

	friend void CThreadRun(CThread* thread); // [KT] inline friend

protected:

	virtual void t_Main(void) = 0;
	/*!< This function is the thread main virtual function.  As soon as a thread starts, this
	 * function is called.  When this function returns, the thread is terminated.
	 *
	 * You have to define this function when you define a new class that inherits CThread class.
	 */

public:

	//! Configuration constants
	enum PTConfigType
	{
		CONFIG_STACK_SIZE = 4096
	};

	enum PTPriorityType
	{
		PRIORITY_HIGH,
		PRIORITY_NORMAL,
		PRIORITY_LOW,
	};

	virtual ~CThread(void) { ASSERT(FlagCreate() == false); }

	bool Create(const char* name, unsigned long stackSize = CONFIG_STACK_SIZE);
	 {
		 ASSERT(FlagCreate() == false);

		 if (m_sync[0].Create(0) == false)
		 {
			 return false;
		 }

		 if (m_sync[1].Create(0) == false)
		 {
			 m_sync[0].Destroy();
			 return false;
		 }

#elif defined _LINUX

		 pthread_attr_t attr;
		 pthread_attr_init(&attr);
		 pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
		 pthread_attr_setstacksize(&attr, stackSize);
		 //struct sched_param schedParam;
		 //schedParam.sched_priority = 50;
		 //pthread_attr_getschedparam(&attr, &schedParam);
		 //pthread_attr_setschedpolicy(&attr, SCHED_RR);

		 m_pid = -1;
		 m_pidSync.Create(0);

		 if (pthread_create((pthread_t*)&m_id, &attr, _Process, this) != 0)
		 {
			 pthread_attr_destroy(&attr);
			 
			 ASSERT(!"CThread not created");

			 m_sync[0].Destroy();
			 m_sync[1].Destroy();

			 return false;
		 }

		 pthread_attr_destroy(&attr); 
		 
		 m_sync[0].Give();   // [KT] now a thread can run
		 return true;
	 }

	//! Check if the instance was created
	bool FlagCreate(void) { return m_sync[0].FlagCreate(); }
	//! Destroy the instance
	virtual void Destroy(void);

	//! Returns the ID of the CThread.
	int Id(void)
	{
		 ASSERT(FlagCreate() == true);
		 return m_id;
	}

	//! Set the priority value for the thread
	bool SetPriority(int priority);
};

The class hiarachy is:

class CThread
{
private:
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];
};

class CQueue : public CMutex

class CTask: private CThread, public CQueue, public CHandler
{
   private:
      void t_Main(void);

   protected:
      virtual bool t_Create(void);
};

class CSIEngineBase : public PCTask

class CSIVoiceEngine : public CSIEngineBase

class CSIEngineManager


<run>

CThread:
	CThreadRun: 
      thread->t_Main();	[pure-virtual] [main-start]

CTask:
              	t_Main()                 [main-end] {template-method} in kt_dev_txt
					{
				   	t_Create();                      [virtual] [create-start]
                 	while (ExecuteEvent() == true)   [thread-loop]
						//	virtual bool ExecuteEvent
						//	{
						//		 PTEvent event;
						//		 Get(&event, m_msec); [copy-in-event]
						//		 event.receiver->OnEvent(&event);
						//	}

					  	t_Destory();
					}

CSIEngineBase:
                                                 
CSIVoiceEngine:
                                                  t_Create() [create-end]
																     SendSelfEvent( OWN_EVENT_TYPE );

<event>

CHandler:
	protected:
	virtual bool t_OnEvent(const PTEvent* event) = 0;

	public:
	inline bool OnEvent
	{
      t_OnEvent() [pure-virtual] [event-start]
	}

CTask: public CHandler
	No OnEvent which means use CHandler one

	protectd:
	bool PCTask::t_OnEvent(const PTEvent* event)
	{
		has default basic event handling
	}

	static Send( PTEvent* event, bool sync = false, bool priority = false);
	  [to-other-task]
	  if ((sync == false) || (event->receiver->Task()->Id() != PCThreadSelf::Id()))
	  {
		 return event->receiver->Task()->Put(event, sync, priority);
		 {
			  In CQueue::Put, use copy-ctor of event structure to copy it into receiver's taks
			  m_event[].

			  m_event[i] *event;
		 }
	  }
	  [self] ends up with a func call
	  event->receiver->OnEvent(event);
		   
CSIEngineBase:
   public:
      bool SendSelfEvent(int nEventType)
      {
         PTEvent evt;
         evt.receiver = this; [event-to-self]
         evt.type = nEventType;
         Send(&evt);   // CTask::
      }

  protected:
      virtual bool t_OnEvent(const PTEvent* event)
      { return true; }

		virtual bool t_OnEvent( PTEvent* ) [event-end]
      {
		   PCTask::t_OnEvent(event); {hook-operation} in {template-method}
		   t_ProcessEvent(event); [virtual] [start]
      }

CSIVoiceEngine:
   SendSelfEvent( OWN_EVENT_TYPE );  // CSIEngineBase::

   virtual bool t_ProcessEvent(event);           [end]
   {
	   handle OWN_EVENT_TYPE;
   }


<create>

CThread:
   Create:
	  pthread_create

CTask:
	public:
   Create()
	  PCQueue::Create();
	  PCHandler::Create(this);
	  CThread::Create(stackSize);

CSIEngineBase: public CTask
	public:
   Create(const char* name)
      return CTask::Create(name);					[create]

CSIVoiceEngine: public CSIEngineBase
   no Create:

// {design-note}
// This can be a application manager which creates all applications and call Create() on them. This
// is platform wide and each application can override t_Create and init their own thing without
// knowing Create() calls made from outside.

class CSIEngineManager:
{
	 private:
	 CSIEngineBase* m_pVoiceEngine;

	 static CSIEngineManager* CSIEngineManager::GetInstance(void) {factory-method}
	 {
		 if(m_EngineManagerInstance == NULL)
		 {
			 m_EngineManagerInstance = 'new' CSIEngineManager;
		 }

		 return m_EngineManagerInstance;
	 }

	 CSIEngineManager::GetEngineInstance
	 {
		  m_pVoiceEngine =  'new' CSIVoiceEngine;
		  m_pVoiceEngine->Create("SIVoiceEngine"); // [create] and use inherited implementation
	 }
}

From review, t_ prefix means a primitive operation and to be overridden and all works are using the
most derived class object. Therefore, dreived version will be used for virtuals as shown
{template-method}


==============================================================================
*kt_linux_core_302*  case: analysis of 200 and 201 case

{how-this-work}

CUserClass instance:
                                       
CUserClass {
   // Eash CTask has a Q and thread
   CTask { CQueue, CHandler : CQueue { CMutex }
      CThread
         : pthread( staic _Process )
      }
}
  
static _Process(this) will run the derived class CTask t_Main which has a message loop. So this
_Process is a template code for all threads. 

Get(&event); in which all threads use the same code for thread routine

CUserB instance:
                                       
CUserB {
   CTask {
      CThread
         : pthread( staic _Process )
      }
}

Mutex class via inheritance:
Mutex #01     Mutex #02    Mutex #03     Mutex #04    Mutex #04  
(Semaphore)   (Semaphore)  (Semaphore)   (Semaphore)  (Semaphore)

Sem member in a class:
Sepmphore #01  Sepmphore #02 ...


Regarding q, each task has a q and other task can call put to insert a mesg to receiver's q.

{Q} Eash has its own pthread cond in Semaphore but all use a single global mutex for signaling. How
about performance? Is it better solution?


==============================================================================
*kt_linux_core_303*  case: msg q between threads

This uses stl q and sems to read, write and count(length) lock:


/** Maximum length of message queue name */
#define MQ_NAME_LENGTH 5

/** Queue Magic identifier Corresponds to ASCII QuEu*/
#define MQ_MAGIC 0x51754575

typedef struct PFMMessageQueueInfo_t_
{
    char                name[ MQ_NAME_LENGTH ];
    PCSemaphore         *readsem;
    PCMutex             *writeLock;
    PCMutex             *readLock;
    uint32_t            magic;
    std::queue<SPfmMessage>* container;
} PFMMessageQueueInfo_t;


extern "C"
{
///////////////////////////////////////////////////
// PFM Queue Create
///////////////////////////////////////////////////
HPfmQueue
pfmQueueCreate(const char* name, uint32_t max_size)
{
    PFMMessageQueueInfo_t *qptr;

    // Allocate queue controll structure
    qptr = (PFMMessageQueueInfo_t *)pfmMalloc( sizeof(PFMMessageQueueInfo_t) );
    if(!qptr)
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc\n");
        return PFM_NULL_HANDLE;
    }

    // create storage container
    qptr->container = new std::queue<SPfmMessage>();
    if (qptr->container == NULL)
    {
        fprintf(stderr,
            "pfmQueueCreate failed to alloc storage\n");
        pfmFree(qptr);
        return PFM_NULL_HANDLE;
    }

    // Create & initialize Read mutex for read serialization
    qptr->readLock = new PCMutex;
    if( qptr->readLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc read mutex\n");

        delete qptr->container;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    qptr->readLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init read mutex\n");

        delete qptr->container;
        delete qptr->readLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create & initialize Write mutex for write serialization
    qptr->writeLock = new PCMutex;
    if( qptr->writeLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }
    qptr->writeLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create and initialize read semaphore
    qptr->readsem = new PCSemaphore;
    if( qptr->readsem == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc semaphore\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    //Initialize read semaphore so it will "block" on try.
    qptr->readsem->Create(0);
    if( !qptr->readsem->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc sem\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        delete qptr->readsem;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Copy semaphore name
    if( !name )
    {
        char nameTmp[] = "SEM";
        PCString::Copy( qptr->name,nameTmp,MQ_NAME_LENGTH);
    }
    else
    {
        PCString::Copy( qptr->name,name,MQ_NAME_LENGTH);
    }

    qptr->name[ (MQ_NAME_LENGTH-1) ] = (char)NULL;
    qptr->magic = MQ_MAGIC;

    return (HPfmQueue)qptr;
}

///////////////////////////////////////////////////
// PFM Queue Destroy
///////////////////////////////////////////////////
pfmerr_t
pfmQueueDestroy(HPfmQueue h)
{
    // Validate input
    if( h == PFM_NULL_HANDLE)
    {
        pfmAssert( h != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)h;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Mark queue as invalid.
    qptr->magic = 0;

    // Release Reader (if exists) & Grab read/write locks.
    // This shall prevent "use while destruct" scenario.
    // Mutexes can be destroyed while locked.
    qptr->readsem->Give();
    qptr->readLock->Lock();
    qptr->writeLock->Lock();

    if( qptr->readsem->FlagCreate() )
    {
        qptr->readsem->Destroy();
    }
    if( qptr->readLock->FlagCreate() )
    {
        qptr->readLock->Destroy();
    }
    if( qptr->writeLock->FlagCreate() )
    {
        qptr->writeLock->Destroy();
    }

    delete qptr->readsem;
    delete qptr->readLock;
    delete qptr->writeLock;

    delete qptr->container;

    pfmFree( qptr );

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Receive
///////////////////////////////////////////////////
pfmerr_t
pfmQueueReceive(HPfmQueue q, uint32_t timeout_ms, SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    unsigned long readLockTickStart;
    unsigned long readLockTickEnd;
    unsigned long tickDiff;
    uint32_t    waitTime;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Convert PFM infinite timeout to Shadwo's infinite timeout
    switch( timeout_ms )
    {
        case PFM_WAIT_NONE:
            waitTime = 0;
            break;
        case PFM_WAIT_FOREVER:
            waitTime = INFINITY;
            break;
        default:
            waitTime = timeout_ms;
            break;
    }

    // Do thread serialized reading
    readLockTickStart = PCTime::Tick();
    if( !qptr->readLock->Try(waitTime) )
    {
        return ERR_TIMEDOUT;
    }

    // If waiting for data, do the tick calculation to
    // potentially reduce wait value
    if( waitTime )
    {
        readLockTickEnd = PCTime::Tick();
        tickDiff = pfmTickDiff( readLockTickStart, readLockTickEnd );

        // Check if we have time to hang on a samephore
        if( tickDiff > waitTime )
        {
            // We have waited longer on a mutex, so wait as short as possible
            waitTime = 0;
        }
        else
        {
            waitTime -= tickDiff;
        }
    }

    // Consume semaphore as it is signalled after every insertion
    qptr->readsem->Try(waitTime);

    // Check if there's data in the queue. This should only happen
    // on wait with timeout. Should not occour in infinite timeout
    // scenario
    if (qptr->container->empty())
    {
        pfmAssert(waitTime != (uint32_t)INFINITY );
        qptr->readLock->Unlock();
        return ERR_TIMEDOUT;
    }

    // read message
    *msg = qptr->container->front();
    qptr->container->pop();

    // This should be the last possible place where
    // a dying queue could be trapped (unlock fails).
    if( !qptr->readLock->Unlock() )
    {
        return ERR_SYS;
    }

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Send
///////////////////////////////////////////////////
pfmerr_t
pfmQueueSend(HPfmQueue q, const SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    pfmerr_t res;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Grab an read Lock
    qptr->writeLock->Lock();

    qptr->container->push(*msg);

    // Inform readers that there's data avaliable.
    qptr->readsem->Give();

    //Unlock queue
    if( !qptr->writeLock->Unlock() )
    {
        // This should handle a dying queue
        res = ERR_SYS;
    }
    else
    {
        res = ERR_OK;
    }

    return res;
}


# ============================================================================
#{
==============================================================================
*kt_linux_core_500* file io

#include <sys/stat.h>
#include <fcntl.h>
#include <sys/wait.h>
#include "tlpi_hdr.h"

int
main(int argc, char *argv[])
{
  int fd, flags;
  char template[] = "/tmp/testXXXXXX";

  setbuf(stdout, NULL);    // {KT} Disable buffering of stdout

  fd = mkstemp(template);  // opens a temporary file
  if (fd == -1)
    errExit("mkstemp");

  printf("File offset before fork(): %lld\n", (long long) lseek(fd, 0, SEEK_CUR));

  flags = fcntl(fd, F_GETFL);    // get file flags
  if (flags == -1)
    errExit("fcntl - F_GETFL");
  
  printf("O_APPEND flag before fork() is: %s\n", (flags & O_APPEND) ? "on" : "off");

  switch (fork()) {
    case -1:
      errExit("fork");

    case 0: /* Child: change file offset and status flags */
      if (lseek(fd, 1000, SEEK_SET) == -1)
        errExit("lseek");

      flags = fcntl(fd, F_GETFL); /* Fetch current flags */
      if (flags == -1)
        errExit("fcntl - F_GETFL");

      flags |= O_APPEND; /* Turn O_APPEND on */
      if (fcntl(fd, F_SETFL, flags) == -1)
        errExit("fcntl - F_SETFL");
      _exit(EXIT_SUCCESS);

    default: /* Parent: can see file changes made by child */
      if (wait(NULL) == -1)
        errExit("wait"); /* Wait for child exit */

      printf("Child has exited\n");
      printf("File offset in parent: %lld\n", (long long) lseek(fd, 0, SEEK_CUR));

      flags = fcntl(fd, F_GETFL);
      if (flags == -1)
        errExit("fcntl - F_GETFL");

      printf("O_APPEND flag in parent is: %s\n", (flags & O_APPEND) ? "on" : "off");
      exit(EXIT_SUCCESS);
  }
}

For an explanation of why we cast the return value from lseek() to long long in Listing 24-2, see
Section 5.10.


={============================================================================
*kt_linux_core_600*  time

{abstime}
The abstime is the system time - the number of seconds and nanoseconds past Jan. 1, 1970, UTC. The
advantage in using the abstime, instead of a delta, is if the function prematurely returns ( perhaps
because of a caught signal ): the function can be called again without having to change the timespec
structure. 


={============================================================================
*kt_linux_core_601*  time: currTime

{implementation}
#include <time.h>
#include "curr_time.h" /* Declares function defined here */

#define BUF_SIZE 1000

// Return a string containing the current time formatted according to the specification in 'format'
// (see strftime(3) for specifiers). If 'format' is NULL, we use "%c" as a specifier (which gives
// the date and time as for ctime(3), but without the trailing newline). Returns NULL on error.

char * currTime(const char *format)
{
  static char buf[BUF_SIZE]; /* Nonreentrant */
  time_t t;
  size_t s;
  struct tm *tm;
  t = time(NULL);
  tm = localtime(&t);
  if (tm == NULL)
    return NULL;
  s = strftime(buf, BUF_SIZE, (format != NULL) ? format : "%c", tm);
  return (s == 0) ? NULL : buf;
}

This '%T' returns '09:34:55' format.

From CTIME(3)                   Linux Programmer's Manual                  CTIME(3)

struct tm *localtime(const time_t *timep);

The ctime(), gmtime() and localtime() functions all take an argument of data  type  time_t which
represents calendar time.  When interpreted as an absolute time value, it represents the  number  of
seconds  elapsed since the Epoch, 1970-01-01 00:00:00 +0000 (UTC).

Broken-down  time  is  stored  in  the structure tm which is defined in <time.h> as follows:

  struct tm {
      int tm_sec;         /* seconds */
      int tm_min;         /* minutes */
      int tm_hour;        /* hours */
      int tm_mday;        /* day of the month */
      int tm_mon;         /* month */
      int tm_year;        /* year */
      int tm_wday;        /* day of the week */
      int tm_yday;        /* day in the year */
      int tm_isdst;       /* daylight saving time */
  };

The localtime() function converts the calendar time timep to broken-down time representation,
    expressed relative to the user's specified timezone. The function acts as if it called tzset(3)
    and sets the external variables tzname with information about the current timezone, timezone
    with the difference between Coordinated Universal Time (UTC) and local standard time in seconds,
    and daylight to a nonzero value if daylight savings time rules apply during some part of the
    year. The return value points to a statically allocated struct which might be overwritten by
    subsequent calls to any of the date and time  functions.


={============================================================================
*kt_linux_core_602*  time: ms and us

{how-to-get-ms-and-us}
<gettimeofday>  # method 01
To get ns(nano) from us(micro). 

#include <sys/time.h>

int gettimeofday(struct timeval *tv, struct timezone *tz);

The  functions  gettimeofday()  and  settimeofday() can get and set the time as well as a timezone.
The tv argument is a  struct  timeval  (as specified in <sys/time.h>):

  struct timeval {
    time_t      tv_sec;     /* seconds */
    suseconds_t tv_usec;    /* microseconds */
  };

and gives the number of seconds and microseconds since the Epoch (see time(2)). The use of the
timezone structure is obsolete; the tz argument should normally be specified as NULL.

#include <iostream>
#include <vector>
#include <sys/time.h>

using namespace std;

typedef uint64_t u64;

// return nano secs
static u64 nsec() {
  struct timeval tv;
  if(gettimeofday(&tv, 0) < 0)
    return -1;
  return (u64)tv.tv_sec*1000*1000*1000 + tv.tv_usec*1000;
}

int main()
{
  std::vector<int> ivec;

  uint64_t be = nsec();

  ivec.push_back(1);
  ivec.push_back(2);
  ivec.push_back(3);
  ivec.push_back(4);

  uint64_t af = nsec();

  cout << "diff(ns): " << af-be << endl;
  cout << "diff(ms): " << (af-be)/1000 << endl;
}

$ ./a.out 
diff(ns): 59000
diff(ms): 59

<clock_gettime> # method 02
NAME
       clock_gettime - Return the current timespec value of tp for the specified clock

SYNOPSIS
       long sys_clock_gettime (clockid_t which_clock, struct timespec *tp);

DESCRIPTION
       clock_gettime  returns  the  current timespec value of tp for the specific clock,
       which_clock.  The values that clockid_t currently supports for POSIX.1b timers, as defined in
       include/linux/time.h, are:

       CLOCK_REALTIME
              Systemwide realtime clock.

       CLOCK_MONOTONIC
              Represents monotonic time. Cannot be set.

       CLOCK_PROCESS_CPUTIME_ID
              High resolution per-process timer.

       CLOCK_THREAD_CPUTIME_ID
              Thread-specific timer.

If clock has 1024 hz resolution, then

1/1024 = 0.000.9765625 (the result is 976562.5 nanoseconds.)
1/1000 = 0.001.
1/2048 = 0.000.48828125

The unit is wider or shorter depending on a resolution.

struct timespec {
  time_t   tv_sec;        /* seconds */
  long     tv_nsec;       /* nanoseconds */     // note nano
};

The CLOCK_REALTIME clock measures the amount of time that has elapsed 'since' 00:00:00 January 1,
1970 Greenwich Mean Time (GMT)

int clock_gettime(clockid_t clock_id, struct timespec *tp); A return value of 0 shall
indicate that the call succeeded.

10-9 	1 nanosecond 	ns 	One billionth of one second
10-6 	1 microsecond 	Âµs 	One millionth of one second
10-3 	1 millisecond 	ms 	One thousandth of one second

This struct increase 'sec' when 'nsec' nano sec passes. so to get the total nano:

(tv_sec * 10+9) + tv_nsec nanoseconds


the following gets time in ms.

1 ms = 1 sec * 10+3
1 ms = 1 nsec * 10-6

Therfore, this is to get time in ms.

static uint32_t get_time()
{
  struct timespec time = {0, 0};
  uint32_t msec = 0U;
  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  msec = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  msec += time.tv_nsec / 1000000;
  return msec;
}

// get time in Âµs 
//
static uint32_t time_get_us(void)
{
	 struct	 timespec ts;
	 xclock_gettime(CLOCK_REALTIME, &ts);
	 return (uint32_t)((ts.tv_sec * 1000000) + (ts.tv_nsec / 10000));
}

// start time
//
static void time_init(void)
{
	tstart = time_get();
}

// get diff between 'start' and 'now' in ms
//
static uint32_t time_get(void)
{
    struct	 timespec ts;
#if __mips__
    xclock_gettime(CLOCK_REALTIME, &ts);
#else
	clock_gettime(CLOCK_REALTIME, &ts);
#endif
	return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000)) - tstart;
}

// usuage
//
thi = time_get();

hour = (thi)/3600000; 				# hour in ms
msec = (thi - (hour*3600000));	# ms remains 
minute = msec / 60000; 				# mins 
msec = msec - (minute * 60000);	# ms remains
sec = msec / 1000; 					# secs
msec = msec - (sec * 1000); 		# ms remains

fprintf(of, "start check at: %.3d:%.2d:%.2d.%.3d\n", hour, minute, sec, msec);


/*
 * time in MIPS
 */
#include<stdio.h>
#include<unistd.h>
#include<linux/unistd.h>
#include<errno.h> 
#include <stdlib.h>
#include <string.h>
#include <time.h>

typedef unsigned int uint32_t;
static uint32_t tstart = 0;

#if __mips__ /* optimization for mips */
static inline void xclock_gettime(unsigned int which_clock, struct timeval * tv);
	
#define _syscall_clock_gettimeX(type,name,atype,a,btype,b) \
type x##name(atype a, btype b) \
{ \
	register unsigned long __a0 asm("$4") = (unsigned long) a; \
	register unsigned long __a1 asm("$5") = (unsigned long) b; \
	register unsigned long __a3 asm("$7"); \
	unsigned long __v0; \
	\
	__asm__ volatile ( \
	".set\tnoreorder\n\t" \
	"li\t$2, %4\t\t\t# " #name "\n\t" \
	"syscall\n\t" \
	"move\t%0, $2\n\t" \
	".set\treorder" \
	: "=&r" (__v0), "=r" (__a3) \
	: "r" (__a0), "r" (__a1), "i" (__NR_##name) \
	: "$2", "$8", "$9", "$10", "$11", "$12", "$13", "$14", "$15", "$24", \
	  "memory"); \
}

_syscall_clock_gettimeX(void, clock_gettime, unsigned int, which_clock, struct timeval *, tv);

#endif

static uint32_t time_get(void)
{
    struct	 timespec ts;
#if __mips__
    xclock_gettime(CLOCK_REALTIME, &ts);
	printf("call xclock_getttime: ts.tv_sec = %ld, ts.tv_nsec = %ld\n", ts.tv_sec, ts.tv_nsec);
#else
	clock_gettime(CLOCK_REALTIME, &ts);
#endif
	return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000)) - tstart;
}

void main(int argc, char* agrv[]) 
{
	int hour = 0, minute = 0, sec = 0, msec = 0;
	uint32_t tlo =0, thi = 0;

	tstart = time_get();

	thi = time_get();
	hour = (thi)/3600000;
	msec = (thi - (hour*3600000));
	minute = msec / 60000;
	msec = msec - (minute * 60000);
	sec = msec / 1000;
	msec = msec - (sec * 1000);
	printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

	sleep(2);

	thi = time_get();
	hour = (thi)/3600000;
	msec = (thi - (hour*3600000));
	minute = msec / 60000;
	msec = msec - (minute * 60000);
	sec = msec / 1000;
	msec = msec - (sec * 1000);
	printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

	thi = 0xFFFFFFFF;
	hour = (thi)/3600000;
	msec = (thi - (hour*3600000));
	minute = msec / 60000;
	msec = msec - (minute * 60000);
	sec = msec / 1000;
	msec = msec - (sec * 1000);
	printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

	return;
}

-sh-3.2# ./a.out 
call xclock_getttime: ts.tv_sec = 946693753, ts.tv_nsec = 932727000
call xclock_getttime: ts.tv_sec = 946693753, ts.tv_nsec = 932829000
thi:   0, time: 000:00:00.000

call xclock_getttime: ts.tv_sec = 946693755, ts.tv_nsec = 933026000
thi:2001, time: 000:00:02.001

thi:  -1, time: 1193:02:47.295


{mktime} # localtime
time_t is a sec used in timespec

char *ctime(const time_t *timep);

The call ctime(t) is equivalent to asctime(localtime(t)). It converts the calendar time t
into a null-terminated string of the form

"Wed Jun 30 21:49:08 1993\n"

struct tm *localtime(const time_t *timep);

The localtime() function converts the calendar time timep to broken-down time
representation, expressed relative to the user's specified timezone.

The ctime(), gmtime() and localtime() functions all take an argument of data type time_t
which represents calendar time. When interpreted as an absolute time value, it represents
the number of seconds elapsed since the Epoch, 1970-01-01 00:00:00 +0000 (UTC).

time_t mktime(struct tm *tm);

The mktime() function converts a broken-down time structure, expressed as local time, to
calendar time representation.  The asctime() and mktime() functions both take an argument
representing broken-down time which is a representation separated into year, month, day,
etc.

Broken-down time is stored in the structure tm which is defined in <time.h> as follows:
this is the broken time is the local time based on time zone.

struct tm {
	int tm_sec;         /* seconds */
	int tm_min;         /* minutes */
	int tm_hour;        /* hours */
	int tm_mday;        /* day of the month */
	int tm_mon;         /* month */
	int tm_year;        /* year */
	int tm_wday;        /* day of the week */
	int tm_yday;        /* day in the year */
	int tm_isdst;       /* daylight saving time */
};

The members of the tm structure are:
tm_year 	The number of years since 1900.
tm_mon		The number of months since January, in the range 0 to 11.


to convert local time to UTC

t3=2011:12:15:13:5:5
sscanf(t3,"%d:%d:%d:%d:%d:%d",&year,&month,&day,&hrs,&mins,&secs);
mytime.tm_year=year - 1900;
mytime.tm_mon=month-1;
mytime.tm_mday=day;
mytime.tm_hour=hrs;
mytime.tm_min=mins;
mytime.tm_sec=secs;
mktime(&mytime)


={============================================================================
*kt_linux_core_700* dbus

{dbus}
http://www.freedesktop.org/wiki/Software/dbus/

What is D-Bus?

D-Bus is a message bus system, a simple way for applications to talk to one another. In addition to
interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and
reliable to code a "single instance" application or daemon, and to launch applications and daemons
on demand when their services are needed.


{tutorial}
from http://dbus.freedesktop.org/doc/dbus-tutorial.html

D-Bus is a system for interprocess communication (IPC). Architecturally, it has several layers:

<libdbus> low-level binding?
A library, libdbus, that allows two applications to connect to each other and exchange messages.

<bus-deamon>
A message bus daemon executable, built on libdbus, that multiple applications can connect to. The
daemon can route messages from one application to zero or more other applications.

The message bus daemon forms the hub of a wheel. Each spoke of the wheel is a one-to-one connection
to an application using libdbus. 

An application sends a message to the bus daemon over its spoke, and the bus daemon forwards the
message to other connected applications as appropriate.  Think of the daemon as a 'router'. 

<binding> high-level binding
Wrapper libraries or bindings based on particular application frameworks. For example, libdbus-glib
and libdbus-qt. There are also bindings to languages such as Python. These wrapper libraries are the
API most people should use, as they simplify the details of D-Bus programming. libdbus is intended
to be a low-level backend for the higher level bindings. Much of the libdbus API is only useful for
binding implementation. 


D-Bus applications. D-Bus is designed for 'two' specific 'cases':

1. The bus daemon has multiple instances on a typical computer. The first instance is a
machine-global singleton, that is, a system daemon similar to sendmail or Apache. This instance has
heavy security restrictions on what messages it will accept, and is used for systemwide
communication. The other instances are created one per user login session. These instances allow
applications in the user's session to communicate with one another.

The systemwide and per-user daemons are separate. Normal within-session IPC does not involve the
systemwide message bus process and vice versa. 

Communication between desktop applications in the 'same' desktop 'session'; to allow integration of
the desktop session as a whole, and address issues of process lifecycle (when do desktop components
    start and stop running).

2. Communication between the desktop session and the operating system, where the operating system
would typically include the kernel and any system daemons or processes. 

For the within-desktop-session use case, the GNOME and KDE desktops have significant previous
experience with different IPC solutions such as CORBA and DCOP. D-Bus is built 'on' that experience
and carefully tailored to meet the needs of these desktop projects in particular. D-Bus may or may
not be appropriate for other applications; the FAQ has some comparisons to 'other' IPC systems.


{concept}

<address>
Applications using D-Bus are either servers or clients. A server listens for incoming connections; a
client connects to a server. Once the connection is established, it is a symmetric flow of messages;
the client-server distinction only matters when setting up the connection.

If you're using the bus daemon, as you probably are, your application will be a client of the bus
daemon. That is, the bus daemon listens for connections and your application initiates a connection
to the bus daemon.

A D-Bus address specifies where a server will listen, and where a client will connect. For example,
  the address unix:path=/tmp/abcdef specifies that the server will listen on a UNIX domain socket at
  the path /tmp/abcdef and the client will connect to that socket. An address can also specify
  TCP/IP sockets, or any other transport defined in future iterations of the D-Bus specification. 

<dbus-name> for application
When each application connects to the bus daemon, the daemon immediately assigns it a name, called
the unique connection name. A unique name begins with a ':' (colon) character. These names are never
reused during the lifetime of the bus daemon - that is, you know a given name will always refer to
the same application.

An example of a unique name might be :34-907. The numbers after the colon have no meaning other than
their 'uniqueness'.

When a name is mapped to a particular application's connection, that application is said to own that
name.

Applications may ask to own additional well-known names. For example, you could write a
specification to define a name called com.mycompany.TextEditor. Your definition could specify that
to own this name, an application should have an object at the path /com/mycompany/TextFileManager
supporting the interface org.freedesktop.FileHandler.

Applications could then send messages to this bus name, object, and interface to execute method calls.

You could think of the unique names as IP addresses, and the well-known names as domain names. So
com.mycompany.TextEditor might map to something like :34-907 just as mycompany.com maps to something
like 192.168.0.5. 

Names have a 'second' important 'use', other than routing messages. They are used to track
'lifecycle'.  When an application exits (or crashes), its connection to the message bus will be
closed by the operating system kernel. The message bus then sends out notification messages telling
remaining applications that the application's names have lost their owner. By tracking these
notifications, your application can reliably monitor the lifetime of other applications. 

<object-path>
The low-level D-Bus protocol, and corresponding libdbus API, does not care about native objects.
However, it provides a concept called an object path. The idea of an object path is that
higher-level bindings can name native object instances, and allow remote applications to refer to
them. 

The object path looks like a filesystem path, for example an object could be named
/org/kde/kspread/sheets/3/cells/4/5. Human-readable paths are nice, but you are free to create an
object named /com/mycompany/c5yo817y0c1y1c5b if it makes sense for your application.

Namespacing object paths is smart, by starting them with the components of a domain name you own
(e.g. /org/kde). This keeps different code modules in the same process from stepping on one
another's toes. 

<method-and-signal>
Each object has members; the two 'kinds' of member are methods and signals. Methods are operations
that can be invoked on an object, with optional input (aka arguments or "in parameters") and output
(aka return values or "out parameters"). Signals are broadcasts from the object to any interested
observers of the object; signals may contain a data payload.

<interface>
Each object supports one or more interfaces. Think of an interface as a named group of methods and
signals, just as it is in GLib or Qt or Java. Interfaces define the type of an object instance.

DBus identifies interfaces with a simple 'namespaced' string, something like
org.freedesktop.Introspectable. Most bindings will map these interface names directly to the
appropriate programming language construct, for example to Java interfaces or C++ pure virtual
classes. 

<proxies>
A proxy object is a convenient native object created to represent a remote object in another
process. The low-level DBus API involves manually creating a method call message, sending it, then
manually receiving and processing the method reply message. Higher-level bindings provide proxies as
an alternative. Proxies look like a normal native object; but when you invoke a method on the proxy
object, the binding converts it into a DBus method call message, waits for the reply message,
  unpacks the return value, and returns it from the native method.


{big-picture} to specify call
Pulling all these concepts together, to specify a particular method call on a particular object
instance, a number of nested components have to be named:

Address -> [Bus Name] -> Path -> Interface -> Method


{dbus-messages}
There are 4 message types:

1. Method call messages ask to invoke a method on an object.  you send a method call message, and
receive either a method return message or an error message in reply. 

2. Method return messages return the results of invoking a method.

3. Error messages return an exception caused by invoking a method.

4. Signal messages are notifications that a given signal has been emitted (that an event has
    occurred). You could also think of these as "event" messages. 

A method call maps very simply to messages: you send a method call message, and receive either a
method return message or an error message in reply.

Each message has a header, including fields, and a body, including arguments. You can think of the
header as the routing information for the message, and the body as the payload. Header fields might
include the sender bus name, destination bus name, method or signal name, and so forth. One of the
header fields is a type signature describing the values found in the body. For example, the letter
"i" means "32-bit integer" so the signature "ii" means the payload has two 32-bit integers. 


{dbus-send}
Usage: dbus-send [--help] [--system | --session | --address=ADDRESS] [--dest=NAME] [--type=TYPE]
[--print-reply[=literal]] [--reply-timeout=MSEC] <destination object path> <message name> [contents
...]

From http://dbus.freedesktop.org/doc/dbus-send.1.html
dbus-send [ --system | --session | --address=ADDRESS ] [--dest=NAME] [ --print-reply [=literal]]
[--reply-timeout=MSEC] [--type=TYPE] OBJECT_PATH INTERFACE.MEMBER [CONTENTS...]


<example> xml has member and type defs as well
[17-02-2015 10:57:31.574183] signal sender=:1.0 -> dest=(null destination) serial=96 path=/org/freedesktop/NetworkManager/Devices/0; interface=org.freedesktop.NetworkManager.Device; member=StateChanged

<node name="/" xmlns:tp="http://telepathy.freedesktop.org/wiki/DbusSpec#extensions-v0">
  <interface name="org.freedesktop.NetworkManager.Device">
    ...
    <signal name="StateChanged">
      <arg name="new_state" type="u" tp:type="NM_DEVICE_STATE">
        <tp:docstring>
          The new state of the device.
        </tp:docstring>
      </arg>
      <arg name="old_state" type="u" tp:type="NM_DEVICE_STATE">
        <tp:docstring>
          The previous state of the device.
        </tp:docstring>
      </arg>
      <arg name="reason" type="u" tp:type="NM_DEVICE_STATE_REASON">
        <tp:docstring>
          A reason for the state transition.
        </tp:docstring>
      </arg>
    </signal>

    <tp:enum name="NM_DEVICE_STATE" type="u">
      <tp:enumvalue suffix="UNKNOWN" value="0">
        <tp:docstring>
          The device is in an unknown state.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="UNMANAGED" value="1">
        <tp:docstring>
          The device is not managed by NetworkManager.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="UNAVAILABLE" value="2">
        <tp:docstring>
          The device cannot be used (carrier off, rfkill, etc).
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="DISCONNECTED" value="3">
        <tp:docstring>
          The device is not connected.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="PREPARE" value="4">
        <tp:docstring>
          The device is preparing to connect.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="CONFIG" value="5">
        <tp:docstring>
          The device is being configured.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="NEED_AUTH" value="6">
        <tp:docstring>
          The device is awaiting secrets necessary to continue connection.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="IP_CONFIG" value="7">
        <tp:docstring>
          The IP settings of the device are being requested and configured.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="ACTIVATED" value="8">
        <tp:docstring>
          The device is active.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="FAILED" value="9">
        <tp:docstring>
          The device is in a failure state following an attempt to activate it.
        </tp:docstring>
      </tp:enumvalue>
    </tp:enum>
    ...

<introspect>
dbus-send  --session --type=method_call --print-reply --dest='org.freedesktop.DBus' / org.freedesktop.DBus.Introspectable.Introspect

method return sender=org.freedesktop.DBus -> dest=:1.255 reply_serial=2
   string "<!DOCTYPE node PUBLIC "-//freedesktop//DTD D-BUS Object Introspection 1.0//EN"
"http://www.freedesktop.org/standards/dbus/1.0/introspect.dtd">
<node>
  <interface name="org.freedesktop.DBus"> ~
    <method name="Hello">
      <arg direction="out" type="s"/>
    </method>
    ...
    <method name="ListNames"> ~
      <arg direction="out" type="as"/>
    </method>
    ...
  </interface>
  <interface name="org.freedesktop.DBus.Introspectable"> ~
    <method name="Introspect">
      <arg direction="out" type="s"/>
    </method>
  </interface>
</node>
"


<lsdbus> to find the owners of all dbus connections
# dbus-send  --session --type=method_call --print-reply --dest='org.freedesktop.DBus' / org.freedesktop.DBus.ListNames

note: org.freedesktop.Dbus means the dbus itself and ListNmaes to get the list of names on the bus

method return sender=org.freedesktop.DBus -> dest=:1.81 reply_serial=2
   array [
      string "org.freedesktop.DBus"
      string ":1.7"
      string ":1.8"
      string "Zinc.BabySitter"
      string "Zinc.Application"
      string "org.freedesktop.NetworkManager"
      string "Zinc.Crb"
      string "Zinc.OEMSystemTime"
      string ":1.81"
      string "Zinc.ApplicationPackages"
      string ":1.41"
      string "Zinc.UsageCollection"
      string ":1.42"
      string ":1.65"
      string ":1.21"
      string ":1.43"
      string ":1.66"
      string ":1.44"
      string ":1.67"
      string ":1.45"
      string "org.freedesktop.NetworkManagerSystemSettings"
      string ":1.46"
      string ":1.69"
      string "Zinc.Media"
      string ":1.47"
      string ":1.48"
      string "Zinc.OEMSystem"
      string ":1.29"
      string "Zinc.ContentAcquisition"
      string "Zinc.Broker"
      string "org.freedesktop.Avahi"
      string "Zinc.DeviceSoftware"
      string "Zinc.RemoteBooking"
      string "Zinc.LinearSource"
      string "Zinc.DeviceManager"
      string "Zinc.Tuner"
      string ":1.70"
      string ":1.71"
      string "Zinc.OEMSystemManager"
      string ":1.72"
      string ":1.73"
      string ":1.75"
      string "Zinc.MetadataProxy"
      string ":1.53"
      string ":1.76"
      string "Zinc.Reminders"
      string ":1.34"
      string "Zinc.Metadata"
      string "Zinc.System"
      string ":1.0"
      string ":1.58"
      string "Zinc.RemoteDiagnostics"
      string ":1.4"
      string ":1.5"
      string ":1.18"
      string ":1.19"
   ]


# lsdbus
793   :1.0                     /opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
793   org.freedesktop.NetworkManager/opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
793   org.freedesktop.NetworkManagerSystemSettings/opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
831   :1.4                     /opt/zinc/bin/litaniumsystemmanagerd            
831   :1.5                     /opt/zinc/bin/litaniumsystemmanagerd            
831   Zinc.OEMSystemManager    /opt/zinc/bin/litaniumsystemmanagerd            
831   Zinc.System              /opt/zinc/bin/litaniumsystemmanagerd            
...


#cat /opt/zinc/devel/bin/lsdbus
#!/bin/sh -e

# Print a list of all dbus connections and the processes that own them.
# Taken from the wiki page:
# https://wiki.youview.co.uk/display/canvas/How+to+Introspect+DBus+from+the+Command+Line

# The format of the output is:
# pid | bus name | command

function ListBusNames {
    dbus-send --session --type=method_call --print-reply \
              --dest=org.freedesktop.DBus / \
              org.freedesktop.DBus.ListNames
}

function GetProcessID {
    dbus-send --session --type=method_call --print-reply \
              --dest=org.freedesktop.DBus / \
              org.freedesktop.DBus.GetConnectionUnixProcessID string:$1 2>/dev/null \
        | xargs -n1 | tail -1
}

for i in $(ListBusNames | grep string | cut -d'"' -f2)
do
    DCNAME=$i
    DCPID=$(GetProcessID $DCNAME)
    if [ -n "$DCPID" ]
    then
        DCPCMD=$(cat /proc/$DCPID/cmdline 2>/dev/null | xargs -0 echo) && \
               printf "%-6s%-25s%-40s%-8s%s\n" \
                  "$DCPID" "$DCNAME" "$DCPCMD"
    fi
done | sort -n


{dbus-monitor}
dbus-monitor [ --system | --session | --address ADDRESS ] [ --profile | --monitor ] [ watch expressions ]

<session>
There are two well-known message buses: the systemwide message bus; installed on many systems as the
"messagebus" service and the per-user-login-session message bus; started each time a user logs in.
The --system and --session options direct dbus-monitor to monitor the system or session buses
respectively. note: If neither is specified, dbus-monitor monitors the session bus.

<monitor>
dbus-monitor has two different output modes, the 'classic'-style monitoring mode and profiling mode.
The profiling format is a compact format with a single line per message and microsecond-resolution
timing information. The --profile and --monitor options select the profiling and monitoring output
format respectively. note: If neither is specified, dbus-monitor uses the monitoring output format.

<watch-expr>
In order to get dbus-monitor to see the messages you are interested in, you should specify a set of
watch expressions as you would expect to be passed to the dbus_bus_add_match function.

# display only method calls, returns and errors. No signals at all will be displayed but it does
# have the benefit of you not getting the PositionChange signals cluttering your display.

dbus-monitor "type=method_call" "type=method_return" "type=error"


dbus-monitor --profile "interface=Zinc.Application.ApplicationManager,member=launchApplication" \ 
   "interface=Zinc.Application.ApplicationManager,member=ApplicationLifecycleEvent"

dbus-monitor > /mnt/hd1/mylogs.log &
dbus-monitor | tee /mnt/hd1/mylogs.log &

# To launch Dbus-Monitor on your STB, and inspect MediaRouter activity, run the following command:
dbus-monitor "interface=Zinc.Media.MediaRouter"


{kdbus}
https://github.com/gregkh/presentation-kdbus
https://github.com/gregkh/kdbus


={============================================================================
*kt_linux_core_800* proc: /proc/mounts

The proc(5) manual page.

from stackoverflow:
The definitive list of mounted filesystems in in /proc/mounts.

If you have any form of containers on your system, /proc/mounts only lists the filesystems that are
in your present container. For example, in a chroot, /proc/mounts lists only the filesystems whose
mount point is within the chroot. (There are ways to escape the chroot, mind.)

There's also a list of mounted filesystems in /etc/mtab. This list is maintained by the mount and
umount commands. That means that if you don't use these commands (which is pretty rare), your action
(mount or unmount) won't be recorded. In practice, it's mostly in a chroot that you'll find
/etc/mtab files that differ wildly from the state of the system (also mounts performed in the chroot
    will be reflected in the chroot's /etc/mtab but not in the main /etc/mtab). 

Actions performed while /etc/mtab is on a read-only filesystem are also not recorded there. The
reason why you'd sometimes want to consult /etc/mtab in preference to or in addition to /proc/mounts
is that because it has access to the mount command line, it's sometimes able to present information
in a way that's easier to understand; for example you see mount options as requested (whereas
    /proc/mounts lists the mount and kernel defaults as well), and bind mounts appear as such in
/etc/mtab.


={============================================================================
*kt_linux_core_801* proc: /proc/PID/status

The parent of any process can be found by looking at the Ppid field provided in the Linux-specific
/proc/PID/status file.

Uid:  1024  1024  1024  1024
Gid:  1025  1025  1025  1025

The credentials of any process can be found by examining the Uid, Gid, and Groups lines provided in
the Linux-specific /proc/PID/status file. The Uid and Gid lines list the identifiers in the order
real, effective, saved set, and file system.

FDSize:	32
Groups:	
VmPeak:	    2292 kB
VmSize:	    2288 kB
VmLck:	       0 kB
VmPin:	       0 kB
VmHWM:	     752 kB
VmRSS:	     752 kB
VmData:	     164 kB
VmStk:	     136 kB
VmExe:	      32 kB
VmLib:	    1904 kB
VmPTE:	      16 kB
VmSwap:	       0 kB
Threads:	1

<signals>
SigQ: 0/3941
SigPnd: 0000000000000000
ShdPnd: 0000000000000000
SigBlk: 0000000000000000
SigIgn: fffffffe57f0d8fc
SigCgt: 00000000280b2603

CapInh:	0000000000000000
CapPrm:	ffffffffffffffff
CapEff:	ffffffffffffffff
CapBnd:	ffffffffffffffff
Cpus_allowed:	1
Cpus_allowed_list:	0
Mems_allowed:	1
Mems_allowed_list:	0
voluntary_ctxt_switches:	310
nonvoluntary_ctxt_switches:	127


={============================================================================
*kt_linux_core_802* proc: /proc/PID/maps

Using the Linux-specific /proc/PID/maps file, we can see the location of the shared memory segments
and shared libraries mapped by a program.

From LPI 48.5.

$ cat /proc/9903/maps
08048000-0804a000 r-xp 00000000 08:05 5526989 /home/mtk/svshm_attach          <1>
0804a000-0804b000 r--p 00001000 08:05 5526989 /home/mtk/svshm_attach
0804b000-0804c000 rw-p 00002000 08:05 5526989 /home/mtk/svshm_attach
b7bed000-b7f0d000 rw-s 00000000 00:09 9666565 /SYSV00000000 (deleted)         <2>
b7f0d000-b7f26000 rw-s 00000000 00:09 9633796 /SYSV00000000 (deleted)
b7f26000-b7f27000 rw-p b7f26000 00:00 0
b7f27000-b8064000 r-xp 00000000 08:06 122031 /lib/libc-2.8.so                 <3>
b8064000-b8066000 r--p 0013d000 08:06 122031 /lib/libc-2.8.so
b8066000-b8067000 rw-p 0013f000 08:06 122031 /lib/libc-2.8.so
b8067000-b806b000 rw-p b8067000 00:00 0
b8082000-b8083000 rw-p b8082000 00:00 0
b8083000-b809e000 r-xp 00000000 08:06 122125 /lib/ld-2.8.so                   <4>
b809e000-b809f000 r--p 0001a000 08:06 122125 /lib/ld-2.8.so
b809f000-b80a0000 rw-p 0001b000 08:06 122125 /lib/ld-2.8.so
bfd8a000-bfda0000 rw-p bffea000 00:00 0 [stack]                               <5>
ffffe000-fffff000 r-xp 00000000 00:00 0 [vdso]                                <6>

<1> Three lines for the main program, shm_attach. These correspond to the text and data segments of
the program. The second of these lines is for a readonly page holding the string constants used by
the program.

<2> Two lines for the attached System V shared memory segments.

<3> Lines corresponding to the segments for two shared libraries. One of these is the standard C
library (libc-version.so). 

<4> The other is the dynamic linker (ld-version.so).

<5> A line labeled [stack]. This corresponds to the process stack.

<columns>
1. A pair of hyphen-separated numbers indicating the virtual address range at which the memory
segment is mapped. The second of these numbers is the address of the next byte after the end of the
segment.

note: From this, can get the size. b7f0d000-b7bed000=0x320000. 3,276,800

2. Protection and flags for this memory segment. The first three letters indicate the protection of
the segment: read (r), write (w), and execute (x). A hyphen (-) in place of any of these letters
indicates that the corresponding protection is disabled. The final letter indicates the mapping flag
for the memory segment; it is either private (p) or shared (s). For an explanation of these flags,
    see the description of the MAP_PRIVATE and MAP_SHARED flags in Section 49.2. 
    
note: A System V shared memory segment is always marked shared.

3. The hexadecimal offset (in bytes) of the segment within the corresponding mapped file. The
meanings of this and the following two columns will become clearer when we describe the mmap()
system call in Chapter 49. 

note: For a System V shared memory segment, the offset is always 0.

4. The device number (major and minor IDs) of the device on which the corresponding mapped file is
located.

5. The i-node number of the mapped file, or, for System V shared memory segments, the 'identifier'
for the segment.

6. The filename or other identifying tag associated with this memory segment. For a System V shared
memory segment, this consists of the string SYSV concatenated with the shmget() key of the segment
(expressed in hexadecimal). In this example, SYSV is followed by zeros because we created the
segments using the key IPC_PRIVATE (which has the value 0). The string (deleted) that appears after
the SYSV field for a System V shared memory segment is an artifact of the implementation of shared
memory segments. Such segments are created as mapped files in an invisible tmpfs file system
(Section 14.10), and then later unlinked. Shared anonymous memory mappings are implemented in the
same manner. (We describe mapped files and shared anonymous memory mappings in Chapter 49.)

<smaps>
Starting with kernel 2.6.14, Linux also provides the /proc/PID/smaps file, which exposes more
information about the memory consumption of each of a process's mappings. For further details, see
the proc(5) manual page.

The snippet from proc man and see man page for more.

/proc/[pid]/smaps (since Linux 2.6.14)

This file shows memory consumption for each of the process's mappings. (The pmap(1) command displays
    similar information, in a form that may be easier for parsing.) For each mapping there is a
series of lines such as the following:

00400000-0048a000 r-xp 00000000 fd:03 960637       /bin/bash
Size:                552 kB
Rss:                 460 kB
Pss:                 100 kB
Shared_Clean:        452 kB
Shared_Dirty:          0 kB
Private_Clean:         8 kB
Private_Dirty:         0 kB
Referenced:          460 kB
Anonymous:             0 kB
AnonHugePages:         0 kB
Swap:                  0 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB
Locked:                0 kB


={============================================================================
*kt_linux_core_803* proc: /proc/PID/exe

The Linux-specific /proc/PID/exe file is a symbolic link containing the absolute pathname of the
executable file being run by the corresponding process.

$ ls -1l /proc/3510/exe
lrwxrwxrwx 1 keitee keitee 0 Feb  2 22:21 /proc/3510/exe -> /home/keitee/bin/vim

note: to check the path name of the process.


={============================================================================
*kt_linux_core_804* proc: /proc/PID/fd

/proc/[pid]/fd

This is a subdirectory containing one entry for each file which the process has open, named by its
file descriptor, and which is a symbolic link to the actual file. Thus, 0 is standard input, 1
standard output, 2 standard error, etc.

In a multithreaded process, the contents of this directory are not available if the main thread has
already terminated (typically by calling pthread_exit(3)).

root       793  0.7  1.0  25528  4176 ?        Sl   16:04   0:00 /opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO

# ll /proc/793/fd
fd/     fdinfo/ 

# ll /proc/793/fd/
dr-x------    2 root     root           0 Feb 16 16:06 ./
dr-xr-xr-x    6 root     root           0 Jan  1  2000 ../
lr-x------    1 root     root          64 Feb 16 16:06 0 -> /dev/null
l-wx------    1 root     root          64 Feb 16 16:06 1 -> /var/tmp/NetworkManager.log
lrwx------    1 root     root          64 Feb 16 16:06 10 -> /dev/nexus_astm
lrwx------    1 root     root          64 Feb 16 16:06 11 -> /dev/nexus_display
lrwx------    1 root     root          64 Feb 16 16:06 12 -> /dev/nexus_graphics2d
lrwx------    1 root     root          64 Feb 16 16:06 13 -> /dev/nexus_surface
lrwx------    1 root     root          64 Feb 16 16:06 14 -> /dev/nexus_audio
lrwx------    1 root     root          64 Feb 16 16:06 15 -> /dev/nexus_video_decoder
lrwx------    1 root     root          64 Feb 16 16:06 16 -> /dev/nexus_transport
lrwx------    1 root     root          64 Feb 16 16:06 17 -> /dev/nexus_dma
lrwx------    1 root     root          64 Feb 16 16:06 18 -> /dev/nexus_security
lrwx------    1 root     root          64 Feb 16 16:06 19 -> /dev/nexus_spi
l-wx------    1 root     root          64 Feb 16 16:06 2 -> /var/tmp/NetworkManager.log
lrwx------    1 root     root          64 Feb 16 16:06 20 -> /dev/nexus_frontend
lrwx------    1 root     root          64 Feb 16 16:06 21 -> /dev/nexus_keypad
lrwx------    1 root     root          64 Feb 16 16:06 22 -> /dev/nexus_rfm
lrwx------    1 root     root          64 Feb 16 16:06 23 -> /dev/nexus_uhf_input
lrwx------    1 root     root          64 Feb 16 16:06 24 -> /dev/nexus_input_capture
lrwx------    1 root     root          64 Feb 16 16:06 25 -> /dev/nexus_ir_blaster
lrwx------    1 root     root          64 Feb 16 16:06 26 -> /dev/nexus_ir_input
lrwx------    1 root     root          64 Feb 16 16:06 27 -> /dev/nexus_led
lrwx------    1 root     root          64 Feb 16 16:06 28 -> /dev/nexus_gpio
lrwx------    1 root     root          64 Feb 16 16:06 29 -> /dev/nexus_i2c
lr-x------    1 root     root          64 Feb 16 16:06 3 -> pipe:[548]|
lrwx------    1 root     root          64 Feb 16 16:06 30 -> /dev/nexus_pwm
...


={============================================================================
*kt_linux_core_813* proc: /proc/sys/kernel, system resource limits

Various files under the /proc/sys/kernel directory can be used to view and modify these limits.

On Linux, the ipcs -l command can be used to list the limits on each of the IPC mechanisms. 


# ============================================================================
#{ SYSCALL
={============================================================================
*kt_linux_sysc_001* pause

pause() causes the calling process (or thread) to sleep until a signal is delivered that either
terminates the process or causes the invocation of a signal-catching function.


={============================================================================
*kt_linux_sysc_002* alarm

Since the final for loop of the program loops forever, this program uses alarm() to establish a
timer to deliver SIGALRM. The arrival of an unhandled SIGALRM signal guarantees process termination,
if the process is not other- wise terminated.

unsigned int alarm(unsigned int seconds);

alarm() arranges for a SIGALRM signal to be delivered to the calling process in seconds seconds.

int main(int argc, char *argv[])
{
  /* An unhandled SIGALRM ensures this process will die if nothing else terminates it */
  alarm(60);

  for(;;) {
    /* Wait for signals */
    pause();
    printf("%ld: caught SIGHUP\n", (long) getpid());
  }
}


={============================================================================
*kt_linux_sysc_003* getenv, setenv

<getenv>
#include <stdlib.h>

char *getenv(const char *name);

RETURN VALUE
The getenv() function returns a pointer to the value in the environment, or NULL if there is no
match.

<setenv>
int setenv(const char *name, const char *value, int overwrite);

The setenv() function adds the variable name to the environment with the value value, if name does
  not already exist. If name does exist in the environment, then its value is changed to value 'if'
  overwrite is 'nonzero'; if overwrite is zero, then the value of name is not changed. This function
  makes copies of the strings pointed to by name and value (by contrast with putenv(3)).


# ============================================================================
#{ LINUX SYS ADMIN
={============================================================================
*kt_linux_sysa_001* sys: cannot execute a file in cdrom, fstab

When tried to run a shell script on a cdrom, got "permission denied" even if it has excutable and
run it as a root. The problem was it was mounted as a read-only. The solution is to edit /etc/fstab
to mount it with exec option as:

# /etc/fstab: static file system information.
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
/dev/sr0        /media/cdrom0   udf,iso9660 user,noauto,exec   0       0

the filesystem is mounted with the noexec option, so the execute permission bits on all files are
ignored, and you cannot directly execute any program residing on this filesystem. Note that the
noexec mount option is implied by the user option in /etc/fstab. ... If you use user and want to
have executable files, use user,exec.


# ============================================================================
#{
={============================================================================
*kt_linux_sete_001* ubuntu: virtualbox

{graphic-issue}
When see a problem with NVIDIA, need to install a driver for ubuntu manually:

http://askubuntu.com/questions/141606/how-to-fix-the-system-is-running-in-low-graphics-mode-error

In short, get console and do:

sudo apt-get install nvidia-current

Also, install the guest addition:

Click on Install Guest Additions from the Devices menu and all will be done automatically.


{sharing-between-os}
http://www.virtualbox.org/manual/ch04.html#sharedfolders

# set sharing folder on the host using virtual box menu and reboot
#
select "Shared folders" from the "Devices" menu, or click on the folder icon on the status bar in
the bottom right corner.

# shared folder is
#
/media/sf_myfiles 

# add group permission when see access error when reading shared folder
#
Access to auto-mounted shared folders is only granted to the user group vboxsf, which is created by
the VirtualBox Guest Additions installer. Hence guest users have to be member of that group to have
read/write access or to have read-only access in case the folder is not mapped writable.

sudo usermod -a -G vboxsf {username}


{sharing-using-samba}
<1> Have samba installation, setting, and user. This is simple and general and see as an example.
http://www.sitepoint.com/ubuntu-12-04-lts-precise-pangolin-file-sharing-with-samba/

sudo apt-get install samba samba-common system-config-samba winbind

change winbind setting

For samba server settings: Workgroup. (this is domain) This field should be the same value as that
used by your Windows Workgroupi.e if your WIndows Users are members of the 'Home' workgroup, type
'Home' in this field.  

For samba users: use windows user name.


<2> By default, VB uses Host-only networking and this means host(windows) cannot see guest. To
enable for host to see guest, change to 'bridged network' as shown in:

6.4. Bridged networking
http://www.virtualbox.org/manual/ch06.html

So change it in setting menu of VB and restart VM. Get IP and can access guest from host windows.

note: must have two network adaptors: NAT for internet and bridged for samba between host and guest.
But cannot ssh to other host. seems firewall problem and when back to host only network, ssh works
but not sharing as host only net gives private net ip.


==============================================================================
*kt_linux_sete_002*	ubuntu: workspace {shortcuts}

https://help.ubuntu.com/community/KeyboardShortcuts

{workspaces}
Known as virtual desktops. You can switch between workspaces with hotkeys by pressing Ctrl-Alt-arrow
key. 

To move apps to other workspaces, press Ctrl-Alt-Shift-arrow key.

{shortcuts}
C-A-T		" open a terminal

{compiz}
To have window like win management: win arrow keys
http://www.howtoforge.com/install-compiz-on-the-unity-desktop-on-ubuntu-12.04-precise-pangolin


==============================================================================
*kt_linux_sete_003*	ubuntu: samba

sudo service smbd start
sudo service smbd stop
sudo service smbd restart


To see what are shared:

$ smbclient -L //106.1.8.6/
Enter keitee.park's password:
Domain=[SERILOCAL] OS=[Unix] Server=[Samba 3.6.3]

        Sharename       Type      Comment
        ---------       ----      -------
        IPC$            IPC       IPC Service (rockford server (Samba, Ubuntu))
        dsk1            Disk      Main Disk
Domain=[SERILOCAL] OS=[Unix] Server=[Samba 3.6.3]

        Server               Comment
        ---------            -------

        Workgroup            Master
        ---------            -------
        WORKGROUP


==============================================================================
*kt_linux_sete_004*	ubuntu: nfs

sudo apt-get install nfs-kernel-server

You can configure the directories to be exported by adding them to the /etc/exports file. For
example:

/ubuntu  *(ro,sync,no_root_squash)
/home    *(rw,sync,no_root_squash)

You can replace * with one of the hostname formats. Make the hostname declaration as specific as
possible so unwanted systems cannot access the NFS mount.

To start the NFS server, you can run the following command at a terminal prompt:
sudo /etc/init.d/nfs-kernel-server start


==============================================================================
*kt_linux_sete_005*	ubuntu: check running services

sudo service --status-all


==============================================================================
*kt_linux_sete_006*	ubuntu: connect from windows remote desktop

sudo apt-get install xrdp

Then run windows remote desktop and connect using ip or hostname. That's it.


={============================================================================
*kt_linux_sete_007*	ubuntu: cpuinfo

cat /proc/cpuinfo 


={============================================================================
*kt_linux_sete_008* ubuntu: change default application setting

Change a map between file type and default application in:

/usr/share/applications/defaults.list -> /etc/gnome/defaults.list


={============================================================================
*kt_linux_sete_100* gnome: windows

{activate-button}
To access your windows and applications, click the Activities button, or just move your mouse
pointer to the top-left hot corner. You can also press the Super key on your keyboard. You can see
your windows and applications in the overview. You can also just start typing to search your
applications, files, folders and the web. note: this is window key.


{tile-windows}
Super+Left     " to left
Super+Right    " to right
Super+Up       " to max
Supaer+Down    " back to original size

{lock-screen}
Super-L


{workspace}
Open the Activities overview.

Click and drag the window toward the right of the screen.

The workspace selector will appear.

Drop the window onto an empty workspace. This workspace now contains the window you have dropped,
and a new empty workspace appears at the bottom of the workspace selector.


{keyboard-shortcuts}
https://help.gnome.org/users/gnome-help/stable/shell-keyboard-shortcuts.html.en


={============================================================================
*kt_linux_sete_101* gnome: terminal

{gnome-terminal}
I have just found that you can open new terminals using 'gnome-terminal'.  You can open multiple
windows and multiple tabs like this:

gnome-terminal --window --tab --window --tab --tab

<key-shortcuts>
New Tab        Shift+Ctrl+T
Close Tab      Shift+Ctrl+W

note: this is to open new terminal
New Window     Shift+Ctrl+N
Close Window   Shift+Ctrl+Q

Copy           Ctrl+Shift+C
Paste          Ctrl+Shift+V

Switch to Previous Tab     Ctrl+Page Up
Switch to Next Tab         Ctrl+Page Down

note: this is to 'move'
Move Tab to the Left       Shift+Ctrl+Page Up
Move Tab to the Right      Shift+Ctrl+Page Down

Switch to Tab 1            Alt+1
Switch to Tab N            Alt+N


={============================================================================
*kt_linux_sete_102* win xserver

{win-xserver}
I stumbled across " VcXsrv Windows X Server" - that seems to be the well maintained and if you want
you can compiled yourself (not that you need to).

http://sourceforge.net/projects/vcxsrv/?source=directory

If you chose to use it, you need to run VcXsrc without OpenGL support but it works - see highlighted
text below ...

"C:\Program Files\VcXsrv\vcxsrv.exe" :0 -ac -terminate -lesspointer -multiwindow -clipboard -nowgl

Note: this server is better since it seems to have more fonts and shows gvim properly than xming
which is old xserver. It replace xming and seems to use the rest since use xlanuch. 'One window' do
not works but 'multiple windows' works.

http://alexcooper.co.uk/blog/2013/09/using-vcxsrv-putty-remote-x-windows/


={============================================================================
*kt_linux_sete_103* firefox key shortcuts

Ctrl+D                     Add Bookmark
Ctrl+B or Ctrl+I           Bookmarks

Backspace or Alt+<-        Back
Shft+Backspace or Alt+->   Forward

Ctrl+W or Ctrl+F4          Close Tab
Ctrl+T                     New Tab
Ctrl+Tab or Ctrl+PageDown  Next Tab

F5 or Ctrl+R               Reload


={============================================================================
*kt_linux_sete_200* which to install?

{pdf-viewer}
Okular


# ============================================================================
#{
={============================================================================
*kt_linux_ref_001* references

{ref-UNP}
Richard Stevens. Unix Network Programming Vol 2 Addision Wesley. 2nd Ed.
http://www.kohala.com/start/unpv22e/unpv22e.html

{ref-LPI}
The Linux Programming Interface: A Linux and UNIX System Programming Handbook 
Michael Kerrisk (Author) 

For sources, http://www.man7.org/tlpi/


-------------------------------------------------------------------------------
Copyright: see |ktkb|  vim:tw=100:ts=3:ft=help:norl:

