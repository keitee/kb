*kt_linux*                                                           tw=100, utf-8

/^[#=]{ 

#{ tool
#{ gdb
#{ linux-core
#{ syscall
#{ sysadmin
#{ syssetup

|kt_linux_bash_000| sh-reference
*kt_linux_bash_005* sh-quote
*kt_linux_bash_019* sh-pipeline sh-subshell
*kt_linux_bash_019* sh-pipeline-case-problem
*kt_linux_bash_019* sh-list-construct
*kt_linux_bash_021* sh-script
*kt_linux_bash_021* sh-exit-status
*kt_linux_bash_021* sh-return-value-from-script
*kt_linux_bash_001* sh-looping-construct sh-compound-command
*kt_linux_bash_001* sh-looping-construct-getopt sh-parse-arg
*kt_linux_bash_001* sh-conditional-construct
*kt_linux_bash_001* sh-conditional-expression
*kt_linux_bash_001* sh-pattern-match
*kt_linux_bash_001* sh-function
*kt_linux_bash_001* sh-variable
*kt_linux_bash_001* sh-positional-parameters
*kt_linux_bash_001* sh-variable-array
*kt_linux_bash_001* sh-expansion
*kt_linux_bash_001* sh-expansion-brace
*kt_linux_bash_001* sh-expansion-tilde
*kt_linux_bash_001* sh-expansion-parameter
*kt_linux_bash_001* sh-expansion-parameter-substitude
*kt_linux_bash_001* sh-expansion-command
*kt_linux_bash_001* sh-expansion-word sh-ifs
*kt_linux_bash_001* sh-expansion-arithmetic sh-expr
*kt_linux_bash_001* sh-expansion-process
*kt_linux_bash_001* sh-expansion-filename
*kt_linux_bash_001* sh-redirection
*kt_linux_bash_001* sh-redirection-here
*kt_linux_bash_001* sh-executing
*kt_linux_bash_024* sh-builtin
*kt_linux_bash_024* sh-builtin-base
*kt_linux_bash_024* sh-builtin-bash
*kt_linux_bash_024* sh-builtin-set
*kt_linux_bash_038* sh-builtin-declare
*kt_linux_bash_024* sh-builtin-job
*kt_linux_bash_024* sh-builtin-help
*kt_linux_bash_025* sh-special-variable
*kt_linux_bash_023* sh-invoking
|kt_linux_bash_030| sh-interactive-or-not
*kt_linux_bash_023* sh-command-history
*kt_linux_bash_032* sh-command-edit
*kt_linux_bash_032* sh-command-search
*kt_linux_bash_032* sh-command-readline-init

*kt_linux_bash_100* sh-tip-colour-prompt
*kt_linux_bash_101* sh-tip-nested-level
|kt_linux_bash_202| sh-tip-recursion-test
*kt_linux_bash_202* sh-tip-get-filenames-from-comment
*kt_linux_bash_202* sh-tip-search-number-range
*kt_linux_bash_202* sh-code-case
*kt_linux_bash_202* sh-code-case-use-config
*kt_linux_bash_202* sh-code-case-use-function-command


#{ tool
|kt_linux_tool_001| md5sum
|kt_linux_tool_002| tool-cut
|kt_linux_tool_003| tool-kill, tool-killall
|kt_linux_tool_004| dmesg
*kt_linux_tool_005* tool-uname
|kt_linux_tool_006| tool-cp
|kt_linux_tool_007| mkdir
|kt_linux_tool_008| strings
*kt_linux_tool_009* tool-head
|kt_linux_tool_009| tool-sort
*kt_linux_tool_009* tool-uniq
*kt_linux_tool_010* tool-grep
|kt_linux_tool_011| tool-find
|kt_linux_tool_012| make a empty file without touch
*kt_linux_tool_013* tool-xargs
|kt_linux_tool_014| tool-ssh
*kt_linux_tool_014* tool-putty
*kt_linux_tool_014* tool-putty-xwin
*kt_linux_tool_015* tool-pgrep, tool-pidof
|kt_linux_tool_018| tool-ls
|kt_linux_tool_019| tool-strace
|kt_linux_tool_020| tool-time-tool-date
|kt_linux_tool_021| tool-chmod
|kt_linux_tool_022| mknod
|kt_linux_tool_023| wc
|kt_linux_tool_024| tool-du, tool-df, tool-stat
*kt_linux_tool_024* tool-quota
|kt_linux_tool_025| tool-ln
*kt_linux_tool_026* tool-rsync
|kt_linux_tool_027| tool-awk
|kt_linux_tool_028| tool-sed, print a range
|kt_linux_tool_029| pyserial and grabserial
|kt_linux_tool_030| tool-diff and patch
|kt_linux_tool_031| tool-zip. tool-tar
|kt_linux_tool_032| tool-split tool-merge
*kt_linux_tool_032* tool-tr
*kt_linux_tool_032* tool-dd
|kt_linux_tool_033| getconf
|kt_linux_tool_034| tool-ps
*kt_linux_tool_034* tool-nproc
|kt_linux_tool_035| wget, curl
|kt_linux_tool_036| nc
|kt_linux_tool_037| port checks
|kt_linux_tool_016| tool-screen
|kt_linux_tool_038| tool-minicom
*kt_linux_tool_039* tool-mount
|kt_linux_tool_039| tool-mount --bind and nfs
*kt_linux_tool_025* tool-mount-fstab cannot execute a file in cdrom
*kt_linux_tool_039* tool-nfs
|kt_linux_tool_040| install
|kt_linux_tool_041| notify-send
*kt_linux_tool_042* tool-less
*kt_linux_tool_042* tool-tail: print multiple files with filename
|kt_linux_tool_044| mc
|kt_linux_tool_045| graphbiz
|kt_linux_tool_046| mktemp
*kt_linux_tool_047* tool-env
*kt_linux_tool_048* tool-dircolors
*kt_linux_tool_048* tool-tput
|kt_linux_tool_048| tool-gnome-terminal
*kt_linux_tool_049* tool-terminator
|kt_linux_tool_050| tool-watch
*kt_linux_tool_051* tool-github set-github
*kt_linux_tool_051* tool-font set-font
*kt_linux_tool_051* tool-echo
*kt_linux_tool_051* tool-echo-compile
*kt_linux_tool_051* tool-tcpdump
*kt_linux_tool_051* tool-wireshark
*kt_linux_tool_051* tool-swapon
*kt_linux_tool_051* tool-htop
*kt_linux_tool_051* tool-xterm: unknown terminal type
*kt_linux_tool_051* tool-lpstat: view and cancel print jobs
*kt_linux_tool_051* tool-i3wm:
*kt_linux_tool_051* tool-hexdump
*kt_linux_tool_051* tool-uptime
*kt_linux_tool_051* tool-lsof
*kt_linux_tool_051* tool-dirname tool-basename tool-readlink
*kt_linux_tool_051* tool-last
*kt_linux_tool_051* tool-fdisk
*kt_linux_tool_051* tool-fs-btrfs
*kt_linux_tool_051* tool-add-user tool-sudo
*kt_linux_tool_051* tool-virtualbox
*kt_linux_tool_051* tool-cron
*kt_linux_tool_100* tool-gprof
*kt_linux_tool_100* tool-chrome

*kt_linux_tool_100* tool-efence-issues
*kt_linux_tool_100* tool-efence-benchmark
*kt_linux_tool_100* tool-efence
*kt_linux_tool_100* tool-efence-duma
*kt_linux_tool_100* tool-efence-dmalloc
*kt_linux_tool_100* tool-efence-asan
*kt_linux_tool_000* tool-efence-asan-llvm-build
*kt_linux_tool_000* tool-efence-asan-gcc-build
*kt_linux_tool_100* tool-efence-others
*kt_linux_tool_100* tool-valgrind
*kt_linux_tool_100* tool-valgrind-memcheck
*kt_linux_tool_100* tool-valgrind-memcheck-doc
*kt_linux_tool_100* tool-valgrind-memcheck-case
*kt_linux_tool_100* tool-valgrind-massif

|kt_linux_tool_100| pkg-apt-xxx to get package
|kt_linux_tool_101| package: pkg-config
|kt_linux_tool_102| package: dpkg and install deb file

#{ make and build
*kt_linux_tool_140* make-cmake
*kt_linux_tool_140* make-cmake-basic
*kt_linux_tool_140* make-cmake-add-subdir
*kt_linux_tool_140* make-cmake-mix-c-and-cpp
*kt_linux_tool_140* make-cmake-compile-flag
*kt_linux_tool_140* make-cmake-include
*kt_linux_tool_140* make-cmake-link-flag

*kt_linux_tool_140* make-gmake-sample
*kt_linux_tool_140* make-gmake-intro
*kt_linux_tool_140* make-gmake-variable
*kt_linux_tool_140* make-gmake-variable-pattern
*kt_linux_tool_140* make-gmake-write
*kt_linux_tool_140* make-gmake-builtin-target
*kt_linux_tool_140* make-gmake-text-function

*kt_linux_tool_150* make-autoconf
*kt_linux_tool_150* make-autoconf-build-system
*kt_linux_tool_150* make-autoconf-model
*kt_linux_tool_150* make-autoconf-write
*kt_linux_tool_150* make-autoconf-macro
*kt_linux_tool_150* make-autoconf-tests
*kt_linux_tool_150* make-autoconf-test-result
*kt_linux_tool_150* make-autoconf-package
*kt_linux_tool_150* make-autoconf-run-configure

|kt_linux_tool_150| make-automake
*kt_linux_tool_150* make-automake-standard targets
*kt_linux_tool_150* make-automake-directory variable
*kt_linux_tool_150* make-automake-distcheck
*kt_linux_tool_150* make-automake-ex
*kt_linux_tool_150* make-automake-ex-explained
*kt_linux_tool_150* make-automake-variables
*kt_linux_tool_150* make-automake-ex-packages
*kt_linux_tool_150* make-automake-macros
*kt_linux_tool_150* make-automake-building programs and libraries
*kt_linux_tool_150* make-automake-building variables
*kt_linux_tool_150* automake: 08: program variables: case issue
*kt_linux_tool_150* automake: 09: scripts
*kt_linux_tool_150* automake: 15: test and make check
|kt_linux_tool_160| ccache

|kt_linux_tool_200| bin-:
*kt_linux_tool_201* tool-readelf
*kt_linux_tool_202* tool-nm-binutil
*kt_linux_tool_203* tool-objdump-binutil
|kt_linux_tool_204| tool-addr2line
|kt_linux_tool_205| binutil: ld
*kt_linux_tool_250* tool-busybox
*kt_linux_tool_300* tool-apache-ivy

#{ gdb
|kt_linux_tool_300| gdb-term and links
*kt_linux_tool_300* gdb-check-built with debug symbols
*kt_linux_tool_300* gdb-sample session
*kt_linux_tool_300* gdb-start
*kt_linux_tool_300* gdb-start: options
*kt_linux_tool_300* gdb-start: repeatedly run a test in GDB, until it fails
*kt_linux_tool_300* gdb-start: what do during startup
*kt_linux_tool_300* gdb-start: init file
*kt_linux_tool_300* gdb-start: init file: ex
*kt_linux_tool_300* gdb-start: specify files
*kt_linux_tool_300* gdb-start: running under gdb
*kt_linux_tool_300* gdb-start: running under gdb: case
*kt_linux_tool_300* gdb-attach-debug already running process
*kt_linux_tool_300* gdb-thread: debug with multiple threads
*kt_linux_tool_300* gdb-thread: tid on ps and gdb
*kt_linux_tool_300* gdb-log
*kt_linux_tool_300* gdb-shell
*kt_linux_tool_300* gdb-cpp:
*kt_linux_tool_300* gdb-general and help
*kt_linux_tool_300* gdb-info
*kt_linux_tool_300* gdb-trace: call trace
*kt_linux_tool_300* gdb-trace: get a call trace promatically

*kt_linux_tool_300* gdb-sources, source path, and substitute
*kt_linux_tool_300* gdb-exem-memory
*kt_linux_tool_300* gdb-exem-print
*kt_linux_tool_300* gdb-exam: between address and code
*kt_linux_tool_300* gdb-exam: symbols
*kt_linux_tool_300* gdb-exam: symbols loading
*kt_linux_tool_300* gdb-exam: slib when with -g and without -g
*kt_linux_tool_300* gdb-exam: core when with -g and without -g
*kt_linux_tool_300* gdb-exam: step when with -g and without -g
*kt_linux_tool_300* gdb-slib: load, search
*kt_linux_tool_300* gdb-slib: case
*kt_linux_tool_300* gdb-case: debugging with strace and gdb in sandbox
|kt_linux_tool_300| gdb-remote:
|kt_linux_tool_300| gdb-remote: args
*kt_linux_tool_300* gdb-remote: remote or local?
*kt_linux_tool_300* gdb-remote: frontend: cgdb
*kt_linux_tool_300* gdb-remote: frontend: eclipse

|kt_linux_tool_300| gdb-core-setting
*kt_linux_tool_300* gdb-core-settting-run-commands when makes core
*kt_linux_tool_300* gdb-core-force-core-signal
*kt_linux_tool_300* gdb-core-force-core-command
*kt_linux_tool_300* gdb-core: stack frame
|kt_linux_tool_300| gdb-core-analysis-on-mips
|kt_linux_tool_300| gdb-step
*kt_linux_tool_309* gdb-break
*kt_linux_tool_309* gdb-break-set-location
*kt_linux_tool_309* gdb-break-set-location-in-cpp
*kt_linux_tool_309* gdb-break-set
*kt_linux_tool_309* gdb-break: conditional ex
*kt_linux_tool_309* gdb-break: info. get you quickly where
*kt_linux_tool_309* gdb-break: ignore
*kt_linux_tool_309* gdb-break: set in shared
*kt_linux_tool_309* gdb-break: set watch
*kt_linux_tool_309* gdb-break: set catch
*kt_linux_tool_310* gdb-break: unset
*kt_linux_tool_310* gdb-break: disable
*kt_linux_tool_310* gdb-break: run command on break
*kt_linux_tool_310* gdb-break: save
*kt_linux_tool_300* gdb-break: dprintf
*kt_linux_tool_310* gdb-dprintf

*kt_linux_tool_400* gdb-stl-step-into-code
*kt_linux_tool_400* gdb-stl-debug-mode
*kt_linux_tool_400* gdb-stl-pretty-printer-gdb-stl-views
*kt_linux_tool_400* gdb-stl: asm, registers, and colored 

#{ gcc
*kt_linux_gcc_400* gcc-libc-uclibc
*kt_linux_gcc_400* gcc-libc-uclibc-ld
*kt_linux_gcc_400* gcc-libc-uclibc--fail to find symbol
*kt_linux_gcc_400* gcc-options-directory-search
*kt_linux_gcc_400* gcc-options-link
*kt_linux_gcc_400* gcc-options-debug
*kt_linux_gcc_400* gcc-options-verbose
*kt_linux_gcc_400* gcc-options-code-gen
*kt_linux_gcc_400* gcc-options-optimization
*kt_linux_gcc_400* gcc-build-target-triplet
*kt_linux_gcc_400* gcc-build-failed-cases
*kt_linux_gcc_400* gcc-build-buildroot
*kt_linux_gcc_400* gcc-build-old-kernel
*kt_linux_gcc_400* gcc-build-options
*kt_linux_gcc_400* gcc-build-reference
*kt_linux_gcc_400* gcc-build-asan
*kt_linux_gcc_400* gcc-build-uclibc-build
*kt_linux_gcc_400* gcc-build-uclibc-build-asan
*kt_linux_gcc_400* gcc-build-cross-ng


#{ linux-performance
|kt_linux_perf_001| bootchart

#{ linux-db
*kt_linux_rmdb_001* sql-rollback

*kt_linux_core_002* linux-c-lib
|kt_linux_core_003| 
|kt_linux_core_004| linux-errno
|kt_linux_core_005| linux-error handling codes, thread-errno
*kt_linux_core_006* linux-portability
*kt_linux_core_006* linux-process-credential
|kt_linux_core_007| /dev/null
|kt_linux_core_008| score: file ownership and permissions

|kt_linux_core_050| signal
*kt_linux_core_051* signal-names
|kt_linux_core_051| signal: example: use signal as synchronization
|kt_linux_core_052| signal: kill: checking for the existence of a process
*kt_linux_core_052* signal-raise-call

|kt_linux_core_100| process
|kt_linux_core_101| process-creation
|kt_linux_core_102| process-termination
|kt_linux_core_103| process-monitor-child-process
*kt_linux_core_104* process-zombie *ex-interview*
|kt_linux_core_105| process-memory-layout
|kt_linux_core_106| process-virtual-memory, user and kernel stack, stack frame
*kt_linux_core_107* linux-process-environment-list
|kt_linux_core_108| process: group
|kt_linux_core_109| process-deamon
|kt_linux_core_110| process-exec-call <exec-wrapper-example>
*kt_linux_core_110* process-resource
*kt_linux_core_110* process-resource-case
*kt_linux_core_050* process-function-reentrant

*kt_linux_core_150* thread-and-process
*kt_linux_core_151* thread-source
*kt_linux_core_152* thread-errno
|kt_linux_core_153| thread-compile
*kt_linux_core_154* thread-create-and-exit
*kt_linux_core_155* thread-join-and-detach
|kt_linux_core_156| thread-id

|kt_linux_core_150| pthread {nptl}
|kt_linux_core_102| how to run three threads sequencially
|kt_linux_core_103| priority and schedule

*kt_linux_core_200* linux-io-model
|kt_linux_core_201| linux-io-extended
|kt_linux_core_202| linux-io-non-blocking
*kt_linux_core_202* linux-io-buffering
*kt_linux_core_203* linux-io-ex
*kt_linux_core_203* linux-io-dev-fd
*kt_linux_core_203* linux-io-temp-file
*kt_linux_core_203* linux-io-fs
*kt_linux_core_203* linux-io-fs-mount
|kt_linux_core_204| system resource limits

|kt_linux_core_200| ipc
|kt_linux_core_201| ipc: system v <ipcs-command>
|kt_linux_core_202| ipc: system v: shm
|kt_linux_core_203| ipc: server consideration

|kt_linux_core_250| ipc: posix

|kt_linux_core_300| ipc: socket: LPI 56

*kt_linux_core_101* ipc-pipe
*kt_linux_core_102* ipc-pipe-check-setup
|kt_linux_core_103| ipc: fifo
|kt_linux_core_104| ipc: semantics of read() and write() on pipes and fifo
|kt_linux_core_105| ipc: which one to use {semaphores-versus-pthreads-mutexes}

|kt_linux_core_200| ipc: sync: semaphore 
*kt_linux_core_201* sync-mutex
*kt_linux_core_202* sync: deadlock
*kt_linux_core_202* sync-cond: why condition variable in UNP example
*kt_linux_core_202* sync-cond: why condition variable in LPI example
*kt_linux_core_202* sync-cond: compare two examples
*kt_linux_core_202* sync-cond: condition variable
*kt_linux_core_202* sync-cond: order, spurious wakeup
*kt_linux_core_202* sync-cond: example:
*kt_linux_core_202* sync-cond: example: use single lock for api and callback
*kt_linux_core_202* sync: between processes
|kt_linux_core_220| sync: read-write lock

|kt_linux_core_230|  sync: file-lock

|kt_linux_core_240|  sync: common problems when use threads {race-condition}

|kt_linux_core_250|  sync: reentrant and thread-safe {thread-specific-data} {thread-local-storage}
|kt_linux_core_260|  sync: atomic operations {lock-free-programming}
|kt_linux_core_261|  sync: ref: locks aren't slow; lock contention is
|kt_linux_core_262|  sync: ref: always use a lightweight mutex {mutex-vs-semaphore}
|kt_linux_core_263|  conc: ref: lock-free code: a false sense of security
|kt_linux_core_264|  conc: ref: the free lunch is over 
*kt_linux_core_265* sync: case: subtle race
*kt_linux_core_266* sync: case: sync with no lock

|kt_linux_core_290|  ref: concurrency in C++. ch01
|kt_linux_core_291|  ref: concurrency in C++. ch02 {std::thread}
|kt_linux_core_293|  ref: concurrency in C++. ch03

|kt_linux_core_300|  case: own semaphore and mutex class using pthread cond-var {cqueue}
|kt_linux_core_301|  case: use of mutex and thread class
|kt_linux_core_302|  case: analysis of 200 and 201 case
|kt_linux_core_303|  case: msg q between threads


*kt_linux_core_401* lib-shared
*kt_linux_core_400* lib-how-static-works
*kt_linux_core_401* lib-dependancy
*kt_linux_core_400* lib-dl ld-library-path
*kt_linux_core_401* slib: --as-needed flag and link error
*kt_linux_core_402* shared-lib-search and resolve
*kt_linux_core_403* slib-case-problem in name resolve. crash
*kt_linux_core_404* slib-case-problem in name resolve. pthread stub 
*kt_linux_core_405* slib-dl-api
*kt_linux_core_405* slib-visibility
|kt_linux_core_406| slib-preload, debug and monitor ld
|kt_linux_core_407| shared library: further information
|kt_linux_core_408| shared library: md5sum
|kt_linux_core_409| shared library: inspect dynamic sections
|kt_linux_core_410| shared library: check libraries that process uses
*kt_linux_core_410* shared library: check libraries loading
*kt_linux_core_411* slib: case problem in open failure
*kt_linux_core_411* slib: points to enhance performance
*kt_linux_core_412* lib-ld.conf lib-ld.cache
*kt_linux_core_412* slib: as-needed and _GLOBAL_OFFSET_TABLE_

|kt_linux_core_500| sandbox

|kt_linux_core_600| linux-time
|kt_linux_core_601| time: conversion
|kt_linux_core_602| time: resolution, jiffies
|kt_linux_core_603| time: posix clock, realtime
*kt_linux_core_604* time: ns timestamp
|kt_linux_core_604| time: ms timestamp
|kt_linux_core_605| time: us timestamp
|kt_linux_core_606| time: sleep

|kt_linux_core_700| dbus
*kt_linux_core_700* dbus-libdbus
*kt_linux_core_700* dbus-bindings
*kt_linux_core_700* dbus-case
*kt_linux_core_700* dbus-case-service
|kt_linux_core_701| dbus-introspection
|kt_linux_core_702| dbus: dbus-send tool
|kt_linux_core_703| dbus: lsdbus tool
|kt_linux_core_704| dbus: dbus-monitor tool
|kt_linux_core_710| dbus: kdbus

|kt_linux_core_800| proc: /proc/mounts
|kt_linux_core_801| proc-status: /proc/PID/status
|kt_linux_core_802| proc-maps: /proc/PID/maps
*kt_linux_core_802* proc-maps: check shared libraries that process uses
|kt_linux_core_803| proc: /proc/PID/exe
|kt_linux_core_804| proc: /proc/PID/fd. which file does process open?
|kt_linux_core_813| proc: /proc/sys/kernel, system resource limits
|kt_linux_core_814| proc: /proc/stat
*kt_linux_core_814* proc: /proc/PID/cmdline, get process name

#{ syscall
*kt_linux_sysc_001* syscall list
|kt_linux_sysc_001| pause
|kt_linux_sysc_002| alarm
|kt_linux_sysc_003| getenv, setenv

#{ syssetup
*kt_linux_sete_001* set-github
*kt_linux_sete_001* set-resize-boot-partition
|kt_linux_sete_002| 
|kt_linux_sete_003| set-samba
*kt_linux_sete_005* admin: check running services
*kt_linux_sete_005* admin: check nvidia version
*kt_linux_sete_005* admin-gnome: check gnome version
*kt_linux_sete_005* admin-gnome: shortcuts
|kt_linux_sete_005| admin: firefox shortcuts
|kt_linux_sete_005| admin: tbird shortcuts
|kt_linux_sete_006| ubuntu: connect from windows remote desktop
|kt_linux_sete_007| ubuntu: cpuinfo
|kt_linux_sete_008| ubuntu: change default application setting

|kt_linux_sete_101| gnome: workspace
*kt_linux_sete_101* gnome: change wallpapers
|kt_linux_sete_105| set: local web server
*kt_linux_sete_105* set: update adobe flash plugin

|kt_linux_sete_200| which to install?

|kt_linux_refe_001|  references


# ============================================================================
#{
={============================================================================
*kt_linux_bash_000* sh-reference

http://www.gnu.org/software/bash/
http://www.gnu.org/software/bash/manual/
http://www.gnu.org/software/bash/manual/bash.html

http://mywiki.wooledge.org/BashGuide

Advanced Bash-Scripting Guide
https://wiki.kldp.org/HOWTO/html/Adv-Bash-Scr-HOWTO/
http://www.tldp.org/LDP/abs/html/

Bash Hackers Wiki Frontpage
http://wiki.bash-hackers.org/start


={============================================================================
*kt_linux_bash_005* sh-quote

3.1.2 Quoting

Quoting is used to remove the special meaning of certain characters or words to
the shell. Quoting can be used to disable special treatment for special
characters, to prevent reserved words from being recognized as such, and to
prevent parameter expansion.

// metacharacter
//    A character that, when unquoted, separates words. A metacharacter is a
//    blank or one of the following characters: ‘|’, ‘&’, ‘;’, ‘(’, ‘)’, ‘<’, or
//    ‘>’.

There are three quoting mechanisms: the escape character, single quotes, and
double quotes. 

* Escape Character: to remove the special meaning from a single character.
 
A non-quoted backslash ‘\’ is the Bash `escape-character`. It preserves the
literal value of the next `character` that follows

<ex>
$ echo You owe \$1250
You owe $1250


*sh-single-double-quote*
* Single Quotes: to inhibit `all` interpretation of a sequence of characters.

Enclosing `characters` in single quotes (‘'’) preserves the literal value of each
character within the quotes. A single quote `may not occur between single quotes`,
even when preceded by a backslash. 

<ex>
Quote all inside the single quote.

$ echo '<-$1250.**>; (update?) [y|n]'
<-$1250.**>; (update?) [y|n]


* Double Quotes: to suppress `most` of the interpretation of a sequence of
  characters.

Enclosing characters in double quotes (‘"’) preserves the literal value of all
characters within the quotes, with the exception of ‘$’, ‘`’, ‘\’, and, when
history expansion is enabled, ‘!’. 

The characters ‘$’ and ‘`’ retain their special meaning within double quotes
(see Shell Expansions). 

The backslash retains its special meaning only when followed by one of the
following characters: ‘$’, ‘`’, ‘"’, ‘\’, or newline. Within double quotes,
backslashes that are followed by one of these characters are removed.
  Backslashes preceding characters without a special meaning are left
  unmodified. A double quote may be quoted within double quotes by preceding it
  with a backslash. If enabled, history expansion will be performed unless an
  ‘!’ appearing in double quotes is escaped using a backslash. The backslash
  preceding the ‘!’ is not removed.

The special parameters ‘*’ and ‘@’ have special meaning when in double quotes
(see Shell Parameter Expansion). 

<ex>
Do not quote all (see below) so that `can use variables` inside the quoted.

$ echo '$USER owes <-$1250.**>; [ as of (`date +%m/%d`) ]'
$USER owes <-$1250.**>; [ as of (`date +%m/%d`) ]

$ echo "$USER owes <-$1250.**>; [ as of (`date +%m/%d`) ]"
parkkt owes <-250.**>; [ as of (01/26) ]

<ex>
To prevent globbing by shell before passing params to grep command.

$ grep '[0-9][0-9]*$' report2 report7

<ex>
ssh theyard "mysql -e "select upload_time from upload where parser_result != 'Not Yet Parsed' $1""      # not work
ssh theyard "mysql -e \"select upload_time from upload where parser_result != 'Not Yet Parsed' $1\""    # work


={============================================================================
*kt_linux_bash_019* sh-pipeline sh-subshell

3.2.2 Pipelines

A pipeline is a sequence of simple commands separated by one of the control
operators | or |&.

The format for a pipeline is

[time [-p]] [!] command1 [ | or |& command2 ] ...

The output of each command in the pipeline is connected via a pipe to the input
of the next command.  That is, each command reads the previous command's output.
This connection is performed before any redirections specified by the command.

If |& is used, command1's standard error, in addition to its standard output, is
connected to command2's standard input through the pipe; it is shorthand for
2>&1 |. This implicit redirection of the standard error to the standard output
is performed after any redirections specified by the command.

If the pipeline is 'not' executed asynchronously (see `list-construct`), the
shell 'waits' for all commands in the pipeline to complete.

*sh-pipeline-subshell*
`each-command` in a pipeline is executed in its own `subshell`. The exit status
of a pipeline is the exit status of the `last-command` in the pipeline, unless
the `pipefail-option` is enabled (see The Set Builtin). 


3.7.3 Command Execution Environment

note: shell's execution-environment

The shell has an `execution-environment`, which consists of the following:

* open files inherited by the shell at invocation, as modified by redirections
  supplied to the exec builtin

* the `current working directory` as set by cd, pushd, or popd, or inherited by
  the shell at invocation

* the file creation mode mask as set by umask or inherited from the shell's
  parent

* current traps set by trap

* `shell parameters` that are set by variable assignment or with set or inherited
  from the shell's parent in the environment

* shell functions defined during execution or inherited from the shell's parent
  in the environment 

* options enabled at invocation (either by default or with command-line arguments)
  or by set 

* options enabled by shopt (see The Shopt Builtin)

* shell aliases defined with alias (see Aliases)

* various process IDs, including those of background jobs (see Lists), the value
  of $$, and the value of $PPID 


note: subshell's execution-environment

When a simple command other than a builtin or shell function is to be
executed, it is invoked in a `separate execution-environment` that consists of
the following. Unless otherwise noted, the values are `inherited from` the
`shell`.

* the shell's open files, plus any modifications and additions specified by
  redirections to the command
* the current working directory
* the file creation mode mask

* shell variables and functions `marked-for-export`, along with variables
  exported for the command, passed in the `environment` (see Environment)

A command invoked in this separate environment cannot affect the shell's
execution environment.

`command-substitution`, commands `grouped-with-parentheses`, and
`asynchronous-commands` are invoked in a `subshell-environment` that is a
duplicate of the shell environment, except that traps caught by the shell are
reset to the values that the shell inherited from its parent at invocation. 

Builtin commands that are invoked as part of a pipeline are also executed in a
`subshell-environment`. Changes made to the subshell environment cannot affect
the shell's execution environment.

Subshells spawned to execute command substitutions inherit the value of the -e
option from the parent shell. When not in POSIX mode, Bash clears the -e option
in such subshells.


3.7.4 Environment

When a program is invoked it is given an array of strings called the
`environment`. This is a list of `name-value` pairs, of the form name=value.

Bash provides several ways to manipulate the environment. 

On invocation, the shell scans its own environment and creates a parameter for
each name found, automatically marking it for `export to child processes`
Executed commands inherit the environment. 

The export and ‘declare -x’ commands allow parameters and functions to be
`added to` and deleted from the environment. If the value of a parameter in the
environment is modified, the new value becomes part of the environment,
replacing the old. The environment inherited by any executed command consists
  of the shell's initial environment, whose values may be modified in the
  shell, less any pairs removed by the unset and ‘export -n’ commands, plus
  any additions via the `export` and ‘declare -x’ commands.

The environment for any simple command or function may be augmented temporarily
by prefixing it with parameter assignments, as described in Shell Parameters.
These assignment statements affect only the environment seen by that command.

If the -k option is set (see The Set Builtin), then all parameter assignments
are placed in the environment for a command, not just those that precede the
command name.

When Bash invokes an external command, the variable ‘$_’ is set to the full
pathname of the command and passed to that command in its environment. 

<ex>
When works on some issue, have to set some environment variables before running
some commands which are affected by these settings. This is repetative and
thought it's better to have a script to set these.

#!/bin/sh
export LD_LIBRARY_PATH="/opt/zinc-trunk/lib:${LD_LIBRARY_PATH}"
export LD_PRELOAD=/usr/local/lib/libdirectfb.so:/lib/libpthread.so.0

Great. Now runs commands like this:

$ ./run-settings.sh
$ commands..

However, start to see that the expected shared library are not loaded. Why?
Suspects gdb settings or configuration for not running as expected since it was
an investigation under gdb. 

After all, the reason is that by setting env vars in the shell script and run
commands outside of script then command don't see these settings done in the
script. Use env vars with default settings.

note:
The shell script uses subshell.


3.2.4.3 Grouping Commands

Bash provides two ways to group a list of commands to be executed `as-a-unit`.
When commands are grouped, redirections may be applied to the entire command
list. For example, the output of all the commands in the list may be redirected
to a single stream.

()
( list )

Placing a list of commands between `parentheses` causes a
`subshell-environment` to be created, and each of the commands in list to be
executed `in that subshell` Since the list is executed in a subshell, variable
assignments do not remain in effect after the subshell completes.

<ex>
$ global strlen                 # strlen() is not found
$ (cd /usr/src/lib; gtags)      # library source
$ (cd /usr/src/sys; gtags)      # kernel source


{}
{ list; }

Placing a list of commands between curly braces causes the list to be executed
in the `current-shell` context. No subshell is created. The semicolon (or
    newline) following list is required. 

There is a subtle difference between these two constructs due to historical
reasons. The braces are reserved words, so they `must-be-separated` from the
list by blanks or other shell metacharacters. The parentheses are operators, and
are recognized as separate tokens by the shell even if they are not separated
from the list by whitespace.

The exit status of both of these constructs is the exit status of list. 

<ex>
Must be spaces between {}.

$ { date;time; }
$ { date;time; } > mylog


={============================================================================
*kt_linux_bash_019* sh-pipeline-case-problem

The task is that if see two log lines in order in a log and then it is to flag
it up as error.

Line xx: ... > VRM_JOB_START: jtVRM=3, jhVRM=0x3000385
Line xx: > XTVFS_WriteEx failed,returned value: E_FSSERVER_STATUS_INVALID_ALIGNMENT 290:

Use function and pipe?

scan_for()
{
  while read line
  do
      echo "$line"
  done
}

A=``mycat $1 | egrep -n "(VRM_JOB_START:|XTVFS_WriteEx)" | scan_for``


The $A will have all lines which are echoed from scan_for(). Okay, expand
this.

scan_for()
{
  STARTED=0
  MATCHED=0

  while read line
  do
    case $line in
      *jtVRM=3*)
          if [ $STARTED -ne 1 ]
          then
             STARTED=1;
             STARTLINE=``echo $line``;
          fi
          ;;
       *XTVFS_WriteEx*E_FSSERVER_STATUS_INVALID_ALIGNMENT*)
          if [ $STARTED -eq 1 ]
          then
             MATCHED=1;
             LASTLINE=``echo $line``;

             # note: can see variables as expected
             echo "started = $STARTED, MATCHED=$MATCHED";

          break;
          fi
          ;;
    esac
  done

  # note: however, see variables are '0' WHY?
  echo "started = $STARTED";
  echo "matched = $MATCHED";

  if [[ $STARTED -eq 1 && $MATCHED -eq 1 ]]
  then
      echo "$LASTLINE";
  fi
}


The reason:

From *sh-pipeline-subshell*

`each-command` in a pipeline is executed in its own `subshell`. The exit status
of a pipeline is the exit status of the `last-command` in the pipeline, unless
the `pipefail-option` is enabled (see The Set Builtin). 

See for more details:
http://mywiki.wooledge.org/BashFAQ/024
I set variables in a loop that's in a pipeline. Why do they disappear after the
loop terminates? Or, why can't I pipe data to read?

Workarounds

Use ProcessSubstitution (Bash/Zsh/Ksh93 only). note: OK 
http://mywiki.wooledge.org/ProcessSubstitution


The problem is changes in subshell cannot be seen in parent. How to solve? Use
redirection from subshell.


<xtvfs_write_error2>
##P2
## PicassoShutdownFail: Park, Kyoung-Taek, 09/01/2012
## Check for XTVFS error on a recording

mycat()
(
    F=cat
    echo "$1" | grep -q '.gz$' && F=zcat
    $F $1
)

scan_for()
{
    while read line	
    do
		case $line in
			*jtVRM=*)
				LINEJOB=`echo $line | cut -d':' -f1`;
				LASTJOB=`echo $line | cut -d'=' -f2 | cut -d',' -f1`;
				;;
			*XTVFS_WriteEx*E_FSSERVER_STATUS_INVALID_ALIGNMENT*)
				if [ $LASTJOB -eq 7 ]
				then
					LASTLINE=`echo $line`;
					echo "JOB $LINEJOB, $LASTJOB, $LASTLINE";
					break;
				fi
				;;
		esac
    done
}

if [ "$1" = "-v" ]
then
    echo "XTVFS Write error causing failed recordings in Soak tests - Jira SI-4063" 
    exit 0
fi
A=`mycat $1 | egrep -n "(VRM_JOB_START:|XTVFS_WriteEx)" | scan_for`

if [ "$A" = "" ]
then
echo 0
exit 0
fi

L=`echo $A | cut -d':' -f1`
X=`echo $A | cut -d'.' -f1 | cut -d'^' -f2`
Y=`echo $A | cut -d'>' -f2 | cut -d',' -f1-2`
echo "$L Time:$X, $Y"
echo "XTVFS Write error causing failed recordings in Soak tests - Jira SI-4063"
exit 1


The same in python:

#!/usr/bin/python
import sys, gzip, os
#
# This script is trying to find a situation where player prints a milestone
# like:
# M:player_api_session.c F:PLAYER_API_Session_Play L:3672 > PLAY 
# but no:
# M:player_api_session.c F:PLAYER_PRV_Session_Play L:5652 > PLAY on target MAIN_TARGET 
#
# This two milestones always come in sequence if the second one is missing 
# the thread is probably blocked.
#

def processFile (fileName):
    WAIT_FOR_PLAY = 0
    WAIT_FOR_PLAY_ON_TARGET = 1
    if fileName.endswith('gz'):
        fp = gzip.open(fileName, 'r')
    else:
        fp = open (fileName, 'r')
    status = WAIT_FOR_PLAY
    lineNo = 1
    errorLineNo = 0
    for line in fp:
        if status == WAIT_FOR_PLAY:
            if -1 != line.find ("PLAYER_API_Session_Play"):
                line = line.strip ()
                if line.endswith ("PLAY"):
                    pos1 = line.find ("^")
                    pos2 = line.find ("!")
                    timePlay = float (line [pos1+1: pos2])
                    status = WAIT_FOR_PLAY_ON_TARGET
                    errorLineNo = lineNo
        else:
            if -1 != line.find ("PLAYER_PRV_Session_Play"):
                if -1 != line.find ("PLAY on target"):
                    pos1 = line.find ("^")
                    pos2 = line.find ("!")
                    timePlayOnTarget = float (line [pos1+1: pos2])
                    if (timePlayOnTarget - timePlay > 30.0):
                        break
                    status = WAIT_FOR_PLAY
        lineNo += 1

    if status == WAIT_FOR_PLAY_ON_TARGET:                   
        return errorLineNo 
    else:
        return -1

def processFolder (folder, fileName):
    lineNo = -1
    fileName = os.path.join (folder, fileName)

    if os.path.isfile (fileName):
        lineNo = processFile (fileName)

    return lineNo

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == '-v':
        verbose = True
        fileName=sys.argv[2]
        print 'Player thread blocked NGDEV-34065.'
        sys.exit(0)
    else:
        verbose = False
        fileName=sys.argv[1]

    lineNo = processFolder (".", fileName)
    if lineNo == -1:
        print "0 0"
        sys.exit (0)
    else:
        print str(lineNo) + " Player blocked"
        sys.exit (1)


={============================================================================
*kt_linux_bash_019* sh-list-construct

3.2.3 Lists of Commands

A list is a sequence of one or more `pipelines` separated by one of the
operators ;, &, &&, or ||, and optionally terminated by one of ;, &, or a
newline. 

note: means do on multiple pipelines

<semicolon>
Commands separated by a ; are executed 'sequentially'; the shell waits for each
command to terminate in turn. The return status is the exit status of the last
command executed. 

If a command is terminated by the control operator ‘&’, the shell executes the
command `asynchronously` in a subshell. This is known as executing the command
in the `background`. The shell does not wait for the command to finish, and the
return status is 0 (true). 

// When job control is not active (see Job Control), the standard input for
// asynchronous commands, in the absence of any explicit redirections, is
// redirected from /dev/null. 


<and-and-or>
AND and OR lists are sequences of one or more pipelines separated by the control
operators ‘&&’ and ‘||’, respectively. AND and OR lists are executed with left
associativity.

An AND list has the form

command1 && command2

command2 is executed if, and only if, command1 returns an exit status of `zero`.

An OR list has the form

command1 || command2

command2 is executed if, and only if, command1 returns a non-zero exit status.

The return status of AND and OR lists is the exit status of the last command
executed in the list. 

<ex>
if [ "$(whoami)" != 'root' ]; then
   echo "You have no permission to run $0 as non-root user."
   exit 1;
fi

With Bash, you can shorten this type of construct. The compact equivalent of the
above test is as follows:

[ "$(whoami)" != 'root' ] && ( echo you are using a non-privileged account; exit 1 )

Similar to the "&&" expression which indicates what to do if the test proves
true, "||" specifies what to do if the test is false.

<ex>
* Check if loginscript is not set. If it's set(defined) then run -f. 
  * if file exist then test return true so no fail() call.
  * if file not exist then test return false so run fail() call.
* If loginscript is not set, no run -f and test return true(0) so no fail() call.

[[ -z "$loginscript" || -f "$loginscript" ]] ||
    fail "Loginscript '$loginscript' doesn't exist."

Progress to next cond when only cond1 is true. [[ cond1 || cond2 ]] || exec

<ex>
To see if meet at least one of options.

[[ -n "$release" || -n "$zincdir" || -n "$jenkinshost" || -n "$jenkinsurl" ||
   -n "$galliumurl" || -n "$localgallium" || -n "$platformconfigurl" ||
   "$virtualinputdriver" -ne 0 || -n "$irdriver" || "$enabledbustcp" -ne 0 ||
   "$directfbsrc" -ne 0 || "$killzincdaemons" -ne 0 || -n "$toolnames" ||
   -n "$networkmanager" || -n "$loginscript" || -n "$startupscript" ||
   "$forcefirsttime" -ne 0 || "$needsreboot" -ne 0 || "$vidmemcapture" -ne 0 ]] ||
{
    fail "You must specify at least one of -[ZzjugcdeEtsknlbTfrv]."
}

<ex>
function x {
  ...

  [[ 
    "$cacheOptionCorrectAtExpectedPosition" = 1 &&
    "$cacheValueCorrectAtExpectedPosition" = 1 &&
    "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
    "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
    "$jarOptionCorrectAtExpectedPosition" = "1" && 
    "$jarValueCorrectAtExpectedPosition" = "1" && 
    "$urlOptionCorrectAtExpectedPosition" = "1" && 
    "$urlValueCorrectAtExpectedPosition" = "1" 
  ]] 
}

From the debug output, shows only 4 since stops as soon as see flase. This may
be confusing when debugging.

+ [[ 1 = 1 ]]
+ [[ 1 = 1 ]]
+ [[ 1 = 1 ]]
+ [[ 0 = 1 ]]

<ex>
The lsr-config returns "0/1" or "true/false" when use --bool and the problem is
when a key is boolean and is not set in a database meaning 0, it still returns
"0" which is not NULL string. So "if [ -z ]" check always becomes false.

The solution is that use variable's value depending on key value to set actual
variable.

#!/bin/sh

# when key is not set or returns "false" then run "echo false" and ends here.
# enable_yv_media="".
#
# when key set and returns "true" then enable_yv_media="true" since lsr-config
# returns zero exit status.

enable_yv_media=""
[ "false" = "$(lsr-config --bool platform.settings.enable-yv-media || echo false)" ] || {
    enable_yv_media="true"
}

USE_YV_MEDIA_ROUTER="$enable_yv_media"

if [ -z "$USE_YV_MEDIA_ROUTER" ]; then
	echo "NOT SET. $USE_YV_MEDIA_ROUTER. WILL USE OEM MR"
else
	echo "SET. $USE_YV_MEDIA_ROUTER. WILL USE YV MR"
fi

However, if use this then goes wrong.

if [ "false" = "$(lsr-config --bool platform.settings.webkit-use-media-router || echo false)" ]; then
    // things to do when true case


={============================================================================
*kt_linux_bash_021* sh-script

3.8 Shell Scripts

A shell script is a text file containing shell commands. When such a file is
used as the first non-option argument when invoking Bash, and neither the -c nor
-s option is supplied (see Invoking Bash), Bash reads and executes commands from
the file, then exits. This mode of operation creates a `non-interactive` shell.

The shell first searches for the file in the current directory, and looks in the
directories in $PATH if not found there.

When Bash runs a shell script, it sets the special parameter 0 to the name of
the file, rather than the name of the shell, and the positional parameters are
set to the remaining arguments, if any are given. If no additional arguments are
supplied, the positional parameters are unset.

A shell script may be `made-executable` by using the chmod command to turn on
the execute bit. When Bash finds such a file while searching the $PATH for a
command, it spawns a `subshell` to execute it. In other words, executing

filename arguments

is equivalent to executing

bash filename arguments

if filename is an executable shell script. This subshell reinitializes itself,
   so that the effect is as if a new shell had been invoked to interpret the
   script, with the exception that the locations of commands remembered by the
   parent (see the description of hash in Bourne Shell Builtins) are retained by
   the child.

Most versions of Unix make this a part of the operating system’s command
execution mechanism. If the first line of a script begins with the two
characters ‘#!’, the remainder of the line specifies an interpreter for the
program. Thus, you can specify Bash, awk, Perl, or some other interpreter and
write the rest of the script file in that language.

The arguments to the interpreter consist of a single optional argument following
the interpreter name on the first line of the script file, followed by the name
of the script file, followed by the rest of the arguments. Bash will perform
this action on operating systems that do not handle it themselves. Note that
some older versions of Unix limit the interpreter name and argument to a maximum
of 32 characters.

Bash scripts often begin with #! /bin/bash (assuming that Bash has been
    installed in /bin), since this ensures that Bash will be used to interpret
the script, even if it is executed under another shell. 


={============================================================================
*kt_linux_bash_021* sh-exit-status

3.2.1 Simple Commands

The return status (see Exit Status) of a simple command is its exit status as
provided by the POSIX 1003.1 waitpid function, or 128+n if the command was
terminated by signal n. 

*sh-exit-status*
The $? expands to the exit status of the 'most' recently executed foreground
pipeline. Can be used in test construct.


3.7.5 Exit Status

The exit status of an executed command is the value returned by the waitpid
system call or equivalent function. Exit statuses fall between 0 and 255,
though, as explained below, the shell may use values above 125 specially. 
  
For the shell's purposes, a command which exits with a `zero-exit-status` has
`succeeded`. A non-zero exit status indicates failure. This seemingly
counter-intuitive scheme is used so there is one well-defined way to indicate
success and a variety of ways to indicate various failure modes. When a command
terminates on a fatal signal whose number is N, Bash uses the value 128+N as the
exit status.

If a command is not found, the child process created to execute it returns a
status of 127. If a command is found but is not executable, the return status is
126.

If a command fails because of an error during expansion or redirection, the exit
status is greater than zero.

All of the Bash builtins return an exit status of zero if they succeed and a
non-zero status on failure, so they may be used by the conditional and list
constructs. All builtins return an exit status of 2 to indicate incorrect usage. 

<ex>
if which ssh-copy-id >/dev/null;
then  
  # ssh-copy-id always returns failure
  ssh-copy-id -i $privatekey root@$stbip || true
fi

<ex>
The grep returns 0(success) when found and 1(fail) when not found.

$ if grep KB readme; then echo "found"; fi
readme:1:this is KB for keitee.
found
$ 

$ if ! grep KB readme; then echo "found"; fi
readme:1:this is KB for keitee.
$ 

$ if true; then echo "found"; fi
found

<ex>
while kill -0 "${parent_pid}" &>/dev/null;
do
   usleep 500
done

<ex>
#include <stdio.h>
#include <stdlib.h>

int main(int argc, char** argv)
{
  printf(" this is a main function.\n");

  if( 2 == argc )
    exit(EXIT_FAILURE);

  exit(EXIT_SUCCESS);
}

$ ./a.out && echo "return success"
 this is a main function.
return success
$ 

$ echo $?
0

$ ./a.out xx && echo "return success"
 this is a main function.
$ 

$ echo $?
1

<ex>
Stash away the output of program and only use its exit status to see if it has
found a element in the system.

$ gst-inspect-1.0 nexussink &>/dev/null && echo "true" || echo "false"
true
$ gst-inspect-1.0 xx &>/dev/null && echo "true" || echo "false"
false
$   


<ref>
http://bencane.com/2014/09/02/understanding-exit-codes-and-how-to-use-them-in-bash-scripts/

What are exit codes?

On Unix and Linux systems, programs can pass a value to their parent process
while terminating. This value is referred to as an exit code or exit status. On
POSIX systems the standard convention is for the program to pass 0 for
successful executions and 1 or higher for failed executions.

Why is this important? 

If you look at exit codes in the context of scripts written to be used for the
command line the answer is very simple. Any script that is useful in some
fashion will inevitably be either used in another script, or wrapped with a bash
one liner. 
     
This becomes especially true to check the status code to determine whether that
script was successful or not.

On top of those reasons, exit codes exist within your scripts even if you don't
define them. By not defining proper exit codes you could be falsely reporting
successful executions which can cause issues depending on what the script does.

What happens if I don't specify an exit code

In Linux any script run from the command line has an exit code. With Bash
scripts, if the exit code is not specified in the script itself the exit code
used will be the exit code of the 'last' command run. To help explain exit codes
a little better we are going to use a quick sample script.

Sample Script:

#!/bin/bash
touch /root/test
echo created file

The above sample script will execute both the touch command and the echo
command. When we execute this script (as a non-root user) the touch command will
fail, ideally since the touch command failed we would want the exit code of the
script to indicate failure with an appropriate exit code. To check the exit code
we can simply print the $? special variable in bash. This variable will print
the exit code of the last run command.

Execution:

$ ./tmp.sh 
touch: cannot touch '/root/test': Permission denied
created file
$ echo $?
0

As you can see after running the ./tmp.sh command the exit code was 0 which
indicates success, even though the touch command failed. The sample script runs
two commands touch and echo, since we did not specify an exit code the script
exits with the exit code of the last run command. In this case, the last run
command is the echo command, which did execute successfully.

Script:

#!/bin/bash
touch /root/test

If we remove the echo command from the script we should see the exit code of the
touch command.

Execution:

$ ./tmp.sh 
touch: cannot touch '/root/test': Permission denied
$ echo $?
1

As you can see, since the last command run was touch the exit code reflects the
true status of the script; failed.  Using exit codes in your bash scripts

While removing the echo command from our sample script worked to provide an exit
code, what happens when we want to perform one action if the touch was
successful and another if it was not. Actions such as printing to stdout on
success and stderr on failure.

Testing for exit codes

Earlier we used the $? special variable to print the exit code of the script. We
can also use this variable within our script to test if the touch command was
successful or not.

Script:

#!/bin/bash

touch /root/test 2> /dev/null

if [ $? -eq 0 ]
then
  echo "Successfully created file"
else
  echo "Could not create file" >&2
fi

In the above revision of our sample script; if the exit code for touch is 0 the
script will echo a successful message. If the exit code is anything other than 0
this indicates failure and the script will echo a failure message to stderr.

Execution:

$ ./tmp.sh
Could not create file

Providing your own exit code

While the above revision will provide an error message if the touch command
fails, it still provides a 0 exit code indicating success.

$ ./tmp.sh
Could not create file
$ echo $?
0

Since the script failed, it would not be a good idea to pass a successful exit
code to any other program executing this script. To add our own exit code to
this script, we can simply use the exit command.

Script:

#!/bin/bash

touch /root/test 2> /dev/null

if [ $? -eq 0 ]
then
  echo "Successfully created file"
  exit 0
else
  echo "Could not create file" >&2
  exit 1
fi

With the exit command in this script, we will exit with a successful message and
0 exit code if the touch command is successful. If the touch command fails
however, we will print a failure message to stderr and exit with a 1 value which
indicates failure.

Execution:

$ ./tmp.sh
Could not create file
$ echo $?
1

Using exit codes on the command line

Now that our script is able to tell both users and programs whether it finished
successfully or unsuccessfully we can use this script with other administration
tools or simply use it with bash one liners.

Bash One Liner:

$ ./tmp.sh && echo "bam" || (sudo ./tmp.sh && echo "bam" || echo "fail")
Could not create file
Successfully created file
bam

The above grouping of commands use what is called list constructs in bash. List
constructs allow you to chain commands together with simple && for and and ||
for or conditions. The above command will execute the ./tmp.sh script, and if
the exit code is 0 the command echo "bam" will be executed. If the exit code of
./tmp.sh is 1 however, the commands within the parenthesis will be executed
next. Within the parenthesis the commands are chained together using the && and
|| constructs again.

The list constructs use exit codes to understand whether a command has
successfully executed or not. If scripts do not properly use exit codes, any
user of those scripts who use more advanced commands such as list constructs
will get unexpected results on failures.

More exit codes

The exit command in bash accepts integers from 0 - 255, in most cases 0 and 1
will suffice however there are other reserved exit codes that can be used for
more specific errors. The Linux Documentation Project has a pretty good table of
reserved exit codes and what they are used for.


<case>
After migrates to the new linux distribution, the interactive shell script fails
to run. The thing was that it reads .bashrc but fails to contiune.

# from ~/.bashc

# enable programmable completion features (you don't need to enable
# this, if it's already enabled in /etc/bash.bashrc and /etc/profile
# sources /etc/bash.bashrc).
if ! shopt -oq posix; then
  if [ -f /usr/share/bash-completion/bash_completion ]; then
    . /usr/share/bash-completion/bash_completion
  elif [ -f /etc/bash_completion ]; then
    . /etc/bash_completion
  fi
fi

# /usr/share/bash-completion/bash_completion

# This function sets correct SysV init directories
#
_sysvdirs()
{
    sysvdirs=( )
    [[ -d /etc/rc.d/init.d ]] && sysvdirs+=( /etc/rc.d/init.d )
    [[ -d /etc/init.d ]] && sysvdirs+=( /etc/init.d )
    # Slackware uses /etc/rc.d
    [[ -f /etc/slackware-version ]] && sysvdirs=( /etc/rc.d )
}

However, /etc/slackware-version do not exist and this scripts quit since the
parent script uses "-ie" option.

To see how this happen:

#!/bin/bash -i

echo "call file test"

# this file does not exist and so exit code is 1.
# [[ -f /etc/slackware-version ]]

# this file does exist and so exit code is 0.
[[ -f /etc/fstab ]]

echo "exit code after call file test: $?"


={============================================================================
*kt_linux_bash_021* sh-return-value-from-script

# a.sh
  echo $LOCATION
  exit 0

# b.sh
    DIR=``a.sh -dir ${OTHER}.tgz``

DIR will have a return from a.sh which uses echo.


={============================================================================
*kt_linux_bash_001* sh-looping-construct sh-compound-command

3.2.4 Compound Commands

* Looping Constructs:     Shell commands for iterative action.
* Conditional Constructs: Shell commands for conditional execution.
* Command Grouping:       Ways to group commands.

`compound-commands` are the shell programming constructs. Each construct begins
with a reserved word or control operator and is terminated by a corresponding
reserved word or operator. 

*sh-redirection-on-compound-commands*
Any redirections associated with a compound command apply to all commands within
that compound command unless explicitly overridden.

*sh-semicolon-newline*
In most cases a list of commands in a compound command's description may be
separated from the rest of the command by one or more `newlines`, and may be
followed by a newline `in-place-of` a semicolon.

<ex>
for item in ${list[@]}; do echo ${item}; pushd ${item}; \
  egrep -anH "Leak de|Could not unravel" LOGlastrun_realtime;popd; done


Bash provides looping constructs, conditional commands, and mechanisms to group
commands and execute them as a unit. 


{sh-for}

for

    The syntax of the for command is:

    for `name` [ [in [`words` ...] ] ; ] do `commands`; done

    Expand `words`, and execute commands once for each member in the resultant
    list, with name bound to the current member. 
    
    If ‘in words’ is not present, the for command executes the commands once
    for each positional parameter that is set, as if ‘in "$@"’ had been
    specified (see Special Parameters).  The return status is the exit status
    of the last command that executes. 
    
    If there are no items in the expansion of `words`, no commands are
    executed, and the return status is zero.

    An alternate form of the for command is also supported:

    for (( expr1 ; expr2 ; expr3 )) ; do commands ; done

    First, the arithmetic expression expr1 is evaluated according to the rules
    described below (see Shell Arithmetic). The arithmetic expression expr2 is
    then evaluated repeatedly until it evaluates to zero. Each time expr2
    evaluates to a non-zero value, commands are executed and the arithmetic
    expression expr3 is evaluated. If any expression is omitted, it behaves as
    if it evaluates to 1. The return value is the exit status of the last
    command in commands that is executed, or false if any of the expressions is
    invalid. 

<ex>
note:
'no' need to specify index and to shift.

# use `newline` as a seperator
for name in word1 word2 ... wordN
do
   list
done

# use `semicolon` as a seperator 
for f in $( ls /var/ ); do
   echo $f
done

<ex-oneline>
# use `semicolon` as a seperator and in `command-line` 
for f in $(ls); do echo "var is: $f"; done

<ex>
declare -ar list=(
    darwin_487032cd821312aea3b11274c2ff7384
    darwin_c4130c208206b80b3963ee05c861b5c3
    darwin_d23afbe329e75a1d9bcabc233373d398
)

<ex>
for FILE in $HOME/.bash*
do
   cp $FILE ${HOME}/public_html
   chmod a+r ${HOME}/public_html/${FILE}
done


{sh-while}

while

    The syntax of the while command is:

    while test-commands; do consequent-commands; done

    Execute consequent-commands as long as test-commands has an
    `zero-exit-status`. The return status is the exit status of the last command
    executed in consequent-commands, or zero if none was executed.

<ex>
while command
do
   list
done

<ex>
while :     # this cause infinite loop. see *sh-colon*
do
   read CMD
   case $CMD in
      [qQ]|[qQ][uU][iI][tT]) break ;;
   *) process $CMD ;;
   esac
done


{break-continue}
The `break` and `continue` builtins (see Bourne Shell Builtins) may be used to
control loop execution. 


={============================================================================
*kt_linux_bash_001* sh-looping-construct-getopt sh-parse-arg

<ex>
note: can use true only in the script but not command line.

read case;
case $case in
    1) echo "You selected bash";;
    2) echo "You selected perl";;
    3) echo "You selected phyton";;
    4) echo "You selected c++";;
    5) exit
esac


while [ true ] ; do ls -l; echo Wait 1 sec ; sleep 1; done

while [ "$1" != "" ]; do
  case $1 in
    -h | -help | --help)
      usage
      exit
      ;;
    -p | --platform | -b | --box)
      shift
      PLATFORM=$1
      ;;
    --project)
      shift
      PROJECT=$1
      ;;
    release)
      RELEASE="true"
      ;;
    *)
      echo "USAGE ERROR"
      usage
      exit
      ;;
  esac
  shift
done

<ex>
This code breaks up when use "--h" because (*) let it contine to run after
while. So should use exit 0 in case (*).

while [ $# -gt 0 ]; do
    case "$1" in
        (-h | -help | --help)
            grep '^#/' "$0" | cut -c 4-
            exit 0;;

        (-d | --debug)
            debug="1"
            shift;;

        (-l | --list)
            list_assets
            exit 0;;

        (*)
            break;;
    esac
done

<ex>
# Parse command line arguments.
parse_args "$@"

# Parse command line arguments.
function parse_args
{
    while [ $# -gt 0 ]
    do
        case "$1" in
            -m | --maximum)
                shift
                MAXIMUM_NUMBER_OF_UPLOADS_TO_FIND=$1
                if [ $MAXIMUM_NUMBER_OF_UPLOADS_TO_FIND -lt 1 ]
                then
                    echo "'$MAXIMUM_NUMBER_OF_UPLOADS_TO_FIND' isn't large enough.  Please enter a value greater than 0."
                    echo ""
                    exit 1
                fi
                ;;
            -l | --list)
                shift
                MAC_ADDRESS_LIST_FILE=$1
                if [ ! -e $MAC_ADDRESS_LIST_FILE ]
                then
                    echo "The MAC address list file '$MAC_ADDRESS_LIST_FILE' cannot be found."
                    echo ""
                    exit 1
                fi
                MAC_ADDRESS_LIST=$(cat $MAC_ADDRESS_LIST_FILE)
                NUMBER_OF_MAC_ADDRESSES=$(cat $MAC_ADDRESS_LIST_FILE | wc -l)
                ;;
            -v | --version)
                shift
                VERSION=$1
                ;;
            -t | --test)
                shift
                TEST_IDS=$1
                ;;
            -h | --help)
                usage
                exit 0
                ;;
            *)
                # Bail out if we hit something unexpected.
                echo "'$1' is not a valid parameter.  Please use the -h or --help parameter for more information."
                echo ""
                exit 1
                ;;
        esac
        shift
    done

    # Check all mandatory parameters are set.
    if [ -z "$MAC_ADDRESS_LIST_FILE" ]
    then
        usage
        echo "The mandatory -l/--list parameter was not provided."
        echo ""
        exit 1
    fi
}


{getopts} shell built-in

getopts optstring name [args]

getopts is used by shell scripts to parse positional parameters. 'optstring'
contains the option characters to be recognized; if a character is followed by a
colon, the option is expected to have an argument, which should be separated
from it by whitespace. 

<option>
Each time it is invoked, getopts 'places' the next option in the shell variable
'name', initializing name if it does not exist, and the 'index' of the next
argument to be processed into the variable 'OPTIND'. OPTIND is initialized to 1
each time the shell or a shell script is invoked. 

<arg>
When an option requires an argument, getopts places that 'argument' into the
variable OPTARG. 

<return>
When the end of options is encountered, getopts exits with a return value
greater than zero. note: false. OPTIND is set to the index of the first
'non'-option argument, and name is set to '?'.


={============================================================================
*kt_linux_bash_001* sh-conditional-construct

3.2.4.2 Conditional Constructs

{sh-if}

if

    The syntax of the if command is:

    if test-commands; then
      consequent-commands;
    [elif more-test-commands; then
      more-consequents;]
    [else alternate-consequents;]
    fi

    The test-commands list is executed, and if its return status is zero, the
    consequent-commands list is executed. If test-commands returns a non-zero
    status, each elif list is executed in turn, and if its exit status is zero,
      the corresponding more-consequents is executed and the command completes.


{sh-case}

case

    The syntax of the case command is:

    case word in [ [(] pattern [| pattern]...) command-list ;;]... esac

    case will selectively execute the command-list corresponding to the first
    `pattern` that matches word. If the shell option nocasematch (see the
        description of shopt in The Shopt Builtin) is enabled, the match is
    performed without regard to the case of alphabetic characters. The ‘|’ is
    used to separate multiple patterns, and the ‘)’ operator `terminates` a
    pattern list. A list of patterns and an associated command-list is known as
    a `clause`.

    Each clause `must-be-terminated` with ‘;;’, ‘;&’, or ‘;;&’. The word
    undergoes tilde expansion, parameter expansion, command substitution,
    arithmetic expansion, and quote removal before matching is attempted. Each
    pattern undergoes tilde expansion, parameter expansion, command
    substitution, and arithmetic expansion.

    There may be an arbitrary number of case clauses, each terminated by a ‘;;’,
    ‘;&’, or ‘;;&’. The first pattern that matches determines the command-list
    that is executed. It’s a common idiom to use ‘*’ as the `final-pattern` to
    define the default case, since that pattern will always match.

    Here is an example using case in a script that could be used to describe one
    interesting feature of an animal:

    echo -n "Enter the name of an animal: "
    read ANIMAL
    echo -n "The $ANIMAL has "
    case $ANIMAL in
      horse | dog | cat) echo -n "four";;
      man | kangaroo ) echo -n "two";;
      *) echo -n "an unknown number of";;
    esac
    echo " legs."

    If the ‘;;’ operator is used, no subsequent matches are attempted after the
    first pattern match. Using ‘;&’ in place of ‘;;’ causes execution to
    continue with the command-list associated with the next clause, if any.
    Using ‘;;&’ in place of ‘;;’ causes the shell to test the patterns in the
    next clause, if any, and execute any associated command-list on a successful
    match.

    The return status is zero if no pattern is matched. Otherwise, the return
    status is the exit status of the command-list executed.


{sh-select}

select

    The select construct allows the easy generation of `menus`. It has almost the
    same syntax as the for command:

    select name [in words ...]; do commands; done

    The list of words following in is expanded, generating a list of items. The
    set of expanded words is printed on the standard error output stream, each
    preceded by a number. If the ‘in words’ is omitted, the positional
    parameters are printed, as if ‘in "$@"’ had been specified. The PS3 prompt
    is then displayed and a line is read from the standard input. If the line
    consists of a number corresponding to one of the displayed words, then the
    value of name is set to that word. If the line is empty, the words and
    prompt are displayed again. If EOF is read, the select command completes.
    Any other value read causes name to be set to null. The line read is saved
    in the variable REPLY.

    The commands are executed after each selection until a break command is
    executed, at which point the select command completes.

    Here is an example that allows the user to pick a filename from the current
    directory, and displays the name and index of the file selected.

    select fname in *;
    do
    	echo you picked $fname \($REPLY\)
    	break;
    done

<ex>
select COM in comp1 comp2 comp3 all none
do
   echo "is in do"
done

$ ./sample.sh  
1) comp1
2) comp2
3) comp3
4) all
5) none
#?

Automatically make numbers and prompts. COM var will have whatever value
entered. Also this example do infinite loop until press C-c since no break
statement in do.

<ex>
select COM in comp1 comp2 comp3 all none
do
  case $COM in
      comp1 | comp2 | comp3) echo "sel is $COM" ;;
      all) echo "sel is $COM" ;;
      none)
          echo "sel is $COM"
          break
           ;;
       *) echo "ERROR. sel is $REPLY" ;;
   esac
done

You can change the prompt displayed by the select loop by altering the variable
PS3. If PS3 is not set, the default prompt, #?, is displayed. Otherwise the
value of PS3 is used as the prompt to display. For example, the commands

$ PS3="Please make a selection => " ; export PS3

note: All loops has 'do .. done' block and careful the place of 'do'.


{sh-[[} *sh-[[*

Use [] whenever you want your script to be 'portable' across shells. Use [[]] if
you want `conditional-expressions` not supported by [] and don't need to be
portable.

[[ is bash's improvement to the [ command. It has several enhancements that make
it a better choice if you write scripts that target bash. My favorites are:

1. It is a syntactical feature of the shell, so it has some special behavior
that [ doesn't have. You `no longer have to quote` variables because [[ handles
empty strings and strings with whitespace more intuitively. 
For example, with [ you have to write

    if [ -f "$FILE" ]

to correctly handle empty strings or file names with spaces in them. With [[ the
quotes are unnecessary:

    if [[ -f $FILE ]]

2. Because it is a syntactical feature, it lets you use && and || operators for
boolean tests and < and > for string comparisons. [ cannot do this because it is
a regular command and > &&, ||, <, and > are not passed to regular commands as
command-line arguments.

3. It has a wonderful =~ operator for doing regular expression matches. With [
  you might write

    if [ "$ANSWER" = y -o "$ANSWER" = yes ]

    With [[ you can write this as

    if [[ $ANSWER =~ ^y(es)?$ ]]

It even lets you access the captured groups which it stores in BASH_REMATCH. For
instance, ${BASH_REMATCH[1]} would be "es" if you typed a full "yes" above.

4. You get pattern matching aka globbing for free. Maybe you're less strict
about how to type yes.  Maybe you're okay if the user types y-anything. Got you
covered:

    if [[ $ANSWER = y* ]]

Keep in mind that it is a `bash-extension`, so if you are writing sh-compatible
scripts then you need to stick with [. Make sure you have the #!/bin/bash
`shebang` line for your script if you use double brackets.


[]

4.1 Bourne Shell Builtins

note: NO pattern and expansion support

test
[

    test expr

    Evaluate a `conditional-expression` expr and return a status of 0 (true) or
    1 (false). Each operator and operand must be a separate argument.
    Expressions are composed of the `primaries`. test does not accept any
    options, nor does it accept and ignore an argument of -- as signifying the
    end of options.

    When the [ form is used, the last argument to the command must be a ].

    Expressions may be combined using the following operators, listed in
    decreasing order of precedence. The evaluation depends on the number of
    arguments; see below. Operator precedence is used when there are five or
    more arguments. 


[[...]]

    [[ expression ]]

    Return a status of 0 or 1 depending on the evaluation of the
    `conditional-expression` expression. Expressions are composed of the
    `primaries`. 
    
    note: this is enhancement?

    Word splitting and filename expansion are not performed on the words between
    the [[ and ]]; tilde expansion, parameter and variable expansion, arithmetic
    expansion, command substitution, process substitution, and quote removal are
    performed.  Conditional operators such as ‘-f’ must be unquoted to be
    recognized as primaries.

    When used with [[, the ‘<’ and ‘>’ operators sort lexicographically using
    the current locale.

    When the ‘==’ and ‘!=’ operators are used, the string to the right of the
    operator is considered a `pattern` and matched according to the rules
    described below in `pattern-matching`, as if the extglob shell option were
    enabled. 
    
    note: The ‘=’ operator is identical to ‘==’. 
    
    If the shell option `nocasematch` is enabled, the match is performed without
    regard to the case of alphabetic characters. 
    
    The return value is 0 if the string matches (‘==’) or does not match
    (‘!=’)the pattern, and 1 otherwise.

    Any part of the pattern may be quoted to force the quoted portion to be
    matched as a string.

    <sh-regex>
    An additional binary operator, ‘=~’, is available, with the same precedence
    as ‘==’ and ‘!=’. When it is used, the string to the right of the operator
    is considered an `extended-regular-expression` and matched accordingly (as
        in regex3)). 

    The return value is 0 if the string matches the pattern, and 1 otherwise. 
    
    If the regular expression is syntactically incorrect, the conditional
    expression’s return value is 2. If the shell option `nocasematch` (see the
        description of shopt in The Shopt Builtin) is enabled, the match is
    performed without regard to the case of alphabetic characters. Any part of
    the pattern may be quoted to force the quoted portion to be matched as a
    string. 
    
    `bracket-expressions` in regular expressions must be treated carefully,
    since normal quoting characters lose their meanings between brackets. If the
    pattern is stored in a shell variable, quoting the variable expansion forces
    the entire pattern to be matched as a string. Substrings matched by
    parenthesized subexpressions within the regular expression are saved in the
    array variable BASH_REMATCH. The element of BASH_REMATCH with index 0 is the
    portion of the string matching the entire regular expression.  The element
    of BASH_REMATCH with index n is the portion of the string matching the nth
    parenthesized subexpression.

    For example, the following will match a line (stored in the shell variable
        `line`) if there is a sequence of characters in the value consisting of
    any number, including zero, of space characters, zero or one instances of
    ‘a’, then a ‘b’:

    [[ $line =~ [[:space:]]*(a)?b ]]
    
    // [[:space:]]*, (a)? and b

    That means values like ‘aab’ and ‘ aaaaaab’ will match, as will a line
    containing a ‘b’ anywhere in its value. 

note: can use sh-expr

note: These are specific to [[ operator.

Expressions may be combined using the following operators, listed in decreasing
order of precedence:

( expression )

    Returns the value of expression. This may be used to override the normal
    precedence of operators.

! expression

    True if expression is false.

expression1 && expression2

    True if both expression1 and expression2 are true.

expression1 || expression2

    True if either expression1 or expression2 is true. 

The && and || operators do not evaluate expression2 if the value of expression1
is sufficient to determine the return value of the entire conditional
expression. 


={============================================================================
*kt_linux_bash_001* sh-conditional-expression

https://www.gnu.org/software/bash/manual/html_node/Bash-Conditional-Expressions.html#Bash-Conditional-Expressions
6.4 Bash Conditional Expressions

`conditional-expressions` are used by the [[ compound command and the test and [
builtin commands.

Expressions may be unary or binary. Unary expressions are often used to examine
the status of a file. There are string operators and numeric comparison
operators as well. If the file argument to one of the primaries is of the form
/dev/fd/N, then file descriptor N is checked. If the file argument to one of the
primaries is one of /dev/stdin, /dev/stdout, or /dev/stderr, file descriptor 0,
1, or 2, respectively, is checked.

When used with [[, the ‘<’ and ‘>’ operators sort lexicographically using the
current locale. The test command uses ASCII ordering.

Unless otherwise specified, `primaries` that operate on files follow symbolic
links and operate on the target of the link, rather than the link itself.

-a file
-e file
    True if file exists.

-f file
    True if file exists and is a regular file.

-x file
    True if file exists and is executable.

-z string
    True if the length of string is zero. `null-string`

-s file
    True if file exists and has a size greater than zero.

<ex>
if [ -z "${ZB_CFG}" ]; then
   # when not set
else
   # when set
fi

-n string
    True if the length of string is non-zero.

string1 == string2
string1 = string2
    True if the strings are equal. When used with the [[ command, this performs
    `pattern` matching as described above (see `conditional-constructs`.)

    ‘=’ should be used with the test command for POSIX conformance.


<ex>
if [ ! -f $file ]; then
  echo "not exist"
fi


<ex>
gender="female"
if [[ "$gender" == f* ]]


arg1 OP arg2
    OP is one of ‘-eq’, ‘-ne’, ‘-lt’, ‘-le’, ‘-gt’, or ‘-ge’. 
    These arithmetic binary operators return true if arg1 is equal to, not
    equal to, less than, less than or equal to, greater than, or greater than
    or equal to arg2, respectively. Arg1 and arg2 may be positive or negative
    `integers`. 


={============================================================================
*kt_linux_bash_001* sh-pattern-match

<ex>
Suppose that there are shell scipts files to be run at boot up and which are
numbered like, S01xx.sh, S02xx.sh, and so on.

steps=$(echo /etc/init.d/S??*)

How to make sure that scripts runs in the numbered order? The answer is that the
globbing returns the `sorted-list`.


3.5.8.1 Pattern Matching

Any character that appears in a pattern, other than the special pattern
characters described below, matches itself. The NUL character may not occur in a
pattern. A backslash escapes the following character; the escaping backslash is
discarded when matching. The special pattern characters must be quoted if they
are to be matched literally.

The `special-pattern-characters` have the following meanings:

*
    Matches any string, including the null string. When the globstar shell
    option is enabled, and ‘*’ is used in a filename expansion context, two
    adjacent ‘*’s used as a single pattern will match all files and
    `zero-or-more` directories and subdirectories. If followed by a ‘/’, two
    adjacent ‘*’s will match only directories and subdirectories.  

?
    Matches any single character. 

[...]
    Matches any one of the enclosed characters. A pair of characters separated
    by a hyphen denotes a range expression; any character that falls between
    those two characters, inclusive, using the current locale’s collating
    sequence and character set, is matched. If the first character following the
    ‘[’ is a ‘!’ or a ‘^’ then any character not enclosed is matched. A ‘-’ may
    be matched by including it as the first or last character in the set. A ‘]’
    may be matched by including it as the first character in the set. The
    sorting order of characters in range expressions is determined by the
    current locale and the values of the LC_COLLATE and LC_ALL shell variables,
if set.

    For example, in the default C locale, ‘[a-dx-z]’ is equivalent to
      ‘[abcdxyz]’. Many locales sort characters in dictionary order, and in
      these locales ‘[a-dx-z]’ is typically not equivalent to ‘[abcdxyz]’; it
      might be equivalent to ‘[aBbCcDdxXyYz]’, for example. To obtain the
      traditional interpretation of ranges in bracket expressions, you can force
      the use of the C locale by setting the LC_COLLATE or LC_ALL environment
      variable to the value ‘C’, or enable the globasciiranges shell option.

    Within ‘[’ and ‘]’, `character-classes` can be specified using the syntax
    [:class:], where class is one of the following classes defined in the POSIX
               standard:

    alnum   alpha   ascii   blank   cntrl   digit   graph   lower
    print   punct   space   upper   word    xdigit

    A character class matches any character belonging to that class. The `word`
    character class matches letters, digits, and the character ‘_’.

    // http://www.gnu.org/software/grep/manual/html_node/Character-Classes-and-Bracket-Expressions.html
    // ‘[:space:]’
    // 
    // Space characters: in the ‘C’ locale, this is tab, newline, vertical tab,
    // form feed, carriage return, and space. See Usage, for more discussion of
    // matching newlines.

    Within ‘[’ and ‘]’, an equivalence class can be specified using the syntax
    [=c=], which matches all characters with the same collation weight (as
        defined by the current locale) as the character c.

    Within ‘[’ and ‘]’, the syntax [.symbol.] matches the collating symbol
    symbol. 


If the `extglob` shell option is enabled using the shopt builtin, several
extended pattern matching operators are recognized. In the following
description, a pattern-list is a list of one or more patterns separated by a
‘|’. Composite patterns may be formed using one or more of the following
sub-patterns:

?(pattern-list)
    Matches zero or one occurrence of the given patterns.

*(pattern-list)
    Matches zero or more occurrences of the given patterns.

+(pattern-list)
    Matches one or more occurrences of the given patterns.

@(pattern-list)
    Matches one of the given patterns.

!(pattern-list)
    Matches anything except one of the given patterns. 

note:
Why you shouldn't parse the output of ls(1)
http://mywiki.wooledge.org/ParsingLs


={============================================================================
*kt_linux_bash_001* sh-function

3.3 Shell Functions

Shell functions are a way to `group commands` for later execution using a single
name for the group. They are executed just like a "regular" command. When the
name of a shell function is used as a simple command name, the list of commands
associated with that function name is executed. Shell functions are executed in
the `current-shell` context; no new process is created to interpret them.

Functions are declared using this syntax:

name () compound-command [ redirections ]

or

function name [()] compound-command [ redirections ]

This defines a shell function named `name`. The reserved word `function` is
`optional`. If the function reserved word is supplied, the parentheses are
`optional`. The body of the function is the compound command `compound-command`.

That command is usually a `list` enclosed `between` { and }, but may be any
compound command listed above. compound-command is executed whenever name is
specified as the name of a command. 

Any redirections (see Redirections) associated with the shell function are
performed when the function is executed.

A function definition may be deleted using the -f option to the unset builtin
(see Bourne Shell Builtins).

The exit status of a function definition is zero unless a syntax error occurs or
a readonly function with the same name already exists. When executed, the exit
status of a function is the exit status of the `last-command` executed in the
body.

<ex>
#!/bin/bash
# BASH FUNCTIONS CAN BE DECLARED IN ANY ORDER

function function_B {
  echo "{ fb"
  echo Function B.
  echo "} fb"
}

function function_A {
  echo "{ fa"
  echo $1
  echo "} fa"
}

function function_D {
  echo "{ fd"
  echo Function D.
  echo "} fd"
}

# function function_C {
function_C () {
  echo "{ fc"
  echo $1
  echo "} fc"
}

# FUNCTION CALLS

# Pass parameter to function A
function_A "Function A via pass"
function_B

# Pass parameter to function C
function_C "Function C via pass"
function_D 

$ ./sbash.sh list 
{ fa
Function A via pass
} fa
{ fb
Function B.
} fb
{ fc
Function C via pass
} fc
{ fd
Function D.
} fd


note: this causes an error as:

function_C {
  echo "{ fc"
  echo $1
  echo "} fc"
}

./sample.sh: line 23: function_C: command not found
{ fc

} fc
./sample.sh: line 27: syntax error near unexpected token `}'
./sample.sh: line 27: `}'


<ex>
get_available_space() {
    declare -a fs_info=( $(stat -f -c "%d %s" "$core_output_path") )
    echo $(( $(IFS="*"; echo "${fs_info[*]}") ))
}

# break the loop if the available space is above the threshold
[ "$(get_available_space)" -lt "$threshold" ] || break


Note that for historical reasons, in the most common usage the curly braces that
surround the body of the function must be separated from the body by blanks or
newlines. This is because the braces are reserved words and are only recognized
as such when they are separated from the command list by whitespace or another
shell metacharacter. Also, when using the braces, the list must be terminated by
a semicolon, a ‘&’, or a newline.

*sh-function-parameters*
When a function is executed, the arguments to the function `become` the
`positional-parameters` during its execution. The special parameter ‘#’ that
expands to the number of positional parameters is updated to reflect the change.
Special parameter 0 is unchanged. The first element of the FUNCNAME variable is
set to the name of the function while the function is executing.

All other aspects of the shell execution environment are identical between a
function and its caller with these exceptions: the DEBUG and RETURN traps are
not inherited unless the function has been given the trace attribute using the
declare builtin or the -o functrace option has been enabled with the set
builtin, (in which case all functions inherit the DEBUG and RETURN traps), and
the ERR trap is not inherited unless the -o errtrace shell option has been
enabled. See Bourne Shell Builtins, for the description of the trap builtin.

If the builtin command `return` is executed in a function, the function
completes and execution resumes with the next command after the function call.
Any command associated with the RETURN trap is executed before execution
resumes. When a function completes, the values of the positional parameters and
the special parameter ‘#’ are `restored` to the values they had prior to the
function’s execution. If a numeric argument is given to `return`, that is the
function’s return status; otherwise the function’s return status is the exit
status of the `last-command` executed before the return.

*sh-variable-local*
Variables local to the function may be declared with the `local` builtin. These
variables are visible only to the function and the commands it invokes.

Function names and definitions may be listed with the -f option to the `declare`
(typeset) builtin command (see Bash Builtins). The -F option to `declare` or
`typeset` will list the function names only (and optionally the source file and
    line number, if the `extdebug` shell option is enabled). Functions may be
exported so that subshells automatically have them defined with the -f option to
the export builtin (see Bourne Shell Builtins). Note that shell functions and
variables with the same name may result in multiple identically-named entries in
the environment passed to the shell’s children. Care should be taken in cases
where this may cause a problem.

Functions may be `recursive`. The FUNCNEST variable may be used to limit the
depth of the function call stack and restrict the number of function
invocations. By default, no limit is placed on the number of recursive calls. 


{exit-and-return}

4.1 Bourne Shell Builtins

exit

    exit [n]

    `exit-the-shell`, returning a status of n to the shell’s parent. If n is
    omitted, the exit status is that of the last command executed. Any trap on
    EXIT is executed before the shell terminates.


return

    return [n]

    Cause a `shell-function` to stop executing and return the value n to its
    caller. If n is not supplied, the return value is the exit status of the
    last command executed in the function. return may also be used to terminate
    execution of a script being executed with the . (source) builtin, returning
    either n or the exit status of the last command executed within the script
    as the exit status of the script. If n is supplied, the return value is its
    least significant 8 bits. Any command associated with the RETURN trap is
    executed before execution resumes after the function or script. The return
    status is non-zero if return is supplied a non-numeric argument or is used
    outside a function and not during the execution of a script by . or source.

<ex>
Unlike a shell convention which use 0 for success, return 1 (okay) when found a
string in a array at the expected position. otherwise, return 0 (fail).

function paramsContainStringAtPosition () {
    local stringToSearchFor="$1"
    local positionToExpectStringAt="$2"
    local stringToSearch="${@:3}"
    local stringToSearchAsArray=($stringToSearch)
    local currentParamPos=0;
    local currentSearchString
        
    for currentSearchString in "${stringToSearchAsArray[@]}"
        do
            if [ "$currentSearchString" == "$stringToSearchFor" ]; then
                if [ "$positionToExpectStringAt" == "$currentParamPos" ]; then
                    return 1;
                fi
            fi
            currentParamPos=$(( $currentParamPos+1 ))
        done
    return 0
}

function x {

   ... 
   paramsContainStringAtPosition "$expected_url" 7 "$@"
   urlValueCorrectAtExpectedPosition=$?   # *sh-exit-status*

   // note: here convert c style return to shell return by comararing "return
   // from function = 1"

   [[
     "$cacheOptionCorrectAtExpectedPosition" = 1 &&
     "$cacheValueCorrectAtExpectedPosition" = 1 &&
     "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
     "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
     "$jarOptionCorrectAtExpectedPosition" = "1" && 
     "$jarValueCorrectAtExpectedPosition" = "1" && 
     "$urlOptionCorrectAtExpectedPosition" = "1" && 
     "$urlValueCorrectAtExpectedPosition" = "1" 
   ]] 
}


={============================================================================
*kt_linux_bash_001* sh-variable

3.4 Shell Parameters

A parameter is an entity that stores values. It can be a 'name', a 'number', or
one of the `special-characters` listed below. A variable is a parameter denoted by
a 'name'. 

A variable has a value and zero or more attributes. Attributes are assigned
using the `declare` builtin command (see the description of the declare builtin
    in Bash Builtins).

A parameter is set if it has been assigned a value. Once a variable is set, it
may be unset only by using the `unset command`

A variable may be assigned to by a statement of the form

name=[value]

note:
No space between "name", "=", "[value]". Otherwise, treat it as command.


If value is not given, the variable is assigned the `null-string` and the null
string is a 'valid' value. 

note:
What's the difference between that value is not given and that variable is not
set? Same since both has the same `null-string`.

<ex>

# No difference if or not it has the following
# VAR_DEFINED_BUT_NOT_SET=
#
# -n string
#    True if the length of string is non-zero.

# 1 non-zero and null-string

if [ -n ${VAR_DEFINED_BUT_NOT_SET} ]; then
  echo "1 non-zero and null-string"
else
  echo "1 zero"
fi

# 1 non-zero and null-string

if [ -n $VAR_DEFINED_BUT_NOT_SET ]; then
  echo "1 non-zero and null-string"
else
  echo "1 zero"
fi

# 1 zero
# see *sh-single-double-quote* *sh-expansion-parameter*

if [ -n "$VAR_DEFINED_BUT_NOT_SET" ]; then
  echo "1 non-zero and null-string"
else
  echo "1 zero"
fi

# 1 zero

if [ -n "${VAR_DEFINED_BUT_NOT_SET}" ]; then
  echo "1 non-zero and null-string"
else
  echo "1 zero"
fi

note: 
To use a `string` use double-quote and shows `null-string` is ''. see *sh-[[*

# 2 zero

if [[ -n ${VAR_DEFINED_BUT_NOT_SET} ]]; then
  echo "2 non-zero and null-string"
else
  echo "2 zero"
fi

# 2 zero

if [[ -n $VAR_DEFINED_BUT_NOT_SET ]]; then
  echo "2 non-zero and null-string"
else
  echo "2 zero"
fi

# 2 zero

if [[ -n "$VAR_DEFINED_BUT_NOT_SET" ]]; then
  echo "2 non-zero and null-string"
else
  echo "2 zero"
fi

# 2 zero

if [[ -n "${VAR_DEFINED_BUT_NOT_SET}" ]]; then
  echo "2 non-zero and null-string"
else
  echo "2 zero"
fi

# debug on
+ '[' -n ']'                          
+ echo '1 non-zero and null-string'
1 non-zero and null-string

+ '[' -n ']'
+ echo '1 non-zero and null-string'
1 non-zero and null-string

+ '[' -n '' ']'
+ echo '1 zero'
1 zero

+ '[' -n '' ']'
+ echo '1 zero'
1 zero


All values undergo tilde 'expansion', parameter and variable expansion, command
substitution, arithmetic expansion, and quote removal. 

If the variable has its `integer` attribute set, then value is evaluated as an
arithmetic expression even if the $((...)) expansion is not used (see Arithmetic
    Expansion). 

Word splitting is not performed, with the exception of "$@" as explained below.
Filename expansion is not performed.  

Assignment statements may also appear as arguments to the alias, declare,
typeset, export, readonly, and local builtin commands. 


{"+="-operator}
In the context where an assignment statement is assigning a value to a shell
variable or array index (see Arrays), the ‘+=’ operator can be used to append to
or add to the variable's previous value. 

When ‘+=’ is applied to a variable for which the integer attribute has been set,
     value is evaluated as an arithmetic expression and added to the variable's
     current value, which is also evaluated.

When ‘+=’ is applied to an array variable using compound assignment (see
    Arrays), the variable's value is not unset (as it is when using ‘=’), and
new values are appended to the array beginning at one greater than the array's
maximum index (for indexed arrays), or added as additional key-value pairs in an
associative array. When applied to a string-valued variable, value is expanded
and appended to the variable’s value.


={============================================================================
*kt_linux_bash_001* sh-positional-parameters

3.4.1 Positional Parameters

A positional parameter is a parameter denoted by one or more digits, other than
the single digit 0. note: not use '0'.

`positional-parameters` are assigned from the shell's arguments when it is
invoked, and may be reassigned using the set builtin command. Positional
parameter N may be referenced as ${N}, or as $N when N consists of a
`single-digit`. Positional parameters may not be assigned to with assignment
statements. The `set` and `shift` builtins are used to set and unset them (see
    Shell Builtin Commands).

note: uses in sh-function
The positional parameters are temporarily replaced when a shell function is
executed (see Shell Functions).

note: use brace to use more parameters than 10 *sh-brace*
When a positional parameter consisting of more than a single digit is expanded,
it must be enclosed in `braces`. 


3.4.2 Special Parameters *sh-special*

The shell treats several parameters specially. These parameters may only be
referenced; assignment to them is not allowed. 

*
($*) Expands to the `positional-parameters`, starting from one. 

When the expansion is `not-within-double-quotes`, each positional parameter
expands to `a separate word` In contexts where it is performed, those words are
subject to further word splitting and pathname expansion. 

When the expansion occurs `within-double-quotes`, it expands to a `single-word`
with the value of each parameter separated by the first character of the `IFS`
special variable. That is, "$*" is equivalent to "$1c$2c..." where c is the
first character of the value of the IFS variable. If IFS is unset, the
parameters are separated by spaces. If IFS is null, the parameters are joined
without intervening separators.

note:
After all, not in double-quote, gets each word for each positional parameter and
in double-quote, gets a single-word which has all parametes.

Both do the same. Why two? So the $* provides more options than $@ since can
make single word.


<ex>
args2=($*)  # `compound-assignments` in array
echo $args2 ${args2[0]} ${args2[1]} ${args2[2]}

args3="$*"
echo $args3

$ ./t_var.sh 1 2 3 4 5
1 1 2 3
1 2 3 4 5

<ex>
This function is to make a `single-word` from a array(list) separated by ":". 

note:
Q: How to redirect output from function call? Why use () in the function? To
crate a subshell?

function list_join() {
    local sep="$1"
    shift

    (
        IFS="$sep"
        echo "$*"
    )
}

declare -ar preload_libs=(
    /usr/local/lib/libdirectfb.so
    /usr/local/lib/libdirect.so
    /usr/local/lib/libinit.so
    /lib/libpthread.so.0
)

vars+=(LD_PRELOAD=$(list_join ":" ${preload_libs[@]}))


@
($@) Expands to the `positional-parameters`, starting from one. 

When the expansion occurs "within double quotes", each parameter expands to a
separate word. That is, "$@" is equivalent to "$1" "$2"... 

If the double-quoted expansion occurs within a word, the expansion of the first
parameter is joined with the beginning part of the original word, and the
expansion of the last parameter is joined with the last part of the original
word. When there are no positional parameters, "$@" and $@ expand to nothing
(i.e., they are removed).

<ex> arrayfy
args=("$@");   # to get 'all' args and set it to array.
echo ${args[0]} ${args[1]} ${args[2]}

echo $@        # to print all args in a one go


0
($0) Expands to the name of the shell or shell script. This is set at shell
initialization. If Bash is invoked with a file of commands (see Shell Scripts),
  $0 is set to the name of that file. If Bash is started with the -c option (see
      Invoking Bash), then $0 is set to the first argument after the string to
  be executed, if one is present. Otherwise, it is set to the filename used to
  invoke Bash, as given by argument zero.

<ex>
This variable is commonly used to determine the behavior of scripts that can be
invoked with more than one name.

$ ln -s mytar listtar
$ ln -s mytar maketar

$ listtar or maketar

#!/bin/sh
case $0 in
   *listtar) TARGS="-tvf $1" ;;
   *maketar) TARGS="-cvf $1.tar $1" ;;
esac
tar $TARGS


#
($#) Expands to `the-number-of` positional parameters in decimal. the number of
args passed in the command line. 

note:
exclued 0th.
$ xxx.sh      // $# is 0
$ xxx.sh 1    // $# is 1


?       *sh-exit-status*
($?) Expands to the `exit-status` of the most recently executed foreground
pipeline.


$
($$) Expands to the `process ID` of the shell. In a () subshell, it expands to
the process ID of the invoking shell, not the subshell.


{global-and-local}
Global variables or 'environment' variables are available in all shells. The env
or printenv commands can be used to display environment variables.

Local variables are only available in the current shell. Using the set built-in
command without any options will display a list of all variables (including
    environment variables) and functions.


{set-variable}
Putting spaces around the equal sign will cause errors.

VARNAME="value"

A variable created like the ones in the example above is only available to the
current shell. It is a local variable: child processes of the current shell will
not be aware of this variable. In order to pass variables to a subshell, we need
to export them using the export built-in command.

export VARNAME="value"

note: parent can export variables to child but not vice versa.

<set-variable-for-child>
In the Bourne shell and its descendants (e.g., bash and the Korn shell), the
following syntax can be used to add values to the environment used to execute a
single program, without affecting the parent shell (and subsequent commands):

$ NAME=value program

This adds a definition to the environment of 'just' the child process executing
the named program. If desired, multiple assignments (delimited by white space)
  can precede the program name.


{constant}
The readonly built-in marks each specified variable as unchangeable. When tried
to set, displays error but execution continues.

readonly CONST=100
echo "my const var is $CONST.."
CONST=200
echo "my const var is $CONST.."

bash: CONST: readonly variable


={============================================================================
*kt_linux_bash_001* sh-variable-array

6.7 Arrays

Bash provides one-dimensional `indexed` and `associative-array` variables. Any
variable may be used as an indexed array; the `declare` builtin will explicitly
declare an array. There is no maximum limit on the size of an array, nor any
requirement that members be indexed or assigned contiguously. Indexed arrays are
referenced using integers (including arithmetic expressions (see Shell
      Arithmetic)) and are zero-based; associative arrays use arbitrary strings.
Unless otherwise noted, indexed array indices must be non-negative integers.

An `indexed-array` is created automatically if any variable is assigned to using
the syntax

name[subscript]=value

The subscript is treated as an arithmetic expression that must evaluate to a
number. To explicitly declare an array, use

declare -a name
declare -a name[subscript]

is also accepted; the subscript is ignored.

`associative-arrays` are created using

declare -A name.

Attributes may be specified for an array variable using the declare and readonly
builtins. Each attribute applies to all members of an array.

Arrays are assigned to using `compound-assignments` of the form

name=(value1 value2 ... )

where each value is of the form [subscript]=string. Indexed array assignments do
not require anything but string. When assigning to indexed arrays, if the
optional subscript is supplied, that index is assigned to; otherwise the index
of the element assigned is the last index assigned to by the statement plus one.
Indexing starts at `zero`.

<ex>
FRUIT[0]=apple
FRUIT[1]=banana
FRUIT[2]=orange

FRUIT=(apple plum blackberry)


When assigning to an associative array, the subscript is required.

This syntax is also accepted by the declare builtin. Individual array elements
may be assigned to using the name[subscript]=value syntax introduced above.

When assigning to an indexed array, if name is subscripted by a negative number,
that number is interpreted as relative to one greater than the maximum index of
  name, so negative indices count back from the end of the array, and an index
  of -1 references the last element.


<to-reference> *sh-brace*
Any element of an array may be referenced using ${name[subscript]}. The `braces`
are required to avoid conflicts with the shell’s `filename-expansion` operators.

If the subscript is ‘@’ or ‘*’, the word expands to `all-members` of the array
name. 

These subscripts differ only when the word appears within double quotes. If the
word is double-quoted, ${name[*]} expands to a `single-word` with the value of
each array member separated by the first character of the IFS variable, and
${name[@]} expands each element of name to a separate word. When there are no
array members, ${name[@]} expands to nothing. If the double-quoted expansion
occurs within a word, the expansion of the first parameter is joined with the
beginning part of the original word, and the expansion of the last parameter is
joined with the last part of the original word. This is `analogous` to the
expansion of the special parameters ‘@’ and ‘*’. 

<ex>
The both are the same.

declare -ar list=(
    'darwin_0019fbd18c9a_487032cd821312aea3b11274c2ff7384'
    'darwin_0019fbb71c47_c4130c208206b80b3963ee05c861b5c3'
    'darwin_0019fbb71bb1_d23afbe329e75a1d9bcabc233373d398'
)

declare -ar list2=(
    darwin_0019fbd18c9a_487032cd821312aea3b11274c2ff7384
    darwin_0019fbb71c47_c4130c208206b80b3963ee05c861b5c3
    darwin_0019fbb71bb1_d23afbe329e75a1d9bcabc233373d398
)


<ex>
echo "all = ${FRUIT[*]}"      # single-word which has all
echo "all = ${FRUIT[@]}"      # same as above


<ex>
farm_hosts=(web03 web04 web05 web06 web07)

for i in ${farm_hosts[@]}; do   # same
for i in ${farm_hosts[*]}; do   # same
  su $login -c "scp $httpd_conf_new ${i}:${httpd_conf_path}"
  su $login -c "ssh $i sudo /usr/local/apache/bin/apachectl graceful"
done


<ex> 
A cron job that fills an array with the possible candidates, uses date +%W to
find the week of the year, and does a modulo operation to find the correct
index. The lucky person gets notified by e-mail.


#!/bin/bash
# This is get-tester-address.sh
#
# First, we test whether bash supports arrays. (Support for arrays was only
# added recently.)
whotest[0]='test' || (echo 'Failure: arrays not supported in this version of bash.' && exit 2)

#
# Our list of candidates. (Feel free to add or remove candidates.)
#
wholist=(
  'Bob Smith <bob@example.com>'
  'Jane L. Williams <jane@example.com>'
  'Eric S. Raymond <esr@example.com>'
  'Larry Wall <wall@example.com>'
  'Linus Torvalds <linus@example.com>'
)

# Count the number of possible testers. (Loop until we find an empty string.)

count=0
while [ "x${wholist[count]}" != "x" ]
do
   count=$(( $count + 1 ))
done

# Now we calculate whose turn it is.

week=``date '+%W'``             # The week of the year (0..53).
week=${week#0}                # Remove possible leading zero.
let "index = $week % $count"  # week modulo count = the lucky person
email=${wholist[index]}       # Get the lucky person's e-mail address.
echo $email                   # Output the person's e-mail address.

This script is then used in other scripts, such as this one, which uses a here
document:

email=`get-tester-address.sh` # Find who to e-mail.
hostname=`hostname` # This machine's name.

# Send e-mail to the right person.

mail $email -s '[Demo Testing]' <<EOF
  The lucky tester this week is: $email
  Reminder: the list of demos is here:
  http://web.example.com:8080/DemoSites
  (This e-mail was generated by $0 on ${hostname}.)
EOF


<ex>
uri="$1"
[[ $uri =~ [1-9][0-9]* ]] && uri="${urls[$uri]}"


${#name[subscript]} expands to the length of ${name[subscript]}. If subscript is
‘@’ or ‘*’, the expansion is the number of elements in the array. Referencing an
array variable without a subscript is equivalent to referencing with a
`subscript-of-0`. 

If the subscript used to reference an element of an indexed array evaluates to a
number less than zero, it is interpreted as relative to one greater than the
maximum index of the array, so negative indices count back from the end of the
array, and an index of -1 refers to the last element.

An array variable is considered set if a subscript has been assigned a value.
The null string is a valid value.

It is possible to obtain the keys (indices) of an array as well as the values.
${!name[@]} and ${!name[*]} expand to the indices assigned in array variable
name. The treatment when in double quotes is similar to the expansion of the
special parameters ‘@’ and ‘*’ within double quotes.

The `unset` builtin is used to destroy arrays. unset name[subscript] destroys the
array element at index subscript. Negative subscripts to indexed arrays are
interpreted as described above. Care must be taken to avoid unwanted side
effects caused by filename expansion. unset name, where name is an array,
removes the entire array. A subscript of ‘*’ or ‘@’ also removes the entire
  array.

The declare, local, and readonly builtins each accept a -a option to specify an
indexed array and a -A option to specify an associative array. If both options
are supplied, -A takes precedence. The read builtin accepts a -a option to
assign a list of words read from the standard input to an array, and can read
values from the standard input into individual array elements. The set and
declare builtins display array values in a way that allows them to be reused as
input. 


={============================================================================
*kt_linux_bash_001* sh-expansion

3.5 Shell Expansions

Expansion is performed on the `command-line` after it has been split into
tokens. There are seven kinds of expansion performed:

note:
Not in the script?

* Brace Expansion: Expansion of expressions within braces.
* Tilde Expansion: Expansion of the ~ character.
* Shell Parameter Expansion: How Bash expands variables to their values.
* Command Substitution: Using the output of a command as an argument.
* Arithmetic Expansion: How to use arithmetic in shell expansions.
* Process Substitution: A way to write and read to and from a command.
* Word Splitting: How the results of expansion are split into separate arguments.
* Filename Expansion: A shorthand for specifying filenames matching patterns.
* Quote Removal: How and when quote characters are removed from words.

The `order-of-expansions` is: 

brace expansion; tilde expansion, parameter and variable expansion, arithmetic
expansion, and command substitution (done in a left-to-right fashion); word
splitting; and filename expansion.

On systems that can support it, there is an additional expansion available:
process substitution. This is performed at the same time as tilde, parameter,
variable, and arithmetic expansion and command substitution.

Only brace expansion, word splitting, and filename expansion can change the
number of words of the expansion; other expansions expand a single word to a
single word. The only exceptions to this are the expansions of "$@" (see Special
    Parameters) and "${name[@]}" (see Arrays).

After all expansions, quote removal (see Quote Removal) is performed. 


={============================================================================
*kt_linux_bash_001* sh-expansion-brace

3.5.1 Brace Expansion

`brace-expansion` is a mechanism by which arbitrary strings may be generated.
This mechanism is similar to filename expansion, but the filenames generated
need not exist. Patterns to be brace expanded take the form of an optional
preamble, followed by either a series of comma-separated strings or a sequence
expression between a pair of braces, followed by an optional postscript. The
preamble is prefixed to each string contained within the braces, and the
postscript is then appended to each resulting string, expanding left to right.

Brace expansions may be nested. The results of each expanded string are not
sorted; left to right order is preserved. For example,

bash$ echo a{d,c,b}e
ade ace abe

A sequence expression takes the form {x..y[..incr]}, where x and y are either
integers or single characters, and incr, an optional increment, is an integer.
When integers are supplied, the expression expands to each number between x and
y, inclusive. Supplied integers may be prefixed with ‘0’ to force each term to
have the same width. When either x or y begins with a zero, the shell attempts
to force all generated terms to contain the same number of digits, zero-padding
where necessary. 

When characters are supplied, the expression expands to each character
lexicographically between x and y, inclusive, using the default C locale. Note
that both x and y must be of the same type. When the increment is supplied, it
is used as the difference between each term. The default increment is 1 or -1 as
appropriate.

Brace expansion is performed before any other expansions, and any characters
special to other expansions are preserved in the result. It is strictly textual.
Bash does not apply any syntactic interpretation to the context of the expansion
or the text between the braces. To avoid conflicts with parameter expansion, the
string ‘${’ is not considered eligible for brace expansion.

A correctly-formed brace expansion must contain unquoted opening and closing
  braces, and at least one unquoted comma or a valid sequence expression. Any
  incorrectly formed brace expansion is left unchanged.

A { or ‘,’ may be quoted with a backslash to prevent its being considered part
  of a brace expression. To avoid conflicts with parameter expansion, the string
    ‘${’ is not considered eligible for brace expansion.

This construct is typically used as shorthand when the common prefix of the
  strings to be generated is longer than in the above example:

mkdir /usr/local/src/bash/{old,new,dist,bugs}

# $ echo /usr/bash/{old,new,dist,bugs}
# /usr/bash/old /usr/bash/new /usr/bash/dist /usr/bash/bugs

or

chown root /usr/{ucb/{ex,edit},lib/{ex?.?*,how_ex}}

# $ echo /usr/{ucb/{ex,edit},lib/{ex?.?*,how_ex}}
# /usr/ucb/ex /usr/ucb/edit /usr/lib/ex?.?* /usr/lib/how_ex


={============================================================================
*kt_linux_bash_001* sh-expansion-tilde

3.5.2 Tilde Expansion

If a word begins with an unquoted tilde character (‘~’), all of the characters
up to the first unquoted slash (or all characters, if there is no unquoted
    slash) are considered a tilde-prefix. 

If none of the characters in the tilde-prefix are quoted, the characters in the
tilde-prefix following the tilde are treated as a possible `login-name`. 

If this login name is the null string, the tilde is replaced with the value of
the HOME shell variable. If HOME is unset, the home directory of the user
executing the shell is substituted instead.  Otherwise, the tilde-prefix is
replaced with the home directory associated with the specified login name.


If the characters following the tilde in the tilde-prefix consist of a number N,
optionally prefixed by a ‘+’ or a ‘-’, the tilde-prefix is replaced with the
  corresponding element from the directory stack, as it would be displayed by
  the `dirs` builtin invoked with the characters following tilde in the
  tilde-prefix as an argument. If the tilde-prefix, sans the tilde, consists of
  a number without a leading ‘+’ or ‘-’, ‘+’ is assumed.


Each `variable-assignment` is checked for unquoted tilde-prefixes immediately
following a ‘:’ or the first ‘=’. In these cases, tilde expansion is also
performed. Consequently, one may use filenames with tildes in assignments to
PATH, MAILPATH, and CDPATH, and the shell assigns the expanded value.

The following table shows how Bash treats unquoted tilde-prefixes:

~ 
    The value of $HOME 

~/foo
    $HOME/foo

~fred/foo
    The subdirectory foo of the home directory of the user fred

~N
    The string that would be displayed by ‘dirs +N’
~+N
    The string that would be displayed by ‘dirs +N’
~-N
    The string that would be displayed by ‘dirs -N’ 


6.8.1 Directory Stack Builtins

pushd

    pushd [-n] [+N | -N | dir]

    Save the current directory on the top of the directory stack and then `cd`
    to dir. With no arguments, pushd exchanges the top two directories.

    -n
        Suppresses the normal change of directory when adding directories to the
        stack, so that only the stack is manipulated. 

        note: means no `cd`


={============================================================================
*kt_linux_bash_001* sh-expansion-parameter

3.5.3 Shell Parameter Expansion 

The `$` character introduces `parameter-expansion`, command substitution, or
arithmetic expansion. The parameter name or symbol to be expanded may be
enclosed in `braces`, which are `optional` but serve to `protect` the variable
to be expanded from characters immediately following it which could be
interpreted as part of the name.

When braces are used, the matching ending brace is the first ‘}’ not escaped by
a backslash or within a quoted string, and not within an embedded arithmetic
expansion, command substitution, or parameter expansion.

The basic form of parameter expansion is ${parameter}. The value of parameter is
substituted. 

*sh-brace*
The parameter is a shell parameter as described above (see *sh-variable*) or an
array reference (see *sh-varaibale-arrays*). The `braces` are required when
parameter is a positional parameter with more than one digit, or when parameter
is followed by a character that is not to be interpreted as part of its name.

In each of the cases below, `word` is subject to `tilde-expansion`,
`parameter-expansion`, `command-substitution`, and arithmetic expansion.

When not performing substring expansion, using the form described below (e.g.,
    ‘:-’), Bash tests for a parameter that is `unset-or-null`. Omitting the
colon results in a test only for a parameter that is unset. Put another way, if
the colon is included, the operator tests for both parameter’s `existence` and
that its value is not null; if the colon is omitted, the operator tests only for
existence. 

${parameter:-word}
    If parameter is `unset-or-null`, the expansion of word is substituted.
    Otherwise, the value of parameter is substituted.

    note:
    Use value of parameter or word

<ex>
The value of parameter does not change. Here TEST is not defined.

$ echo $TEST
$ echo ${TEST:-test}
test
$ echo $TEST
$

<ex>
Use 'default' to make sure that the prompt is always set correctly.

PS1=${HOST:-localhost}"$ " ; export PS1 ;

<ex>
To set default 80 when it is not defined before. Why not use {parameter:=word}
form? 

# [ -z STRING ] True of the length of "STRING" is zero.

[ -z "${COLUMNS:-}" ] && COLUMNS=80

It is a shorter notation for

if [ -z "${COLUMNS:-}" ]; then
   COLUMNS=80
fi


<ex>
[ -z "${COLUMNS:-}" ] && COLUMNS=80
echo "${COLUMNS}"

unset COLUMNS

# error when use "${COLUMNS:=80}" or ${COLUMNS:=80} since 80 command not found.
# Here [] `evaluate` and return status which are not used.

[ ${COLUMNS:=80} ]
echo "${COLUMNS}"

80
80


${parameter:=word}
    If parameter is `unset-or-null`, the expansion of word is `assigned-to`
      parameter. The value of parameter is then substituted. Positional
      parameters and special parameters may not be assigned to in this way.


${parameter:offset}
${parameter:offset:length}
    This is referred to as `substring-expansion`. It expands to up to `length`
      characters of the value of parameter starting at the character specified
      by `offset`. 
      
<ex>
$ STRING="thisisaverylongname"
$ echo ${STRING:4}
isaverylongname
$ echo ${STRING:6:5}
avery


<expansion-on-arg>
If parameter is @, the result is length positional parameters 'beginning' at
offset. A negative offset is taken relative to one greater than the greatest
positional parameter, so an offset of -1 evaluates to the last positional
parameter. It is an expansion error if length evaluates to a number less than
zero.

The following examples illustrate substring expansion using positional
parameters:

$ set -- 1 2 3 4 5 6 7 8 9 0 a b c d e f g h
$ echo ${@:7}
7 8 9 0 a b c d e f g h
$ echo ${@:7:0}

$ echo ${@:7:2}
7 8

$ echo ${@:7:-2}
bash: -2: substring expression < 0

$ echo ${@: -7:2}
b c

note: this shows all args
$ echo ${@}
1 2 3 4 5 6 7 8 9 0 a b c d e f g h

$ echo ${@:0}
./bash 1 2 3 4 5 6 7 8 9 0 a b c d e f g h

$ echo ${@:0:2}
./bash 1


note: Q? this works as it has default value when parameter is not set, null.
what is it?

box_ip=${2-localhost}
box_port=${3-2033}


={============================================================================
*kt_linux_bash_001* sh-expansion-parameter-substitude

${parameter/pattern/string}

    The pattern is expanded to produce a pattern just as in filename expansion.
    `parameter` is expanded and the longest match of pattern against its value
    is replaced with string. If pattern begins with ‘/’, all matches of pattern
    are replaced with string. Normally only the first match is replaced. If
    pattern begins with ‘#’, it must match at the beginning of the expanded
    value of parameter. If pattern begins with ‘%’, it must match at the end of
    the expanded value of parameter. 
    
    If string is null, matches of pattern are deleted and the / following
    pattern may be omitted. 
    
    If parameter is ‘@’ or ‘*’, the substitution operation is applied to each
    positional parameter in turn, and the expansion is the resultant list. If
    parameter is an array variable subscripted with ‘@’ or ‘*’, the substitution
    operation is applied to each member of the array in turn, and the expansion
    is the resultant list.

<ex>
The both has the same result.

LDFLAGS='-L/home/builds-src-dev/zinc-trunk/lib -Wl,--as-needed -Wl,-rpath-link'

-LDFLAGS=``echo $LDFLAGS | sed -e 's/\s-Wl,--as-needed//g'``

+LDFLAGS="${LDFLAGS/[[:space:]]-Wl,--as-needed}"


${parameter#word}
${parameter##word}

    The `word` is expanded to produce a pattern just as in filename expansion
      (see Filename Expansion). If the pattern matches the beginning of the
      expanded value of parameter, then the result of the expansion is the
      expanded value of parameter with the `shortest` matching pattern (the ‘#’
          case) or the longest matching pattern (the ‘##’ case) `deleted`. 
      
    If parameter is ‘@’ or ‘*’, the pattern removal operation is applied to each
    positional parameter in turn, and the expansion is the resultant list. If
    parameter is an array variable subscripted with ‘@’ or ‘*’, the pattern
    removal operation is applied to each member of the array in turn, and the
    expansion is the resultant list.


<ex>
test_fun=test_that_generates_url
test_handler="${test_fun#test_*}_handler"

echo $test_handler
_that_generates_url


={============================================================================
*kt_linux_bash_001* sh-expansion-command

3.5.4 Command Substitution

`command-substitution` allows `the output of a command` to replace the command
itself. Command substitution occurs when a command is enclosed as follows:

$(command)

or

``command`` 

Bash performs the expansion by executing command and replacing the command
substitution with the standard output of the command, with any trailing newlines
deleted. 

Embedded newlines are not deleted, but may be removed during `word splitting` 

note:
The command substitution $(cat file) can be replaced by the equivalent but
faster $(< file).

When the `old-style backquote` form of substitution is used, backslash retains its
literal meaning except when followed by ‘$’, ‘`’, or ‘\’. The first backquote
not preceded by a backslash terminates the command substitution. When using the
$(command) form, all characters between the parentheses make up the command;
none are treated specially.

Command substitutions may be nested. To nest when using the backquoted form,
        escape the inner backquotes with backslashes.

If the substitution appears within `double-quotes`, word splitting and filename
expansion are not performed on the results. 


={============================================================================
*kt_linux_bash_001* sh-expansion-word sh-ifs

3.5.7 Word Splitting

The shell scans the results of parameter expansion, command substitution, and
arithmetic expansion that did not occur within double quotes for word splitting.

The shell treats each character of $IFS as a delimiter, and splits the `results`
of the other expansions into words using these characters as `field terminators`

If IFS is unset, or its value is exactly <space><tab><newline>, the default,
then sequences of <space>, <tab>, and <newline> at the beginning and end of the
  results of the previous expansions are ignored, and any sequence of IFS
  characters not at the beginning or end serves to delimit words. 
  
If IFS has a value other than the default, then sequences of the whitespace
characters space and tab are ignored at the beginning and end of the word, as
long as the whitespace character is in the value of IFS (an IFS whitespace
    character). Any character in IFS that is not IFS whitespace, along with any
adjacent IFS whitespace characters, delimits a field. A sequence of IFS
whitespace characters is also treated as a delimiter. If the value of IFS is
null, no word splitting occurs.

Explicit null arguments ("" or '') are retained. Unquoted implicit null
arguments, resulting from the expansion of parameters that have no values, are
removed. If a parameter with no value is expanded within double quotes, a null
argument results and is retained.

Note that if no expansion occurs, no splitting is performed. 

<ex>
The ls shows many files and directories but $FILES shows one long line due to
command expansion and word splitting.

$ls
$FILES=``ls``
$echo $FILES | wc -l


note: $FILES do not change but echo changes depending on IFS Why? Due to
*sh-expansion-parameter*

$ IFS=$'\n'
$ echo $FILES
brentwood_macs_and_names_phase1.txt brentwood_macs_and_names_r36.txt brentwood_macs_and_names.txt find_matching_tests_by_version_and_mac.sh find_uploads_by_viewing_card.sh
$ echo $FILES | wc -l
1

$ IFS=$' '
$ echo $FILES
brentwood_macs_and_names_phase1.txt
brentwood_macs_and_names_r36.txt
brentwood_macs_and_names.txt
find_matching_tests_by_version_and_mac.sh
find_uploads_by_viewing_card.sh
$ echo $FILES | wc -l
5


<ex>
OLD_IFS=IFS
IFS=$'\n'

# LIST file has a format and read a line by line
# xxxx xxxx
# xxxx xxxx
# ....
MAC_ADDRESS_LIST=$(cat $MAC_ADDRESS_LIST_FILE)

IFS=OLD_IFS


={============================================================================
*kt_linux_bash_001* sh-expansion-arithmetic sh-expr

3.5.5 Arithmetic Expansion

Arithmetic expansion allows the evaluation of an arithmetic expression and the
substitution of the result. The format for arithmetic expansion is:

$(( expression ))

The expression is treated as if it were within double quotes, but a double quote
inside the parentheses is not treated specially. All tokens in the expression
undergo parameter and variable expansion, command substitution, and quote
removal. The result is treated as the arithmetic expression to be evaluated.
Arithmetic expansions may be nested.

The evaluation is performed according to the rules listed below (see
    `shell-arithmetic`). If the expression is invalid, Bash prints a message
indicating failure to the standard error and no substitution occurs. 

<ex>
The both makes the same result.

COUNT=10

while [ $COUNT -gt 0 ]; do
   echo Value of count is: $COUNT
   let COUNT=COUNT-1
done

COUNT=10

while [ $COUNT -gt 0 ]; do
   echo Value of count is: $COUNT
   COUNT=$(( $COUNT-1 ))
done


6.5 Shell Arithmetic

The shell allows `arithmetic-expressions` to be evaluated, as one of the shell
expansions or by the `let` and the -i option to the `declare` builtins.

Evaluation is done in fixed-width integers with no check for overflow, though
division by 0 is trapped and flagged as an error. The operators and their
precedence, associativity, and values are the same as in the C language. The
following list of operators is grouped into levels of equal-precedence
operators. The levels are listed in order of decreasing precedence.

id++ id--
    variable post-increment and post-decrement

++id --id
    variable pre-increment and pre-decrement


{sh-expr}
`expr` is external command but not shell builtin. 

https://www.gnu.org/software/coreutils/manual/html_node/expr-invocation.html#expr-invocation

16.4 expr: Evaluate expressions

expr evaluates an expression and writes the result on standard output. Each
token of the expression must be a separate argument. 

$ which expr
/usr/bin/expr

<ex>
Shows the same result.

#!/bin/bash

SIZE=1265403

ONE_TWO_THIRD=$(($SIZE * 2 / 3))
TWO_TWO_THIRD=``expr $SIZE \* 2 / 3``

echo "ONE=$ONE_TWO_THIRD"
echo "TWO=$TWO_TWO_THIRD"


<ex>
I mostly used expr for its regular expression matching. It is sometimes more
descriptive than sed or grep:

if expr "$value" : '.*bar$' >/dev/null; then ...

Compared to:

if echo "$value" | grep '.*bar$' >/dev/null; then ...

Or:

name=`expr "$filename" : '.*/\(.*\)\.[^.]*$'`

Compared to:

name=`echo "$filename" | sed 's!.*/\(.*\)\.[^.]*$/\1/'`

They are functionally equivalent, but expr was slightly faster. Especially in
the older days when some shells had expr as a builtin (DEC Ultrix, for
    example).

Also, I program in strict Bourne shell syntax, so I still use expr for basic
arithmatic, for example:

count=0
while [ $count -lt 10 ]; do
    # something
    count=`expr $count + 1`
done

As the question was tagged with bash, 
I would prefer 

if [[ "$value" =~ .*bar$ ]]; then instead your two if examples, and 
with [[ "$filename" =~ .*/(.*)\.[^.]*$ ]]; name="${BASH_REMATCH[1]}" the other two examples. –
manatwork Dec 1 '11 at 14:55 


={============================================================================
*kt_linux_bash_001* sh-expansion-process

3.5.6 Process Substitution

`process-substitution` is supported on systems that support named pipes (FIFOs)
or the /dev/fd method of naming open files. It takes the form of

<(list)

or

>(list)

The process `list` is run with its input or output connected to a FIFO or some
file in /dev/fd. The name of this file is passed as an argument to the current
command as the result of the expansion. If the >(list) form is used, writing to
the file will provide input for list. If the <(list) form is used, the file
passed as an argument should be read to obtain the output of list. Note that no
space may appear between the < or > and the left parenthesis, otherwise the
construct would be interpreted as a redirection.

When available, process substitution is performed simultaneously with parameter
and variable expansion, command substitution, and arithmetic expansion. 


={============================================================================
*kt_linux_bash_001* sh-expansion-filename sh-pattern

3.5.8 Filename Expansion

After word splitting, unless the -f option has been set, Bash scans each word
for the characters ‘*’, ‘?’, and ‘[’. If one of these characters appears, then
  the word is regarded as a `pattern`, and replaced with an
    `alphabetically-sorted` list of filenames matching the pattern. 
    
If no matching filenames are found, and the shell option `nullglob` is disabled,
   the word is left unchanged. If the nullglob option is set, and no matches are
     found, the word is removed. If the failglob shell option is set, and no
     matches are found, an error message is printed and the command is not
     executed. If the shell option nocaseglob is enabled, the match is performed
     without regard to the case of alphabetic characters.

When a pattern is used for filename expansion, the character ‘.’ at the start of
a filename or immediately following a slash must be matched explicitly, unless
the shell option dotglob is set. When matching a filename, the slash character
must always be matched explicitly. In other cases, the ‘.’ character is not
treated specially.

See the description of shopt in The Shopt Builtin, for a description of the
nocaseglob, nullglob, failglob, and dotglob options.

The GLOBIGNORE shell variable may be used to restrict the set of filenames
matching a pattern. If GLOBIGNORE is set, each matching filename that also
matches one of the patterns in GLOBIGNORE is removed from the list of matches.
The filenames . and .. are always ignored when GLOBIGNORE is set and not null.
However, setting GLOBIGNORE to a non-null value has the effect of enabling the
dotglob shell option, so all other filenames beginning with a ‘.’ will match. To
get the old behavior of ignoring filenames beginning with a ‘.’, make ‘.*’ one
of the patterns in GLOBIGNORE. The dotglob option is disabled when GLOBIGNORE is
unset. 


={============================================================================
*kt_linux_bash_001* sh-redirection

3.6 Redirections

Before a command is executed, its input and output may be redirected using a
special notation interpreted by the shell. Redirection allows commands’ file
handles to be duplicated, opened, closed, made to refer to different files, and
can change the files the command reads from and writes to. Redirection may also
be used to modify file handles in the current shell execution environment. The
following redirection operators may precede or appear anywhere within a simple
command or may follow a command. Redirections are processed in the order they
appear, from left to right. 

In the following descriptions, if the file descriptor number is omitted, and the
first character of the redirection operator is ‘<’, the redirection refers to
the `standard-input` (file descriptor 0). If the first character of the
redirection operator is ‘>’, the redirection refers to the `standard-output`
(file descriptor 1).

The `word` following the redirection operator in the following descriptions,
unless otherwise noted, is subjected to brace expansion, tilde expansion,
parameter expansion, command substitution, arithmetic expansion, quote removal,
filename expansion, and word splitting. If it expands to more than one word,
Bash reports an error.

Note that the order of redirections is significant. For example, the command

ls > dirlist 2>&1

directs both standard output (file descriptor 1) and standard error (file
    descriptor 2) to the file dirlist, while the command

ls 2>&1 > dirlist

`directs only` the standard output to file dirlist, because the standard error was
made a copy of the standard output before the standard output was redirected to
dirlist.

Bash handles several filenames specially when they are used in redirections, as
described in the following table:

/dev/fd/fd
    If fd is a valid integer, file descriptor fd is duplicated.

/dev/stdin
    File descriptor 0 is duplicated.
/dev/stdout
    File descriptor 1 is duplicated.
/dev/stderr
    File descriptor 2 is duplicated.

/dev/tcp/host/port
    If host is a valid hostname or Internet address, and port is an integer port
    number or service name, Bash attempts to open the corresponding TCP socket.

/dev/udp/host/port
    If host is a valid hostname or Internet address, and port is an integer port
    number or service name, Bash attempts to open the corresponding UDP socket. 

A failure to open or create a file causes the redirection to fail.

Redirections using file descriptors greater than 9 should be used with care, as
they may conflict with file descriptors the shell uses internally. 


3.6.1 Redirecting Input

Redirection of input causes the file whose name results from the expansion of
word to be opened for reading on file descriptor n, or the standard input (file
    descriptor 0) if n is not specified.

The general format for redirecting input is:

[n]<word

<ex>
$ <file `command`


3.6.2 Redirecting Output

Redirection of output causes the file whose name results from the expansion of
word to be opened for writing on file descriptor n, or the standard output (file
    descriptor 1) if n is not specified. If the file does not exist it is
`created`; if it does exist it is truncated to `zero-size`.

The general format for redirecting output is:

[n]>[|]word

If the redirection operator is ‘>’, and the `noclobber` option to the `set`
builtin has been enabled, the redirection will fail if the file whose name
results from the expansion of word exists and is a regular file. If the
redirection operator is ‘>|’, or the redirection operator is ‘>’ and the
noclobber option is not enabled, the redirection is attempted even if the file
named by word exists. 


3.6.4 Redirecting Standard Output and Standard Error

This construct allows both the standard output (file descriptor 1) and the
standard error output (file descriptor 2) to be redirected to the file whose
name is the expansion of word.

There are two formats for redirecting standard output and standard error:

&>word

and

>&word

Of the two forms, `the first is preferred` This is semantically equivalent to

>word 2>&1

When using the second form, word may not expand to a number or ‘-’. If it does,
other redirection operators apply (see Duplicating File Descriptors below) for
  compatibility reasons. 


<ex>
head &>/dev/null


<ex>
Wants to this cover two cases: 
one when a file has null first bytes, read fails and $? is 1. The other when a
file does not exist, read fails and $? is 1. But there is a error message from
head command when there is no file. So seems fine? 

NO

head -c 10 PCAT.DBJ &>/dev/null | tr -d '\0' | read -n 1

This makes read command always fails since redirect stdin and stdout to
/dev/null and there will be no input to read command! 

So have to redirect ONLY stderr.

head -c 10 PCAT.DBJ 2>/dev/null | tr -d '\0' | read -n 1


<ex>
One of the most common uses of file descriptors is to redirect STDOUT and STDERR
to separate files.  The basic syntax is

command 1> file1 2> file2

Often the STDOUT file descriptor, 1, is not written, so a shorter form of the
basic syntax is

command > file1 2> file2

*tool-tee*
<ex> the all are the same
prog1 > /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log 2>&1
prog1 2>&1 | tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log

prog1 |& tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log

$ (ZB_CFG=humax.2100 zb-make-g listprojects) > 2100.log 2>&1


<ex>
How does "done < $1" work? This is `redirection` since:

*sh-redirection-on-compound-commands*
Any redirections associated with a compound command apply to all commands within
that compound command unless explicitly overridden.


#!/bin/sh

if [ -f "$1" ] ; then
  i=0
  while read LINE
  do
    i=`echo "$i + 1" | bc`    # essentially, inc i
    echo "inner $i..."
  done < "$1"
  echo $i
fi

echo $i

$ ./sbash.sh list 
inner 1...
inner 2...
inner 3...
inner 4...
4
4


={============================================================================
*kt_linux_bash_001* sh-redirection-here

3.6.6 Here Documents

This type of redirection instructs the shell to read input from the current
source until a line containing only `word` (with no trailing blanks) is seen.
All of the lines read up to that point are then used `as-the-standard-input` for
a `command`.

The format of here-documents is:

<<[-]word
        here-document
delimiter

No parameter and variable expansion, command substitution, arithmetic expansion,
or filename expansion is performed on `word`. 
  
If any characters in `word` are quoted, the `delimiter` is the result of quote
removal on word, and the lines in the here-document are not expanded. If word is
unquoted, all lines of the here-document are subjected to parameter expansion,
command substitution, and arithmetic expansion, the character sequence \newline
  is ignored, and ‘\’ must be used to quote the characters ‘\’, ‘$’, and ‘`’.

If the redirection operator is ‘<<-’, then all leading tab characters are
stripped from input lines and the line containing delimiter. This allows
here-documents within shell scripts to be indented in a natural fashion.


3.6.7 Here Strings

A variant of here documents, the format is:

<<< word

The word undergoes brace expansion, tilde expansion, parameter and variable
expansion, command substitution, arithmetic expansion, and quote removal.

Pathname expansion and word splitting are not performed. The result is supplied
as a single string to the command on its standard input. 

<ex>

command << delimiter
document
delimiter

The delimiter must be a single word that does not contain spaces or tabs. For
example, to print a quick list of URLs, you could use the following here
document:

lpr << MYURLS
http://www.csua.berkeley.edu/~ranga/
http://www.cisco.com/
http://www.marathon.org/story/
http://www.gnu.org/
MYURLS

For example, you can use the following command to create a file with the short
list of URLs given previously:

cat > urls << MYURLS
http://www.csua.berkeley.edu/~ranga/
http://www.cisco.com/
http://www.marathon.org/story/
http://www.gnu.org/
MYURLS

note:
Output as seen between delimeters so will have spaces.

lpr << MYURLS
    http://www.csua.berkeley.edu/~ranga/
    http://www.cisco.com/
    http://www.marathon.org/story/
    http://www.gnu.org/
MYURLS

note:
Syntax error. Seems that here shall starts from beginning of a line. 

function() {
    lpr << MYURLS
    http://www.csua.berkeley.edu/~ranga/
    http://www.cisco.com/
    http://www.marathon.org/story/
    http://www.gnu.org/
    MYURLS
}


={============================================================================
*kt_linux_bash_001* sh-executing

3.7 Executing Commands

Q: Which one when there are script and exe binary of the same name in the same
dir?

A: The executable binary gets executed.

Since

3.7.2 Command Search and Execution

3. If the name is neither a shell function nor a builtin, and contains no
slashes, Bash searches each element of $PATH for a directory containing an
executable file by that name.

4. If the search is successful, or if the command name contains one or more
slashes, the shell executes the named program in a separate execution
environment. Argument 0 is set to the name given, and the remaining arguments to
the command are set to the arguments supplied, if any.

5. If this execution fails because the file is not in executable format, and the
file is not a directory, it is assumed to be a shell script and the shell
executes it as described in Shell Scripts. 


={============================================================================
*kt_linux_bash_024* sh-builtin

4 Shell Builtin Commands

`builtin-commands` are contained within the shell itself. When the name of a
builtin command is used as the first word of a simple command (see Simple
    Commands), the shell executes the command directly, without invoking another
program. Builtin commands are necessary to implement functionality impossible or
inconvenient to obtain with separate utilities.

Unless otherwise noted, each builtin command documented as accepting `options`
preceded by ‘-’ accepts ‘--’ to signify the end of the options. 

The :, true, false, and test builtins do not accept options and do not treat
‘--’ specially.  The exit, logout, break, continue, let, and shift builtins
accept and process arguments beginning with ‘-’ without requiring ‘--’. Other
builtins that accept arguments but are not specified as accepting options
interpret arguments beginning with ‘-’ as invalid options and require ‘--’ to
prevent this interpretation. 


={============================================================================
*kt_linux_bash_024* sh-builtin-base

4.1 Bourne Shell Builtins

The following shell builtin commands are inherited from the Bourne Shell. These
commands are implemented as specified by the POSIX standard. 

{sh-exec}
exec

    exec [-cl] [-a name] [command [arguments]]

    If command is supplied, it `replaces` the shell without creating a new
    process. 
    
    If the -l option is supplied, the shell places a dash at the beginning of
    the zeroth argument passed to command. This is what the login program does.
    The -c option causes command to be executed with an empty environment. If -a
    is supplied, the shell passes name as the zeroth argument to command. 
    
    If command cannot be executed for some reason, a non-interactive shell
    exits, unless the execfail shell option is enabled. In that case, it returns
    failure. An interactive shell returns failure if the file cannot be
    executed. If no command is specified, redirections may be used to affect the
    current shell environment. 
    
    If there are no redirection errors, the return status is zero; otherwise the
    return status is non-zero.

    // This is the second use? Usecase?
    // Set redirections for the program to execute or for the current shell. If
    // only redirections are given, the redirections affect the current shell
    // without executing any program. 

<ex>
As you can see, the subshell is replaced by echo.

user@host:~$ PS1="supershell$ "
supershell$ bash
user@host:~$ PS1="subshell$ "
subshell$ exec echo hello
hello
supershell$ 


{sh-shift}
shift

    shift [n]

    Shift the positional parameters `to-the-left` by n. The positional
    parameters from n+1 ... $# are renamed to $1 ... $#-n. Parameters
    represented by the numbers $# to $#-n+1 are unset. n must be a non-negative
    number less than or equal to $#. If n is zero or greater than $#, the
    positional parameters are not changed. If n is not supplied, it is assumed
    to be 1. The return status is zero unless n is greater than $# or less than
    zero, non-zero otherwise.

<ex>
See that use S1 all the way and $# decreases

#!/bin/bash
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"
shift;
echo "this is 1 $1, num $#"

$ ./sbash.sh 1 2 3 4 5 6 
this is 1 1, num 6
this is 1 2, num 5
this is 1 3, num 4
this is 1 4, num 3
this is 1 5, num 2

So can use 'loop' until there is no more args to process:

while [ "$1" != "" ]; do
   case $1 in
      -h | -help | --help)
         echo "help.."
         exit
         ;;
      release)
         echo "release..."
         exit
         ;;
      *)
         echo "else..."
         exit
         ;;
   esac
   shift
done


{sh-trap}
trap

    trap [-lp] [arg] [sigspec ...]

    The commands in arg are to be read and executed when the shell receives
    signal sigspec. 
    
    If arg is absent (and there is a single sigspec) or equal to ‘-’, each
    specified signal’s disposition is reset to the value it had when the shell
    was started. 
    
    If arg is the null string, then the signal specified by each sigspec is
    `ignored` by the shell and commands it invokes. 
    
    If arg is not present and -p has been supplied, the shell displays the trap
    commands associated with each sigspec. If no arguments are supplied, or only
    -p is given, trap prints the list of commands associated with each signal
    number in a form that may be reused as shell input. 
    
    The -l option causes the shell to print a list of signal names and their
    corresponding numbers. 
    
    Each `sigspec` is either a signal name or a signal number. Signal names are
    case insensitive and the SIG prefix is optional.

    If a sigspec is 0 or `EXIT`, arg is executed when the `shell-exits`. 
    
    If a sigspec is DEBUG, the command arg is executed before every simple
    command, for command, case command, select command, every arithmetic for
    command, and before the first command executes in a shell function. Refer to
    the description of the extdebug option to the shopt builtin (see The Shopt
        Builtin) for details of its effect on the DEBUG trap. If a sigspec is
    RETURN, the command arg is executed each time a shell function or a script
    executed with the . or source builtins finishes executing.

    If a sigspec is ERR, the command arg is executed whenever a pipeline (which
        may consist of a single simple command), a list, or a compound command
    returns a non-zero exit status, subject to the following conditions. The ERR
    trap is not executed if the failed command is part of the command list
    immediately following an until or while keyword, part of the test following
    the if or elif reserved words, part of a command executed in a && or || list
    except the command following the final && or ||, any command in a pipeline
    but the last, or if the command’s return status is being inverted using !.
    These are the same conditions obeyed by the errexit (-e) option.

    Signals ignored upon entry to the shell cannot be trapped or reset. Trapped
    signals that are not being ignored are reset to their original values in a
    subshell or subshell environment when one is created.

    The return status is zero unless a sigspec does not specify a valid signal.

<ex>
To execute function clean_up when shell exits and reset sigspec in that
function.

trap clean_up EXIT

clean_up() {
    trap - {0..15}

    echo
    echo "Cleaning up ..."

    # NOTE: Since this is usually run in an interactive shell, Ctrl-C will send
    #       SIGINT to child processes as well so they might be terminated
    #       already.
    echo "Terminating dbussenddaemon ..."
    kill %?dbussenddaemon &>/dev/null
}


{sh-colon}
: (a colon)

    : [arguments]

    Do nothing beyond expanding `arguments` and performing redirections. The
    return status is zero.

<ex> can use as 'no-op'
Used as a no-op, which is a command that does nothing and thus can be safely
inserted anywhere a command is needed for purely syntactical reasons:

if [ -x $CMD ]
then :
else
   echo Error: $CMD is not executable >&2
fi

In this example, assume you are not quite ready to write the code to follow the
then statement. The shell flags a syntax error if you leave that code out
completely, so you insert the : command as a temporary noop command that can be
replaced by the desired code later.

<ex>
Because the : always returns a successful result, it is sometimes used to create
an infinite loop:

while :
do
  echo "Enter some input: \c"
  read INPUT
  [ "$INPUT" = stop ] && break
Done

Because the : always returns a successful or true result, the while loop will
continue forever or until a break is executed within the loop. Sometimes you
might find that "while true" used in place of while : but using the : is more
efficient because it is a shell built-in command, whereas true is a command that
must be read from a disk file, if you are in the Bourne shell. *sh-true*


{sh-export}
export

    export [-fn] [-p] [name[=value]]

    Mark each name to be passed to child processes in the environment. 
    
    If the `-f option` is supplied, the names refer to shell functions;
    otherwise the names refer to shell variables. 
    
    The -n option means to no longer mark each name for export. If no names
    are supplied, or if the -p option is given, a list of names of all
    exported variables is displayed. The -p option displays output in a form
    that may be reused as input. If a variable name is followed by =value, the
    value of the variable is set to value.

    The return status is zero unless an invalid option is supplied, one of the
    names is not a valid shell variable name, or -f is supplied with a name
    that is not a shell function.


{sh-let}
let

    let expression [expression ...]

    The let builtin allows arithmetic to be performed on shell variables. Each
    expression is evaluated according to the rules given below in Shell
    Arithmetic. If the last expression evaluates to 0, let returns 1;
    otherwise 0 is returned.


={============================================================================
*kt_linux_bash_024* sh-builtin-bash

4.2 Bash Builtin Commands

This section describes builtin commands which are unique to or have been
extended in Bash. Some of these commands are specified in the POSIX standard. 

{sh-echo}
echo

    echo [-neE] [arg ...]

    Output the args, separated by spaces, terminated with a `newline`. The
    return status is 0 unless a write error occurs. If -n is specified, the
    trailing newline is suppressed. 
    
    If the -e option is given, interpretation of the following backslash-escaped
    characters is enabled. 
    
    The -E option disables the interpretation of these escape characters, even
    on systems where they are interpreted by default. The xpg_echo shell option
    may be used to dynamically determine whether or not echo expands these
    escape characters by default. echo does not interpret -- to mean the end of
    options.

note:
The `echo` prints out `newline` regardless of value of variable. The following
prints out value of variable and newline. When variable is not set or do not
have value, it prints out newline only and it the same as using echo only.

echo $variable

    echo interprets the following `escape-sequences`: 


{sh-command}
command

    command [-pVv] command [arguments …]

    Runs command with arguments ignoring any `shell-function` named command.
    Only shell builtin commands or commands found by searching the PATH are
    executed. If there is a shell function named ls, running ‘command ls’ within
    the function will execute the external command ls instead of calling the
    function recursively. The -p option means to use a default value for PATH
    that is guaranteed to find all of the standard utilities. The return status
    in this case is 127 if command cannot be found or an error occurred, and the
    exit status of command otherwise.

    If either the -V or -v option is supplied, a description of command is
    printed. The -v option causes a single word indicating the command or file
    name used to invoke command to be displayed; the -V option produces a more
    verbose description. In this case, the return status is zero if command is
    found, and non-zero if not.

<ex>
if [ -x "$(command -v nexus-inspect)" ]; then
    echo "Running on a Nexus enabled system"

note: 
-v returns a full path. On host, -v returns filename so shows the same when use
which command. That is filename when there it is and null when not.

++ command -v nexus-inspect
+ '[' -x /opt/zinc/bin/nexus-inspect ']'
+ echo 'Running on a Nexus enabled system'
Running on a Nexus enabled system


{sh-read}
read

    read [-ers] [-a aname] [-d delim] [-i text] [-n nchars]
        [-N nchars] [-p prompt] [-t timeout] [-u fd] [`name` ...]

    One line is `read from` the `standard-input`, or from the file descriptor fd
    supplied as an argument to the -u option, and the first word is assigned to
    the first `name`, the second word to the second `name`, and so on, with leftover
    words and their intervening separators assigned to the last name. If there
    are fewer words read from the input stream than names, the remaining names
    are assigned empty values. 
    
    *sh-ifs*
    The characters in the value of the `IFS` variable are used to split the
    line into words using the same rules the shell uses for expansion. The
    backslash character ‘\’ may be used to remove any special meaning for the
    next character read and for line continuation. If no names are supplied,
    the line read is assigned to the variable REPLY. 
    
    The return code is `zero-exit-status`, unless end-of-file is encountered,
    read times out (in which case the return code is greater than 128), a
    variable assignment error (such as assigning to a readonly variable)
  occurs, or an invalid file descriptor is supplied as the argument to -u. 


<ex>
A common use of input redirection in conjunction with the read command is the
reading of a file one line at a time using the while loop.

#!/bin/bash

cat list | while read f; do
  echo "line $f"
done


<ex> 
Get the `second word` from input line

$ grep 'sda' DF.LOG | grep 'FAT'
/dev/sda1              63   976751999   488375968   1 FAT12

$ echo "/dev/sda1              63   976751999   488375968   1 FAT12" | \
    while read a b c; do echo $b; done
63

In the script:

A=``grep 'sda' $F | grep 'FAT' | while read a b c;do echo $b;done``


<ex>
To search through the same errors in logs which was uploaded in Feburary and was
under translation. For example, translation/Zone9-Box5_Feb_18_09_38_42b3e5c91

ls translation/ \
    | grep Feb | while read x; do pushd -n translation/$x;
    pwd; egrep --color -ano 'btreePageFromDbPage' .detailed_output; popd -n; 
    done 

WHY? Since "pushd -n" do not `cd` and means runs those command on the parent?   


{sh-printf}

printf

    printf [-v var] format [arguments]

    Write the formatted `arguments` to the standard output under the control of
    the `format`. The -v option causes the output to be assigned to the variable
    var rather than being printed to the standard output.

    The format is a character string which contains three types of objects:
    plain characters, which are simply copied to standard output, character
    escape sequences, which are converted and copied to the standard output, and
    format specifications, each of which causes printing of the next successive
    argument. In addition to the standard `printf(1)` formats, printf interprets
    the following extensions:

    %b
        Causes printf to expand backslash escape sequences in the corresponding
        argument, except that ‘\c’ terminates output, backslashes in ‘\'’, ‘\"’,
        and ‘\?’ are not removed, and octal escapes beginning with ‘\0’ may
        contain up to four digits. 

    %q
        Causes printf to output the corresponding argument in a format that can
        be reused as shell input. 

    %(datefmt)T
        Causes printf to output the date-time string resulting from using
        datefmt as a format string for strftime(3). The corresponding argument
        is an integer representing the number of seconds since the epoch. Two
        special argument values may be used: -1 represents the current time, and
        -2 represents the time the shell was invoked. If no argument is
        specified, conversion behaves as if -1 had been given. This is an
        exception to the usual printf behavior. 

    Arguments to non-string format specifiers are treated as C language
    constants, except that a leading plus or minus sign is allowed, and if the
    leading character is a single or double quote, the value is the ASCII value
    of the following character.

    The format is reused as necessary to consume all of the arguments. If the
    format requires more arguments than are supplied, the extra format
    specifications behave as if a zero value or null string, as appropriate, had
    been supplied. The return value is zero on success, non-zero on failure.

<ex>

debug() {
    # Note: echo -e doesn't work on OS X's default bash (3.2).
    printf '\n\033[0;32m%s\n' "$*"
    tput sgr0
}


={============================================================================
*kt_linux_bash_024* sh-builtin-set

*sh-set*

4.3.1 The Set Builtin

This builtin is so complicated that it deserves its own section. `set-command`
allows you to change the values of shell options and set the positional
parameters, or to display the names and values of shell variables. 

set

    set [--abefhkmnptuvxBCEHPT] [-o option-name] [argument ...]
    set [+abefhkmnptuvxBCEHPT] [+o option-name] [argument ...]

    If no options or arguments are supplied, set displays the names and values
    of all shell variables and functions, sorted according to the current
    locale, in a format that may be reused as input for setting or resetting
    the currently-set variables. Read-only variables cannot be reset. In POSIX
    mode, only shell variables are listed. 

-x
    Print a trace of simple commands, for commands, case commands, select
    commands, and arithmetic for commands and their arguments or associated
    word lists after they are expanded and before they are executed. The value
    of the PS4 variable is expanded and the resultant value is printed before
    the command and its expanded arguments.

<ex>
$ set -o
allexport      	off
braceexpand    	on
emacs          	on
errexit        	off
errtrace       	off
functrace      	off
hashall        	on
histexpand     	on
history        	on
ignoreeof      	off
interactive-comments	on
keyword        	off
monitor        	on
noclobber      	off
noexec         	off
noglob         	off
nolog          	off
notify         	off
nounset        	off
onecmd         	off
physical       	off
pipefail       	off
posix          	off
privileged     	off
verbose        	off
vi             	off
xtrace         	off


4.3.2 The Shopt Builtin

This builtin allows you to change additional shell `optional` behavior.

shopt

    shopt [-pqsu] [-o] [optname ...]


direxpand
    If set, Bash replaces directory names with the results of word expansion
    when performing filename completion. This changes the contents of the
    readline editing buffer. If not set, Bash attempts to preserve what the user
    typed.


With no options, or with the -p option, a list of all settable options is
displayed, with an indication of whether or not each is set.

$ shopt
autocd         	off
cdable_vars    	off
cdspell        	off
checkhash      	off
checkjobs      	off
checkwinsize   	on
cmdhist        	on
compat31       	off
compat32       	off
compat40       	off
compat41       	off
direxpand      	on
dirspell       	off
dotglob        	off
execfail       	off
expand_aliases 	on
extdebug       	off
extglob        	on
extquote       	on
failglob       	off
force_fignore  	on
globstar       	off
gnu_errfmt     	off
histappend     	on
histreedit     	off
histverify     	off
hostcomplete   	off
huponexit      	off
interactive_comments	on
lastpipe       	off
lithist        	off
login_shell    	off
mailwarn       	off
no_empty_cmd_completion	off
nocaseglob     	off
nocasematch    	off
nullglob       	off
progcomp       	on
promptvars     	on
restricted_shell	off
shift_verbose  	off
sourcepath     	on
xpg_echo       	off


={============================================================================
*kt_linux_bash_038* sh-builtin-declare

declare

    declare [-aAfFgilnrtux] [-p] [name[=value] ...]

    Declare variables and give them attributes. If no names are given, then
    display the values of variables instead. 

    The -p option will display the attributes and values of each name. When -p
    is used with name arguments, additional options, other than -f and -F, are
    ignored.

    When -p is supplied without name arguments, declare will display the
    attributes and values of all variables having the attributes specified by
    the additional options. If no other options are supplied with -p, declare
    will display the attributes and values of all shell variables. The -f option
    will restrict the display to shell functions.

    The -F option inhibits the display of function definitions; only the
    function name and attributes are printed. If the extdebug shell option is
    enabled using shopt (see The Shopt Builtin), the source file name and line
    number where the function is defined are displayed as well. -F implies -f.

    The -g option forces variables to be created or modified at the global
    scope, even when declare is executed in a shell function. It is ignored in
    all other cases.

    The following options can be used to restrict output to variables with the
    specified attributes or to give variables attributes: 

    -a
    Each name is an indexed array variable (see Arrays).

    -A
    Each name is an associative array variable (see Arrays).

    -r
    Make names readonly. These names cannot then be assigned values by
    subsequent assignment statements or unset.

    -x     
    Mark  names  for  export  to  subsequent commands via the environment.


={============================================================================
*kt_linux_bash_024* sh-builtin-job

7.2 Job Control Builtins

kill

    kill [-s sigspec] [-n signum] [-sigspec] jobspec or pid
    kill -l [exit_status]

    Send a signal specified by `sigspec` or `signum` to the process named by job
    specification jobspec or process ID pid. sigspec is either a
    case-insensitive signal name such as SIGINT (with or `without` the SIG
        prefix) or a signal number; signum is a signal number. If sigspec and
    signum are not present, SIGTERM is used. The -l option lists the signal
    names. If any arguments are supplied when -l is given, the names of the
    signals corresponding to the arguments are listed, and the return status is
    zero. exit_status is a number specifying a signal number or the exit status
    of a process terminated by a signal. The return status is zero if at least
    one signal was successfully sent, or non-zero if an error occurs or an
    invalid option is encountered.  
    
wait

    wait [-n] [jobspec or pid ...]

    Wait until the child process specified by each process ID pid or job
    specification jobspec exits and return the exit status of the last command
    waited for. If a job spec is given, all processes in the job are waited for.
    If no arguments are given, all currently active child processes are waited
    for, and the return status is zero. If the -n option is supplied, wait waits
      for any job to terminate and returns its exit status. If neither jobspec
        nor pid specifies an active child process of the shell, the return
          status is 127.


={============================================================================
*kt_linux_bash_024* sh-builtin-help

4.2 Bash Builtin Commands

help

    help [-dms] [pattern]

    Display helpful information about builtin commands. If pattern is specified,
    help gives detailed help on all commands matching pattern, otherwise a list
    of the builtins is printed.

    Options, if supplied, have the following meanings:

    -d
        Display a short description of each pattern 
    -m
        Display the description of each pattern in a manpage-like format 
    -s
        Display only a short usage synopsis for each pattern 

    The return status is zero unless no command matches pattern.


={============================================================================
*kt_linux_bash_025* sh-special-variable

5.1 Bourne Shell Variables

Bash uses certain shell variables in the same way as the Bourne shell. In some
cases, Bash assigns a default value to the variable. 

PATH
    A `colon-separated` list of directories in which the shell looks for
    commands. A zero-length (null) directory name in the value of PATH indicates
    the current directory. A null directory name may appear as two adjacent
    colons, or as an initial or trailing colon.

PS1
    The primary prompt string. The default value is ‘\s-\v\$ ’. See 6.9
    Controlling the Prompt, for the complete list of escape sequences that are
    expanded before PS1 is displayed.

6.9 Controlling the Prompt

\t
    The time, in 24-hour HH:MM:SS format. 
\T
    The time, in 12-hour HH:MM:SS format. 


PS2
    The secondary prompt string. The default value is ‘> ’.


5.2 Bash Variables

These variables are set or used by Bash, but other shells do not normally treat
them specially. 


={============================================================================
*kt_linux_bash_023* sh-invoking

<ex>
# in the script
#!/bin/bash -eix


6.1 Invoking Bash

bash [long-opt] 
    [-ir] [-abefhkmnptuvxdBCDHP] [-o option] [-O shopt_option] [argument ...]
bash [long-opt] 
    [-abefhkmnptuvxdBCDHP] [-o option] [-O shopt_option] -c string [argument
    ...]
bash [long-opt] 
    -s [-abefhkmnptuvxdBCDHP] [-o option] [-O shopt_option] [argument ...]

All of the `single-character` options used with the `set` builtin can be used as
options when the shell is invoked. 

In addition, there are several multi-character options that you can use. These
options must appear on the 'command' line before the single-character options to
be recognized. 

--debugger
    Arrange for the debugger profile to be executed before the shell starts.
    Turns on extended debugging mode (see The Shopt Builtin for a description of
        the extdebug option to the shopt builtin).

    note: http://bashdb.sourceforge.net/bashdb.html

There are several single-character options that may be supplied at invocation
which are not available with the `set` builtin. 

-i
    Force the shell to run interactively. Interactive shells are described in
    Interactive Shells.


{end-of-option}
--
    A -- signals the `end-of-options` and disables further option processing.
    Any arguments after the -- are treated as filenames and arguments. 

<ex>
From the below command line, what is "--"? The "--" signify the `end-of-options`

xxx.sh -o humax.1000 -- -r

xxx.sh:
   ...
	script-two "$@" "$host"

So the "-r" is passed to script-two as `argument` which is from xxx.sh

<ex>
This syntax ensures that you can run commands on the remote server without ssh
parsing them:

ssh nixcraft@server1.cyberciti.biz -- command1 --arg1 --arg2

The above syntax tell ssh not try to parse --arg1 and --arg2 after -- command
line options. This ensures that command1 will accept --arg1 and --arg2 as
arguments.

# safe examples
ssh nixcraft@server1.cyberciti.biz -- --commandName --arg1 --arg2

This kind of behavior is mostly defined and handled by the ssh command and not
by your bash/ksh/csh shell. This is also true for many other commands. 

For example you can not create or view a file named --file or -f using cat
command

# fail
cat --file    # cat: unrecognized option '--f'
cat -f        # cat: invalid option -- 'f'

Instead try passing double dash "--" to instruct cat command not to try to parse
what comes after command line options:

# works
cat -- --file
cat -- -f


4.3.1 The Set Builtin

This builtin is so complicated that it deserves its own section. `set` allows
you to change the values of shell options and set the positional parameters, or
to display the names and values of shell variables.

set

    set [--abefhkmnptuvxBCEHPT] [-o option-name] [argument ...]
    set [+abefhkmnptuvxBCEHPT] [+o option-name] [argument ...]

If no options or arguments are supplied, `set` displays the names and values of
all shell variables and functions, sorted according to the current locale, in a
format that may be reused as input for setting or resetting the currently-set
variables. Read-only variables cannot be reset. In POSIX mode, only shell
variables are listed.

When options are supplied, they set or unset shell attributes. Options, if
specified, have the following meanings: 

-e
    Exit immediately if a pipeline (see Pipelines), which may consist of a
    single simple command (see Simple Commands), a list (see Lists), or a
    compound command (see Compound Commands) returns a non-zero status. The
    shell does not exit if the command that fails is part of the command list
    immediately following a while or until keyword, part of the test in an if
    statement, part of any command executed in a && or || list except the
    command following the final && or ||, any command in a pipeline but the
    last, or if the command’s return status is being inverted with !. If a
    compound command other than a subshell returns a non-zero status because a
    command failed while -e was being ignored, the shell does not exit. A trap
    on ERR, if set, is executed before the shell exits.

    This option applies to the shell environment and each subshell environment
    separately (see Command Execution Environment), and may cause subshells to
    exit before executing all the commands in the subshell.

    If a compound command or shell function executes in a context where -e is
    being ignored, none of the commands executed within the compound command or
    function body will be affected by the -e setting, even if -e is set and a
    command returns a failure status. If a compound command or shell function
    sets -e while executing in a context where -e is ignored, that setting will
    not have any effect until the compound command or the command containing the
    function call completes.

-x
    Print a trace of simple commands, for commands, case commands, select
    commands, and arithmetic for commands and their arguments or associated word
    lists after they are expanded and before they are executed. The value of the
    PS4 variable is expanded and the resultant value is printed before the
    command and its expanded arguments.


={============================================================================
*kt_linux_bash_030* sh-interactive-or-not

http://www.gnu.org/software/bash/manual/html_node/Interactive-Shells.html#Interactive-Shells

6.3.1 What is an Interactive Shell?

An `interactive-shell` is one started without non-option arguments, unless -s is
specified, without specifying the -c option, and whose input and error output
are both connected to terminals (as determined by isatty(3)), or one started
with the -i option.

An interactive shell generally reads from and writes to a user's 'terminal'.

The -s invocation option may be used to set the positional parameters when an
interactive shell is started. 


6.3.3 Interactive Shell Behavior

When the shell is running interactively, it changes its behavior in several
ways.

    Startup files are read and executed as described in Bash Startup Files.

    // ...


6.2 Bash Startup Files

This section describes how Bash executes its startup files. If any of the files
exist but cannot be read, Bash reports an error. Tildes are expanded in
filenames as described above under Tilde Expansion (see Tilde Expansion).

Invoked as an interactive login shell, or with --login

When Bash is invoked as an interactive login shell, or as a non-interactive
shell with the --login option, it first reads and executes commands from the
file /etc/profile, if that file exists. After reading that file, it looks for
~/.bash_profile, ~/.bash_login, and ~/.profile, in that order, and reads and
executes commands from the first one that exists and is readable. The
--noprofile option may be used when the shell is started to inhibit this
behavior.

When a login shell exits, Bash reads and executes commands from the file
~/.bash_logout, if it exists.


Invoked as an interactive non-login shell

When an interactive shell that is not a login shell is started, Bash reads and
executes commands from note: ~/.bashrc, if that file exists. This may be
inhibited by using the --norc option. The --rcfile file option will force Bash
to read and execute commands from file instead of ~/.bashrc.

So, typically, your ~/.bash_profile contains the line

if [ -f ~/.bashrc ]; then . ~/.bashrc; fi

after (or before) any login-specific initializations.

note:
When use -i option in the script, it does exit then reaches the end and there
should be other way to keep that shell running.


Invoked non-interactively

When Bash is started non-interactively, to run a `shell-script`, for example, it
looks for the variable BASH_ENV in the environment, expands its value if it
appears there, and uses the expanded value as the name of a file to read and
execute. Bash behaves as if the following command were executed:

if [ -n "$BASH_ENV" ]; then . "$BASH_ENV"; fi

but the value of the PATH variable is not used to search for the filename.

As noted above, if a non-interactive shell is invoked with the --login option,
   Bash attempts to read and execute commands from the login shell startup
     files.


Invoked with name sh

If Bash is invoked with the name sh, it tries to mimic the startup behavior of
historical versions of sh as closely as possible, while conforming to the POSIX
standard as well.

When invoked as an interactive login shell, or as a non-interactive shell with
the --login option, it first attempts to read and execute commands from
/etc/profile and ~/.profile, in that order. The --noprofile option may be used
to inhibit this behavior. When invoked as an interactive shell with the name sh,
Bash looks for the variable ENV, expands its value if it is defined, and uses
  the expanded value as the name of a file to read and execute. Since a shell
  invoked as sh does not attempt to read and execute commands from any other
  startup files, the --rcfile option has no effect. A non-interactive shell
  invoked with the name sh does not attempt to read any other startup files.

When invoked as sh, Bash enters POSIX mode after the startup files are read.


Invoked by remote shell daemon

Bash attempts to determine when it is being run with its standard input
connected to a network connection, as when executed by the remote shell daemon,
usually rshd, or the secure shell daemon sshd. If Bash determines it is being
  run in this fashion, it reads and executes commands from ~/.bashrc, if that
  file exists and is readable. It will not do this if invoked as sh. The --norc
  option may be used to inhibit this behavior, and the --rcfile option may be
  used to force another file to be read, but neither rshd nor sshd generally
  invoke the shell with those options or allow them to be specified. 


http://tldp.org/LDP/abs/html/intandnonint.html

36.1. Interactive and non-interactive shells and scripts

An interactive shell reads commands from user input on a tty. Among other
things, such a shell reads startup files on activation, displays a prompt, and
enables job control by default. The user can interact with the shell.

A shell running a script is always a non-interactive shell.

Let us consider an interactive script to be one that requires input from the
user, usually with read statements (see Example 15-3). "Real life" is actually a
bit messier than that. For now, assume an interactive script is bound to a tty,
a script that a user has invoked from the console or an xterm.

Init and startup scripts are necessarily non-interactive, since they must run
without human intervention. Many administrative and system maintenance scripts
are likewise non-interactive. Unvarying repetitive tasks cry out for automation
by non-interactive scripts.

Non-interactive scripts can run in the background, but interactive ones hang,
waiting for input that never comes.


={============================================================================
*kt_linux_bash_023* sh-command-history

9.1 Bash History Facilities

The value of the HISTSIZE shell variable is used as the number of commands to
save in a history list.

When the shell starts up, the history is initialized from the file named by the
HISTFILE variable (default ~/.bash_history). The file named by the value of
HISTFILE is truncated, if necessary, to contain no more than the number of lines
specified by the value of the HISTFILESIZE variable. When a shell with history
enabled exits, the last $HISTSIZE lines are copied from the history list to the
file named by $HISTFILE. If the histappend shell option is set (see Bash
    Builtins), the lines are appended to the history file, otherwise the history
file is overwritten. If HISTFILE is unset, or if the history file is unwritable,
the history is not saved. After saving the history, the history file is
  truncated to contain no more than $HISTFILESIZE lines. If HISTFILESIZE is
  unset, or set to null, a non-numeric value, or a numeric value less than zero,
the history file is not truncated.


{erase-duplicates}
export HISTCONTROL=erasedups

5.2 Bash Variables

HISTCONTROL

    A colon-separated list of values controlling how commands are saved on the
    history list. If the list of values includes ‘ignorespace’, lines which
    begin with a space character are not saved in the history list. A value of
    ‘ignoredups’ causes lines which match the previous history entry to not be
    saved. A value of ‘ignoreboth’ is shorthand for ‘ignorespace’ and
    ‘ignoredups’. A value of `erasedups` causes all previous lines matching the
    current line to be removed from the history list before that line is saved.
    Any value not in the above list is ignored. If HISTCONTROL is unset, or does
    not include a valid value, all lines read by the shell parser are saved on
    the history list, subject to the value of HISTIGNORE. The second and
    subsequent lines of a multi-line compound command are not tested, and are
    added to the history regardless of the value of HISTCONTROL.


<history-per-terminal>
# in .bash_profile
echo $SSH_TTY
export HISTFILE=/home/NDS-UK/parkkt/.bash_history_$(echo $SSH_TTY | cut -f 4 -d'/')
export HISTFILESIZE=10000
export HISTSIZE=10000 

note: better to have TTY number in the prompt as well.


9.2 Bash History Builtins

Bash provides two builtin commands which manipulate the history list and history
file.

fc

    fc [-e ename] [-lnr] [first] [last]

This is not quite what you want as it will launch an editor first, but that is
probably a good thing since it gives you a chance to double check that you have
the correct commands and even edit them using all the capabilities of your
favorite editor. Once you save you changes and exit the editor, the commands
will be run.

The first form selects a range of commands from first to last from the history
list and displays or edits and re-executes them.

If the -l flag is given, the commands are listed on standard output. The -n
flag suppresses the command numbers when listing. The -r flag reverses the
order of the listing. When editing is complete, the edited commands are echoed
and executed. 

$ fc 100 120


    fc -s [pat=rep] [command]

In the second form, command is re-executed after each instance of
`pat`(pattern) in the selected command is replaced by rep. command is
intepreted the same as first above.

A useful alias to use with the fc command is r='fc -s', so that typing ‘r cc’
runs the last command `beginning` with cc and typing ‘r’ re-executes the last
command (see Aliases). 


={============================================================================
*kt_linux_bash_032* sh-command-edit

https://www.gnu.org/software/bash/manual/bash.html#Command-Line-Editing

8 Command Line Editing

This chapter describes the basic features of the GNU command line editing
interface. Command line editing is provided by the Readline library, which is
used by several different programs, 'including' Bash. Command line editing is
enabled `by-default` when using an interactive shell, unless the --noediting
option is supplied at shell invocation. Line editing is also used when using the
-e option to the read builtin command (see Bash Builtins). 

By default, the line editing commands are similar to those of Emacs. A vi-style
line editing interface is also available. Line editing can be enabled at any
time using the `-o emacs` or `-o vi` options to the set builtin command (see The
    Set Builtin), or disabled using the +o emacs or +o vi options to set. 


8.1 Introduction to Line Editing

The following paragraphs describe the notation used to represent keystrokes.

The text C-k is read as ‘Control-K’ and describes the character produced when
the k key is pressed while the Control key is depressed.

The text M-k is read as ‘Meta-K’ and describes the character produced when the
Meta key (if you have one) is depressed, and the k key is pressed. The Meta key
is labeled `ALT` on many keyboards. 

If you do not have a Meta or ALT key, or another key working as a Meta key, the
identical keystroke can be generated by typing `ESC` first, and then typing k.
Either process is known as metafying the k key. 


8.2 Readline Interaction

Often during an interactive session you type in a long line of text, only to
notice that the first word on the line is misspelled. The Readline library gives
you a set of commands for manipulating the text as you type it in, allowing you
to just fix your typo, and not forcing you to retype the majority of the line.

Using these editing commands, you move the cursor to the place that needs
correction, and delete or insert the text of the corrections. Then, when you are
satisfied with the line, you simply press RET. You do not have to be at the end
of the line to press RET; the entire line is accepted regardless of the location
of the cursor within the line. 


*sh-edit-key*

C-b
    Move back one character. 
C-f
    Move forward one character. 

*C-a*
    Move to the start of the line. 
*C-e*
    Move to the end of the line. 

*M-f* note: M is ALT
    Move forward a `word`, where a word is composed of letters and digits. 
*M-b*
    Move backward a word. 

DEL or Backspace
    Delete the character to the left of the cursor. 

C-d
    Delete the character underneath the cursor. 

C-l
    Clear the screen, reprinting the current line at the top. 

*C-k*
    Kill the text from the current cursor position to the `end-of-the-line`.

*M-d*
    Kill from the cursor to the `end` of the current word, or, if between words,
to the end of the next word. Word boundaries are the same as those used by M-f.

// note:
// Do NOT work and instead use M-BACK
// M-DEL
//     Kill from the cursor the `start` of the current word, or, if between
//     words, to the start of the previous word. Word boundaries are the same as
//     those used by M-b.

*C-w*
    Kill from the cursor to the previous whitespace. This is different than
    M-DEL because the word boundaries differ.

ESC Backspace
    Kill from the cursor to the previous word boundaries.

// <ex>
// use cut and paste
//
// git init --bare /path/to/repo.git
// git remote add origin /path/to/repo.git
// 
// Notice that the second command uses the same path at the end. Instead of
// typing that path twice, you could copy and paste it from the first command,
// using this sequence of keystrokes:
// 
// Press the up arrow to bring back the previous command.
// 
// Press Ctrl-w to cut the path part: "/path/to/repo.git".
// 
// Press Ctrl-c to cancel the current command.
// 
// Type git remote add origin, and press Ctrl-y to paste the path. 


8.5 Readline vi Mode

While the Readline library does not have a full set of vi editing functions, it
does contain enough to allow simple editing of the line. The Readline vi mode
behaves as specified in the POSIX standard.

In order to switch interactively between emacs and vi editing modes, use the
‘set -o emacs’ and ‘set -o vi’ commands (see The Set Builtin). The Readline
default is emacs mode.

When you enter a line in vi mode, you are already placed in 'insertion' mode, as
if you had typed an "i". Pressing ESC switches you into 'command' mode, where
  you can edit the text of the line with the standard vi movement keys, move to
    previous history lines with "k" and subsequent lines with "j", and so forth. 


={============================================================================
*kt_linux_bash_032* sh-command-search

8.2.5 Searching for Commands in the History

Readline provides commands for searching through the command history
*sh-history* for lines containing a specified string. There are two search
modes: incremental and non-incremental.

To `incremental-search` backward in the history for a particular string, type
`C-r`. Typing C-s searches forward through the history. The characters present
in the value of the isearch-terminators variable are used to terminate an
incremental search. If that variable has not been assigned a value, the `ESC`
and C-J characters will terminate an incremental search. `C-g` will abort an
incremental search and restore the original line. When the search is terminated,
the history entry containing the search string becomes the current line.

To find other matching entries in the history list, type C-r or C-s as
appropriate. This will search backward or forward in the history for the next
entry matching the search string typed so far. 

Any other key sequence bound to a Readline command will terminate the search and
execute that command. For instance, a `RET` will terminate the search and accept
the line, thereby executing the command from the history list. 

A movement command will terminate the search, make the last line found the
current line, and begin editing.

Readline remembers the last incremental search string. If two C-rs are typed
without any intervening characters defining a new search string, any remembered
search string is used.

<ex>
Once you've found the command you have several options:

1. Press RET to run it
2. <C-r> to cycle through other commands that are filterd out with the letters
you've typed.
3. <C-g> to quit the search and back to the command line empty-handed
4. Press ESC to take one and edit the command.


={============================================================================
*kt_linux_bash_032* sh-command-readline-init

8.3.1 Readline Init File Syntax

There are only a few basic constructs allowed in the Readline init file. Blank
lines are ignored.  Lines beginning with a `#' are comments. Lines beginning
with a `$' indicate conditional constructs (see section 8.3.2 Conditional Init
    Constructs). Other lines denote variable settings and key bindings.

Variable Settings
You can modify the run-time behavior of Readline by altering the values of
variables in Readline using the set command within the init file. The syntax is
simple:

    set variable value

Here, for example, is how to change from the default Emacs-like key binding to
use vi line editing commands:

    set editing-mode vi

To set binding to up/down key to history search:

# ~/.inputrc
"\e[A": history-search-backward
"\e[B": history-search-forward

or equivalently,

# ~/.bashrc
bind '"\e[A": history-search-backward'
bind '"\e[B": history-search-forward'

Normally, Up and Down are bound to the Readline functions previous-history and
next-history respectively. I prefer to bind PgUp/PgDn to these functions,
instead of displacing the normal operation of Up/Down.

# ~/.inputrc
"\e[5~": history-search-backward
"\e[6~": history-search-forward

After you modify ~/.inputrc, restart your shell or use Ctrl+X, Ctrl+R to tell it
to re-read ~/.inputrc. By the way, if you're looking for relevant documentation:
Bash uses The GNU Readline Library for the shell prompt and history.


={============================================================================
*kt_linux_bash_100* sh-tip-colour-prompt

Uncomment the line that says force_color_prompt=yes then logout and back in in
the .bashrc.


={============================================================================
*kt_linux_bash_101* sh-tip-nested-level

5.2 Bash Variables

SHLVL
    Incremented by one each time a new instance of Bash is started. This is
    intended to be a count of how deeply your Bash shells are nested.

$ echo $SHLVL

<ex>
keitee@debian-keitee:~/github/kb$ echo $PS1
\[\e]0;\u@\h: \w\a\]${debian_chroot:+($debian_chroot)}\u@\h:\w\$

PS1="\[\e]0;\u@\h: \w\a\]${debian_chroot:+($debian_chroot)}\u@\h:\w\:$SHLVL$"

keitee@debian-keitee:~/github/kb\:2$


={============================================================================
*kt_linux_bash_202* sh-tip-recursion-test

The same script file is used to define tests and routines to check result using
exec trick and communication between parent process that runs the the script and
the subshell that runs each test case.

// test.sh

test_xxx () {
  set local vars

  run.sh args...        
  # run.sh to exec the subprocess with the given cmd so the subprocess will run
  # the cmd.
}

test_xxx_handler() {

}

# when test_handler is zero
if [ -z "$test_handler" ]; then
  for each test case

  # create a subshell(subprocess)
  # run a test case that runs the other script and exec call eventually.
  (
    set env vars to pass to subprocess.

    " note: use env var to pass data to a subprocess
    " note: use a single line to run

    ENVS \
    test_handler="${test_xxx}_handler" \
    test_xxx
   )
   done

else
  xxx_handler     " that checks the results
fi

So the parent process runs the test.sh and drives the whole test. For each test,
   set env vars and create a subprocess that exec and run the given cmd. By
   having cmd as the same test.sh and the test_handler env var set, subprocess
   runs the function in the same script and env var set. So run the different
   path and run check call in this case. So looks like a recursion or callback.

<code>
test_that_generates_url() {

    local app_dir="$scratch_dir/app"
    createApp "$app_dir"

    runBrowser.sh --app "$app_dir" --data "$default_app_data_dir" \
        --app-launch-parameters \
        "launch_context.ui.youview.com" "portal" \
        "some.test.param" "some.test.param.value" \
        "test.param.spaces" "param value with spaces"
}


that_generates_url_handler() {

    echo "that_generates_url_handler() - Got args: '$@'"

    local expected_url="http://youview.tv/test-player?\
    launch_context.ui.youview.com=portal&some.test.param=some.test.param.value&\
    test.param.spaces=param%20value%20with%20spaces"

    paramsContainStringAtPosition "-cache" 0 "$@"
    cacheOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "/tmp/client-cache" 1 "$@"
    cacheValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-cache-size" 2 "$@"
    cacheSizeOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "0" 3 "$@"
    cacheSizeValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-jar" 4 "$@"
    jarOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "$default_cookie_jar_location" 5 "$@"
    jarValueCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "-url" 6 "$@"
    urlOptionCorrectAtExpectedPosition=$?

    paramsContainStringAtPosition "$expected_url" 7 "$@"
    urlValueCorrectAtExpectedPosition=$?

    [[ 
      "$cacheOptionCorrectAtExpectedPosition" = 1 &&
      "$cacheValueCorrectAtExpectedPosition" = 1 &&
      "$cacheSizeOptionCorrectAtExpectedPosition" = 1 &&
      "$cacheSizeValueCorrectAtExpectedPosition" = 1 &&
      "$jarOptionCorrectAtExpectedPosition" = "1" && 
      "$jarValueCorrectAtExpectedPosition" = "1" && 
      "$urlOptionCorrectAtExpectedPosition" = "1" && 
      "$urlValueCorrectAtExpectedPosition" = "1" 
    ]] 
}


# In test configuration runBrowser.sh will not actually launch the browser but
# will instead call back into this test script recursively.  We then check the
# environment in the test to see if the browser would have been launched with
# the appropriate environment and parameters.  This is done with pairs of
# functions, one called test_xxx and the other xxx_handler.  test_x functions
# are tests and the xxx_handler functions are what will be run when we
# runBrowser.sh calls us back.
#
# The recursion and the callback are communicated through the environment
# variable $test_handler.
if [ -z "$test_handler" ]; then

        failures=0
        successes=0

        ## foreach test
        for test_fun in $(compgen -A function test_); do

            echo -e -n "\n$test_fun ... "

            setup
            (

                RUN_BROWSER_CMD="$this_script" \
                RUN_BROWSER_DATA_DIR="${datadir:-}" \
                GST_INSPECT_CMD="touch $scratch_dir/gst-inspect" \
                test_handler="${test_fun#test_*}_handler" \
                $test_fun

            ) >> $logfile 2>&1

            status=$?
            if [ $status -eq 0 ]; then
                echo "OK"
                ((++successes))
            else
                echo "FAILURE"
                ((++failures))
            fi

            [[ "$1" == '-v' || $status -ne 0 ]] && { cat $logfile; echo; }
            teardown &> /dev/null

        done

        printf "$invoked_as complete. %i PASSES %i FAILURES\n" \
            $successes $failures

        exit $failures

else

        $test_handler "$@"

fi


={============================================================================
*kt_linux_bash_202* sh-tip-get-filenames-from-comment

#!/bin/bash -x

# Exit immediately if any unexpected error occurs
set -e

#/ DASH: unencrypted, pseudo-live
#/ http://dash.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd
#/
#/ DASH: encrypted, non-live
#/ https://ms3.co.uk/s/Big+Buck+Bunny+DASH+2#http://bbb/avc3/1/2drm_manifest.mpd
#/
#/ HLS: unencrypted, non-live
#/ http://184.72.239.149/vod/smil:bigbuckbunnyiphone.smil/playlist.m3u8
#/
declare -ar urls=($(grep '^#/ http' "$0" | cut -c 4-))

echo "urls 0 : ${urls[0]}"


={============================================================================
*kt_linux_bash_202* sh-tip-search-number-range

Suppose to want to flag up the line from the log only when time difference in
the line exceeds certain limit.

"time diff = 136, " "warn value 300

A=`cat $FILE | strings | grep -n "\-MS_WATCHDOG-.*E_MS_PORT_THREAD_TYPE_CDI_WT.*time diff = " 2>/dev/null |
grep -v 'at time 0' 2>/dev/null |
grep -vE "time diff = [0-9]," 2>/dev/null | 
grep -vE "time diff = [0-9][0-9]," 2>/dev/null | 
grep -vE "time diff = [0-9][0-9][0-9]," 2>/dev/null | 
grep -vE "time diff = [0-9][0-9][0-9][0-9]," 2>/dev/null |
grep -vE "time diff = [0-9][0-9][0-9][0-9][0-9]," 2>/dev/null | head -1`

// when not found
if [ "$A" = "" ]

So filters out time difference in beteen 1 and 5 digits. If there is no line
  which is bigger than 5 digits then no result.

Q: this works: egrep -v "time diff = [0-9]{1,6},"


={============================================================================
*kt_linux_bash_202* sh-code-case

#!/bin/bash

mycat()
(
    F=cat
    echo "$1" | grep -q '.gz$' && F=zcat
    $F $1
)

if [ "$1" = "-v" ]
then
    VERBOSE=1
    FNAME=$2
else
    VERBOSE=0
    FNAME=$1
fi

SIZE=`mycat $FNAME | wc -l`
if [ $SIZE -lt 100000 ]
then
    echo 0 0
    exit 0
fi


TWO_THIRDS=`expr $SIZE \* 2 / 3`
ONE_TENTH=`expr $SIZE / 10`
ONE_HUNDREDTH=`expr $SIZE / 100`

// 3% and 0.3%
THREE_PERCENT=`expr $SIZE / 33`
POINTTHREE_PERCENT=`expr $SIZE / 333`

// take 1/10 from 2/3 of file to find out the line seem most from that
// section.
//
// NDS: ^0946711584.182091 !WARN   -PCATC                   < p:0x0000020e P:APP t:0x2dfa4520 T:no name M:value_list_man_clnt.c F:PCAT_VIEW_VL_LogAllocVL L:2816 > \
//  VL hdl=0x11ac1 allocated but not freed for view PLANNER29 hdl=0x7000020, " "flags=0, ObType=0, num_rows=1, total_size=1242, curpos=0
//
// $ cat LOGlastrun_realtime | head -1130954 | tail -169643 | grep "^NDS" | cut -d'!' -f2- | grep -vE "^ENTRY|^EXIT|^INFO" | head -2
// WARN   -PCATC                   < p:0x0000020e P:APP t:0x010a5520 T:PLANNER_MAIN M:value_list_man_clnt.c F:PCAT_VIEW_VL_LogAllocVL L:2816 > \
//  VL hdl=0x17794 allocated but not freed for view PLANNER29 hdl=0x7000020, " "flags=0, ObType=0, num_rows=1, total_size=1193, curpos=0
//
// remove matches to sort only for lines from mw and excludes entry/exit/info.
//
// $ cat LOGlastrun_realtime | head -1130954 | tail -169643 | grep "^NDS" | cut -d'!' -f2- | grep -vE "^ENTRY|^EXIT|^INFO" | sed -e's/[0-9a-fA-F]//g' | head -2
// WRN   -PT                       < p:x P:PP t:x T:PLNNR_MIN M:vlu_list_mn_lnt. :PT_VIW_VL_LoglloVL L: > \
//  VL hl=x llot ut not r or viw PLNNR hl=x, " "lgs=, OTyp=, num_rows=, totl_siz=, urpos=
//
// gets one line which seen most in the section
//
// $ cat LOGlastrun_realtime | head -1130954 | tail -169643 | grep "^NDS" | cut -d'!' -f2- | grep -vE "^ENTRY|^EXIT|^INFO" | sed -e's/[0-9a-fA-F]//g' | sort | uniq -c | sort -n | tail -1
// + POSSIBLE='  43546 WRN   -PT                   < p:x P:PP t:x T:no nm M:vlu_list_mn_lnt. :PT_VIW_VL_LoglloVL L: > \
//  VL hl=x llot ut not r or viw PLNNR hl=x, " "lgs=, OTyp=, num_rows=, totl_siz=, urpos= '
//
// note: "sed -e's/[0-9a-fA-F]//g'"
// 
// The trick is to remove numbers and some chars to make log lines "netural"
// and to be sorted as the same.
// 
// !WARN   -PCATC < p:0x0000020e P:APP t:0x2dfa4520 T:no name M:value_list_man_clnt.c F:PCAT_VIEW_VL_LogAllocVL L:2816 > VL hdl=0x11ac1 allocated but not freed for view PLANNER29 hdl=0x7000020, " "flags=0, ObType=0, num_rows=1, total_size=1242, curpos=0
// !WARN   -PCATC < p:0x0000020e P:APP t:0x2dfa4520 T:no name M:value_list_man_clnt.c F:PCAT_VIEW_VL_LogAllocVL L:2816 > VL hdl=0x76d4 allocated but not freed for view PLANNER17 hdl=0x7000014, " "flags=0, ObType=0, num_rows=1, total_size=712, curpos=0
// !WARN   -PCATC < p:0x0000020e P:APP t:0x2dfa4520 T:no name M:value_list_man_clnt.c F:PCAT_VIEW_VL_LogAllocVL L:2816 > VL hdl=0x115a6 allocated but not freed for view PLANNER17 hdl=0x7000014, " "flags=0, ObType=0, num_rows=1, total_size=703, curpos=0
//
// WRN   -PT                   < p:x P:PP t:x T:no nm M:vlu_list_mn_lnt. :PT_VIW_VL_LoglloVL L: > VL hl=x llot ut not r or viw PLNNR hl=x, " "lgs=, OTyp=, num_rows=, totl_siz=, urpos= '
//
// note: "sort | uniq -c | sort -n | tail -1"
//
// The trick is to sort out lines by `occurrences` and take a last line which
// is the line seem `most`

POSSIBLE=`mycat $FNAME 2>/dev/null | head -$TWO_THIRDS | tail -$ONE_TENTH | grep "^NDS" | cut -d'!' -f2- | grep -vE "^ENTRY|^EXIT|^INFO" | sed -e's/[0-9a-fA-F]//g' | sort | uniq -c | sort -n | tail -1`
POSS_COUNT=`echo $POSSIBLE | cut -d' ' -f1`
TIME_AT_START=`mycat $FNAME 2>/dev/null | head -$TWO_THIRDS | grep "^NDS" | tail -$ONE_TENTH | head -1 | cut -d'.' -f1 | cut -d'^' -f2`
TIME_AT_END=`mycat $FNAME 2>/dev/null | head -$TWO_THIRDS | grep "^NDS" | tail -1 | cut -d'.' -f1 | cut -d'^' -f2`
TOTAL_TIME=`expr $TIME_AT_END - $TIME_AT_START`
SECONDS_BETWEEN_MESSAGES=`expr $TOTAL_TIME / $POSS_COUNT`

if [ $SECONDS_BETWEEN_MESSAGES -gt 3 ]
then
    POSS_COUNT=0
fi

if [ $POSS_COUNT -gt $POINTTHREE_PERCENT ]
then
    POSS_HIPPO=`echo "$POSSIBLE" | sed -e's/^[ ]*//' | cut -d' ' -f2- | sed -e's/\[/\\\[/g' | sed -e's/\]/\\\]/g'`

    // `the line to match`
    // + POSSIBLE='  43546 WRN   -PT < p:x P:PP t:x T:no nm M:vlu_list_mn_lnt. :PT_VIW_VL_LoglloVL L: > VL hl=x llot ut not r or viw PLNNR hl=x, " "lgs=, OTyp=, num_rows=, totl_siz=, urpos= '
    // + POSS_HIPPO=      'WRN   -PT < p:x P:PP t:x T:no nm M:vlu_list_mn_lnt. :PT_VIW_VL_LoglloVL L: > VL hl=x llot ut not r or viw PLNNR hl=x, " "lgs=, OTyp=, num_rows=, totl_siz=, urpos= '

    // `how often seen?` Q: possible that the most seen line is different from
    // `the line to match`?
    HIPPO_COUNT=`mycat $FNAME 2>/dev/null | grep "^NDS" | cut -d'!' -f2- | sed -e's/[0-9a-fA-F]//g' | sort | uniq -c | sort -n | tail -1 | while read a b;do echo $a;done`
    if [ $HIPPO_COUNT -lt $THREE_PERCENT ]
    then
        echo 0 0
        exit 0
    fi
    if [ $VERBOSE -eq 1 ]
    then
        // gets `the first line` from a whole file which matches to `the line to match`
        //
        // =`mycat $FNAME 2>/dev/null | cut -d'!' -f2- | sed -e's/[0-9a-fA-F]//g' | grep -n "$POSS_HIPPO"`
        // shows all lines which matches to POS_HIPPO from a whole file.
        POS_OF_MESS=`mycat $FNAME 2>/dev/null | cut -d'!' -f2- | sed -e's/[0-9a-fA-F]//g' | grep -n "$POSS_HIPPO" | cut -d':' -f1 | head -1`
        MESSAGE=`mycat $FNAME 2>/dev/null | head -$POS_OF_MESS | tail -1`

        echo "$POS_OF_MESS Message appearing too often ($HIPPO_COUNT times/$SIZE lines): ($MESSAGE)"

        // + echo '705549 Message appearing too often (207618 times/1696431 lines): 
        // (NDS: ^0946711584.182091 !WARN   -PCATC                        < p:0x0000020e P:APP t:0x2dfa4520 T:no name M:value_list_man_clnt.c F:P
        // CAT_VIEW_VL_LogAllocVL L:2816 > VL hdl=0x11ac1 allocated but not freed for view PLANNER29 hdl=0x7000020, " "flags=0, ObType=0, num_rows=1, total_size=1242, curpos=0 )'

    else
        echo $TWO_THIRDS "Too many instances of" $POSSIBLE
    fi
    exit 1
fi
echo 0 0
exit 0


={============================================================================
*kt_linux_bash_202* sh-code-case-use-config

gets a setting from a config file

// .cfg
MIRRORMACHINE=theyard
MIRRORDIR=/home/si_logs/mirrored/upload
LOCALUSER=si_logs
MIRRORUSER=si_logs_mirror
TXDIR=/home/si_logs/tx
TRANSLATIONDIR=/home/si_logs/translation
SORTFILE=/home/si_logs/mac_list
RAWUPLOADLIST=/home/si_logs/raw_upload_list


// xx.sh

CFG=$D/.cfg

get_val()
{
    if [ -f $CFG ]
    then
        grep "^$1=" $CFG | head -1 | cut -d'=' -f2-
    fi
}

# get defaults
LOGS_DIR=``get_val TRANSLATIONDIR``


={============================================================================
*kt_linux_bash_202* sh-code-case-use-function-command

// .bashrc

t()
{
    cd $HOME/translation
    L=`echo *$1 | grep -v '*' | wc -w`
    echo "Match count $L"
    if [ $L -eq 0 ]
    then
        echo "No such directory found: $1"
    else
        if [ $L -gt 1 ]
        then
            echo "Multiple directories found - specify further:"
            ls -ld *$1
        else
            echo "Changing to \"$1\""
            cd *$1
        fi
    fi
}


// command line

when no args are given, "echo *$1" yields all files in the current directory
and runs "ls -ld <list>" Q: *$1?
$ t

$ t 123213


# ============================================================================
#{
={============================================================================
*kt_linux_tool_001* md5sum

md5sum - compute and check MD5 message digest

Another way let say you have more files to verify, you can create a text file, such as md5sum.txt

283158c7da8c0ada74502794fa8745eb  ubuntu-6.10-alternate-amd64.iso
549ef19097b10ac9237c08f6dc6084c6  ubuntu-6.10-alternate-i386.iso
5717dd795bfd74edc2e9e81d37394349  ubuntu-6.10-alternate-powerpc.iso
99c3a849f6e9a0d143f057433c7f4d84  ubuntu-6.10-desktop-amd64.iso
b950a4d7cf3151e5f213843e2ad77fe3  ubuntu-6.10-desktop-i386.iso
a3494ff33a3e5db83669df5268850a01  ubuntu-6.10-desktop-powerpc.iso
2f44a48a9f5b4f1dff36b63fc2115f40  ubuntu-6.10-server-amd64.iso
cd6c09ff8f9c72a19d0c3dced4b31b3a  ubuntu-6.10-server-i386.iso
6f165f915c356264ecf56232c2abb7b5  ubuntu-6.10-server-powerpc.iso
4971edddbfc667e0effbc0f6b4f7e7e0  ubuntu-6.10-server-sparc.iso

First column is the md5 string and second column is the location of the file. To check all them from
file, do this:

md5sum -c md5sum.txt


={============================================================================
*kt_linux_tool_002* tool-cut

       -b, --bytes=LIST
              select only these bytes

       -c, --characters=LIST
              select only these characters

       -d, --delimiter=DELIM
              use DELIM instead of TAB for field delimiter
              note: `instead of TAB` so TAB by default

       -f, --fields=LIST
              select only these fields;  also print any line that contains no
              delimiter character, unless the -s option is specified

       Use one, and only one of -b, -c or -f.  Each LIST is made up of one
       `range`, or many ranges separated by commas.  Selected input is written in
       the same order that it is read, and is written exactly once.  Each range
       is one of:

       N      N'th byte, character or field, counted from 1

       N-     from N'th byte, character or field, to end of line

       N-M    from N'th to M'th (included) byte, character or field

       -M     from first to M'th (included) byte, character or field

# input file
WIFI_SSID="SKY0F227"

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d=`
# this lead to WIFI_SSID='"SKY0F227"' and cannot use it as a var

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d \"`
# this fixes the problem.

<ex>
~/source/DEVARCH$ git symbolic-ref HEAD
refs/heads/topic
123456789012
~/source/DEVARCH$ git symbolic-ref HEAD | cut -b 12-
topic

<ex>
$ gl gst_clock_id_wait_async | cut -d' ' -f 3-4 | xargs echo

<ex>
Select first 3 chars

FIRST3=``echo $2 | cut -c-3``


={============================================================================
*kt_linux_tool_003* tool-kill, tool-killall

{kill-0}

while kill -0 "${parent_pid}" &>/dev/null;
do
   usleep 500
done

From man 2 kill

If sig is 0, then no signal is sent, but error checking is still performed; this
can be used to check for the existence of a process ID or process group ID.

<ex>
#!/bin/bash
kill -0 323232 && echo "pid 323232 is present.."
kill -0 4355 && echo "pid 4355 x sess mgr is present.."

$ ./sbsh.sh 
./sbsh.sh: line 3: kill: (323232) - No such process
pid 4355 x sess mgr is present..

<ex>
$ killall -0 `name`             // when the process is running
$
$ killall -0 `name`             // when no process is running
killall: `name`: no process killed


{kill}
       -l, --list [signal]
              List  signal  names.   This  option has optional argument, which
              will convert signal number to signal name, or other way round.


// for embedded

# kill -l
HUP INT QUIT ILL TRAP ABRT EMT FPE KILL BUS SEGV SYS PIPE ALRM TERM USR1 USR2
CHLD PWR WINCH URG IO STOP TSTP CONT TTIN TTOU VTALRM PROF XCPU XFSZ

// for pc

$ kill -l
 1) SIGHUP	 2) SIGINT	 3) SIGQUIT	 4) SIGILL	 5) SIGTRAP
 6) SIGABRT	 7) SIGBUS	 8) SIGFPE	 9) SIGKILL	10) SIGUSR1
11) SIGSEGV	12) SIGUSR2	13) SIGPIPE	14) SIGALRM	15) SIGTERM
16) SIGSTKFLT	17) SIGCHLD	18) SIGCONT	19) SIGSTOP	20) SIGTSTP
21) SIGTTIN	22) SIGTTOU	23) SIGURG	24) SIGXCPU	25) SIGXFSZ
26) SIGVTALRM	27) SIGPROF	28) SIGWINCH	29) SIGIO	30) SIGPWR
31) SIGSYS	34) SIGRTMIN	35) SIGRTMIN+1	36) SIGRTMIN+2	37) SIGRTMIN+3
...
63) SIGRTMAX-1	64) SIGRTMAX	


{default-signal}
The signal name difference cause compatibility between machines. For example,
    the below do not work on embedded.

kill -SIGINT dbussenddaemon &>/dev/null

The default signal for kill is TERM. So uses default signal to kill remaining
processes

-INT only interrupted a process rather request terminating it.

-    kill -INT %?dbussenddaemon &>/dev/null
+    kill %?dbussenddaemon &>/dev/null

       -s <signal>
       --signal <signal>
              Specify the signal to be sent.  The signal can be specified by
              using name or number.  The behavior of signals is explained in
              signal(7) manual page.


{killall}
killall - kill processes by name

killall sends a signal to all processes running any of the specified commands.
If no signal name is specified, SIGTERM is sent.


<ex> from core(5) man example

           $ sleep 100
           ^\                     # type control-backslash
           Quit (core dumped)

           argc[4]=<sig=3>

^\ causes SIGQUIT and quit but not cause a core.


={============================================================================
*kt_linux_tool_004* dmesg

To show the boot log and can see the kernel version.


={============================================================================
*kt_linux_tool_005* tool-uname

       -n, --nodename
              print the network node hostname

       -a, --all
              print all information, in the following order, except omit -p
              and -i if unknown:

$ uname -a
Linux debian-keitee 3.2.0-4-486 #1 Debian 3.2.63-2+deb7u2 i686 GNU/Linux

// shows the summary
lscpu

// shows all processors
cat /proc/cpuinfo


{distribution}
The machine hardware name lists whether your system is 32-bit (“i686” or
“i386”) or 64-bit (“x86_64”).

cat /proc/version

lsb_release -a

// ubuntu
$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=12.04
DISTRIB_CODENAME=precise
DISTRIB_DESCRIPTION="Ubuntu 12.04.1 LTS"

// debian
$ cat /etc/os-release 
PRETTY_NAME="Debian GNU/Linux 7 (wheezy)"
NAME="Debian GNU/Linux"
VERSION_ID="7"
VERSION="7 (wheezy)"
ID=debian
ANSI_COLOR="1;31"
HOME_URL="http://www.debian.org/"
SUPPORT_URL="http://www.debian.org/support/"
BUG_REPORT_URL="http://bugs.debian.org/"


={============================================================================
*kt_linux_tool_006* tool-cp

       -p     same as --preserve=mode,ownership,timestamps

{copy-symbolic}
To copy symbolic links as well, cp -r don't work and use

       -L, --dereference
              always follow symbolic links in SOURCE

{careful}
cp -r /xx/dir/ /dst/ 

This cause that files copied under /dst/dir/. If want to copy only files under
dir then use

cp -r /xx/dir/* /dst/


={============================================================================
*kt_linux_tool_007* mkdir

{p-option}
Use to make parent directories as well.


={============================================================================
*kt_linux_tool_008* strings

To find string in the library.

keitee@linux:~/share/temp> strings sec_getba.a | grep GCC
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6
GCC: (GNU) 3.4.6


={============================================================================
*kt_linux_tool_009* tool-head

       Print  the  first 10 lines of each FILE to standard output.  With more
       than one FILE, precede each with a header giving the file name.  With
       no FILE, or when FILE is -, read stan- dard input.

       -n, --lines=[-]K
              print the first K lines instead of the first 10; with the
              leading '-', print all but the last K lines of each file

       -c, --bytes=[-]K
              print the first K bytes of each file; with the leading ‘-’,
              print all but the last K bytes of each file

<get-matched-line>
When POS_OF_MESS is a line no to see.

MESSAGE=`mycat $FNAME 2>/dev/null | head -$POS_OF_MESS | tail -1`


<to-check-if-first-n-bytes-are-null>
PCAT.DB has data but PCAT.DBJ has null bytes in first few bytes as can see:

$ hexdump -C -n 10 PCAT.DB
00000000  53 51 4c 69 74 65 20 66  6f 72                    |SQLite for|
0000000a
$ hexdump -C -n 10 PCAT.DBJ.org
00000000  00 00 00 00 00 00 00 00  00 00                    |..........|
0000000a

$ head -c 10 PCAT.DB
SQLite fork$ head -c 10 PCAT.DBJ
$

Both `head command` returns($?) 0 so cannot use this to say if it has first
few null bytes. Then how?

$ head -c 10 PCAT.DB | read -n 1      # expect $? is 0 since read is okay
$ head -c 10 PCAT.DBJ | read -n 1     # expect $? is 1 since read is fail (no input)

However, both `read command` retuns 0. WTF? The reason is that head outputs
"10" long even if these are null. 


The fix:

# when there are first null bytes, tr deletes null bytes, so no input to read
# command. Hence read fails and return 1

$ head -c 10 PCAT.DBJ | tr -d '\0' | read -n 1
$ echo $?
1

# when there are no first null bytes and read returns okay.

$ head -c 10 PCAT.DB | tr -d '\0' | read -n 1
$ echo $?
0


={============================================================================
*kt_linux_tool_009* tool-sort

See the unix power tool 22.6 for more. -u remove duplicates and sort against
field [4,7].

sort -u -k 4,7

http://www.gnu.org/software/coreutils/manual/html_node/sort-invocation.html

--key=pos1[,pos2]

Specify a sort field that consists of the part of the line between pos1 and pos2
(or the end of the line, if pos2 is omitted), inclusive.  Each `pos` has the form
'f[.c][opts]', where f is the number of the field to use, and c is the number of
the first character from the beginning of the field. 

Fields and character positions are numbered `starting with 1`; a character
position of zero in pos2 indicates the field's last character. 

If '.c' is omitted from pos1, it defaults to 1 (the beginning of the field);
if omitted from pos2, it defaults to 0 (the end of the field). opts are
ordering options, allowing individual keys to be sorted according to different
rules; see below for details. Keys can span multiple fields.  Example: To sort
on the second field, use --key=2,2 (-k 2,2).

See below for more notes on keys and more examples. See also the --debug option
to help determine the part of the line being used in the sort.

To sort
NDS: ^0946684946.752246 !ERROR -aem          < M:aem_list.c F:AEM_ListGetApplication L:01808 > Can't find
-1-- ----------2------- ---3-- -4--         -5 ------6----- ---------------7-------- ---8--- 9 -10--

sort -u -k 4,10 ndsfusion.test > ndsfusion.dic 


-t separator --field-separator=separator

Use character separator as the field separator when finding the sort keys in
each line. By default, fields are separated by the `empty string` between a
non-blank character and a blank character. By default a blank is a space or a
tab, but the LC_CTYPE locale can change this.

That is, given the input line ' foo bar', sort breaks it into fields ' foo' and
' bar'. The field separator is not considered to be part of either the field
preceding or the field following, so with 'sort -t " "' the same input line has
three fields: an empty field, 'foo', and 'bar'. However, fields that extend to
the end of the line, as -k 2, or fields consisting of a range, as -k 2,3, retain
the field separators present between the endpoints of the range.  To specify
ASCII NUL as the field separator, use the two-character string '\0', e.g., 'sort
-t '\0''. 


mh5a_variable.c
mh5b_variable.c

sort -t _ -k 2,2

Use this to list out member functions from sources

egrep -r 'g_pAppWindow->' . | sort -t - -k 2,3 >> in.txt

<ex>
$ df -h | sort -rnk 5


<options>
       -r, --reverse
              reverse the result of comparisons

       -n, --numeric-sort
              compare according to string numerical value

       -t, --field-separator=SEP
              use SEP instead of non-blank to blank transition


={============================================================================
*kt_linux_tool_009* tool-uniq

NAME
       uniq - report or omit repeated lines

SYNOPSIS
       uniq [OPTION]... [INPUT [OUTPUT]]

DESCRIPTION
       Filter `adjacent` matching lines from INPUT (or standard input), writing to
       OUTPUT (or standard output).

       With no options, matching lines are merged to the first occurrence.

       Mandatory arguments to long options are mandatory for short options too.

       -c, --count
              prefix lines by `the number of occurrences`


={============================================================================
*kt_linux_tool_010* tool-grep

       grep [OPTIONS] PATTERN [FILE...]
       grep [OPTIONS] [-e PATTERN | -f FILE] [FILE...]


<busybox>
grep

    grep [-HhrilLnqvsoweFEABCz] PATTERN [FILE]...

    Search for PATTERN in each FILE or standard input

    Options:

            -H      Prefix output lines with filename where match was found
            -h      Suppress the prefixing filename on output
            -r      Recurse
            -i      Ignore case distinctions
            -l      List names of files that match
            -L      List names of files that do not match
            -n      Print line number with output lines
            -q      Quiet. Return 0 if PATTERN is `found`, 1 otherwise
            -v      Select non-matching lines
            -s      Suppress file open/read error messages
            -c      Only print count of matching lines
            -o      Show only the part of a line that matches PATTERN
            -m MAX  Match up to MAX times per file
            -w      Match whole words only
            -F      PATTERN is a set of newline-separated strings
            -E      PATTERN is an `extended regular expression`
            -e PTRN Pattern to match
            -f FILE Read pattern from file
            -A      Print NUM lines of trailing context
            -B      Print NUM lines of leading context
            -C      Print NUM lines of output context
            -z      Input is NUL terminated

<ex>
$ egrep -lr --include *.c mhvSessionCancel .


={============================================================================
*kt_linux_tool_011* tool-find

{find-mtime}
24(one day) hours based so -mtime 3 means that 72 hours(three days) from the
current time and -mtime -3 means 72 before. 

        72              96              120
    ----*---------------*---------------*--------------
        (               ](              ]

To specify between 72 and 96, -mtime 3 and after 96, -mtime +3. If want to find
file or dirs changed most recently, use -mtime 0 or 1.

$ find . -mime -3 -name


<print0>
-print0

print the full file name on the standard output, followed by a null character
(instead of the newline character that -print uses). This allows file names that
contain newlines or other types of white space to be correctly interpreted by
programs that process the find output. This option corresponds to the -0 option
of xargs. note: see xargs

$ find . -name CMakeLists.txt -print0
./CMakeLists.txt./Source/CMakeLists.txt./Source/cmake/gtest/CMakeLists.txt./Sou\
rce/WebKit2/CMakeLists.txt./Source/WebKit2/UIProcess/efl/po_tizen/CMakeLists.tx\
t./Source/WebKit/CMakeLists.txt./Source/WebKit/efl/DefaultTheme/CMakeLists.txt.\
/Source/JavaScriptCore/CMakeLists.txt./Source/JavaScriptCore/shell/CMakeLists.t\
xt./Source/WebCore/CMakeLists.txt./Source/ThirdParty/gtest/CMakeLists.txt./Sour\
ce/CMakeLists.txtkeitee.park@rockford /home/tbernard/Git/vdTizen/webkit


$ find folder1 folder2 -name "*.txt" -print0 | xargs -0 myCommand


<print-fomrmat>
       -printf format
              True; print format on the standard output, interpreting '\'
              escapes and '%' directives.  Field widths and precisions can be
              specified as with the 'printf' C function.  Please note  that
              many  of  the fields  are  printed  as  %s  rather  than  %d, and
              this may mean that flags don't work as you might expect.  This
              also means that the '-' flag does work (it forces fields to be
                      left-aligned).  Unlike -print, -printf does not add a
              newline at the end of the string.  The escapes and directives are:


       %Ak    File's last access time in the format specified by k, which
              is either '@' or a directive for the C 'strftime' function.  The
              possible values for k are listed below; some of them  might  not
              be available on all systems, due to differences in 'strftime'
              between systems.


            @ seconds since Jan. 1, 1970, 00:00 GMT, with fractional part.

            Time fields:

            c locale's date and time (Sat Nov 04 12:02:33 EST 1989). The format
              is the same as for ctime(3) and so to preserve compatibility with
              that format, there is no fractional part in the seconds field.


        %p     File's name.

        %Tk    File's last modification time in the format specified by k, which
               is the same as for %A.


<case>
Want to see test failures made 20 days before. Use find since there will be a
file if there were failures. However, find prints out ascending order, from old
to newer. Want to see from the latest failure. Two problems: find output don't
have times and don't have options to change output order.

$ find . -mtime -20 -name Nickel.System.GStreamer.log -printf '%T@ %Tc %p\n' | sort -r


{find-dir}
find . -type d -name [dir]


{find-to-get-the-number-of-files}
find -type f | wc -l

find . -maxdepth 1 -type l



{find-exec}
{} where the filename will be inserted. Add \; at the end of the command to
complete the required syntax. note: there must be a space after {}

$ find . -name CMakeLists.txt -exec egrep PROJECT {} \;

To run dirtags script for each directory:

$ find * -type d -exec dirtags {} \;


{find-oring}
You can specify a logical "or" condition using -o:

find / \( -size +50 -o -mtime -3 \) -print
find /my/project/dir -name '*.c' -o -name '*.h'
find -name *.[ch]

This is from bash  expr: expr1 -or expr2 and this means expr1 -o expr2, but not
POSIX compliant.


{find-ignore}
-path pattern

File name matches shell pattern pattern.  The metacharacters do not treat `/' or `.' specially; so,
for example,

find . -path "./sr*sc"

will print an entry for a directory called `./src/misc' (if one exists).  To ignore a  whole
directory tree,  use  -prune  rather  than  checking  every file in the tree.  For example, to skip
the directory `src/emacs' and all files and directories under it, and print the names of the other
files  found,  do something like this:

find . -path ./src/emacs -prune -o -print

Note  that the pattern match test applies to the whole file name, starting from one of the start
points named on the command line.  It would only make sense to use an absolute path name here if the
relevant start point is also an absolute path.  This means that this command will never match
anything:

find bar -path /foo/bar/myfile -print

The  predicate  -path is also supported by HP-UX find and will be in a forthcoming version of the
POSIX standard.


{find-sym}
-L     
Follow symbolic links.


={============================================================================
*kt_linux_tool_012* make a empty file without touch

can use 'touch' but when busybox do not support touch, can use following to
make a empty file or to reset a file.

cat /dev/null > file


={============================================================================
*kt_linux_tool_013* tool-xargs

--null
-0     

Input items are terminated by a null character instead of by whitespace, and the
quotes and backslash are not special (every character is taken literally).
Disables the end of file string, which is treated like any other argument.
Useful when input items might contain white space, quote marks, or  backslashes.

The GNU find -print0 option produces input suitable for this mode. note: see
find-print0

-I replace-str

Replace occurrences of replace-str in the initial-arguments with names read from
standard input.  Also, unquoted  blanks  do not terminate input items; instead
the separator is the newline character. Implies -x and -L 1.

$ ls | grep Nov_ | xargs -I{} find {} -name LOGlastrun_realtime -print \
| xargs egrep -an \ 
"(NCM_ADDRESSING_TYPE_DHCP address still in status list)"


<xargs-vs-find>
The xargs command builds and executes command lines from standard input. This
has the advantage that the command line is filled until the system limit is
reached. Only then will the command to execute be called, in the above example
this would be rm. If there are more arguments, a new command line will be used,
until that one is full or until there are no more arguments. The same thing
    using find -exec calls on the command to execute on the found files every
    time a file is found. Thus, using xargs greatly 'speeds' up your scripts and
    the performance of your machine.


<without-xarg>
$ ls -al $(find . -name "*.log")
$ tar -cjf daemon-logs.tar.bz2 $(find /opt/zinc*/var/daemons/ -name "*.log")


<ex>
$ find . -type d -name \*stb-4 | xargs -I{} find {} -name \*nickel\* | xargs ls -alF


<busybox>

xargs

    xargs [OPTIONS] [PROG [ARGS]]

    Run PROG on every item given by standard input

    Options:

            -p      Ask user whether to run each command
            -r      Do not run command if input is empty  note:
            -0      Input is separated by NUL characters
            -t      Print the command on stderr before execution
            -e[STR] STR stops input processing
            -n N    Pass no more than N args to PROG
            -s N    Pass command line of no more than N bytes
            -x      Exit if size is exceeded


={============================================================================
*kt_linux_tool_014* tool-ssh

{aim}
To switch hosts using key base instead of using password.

<ssh-keygen>
$ ssh-keygen

$ ls -al .ssh/
total 20
drwx------  2 parkkt ccusers 4096 Dec  9 13:24 .
drwxr-xr-x 15 parkkt ccusers 4096 Dec  9 12:47 ..
-rw-------  1 parkkt ccusers 1675 Dec  9 13:24 id_rsa
-rw-r--r--  1 parkkt ccusers  411 Dec  9 13:24 id_rsa.pub

     -p      Requests changing the passphrase of a private key file instead of
     creating a new private key.  The program will prompt for the file
     containing the private key, for the old passphrase, and twice for the new
     passphrase.

     -o      Causes ssh-keygen to save SSH protocol 2 private keys using the
     new OpenSSH format rather than the more compatible PEM format.  The new
     format has increased resistance to brute-force pass‐ word cracking but is
     not supported by versions of OpenSSH prior to 6.5.  Ed25519 keys always
     use the new private key format.


<ssh-copy-id>
NAME
       ssh-copy-id - install your public key in a remote machine's
       authorized_keys

SYNOPSIS
       ssh-copy-id [-i [identity_file]] [user@]machine

DESCRIPTION
       ssh-copy-id is a script that uses ssh to log into a remote machine and
       append the indicated identity file to that machine's
       ~/.ssh/authorized_keys file.

       If the -i option is given then the identity file (defaults to
           ~/.ssh/id_rsa.pub) is used, regardless of whether there are any keys
       in your ssh-agent.  Otherwise, if this:

             ssh-add -L

       provides any output, it uses that in preference to the identity file.


local$ ssh-copy-id root@linuxconfig.org
root@linuxconfig.org's password:

// Now try logging into the machine, with "ssh 'root@linuxconfig.org'", and 
// check in: .ssh/authorized_keys
// to make sure we haven't added extra keys that you weren't expecting.

This make one-way ssh connection which means the machine you are on is added the
authorized_keys of the server so can run scp from the machine to the server:


<ssh-utilities>
scp remote-server:{path}/filename .
scp filename remote-server:{path}/filename

note: 
If there is working ssh connection, can use tab key to get file completion when
use scp.


{to-check-match-pair}
$ ssh-keygen /?
  -y          Read private key file and print public key.

$ ssh-keygen -y -f id_rsa


{caution}
note:
If the file permissions are too open then ssh will not trust them, and will
still prompt you for your password. 

chmod 700 ~/.ssh
chmod 644 ~/.ssh/authorized_keys  // check this as caused big grief when different
chmod 644 ~/.ssh/id_dsa_pub
chmod 644 ~/.ssh/known_hosts
chmod 600 ~/.ssh/id_dsa


{ssh-config}

man ssh_config

<different-user-name>
When user name is different between servers, must have an entry in this file for
servers to connect.

note:
The username that can be different from real user.

$ cat ~/.ssh/config
Host tizen
        Hostname 168.219.241.167
        IdentityFile ~/.ssh/id_rsa
        User keitee.park
        Port 29418

To debug ssh. note: shall use name on the command line

-v  # -vv
Verbose mode. Causes ssh to print debugging messages about its progress. This is
helpful in debugging connection, authentication, and configuration problems.
Multiple -v options increase the verbosity. The maximum is 3. 

$ ssh -vT tizen


{github-when-ssh-do-not-work}
Using SSH over the HTTPS port

Sometimes the administrator of a firewall will refuse to allow SSH connections
entirely. If using HTTPS cloning with credential caching is not an option, you
can attempt to clone using an SSH connection made over the HTTPS port. Most
firewall rules should allow this, but proxy servers may interfere. 

Testing

To test if SSH over the HTTPS port is possible, run this ssh command:

ssh -T -p 443 git@ssh.github.com
# Hi username! You've successfully authenticated, but GitHub does not provide
# shell access.

Make it so

If you are able to ssh to git@ssh.github.com over port 443, you can override
your ssh settings to force any connection to github.com to run though that
server and port. To set this in your ssh config, edit the file at ~/.ssh/config
and add this section:

Host github.com
  Hostname ssh.github.com
  Port 443

You can test that this works by connecting to github.com:

ssh -T git@github.com
# Hi username! You've successfully authenticated, but GitHub does not provide
# shell access.


{ssh-error}
https://help.github.com/articles/error-agent-admitted-failure-to-sign/
Error: Agent admitted failure to sign

In rare circumstances, connecting to GitHub via SSH on Linux produces the
error "Agent admitted failure to sign using the key". Follow these steps to
resolve the problem.

Resolution

You should be able to fix this error by loading your keys into your SSH agent
with ssh-add:

# start the ssh-agent in the background
eval "$(ssh-agent -s)"
Agent pid 59566
ssh-add
Enter passphrase for /home/you/.ssh/id_rsa: [tippy tap]
Identity added: /home/you/.ssh/id_rsa (/home/you/.ssh/id_rsa)


{run-command}
$ ssh root@172.20.33.192 ls -al


     -s      May be used to request invocation of a subsystem on the remote
     system.  Subsystems are a feature of the SSH2 protocol which facilitate
     the use of SSH as a secure transport for other applications (eg.
         sftp(1)).  The subsystem is specified as the remote command.


<options>
     Compression
             Specifies whether to use compression.  The argument must be “yes”
             or “no”.  The default is “no”.


     ForwardX11
             Specifies whether X11 connections will be automatically
             redirected over the secure channel and DISPLAY set.  The argument
             must be “yes” or “no”.  The default is “no”.

             X11 forwarding should be enabled with caution.  Users with the
             ability to bypass file permissions on the remote host (for the
                 user's X11 authorization database) can access the local X11
             display through the forwarded connection.  An attacker may then
             be able to perform activities such as keystroke monitoring if the
             ForwardX11Trusted option is also enabled.

             For this reason, X11 forwarding is subjected to X11 SECURITY
             extension restrictions by default.  Please refer to the ssh -Y
             option and the ForwardX11Trusted directive in ssh_config(5) for
             more information.

             note:
              $ssh -X hostname

              -X      Enables X11 forwarding.
                      This can also be specified on a `per-host` basis in a
                      configuration file.

     note:
     How to prevent "Write Failed: broken pipe" on SSH connection?

     I have tried this in /etc/ssh/ssh_config for Linux and ~/.ssh/config for
     Mac:

     Host *
     ServerAliveInterval 120

     This is how often, in seconds, it should send a keepalive message to the
     server. If that doesn't work then train a monkey to press enter every
     two minutes while you work.

     You could set either ServerAliveInterval in /etc/ssh/ssh_config of the
     client machine or ClientAliveInterval in /etc/ssh/sshd_config of the
     server machine.  Try reducing the interval if you are still getting the
     error.

     ServerAliveInterval
             Sets a timeout interval in seconds after which if no data has
             been received from the server, ssh(1) will send a message through
             the encrypted channel to request a response from the server.  The
             default is 0, indicating that these messages will not be sent to
             the server, or 300 if the BatchMode option is set.  
             
             This option applies to protocol version 2 only.
             ProtocolKeepAlives and SetupTimeOut are Debian-specific
             compatibility aliases for this option.

     note:
     How to check protocol version?

     ssh -vv xxx
     ...
     debug1: Enabling compatibility mode for protocol 2.0
     debug1: Local version string SSH-2.0-OpenSSH_6.7p1 Debian-5+deb8u2
     ...

     Host    Restricts the following declarations (up to the next Host or
             Match keyword) to be only for those hosts that match one of the
             `patterns` given after the keyword.  

             If more than one pattern is provided, they should be separated by
             whitespace.  

             A single ‘*’ as a pattern can be used to provide global defaults
             for all hosts.
             
             A pattern entry may be negated by prefixing it with an
             exclamation mark (‘!’).  If a negated entry is matched, then the
             Host entry is ignored, regardless of whether any other patterns
             on the line match.  Negated matches are therefore useful to
             provide exceptions for wildcard matches.

             See PATTERNS for more information on patterns.

<x-option>
This is by far the easiest method because ssh does everything for you! As long
as the remote machine is running an ssh daemon, all you have to do is type:

ssh -X <remote-username>@<remote-machine>

It will then ask you for your password on the remote machine. As long as your
DISPLAY variable was set correctly before you typed the command (see below),
it should all work without any problems. 


={============================================================================
*kt_linux_tool_014* tool-putty

{putty-ssh-setup}
When use keys generated from putty.

o run puttygen to make key pairs. rsa or dsa
o get a pub key and save a pri key(ppk)
o run putty and set ssh key to use
  menu: connection: ssh: auth: private key file for auth: specify the path to a pri key.
o login to the host and add a pub key in the auth key list


{convert-rsa-key-to-putty-ppk}
To convert keys from linux machine to putty ppk and from ppk to lunux(opsnssh
    keys)

o run puttygen and menu: conversion: import key:
o save it as a pri key(ppk)


={============================================================================
*kt_linux_tool_014* tool-putty-xwin

https://my.cqu.edu.au/web/eresearch/windows-x-server

Compared with VNC method, some of the benefits of using this method to display
GUI applications includes:

It natively uses your windows environment, so you can easily resize windows
and move windows to a separate screens (if using more than one computer
monitor). 

Interacting with the application may be faster and smoother, as there is no
delay in moving the mouse, when compared running the application within a VNC
session.


<on-putty-side>
* Click on the "Enable X11 Forwarding" check box.

* Advised to enable "compression", has it will decrease the amount for
  bandwidth required.  To do this, select the Putty Category Connection -> SSH
  and tick the "Enable compression" check box


{xserver}
I stumbled across " VcXsrv Windows X Server" - that seems to be the well
maintained and if you want you can compiled yourself (not that you need to).

http://sourceforge.net/projects/vcxsrv/?source=directory

// If you chose to use it, you need to run VcXsrc without OpenGL support but it
// works - see highlighted text below ...
// 
// "C:\Program Files\VcXsrv\vcxsrv.exe" :0 -ac -terminate -lesspointer -multiwindow -clipboard -nowgl

note: 
this server is better since it seems to have more fonts and shows gvim
properly than xming which is old xserver. It replace xming and seems to use
the rest since use xlanuch. 'One window' do not works but 'multiple windows'
works.

note:
Only multiple window option works

<run-config>
"C:\Program Files\VcXsrv\xlaunch.exe" -run C:\kit\xconfig


={============================================================================
*kt_linux_tool_015* tool-pgrep, tool-pidof

pidof -- find the process ID of a running program.

Pidof finds the process id's (pids) of the named  programs. It prints those id's
on the standard output. 

$ pidof getty
2974 2973 2972 2971 2970 2969


{pgrep}
       pgrep, pkill - look up or signal processes based on name and other
       attributes

SYNOPSIS
       pgrep [options] pattern
       pkill [options] pattern

DESCRIPTION
       pgrep looks through the currently running processes and lists the process
       IDs which match the selection criteria to stdout.  All the criteria have
       to match.  For example,

              $ pgrep -u root sshd

       will only list the processes called sshd AND owned by root.  On the other
       hand,

              $ pgrep -u root,daemon

       will list the processes owned by root OR daemon.

       pkill will send the specified signal (by default SIGTERM) to each process
       instead of listing them on stdout.

$ pgrep -h    
pgrep: invalid option -- h
BusyBox v1.21.0 (2015-06-02 11:02:10 BST) multi-call binary.

Usage: pgrep [-flnovx] [-s SID|-P PPID|PATTERN]

Display process(es) selected by regex PATTERN

	-l	Show command name too
	-f	Match against entire command line
	-n	Show the newest process only
	-o	Show the oldest process only
	-v	Negate the match
	-x	Match whole name (not substring)
	-s	Match session ID (0 for current)
	-P	Match parent process ID


$ pgrep linearsourced
1079

$ pgrep -l linearsourced
1079 /opt/zinc-trunk/bin/linearsourced


={============================================================================
*kt_linux_tool_018* tool-ls

{get-filename-only}
ls -1

<sort-by-time>

       -l     use a long listing format

       -r, --reverse
              reverse order while sorting

       -t     sort by modification time, newest first

       -c     with -lt: sort by, and show, ctime (time of last modification of
           file status information) 

              with -l: show ctime and sort by name otherwise: sort by ctime,
              newest first

       -S     sort by file size


={============================================================================
*kt_linux_tool_019* tool-strace

strace - trace `system-calls` and `signals` 

-f
Trace child processes as they are created by currently traced processes as a
result of the fork(2) system call. 


-o filename
Write the trace output to the file filename rather than to stderr. Use
filename.pid if -ff is used.  If the argument begins with '|' or with '!' then
the rest of the argument is treated as a command and all output is piped to it.
This is convenient for piping the debugging output to a program without
affecting the redirections of executed programs. 


-ff         
If the -o filename option is in effect, each processes trace is written to
filename.pid where pid is the numeric process id of each process.  This is
incompatible with -c, since no per-process counts are kept.

note: 
This is important to get every thread output into a separate file (-ff option).


-p pid      
Attach to the process with the process ID pid and begin tracing. The trace may
be terminated at any time by a keyboard interrupt signal (CTRL-C).  strace will
respond by detaching itself from the traced process(es) leaving it (them) to
continue running.  Multiple -p options can be used to attach to up to 32
processes in addition to command (which is optional if at least one -p option is
    given).

-s strsize  
Specify the maximum string size to print (the default is 32).  Note that filenames are not
considered strings and are always printed in full.

-E var=val  Run command with var=val in its list of environment variables.

-e expr     

A qualifying expression which modifies which events to trace or how to trace them.  The format of
the expression is:

[qualifier=][!]value1[,value2]...

where qualifier is one of trace, abbrev, verbose, raw, signal, read, or write and value is a
qualifier-dependent symbol or number. The 'default' qualifier is trace. Using an exclamation mark
negates the set of values.  

For example, -e open means literally -e trace=open which in turn means trace only the open system
call. By contrast, -e trace=!open means to trace every system call except open. In addition, the
special values all and none have the obvious meanings.

Note that some shells use the exclamation point for history expansion even inside quoted arguments.
If so, you must escape the exclamation point with a back-slash.


<ex>
When use to run a script:

$ strace -f -o trace sh ./compile.sh

When use to run a program:

$ strace ./appname

strace -ff -s 128 -v -o dbus-trace.log -p <dbus daemon PID>


<how-to-run-app-with-strace>
# this is the original line
LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib" LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
exec "${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"

# this is the same line with strace
exec strace -ff -o /opt/adobe/stagecraft/data/trace.log -s 128 \
-E LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib" -E LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
"${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"


<ex>
LD_PRELOAD=/libyouviewrcushim.so strace /bin/true


={============================================================================
*kt_linux_tool_020* tool-time-tool-date

{tool-time}

~/tizencore$ time -f "%E real,%U user,%S sys" ls -Fs
-f: command not found

real	0m0.143s
user	0m0.068s
sys	0m0.040s

kit@kit-vb:~/tizencore$ /usr/bin/time -f "%E real,%U user,%S sys" ls -Fs
total 24
4 app-core/  4 appfw/  4 application/  4 app-service/  4 dlog/	4 README
0:00.00 real,0.00 user,0.00 sys
kit@kit-vb:~/tizencore$ 

Users of the bash shell need to use an explicit path in order to run the
external time command and not the shell builtin variant. On system where time is
installed in /usr/bin, the first example would become /usr/bin/time wc
/etc/hosts


{tool-date}
NAME
       date - print or set the system date and time

SYNOPSIS
       date [OPTION]... [+FORMAT]

       %s     seconds since 1970-01-01 00:00:00 UTC

dbus-monitor &> /var/log/dbus-monitor/$(date +%s).log &

$ date --rfc-3339=s
2015-01-27 13:49:11+00:00

$ date
Tue Jan 27 13:49:19 UTC 2015


={============================================================================
*kt_linux_tool_021* tool-chmod

{to-set-uid}
That require root access to function properly even when invoked by a nonroot
user

%chmod +s /sample_file


={============================================================================
*kt_linux_tool_022* mknod

%mknod $name c $major $minor


={============================================================================
*kt_linux_tool_023* tool-wc

-l, --lines
     print the newline counts

{how-to-count-lines-recursively}

find . -name '*.php' | xargs wc -l

# should be re-written using sh scripting
#!/bin/bash
echo "debug..."
find debug -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "dsm"
find dsm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "include"
find include -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "main"
find main -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

#
echo "mah"
find mah -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5dec"
find mh5dec -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5eng"
find mh5eng -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5gpi"
find mh5gpi -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mhv"
find mhv -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "pfm"
find pfm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l


={============================================================================
*kt_linux_tool_024* tool-du, tool-df, tool-stat

' to print one level
du -h --max-depth=1

-h, --human-readable
print sizes in human readable format (e.g., 1K 234M 2G)

' to show all and total
du -ach
du -sh


<tool-stat>
BusyBox v1.19.4 (2015-11-13 23:53:27 KST) multi-call binary.

Usage: stat [OPTIONS] FILE...

Display file (default) or filesystem status

        -c fmt  Use the specified format
        -f      Display filesystem status
        -L      Follow links
        -t      Display info in terse form

Valid format sequences for file systems:
 %a     Free blocks available to non-superuser
 %b     Total data blocks in file system
 %c     Total file nodes in file system
 %d     Free file nodes in file system
 %f     Free blocks in file system
 %i     File System ID in hex
 %l     Maximum length of filenames
 %n     File name
 %s     Block size (for faster transfer)
 %S     Fundamental block size (for block counts)
 %t     Type in hex
 %T     Type in human readable form


[root@HUMAX /]# stat -f -c "%a %s" /mnt/hd1


={============================================================================
*kt_linux_tool_024* tool-quota

       quota - display disk usage and limits

SYNOPSIS
       quota [ -F format-name ] [ -guqvswi ] [ -l | [ -QAm ]]
       quota [ -F format-name ] [ -qvswi ] [ -l | [ -QAm ]] -u user...
       quota [ -F format-name ] [ -qvswi ] [ -l | [ -QAm ]] -g group...
       quota [ -F format-name ] [ -qvswugQm ] -f filesystem...

DESCRIPTION
       quota displays users' disk usage and limits.  By default only the user
       quotas are printed.

       quota  reports the quotas of all the filesystems listed in /etc/mtab.
       For filesystems that are NFS-mounted a call to the rpc.rquotad on the
       server machine is performed to get the information.

       -u, --user
              flag is equivalent to the default.

       -v, --verbose
              will display quotas on filesystems where no storage is
              allocated.

       -s, --human-readable
              option will make quota(1) try to choose units for showing
              limits, used space and used inodes.


The meaning of the the columns in the output is as follows:

Value       Description

Filesystem  
The mounted file system, usually in the form <server>:/<directory>. Use df
<directory> to determine the mounted file system for a given directory

blocks      Used disk space in kilobytes

quota
Soft limit for disk usage (you can still write new files or data when exceeded
but only until the grace time is up)

limit       
Hard limit for disk usage (you cannot write new files or data anymore when
exceeded)

grace       
The grace time indicates the time left to remove data when you exceed the soft
quota (when this time is up you cannot write new files or data anymore)

file        
Number of files

quota       
Soft limit for number of files (you can still write new files when exceeded
but only until the grace time is up)

limit       
Hard limit for number of files (you cannot write new files anymore when
exceeded)

grace       
The grace time indicates the time left to delete files when you exceed the
soft quota (when this time is up you cannot write new files anymore)


={============================================================================
*kt_linux_tool_025* tool-ln

       -n, --no-dereference
              treat destination that is a symlink to a directory as if it were
              a normal file

       -f, --force
              remove existing destination files

       -s, --symbolic
              make symbolic links instead of hard links

ln -sf input output. E.g., ln -sf linux-2.4.25-2.8 linux

<ex>
$ ln -s /usr/src/linux .
lrwxrwxrwx  1 keitee keitee    14 Feb 18 23:50 linux -> /usr/src/linux/

<ex>
To create mutiple link at one go. Assume that there are two files under /bin
e.g., gdb and gdbserver then it will create two links in /usr/local/bin/.

ln -sf /opt/zinc/oss/debugtools/bin/gdb* /usr/local/bin/

<ex>
Supports multiple matches:

ln -sf ${yv_htmlengine_prefix}/tests/\
{vanadium-w3c-engine,vanadium-webkit-video-element,zinc-jscore-binding-runtime} \
${yv_prefix}/tests/


={============================================================================
*kt_linux_tool_026* tool-rsync

NAME
       rsync - a fast, versatile, remote (and local) file-copying tool

SYNOPSIS
       Local:  rsync [OPTION...] SRC... [DEST]

       Access via remote shell:
         Pull: rsync [OPTION...] [USER@]HOST:SRC... [DEST]
         Push: rsync [OPTION...] SRC... [USER@]HOST:DEST

       Access via rsync daemon:
         Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST]
               rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST]
         Push: rsync [OPTION...] SRC... [USER@]HOST::DEST
               rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST

       Usages with just one SRC arg and no DEST arg will list the source files
       instead of copying.

Rsync, which stands for "remote sync", is a remote and local file
synchronization tool. It uses an algorithm that minimizes the amount of data
copied by only moving the portions of files that have changed.


DESCRIPTION
       Rsync  is  a  fast  and extraordinarily versatile file copying tool.  It
       can copy locally, to/from another host over any remote shell, or to/from
       a remote rsync daemon.  It offers a large number of options that control
       every aspect of its behavior and permit very flexible specification of
       the set of files to be copied.  It is famous for its 'delta'-transfer
       algorithm, which reduces the amount of data sent over the network by
       sending only the  differences between the source files and the existing
       files in the destination.  Rsync is widely used for backups and mirroring
       and as an improved copy command for everyday use.

       Rsync finds files that need to be transferred using a "quick check"
       algorithm (by default) that looks for files that have changed in 'size' or
       in last-modified 'time'.  Any changes in the other preserved attributes (as
           requested by options) are made on the destination file directly when
       the quick check indicates that the file's data does not need to be
       updated.

       Some of the additional features of rsync are:

       o      support for copying links, devices, owners, groups, and
              permissions

       o      exclude and exclude-from options similar to GNU tar

       o      a CVS exclude mode for ignoring the same files that CVS would
              ignore

       o      can use any transparent remote shell, including ssh or rsh

       o      does not require super-user privileges

       o      pipelining of file transfers to minimize latency costs

       o      support for anonymous or authenticated rsync daemons (ideal for
              mirroring)


        -r, --recursive             recurse into directories

GENERAL
       Rsync copies files either to or from a remote host, or locally on the
       current host (it does 'not' support copying files between two remote
           hosts).

       There are two different ways for rsync to contact a remote system: using
       a remote-shell program as the transport (such as ssh or rsh) or
       contacting an rsync daemon directly via TCP.  
       
       The remote-shell transport is used  whenever  the  source  or destination
       path contains a single colon (:) separator after a host specification.  
       
       Contacting an rsync daemon directly happens when the source or
       destination path contains a double colon (::) separator after a host
       specification, OR when an rsync:// URL is specified (see also the "USING
       RSYNC-DAEMON FEATURES VIA A REMOTE-SHELL CONNECTION" section for an
       exception to this latter rule).

       As a special case, if a single source arg is specified without a
       destination, the files are listed in an output format similar to "ls -l".

       As expected, if neither the source or destination path specify a remote
       host, the copy occurs locally (see also the --list-only option).

       Rsync refers to the local side as the "client" and the remote side as the
       "server".  Don't confuse "server" with an rsync daemon -- a daemon is
       always a server, but a server can be either a daemon or a  remote-shell
       spawned process.

USAGE
       You use rsync in the same way you use rcp. You must specify a source and
       a destination, one of which may be remote.

       Perhaps the best way to explain the syntax is with some examples:

       -t, --times                 preserve modification times

              rsync -t *.c foo:src/

       This would transfer all files matching the pattern *.c from the current
       directory to the directory src on the machine foo. If any of the files
       already exist on the remote system then the rsync remote-update protocol
       is used to update the file by sending only the differences. See the tech
       report for details.

        -v, --verbose               increase verbosity
        -a, --archive               archive mode; 'equals' -rlptgoD (no -H,-A,-X)
            --no-OPTION             turn off an implied OPTION (e.g. --no-D)
        -z, --compress              compress file data during the transfer

              rsync -avz foo:src/bar /data/tmp

       This  would  recursively transfer all files from the directory src/bar on
       the machine foo into the /data/tmp/bar directory on the local machine.
       The files are transferred in "archive" mode, which ensures that symbolic
       links, devices, attributes, permissions, ownerships, etc. are preserved
       in the transfer.  Additionally, compression will be used to reduce the
       size of data portions of the transfer.

              rsync -avz foo:src/bar/ /data/tmp

       A 'trailing' slash on the source changes this behavior to avoid creating an
       additional directory level at the destination.  You can think of a
       trailing / on a source as meaning "copy the contents of this directory"
       as  opposed to  "copy  the directory by name", but in both cases the
       attributes of the containing directory are transferred to the containing
       directory on the destination.  In other words, each of the following
       commands copies the files in the same way, including their setting of the
       attributes of /dest/foo:

              rsync -av /src/foo /dest
              rsync -av /src/foo/ /dest/foo

       Note also that host and module references don't require a trailing slash
       to copy the contents of the default directory.  For example, both of
       these copy the remote directory's contents into "/dest":

              rsync -av host: /dest
              rsync -av host::module /dest

       You can also use rsync in local-only mode, where both the source and
       destination don't have a ':' in the name. In this case it behaves like an
       improved copy command.

       Finally, you can 'list' all the (listable) 'modules' available from a
       particular rsync daemon by leaving off the module name:

              rsync somehost.mydomain.com::

              <ex>
              ~/source/DEVARCH$ rsync zinc@humax-04535::
              Root           	Humax box

       See the following section for more details.

CONNECTING TO AN RSYNC DAEMON
       It is also possible to use rsync without a remote shell as the transport.
       In this case you will directly connect to a remote rsync daemon, 
       typically using TCP port 873.  

       (This obviously requires the daemon to be running on the remote system,
        so refer to the STARTING AN RSYNC DAEMON TO ACCEPT CONNECTIONS section
        below for information on that.)

       Using rsync in this way is the same as using it with a remote shell
       except that:

       o      you either use a double colon :: instead of a single colon to
              separate the hostname from the path, or you use an rsync:// URL.

       o      the first word of the "path" is actually a module name.

       o      the remote daemon may print a message of the day when you connect.

       o      if you specify no path name on the remote daemon then the list of
              accessible paths on the daemon will be shown.

       o      if you specify no local destination then a listing of the
              specified files on the remote daemon is provided.

       o      you must not specify the --rsh (-e) option.

       An example that copies all the files in a remote module named "src":

           rsync -av host::src /dest

       Some  modules  on the remote daemon may require authentication. 

       If so, you will receive a password prompt when you connect. You can avoid
       the password prompt by setting the environment variable RSYNC_PASSWORD to
       the password you want to use or using the --password-file option. This
       may be useful when scripting rsync.

       WARNING: On some systems environment variables are visible to all users.
       On those systems using --password-file is recommended.

STARTING AN RSYNC DAEMON TO ACCEPT CONNECTIONS
       In  order  to connect to an rsync daemon, the remote system needs to have
       a daemon already running (or it needs to have configured something like
           inetd to spawn an rsync daemon for incoming connections on a
           particular port).  
       
       For full information on how to start a daemon that will handling incoming
       socket connections, see the rsyncd.conf(5) man page -- that is the config
       file for the daemon, and it contains the full details for  how  to  run
       the daemon (including stand-alone and inetd configurations).

       If you're using one of the remote-shell transports for the transfer,
       there is no need to manually start an rsync daemon.

EXAMPLES
       Here are some examples of how I use rsync.

       To backup my wife's home directory, which consists of large MS Word files and mail folders, I use a cron job that runs

              rsync -Cavz . arvidsjaur:backup

       each night over a PPP connection to a duplicate directory on my machine "arvidsjaur".

       To synchronize my samba source trees I use the following Makefile targets:

           get:
                   rsync -avuzb --exclude '*~' samba:samba/ .
           put:
                   rsync -Cavuzb . samba:samba/
           sync: get put

       this allows me to sync with a CVS directory at the other end of the connection. I then do CVS operations on the remote machine, which saves a lot of time as the remote CVS protocol isn't very efficient.

       I mirror a directory between my "old" and "new" ftp sites with the command:

       rsync -az -e ssh --delete ~ftp/pub/samba nimbus:"~ftp/pub/tridge"

       This is launched from cron every few hours.

OPTIONS
       Rsync  accepts  both  long  (double-dash  +  word)  and  short
       (single-dash + letter) options.  The full list of the available options
       are described below.  
       
       If an option can be specified in more than one way, the choices are
       comma-separated.  Some options only have a long variant, not a short.  If
       the option takes a parameter, the parameter is only listed after the long
       variant, even though it must also be specified for the short.  When
       specify- ing  a  parameter,  you  can either use the form --option=param
       or replace the '=' with whitespace.  The parameter may need to be quoted
       in some manner for it to survive the shell's command-line parsing.  Keep
       in mind that a leading tilde (~) in a filename is substituted by your
       shell, so --option=~/foo will not change the tilde into your home
       directory (remove the '=' for that).


       -n, --dry-run
              This  makes  rsync  perform  a  trial run that doesn't make any
              changes (and produces mostly the same output as a real run).  It
              is most commonly used in combination with the -v, --verbose and/or
              -i, --itemize-changes options to see what an rsync command is
              going to do before one actually runs it.

              The output of --itemize-changes is supposed to be exactly the same
              on a dry run and a subsequent real run (barring intentional
                  trickery and system call failures); if it isn't, that's a bug.
              Other  output  should  be mostly unchanged, but may differ in some
              areas.  Notably, a dry run does not send the actual data for file
              transfers, so --progress has no effect, the "bytes sent", "bytes
              received", "literal data", and "matched data" statistics are too
              small, and the "speedup" value is equivalent to a run where no
              file transfers were needed.

       --delete
              note: delete(sync) when there are deleted files in source side.

              This tells rsync to delete extraneous files from the receiving
              side (ones that aren't on the sending side), but only for the
              directories that are being synchronized.  You must have asked
              rsync to send the whole directory (e.g. "dir" or "dir/") without
              using a wildcard for the directory's contents (e.g. "dir/*") since
              the wildcard is expanded by the shell and rsync thus gets a
              request to transfer individual files, not  the  files' parent
              directory.   Files  that  are  excluded  from  the  transfer  are
              also  excluded  from being deleted unless you use the
              --delete-excluded option or mark the rules as only matching on the
              sending side (see the include/exclude modifiers in the FILTER
                  RULES section).

              Prior to rsync 2.6.7, this option would have no effect unless
              --recursive was enabled.  Beginning with 2.6.7, deletions will
              also occur when --dirs (-d) is enabled, but only for directories
              whose  contents  are  being copied.

              This option can be dangerous if used incorrectly!  It is a very
              good idea to first try a run using the --dry-run option (-n) to
              see what files are going to be deleted.

              If  the  sending side detects any I/O errors, then the deletion of
              any files at the destination will be automatically disabled. This
              is to prevent temporary filesystem failures (such as NFS errors)
              on the sending side from causing a massive deletion of files on
              the destination.  You can override this with the --ignore-errors
              option.

              The --delete option may be 'combined' with one of the
              --delete-WHEN options without conflict, as well as
              --delete-excluded.  However, if  none  of  the  --delete-WHEN
              options  are  specified,  rsync  will  choose the --delete-during
              algorithm when talking to rsync 3.0.0 or newer, and the
              --delete-before algorithm when talking to an older rsync.  See
              also --delete-delay and --delete-after.


       --delete-excluded
              In addition to deleting the files on the receiving side that are
              'not' on the sending side, this tells rsync to also delete any
              files on the receiving side that are 'excluded' (see --exclude).
              See the FILTER  RULES  section for a way to make individual
              exclusions behave this way on the receiver, and for a way to
              protect files from --delete-excluded.  See --delete (which is
                  implied) for more details on file-deletion.


       -f, --filter=RULE
              This option allows you to add rules to selectively 'exclude'
              certain files from the list of files to be transferred. This is
              most useful in combination with a recursive transfer.

              You  may  use  as  many --filter options on the command line as
              you like to build up the list of files to exclude.  If the filter
              contains whitespace, be sure to quote it so that the shell gives
              the rule to rsync as a single argument.  The text below also
              mentions that you can use an underscore to replace the space that
              separates a rule from its arg.

              See the FILTER RULES section for detailed information on this
              option.

FILTER RULES
       The filter rules allow for flexible selection of which files to transfer
       (include) and which files to skip (exclude).  The rules either directly
       specify include/exclude  patterns  or  they  specify  a  way  to  acquire
       more include/exclude patterns (e.g. to read them from a file).

       As  the list of files/directories to transfer is built, rsync checks each
       name to be transferred against the list of include/exclude patterns in
       turn, and the first matching pattern is acted on:  if it is an exclude
       pattern, then that file is skipped; if it is an include pattern then that
       filename is not skipped; if no matching pattern is found, then the
       filename is not skipped.

       Rsync builds an ordered list of filter rules as specified on the
       command-line.  Filter rules have the following syntax:

              RULE [PATTERN_OR_FILENAME]
              RULE,MODIFIERS [PATTERN_OR_FILENAME]

       You have your choice of using either short or long RULE names, as
       described below.  If you use a short-named rule, the ',' separating the
       RULE from the MODIFIERS is optional.  The  PATTERN  or  FILENAME  that
       follows  (when present) must come after either a single space or an
       underscore (_).  
       
       Here are the available rule 'prefixes':

              exclude, - specifies an exclude pattern.
              include, + specifies an include pattern.

              merge, . specifies a merge-file to read for more rules.

              dir-merge, : specifies a per-directory merge-file.
              hide, H specifies a pattern for hiding files from the transfer.
              show, S files that match the pattern are not hidden.

              protect, P specifies a pattern for protecting files from deletion.

              risk, R files that match the pattern are not protected.
              clear, ! clears the current include/exclude list (takes no arg)

       When rules are being read from a file, empty lines are ignored, as are
       comment lines that start with a "#".

INCLUDE/EXCLUDE PATTERN RULES
       You can include and exclude files by specifying patterns using the filter
       rules.  The include/exclude rules each specify a pattern that is matched
       against  the names of the files that are going to be transferred.  These
       patterns can take several forms:

       o      if the pattern 'ends' with a / then it will only match a directory,
              not a regular file, symlink, or device.

       o      rsync chooses between doing a simple string match and wildcard
       matching by checking if the pattern contains one of these three wildcard
       characters: '*', '?', and '[' .

       o      a '*' matches any path component, but it stops at slashes.

       o      use '**' to match anything, including slashes.

       o      a '?' matches any character except a slash (/).

       o      a '[' introduces a character class, such as [a-z] or [[:alpha:]].

       o      if the pattern contains a / (not counting a trailing /) or a "**",
       then it is matched against the full pathname, including any leading
         directories. If the pattern doesn't contain a / or a "**", then it is
         matched only against the final component of the filename.  (Remember
             that the algorithm is applied recursively so "full filename" can
             actually be any portion of a path from the starting directory on
             down.)

MERGE-FILE FILTER RULES
       You can merge whole files into your filter rules by specifying either a
       merge (.) or a dir-merge (:) filter rule (as introduced in the FILTER
           RULES section above).

       There are two kinds of merged files -- single-instance ('.') and
       per-directory (':').  A single-instance merge file is read one time, and
       its rules are incorporated into the filter list in the place of  the  "."
       rule.   For per-directory  merge  files,  rsync  will scan every
       directory that it traverses for the named file, merging its contents when
       the file exists into the current list of inherited rules.  These
       per-directory rule files must be created on the sending side because it
       is the sending side that is being scanned for the available files to
       transfer.  These rule files may also need to be transferred to the
       receiving side if you want them  to  affect  what files don't get deleted
       (see PER-DIRECTORY RULES AND DELETE below).

       Some examples:

              merge /etc/rsync/default.rules
              . /etc/rsync/default.rules
              dir-merge .per-dir-filter
              dir-merge,n- .non-inherited-per-dir-excludes
              :n- .non-inherited-per-dir-excludes

       The following modifiers are accepted after a 'merge' or dir-merge rule:

       o      A - specifies that the file should consist of only exclude
       patterns, with no other rule-parsing except for in-file comments.

       <ex>
        ~/source/DEVARCH$ cat /home/kpark/source/setup-humax/rsync.filter 
        P var/applications/
        - include/
        - **.a

        # files
        - oss/bin/msggrep
        - oss/bin/envsubst
        - oss/lib/gstreamer-1.0/*.la
        - oss/lib/libmpeg*


<config>

DESCRIPTION

The rsyncd.conf file is the runtime configuration file for rsync when run with
the --daemon option. When run in this way rsync becomes a rsync server listening
on TCP port 873. Connections from rsync clients are accepted for either
anonymous or authenticated rsync sessions.

The rsyncd.conf file controls authentication, access, logging and available
modules. 

MODULE OPTIONS

After the global options you should define a number of modules, each module
exports a directory tree as a symbolic name. Modules are exported by specifying
a module name in square brackets [module] followed by the options for that
module. 

secrets file
    The "secrets file" option specifies the name of a file that contains the
    username:password pairs used for authenticating this 'module'. This file is
    only consulted if the "auth users" option is specified. The file is line
    based and contains username:password pairs separated by a single colon. Any
    line starting with a hash (#) is considered a comment and is skipped. The
    passwords can contain any characters but be warned that many operating
    systems limit the length of passwords that can be typed at the client end,
    so you may find that passwords longer than 8 characters don't work.  There
    is no default for the "secrets file" option, you must choose a name (such as
    /etc/rsyncd.secrets). The file must normally not be readable by "other"; see
    "strict modes". 


[root@HUMAX /]# cat /etc/rsyncd.conf 
log file = /var/log/rsyncd.log
pid file = /run/rsyncd.pid
lock file = /run/rsync.lock

[Root]
   path = /
   comment = Humax box
   uid = root
   gid = root
   read only = no
   list = yes
   auth users = zinc
   secrets file = /etc/rsyncd.scrt

[root@HUMAX /]# cat /etc/rsyncd.scrt 
zinc:zinc


<ex>
$ mkdir dir1 dir2
$ touch dir1/file{1..10}
$ ls dir1
file1  file10  file2  file3  file4  file5  file6  file7  file8  file9

$ rsync -r dir1/ dir2      " okay as synced
$ ls dir2
file1  file10  file2  file3  file4  file5  file6  file7  file8  file9

note: This trailing / is necessary to mean "the contents of dir1".

$ rsync -r dir1 dir2       " not okay as
$ ls dir2
dir1


The -a option is a combination flag.

It stands for "archive" and syncs recursively and preserves symbolic links, special and device
files, modification times, group, owner, and permissions. It is more commonly used than -r and is
usually what you want to use.

The -n or --dry-run options. 

The -v flag (for verbose). 

The -P flag is very helpful. 
It combines the flags --progress and --partial. The first of these gives you a progress bar for the
transfers and the second allows you to resume interrupted transfers:

The -z flag. 
If you are transferring files that have not already been compressed, like text files, you can reduce
the network transfer by adding compression with the -z option.

<update>
Update the modification time on some of the files and see that rsync intelligently re-copies only
the changed files:

<delete>
By default, rsync does not delete anything from the destination directory. We can change this
behavior with the --delete option. Before using this option, use the --dry-run option and do testing
to prevent data loss:

rsync -a --delete source destination


<example> over SSH
rsync -a ~/dir1 username@remote_host:destination_directory
rsync -a username@remote_host:/home/username/dir1 place_to_sync_on_local_machine

<example>
Want to copy all expect .git from source to destination

rsync -av --progress /home/kit/mheg-remote-git/mag_shared/ . --exclude .git

<example>
setupRsyncDaemon() {

    # Setup rsync daemon for faster non-encrypted transfer
    rsync ${privatekey:+ --rsh="ssh -i $privatekey"} \
        "$(thisScriptSrcDir)"/rsyncd.{conf,scrt} root@$stbip:/etc/
    $ssh "chmod o-rwx /etc/rsyncd.scrt && rsync --daemon --ipv4"
    export RSYNC_PASSWORD=zinc
}


={============================================================================
*kt_linux_tool_027* tool-awk

Another popular stream editor. The basic function of awk is to search files for lines or other text
units containing one or more patterns. When a line matches one of the patterns, special actions are
performed on that line.

<data-driven>
Programs in awk are different from programs in most other languages, because awk programs are
"data-driven": you describe the data you want to work with and then what to do when you find it.
Most other languages are "procedural."

<rule>
The program consists of a series of rules. Each rule specifies one pattern to search for and one
action to perform upon finding the pattern.

<command>
print

The print command in awk outputs selected data from the input file. $0 (zero)
holds the value of the entire line.

ls -l | awk '{ print $5 $9 }'

With formatting.

awk '{ print "Size is " $5 " bytes for " $9 }'

<regex>
awk 'EXPRESSION { PROGRAM }' file(s)

For files ending in ".conf" and starting with either "a" or "x", using extended regular expressions
note: the below do not work.

ls -l | awk '/\<(a|x).*\.conf$/ { print $9 }'

To add text before output: begin message. beginning of execution before any input has been processed

awk 'BEGIN { print "Files found:\n" } /\<[a|x].*\.conf$/ { print $9 }'

To add text after output:

awk '/\<[a|x].*\.conf$/ { print $9 } END { print \ "Can I do anything else for you, mistress?" }'

<variables>
field separator

awk 'BEGIN { FS=":" } { print $1 "\t" $5 }' /etc/passwd

output field separator

> cat test
record1 data1
record2 data2

> awk '{ print $1 $2}' test
record1data1
record2data2

> awk '{ print $1, $2}' test
record1 data1
record2 data2

<example>
To make a filename from the date:

date: Fri Jul 25 05:30:12 BST 2014 -> log-Fri-Jul-25-...

script -f /home/kit/log/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`

To make a gateway from the ip address:
gateway=`echo $ip | awk 'BEGIN { FS="." } ; { print $1"."$2"."$3"."1 }'`


={============================================================================
*kt_linux_tool_028* tool-sed

https://www.gnu.org/software/sed/manual/html_node/index.html#Top

A stream editor is used to perform basic transformations on text read from a
file or a pipe. The result is sent to standard output. The editor does not
modify the original input.

What distinguishes sed from other editors, such as vi and ed, is its ability to
filter text that it gets from a pipeline feed. You do not need to interact with
the editor while it is running. 

This feature allows use of editing commands in scripts, greatly easing
`repetitive editing tasks` When facing replacement of text in a 'large' number
of files, sed is a great help.


{command}
SYNOPSIS
       sed [OPTION]... {script-only-if-no-other-script} [input-file]...

<option>
       -e script, --expression=script
              add the script to the commands to be executed

       -i[SUFFIX], --in-place[=SUFFIX]
              edit files in place (makes backup if SUFFIX supplied)

       -n, --quiet, --silent
              suppress automatic printing of pattern space

        note: since sed print out line which are not in matches so use -n to
        output only matches.

       -u, --unbuffered
              load minimal amounts of data from the input files and flush the
              output buffers more often

       s/regexp/replacement/
              Attempt  to  match  regexp  `against the pattern space` .  If
              successful, replace that portion matched with replacement.  The
              replacement may contain the special character & to refer to that
              portion of the pattern space which matched, and 
              
              the special escapes \1 through \9 to refer to the corresponding
              matching `sub-expressions` in the regexp.


   Commands which accept address ranges

       p      Print the current pattern space.


   Zero- or One- address commands

       i \
       `text`   Insert `text`, which has each embedded newline preceded by a
       backslash.

       \\ from online gnu sed
       \\
       \\ i\
       \\ text
       \\ As a GNU extension, this command accepts two addresses.
       \\
       \\ Immediately output the lines of text which follow this command (each
       \\  but the last ending with a \, which are removed from the output). 

<ex>
Add it at the line 3. So if calls it repeatdly then it grows downwards.

sed -i '3i NDS_BUILD_TYPE_'$1' = debug' ${CWD}/projects/$PROJECT/build_options.mk


<ex>
To find and replace string in multiple files.

$ egrep -lr --include *.c mhvSessionCancel .
./mh5eng/mh5a_application.c

$ egrep -lr --include *.c mhvSessionCancel . \
  | xargs sed -i 's/mhvSessionCancel/mmhv_session_cancel/'


<ex>

echo "translation/darwin_783e53e0c7f8_36feaec3b67d50d62258f0d5ae9cdb40/.detailed_output \
       translation/darwin_783e53ffa5bd_8a5775d303cba39df2721bfce433cc58/.detailed_output"

$FILES has one string which has many files from commends output. If run grep
on that, will give 1 even if there are multiple matches.

NUM_FOUND=``echo $FILES | grep -c detailed_output``

To make one string multiple lines, run sed using sub expression.

NUM_FOUND=``echo $FILES | sed 's/\(detailed_output\)/\2\n/g' | grep -c detailed_output``


<address>
3.2 Selecting lines with sed

Addresses in a sed `script` can be in any of the following forms:

number
    Specifying a line number will match only that line in the input. 
    (Note that sed counts lines continuously across all input files unless -i
     or -s options are specified.) 


<ex>
sed -n '2,4p' example
sed -n '3,$p' example

// before
libTitaniumDeviceAuthoritySystemDbusClient.so createDbusSystemFactory

sed -i \
  -e '1i libTitaniumDeviceAuthoritySystemOff.so createOffSystemFactory' \
  -e 's/^/# DA disabled by patch-a-tron: /' [FILE]

// after
libTitaniumDeviceAuthoritySystemOff.so createOffSystemFactory
# DA disabled by patch-a-tron: libTitaniumDeviceAuthoritySystemDbusClient.so
      createDbusSystemFactory


<ex>
Here 's/&.*//', '&' is literal but not special.

$ echo "STREAM=R015&PLATFORM=drx890&PRIORITY=1&MACLIST=&LOGS=999999" | \
    grep 'PLATFORM=' | sed -e's/.*PLATFORM=//' -e 's/&.*//'
drx890


<busybox>
sed

    sed [-efinr] SED_CMD [FILE]...

    Options:

            -e CMD  Add CMD to sed commands to be executed
            -f FILE Add FILE contents to sed commands to be executed
            -i      Edit files in-place
            -n      Suppress automatic printing of pattern space
            -r      Use extended regex syntax

    If no -e or -f is given, the first non-option argument is taken as the sed
    command to interpret. All remaining arguments are names of input files; if
    no input files are specified, then the standard input is read. Source files
    will not be modified unless -i option is given.


={============================================================================
*kt_linux_tool_029* pyserial and grabserial

http://elinux.org/Grabserial
https://github.com/tbird20d/grabserial

If no options are specified, grabserial uses serial port /dev/ttyS0, at 115200 baud with "8, None
and 1" (8N1) settings. 

alias gse="sudo grabserial -v -d "/dev/ttyUSB0" -b 115200 -w 8 -p N -s 1 \
| tee 2>&1 ~/logs/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`"

alias gse="sudo grabserial -v -d /dev/ttyUSB0 \
| tee 2>&1 ~/logs/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`"


={============================================================================
*kt_linux_tool_030* tool-diff and patch

{diff}
<from-to-and-unified>
To compare two files:

diff [options] from-file to-file
diff -u file1 file2

-u    Use the 'unified' output format.

This outputs a description of how to transform file1 'into' file2 to stdout in
unified format which is the easiest to read. The description is called a
patch.

You can compare a whole directory tree:

diff -u -r directory1 directory2

This recurses the directory structures. Whenever a file differs, the patch for that file is appended
to the output.

To save the patches to a file that you can store or send to someone else, simply redirect stdout to
a file, e.g:

diff -u file1 file2 > my_changes.patch

{unified-format}
A typical patch looks like this:

--- a/vobs/DARWIN_APPLICATIONS/DARWIN_GAUDI_ORCHID/build/applications/GAUDI_Orchid/EPG/statmgr/xtv_states/QhsmPVR.c
+++ b/vobs/DARWIN_APPLICATIONS/DARWIN_GAUDI_ORCHID/build/applications/GAUDI_Orchid/EPG/statmgr/xtv_states/QhsmPVR.c
@@ -2815,7 +2815,6 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
 
         /*set the playback speed to normal*/
         DBGMSG_M("QHsmPVR_backWard_skip - PVR_NAV_SetSpeed(PVR_NAV_PLAY)\n");
-        PVR_NAV_SetSpeed(PVR_NAV_PLAY);
 
         /*check if we are in RB mode*/
         CONSELECTCO_GetCurrentPlayRBFlag( &state);
@@ -2827,6 +2826,7 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
         {
             /*In RB mode*/
             PVR_NAV_SetPosition ( PVR_NAV_RELATIVE_POSITION_START , 1000);
+           PVR_NAV_SetSpeed(PVR_NAV_PLAY);^M
         }
         else{
             /*We are in playback mode*/
@@ -2840,6 +2840,7 @@ QSTATE QHsmPVR_backWard_skip(QHsmPVR* me, QEvent * e)
             if (rc != PVR_NAV_RC_OK) {
                 DBGMSG_M("No previous skip point; skipping to beginning and waiting for timeout;");
                 PVR_NAV_SetPosition ( PVR_NAV_RELATIVE_POSITION_START , 0);
+               PVR_NAV_SetSpeed(PVR_NAV_PLAY);^M
                 prevPoint = -1;
             }
             else {

<header>
The first two lines describe the files to be transformed. The file indicated by "---" is transformed
into the file indicated by "+++". Let's call them the old file and the new file.

<hunks>
This header is followed by a series of "hunks". Each hunk describes a changes to make to a section
of the file. The @@ symbols indicate the 'start' of a hunk. The first set of numbers, e.g. "-2815,7"
indicates that in the old file, the section 'started' on line 2815 and 'lasted' for 7 lines. The second
set of numbers, e.g. "+2815,6" indicates that in the new file, the section starts on like 2815 and
lasts for 6 lines. (You can guess that we are going to remove a line.)

<p-option>
The text after the @@ symbol indicates the function name that the change is in. This is generated by
the -p option of diff so may not always be there. It is just to make the patch easier to read.

       -p, --show-c-function
              show which C function each change is in

<context>
Next we have three lines of context. These are just the three lines before the change. They are used
to check that the change is going to be made in the right place. For example, it is possible that
you want to apply the patch to a different version of the file it was created with. These lines help
you manually or automatically apply the change in the right place, even if the line numbers have
changed. The context is also used to detect when a patch cannot be applied because of a conflict.
More on this later.

<changes>
Next we have the change itself. Lines are either removed (indicated by a - sign) or added, indicated
by a + sign. When a line is changed, it usually appears as a removed line followed by an added line.

Finally we have three more lines of context.

So now you can read patches and even manually apply simple ones. But with complicated patches, you
would prefer to apply them automatically.


{patch}
GNU patch reads a patch file and applies the transformations it describes. Typical use:

cd directory_containing_file_to_change

patch < my_changes.patch

GNU patch reads patches from stdin. In this case we have redirected stdin to the patch file with the "<" symbol.

If you have a patch with lots of files in different directories, you might do this:

cd parent_directory

patch -p1 < my_changes.patch

<p-option>
The number after the p tells patch how 'many' directory names to strip from the filenames before
applying the patch. For example, typically the person generating the patch did something like this:

cd my_code
cd ..
cp -R my_code my_original_code

...make changes to lots of files...

diff -u -r my_original_code my_code

This means that the patch file will contain filenames like this:

--- my_original_code/directory/file.c
+++ my_code/directory/file.c

Now you probably don't have directories called "my_code" and "my_original_code", so you would cd to
"directory" and tell patch to ignore the first directory in each file path by using the "-p1"
option.

<patching-non-identical-files>
GNU patch will try to apply the changes even if you are applying them to a different version of the
file to the one the patch was originally created against. It uses the context lines to do this. As
long as enough context lines appear, and they have not moved to far from their original positions,
     the change will be applied.

So, for example, if a new function was added to the top of the file, the change to a function
further down the file would still be correctly applied. This means it can be more useful to send
someone a patch than a whole file, because it will often still work if the person you send it to is
starting with a different version of the file.

If GNU patch decides it cannot apply a change, you will see:

HUNK FAILED

along with details of which hunk it was. You can then investigate why the hunk failed - patches are
easy enough to read to do this.

<concatenating-patches>
Patches can be concatenated, so you can combine multiple patches into one:

diff -u my_code_orig/file1.c my_code/file1.c > patch1
diff -u my_code_orig/file2.c my_code/file2.c > patch2
cat patch1 patch2 > big_patch


{example}
-a     Treat all files as text and compare them line-by-line, even if they do not seem to be text.
-N
--new-file
   In directory comparison, if a file is found in only one directory, treat it as present but empty
   in the other directory.

diff -Naur darwin-spk-1.18_orig/target/device/Sky/ams-drx890/kernel-debug.config darwin-spk-1.18/target/device/Sky/ams-drx890/kernel-debug.config
--- darwin-spk-1.18_orig/target/device/Sky/ams-drx890/kernel-debug.config	2011-10-05 12:36:12.000000000 +0100
+++ darwin-spk-1.18/target/device/Sky/ams-drx890/kernel-debug.config	2012-01-18 15:23:41.000000000 +0000
@@ -1157,7 +1157,7 @@
 CONFIG_DEBUG_FS=y
 # CONFIG_WANT_EXTRA_DEBUG_INFORMATION is not set
 CONFIG_CROSSCOMPILE=y
-CONFIG_CMDLINE="mem=160M rw console=uart,mmio,0x10400b00,115200n8 ip_conntrack_tftp.ports=4085"
+CONFIG_CMDLINE="mem=166M rw console=uart,mmio,0x10400b00,115200n8 ip_conntrack_tftp.ports=4085"
 CONFIG_SYS_SUPPORTS_KGDB=y
 # CONFIG_MIPS_BRCM_SIM is not set

<patch-options> in the script

       -d dir  or  --directory=dir
          Change to the directory dir immediately, before doing anything else.

       --dry-run
          Print  the results of applying the patches without actually changing
          any files.

       -E  or  --remove-empty-files
          Remove output files that are  empty  after  the  patches  have  been
          applied.  Normally this option is unnecessary, since patch can examâ
          ine the time stamps on the header to determine whether a file should
          exist  after  patching.  However, if the input is not a context diff
          or if patch is conforming to POSIX,  patch  does  not  remove  empty
          patched  files  unless  this  option is given.  When patch removes a
          file, it also attempts to remove any empty ancestor directories.

       -f  or  --force
          Assume that the user knows exactly what he or she is doing,  and  do
          not  ask any questions.  Skip patches whose headers do not say which
          file is to be patched; patch files even though they have  the  wrong
          version  for  the Prereq: line in the patch; and assume that patches
          are not reversed even if they look like they are.  This option  does
          not suppress commentary; use -s for that.

       -s  or  --silent  or  --quiet
          Work silently, unless an error occurs.

   patch)
      # For each patch, try to derive whether it was created as -p0 or -p1.
      # p0 is typically from svn/cvs/git, while p1 will typically come from diff.
      patchlevel=

      patch -d $SPKDIR -p1 --dry-run --quiet -f < $p > /dev/null 2>&1
      p1=$?
      if [ $p1 -eq 0 ]; then
         patchlevel=-p1
      fi

      patch -d $SPKDIR -p0 --dry-run --quiet -f < $p > /dev/null 2>&1
      p0=$?
      if [ $p0 -eq 0 ]; then
         patchlevel=-p0
      fi
      if [ $p0 -eq 0 -a $p1 -eq 0 ]; then
         echo "WARNING: patch '$p' can be applied -p0 or -p1. Using -p0."
         patchlevel=-p0
      fi
      if [ -z "$patchlevel" ]; then
         echo "Unable to apply patch '$p'"
         exit 1
      fi
      echo "Applying patch '$p'..."
      patch -d $SPKDIR $patchlevel -E < $p
      if [ $? -ne 0 ]; then
         echo "Patch $p failed. Aborting."
         exit 1
      fi
      ;;


={============================================================================
*kt_linux_tool_031* tool-zip tool-tar

{zip}
-d --decompress --uncompress 
-l --list

" maintain the original file
       -c --stdout --to-stdout
              Write output on standard output; keep original files unchanged.  If there are
              several input files, the output consists of a sequence of independently  com-
              pressed  members.  To  obtain better compression, concatenate all input files
              before compressing them.

gzip -c ramdisk_rootfs_img > ramdisk_rootfs_img.gz


{tar}

SYNOPSIS
       tar [OPTION...] [FILE]...

DESCRIPTION
       GNU tar saves many files together into a single tape or disk `archive`,
       and can restore individual files `from the archive`


       tar -cf archive.tar foo bar
              # Create archive.tar from files foo and bar.


# maintain permissions when create a archive
% tar cvzfp xxx.tgz ./

# to extract
% tar zxvf *.tgz

-f, --file ARCHIVE
use archive file or device ARCHIVE

# '-' is used instead of filename after -f
# to extract from stdin. wget write to stdout and tar read from stdin. '-' used differently.
wget http://xxx.tar.bz2 -O - | tar -xjf -

" to extract bz file
% tar xjf *.bz2

" to create bzip2
% tar cvjfp filename
% bzip2 -d gdb.bz2

" to list
% tar tvf 


# extract single file or folder
tar xvf WALRUS_6833.tgz deps/FUSIONOS_INPUT_DEL_2/DEL_AMS_BCM_MIPS4K_LNUX_DARWIN_01/Tarball/

<options>
       -C, --directory=DIR
              change to directory DIR
              note: can use to change dir and to extract there

        ex. 
        tar -cjvf "$SPK_TARBALL_MW" -C nds-mw.

       -f, --file=ARCHIVE
              use archive file or device ARCHIVE

       -c, --create
              create a new archive

       -z, --gzip
              filter the archive through gzip

Since when run $nds-mw> tar -cjvf xx.bz . then there will be xx.bz which is
empty in the archive. To get around this, use -C to move dir and do zip but
the output will be make before moving a dir.

<symlink>
When use tar on dir which is a symlink, then tar don not work. Any options?


{problem}
Have a x.gz which is actually tar file and gunzip to unzip but has one single
file. The problem is that the single file has lots of broken contents.

When use tar command to untar, see lots of files in that. So seems that when
unzip that, it puts all files into one file? 


tar --strip-components=1 -C /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4 \
--exclude='libjava/*'  --exclude='libgo/*'  --exclude='gcc/testsuite/*'  --exclude='libstdc++-v3/testsuite/*'   -xf -


={============================================================================
*kt_linux_tool_032* tool-split tool-merge

       split [OPTION]... [INPUT [PREFIX]]

       Output  fixed-size  pieces of INPUT to PREFIXaa, PREFIXab, ...; default
       size is 1000 lines, and default PREFIX is 'x'.  

       -b, --bytes=SIZE
              put SIZE bytes per output file

$ split -b 200MB build-full-after-clean.log build-full-after-clean.

build-full-after-clean.aa
...
build-full-after-clean.ci

To merge:

cat xxx.* > outfile


={============================================================================
*kt_linux_tool_032* tool-tr

NAME
       tr - translate or delete characters

       -d, --delete
              delete characters in SET1, do not translate


={============================================================================
*kt_linux_tool_032* tool-dd

DD(1)

NAME
       dd - convert and copy a file

SYNOPSIS
       dd [OPERAND]...
       dd OPTION

DESCRIPTION
       Copy a file, converting and formatting according to the operands.

       bs=BYTES
              read and write up to BYTES bytes `at a time`

       if=FILE
              read from FILE instead of stdin

       of=FILE
              write to FILE instead of stdout

       seek=BLOCKS
              skip BLOCKS obs-sized blocks at start of output

       count=BLOCKS
              copy only BLOCKS input blocks

       note:
       Must use `count` since not know the default and if not use, will have
       unknown size of input/output. May be bigger than you need.

       BLOCKS  and  BYTES  may  be  followed  by  the  following
       multiplicative suffixes: c =1, w =2, b =512, kB =1000, K =1024, MB
       =1000*1000, M =1024*1024, xM =M GB =1000*1000*1000, G =1024*1024*1024,
       and so on for T, P, E, Z, Y.

<ex>
dd of="${core_output_path}/core.$pid" bs=1M

<ex> create a binary file filled with zeros. must use seek.
dd of=zeros.bin if=/dev/null bs=10 seek=2

<ex>
$ hexdump -C -n 20 PCAT.DB
00000000  53 51 4c 69 74 65 20 66  6f 72 6d 61 74 20 33 00  |SQLite format 3.|
00000010  08 00 01 01                                       |....|
00000014

$ dd if=PCAT.DB bs=10c count=1
SQLite for1+0 records in
1+0 records out
10 bytes (10 B) copied, 1.0746e-05 s, 931 kB/s

$ dd if=PCAT.DB bs=20c count=1
SQLite format 1+0 records in
1+0 records out
20 bytes (20 B) copied, 1.3354e-05 s, 1.5 MB/s

$ dd if=PCAT.DB bs=30c count=1
SQLite format @  ▒1+0 records in
1+0 records out
30 bytes (30 B) copied, 1.2526e-05 s, 2.4 MB/s

$ hexdump -C -n 20 PCAT.DBJ
00000000  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|
00000010  00 00 00 00                                       |....|
00000014

$ dd if=PCAT.DBJ bs=20c count=1
1+0 records in
1+0 records out
20 bytes (20 B) copied, 1.427e-05 s, 1.4 MB/s

So dd tries to print out outputs on stdin and prints out only displayable
chars? Anyway, there will be no output for null byte stream.


={============================================================================
*kt_linux_tool_033* getconf

       getconf - Query system configuration variables

       getconf [-v specification] system_var

       -a
       Displays all configuration variables for the current system
       and their values.

       -v
       Indicate the specification and version for which to obtain
       configuration variables.

       system_var
       A system configuration variable, as defined by sysconf(3) or
       confstr(3).

$ getconf PAGESIZE
4096


={============================================================================
*kt_linux_tool_034* tool-ps tool-top

DESCRIPTION
       ps displays information about a selection of the active processes.  If
       you want a repetitive update of the selection and the displayed
       information, use top(1) instead.

       This version of ps accepts several kinds of options:

       1   UNIX options, which may be grouped and must be preceded by a dash.
       2   BSD options, which may be grouped and must not be used with a dash.
       3   GNU long options, which are preceded by two dashes.

       Options of different types may be freely mixed, but conflicts can appear.
       There are some synonymous options, which are functionally identical, due
       to the many standards and ps implementations that this ps is compatible
       with.

       Note that "ps -aux" is distinct from "ps aux".  The POSIX and UNIX
       standards require that "ps -aux" print all processes owned by a user
       named "x", as well as printing all processes that would be selected by
       the -a option.  If the user named "x" does not exist, this ps may
       interpret the command as "ps aux" instead and print a warning.  This
       behavior is intended to aid in transitioning old scripts and habits.  It
       is fragile, subject to change, and thus should not be relied upon.

       By default, ps selects 'all' processes with the same effective user ID
       (euid=EUID) as the current user and associated with the same terminal as
       the invoker.  It displays the process ID (pid=PID), the terminal
       associated with the process (tname=TTY), the cumulated CPU time in
       [DD-]hh:mm:ss format (time=TIME), and the executable name (ucmd=CMD).
       Output is unsorted by default.

       The use of BSD-style options will add process state (stat=STAT) to the
       default display and show the command args (args=COMMAND) instead of the
       executable name.  You can override this with the PS_FORMAT environment
       variable.  The use of BSD-style options will also change the process
       selection to include processes on other terminals (TTYs) that are owned
       by you; alternately, this may be described as setting the selection to be
       the set of all processes filtered to exclude processes owned by other
       users or not on a terminal.  These effects are not considered when
       options are described as being "identical" below, so -M will be
       considered identical to Z and so on.

       Except as described below, process selection options are additive.  The
       default selection is discarded, and then the selected processes are added
       to the set of processes to be displayed.  A process will thus be shown if
       it meets any of the given selection criteria.

SIMPLE PROCESS SELECTION
       -A     Select all processes.  Identical to -e.
       -e     Select all processes.  Identical to -A.

THREAD DISPLAY
       H      Show threads as if they were processes.

       -L     Show threads, possibly with LWP and NLWP columns.

OUTPUT FORMAT CONTROL
       These options are used to choose the information displayed by ps.  The
       output may differ by personality.

       -f     Do full-format listing. This option can be combined with many
       other UNIX-style options to add additional columns.  It also causes the
       command arguments to be printed.  When used with -L, the NLWP (number of
               threads) and LWP (thread ID) columns will be added.  See the c
       option, the format keyword args, and the format keyword comm.

STANDARD FORMAT SPECIFIERS
       Here are the different keywords that may be used to control the output
       format (e.g. with option -o) or to sort the selected processes with the
       GNU-style --sort option.

       Some keywords may not be available for sorting.

       CODE        HEADER    DESCRIPTION

       args        COMMAND   command with all its arguments as a string.

       Modifications to the arguments may be shown. The output in this column
       may contain spaces. A process marked <defunct> is partly dead, waiting
       to be fully destroyed by its parent.  Sometimes the process args will be
       unavailable; when this happens, ps will instead print the executable name
       in brackets. (alias cmd, command).
       
       See also the comm format keyword, the -f option, and the c option.  When
       specified last, this column will extend to the edge of the display.  If
       ps can not determine display width, as when output is redirected (piped)
       into a file or another command, the output width is undefined (it may be
               80, unlimited, determined by the TERM variable, and so on).  The
       COLUMNS environment variable or --cols option may be used to exactly
       determine the width in this case.  The w or -w option may be also be used
       to adjust width.

       command     COMMAND   See args.  (alias args, command).

       wchan       WCHAN     name of the kernel function in which the process is
       sleeping, a "-" if the process is running, or a "*" if the process is
       multi-threaded and ps is not displaying threads.


       lwp         LWP       light weight process (thread) ID of the
       dispatchable entity (alias spid, tid).  See tid for additional
       information.


<ex>
UID        PID  PPID   LWP  C NLWP    SZ   RSS PSR STIME TTY          TIME CMD
16384     3051  1272  3089  0   32 81585 62568   0 Jan21 ?        00:00:00 /opt/stagecraft-2.0/bin/stagecraft --astrace --bgalpha 0 --outputrect 0,0,1280,720 --extensionsdir /opt/zinc-trunk/lib/stagecraft2-extensions --profile extendedTV --modulemap IGraphicsDriver:/opt/stagecraft-2.0/bin/libIGraphicsDriver2.so /app


<ex>
" to get gpid of the shell
$ ps -o pgid $$
 PGID
 7523

$ ps -p $$ -o 'pid pgid sid command'
  PID  PGID   SID COMMAND
 3697  3697  3697 bash


<ex>
The number of threads, per process

ps -eLf | grep -E 'zin[c]|df[b]' | awk '{print $10}' | uniq -c | sort -nr

Can also show to see what the threads are up to right now:

ps -eAL -o command,wchan | grep -E 'zin[c]|df[b]' | sort | uniq -c | sort -nr


<top>
Help for Interactive Commands - procps version 3.2.7
Window 1:Def: Cumulative mode Off.  System: Delay 3.0 secs; Secure mode Off.

  Z,B       Global: 'Z' change color mappings; 'B' disable/enable bold
  l,t,m     Toggle Summaries: 'l' load avg; 't' task/cpu stats; 'm' mem info
  1,I       Toggle SMP view: '1' single/separate states; 'I' Irix/Solaris mode

  f,o     . Fields/Columns: 'f' add or remove; 'o' change display order
  F or O  . Select sort field
  <,>     . Move sort field: '<' next col left; '>' next col right
  R,H     . Toggle: 'R' normal/reverse sort; 'H' show threads
  c,i,S   . Toggle: 'c' cmd name/line; 'i' idle tasks; 'S' cumulative time
  x,y     . Toggle highlights: 'x' sort field; 'y' running tasks
  z,b     . Toggle: 'z' color/mono; 'b' bold/reverse (only if 'x' or 'y')
  u       . Show specific user only
  n or #  . Set maximum tasks displayed

  k,r       Manipulate tasks: 'k' kill; 'r' renice
  d or s    Set update interval
  W         Write configuration file
  q         Quit
          ( commands shown with '.' require a visible task display window )
Press 'h' or '?' for help with Windows,
any other key to continue


={============================================================================
*kt_linux_tool_034* tool-nproc

NAME
       nproc - print the number of processing units available


={============================================================================
*kt_linux_tool_035* tool-wget, tool-curl

{wget}
wget http://xxx.tar.bz2 -O - | tar -xjf -

-O file
--output-document=file
The documents will not be written to the appropriate files, but all will be
concatenated together and written to file. If - is used as file, documents will
be printed to standard output, disabling link conversion. 

--no-check-certificate

<to-get-all>
wget -e robots=off -r -l 1 -nd https://stb-tester-master/s/

       -e command
       --execute command
           Execute command as if it were a part of .wgetrc.  A command thus
           invoked will be executed after the commands in .wgetrc, thus taking
           precedence over them.  If you need to specify more than one wgetrc
           command, use multiple instances of -e.

   Recursive Retrieval Options
       -r
       --recursive
           Turn on recursive retrieving.    The default maximum depth is 5.

       -l depth
       --level=depth
           Specify recursion maximum depth level depth.


   Directory Options
       -nd
       --no-directories
           Do not create a hierarchy of directories when retrieving recursively.
           With this option turned on, all files will get saved to the current
           directory, without clobbering (if a name shows up more than once, the
                   filenames will get extensions .n).


{curl}
curl  is  a  tool  to  transfer data from or to a server, using one of the
supported protocols (DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS, IMAP, IMAPS,
        LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, TELNET and
        TFTP). The command is designed to work without user interaction.

curl offers a busload of useful tricks like proxy support, user authentication,
     FTP upload, HTTP post, SSL connections, cookies, file transfer resume and
     more. As  you  will  see below, the number of features will make your head
     spin!

curl is powered by libcurl for all transfer-related features. See libcurl(3) for
details.

PROGRESS METER
       curl  displays  this  data  to  the  terminal  by default, so if you
       invoke curl to do an operation and it is about to write data to the
       terminal, it disables the progress meter as otherwise it would mess up
       the output mixing progress meter and response data.

curl --silent --show-error -o "$(basename $galliumurl)" "$galliumurl"

<ex>
When wget fails, use curl 

wget https://zinc-logs.gz

ERROR: The certificate of 'stb.co.uk' is not trusted.
ERROR: The certificate of 'stb.co.uk' hasn't got a known issuer.

curl --insecure https://zinc-logs.gz > zinc-logs.gz


{libcurl}
https://curl.haxx.se/libcurl/c/curl_easy_perform.html
https://curl.haxx.se/libcurl/c/curl_easy_cleanup.html

https://curl.haxx.se/changes.html

 Fixed in 7.33.0 - October 14 2013

Changes:
    cookies: add expiration 


={============================================================================
*kt_linux_tool_036* nc

netcat is a simple unix utility which reads and writes data across network connections, using
TCP or UDP protocol. It is designed to be a reliable "back-end" tool that can be used directly or
easily driven by other programs and scripts. At the same time, it is a feature-rich network
debugging and exploration tool, since it can create almost any  kind of connection you would need
and has several interesting built-in capabilities. 


In the simplest usage, "nc host port" creates a TCP connection to the given port on the given target
host. Your standard input is then sent to the host, and anything that  comes back across the
connection is sent to your standard output. This continues indefinitely, until the network side of
the connection shuts down.  Note that this behavior is different from most other applications which
shut everything down and exit after an end-of-file on the standard input.


note: on host but not the target, when run nc without -q then no prompt back when run so need to
specify -q to get prompt back.

printf "D\t${key}\n\0U\t${key}\n\0" | nc -q 1 $box_ip $box_port

-q seconds   after EOF on stdin, wait the specified number of seconds and then quit. If seconds is
negative, wait forever.


={============================================================================
*kt_linux_tool_037* port checks

Type the following command to see list well-known of TCP and UDP port numbers:

$ less /etc/services
$ grep -w 80 /etc/services 


{port-numbers}
Typically port number less than 1024 are used by well know network servers such as Apache. Under
UNIX and Linux like oses root (super user) privileges are required to open privileged ports. Almost
all clients uses a high port numbers for short term use.

The port numbers are divided into three ranges:

1. Well Known Ports: those from 0 through 1023.
2. Registered Ports: those from 1024 through 49151
3. Dynamic and/or Private Ports: those from 49152 through 65535

You can increase local port range by typing the following command (Linux specific example):
# echo 1024 65535 > /proc/sys/net/ipv4/ip_local_port_range
#
You can also increase or decrease socket timeout (Linux specific example):
# echo 2000 > /proc/sys/net/ipv4/tcp_keepalive_time

# for debian pc and embedded linux
#
root# cat /proc/sys/net/ipv4/ip_local_port_range 
32768	61000

<to-see-open-ports>

netstat [address_family_options] [--tcp|-t] [--udp|-u] [--raw|-w] [--listening|-l] [--all|-a]
[--numeric|-n] [--numeric-hosts] [--numeric-ports] [--numeric-users] [--symbolic|-N]
[--extend|-e[--extend|-e]] [--timers|-o] [--program|-p] [--verbose|-v] [--continuous|-c]


$ netstat -tulpn
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      -               
tcp        0      0 0.0.0.0:47541           0.0.0.0:*               LISTEN      -               
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:3350          0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      -               
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      -               
tcp        0      0 0.0.0.0:3389            0.0.0.0:*               LISTEN      -               
tcp6       0      0 :::111                  :::*                    LISTEN      -               
tcp6       0      0 :::22                   :::*                    LISTEN      -               
tcp6       0      0 ::1:631                 :::*                    LISTEN      -               
tcp6       0      0 :::33815                :::*                    LISTEN      -               
tcp6       0      0 ::1:25                  :::*                    LISTEN      -               

To displays listening sockets (open ports)

$ netstat -nat | grep LISTEN


To list open IPv4 connections use the lsof command:

$ lsof -Pnl +M -i4
COMMAND     PID     USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME
icedove    3900     1024   58u  IPv4 1836476      0t0  TCP 172.20.33.215:38273->132.245.226.18:993 (ESTABLISHED)
skype      4588     1024   11u  IPv4   15436      0t0  UDP 127.0.0.1:35553 
skype      4588     1024   35u  IPv4 1859784      0t0  TCP 172.20.33.215:51348->157.56.192.193:443 (ESTABLISHED)
skype      4588     1024   36w  IPv4   17138      0t0  TCP *:28890 (LISTEN)
skype      4588     1024   45u  IPv4   17139      0t0  UDP *:28890 
skype      4588     1024   54u  IPv4   18119      0t0  TCP 172.20.33.215:54124->157.55.235.145:40030 (ESTABLISHED)
skype      4588     1024   92u  IPv4   15527      0t0  TCP 172.20.33.215:41335->91.190.218.55:12350 (ESTABLISHED)
iceweasel  9539     1024   62u  IPv4 1585820      0t0  TCP 172.20.33.215:36101->216.58.208.69:443 (ESTABLISHED)
iceweasel  9539     1024   65u  IPv4 1856332      0t0  TCP 172.20.33.215:44701->74.125.195.189:443 (ESTABLISHED)
iceweasel  9539     1024   74u  IPv4 1813863      0t0  TCP 172.20.33.215:42211->31.221.26.57:80 (ESTABLISHED)
iceweasel  9539     1024   77u  IPv4 1545135      0t0  TCP 172.20.33.215:44615->185.45.5.50:443 (ESTABLISHED)
iceweasel  9539     1024   87u  IPv4 1839953      0t0  TCP 172.20.33.215:56696->185.45.5.43:443 (ESTABLISHED)
hipchat.b 11542     1024   33u  IPv4   57796      0t0  TCP 172.20.33.215:36851->54.161.161.10:5222 (ESTABLISHED)
ssh       19536     1024    3r  IPv4 1852553      0t0  TCP 172.20.33.215:42930->172.20.33.192:22 (ESTABLISHED)
ssh       19607     1024    3r  IPv4 1744563      0t0  TCP 172.20.33.215:43054->172.20.33.192:22 (ESTABLISHED)


<firewall>
In other words, Apache port is open but it may be blocked by UNIX (pf) or Linux (iptables) firewall.
You also need to open port at firewall level. In this example, open tcp port 80 using Linux iptables
firewall tool:

$ /sbin/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT
$ /sbin/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 2033 -j ACCEPT
$ service iptables save


={============================================================================
*kt_linux_tool_016* tool-screen

Screen is a full-screen software program that can be used to multiplexes a
physical console between several processes (typically interactive shells). It
offers a user to open several separate terminal instances inside a one single
terminal window manager.

sudo screen -S name -a -D -R -fn -l /dev/ttyUSB0 115200,cs8

<help>
       C-a ?       (help)        Show key bindings.

<close>
to close screen use Ctrl-A, k, y. Do not use Ctrl-C as it can kill processes
running on the box.

       C-a k
       C-a C-k     (kill)        Destroy current window.


{logging}
Use -L for auto logging and will log on 'screenlog.0'. Use this file to see:

-L   tells screen to turn on automatic output logging for the windows.

screen -a -D -R -fn -l -L /dev/ttyUSB0 115200,cs8


       C-a h       (hardcopy)    Write a hardcopy of the current window to the
       file "hardcopy.n".

       C-a H       (log)         Begins/ends logging of the current window to
       the file "screenlog.n".

<monitoring>
       C-a M       (monitor)     Toggles monitoring of the current window.


={============================================================================
*kt_linux_tool_038* tool-minicom

SYNOPSIS
       minicom [-somMlwz8] [-c on|off] [-S script] [-d entry]
               [-a on|off] [-t term] [-p pty] [-C capturefile] [configuration]

       -c   Color usage. Some terminals (such as the Linux console) support
       color with the standard ANSI escape sequences. Because there is
       apparently no termcap support for color, these escape sequences are
       hard-coded  into  mini- com. Therefore this option is off by default.
       You can turn it on with '-c on'. This, and the '-m' option, are good
       candidates to put into the MINICOM environment variable.

1. Setup minicon with -s switch

$ minicom -s

Choose Serial port setup and specify below settings, come out of Serial port
setup by pressing Esc and then save configuration file.


<config>
Once saved, it will store the configuration in /etc/minicom/. To connect to your
device, run minicom with config file name:

$ minicom cisco or minicom -c on cisco

To Exit from minicom, Click Ctrl-A and Z then X.

alias mc='sudo minicom -C hmax.log -c on hmax'

<when-cannot-get-shell-prompt>
Without knowing either your set-top box or your cable, I would first try
disabling hardware flow control, since the set top probably doesn't implement
it. Essentially your Linux client is waiting for an "OK to send" signal that it
will never receive because there's no physical wire in the set top to send it

<log>
C-A Z L

minicom.cap


={============================================================================
*kt_linux_tool_039* tool-mount

       mount [-fnrsvw] [-t vfstype] [-o options] device dir

       The standard form of the mount command, is

              mount -t type device dir

       This tells the kernel to attach the filesystem found on `device` (which
           is of type `type`) `at` the directory `dir`.  The previous contents
       (if any) and  owner  and  mode  of  dir  become invisible, and as long
       as this filesystem remains mounted, the pathname dir refers to the root
       of the filesystem on device.


{after-change-fstab}
mount -a

       -a, --all
              Mount all filesystems (of the given types) mentioned in fstab
              (except for those whose line contains the noauto keyword).  The
              filesystems are mounted following their order in fstab.


{find-rfs}
[root@HUMAX /]# mount
rootfs on / type rootfs (rw)
/dev/root on / type ext3 (rw,relatime,errors=continue,barrier=1,data=ordered)
proc on /proc type proc (rw,relatime)
...

For the / mount point, you are just told that it corresponds to /dev/root, which
is not the real device you are looking for.

Of course, you can look at the kernel command line and see on which initial root
filesystem Linux was instructed to boot (root parameter):

[root@HUMAX /]# cat /proc/cmdline 
root=/dev/sda1 rw  macaddr=28:32:C5:3F:59:4E bmem=360M@512M wakeup=REBOOT ...


<options>
       -o, --options opts
              Use the specified mount options.  The opts argument is a
              comma-separated list.  For example:

                     mount LABEL=mydisk -o noatime,nodev,nosuid

              For more details, see the FILESYSTEM-INDEPENDENT MOUNT OPTIONS and
              FILESYSTEM-SPECIFIC MOUNT OPTIONS sections.


<when-cannot-run>
./yv-free: Permission denied

is it on a mount point with noexec?

/dev/mapper/sda5 on /var type ext3 (rw,nosuid,nodev,noexec,relatime,errors=continue,barrier=0,data=ordered)

/tmp is a link to /var/tmp

Try

mount -o remount,exec /var

that looks to have done the trick


<busybox-mount>
BusyBox v1.13.1 (2016-07-19 14:09:40 BST) multi-call binary

Usage: mount [flags] DEVICE NODE [-o options,more-options]

Mount a filesystem. Filesystem autodetection requires /proc be mounted.

Options:
        -a              Mount all filesystems in fstab
        -r              Read-only mount
        -t fs-type      Filesystem type
        -w              Read-write mount (default)
-o option:
        loop            Ignored (loop devices are autodetected)
        [a]sync         Writes are asynchronous / synchronous
        [no]atime       Disable / enable updates to inode access times
        [no]diratime    Disable / enable atime updates to directories
        [no]relatime    Disable / enable atime updates relative to modification time
        [no]dev         Allow use of special device files / disallow them
        [no]exec        Allow use of executable files / disallow them
        [no]suid        Allow set-user-id-root programs / disallow them
        [r]shared       Convert [recursively] to a shared subtree
        [r]slave        Convert [recursively] to a slave subtree
        [r]private      Convert [recursively] to a private subtree
        [un]bindable    Make mount point [un]able to be bind mounted
        bind            Bind a directory to an additional location
        move            Relocate an existing mount point
        remount         Remount a mounted filesystem, changing its flags
        ro/rw           Mount for read-only / read-write

There are EVEN MORE flags that are specific to each filesystem
You'll have to see the written documentation for those filesystems


={============================================================================
*kt_linux_tool_039* tool-mount --bind and nfs

The bind mounts.

Since Linux 2.4.0 it is possible to remount part of the file hierarchy somewhere
else. The call is

mount --bind olddir newdir

or shortoption
mount -B olddir newdir

or fstab entry is:
/olddir /newdir none bind

After this call the same contents is accessible in two places. One can also
remount a single file (on a single file). It's also possible to use the bind
mount to create a mountpoint from a regular directory, for example:

mount --bind foo foo

The bind mount call attaches only (part of) a single filesystem, not possible
submounts. The entire file hierarchy including submounts is attached a second
place using

mount --rbind olddir newdir

or shortoption

mount -R olddir newdir

Note that the filesystem mount options will remain the same as those on the
original mount point, and cannot be changed by passing the -o option along with
--bind/--rbind. The mount options can be changed by a separate remount command,
  for example:

mount --bind olddir newdir
mount -o remount,ro newdir

Note that behavior of the remount operation depends on the /etc/mtab file. The
first command stores the 'bind' flag to the /etc/mtab file and the second
command reads  the flag  from  the  file.   If you have a system without the
/etc/mtab file or if you explicitly define source and target for the remount
command (then mount(8) does not read /etc/mtab), then you have to use bind flag
(or option) for the remount command too. For example:

mount --bind olddir newdir
mount -o remount,ro,bind olddir newdir


={============================================================================
*kt_linux_tool_025* tool-mount-fstab cannot execute a file in cdrom

When tried to run a shell script on a cdrom, got "permission denied" even if
it has excutable and run it as a root. The problem was it was mounted as a
read-only. The solution is to edit /etc/fstab to mount it with exec option as:

# /etc/fstab: static file system information.
/dev/sr0        /media/cdrom0   udf,iso9660 user,noauto,exec   0       0

the filesystem is mounted with the noexec option, so the execute permission
bits on all files are ignored, and you cannot directly execute any program
residing on this filesystem. Note that the noexec mount option is implied by
the user option in /etc/fstab. ... If you use user and want to have executable
files, use user,exec.

<ex>
kyoupark@kit-debian:~$ cat /etc/fstab 
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a
# device; this may be used with UUID= as a more robust way to name devices
# that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
# / was on /dev/sda1 during installation
UUID=3f053275-8cc0-4ce8-89c6-412cc9773f1c /               ext4    errors=remount-ro 0       1
# swap was on /dev/sda5 during installation
UUID=64bc1edf-1b58-4e86-b1a9-521c09fbf263 none            swap    sw              0       0
# /dev/sr0        /media/cdrom0   udf,iso9660 user,exec     0       0
/dev/disk/by-uuid/91479737-b91b-4271-87d1-806f084a4c57 /mnt/ext auto nosuid,nodev,nofail,x-gvfs-show 0 0
# nfs
#10.209.62.232:/home/NDS-UK/kyoupark/STB_SW 	/home/kyoupark/bld_STB_SW 	nfs 	rw,sync,hard,intr 	0 0
#10.209.62.232:/home/NDS-UK/kyoupark/si_logs 	/home/kyoupark/bld_si_logs	nfs 	rw,sync,hard,intr 	0 0


={============================================================================
*kt_linux_tool_039* tool-nfs

<install> 
sudo apt-get install nfs-kernel-server
sudo apt-get install nfs-common

note:
Install nfs-common for a client.


<export>
The `two different access modes` of the nfs server. 

The directory /home/client1 is shared in standard mode, so all files written
to this directory are stored as user `nobody` and group `nogroup`. 

For the directory /var/www I use the no_root_squash option which instructs the
nfs server to preserve permissions and ownerships of the files. 

mkdir /home/client1
chown nobody:nogroup /home/client1
chmod 755 /home/client1

mkdir /var/www
chown root:root /var/www
chmod 755 /var/www

Now we must modify /etc/exports where we "export" our NFS shares. We specify
/home/client1 and /var/www as NFS shares and tell NFS to make accesses to
/home/client1 as user nobody.

note
To learn more about /etc/exports, its format and available options, take a
look at `man 5 exports`


// /etc/exports

/home/client1   192.168.0.101(rw,sync,no_subtree_check)
/var/www        192.168.0.101(rw,sync,fsid=0,crossmnt,no_subtree_check,no_root_squash)

note: 
You can replace * with one of the hostname formats. Make the hostname
declaration as specific as possible so unwanted systems cannot access the NFS
mount.

/ubuntu  *(ro,sync,no_root_squash)
/home    *(rw,sync,no_root_squash)


<check-permission>
The no_root_squash option makes that /var/www will be accessed as root.

note:
See the user/group permission.

client:

touch /mnt/nfs/home/client1/test.txt
touch /var/www/test.txt

server:

ls -l /home/client1/
total 0
-rw-r--r-- 1 nobody nogroup 0 Feb 02 16:58 test.txt

ls -l /var/nfs
-rw-r--r-- 1 root root 0 Feb 02 16:58 test.txt


<restart>
To apply the changes in /etc/exports, we restart the kernel nfs server

/etc/init.d/nfs-kernel-server restart
sudo /etc/init.d/nfs-kernel-server stop/start
sudo service nfs-kernel-server restart
sudo service idmapd restart 


root@kit-debian:/home/kit# service nfs-kernel-server status
● nfs-kernel-server.service - LSB: Kernel NFS server support
   Loaded: loaded (/etc/init.d/nfs-kernel-server)
   Active: active (running) since Wed 2016-07-06 12:06:00 BST; 2s ago
  Process: 15454 ExecStop=/etc/init.d/nfs-kernel-server stop (code=exited, status=0/SUCCESS)
  Process: 15461 ExecStart=/etc/init.d/nfs-kernel-server start (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/nfs-kernel-server.service
           └─15489 /usr/sbin/rpc.mountd --manage-gids

Jul 06 12:06:00 kit-debian nfs-kernel-server[15461]: Exporting directories for NFS kernel daemon...exportfs: /etc/exports [1]: Neither 'subtree_check' or 'no_subtree_check' specified for expo.../kit/STB_SW".
Jul 06 12:06:00 kit-debian nfs-kernel-server[15461]: Assuming default behaviour ('no_subtree_check').
Jul 06 12:06:00 kit-debian nfs-kernel-server[15461]: NOTE: this default has changed since nfs-utils version 1.0.x
Jul 06 12:06:00 kit-debian nfs-kernel-server[15461]: .
Jul 06 12:06:00 kit-debian nfs-kernel-server[15461]: Starting NFS kernel daemon: nfsd mountd.
Jul 06 12:06:00 kit-debian rpc.mountd[15489]: Version 1.2.8 starting
Hint: Some lines were ellipsized, use -l to show in full.


<mount>
mount -o nolock -t nfs 172.18.253.143:/home/NDS-UK/parkkt/fob/logs /mnt/temp/
mount -o nolock -t nfs 172.18.200.185:/mnt ./
mount -t nfs 172.18.253.143:/home/NDS-UK/parkkt/fob /mnt/nfs


note: "-o xxx" makes a 'difference' and this was from a board, that is busybox.

http://lists.busybox.net/pipermail/busybox/2002-July/040613.html

// host side
/home/kpark/src-dev     *(rw,sync,no_root_squash,no_subtree_check)

// target side - not work
[root@HUMAX /]# mount -t nfs 172.20.33.215:/home/kpark/src-dev /mnt/tmp
mount: mounting 172.20.33.215:/home/kpark/src-dev on /mnt/tmp failed: Connection refused

// target side - work
[root@HUMAX /]# mount -t nfs -o sync,nolock 172.20.33.215:/home/kpark/src-dev /mnt/tmp
[root@HUMAX /]# ls /mnt/tmp
DEVARCH/         GPATH            GRTAGS           GTAGS            


<check-on-client>
df -h
mount


<check-on-server>
root# showmount -e
Export list for kit-debian:
/home/kit/STB_SW *


<mount-on-fstab>
Instead of mounting the NFS shares manually on the client, you could modify
/etc/fstab so that the NFS shares get mounted automatically when the client
boots. 

// vi /etc/fstab

192.168.0.100:/home/client1  /mnt/nfs/home/client1   nfs      rw,sync,hard,intr  0     0
192.168.0.100:/var/www  /var/www   nfs      rw,sync,hard,intr  0     0

Instead of rw,sync,hard,intr you can use different mount options. To learn
more about available options, take a look at man nfs


To test if your modified /etc/fstab is working, unmount the shares and run
mount -a:

umount /mnt/nfs/home/client1
umount /var/www
mount -a

You should now see the two NFS shares in the outputs of

df -h
mount


<nfs-check-version>
This shows me that my NFS server offers versions 2, 3, and 4 of the NFS
protocol all over UDP and TCP.

$ rpcinfo -p localhost
   program vers proto   port  service
   ...
    100003    2   tcp   2049  nfs
    100003    3   tcp   2049  nfs
    100003    4   tcp   2049  nfs
    100227    2   tcp   2049  nfs_acl
    100227    3   tcp   2049  nfs_acl
    100003    2   udp   2049  nfs
    100003    3   udp   2049  nfs
    100003    4   udp   2049  nfs
    100227    2   udp   2049  nfs_acl
    100227    3   udp   2049  nfs_acl


<user-mapping>

kyoupark@ukstbuild2$ cat /etc/exports
/home/           *(rw,no_root_squash,no_all_squash,sync,insecure)
/tftpboot *(rw,no_root_squash,insecure)


From `man 5 exports`

   User ID Mapping
       `nfsd` bases its access control to files on the server machine on the
       uid and gid provided in each NFS  RPC  request.  The  normal behavior a
       user would expect is that she can access her files on the server just
       as she would on a nor‐ mal file system. This requires that the same
       uids and gids are used on the client and the server machine.  This  is
       not always true, nor is it always desirable.

       Very often, it is not desirable that the root user on a client machine
       is also treated as root when accessing files on the NFS server. 
       
       To this end, uid 0 is normally mapped to a different id: the so-called
       anonymous or `nobody`  uid.  This mode of operation (called `root
           squashing') is the `default`, and can be turned off with
       no_root_squash.

       By default, exportfs chooses a uid and gid of 65534 for squashed
       access. These values can also be overridden by the `anonuid` and
       `anongid` options.  Finally, you can map all user  requests  to  the
       anonymous  uid  by  specifying  the `all_squash` option.

       Here's the complete list of mapping options:

       root_squash
              Map  requests  from  uid/gid  0 to the anonymous uid/gid. Note
              that this does `not` apply to any other uids or gids that might be
              equally sensitive, such as user bin or group staff.

       no_root_squash
              Turn off root squashing. This option is mainly useful for
              diskless clients.

       all_squash
              Map all uids and gids to the anonymous user. Useful for
              NFS-exported  public  FTP  directories,  news  spool
              directories, etc. The opposite option is `no_all_squash`, which is
              the `default` setting.


<idmapd>
http://serverfault.com/questions/514118/mapping-uid-and-gid-of-local-user-to-the-mounted-nfs-share

// note:
// Don't needs to do for my case. That's probably because have matched uid and
// gid between two hosts.
//
// This is what idmapping is suppose to do. First of all, enable is on the
// client and server:
// 
// # echo N > /sys/module/nfs/parameters/nfs4_disable_idmapping
// 
// clean idmap cache and restart idmap daemon:
// 
// # nfsidmap -c 
// # service rpcidmapd restart


// note:
// This works.

On your nfs client, edit /etc/idmapd.conf and change

[Mapping]

Nobody-User = myappuser
Nobody-Group = myappuser


={============================================================================
*kt_linux_tool_040* install

{case}
See a case that the build output, executable, does not match with the build time. In this example,
    there are 'build-root' and 'install-root' where build-root has all temporary files from the
    build and includes the final binary and 'install-root' has only the final binary. 

However, found that when do a build, a timestamp of install-root is older than the build one. So
wondered if the build system is broken and the build is not populated to the install-root.

Found that that's due to install command but not the build system.

/usr/bin/install -C {build-root}/nexus-inspect {install-root}/nexus-inspect

-C, --compare
compare each pair of source and destination files, and in some cases, do not modify the destination
at all

That is the install-root will not copied if two are the same.


={============================================================================
*kt_linux_tool_041* notify-send

wget URL && notify-send "Done" || notify-send "Failed"

notify-send --hint=int:transient:1 -i
/usr/share/icons/gnome/32x32/status/stock_dialog-warning.png 'Building
zb/DEVARCH-8092-TEMP: failure'


={============================================================================
*kt_linux_tool_042* tool-less

$ nm MW_Process | less -N +4213


={============================================================================
*kt_linux_tool_042* tool-tail: print multiple files with filename

$ grep "" *-config
default-system-factory.plugin-config:1:libNickelSystemDbusClient.so createDbusSystemFactory
http-application%2Fdash%2Bxml.plugin-config:1:libNickelSystemGstreamer.so createGstSystemFactory
https-application%2Fdash%2Bxml.plugin-config:1:libNickelSystemGstreamer.so createGstSystemFactory

$ tail -n +1 *-config
==> default-proxy-config <==

==> default-system-factory.plugin-config <==
libNickelSystemDbusClient.so createDbusSystemFactory

==> http-application%2Fdash%2Bxml.plugin-config <==
libNickelSystemGstreamer.so createGstSystemFactory

==> https-application%2Fdash%2Bxml.plugin-config <==
libNickelSystemGstreamer.so createGstSystemFactory

       -n, --lines=K
              output the last K lines, instead of the last 10; or use -n +K to
              output lines starting with the Kth

<f-option>
       -f, --follow[={name|descriptor}]
              output appended data as the file grows; -f, --follow, and
              --follow=descriptor are equivalent


={============================================================================
*kt_linux_tool_044* mc

https://www.midnight-commander.org/

apt-get install mc


={============================================================================
*kt_linux_tool_045* graphbiz

sudo apt-get install graphviz

dot -Tjpeg gst-launch.PLAYING_PAUSED.dot -o gst-launch.PLAYING_PAUSED.jpg


={============================================================================
*kt_linux_tool_046* mktemp

Create  a  temporary  file  or  directory,  safely,  and  print its name.  TEMPLATE must contain at
least 3 consecutive `X's in last component.  If TEMPLATE is not specified, use tmp.XXXXXXXXXX, and
--tmpdir is implied.  Files are created u+rw, and directories u+rwx, minus umask restrictions.


$ mktemp
/tmp/tmp.eezueA0X5X


={============================================================================
*kt_linux_tool_047* tool-env

env - run a program in a modified environment

LD_PRELOAD=/usr/local/lib/libdirectfb.so:/usr/local/lib/libdirect.so:/usr/local/lib/libinit.so

env ${LD_PRELOAD} nickelmediad \
    --no-mediasettings --no-localmedialibrary \
    --no-outputmanager --no-servicelistbuilder \
    -b $BUS_NAME -f $cfg &


={============================================================================
*kt_linux_tool_048* tool-dircolors

{get-color}
For bash, copy /etc/DIR_COLORS into home as .dir_colors and edit it to change
default values. Run man dir_colors for help.

IMPORTANT NOTE FOR TERMINAL USERS:

If you are going to use Solarized in Terminal mode (i.e. not in a GUI version
like gvim or macvim), please please please consider setting your terminal
emulator's colorscheme to used the Solarized palette. I've included palettes
for some popular terminal emulator as well as Xdefaults in the official
Solarized download available from Solarized homepage. If you use Solarized
without these colors, Solarized will need to be told to degrade its
colorscheme to a set compatible with the limited 256 terminal palette (whereas
by using the terminal's 16 ansi color values, you can set the correct,
specific values for the Solarized palette).

http://www.webupd8.org/2011/04/solarized-must-have-color-paletter-for.html

{dircolors}

#############################################################################
# Below are the color init strings for the basic file types. A color init
# string consists of one or more of the following numeric codes:
#
# Attribute codes:
#   00=none 01=bold 04=underscore 05=blink 07=reverse 08=concealed
# Text color codes:
#   30=black 31=red 32=green 33=yellow 34=blue 35=magenta 36=cyan 37=white
# Background color codes:
#   40=black 41=red 42=green 43=yellow 44=blue 45=magenta 46=cyan 47=white
#
# NOTES:
# - See http://www.oreilly.com/catalog/wdnut/excerpt/color_names.html
# - Color combinations
#   ANSI Color code       Solarized  Notes                Universal             SolDark              SolLight
#   ~~~~~~~~~~~~~~~       ~~~~~~~~~  ~~~~~                ~~~~~~~~~             ~~~~~~~              ~~~~~~~~
#   00    none                                            NORMAL, FILE          <SAME>               <SAME>
#   30    black           base02
#   01;30 bright black    base03     bg of SolDark
#   31    red             red                             docs & mm src         <SAME>               <SAME>
#   01;31 bright red      orange                          EXEC                  <SAME>               <SAME>
#   32    green           green                           editable text         <SAME>               <SAME>
#   01;32 bright green    base01                          unimportant text      <SAME>
#   33    yellow          yellow     unclear in light bg  multimedia            <SAME>               <SAME>
#   01;33 bright yellow   base00     fg of SolLight                             unimportant non-text
#   34    blue            blue       unclear in dark bg   user customized       <SAME>               <SAME>
#   01;34 bright blue     base0      fg in SolDark                                                   unimportant text
#   35    magenta         magenta                         LINK                  <SAME>               <SAME>
#   01;35 bright magenta  violet                          archive/compressed    <SAME>               <SAME>
#   36    cyan            cyan                            DIR                   <SAME>               <SAME>
#   01;36 bright cyan     base1                           unimportant non-text                       <SAME>
#   37    white           base2
#   01;37 bright white    base3      bg in SolLight
#   05;37;41                         unclear in Putty dark


// since light is better
// wget --no-check-certificate https://raw.github.com/seebi/dircolors-solarized/master/dircolors.ansi-dark
// mv dircolors.ansi-dark .dircolors
// eval `dircolors ~/.dircolors`

wget --no-check-certificate https://raw.github.com/seebi/dircolors-solarized/master/dircolors.ansi-light
mv dircolors.ansi-light .dircolors
eval `dircolors ~/.dircolors`


$ man dircolors

NAME
       dircolors - color setup for ls

       -p, --print-database
              output defaults

note:
No /etc/DIR_COLORS in debian.

As for where dircolors gets its settings from, when you don't specify a file
it just uses some builtin defaults.


{gonme}
git clone https://github.com/sigurdga/gnome-terminal-colors-solarized.git
cd gnome-terminal-colors-solarized

" can set it to light or dark using the following commands:

./set_dark.sh
./set_light.sh


{putty}
https://github.com/altercation/solarized/tree/master/putty-colors-solarized

PuTTY Tray can store sessions in text files as opposed to the Windows
registry. To modify an existing session to use Solarized colors, open the file
in a text editor and replace the lines beginning with Colour## (0-21) with the
version from solarized_dark_puttytray.txt or solarized_light_puttytray.txt.

https://puttytray.goeswhere.com/

Saves a session as a text which is made under "session" from where putty runs.

// solarized/putty-colors-solarized/solarized_light_puttytray.txt

Colour21\253,246,227\
Colour20\238,232,213\
Colour19\147,161,161\
Colour18\42,161,152\
Colour17\108,113,196\
Colour16\211,54,130\
Colour15\131,148,150\
Colour14\38,139,210\
Colour13\101,123,131\
Colour12\181,137,0\
Colour11\88,110,117\
Colour10\133,153,0\
Colour9\203,75,22\
Colour8\220,50,47\
Colour7\0,43,54\
Colour6\7,54,66\
Colour5\101,123,131\
Colour4\238,232,213\
Colour3\238,232,213\
Colour2\253,246,227\
Colour1\88,110,117\
Colour0\101,123,131\

// original value

Colour21\255,255,255\
Colour20\187,187,187\
Colour19\85,255,255\
Colour18\0,187,187\
Colour17\255,85,255\
Colour16\187,0,187\
Colour15\85,85,255\
Colour14\0,0,187\
Colour13\255,255,85\
Colour12\187,187,0\
Colour11\85,255,85\
Colour10\0,187,0\
Colour9\255,85,85\
Colour8\187,0,0\
Colour7\85,85,85\
Colour6\0,0,0\
Colour5\0,255,0\
Colour4\0,0,0\
Colour3\85,85,85\
Colour2\0,0,0\
Colour1\255,255,255\
Colour0\187,187,187\


={============================================================================
*kt_linux_tool_048* tool-tput

NAME
       tput, reset - initialize a terminal or query terminfo database

<ex>

# Definitions for bold/normal formatting.  Used to prettify the output.
bold=$(tput bold)
normal=$(tput sgr0)

    echo ""
    echo "${bold}********************************************${normal}"
    echo "${bold}** Find Latest Uploads from MAC Addresses **${normal}"
    echo "${bold}********************************************${normal}"


={============================================================================
*kt_linux_tool_048* tool-gnome-terminal

{gnome-terminal}
I have just found that you can open new terminals using 'gnome-terminal'.  You can open multiple
windows and multiple tabs like this:

gnome-terminal --window --tab --window --tab --tab

<key-shortcuts>
New Tab        Shift+Ctrl+T
Close Tab      Shift+Ctrl+W

note: this is to open new terminal
New Window     Shift+Ctrl+N
Close Window   Shift+Ctrl+Q

Copy           Ctrl+Shift+C
Paste          Ctrl+Shift+V

Switch to Previous Tab     Ctrl+Page Up
Switch to Next Tab         Ctrl+Page Down

note: this is to 'move' a tab
Move Tab to the Left       Shift+Ctrl+Page Up
Move Tab to the Right      Shift+Ctrl+Page Down

Switch to Tab 1            Alt+1
Switch to Tab N            Alt+N

<page-up>
Shift+PgUp/PgDn/Home/End will scroll in gnome-terminal and Terminal.

<find>
Using the Search menu or a keyboard short-cut Shift+Ctrl+F


={============================================================================
*kt_linux_tool_049* tool-terminator

https://launchpad.net/terminator

./setup.py install --record=install-files.txt

For more keyboard shortcuts and also the command line options, please see the
manpage "terminator". For configuration options, see the manpage
"terminator_config".

/home/keitee/.config/terminator/config

// from terminator_config

keybindings

<open-close>
new_tab
Open a new tab.  Default value: <Ctrl><Shift>T

close_term
Close the current terminal.  Default value: <Ctrl><Shift>W

close_window
Quit Terminator.  Default value: <Ctrl><Shift>Q

new_window
Open a new Terminator window as part of the existing process.  Default value: <Ctrl><Shift>I

new_terminator
Spawn a new instance of Terminator.  Default value: <Super>i


<switch-tab>
next_tab
Move to the next tab.  Default value: <Ctrl>Page_Down

prev_tab
Move to the previous tab.  Default value: <Ctrl>Page_Up


<switch-termial>
about terminals in a tab

go_up  
Move cursor focus to the terminal above.  Default value: <Alt>Up

go_down
Move cursor focus to the terminal below.  Default value: <Alt>Down

go_left
Move cursor focus to the terminal to the left.  Default value: <Alt>Left

go_right
Move cursor focus to the terminal to the right.  Default value: <Alt>Right


<split-terminal>
Q: how to change the size of split?. Use resize instead?

split_horiz
Split the current terminal horizontally.  Default value: <Ctrl><Shift>O

split_vert
Split the current terminal vertically.  Default value: <Ctrl><Shift>E


<resize-terminal>
resize_up
Move the parent dragbar upwards. value: <Ctrl>Up

resize_down
Move the parent dragbar downwards. value: <Ctrl>Down

resize_left
Move the parent dragbar left.

resize_right
Move the parent dragbar right.


<copy-paste>
copy   
Copy the currently selected text to the clipboard.  Default value: <Ctrl><Shift>C

paste  
Paste the current contents of the clipboard.  Default value: <Ctrl><Shift>V


<search>
Q: shows if there is match or not but not highlight or scroll.

search 
Search for text in the terminal scrollback history.  Default value: <Ctrl><Shift>F


<zoom>
toggle_zoom
Zoom/Unzoom the current terminal to fill the window.  Default value: <Ctrl><Shift>X


<reset>
reset_clear
Reset the terminal state and clear the terminal window.  Default value: <Ctrl><Shift>G


={============================================================================
*kt_linux_tool_050* tool-watch

$watch -n1 -d ccache -s

NAME
       watch - execute a program periodically, showing output fullscreen

SYNOPSIS
       watch [options] command

DESCRIPTION
       watch runs command repeatedly, displaying its output and errors (the
               first screenfull).  This allows you to watch the program output
       change over time.  By default, the program is run every 2 seconds.  By
       default, watch will run until interrupted.

OPTIONS
       -d, --differences [permanent] Highlight the differences between
       successive updates.  Option will read optional argument that changes
       highlight to be permanent, allowing to see what has changed at least once
       since first iteration.

       -n, --interval seconds Specify update interval.  The command will not
       allow quicker than 0.1 second interval, in which the smaller values are
       converted.


={============================================================================
*kt_linux_tool_051* tool-github set-github

git@github.com:/keitee/kb.git

git clone git://github.com/keitee/kb.git
git clone git://github.com/keitee/vim.git


={============================================================================
*kt_linux_tool_051* tool-font set-font

Copy a font file in the directory /usr/share/fonts/truetype (for all users) or
~/.fonts (for a specific user). 

// Remember to verify font's permissions on disk (777); if you save your fonts in
// $ chmod -R 777 /usr/local/share/fonts

fc-list       // lists fonts
fc-cache -fv  // rebuilds cached list of fonts 

http://sourcefoundry.org/hack/


={============================================================================
*kt_linux_tool_051* tool-echo

-e     enable interpretation of backslash escapes

If -e is in effect, the following sequences are recognized:

\e     escape

echo -e "\E[1;31mThe commit subject does not match \"DEVARCH-xxxx: <subject>\"";

However, "\E" also works.


<busybox>
echo

    echo [-neE] [ARG...]

    Print the specified ARGs to stdout

    Options:

            -n      Suppress trailing newline
            -e      Interpret backslash-escaped characters (i.e., \t=tab)
            -E      Disable interpretation of backslash-escaped characters

echo -e "\E[1;31mThe commit subject does not match \"DEVARCH-xxxx: <subject>\"";


={============================================================================
*kt_linux_tool_051* tool-echo-compile

echo -e "#include <utility>\nint main() { const char* const null_ptr = 0; std::pair<bool, const char*> x(false, null_ptr); return 0; }" | g++ -Wall -std=c++0x -x c++ -o foo -


={============================================================================
*kt_linux_tool_051* tool-tcpdump

Tcpdump can capture network activity and log it to a file for analysis. If we
can compile it for MIPS, we can run it on a set top box and even capture
packets from a Verifier callback. This has been used for debugging UAM and
SSP.

You can download it from http://www.tcpdump.org/

There's a good chance these instructions will work for other software that
uses autoconf. 


For the MIPS based box (such as the AMS890), we'll make a variable called MIPS
that points to the toolchain. You can find it in a Clearcase view. Make sure
this is an absolute path. Then we set up a bunch of other variables that are
used by configure and make.

export MIPS=/home/fisherr/rf890/vobs/SYSTEM_BIN_2/mips4k_gcc_x86_linux_hound
export CC=$MIPS/bin/mips-linux-gcc
export PATH=$MIPS/bin:$PATH
export AR=$MIPS/bin/mips-linux-ar
export LDFLAGS=" -s -Xlinker -rpath /lib -Xlinker -rpath-link $MIPS/lib/gcc/mips-linux/4.2.0"
export ac_cv_linux_vers=2

*mips-endian*
For the MIPSel (little endian) boxes (such as the Fusion product box used for
    component tests, configure the environment as such:

export MIPS=/home/fisherr/rf890/vobs/SYSTEM_BIN/mips4k_gcc_x86_linux_03
export CC=$MIPS/bin/mipsel-linux-gcc
export PATH=$MIPS/bin:$PATH
export AR=$MIPS/bin/mipsel-linux-ar
export LDFLAGS=" -s -Xlinker -rpath /lib -Xlinker -rpath-link $MIPS/lib/gcc/mipsel-linux/4.2.0"
export ac_cv_linux_vers=2


libpcap

cd libpcap-1.1.1
./configure --host=mips --with-pcap=linux
make


tcpdump

The configure script will find the libpcap you just built if it is in an
adjacent directory.

cd ../tcpdump-4.1.1/
./configure --host=mips
make

Using NFS, mount the filesystem containing your MIPS tcpdump executable. Use
the "any" interface to make sure you capture even packets routed to the modem
over PPP. You can log to a file and then view the file in `wireshark`.

/mnt/nfs/tcpdump/tcpdump-4.1.1/tcpdump -i any -w /mnt/nds/disk/FSN_DATA/TCPLOG


={============================================================================
*kt_linux_tool_051* tool-wireshark

$ sudo wireshark


filter the response to a matched HTTP request

https://ask.wireshark.org/questions/30972/filter-the-response-to-a-matched-http-request

you can do this:

  Filter for the request: http.request.uri contains "/test"
  http.request.uri contains TT.mpd

  Get the TCP stream number(s) of those frames (tcp.stream)
  Field name is tcp.stream and displayed as "Stream index:"

  Then filter for: tcp.stream eq xxx and frame contains "HTTP/1.1 200 OK" (or
      HTTP/1.0)


You can automate that with tshark and some scripting.

tshark -nr input.pcap -R 'http.request.uri contains "/test"' -T fields -e tcp.stream
Read the tcp streams with a script and create new filters based on them
tshark -nr input.pcap -R 'tcp.stream eq xxx and frame contains "HTTP/1.1 200 OK"'

See also my answer to a similar question


={============================================================================
*kt_linux_tool_051* tool-swapon

http://www.tecmint.com/commands-to-monitor-swap-space-usage-in-linux/

NAME
       swapon, swapoff - enable/disable devices and files for paging and
       swapping

DESCRIPTION
       swapon is used to specify devices on which paging and swapping are to
       take place.

       The device or file used is given by the specialfile parameter. It may
       be of the form -L label or -U uuid to indicate a device by label or
       uuid.

       Calls  to  swapon  normally  occur `in the system boot scripts` making
       all swap devices available, so that the paging and swapping activity is
       interleaved across several devices and files.

       -a, --all
              All  devices  marked  as  ‘‘swap’’  in /etc/fstab are made
              available, except for those with the ‘‘noauto’’ option.  Devices
              that are already being used as swap are silently skipped.

       -s, --summary
              Display swap usage summary by device. Equivalent to "cat
              /proc/swaps".  Not available before Linux 2.1.25.


<setup-swap>
9. Setting Up Swap Space

9.1. Swap Files

Normally, there are only two steps to setting up swap space, creating the
partition and adding it to /etc/fstab. A typical fstab entry for a swap
partition at /dev/hda6 would look like this:

/dev/hda6	swap	swap	defaults	0	0


The next time you reboot, the initialization scripts will activate it
automatically and there's nothing more to be done.

However, if you want to make use of it right away, you'll need to activate it
maually. As root, type:

mkswap -f /dev/hda6
swapon /dev/hda6


={============================================================================
*kt_linux_tool_051* tool-htop

http://www.thegeekstuff.com/2011/09/linux-htop-examples | Top on Steroids  15 Practical Linux HTOP Examples


={============================================================================
*kt_linux_tool_051* tool-xterm: unknown terminal type

http://blog.timmattison.com/archives/2012/04/12/tip-fix-xterm-unknown-terminal-type-messages-in-debian/

This one has been a bit of a nuisance on newly spooled up Debian instances for
me lately. When I try to run "top" or "clear" or really anything that does
something with the terminal I get the following message:

'xterm': unknown terminal type.

This is because either you haven't installed ncurses-term (unlikely) or a
symlink from /lib/terminfo/x/xterm to /usr/share/terminfo/x/xterm is missing. To
cover all possibilities do this:

sudo apt-get install ncurses-term
sudo ln -s /lib/terminfo/x/xterm /usr/share/terminfo/x/xterm

Poof, your terminal works again!


={============================================================================
*kt_linux_tool_051* tool-lpstat: view and cancel print jobs

Command line:

lpstat -o

to view outstanding print jobs.

cancel -a {printer}

to cancel ALL jobs or ...

cancel {printerjobid}

to cancel 1 job.

$ lpstat -o
SydneyColour-233        kpark          5653504   Tue 05 Jan 2016 07:59:13 UTC
SydneyColour-234        kpark             1024   Tue 05 Jan 2016 08:09:21 UTC
$ cancel -a SydneyColour    
$ lpstat -o
$ 


={============================================================================
*kt_linux_tool_051* tool-i3wm:

https://i3wm.org/


={============================================================================
*kt_linux_tool_051* tool-hexdump:

$ hexdump -C -n 100 DTT_Radio1_20150225_1603.ts

     -C          Canonical hex+ASCII display.  Display the input offset in
     hexadecimal, followed by sixteen space-separated, two column, hexadecimal
     bytes, followed by the same sixteen bytes in %_p format enclosed in ‘‘|’’
     characters.

     -n length   Interpret only length bytes of input.


={============================================================================
*kt_linux_tool_051* tool-uptime

/opt/zinc/oss/bin/busybox uptime; /opt/zinc/oss/bin/busybox lsof | wc -l


={============================================================================
*kt_linux_tool_051* tool-lsof

/opt/zinc/oss/bin/busybox uptime; /opt/zinc/oss/bin/busybox lsof | wc -l


={============================================================================
*kt_linux_tool_051* tool-dirname tool-basename tool-readlink

*tool-readlink*
readlink - print value of a symbolic link or canonical file name


*tool-dirname*
dirname - strip last component from file name 

<ex>
Suppose that the script is in PATH, can run that anywhere, and $0 will be
fullpath where the script exist whereever you run.

This code is to get the real path of where the script exist if it has a
symbloc link and add it to PATH.

MYPATH=$(readlink -f ``dirname $0``)
PATH=$MYPATH:$PATH


*tool-basename*

DESCRIPTION

Print NAME with any leading directory components removed. If 'specified', also
remove a trailing SUFFIX.

$ basename /home/kt/kb
kb
$ basename cfgversion.out  
cfgversion.out

// remove traling

$ basename cfgversion.out .out
cfgversion


={============================================================================
*kt_linux_tool_051* tool-last

last command will give login history for a specific username. If we don’t give
any argument for this command, it will list login history for all users. By
default this information will read from /var/log/wtmp file. The output of this
command contains the following columns:


={============================================================================
*kt_linux_tool_051* tool-fdisk

       -l, --list
              List the partition tables for the specified devices and then
              exit.  If no devices are given, those mentioned in
              /proc/partitions (if that file exists) are used.

       -u     When listing partition tables, give sizes in sectors instead of
              cylinders.


fdisk -lu

Disk /dev/sda: 2000.3 GB, 2000398934016 bytes
255 heads, 63 sectors/track, 243201 cylinders, total 3907029168 sectors
Units = sectors of 1 * 512 = 512 bytes

   Device Boot      Start         End      Blocks  Id System
/dev/sda1              64  3907007999  1953503968   1 FAT12


<set-swap>
Command (m for help): t
Partition number (1-3, default 3): 3
Hex code (type L to list all codes): L

 1  FAT12           27  Hidden NTFS Win 82  Linux swap / So c1  DRDOS/sec (FAT-

Hex code (type L to list all codes): 82

Changed type of partition 'Linux' to 'Linux swap / Solaris'.

Command (m for help): p
Disk /dev/sda: 30.5 TiB, 33554432000000 bytes, 65536000000 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x4d92b346

Device     Boot    Start      End  Sectors  Size Id Type
/dev/sda1  *        2048 62916607 62914560   30G 83 Linux
/dev/sda2       62916608 64350207  1433600  700M 82 Linux swap / Solaris
/dev/sda3       64350208 85321727 20971520   10G 82 Linux swap / Solaris


={============================================================================
*kt_linux_tool_051* tool-fs-lvm

http://www.howtogeek.com/howto/40702/how-to-manage-and-use-lvm-logical-volume-management-in-ubuntu/


={============================================================================
*kt_linux_tool_051* tool-fs-btrfs

https://btrfs.wiki.kernel.org/index.php/Main_Page

Btrfs is a new copy on write (CoW) filesystem for Linux aimed at implementing
advanced features while focusing on fault tolerance, repair and easy
administration. Jointly developed at multiple companies, Btrfs is licensed under
the GPL and open for contribution from anyone. Not too many companies have said
that they are using Btrfs in production, but we welcome those who can say so on
the production users page.

The main Btrfs features available at the moment include:

Writable snapshots, read-only snapshots 
Efficient Incremental Backup

Does it support SSD optimizations?

    Yes, Debian Jessie and later automatically detect non-rotational hard disks
    and ssd is added to the btrfs mount options. For more details on using SSDs
    with Debian, refer to SSDOptimization. 
    https://wiki.debian.org/SSDOptimization

$ sudo apt-get install btrfs-tools


={============================================================================
*kt_linux_tool_051* tool-add-user tool-sudo

<sudoer>
apt-get install sudo
su
adduser <username> sudo
re-login


<groupadd>
       The groupadd command creates a new group account using the values
       specified on the command line plus the default values from the system.
       The new group will be entered into the system files as needed.


       -g, --gid GID

           The numerical value of the group's ID. This value must be unique,
unless the -o option is used. The value must be non-negative. The default is
  to use the smallest ID value greater than or equal to GID_MIN and greater
  than every other group.

$ sudo groupadd --gid 16777244 ccusers

or

$ sudo gpasswd -a ${USER} docker


Want to change user 'peter' to 'paul'.

$ usermod -d /home/paul -m -g paul -l paul peter

<usermod>
       The usermod command modifies the system account files to reflect the
       changes that are specified on the command line.

       -d, --home HOME_DIR
           The user's new login directory.

           If the -m option is given, the contents of the current home
           directory will be moved to the new home directory, which is created
           if it does not already exist.


       -g, --gid GROUP
           The group name or number of the user's new initial login group. The
           group `must exist`

           Any file from the user's home directory owned by the previous
           primary group of the user will be owned by this new group.

           The group ownership of files outside of the user's home directory
           must be fixed manually.

       -m, --move-home
           Move the content of the user's home directory to the new location.

           This option is only valid in combination with the -d (or --home)
           option.

           usermod will try to adapt the ownership of the files and to copy
           the modes, ACL and extended attributes, but manual changes might be
           needed afterwards.

       -l, --login NEW_LOGIN
           The name of the user will be changed from LOGIN to NEW_LOGIN.
           Nothing else is changed.  In particular, the user's home directory
           or mail spool should probably be renamed manually to reflect the
           new login name.

           note: this is to change username

<adduser> 
       useradd is a low level utility for adding users. On `Debian`,
administrators should usually use adduser(8) instead.

       adduser  and  addgroup  add users and groups to the system according to
       command line options and configuration information in
       /etc/adduser.conf.  They are friendlier front ends  to  the low level
       tools like useradd, groupadd and usermod programs, by default choosing
       Debian pol‐ icy conformant UID and GID values, creating a home
       directory  with  skeletal  configuration, running a custom script, and
       other features.  adduser and addgroup can be run in one of five modes:

// add user
$ sudo adduser kyoupark

// add user into a group
$ sudo adduser kyoupark ccusers


<deluser>
       deluser and delgroup remove users and groups from  the  system
       according  to  command  line options  and configuration information in
       /etc/deluser.conf and /etc/adduser.conf.  They are friendlier front
       ends to the userdel and groupdel programs, removing the home  directory
       as option  or  even  all  files on the system owned by the user to be
       removed, running a custom script, and other features.  deluser and
       delgroup can be run in one of three modes:

       deluser  [options]  [--force]  [--remove-home]  [--remove-all-files]
       [--backup] [--backup-to DIR] user

       deluser --group [options] group
       delgroup [options] [--only-if-empty] group

       deluser [options] user group

<sudo>
As an alternative to using the root account directly, users can be allowed to
temporarily give their own accounts Super User privileges using the sudo
command. Sudo provides specified users or groups of users with limited root
access. Users assume root privileges with sudo using their personal passwords,
and any activity performed is logged by the system logger (systemlogd), as are
  unsuccessful attempts to gain sudo privileges.

Access to the sudo command is policy based, with the /etc/sudoers containing
the sudo policy for a system. The visudo command opens the /etc/sudoers policy
file in a vi text editor, while this is indeed possible, it is NOT
recommended. Using visudo locks the file for edition and ensures only one
person is making changes to it at the same time.

The /etc/sudoers files contains many sane defaults, and a number of common or
useful example sudo policies. Here are some of the important concepts in the
sudoers file:

Concept: Command aliases

Cmnd_Alias SERVICES = /sbin/service, /sbin/chkconfig

In this example, an alias is declared for a group of commands, in this case, 2
service related commands. Grouping multiple commands into an alias allows the
administrator to give users or groups access to a number of commands at once.

Concept 2: Sudo policy

%wheel (ALL)=(ALL) ALL

In this example, members of the wheel group are allowed to perform any
command. The policy statement has 4 parts.

%wheel: The group or user to whom the policy applies. If the policy is
%specific to a single
user, the % is omitted.

(ALL): The first (ALL) in this example is `the host` where the policy applies.
       It could also be localhost, or other hosts that use this sudoers file
       to determine sudo policy.

(ALL): The second (ALL) in this example is `the user` with whose permissions any
       commands affected by the policy will be run. (ALL) means that any
       command affected by this policy can be run with the permissions of any
       user. If no user or group of users is specified, the root user is
       assumed.

ALL: The third ALL in this example, with no brackets, are the commands
  affected by this policy. In this case, all commands are affected, but a
  command alias or comma separated list of commands could also have been used.

<ex>
1. Create a command alias for the commands involved in creating RAID arrays.
2. Create a command alias for user administration that includes commands for
adding and removing users.

3. Create a policy that allows members of the users group to run the RAID
commands as your user on localhost.

4. Create a policy that allows members of the users group to run the user
administration commands as root on localhost.

Answer Key

1. Cmnd_Alias RAIDSTUFF = /usr/sbin/mdadm, /usr/sbin/parted
2. Cmnd_Alias USERSTUFF = /usr/sbin/useradd, /usr/sbin/userdel

3. %users localhost=USERNAME ALL RAIDSTUFF
Where USERNAME is your username.

4. %users localhost=USERSTUFF


={============================================================================
*kt_linux_tool_051* tool-docker

<not-a-vm>
Use debian as the container OS just for kicks (it's also smaller than ubuntu)

docker run -i -t debian /bin/bash

will present you after some downloading with the following

root@482791e6cc1a: 

A root shell in the container. Yay.

now uname `still gives ubuntu`, because this is a container and not a true VM..

root@482791e6cc1a: uname -a
Linux ubuntu-14 3.13.0-32-generic #57-Ubuntu SMP Tue Jul 15 03:51:08 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux


docker run creates a container, configures it and runs a command in it. You
need to run to configure a container, every run creates a new container. So a
lot of containers will be generated. Observe them with `docker ps -a` You will
want to periodically clean up all the cruft with `docker rm`


A container is not a VM, it doesn't start any services or run init.d, as far
as I can tell. The command you give it is crucial. Think of it more like an
application wrapper, than a VM.

When you need to reconfigure a container, you need to "commit" it. That
creates an image out of the container, that can then be reconfigured with a
new run command. There is no other way. For a container, especially with
networking configured, expect to do a lot of commits and runs.


<attach>
Use CTRL-P CTRL-Q to step out of the container shell and keep the shell
running. Reattach with docker attach

kyoupark@kit-debian64:~$ sudo docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
3591df06b6f1        debian              "/bin/bash"         10 minutes ago      Up 10 minutes                           lonely_williams

kyoupark@kit-debian64:~$ sudo docker attach 3591df06b6f1
root@3591df06b6f1:/# 


={============================================================================
*kt_linux_tool_051* tool-virtualbox

{graphic-issue}
When see a problem with NVIDIA, need to install a driver for ubuntu manually:

http://askubuntu.com/questions/141606/how-to-fix-the-system-is-running-in-low-graphics-mode-error

In short, get console and do:

sudo apt-get install nvidia-current

Also, install the guest addition:

Click on Install Guest Additions from the Devices menu and all will be done automatically.


{sharing-between-os}
http://www.virtualbox.org/manual/ch04.html#sharedfolders

# set sharing folder on the host using virtual box menu and reboot
#
select "Shared folders" from the "Devices" menu, or click on the folder icon on the status bar in
the bottom right corner.

# shared folder is
#
/media/sf_myfiles 

# add group permission when see access error when reading shared folder
#
Access to auto-mounted shared folders is only granted to the user group vboxsf, which is created by
the VirtualBox Guest Additions installer. Hence guest users have to be member of that group to have
read/write access or to have read-only access in case the folder is not mapped writable.

sudo usermod -a -G vboxsf {username}


{sharing-using-samba}
<1> Have samba installation, setting, and user. This is simple and general and see as an example.
http://www.sitepoint.com/ubuntu-12-04-lts-precise-pangolin-file-sharing-with-samba/

sudo apt-get install samba samba-common system-config-samba winbind

change winbind setting

For samba server settings: Workgroup. (this is domain) This field should be the same value as that
used by your Windows Workgroupi.e if your WIndows Users are members of the 'Home' workgroup, type
'Home' in this field.  

For samba users: use windows user name.


2. By default, VB uses Host-only networking and this means host cannot see
guest. To enable for host to see guest, change to `bridged network` as shown
in:

6.4. Bridged networking
http://www.virtualbox.org/manual/ch06.html

So change it in setting menu of VB and restart VM. Get IP and can access guest
from host windows.

note: must have two network adaptors: NAT for internet and bridged for samba
between host and guest.  But cannot ssh to other host. seems firewall problem
and when back to host only network, ssh works but not sharing as host only net
gives private net ip.

note: must have two network adaptors: NAT for internet and bridged for samba
between host and guest.  But cannot ssh to other host. seems firewall problem
and when back to host only network, ssh works but not sharing as host only net
gives private net ip.


{resize}
C:\Program Files\Oracle\VirtualBox>VBoxManage.exe modifymedium \
    "C:\Users\kyoupark\VirtualBox VMs\debian\debian.vdi" --resize 92106
0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%
C:\Program Files\Oracle\VirtualBox>


{addition}
/tmp/vbox.0/Makefile.include.header:97: *** Error: unable to find the sources
of your current Linux kernel. Specify KERN_DIR= and run Make again.  Stop.

$ sudo apt-get install linux-headers-$(uname -r)
// needs to install gcc if there is not
$ bash VBoxLinuxAdditions.run


={============================================================================
*kt_linux_tool_051* tool-cron

http://www.unixgeeks.org/security/newbie/unix/cron-1.html

Cron is the name of program that enables unix users to execute commands or
scripts (groups of commands) automatically at a specified time/date. It is
normally used for sys admin commands, like makewhatis, which builds a search
database for the man -k command, or for running a backup script, but can be
used for anything. A common use for it today is connecting to the internet and
downloading your email.

This file will look at Vixie Cron, a version of cron authored by Paul Vixie.


The fields are:

minute hour dom month dow user cmd

minute	
This controls what minute of the hour the command will run on, and is between
'0' and '59'

hour	
This controls what hour the command will run on, and is specified in the 24
hour clock, values must be between 0 and 23 (0 is midnight)

dom	
This is the Day of Month, that you want the command run on, e.g. to run a
command on the 19th of each month, the dom would be 19.

month	
This is the month a specified command will run on, it may be specified
numerically (0-12), or as the name of the month (e.g. May)

dow	
This is the Day of Week that you want a command to be run on, it can also be
numeric (0-7) or as the name of the day (e.g. sun).

user	
This is the user who runs the command.

cmd	
This is the command that you want run. This field may contain multiple words
or spaces.

If you don't wish to specify a value for a field, just place a * in the 
field.

e.g.
01 * * * * root echo "This command is run at one min past every hour"
17 8 * * * root echo "This command is run daily at 8:17 am"
17 20 * * * root echo "This command is run daily at 8:17 pm"
00 4 * * 0 root echo "This command is run at 4 am every Sunday"
* 4 * * Sun root echo "So is this"
42 4 1 * * root echo "This command is run 4:42 am every 1st of the month"
01 * 19 07 * root echo "This command is run hourly on the 19th of July"

Notes:

Under dow 0 and 7 are both Sunday.

If both the `dom` and `dow` are specified, the command will be executed when
either of the events happen. 

* 12 16 * Mon root cmd

Will run cmd at midday every Monday and every 16th, and will produce the same
result as both of these entries put together would:

* 12 16 * * root cmd
* 12 * * Mon root cmd


ixie Cron also accepts `lists` in the fields. Lists can be in the form, 1,2,3
(meaning 1 and 2 and 3) or 1-3 (also meaning 1 and 2 and 3).

59 11 * * 1,2,3,4,5 root backup.sh

Will run backup.sh at 11:59 Monday, Tuesday, Wednesday, Thursday and Friday,
as will:

59 11 * * 1-5 root backup.sh 


Cron also supports `step` values.

A value of */2 in the dom field would mean the command runs every two days and
likewise, */5 in the hours field would mean the command runs every 5 hours.

e.g. 
* 12 10-16/2 * * root backup.sh
is the same as:
* 12 10,12,14,16 * * root backup.sh

*/15 9-17 * * * root connection.test
Will run connection.test every 15 mins between the hours or 9am and 5pm

<ex>
[si_logs@theyard bin]$ cat /etc/crontab
SHELL=/bin/bash
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
HOME=/

# For details see man 4 crontabs

# Example of job definition:
# .---------------- minute (0 - 59)
# |  .------------- hour (0 - 23)
# |  |  .---------- day of month (1 - 31)
# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
# |  |  |  |  |
# *  *  *  *  * user-name command to be executed
17 *    * * *   root    cd / && run-parts --report /etc/cron.hourly
25 6    * * *   root    test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.daily )
47 6    * * 7   root    test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.weekly )
52 6    1 * *   root    test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.monthly )


={============================================================================
*kt_linux_tool_100* tool-gprof

http://sourceware.org/binutils/docs/gprof/


={============================================================================
*kt_linux_tool_100* tool-chrome

http://www.gamasutra.com/view/news/176420/Indepth_Using_Chrometracing_to_view_your_inline_profiling_data.php
chrome://tracing/


={============================================================================
*kt_linux_tool_100* tool-efence-issues
  
<double-free>
When do double free under pc linux that uses glibc, make a core as below but
run on a embedded linux that uses uclibc, shows no problem.

$ ./a.out 
pstr = 
this is..
*** glibc detected *** ./a.out: double free or corruption (fasttop): 0x000000001834b010 ***
======= Backtrace: =========
/lib64/libc.so.6[0x393c07230f]
/lib64/libc.so.6(cfree+0x4b)[0x393c07276b]
./a.out[0x40062f]
/lib64/libc.so.6(__libc_start_main+0xf4)[0x393c01d994]
./a.out[0x4004b9]
======= Memory map: ========
00400000-00401000 r-xp 00000000 fd:02 37781700                           /data/home/NDS-UK/parkkt/work/a.out
00600000-00601000 rw-p 00000000 fd:02 37781700                           /data/home/NDS-UK/parkkt/work/a.out
1834b000-1836c000 rw-p 1834b000 00:00 0                                  [heap]
393bc00000-393bc1c000 r-xp 00000000 fd:00 3538948                        /lib64/ld-2.5.so
393be1b000-393be1c000 r--p 0001b000 fd:00 3538948                        /lib64/ld-2.5.so
393be1c000-393be1d000 rw-p 0001c000 fd:00 3538948                        /lib64/ld-2.5.so
393c000000-393c14e000 r-xp 00000000 fd:00 3538955                        /lib64/libc-2.5.so
393c14e000-393c34d000 ---p 0014e000 fd:00 3538955                        /lib64/libc-2.5.so
393c34d000-393c351000 r--p 0014d000 fd:00 3538955                        /lib64/libc-2.5.so
393c351000-393c352000 rw-p 00151000 fd:00 3538955                        /lib64/libc-2.5.so
393c352000-393c357000 rw-p 393c352000 00:00 0 
394b200000-394b20d000 r-xp 00000000 fd:00 3539281                        /lib64/libgcc_s-4.1.2-20080825.so.1
394b20d000-394b40d000 ---p 0000d000 fd:00 3539281                        /lib64/libgcc_s-4.1.2-20080825.so.1
394b40d000-394b40e000 rw-p 0000d000 fd:00 3539281                        /lib64/libgcc_s-4.1.2-20080825.so.1
2ba0f68d2000-2ba0f68d4000 rw-p 2ba0f68d2000 00:00 0 
2ba0f68ec000-2ba0f68ee000 rw-p 2ba0f68ec000 00:00 0 
7fff1e6e4000-7fff1e6f9000 rw-p 7ffffffe9000 00:00 0                      [stack]
ffffffffff600000-ffffffffffe00000 ---p 00000000 00:00 0                  [vdso]
Aborted


// from man page

The free() function frees the memory space pointed to by ptr, which must have
  been returned by a previous call to malloc(), calloc() or realloc().
  Otherwise, or if free(ptr) has already been called before, 'undefined'
  behavior occurs. If ptr is NULL, no operation is performed.


<use-after-free>
Here is an example usage (classical use-after-free error):

$ cat uaf.c

#include <stdlib.h>
int main() {
  int *x = malloc(10 * sizeof(int));
  free(x);
  return x[5];
}


from ansic, p167

for( p = haed; p != NULL; p = p->next )
   free(p);

shall be:

for( p = head; p != NULL; p = q )
{
   q = p->next;
   free(p);
}

for( p = head; p != NULL; )
{
  q = p;
  p = p->next;
  free(q);
}


{overrun}
Accessing out of index of array is 'undefined' so sometimes work or sometimes
not.

<01>
int g_arr[3];

int main(int argc, char* argv[])
{
  if( g_arr[5] == 0 )
    printf("idx 5 is false\n");
  else
    printf("idx 5 is true\n");
}

$ ./a.out 
idx 5 is false

<02>
The issue was that index(dbConId) was -1(0xFFFFFFFF used as a handle) and worked
sometimes. But crashed

static MFS_STATUS MFSDBWRAP_GetDbContext(MFS_STORAGE_DB_CONTEXT_ID dbConId, 
    MFS_DB_CONTEXT **ppContext)
{
  MFS_STATUS mfsStatus = MFS_STATUS_OK;

  XDEBUG_DEFINE_FUNC_INFO("MFSDBWRAP_GetDbContext");
  XDEBUG_TRACE2_1("start: dbConId=%d", dbConId);

  // the dbConId is the index into the db context array, so we can directly
  // access it.  First check that the context is really in use. If not this is a
  // bug.

  if (db_contexts_array[dbConId].bInUse == XFALSE)
  {
    XDEBUG_ERR2_0("No free slots for contextes.  Returning the first one");
    mfsStatus = MFS_STATUS_ERROR;
  }
  else
  {
    *ppContext = &db_contexts_array[dbConId];
  }

  return mfsStatus;
}

<3>
Seen on NDS MW issue

MW_Process crash with core dump in SIM_dbc_query_ReadProgrammeInstanceInfo
(during memcpy) 

// void *memcpy(void *dest, const void *src, size_t n);
(void)memcpy((void*)&obj_array[offset].sort_title[0], 
    (const char*)results[24].value.text, SIM_DBC_MAX_SORT_STRING_LEN);

// source when crashed
0x1d8dfec: 0x54495050 0x494e4720 0x504f494e 0x54000000
0x1d8dffc: 0xc009 0x0 0x0 0x0

// dest
x/40a obj_array[0].sort_title
0x204eb20: 0x54495050 0x494e4720 0x504f494e 0x54000000
0x204eb30: 0x0 0x0 0x0 0x0

// solution
strncpy(&obj_array[offset].sort_title[0], 
    (const char*)results[24].value.text, SIM_DBC_MAX_SORT_STRING_LEN);

So the problem is the accessing out of index. Why? When doing memcpy, there is
  no gurantee that src and dest has all the vaild address up to the given size,
     SIM_DBC_MAX_SORT_STRING_LEN(40). So when the valid length of src is smaller
       than the size then the area which is over the length of src, may not
       accessiable such as not contiguous, not valid for this process, etc. 

So strncpy will ensure only the length of the value.text is copied when there is
a null in src (destination then padded out with nulls) or a maximum of 40 is
copied if the source string is over 40.


={============================================================================
*kt_linux_tool_100* tool-efence-benchmark

                  | HOF       | RAF   | WAF     | DF          | SOF   | GBOF
                  -------------------------------------------------------
gcc, host         | n/d       | n/d   | n/d     | glibc trace | n/d   | n/d
gcc, target       | n/d       | n/d   | n/d     | n/d         | n/d   | n/d
dmalloc, target   | n/d       | n/d   | n/d     | detected    | n/d   | n/d
asn, host         | d         | d     | d       | d           | d     | d
duma, target      | d         | d     | d       | d           | n/d   | n/d
mudflap, host

heap overflow, read after free, write after free, double free
stack overflow, global buffer overflow


<reference>
/duma_2_5_15/comparisons/memCheckers.html


={============================================================================
*kt_linux_tool_100* tool-efence

{efence}
http://linux.die.net/man/3/efence

Synopsis

#include <stdlib.h>

void * malloc (size_t size);

void free (void *ptr);

void * realloc (void *ptr, size_t size);

void * calloc (size_t nelem, size_t elsize);

void * memalign (size_t alignment, size_t size);

void * valloc (size_t size);

extern int EF_ALIGNMENT;

extern int EF_PROTECT_BELOW;

extern int EF_PROTECT_FREE;

extern int EF_ALLOW_MALLOC_0;

extern int EF_FILL;

Author

Bruce Perens 

Description

Electric Fence helps you detect two common programming bugs: software that
overruns the boundaries of a malloc() memory allocation, and software that
touches a memory allocation that has been released by free(). Unlike other
malloc() debuggers, Electric Fence will detect read accesses as well as
writes, and it will pinpoint the exact instruction that causes an error. It
has been in use at Pixar since 1987, and at many other sites for years.

Electric Fence uses the `virtual memory hardware` of your computer to place an
inaccessible memory page immediately after (or before, at the user's option)
each memory allocation. When software reads or writes this inaccessible page,
     the hardware issues a segmentation fault, stopping the program at the
     offending instruction. It is then trivial to find the erroneous statement
     using your favorite debugger. In a similar manner, memory that has been
     released by free() is made inaccessible, and any code that touches it
     will get a segmentation fault.

Simply linking your application with libefence.a will allow you to detect
most, but not all, malloc buffer overruns and accesses of free memory. If you
want to be reasonably sure that you've found all bugs of this type, you'll
have to read and understand the rest of this man page. 


={============================================================================
*kt_linux_tool_100* tool-efence-duma

http://duma.sourceforge.net/

CHANGELOG:
==========

2.5.15
  * added alternative locking implementation in sem_inc.c
    using critical sections.
    code from Peter Harris, see
    http://code.google.com/p/electric-fence-win32/
  * added Windows Threading to testmt.c
    added win32-msvc.net project file
  * use of WIN32_SEMAPHORES on Win32-Cygwin in sem_inc.c
      - the locking mechanism for multi-threading,
      with this configuration testmt.c works either
      with pthreads and with the Win32 API
  * CreateSemaphore() now used with maximum count = initial count = 1
  (`2008`-08-03, HA)
  * removed usage of strlen() in strncpy()
  (2009-03-19, HA)
  * PATCH from Andre Landwehr <andrel@cybernoia.de>
    fixes race condition when using preprocessor macro 'delete'
  (2009-04-07, HA)
  * bugfix in strncpy()
    Roman Jokl reported the bug: error check was too rigorous
  (2009-04-11, HA)

Purpose        User dynamic Memory Corruption
Technology     Library
ARCH           ARM, Mips
OS             Linux

DUMA used for finding memory usage errors such as Overflow, Underflow, Memory
used after free, allocator/deallcator mismatch in user program. Below are the
allocator Supported in DUMA(16 type)

DUMA is an open-source library (under GNU General Public License) to detect
buffer overruns and under-runs in C and C++ programs.  This library is a fork
of `Buce Perens Electric Fence` library and adds some new features to it.

<readme>
Title: README
D.U.M.A. - Detect Unintended Memory Access - A Red-Zone memory allocator:


DESCRIPTION:

DUMA helps you detect two common programming bugs:

software that overruns the boundaries of a malloc() memory allocation, and
software that touches a memory allocation that has been released by free().

Unlike other malloc() debuggers, DUMA will `detect read accesses` as well as
writes, and it will pinpoint the exact instruction that causes an error.
It has been in use at Pixar since 1987, and at many other sites for years.

DUMA uses the virtual memory hardware of your computer to place an inaccessible
memory page immediately after (or before, at the user's option) each memory
allocation. When software reads or writes this inaccessible page, the hardware
issues a segmentation fault, stopping the program at the offending instruction.
It is then trivial to find the erroneous statement using your favorite
debugger. In a similar manner, memory that has been released by free() is made
inaccessible, and any code that touches it will get a segmentation fault.

Simply linking your application with libduma.a will allow you to detect most,
but not all, malloc buffer overruns and accesses of free memory. If you want
to be reasonably sure that you've found all bugs of this type, you'll have to
read and understand the rest of this man page.

note: how? use `segmentation fault`


<how-to-use>
CATCHING THE ERRONEOUS LINE:

note: staic

1. Compile your program (with debugging information) without DUMA.
2. Set 'ulimit -c unlimited' to get core files
3. Start your program, choose one of following options
   a) Start your program (linked statically with DUMA)
   b) Start your program with duma.sh <your_program>
4. Wait for a segmentation fault. this should have created a core[.<pid>]
   file. You can get into a debugger f.e. with 'gdb <program> -c <core file>'


<configuration>

GLOBAL AND ENVIRONMENT VARIABLES:

note: two ways to configure; env variable or macro.

You can use the gdb command 'set environment variable value' to set shell
environment variables only for the program you are going to debug. This is
useful especially if you are using the shared DUMA library.

Instead you can call macro function to set some variables.


DUMA_PROTECT_BELOW - DUMA usually places an inaccessible page immediately after
  each memory allocation, so that software that runs past the end of the
  allocation will be detected. Setting DUMA_PROTECT_BELOW to 1 causes DUMA to
  place the inaccessible page before the allocation in the address space, so
  that `under-runs` will be detected instead of over-runs.
  To change this value, set DUMA_PROTECT_BELOW in the shell environment to an
  integer value, or call the macro function DUMA_SET_PROTECT_BELOW() from your
  code.

  note: overrun is default?


// NOT IMPLEMENTED. NEED TO CONSIDER
// DUMA_SKIPCOUNT_INIT - DUMA usually does its initialization with `the first`
//   memory allocation. On some systems this may collide with initialization of
//   pthreads or other libaries and produce a hang. To get DUMA work even in these
//   situations you can control (with this environment variable) after how many
//   allocations the full internal initialization of DUMA is done. `default is 0`

//   note: shall use it? changed source directly

// #ifdef DUMA_SKIPCOUNT_INIT
//     _duma_s.SKIPCOUNT_INIT = 10000;
// #endif

void * malloc(size_t size)
{
  return _duma_malloc(size  DUMA_PARAMS_UK);
}

// _duma_g.allocList is set in _duma_init()
void * _duma_malloc(size_t size  DUMA_PARAMLIST_FL)
{
  if ( _duma_g.allocList == 0 )
    _duma_init();  /* This sets DUMA_ALIGNMENT, DUMA_PROTECT_BELOW, DUMA_FILL, ... */

  return _duma_allocate();
}


DUMA_EXPLICIT_INIT

// Function: _duma_init
//
// _duma_init sets up the memory allocation arena and the run-time
// configuration information. We will call duma_init `unless` DUMA_EXPLICIT_INIT
// is defined at compile time.
//
// See Also: duma_init
//
// _duma_init is full initialization, is called from _duma_malloc() and
// others. so set DUMA_EXPLICIT_INIT and then duma_int() will be external, and
// _duma_init will call it.


DUMA_FILL - When set to a value between 0 and 255, every byte of allocated
  memory is initialized to that value. This can help detect reads of
  uninitialized memory. When set to -1, DUMA does not initialise memory on
  allocation. But some memory is filled with zeroes (the operating system
  default on most systems) and some memory will retain the values written to
  it during its last use.
  Per `default` DUMA will initialise all allocated bytes to 255 (=0xFF).
  To change this value, set DUMA_FILL in the shell environment to an
  integer value, or call the macro function DUMA_SET_FILL() from your
  code.


DUMA_PROTECT_FREE - DUMA usually returns free memory to a pool from which it
  may be re-allocated. If you suspect that a program may be touching free
  memory, set DUMA_PROTECT_FREE shell environment to -1. `This is the default`
  and will cause DUMA not to re-allocate any memory.

  note: such as use after free?

  For programs with many allocations and deallocations this may lead to the
  consumption of the full address space and thus to the failure of malloc().
  To avoid such failures you may limit the amount of protected deallocated
  memory by setting DUMA_PROTECT_FREE to a positive value. This value in kB
  will be the limit for such protected free memory.

  `A value of 0 will disable` protection of freed memory.

  note: shall disable it?  added

#ifdef DUMA_NO_PROTECT_FREE
    _duma_s.PROTECT_FREE = 0;
#endif


DUMA_SUPPRESS_ATEXIT - Set this shell environment variable to non-zero when
  DUMA `should skip` the installation of its exit handler. The exit handler is
  called at the end of the main program and checks for memory leaks, so the
  handler's installation should usually not be suppressed. One reason for
  doing so regardless are some buggy environments, where calls to the standard
  C library's atexit()-function hangs.

  note: shall disable it? do not define DUMA_PREFER_ATEXIT to disable


DUMA_OUTPUT_STDOUT - Set this shell environment variable to non-zero to output
  all DUMA messages to STDOUT. This option is `off by default`

DUMA_OUTPUT_STDERR - Set this shell environment variable to non-zero to output
  all DUMA messages to STDERR. This option is `on by default`

DUMA_OUTPUT_FILE - Set this shell environment variable to a filename where all
  DUMA messages should be written to. This option is `off by default`


<configuration-alignment>

DUMA_ALIGNMENT - This is an integer that specifies the alignment for any memory
  allocations that will be returned by malloc(), calloc(), and realloc().
  The value is `specified in bytes`, thus a value of 4 will cause memory to be
  aligned to 32-bit boundaries unless your system doesn't have a 8-bit
  characters. DUMA_ALIGNMENT is set to the minimum required alignment specific
  to your environment by default. The minimum required alignment is detected by
  `createconf` and stored in the file duma_config.h.

  note:
  `createconf` generates duma_config.h

  no DUMA_MIN_ALIGNMENT env var but it is define used in code and defined in
  duma_config.h

  duma.c:573:  , {   DUMA_MIN_ALIGNMENT
  dumatest.c:308:  DUMA_SET_ALIGNMENT(DUMA_MIN_ALIGNMENT);

  get 1 for mips:
  duma_config_mips.h:194:#define DUMA_MIN_ALIGNMENT 1

  If your program requires that allocations be aligned to 64-bit boundaries
  you'll have to set this value to 8. This is the case when compiling with the
  '-mips2' flag on MIPS-based systems such as those from SGI. For some
  architectures the default is defined to even more - x86_64 uses alignment to
  16 bytes by default.

  DUMA internally uses a smaller value if the requested memory size is smaller
  than the alignment value: the next smaller power of 2 is used.
  Thus allocating blocks smaller than DUMA_ALIGNMENT may result into smaller
  alignments - for example when allocating 3 bytes, they would be aligned to 2
  byte boundary. `This allows better detection of overrun`
  For this reason, you will sometimes want to set `DUMA_ALIGNMENT to 1` (no
  alignment), so that you can detect overruns of less than your CPU's word
  size. Be sure to read the section 'WORD-ALIGNMENT AND OVERRUN DETECTION' in
  this manual page before you try this.

  To change this value, set DUMA_ALIGNMENT in the shell environment to an
  integer value, or call the macro function DUMA_SET_ALIGNMENT() from your
  code.
  You don't need to change this setting, if you just need bigger alignment for
  some special buffers. In this case you may use the function
  memalign(alignment, userSize).


WORD-ALIGNMENT AND OVERRUN DETECTION:

There is a conflict between the alignment restrictions that malloc() operates
under and the debugging strategy used by DUMA. When detecting overruns, DUMA
malloc() allocates two or more virtual memory pages for each allocation. The
last page is made inaccessible in such a way that any read, write, or execute
access will cause a segmentation fault. Then, DUMA malloc() will return an
address such that the first byte after the end of the allocation is on the
inaccessible page. Thus, any overrun of the allocation will cause a
segmentation fault.

It follows that the address returned by malloc() is the address of the
inaccessible page minus the size of the memory allocation. 

note:
After all,

 ===================  inaccessible page
 ===================
 ===================
 01 ----------------  accessible page. return X-size
 -------------------
 -------------------
 01 ================  inaccessible page, X
 ===================
 ===================


Unfortunately, malloc() is required to return word-aligned allocations, since
many CPUs can only access a word when its address is aligned. The conflict
happens when software makes a memory allocation using a size that is not a
multiple of the word size, and expects to do word accesses to that allocation.
The location of the inaccessible page is fixed by hardware at a word-aligned
address. If DUMA malloc() is to return an aligned address, it must increase
the size of the allocation to a multiple of the word size.

In addition, the functions memalign() and valloc() must honor explicit
specifications on the alignment of the memory allocation, and this, as well
can only be implemented by increasing the size of the allocation. 

Thus, there will be situations in which the end of a memory allocation
contains some `padding space, and accesses of that padding space will not` be
detected, even if they are overruns.

DUMA provides the variable DUMA_ALIGNMENT so that the user can control the
default alignment used by malloc(), calloc(), and realloc(). To debug overruns
as small as a single byte, you can set DUMA_ALIGNMENT to one. This will result
in DUMA malloc() returning unaligned addresses for allocations with sizes that
are not a multiple of the word size. 

This is not a problem in most cases, because `compilers must pad` the size of
objects so that alignment restrictions are honored when storing those objects
in arrays. 

The problem surfaces when software allocates odd-sized buffers for objects
that must be word-aligned. One case of this is software that allocates a
buffer to contain a structure and a string, and the string has an odd size
(this example was in a popular TIFF library). If word references are made to
un-aligned buffers, you will see a bus error (SIGBUS) instead of a
segmentation fault. The only way to fix this is to re-write the offending code
to make byte references or not make odd-sized allocations, or to set
DUMA_ALIGNMENT to the word size.

Another example of software incompatible with DUMA_ALIGNMENT < word-size
is the strcmp() function and other string functions on SunOS (and probably
Solaris), which make word-sized accesses to character strings, and may attempt
to access up to three bytes beyond the end of a string. These result in a
segmentation fault (SIGSEGV). The only way around this is to use versions of
the string functions that perform byte references instead of word references.


MEMORY USAGE AND EXECUTION SPEED:

Since DUMA `uses at least two virtual memory pages for each` of its allocations,
it's a terrible memory hog. I've sometimes found it necessary to add a swap
  file using swapon(8) so that the system would have enough virtual memory to
  debug my program. Also, the way we manipulate memory results in various
  cache and translation buffer entries being flushed with each call to malloc
  or free. 

`The end result is that program will be much slower and use more resources`
while you are debugging it with DUMA.


The Linux kernel limits the number of different page mappings `per process`

Have a look for

/proc/sys/vm/max_map_count
f.e. under
http://www.redhat.com/docs/manuals/enterprise/RHEL-4-Manual/en-US/Reference_Guide/s3-proc-sys-vm.html
You may have to increase this value to allow debugging with DUMA with a
command like:
sudo sysctl -w vm.max_map_count=1000000
sysctl -w vm.max_map_count=1000000

note: From mips platform
-sh-3.2# cat /proc/sys/vm/max_map_count
65536

Don't leave libduma.a linked into production software! Use it only for
debugging. See section 'COMPILATION NOTES FOR RELEASE/PRODUCTION' below.


MEMORY LEAK DETECTION:

All memory allocation is protocoled from DUMA together with the filename and
linenumber of the calling function. The atexit() function checks if each
allocated memory block was freed. To disable leak detection add the
`preprocessor definition` 'DUMA_SO_NO_LEAKDETECTION' or
'DUMA_LIB_NO_LEAKDETECTION' to DUMA_OPTIONS in Makefile.

note:
If set 'DUMA_LIB_NO_LEAKDETECTION' then `creatconf` will set
`DUMA_NO_LEAKDETECTION` in duma_config.h This means that use the same makefile
to build `creatconf`

`DUMA_NO_LEAKDETECTION` has to be set. Otherwise, will not use duma calls in
duma.h

note:
This is a tip when know the exact address

If a leak is reported without source filename and line number but is
reproducible with the same pointer, set a conditional breakpoint on the
function 'void * duma_alloc_return( void * address)' f.e. with gdb command
'break duma_alloc_return if address==0x123'


C++ MEMORY OPERATORS AND LEAK DETECTION:

Macros for "new" and "delete" are defined in dumapp.h. These macros give
filename and linenumber of the calling functions to DUMA, thus allowing the
same leak detection reports as for malloc and free. 'dumapp.h' needs to be
included from your source file(s).

For disabling the C++ new/delete/new[] and delete[] operators, add the
`preprocessor definition DUMA_NO_CPP_SUPPORT` to DUMA_OPTIONS in Makefile.

note: shall disable


<configuration-for-mips>
// duma_config.h for using in clien

// for not directing new/delete to malloc/free
#define DUMA_NO_CPP_SUPPORT
#define DUMA_LIB_PREFER_GETENV
#define DUMA_LIB_NO_HANG_MSG
#define DUMA_NO_GLOBAL_MALLOC_FREE
#define DUMA_EXPLICIT_INIT
#define DUMA_OLD_NEW_MACRO
#define DUMA_OLD_DEL_MACRO
#define DUMA_NO_STRERROR
#define DUMA_SKIPCOUNT_INIT
#define DUMA_NO_PROTECT_FREE

// GNUmakefile for building a lib
# edit following line (for createconf)
DUMA_OPTIONS =-DUMA_NO_CPP_SUPPORT
# DUMA_OPTIONS += -DUMA_PREFER_ATEXIT
DUMA_OPTIONS += -DUMA_LIB_PREFER_GETENV
DUMA_OPTIONS += -DUMA_LIB_NO_HANG_MSG
DUMA_OPTIONS += -DUMA_NO_GLOBAL_MALLOC_FREE
DUMA_OPTIONS += -DUMA_EXPLICIT_INIT
# DUMA_OPTIONS += -DUMA_NO_THREAD_SAFETY
DUMA_OPTIONS += -DUMA_OLD_NEW_MACRO
DUMA_OPTIONS += -DUMA_OLD_DEL_MACRO
DUMA_OPTIONS += -DUMA_NO_STRERROR
DUMA_OPTIONS += -DUMA_SKIPCOUNT_INIT
DUMA_OPTIONS += -DUMA_NO_PROTECT_FREE


<readme-files>

SubDirectories:
---------------

// note: removed
// win32-vide/*    project files for VIDE 1.24 (see http://www.objectcentral.com)
//                 using the Borland C++ Builder 5.5 compiler
//                 (FreeCommandLineTools, see http://www.borland.com)
// win32-devcpp/*  project files for Dev-C++ 4.9.6 (see http://www.bloodshed.net)
//                 using the gcc compiler (see http://gcc.gnu.org)
// win32-msvc/*    projects files for Microsoft Visual C++ 6.0 IDE/compiler
//                 (see http://www.microsoft.com)
// debian/*        don't know; maybe some files for the Debian Linux distribution?
// detours/        microsoft detours related
// kduma/          This is the begining of a linux kernel model duma.


Projects:
---------
dumalib         DUMA library. this library should be linked with YOUR program
`dumatest`        first small test program
`tstheap`         second small test program

Files:
------
COPYING-*       License files; reade carefully!
README          this text file
CHANGES         text file listing done CHANGES

duma.h          belongs to dumalib
                this header file should be included from within YOUR C source
// dumapp.h        belongs to dumalib
//                 this header file should be included from within YOUR C++ source
duma.c          belongs to dumalib
                contains malloc/free/.. functions
// dumapp.cpp      belongs to dumalib
//                 contains C++ new/delete/.. functions redirecting them
//                   to ANSI C malloc/free

// note: not exist in souce
// page.c          belongs to dumalib
//                 library internal source file: contains paging functions

print.c         belongs to dumalib; library internal source file: contains
                  printing/aborting functions

dumatest.c      belongs to dumatest
                small test program; checks wether dumalib's paging does its job
                `should work without any errors`

tstheap.c       belongs to tstheap
                small test program; checks wether dumalib's heap does its job
                should report many memory leaks after execution.
Makefile        Makefile for UNIX/Linux/..
// duma.3          source for UNIX man page
// duma.sh         script for UNIX/Linux to start other programs using the
//                   LD_PRELOAD mechanism


<features>

* "overloads" all standard memory allocation functions like malloc(),
  calloc(), memalign(), strdup(), operator new, operator new[] and also their
    counterpart deallocation functions like free(), operator delete and
    operator delete[]

malloc
calloc
free
memalign
posix_mem
realloc
valloc
strdup
memcpy
strcpy
strncpy
strcat
strncat
strndup
vasprintf
asprintf


* utilizes the MMU (memory management unit) of the CPU:

* allocates and protects an extra memory page to detect any illegal access
  beyond the top of the buffer (or bottom, `at the user's option`)

* stops the program at exactly that instruction, which does the erroneous
  access to the protected memory page, allowing location of the defectice
  source code in a debugger

* detects erroneous writes at the non-protected end of the memory block at
  deallocation of the memory block

* detects mismatch of allocation/deallocation functions: f.e. allocation with
  malloc() but deallocation with operator delete

* leak detection: detect memory blocks which were not deallocated until
  program exit

* runs on Linux / U*ix and MS Windows NT/2K/XP operating systems

* preloading of the library on Linux (and some U*ix) systems allowing tests
  without necessity of changing source code or recompilation


<source>
https://sourceforge.net/projects/duma/?source=typ_redirect


<ex>
$ ./out_duma_host 1
DUMA 2.5.15 (static library)
Copyright (C) 2006 Michael Eddington <meddington@gmail.com>
Copyright (C) 2002-2008 Hayati Ayguen <h_ayguen@web.de>, Procitec GmbH
Copyright (C) 1987-1999 Bruce Perens <bruce@perens.com>


=== DUMA: KIT: _duma_allocate
testmain: argv[1]
=== test read after free

=== DUMA: KIT: _duma_allocate
Segmentation fault (core dumped)


<global-malloc>
When use #define DUMA_NO_GLOBAL_MALLOC_FREE, define _duma_xxx ones and use
preprocessor scheme.

0000337c T _duma_free
00004068 D _duma_g
000013f0 T _duma_init
00000750 t _duma_init_slack
00003204 T _duma_malloc

// duma.h
      #define malloc(SIZE)                _duma_malloc(SIZE, __FILE__, __LINE__)
      #define calloc(ELEMCOUNT, ELEMSIZE) _duma_calloc(ELEMCOUNT, ELEMSIZE, __FILE__, __LINE__)

Made a binary statically linked with libduma.a, saw the module(c file) calls
the duma. However, not all the rest in the binary since other modules are not
compiled with duma preprocessor on. How to address this?

1. Find every single c files to build a binary and make sure that includes
duma header and macro on?

1. Find global header of malloc families and repalce it with duma one? But
this make all binary(process) to use duma but not only one binary.


When NOT use #define DUMA_NO_GLOBAL_MALLOC_FREE, this define malloc families
in the libary.

#include <stdio.h>
#include <stdlib.h>

int main(int argc, char **argv)
{
    int *x = malloc(10*sizeof(int));
    free(x);
    return x[5];
}

$ nm libduma.a.without.global.malloc | grep malloc
35:00003204 T _duma_malloc

$ nm out_duma | grep malloc
35:         U malloc

So not use duma. However, when use global mallocs,

$ nm libduma.a | grep malloc
35:00003268 T _duma_malloc
60:00004108 T malloc

$ nm out_duma | grep malloc
55:00403f58 T _duma_malloc
103:00404df8 T malloc


={============================================================================
*kt_linux_tool_100* tool-efence-dmalloc

note:
No longer maintained

Dmalloc Releases

5.5.2 	20070514

http://dmalloc.com/docs/latest/online/dmalloc_3.html#SEC3

* build `libdmallocth.a' which is the threaded version of the library.

2.5 How the Library Checks Your Program 

* The dmalloc library replaces the heap library calls normally found in your
  system libraries with its own versions.

In addition to per-pointer checks, you can configure the library to perform
`complete heap checks` These complete checks verify all internal heap
structures and include walking all of the known allocated pointers to verify
each one in turn. You need this level of checking to find random pointers in
your program which got corrupted but that won't be freed for a while. To turn
on these checks, you will need to enable the `check-heap` debug token. See
section Description of the `Debugging Tokens` By default this will cause the
heap to be fully checked each and every time dmalloc is called whether it is a
malloc, free, realloc, or another dmalloc overloaded function. 


Performing a full heap check can take a good bit of CPU and it may be that you
will want to run it sporadically.


3.5 Additional Non-standard Routines 

Function: void dmalloc_debug_setup ( const char * options_str )

    This routine sets the global debugging functionality as an option string.
    Normally this would be passed in in the DMALLOC_OPTIONS environmental
    variable. This is here to override the env or for circumstances where
    modifying the environment is not possible or does not apply such as
    servers or cgi-bin programs.


Function: int dmalloc_verify ( char * pnt )

    This function verifies individual memory pointers that are suspect of
    memory problems. To check the `entire heap` pass in a NULL or 0 pointer. The
    routine returns DMALLOC_VERIFY_ERROR or DMALLOC_VERIFY_NOERROR.

    NOTE: `dmalloc_verify()' can only check the heap with the functions that
    have been enabled. For example, if fence-post checking is not enabled,
`dmalloc_verify()' cannot check the fence-post areas in the heap. 


// dmalloc.h

#define malloc(size) \
  dmalloc_malloc(__FILE__, __LINE__, (size), DMALLOC_FUNC_MALLOC, 0, 0)


<dmalloc-build>

1. change app_process_main.c

/* Token 12 */
dmalloc_debug_setup("log-stats,log-non-free,check-fence,error-free-null,print-messages");

pagesize = dmalloc_page_size();
printf("APP_Process dmalloc pagesize = %d system pagesize = %d\n", pagesize, getpagesize());
printf("APP_Process size of long = %d\n", sizeof(long));


2. change makefile to link with dmalloc lib for each process

/build/processes/APP_Process/makefile

COMPONENT_SOURCE_LIBRARIES+=\
	$(NDS_ROOT)/libdmallocth.a
COMPONENT_PREBUILT_OBJECTS+=\
	$(NDS_ROOT)/libdmallocth.a


3. enable console
note: careful since it's different from release to release

/build/make_image.sh

  SPK_CONFIG+=" console"


4. copy two files

vobs/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/dmalloc.h
vobs/FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION/build/libdmallocth.a


25:08:16 15:46:17 946684826: 593: WARNING: tried to free(0) from 'unknown' 
25:08:16 15:46:17 946684826: 593:   error details: invalid 0L pointer 
25:08:16 15:46:17 946684826: 593:   from 'unknown' prev access 'unknown' 
25:08:16 15:46:17 946684826: 593: ERROR: free: pointer is null (err 20) 
25:08:16 15:46:17 946684826: 600: WARNING: tried to free(0) from 'unknown' 
25:08:16 15:46:17 946684826: 600:   error details: invalid 0L pointer 
25:08:16 15:46:17 946684826: 600:   from 'unknown' prev access 'unknown' 
25:08:16 15:46:17 946684826: 600: ERROR: free: pointer is null (err 20) 
25:08:16 15:46:17 946684826: 607: WARNING: tried to free(0) from 'unknown' 
25:08:16 15:46:17 946684826: 607:   error details: invalid 0L pointer 
25:08:16 15:46:17 946684826: 607:   from 'unknown' prev access 'unknown' 
25:08:16 15:46:17 946684826: 607: ERROR: free: pointer is null (err 20) 
25:08:16 15:46:17 946684826: 628: WARNING: tried to free(0) from 'unknown' 
25:08:16 15:46:17 946684826: 628:   error details: invalid 0L pointer 
25:08:16 15:46:17 946684826: 628:   from 'unknown' prev access 'unknown' 
25:08:16 15:46:17 946684826: 628: ERROR: free: pointer is null (err 20) 


// 

int func_double_free()
{
    int *x = malloc(10*sizeof(int));
    free(x);
    free(x);
    return 1;
}

=== test double free
946784562: 10:   error details: finding address in heap
946784562: 10:   pointer '0x483f88' from 'testmain.c:32' prev access 'unknown'
946784562: 10: ERROR: free: tried to free previously freed pointer (err 61) ~
946784562: 10: Dumping Chunk Statistics:
946784562: 10: basic-block 4096 bytes, alignment 8 bytes
946784562: 10: heap address range: 0x47d000 to 0x487000, 40960 bytes
946784562: 10:     user blocks: 2 blocks, 8000 bytes (19%)
946784562: 10:    admin blocks: 8 blocks, 32768 bytes (80%)
946784562: 10:    total blocks: 10 blocks, 40960 bytes
946784562: 10: heap checked 0
946784562: 10: alloc calls: malloc 4, calloc 0, realloc 0, free 5
946784562: 10: alloc calls: recalloc 0, memalign 0, valloc 0
946784562: 10: alloc calls: new 0, delete 0
946784562: 10:   current memory in use: 320 bytes (1 pnts)
946784562: 10:  total memory allocated: 800 bytes (4 pnts)
946784562: 10:  max in use at one time: 720 bytes (2 pnts)
946784562: 10: max alloced with 1 call: 400 bytes
946784562: 10: max unused memory space: 304 bytes (29%)
946784562: 10: top 10 allocations:
946784562: 10:  total-size  count in-use-size  count  source
946784562: 10:         400      1           0      0  testmain.c:14
946784562: 10:          40      1           0      0  testmain.c:23
946784562: 10:          40      1           0      0  testmain.c:30
946784562: 10:         480      3           0      0  Total of 3
946784562: 10: Dumping Not-Freed Pointers Changed Since Start:
946784562: 10:  not freed: '0x47de00|s1' (320 bytes) from 'unknown'
946784562: 10:  total-size  count  source
946784562: 10:           0      0  Total of 0
946784562: 10: ending time = 946784562, elapsed since start = 0:00:00


// when ends without detecting an error

946784741: 7: Dumping Chunk Statistics:
946784741: 7: basic-block 4096 bytes, alignment 8 bytes
946784741: 7: heap address range: 0x47d000 to 0x487000, 40960 bytes
946784741: 7:     user blocks: 2 blocks, 8000 bytes (19%)
946784741: 7:    admin blocks: 8 blocks, 32768 bytes (80%)
946784741: 7:    total blocks: 10 blocks, 40960 bytes
946784741: 7: heap checked 0
946784741: 7: alloc calls: malloc 3, calloc 0, realloc 0, free 3
946784741: 7: alloc calls: recalloc 0, memalign 0, valloc 0
946784741: 7: alloc calls: new 0, delete 0
946784741: 7:   current memory in use: 320 bytes (1 pnts)
946784741: 7:  total memory allocated: 760 bytes (3 pnts)
946784741: 7:  max in use at one time: 720 bytes (2 pnts)
946784741: 7: max alloced with 1 call: 400 bytes
946784741: 7: max unused memory space: 304 bytes (29%)
946784741: 7: top 10 allocations:
946784741: 7:  total-size  count in-use-size  count  source
946784741: 7:         400      1           0      0  testmain.c:14
946784741: 7:          40      1           0      0  testmain.c:23
946784741: 7:         440      2           0      0  Total of 2
946784741: 7: Dumping Not-Freed Pointers Changed Since Start:
946784741: 7:  not freed: '0x47de00|s1' (320 bytes) from 'unknown'
946784741: 7:  total-size  count  source
946784741: 7:           0      0  Total of 0
946784741: 7: ending time = 946784741, elapsed since start = 0:00:00


={============================================================================
*kt_linux_tool_100* tool-efence-dml

{DML}
Purpose        User dynamic Memory Accounting
Technology     Library
ARCH           ARM, Mips
OS             Linux
Description

DML is used for memory accounting and find out memory 'leaks' in user program.
Below are the allocator Supported in DML(17 type)

memalign
valloc
posix_memalign
asprintf
strndup
strdup
wcsdup
malloc
realloc
calloc
free
operator new[]
operator new
operator delete
operator delete[]
prctl
pthread_create


{KDEBUGD}
Purpose        User tracer, debugger, and profiler
Technology     Kernel hooking
ARCH           ARM, Mips
OS             Linux
Description

Kdebugd is kernel mode debuger, tracer and profiler for user space programs
below are the Major feature provided by Kdebugd.

1- Find User program statistics (Maps/Stack and Register)
2- Run time Kernel and User backtrace
3- Resource Monitoring
4- Lock profiler
5- User program Profiling


{FTRACE}
Purpose        Kernel tracer
Technology     Tracepoint
ARCH           ARM, Mips
OS             Linux
Description

Ftrace is kernel mode tracer for kernel threads


{MEMPS} 
note: not open source
A 'Memps' tool is developed to show memory accounting information for the
complete system and individual processes through one common interface.

This tool combines various open source proc interface output and combined them
together to show all useful information at single place.

Various proc interfaces internally called are as follows:
a./proc/meminfo
b./proc/pid/status
c./proc/pid/cmdline
d./proc/mounts
e./proc/pid/smaps
f./proc/sys/vm/drop_caches

sh-4.1# ./memps --help

Usage: ./memps [OPTION]...

Options:
  -o, --output=FILE
  -p, --pid=PID
  -v, --verbose
  -t, --tmpfs
  -g, --geminfo
  -m, --maliinfo
  -r, --dropcaches
  -d, --description
  -c, --color
  -h, --help

If run without any argument output of memps utility would show
a.
PID     CODE     DATA     PEAK      PSS DEV(PSS) COMMAND
Output for all running processes in the system

b.
MemTotal       MemFree              MemUsed*       Buffers        Cached      SwapCached
Active         Inactive             Active(anon)   Inactive(anon)
Active(file)   Inactive(file)       SwapTotal  SwapFree    AnonPages
Free*          Used*             GemTotal    MaliTotal

Output for complete system, along with information for Graphics driver and Mali drivers.

sh-4.1#> ./memps
     PID     CODE     DATA     PEAK      PSS DEV(PSS) COMMAND
       1      292       36      328      200        0 init
      73      404       80      484      356        0 -/bin/sh
     211      556       76      632      632        0 ./memps

  TOTAL:     CODE     DATA               PSS DEV(PSS)
            1,252      192             1,188        0

MemTotal:  1181 MB ( 1,209,684 kB)
MemFree:  1160 MB ( 1,188,232 kB)
MemUsed*:    20 MB (    21,452 kB)
Buffers:     2 MB (     2,472 kB)
Cached:     3 MB (     3,976 kB)
SwapCached:     0 MB (         0 kB)
Active:     1 MB (     1,936 kB)
Inactive:     4 MB (     4,668 kB)
Active(anon):     0 MB (       164 kB)
Inactive(anon):     0 MB (         4 kB)
Active(file):     1 MB (     1,772 kB)
Inactive(file):     4 MB (     4,664 kB)
SwapTotal:     0 MB (         0 kB)
SwapFree:     0 MB (         0 kB)
AnonPages:     0 MB (       156 kB)
Free*:  1166 MB ( 1,194,668 kB)
Used*:    14 MB (    15,016 kB)
GemTotal:     0 MB (         0 kB)
(Contiguous:           0 kB)
(Non-contiguous:       0 kB)
MaliTotal:     0 MB (         0 kB)


={============================================================================
*kt_linux_tool_100* tool-efence-asan

AddressSanitizer is a tool that detects `memory corruption bugs` such as
buffer overflows or accesses to a dangling pointer (use-after-free). 


From doc:
Applications are implemented in `unmanaged` programming languages (C and C++)
which do not provide any protection against invalid memory accesses. Such
accesses often result in memory corruption and eventually cause program
crashes or other abnormal behavior. AddressSanitizer (or ASan for short) is a
part of Google toolsuite for program quality assurance (the other tools being
    ThreadSanitizer, MemorySanitizer and UBSanitizer). 

https://github.com/google/sanitizers
https://github.com/google/sanitizers/wiki/AddressSanitizer

Current AddressSanitizer handles the following classes of errors (see
http://code.google.com/p/address-sanitizer/wiki/AddressSanitizer#Introduction):

- use after free
- buffer overruns and wild pointers

This is not guaranteed to always work for static and stack variables. Runtime
will only be able to detect overrun (or wild pointer access) if it falls into
one of the redzones which separate objects on stack or in global memory. If
buffer offset is too large it'll be able to overcome the redzone and fool the
checker. ASan developers report that >95% of bugs are detectable with current
redzone sizes.

- use after return / use after end-of-block


<pros>
This tool is very fast. The average slowdown of the instrumented program is
~2x (see AddressSanitizerPerformanceNumbers).

Actively evolving (in contrast to mudflap which has been recently removed from
    Gcc)


<ex> `use after free`

Here is an example usage (classical use-after-free error):

$ cat uaf.c

#include <stdlib.h>
int main() {
  int *x = malloc(10 * sizeof(int));
  free(x);
  return x[5];
}

Let's examine the generated code for return statement:

$ /opt/vd/arm-v7a15v5r1/bin/arm-v7a15v5r1-linux-gnueabi-gcc \
    -fsanitize=address -O2 -S -o - uaf.c

Below code is annotated for brevity:

// int *x = malloc(10 * sizeof(int));
  .loc 1 3 0
  mov     r0, #40
  bl      malloc
  mov     r4, r0
.LVL0:
// free(x);
  .loc 1 4 0
  bl      free
.LVL1:
// ASan `instrumentation`: check memory access
  .loc 1 5 0
  add     r0, r4, #20           // Checked memory address
  mov     r3, #536870912        // Base address of shadow region
  ldrb    r3, [r3, r0, lsr #3]
  and     r2, r0, #7
  add     r2, r2, #3
  sxtb    r3, r3
  cmp     r2, r3
  movlt   r2, #0
  movge   r2, #1
  cmp     r3, #0
  moveq   r2, #0
  cmp     r2, #0
  bne     .L4                    // Poisoned access, report error
  .loc 1 6 0
// return x[5];
  ldr     r0, [r4, #20]
  ldmfd   sp!, {r4, pc}
.L4:
// ASan instrumentation: handle poisoned access
  .loc 1 5 0
  bl      __asan_report_load4

We can see that ASan has inserted 11 additional instructions to check one
memory access. Let's now check that runtime behavior matches our expectations:

$ /opt/vd/arm-v7a15v5r1/bin/arm-v7a15v5r1-linux-gnueabi-gcc \
    -fsanitize=address uaf.c

$ scp a.out root@myboard:/
a.out
100%   12KB  12.1KB/s   00:00   

$ ssh root@myboard /a.out
=================================================================
==154== ERROR: AddressSanitizer: heap-use-after-free on address 0xb5500fe4
at pc 0x86b0 bp 0xbec1ecd4 sp 0xbec1eccc
READ of size 4 at 0xb5500fe4 thread T0
    #0 0x86af (/a.out+0x86af)
    #1 0xb57f3bcb (/lib/libc.so.6+0x17bcb)
0xb5500fe4 is located 20 bytes inside of 40-byte region [0xb5500fd0,0xb5500ff8)
freed by thread T0 here:
    #0 0xb59320f3 (/lib/libasan.so.0+0x150f3)
    #1 0x8647 (/a.out+0x8647)
    #2 0xb57f3bcb (/lib/libc.so.6+0x17bcb)
previously allocated by thread T0 here:
    #0 0xb59321bb (/lib/libasan.so.0+0x151bb)
    #1 0x8637 (/a.out+0x8637)
    #2 0xb57f3bcb (/lib/libc.so.6+0x17bcb)
Shadow bytes around the buggy address:
  0x36aa01a0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa01b0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa01c0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa01d0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa01e0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
=>0x36aa01f0: fa fa fa fa fa fa fa fa fa fa `fd fd[fd]fd fd` fa
  0x36aa0200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0210: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0220: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0230: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0240: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (`one shadow byte represents 8 application bytes`):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:     `fa`
  Heap righ redzone:     fb
  Freed Heap region:     `fd`
  Stack left redzone:    f1
  Stack mid redzone:     f2
  Stack right redzone:   f3
  Stack partial redzone: f4
  Stack after return:    f5
  Stack use after scope: f8
  Global redzone:        f9
  Global init order:     f6
  Poisoned by user:      f7
  ASan internal:         fe
==154== ABORTING

`one shadow byte represents 8 application bytes` so
x[0], x[1]    to 8 bytes    1st fd
x[2], x[3]    to 16 bytes   2nd fd
x[4], x[5]    to 24 bytes   3rd fd


<ex> `heap buffer overflow`
Let's now examine another classical memory error: buffer overflow. These are
handled differently depending on memory type (stack, static or heap) which
we'll examine separately.

$ cat heap_overflow.c

int main(int argc, char **argv) {
  int *array = new int[100];
  array[0] = 0;
  int res = array[argc + 100];  // Touch memory past end of buffer
  delete [] array;
  return res;
}

$ ssh root@myboard /a.out
=================================================================
==126== ERROR: AddressSanitizer: heap-buffer-overflow on address 0xb5503fb4
at pc 0x8788 bp 0xbefc7cec sp 0xbefc7ce4
READ of size m
    #0 0x8787 (/a.out+0x8787)
    #1 0xb5737bcb (/lib/libc.so.6+0x17bcb)
0xb5503fb4 is located 4 bytes to the right of `400-byte region` [0xb5503e20,0xb5503fb0)
allocated by thread T0 here:
    #0 0xb59a8a77 (/lib/libasan.so.0+0x10a77)
    #1 0x8697 (/a.out+0x8697)
    #2 0xb5737bcb (/lib/libc.so.6+0x17bcb)
Shadow bytes around the buggy address:
  0x36aa07a0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa07b0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa07c0: fa fa fa fa 00 00 00 00 00 00 00 00 00 00 00 00   // 8*12 = 96
  0x36aa07d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00   // 8*16 = 128
  0x36aa07e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00   // 8*16 = 128
=>0x36aa07f0: 00 00 00 00 00 00[fa]fa fa fa fa fa fa fa fa fa   // 8*6  = 48
  0x36aa0800: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0810: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0820: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0830: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x36aa0840: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:     fa
  Heap righ redzone:     fb
  Freed Heap region:     fd
  Stack left redzone:    f1
  Stack mid redzone:     f2
  Stack right redzone:   f3
  Stack partial redzone: f4
  Stack after return:    f5
  Stack use after scope: f8
  Global redzone:        f9
  Global init order:     f6
  Poisoned by user:      f7
  ASan internal:         fe
==126== ABORTING


<e> `stack buffer overflow`
https://github.com/google/sanitizers/wiki/AddressSanitizerExampleStackOutOfBounds

Now to stack buffers:

$ cat stack_oob.c

int main(int argc, char **argv) {
  int stack_array[100];
  stack_array[1] = 0;
  return stack_array[argc + 100];  // Touch memory past end of buffer
}

$ ssh root@myboard /a.out
=================================================================
==132== ERROR: AddressSanitizer: stack-buffer-overflow
on address 0xbec82ccc at pc 0x875c bp 0xbec82b0c sp 0xbec82b04
READ of size 4 at 0xbec82ccc thread T0
    #0 0x875b (/a.out+0x875b)
    #1 0xb56d6bcb (/lib/libc.so.6+0x17bcb)
Address 0xbec82ccc is located at offset 436 in frame <main> of T0's stack:
  This frame has 1 object(s):
    [32, 432) 'stack_array'
HINT: this may be a false positive if your program uses some custom stack unwind mechanism or swapcontext
      (longjmp and C++ exceptions *are* supported)
Shadow bytes around the buggy address:
  0x37d90540: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d90550: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d90560: 00 00 00 f1 f1 f1 f1 00 00 00 00 00 00 00 00 00
  0x37d90570: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d90580: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
=>0x37d90590: 00 00 00 00 00 00 00 00 00[f4]f4 f3 f3 f3 f3 00
  0x37d905a0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d905b0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d905c0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d905d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x37d905e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:     fa
  Heap righ redzone:     fb
  Freed Heap region:     fd
  Stack left redzone:    `f1`
  Stack mid redzone:     f2
  Stack right redzone:   `f3`
  Stack partial redzone: f4
  Stack after return:    f5
  Stack use after scope: f8
  Global redzone:        f9
  Global init order:     f6
  Poisoned by user:      f7
  ASan internal:         fe
==132== ABORTING


<ex> `global buffer overflow`
https://github.com/google/sanitizers/wiki/AddressSanitizerExampleGlobalOutOfBounds
And finally static buffers:

$ cat global_oob.c

int global_array[100] = {-1};
int main(int argc, char **argv) {
  return global_array[argc + 100];  // Touch memory past end of buffer
}

$ ssh root@myboard /a.out
=================================================================
==138== ERROR: AddressSanitizer: global-buffer-overflow
on address 0x00010b54 at pc 0x86f0 bp 0xbee97cf4 sp 0xbee97cec
READ of size 4 at 0x00010b54 thread T0
    #0 0x86ef (/a.out+0x86ef)
    #1 0xb56ccbcb (/lib/libc.so.6+0x17bcb)
0x00010b54 is located 4 bytes to the right of global variable 'global_array (global_oob.c)' (0x109c0) of size 400
Shadow bytes around the buggy address:
  0x20002110: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20002120: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20002130: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20002140: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20002150: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
=>0x20002160: 00 00 00 00 00 00 00 00 00 00[f9]f9 f9 f9 f9 f9
  0x20002170: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20002180: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x20002190: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x200021a0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x200021b0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:     fa
  Heap righ redzone:     fb
  Freed Heap region:     fd
  Stack left redzone:    f1
  Stack mid redzone:     f2
  Stack right redzone:   f3
  Stack partial redzone: f4
  Stack after return:    f5
  Stack use after scope: f8
  Global redzone:        `f9`
  Global init order:     f6
  Poisoned by user:      f7
  ASan internal:         fe
==138== ABORTING


<appendix>: AddressSanitizer Design
AddressSanitizer is supported by two major open-source toolchains: LLVM and GCC (LLVM being the main
development platform). The tool introduces a new compiler pass which instruments the generated
memory operations by inserting instructions to perform memory checks.

Here's the bird's eye view of the performed instrumentation:

// Before
*address = ...;  // or: ... = *address;

// After
if (IsPoisoned(address)) {
  ReportError(address, kAccessSize, kIsWrite);
}
*address = ...;  // or: ... = *address;

The IsPoisoned check is implemented in such a way so as to minimize overhead in common case (i.e.
when memory access is valid). It works by mapping the memory address in question to a bit in a
special memory region called <shadow-memory>:

byte *shadow_address = MemToShadow(address);
byte shadow_value = *shadow_address;
if (shadow_value) {
  if (SlowPathCheck(shadow_value, address, kAccessSize)) {
    ReportError(address, kAccessSize, kIsWrite);
  }
}

// Check the cases where we access first k bytes of the qword
// and these k bytes are unpoisoned.
bool SlowPathCheck(shadow_value, address, kAccessSize) {
  last_accessed_byte = (address & 7) + kAccessSize - 1;
  return last_accessed_byte >= shadow_value);
}

inline byte *MemToShadow(byte *address) {
  return (address >> 3) | ShadowStartOffset;   // ShadowStartOffset depends on the target arch
}

The shadow memory region is invisible to the program and occupies 1/8 of address space . It?s
initialized in the following way:

- bits that correspond to static variables are marked as unpoisoned
- whenever new stack frame is allocated, addresses which correspond to entries are marked as
  unpoisoned; upon frame termination bits are reset Stack and static objects are interspersed with
  redzones (short poisoned memory blocks) to allow for catching of buffer overruns.

To allow checking of heap-allocated memory standard memory routines are overridden by ASan runtime
library (libasan.so) to check/change poisonness of memory regions. For example malloc changes the
status of allocated memory buffer to unpoisoned while free re-poisons it. Here is the full list of
instrumented functions:

// signal.h
sigaction
signal
 
// setjmp.h
longjmp
_longjmp
siglongjmp
__cxa_throw
 

// string.h
memcmp
memmove
memcpy
memset
strchr
strcat
strncat
strcpy
strncpy
strcmp
strncmp
strlen
strcasecmp
strncasecmp
strdup
strnlen
index

// stdlib.h
atoi
atol
strtol
atoll
strtoll
read
pread
pread64
write
pwrite
malloc
free
realloc
calloc
valloc
posix_memalign
 
// pthread.h
pthread_create

// time.h
localtime
localtime_r
gmtime
gmtime_r
ctime
ctime_r
asctime
asctime_r
vscanf
vsscanf
vfscanf
scanf
fscanf
sscanf
 
They also instrument builtin compiler intrinsic e.g. __builtin_memcpy, etc.


<ex-real> `heap buffer-overflow` was detected by ASan:

==136== ERROR: AddressSanitizer: heap-buffer-overflow on address 0xad651b10
at pc 0xb4e03054 bp 0x75c35ac4 sp 0x75c35aac
READ of size 17 at 0xad651b10 thread T457 (homesetting)
    #0 0xb4e03053 (/mtd_exe/lib/libasan.so.0+0xf053)
    #1 0x1281917 (/mtd_exe/exeAPP+0x1281917)
    #3 0x1a27f2f (/mtd_exe/exeAPP+0x1a27f2f)
    #4 0x25d78e3 (/mtd_exe/exeAPP+0x25d78e3)
    #5 0x25d7ad7 (/mtd_exe/exeAPP+0x25d7ad7)
    #6 0x12841bb (/mtd_exe/exeAPP+0x12841bb)
    #7 0x128924b (/mtd_exe/exeAPP+0x128924b)
    #8 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #9 0xae7b50f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
0xad651b1a is located 0 bytes to the right of 10-byte region [0xad651b10,0xad651b1a)
Shadow bytes around the buggy address:
  0x35aca310: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x35aca320: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x35aca330: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x35aca340: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x35aca350: fa fa 00 04 fa fa 00 02 fa fa 00 02 fa fa fa fa
=>0x35aca360: fa fa[00]02 fa fa fa fa fa fa fa fa fa fa fa fa 
  0x35aca370: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x35aca380: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x35aca390: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x35aca3a0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x35aca3b0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:     fa
  Heap righ redzone:     fb
  Freed Heap region:     fd
  Stack left redzone:    f1
  Stack mid redzone:     f2
  Stack right redzone:   f3
  Stack partial redzone: f4
  Stack after return:    f5
  Stack use after scope: f8
  Global redzone:        f9
  Global init order:     f6
  Poisoned by user:      f7
  ASan internal:         fe


Reproducing scenario: play video from usb source. Here is the backtrace
analyzed with `addr2line` tool:

`#0` 0xb4e03053 (/mtd_exe/lib/libasan.so.0+0xf053)
__interceptor_strlen /home/ygribov/build/tools/v4/sources/gcc_1/libsanitizer/asan/asan_interceptors.cc:454

note that 'strlen' in __interceptor_strlen

452   uptr length = REAL(strlen)(s);
453   if (flags()->replace_str) {
`454`     ASAN_READ_RANGE(s, length + 1);
                           ^^^^^^^^^^
455   }

#1 0x1281917 (/mtd_exe/exeAPP+0x1281917)
PCString::Length(char const*)
/Smart/CSP_Smart_2014_Prj/REL_4211/BP_CSP/CSP-4.0/CSP/Src/PCString.cpp:347

#2 0x1a286d7 (/mtd_exe/exeAPP+0x1a286d7)
CAppStateProp::operator=(CAppStateProp&)
m.guseva/TV_2014/DTV/BP_APP/BP_AppCM/Src/Common/AppStateProp.cpp:326

322    char* ntmp = prop.GetStateName();
323   if(ntmp != NULL)
324   {
325       m_pStateName = new char[PCString::Length(ntmp)+1];
`326`       PCString::Copy(m_pStateName, ntmp, PCString::Length(ntmp));
                                             ^^^^^^^^^^^^^^^^^^^^^^
327       m_pStateName[PCString::Length(ntmp)] = '\0';
328     }

So the issue happened in strlen() function called for the CAppStateProp member
during coping from one string to another.

The root cause is in CAppStateProp constructor method where m_pStateName is
initialized without null termitaning symbol. So the proposed fix is to add
null symbol to the Length position.

diff -p AppStateProp.cpp.orig  AppStateProp.cpp
*** AppStateProp.cpp.orig       2013-12-18 12:48:15.284055908 +0400
--- AppStateProp.cpp    2013-12-18 12:48:31.668057275 +0400
*************** CAppStateProp::CAppStateProp(const char*
*** 46,51 ****
--- 46,52 ----
{
  m_pStateName = new char[PCString::Length("UNDEFINED")+1];
  PCString::Copy(m_pStateName, "UNDEFINED", PCString::Length("UNDEFINED"));
+ m_pStateName[PCString::Length("UNDEFINED")] = '\0';
}

m_coexistBanner.clear();


note:
Although can see what was wrong, but not sure how it came to conclusion
that is heap overflow and why the first call of Length was not a problem.


<ex-real> `unknown crash` was detected by ASan:
==215== ERROR: AddressSanitizer: unknown-crash on address 0x8b8eb924
at pc 0xb4e03054 bp 0x789acad4 sp 0x789acabc
READ of size 65 at 0x8b8eb924 thread T247 (webserver)
    #0 0xb4e03053 (/mtd_exe/lib/libasan.so.0+0xf053)
    #1 0x8b8d8f7b (/mtd_exe/WebServerApp/bin/libwebserver.so+0x7f7b)
    #2 0x8b8d9727 (/mtd_exe/WebServerApp/bin/libwebserver.so+0x8727)
    #3 0x8b8d9ffb (/mtd_exe/WebServerApp/bin/libwebserver.so+0x8ffb)
    #4 0x8b8ddecf (/mtd_exe/WebServerApp/bin/libwebserver.so+0xcecf)
    #5 0x7d5c68b (/mtd_exe/exeAPP+0x7d5c68b)
    #6 0x7d471db (/mtd_exe/exeAPP+0x7d471db)
    #7 0x26f095f (/mtd_exe/exeAPP+0x26f095f)
    #8 0x132cca3 (/mtd_exe/exeAPP+0x132cca3)
    #9 0x132cb9b (/mtd_exe/exeAPP+0x132cb9b)
    #10 0x1331c07 (/mtd_exe/exeAPP+0x1331c07)
    #11 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #12 0xae5b70f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
0x8b8eb964 is located 0 bytes to the right of global variable 'g_lighttpd (WebServer.cpp)'
(0x8b8eb920) of size 68
==215== SetCurrent: 0xad89b000 for thread 0x7b4bf430
==215== T485: stack [0x7b480000,0x7b4c0000) size 0x40000; local=0x7b4bed74
Shadow bytes around the buggy address:
  0x3171d6d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x3171d6e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x3171d6f0: 00 00 00 00 00 00 00 00 00==2 00 00 00 f9 f9 f9
  0x3171d700: f9 f9 f9 f9 04 f9 f9 f9 f9 f9 f9 f9 04 f9 f9 f9
 ==215== T486 exited
 ==215== T486 TSDDtor
  0x3171d710: f9 f9 f9 f9 00 00 00 00 00 00 00 00 04 f9 f9
 ==215== T487: stack [0x556e8000,0x558e8000) size 0x200000; local=0x558e6d74
 ==215== SetCurrent: 0x8fa37000 for thread 0x847f3430
 ==215== T488: stack [0x845f4000,0x847f4000) size 0x200000; local=0x847f2d74
=>0x3171d720: f9 f9 f9[00]00 00 00 00 00 00 00 04 f9 f9 f9
==215== T488 TSDDtor
  0x3171d730: f9 f9 f9 f9 04 f9 f9 f9 f9 f9 f9 f9 00 f9 f9 f9
  0x3171d740: f9 f9 f9 f9 01 f9 f9 f9 f9 f9 f9 00 f9 f9 f9
==215== SetCurrent: 0x8fa36000 for thread 0x573ff430
==2 = T489: stack [0x57300000,0x57400000) size 0x100000; local=0x573fed74
==215== T489 exited
==215== T489 TSDDtor
              f9 f9 f9 f9 04 f9 f9 f9 f9 f9 f9 f9 04 f9 f9 f9
  0x3171d760: f9 f9 f9 f9 00 00 00 00 00 00 00 00 00 00 00 00
  0x3171d770: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00

Here is the backtrace analyzed with addr2line tool:

#0 0xb4e03053 (/mtd_exe/lib/libasan.so.0+0xf053)
__interceptor_strlen
/home/ygribov/build/tools/v4/sources/gcc_1/libsanitizer/asan/asan_interceptors.cc:454
453   if (flags()->replace_str) {
454     ASAN_READ_RANGE(s, length + 1);
455   }

#1 0x8b8d8f7b (/mtd_exe/WebServerApp/bin/libwebserver.so+0x7f7b)
ProcessLauncher
/home2/m.guseva/TV_2014/AP/MAIN2014/Smart/AP_WP_Smart_2014_Prj/REL/AP_WebServer/WebServer
/Swift/src/runtime/webserver/WebServer.cpp:297
294   char k_command_str[80] = {'\0'}; //"-k + secure_db_key + '\0' = 36 bytes;
295   strncat(k_command_str,"-k ", 3);
296   strncat(k_command_str,_secure_db_key, strlen(_secure_db_key));
297  //logger().debug("WEBSERVER: k_command str: %s\n",k_command_str);

...

#6 0x7d471db (/mtd_exe/exeAPP+0x7d471db)
CWebServerAppBase::t_OnUserMessage(PTEvent const*)
/home2/m.guseva/TV_2014/DTV/AP_WP/AP_WebServer/WebServer/Src/WebServerAppBase.cpp:525
521  //Retrieve key from secure storage and set for Lighttpd and Pairing plugin.
522  std::string secureKey = getSecureDBKey();  <here>
523  WebServerSetDBKey(secureKey.c_str());
524
525  WebServerStart();
526  WebServerLoadModule("libfcgicallbackplugin.so",
                                     (void*)WebConv_internal_dispatch );
527  WS_DEBUG("Pre call to WebServerLoadModule");
528  WebServerLoadModule("libpairingplugin.so",(void*)GetPINData);

The issue happened in strlen() function called for the _secure_db_key field which was set to the
string secureKey (converted to C-string char*) via WebServerSetDBKey() call.

The root cause is in WebServer and Lighttpd classes definitions and setLighttpdDBKey() member
functions. The proposed fix is below:

$ diff WebServer.h.orig WebServer.h
96a97
> #define SECURE_DB_KEY_LEN 64
121c122
<     char _secure_db_key[64];
---
>     char _secure_db_key[SECURE_DB_KEY_LEN+1];

$ diff WebServer.cpp.orig WebServer.cpp
88a89
>
285c286
<               char _secure_db_key[64];
---
>               char _secure_db_key[SECURE_DB_KEY_LEN+1];
289c290
<                       memset(_secure_db_key,'\0',64);
---
>                       memset(_secure_db_key,'\0', SECURE_DB_KEY_LEN+1);
449c450,451
< memcpy(_secure_db_key,db_key_value,64);
---
> memcpy(_secure_db_key,db_key_value,SECURE_DB_KEY_LEN);
>                           _secure_db_key[SECURE_DB_KEY_LEN] = '\0';
506c508
<         memset(_secure_db_key,'\0',64);
---
>         memset(_secure_db_key,'\0', SECURE_DB_KEY_LEN+1);
601c603,604
<             memcpy(_secure_db_key,db_key_value,64);
---
>             memcpy(_secure_db_key,db_key_value,SECURE_DB_KEY_LEN);
>             _secure_db_key[SECURE_DB_KEY_LEN] = '\0';
 

<ex-real> `heap buffer-overflow` was detected by ASan:
==214== ERROR: AddressSanitizer: heap-buffer-overflow on address 0x75929a70
at pc 0x3b6853c bp 0x533fed54 sp 0x533fed44
READ of size 4 at 0x75929a70 thread T488 (SMPA_CPHandover)
    #0 0x3b6853b (/mtd_exe/exeAPP+0x3b6853b)
    #1 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #2 0xae5b70f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
0xad38d7d2 is located 0 bytes to the right of 2-byte region [0xad38d7d0,0xad38d7d2)
allocated by thread T399 (SMP_AsyncSelect) here:
    #0 0x3b6853b (/mtd_exe/exeAPP+0x3b6853b)
    #1 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #2 0xae5b70f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
0x75929a72 is located 0 bytes to the right of 2-byte region [0x75929a70,0x75929a72)
allocated by thread T399 (SMP_AsyncSelect) here:
Thread T487 (SMPA_CPHandover) created by T399 (SMP_AsyncSelect) here:
Thread T488 (SMPA_CPHandover) created by T399 (SMP_AsyncSelect) here:
    #0 0xb4dff22f (/mtd_exe/lib/libasan.so.0+0xb22f)
    #1 0x3b67d5b (/mtd_exe/exeAPP+0x3b67d5b)
    #2 0x3b6014f (/mtd_exe/exeAPP+0x3b6014f)
    #3 0x3bd6a4b (/mtd_exe/exeAPP+0x3bd6a4b)
    #4 0x3c1116b (/mtd_exe/exeAPP+0x3c1116b)
    #5 0x3c16beb (/mtd_exe/exeAPP+0x3c16beb)
    #6 0x3c2d493 (/mtd_exe/exeAPP+0x3c2d493)
    #7 0x3ca049f (/mtd_exe/exeAPP+0x3ca049f)
    #8 0x3ca0ac7 (/mtd_exe/exeAPP+0x3ca0ac7)
    #9 0x3c2dc73 (/mtd_exe/exeAPP+0x3c2dc73)
    #10 0x3c9c713 (/mtd_exe/exeAPP+0x3c9c713)
    #11 0x3c9d46f (/mtd_exe/exeAPP+0x3c9d46f)
    #12 0x3c2f933 (/mtd_exe/exeAPP+0x3c2f933)
    #13 0x3c785ef (/mtd_exe/exeAPP+0x3c785ef)
    #14 0x3c610e3 (/mtd_exe/exeAPP+0x3c610e3)
    #15 0x3c60487 (/mtd_exe/exeAPP+0x3c60487)
    #16 0x3c606cb (/mtd_exe/exeAPP+0x3c606cb)
    #17 0x3c6090f (/mtd_exe/exeAPP+0x3c6090f)
    #18 0x3c53c13 (/mtd_exe/exeAPP+0x3c53c13)
    #0 0xb4dff22f    #19 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0xb22f)
 (/mtd_exe/lib/libasan.so.0+0x1d26b)
 (/mtd_exe/exeAPP+0x3b67d5b)
    #2 0x3b5fb87 (/mtd_exe/exeAPP+0x3b5fb87)
    #3 0x3bd6a4b (/mtd_exe/exeAPP+0x3bd6a4b)
    #4 0x3c1116b (/mtd_exe/exeAPP+0x3c1116b)
    #5 0x3c16beb
    #6 0x3c2d493 created by T395 (dualtv) here:
    #7 0x3ca049f (/mtd_exe/exeAPP+0x3ca049f)
    #8 0x3ca0ac7 (/mtd_exe/exeAPP+0x3ca0ac7)
    #9 0x3c2dc73 (/mtd_exe/exeAPP+0x3c2dc73)
    #10 0x3c9c713 (/mtd_exe/exeAPP+0x3c9c713)
    #11 0x3c9d46f (/mtd_exe/exeAPP+0x3c9d46f)
    #12 0x3c2f933 (/mtd_exe/exeAPP+0x3c2f933)
    #13 0x3c785ef (/mtd_exe/exeAPP+0x3c785ef)
    #14 0x3c610e3 (/mtd_exe/exeAPP+0x3c610e3)
    #15 0x3c60487 (/mtd_exe/exeAPP+0x3c60487)
    #16 0x3c606cb (/mtd_exe/exeAPP+0x3c606cb)
    #17 0x3c6090f (/mtd_exe/exeAPP+0x3c6090f)
    #18 0x3c53c13 (/mtd_exe/exeAPP+0x3c53c13)
    #19 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #20 0xae5b70f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
Shadow bytes around the buggy address:
  0x2eb252f0: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x2eb25300: fa fa 00 04 fa fa 00 04 fa fa 00 04 fa fa 00 04
  0x2eb25310: fa fa 00 04 fa fa 00 04 fa fa fd fd fa fa fd fd
  0x2eb25320: fa fa fd fd fa fa fd fa fa fa fd fd fa fa fd fd
=>0x2eb25340: fa fd fa fa fa fa fa[02]fa fa fa fa fd fd fa fa


Here is the backtrace analyzed with addr2line tool:

note: 
Follow T488.

#0 0x3b6853b (/mtd_exe/exeAPP+0x3b6853b)
_SMPACPHandOver(void*)
/AP_CNC/AP_DLNA/SMP/smpadaptation/src/controlpoint/SMPACPInternals.cpp:2455

2453         pstCPActionId = ( stCP_ActionIdList * ) SMPMemAllocMA( sizeof( stCP_ActionIdList ) );
2454         pstCPActionId->cpHandle = nCPHandle;
2455         pstCPActionId->actionId = ((ControlResult*)pCbInfo)->nActionId;

#1 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
`__asan::ASanThread::ThreadStart`()
/home/ygribov/build/tools/v4/sources/gcc_1/libsanitizer/asan/asan_thread.cc:99

   #1 0x3b67d5b (/mtd_exe/exeAPP+0x3b67d5b)
   `_SMPASendAppInfo`(int nCPHandle, int nMessageType, void* pCbInfo)

   //AP_DLNA/SMP/smpadaptation/src/controlpoint/SMPACPInternals.cpp:2579
   2571 pstAppCbArgs->pCbInfo = pCbInfo;
   ...
   2578 SMPADebug(SMP_INFO_LEVEL,"[_SMPASendAppInfo] Creating Thread");
   2579 nRet = pthread_create( &pHandOverTask, &pHandOverAttrTask,
                     _SMPACPHandOver, (SMPVoid *)pstAppCbArgs);
   
   #2 0x3b6014f (/mtd_exe/exeAPP+0x3b6014f)
   SMPAGenericDeviceNotifyCb(int, SMP_EVENT, SMP_ERROR_CODE, stSMPAppList*)

   //AP_DLNA/SMP/smpadaptation/src/controlpoint/SMPAControlPoint.cpp:327
   324 SMPInt8* pCopiedDeviceHandle = NULL;
   325 SMPString* pDeviceHandleString = new SMPString((SMPChar*)pDeviceHandle);
   326 pCopiedDeviceHandle = (SMPInt8*)pDeviceHandleString->GetString();                          
   327 nErrorCode = _SMPASendAppInfo( nIntendedCP, DEVICE_ADDED, pCopiedDeviceHandle );

The root cause is in SMPAGenericDeviceNotifyCb() function when
_SMPASendAppInfo() is called. The proposed fix is below:

$ diff SMPAControlPoint.cpp.orig SMPAControlPoint.cpp
227,228c227,228
<        SMPInt8* pCopiedDeviceHandle = NULL;
<
---
>        SMPInt8* pCopiedDeviceHandle = (SMPInt8*)malloc(16);
>        SMPMemSet(pCopiedDeviceHandle, 0, 16);
230c230,232
<        pCopiedDeviceHandle = (SMPInt8*)pDeviceHandleString->GetString();
---
>        SMPSize  DeviceHandleLength = pDeviceHandleString->GetStringLength();
>        if (DeviceHandleLength > 16) DeviceHandleLength = 16;
>        SMPMemCpy(pCopiedDeviceHandle, pDeviceHandleString->GetString(), DeviceHandleLength);
324,326c326,331
<  SMPInt8* pCopiedDeviceHandle = NULL;
<  SMPString* pDeviceHandleString = new SMPString((SMPChar*)pDeviceHandle);
<  pCopiedDeviceHandle = (SMPInt8*)pDeviceHandleString->GetString();
---
>  SMPInt8* pCopiedDeviceHandle = (SMPInt8*)malloc(16);
>  SMPMemSet(pCopiedDeviceHandle, 0, 16);
>  SMPString* pDeviceHandleString = new SMPString((SMPChar*)pDeviceHandle);
>  SMPSize  DeviceHandleLength = pDeviceHandleString->GetStringLength();
>  if (DeviceHandleLength > 16) DeviceHandleLength = 16;
>  SMPMemCpy(pCopiedDeviceHandle, pDeviceHandleString->GetString(), DeviceHandleLength);


<ex-real> `stack buffer-overflow` was detected by ASan:
==239== ERROR: AddressSanitizer: stack-buffer-overflow on address 0xab43da68
at pc 0xb4e023f4 bp 0xab43d644 sp 0xab43d224
READ of size 4 at 0xab43da68 thread T45 (AppInitializer)
    #0 0xb4e023f3 (/mtd_exe/lib/libasan.so.0+0xe3f3)
    #1 0x143e08f (/mtd_exe/exeAPP+0x143e08f)
    #2 0x144a547 (/mtd_exe/exeAPP+0x144a547)
    #3 0x1448cc3 (/mtd_exe/exeAPP+0x1448cc3)
    #4 0x5fb86bb (/mtd_exe/exeAPP+0x5fb86bb)
    #5 0x5fb756f (/mtd_exe/exeAPP+0x5fb756f)
    #6 0x4be0477 (/mtd_exe/exeAPP+0x4be0477)
    #7 0x4b646bf (/mtd_exe/exeAPP+0x4b646bf)
    #8 0x5b3fa7f (/mtd_exe/exeAPP+0x5b3fa7f)
    #9 0x5ad6ceb (/mtd_exe/exeAPP+0x5ad6ceb)
    #10 0x5ae930b (/mtd_exe/exeAPP+0x5ae930b)
    #11 0x4be7143 (/mtd_exe/exeAPP+0x4be7143)
    #12 0x4b64b3f (/mtd_exe/exeAPP+0x4b64b3f)
    #13 0x46eb767 (/mtd_exe/exeAPP+0x46eb767)
    #14 0x1433bbf (/mtd_exe/exeAPP+0x1433bbf)
    #15 0x1438c4f (/mtd_exe/exeAPP+0x1438c4f)
    #16 0xb4e1126b (/mtd_exe/lib/libasan.so.0+0x1d26b)
    #17 0xae2a00f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
==239== T43 exited
==239== T43 TSDDtor
Address 0xab43da68 is located at offset 416 in frame <ResetComponentsProfile> of T45's stack:
  This frame has 55 object(s):
    [32, 33) 'wlanEncryptionInfo'
    [96, 97) 'WFDSetupInfo'
    [160, 161) 'wlanSetupDone'
    [224, 225) 'moip_auto_start'
    [288, 289) 'moip_update_available'
    [352, 353) 'ucRecognitionGestureHelpbar'
    [416, 417) 'ucCamPosition'
    [480, 481) 'val'
    [544, 545) 'ucRecognitionVoice'
    [608, 609) 'ucRecognitionVoiceTVWakeOn'
    [672, 673) 'ucRecognitionVoiceHelpbar'
    [736, 737) 'ucRecognitionGesture'
    [800, 801) 'ucRecognitionFace'
    [864, 865) 'ucRecognitionFirstRecognized'
    [928, 929) 'b3DGlassPaired'
    [992, 993) 'eDTVStatus'
    [1056, 1057) 'MasterNum'
    [1120, 1121) 'TVNumForSlave'
    [1184, 1188) 'cec_enable_option'
    [1248, 1252) 'cec_auto_standby_option'
    [1312, 1316) 'cec_receiver_auto_turn_on_option'
    [1376, 1380) 'nRecognitionVoiceLanguage'
    [1440, 1444) 'nRecognitionVoiceMagicWord'
    [1504, 1508) 'nRecognitionGestureHandTheme'
    [1568, 1572) 'nRecognitionGesturePointerSpeed'
    [1632, 1636) 'nSensitivityLevel'
    [1696, 1700) 'nShopMode'
    [1760, 1764) 'nHowlingLevel'
    [1824, 1828) 'uiVFreq'
    [1888, 1892) 'nRecognitionFrequencey'
    [1952, 1956) 'nEnableTTS'
    [2016, 2020) 'nTTSSpeaker'
    [2080, 2084) 'nVolume'
    [2144, 2148) 'nSpeed'
    [2208, 2212) 'nEnableTTS_tv'
    [2272, 2276) 'nTTSSpeaker_tv'
    [2336, 2340) 'nVolume_tv'
    [2400, 2404) 'nSpeed_tv'
    [2464, 2468) 'nVoiceGuideUserLevel'
    [2528, 2532) 'nHandWlang'
    [2592, 2596) 'tValue'
    [2656, 2662) 'TVAddrForSlave'
    [2720, 2728) 'pms'
    [2784, 2812) 'NetworkDescriptor'
    [2848, 2876) 'NetworkDescriptor_2'
    [2912, 2940) 'WFDDescriptor'
    [2976, 3004) 'CEC_Descriptor'
    [3040, 3068) 'MoIP_Descriptor'
    [3104, 3132) 'MotionApp_Descriptor'
    [3168, 3196) 'VoiceApp_Descriptor'
    [3232, 3260) 'RecognitionMW_Descriptor'
    [3296, 3324) 'BluetoothDescriptor_3DDTVStatus'
    [3360, 3388) 'BluetoothDescriptor_MasterNum'
    [3424, 3452) 'BluetoothDescriptor_tvAddrForSlave'
    [3488, 3516) 'BluetoothDescriptor_MasterNumForSlave'
HINT: this may be a false positive if your program uses some custom stack unwind mechanism or swapcontext
      (longjmp and C++ exceptions *are* supported)
Shadow bytes around the buggy address:
  0x35687af0: f3 f3 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x35687b00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x35687b10: 00 00 00 00 00 00 00 00 00 f1 f1 f1 f1 01 f4 f4
  0x35687b20: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
  0x35687b30: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
=>0x35687b40: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2[01]f4 f4
  0x35687b50: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
  0x35687b60: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
  0x35687b70: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
  0x35687b80: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4
  0x35687b90: f4 f2 f2 f2 f2 01 f4 f4 f4 f2 f2 f2 f2 01 f4 f4


Here is the backtrace analyzed with addr2line tool:

#0 0xb4e023f3 (/mtd_exe/lib/libasan.so.0+0xe3f3)
`__interceptor_memcpy` 
/build/tools/v4/sources/gcc_1/libsanitizer/asan/asan_interceptors.cc:288
288     ASAN_READ_RANGE(from, size);
289     ASAN_WRITE_RANGE(to, size);

#1 0x143e08f (/mtd_exe/exeAPP+0x143e08f)
CSP0400::PCParcel::Write(void const*, int)
.../PCParcel.cpp:340

#2 0x144a547 (/mtd_exe/exeAPP+0x144a547)
CCPMSProxy::Set(int profile, unsigned char const* pBuffer, unsigned int size)
/home2/m.guseva/TV_2014/DTV/BP_MW/BP_CommSS/Src/PMS/PROXY/PMSProxy.cpp:177
176         CHECK_MARSHALLING(data_parcel.WriteInt32(size));
177         CHECK_MARSHALLING(data_parcel.Write(pBuffer, size));
178

#3 0x1448cc3 (/mtd_exe/exeAPP+0x1448cc3)
CCPMS::Set(int profile, unsigned char const* pBuffer, unsigned int size)
/home2/m.guseva/TV_2014/DTV/BP_MW/BP_CommSS/Src/PMS/COMMON/PMS.cpp:75
 73 {
 74         CC_INT_ASSERT(m_pImp);
 75         return m_pImp->Set(profile, pBuffer, size);
 76 }

#4 0x5fb86bb (/mtd_exe/exeAPP+0x5fb86bb)
CAPCNCTaskManager::ResetComponentsProfile()
.../APCNCTaskConfig.cpp:1151
1150  unsigned char ucCamPosition = 0;
1151  if (pms.Set(PROFILE_MOTIONAPP_CAMERA_POSITION, &ucCamPosition,
               PROFILE_SIZE_MOTIONAPP_CAMERA_POSITION) == false)

So the issue happened in memcpy() function. The proposed workaround:

$diff APCNCTaskConfig.cpp.orig APCNCTaskConfig.cpp
1151c1151
<       if (pms.Set(PROFILE_MOTIONAPP_CAMERA_POSITION, &ucCamPosition,
               PROFILE_SIZE_MOTIONAPP_CAMERA_POSITION) == false)
---
>       if (pms.Set(PROFILE_MOTIONAPP_CAMERA_POSITION, &ucCamPosition,
               sizeof(ucCamPosition)) == false)


<ex-real> `stack buffer-overflow` was detected by ASan:
<strncpy>
==283== ERROR: AddressSanitizer: heap-buffer-overflow on address 0x7c25d990
at pc 0xb4e02520 bp 0x989fe85c sp 0x989fe43c
READ of size 4 at 0x7c25d990 thread T145 (CnS Server Mana)
==283== SetCurrent: 0x9c230000 for thread 0x641ff430
==283== T503: stack [0x64100000,0x64200000) size 0x100000; local=0x641fed74
0x7d519d53 is located 0 bytes to the right of 3-byte region [0x7d519d50,0x7d519d53)
allocated by thread T144 (CnS Server Mana) here:
    #0 0xb4e0251f (/mtd_exe/lib/libasan.so.0+0xe51f)
    #1 0xa8cde413 (/mtd_exe/Comp_LIB/libjson.so+0x20413)
    #2 0xa8bf81f7 (/mtd_exe/lib/libstdc++.so.6.0.17+0xa31f7)
    #3 0x7cb85c3 (/mtd_exe/exeAPP+0x7cb85c3)
    #4 0x7cb8803 (/mtd_exe/exeAPP+0x7cb8803)
    #5 0x7c73cbf (/mtd_exe/exeAPP+0x7c73cbf)
    #6 0x7c8e13f (/mtd_exe/exeAPP+0x7c8e13f)
    #7 0x7c1d60f (/mtd_exe/exeAPP+0x7c1d60f)
    #8 0x7c1e74b (/mtd_exe/exeAPP+0x7c1e74b)
    #9 0x158c3db (/mtd_exe/exeAPP+0x158c3db)
    #10 0x158c2d3 (/mtd_exe/exeAPP+0x158c2d3)
    #11 0x159133f (/mtd_exe/exeAPP+0x159133f)
    #12 0xb4e11397 (/mtd_exe/lib/libasan.so.0+0x1d397)
    #13 0xa93070f7 (/mtd_exe/lib/libpthread-2.17.so+0x70f7)
Shadow bytes around the buggy address:
  0x2faa3350: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fd fd
  0x2faa3360: fa fa fa fa fa fa fa fa fa fa fd fd fa fa fd fd
  0x2faa3370: fa fa fd fd fa fa fd fa fa fa fd fa fa fa fa fa
  0x2faa3380: fa fa fd fd fa fa fa fa fa fa fd fd fa fa fd fd
  0x2faa3390: fa fa fd fd fa fa fd fd fa fa fd fd fa fa fd fd
=>0x2faa33a0: fa fa fd fd fa fa 00 04 fa fa[03]fa fa fa fa fa
  0x2faa33b0: fa fa fd fa fa fa fd fd fa fa fa fa fa fa fd fd
  0x2faa33c0: fa fa fa fa fa fa fd fd fa fa fa fa fa fa fa fa
  0x2faa33d0: fa fa fa fa fa fa fd fa fa fa fd fd fa fa fd fa
  0x2faa33e0: fa fa fd fd fa fa fd fa fa fa fd fd fa fa fa fa
  0x2faa33f0: fa fa fd fa fa fa fd fd fa fa fd fa fa fa fd fd

Here is the backtrace analyzed with addr2line tool:

#0 0xb4e0251f (/mtd_exe/lib/libasan.so.0+0xe51f)
`__interceptor_memcpy`
/fox-p/vd47_a15/toolchain/build.arm.cortex-a15/sources/gcc_1/libsanitizer/
asan/asan_interceptors.cc:288
288     ASAN_READ_RANGE(from, size);
289     ASAN_WRITE_RANGE(to, size);

#1 0xa8cde413 (/mtd_exe/Comp_LIB/libjson.so+0x20413)
char* std::string::_S_construct<char const*>(char const*, char const*, std::allocator<char> const&, std::forward_iterator_tag)
??:?

#2 0xa8bf81f7 (/mtd_exe/lib/libstdc++.so.6.0.17+0xa31f7)
_S_construct_aux<const char*>
/fox-p/vd47_a15/toolchain/build/obj/gcc_final/arm-v7a15v4r3-linux-gnueabi/
libstdc++-v3/include/bits/basic_string.h:1722

#3 0x7cb85c3 (/mtd_exe/exeAPP+0x7cb85c3)
CCNSVTP::PrintVTPLocal(CCNSVTP::CCNSVTP_TYPE type, char* str)
/GOLFP_2014/AP_MM/AP_ConversationApp/ClientAgent/Src/CnSProfile.cpp:416
414         if (NULL != str)^M
415         {^M
416                 std::string str_text = (std::string)str;^M

#4 0x7cb8803 (/mtd_exe/exeAPP+0x7cb8803)
CCNSVTP::PrintVTP(CCNSVTP::CCNSVTP_TYPE type, char * str, int bool_result)
/GOLFP_2014/AP_MM/AP_ConversationApp/ClientAgent/Src/CnSProfile.cpp:241
241   PrintVTPLocal(VTP_CONVERSATION_STATE_DATA,str);^M
242   break;^M


#5 0x7c73cbf (/mtd_exe/exeAPP+0x7c73cbf)
CCnSParsePacket::t_ParseConnectionInfo(std::string&)
/GOLFP_2014/AP_MM/AP_ConversationApp/ClientAgent/Src/CnSParsePacket.cpp:639
635   char * pChar = NULL ;
636   int length = stConfServer.szStat.size();
637   pChar = new char[length+1];
638  ::strncpy(pChar,stConfServer.szStat.c_str(),length);
639  CCNSVTP::GetInstance()->PrintVTP(CCNSVTP::CCNSVTP_TYPE_STATE_DATA,pChar,0);

According the strncpy POSIX specification:
If there is no null byte in the first n bytes of the array pointed to by s2,
   the result will not be null-terminated.

So the pChar array actually is not null terminated. The proposed fix is:

$ diff CnSParsePacket.cpp.orig CnSParsePacket.cpp
638c638
<  ::strncpy(pChar,stConfServer.szStat.c_str(),length);
---
>  ::strncpy(pChar,stConfServer.szStat.c_str(),length+1);


{KSAN}
Preface
Kernel address sanitizer is a solution suitable to catch bugs known as ?use
after free?, ?buffer-overflow?, ?use of non initialized memory?, etc. This
system software helps to inspect kernel code dynamically during its
execution. Testing of VDLinux kernel with KASAN has showed that it is a
good and efficient tool that make it possible to find really serious bugs in
kernel code (including Linux kernel generic functions).
The scope of issues that can be detected is much higher than in other
memory checking solutions, the performance is also a good side of KASAN
[1].
So, use of the tool can greatly help developers to fix most of mistakes made
during their projects and prepare the solutions to be integrated in the final
product.

Now KASAN supports a number of checkers: stack checker, SLUB checker, vmalloc checker and buddy-level checker.

Preparation
To check the kernel with KASAN you must perform a number of simple
preliminary steps (KASAN patches must be applied before the first step of the
following list):
1. Enable CONFIG_SLUB_DEBUG option in the kernel ?.config? file
(KASAN can work wihout the option, but it is highly recommended to
run KASAN with slub debug functionality enabled);
2. Enable CONFIG_KASAN option to turn on basic KASAN support in the
kernel;
3. Enable CONFIG_KASAN_SLUB option if SLUB-related analysis is
necessary;
4. Enable CONFIG_KASAN_VMALLOC option if vmalloc-related analysis is
necessary;
5. Enable CONFIG_KASAN_STACK option to turn on stack issues detection
6. Enable CONFIG_KASAN_GLOBALS option for usage of global variables
checker;
7. Enable CONFIG_KASAN_UAR option to use use-after-return checker;
8. Build the kernel and modules with appropriate toolchain which
supports KASAN instrumentation options
(arm-v7a15v5r2-linux-gnueabi).
Note
If you want to check the whole kernel enable the option
CONFIG_KASAN_SANITIZE_ALL. In other cases see the section ?Usage of
KASAN with Separate Kernel Files?.
KASAN-related build options enable all necessary toolchain flags. As a result
after these steps KASAN-instrumented kernel will be ready for debugging.
Note
On Orsay platform kernel image can be larger than available mmc partition,
in this case compressed kernel image should be used. An additional option is
required to be disabled: CONFIG_AUTO_ZRELADDR.
To run user space software with instrumented kernel it is necessary to rebuild
all kernel modules to instrument them with KASAN, because stack
instrumentation changes size of thread_info structure and this change affects
all the system. If some modules are not rebuilt the system cannot be
started properly.
Separate Instrumentation
It is possible to instrument only loads or stores by usage of special toolchain
parameters. The syntax for this parameters is:
--param <parameter name>=<value>
Where <parameter name> in this case can be asan-instrument-reads and
asan-instrument-writes and <value> is ?0? or ?1?. To enable disable
instrumentation of writes, for example, it is enough to provide an additional
parameter to the compiler:
--param asan-instrument-writes=0
To disable instrumentation of reads or writes in the context of the kernel it is
necessary to provide additional CFLAGS for the whole kernel or for separate
modules.
Usage of KASAN with Separate Kernel Files
To instrument only a set of files with KASAN the option
CONFIG_KASAN_SANITIZE_ALL should be disabled. For each file that should
be instrumented corresponding Makefile should include an entry of the
following format:
KASAN_SANITIZE_<object file name>.o := y
<object file name> here is a file name of the object file to be instrumented.

Example of Analysis
When all preliminary work is done it is possible to check the kernel with
KASAN. Kernel should be flashed to MMC using SERET ordinary way,
modules should be changed to instrumented modules (see the point 7 in
Preparation part of the document).
KASAN analysis is an automatic activity, no assistance of human is required.
For every detected issue KASAN prints a report which has typical structure
regardless of type of the issue.

Typical report has the following format:
AddressSanitizer: buffer overflow in kasan_do_bo_kmalloc+0x58/0x7c at addr
e07d0012
=============================================================================
BUG kmalloc-64 (Tainted: G B ): kasan error
-----------------------------------------------------------------------------
INFO: Allocated in kasan_do_bo_kmalloc+0x34/0x7c age=0 cpu=0 pid=1
        kasan_do_bo_kmalloc+0x34/0x7c
        kasan_tests_init+0x10/0x44
        do_one_initcall+0x11c/0x2bc
        kernel_init_freeable+0x29c/0x350
        kernel_init+0x14/0xf8
        ret_from_fork+0x14/0x3c
INFO: Freed in kasan_tests_init+0xc/0x44 age=1 cpu=0 pid=1
        do_one_initcall+0x11c/0x2bc
        kernel_init_freeable+0x29c/0x350
        kernel_init+0x14/0xf8
        ret_from_fork+0x14/0x3c
INFO: Slab 0xe1719a00 objects=16 used=1 fp=0xe07d0100 flags=0x0080
INFO: Object 0xe07d0000 @offset=0 fp=0xe07d0f00
Object e07d0000: 00 0f 7d e0 6e 5f 74 65 73 74 73 5f 69 6e 69
74 ..}.n_tests_init Object e07d0010: 00 5f 63 61 63 68 65 00 00 00 00 00 00 00
00 00 ._cache.........
Object e07d0020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 ................
Object e07d0030: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 ................
Padding e07d00e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 ................
Padding e07d00f0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 ................
CPU: 0 PID: 1 Comm: swapper/0 Tainted: G B 3.16.0-rc1+ #129
[<8001a7c0>] (unwind_backtrace) from [<80015e6c>] (show_stack+0x14/0x20)
[<80015e6c>] (show_stack) from [<806bd594>] (dump_stack+0xa4/0xcc) [<806bd594>]
(dump_stack) from [<8014de0c>] (kasan_report_error+0x300/0x364) [<8014de0c>]
(kasan_report_error) from [<8014d3a0>] (check_memory_region+0x16c/0x234)
[<8014d3a0>] (check_memory_region) from [<8014df4c>]
(kasan_do_bo_kmalloc+0x58/0x7c) [<8014df4c>] (kasan_do_bo_kmalloc) from
[<808515b8>] (kasan_tests_init+0x10/0x44) [<808515b8>] (kasan_tests_init) from
[<80008d48>] (do_one_initcall+0x11c/0x2bc) [<80008d48>] (do_one_initcall) from
[<8083e0c8>] (kernel_init_freeable+0x29c/0x350)
[<8083e0c8>] (kernel_init_freeable) from [<806b6a30>] (kernel_init+0x14/0xf8)
[<806b6a30>] (kernel_init) from [<800109b8>] (ret_from_fork+0x14/0x3c) Write of
size 1 by thread T1:
Memory state around the buggy address:
e07cfd80: fd fd 00 00 00 00 00 00 00 00 00 00 00 fc fc fc
e07cfe00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
e07cfe80: fc 00 00 00 00 00 00 00 00 00 00 00 fc fc fc fc
e07cff00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
e07cff80: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
>e07d0000: 00 00 01 fc fc fc fc fc fc fc fc fc fc fc fc fc
^
e07d0080: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
e07d0100: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
e07d0180: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
e07d0200: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
e07d0280: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
==================================================================

This listing corresponds to newly detected buffer overflow issue at the
address 0xE07D0012. This log shows allocation/deallocation paths of the
memory, memory state around the address 0xE07D0012 and state of the
shadow memory.
Thus, it is possible to realize what kind of issue was found, which client
allocated the memory and which freed, what call-chain caused the issue and
why KASAN considers this behavior as a bug (shadow memory state helps to
understand the reason).
This shadow memory state shows that seven bytes correspond to redzone and
only one is allocated (because state of shadow memory entry corresponding
to the address is 0x01).
All possible states of shadow memory entries are defined in
?mm/kasan/kasan.h?.

#define KASAN_FREE_PAGE 0xFF /* page was freed */
#define KASAN_PAGE_REDZONE 0xFE /* redzone for kmalloc_large allocations
*/
#define KASAN_SLAB_REDZONE 0xFD /* Slab page redzone, does not belong to
any slub object */
#define KASAN_KMALLOC_REDZONE 0xFC /* redzone inside slub object */
#define KASAN_KMALLOC_FREE 0xFB /* object was freed
(kmem_cache_free/kfree) */
#define KASAN_SLAB_FREE 0xFA /* free slab page */
#define KASAN_SHADOW_GAP 0xF9 /* address belongs to shadow memory */
#define KASAN_VMALLOC_REDZONE 0xF8 /* address belongs to vmalloc guard page
*/
#define KASAN_VMALLOC_FREE 0xF7 /* memory was freed by vfree call */
/* Stack redzones */
#define KASAN_STACK_LEFT 0xF1
#define KASAN_STACK_MID 0xF2
#define KASAN_STACK_RIGHT 0xF3
#define KASAN_STACK_PARTIAL 0xF4

SLUB issues are represented by allocation/deallocation paths, backtrace
(sometimes it is not available), states of memory and shadow memory.
BUDDY (page) issues are represented by ?dump_page out?, backtrace and
shadow memory state.
Stack issues are represented by a backtrace and shadow memory state.
In all cases shadow memory state will be available for analysis.
Note
During development of KASAN 14_VDFuture project's team has found a
number of issues in the very core of the Linux kernel. We think that it is
usable and very effective tool appropriate for kernel engineers with any level
of expertise.
Often it is enough to know the address where the problem is happened, type
of the issue and a backtrace to fix it. Shadow memory helps to understand the
problem little bit deeper.

Examples of Real World Issues
To demonstrate an approach to find issues with KASAN a number of
real-world issues are described. All these issues were fixed and included in
KASAN patch set.
Lets look at log messages generated by KASAN stack checker:
First log:
==================================================================
AddressSanitizer: out of bounds on stack in idr_for_each+0x168/0x1d0 at addr
dfe49d3c
CPU: 0 PID: 128 Comm: fsnotify_mark Not tainted 3.16.0-rc3+ #256
[<8001a530>] (unwind_backtrace) from [<80015bf0>] (show_stack+0x14/0x20)
[<80015bf0>] (show_stack) from [<80699ad8>] (dump_stack+0x90/0xa0)
[<80699ad8>] (dump_stack) from [<8013dff8>] (kasan_report_error+0x158/0x3a4)
[<8013dff8>] (kasan_report_error) from [<8013d764>]
(check_memory_region+0x148/0x210)
[<8013d764>] (check_memory_region) from [<80336628>] (idr_for_each+0x168/0x1d0)
[<80336628>] (idr_for_each) from [<8019e688>]
(inotify_free_group_priv+0x28/0x70)
[<8019e688>] (inotify_free_group_priv) from [<8019b944>]
(fsnotify_final_destroy_group+0x38/0x50)
[<8019b944>] (fsnotify_final_destroy_group) from [<8019c6a0>]
(fsnotify_put_mark+0x50/0x7c)
[<8019c6a0>] (fsnotify_put_mark) from [<8019c894>]
(fsnotify_mark_destroy+0x1c8/0x254)
[<8019c894>] (fsnotify_mark_destroy) from [<8005b7c8>] (kthread+0x19c/0x1bc)
[<8005b7c8>] (kthread) from [<80010838>] (ret_from_fork+0x14/0x3c)
Read of size 4 by thread T128:
Memory state around the buggy address:
dfe49a80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
dfe49b00: 00 00 00 00 f1 f1 f1 f1 00 00 00 00 00 00 00 00
dfe49b80: 00 00 00 00 00 00 00 00 03 f4 f4 f4 f3 f3 f3 f3
dfe49c00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
dfe49c80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
>dfe49d00: 00 00 00 00 f1 f1 f1 f1 00 00 04 f4 f3 f3 f3 f3
^
dfe49d80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
dfe49e00: 00 00 00 00 f1 f1 f1 f1 00 f4 f4 f4 f2 f2 f2 f2
dfe49e80: 00 00 04 f4 f3 f3 f3 f3 00 00 00 00 00 00 00 00
dfe49f00: f1 f1 f1 f1 00 00 00 00 00 04 f4 f4 f3 f3 f3 f3
dfe49f80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
==================================================================

Stack checker has found an out-of-bounds access issue on the stack. The first
one was detected in the function idr_for_each and the second in idr_destroy.
Both logs point to the same address: 0xdfe49d3c. Both logs provide back
traces showing how these functions were invoked. So, the faulting trace is
fsnotify_mark_destroy ? fsnotify_put_mark ? fsnotify_final_destroy_group ?
inotify_free_group_priv ? (idr_for_each/idr_destroy) .
Checking memory state helps to understand why KASAN thinks that this is
out-of-bounds issue: 0xF1 in both cases corresponds to KASAN_STACK_LEFT
redzone, thus it was an access at the left stack boundary.

Lets examine both faulty functions.
The first one:
int idr_for_each(struct idr *idp, int (*fn)(int id, void *p, void *data), void *data)
{
    int n, id, max, error = 0;
    struct idr_layer *p;
    struct idr_layer *pa[MAX_IDR_LEVEL];
    struct idr_layer **paa = &pa[0];
   
    n = idp->layers * IDR_BITS;
   
    p = rcu_dereference_raw(idp->top);
    max = 1 << n;
   
    id = 0;
    while (id < max)
    {
        while (n > 0 && p)
        {
            n -= IDR_BITS;
            *paa++ = p;
            p = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);
        }
        if (p)
        {
            error = fn(id, (void *)p, data);
            if (error)
            break;
        }
        id += 1 << n;
       
        while (n < fls(id))
        {
            n += IDR_BITS;
            p = *--paa;
        }
    }
    return error;
}

The pointer array pa is an evident candidate for out-of-bounds access. After
some analysis of a function it is easy to make sure that both functions access
the array out of bounds. The simplest fix is growing the array on the size of
one pointer and setting paa to point initially to the second (&pa[1]) entry of
the array.

Non-supported Functionality
This time only non-initialized-read checker is not implemented.
?Use-after-return? and global data ?out-of-bounds? checkers were
implemented in KASAN in the latest release, but little bit different to their
user space analogues.


={============================================================================
*kt_linux_tool_000* tool-efence-asan-llvm-build

http://llvm.org/

The compiler-rt project provides highly tuned implementations of the low-level
code generator support routines like "__fixunsdfdi" and other calls generated
when a target doesn't have a short sequence of native instructions to
implement a core IR operation. 

It also provides implementations of run-time libraries for dynamic testing
tools such as AddressSanitizer, ThreadSanitizer, MemorySanitizer, and
DataFlowSanitizer. 

Not all LLVM projects require LLVM for all use cases. For example compiler-rt
can be built without LLVM, and the compiler-rt sanitizer libraries are used
with GCC.

https://github.com/google/sanitizers/wiki/AddressSanitizerHowToBuild
http://llvm.org/docs/GettingStarted.html#getting-a-modern-host-c-toolchain

Download and install CMake (you'll need at least CMake 2.8.8).

note: 07/10/2016
when runs, errors :
CMake 3.4.3 or higher is required.  You are running version 3.0.2

Get llvm, clang and compiler-rt sources (see above).
Make sure you have a modern C++ toolchain (see above).
Set configuration and `build LLVM`


note:
To use a different GCC toolchain than one installed. When use own gcc, add
the path of latest cmake to the path before running cmake

DO NOT WORK

Once you have a GCC toolchain, configure your build of LLVM to use the new
toolchain for your host compiler and C++ standard library. Because the new
version of libstdc++ is not on the system library search path, you need to
pass extra linker flags so that it can be found at link time (-L) and at
runtime (-rpath). If you are using CMake, this invocation should produce
working binaries:

% mkdir llvm_cmake_build && cd llvm_cmake_build 
or
% mkdir build_x86 && cd build_x86

CC=$HOME/toolchains/bin/gcc CXX=$HOME/toolchains/bin/g++ \
cmake .. -DCMAKE_CXX_LINK_FLAGS="-Wl,-rpath,$HOME/toolchains/lib64 -L$HOME/toolchains/lib64"

If you fail to set rpath, most LLVM binaries will fail on startup with a
message from the loader similar to libstdc++.so.6: version `GLIBCXX_3.4.20'
not found. This means you need to tweak the -rpath linker flag.

# Choose the host compiler
# Choose CMAKE_BUILD_TYPE {Debug, Release}
# Choose LLVM_ENABLE_ASSERTIONS {ON,OFF}
# Choose LLVM_ENABLE_WERROR {ON,OFF}
# Set LLVM_TARGETS_TO_BUILD to X86 to speed up the build
[CC=clang CXX=clang++] cmake -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON [-DLLVM_ENABLE_WERROR=ON] [-DLLVM_TARGETS_TO_BUILD=X86] /path/to/llvm/checkout

CC=$HOME/toolchains/bin/gcc CXX=$HOME/toolchains/bin/g++ \
cmake .. -DCMAKE_CXX_LINK_FLAGS="-Wl,-rpath,$HOME/toolchains/lib64 -L$HOME/toolchains/lib64" -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON -DLLVM_ENABLE_WERROR=ON -DLLVM_TARGETS_TO_BUILD=X86


{x86}
cmake .. -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON -DLLVM_ENABLE_WERROR=ON -DLLVM_TARGETS_TO_BUILD=X86
cmake --trace ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON -DLLVM_ENABLE_WERROR=ON -DLLVM_TARGETS_TO_BUILD=X86

builds fine when increase swap space

$ more use-after-free.c 
#include <stdlib.h>
int main() {
  char *x = (char*)malloc(10 * sizeof(char*));
  free(x);
  return x[5];
}

~/si-logs/asn-llvm/build_x86$ ./bin/clang -fsanitize=address -O1 -fno-omit-frame-pointer -g use-after-free.c 
/usr/bin/ld: cannot find /home/kyoupark/si-logs/asn-llvm/build_x86/bin/../lib/clang/4.0.0/lib/linux/libclang_rt.asan-i686.a: No such file or directory
clang-4.0: error: linker command failed with exit code 1 (use -v to see invocation)

~/si-logs/asn-llvm/build_x86$ ./bin/clang -O1 -fno-omit-frame-pointer -g use-after-free.c 
kyoupark@kit-debian:~/si-logs/asn-llvm/build_x86$ ls

../lib/clang/4.0.0/lib/linux/libclang_rt.asan-i686.a
There are files which has 386 namings but why 686? Looks like that asan use
686 only.

~/si-logs/asn-llvm/llvm/projects/compiler-rt/lib/asan$
scripts/asan_device_setup
98:        _ARCH=i686

So makes a sym links to 686 files for 383 and asn works fine.


{mips}
cmake ../llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON -DLLVM_ENABLE_WERROR=ON -DLLVM_TARGETS_TO_BUILD=Mips -DLLVM_TARGET_ARCH=Mips

-- LLVM host triple: i686-pc-linux-gnu
-- LLVM default target triple: i686-pc-linux-gnu
-- Building with -fPIC
-- Targeting Mips

[ 98%] Built target gtest
Scanning dependencies of target Asan-i386-with-calls-Test
[ 98%] Generating ASAN_INST_TEST_OBJECTS.gtest-all.cc.i386-with-calls.o
clang (LLVM option parsing): Unknown command line argument '-asan-instrument-assembly'.  Try: 'clang (LLVM option parsing) -help'
clang (LLVM option parsing): Did you mean '-asan-instrument-atomics'?
projects/compiler-rt/lib/asan/tests/CMakeFiles/Asan-i386-with-calls-Test.dir/build.make:76: recipe for target 'projects/compiler-rt/lib/asan/tests/ASAN_INST_TEST_OBJECTS.gtest-all.cc.i386-with-calls.o' failed
make[3]: *** [projects/compiler-rt/lib/asan/tests/ASAN_INST_TEST_OBJECTS.gtest-all.cc.i386-with-calls.o] Error 1
CMakeFiles/Makefile2:8586: recipe for target 'projects/compiler-rt/lib/asan/tests/CMakeFiles/Asan-i386-with-calls-Test.dir/all' failed
make[2]: *** [projects/compiler-rt/lib/asan/tests/CMakeFiles/Asan-i386-with-calls-Test.dir/all] Error 2
CMakeFiles/Makefile2:9338: recipe for target 'projects/compiler-rt/test/asan/CMakeFiles/check-asan.dir/rule' failed
make[1]: *** [projects/compiler-rt/test/asan/CMakeFiles/check-asan.dir/rule] Error 2
Makefile:2836: recipe for target 'check-asan' failed
make: *** [check-asan] Error 2

Well, make clang is fine.

~/si-logs/asn-llvm/build_mips$ ./bin/clang -O1 -fno-omit-frame-pointer -g use-after-free.c 
error: unable to create target: 'No available targets are compatible with this triple.'
1 error generated.

However, it shows

~/si-logs/asn-llvm/build_mips$ ./bin/clang --version
clang version 4.0.0 (trunk 283488)
`Target: i686-pc-linux-gnu`
Thread model: posix
InstalledDir: /home/kyoupark/si-logs/asn-llvm/build_mips/./bin


si-logs/asn-llvm/build_mips$ ./bin/clang --target=mips -g use-after-free.c
In file included from use-after-free.c:1:
In file included from /usr/include/stdlib.h:24:
/usr/include/features.h:374:12: fatal error: 'sys/cdefs.h' file not found
#  include <sys/cdefs.h>
           ^
1 error generated.


{how-to-build-on-arms}
How To Build On ARM
http://llvm.org/docs/HowToBuildOnARM.html

note:
This is to build LLVM/clang to run ARM board. Not to cross-compile.

<back-end>
It's also a lot quicker to only build the relevant back-ends (ARM and
    AArch64), since it’s very unlikely that you'll use an ARM board to
cross-compile to other arches. If you're running Compiler-RT tests, also
include the x86 back-end, or some tests will fail.

cmake $LLVM_SRC_DIR -DCMAKE_BUILD_TYPE=Release \
                    -DLLVM_TARGETS_TO_BUILD="ARM;X86;AArch64"


{how-to-cross-compile}
How To Cross-Compile Clang/LLVM using Clang/LLVM
http://llvm.org/docs/HowToCrossCompileLLVM.html

Cross-compiling from an x86_64 host (most Intel and AMD chips nowadays) to a
hard-float ARM target (most ARM targets nowadays).

The packages you’ll need are:

        cmake
        ninja-build (from backports in Ubuntu)
        gcc-4.7-arm-linux-gnueabihf
        gcc-4.7-multilib-arm-linux-gnueabihf
        binutils-arm-linux-gnueabihf
        libgcc1-armhf-cross
        libsfgcc1-armhf-cross
        libstdc++6-armhf-cross
        libstdc++6-4.7-dev-armhf-cross


The CMake options you need to add are:

        -DCMAKE_CROSSCOMPILING=True
        -DCMAKE_INSTALL_PREFIX=<install-dir>
        -DLLVM_TABLEGEN=<path-to-host-bin>/llvm-tblgen
        -DCLANG_TABLEGEN=<path-to-host-bin>/clang-tblgen
        -DLLVM_DEFAULT_TARGET_TRIPLE=arm-linux-gnueabihf
        -DLLVM_TARGET_ARCH=ARM
        -DLLVM_TARGETS_TO_BUILD=ARM


From http://llvm.org/docs/CMake.html

LLVM_TARGETS_TO_BUILD:STRING
    Semicolon-separated list of targets to build, or all for building all
    targets. Case-sensitive. Defaults to all. Example:
    -DLLVM_TARGETS_TO_BUILD="X86;PowerPC".

LLVM_TARGET_ARCH:STRING
    LLVM target to use for native code generation. This is required for JIT
    generation. It defaults to “host”, meaning that it shall pick the
    architecture of the machine where LLVM is being built. If you are
    cross-compiling, set it to the target architecture name.

LLVM_TABLEGEN:STRING
    Full path to a native TableGen executable (usually named llvm-tblgen).
    `This is intended for cross-compiling`: if the user sets this variable, no
    native TableGen will be created.


{supported-arch}
http://llvm.org/docs/doxygen/html/Triple_8h_source.html):


For clang:
/home/NDS-UK/kyoupark/asn/llvm/projects/compiler-rt/lib/asan


={============================================================================
*kt_linux_tool_000* tool-efence-asan-gcc-build

The tool works on x86, ARM, MIPS (both 32- and 64-bit versions of all
architectures), PowerPC64. The supported operation systems are Linux,
Darwin (OS X and iOS Simulator), FreeBSD, Android:

AddressSanitizer, a fast memory error detector, has been added and can be
enabled via -fsanitize=address. Memory access instructions will be
instrumented to detect heap-, stack-, and global-buffer overflow as well as
use-after-free bugs. To get nicer stacktraces, use -fno-omit-frame-pointer.
The AddressSanitizer is available on IA-32/x86-64/x32/PowerPC/PowerPC64
GNU/Linux and on x86-64 Darwin.

AddressSanitizer is based on compiler instrumentation and directly-mapped
shadow memory. 

note:
AddressSanitizer is a part of LLVM starting with version 3.1 and a part of GCC
starting with version `4.8`

<code-to-test-asan>
#include <stdio.h>
#include <stdlib.h>

//      gcc -fsanitize=address testmain.c -o out_asn
int main(int argc, char **argv)
{
    printf("====> \n" );
    printf("====> this is mips target and use after free..\n" );
    printf("====> \n" );
    int *x = malloc(10*sizeof(int));
    free(x);
    x[5] = 10;
    return;
}

$ /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/mips-buildroot-linux-uclibc-gcc -fsanitize=address x.c

<error-when-no-asan>
kyoupark@ukstbuild2:~$ ~/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin/aarch64-buildroot-linux-musl-gcc -fsanitize=address x.c
x.c:1:0: warning: -fsanitize=address not supported for this target
 #include <stdio.h>
 ^
x.c: In function ‘main’:
x.c:9:14: warning: incompatible implicit declaration of built-in function ‘malloc’
     int *x = malloc(10*sizeof(int));
              ^
x.c:10:5: warning: incompatible implicit declaration of built-in function ‘free’
     free(x);
     ^
/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2015.08.1-for-arm64/output/host/usr/bin/../lib/gcc/aarch64-buildroot-linux-musl/4.9.4/../../../../aarch64-buildroot-linux-musl/bin/ld: cannot find libasan_preinit.o: No such file or directory
/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2015.08.1-for-arm64/output/host/usr/bin/../lib/gcc/aarch64-buildroot-linux-musl/4.9.4/../../../../aarch64-buildroot-linux-musl/bin/ld: cannot find -lasan
collect2: error: ld returned 1 exit status


{for-x86}
ASAN gets built as a part of gcc build and as a shared library.

https://gcc.gnu.org/gcc-4.8/changes.html

<vm>
On debian VM, and worked fine. This is gcc installed by default.

kyoupark@kit-debian:~$ gcc --version
gcc (Debian 4.9.2-10) 4.9.2

<build-server>
Has 4.4.7 which do not have ASAN. So have 4.8.2 built and ASAN works when
set a library path as:

kyoupark@ukstbuild2:~/toolchains/bin$ ./a.out
./a.out: error while loading shared libraries: libasan.so.0: cannot open shared object file: No such file or directory

$ ./gcc -fsanitize=address -Wl,-rpath=/home/NDS-UK/kyoupark/toolchains/lib64 uaf.c


{for-mips-without-asn}
For mips but without asn, builds okay and have toolchain built

BR2_mips=y
BR2_ARCH="mips"
BR2_ENDIAN="BIG"
BR2_GCC_TARGET_ARCH="mips32"
BR2_GCC_TARGET_ABI="32"

BR2_WGET="wget --passive-ftp -nd -t 3 --no-check-certificate"

BR2_DEFAULT_KERNEL_HEADERS="3.2.81"
BR2_TOOLCHAIN_BUILDROOT_UCLIBC=y

BR2_GCC_VERSION="4.9.4"

/output/host/usr/bin/mips-buildroot-linux-uclibc-gcc --version
mips-buildroot-linux-uclibc-gcc.br_real (Buildroot 2016.08.1) 4.9.4
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

/broot-latest/buildroot-2016.08.1/output/host/usr/bin$ ls
mips-buildroot-linux-uclibc-gcc                
mips-buildroot-linux-uclibc-gcc-4.9.4          
...

{for-aarch64}
Enabled C++ support, used musl since there is no selection for uclibc.

kyoupark@ukstbuild2:~/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin$ ./aarch64-buildroot-linux-musl-gcc --version
aarch64-buildroot-linux-musl-gcc.br_real (Buildroot 2016.08.1) 4.9.4
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

toolchain has built but failed to compile with asan. Why?

// home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/package/gcc/gcc.mk
// since not happy with musl.

# libsanitizer requires wordexp, not in default uClibc config. Also
# doesn't build properly with musl.
ifeq ($(BR2_TOOLCHAIN_BUILDROOT_UCLIBC)$(BR2_TOOLCHAIN_BUILDROOT_MUSL),y)
HOST_GCC_COMMON_CONF_OPTS += --disable-libsanitizer
endif


Changes to use glibc, have it built but still not have asan enabled. 

note:
1. glibc gets built which is different when do on debian VM.
2. Since not use UCLIBC or MUSL, not set "--disable-libsanitizer" and will
have config checking:

// checking for libsanitizer support... no


// full command to run config. note that cd first and when use "./configure
// ...", do not work.

// no changes
PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" CXXFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null ./configure --prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr" --sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/etc" --localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/var" --enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html --disable-doc --disable-docs --disable-documentation --disable-debug --with-xmlto=no --with-fop=no --disable-dependency-tracking  --target=aarch64-buildroot-linux-gnu --with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/aarch64-buildroot-linux-gnu/sysroot --disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib --with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --with-pkgversion="Buildroot 2016.08.1" --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath --enable-tls --disable-libmudflap --enable-threads --with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --without-isl --without-cloog --disable-decimal-float --enable-languages=c --disable-shared --without-headers --disable-threads --with-newlib --disable-largefile --disable-nls 

// use --enable-libsanitizer
PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" CXXFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null ./configure --prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr" --sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/etc" --localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/var" --enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html --disable-doc --disable-docs --disable-documentation --disable-debug --with-xmlto=no --with-fop=no --disable-dependency-tracking  --target=aarch64-buildroot-linux-gnu --with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/aarch64-buildroot-linux-gnu/sysroot --disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib --with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --with-pkgversion="Buildroot 2016.08.1" --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath --enable-tls --disable-libmudflap --enable-threads --with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr --without-isl --without-cloog --disable-decimal-float --enable-languages=c --disable-shared --without-headers --disable-threads --with-newlib --disable-largefile --disable-nls --enable-libsanitizer

PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin"
AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm"
CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc"
CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy"
RANLIB="/usr/bin/ranlib"
CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include"
CFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include"
CXXFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include"
LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib
-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib
-Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib"
PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1
PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/bin/pkg-config"
PKG_CONFIG_SYSROOT_DIR="/"
PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/share/pkgconfig"
INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/include"
LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/lib
-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib
-Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/lib"
MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE
-D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE
-D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null
./configure
--prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr"
--sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/etc"
--localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/var"
--enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html
--disable-doc --disable-docs --disable-documentation --disable-debug
--with-xmlto=no --with-fop=no --disable-dependency-tracking
--target=aarch64-buildroot-linux-gnu
--with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr/aarch64-buildroot-linux-gnu/sysroot
--disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib
--with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr
--with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr
--with-pkgversion="Buildroot 2016.08.1"
--with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath --enable-tls
--disable-libmudflap --enable-threads
--with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/host/usr
--without-isl --without-cloog --disable-decimal-float --enable-languages=c
--disable-shared --without-headers --disable-threads --with-newlib
--disable-largefile --disable-nls


Build fails on host-gcc-final 4.9.4


{for-mips-with-asn}
https://gcc.gnu.org/ml/gcc/2013-10/threads.html#00253

/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/libsanitizer/configure.tgt

I'm afraid GCC doesn't support ASan for MIPS even now on trunk:

$ cat libsanitizer/configure.tgt

case "${target}" in
  x86_64-*-linux* | i?86-*-linux*)
        if test x$ac_cv_sizeof_void_p = x8; then
                TSAN_SUPPORTED=yes
                LSAN_SUPPORTED=yes
                TSAN_TARGET_DEPENDENT_OBJECTS=tsan_rtl_amd64.lo
        fi
        ;;
  powerpc*-*-linux*)
        ;;
  sparc*-*-linux*)
        ;;
  arm*-*-linux*)
        ;;
  aarch64*-*-linux*)
        if test x$ac_cv_sizeof_void_p = x8; then
                TSAN_SUPPORTED=yes
                LSAN_SUPPORTED=yes
                TSAN_TARGET_DEPENDENT_OBJECTS=tsan_rtl_aarch64.lo
        fi
        ;;
  x86_64-*-darwin[1]* | i?86-*-darwin[1]*)
        TSAN_SUPPORTED=no
        ;;
  *)
        UNSUPPORTED=1
        ;;
esac


// libsanitizer/configure.tgt
  mips*-*-linux*)
	TSAN_SUPPORTED=yes
	LSAN_SUPPORTED=yes
	;;

For clang:
/home/NDS-UK/kyoupark/asn/llvm/projects/compiler-rt/lib/asan


<changes-to-buildroot>
//home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/package/gcc/gcc.mk

# libsanitizer requires wordexp, not in default uClibc config. Also
# doesn't build properly with musl.
ifeq ($(BR2_TOOLCHAIN_BUILDROOT_UCLIBC)$(BR2_TOOLCHAIN_BUILDROOT_MUSL),y)
HOST_GCC_COMMON_CONF_OPTS += --disable-libsanitizer
endif

changes to enable ASAN when use uclibc:
ifeq ($(BR2_TOOLCHAIN_BUILDROOT_UCLIBC)$(BR2_TOOLCHAIN_BUILDROOT_MUSL),y)
HOST_GCC_COMMON_CONF_OPTS += --enable-libsanitizer
endif


//home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2015.08.1-for-mips-w-asn/package/uclibc/uClibc-ng.config

add this to to enalbe wordexp:
UCLIBC_HAS_WORDEXP=y

note:
Use $make clean all for buildroot rebuild


These changes ./configure param as:

 `--enable-libsanitizer`

>>> host-gcc-initial 4.9.4 Configuring
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build
ln -sf ../configure /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build/configure

(cd /home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/build/host-gcc-initial-4.9.4/build && rm -rf config.cache; 
 
PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include" CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include" CXXFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include" LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/lib -L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib -Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null ./configure --prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr" --sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/etc" --localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/var" --enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html --disable-doc --disable-docs --disable-documentation --disable-debug --with-xmlto=no --with-fop=no --disable-dependency-tracking  --target=mips-buildroot-linux-uclibc --with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/mips-buildroot-linux-uclibc/sysroot --disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib --with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr --with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr --with-pkgversion="Buildroot 2016.08.1" --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath --enable-libsanitizer --enable-tls --disable-libmudflap --enable-threads --with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr --without-isl --without-cloog --with-float=soft --disable-decimal-float --with-arch="mips32" --with-abi="32" --enable-languages=c --disable-shared --without-headers --disable-threads --with-newlib --disable-largefile --disable-nls --config-cache

PATH="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/sbin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/bin:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/sbin:/home/nds-uk/kyoupark/inst/bin:/home/nds-uk/kyoupark/viminst/bin:/home/nds-uk/kyoupark/github-kb/bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/nds-uk/kyoupark/bin"
AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm"
CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc"
CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy"
RANLIB="/usr/bin/ranlib"
CPPFLAGS="-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include"
CFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include"
CXXFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include"
LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/lib
-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib
-Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib"
PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1
PKG_CONFIG="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/bin/pkg-config"
PKG_CONFIG_SYSROOT_DIR="/"
PKG_CONFIG_LIBDIR="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib/pkgconfig:/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/share/pkgconfig"
INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2
-I/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/include"
LDFLAGS="-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/lib
-L/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib
-Wl,-rpath,/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/lib"
MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE
-D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE
-D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null
./configure
--prefix="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr"
--sysconfdir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/etc"
--localstatedir="/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/var"
--enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html
--disable-doc --disable-docs --disable-documentation --disable-debug
--with-xmlto=no --with-fop=no --disable-dependency-tracking
`--target=mips-buildroot-linux-uclibc`
--with-sysroot=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr/mips-buildroot-linux-uclibc/sysroot
--disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib
--with-gmp=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr
--with-mpfr=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr
--with-pkgversion="Buildroot 2016.08.1"
--with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath
`--enable-libsanitizer` --enable-tls --disable-libmudflap --enable-threads
--with-mpc=/home/nds-uk/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-mips-w-asn/output/host/usr
--without-isl --without-cloog --with-float=soft --disable-decimal-float
--with-arch="mips32" --with-abi="32" --enable-languages=c --disable-shared
--without-headers --disable-threads --with-newlib --disable-largefile
--disable-nls 


Therefore, this will not use checking:

// configure:2416: checking target system type
// configure:2429: result: aarch64-buildroot-linux-gnu
// checking for libsanitizer support... no
// home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1-for-arm64/output/build/host-gcc-initial-4.9.4/configure.ac

# Disable libsanitizer on unsupported systems.
if test -d ${srcdir}/libsanitizer; then
  if test x$enable_libsanitizer = x; then
     AC_MSG_CHECKING([for libsanitizer support])
     if (srcdir=${srcdir}/libsanitizer; \
        . ${srcdir}/configure.tgt; \
        test -n "$UNSUPPORTED")
     then
         AC_MSG_RESULT([no])
         noconfigdirs="$noconfigdirs target-libsanitizer"
     else
         AC_MSG_RESULT([yes])
     fi
  fi
fi


// from log

checking target system type... mips-buildroot-linux-uclibc

*** This configuration is not supported in the following subdirectories:
     target-libquadmath target-libcilkrts target-libitm target-libvtv gnattools target-libada target-libstdc++-v3 target-libgfortran target-libffi target-libbacktrace target-zlib target-libobjc target-libssp target-boehm-gc 
  `target-libsanitizer`
    (Any other directories should still work fine.)


WHY???

// buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/configure.ac

# Disable libitm, libsanitizer, libvtv if we're not building C++
case ,${enable_languages}, in
  *,c++,*) ;;
  *)
    noconfigdirs="$noconfigdirs target-libcilkrts target-libitm target-libsanitizer target-libvtv"
    ;;
esac

# Remove the entries in $skipdirs and $noconfigdirs from $configdirs,
# $build_configdirs and $target_configdirs.
# If we have the source for $noconfigdirs entries, add them to $notsupp.

notsupp=""
for dir in . $skipdirs $noconfigdirs ; do
  dirname=`echo $dir | sed -e s/target-//g -e s/build-//g`
  if test $dir != .  && echo " ${configdirs} " | grep " ${dir} " >/dev/null 2>&1; then
    configdirs=`echo " ${configdirs} " | sed -e "s/ ${dir} / /"`
    if test -r $srcdir/$dirname/configure ; then
      if echo " ${skipdirs} " | grep " ${dir} " >/dev/null 2>&1; then
	true
      else
	notsupp="$notsupp $dir"
      fi
    fi
  fi
  if test $dir != .  && echo " ${build_configdirs} " | grep " ${dir} " >/dev/null 2>&1; then
    build_configdirs=`echo " ${build_configdirs} " | sed -e "s/ ${dir} / /"`
    if test -r $srcdir/$dirname/configure ; then
      if echo " ${skipdirs} " | grep " ${dir} " >/dev/null 2>&1; then
	true
      else
	notsupp="$notsupp $dir"
      fi
    fi
  fi
  if test $dir != . && echo " ${target_configdirs} " | grep " ${dir} " >/dev/null 2>&1; then
    target_configdirs=`echo " ${target_configdirs} " | sed -e "s/ ${dir} / /"`
    if test -r $srcdir/$dirname/configure ; then
      if echo " ${skipdirs} " | grep " ${dir} " >/dev/null 2>&1; then
	true
      else
	notsupp="$notsupp $dir"
      fi
    fi
  fi
done


# Produce a warning message for the subdirs we can't configure.
# This isn't especially interesting in the Cygnus tree, but in the individual
# FSF releases, it's important to let people know when their machine isn't
# supported by the one or two programs in a package.

if test -n "${notsupp}" && test -z "${norecursion}" ; then
  # If $appdirs is non-empty, at least one of those directories must still
  # be configured, or we error out.  (E.g., if the gas release supports a
  # specified target in some subdirs but not the gas subdir, we shouldn't
  # pretend that all is well.)
  if test -n "$appdirs" ; then
    for dir in $appdirs ; do
      if test -r $dir/Makefile.in ; then
	if echo " ${configdirs} " | grep " ${dir} " >/dev/null 2>&1; then
	  appdirs=""
	  break
	fi
	if echo " ${target_configdirs} " | grep " target-${dir} " >/dev/null 2>&1; then
	  appdirs=""
	  break
	fi
      fi
    done
    if test -n "$appdirs" ; then
      echo "*** This configuration is not supported by this package." 1>&2
      exit 1
    fi
  fi
  # Okay, some application will build, or we don't care to check.  Still
  # notify of subdirs not getting built.
  echo "*** This configuration is not supported in the following subdirectories:" 1>&2
  echo "    ${notsupp}" 1>&2
  echo "    (Any other directories should still work fine.)" 1>&2
fi


So add C++ support:

+BR2_TOOLCHAIN_BUILDROOT_CXX=y

BR builds libsanitizer. Great but build error:

note:
host-gcc-initial-4.9.4 build still shows the above warning but not for
host-gcc-final-4.9.4 and build fails on that.

make[3]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/mips-buildroot-linux-uclibc/libsanitizer'
/usr/bin/make "AR_FLAGS=rc" "CC_FOR_BUILD=/usr/lib64/ccache/gcc" "CFLAGS=-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os  -minterlink-mips16" "CXXFLAGS=-D_LARGEFILE_SOURCE -D_LARGEFILE6
4_SOURCE -D_FILE_OFFSET_BITS=64  -Os  -D_GNU_SOURCE -minterlink-mips16" "CFLAGS_FOR_BUILD=-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" "CFLAGS_FOR_TARGET=-D_LARGE
FILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os  -minterlink-mips16" "INSTALL=/usr/bin/install -c" "INSTALL_DATA=/usr/bin/install -c -m 644" "INSTALL_PROGRAM=/usr/bin/install -c" "INSTALL_SCRIP
T=/usr/bin/install -c" "JC1FLAGS=" "LDFLAGS=" "LIBCFLAGS=-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os  -minterlink-mips16" "LIBCFLAGS_FOR_TARGET=-D_LARGEFILE_SOURCE -D_LARGEFILE64_SO
URCE -D_FILE_OFFSET_BITS=64  -Os  -minterlink-mips16" "MAKE=/usr/bin/make" "MAKEINFO=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/missing makeinfo --split-size
=5000000 --split-size=5000000 " "PICFLAG=" "PICFLAG_FOR_TARGET=" "SHELL=/bin/sh" "RUNTESTFLAGS=" "exec_prefix=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr" "infodir=/home/NDS-UK
/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/share/info" "libdir=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" "prefix=/home/NDS-UK/kyoupark/asn/broot-late
st/buildroot-2016.08.1/output/host/usr" "includedir=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" "AR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/h
ost/usr/mips-buildroot-linux-uclibc/bin/ar" "AS=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/./gcc/as" "LD=/home/NDS-UK/kyoupark/asn/broot-latest/buildro
ot-2016.08.1/output/build/host-gcc-final-4.9.4/build/./gcc/collect-ld" "LIBCFLAGS=-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os  -minterlink-mips16" "NM=/home/NDS-UK/kyoupark/asn/broo
t-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/./gcc/nm" "PICFLAG=" "RANLIB=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/bin/ra
nlib" "DESTDIR=" all-recursive
make[4]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/mips-buildroot-linux-uclibc/libsanitizer'
Making all in sanitizer_common


libtool: compile:  /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/./gcc/xgcc -shared-libgcc -B/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/ou
tput/build/host-gcc-final-4.9.4/build/./gcc -nostdinc++ -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/mips-buildroot-linux-uclibc/libstdc++-v3/src -L/ho
me/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/mips-buildroot-linux-uclibc/libstdc++-v3/src/.libs -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1
/output/build/host-gcc-final-4.9.4/build/mips-buildroot-linux-uclibc/libstdc++-v3/libsupc++/.libs -B/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/bin/
 -B/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/lib/ -isystem /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildro
ot-linux-uclibc/include -isystem /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/sys-include -D_GNU_SOURCE -D_DEBUG -D__STDC_CONSTANT_MACROS -D__STDC_FO
RMAT_MACROS -D__STDC_LIMIT_MACROS -I. -I../../../../libsanitizer/sanitizer_common -I.. -I ../../../../libsanitizer/include -isystem ../../../../libsanitizer/include/system -Wall -W -Wno-unused-parameter -Wwr
ite-strings -pedantic -Wno-long-long -fPIC -fno-builtin -fno-exceptions -fno-rtti -fomit-frame-pointer -funwind-tables -fvisibility=hidden -Wno-variadic-macros -I../../libstdc++-v3/include -I../../libstdc++-
v3/include/mips-buildroot-linux-uclibc -I../../../../libsanitizer/../libstdc++-v3/libsupc++ -DSANITIZER_LIBBACKTRACE -DSANITIZER_CP_DEMANGLE -I ../../../../libsanitizer/../libbacktrace -I ../libbacktrace -I
../../../../libsanitizer/../include -include ../../../../libsanitizer/libbacktrace/backtrace-rename.h -D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -Os -D_GNU_SOURCE -minterlink-mips16 -MT
 sanitizer_mac.lo -MD -MP -MF .deps/sanitizer_mac.Tpo -c ../../../../libsanitizer/sanitizer_common/sanitizer_mac.cc -o sanitizer_mac.o >/dev/null 2>&1
In file included from ../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:20:0:
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:67:16: error: ‘struct___old_kernel_stat_sz’ was not declared in this scope
 COMPILER_CHECK(struct___old_kernel_stat_sz == sizeof(struct __old_kernel_stat));
                ^
../../../../libsanitizer/sanitizer_common/sanitizer_internal_defs.h:257:65: note: in definition of macro ‘IMPL_COMPILER_ASSERT’
     typedef char IMPL_PASTE(assertion_failed_##_, line)[2*(int)(pred)-1]
                                                                 ^
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:67:1: note: in expansion of macro ‘COMPILER_CHECK’
 COMPILER_CHECK(struct___old_kernel_stat_sz == sizeof(struct __old_kernel_stat));
 ^
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:67:78: error: invalid application of ‘sizeof’ to incomplete type ‘__old_kernel_stat’
 COMPILER_CHECK(struct___old_kernel_stat_sz == sizeof(struct __old_kernel_stat));
                                                                              ^
../../../../libsanitizer/sanitizer_common/sanitizer_internal_defs.h:257:65: note: in definition of macro ‘IMPL_COMPILER_ASSERT’
     typedef char IMPL_PASTE(assertion_failed_##_, line)[2*(int)(pred)-1]
                                                                 ^
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:67:1: note: in expansion of macro ‘COMPILER_CHECK’
 COMPILER_CHECK(struct___old_kernel_stat_sz == sizeof(struct __old_kernel_stat));
 ^
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:70:16: error: ‘struct_kernel_stat_sz’ was not declared in this scope
 COMPILER_CHECK(struct_kernel_stat_sz == sizeof(struct stat));
                ^
../../../../libsanitizer/sanitizer_common/sanitizer_internal_defs.h:257:65: note: in definition of macro ‘IMPL_COMPILER_ASSERT’
     typedef char IMPL_PASTE(assertion_failed_##_, line)[2*(int)(pred)-1]
                                                                 ^
../../../../libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cc:70:1: note: in expansion of macro ‘COMPILER_CHECK’
 COMPILER_CHECK(struct_kernel_stat_sz == sizeof(struct stat));
 ^

// sanitizer_common/sanitizer_platform_limits_linux.cc

// This header seems to contain the definitions of _kernel_ stat* structs.
#include <asm/stat.h>


#if !defined(__powerpc64__) && !defined(__x86_64__) && !defined(__sparc__)
COMPILER_CHECK(struct___old_kernel_stat_sz == sizeof(struct __old_kernel_stat));
#endif


#define COMPILER_CHECK(pred) IMPL_COMPILER_ASSERT(pred, __LINE__)

#define IMPL_PASTE(a, b) a##b
#define IMPL_COMPILER_ASSERT(pred, line) \
    typedef char IMPL_PASTE(assertion_failed_##_, line)[2*(int)(pred)-1]


// buildroot-2016.08.1/output/build/linux-headers-3.2.81/arch/mips/include/asm/stat.h

struct stat {};

// buildroot-2016.08.1/output/build/linux-headers-3.2.81/arch/mips/include/asm/x86/include/asm/stat.h

/* for 32bit emulation and 32 bit kernels */
struct __old_kernel_stat {};


// useful?
https://gcc.gnu.org/ml/gcc-patches/2013-05/msg00338.html


={============================================================================
*kt_linux_tool_100* tool-valgrind

1.1. An Overview of Valgrind

Valgrind is an instrumentation framework for building dynamic analysis tools. It
comes with a set of tools each of which performs some kind of debugging,
profiling, or similar task that helps you improve your programs. Valgrind's
  architecture is modular, so new tools can be created easily and without
  disturbing the existing structure.

A number of useful tools are supplied as standard.

    Memcheck is a memory error detector. It helps you make your programs,
    particularly those written in C and C++, more correct.

    Cachegrind is a cache and branch-prediction profiler. It helps you make your
    programs run faster.

    Callgrind is a call-graph generating cache profiler. It has some overlap
    with Cachegrind, but also gathers some information that Cachegrind does not.

    Helgrind is a thread error detector. It helps you make your multi-threaded
    programs more correct.

    DRD is also a thread error detector. It is similar to Helgrind but uses
    different analysis techniques and so may find different problems.

    Massif is a heap profiler. It helps you make your programs use less memory.

    DHAT is a different kind of heap profiler. It helps you understand issues of
    block lifetimes, block utilisation, and layout inefficiencies.

    SGcheck is an experimental tool that can detect overruns of stack and global
    arrays. Its functionality is complementary to that of Memcheck: SGcheck
    finds problems that Memcheck can't, and vice versa..

    BBV is an experimental SimPoint basic block vector generator. It is useful
    to people doing computer architecture research and development.


1.2. How to navigate this manual

This manual's structure reflects the structure of Valgrind itself. First, we
describe the Valgrind core, how to use it, and the options it supports. Then,
each tool has its own chapter in this manual. You only need to read the
  documentation for the core and for the tool(s) you actually use, although you
  may find it helpful to be at least a little bit familiar with what all tools
  do. If you're new to all this, you probably want to run the Memcheck tool and
    you might find the The Valgrind Quick Start Guide useful.

Be aware that the core understands some command line options, and the tools have
their own options which they know about. This means there is no central place
describing all the options that are accepted -- you have to read the options
documentation both for Valgrind's core and for the tool you want to use.


2.3. The Commentary

Valgrind tools write a commentary, a stream of text, detailing error reports and
other significant events. All lines in the commentary have following form:

==12345== some-message-from-Valgrind

The 12345 is the process ID. This scheme makes it easy to distinguish program
output from Valgrind commentary, and also easy to differentiate commentaries
from different processes which have become merged together, for whatever reason.

By default, Valgrind tools write only essential messages to the commentary, so
as to avoid flooding you with information of secondary importance. If you want
more information about what is happening, re-run, passing the `-v` option to
Valgrind. A second -v gives yet more detail.

You can direct the commentary to three different places:

    The default: send it to a file descriptor, which is by default 2 (stderr).
    So, if you give the core no options, it will write commentary to the
    standard error stream. If you want to send it to some other file descriptor,
             for example number 9, you can specify --log-fd=9.

    This is the simplest and most common arrangement, but can cause problems
      when Valgrinding entire trees of processes which expect specific file
      descriptors, particularly stdin/stdout/stderr, to be available for their
      own use.

    A less intrusive option is to write the commentary to a file, which you
    specify by `--log-file=filename`. There are special format specifiers that can
    be used to use a process ID or an environment variable name in the log file
    name. These are useful/necessary if your program invokes multiple processes
    (especially for MPI programs). See the basic options section for more
    details.

    The least intrusive option is to send the commentary to a network socket.
    The socket is specified as an IP address and port number pair, like this:
    --log-socket=192.168.0.1:12345 if you want to send the output to host IP
    192.168.0.1 port 12345 (note: we have no idea if 12345 is a port of
        pre-existing significance). You can also omit the port number:
    --log-socket=192.168.0.1, in which case a default port of 1500 is used. This
    default is defined by the constant VG_CLO_DEFAULT_LOGPORT in the sources.

    // skipped


2.5. Suppressing errors

The error-checking tools detect numerous problems in the system libraries, such
as the C library, which come pre-installed with your OS. You can't easily fix
these, but you don't want to see these errors (and yes, there are many!) So
Valgrind reads a list of errors to suppress at startup. A default suppression
file is created by the ./configure script when the system is built.

You can modify and add to the suppressions file at your leisure, or, better,
    write your own. Multiple suppression files are allowed. This is useful if
      part of your project contains errors you can't or don't want to fix, yet
      you don't want to continuously be reminded of them.

Note: By far the easiest way to add suppressions is to use the
`--gen-suppressions=yes` option described in Core Command-line Options. This
generates suppressions automatically. For best results, though, you may want to
edit the output of --gen-suppressions=yes by hand, in which case it would be
advisable to read through this section. 

 --gen-suppressions=<yes|no|all> [default: no]

    When set to yes, Valgrind will pause after every error shown and print the
    line:

        ---- Print suppression ? --- [Return/N/n/Y/y/C/c] ----

    Pressing Ret, or N Ret or n Ret, causes Valgrind continue execution without
    printing a suppression for this error.

    Pressing Y Ret or y Ret causes Valgrind to write a suppression for this
    error. You can then cut and paste it into a suppression file if you don't
    want to hear about the error in the future.

    When set to all, Valgrind will print a suppression for every reported error,
    without querying the user.

    This option is particularly useful with C++ programs, as it prints out the
    suppressions with mangled names, as required.

    Note that the suppressions printed are as specific as possible. You may want
    to common up similar ones, by adding wildcards to function names, and by
    using frame-level wildcards. The wildcarding facilities are powerful yet
    flexible, and with a bit of careful editing, you may be able to suppress a
    whole family of related errors with only a few suppressions.

    Sometimes two different errors are suppressed by the same suppression, in
    which case Valgrind will output the suppression more than once, but you only
    need to have one copy in your suppression file (but having more than one
        won't cause problems). Also, the suppression name is given as <insert a
    suppression name here>; the name doesn't really matter, it's only used with
    the -v option which prints out all used suppression records.


={============================================================================
*kt_linux_tool_100* tool-valgrind-memcheck

http://valgrind.org/docs/manual/quick-start.html

The Valgrind Quick Start Guide

1. Introduction

The Valgrind tool suite provides a number of debugging and profiling tools that
help you make your programs faster and more correct. The most popular of these
tools is called Memcheck. It can detect many memory-related errors that are
common in C and C++ programs and that can lead to crashes and unpredictable
behaviour.

The rest of this guide gives the minimum information you need to start detecting
memory errors in your program with Memcheck. For full documentation of Memcheck
and the other tools, please read the User Manual.

2. Preparing your program

Compile your program with -g to include debugging information so that Memcheck's
error messages include exact line numbers. Using -O0 is also a good idea, if you
can tolerate the slowdown. With -O1 line numbers in error messages can be
inaccurate, although generally speaking running Memcheck on code compiled at -O1
works fairly well, and the speed improvement compared to running -O0 is quite
significant. Use of -O2 and above is not recommended as Memcheck occasionally
reports uninitialised-value errors which don't really exist.

3. Running your program under Memcheck

If you normally run your program like this:

  myprog arg1 arg2

Use this command line:

  valgrind --leak-check=yes myprog arg1 arg2

Memcheck is the default tool. The `--leak-check` option turns on the detailed
memory leak detector.

Your program will run much slower (eg. 20 to 30 times) than normal, and use a
lot more memory. Memcheck will issue messages about memory errors and leaks that
it detects.

4. Interpreting Memcheck's output

Here's an example C program, in a file called a.c, with a memory error and a
memory leak.

  #include <stdlib.h>

  void f(void)
  {
     int* x = malloc(10 * sizeof(int));
     x[10] = 0;        // problem 1: heap block overrun
  }                    // problem 2: memory leak -- x not freed

  int main(void)
  {
     f();
     return 0;
  }

Most error messages look like the following, which describes problem 1, the heap
  block overrun:

  ==19182== Invalid write of size 4
  ==19182==    at 0x804838F: f (example.c:6)
  ==19182==    by 0x80483AB: main (example.c:11)
  ==19182==  Address 0x1BA45050 is 0 bytes after a block of size 40 alloc'd
  ==19182==    at 0x1B8FF5CD: malloc (vg_replace_malloc.c:130)
  ==19182==    by 0x8048385: f (example.c:5)
  ==19182==    by 0x80483AB: main (example.c:11)

Things to notice:

    There is a lot of information in each error message; read it carefully.

    The 19182 is the process ID; it's usually unimportant.

    The first line ("Invalid write...") tells you what kind of error it is.
    Here, the program wrote to some memory it should not have due to a heap
    block overrun.

    Below the first line is a stack trace telling you where the problem
    occurred. Stack traces can get quite large, and be confusing, especially if
    you are using the C++ STL. Reading them from the bottom up can help. If the
    stack trace is not big enough, use the `--num-callers` option to make it
    bigger.

    The code addresses (eg. 0x804838F) are usually unimportant, but occasionally
    crucial for tracking down weirder bugs.

    Some error messages have a second component which describes the memory
    address involved. This one shows that the written memory is just past the
    end of a block allocated with malloc() on line 5 of example.c.

It's worth fixing errors in the order they are reported, as later errors can be
caused by earlier errors. Failing to do this is a common cause of difficulty
with Memcheck.

Memory leak messages look like this:

  ==19182== 40 bytes in 1 blocks are definitely lost in loss record 1 of 1
  ==19182==    at 0x1B8FF5CD: malloc (vg_replace_malloc.c:130)
  ==19182==    by 0x8048385: f (a.c:5)
  ==19182==    by 0x80483AB: main (a.c:11)

The stack trace tells you where the leaked memory was allocated. Memcheck cannot
tell you why the memory leaked, unfortunately. (Ignore the
    "vg_replace_malloc.c", that's an implementation detail.)

There are several kinds of leaks; the two most important categories are:

    "definitely lost": your program is leaking memory -- fix it!

    "probably lost": your program is leaking memory, unless you're doing funny
    things with pointers (such as moving them to point to the middle of a heap
        block).

Memcheck also reports uses of uninitialised values, most commonly with the
message "Conditional jump or move depends on uninitialised value(s)". It can be
difficult to determine the root cause of these errors. Try using the
`--track-origins`=yes to get extra information. This makes Memcheck run slower,
  but the extra information you get often saves a lot of time figuring out where
    the uninitialised values are coming from.

If you don't understand an error message, please consult Explanation of error
messages from Memcheck in the Valgrind User Manual which has examples of all the
error messages Memcheck produces.

5. Caveats

Memcheck is not perfect; it occasionally produces false positives, and there are
mechanisms for suppressing these (see Suppressing errors in the Valgrind User
    Manual). 

However, it is typically right 99% of the time, so you should be wary of
ignoring its error messages. After all, you wouldn't ignore warning messages
produced by a compiler, right? The suppression mechanism is also useful if
Memcheck is reporting errors in library code that you cannot change. The default
suppression set hides a lot of these, but you may come across more.

Memcheck cannot detect every memory error your program has. For example, it
can't detect out-of-range reads or writes to arrays that are allocated
statically or on the stack. But it should detect many errors that could crash
your program (eg. cause a segmentation fault).

Try to make your program so clean that Memcheck reports no errors. Once you
achieve this state, it is much easier to see when changes to the program cause
Memcheck to report new errors. Experience from several years of Memcheck use
shows that it is possible to make even huge programs run Memcheck-clean. For
example, large parts of KDE, OpenOffice.org and Firefox are Memcheck-clean, or
very close to it.


={============================================================================
*kt_linux_tool_100* tool-valgrind-memcheck-doc

http://valgrind.org/docs/manual/mc-manual.html#mc-manual.leaks

To use this tool, you may specify --tool=memcheck on the Valgrind command line.
You don't have to, though, since Memcheck is the 'default' tool.

4.1. Overview

Memcheck is a memory error detector. It can detect the following problems that
are common in C and C++ programs.

    Accessing memory you shouldn't, e.g. overrunning and underrunning heap
    blocks, overrunning the top of the stack, and accessing memory after it has
    been freed.

    Using undefined values, i.e. values that have not been initialised, or that
    have been derived from other undefined values.

    Incorrect freeing of heap memory, such as double-freeing heap blocks, or
    mismatched use of malloc/new/new[] versus free/delete/delete[]

    Overlapping src and dst pointers in memcpy and related functions.

    Passing a fishy (presumably negative) value to the size parameter of a
    memory allocation function.

    Memory leaks.

Problems like these can be difficult to find by other means, often remaining
undetected for long periods, then causing occasional, difficult-to-diagnose
crashes.


4.2.8. Memory leak detection

Memcheck keeps track of all heap blocks issued in response to calls to
malloc/new et al. So when the program 'exits', it knows which blocks have not
been freed.

If --leak-check is set appropriately, for each remaining block, Memcheck
determines if the block is 'reachable' from pointers within the root-set. The
root-set consists of (a) general purpose registers of all threads, and (b)
initialised, aligned, pointer-sized data words in accessible client memory,
including stacks.

// skppied

The following is an example leak summary.

LEAK SUMMARY:
   definitely lost: 48 bytes in 3 blocks.
   indirectly lost: 32 bytes in 2 blocks.
     possibly lost: 96 bytes in 6 blocks.
   still reachable: 64 bytes in 4 blocks.
        suppressed: 0 bytes in 0 blocks.

If heuristics have been used to consider some blocks as reachable, the leak
summary details the heuristically reachable subset of 'still reachable:' per
heuristic. In the below example, of the 95 bytes still reachable, 87 bytes
(56+7+8+16) have been considered heuristically reachable.

LEAK SUMMARY:
   definitely lost: 4 bytes in 1 blocks
   indirectly lost: 0 bytes in 0 blocks
     possibly lost: 0 bytes in 0 blocks
   still reachable: 95 bytes in 6 blocks
                      of which reachable via heuristic:
                        stdstring          : 56 bytes in 2 blocks
                        length64           : 16 bytes in 1 blocks
                        newarray           : 7 bytes in 1 blocks
                        multipleinheritance: 8 bytes in 1 blocks
        suppressed: 0 bytes in 0 blocks

If `--leak-check=full` is specified, Memcheck will give details for each
definitely lost or possibly lost block, including where it was 'allocated'.
(Actually, it merges results for all blocks that have the same leak kind and
 sufficiently similar stack traces into a single "loss record". The
 --leak-resolution lets you control the meaning of "sufficiently similar".) 

It cannot tell you when or how or why the pointer to a leaked block was lost;
you have to work that out for yourself. In general, you should attempt to ensure
  your programs do not have any definitely lost or possibly lost blocks at exit.

For example:

8 bytes in 1 blocks are definitely lost in loss record 1 of 14
   at 0x........: malloc (vg_replace_malloc.c:...)
   by 0x........: mk (leak-tree.c:11)
   by 0x........: main (leak-tree.c:39)

88 (8 direct, 80 indirect) bytes in 1 blocks are definitely lost in loss record
13 of 14
   at 0x........: malloc (vg_replace_malloc.c:...)
   by 0x........: mk (leak-tree.c:11)
   by 0x........: main (leak-tree.c:25)

The first message describes a simple case of a single 8 byte block that has been
definitely lost. The second case mentions another 8 byte block that has been
definitely lost; the difference is that a further 80 bytes in other blocks are
indirectly lost because of this lost block. The loss records are not presented
in any notable order, so the loss record numbers aren't particularly meaningful.
The loss record numbers can be used in the Valgrind gdbserver to list the
addresses of the leaked blocks and/or give more details about how a block is
still reachable.


The option `--show-leak-kinds=<set>` controls the set of leak kinds to show when
--leak-check=full is specified.

The <set> of leak kinds is specified in one of the following ways:

    a comma separated list of one or more of definite indirect possible
    reachable.

    all to specify the complete set (all leak kinds).

    none for the empty set.

The 'default' value for the leak kinds to show is
--show-leak-kinds=definite,possible.

To also show the reachable and indirectly lost blocks in addition to the
definitely and possibly lost blocks, you can use --show-leak-kinds=all. 

To only show the reachable and indirectly lost blocks, use
--show-leak-kinds=indirect,reachable. The reachable and indirectly lost blocks
will then be presented as shown in the following two examples.

64 bytes in 4 blocks are still reachable in loss record 2 of 4
   at 0x........: malloc (vg_replace_malloc.c:177)
   by 0x........: mk (leak-cases.c:52)
   by 0x........: main (leak-cases.c:74)

32 bytes in 2 blocks are indirectly lost in loss record 1 of 4
   at 0x........: malloc (vg_replace_malloc.c:177)
   by 0x........: mk (leak-cases.c:52)
   by 0x........: main (leak-cases.c:80)

Because there are different kinds of leaks with different severities, an
interesting question is: which leaks should be counted as true "errors" and
which should not?

The answer to this question affects the numbers printed in the ERROR SUMMARY
line, and also the effect of the --error-exitcode option. First, a leak is
'only' counted as a true "error" if --leak-check=full is specified. Then, the
option --errors-for-leak-kinds=<set> controls the set of leak kinds to consider
as errors. The default value is --errors-for-leak-kinds=definite,possible 


={============================================================================
*kt_linux_tool_100* tool-valgrind-memcheck-case

{double-free}
When building on x86_64, it was found that linearsourcetest would fail with a
segmentation fault. Running with:

  valgrind --tool=memcheck ./linearsourcetest

reported several errors which looked like double-free. For example:

  ==27003==  Address 0xdab2c18 is 8 bytes inside a block of size 40 free'd
  ==27003==    at 0x4A077E6: free (vg_replace_malloc.c:446)
  ==27003==    by 0x54B3311: g_mutex_impl_free (in libglib-2.0.so.0.3200.4)
  ==27003==    by 0x54B3411: g_mutex_clear (in libglib-2.0.so.0.3200.4)
  ==27003==    by 0x4EA90A8: gst_object_finalize (gstobject.c:409)
  ==27003==    by 0x4EDF344: gst_element_finalize (gstelement.c:2955)
  ==27003==    by 0xB5360DA: gst_base_src_finalize (gstbasesrc.c:487)
  ==27003==    by 0xF7E587C: gst_vqesrc_finalize (gstvqesrc.c:557)
  ==27003==    by 0x51D8184: g_object_unref (in libgobject-2.0.so.0.3200.4)
  ==27003==    by 0x4EA8B77: gst_object_unref (gstobject.c:275)
  ==27003==    by 0x4EB0A18: gst_bin_remove_func (gstbin.c:1564)
  ==27003==    by 0x4EB0C7B: gst_bin_remove (gstbin.c:1617)
  ==27003==    by 0x4EADFBD: gst_bin_dispose (gstbin.c:528)

`gst_vqesrc_finalize()` takes calls `finalize()` whilst holding a lock obtained
with `GST_OBJECT_LOCK()`.

This is a bad idea because finalize() will cause `g_mutex_clear()` to be called
on the mutex on which the lock is held, and according to the glib documentation
[1]:

"Calling g_mutex_clear() on a locked mutex leads to undefined behaviour."

This patch moves the call to `finalize()` such that it is called once the mutex
is released. This makes the valgrind errors go away, `linearsourcetest` now
completes successfully.

[1] https://developer.gnome.org/glib/2.32/glib-Threads.html#g-mutex-clear                   


={============================================================================
*kt_linux_tool_100* tool-valgrind-massif

valgrind massif and massif-visualizer are powerful tools to examine memory use.

valgrind --tool=massif --pages-as-heap=no --show-below-main=yes uimanagerd -A -N
cp massif.out.* /mnt/hgfs/desktop_share/logs/uimanagerd/massif/

Massif outputs a data file that is best viewed with a reporting tool. It comes
with a basic plain text output tool (ms_print), but I find that hard to use. The
best graphical viewer I've found is massif-visualizer.

Building massif-visualizer

Install the deps:
sudo yum install graphviz-devel

You may have other unfulfilled build deps. Use:
sudo yum provides */<foo>

to discover which package they are in.

download, build, install kgraphviewer-2.1.1

ditto massif-visualizer-0.2


# ============================================================================
#{
={============================================================================
*kt_linux_tool_100* pkg-apt

<list>
apt-cache search <program name>

// dpkg - package manager for Debian
// List all packages `installed`
$ dpkg-query -l

// List packages using a search pattern: It is possible to add a search pattern
// to list packages: 

$ dpkg-query -l 'ibus*'

dpkg --info skype-debian_4.2.0.11-1_i386.deb 
rmp -qa

There's an easy way to see the locations of all the files installed as part of
the package, using the dpkg utility.

dpkg -L `packagename`
rpm -ql `packagename`


<install>
To install a package:
sudo apt-get install tk8.5 

# -i, --install
$ sudo dpkg --install skype-debian_4.2.0.11-1_i386.deb

To remove a package:
sudo apt-get purge tk8.5 


<aptitude>
aptitude search ^wine
aptitude install "name"
aptitude remove "name"


{update-and-upgrade}
The apt-get install command is 'recommended' because it upgrades one or more
already installed packages without upgrading every package installed, whereas
the apt-get upgrade command installs the newest version of all currently
installed packages. In additon, apt-get update command must be executed before
an upgrade to resynchronize the package index files.


{update-error}
When see:
Update Error: Require Installation Of Untrusted Packages

Run manually on console

sudo apt-get upgrade xxx

The simplest way to get the required packages is using apt-get build-dep wine
respectively aptitude build-dep wine. 


{source-list}
/etc/apt/sources.list

As part of its operation, Apt uses a file that lists the 'sources' from which
packages can be obtained. This file is /etc/apt/sources.list.

The entries in this file normally follow this format (the entries below are
    fictitious and should not be used):

deb http://site.example.com/debian distribution component1 component2 component3
deb-src http://site.example.com/debian distribution component1 component2 component3

Archive type: deb or deb-src

The first word on each line, deb or deb-src, indicates the type of archive. Deb
indicates that the archive contains binary packages (deb), the pre-compiled
packages that we normally use. Deb-src indicates source packages, which are the
original program sources plus the Debian control file (.dsc) and the diff.gz
containing the changes needed for packaging the program.

Repository URL

The next entry on the line is a URL to the repository that you want to download
the packages from. The main list of Debian repository mirrors is located here.

Distribution

The 'distribution' can be either the release 'code' name / alias (wheezy,
    jessie, stretch, sid) or the release 'class' (oldstable, stable, testing,
      unstable) respectively. If you mean to be tracking a release class then
    use the class name, if you want to track a Debian point release, use the
    code name.

For example, if you have a system running Debian 8.2 "jessie" and don't want to
upgrade when Debian stretch releases, use 'jessie' instead of 'stable' for the
distribution. If you always want to help test the testing release, use
'testing'. If you are tracking stretch and want to stay with it from testing to
end of life, use 'stretch'.

Component: main, contrib, ...

main consists of DFSG-compliant packages, which do not rely on software outside
this area to operate. These are the only packages considered part of the Debian
distribution.

contrib packages contain DFSG-compliant software, but have dependencies not in
main (possibly packaged for Debian in non-free).

non-free contains software that does not comply with the DFSG.

Example sources.list for Debian 8 "Jessie"

deb http://httpredir.debian.org/debian jessie main
deb-src http://httpredir.debian.org/debian jessie main

deb http://httpredir.debian.org/debian jessie-updates main
deb-src http://httpredir.debian.org/debian jessie-updates main

deb http://security.debian.org/ jessie/updates main
deb-src http://security.debian.org/ jessie/updates main

If you also want the contrib and non-free components, add contrib non-free after
main. 

<package-manager-database>
$ cat /etc/apt/sources.list
# 
# deb cdrom:[Debian GNU/Linux 7.7.0 _Wheezy_ - Official amd64 CD Binary-1 20141018-13:06]/ wheezy main

deb http://mirrors.kernel.org/debian wheezy main contrib non-free
deb-src http://mirrors.kernel.org/debian wheezy main contrib non-free

deb http://security.debian.org/ wheezy/updates main
deb-src http://security.debian.org/ wheezy/updates main

# wheezy-updates, previously known as 'volatile'
# deb http://mirrors.kernel.org/debian wheezy-updates main
# deb-src http://mirrors.kernel.org/debian wheezy-updates main
# Devarch Packages
deb http://devarch-deb.dev.youview.co.uk:8080/job/DEBs/ws wheezy main contrib non-free
deb http://http.debian.net/debian/ wheezy-backports main contrib non-free
# deb http://ftp.uk.debian.org/debian wheezy main
deb http://mozilla.debian.net/ wheezy-backports iceweasel-release


={============================================================================
*kt_linux_tool_101* package: pkg-config

The pkg-config program is used to retrieve information about installed libraries
in the system.  It is typically used to compile and link against one or more
libraries.  Here is a typical usage scenario in a Makefile:

program: program.c
   cc program.c $(pkg-config --cflags --libs gnomeui)

pkg-config retrieves information about packages from special metadata  files.
These files  are named after the package, and has a .pc extension.

It will additionally look in the colon-separated (on Windows,
    semicolon-separated) list of directories  specified  by the PKG_CONFIG_PATH
environment variable.

       --cflags
              This  prints pre-processor and compile flags required to compile
              the packages on the command line, including flags for all their
              dependencies.  Flags  are "compressed"  so that each identical
              flag appears only once. pkg-config exits with a nonzero code if it
              can't find metadata for one or more of the packages on the command
              line.

       --cflags-only-I
              This  prints the -I part of "--cflags". That is, it defines the
              header search path but doesn't specify anything else.

       --libs 
              This option is identical to "--cflags", only it prints  the  link
              flags.  As with  "--cflags",  duplicate  flags are merged
              (maintaining proper ordering), and flags for dependencies are
              included in the output.

       --list-all
              List all modules found in the pkg-config path.


       PKG_CONFIG_PATH
              A  colon-separated  (on  Windows,  semicolon-separated)  list  of
              directories  to  search  for  .pc  files.   The  default
              directory  will  always  be   searched   after   searching   the
              path;   the   default   is libdir/pkgconfig:datadir/pkgconfig
              where libdir is the libdir for pkg-config and datadir is the
              datadir for pkg-config when it was installed.

note:
Use to check the version found in .pc file in PKG_CONFIG_PATH:

pkg-config --modversion `name`

  --modversion                            output version for package

$ pkg-config --modversion gstreamer-1.0
1.6.2


={============================================================================
*kt_linux_tool_140* make-cmake

https://cmake.org/


={============================================================================
*kt_linux_tool_140* make-cmake-basic

https://cmake.org/cmake-tutorial/

Note that this example uses lower case commands in the CMakeLists.txt file.
Upper, lower, and mixed case commands are supported by CMake. 


http://llvm.org/docs/CMakePrimer.html

<list>
Lists of Lists

One of the more complicated patterns in CMake is lists of lists. Because a
list cannot contain an element with a semi-colon to construct a list of lists
you make a list of variable names that refer to other lists. For example:

set(list_of_lists a b c)
set(a 1 2 3)
set(b 4 5 6)
set(c 7 8 9)

With this layout you can iterate through the list of lists printing each value
with the following code:

foreach(list_name IN LISTS list_of_lists)
  foreach(value IN LISTS ${list_name})
    message(${value})
  endforeach()
endforeach()

You'll notice that the inner foreach loop’s list is doubly dereferenced. This
is because the first dereference turns list_name into the name of the sub-list
(a, b, or c in the example), then the second dereference is to get the value
of the list.

This pattern is used throughout CMake, the most common example is the compiler
flags options, which CMake refers to using the following variable expansions:
CMAKE_${LANGUAGE}_FLAGS and CMAKE_${LANGUAGE}_FLAGS_${CMAKE_BUILD_TYPE}.


<scope>
Scope

CMake inherently has a `directory-based scoping` Setting a variable in a
CMakeLists file, will set the variable for that file, and all subdirectories.
Variables set in a CMake module that is included in a CMakeLists file will be
set in the scope they are included from, and all subdirectories.

When a variable that is already set is set again in a subdirectory it
overrides the value in that scope and any deeper subdirectories.

The CMake set command provides two scope-related options. PARENT_SCOPE sets a
variable into the parent scope, and not the current scope. The CACHE option
sets the variable in the CMakeCache, which results in it being set in all
scopes. The CACHE option will not set a variable that already exists in the
CACHE unless the FORCE option is specified.

In addition to directory-based scope, CMake functions also have their own
scope. This means variables set inside functions do not bleed into the parent
scope. This is not true of macros, and it is for this reason LLVM prefers
functions over macros whenever reasonable.


Note
Unlike C-based languages, CMake’s loop and control flow blocks do not have
their own scopes.

The single most important thing to know about CMake’s if blocks coming from a
C background is that they do not have their own scope. Variables set inside
conditional blocks persist after the endif().


<module>
Modules, Functions and Macros

Modules

Modules are CMake's vehicle for enabling code reuse. CMake modules are just
CMake script files. They can contain code to execute on include as well as
definitions for commands.

In CMake macros and functions are universally referred to as commands, and
they are the primary method of defining code that can be called multiple
times.

In LLVM we have several CMake modules that are included as part of our
distribution for developers who don’t build our project from source. Those
modules are the fundamental pieces needed to build LLVM-based projects with
CMake. We also rely on modules as a way of organizing the build system’s
functionality for maintainability and re-use within LLVM projects.


Argument Handling

When defining a CMake command handling arguments is very useful. The examples
in this section will all use the CMake function block, but this all applies to
the macro block as well.

CMake commands can have named arguments, but all commands are implicitly
`variable argument` (note, variable number of argument). If the command has
named arguments they are required and must be specified at every call site.

Below is a trivial example of providing a wrapper function for CMake’s built
in function add_dependencies.

function(add_deps target)
  add_dependencies(${target} ${ARGV})
endfunction()

This example defines a new macro named add_deps which takes a required first
argument, and just calls another function passing through the first argument
and `all trailing arguments` When variable arguments are present CMake defines
them in a list named ARGV, and the count of the arguments is defined in ARGN.

CMake provides a module CMakeParseArguments which provides an implementation
of advanced argument parsing. We use this all over LLVM, and it is recommended
for any function that has complex argument-based behaviors or optional
  arguments. 

CMake's official documentation for the module is in the cmake-modules manpage,
  and is also available at the cmake-modules online documentation.

Note
As of CMake 3.5 the cmake_parse_arguments command has become a native command
and the CMakeParseArguments module is empty and only left around for
compatibility.


={============================================================================
*kt_linux_tool_140* make-cmake-add-subdir

./mheg
+-- CMakeLists.txt
+-- include
  +-- dbg.h
  +-- def.h
+-- main
  +-- main.c

To:

./mheg
+-- CMakeLists.txt
+-- include
  +-- dbg.h
  +-- def.h
+-- main
  +-- main.c
+-- mh5eng
  +-- CMakeLists.txt
  +-- sample.c
  +-- sample.h


The changes made to build:

../mheg/CMakeLists.txt

	 INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/mh5eng)

	 ADD_EXECUTABLE(${PROJECT_NAME}
		 main/main.c # uses sample.h
	 )

	 ADD_SUBDIRECTORY(mh5eng)

	 TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})

../mheg/mh5eng/CMakeLists.txt

	 SET(MH5ENG_HEADERS
		 sample.h
	 )

	 SET(MH5ENG_SOURCES
		 sample.c
	 )

	 ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES})

This create libmh5eng.a as a output


={============================================================================
*kt_linux_tool_140* make-cmake-mix-c-and-cpp

To compile the mix of c and cpp file, when try followings:

PROJECT(mhegproto C)

SET(MH5ENG_SOURCES
	xx.c
	mh5i_residentprogram_db.cpp
)

ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES} )

cmake do not compile cpp file although it is defined in the set variable. To
make it built, must add c++ build as below:

PROJECT(mhegproto C CXX)

From http://cmake.org/cmake/help/v2.8.8/cmake.html

project: Set a name for the entire project.

project(<projectname> [languageName1 languageName2 ... ] )

Sets the name of the project. Additionally this sets the variables
<projectName>_BINARY_DIR and <projectName>_SOURCE_DIR to the respective
values.

Optionally you can specify which languages your project supports. Example
languages are CXX (i.e.  C++), C, Fortran, etc. By default C and CXX are
enabled. E.g. if you do not have a C++ compiler, you can disable the check for
it by explicitly listing the languages you want to support, e.g. C. By using
the special language "NONE" all checks for any language can be disabled. If a
variable exists called CMAKE_PROJECT_<projectName>_INCLUDE_FILE, the file
pointed to by that variable will be included as the last step of the project
command.

note: by default? seems not.


={============================================================================
*kt_linux_tool_140* make-cmake-compile-flag

To enable preprocessor, set -E as below.

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "-E ${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")


={============================================================================
*kt_linux_tool_140* make-cmake-include

Found that building a library in the subdir cannot find necessary includes.

root CMakeList.txt:

INCLUDE(FindPkgConfig)
pkg_check_modules(APPS_PKGS REQUIRED
	capi-appfw-application
	dlog
	edje
	elementary
	ecore-x
	evas
	utilX
	x11
	aul
	ail
)

ADD_EXECUTABLE(${PROJECT_NAME}
	main/main.c
	main/viewmgr.c
	main/view_main.c
)

ADD_SUBDIRECTORY(mh5eng)
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)

Here there is no problem to use headers from packages such as elementary but
when add the same to the one in /mh5eng then cannot find headers. When looked
at flags used to build mh5eng, there is no necessary -I. This is the same when
add pkg_check_moudles in the mh5eng/CMakeList.txt. Thing is build faild to
update flags as expected.

The finding is that when move mh5eng then it builds without adding
pkg_check_moudles in the mh5eng/CMakeList.txt

...
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)
ADD_SUBDIRECTORY(mh5eng) [KT] moved to here

That suggests that where to put is important and not sure it is a GBS(git
    build system) or cmake itself problem. 


Found that this error still happens when build cpp file and the solution is:

From:
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

To:
MESSAGE("KT >>>>>PKGS_LDFLAGS>>>>>" : ${APPS_PKGS_CFLAGS})
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")
SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${EXTRA_CFLAGS}")

Turns out that PKGS_CFLAGS will have necessary includes depending on pkg
selection.


={============================================================================
*kt_linux_tool_140* make-cmake-link-flag

To solve {cyclic-dependencies} in link, can use this:

From defining each: 
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
...

To use group:
TARGET_LINK_LIBRARIES(${PROJECT_NAME} -Wl,--start-group mhdebug pfm mh5eng mh5dec mhv mah
-Wl,--end-group ${APPS_PKGS_LDFLAGS})

 On 05/19/2011 11:11 AM, Anton Sibilev wrote:
> Hello!
> I'm wondering how I can use "--start-group archives --end-group"
> linker flags with "Unix Makefiles".
> May be somebody know the right way?

You might specify these flags immediately in TARGET_LINK_LIBRARIES():

CMAKE_MINIMUM_REQUIRED(VERSION 2.8 FATAL_ERROR)
PROJECT(LINKERGROUPS C)
SET(CMAKE_VERBOSE_MAKEFILE ON)
FILE(WRITE ${CMAKE_BINARY_DIR}/f.c "void f(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g1.c "void g1(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g2.c "void g2(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/h.c "void h(void){}\n")
ADD_LIBRARY(f STATIC f.c)
ADD_LIBRARY(g1 STATIC g1.c)
ADD_LIBRARY(g2 STATIC g2.c)
ADD_LIBRARY(h STATIC h.c)
FILE(WRITE ${CMAKE_BINARY_DIR}/main.c "int main(void){return 0;}\n")
ADD_EXECUTABLE(main main.c)
TARGET_LINK_LIBRARIES(main f -Wl,--start-group g1 g2 -Wl,--end-group h)

However, do you really need these flags? Refer to the target properties
[IMPORTED_]LINK_INTERFACE_MULTIPLICITY[_<CONFIG>] and the documentation
of TARGET_LINK_LIBRARIES().

Regards,

Michael

LINK_INTERFACE_MULTIPLICITY

Repetition count for STATIC libraries with cyclic dependencies.

When linking to a STATIC library target with cyclic dependencies the linker
may need to scan more than once through the archives in the strongly connected
component of the dependency graph. CMake by default constructs the link line
so that the linker will scan through the component at least twice.  This
property specifies the minimum number of scans if it is larger than the
default. CMake uses the largest value specified by any target in a component.

={============================================================================
*kt_linux_tool_140* make-kbuild

http://www.linuxjournal.com/article/6568?page=0,0

In versions before the 2.5 kernel, configuration was driven by a `Config.in`
file within every subdirectory and a main help file,
Documentation/Configure.help. The language used to describe the build process
  was based loosely on a shell-style language that would control which
  configuration options were presented to the user, depending on which options
  were currently presented. 

This article describes the format of the makefile and configuration files in
the 2.5 kernel and shows how to add a new driver to the build process.

Configuring the Kernel

To configure different kernel options, a user runs either a text-mode or a
graphical kernel configurator. The text-mode configurator can be run with make
config and prompts the user to select configuration options in order. 

The ncurses text version is more popular and is run with the `make menuconfig`
option. The graphical configurator is run with make xconfig and uses Qt as the
widget set.

When the kernel configurator is run, it reads the main kernel configuration
file, located in arch/i386/Kconfig for the i386 platform. Other architectures
have the main configuration files located in their main directories. This main
configuration file then includes other configuration files from the different
subdirectories in the kernel directory tree. Those configuration files also
can include other configuration files as needed. For example, the
arch/i386/Kconfig file contains the line:

source "sound/Kconfig"

which will read information from that file. This sound/Kconfig file then
includes a lot of other files: 

The sound/usb/Kconfig file describes all of the ALSA USB driver options, like
this:

# ALSA USB drivers
menu "ALSA USB devices"
    depends on SND!=n && USB!=n

config SND_USB_AUDIO
    tristate "USB Audio/MIDI driver"
    depends on SND && USB
    help
      Say 'Y' or 'M' to include support for
      USB audio and USB MIDI devices.
endmenu

The # character can be used to comment Kconfig files. Anything written after
it on the same line is not read by the configurator, but it is useful for
documenting what the file is for and what it should do.

The menu and endmenu commands tell the configurator to declare a new menu
level or new screen in some of the configuration programs. On the menu line,
the name of the menu should be specified within “ characters. For this file,
the menu is called "ALSA USB devices".

Menus and configuration options can be controlled to display or not. In this
example, the USB option menu is `only displayed if` the CONFIG_SND and
CONFIG_USB options are selected, which is controlled by the line depends on
SND!=n && USB!=n. To help decrease the amount of typing involved, all
configuration options automatically start with CONFIG, which is not used
within the configuration language. The valid states for a configuration option
are:

    y—the option is enabled.

    n—the option is not enabled.

    m—the option is set to be built as a module.

If both the CONFIG_SND and CONFIG_USB options are not set to n (meaning they
    are set either to be built in to the kernel or to build as a module), the
CONFIG_SND_USB_AUDIO option is presented to the user. This option can be set
to one of the three values, and it is described as a “tristate” value. The
text that should be shown to the user is "USB Audio/MIDI driver":

tristate "USB Audio/MIDI driver"

The valid values for describing a configuration variable are:

    bool—the variable can be set only to y or n.

    tristate—the variable can be set to y, n or m.

    int—the variable can be set to any numeric value.

This configuration option is controlled by a depends logic line, which follows
the same logic as a menu option. The CONFIG_SND_USB_AUDIO option depends on
`both the CONFIG_SND and CONFIG_USB options`, meaning that 

if one of these options is set to a module, then the CONFIG_SND_USB_AUDIO
  option also should be set to a module. 

If both of the controlling options are not enabled (meaning both are set to
    n), this option will not be displayed. 

If both of these options are set to y, this option can be selected as n, y or
m. All of this is defined with the simple line:

depends on SND && USB

Within the kernel code, the configuration variable will be seen (the
    CONFIG_SND_USB_AUDIO in the above example), so the code can test for it or
any other kernel configuration option's existence. However, using #ifdef
within a .c file to test for different configuration options is against the
kernel-style programming guidelines, which I covered in my article “Proper
Linux Kernel Coding Style” [LJ, July 2002, www.linuxjournal.com/article/5780].

Instead, limit the use of #ifdef to .h files, keeping the .c files cleaner and
easier to read.

Previously, the help text for a configuration option was placed in one big
Configuration.help file. Now the help text is placed right after the depends
line within the Kconfig file. It begins with a line containing either help or
---help---, followed by a number of lines of help text that are indented two
spaces from the help line.


Building the Kernel

The kernel is built with a system of individual makefiles that are all linked
together when the kernel is built, forming a large makefile. The individual
makefiles do not look like any standard makefile, but instead follow a special
format that is unique to the kernel build process. The makefile needs to build
only the necessary files, depending on the configuration options enabled, in
the proper format (as modules or built in to the kernel). 

As an example, drivers/usb/misc/Makefile in the 2.5.59 kernel release looks
like:

#
# Makefile for the rest of the USB drivers
# (the ones that don't fit into any other
# categories)
#
obj-$(CONFIG_USB_AUERSWALD)  += auerswald.o
obj-$(CONFIG_USB_BRLVGER)    += brlvger.o
obj-$(CONFIG_USB_EMI26)      += emi26.o
obj-$(CONFIG_USB_LCD)        += usblcd.o
obj-$(CONFIG_USB_RIO500)     += rio500.o
obj-$(CONFIG_USB_SPEEDTOUCH) += speedtch.o
obj-$(CONFIG_USB_TEST)       += usbtest.o
obj-$(CONFIG_USB_TIGL)       += tiglusb.o
obj-$(CONFIG_USB_USS720)     += uss720.o
speedtch-objs := speedtouch.o atmsar.o

The line:

obj-$(CONFIG_USB_LCD)        += usblcd.o

builds the usblcd.c file into a module if the CONFIG_USB_LCD configuration
option is set to m. Otherwise, it is built into the kernel directly if that
configuration option is set to y. This step is all that is necessary to add to
a kernel makefile if the module is made from only a single .c file.

If the driver consists of multiple .c files, the name of the files needs to be
listed on separate lines, along with the name of the module that this driver
is called. In the previous example file, this listing of file and driver names
looks like:

obj-$(CONFIG_USB_SPEEDTOUCH) += speedtch.o

and

speedtch-objs := speedtouch.o atmsar.o

The first line controls whether the speedtch module is built. If it is, the
line indicates whether it is compiled into the kernel or stands as a module.
The second line explains that the speedtouch.c and atmsar.c files will be
built into .o files and then linked together into the speedtch.o module.

In older kernels, if a file exported symbols, it needed to be explicitly
mentioned in the kernel makefiles. In 2.5 and later kernels, that mention is
no longer necessary.


Adding a New Driver to the Build Process

To add a new driver to the kernel build process, a single line needs to be
added if the driver is contained within a single file. Based on the previous
example of the FooBar USB speaker device, the line:

obj-$(CONFIG_SND_USB_FOOBAR) += usbfoobar.o

is added to sound/usb/Makefile.

If the driver is contained in two files, such as foobar1.c and foobar2.c, an
additional line needs to be added:

usbfoobar-objs := foobar1.o foobar2.o

Conclusion

The kernel configuration and build process in the 2.5 kernel is much simpler
and more flexible than in the previous kernel versions. Thanks go to Roman
Zippel and Kai Germaschewski for doing the work to make it easier for kernel
developers to focus on writing code and not have to worry about the
intricacies of the kernel build process.


http://www.linuxjournal.com/content/kbuild-linux-kernel-build-system?page=0,0
http://www.linuxfromscratch.org/hints/downloads/files/kernel-configuration.txt


={============================================================================
*kt_linux_tool_140* make-gmake-sample

# simple makefile from gmake

FILE = use-main.cpp
CC = g++ -std=c++0x -I.

main : main.o fsm.o
	echo '>> bulid main'
	$(CC) -o out main.o fsm.o

fsm.o : Fsm.c
	echo '>> build fsm.c'
	$(CC) -o fsm.o -c Fsm.c

main.o : main.cpp
	echo '>> build main.cpp'
	$(CC) -o main.o -c main.cpp

clean :
	rm main.o fsm.o


={============================================================================
*kt_linux_tool_140* make-gmake-intro

<make-rule>

target ... : prerequisites ...
  recipe
  ...

A `target` is usually `the name of a file` that is generated by a program;
examples of targets are executable or object files. Can also be the `name of
  an action` to carry out, such as ‘clean’.

A `prerequisite` is a file that is used as input to create the target. A target
often depends on several files.

A `recipe` is an action that make carries out. A recipe may have more than one
command, either on the same line or each on its own line. 
  
Please note: you need to put a `tab character` at the beginning of every
recipe line! This is an obscurity that catches the unwary. 

Usually a recipe is in a rule with prerequisites and serves to create a target
file if any of the prerequisites change. However, the rule that specifies a
recipe for the target `need not have prerequisites` For example, the rule
containing the delete command associated with the target ‘clean’ does not have
prerequisites.

A rule, then, explains `how and when` to remake certain files which are the
targets of the particular rule. make carries out the recipe on the
prerequisites to create or update the target. A rule can also explain how and
when to carry out an action.


When a target is a file, it needs to be recompiled or relinked if any of its
prerequisites change. In addition, any prerequisites that are themselves
automatically generated should be updated first.


<phony-target>
Targets that do not refer to files but are just actions are `phony targets`


<default-goal>
By default, make starts with the `first target` (not targets whose names start
with ‘.’). This is called the `default goal`


<implicit-rule>

2.5 Letting make Deduce the Recipes

It is not necessary to spell out the recipes for compiling the individual C
source files, because make can figure them out: it has an `implicit rule` for
updating a ‘.o’ file from a correspondingly named ‘.c’ file using a ‘cc -c’
command.

# example

objects = main.o kbd.o command.o display.o \
  insert.o search.o files.o utils.o

edit : $(objects)
  cc -o edit $(objects)

main.o : main.c defs.h
  cc -c main.c

kbd.o : kbd.c defs.h command.h
  cc -c kbd.c

command.o : command.c defs.h command.h
  cc -c command.c

display.o : display.c defs.h buffer.h
  cc -c display.c

insert.o : insert.c defs.h buffer.h
  cc -c insert.c

search.o : search.c defs.h buffer.h
  cc -c search.c

files.o : files.c defs.h buffer.h command.h
  cc -c files.c

utils.o : utils.c defs.h
  cc -c utils.c

clean :
  rm edit $(objects)


# When use `implicit rule`

objects = main.o kbd.o command.o display.o \
  insert.o search.o files.o utils.o

edit : $(objects)
  cc -o edit $(objects)

main.o : defs.h
kbd.o : defs.h command.h
command.o : defs.h command.h
display.o : defs.h buffer.h
insert.o : defs.h buffer.h
search.o : defs.h buffer.h
files.o : defs.h buffer.h command.h
utils.o : defs.h

.PHONY : clean
clean :
  rm edit $(objects)


This prevents make from getting confused by an actual file called clean and
causes it to continue in spite of errors from rm. 

A rule such as this should not be placed at the beginning of the makefile,
because we do not want it to run by default!


={============================================================================
*kt_linux_tool_140* make-gmake-variable

6 How to Use Variables

Variables and functions in all parts of a makefile `are expanded` when read,
except for in recipes, the right-hand sides of variable definitions using ‘=’,
and the bodies of variable definitions using the define directive.

Variable names are case-sensitive.

It is traditional to use upper case letters in variable names, but we
recommend using lower case letters for variable names that serve internal
purposes in the makefile, and reserving upper case for parameters that control
implicit rules or for parameters that the user should override with command
options.

note: three ways to reference a variable

To substitute a variable’s value, write a dollar sign followed by the name of
the variable in parentheses or braces: either `$(foo)` or `${foo}` is a valid
reference to the variable foo.

Variable references can be used in any context: targets, prerequisites,
recipes, most directives, and new variable values.

A dollar sign followed by a character other than a dollar sign,
open-parenthesis or openbrace treats that single character as the variable
  name. Thus, you could reference the variable x with `$x`. However, this
  practice is strongly discouraged, except in the case of the automatic
  variables


={============================================================================
*kt_linux_tool_140* make-gmake-variable-pattern

<variable-dereferencing>
One big "Gotcha" with variable dereferencing is that if commands `implicitly`
dereference values. This has some unexpected results. For example:

if("${SOME_VAR}" STREQUAL "MSVC")

In this code sample MSVC will be implicitly dereferenced, which will result in
the if command comparing the value of the dereferenced variables SOME_VAR
and MSVC. A common workaround to this solution is to prepend strings being
compared with an x.

if("x${SOME_VAR}" STREQUAL "xMSVC")

This works because while MSVC is a defined variable, xMSVC is not. This
  pattern is uncommon, but it does occur in LLVM’s CMake scripts.

note:
To prevent name colision by making it string.

note:
GNU autotools also use this


={============================================================================
*kt_linux_tool_140* make-gmake-write

3.1 What Makefiles Contain

`Comments within a recipe` are passed to the shell, just as with any other
recipe text.  The shell decides how to interpret it: whether or not this is a
comment is up to the shell.


<include-directive>
If you want make to simply ignore a makefile which does not exist or cannot be
remade, `with no error message`, use the -include directive instead of include,
like this:

-include filenames...


<empty-recipe>
If you know that one or more of your makefiles cannot be remade and you want
to `keep make from performing an implicit rule search on them`, perhaps for
efficiency reasons, you can use any normal method of preventing implicit rule
look-up to do so. For example, you can write an explicit rule with the
makefile as the target, and an empty recipe (see Section 5.9 [Using Empty
    Recipes], page 57).


={============================================================================
*kt_linux_tool_140* make-gmake-builtin-target

4.8 Special Built-in Target Names

Certain names have special meanings if they appear as targets.

.SUFFIXES
The prerequisites of the special target .SUFFIXES are the list of suffixes to
be used in checking for suffix rules. See Section 10.7 [Old-Fashioned Suffix
Rules], page 125.


.NOTPARALLEL
If .NOTPARALLEL is mentioned as a target, then this invocation of make will be
run serially, even if the ‘-j’ option is given. Any recursively invoked make
command will still run recipes in parallel (unless its makefile also contains
    this target). Any prerequisites on this target are ignored.

<ex>
.NOTPARALLEL: install-exec-local install-data-local uninstall-local


={============================================================================
*kt_linux_tool_140* make-gmake-text-function

https://www.gnu.org/software/make/manual/html_node/Text-Functions.html

$(strip string)

    Removes leading and trailing whitespace from string and replaces each
    internal sequence of one or more whitespace characters with a single
    space. Thus, ‘$(strip a b c )’ results in ‘a b c’.

    The function `strip` can be very useful when used in conjunction with
    conditionals. When comparing something with the empty string ‘’ using ifeq
    or ifneq, you usually want a string of just whitespace to match the empty
    string (see Conditionals).

    Thus, the following may fail to have the desired results:

    .PHONY: all
    ifneq   "$(needs_made)" ""
    all: $(needs_made)
    else
    all:;@echo 'Nothing to make!'
    endif

    Replacing the variable reference ‘$(needs_made)’ with the function call
    ‘$(strip $(needs_made))’ in the ifneq directive would make it more robust.



={============================================================================
*kt_linux_tool_150* make-autoconf-autotool

http://autotoolset.sourceforge.net/tutorial.html


={============================================================================
*kt_linux_tool_150* make-autoconf

http://www.gnu.org/software/autoconf/autoconf.html
http://www.gnu.org/software/autoconf/manual/index.html

Autoconf is a tool for `producing shell scripts` that automatically configure
software source code packages to adapt to many kinds of Posix-like systems. The
configuration scripts produced by Autoconf are independent of Autoconf when they
are run, so their users do not need to have Autoconf.


The configuration scripts produced by Autoconf require no manual user
intervention when run; they do not normally even need an argument specifying the
system type. Instead, they individually `test for the presence of each feature`
that the software package they are for might need. (Before each check, they
    print a one-line message stating what they are checking for, so the user
    doesn't get too bored while waiting for the script to finish.) As a result,
they deal well with systems that are hybrids or customized from the more common
  Posix variants. There is no need to maintain files that list the features
  supported by each release of each variant of Posix.

For each software package that Autoconf is used with, it 
`creates a configuration script from a template file` that lists the system
features that the package needs or can use. 


After the shell code to recognize and respond to a system
feature has been written, Autoconf allows it to be shared by many software
packages that can use (or need) that feature. If it later turns out that the
shell code needs adjustment for some reason, it needs to be changed in only one
place; all of the configuration scripts can be regenerated automatically to take
advantage of the updated code.

Those who do not understand Autoconf are condemned to reinvent it, poorly. The
primary goal of Autoconf is making the user's life easier; making the
maintainer's life easier is only a secondary goal. Put another way, the primary
goal is not to make the generation of configure automatic for package
maintainers (although patches along that front are welcome, since package
    maintainers form the user base of Autoconf); rather, the goal is to make
configure painless, portable, and predictable for the end user of each
autoconfiscated package. And to this degree, Autoconf is highly successful at
its goal — most complaints to the Autoconf list are about difficulties in
writing Autoconf input, and not in the behavior of the resulting configure. Even
packages that don't use Autoconf will generally provide a configure script, and
the most common complaint about these alternative home-grown scripts is that
they fail to meet one or more of the GNU Coding Standards (see Configuration)
that users have come to expect from Autoconf-generated configure scripts.

Autoconf requires GNU M4 version 1.4.6 or later in order to generate the
scripts. It uses features that some versions of M4, including GNU M4 1.3, do not
have. Autoconf works better with GNU M4 version 1.4.14 or later, though this is
not required. 


={============================================================================
*kt_linux_tool_150* make-autoconf-build-system

2 The GNU Build System

the GNU build system, whose most important components are Autoconf, Automake,
and Libtool.

2.1 Automake

The ubiquity of make means that a makefile is almost the only viable way to
distribute automatic build rules for software, but one quickly runs into its
numerous limitations.  Its lack of support for automatic dependency tracking,
recursive builds in subdirectories, reliable timestamps (e.g., for network
    file systems), and so on, mean that developers must painfully (and often
      incorrectly) reinvent the wheel for each project. 
    
Portability is nontrivial, thanks to the quirks of make on many systems. On
top of all this is the manual labor required to implement the many standard
targets that users have come to expect (make install, make distclean, make
    uninstall, etc.). Since you are, of course, using Autoconf, you also have
to insert repetitive code in your ‘Makefile.in’ to recognize @CC@, @CFLAGS@,
and other substitutions provided by configure.


2.3 Libtool

Producing `shared libraries portably`, however, is the stuff of
nightmares-each system has its own incompatible tools, compiler flags, and
magic incantations.  Fortunately, GNU provides a solution: Libtool.


={============================================================================
*kt_linux_tool_150* make-autoconf-model

3 Making configure Scripts

The configuration scripts that Autoconf produces are by convention called
`configure`.  When run, `configure creates several files`, replacing
configuration parameters in them with appropriate values. The files that
configure creates are:

  * one or more ‘Makefile’ files, usually one in each subdirectory of the
    package (see Section 4.8 [Makefile Substitutions], page 23);

  * optionally, a C header file, the name of which is configurable, containing
    #define directives (see Section 4.9 [Configuration Headers], page 33);

  * a shell script called `config.status` that, when run, recreates the files
    listed above (see Chapter 17 [config.status Invocation], page 301);

  * an optional shell script normally called `config.cache` (created when
    using `configure --config-cache`) that saves the results of running many
    of the tests (see Section 7.4.2 [Cache Files], page 119);

  * a file called `config.log` containing any messages produced by compilers,
    to help debugging if configure makes a mistake.

To create a configure script with Autoconf, you need to write an Autoconf
input file `configure.ac` (or ‘configure.in’) and run autoconf on it. 

If you write `your own feature tests` to supplement those that come with
Autoconf, you might also write files called `aclocal.m4` and ‘acsite.m4’. 

If you use a C header file to contain #define directives, you might also run
autoheader, and you can distribute the generated file `config.h.in` with the
package.

Here is a diagram showing how the files that can be used in configuration are
produced.  Programs that are executed are suffixed by ‘*’. Optional files are
enclosed in square brackets (‘[]’). autoconf and autoheader also read the
installed Autoconf macro files (by reading ‘autoconf.m4’).

Files used in preparing a software package for distribution, when using just
Autoconf:

your source files --> [autoscan*] --> [configure.scan] --> configure.ac

configure.ac --.
               |   .------> `autoconf*` -----> configure
[aclocal.m4] --+---+
               |   ‘-----> [autoheader*] --> [config.h.in]
[acsite.m4] ---’

Makefile.in

<when-use-automake>
Additionally, if you use Automake, the following additional productions come
into play:

[acinclude.m4] --.
                 |
[local macros] --+--> aclocal* --> aclocal.m4
                 |
configure.ac ----’

configure.ac --.
               +--> `automake*` --> Makefile.in
Makefile.am ---’


Files used in configuring a software package:

                       .-------------> [config.cache]
configure* ------------+-------------> config.log
                       |
[`config.h.in`] -.       v            .-> [config.h] -.
               +--> config.status* -+               +--> make*
`Makefile.in` ---’                    ‘-> `Makefile` ---’


={============================================================================
*kt_linux_tool_150* make-autoconf-write

3.1 Writing configure.ac

To produce a configure script for a software package, create a file called
configure.ac that contains invocations of the Autoconf macros that test the
system features your package needs or can use. Autoconf macros already exist to
check for many features; see Existing Tests, for their descriptions. For most
other features, you can use Autoconf template macros to produce custom checks;
see Writing Tests, for information about them. 
  
Previous versions of Autoconf promoted the name configure.in. Using
`configure.ac is now preferred`

  *what-autoconf-do*
  So, what is really needed is some kind of compiler, autoconf, that takes an
  Autoconf program, ‘configure.ac’, and transforms it into a portable shell
  script, configure.


3.1.2 The Autoconf Language

Therefore, we need a means to distinguish `literal strings` from text to be
expanded: quotation. `When calling macros that take arguments`, there must not
be any white space between the macro name and the open parenthesis.

AC_INIT ([oops], [1.0]) # incorrect
AC_INIT([hello], [1.0]) # good

Arguments should be enclosed within the `quote characters [ and ]`, and be
separated by commas.


Some macros take optional arguments, which this documentation represents as
[arg] (not to be confused with the quote characters). You may just leave them
empty, or use ‘[]’ to make the emptiness of the argument explicit, or you may
  simply omit the trailing commas.

The three lines below are equivalent:

AC_CHECK_HEADERS([stdio.h], [], [], [])
AC_CHECK_HEADERS([stdio.h],,,)
AC_CHECK_HEADERS([stdio.h])


You can include `comments` in ‘configure.ac’ files by starting them with the
‘#’. For example, it is helpful to begin ‘configure.ac’ files with a line like
this:


3.1.3 Standard ‘configure.ac’ Layout

The order in which ‘configure.ac’ calls the Autoconf macros is not important,
    with a few exceptions. 

Every configure.ac `must contain` a call to AC_INIT before the checks, and a
call to AC_OUTPUT at the end. 

Additionally, some macros rely on other macros having been called first,
because they check previously set values of some variables to decide what to
do. 

These macros are noted in the individual descriptions (see Chapter 5 [Existing
    Tests], page 41), and they also warn you when configure is created if they
  are called out of order.

Generally speaking, the things near the end of this list are those that could
depend on things earlier in it.


3.3 Using ifnames to List Conditionals

*tool-ifnames*
`ifnames` scans all of the C source files named on the command line (or the
    standard input, if none are given) and writes to the standard output a
sorted list of all the identifiers that appear in those files in
#if, #elif, #ifdef, or #ifndef directives. 

It prints each identifier on a line, followed by a space-separated list of the
files in which that identifier occurs.

<ex>
$ ifnames planner_common_pdl.c 
BSKYB_SUPPORT_HEP_WITH_OIG planner_common_pdl.c
HEADEND_METADATA_CMDC_CMC planner_common_pdl.c


3.4 Using autoconf to Create configure

`To create 'configure' from configure.ac`, run the autoconf program with no
arguments. autoconf processes configure.ac with the M4 macro processor, using
the Autoconf macros. If you give autoconf an argument, it reads that file
instead of configure.ac and writes the configuration script to the standard
output instead of to configure. If you give autoconf the argument -, it reads
from the standard input instead of configure.ac and writes the configuration
script to the standard output.

The Autoconf macros are defined in several files. If a macro is defined in
more than one of the files that autoconf reads, the last definition it reads
overrides the earlier ones.


<autoconf-options>
autoconf accepts the following options: 

‘--verbose’
‘-v’ Report processing steps.

‘--debug’
‘-d’ Don’t remove the temporary files.

‘--force’
‘-f’ Remake ‘configure’ even if newer than its input files.


‘--warnings=category’
‘-W category’
Report the warnings related to category (which can actually be a comma
separated list). See Section 10.3 [Reporting Messages], page 179, macro AC_
DIAGNOSE, for a comprehensive list of categories. Special values include:

‘all’ report all the warnings
‘none’ report none
‘error’ treats warnings as errors
‘no-category’ disable warnings falling into category

Warnings about ‘syntax’ are enabled by default, and the environment
variable WARNINGS, a comma separated list of categories, is honored as well.
Passing ‘-W category’ actually behaves as if you had passed ‘--warnings
syntax,$WARNINGS,category’. To disable the defaults and WARNINGS, and
then enable warnings about obsolete constructs, use ‘-W none,obsolete’.

<backtrace>
Because autoconf uses autom4te behind the scenes, it `displays a back trace`
for errors, but not for warnings; if you want them, just pass ‘-W error’. See
Section 8.2.1 [autom4te Invocation], page 132, for some examples.

<trace>
‘--trace=macro[:format]’
‘-t macro[:format]’

Do not create the configure script, but `list the calls to macro` according to
the format. Multiple ‘--trace’ arguments can be used to list several macros.
Multiple ‘--trace’ arguments for a single macro are not cumulative; instead,
         you should just make format as long as needed.

The format is a regular string, with newlines if desired, and several special
escape codes. It defaults to ‘$f:$l:$n:$%’; see Section 8.2.1 [autom4te
Invocation], page 132, for details on the format.


It is often necessary `to check the content of a ‘configure.ac’ file`, but
parsing it yourself is extremely fragile and error-prone. It is suggested that
you rely upon ‘--trace’ to scan ‘configure.ac’. For instance, to find the list
of variables that are substituted, use:

$ autoconf -t AC_SUBST
configure.ac:2:AC_SUBST:ECHO_C
configure.ac:2:AC_SUBST:ECHO_N
configure.ac:2:AC_SUBST:ECHO_T
More traces deleted


3.5 Using autoreconf to Update configure Scripts

`autoreconf runs` autoconf, autoheader, aclocal, automake, libtoolize, and
autopoint (when appropriate) repeatedly to update `the GNU Build System` in
the specified directories and their subdirectories


={============================================================================
*kt_linux_tool_150* make-autoconf-macro

4.1 Initializing configure

Every configure.ac must call AC_INIT before doing anything else that produces
output.

AC_INIT (package, version, [bug-report], [tarname], [url]) [Macro]

Process any command-line arguments and perform initialization and
verification.

The following M4 macros (e.g., AC_PACKAGE_NAME), output variables (e.g.,
    PACKAGE_ NAME), and preprocessor symbols (e.g., PACKAGE_NAME), are
`defined by AC_INIT`:


4.2 Dealing with Autoconf versions

The following optional macros can be used to help choose the minimum version
of Autoconf that can successfully compile a given ‘configure.ac’.

AC_PREREQ (version) [Macro]

Ensure that a recent enough version of Autoconf is being used. If the version
of Autoconf being used to create configure is earlier than version, print an
error message to the standard error output and exit with failure (exit status
is 63). For example: AC_PREREQ([2.69])

This macro may be used before AC_INIT.


4.4 Finding configure Input

AC_CONFIG_SRCDIR (unique-file-in-source-dir) [Macro]

unique-file-in-source-dir is some file that is in the package’s source
directory; configure checks for this `file's existence` to make sure that the
directory that it is told contains the source code in fact does. Occasionally
people accidentally specify the wrong directory with ‘--srcdir’; this is a
safety check. See Section 16.10 [configure Invocation], page 299, for more
information.

Packages that do manual configuration or use the install program might need to
  tell configure where to find some other shell scripts by calling
  AC_CONFIG_AUX_DIR, though the default places it looks are correct for most
  cases.


4.5 Outputting Files

Every Autoconf script, e.g., ‘configure.ac’, should finish by calling
AC_OUTPUT. (When runs `configure` scrupt) That is the macro that generates and
runs ‘config.status’, which in turn `creates the makefiles` and any other files
resulting from configuration.  This is the only required macro besides AC_INIT

AC_OUTPUT [Macro]
Generate ‘config.status’ and launch it. Call this macro once, at the end of
‘configure.ac’.

‘config.status’ performs all the configuration actions: all the output files,
header files, commands , links, subdirectories to configure are honored.

The location of your AC_OUTPUT invocation is the exact point where
configuration actions are taken: any code afterwards is executed by configure
once config.status was run.


<config-status>
4.6 Performing Configuration Actions

‘configure’ is designed so that it appears to do everything itself, but there
is actually a hidden slave: ‘config.status’. 

‘configure’ is in charge of examining your system, but it is ‘config.status’
that actually takes the proper actions based on the results of ‘configure’.
The most typical task of ‘config.status’ is `to instantiate files` 

This section describes the common behavior of the four standard instantiating
macros: AC_CONFIG_FILES, AC_CONFIG_HEADERS, AC_CONFIG_COMMANDS and
AC_CONFIG_LINKS.

They all have this prototype:

AC_CONFIG_ITEMS(tag..., [commands], [init-cmds])

where the arguments are:

`tag`

A `blank-or-newline-separated list` of tags, which are typically the names of
the files `to instantiate` You are encouraged to use `literals` as tags.

The macros AC_CONFIG_FILES and AC_CONFIG_HEADERS use special tag values: they
may have the form ‘output’ or ‘output:inputs’. The file output is instantiated
from its templates, `inputs` (`defaulting to output.in`).

‘AC_CONFIG_FILES([Makefile:boiler/top.mk:boiler/bot.mk])’, for example, asks
for the creation of the file ‘Makefile’ that contains the expansion of the
  output variables in the concatenation of ‘boiler/top.mk’ and
    ‘boiler/bot.mk’.

note:
Means that Makefile can be created from `Makefile.in`, bolier/top.mk, or
boiler/bot.mk.


4.7 Creating Configuration Files

AC_CONFIG_FILES (file . . . , [cmds], [init-cmds]) [Macro]

Make AC_OUTPUT `create each file` by copying an input file (by default
    ‘file.in’), `substituting the output variable values` This macro is one of
the instantiating macros;

This macro creates the directory that the file is in if it doesn't exist.
  Usually, makefiles are created this way, but other files


4.8 Substitutions in Makefiles

Each subdirectory in a distribution that contains something to be compiled or
installed should come with a file ‘Makefile.in’, from which configure creates
a file ‘Makefile’ in that directory. `To create Makefile`, configure performs
a simple variable substitution, replacing occurrences of `@variable@` in
`Makefile.in with the value that configure has determined for that variable`

Variables that are substituted into output files in this way are called
`output variables` They are ordinary shell variables that are set in
configure. 

To make configure substitute a particular variable into the output files, the
macro `AC_SUBST` must be called with that variable name as an argument. Any
occurrences of ‘@variable@’ for other variables are left unchanged. See
Section 7.2 [Setting Output Variables], for more information on creating
output variables with AC_SUBST.

A software package that uses a configure script should be distributed with a
file ‘Makefile.in’, but no makefile; that way, the user has to properly
configure the package for the local system before compiling it.

See Section “Makefile Conventions” in The GNU Coding Standards, for more
information on what to put in makefiles.


4.8.1 Preset Output Variables

Some output variables are preset by the Autoconf macros. Some of the Autoconf
macros set additional output variables, which are mentioned in the
descriptions for those macros.  See Section B.2 [Output Variable Index], page
364, for a complete list of output variables.  See Section 4.8.2 [Installation
Directory Variables], page 27, for the list of the preset ones related to
installation directories. 

Below are listed the other preset ones, many of which are precious variables


CFLAGS [Variable]

Debugging and optimization options for the C compiler. If it is not set in the
environment when configure runs, the default value is set when you call
AC_PROG_CC (or empty if you don’t). configure uses this variable when
compiling or linking programs to test for C features.

If a compiler option affects only the behavior of the preprocessor (e.g.,
    ‘-Dname’), it should be put into CPPFLAGS instead. If it affects only the
linker (e.g., ‘-Ldirectory’), it should be put into LDFLAGS instead. If it
affects only the compiler proper, CFLAGS is the natural home for it. If an
option affects multiple phases of the compiler, though, matters get tricky.
One approach to put such options directly into CC, e.g., CC=’gcc -m64’.
Another is to put them into both CPPFLAGS and LDFLAGS, but not into CFLAGS.

However, remember that some ‘Makefile’ variables are reserved by the GNU
Coding Standards for the use of the "user"—the person building the package.
For instance, CFLAGS is one such variable.

Sometimes package developers are tempted to set user variables such as CFLAGS
because it appears to make their job easier. However, the package itself
should never set a user variable, particularly not to include switches that
are required for proper compilation of the package. Since these variables are
documented as being for the package builder, that person rightfully expects to
be able to override any of these variables at build time. If the package
developer needs to add switches without interfering with the user, the proper
way to do that is to introduce an additional variable. Automake makes this
easy by introducing AM_CFLAGS (see Section “Flag Variables Ordering” in GNU
    Automake), but the concept is the same even if Automake is not used.


CPPFLAGS [Variable]

Preprocessor options for the C, C++, Objective C, and Objective C++
preprocessors and compilers. If it is not set in the environment when
configure runs, the default value is empty.

This variable’s contents should contain options like ‘-I’, ‘-D’, and ‘-U’ that
affect only the behavior of the preprocessor. Please see the explanation of
CFLAGS for what you can do if an option affects other phases of the compiler
as well.

Currently, configure always links as part of a single invocation of the
compiler that also preprocesses and compiles, so it uses this variable also
when linking programs.  However, `it is unwise to depend on this behavior`
because the GNU Coding Standards do not require it and many packages do not
use CPPFLAGS when linking programs.

See Section 7.3 [Special Chars in Variables], page 116, for limitations that
CPPFLAGS might run into.


CXXFLAGS [Variable]

Debugging and optimization options for the C++ compiler. It acts like CFLAGS,
but for C++ instead of C.


DEFS [Variable]

‘-D’ options to pass to the C compiler. If AC_CONFIG_HEADERS is called,
configure replaces ‘@DEFS@’ with ‘-DHAVE_CONFIG_H’ instead (see Section 4.9
    [Configuration Headers], page 33). This variable is not defined while
  configure is performing its tests, only when creating the output files.


LDFLAGS [Variable]
Options for the linker. If it is not set in the environment when configure
runs, the default value is empty. configure uses this variable when linking
programs to test for C, C++, Objective C, Objective C++, Fortran, and Go
features.  This variable’s contents should contain options like ‘-s’ and ‘-L’
that affect only the behavior of the linker. 

Don’t use this variable `to pass library names` (‘-l’) to the linker; use LIBS
instead.

<ex>
configure: using LDFLAGS:
-L/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/lib
-L/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/oss/lib
-Wl,--as-needed
-Wl,-rpath-link,/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/lib
-Wl,-rpath-link,/data/ws/zinc-install-root/release/humax-dtr_t4000/opt/zinc-trunk/oss/lib
-L/data/ws/zinc-install-root/release/humax-dtr_t4000/lib64
-L/opt/oem-staging/humax-dtr_t4000/usr/local/lib  -mcpu=cortex-a15
-mtune=cortex-a15 -mfloat-abi=softfp -mfpu=neon-vfpv4 -D__arm__
-L/opt/oem-staging/humax-dtr_t4000/usr/local/lib
-Wl,-rpath-link,/opt/oem-staging/humax-dtr_t4000/usr/local/lib

configure: LDFLAGS error: LDFLAGS may only be used to specify linker flags, not
macro definitions. Use CPPFLAGS for: -D__arm__


LIBS [Variable]

‘-l’ options to pass to the linker. The default value is empty, but some
Autoconf macros may prepend extra libraries to this variable if those
libraries are found and provide necessary functions, see Section 5.4
[Libraries], page 49. configure uses this variable when linking programs to
test for C, C++, Objective C, Objective C++, Fortran, and Go features.


srcdir [Variable]
The name of the directory that contains the source code for that makefile.


4.8.2 Installation Directory Variables

The following variables specify the directories for package installation, see
Section “Variables for Installation Directories” in The GNU Coding Standards,
for more information.

exec_prefix [Variable]
The installation prefix for `architecture-dependent files` By default it’s the
same as `prefix`. You should avoid installing anything directly to exec_prefix.
However, the default value for directories containing architecture-dependent
files should be relative to exec_prefix.

prefix [Variable]
The common installation prefix for all files. If exec_prefix is defined to a
different value, prefix is used only for `architecture-independent files`

$ ./configure --help

Installation directories:
  --prefix=PREFIX         install architecture-independent files in PREFIX
                          [/usr/local]
  --exec-prefix=EPREFIX   install architecture-dependent files in EPREFIX
                          [PREFIX]

By default, `make install' will install all the files in `/usr/local/bin',
`/usr/local/lib' etc.  You can specify an installation prefix other than
    `/usr/local' using `--prefix', for instance `--prefix=$HOME'.

datadir [Variable]
The directory for installing idiosyncratic read-only architecture-independent
data.


Most of these variables have values that rely on prefix or exec_prefix. It is
deliberate that the directory output variables keep them unexpanded:

For instance, it is essential that datarootdir remains defined as
‘${prefix}/share’, so that its value can be expanded based on the current
value of prefix.

note:
Should be used only in makefiles.

A corollary is that you should not use these variables except in makefiles.
For instance, instead of trying to evaluate datadir in ‘configure’ and
hard-coding it in makefiles using e.g., ‘AC_DEFINE_UNQUOTED([DATADIR],
    ["$datadir"], [Data directory.])’, you should add ‘-DDATADIR=’$(datadir)’’
to your makefile’s definition of CPPFLAGS (AM_CPPFLAGS if you are also using
    Automake).

Similarly, you should not rely on AC_CONFIG_FILES to replace bindir and
friends in your shell scripts and other files; instead, let make manage their
replacement. For instance Autoconf ships templates of its shell scripts ending
with ‘.in’, and uses a makefile snippet similar to the following to build
scripts like autoheader and autom4te:


4.9 Configuration Header Files

When a package contains more than a few tests that define C preprocessor
symbols, the command lines to pass ‘-D’ options to the compiler can get quite
long. This causes two problems. One is that the make output is hard to
visually scan for errors. More seriously, the command lines can exceed the
length limits of some operating systems. 

As an alternative to passing ‘-D’ options to the compiler, configure scripts
can create a C header file containing ‘#define’ directives. The
`AC_CONFIG_HEADERS` macro selects this kind of output. Though it can be called
anywhere between AC_INIT and AC_OUTPUT, it is customary to call it right after
AC_INIT.

The package should ‘#include’ the configuration header file before any other
header files, to prevent inconsistencies in declarations (for example, if it
redefines const).

To provide for VPATH builds, remember to pass the C compiler a ‘-I.’ option
(or ‘-I..’; whichever directory contains ‘config.h’). Even if you use
‘#include "config.h"’, the preprocessor searches only the directory of the
currently read file, i.e., the source directory, not the build directory.

With the appropriate ‘-I’ option, you can use ‘#include <config.h>’. Actually,
it’s a good habit to use it, because in the rare case when the source
  directory contains another ‘config.h’, the build directory should be
  searched first.

AC_CONFIG_HEADERS (header . . . , [cmds], [init-cmds]) [Macro]

This macro is one of the instantiating macros; see Section 4.6 [Configuration
Actions], page 21. Make AC_OUTPUT create the file(s) in the
blank-or-newline-separated list header containing C preprocessor #define
statements, and replace ‘@DEFS@’ in generated files with ‘-DHAVE_CONFIG_H’
instead of the value of DEFS. The usual name for header is ‘config.h’.

configure
3527:DEFS=-DHAVE_CONFIG_H


4.9.1 Configuration Header Templates

Your distribution should contain a template file that looks as you want the
final header file to look, including comments, with #undef statements which
are used as hooks. For example, suppose your ‘configure.ac’ makes these calls:

AC_CONFIG_HEADERS([conf.h])
AC_CHECK_HEADERS([unistd.h])

Then you could have code like the following in ‘conf.h.in’. The ‘conf.h’
created by configure defines ‘HAVE_UNISTD_H’ to 1, if and only if the system
has ‘unistd.h’.
/* Define as 1 if you have unistd.h. */
#undef HAVE_UNISTD_H

note:
Can use autoreconof to create config.h.in


={============================================================================
*kt_linux_tool_150* make-autoconf-tests

5 Existing Tests

These macros test for particular system features that packages might need or
want to use. If you need to test for a kind of feature that none of these
macros check for, you can probably do it by calling primitive test macros with
appropriate arguments (see Chapter 6 [Writing Tests], page 101).

These tests print messages telling the user which feature they're checking
for, and what they find. They cache their results for future configure runs
  (see Section 7.4 [Caching Results], page 117).

`Some of these macros set output variables` See Section 4.8 [Makefile
Substitutions], page 23, for how to get their values. 
  
The phrase “define name” is used below as a shorthand to mean “define the C
preprocessor symbol name to the value 1”. See Section 7.1 [Defining Symbols],
page 113, for how to get those symbol definitions into your program.  
  

5.1 Common Behavior 

Much effort has been expended to make Autoconf easy to learn. The most obvious
way to reach this goal is simply to enforce standard interfaces and behaviors,
avoiding exceptions as much as possible. Because of history and inertia,
unfortunately, there are still too many exceptions in Autoconf; nevertheless,
this section describes some of the common rules.


5.5.3 Generic Function Checks

These macros are used to find functions not covered by the "particular"
test macros. If the functions might be in libraries other than the
default C library, first call `AC_CHECK_LIB' for those libraries. 

If you need to check the behavior of a function as well as find out whether it
is present, you have to write your own test for it (*note Writing Tests::).

 -- Macro: AC_CHECK_FUNC (FUNCTION, [ACTION-IF-FOUND],
          [ACTION-IF-NOT-FOUND])

     If C function FUNCTION is available, run shell commands ACTION-IF-FOUND,
otherwise ACTION-IF-NOT-FOUND.  If you just want to define a symbol if the
  function is available, consider using `AC_CHECK_FUNCS' instead.  This macro
  checks for functions with C linkage even when `AC_LANG(C++)' has been
  called, since C is more standardized than C++.  

     This macro `caches its result in the ac_cv_func_FUNCTION variable`


5.10 Compilers and Preprocessors

All the tests for compilers (AC_PROG_CC, AC_PROG_CXX, AC_PROG_F77) define the
output variable EXEEXT based on the output of the compiler, typically to the
empty string if Posix and ‘.exe’ if a DOS variant.

They also define the output variable `OBJEXT` based on the output of the
compiler, after ‘.c’ files have been excluded, typically to ‘o’ if Posix,
‘obj’ if a DOS variant.

If the compiler being used does not produce executables, the tests fail. If
the executables can’t be run, and cross-compilation is not enabled, they fail
too. See Chapter 14 [Manual Configuration], page 281, for more on support for
cross compiling.


5.10.2 Generic Compiler Characteristics

 -- Macro: AC_CHECK_SIZEOF (TYPE-OR-EXPR, [UNUSED], [INCLUDES =
          `AC_INCLUDES_DEFAULT'])

     Define `SIZEOF_TYPE-OR-EXPR' (*note Standard Symbols::) to be the
     size in bytes of TYPE-OR-EXPR, which may be either a type or an
     expression returning a value that has a size.  If the expression
     `sizeof (TYPE-OR-EXPR)' is invalid, the result is 0.  INCLUDES is
     a series of include directives, defaulting to
     `AC_INCLUDES_DEFAULT' (*note Default Includes::), which are used
     prior to the expression under test.

     This macro now works even when cross-compiling.  The UNUSED
     argument was used when cross-compiling.

     For example, the call

          AC_CHECK_SIZEOF([int *])

     defines `SIZEOF_INT_P' to be 8 on DEC Alpha AXP systems.

     This macro caches its result in the `ac_cv_sizeof_TYPE-OR-EXPR`
     variable, with `*' mapped to `p' and other characters not suitable
     for a variable name mapped to underscores.


={============================================================================
*kt_linux_tool_150* make-autoconf-test-result

7 Results of Tests

Once configure has determined whether a feature exists, what can it do to
record that information? There are four sorts of things it can do: define a C
preprocessor symbol, set a variable in the output files, save the result in a
cache file for future configure runs, and print a message letting the user
know the result of the test.

7.1 Defining C Preprocessor Symbols

A common action to take in response to a feature test is to define a C
preprocessor symbol indicating the results of the test. That is done by
calling AC_DEFINE or AC_DEFINE_UNQUOTED.

By default, `AC_OUTPUT' places the symbols defined by these macros into the
output variable DEFS, which contains an option -DSYMBOL=VALUE for each
symbol defined.  Unlike in Autoconf version 1, there is no variable `DEFS'
defined while `configure' is running.

// not use DEFS so

To check whether Autoconf macros have already defined a certain C preprocessor
symbol, test the value of the appropriate `cache variable`, as in this example:

     AC_CHECK_FUNC([vprintf], [AC_DEFINE([HAVE_VPRINTF], [1],
                               [Define if vprintf exists.])])
     if test "x$ac_cv_func_vprintf" != xyes; then
       AC_CHECK_FUNC([_doprnt], [AC_DEFINE([HAVE_DOPRNT], [1],
                                 [Define if _doprnt exists.])])
     fi

If AC_CONFIG_HEADERS has been called, then instead of creating DEFS, AC_OUTPUT
`creates a header file` by substituting the correct values into #define
statements in a template file.  *Note Configuration Headers::, for more
information about this kind of output.

 -- Macro: AC_DEFINE (VARIABLE, VALUE, [DESCRIPTION])
 -- Macro: AC_DEFINE (VARIABLE)
     Define VARIABLE to VALUE (verbatim), by defining a C preprocessor
     macro for VARIABLE.  VARIABLE should be a `C identifier`, optionally
     suffixed by a parenthesized argument list to define a C
     preprocessor macro with arguments.  The macro argument list, if
     present, should be a comma-separated list of C identifiers,
     possibly terminated by an ellipsis `...' if C99 syntax is employed.
     VARIABLE should not contain comments, white space, trigraphs,
     backslash-newlines, universal character names, or non-ASCII
     characters.

     VALUE may contain backslash-escaped newlines, which will be
     preserved if you use 'AC_CONFIG_HEADERS' but flattened if passed
     via `@DEFS@' (with no effect on the compilation, since the
     preprocessor sees only one line in the first place).  VALUE should
     not contain raw newlines.  If you are not using
     'AC_CONFIG_HEADERS', VALUE should not contain any '#' characters,
     as `make' tends to eat them.  To use a shell variable, use
     'AC_DEFINE_UNQUOTED' instead.

     DESCRIPTION is only useful if you are using 'AC_CONFIG_HEADERS'.
     In this case, DESCRIPTION is put into the generated 'config.h.in'
     as the comment before the macro define.  The following example
     `defines the C preprocessor variable` 'EQUATION' to be the string
     constant `"$a > $b"':

          AC_DEFINE([EQUATION], ["$a > $b"],
            [Equation string.])

     If neither VALUE nor DESCRIPTION are given, then VALUE defaults to
     1 instead of to the empty string.  This is for backwards
     compatibility with older versions of Autoconf, but this usage is
     obsolescent and may be withdrawn in future versions of Autoconf.

     If the VARIABLE is a literal string, it is passed to
     `m4_pattern_allow' (*note Forbidden Patterns::).

     If multiple `AC_DEFINE' statements are executed for the same
     VARIABLE name (not counting any parenthesized argument list), the
     last one wins.


7.2 Setting Output Variables

Another way to record the results of tests is to set `output variables`, which
are `shell variables` whose values are substituted into files that `configure'
outputs.  The two macros below create new output variables. *Note Preset
Output Variables::, for a list of output variables that are always available.

 -- Macro: AC_SUBST (VARIABLE, [VALUE])
     Create an output variable from a shell variable.  Make `AC_OUTPUT'
     substitute the variable VARIABLE into output files (typically one
     or more makefiles). 
     
     `This means` that AC_OUTPUT replaces instances of `@VARIABLE@` in input
     files with the value that the shell variable VARIABLE has when
     `AC_OUTPUT' is called.  
     
     The value can contain any non-`NUL' character, including newline.  If you
     are using Automake 1.11 or newer, for newlines in values you might want
     to consider using `AM_SUBST_NOTMAKE' to prevent `automake' from adding a
     line VARIABLE = @VARIABLE@' to the Makefile.in files (*note Automake:
         (automake)Optional.).

     Variable occurrences should not overlap: e.g., an input file should
     not contain `@VAR1@VAR2@' if VAR1 and VAR2 are variable names.
     The substituted value is not rescanned for more output variables;
     occurrences of `@VARIABLE@' in the value are inserted literally
     into the output file.  (The algorithm uses the special marker
     `|#_!!_#|' internally, so neither the substituted value nor the
     output file may contain `|#_!!_#|'.)

     If VALUE is given, in addition assign it to VARIABLE.

     The string VARIABLE is passed to `m4_pattern_allow' (*note
     Forbidden Patterns::).


7.4 Caching Results
===================

To avoid checking for the same features repeatedly in various `configure'
scripts (or in repeated runs of one script), `configure' can `optionally` save
the results of many checks in a "cache file" (*note Cache Files::).  If a
`configure' script runs with caching enabled and finds a cache file, it reads
the results of previous runs from the cache and avoids rerunning those checks.
As a result, `configure' can then run much faster than if it had to perform
all of the checks every time.

 -- Macro: AC_CACHE_VAL (CACHE-ID, COMMANDS-TO-SET-IT)
     Ensure that the results of the check identified by CACHE-ID are
     available.  If the results of the check `were in the cache file` that was
     read, and configure was not given the --quiet or --silent option, print a
     message saying that the result was cached; otherwise, run the shell
     commands COMMANDS-TO-SET-IT.  If the shell commands are run to determine
     the value, the value is saved in the cache file just before `configure'
     creates its output files.  *Note Cache Variable Names::, for how to
     choose the name of the CACHE-ID variable.

     The COMMANDS-TO-SET-IT _must have no side effects_ except for setting the
     variable CACHE-ID, see below.

 -- Macro: AC_CACHE_CHECK (MESSAGE, CACHE-ID, COMMANDS-TO-SET-IT)
     A wrapper for `AC_CACHE_VAL' that takes care of printing the
     messages.  This macro provides a convenient shorthand for the most
     common way to use these macros.  It calls `AC_MSG_CHECKING' for
     MESSAGE, then `AC_CACHE_VAL' with the CACHE-ID and COMMANDS
     arguments, and `AC_MSG_RESULT' with CACHE-ID.

     The COMMANDS-TO-SET-IT _must have no side effects_ except for
     setting the variable CACHE-ID, see below.

   It is common to find buggy macros using `AC_CACHE_VAL' or AC_CACHE_CHECK,
      because people are tempted to call `AC_DEFINE' in the
        COMMANDS-TO-SET-IT.  Instead, the code that _follows_ the call to
        AC_CACHE_VAL' should call `AC_DEFINE', by examining the value of the
        cache variable.  For instance, the following macro is broken:

     AC_DEFUN([AC_SHELL_TRUE],
     [AC_CACHE_CHECK([whether true(1) works], [my_cv_shell_true_works],
                     [my_cv_shell_true_works=no
                      (true) 2>/dev/null && my_cv_shell_true_works=yes
                      if test "x$my_cv_shell_true_works" = xyes; then
                        AC_DEFINE([TRUE_WORKS], [1],
                                  [Define if `true(1)' works properly.])
                      fi])
     ])

   This fails if the cache is enabled: the second time this macro is run,
   `TRUE_WORKS' _will not be defined_.  The proper implementation is:

     AC_DEFUN([AC_SHELL_TRUE],
     [AC_CACHE_CHECK([whether true(1) works], [my_cv_shell_true_works],
                     [my_cv_shell_true_works=no
                      (true) 2>/dev/null && my_cv_shell_true_works=yes])
      if test "x$my_cv_shell_true_works" = xyes; then
        AC_DEFINE([TRUE_WORKS], [1],
                  [Define if `true(1)' works properly.])
      fi
     ])

   Also, COMMANDS-TO-SET-IT should not print any messages, for example with
   `AC_MSG_CHECKING'; do that before calling `AC_CACHE_VAL', so the messages
   are printed regardless of whether the results of the check are retrieved
   from the cache or determined by running the shell commands.


7.4.1 Cache Variable Names
--------------------------

The names of cache variables should have the following format:

     PACKAGE-PREFIX_cv_VALUE-TYPE_SPECIFIC-VALUE_[ADDITIONAL-OPTIONS]

for example, `ac_cv_header_stat_broken' or
`ac_cv_prog_gcc_traditional'.  The parts of the variable name are:

PACKAGE-PREFIX
     An abbreviation for your package or organization; the same prefix
     you begin local Autoconf macros with, except lowercase by
     convention.  For cache values used by the distributed Autoconf
     macros, this value is `ac'.

`_cv_'
     Indicates that this shell variable is a `cache value`  This string
     _must_ be present in the variable name, including the leading
     underscore.

VALUE-TYPE
     A convention for classifying cache values, to produce a rational
     naming system.  The values used in Autoconf are listed in *note
     Macro Names::.

SPECIFIC-VALUE
     Which member of the class of cache values this test applies to.
     For example, which function (`alloca'), program (`gcc'), or output
     variable (`INSTALL').

ADDITIONAL-OPTIONS
     Any particular behavior of the specific member that this test
     applies to.  For example, `broken' or `set'.  This part of the
     name may be omitted if it does not apply.

   The values assigned to cache variables may not contain newlines.
Usually, their values are Boolean (`yes' or `no') or the names of files
or functions; so this is not an important restriction.  *note Cache
Variable Index:: for an index of cache variables with documented
semantics.


7.4.2 Cache Files
-----------------

A cache file is a shell script that caches the results of configure
tests run on one system so they can be shared between configure scripts
and configure runs.  It is not useful on other systems.  If its contents
are invalid for some reason, the user may delete or edit it, or override
documented cache variables on the `configure' command line.

   By default, `configure' uses no cache file, to avoid problems caused
by accidental use of stale cache files.

   To enable caching, `configure' accepts `--config-cache' (or `-C') to
cache results in the file `config.cache'.  Alternatively,
`--cache-file=FILE' specifies that FILE be the cache file.  The cache
file is created if it does not exist already.  When `configure' calls
`configure' scripts in subdirectories, it uses the `--cache-file'
argument so that they share the same cache.  *Note Subdirectories::,
for information on configuring subdirectories with the
`AC_CONFIG_SUBDIRS' macro.

   `config.status' only pays attention to the cache file if it is given
the `--recheck' option, which makes it rerun `configure'.

   It is wrong to try to distribute cache files for particular system
types.  There is too much room for error in doing that, and too much
administrative overhead in maintaining them.  For any features that
can't be guessed automatically, use the standard method of the canonical
system type and linking files (*note Manual Configuration::).

   The site initialization script can specify a site-wide cache file to
use, instead of the usual per-program cache.  In this case, the cache
file gradually accumulates information whenever someone runs a new
`configure' script.  (Running `configure' merges the new cache results
with the existing cache file.)  This may cause problems, however, if
the system configuration (e.g., the installed libraries or compilers)
changes and the stale cache file is not deleted.

   If `configure' is interrupted at the right time when it updates a
cache file outside of the build directory where the `configure' script
is run, it may leave behind a temporary file named after the cache file
with digits following it.  You may safely delete such a file.


={============================================================================
*kt_linux_tool_150* make-autoconf-package

15 Site Configuration
*********************

`configure' scripts support several kinds of local configuration
decisions.  There are ways for users to specify where external software
packages are, include or exclude optional features, install programs
under modified names, and set default values for `configure' options.


15.2 Working With External Software
===================================

Some packages require, or can optionally use, other software packages
that are already installed.  The user can give `configure' command line
options to specify which such external software to use.  The options
have one of these forms:

     --with-PACKAGE[=ARG]
     --without-PACKAGE

   For example, `--with-gnu-ld' means work with the GNU linker instead
of some other linker.  `--with-x' means work with The X Window System.

   The user can give an argument by following the package name with `='
and the argument.  Giving an argument of `no' is for packages that are
used by default; it says to _not_ use the package.  An argument that is
neither `yes' nor `no' could include a name or number of a version of
the other package, to specify more precisely which other package this
program `is supposed to work with`  If no argument is given, it defaults
to `yes`.  `--without-PACKAGE' is equivalent to `--with-PACKAGE=no'.

   Normally configure scripts complain about `--with-PACKAGE' options
that they do not support.  *Note Option Checking::, for details, and
for how to override the defaults.

   For each external software package that may be used, `configure.ac'
should call `AC_ARG_WITH' `to detect` whether the `configure' user asked
to use it.  Whether each package is used or not by default, and which
arguments are valid, is up to you.

 -- Macro: AC_ARG_WITH (PACKAGE, HELP-STRING, [ACTION-IF-GIVEN],
          [ACTION-IF-NOT-GIVEN])
     If the user gave configure the option `--with-PACKAGE` or
     `--without-PACKAGE', `run shell commands ACTION-IF-GIVEN`  
     
     If neither option was given, run shell commands ACTION-IF-NOT-GIVEN.

     note:
     This seeems to be different from the above:
     "If no argument is given, it defaults to `yes`"

     The name PACKAGE indicates another software package that this
     program should work with.  It should consist only of alphanumeric
     characters, dashes, plus signs, and dots.

     The option's argument is available to the shell commands
     ACTION-IF-GIVEN in the shell variable `withval', which is actually
     just the value of the shell variable named `with_PACKAGE', with
     any non-alphanumeric characters in PACKAGE changed into `_'.  You
     may use that variable instead, if you wish.

     The argument HELP-STRING is a description of the option that looks
     like this:
            --with-readline         support fancy command line editing

     HELP-STRING may be more than one line long, if more detail is
     needed.  Just make sure the columns line up in `configure --help'.
     Avoid tabs in the help string.  The easiest way to provide the
     proper leading whitespace is to format your HELP-STRING with the
     macro `AS_HELP_STRING' (*note Pretty Help Strings::).

     The following example shows how to use the `AC_ARG_WITH' macro in
     a common situation.  You want to let the user decide whether to
     enable support for an external library (e.g., the readline
     library); if the user specified neither `--with-readline' nor
     `--without-readline', you want to enable support for readline 
     `only if the library is available on the system.`

          AC_ARG_WITH([readline],
            [AS_HELP_STRING([--with-readline],
              [support fancy command line editing @<:@default=check@:>@])],
            [],
            [with_readline=check])

          LIBREADLINE=
          AS_IF([test "x$with_readline" != xno],
            [AC_CHECK_LIB([readline], [main],
              [AC_SUBST([LIBREADLINE], ["-lreadline -lncurses"])
               AC_DEFINE([HAVE_LIBREADLINE], [1],
                         [Define if you have libreadline])
              ],
              [if test "x$with_readline" != xcheck; then
                 AC_MSG_FAILURE(
                   [--with-readline was given, but test for readline failed])
               fi
              ], -lncurses)])

     The next example shows how to use `AC_ARG_WITH' to give the user
     the possibility to enable support for the readline library, in
     case it is still experimental and not well tested, and is
     therefore disabled by default.

          AC_ARG_WITH([readline],
            [AS_HELP_STRING([--with-readline],
              [enable experimental support for readline])],
            [],
            [with_readline=no])

          LIBREADLINE=
          AS_IF([test "x$with_readline" != xno],
            [AC_CHECK_LIB([readline], [main],
              [AC_SUBST([LIBREADLINE], ["-lreadline -lncurses"])
               AC_DEFINE([HAVE_LIBREADLINE], [1],
                         [Define if you have libreadline])
              ],
              [AC_MSG_FAILURE(
                 [--with-readline was given, but test for readline failed])],
              [-lncurses])])

     The last example shows how to use `AC_ARG_WITH' to give the user
     the possibility to disable support for the readline library, given
     that it is an important feature and that it should be enabled by
     default.

          AC_ARG_WITH([readline],
            [AS_HELP_STRING([--without-readline],
              [disable support for readline])],
            [],
            [with_readline=yes])

          LIBREADLINE=
          AS_IF([test "x$with_readline" != xno],
            [AC_CHECK_LIB([readline], [main],
              [AC_SUBST([LIBREADLINE], ["-lreadline -lncurses"])
               AC_DEFINE([HAVE_LIBREADLINE], [1],
                         [Define if you have libreadline])
              ],
              [AC_MSG_FAILURE(
                 [readline test failed (--without-readline to disable)])],
              [-lncurses])])

     These three examples can be easily adapted to the case where
     AC_ARG_ENABLE should be preferred to `AC_ARG_WITH' (see *note
     Package Options::).


={============================================================================
*kt_linux_tool_150* make-autoconf-run-configure

<ex>
Appears that "--xxx" populates to "$xxx" in configure.ac since when runs
with "--enable-libsanitizer", $enable-libsanitizer and  the following checks
is not in effect. This is the same as ${prefix}

// configure.ac
# Disable libsanitizer on unsupported systems.
if test -d ${srcdir}/libsanitizer; then
  if test x$enable_libsanitizer = x; then
     AC_MSG_CHECKING([for libsanitizer support])
     if (srcdir=${srcdir}/libsanitizer; \
        . ${srcdir}/configure.tgt; \
        test -n "$UNSUPPORTED")
     then
         AC_MSG_RESULT([no])
         noconfigdirs="$noconfigdirs target-libsanitizer"
     else
         AC_MSG_RESULT([yes])
     fi
  fi
fi


16.4 Installation Names
=======================

`By default`, make install' installs the package's commands under
`/usr/local/bin`, include files under `/usr/local/include', etc.  You can
specify an installation prefix other than `/usr/local' by giving configure'
the option `--prefix=PREFIX', where PREFIX must be an absolute file name.


16.5 Optional Features
======================

If the package supports it, you can cause programs to be installed with
an extra prefix or suffix on their names by giving `configure' the
option --program-prefix=PREFIX or --program-suffix=SUFFIX.

   Some packages pay attention to `--enable-FEATURE' options to configure,
where FEATURE indicates `an optional part of the package`  They may also pay
  attention to `--with-PACKAGE' options, where PACKAGE is something like
  gnu-as or x (for the X Window System).  The `README' should mention any
  `--enable-' and `--with-' options that the package recognizes.

   For packages that use the X Window System, `configure' can usually
find the X include and library files automatically, but if it doesn't,
you can use the `configure' options `--x-includes=DIR' and
`--x-libraries=DIR' to specify their locations.

   Some packages offer the ability to configure how verbose the
execution of `make' will be.  For these packages, running `./configure
--enable-silent-rules' sets the default to minimal output, which can be
overridden with `make V=1'; while running `./configure
--disable-silent-rules' sets the default to verbose, which can be
overridden with `make V=0'.


16.9 Defining Variables
=======================

Variables not defined in a site shell script can be set in the environment
passed to `configure'.  However, some packages may run configure again during
the build, and the customized values of these variables may be lost.  In order
to avoid this problem, you should set them in the configure command line,
using VAR=value.  For example:

     ./configure CC=/usr/local2/bin/gcc

causes the specified gcc to be used as the C compiler (unless it is overridden
    in the site shell script).


16.10 `configure' Invocation
============================

configure recognizes the following options to control how it operates.

`--help'
`-h'
     Print a summary of all of the options to `configure', and exit.

`--help=short'
`--help=recursive'
     Print a summary of the options `unique to this package's`
     `configure', and exit.  The `short' variant lists options used
     only in the top level, while the `recursive' variant lists options
     also present in any nested packages.


`--version'
`-V'
     Print the version of Autoconf `used to generate` the `configure'
     script, and exit.

`--cache-file=FILE'
     Enable the cache: use and save the results of the tests in FILE,
     traditionally `config.cache'.  FILE defaults to `/dev/null' to
     disable caching.

`--config-cache'
`-C'
     Alias for `--cache-file=config.cache'.

`configure' also accepts some other, not widely useful, options.  Run
`configure --help' for more details.

note:
configure script has `here` document for a help.

#
# Report the --help message.
#
if test "$ac_init_help" = "long"; then
  # Omit some internal or obsolete options to make the list less imposing.
  # This message is too long to be a string in the A/UX 3.1 sh.
  cat <<_ACEOF
\`configure' configures this package to adapt to many kinds of systems.

Usage: $0 [OPTION]... [VAR=VALUE]...

To assign environment variables (e.g., CC, CFLAGS...), specify them as
VAR=VALUE.  See below for descriptions of some of the useful variables.

(skipped)

Optional Features:
  --disable-option-checking  ignore unrecognized --enable/--with options
  --disable-FEATURE       do not include FEATURE (same as --enable-FEATURE=no)
  --enable-FEATURE[=ARG]  include FEATURE [ARG=yes]


={============================================================================
*kt_linux_make_150* make-automake

https://www.gnu.org/software/automake/

https://www.gnu.org/software/automake/manual/automake.html
https://www.gnu.org/software/automake/manual/automake.txt


1 Introduction

Automake is a tool for automatically generating `Makefile.ins` 'from'
`Makefile.am`. Each Makefile.am is basically a series of make variable
definitions, with rules being thrown in occasionally. The generated
Makefile.ins are compliant with the GNU Makefile standards

Automake does constrain a project in 'certain' ways; for instance, it
'assumes' that the project uses `autoconf`, and enforces certain restrictions
on the configure.ac contents.

2.2.1 Basic Installation

make check 

causes the package's tests to be run. This step is not mandatory, but it is
often good to make sure the programs that have been built behave as they should,
before you decide to install them. Our example does not contain any tests, so
  running make check is a no-op.

make install

After everything has been built, and maybe tested, it is time to install it on
the system. That means copying the programs, libraries, header files, scripts,
and other data files from the source directory to their final destination on the
  system. The command make install will do that. 

make installcheck

A last and optional step is to run make installcheck. This command may run tests
on the installed files. make check tests the files in the source tree, while
make installcheck tests their installed copies. The tests run by the latter can
be different from those run by the former. For instance, there are tests that
cannot be run in the source tree. Conversely, some packages are set up so that
make installcheck will run the very same tests as make check, only on different
files (non-installed vs. installed). It can make a difference, for instance when
the source tree's layout is different from that of the installation. Furthermore
it may help to diagnose an incomplete installation.

Presently most packages do not have any installcheck tests because the existence
of installcheck is little known, and its usefulness is neglected. Our little toy
package is no better: make installcheck does nothing. 


={============================================================================
*kt_linux_make_150* make-automake-standard targets

2.2.2 Standard Makefile Targets

So far we have come across four ways to run make in the GNU Build System: make,
make check, make install, and make installcheck. The words check, install, and
  installcheck, passed as arguments to make, are called `targets`. 
  
*make-all*
make is a shorthand for make all, all being the default target in the GNU
Build System.


Here is a list of the most useful targets that the GNU Coding Standards specify.

make all
    Build programs, libraries, documentation, etc. (same as make). 

make install
    Install what needs to be installed, copying the files from the package’s
    tree to system-wide directories.  make install-strip

    Same as make install, then strip debugging symbols. Some users like to trade
    space for useful bug reports... 

make uninstall
    The opposite of make install: erase the installed files. (This needs to be
        run from the same build tree that was installed.) 

make clean
    Erase from the build tree the files built by make all. 

make `distclean`
    Additionally erase anything ./configure created. 

make check
    Run the test suite, if any. 

make installcheck
    Check the installed programs or libraries, if supported. 

make dist
    Recreate package-version.tar.gz from all the source files. 


From GNU Coding Standards

http://www.gnu.org/prep/standards/html_node/Standard-Targets.html#Standard-Targets

7.2.6 Standard Targets for Users

All GNU programs should have the following targets in their Makefiles:

‘all’

    Compile the entire program. This should be the default target. This target
    need not rebuild any documentation files; Info files should normally be
    included in the distribution, and DVI (and other documentation format) files
    should be made only when explicitly asked for.

    By default, the Make rules should compile and link with ‘-g’, so that
    executable programs have debugging symbols. Otherwise, you are essentially
    helpless in the face of a crash, and it is often far from easy to reproduce
    with a fresh build.  
    
‘install’

    Compile the program and copy the executables, libraries, and so on to the
    file names where they should reside for actual use. If there is a simple
    test to verify that a program is properly installed, this target should run
    that test.

    Do not strip executables when installing them. This helps eventual debugging
    that may be needed later, and nowadays disk space is cheap and dynamic
    loaders typically ensure debug sections are not loaded during normal
    execution. Users that need stripped binaries may invoke the install-strip
    target to do that.

    If possible, write the install target rule so that it does not modify
    anything in the directory where the program was built, provided ‘make all’
    has just been done. This is convenient for building the program under one
    user name and installing it under another.

    The commands should create all the directories in which files are to be
    installed, if they don’t already exist. This includes the directories
    specified as the values of the variables prefix and exec_prefix, as well as
    all subdirectories that are needed. One way to do this is by means of an
    installdirs target as described below.

    Use ‘-’ before any command for installing a man page, so that make will
    ignore any errors. This is in case there are systems that don’t have the
    Unix man page documentation system installed.

    The way to install Info files is to copy them into $(infodir) with
    $(INSTALL_DATA) (see Command Variables), and then run the install-info
    program if it is present. install-info is a program that edits the Info dir
    file to add or update the menu entry for the given Info file; it is part of
    the Texinfo package.

    Here is a sample rule to install an Info file that also tries to handle some
    additional situations, such as install-info not being present.

    do-install-info: foo.info installdirs
            $(NORMAL_INSTALL)
    # Prefer an info file in . to one in srcdir.
            if test -f foo.info; then d=.; \
             else d="$(srcdir)"; fi; \
            $(INSTALL_DATA) $$d/foo.info \
              "$(DESTDIR)$(infodir)/foo.info"
    # Run install-info only if it exists.
    # Use 'if' instead of just prepending '-' to the
    # line so we notice real errors from install-info.
    # Use '$(SHELL) -c' because some shells do not
    # fail gracefully when there is an unknown command.
            $(POST_INSTALL)
            if $(SHELL) -c 'install-info --version' \
               >/dev/null 2>&1; then \
              install-info --dir-file="$(DESTDIR)$(infodir)/dir" \
                           "$(DESTDIR)$(infodir)/foo.info"; \
            else true; fi

    When writing the install target, you must classify all the commands into
      three categories: normal ones, pre-installation commands and
      post-installation commands. See Install Command Categories.

‘install-html’
‘install-dvi’
‘install-pdf’
‘install-ps’

    These targets install documentation in formats other than Info; they’re
    intended to be called explicitly by the person installing the package, if
    that format is desired. GNU prefers Info files, so these must be installed
    by the install target.

    When you have many documentation files to install, we recommend that you
    avoid collisions and clutter by arranging for these targets to install in
    subdirectories of the appropriate installation directory, such as htmldir.
    As one example, if your package has multiple manuals, and you wish to
    install HTML documentation with many files (such as the “split” mode output
        by makeinfo --html), you’ll certainly want to use subdirectories, or two
    nodes with the same name in different manuals will overwrite each other.

    Please make these install-format targets invoke the commands for the format
    target, for example, by making format a dependency.

‘uninstall’

    Delete all the installed files—the copies that the ‘install’ and ‘install-*’
    targets create.

    This rule should not modify the directories where compilation is done, only
    the directories where files are installed.

    The uninstallation commands are divided into three categories, just like the
    installation commands. See Install Command Categories.  
    
‘install-strip’

    Like install, but strip the executable files while installing them. In
    simple cases, this target can use the install target in a simple way:

    install-strip:
            $(MAKE) INSTALL_PROGRAM='$(INSTALL_PROGRAM) -s' \
                    install

    But if the package installs scripts as well as real executables, the
    install-strip target can’t just refer to the install target; it has to strip
    the executables but not the scripts.

    install-strip should not strip the executables in the build directory which
    are being copied for installation. It should only strip the copies that are
    installed.

    Normally we do not recommend stripping an executable unless you are sure the
    program has no bugs. However, it can be reasonable to install a stripped
    executable for actual execution while saving the unstripped executable
    elsewhere in case there is a bug.

‘clean’

    Delete all files in the current directory that are normally created by
    building the program. Also delete files in other directories if they are
    created by this makefile. However, don’t delete the files that record the
    configuration. Also preserve files that could be made by building, but
    normally aren’t because the distribution comes with them. There is no need
    to delete parent directories that were created with ‘mkdir -p’, since they
    could have existed anyway.

    Delete .dvi files here if they are not part of the distribution.

‘distclean’

    Delete all files in the current directory (or created by this makefile) that
    are created by configuring or building the program. If you have unpacked the
    source and built the program without creating any other files, ‘make
    distclean’ should leave only the files that were in the distribution.
    However, there is no need to delete parent directories that were created
    with ‘mkdir -p’, since they could have existed anyway.

‘mostlyclean’

    Like ‘clean’, but may refrain from deleting a few files that people normally
    don’t want to recompile. For example, the ‘mostlyclean’ target for GCC does
    not delete libgcc.a, because recompiling it is rarely necessary and takes a
    lot of time.  

‘maintainer-clean’

    Delete almost everything that can be reconstructed with this Makefile. This
    typically includes everything deleted by distclean, plus more: C source
    files produced by Bison, tags tables, Info files, and so on.

    The reason we say “almost everything” is that running the command ‘make
    maintainer-clean’ should not delete configure even if configure can be
    remade using a rule in the Makefile. More generally, ‘make maintainer-clean’
    should not delete anything that needs to exist in order to run configure and
    then begin to build the program. Also, there is no need to delete parent
    directories that were created with ‘mkdir -p’, since they could have existed
    anyway. These are the only exceptions; maintainer-clean should delete
    everything else that can be rebuilt.

    The ‘maintainer-clean’ target is intended to be used by a maintainer of the
    package, not by ordinary users. You may need special tools to reconstruct
    some of the files that ‘make maintainer-clean’ deletes. Since these files
    are normally included in the distribution, we don’t take care to make them
    easy to reconstruct. If you find you need to unpack the full distribution
    again, don’t blame us.

    To help make users aware of this, the commands for the special
    maintainer-clean target should start with these two:

    @echo 'This command is intended for maintainers to use; it'
    @echo 'deletes files that may need special tools to rebuild.'

‘TAGS’

    Update a tags table for this program.

‘info’

    Generate any Info files needed. The best way to write the rules is as
    follows:

    info: foo.info

    foo.info: foo.texi chap1.texi chap2.texi
            $(MAKEINFO) $(srcdir)/foo.texi

    You must define the variable MAKEINFO in the Makefile. It should run the
    makeinfo program, which is part of the Texinfo distribution.

    Normally a GNU distribution comes with Info files, and that means the Info
    files are present in the source directory. Therefore, the Make rule for an
    info file should update it in the source directory. When users build the
    package, ordinarily Make will not update the Info files because they will
    already be up to date.

‘dvi’
‘html’
‘pdf’
‘ps’

    Generate documentation files in the given format. These targets should
    always exist, but any or all can be a no-op if the given output format
    cannot be generated. These targets should not be dependencies of the all
    target; the user must manually invoke them.

    Here’s an example rule for generating DVI files from Texinfo:

    dvi: foo.dvi

    foo.dvi: foo.texi chap1.texi chap2.texi
            $(TEXI2DVI) $(srcdir)/foo.texi

    You must define the variable TEXI2DVI in the Makefile. It should run the
    program texi2dvi, which is part of the Texinfo distribution. (texi2dvi uses
        TeX to do the real work of formatting. TeX is not distributed with
        Texinfo.) Alternatively, write only the dependencies, and allow GNU make
    to provide the command.

    Here’s another example, this one for generating HTML from Texinfo:

    html: foo.html

    foo.html: foo.texi chap1.texi chap2.texi
            $(TEXI2HTML) $(srcdir)/foo.texi

    Again, you would define the variable TEXI2HTML in the Makefile; for example,
  it might run makeinfo --no-split --html (makeinfo is part of the Texinfo
      distribution).

‘dist’

    Create a distribution tar file for this program. The tar file should be set
    up so that the file names in the tar file start with a subdirectory name
    which is the name of the package it is a distribution for. This name can
    include the version number.

    For example, the distribution tar file of GCC version 1.40 unpacks into a
    subdirectory named gcc-1.40.

    The easiest way to do this is to create a subdirectory appropriately named,
    use ln or cp to install the proper files in it, and then tar that
      subdirectory.

    Compress the tar file with gzip. For example, the actual distribution file
    for GCC version 1.40 is called gcc-1.40.tar.gz. It is ok to support other
      free compression formats as well.

    The dist target should explicitly depend on all non-source files that are in
    the distribution, to make sure they are up to date in the distribution. See
    Making Releases.

‘check’

    Perform self-tests (if any). The user must build the program before running
    the tests, but need not install the program; you should write the self-tests
    so that they work when the program is built but not installed. 

The following targets are suggested as conventional names, for programs in which
they are useful.

installcheck

    Perform installation tests (if any). The user must build and install the
    program before running the tests. You should not assume that $(bindir) is in
    the search path.  
    
installdirs

    It’s useful to add a target named ‘installdirs’ to create the directories
    where files are installed, and their parent directories. There is a script
    called mkinstalldirs which is convenient for this; you can find it in the
    Gnulib package. You can use a rule like this:

    # Make sure all installation directories (e.g. $(bindir))
    # actually exist by making them if necessary.
    installdirs: mkinstalldirs
            $(srcdir)/mkinstalldirs $(bindir) $(datadir) \
                                    $(libdir) $(infodir) \
                                    $(mandir)

    or, if you wish to support DESTDIR (strongly encouraged),

    # Make sure all installation directories (e.g. $(bindir))
    # actually exist by making them if necessary.
    installdirs: mkinstalldirs
            $(srcdir)/mkinstalldirs \
                $(DESTDIR)$(bindir) $(DESTDIR)$(datadir) \
                $(DESTDIR)$(libdir) $(DESTDIR)$(infodir) \
                $(DESTDIR)$(mandir)

    This rule should not modify the directories where compilation is done. It
    should do nothing but create installation directories. 


={============================================================================
*kt_linux_make_150* make-automake-directory variable

2.2.3 Standard Directory Variables

The GNU Coding Standards also specify a hierarchy of variables to denote
installation directories. Some of these are:

Directory variable	Default value
prefix	/usr/local
  exec_prefix	${prefix}
    bindir	${exec_prefix}/bin
    libdir	${exec_prefix}/lib
    ...
  includedir	${prefix}/include
  datarootdir	${prefix}/share
    datadir	${datarootdir}
    mandir	${datarootdir}/man
    infodir	${datarootdir}/info
    docdir	${datarootdir}/doc/${PACKAGE}


2.2.6 Parallel Build Trees (a.k.a. VPATH Builds)

The GNU Build System distinguishes `two trees`: the 'source' tree, and the
'build' tree.

The source tree is rooted in the directory containing configure. It contains all
the sources files (those that are distributed), and may be arranged using
several subdirectories.

The build tree is rooted in the directory in which 'configure' was 'run', and is
populated with all object files, programs, libraries, and other derived files
built from the sources (and hence not distributed). The build tree usually has
the same subdirectory layout as the source tree; its subdirectories are created
automatically by the build system.

If configure is executed in its own directory, the source and build trees are
combined: derived files are constructed in the same directories as their
sources. This was the case in our first installation example (see Basic
    Installation).

A common request from users is that they want to confine all derived files to a
single directory, to keep their source directories uncluttered. Here is how we
could run configure to build everything in a subdirectory called build/.

~ % tar zxf ~/amhello-1.0.tar.gz
~ % cd amhello-1.0
~/amhello-1.0 % mkdir build && cd build

~/amhello-1.0/build % ../configure    note: see run configure in different dir
...
~/amhello-1.0/build % make
...

These setups, where source and build trees are different, are often called
parallel builds or VPATH builds. The expression parallel build is misleading:
the word parallel is a reference to the way the build tree shadows the source
tree, it is not about some concurrency in the way build commands are run. For
this reason we refer to such setups using the name VPATH builds in the
following. VPATH is the name of the make feature used by the Makefiles to allow
these builds (see VPATH Search Path for All Prerequisites in The GNU Make
    Manual).

VPATH builds have other interesting uses. One is to build the same sources with
'multiple' configurations. For instance:

~ % tar zxf ~/amhello-1.0.tar.gz
~ % cd amhello-1.0
~/amhello-1.0 % mkdir debug optim && cd debug
~/amhello-1.0/debug % ../configure CFLAGS='-g -O0'
...
~/amhello-1.0/debug % make
...
~/amhello-1.0/debug % cd ../optim
~/amhello-1.0/optim % ../configure CFLAGS='-O3 -fomit-frame-pointer'
...
~/amhello-1.0/optim % make
...


2.2.8 Cross-Compilation

To cross-compile is to build on one platform a binary that will run on another
platform. When speaking of cross-compilation, it is important to distinguish
between the build platform on which the compilation is performed, and the host
platform on which the resulting executable is expected to run. The following
configure options are used to specify each of them:

--build=build

    The system on which the package is built. 

--host=host

    The system where built programs and libraries will run. 

When the --host is used, configure will search for the cross-compiling suite for
this platform. Cross-compilation tools commonly have their target architecture
as prefix of their name. For instance my cross-compiler for MinGW32 has its
binaries called i586-mingw32msvc-gcc, i586-mingw32msvc-ld, i586-mingw32msvc-as,
         etc.

Here is how we could build amhello-1.0 for i586-mingw32msvc on a GNU/Linux PC.

~/amhello-1.0 % ./configure --build i686-pc-linux-gnu --host i586-mingw32msvc
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking for i586-mingw32msvc-strip... i586-mingw32msvc-strip
checking for i586-mingw32msvc-gcc... i586-mingw32msvc-gcc
checking for C compiler default output file name... a.exe
checking whether the C compiler works... yes
checking whether we are cross compiling... yes
checking for suffix of executables... .exe
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether i586-mingw32msvc-gcc accepts -g... yes
checking for i586-mingw32msvc-gcc option to accept ANSI C...
...
~/amhello-1.0 % make
...
~/amhello-1.0 % cd src; file hello.exe
hello.exe: MS Windows PE 32-bit Intel 80386 console executable not relocatable


The --host and --build options are usually all we need for cross-compiling. The
only exception is if the package being built is itself a cross-compiler: we need
a third option to specify its target architecture.

--target=target

    When building compiler tools: the system for which the tools will create
    output. 

For instance when installing GCC, the GNU Compiler Collection, we can use
--target=target to specify that we want to build GCC as a cross-compiler for
target. Mixing --build and --target, we can actually cross-compile a
cross-compiler; such a three-way cross-compilation is known as a Canadian cross. 


2.2.10 Building Binary Packages Using DESTDIR

The GNU Build System's make install and make uninstall interface does not
exactly fit the needs of a system administrator who has to deploy and upgrade
packages on lots of hosts. In other words, the GNU Build System does not replace
a package manager.

Such package managers usually need to know which files have been installed by a
package, so a mere make install is inappropriate.

The DESTDIR variable can be used to perform a staged installation. The package
should be configured as if it was going to be installed in its final location
(e.g., --prefix /usr), but when running make install, the DESTDIR should be set
to the absolute name of a directory into which the installation will be
diverted. From this directory it is easy to review which files are being
installed where, and finally copy them to their final location by some means. 


2.2.12 Automatic Dependency Tracking

Dependency tracking is performed as a side-effect of compilation. Each time the
build system compiles a source file, it computes its list of dependencies (in C
    these are the header files included by the source being compiled). Later,
any time make is run and a dependency appears to have changed, the dependent
  files will be rebuilt.

Automake generates code for automatic dependency tracking by default, unless the
developer chooses to override it; for more information, see Dependencies.

When configure is executed, you can see it probing each compiler for the
dependency mechanism it supports (several mechanisms can be used):

~/amhello-1.0 % ./configure --prefix /usr
...
checking dependency style of gcc... gcc3
...

Because dependencies are only computed as a side-effect of the compilation, no
dependency information exists the first time a package is built. This is OK
because all the files need to be built anyway: make does not have to decide
which files need to be rebuilt. 


={============================================================================
*kt_linux_tool_150* make-automake-distcheck

2.2.11 Preparing Distributions

We have already mentioned make dist. This target collects all your source files
and the necessary parts of the build system to create a tarball named
package-version.tar.gz.

Another, more useful command is make distcheck. The distcheck target constructs
package-version.tar.gz just as well as dist, but it additionally ensures 'most'
of the use cases presented so far work:

-. It attempts a full compilation of the package (see Basic Installation),
unpacking the newly constructed tarball, running make, make check, make install,
as well as make installcheck, and even make dist,

-. it tests VPATH builds with read-only source tree (see VPATH Builds),

-. it makes sure make clean, make distclean, and make uninstall do not omit any
  file (see Standard Targets),

-. and it checks that DESTDIR installations work (see DESTDIR). 

All of these actions are performed in a 'temporary' directory, so that no root
privileges are required. 

Please note that the exact location and the exact structure of such a
subdirectory (where the extracted sources are placed, how the temporary build
    and install directories are named and how deeply they are nested, etc.) is
to be considered an implementation detail, which can change at any time; so do
not rely on it.

Releasing a package that fails make distcheck means that one of the scenarios we
presented will not work and some users will be disappointed. 

Therefore it is a good 'practice' to release a package only after a successful
make distcheck. This of course does not imply that the package will be flawless,
but at least it will prevent some of the embarrassing errors you may find in
  packages released by people who have never heard about distcheck (like DESTDIR
      not working because of a typo, or a distributed file being erased by make
      clean, or even VPATH builds not working).

See Creating amhello, to recreate amhello-1.0.tar.gz using make distcheck. See
Checking the Distribution, for more information about distcheck. 


14.4 Checking the Distribution

Automake also 'generates' a distcheck rule that can be of help to ensure that a
given distribution will actually work. Simplifying a bit, we can say this rule
first makes a distribution, and then, operating from it, takes the following
steps:

-. tries to do a VPATH build (see VPATH Builds), with the srcdir and all its
content made read-only;

-. runs the test suite (with make check) on this fresh build;

-. installs the package in a temporary directory (with make install), and tries
  runs the test suite on the resulting installation (with make installcheck);

-. checks that the package can be correctly uninstalled (by make uninstall) and
  cleaned (by make distclean);

-. finally, makes another tarball to ensure the distribution is self-contained. 


distcheck-hook

If the distcheck-hook rule is defined in your top-level Makefile.am, then it
will be invoked by distcheck after the new distribution has been unpacked, but
before the unpacked copy is configured and built. Your distcheck-hook can do
almost anything, though as always caution is advised. Generally this hook is
used to check for potential distribution errors not caught by the standard
mechanism. Note that distcheck-hook as well as AM_DISTCHECK_CONFIGURE_FLAGS and
DISTCHECK_CONFIGURE_FLAGS are not honored in a subpackage Makefile.am, but the
flags from AM_DISTCHECK_CONFIGURE_FLAGS and DISTCHECK_CONFIGURE_FLAGS are passed
down to the configure script of the subpackage. 


={============================================================================
*kt_linux_tool_150* make-automake-example

2.4 A Small Hello World

In this section we recreate the amhello-1.0 package from scratch. The first
subsection shows how to call the Autotools to instantiate the GNU Build System,
while the second explains the meaning of the configure.ac and Makefile.am files
  read by the Autotools. 


2.4.1 Creating amhello-1.0.tar.gz

Create the following files in an empty directory.

src/main.c is the source file for the hello program. We store it in the src/
subdirectory, because later, when the package evolves, it will ease the
addition of a man/ directory for man pages, a data/ directory for data files,
etc.

<source>
~/amhello % cat src/main.c
#include <config.h>             // note:
#include <stdio.h>

int
main (void)
{
  puts ("Hello World!");
  puts ("This is " PACKAGE_STRING ".");
  return 0;
}

<readme>
README contains some very limited documentation for our little package.

~/amhello % cat README
This is a demonstration package for GNU Automake.
Type 'info Automake' to read the Automake manual.

<am-file>
Makefile.am and src/Makefile.am contain Automake 'instructions' for these
two directories.

~/amhello % cat src/Makefile.am
bin_PROGRAMS = hello
hello_SOURCES = main.c

~/amhello % cat Makefile.am
SUBDIRS = src
dist_doc_DATA = README


<ac-file>
Finally, `configure.ac` contains Autoconf instructions to create the configure
script.

~/amhello % cat configure.ac
AC_INIT([amhello], [1.0], [bug-automake@gnu.org])
AM_INIT_AUTOMAKE([-Wall -Werror foreign])
AC_PROG_CC
AC_CONFIG_HEADERS([config.h])
AC_CONFIG_FILES([
 Makefile
 src/Makefile
])
AC_OUTPUT


Once you have these five files, it is time to run the Autotools to instantiate
the build system. Do this using the autoreconf command as follows:

~/amhello % autoreconf --install
configure.ac: installing './install-sh'
configure.ac: installing './missing'
configure.ac: installing './compile'
src/Makefile.am: installing './depcomp'

At this point `the build system is complete` 

In addition to the three scripts mentioned in its output, you can see that
autoreconf 'created' four other files: 

configure, config.h.in, Makefile.in, and src/Makefile.in. 

The `latter three files` are `templates` that will be adapted to the system by
configure under the names config.h, Makefile, and src/Makefile.


~/amhello % autoreconf --install


<makefile-in>
~/amhello % cat Makefile.am
SUBDIRS = src
dist_doc_DATA = README

becomes `Makefile.in`

DIST_SUBDIRS = $(SUBDIRS)

	@list='$(DIST_SUBDIRS)'; for subdir in $$list; do \
	  if test "$$subdir" = .; then :; else \
	    $(am__make_dryrun) \
	      || test -d "$(distdir)/$$subdir" \
	      || $(MKDIR_P) "$(distdir)/$$subdir" \
	      || exit 1; \
	    dir1=$$subdir; dir2="$(distdir)/$$subdir"; \
	    $(am__relativize); \
	    new_distdir=$$reldir; \
	    dir1=$$subdir; dir2="$(top_distdir)"; \
	    $(am__relativize); \
	    new_top_distdir=$$reldir; \
	    echo " (cd $$subdir && $(MAKE) $(AM_MAKEFLAGS) top_distdir="$$new_top_distdir" distdir="$$new_distdir" \\"; \
	    echo "     am__remove_distdir=: am__skip_length_check=: am__skip_mode_fix=: distdir)"; \
	    ($(am__cd) $$subdir && \
	      $(MAKE) $(AM_MAKEFLAGS) \
	        top_distdir="$$new_top_distdir" \
	        distdir="$$new_distdir" \
		am__remove_distdir=: \
		am__skip_length_check=: \
		am__skip_mode_fix=: \
	        distdir) \
	      || exit 1; \
	  fi; \
	done


~/amhello % cat src/Makefile.am
bin_PROGRAMS = hello
hello_SOURCES = main.c

becomes `src/Makefile.in`


<run-configure>
~/amhello % ./configure
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for gawk... no
checking for mawk... mawk
checking whether make sets $(MAKE)... yes
...

configure: creating ./config.status
`config.status: creating Makefile`
`config.status: creating src/Makefile`
`config.status: creating config.h`
config.status: executing depfiles commands


$ ls -alR
.:
total 364
drwxr-xr-x 4 kpark kpark   4096 Sep 24 13:57 .
drwxr-xr-x 8 kpark kpark   4096 Sep 23 17:14 ..
-rw-r--r-- 1 kpark kpark  34939 Sep 24 13:49 aclocal.m4
drwxr-xr-x 2 kpark kpark   4096 Sep 24 13:49 autom4te.cache
-rw-r--r-- 1 kpark kpark    774 Sep 24 13:57 config.h ~
-rw-r--r-- 1 kpark kpark    625 Sep 24 13:49 config.h.in
-rw-r--r-- 1 kpark kpark   8255 Sep 24 13:57 config.log
-rwxr-xr-x 1 kpark kpark  32599 Sep 24 13:57 config.status
-rwxr-xr-x 1 kpark kpark 139386 Sep 24 13:49 configure
-rw-r--r-- 1 kpark kpark    188 Sep 24 13:47 configure.ac
-rwxr-xr-x 1 kpark kpark  20899 Sep 24 13:49 depcomp
-rwxr-xr-x 1 kpark kpark  13998 Sep 24 13:49 install-sh
-rw-r--r-- 1 kpark kpark  17972 Sep 24 13:57 Makefile ~
-rw-r--r-- 1 kpark kpark     36 Sep 23 15:39 Makefile.am
-rw-r--r-- 1 kpark kpark  17822 Sep 24 13:49 Makefile.in
-rwxr-xr-x 1 kpark kpark  10346 Sep 24 13:49 missing
-rw-r--r-- 1 kpark kpark     56 Sep 23 15:36 README
drwxr-xr-x 3 kpark kpark   4096 Sep 24 13:57 src
-rw-r--r-- 1 kpark kpark     23 Sep 24 13:57 stamp-h1

./autom4te.cache:
total 352
drwxr-xr-x 2 kpark kpark   4096 Sep 24 13:49 .
drwxr-xr-x 4 kpark kpark   4096 Sep 24 13:57 ..
-rw-r--r-- 1 kpark kpark 139847 Sep 24 13:49 output.0
-rw-r--r-- 1 kpark kpark 139847 Sep 24 13:49 output.1
-rw-r--r-- 1 kpark kpark   6687 Sep 24 13:49 requests
-rw-r--r-- 1 kpark kpark  34070 Sep 24 13:49 traces.0
-rw-r--r-- 1 kpark kpark  20296 Sep 24 13:49 traces.1

./src:
total 52
drwxr-xr-x 3 kpark kpark  4096 Sep 24 13:57 .
drwxr-xr-x 4 kpark kpark  4096 Sep 24 13:57 ..
drwxr-xr-x 2 kpark kpark  4096 Sep 24 13:57 .deps
-rw-r--r-- 1 kpark kpark   133 Sep 23 15:35 main.c
-rw-r--r-- 1 kpark kpark 14541 Sep 24 13:57 Makefile
-rw-r--r-- 1 kpark kpark    45 Sep 23 15:39 Makefile.am
-rw-r--r-- 1 kpark kpark 14732 Sep 24 13:49 Makefile.in

./src/.deps:
total 12
drwxr-xr-x 2 kpark kpark 4096 Sep 24 13:57 .
drwxr-xr-x 3 kpark kpark 4096 Sep 24 13:57 ..
-rw-r--r-- 1 kpark kpark    8 Sep 24 13:57 main.Po

You can see Makefile, src/Makefile, and config.h being created at the end after
configure has probed the system. It is now possible to run all the targets we
wish 

<run-make>
~/amhello % make
…
~/amhello % src/hello
Hello World!
This is amhello 1.0.
~/amhello % make distcheck
...
=============================================
amhello-1.0 archives ready for distribution:
amhello-1.0.tar.gz
=============================================


<files-created>
Note that running `autoreconf` is only needed initially when the GNU Build
System does not exist. When you later change some instructions in a
Makefile.am or configure.ac, the relevant part of the build system will be
`regenerated automatically when you execute make`

However, because Autoconf and Automake have separate manuals, the important
point to understand is that autoconf is in charge of 'creating' configure
'from' configure.ac, while automake is in charge of 'creating' Makefile.ins
'from' Makefile.am and configure.ac. This should at least direct you to the
right manual when seeking answers. 


={============================================================================
*kt_linux_tool_150* make-automake-example-explained

2.4.2 amhello’s configure.ac Setup Explained

Let us begin with the contents of `configure.ac`.

     AC_INIT([amhello], [1.0], [bug-automake@gnu.org])
     AM_INIT_AUTOMAKE([-Wall -Werror foreign])
     AC_PROG_CC
     AC_CONFIG_HEADERS([config.h])
     AC_CONFIG_FILES([
      Makefile
      src/Makefile
     ])
     AC_OUTPUT

This file is read `by both ‘autoconf’ (to create ‘configure’) and automake`
(to create the various ‘Makefile.in’s). It contains a series of M4 macros that
will be expanded as `shell code` to finally form the ‘configure’ script.  


<autoconf-automake-macros>
The macros prefixed with ‘AC_’ are `autoconf macros`, documented in the Autoconf
manual. The macros that start with ‘AM_’ are `automake macros`


The first two lines of ‘configure.ac’ initialize Autoconf and Automake.
‘AC_INIT’ takes in as parameters the name of the package, its version number,
and a contact address for bug-reports about the package.

The argument to AM_INIT_AUTOMAKE is a list of options for automake (see
    Chapter 17 [Options], page 117). -Wall and -Werror ask automake to turn on
all warnings and report them as errors. `We are speaking of Automake warnings`
here, such as dubious instructions in Makefile.am. This has absolutely nothing
to do with how the compiler will be called, even though it may support options
with similar names. Using -Wall -Werror is a safe setting when starting to
work on a package: you do not want to miss any issues. Later you may decide to
relax things a bit. 

The `foreign option` tells Automake that this package will not follow the GNU
Standards. GNU packages should always distribute additional files such as
ChangeLog, AUTHORS, etc. We do not want automake to complain about these
missing files in our small example.
  
The `AC_PROG_CC` line causes the ‘configure’ script to search for a C compiler
and define the variable ‘CC’ with its name. 

The ‘src/Makefile.in’ file generated by Automake uses the variable ‘CC’ to
build ‘hello’, so when ‘configure’ creates ‘src/Makefile’ from
‘src/Makefile.in’, it will define ‘CC’ with the value it has found. If
Automake is asked to create a ‘Makefile.in’ that uses ‘CC’ but ‘configure.ac’
does not define it, it will suggest you add a call to ‘AC_PROG_CC’.


The ‘AC_CONFIG_HEADERS([config.h])’ invocation causes the ‘configure’ script
to create a ‘config.h’ file gathering ‘#define’s defined by other macros in
‘configure.ac’. In our case, the ‘AC_INIT’ macro already defined a few of
them.  Here is an excerpt of ‘config.h’ after ‘configure’ has run:

     ...
     /* Define to the address where bug reports for this package should be sent. */
     #define PACKAGE_BUGREPORT "bug-automake@gnu.org"

     /* Define to the full name and version of this package. */
     #define PACKAGE_STRING "amhello 1.0"
     ...

As you probably noticed, ‘src/main.c’ includes ‘config.h’ so it can use
‘PACKAGE_STRING’. In a real-world project, ‘config.h’ can grow really big,
with one ‘#define’ per feature probed on the system.


The `AC_CONFIG_FILES` macro declares the list of files that `configure` should
create `from their *.in templates` Automake also scans this list to find the
‘Makefile.am’ files it must process. (This is important to remember: when
    adding a new directory to your project, you should add its ‘Makefile’ to
    this list, otherwise Automake will never process the new ‘Makefile.am’ you
    wrote in that directory.)

Finally, the ‘AC_OUTPUT’ line is a closing command that actually produces the
part of the script in charge of creating the files registered with
‘AC_CONFIG_HEADERS’ and ‘AC_CONFIG_FILES’.


2.4.3 ‘amhello’’s ‘Makefile.am’ Setup Explained
-----------------------------------------------

We now turn to ‘src/Makefile.am’. This file contains Automake instructions to
build and install ‘hello’.

     bin_PROGRAMS = hello
     hello_SOURCES = main.c

<create-makefilein>
A ‘Makefile.am’ has the `same syntax` as an ordinary ‘Makefile’. When automake
processes a ‘Makefile.am’ it copies the entire file into the output
‘Makefile.in’ (that will be later `turned into 'Makefile' by configure`) but
will react to certain variable definitions by generating some build rules and
other variables. Often ‘Makefile.am’s contain only a list of variable
definitions as above, but they can also contain other variable and rule
definitions that automake will pass along without interpretation.


//     bin_PROGRAMS = hello

Variables that 'end' with `_PROGRAMS` are special variables that list 'programs'
that the resulting `Makefile should build` In Automake speak, this ‘_PROGRAMS’
suffix is called a `primary`; Automake recognizes other primaries such as
‘_SCRIPTS’, ‘_DATA’, ‘_LIBRARIES’, etc. corresponding to different 'types' of
files.


The ‘bin’ part of the ‘bin_PROGRAMS’ tells ‘automake’ that the resulting
programs `should be installed` in BINDIR. Recall that the GNU Build System
uses a set of variables to denote destination directories and allow users to
customize these locations (*note Standard Directory Variables::). Any such
directory variable can be put in front of a primary (omitting the ‘dir’
    suffix) to tell ‘automake’ where to install the listed files.


//     hello_SOURCES = main.c

Programs need to be built from source files, so for each program ‘PROG’ listed
in a ‘_PROGRAMS’ variable, ‘automake’ will look for another variable named
`PROG_SOURCES` listing its source files. There may be more than one source
file: they will all be compiled and linked together.

Automake also knows that source files need to be distributed when creating a
tarball (unlike built programs).  So a side-effect of this ‘hello_SOURCES’
declaration is that ‘main.c’ will be part of the tarball created by ‘make
dist’.


Finally here are some explanations regarding the top-level ‘Makefile.am’.

     SUBDIRS = src
     dist_doc_DATA = README

`SUBDIRS` is a special variable listing all directories that ‘make’ should
`recurse into before` processing the current directory.  So this line is
responsible for ‘make’ building ‘src/hello’ even though we run it from the
top-level.  This line also causes ‘make install’ to install ‘src/hello’ before
installing ‘README’ (not that this order matters).

The line ‘dist_doc_DATA = README’ causes ‘README’ to be distributed and
installed in DOCDIR.  Files listed with the ‘_DATA’ primary are not
automatically part of the tarball built with ‘make dist’, so we add the ‘dist_’
prefix so they get distributed.  However, for ‘README’ it would not have been
necessary: ‘automake’ automatically distributes any ‘README’ file it encounters
(the list of other files automatically distributed is presented by ‘automake
 --help’).  The only important effect of this second line is therefore to
install ‘README’ during ‘make install’.

One thing not covered in this example is accessing the installation directory
values (*note Standard Directory Variables::) from your program code, that is,
converting them into defined macros.  For this, *note (autoconf)Defining
  Directories::.


={============================================================================
*kt_linux_tool_150* make-automake-variables

3.1 General Operation

<create-makefilein>
Automake works by reading a ‘Makefile.am’ and generating a ‘Makefile.in’.

Certain variables and rules defined in the ‘Makefile.am’ instruct Automake to
generate more specialized code; for instance, a ‘bin_PROGRAMS’ variable
definition will cause 'rules' for 'compiling' and 'linking' programs to be
generated.

Note that most GNU make extensions are not recognized by Automake.  Using such
extensions in a ‘Makefile.am’ will lead to errors or confusing behavior.

A special exception is that the GNU make append operator, ‘+=’, is supported.
This operator appends its right hand argument to the variable specified on the
left.  Automake will translate the operator into an ordinary ‘=’ operator; ‘+=’
will thus work with any make program.


Generally, Automake is not particularly smart in the parsing of unusual Makefile
constructs, so you’re advised to avoid fancy constructs or “creative” use of
whitespace.  For example, <TAB> characters cannot be used between a target name
and the following “‘:’” character, and variable assignments shouldn’t be
indented with <TAB> characters.  Also, using more complex macro in target names
can cause trouble:

     % cat Makefile.am
     $(FOO:=x): bar
     % automake
     Makefile.am:1: bad characters in variable name '$(FOO'
     Makefile.am:1: ':='-style assignments are not portable


A rule defined in ‘Makefile.am’ generally 'overrides' any such rule of a similar
name that would be automatically generated by ‘automake’.  Although this is a
supported feature, it is generally best to avoid making use of it, as sometimes
the generated rules are very particular.


Similarly, a variable defined in ‘Makefile.am’ or ‘AC_SUBST’ed from
‘configure.ac’ will 'override' any definition of the variable that ‘automake’
would ordinarily create.  This feature is more often 'useful' than the ability
to override a rule.  Be warned that many of the variables generated by
‘automake’ are considered to be for internal use only, and their names might
change in future releases.

<variable-expansion>
When examining a variable definition, Automake will recursively examine
variables referenced in the definition.  For example, if Automake is looking at
the content of ‘foo_SOURCES’ in this snippet

     xs = a.c b.c
     foo_SOURCES = c.c $(xs)

it would use the files ‘a.c’, ‘b.c’, and ‘c.c’ as the contents of ‘foo_SOURCES’.


<comments>
Automake also allows a form of comment that is _not_ copied into the output; all
lines beginning with ‘##’ (leading spaces allowed) are completely ignored by
Automake.


3.3 The Uniform Naming Scheme

Automake variables generally follow a "uniform naming scheme" that makes it easy
to decide how programs (and other derived objects) are built, and how they are
installed.  
  
At ‘make’ time, certain variables are used to determine which objects are to be
built.  The variable names are made of several pieces that are concatenated
together.

<primary>
The piece that tells ‘automake’ what is being built is commonly called the
"primary".  For instance, the primary ‘PROGRAMS’ holds a list of programs that
are to be compiled and linked.


<where-prefix>
A different set of names is used to decide where the built objects should be
'installed'.  These names are 'prefixes' to the primary, and they indicate which
standard directory should be used as the installation directory.  The standard
directory names are given in the GNU standards (*note (standards)Directory
    Variables::).  

Automake extends this list with ‘pkgdatadir’, ‘pkgincludedir’, ‘pkglibdir’, and
‘pkglibexecdir’; these are the same as the non-‘pkg’ versions, but with
‘$(PACKAGE)’ appended.  For instance, ‘pkglibdir’ is defined as
‘$(libdir)/$(PACKAGE)’.

For each primary, there is one additional variable named by prepending ‘EXTRA_’
to the primary name.  This variable is used to list objects that may or may not
be built, depending on what ‘configure’ decides.  This variable is required
because Automake must statically know the entire list of objects that may be
built in order to generate a ‘Makefile.in’ that will work in all cases.

For instance, ‘cpio’ decides at configure time which programs should be built.
Some of the programs are installed in ‘bindir’, and some are installed in
‘sbindir’:

     EXTRA_PROGRAMS = mt rmt
     bin_PROGRAMS = cpio pax
     sbin_PROGRAMS = $(MORE_PROGRAMS)

Defining a primary without a prefix as a variable, e.g., ‘PROGRAMS’, is an
error.

<bindir>
Note that the common ‘dir’ suffix is 'left' off when constructing the variable
names; thus one writes ‘bin_PROGRAMS’ and not ‘bindir_PROGRAMS’.

Not every sort of object can be installed in every directory.
Automake will flag those attempts it finds in error (but see below how
to override the check if you really need to).  Automake will also
diagnose obvious misspellings in directory names.

<non-standard-directory>
Sometimes the standard directories are not enough.  In particular it is
sometimes useful, for clarity, to install objects in a subdirectory of some
predefined directory.  To this end, Automake allows you to extend the list of
possible installation directories.  A given prefix (e.g., ‘zar’) is valid if a
variable of the same name with ‘dir’ appended is defined (e.g., ‘zardir’).

For instance, the following snippet will install ‘file.xml’ into
‘$(datadir)/xml’.

     xmldir = $(datadir)/xml
     xml_DATA = file.xml

This feature can also be used to override the sanity checks Automake performs to
diagnose suspicious directory/primary couples (in the unlikely case these checks
    are undesirable, and you really know what you’re doing).  For example,
               Automake would error out on this input:

     # Forbidden directory combinations, automake will error out on this.
     pkglib_PROGRAMS = foo
     doc_LIBRARIES = libquux.a

but it will succeed with this:

     # Work around forbidden directory combinations.  Do not use this
     # without a very good reason!
     my_execbindir = $(pkglibdir)
     my_doclibdir = $(docdir)
     my_execbin_PROGRAMS = foo
     my_doclib_LIBRARIES = libquux.a

The ‘exec’ substring of the ‘my_execbindir’ variable lets the files be installed
at the right time (*note The Two Parts of Install::).

<noinst-prefix>
The special prefix ‘noinst_’ indicates that the objects in question should be
built but not installed at all.  This is usually used for objects required to
build the rest of your package, for instance static libraries (*note A
    Library::), or helper scripts.

The special prefix ‘check_’ indicates that the objects in question should not be
built 'until' the ‘make check’ command is run.  Those objects are not installed
either.

<supported-primary>
The current primary names are:

‘PROGRAMS’, ‘LIBRARIES’, ‘LTLIBRARIES’, ‘LISP’, ‘PYTHON’, ‘JAVA’, ‘SCRIPTS’,
                ‘DATA’, ‘HEADERS’, ‘MANS’, and ‘TEXINFOS’.

Some primaries also allow additional prefixes that control other aspects of
‘automake’’s behavior.  The currently defined prefixes are ‘dist_’, ‘nodist_’,
                ‘nobase_’, and ‘notrans_’. 

These prefixes are explained later (*note Program and Library Variables::)
(*note Man Pages::).


3.5 How derived variables are named

// bin_PROGRAMS = hello
// hello_SOURCES = main.c

Sometimes a Makefile variable name is derived from some text the maintainer
supplies.  For instance, a program name listed in ‘_PROGRAMS’ is rewritten into
the name of a ‘_SOURCES’ variable.  In cases like this, Automake canonicalizes
the text, so that program names and the like do not have to follow Makefile
variable naming rules.  

<to-underscores>
All characters in the name except for letters, numbers, the strudel (@), and the
underscore are 'turned' into 'underscores' when making variable references.

For example, if your program is named ‘sniff-glue’, the derived variable name
would be ‘sniff_glue_SOURCES’, not ‘sniff-glue_SOURCES’.  Similarly the sources
for a library named ‘libmumble++.a’ should be listed in the
  ‘libmumble___a_SOURCES’ variable.

The strudel is an addition, to make the use of Autoconf substitutions in
variable names less obfuscating.


3.6 Variables reserved for the user *user-variable*

Some ‘Makefile’ variables are 'reserved' by the GNU Coding Standards for the use
of the 'user' - the person 'building' the package. For instance, ‘CFLAGS’ is one
such variable.

Sometimes package developers are tempted to set user variables such as ‘CFLAGS’
because it appears to make their job easier.  However, the package itself should
never set a user variable, particularly not to include switches that are
required for proper compilation of the package.  Since these variables are
documented as being for the package builder, that person rightfully expects to
be able to override any of these variables at build time.

<shadow-variable>
To get around this problem, Automake introduces an automake-specific shadow
variable for 'each' user flag variable. Shadow variables are not introduced for
variables like ‘CC’, where they would make no sense. 

The shadow variable is named by prepending ‘AM_’ to the user variable’s name.
For instance, the shadow variable for ‘YFLAGS’ is ‘AM_YFLAGS’.  The package
maintainer—that is, the author(s) of the ‘Makefile.am’ and ‘configure.ac’
files—may adjust these shadow variables however necessary.


={============================================================================
*kt_linux_tool_150* make-automake-ex-packages

4 Some example packages

The second example shows how two programs can be built from the same file, using
different compilation parameters.  It contains some technical digressions that
are probably best skipped on first read.

Here is another, trickier example.  It shows how to generate two programs
(‘true’ and ‘false’) from the same source file (‘true.c’).  The difficult part
is that each compilation of ‘true.c’ requires different ‘cpp’ flags.

     bin_PROGRAMS = true false
     false_SOURCES =
     false_LDADD = false.o

     true.o: true.c
             $(COMPILE) -DEXIT_CODE=0 -c true.c

     false.o: true.c
             $(COMPILE) -DEXIT_CODE=1 -o false.o -c true.c

However if you were to build ‘true’ and ‘false’ in real life, you would probably
use per-program compilation flags, like so:

     bin_PROGRAMS = false true

     false_SOURCES = true.c
     false_CPPFLAGS = -DEXIT_CODE=1

     true_SOURCES = true.c
     true_CPPFLAGS = -DEXIT_CODE=0

In this case Automake will cause ‘true.c’ to be compiled twice, with different
flags.  In this instance, the names of the object files would be chosen by
automake; they would be ‘false-true.o’ and ‘true-true.o’. The name of the object
files rarely matters.


={============================================================================
*kt_linux_tool_150* make-automake-marco

6 Scanning configure.ac, using aclocal

`Automake scans the package’s configure.ac` to determine certain information
about the package. `Some autoconf macros are required` and `some variables` must
be defined in configure.ac. Automake will also use information from
configure.ac to further tailor its output.

Automake also `supplies some Autoconf macros` to make the maintenance easier.
These macros can automatically be put into your aclocal.m4 using the aclocal
program.


6.1 Configuration requirements

Here are the other macros that Automake requires but which are not run by
AM_INIT_AUTOMAKE:

AC_CONFIG_FILES
AC_OUTPUT

AC_CONFIG_FILES([
Makefile
doc/Makefile
src/Makefile
src/lib/Makefile
...
])
AC_OUTPUT

Automake uses these `to determine which files to create` (see Section
    “Creating Output Files” in The Autoconf Manual).
‘AC_CONFIG_FILES([foo/Makefile])’ will cause Automake to generate
foo/Makefile.in `if foo/Makefile.am exists`


6.2 Other things Automake recognizes

Every time Automake is run it `calls Autoconf to trace configure.ac` This way
it can recognize the use of certain macros and tailor the generated
Makefile.in appropriately.

Currently recognized macros and their effects are:

AC_CONFIG_HEADERS

Automake will generate rules to rebuild these headers from the corresponding
templates (usually, the template for a foo.h header being foo.h.in).


AM_CONDITIONAL

This introduces an Automake conditional (see Chapter 20 [Conditionals],
page 124).


6.4 Autoconf macros supplied with Automake

Automake ships with several Autoconf macros that you can use from your
configure.ac.  When you use one of them it will be included by aclocal in
aclocal.m4.

6.4.1 Public Macros

AM_INIT_AUTOMAKE([OPTIONS])

Runs many macros required for proper operation of the generated Makefiles.
Today, AM_INIT_AUTOMAKE is called with a single argument: a space-separated
list of Automake options that should be applied to every Makefile.am in the
tree. The effect is as if each option were listed in AUTOMAKE_OPTIONS (see
Chapter 17 [Options], page 117).


17.1 Options generalities

Various features of Automake can be controlled by options. Except where noted
otherwise, options can be specified in one of several ways. Most options can
be applied on a per-Makefile basis when listed in a special Makefile variable
named AUTOMAKE_OPTIONS. Some of these options only make sense when specified
in the toplevel Makefile.am file. 

Options are applied globally to all processed Makefile files when listed in
the first argument of AM_INIT_AUTOMAKE in configure.ac, and some options
which require changes to the configure script can only be specified there.
These are annotated below.

As a general rule, options specified in AUTOMAKE_OPTIONS take precedence over
those specified in AM_INIT_AUTOMAKE, which in turn take precedence over those
specified on the command line.


17.2 List of Automake options

foreign

Set the strictness as appropriate.

no-dist 

Don’t emit any code related to dist target. This is useful when a package has
its own method for making distributions.

-Wcategory or --warnings=category

These options behave exactly like their command-line counterpart (see Chapter
    5 [automake Invocation], page 26). This allows you to enable or disable
some warning categories on a per-file basis. You can also setup some warnings
for your entire project; for instance, try ‘AM_INIT_AUTOMAKE([-Wall])’ in your
  configure.ac.


note:
From For version 1.4, 10 January 1999 but not from the latest.

AM_ENABLE_MULTILIB
    This is used when a "multilib" library is being built. A multilib library
    is one that is built multiple times, once per target flag combination.
    This is only useful when the library is intended to be cross-compiled. 
    
    The first optional argument is the name of the `Makefile' being generated;
it defaults to `Makefile'. The second option argument is used to find the top
  source directory; it defaults to the empty string (generally this should not
      be used unless you are familiar with the internals). 


={============================================================================
*kt_linux_tool_150* make-automake-building programs and libraries

8 Building Programs and Libraries

A large part of Automake’s functionality is dedicated to making it easy to build
programs and libraries.

In order to build a program, you need to tell Automake which sources are
part of it, and which libraries it should be linked with.

This section also covers conditional compilation of sources or programs. Most
of the comments about these `also apply to libraries and libtool libraries`


8.1.1 Defining program sources

In a directory containing source that gets built into a program (`as opposed
    to` a library or a script), the `PROGRAMS primary` is used.  Programs can
be installed in ‘bindir’, ‘sbindir’, ‘libexecdir’, ‘pkglibexecdir’, or not at
all (‘noinst_’).  They can also be built only for ‘make check’, in which case
the prefix is ‘check_’.

     bin_PROGRAMS = hello

In this simple case, the resulting ‘Makefile.in’ will contain code to generate
a program named ‘hello’.

The variable ‘hello_SOURCES’ is used to specify which source files get built
into an executable:

     hello_SOURCES = hello.c version.c getopt.c getopt1.c getopt.h system.h

This causes each mentioned ‘.c’ file to be compiled into the corresponding
‘.o’.  Then all are linked to produce ‘hello’.

Header files listed in a ‘_SOURCES’ `will be included in the distribution` but
otherwise ignored.  In case it isn’t obvious, you should not include the
header file generated by ‘configure’ in a ‘_SOURCES’ variable; this file
should not be distributed.


8.1.2 Linking the program

If you need to link against libraries `that are 'not' found by ‘configure’`,
you can use `LDADD` to do so. This variable is used to specify 'additional'
  objects or libraries to link with; it is inappropriate for specifying
  specific linker 'flags', you should use `AM_LDFLAGS` for this purpose.


8.2 Building a library

Building a library is much like building a program.  In this case, the name of
the primary is `LIBRARIES`.  Libraries can be installed in ‘libdir’ or
‘pkglibdir’.

*Note A Shared Library::, for information on how to build shared libraries
using libtool and the ‘LTLIBRARIES’ primary.


For instance, to create a library named ‘libcpio.a’, but not install it, you
would write:

     noinst_LIBRARIES = libcpio.a
     libcpio_a_SOURCES = ...
     
The sources that go into a library are determined exactly as they are for
programs, via the ‘_SOURCES’ variables.  Note that the library name is
canonicalized, so the ‘_SOURCES’ variable corresponding to ‘libcpio.a’ is
`libcpio_a_SOURCES`, not ‘libcpio.a_SOURCES’.

Extra objects can be added to a library using the ‘LIBRARY_LIBADD’ variable.
This should be used for objects determined by ‘configure’.  Again from ‘cpio’:

     libcpio_a_LIBADD = $(LIBOBJS) $(ALLOCA)


Building a static library is done by compiling all object files, then by
invoking ‘$(AR) $(ARFLAGS)’ followed by the name of the library and the list
of objects, and finally by calling ‘$(RANLIB)’ on that library.  You should
call ‘AC_PROG_RANLIB’ from your ‘configure.ac’ to define ‘RANLIB’ (Automake
    will complain otherwise).

You `can override` the ‘AR’ variable by defining a per-library ‘maude_AR’
variable (*note Program and Library Variables::).

<use-static-library>
To use a static library when building a program, add it to ‘LDADD’ for this
program.  In the following example, the program ‘cpio’ is statically linked
with the library ‘libcpio.a’.

     noinst_LIBRARIES = libcpio.a
     libcpio_a_SOURCES = ...

     bin_PROGRAMS = cpio
     cpio_SOURCES = cpio.c ...
     cpio_LDADD = libcpio.a     // note: see


8.3 Building a Shared Library

<libtool>
Building shared libraries portably is a relatively complex matter.  For this
reason, GNU Libtool was created to help build shared libraries in a
platform-independent way.

8.3.1 The Libtool Concept
-------------------------

<la-lo-suffix>

Libtool abstracts shared and static libraries into a unified concept henceforth
called "libtool libraries".  Libtool libraries are files using the ‘.la’
'suffix', and can designate a static library, a shared library, or maybe both.
Their exact nature cannot be determined until ‘./configure’ is run: not all
platforms support all kinds of libraries, and users can explicitly select which
libraries should be built.  (However the package’s maintainers can tune the
    default, *note The ‘AC_PROG_LIBTOOL’ macro: (libtool)AC_PROG_LIBTOOL.)

Because object files for shared and static libraries must be compiled
'differently', libtool is also used during compilation.  Object files built by
libtool are called "libtool objects": these are files using the ‘.lo’ suffix.
Libtool libraries are built from these libtool objects.

You should not assume anything about the structure of ‘.la’ or ‘.lo’ files and
how libtool constructs them: this is libtool’s concern, and the last thing one
wants is to learn about libtool’s guts.  However the existence of these files
matters, because they are used as targets and dependencies in ‘Makefile’s rules
when building libtool libraries.  There are situations where you may have to
refer to these, for instance when expressing dependencies for building source
files conditionally


8.3.2 Building Libtool Libraries
--------------------------------

Automake uses libtool to build libraries declared with the ‘LTLIBRARIES’
primary.  Each ‘_LTLIBRARIES’ variable is a list of libtool libraries to build.
For instance, to create a libtool library named ‘libgettext.la’, and install it
in ‘libdir’, write:

     lib_LTLIBRARIES = libgettext.la
     libgettext_la_SOURCES = gettext.c gettext.h ...
     include_HEADERS = gettext.h ...

Automake predefines the variable ‘pkglibdir’, so you can use
‘pkglib_LTLIBRARIES’ to install libraries in ‘$(libdir)/@PACKAGE@/’.

If ‘gettext.h’ is a public header file that needs to be installed in order for
people to use the library, it should be declared using a ‘_HEADERS’ variable,
       not in ‘libgettext_la_SOURCES’.  Headers listed in the latter should be
         internal headers that are not part of the public interface.


The following example builds a program named ‘hello’ that is linked with
‘libgettext.la’.

     lib_LTLIBRARIES = libgettext.la
     libgettext_la_SOURCES = gettext.c …

     bin_PROGRAMS = hello
     hello_SOURCES = hello.c …
     hello_LDADD = libgettext.la

Whether ‘hello’ is statically or dynamically linked with ‘libgettext.la’ is not
yet known: this will depend on the configuration of libtool and the capabilities
of the host.


8.3.5 Libtool Convenience Libraries
-----------------------------------

Sometimes you want to build libtool libraries that should 'not' be installed.
These are called "libtool convenience libraries" and are typically used to
encapsulate many sublibraries, later gathered into one 'big' installed library.

Unlike installed libtool libraries they do not need an ‘-rpath’ flag at link
time (actually this is the only difference).

Convenience libraries listed in ‘noinst_LTLIBRARIES’ are always built.  Those
listed in ‘check_LTLIBRARIES’ are built only upon ‘make check’.  Finally,
libraries listed in ‘EXTRA_LTLIBRARIES’ are never built explicitly: Automake
  outputs rules to build them, but if the library does not appear as a Makefile
  dependency anywhere it won’t be built (this is why ‘EXTRA_LTLIBRARIES’ is used
      for conditional compilation).

Here is a sample setup merging libtool convenience libraries from subdirectories
into one main ‘libtop.la’ library.

     # -- Top-level Makefile.am --
     SUBDIRS = sub1 sub2 ...
     lib_LTLIBRARIES = libtop.la
     libtop_la_SOURCES =
     libtop_la_LIBADD = \
       sub1/libsub1.la \
       sub2/libsub2.la \
       ...

     # -- sub1/Makefile.am --
     noinst_LTLIBRARIES = libsub1.la
     libsub1_la_SOURCES = ...

     # -- sub2/Makefile.am --
     # showing nested convenience libraries
     SUBDIRS = sub2.1 sub2.2 ...
     noinst_LTLIBRARIES = libsub2.la
     libsub2_la_SOURCES =
     libsub2_la_LIBADD = \
       sub21/libsub21.la \
       sub22/libsub22.la \
       ...

When using such setup, beware that ‘automake’ will assume ‘libtop.la’ is to be
linked with the C linker. This is because ‘libtop_la_SOURCES’ is empty, so
‘automake’ picks C as 'default' language. If ‘libtop_la_SOURCES’ was not empty,
‘automake’ would select the linker as explained in *note How the Linker is
  Chosen::.

If one of the sublibraries contains non-C source, it is important that the
appropriate linker be chosen. One way to achieve this is to pretend that there
is such a non-C file among the sources of the library, thus 'forcing' ‘automake’
to select the appropriate linker.  Here is the top-level ‘Makefile’ of our
example updated to force C++ linking.

     SUBDIRS = sub1 sub2 ...
     lib_LTLIBRARIES = libtop.la
     libtop_la_SOURCES =
     # Dummy C++ source to cause C++ linking.
     nodist_EXTRA_libtop_la_SOURCES = dummy.cxx
     libtop_la_LIBADD = \
       sub1/libsub1.la \
       sub2/libsub2.la \
       ...

‘EXTRA_*_SOURCES’ variables are used to keep track of source files that might be
compiled (this is mostly useful when doing conditional compilation using
    ‘AC_SUBST’, *note Conditional Libtool Sources::), and the ‘nodist_’ prefix
means the listed sources are not to be distributed (*note Program and Library
    Variables::).  

In effect the file ‘dummy.cxx’ does not need to exist in the source tree.  Of
course if you have some real source file to list in ‘libtop_la_SOURCES’ there is
no point in cheating with ‘nodist_EXTRA_libtop_la_SOURCES’.


<am-dummy-file>
// This file is necessary because automake assumes that a shared library with
// no source files (i.e. a shared library built from an archive library) is a C
// library, so it incorrectly links it with the C compiler, omitting libstdc++
// and causing undefined symbols
//
// TL;DR: This file is necessary because automake is rubbish.

libVanadiumWebKitVideoElement_la_SOURCES = src/Dummy.cpp

libVanadiumWebKitVideoElement_la_LIBADD = \
  libVanadiumWebKitVideoElementStatic.la \
  $(LIBADDS)


8.3.7 ‘_LIBADD’, ‘_LDFLAGS’, and ‘_LIBTOOLFLAGS’
------------------------------------------------

note: LIBRARY_LIBADD means xxx_LIBADD

As shown in previous sections, the ‘LIBRARY_LIBADD’ variable should be used to
list extra libtool objects (‘.lo’ files) or libtool libraries (‘.la’) to add to
LIBRARY.

The ‘LIBRARY_LDFLAGS’ variable is the place to list 'additional' libtool linking
'flags', such as ‘-version-info’, ‘-static’, and a lot more.  *Note Link mode:
(libtool)Link mode.

The ‘libtool’ command has 'two' kinds of options: mode-specific options and
generic options.  Mode-specific options such as the aforementioned linking flags
should be lumped with the other flags passed to the tool invoked by ‘libtool’
(hence the use of ‘LIBRARY_LDFLAGS’ for libtool linking flags).  


Generic options include ‘--tag=TAG’ and ‘--silent’ (*note Invoking ‘libtool’:
    (libtool)Invoking libtool. for more options) should appear before the mode
selection on the command line; in ‘Makefile.am’s they should be listed in the
‘LIBRARY_LIBTOOLFLAGS’ variable.


If ‘LIBRARY_LIBTOOLFLAGS’ is not defined, then the variable ‘AM_LIBTOOLFLAGS’ is
used instead.

These flags are passed to libtool after the ‘--tag=TAG’ option computed by
Automake (if any), so ‘LIBRARY_LIBTOOLFLAGS’ (or ‘AM_LIBTOOLFLAGS’) is a good
place to override or supplement the ‘--tag=TAG’ setting.

The libtool rules also use a ‘LIBTOOLFLAGS’ variable that should not be set in
‘Makefile.am’: this is a user variable (*note Flag Variables Ordering::.  It
    allows users to run ‘make LIBTOOLFLAGS=--silent’, for instance.  Note that
    the verbosity of ‘libtool’ can also be influenced by the Automake support
    for silent rules (*note Automake Silent Rules::).


={============================================================================
*kt_linux_tool_150* make-automake-building variables

8.4 Program and Library Variables

Associated with each program is a collection of variables that can be used to
modify how that program is built.  There is a similar list of such variables
for each library.  The canonical name of the program (or library) is used as a
base for naming these variables.

In the list below, we use the name “maude” to refer to the 'program' or
'library'.  In your ‘Makefile.am’ you would replace this with the canonical
name of your program.  This list also refers to “maude” as a program, but in
general the same rules apply for both static and dynamic libraries; the
documentation below notes situations `where programs and libraries differ`

‘maude_SOURCES’
     This variable, if it exists, lists all the source files that are compiled
     to build the program.  These files are added to the distribution by
     default.  When building the program, Automake will cause each source file
     to be compiled to a single ‘.o’ file (or ‘.lo’ when using libtool).

     Normally these object files are named after the source file, but other
     factors can change this.  If a file in the ‘_SOURCES’ variable has an
     unrecognized extension, Automake will do one of two things with it.  If a
     suffix rule exists for turning files with the unrecognized extension into
     ‘.o’ files, then ‘automake’ will treat this file as it will any other
     source file.  Otherwise, the file will be ignored as though it were a
     header file.

     The prefixes ‘dist_’ and ‘nodist_’ can be used to control whether files
     listed in a ‘_SOURCES’ variable are distributed. ‘dist_’ is 'redundant',
            as sources are distributed by default, but it can be specified for
              clarity if desired.

     It is possible to have both ‘dist_’ and ‘nodist_’ variants of a given
     ‘_SOURCES’ variable at once; this lets you easily distribute some files and
     not others, for instance:

          nodist_maude_SOURCES = nodist.c
          dist_maude_SOURCES = dist-me.c

     By default the output file (on Unix systems, the ‘.o’ file) will be put
     into the current 'build' directory.  However, if the option
     ‘subdir-objects’ is in effect in the current directory then the ‘.o’ file
     will be put into the subdirectory named after the source file.  For
     instance, with ‘subdir-objects’ enabled, ‘sub/dir/file.c’ will be compiled
     to ‘sub/dir/file.o’.  Some people prefer this mode of operation.  You can
     specify ‘subdir-objects’ in ‘AUTOMAKE_OPTIONS’ (*note Options::).

‘EXTRA_maude_SOURCES’
     Automake needs to know the list of files you intend to compile
     _statically_.  For one thing, this is the only way Automake has of knowing
     what sort of language support a given ‘Makefile.in’ requires.  (1) This
     means that, for example, you can’t put a configure substitution like
     ‘@my_sources@’ into a ‘_SOURCES’ variable.  If you intend to conditionally
     compile source files and use ‘configure’ to substitute the appropriate
     object names into, e.g., ‘_LDADD’ (see below), then you should list the
     corresponding source files in the ‘EXTRA_’ variable.

     This variable also supports ‘dist_’ and ‘nodist_’ prefixes.

‘maude_AR’
     A static library is created by default by invoking ‘$(AR) $(ARFLAGS)’
     followed by the name of the library and then the objects being put into the
     library.  You can override this by setting the ‘_AR’ variable.  This is
     usually used with C++; some C++ compilers require a special invocation in
     order to instantiate all the templates that should go into a library.  For
     instance, the SGI C++ compiler likes this variable set like so:

     libmaude_a_AR = $(CXX) -ar -o

‘maude_LIBADD’
     Extra objects can be added to a _library_ using the ‘_LIBADD’ variable.
     For instance, this `should be used for objects determined by ‘configure’`
     (*note A Library::).

     In the case of libtool libraries, ‘maude_LIBADD’ can also refer to other
     libtool libraries.


‘maude_LDADD’
     Extra objects (‘*.$(OBJEXT)’) and libraries (‘*.a’, ‘*.la’) can be added to
     a _program_ by listing them in the ‘_LDADD’ variable.  For instance, this
     should be used for objects determined by ‘configure’ (*note Linking::).

     ‘_LDADD’ and ‘_LIBADD’ are `inappropriate for passing program-specific`
     linker flags (except for ‘-l’, ‘-L’, ‘-dlopen’ and ‘-dlpreopen’).  Use
     the ‘_LDFLAGS’ variable for this purpose.

     For instance, if your ‘configure.ac’ uses ‘AC_PATH_XTRA’, you could link
     your program against the X libraries like so:

          maude_LDADD = $(X_PRE_LIBS) $(X_LIBS) $(X_EXTRA_LIBS)

     We recommend that you use ‘-l’ and ‘-L’ only when referring to
     third-party libraries, and give the explicit file names of any library
     built by your package.  Doing so will ensure that ‘maude_DEPENDENCIES’
     (see below) is correctly defined by default.


‘maude_LDFLAGS’
     This variable is used to pass 'extra''flags' to the link step of a program
     or a shared library.  It overrides the ‘AM_LDFLAGS’ variable.

‘maude_LIBTOOLFLAGS’
     This variable is used to pass extra options to ‘libtool’.  It overrides the
     ‘AM_LIBTOOLFLAGS’ variable.  These options are output before ‘libtool’’s
     ‘--mode=MODE’ option, so they should not be mode-specific options (those
         belong to the compiler or linker flags).  *Note Libtool Flags::.

‘maude_DEPENDENCIES’
‘EXTRA_maude_DEPENDENCIES’
     It is also occasionally useful to have a target (program or library) depend
     on some other file that is not actually part of that target.  This can be
     done using the ‘_DEPENDENCIES’ variable.  Each target depends on the
     contents of such a variable, but no further interpretation is done.

     Since these dependencies are associated to the link rule used to
     create the programs they should normally list files used by the
     link command.  That is ‘*.$(OBJEXT)’, ‘*.a’, or ‘*.la’ files for
     programs; ‘*.lo’ and ‘*.la’ files for Libtool libraries; and
     ‘*.$(OBJEXT)’ files for static libraries.  In rare cases you may
     need to add other kinds of files such as linker scripts, but
     _listing a source file in ‘_DEPENDENCIES’ is wrong_.  If some
     source file needs to be built before all the components of a
     program are built, consider using the ‘BUILT_SOURCES’ variable
     (*note Sources::).

     If ‘_DEPENDENCIES’ is not supplied, it is computed by Automake.
     The automatically-assigned value is the contents of ‘_LDADD’ or
     ‘_LIBADD’, with most configure substitutions, ‘-l’, ‘-L’, ‘-dlopen’
     and ‘-dlpreopen’ options removed.  The configure substitutions that
     are left in are only ‘$(LIBOBJS)’ and ‘$(ALLOCA)’; these are left
     because it is known that they will not cause an invalid value for
     ‘_DEPENDENCIES’ to be generated.

     ‘_DEPENDENCIES’ is more likely used to perform conditional
     compilation using an ‘AC_SUBST’ variable that contains a list of
     objects.  *Note Conditional Sources::, and *note Conditional
     Libtool Sources::.

     The ‘EXTRA_*_DEPENDENCIES’ variable may be useful for cases where
     you merely want to augment the ‘automake’-generated ‘_DEPENDENCIES’
     variable rather than replacing it.

‘maude_LINK’
     You can override the linker on a per-program basis.  By default the
     linker is chosen according to the languages used by the program.
     For instance, a program that includes C++ source code would use the
     C++ compiler to link.  The ‘_LINK’ variable must hold the name of a
     command that can be passed all the ‘.o’ file names and libraries to
     link against as arguments.  Note that the name of the underlying
     program is _not_ passed to ‘_LINK’; typically one uses ‘$@’:

          maude_LINK = $(CCLD) -magic -o $@

     If a ‘_LINK’ variable is not supplied, it may still be generated
     and used by Automake due to the use of per-target link flags such
     as ‘_CFLAGS’, ‘_LDFLAGS’ or ‘_LIBTOOLFLAGS’, in cases where they
     apply.


<per-target-compilation>

‘maude_CCASFLAGS’
‘maude_CFLAGS’
‘maude_CPPFLAGS’
‘maude_CXXFLAGS’
‘maude_FFLAGS’
‘maude_GCJFLAGS’
‘maude_LFLAGS’
‘maude_OBJCFLAGS’
‘maude_OBJCXXFLAGS’
‘maude_RFLAGS’
‘maude_UPCFLAGS’
‘maude_YFLAGS’
     Automake allows you to set 'compilation' flags on a per-program (or
         per-library) basis.  A single source file can be included in several
     programs, and it will potentially be compiled with different flags for each
     program.  This works for any language directly supported by Automake.

     These `per-target compilation flags` are 
     ‘_CCASFLAGS’, ‘_CFLAGS’, ‘_CPPFLAGS’, ‘_CXXFLAGS’, ‘_FFLAGS’, ‘_GCJFLAGS’, 
     ‘_LFLAGS’, ‘_OBJCFLAGS’, ‘_OBJCXXFLAGS’, ‘_RFLAGS’, ‘_UPCFLAGS’,
     and ‘_YFLAGS’.

     When using a per-target compilation flag, Automake will choose a
     `different name` for the intermediate object files.  Ordinarily a file like
     ‘sample.c’ will be compiled to produce ‘sample.o’.  However, if the
     program’s ‘_CFLAGS’ variable is set, then the object file will be named,
     for instance, ‘maude-sample.o’.  (See also *note Renamed Objects::).

     In compilations with per-target flags, the ordinary ‘AM_’ form of the flags
     variable is _not_ automatically included in the compilation (however, the
         user form of the variable _is_ included).

     So for instance, if you want the hypothetical ‘maude’ compilations
     to also use the value of ‘AM_CFLAGS’, you would need to write:

          maude_CFLAGS = … your flags ... $(AM_CFLAGS)

     *Note Flag Variables Ordering::, for more discussion about the interaction
     between 'user' variables, ‘AM_’ 'shadow' variables, and per-'target'
     variables.

‘maude_SHORTNAME’
     On some platforms the allowable file names are very short.  In
     order to support these systems and per-target compilation flags at
     the same time, Automake allows you to set a “short name” that will
     influence how intermediate object files are named.  For instance,
     in the following example,

          bin_PROGRAMS = maude
          maude_CPPFLAGS = -DSOMEFLAG
          maude_SHORTNAME = m
          maude_SOURCES = sample.c …

     the object file would be named ‘m-sample.o’ rather than
     ‘maude-sample.o’.

     This facility is rarely needed in practice, and we recommend
     avoiding it until you find it is required.

   ---------- Footnotes ----------

   (1) There are other, more obscure reasons for this limitation as well.


8.6 Special handling for LIBOBJS and ALLOCA

The ‘$(LIBOBJS)’ and ‘$(ALLOCA)’ variables `list object files` that should be
compiled into the project to provide an implementation for functions that are
missing or broken on the host system. `They are substituted by configure`

These variables are defined by Autoconf macros such as AC_LIBOBJ,
AC_REPLACE_FUNCS, or AC_FUNC_ALLOCA. Many other Autoconf macros call AC_LIBOBJ
  or AC_REPLACE_FUNCS `to populate` ‘$(LIBOBJS)’.

// more


8.7 Variables used when building a program

Occasionally it is useful to know `which ‘Makefile’ variables Automake uses` for
compilations, and in `which order`; for instance, you might need to do your own
compilation in some special cases.

   Some variables are 'inherited' from Autoconf; these are 

   ‘CC’, ‘CFLAGS’, ‘CPPFLAGS’, ‘DEFS’, ‘LDFLAGS’, and ‘LIBS’.

   There are some additional variables that Automake defines on its own:

‘AM_CPPFLAGS’
     The contents of this variable are passed to `every compilation` that invokes
     the C 'preprocessor'; it is a list of arguments to the preprocessor.  For
     instance, ‘-I’ and ‘-D’ options should be listed here.

     Automake already provides some ‘-I’ options automatically, in a separate
     variable that is also passed to every compilation that invokes the C
     preprocessor. In particular it generates ‘-I.’, ‘-I$(srcdir)’, and a ‘-I’
     pointing to the directory holding ‘config.h’ (if you’ve used
         ‘AC_CONFIG_HEADERS’).  You can disable the default ‘-I’ options using
     the `nostdinc` option.

     When a file to be included is generated during the build and not
     part of a distribution tarball, its location is under
     ‘$(builddir)’, not under ‘$(srcdir)’.  This matters especially for
     packages that use header files placed in sub-directories and want
     to allow builds outside the source tree (*note VPATH Builds::).  In
     that case we recommend to use a pair of ‘-I’ options, such as,
     e.g., ‘-Isome/subdir -I$(srcdir)/some/subdir’ or
     ‘-I$(top_builddir)/some/subdir -I$(top_srcdir)/some/subdir’.  Note
     that the reference to the build tree should come before the
     reference to the source tree, so that accidentally leftover
     generated files in the source directory are ignored.

     `AM_CPPFLAGS is ignored` in preference to a per-executable (or
         per-library) ‘_CPPFLAGS’ variable `if it is defined`

// ‘INCLUDES’
//      This does the same job as ‘AM_CPPFLAGS’ (or any per-target
//      ‘_CPPFLAGS’ variable if it is used).  It is an older name for the
//      same functionality.  `This variable is deprecated`; we suggest using
//      ‘AM_CPPFLAGS’ and per-target ‘_CPPFLAGS’ instead.

‘AM_CFLAGS’
     This is the variable the ‘Makefile.am’ author can use to pass in
     additional C compiler flags.  In some situations, this is not used, in
     preference to the per-executable (or per-library) ‘_CFLAGS’.

‘COMPILE’
     This is the command used to actually compile a C source file.  The file
     name is appended to form the complete command line.

‘AM_LDFLAGS’
     This is the variable the ‘Makefile.am’ author can use to pass in
     additional linker flags.  In some situations, this is not used, in
     preference to the per-executable (or per-library) ‘_LDFLAGS’.

‘LINK’
     This is the command used to actually link a C program.  It already
     includes ‘-o $@’ and the usual variable references (for instance,
         ‘CFLAGS’); it takes as “arguments” the names of the object files and
     libraries to link in.  This variable is not used when the linker is
     overridden with a per-target ‘_LINK’ variable or per-target flags cause
     Automake to define such a ‘_LINK’ variable.


8.9 C++ Support

Automake includes full support for C++.

Any package including C++ code 'must' define the output variable ‘CXX’ in
‘configure.ac’; the simplest way to do this is to use the ‘AC_PROG_CXX’ macro
(*note Particular Program Checks: (autoconf)Particular Programs.).

A few additional variables are defined when a C++ source file is seen:

‘CXX’
     The name of the C++ compiler.

‘CXXFLAGS’
     Any flags to pass to the C++ compiler.

‘AM_CXXFLAGS’
     The maintainer’s variant of ‘CXXFLAGS’.

‘CXXCOMPILE’
     The command used to actually compile a C++ source file.  The file name is
     appended to form the complete command line.

‘CXXLINK’
     The command used to actually link a C++ program.


={============================================================================
*kt_linux_tool_150* make-automake-building variable order

27.6 Flag Variables Ordering

What is the difference between ‘AM_CFLAGS’, ‘CFLAGS’, and ‘mumble_CFLAGS’?

Why does ‘automake’ output ‘CPPFLAGS’ after ‘AM_CPPFLAGS’ on compile lines?
Shouldn’t it be the converse?

My ‘configure’ adds some warning flags into ‘CXXFLAGS’.  In one ‘Makefile.am’ I
would like to append a new flag, however if I put the flag into ‘AM_CXXFLAGS’ it
is `prepended` to the other flags, `not appended`


Compile Flag Variables

This section attempts to answer all the above questions. We will mostly
discuss ‘CPPFLAGS’ in our examples, but actually the answer holds for 'all'
the compile flags used in Automake: ‘CCASFLAGS’, ‘CFLAGS’, ‘CPPFLAGS’,
‘CXXFLAGS’, ‘FCFLAGS’, ‘FFLAGS’, ‘GCJFLAGS’, ‘LDFLAGS’, ‘LFLAGS’,
‘LIBTOOLFLAGS’, ‘OBJCFLAGS’, ‘OBJCXXFLAGS’, ‘RFLAGS’, ‘UPCFLAGS’, and
  ‘YFLAGS’.

‘CPPFLAGS’, ‘AM_CPPFLAGS’, and ‘mumble_CPPFLAGS’ are three variables that can
be used to pass flags to the C preprocessor (actually these variables are also
    used for other languages like C++ or preprocessed Fortran). ‘CPPFLAGS’ is
the user variable *user-variable* , ‘AM_CPPFLAGS’ is the Automake variable,
and ‘mumble_CPPFLAGS’ is the variable specific to the ‘mumble’ target (we call
    this a per-target variable).


<variable-order>
Automake always uses two of these variables when compiling C sources files.
When compiling an object file for the ‘mumble’ target, the first variable will
be ‘mumble_CPPFLAGS’ if it is defined, or ‘AM_CPPFLAGS’ otherwise. The second
variable is always ‘CPPFLAGS’.

   In the following example,

     bin_PROGRAMS = foo bar
     foo_SOURCES = xyz.c
     bar_SOURCES = main.c
     foo_CPPFLAGS = -DFOO
     AM_CPPFLAGS = -DBAZ

‘xyz.o’ will be compiled with ‘$(foo_CPPFLAGS) $(CPPFLAGS)’, (because ‘xyz.o’
    is part of the ‘foo’ target), 

while ‘main.o’ will be compiled with ‘$(AM_CPPFLAGS) $(CPPFLAGS)’ (because
    there is no per-target variable for target ‘bar’).


The difference between ‘mumble_CPPFLAGS’ and ‘AM_CPPFLAGS’ being clear enough,
    let’s focus on ‘CPPFLAGS’. ‘CPPFLAGS’ is a user variable, i.e., a variable
      that users are entitled to modify in order to compile the package.  This
      variable, like many others, is documented at the end of the output of
      ‘configure --help’.

For instance, someone who needs to add ‘/home/my/usr/include’ to the C
compiler’s search path would configure a package with

     ./configure CPPFLAGS='-I /home/my/usr/include'

and this flag would be 'propagated' to the compile rules of `all Makefile’s`


<make-time-change>
It is also not uncommon to override a user variable at `make-time`.  Many
installers do this with ‘prefix’, but this can be useful with compiler flags
too. For instance, if, while debugging a C++ project, you need to 'disable'
optimization `in one specific object file`, you can run something like

     rm file.o
     make CXXFLAGS=-O0 file.o
     make

The reason ‘$(CPPFLAGS)’ appears after ‘$(AM_CPPFLAGS)’ or ‘$(mumble_CPPFLAGS)’
in the compile command is `that users should always have the last say`  

*should-not-use-user-variable-in-makefile-or-config-ac*

It probably makes more sense if you think about it while looking at the
‘CXXFLAGS=-O0’ above, which should 'supersede' any other switch from
‘AM_CXXFLAGS’ or ‘mumble_CXXFLAGS’ (and this of course replaces the previous
    value of ‘CXXFLAGS’).

You should never redefine a user variable such as ‘CPPFLAGS’ in ‘Makefile.am’.
Use ‘automake -Woverride’ to diagnose such mistakes.

Even something like

     CPPFLAGS = -DDATADIR=\"$(datadir)\" @CPPFLAGS@

is erroneous.  Although this preserves ‘configure’’s value of ‘CPPFLAGS’, the
definition of ‘DATADIR’ will disappear if a user attempts to override ‘CPPFLAGS’
from the ‘make’ command line.

     AM_CPPFLAGS = -DDATADIR=\"$(datadir)\"

is all that is needed here if no per-target flags are used.

You should not add options to these user variables within ‘configure’ either,
for the same reason.  Occasionally you need to modify these variables to perform
  a test, but you should reset their values afterwards.  In contrast, it is OK
    to modify the ‘AM_’ variables within ‘configure’ if you ‘AC_SUBST’ them, but
    it is rather rare that you need to do this, unless you really want to change
    the default definitions of the ‘AM_’ variables in all ‘Makefile’s.

What we recommend is that you define extra flags in separate variables.  For
instance, you may write an Autoconf macro that computes a set of warning options
for the C compiler, and ‘AC_SUBST’ them in ‘WARNINGCFLAGS’; you may also have an
  Autoconf macro that determines which compiler and which linker flags should be
    used to link with library ‘libfoo’, and ‘AC_SUBST’ these in ‘LIBFOOCFLAGS’
    and ‘LIBFOOLDFLAGS’.  Then, a ‘Makefile.am’ could use these variables as
    follows:

     AM_CFLAGS = $(WARNINGCFLAGS)
     bin_PROGRAMS = prog1 prog2
     prog1_SOURCES = ...
     prog2_SOURCES = ...
     prog2_CFLAGS = $(LIBFOOCFLAGS) $(AM_CFLAGS)
     prog2_LDFLAGS = $(LIBFOOLDFLAGS)

In this example both programs will be compiled with the flags substituted into
‘$(WARNINGCFLAGS)’, and ‘prog2’ will additionally be compiled with the flags
required to link with ‘libfoo’.

Note that listing ‘AM_CFLAGS’ in a per-target ‘CFLAGS’ variable is a common
idiom to ensure that ‘AM_CFLAGS’ applies to every target in a ‘Makefile.in’.

Using variables like this gives you full control over the ordering of the flags.
For instance, if there is a flag in $(WARNINGCFLAGS) that you want to negate for
a particular target, you can use something like ‘prog1_CFLAGS = $(AM_CFLAGS)
-no-flag’.  If all of these flags had been forcefully appended to ‘CFLAGS’,
  there would be no way to disable one flag.  Yet another reason to leave user
    variables to users.

Finally, we have avoided naming the variable of the example ‘LIBFOO_LDFLAGS’
(with an underscore) because that would cause Automake to think that this is
actually a per-target variable (like ‘mumble_LDFLAGS’) for some non-declared
‘LIBFOO’ target.

Other Variables
---------------

There are other variables in Automake that follow similar principles to allow
user options.  For instance, Texinfo rules (*note Texinfo::) use ‘MAKEINFOFLAGS’
and ‘AM_MAKEINFOFLAGS’.  Similarly, DejaGnu tests (*note DejaGnu Tests::) use
‘RUNTESTDEFAULTFLAGS’ and ‘AM_RUNTESTDEFAULTFLAGS’.  The tags and ctags rules
(*note Tags::) use ‘ETAGSFLAGS’, ‘AM_ETAGSFLAGS’, ‘CTAGSFLAGS’, and
‘AM_CTAGSFLAGS’.  Java rules (*note Java::) use ‘JAVACFLAGS’ and
‘AM_JAVACFLAGS’.  None of these rules support per-target flags (yet).

To some extent, even ‘AM_MAKEFLAGS’ (*note Subdirectories::) obeys this naming
scheme.  The slight difference is that ‘MAKEFLAGS’ is passed to sub-‘make’s
implicitly by ‘make’ itself.

‘ARFLAGS’ (*note A Library::) is usually defined by Automake and has neither
‘AM_’ nor per-target cousin.

Finally you should not think that the existence of a per-target variable implies
the existence of an ‘AM_’ variable or of a user variable.  For instance, the
‘mumble_LDADD’ per-target variable overrides the makefile-wide ‘LDADD’ variable
(which is not a user variable), and ‘mumble_LIBADD’ exists only as a per-target
variable.  *Note Program and Library Variables::.


={============================================================================
*kt_linux_tool_150* automake: 08: program variables: case issue

In short LDFLAGS is added before the object files on the command line and LDADD
is added afterwards.

<fail-case>
nexus_inspect_LDADD = \
   @NEXUS_LIBS@            note: this has -Wl,--as-needed

nexus_inspect_LDFLAGS = \
   -lpthread \
   -linit \
   -rdynamic

-Wl,--as-needed -L/zinc-install-root/release/huawei-bcm7409/lib
...
-lnexusMgr -ldirectfb -lfusion -ldirect -linit -lnexus -lz -lpthread

<okay-case>
nexus_inspect_LDFLAGS = @NEXUS_LIBS@

So -lxxx and then --as-needed

-lnexusMgr -ldirectfb -lfusion -ldirect -linit -lnexus -lz -lpthread
...
-Wl,--as-needed -L/zinc-install-root/release/huawei-bcm7409/lib

So not sure how automake combines all but it makes different order. This caues
the problem because shared libraries after as-needed may not linked at the time
of linking and cause undefined symbol when making executable binary.


={============================================================================
*kt_linux_tool_150* automake: 09: scripts

9 Other Derived Objects
***********************

Automake can handle derived objects that are not C programs.  Sometimes the
support for actually building such objects must be explicitly supplied, but
Automake will still automatically handle installation and distribution.

9.1 Executable Scripts
======================

<when-copied>
It is possible to define and install programs that are scripts.  Such
programs are listed using the ‘SCRIPTS’ primary name.  When the script
is distributed in its final, installable form, the ‘Makefile’ usually
looks as follows:

     # Install my_script in $(bindir) and distribute it.
     dist_bin_SCRIPTS = my_script

Scripts are not distributed by default; as we have just seen, those that should
be distributed can be specified using a ‘dist_’ prefix as with other primaries.

Scripts can be installed in ‘bindir’, ‘sbindir’, ‘libexecdir’, ‘pkglibexecdir’,
or ‘pkgdatadir’.

Scripts that need not be installed can be listed in ‘noinst_SCRIPTS’, and among
them, those which are needed only by ‘make check’ should go in ‘check_SCRIPTS’.

<when-to-be-built>
When a script needs to be built, the ‘Makefile.am’ should include the
appropriate rules.  For instance the ‘automake’ program itself is a Perl script
that is generated from ‘automake.in’.  Here is how this is handled:

     bin_SCRIPTS = automake
     CLEANFILES = $(bin_SCRIPTS)
     EXTRA_DIST = automake.in

     do_subst = sed -e 's,[@]datadir[@],$(datadir),g' \
                 -e 's,[@]PERL[@],$(PERL),g' \
                 -e 's,[@]PACKAGE[@],$(PACKAGE),g' \
                 -e 's,[@]VERSION[@],$(VERSION),g' \
                 …

     automake: automake.in Makefile
             $(do_subst) < $(srcdir)/automake.in > automake
             chmod +x automake

Such scripts for which a build rule has been supplied need to be deleted
explicitly using ‘CLEANFILES’ (*note Clean::), and their sources have to be
distributed, usually with ‘EXTRA_DIST’ (*note Basics of Distribution::).

Another common way to build scripts is to process them from ‘configure’ with
‘AC_CONFIG_FILES’.  In this situation Automake knows which files should be
cleaned and distributed, and what the rebuild rules should look like.

For instance if ‘configure.ac’ contains

     AC_CONFIG_FILES([src/my_script], [chmod +x src/my_script])

to build ‘src/my_script’ from ‘src/my_script.in’, then a ‘src/Makefile.am’ to
install this script in ‘$(bindir)’ can be as simple as

     bin_SCRIPTS = my_script
     CLEANFILES = $(bin_SCRIPTS)

There is no need for ‘EXTRA_DIST’ or any build rule: Automake infers them from
‘AC_CONFIG_FILES’ (*note Requirements::).  ‘CLEANFILES’ is still useful, because
by default Automake will clean targets of ‘AC_CONFIG_FILES’ in ‘distclean’, not
‘clean’.

Although this looks simpler, building scripts this way has one drawback:
directory variables such as ‘$(datadir)’ are not fully expanded and may refer to
other directory variables.


={============================================================================
*kt_linux_tool_150* automake: 15: test and make check

15 Support for test suites
**************************

Automake can generate code to handle two kinds of test suites.  One is based on
integration with the ‘dejagnu’ framework.  The other (and most used) form is
based on the use of generic test scripts, and its 'activation' is triggered by
the definition of the special ‘TESTS’ variable.  This second form allows for
various degrees of sophistication and customization; in particular, it allows
for concurrent execution of test scripts, use of established test protocols such
  as TAP, and definition of custom test drivers and test runners.

In either case, the testsuite is 'invoked' via ‘make check’.


15.2.1 Scripts-based Testsuites
-------------------------------

If the 'special' variable ‘TESTS’ is defined, its value is taken to be a list of
'programs' or 'scripts' to 'run' in order to do the testing.  Under the
appropriate circumstances, it’s possible for ‘TESTS’ to list also data files to
be passed to one or more test scripts defined by different means (the so-called
    “log compilers”, *note Parallel Test Harness::).

Test scripts can be executed serially or concurrently.  Automake supports both
these kinds of test execution, with the 'parallel' test harness being the
'default'.  The concurrent test harness relies on the concurrence capabilities
(if any) offered by the underlying ‘make’ implementation, and can thus only be
as good as those are.

By default, only the exit statuses of the test scripts are considered when
determining the testsuite outcome.  But Automake allows also the use of more
complex test protocols, either standard (*note Using the TAP test protocol::) or
custom (*note Custom Test Drivers::). In the rest of this section we are going
to concentrate mostly on protocol-less tests, since we cover test protocols in a
later section (again, *note Custom Test Drivers::).

<return-value>
When no test protocol is in use, an exit status of 0 from a test script will
denote a success, an exit status of 77 a skipped test, an exit status of 99 an
hard error, and any other exit status will denote a failure.

You may define the variable ‘XFAIL_TESTS’ to a list of tests (usually a subset
    of ‘TESTS’) that are expected to fail; this will effectively reverse the
result of those tests (with the provision that skips and hard errors remain
    untouched).  You may also instruct the testsuite harness to treat hard
errors like simple failures, by defining the ‘DISABLE_HARD_ERRORS’ make variable
to a nonempty value.

Note however that, for tests based on more complex test protocols, the exact
effects of ‘XFAIL_TESTS’ and ‘DISABLE_HARD_ERRORS’ might change, or they might
even have no effect at all (for example, in tests using TAP, there is not way to
    disable hard errors, and the ‘DISABLE_HARD_ERRORS’ variable has no effect on
    them).

The result of each test case run by the scripts in ‘TESTS’ will be 'printed' on
standard output, along with the test name.  For test protocols that allow more
test cases per test script (such as TAP), a number, identifier and/or brief
description specific for the single test case is expected to be printed in
addition to the name of the test script.  The possible results (whose meanings
    should be clear from the previous *note Generalities about Testing::) are
‘PASS’, ‘FAIL’, ‘SKIP’, ‘XFAIL’, ‘XPASS’ and ‘ERROR’.  Here is an example of
output from an hypothetical testsuite that uses both plain and TAP tests:

     PASS: foo.sh
     PASS: zardoz.tap 1 - Daemon started
     PASS: zardoz.tap 2 - Daemon responding
     SKIP: zardoz.tap 3 - Daemon uses /proc # SKIP /proc is not mounted
     PASS: zardoz.tap 4 - Daemon stopped
     SKIP: bar.sh
     PASS: mu.tap 1
     XFAIL: mu.tap 2 # TODO frobnication not yet implemented

A testsuite summary (expected to report at least the number of run, skipped and
    failed tests) will be printed at the end of the testsuite run.

If the standard output is connected to a capable terminal, then the test results
and the summary are colored appropriately.  The developer and the user can
disable colored output by setting the ‘make’ variable ‘AM_COLOR_TESTS=no’; the
user can in addition force colored output even without a connecting terminal
with ‘AM_COLOR_TESTS=always’.  It’s also worth noting that some ‘make’
implementations, when used in parallel mode, have slightly different semantics
(*note (autoconf)Parallel make::), which can break the automatic detection of a
connection to a capable terminal.  If this is the case, the user will have to
resort to the use of ‘AM_COLOR_TESTS=always’ in order to have the testsuite
output colorized.


Test programs that need data files should look for them in ‘srcdir’ (which is
    both a make variable and an environment variable made available to the
    tests), so that they work when building in a separate directory (*note Build
      Directories: (autoconf)Build Directories.), and in particular for the
    ‘distcheck’ rule (*note Checking the Distribution::).

The ‘AM_TESTS_ENVIRONMENT’ and ‘TESTS_ENVIRONMENT’ variables can be used to run
initialization code and set environment variables for the test scripts.  The
former variable is developer-reserved, and can be defined in the ‘Makefile.am’,
       while the latter is reserved for the user, which can employ it to extend
         or override the settings in the former; for this to work portably,
            however, the contents of a non-empty ‘AM_TESTS_ENVIRONMENT’ _must_
              be terminated by a semicolon.

The ‘AM_TESTS_FD_REDIRECT’ variable can be used to define file descriptor
redirections for the test scripts.  One might think that ‘AM_TESTS_ENVIRONMENT’
could be used for this purpose, but experience has shown that doing so portably
is practically impossible.  The main hurdle is constituted by Korn shells, which
usually set the close-on-exec flag on file descriptors opened with the ‘exec’
builtin, thus rendering an idiom like ‘AM_TESTS_ENVIRONMENT = exec 9>&2;’
ineffectual.  This issue also affects some Bourne shells, such as the HP-UX’s
‘/bin/sh’,

     AM_TESTS_ENVIRONMENT = \
     ## Some environment initializations are kept in a separate shell
     ## file 'tests-env.sh', which can make it easier to also run tests
     ## from the command line.
       . $(srcdir)/tests-env.sh; \
     ## On Solaris, prefer more POSIX-compliant versions of the standard
     ## tools by default.
       if test -d /usr/xpg4/bin; then \
         PATH=/usr/xpg4/bin:$$PATH; export PATH; \
       fi;
     ## With this, the test scripts will be able to print diagnostic
     ## messages to the original standard error stream, even if the test
     ## driver redirects the stderr of the test scripts to a log file
     ## before executing them.
     AM_TESTS_FD_REDIRECT = 9>&2

Note however that ‘AM_TESTS_ENVIRONMENT’ is, for historical and implementation
reasons, _not_ supported by the serial harness (*note Serial Test Harness::).

Automake ensures that each file listed in ‘TESTS’ is built 'before' it is run;
     you can list both source and derived programs (or scripts) in ‘TESTS’; the
       generated rule will look both in ‘srcdir’ and ‘.’.  For instance, you
       might want to run a C program as a test.  To do this you would list its
       name in ‘TESTS’ and also in ‘check_PROGRAMS’, and then specify it as you
       would any other program.

Programs listed in ‘check_PROGRAMS’ (and ‘check_LIBRARIES’,
    ‘check_LTLIBRARIES’...)  are only built 'during' ‘make check’, not during
‘make all’.  You should list there any program needed by your tests that does
'not' need to be built by ‘make all’.  Note that ‘check_PROGRAMS’ are _not_
automatically added to ‘TESTS’ because ‘check_PROGRAMS’ usually lists programs
used by the tests, not the tests themselves.  Of course you can set ‘TESTS =
$(check_PROGRAMS)’ if all your programs are test cases.


15.2.3 Parallel Test Harness
----------------------------

By default, Automake generated a parallel (concurrent) test harness.  It
features automatic collection of the test scripts output in ‘.log’ files,
         concurrent execution of tests with ‘make -j’, specification of
           inter-test dependencies, lazy reruns of tests that have not completed
           in a prior run, and hard errors for exceptional failures.

The parallel test harness operates by defining a set of ‘make’ rules that run
the test scripts listed in ‘TESTS’, and, for each such script, save its output
in a corresponding ‘.log’ file and its results (and other “metadata”, *note API
    for Custom Test Drivers::) in a corresponding ‘.trs’ (as in Test ReSults)
file.  The ‘.log’ file will contain all the output emitted by the test on its
standard output and its standard error.  The ‘.trs’ file will contain, among the
other things, the results of the test cases run by the script.

The parallel test harness will also create a summary log file, ‘TEST_SUITE_LOG’,
which defaults to ‘test-suite.log’ and requires a ‘.log’ suffix.  This file
  depends upon all the ‘.log’ and ‘.trs’ files created for the test scripts
  listed in ‘TESTS’.

As with the serial harness above, by default one status line is printed per
completed test, and a short summary after the suite has completed.  However,
standard output and standard error of the test are redirected to a per-test log
  file, so that parallel execution does not produce intermingled output.  The
  output from failed tests is collected in the ‘test-suite.log’ file.  If the
  variable ‘VERBOSE’ is set, this file is output after the summary.

Each couple of ‘.log’ and ‘.trs’ files is created when the corresponding test
has completed.  The set of log files is listed in the read-only variable
‘TEST_LOGS’, and defaults to ‘TESTS’, with the executable extension if any
(*note EXEEXT::), as well as any suffix listed in ‘TEST_EXTENSIONS’ removed, and
‘.log’ appended.  Results are undefined if a test file name ends in several
concatenated suffixes.  ‘TEST_EXTENSIONS’ defaults to ‘.test’; it can be
overridden by the user, in which case any extension listed in it must be
constituted by a dot, followed by a non-digit alphabetic character, followed by
any number of alphabetic characters.  For example, ‘.sh’, ‘.T’ and ‘.t1’ are
valid extensions, while ‘.x-y’, ‘.6c’ and ‘.t.1’ are not.

It is important to note that, due to current limitations (unlikely to be
    lifted), configure substitutions in the definition of ‘TESTS’ can only work
if they will expand to a list of tests that have a suffix listed in
  ‘TEST_EXTENSIONS’.

For tests that match an extension ‘.EXT’ listed in ‘TEST_EXTENSIONS’, you can
provide a custom “test runner” using the variable ‘EXT_LOG_COMPILER’ (note the
    upper-case extension) and pass options in ‘AM_EXT_LOG_FLAGS’ and allow the
user to pass options in ‘EXT_LOG_FLAGS’.  It will cause all tests with this
extension to be called with this runner.  For all tests without a registered
extension, the variables ‘LOG_COMPILER’, ‘AM_LOG_FLAGS’, and ‘LOG_FLAGS’ may be
used.  For example,

     TESTS = foo.pl bar.py baz
     TEST_EXTENSIONS = .pl .py
     PL_LOG_COMPILER = $(PERL)
     AM_PL_LOG_FLAGS = -w
     PY_LOG_COMPILER = $(PYTHON)
     AM_PY_LOG_FLAGS = -v
     LOG_COMPILER = ./wrapper-script
     AM_LOG_FLAGS = -d

will invoke ‘$(PERL) -w foo.pl’, ‘$(PYTHON) -v bar.py’, and ‘./wrapper-script -d
baz’ to produce ‘foo.log’, ‘bar.log’, and ‘baz.log’, respectively.  The
‘foo.trs’, ‘bar.trs’ and ‘baz.trs’ files will be automatically produced as a
side-effect.

It’s important to note that, differently from what we’ve seen for the serial
test harness (*note Serial Test Harness::), the ‘AM_TESTS_ENVIRONMENT’ and
‘TESTS_ENVIRONMENT’ variables _cannot_ be use to define a custom test runner;
     the ‘LOG_COMPILER’ and ‘LOG_FLAGS’ (or their extension-specific
         counterparts) should be used instead:

     ## This is WRONG!
     AM_TESTS_ENVIRONMENT = PERL5LIB='$(srcdir)/lib' $(PERL) -Mstrict -w

     ## Do this instead.
     AM_TESTS_ENVIRONMENT = PERL5LIB='$(srcdir)/lib'; export PERL5LIB;
     LOG_COMPILER = $(PERL)
     AM_LOG_FLAGS = -Mstrict -w

By default, the test suite harness will run all tests, but there are several
ways to 'limit' the set of tests that are run:

• You can set the ‘TESTS’ variable.  For example, you can use a command like
this to run only a subset of the tests:

          env TESTS="foo.test bar.test" make -e check

Note however that the command above will unconditionally overwrite the
‘test-suite.log’ file, thus clobbering the recorded results of any previous
testsuite run.  This might be undesirable for packages whose testsuite takes
long time to execute.  Luckily, this problem can easily be avoided by overriding
also ‘TEST_SUITE_LOG’ at runtime; for example,

          env TEST_SUITE_LOG=partial.log TESTS="..." make -e check

will write the result of the partial testsuite runs to the ‘partial.log’,
without touching ‘test-suite.log’.

• You can set the ‘TEST_LOGS’ variable.  By default, this variable is computed
at ‘make’ run time from the value of ‘TESTS’ as described above.  For example,
you can use the following:

          set x subset*.log; shift
          env TEST_LOGS="foo.log $*" make -e check

The comments made above about ‘TEST_SUITE_LOG’ overriding applies here too.

• By default, the test harness removes all old per-test ‘.log’ and ‘.trs’ files
before it starts running tests to regenerate them.  The variable ‘RECHECK_LOGS’
contains the set of ‘.log’ (and, by implication, ‘.trs’) files which are
removed.  ‘RECHECK_LOGS’ defaults to ‘TEST_LOGS’, which means all tests need to
be rechecked.  By overriding this variable, you can choose which tests need to
be reconsidered.  For example, you can lazily rerun only those tests which are
outdated, i.e., older than their prerequisite test files, by setting this
variable to the empty value:

          env RECHECK_LOGS= make -e check

• You can ensure that all tests are rerun which have failed or passed
unexpectedly, by running ‘make recheck’ in the test directory.  This convenience
target will set ‘RECHECK_LOGS’ appropriately before invoking the main test
harness.

In order to guarantee an ordering between tests even with ‘make -jN’,
dependencies between the corresponding ‘.log’ files may be specified through
  usual ‘make’ dependencies.  For example, the following snippet lets the test
  named ‘foo-execute.test’ depend upon completion of the test
  ‘foo-compile.test’:

     TESTS = foo-compile.test foo-execute.test
     foo-execute.log: foo-compile.log

Please note that this ordering ignores the _results_ of required tests, thus the
test ‘foo-execute.test’ is run even if the test ‘foo-compile.test’ failed or was
skipped beforehand.  Further, please note that specifying such dependencies
currently works only for tests that end in one of the suffixes listed in
‘TEST_EXTENSIONS’.

Tests without such specified dependencies may be run concurrently with parallel
‘make -jN’, so be sure they are prepared for concurrent execution.

The combination of lazy test execution and correct dependencies
between tests and their sources may be exploited for efficient unit
testing during development.  To further speed up the edit-compile-test
cycle, it may even be useful to specify compiled programs in
‘EXTRA_PROGRAMS’ instead of with ‘check_PROGRAMS’, as the former allows
intertwined compilation and test execution (but note that
‘EXTRA_PROGRAMS’ are not cleaned automatically, *note Uniform::).

The variables ‘TESTS’ and ‘XFAIL_TESTS’ may contain conditional parts as well as
configure substitutions.  In the latter case, however, certain restrictions
apply: substituted test names must end with a nonempty test suffix like ‘.test’,
so that one of the inference rules generated by ‘automake’ can apply.  For
  literal test names, ‘automake’ can generate per-target rules to avoid this
  limitation.

Please note that it is currently not possible to use ‘$(srcdir)/’ or
‘$(top_srcdir)/’ in the ‘TESTS’ variable.  This technical limitation is
necessary to avoid generating test logs in the source tree and has the
unfortunate consequence that it is not possible to specify distributed tests
that are themselves generated by means of explicit rules, in a way that is
portable to all ‘make’ implementations (*note (autoconf)Make Target Lookup::,
    the semantics of FreeBSD and OpenBSD ‘make’ conflict with this).  In case of
doubt you may want to require to use GNU ‘make’, or work around the issue with
inference rules to generate the tests.



={============================================================================
*kt_linux_tool_160* ccache

The ccache tool tries to speed up a build by taking object files from cache when
the preprocessed source didn't change. In addition, it can often avoid
preprocessing, as a cache lookup on the source file path and compiler options
returns a list of include files and their modification times. If those weren't
updated, that counts as a 'direct hit'. Otherwise it falls back to preprocessing
the source and trying to look-up its hash in the cache. If successful, that
counts as a 'preprocessed hit'.

<problem>
The main barrier preventing cache sharing across git branches has to do with the fact we create
separate build slaves for each git branch, which results in different source and binary directories
being seen by the compiler. More specifically:

The compiler sets the __FILE__ macro to be the absolute path to the source file. The value of that
macro leaks to preprocessed source through the use of assert().

We pass preprocessor definitions like -DMACRO__prefix="...", -DMACRO__builddir="...",
   -DMACRO__top_srcdir="...", which can also leak to preprocessed source.

note: After all, like to have speed-up when switching between branches since the most are the same
between them.

branch 01                                    branch 02                     
 source dirs, build root dirs                 source dirs, build root dirs

             -> maps                             <- maps to virtual
                        _virtual_

So it is not to save space but to make ccache see _virtual_ when switching branches so that have
more cache hit.

To address this issue, the new zb-virtual-slave tool was developed. It's meant to wrap around other
build tools, such as zb-build-with-progress, zb-deploy, zb-shell, etc. and leading them to believe
we are always building a branch called _virtual_. This way, the compiler will see identical source
and binary locations, enabling cache hits across different git branches. zb-virtual-slave uses the
map-dir-and-exec tool internally, which you will have to build and install yourself.

note: need to install map-dir-and-exec package

CCACHE_DIR

The CCACHE_DIR environment variable specifies where ccache will keep its cached
compiler output. The default is $HOME/.ccache.

You can monitor the current usage of ccache by executing

CCACHE_DIR=.. $watch -n1 -d ccache -s


={============================================================================
*kt_linux_tool_161* icecream

icecream is a better version of distcc - a tool for distributing build jobs
across multiple machines.

It has a central scheduler (running `icecc-scheduler`) which runs on some server
and one or more nodes (machines running `iceccd`). The scheduler distributes
jobs depending on the load and speed of nodes - always picking the fastest and
least-loaded (preferring localhost when load is low).

One nice thing is it lets you specify what toolchain you are using and icecream
will distribute your toolchain for you so the remote machine can build
compatible object files.

 
note: do all as a root

Building icecream
-----------------
To build icecream the following worked for me, putting all files under
/opt/icecream


darren@djg-ubuntu:/data/software/icecream/build$ cat /data/documents/recipes/icecream.sh

#!/bin/bash
 
# Get source
git clone https://github.com/icecc/icecream.git
cd icecream
 
# Get dependencies
sudo apt-get install -y libcap-ng-dev liblzo2-dev docbook2x

or

sudo apt-get install -y --force-yes libcap-ng-dev liblzo2-dev docbook2x
 
# Build it
mkdir build
cd build
../autogen.sh
../configure --with-pic --prefix=/opt/icecream
make -j8
sudo make install

note: should run build on /opt/icecream but not /opt/icecream/build since fails
to search headers when building.


Preparing the Toolchain
-----------------------
toolchainTarball=$(/opt/icecream/bin/icecc --build-native | grep creating | cut -d' ' -f2)
sudo mkdir -p /opt/icecream/etc/
sudo cp -a $toolchainTarball /opt/icecream/etc/gcc-native.tar.gz


Preparing the Scheduler Node
-----------------------
Just pick a fast server for this. The scheduler doesn't appear to use much in
the way of resources. Then run: 

/opt/icecream/sbin/icecc-scheduler -d


Preparing Client Nodes
-----------------------

Add the icecc user (`sudo useradd icecc`).
Run the iceccd daemon (`sudo /opt/icecream/sbin/iceccd -d`). Note that it drops
privileges after chrooting.

If you're on Debian, this should work for you:

sudo apt-get install liblzo2-d libcap-ng-dev
curl devnfs2/~darren.garvey/icecream-debian.tar.bz2 | sudo tar -C / -xjf - && sudo useradd icecc && sudo /opt/icecream/sbin/iceccd -d

note: For debian whiz: sudo apt-get install liblzo2-2 libcap-ng-dev


={============================================================================
*kt_linux_tool_200* bin-:

{gcc-binutil}
https://sourceware.org/binutils/
http://sourceware.org/binutils/docs-2.20/
http://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html

GCC provides a large number of built-in functions other than the ones mentioned
above. Some of these are for internal use in the processing of exceptions or
variable-length argument lists and are not documented here because they may
change from time to time; we do not recommend general use of these functions. 


={============================================================================
*kt_linux_tool_201* tool-readelf

Program Header in ELF

Executable and shared object files statically represent programs. To execute
such programs, the system uses the files to create dynamic program
representations, or process images. A process image has segments that hold its
text, data, stack, and so on. The major sections in this part discuss the
following.

The primary data structure, a program header table, locates segment images
within the file and contains other information necessary to create the memory
image for the program. Each entry describing a segment or other information the
system needs to prepare the program for execution. An object file segment
contains one or more sections.

Program headers are meaningful only for executable and shared object files.

<all-headers>

       -a
       --all
           Equivalent to specifying --file-header, --program-headers,
           --sections, --symbols, --relocs, --dynamic, --notes 
           and --version-info.

-e : headers
-e --headers           Equivalent to: -h -l -S
  -h --file-header       Display the ELF file header
  -l --program-headers   Display the program headers
     --segments          An alias for --program-headers
  -S --section-headers   Display the sections' header
     --sections          An alias for --section-headers

# mipsel-linux-uclibc-readelf -e vmlinux
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           MIPS R3000
  Version:                           0x1
  Entry point address:               0x802de000
  Start of program headers:          52 (bytes into file)
  Start of section headers:          3158368 (bytes into file)
  Flags:                             0x50001001, noreorder, o32, mips32
  Size of this header:               52 (bytes)
  Size of program headers:           32 (bytes)
  Number of program headers:         1
  Size of section headers:           40 (bytes)
  Number of section headers:         21
  Section header string table index: 18

Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .text             PROGBITS        80001000 001000 26f89c 00  AX  0   0 2048
  [ 2] __ex_table        PROGBITS        802708a0 2708a0 001a68 00   A  0   0  4
  [ 3] .rodata           PROGBITS        80272310 272310 02b8e0 00   A  0   0 16
  [ 4] .pci_fixup        PROGBITS        8029dbf0 29dbf0 0002e0 00   A  0   0  4
  [ 5] __ksymtab         PROGBITS        8029ded0 29ded0 0039a0 00   A  0   0  4
  [ 6] __ksymtab_gpl     PROGBITS        802a1870 2a1870 000678 00   A  0   0  4
  [ 7] __ksymtab_strings PROGBITS        802a1ee8 2a1ee8 009080 00   A  0   0  4
  [ 8] __param           PROGBITS        802aaf68 2aaf68 0001f4 00   A  0   0  4
  [ 9] .data             PROGBITS        802ac000 2ac000 02fa10 00  WA  0   0 8192
  [10] .data.cacheline_a PROGBITS        802dc000 2dc000 0010c0 00  WA  0   0 32
  [11] .init.text        PROGBITS        802de000 2de000 021bbc 00  AX  0   0  4
  [12] .init.data        PROGBITS        802ffbc0 2ffbc0 0025e4 00  WA  0   0  8
  [13] .init.setup       PROGBITS        803021b0 3021b0 0001d4 00  WA  0   0  4
  [14] .initcall.init    PROGBITS        80302384 302384 000168 00  WA  0   0  4
  [15] .con_initcall.ini PROGBITS        803024ec 3024ec 000004 00  WA  0   0  4
  [16] .init.ramfs       PROGBITS        80303000 303000 000085 00   A  0   0  1
  [17] .bss              NOBITS          80304000 303085 024020 00  WA  0   0 4096
  [18] .shstrtab         STRTAB          00000000 303085 0000d8 00      0   0  1
  [19] .symtab           SYMTAB          00000000 3034a8 03a800 10     20 10448  4
  [20] .strtab           STRTAB          00000000 33dca8 044b17 00      0   0  1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings)
  I (info), L (link order), G (group), x (unknown)
  O (extra OS processing required) o (OS specific), p (processor specific)

Program Headers:
  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
  LOAD           0x001000 0x80001000 0x80001000 0x302085 0x327020 RWE 0x2000

 Section to Segment mapping:
  Segment Sections...
   00     .text __ex_table .rodata .pci_fixup __ksymtab __ksymtab_gpl
   __ksymtab_strings __param .data .data.cacheline_aligned .init.text .init.data
   .init.setup .initcall.init .con_initcall.init .init.ramfs .bss


# mipsel-linux-uclibc-readelf -e xx.a
File: /home/NDS-UK/parkkt/platforms/mstar/libverifier.a(pairglbo.o)
ELF Header:
  Magic:   7f 45 4c 46 01 02 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, big endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              REL (Relocatable file)
  Machine:                           MIPS R3000
  Version:                           0x1
  Entry point address:               0x0
  Start of program headers:          0 (bytes into file)
  Start of section headers:          5304 (bytes into file)
  Flags:                             0x70001005, noreorder, cpic, o32, mips32r2
  Size of this header:               52 (bytes)
  Size of program headers:           0 (bytes)
  Number of program headers:         0
  Size of section headers:           40 (bytes)
  Number of section headers:         14
  Section header string table index: 11

Section Headers:

There are no program headers in this file.

File: /home/NDS-UK/parkkt/platforms/mstar/libverifier.a(p_posix.o)
ELF Header:
  Magic:   7f 45 4c 46 01 02 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF32
  Data:                              2's complement, big endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              REL (Relocatable file)
  Machine:                           MIPS R3000
  Version:                           0x1
  Entry point address:               0x0
  Start of program headers:          0 (bytes into file)
  Start of section headers:          5180 (bytes into file)
  Flags:                             0x70001005, noreorder, cpic, o32, mips32r2
  Size of this header:               52 (bytes)
  Size of program headers:           0 (bytes)
  Number of program headers:         0
  Size of section headers:           40 (bytes)
  Number of section headers:         15
  Section header string table index: 12

Section Headers:


<speficy-section>
mipsel-linux-uclibc-readelf -x 13 vmlinux

<sections>
The .text section contains the executable program code. 

The .rodata section contains constant data in your program. 

The .data section generally contains initialized global data used by the C
library prologue code and can contain large initialized data items from your
application.  The .sdata section is used for smaller initialized global data
items and exists only on some architectures. Some processor architectures can
make use of optimized data access when the attributes of the memory area are
known. The .sdata and .sbss sections enable these optimizations. 

The .bss and .sbss sections contain uninitialized (global) data in your program.
These sections occupy no space in the program image their memory space is
allocated and initialized to zero on program startup by C library prologue code.


{example-analysis}
/*
** This is sample program to see how elfs is made and allocated
**
** ktpark
*/

/* bss */
int kt_bss_vars[100];

/* data */
int kt_data_vars[100]={0x01};

/* constant */
char* kt_const = "this is constant string array.\n";

int main( int argc, char** argv )
{
  int kt_local_bss_vars[100];
  int kt_local_data_vars[100]={0xFF};
  char* kt_local_const = "this is local constant string array.\n";
  int i;

  printf("\n\n this is sample program to see elf.\n\n");

  for(i=0; i <= 10;i++)
  {
    sleep(1000);
  }

  printf("\nend of program.\n");
}
--

# free (before)
              total         used         free       shared      buffers
  Mem:       116472        18052        98420            0         8192
 Swap:            0            0            0
Total:       116472        18052        98420

# free (after)
              total         used         free       shared      buffers
  Mem:       116472        18068        98404            0         8192
 Swap:            0            0            0
Total:       116472        18068        98404

16K used.

<read-sections>
-S
--sections
--section-headers

Displays the information contained in the file's section headers, if it has any.


$ readelf -S a.out 
There are 26 section headers, starting at offset 0xfac:

Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .interp           PROGBITS        00400114 000114 000014 00   A  0   0  1
  [ 2] .reginfo          MIPS_REGINFO    00400128 000128 000018 18   A  0   0  4
  [ 3] .dynamic          DYNAMIC         00400140 000140 0000d8 08   A  6   0  4
  [ 4] .hash             HASH            00400218 000218 0000a4 04   A  5   0  4
  [ 5] .dynsym           DYNSYM          004002bc 0002bc 000160 10   A  6   1  4
  [ 6] .dynstr           STRTAB          0040041c 00041c 0000eb 00   A  0   0  1
  [ 7] .init             PROGBITS        00400508 000508 00008c 00  AX  0   0  4
  [ 8] .text             PROGBITS        004005a0 0005a0 000320 00  AX  0   0 16 (800)
  [ 9] .MIPS.stubs       PROGBITS        004008c0 0008c0 000050 00  AX  0   0  4
  [10] .fini             PROGBITS        00400910 000910 000050 00  AX  0   0  4
  [11] .rodata           PROGBITS        00400960 000960 000220 00   A  0   0 16 (544)
  [12] .eh_frame         PROGBITS        00400b80 000b80 000004 00   A  0   0  4
  [13] .ctors            PROGBITS        00410b84 000b84 000008 00  WA  0   0  4
  [14] .dtors            PROGBITS        00410b8c 000b8c 000008 00  WA  0   0  4
  [15] .jcr              PROGBITS        00410b94 000b94 000004 00  WA  0   0  4
  [16] .data             PROGBITS        00410ba0 000ba0 0001d0 00  WA  0   0 16 (464)
  [17] .rld_map          PROGBITS        00410d70 000d70 000004 00  WA  0   0  4
  [18] .got              PROGBITS        00410d80 000d80 00004c 04 WAp  0   0 16
  [19] .bss              NOBITS          00410dd0 000dcc 0001b0 00  WA  0   0 16 (432)
  [20] .comment          PROGBITS        00000000 000dcc 00005a 00      0   0  1
  [21] .mdebug.abi32     PROGBITS        0000005a 000e26 000000 00      0   0  1
  [22] .pdr              PROGBITS        00000000 000e28 0000c0 00      0   0  4
  [23] .shstrtab         STRTAB          00000000 000ee8 0000c3 00      0   0  1
  [24] .symtab           SYMTAB          00000000 0013bc 0004b0 10     25  46  4
  [25] .strtab           STRTAB          00000000 00186c 000254 00      0   0  1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings)
  I (info), L (link order), G (group), x (unknown)
O (extra OS processing required) o (OS specific), p (processor specific)

<read-rodata>
$ readelf -x 11 a.out (rodata)

Hex dump of section '.rodata':
  0x00400960 746e6174 736e6f63 20736920 73696874 this is constant
  0x00400970 000a2e79 61727261 20676e69 72747320  string array...
  0x00400980 00000000 00000000 00000000 000000ff ................
  0x00400990 00000000 00000000 00000000 00000000 ................
  0x004009a0 00000000 00000000 00000000 00000000 ................
  0x004009b0 00000000 00000000 00000000 00000000 ................
  0x004009c0 00000000 00000000 00000000 00000000 ................
  0x004009d0 00000000 00000000 00000000 00000000 ................
  0x004009e0 00000000 00000000 00000000 00000000 ................
  0x004009f0 00000000 00000000 00000000 00000000 ................
  0x00400a00 00000000 00000000 00000000 00000000 ................
  0x00400a10 00000000 00000000 00000000 00000000 ................
  0x00400a20 00000000 00000000 00000000 00000000 ................
  0x00400a30 00000000 00000000 00000000 00000000 ................
  0x00400a40 00000000 00000000 00000000 00000000 ................
  0x00400a50 00000000 00000000 00000000 00000000 ................
  0x00400a60 00000000 00000000 00000000 00000000 ................
  0x00400a70 00000000 00000000 00000000 00000000 ................
  0x00400a80 00000000 00000000 00000000 00000000 ................
  0x00400a90 00000000 00000000 00000000 00000000 ................
  0x00400aa0 00000000 00000000 00000000 00000000 ................
  0x00400ab0 00000000 00000000 00000000 00000000 ................
  0x00400ac0 00000000 00000000 00000000 00000000 ................
  0x00400ad0 00000000 00000000 00000000 00000000 ................
  0x00400ae0 00000000 00000000 00000000 00000000 ................
  0x00400af0 00000000 00000000 00000000 00000000 ................
  0x00400b00 00000000 00000000 00000000 00000000 ................
  0x00400b10 6f63206c 61636f6c 20736920 73696874 this is local co
  0x00400b20 72612067 6e697274 7320746e 6174736e nstant string ar
  0x00400b30 20736968 74200a0a 0000000a 2e796172 ray....... this 
  0x00400b40 6172676f 72702065 6c706d61 73207369 is sample progra
  0x00400b50 000a0a2e 666c6520 65657320 6f74206d m to see elf....
  0x00400b60 2e6d6172 676f7270 20666f20 646e650a .end of program.
  0x00400b70 00000000 00000000 00000000 0000000a ................


{example-analysis}
#include <stdio.h>

int bss_var; /* Uninitialized global variable */
int data_var = 1; /* Initialized global variable */

int main(int argc, char **argv)
{
  void *stack_var; /* Local variable on the stack */
  stack_var = (void *)main; /* Don't let the compiler optimize it out */

  printf("Hello, World! Main is executing at %p\n", stack_var);
  printf("This address (%p) is in our stack frame\n", &stack_var);

  /* bss section contains uninitialized data */
  printf("This address (%p) is in our bss section\n", &bss_var);

  /* data section contains initializated data */
  printf("This address (%p) is in our data section\n", &data_var);

  return 0;
}

root@amcc:~# ./hello
Hello, World! Main is executing at 0x10000418
This address (0x7ff8ebb0) is in our stack frame
This address (0x10010a1c) is in our bss section
This address (0x10010a18) is in our data section


{section-and-nm-map}
(from the readelf of kernel)
Section Headers:
  [Nr] Name              Type            Addr     Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            00000000 000000 000000 00      0   0  0
  [ 1] .init             PROGBITS      40018000 008000 01c000 00 WAX  0   0 32
  [ 2] .text             PROGBITS      40034000 024000 2af998 00  AX  0   0 32
  [ 3] .pci_fixup        PROGBITS        402e4000 2d4000 000490 00   A  0   0  4
  [ 4] __ksymtab         PROGBITS        402e4490 2d4490 004020 00   A  0   0  4
  [ 5] __ksymtab_gpl     PROGBITS        402e84b0 2d84b0 000f40 00   A  0   0  4
  [ 6] __ksymtab_gpl_fut PROGBITS        402e93f0 2d93f0 000018 00   A  0   0  4
  [ 7] __ksymtab_strings PROGBITS        402e9408 2d9408 00b4d8 00   A  0   0  4
  [ 8] __param           PROGBITS        402f5000 2e5000 0004b0 00   A  0   0  4
  [ 9] .data             PROGBITS        402f8000 2e8000 05c210 00  WA  0   0 32
  [10] .bss              NOBITS          40354220 344210 020438 00  WA  0   0 32
  [11] .comment          PROGBITS        00000000 344210 002e68 00      0   0  1
  [12] .ARM.attributes   ARM_ATTRIBUTES  00000000 347078 000010 00      0   0  1
  [13] .debug_abbrev     PROGBITS        00000000 347088 0b55ee 00      0   0  1
  [14] .debug_info       PROGBITS        00000000 3fc676 16124fa 00      0   0  1
  [15] .debug_line       PROGBITS        00000000 1a0eb70 17d764 00      0   0  1
  [16] .debug_pubnames   PROGBITS        00000000 1b8c2d4 0210e5 00      0   0  1
  [17] .debug_str        PROGBITS        00000000 1bad3b9 09173a 01  MS  0   0  1
  [18] .debug_aranges    PROGBITS        00000000 1c3eaf3 006408 00      0   0  1
  [19] .debug_frame      PROGBITS        00000000 1c44efc 05b4b0 00      0   0  4
  [20] .debug_loc        PROGBITS        00000000 1ca03ac 23ee57 00      0   0  1
  [21] .debug_ranges     PROGBITS        00000000 1edf203 069400 00      0   0  1
  [22] .shstrtab         STRTAB          00000000 1f48603 000113 00      0   0  1
  [23] .symtab           SYMTAB          00000000 1f48b00 07f8a0 10     24 26436  4
  [24] .strtab           STRTAB          00000000 1fc83a0 05e3c9 00      0   0  1
  
(from the map)
0000000040034000 T _text
00000000402f54b0 A _etext
00000000402f54b0-0000000040034000=0x2C14B0(2,888,880)

Why these are different in size??

<dynamic>
  -d --dynamic           Display the dynamic section (if present)


={============================================================================
*kt_linux_tool_202* tool-nm-binutil

%nm *.a

%/opt/toolchains/bin/mipsel-linux-uclibc-nm
%nm libicammulti.a | grep vendor

00000bc0 t _GLOBAL__I_vendor_init
00000980 T vendor_cleanup
00000000 T vendor_init
000002c8 T vendor_setup
000000bc r _ZZ12vendor_setupE12__FUNCTION__
00000088 r _ZZ12vendor_setupE19__PRETTY_FUNCTION__


$ nm -A /usr/lib/lib*.so 2> /dev/null | grep ' crypt$'
/usr/lib/libcrypt.so:00007080 W crypt

<demangle>
       -C
       --demangle[=style]
           Decode (demangle) low-level symbol names into user-level names.
           Besides removing any initial underscore prepended by the system,
this makes C++ function names readable.  Different compilers have different
  mangling styles. The optional demangling style argument can be used to
  choose an appropriate demangling style for your compiler.


-A
-o
--print-file-name
Precede each symbol by the name of the input file (or archive member) in which
it was found, rather than identifying the input file once only, before all of
its symbols.

`uppercase` means global and `lowercase` means local. 

B        is bss
T/t      is text and means symbols defined in this object file. 
D        data
A        is this address is absolute and is not subject to modification by an
         additional link stage

`U`        is `undefined` meaning extern

"W"
"w" The symbol is a weak symbol that has not been specifically tagged as a weak
object symbol. When a weak defined symbol is linked with a normal defined
symbol, the normal defined symbol is used with no error. When a weak undefined
symbol is linked and the symbol is not defined, the value of the symbol is
determined in a system-specific manner without error. On some systems, uppercase
indicates that a default value has been specified.


       --defined-only
           Display only defined symbols for each object file.

<on-shared-stripped>

       -D
       --dynamic
           Display the dynamic symbols rather than the normal symbols.  This is
           only meaningful for dynamic objects, such as certain types of shared
           libraries.

note: this shows only dynamic symbols and shows a smaller set than running it
without option.

$ nm libcvms.so.1.0
nm: libcvms.so.1.0: no symbols

$ nm -D libcvms.so.1.0
000add7c T APUC_Decompress
000831f8 T AP_AM_AfterFormatProcessingHDD
00081e04 T AP_AM_BeforeFormatProcessingHDD
000821f4 T AP_AM_ChangeChasePbToNormalPb
...


<ex>
$ nm libhttp_curl.a | grep NDS_HTTP_strdup
279:         U NDS_HTTP_strdup
2169:         U NDS_HTTP_strdup
2170:00000000 T NDS_HTTP_strdup_DEBUG

$ nm libhttp_upset.a | grep NDS_HTTP_strdup
12:00000000 T NDS_HTTP_strdup


<demangle>
       --demangle[=style]
           Decode (demangle) low-level symbol names into user-level names.
           Besides removing any initial underscore prepended by the system, this
           makes C++ function names readable. Different compilers have different
           mangling styles. The optional demangling style argument can be used
           to choose an appropriate demangling style for your compiler.


<map-file>
Have statically linked application binary, can check if there is a symbol by
using nm. However, cannot tell from where this symbol. How?

Use map file for that application and this will show:

  Archive member included because of file (symbol)

  //BigEndian/release_dbg//components/media_services/asm/src/libasm.a(asm_version.o)
  //BigEndian/release_dbg/components/../processes/mw_process_main.o (ASM_GetVersionString)

  49629  .text.Curl_cookie_add
  49630                 0x000000000076cb40      0xb5c /FUSION_SYSTEM_INTEGRATION/BSKYB_INTEGRATION
    /build/picasso/AMS_BCM_MIPS4K_LNUX_DARWIN_01/BigEndian/release_dbg//components/misc_libraries/http/src/libhttp.a(cookie.o)

  49631                 0x000000000076cb40                Curl_cookie_add


={============================================================================
*kt_linux_tool_203* tool-objdump-binutil

gcc -S
objdump -d ELF > out.txt

note: difference between gcc -S and objdump? objdump do on objects after
complilation.

       -d
       --disassemble
           Display the assembler mnemonics for the machine instructions from
           objfile.  This option only disassembles those sections which are
           expected to contain instructions.

       -S
       --source
           Display source code intermixed with disassembly, if possible.
           Implies -d.

       -T
       --dynamic-syms
           Print the dynamic symbol table entries of the file.  This is only
           meaningful for dynamic objects, such as certain types of shared
           libraries.  This is similar to the information provided by the nm
           program when given the -D (--dynamic) option.


={============================================================================
*kt_linux_tool_204* tool-addr2line

Usage: mipsel-linux-uclibc-addr2line [option(s)] [addr(s)]

Convert addresses into line number/file name pairs. If no addresses are
specified on the command line, they will be read from stdin The options are:

  @<file>                Read options from <file>
  -b --target=<bfdname>  Set the binary file format
  -i --inlines           Unwind inlined functions
  -j --section=<name>    Read section-relative offsets instead of addresses
  
  -e --exe=<executable>  Set the input file name (default is a.out)
  -s --basenames         Strip directory names
  -f --functions         Show function names
  -C --demangle[=style]  Demangle function names

  -h --help              Display this information
  -v --version           Display the program's version

./mipsel-linux-uclibc-addr2line: supported targets: elf32-tradlittlemips
elf32-tradbigmips ecoff-littlemips ecoff-bigmips elf32-ntradlittlemips
elf64-tradlittlemips elf32-ntradbigmips elf64-tradbigmips elf64-little
elf64-big elf32-little elf32-big srec symbolsrec tekhex binary ihex Report
bugs to URL:http://www.sourceware.org/bugzilla/

If the file name or function name can not be determined, addr2line will print
two question marks in their place. 

If the line number can not be determined, addr2line will print 0

mips-uclibc-addr2line -f -s -e fs/NDS/bin/MW_Process 5dd134 5dbe00 5d3ee4 5d6cb4 5ddb64 5cef3c a5f298   


={============================================================================
*kt_linux_tool_205* binutil: ld

https://sourceware.org/binutils/docs/ld/index.html#Top
This file documents the gnu linker ld (GNU Binutils) version 2.25. 

<debian>
keitee@debian-keitee:~/work/xxx$ which ld
/usr/bin/ld
keitee@debian-keitee:~/work/xxx$ ld --version
GNU ld (GNU Binutils for Debian) 2.22
Copyright 2011 Free Software Foundation, Inc.


={============================================================================
*kt_linux_tool_250* tool-busybox

http://www.busybox.net/source.html
http://www.busybox.net/downloads/BusyBox.html
https://git.busybox.net/busybox/tree/coreutils

# ============================================================================
#{ apache-ivy
={============================================================================
*kt_linux_tool_300* tool-apache-ivy

http://ant.apache.org/ivy/history/latest-milestone/index.html

What is Ivy?

Ivy is a tool for managing (recording, tracking, resolving and reporting)
project dependencies. It is characterized by the following:

    flexibility and configurability - Ivy is essentially process agnostic and
    is not tied to any methodology or structure. Instead it provides the
    necessary flexibility and configurability to be adapted to a broad range
    of dependency management and build processes.

    
    tight integration with Apache Ant - while available as a standalone tool,
Ivy works particularly well with Apache Ant providing a number of powerful Ant
  tasks ranging from dependency resolution to dependency reporting and
  publication.

Ivy is open source and released under a very permissive Apache License.

Ivy has a lot of powerful Features, the most popular and useful being its
flexibility, integration with ant, and its strong transitive dependencies
management engine.

The transitive dependencies management is a feature which lets you get
dependencies of your dependencies, transitively. In order to address this
general problem, ivy needs to find metadata about your modules, usually in an
ivy file. To find the metadata and your dependencies' artifacts (usually
    jars), Ivy can be configured to use a lot of different repositories.


http://ant.apache.org/ivy/history/latest-milestone/tutorial/start.html

Quick Start

In this tutorial, you will see one of the simplest ways to use Ivy. With no
specific settings, Ivy uses the maven 2 repository to resolve the dependencies
you declare in an Ivy file. Let's have a look at the content of the files
involved.

You'll find this tutorial's sources in the ivy distribution in the
src/example/hello-ivy directory.


The ivy.xml file

This file is used to describe the dependencies of the project on other
libraries. Here is the sample:

<ivy-module version="2.0">
    <info organisation="org.apache" module="hello-ivy"/>
    <dependencies>
        <dependency org="commons-lang" name="commons-lang" rev="2.0"/>
        <dependency org="commons-cli" name="commons-cli" rev="1.0"/>
    </dependencies>
</ivy-module>

The format of this file should pretty easy to understand, but let's give some
details about what is declared here. First, the root element ivy-module, with
the version attribute used to tell Ivy which version of Ivy this file uses.

Then there is an info tag, which is used to give information about the module
for which we are defining dependencies. Here we define only the organization
  and module name. You are free to choose whatever you want for them, but we
    recommend avoiding spaces for both.

Finally, the dependencies section lets you define dependencies. Here this
module `depends on two libraries`: commons-lang and commons-cli. 

As you can see, we use the `org` and `name` attributes to define the organization
and module name of the dependencies we need. The `rev` attribute is used to
specify the version of the module you depend on.

`To know what to put in these attributes`, you need to know the exact
information for the libraries you depend on. Ivy uses the maven 2 repository
by default, so we recommend you use mvnrepository.com to look for the module
you want. Once you find it, you will have the details on how to declare the
dependency in a maven POM. For instance:

<dependency>
    <groupId>commons-lang</groupId>
    <artifactId>commons-lang</artifactId>
    <version>2.0</version>
</dependency>

To convert this into an Ivy dependency declaration, all you have to do is use
the groupId as organization, the artifactId as module name, and the version as
revision. That's what we did for the dependencies in this tutorial, that is
commons-lang and commons-cli. 

Note that having commons-lang and commons-cli as organization is not the best
example of what the organization should be. It would be better to use
org.apache, org.apache.commons or org.apache.commons.lang. However, this is
how these modules are identified in the maven 2 repository, so the simplest
way to get them is to use the details as is (you will see in Building a
    repository that you can use namespaces to redefine these names if you want
    something cleaner).

If you want more details on what you can do in Ivy files, you can have a look
at the Ivy files reference documentation.


The build.xml file

The corresponding build file contains a set of targets, allowing you to
resolve dependencies declared in the Ivy file, to compile and run the sample
code, produce a report of dependency resolution, and clean the cache or the
project.  You can use the standard "ant -p" to get the list of available
targets. Feel free to have a look at the whole file, but here is the part
relevant to dependency resolution:

<project xmlns:ivy="antlib:org.apache.ivy.ant" name="hello-ivy" default="run">
    
    ...
    
    <!-- ================================= 
          target: resolve              
         ================================= -->
    <target name="resolve" description="--> retrieve dependencies with ivy">
        <ivy:retrieve />
    </target>
</project>


As you can see, it's very easy to call Ivy to resolve and retrieve
dependencies: all you need if Ivy is properly installed is to define an XML
namespace in your Ant file (xmlns:ivy="antlib:org.apache.ivy.ant"). Then all
the Ivy ant tasks will be available in this namespace.

Here we use only one task: the retrieve task. With no attributes, it will use
default settings and look for a file named ivy.xml for the dependency
definitions. That's exactly what we want, so we need nothing more than that.

Note that in this case we define a resolve target and call the retrieve task.
This may sound confusing, actually the retrieve task performs a resolve (which
    resolves dependencies and downloads them to a cache) followed by a
retrieve (a copy of those file to a local project directory). Check the How
does it work ? page for details about that.  


Running the project 

OK, now that we have seen the files involved, let's run the sample to see what
happens. Open a shell (or command line) window, and enter the hello-ivy
example directory.  Then, at the command prompt, run ant:


kyoupark@ukstbuild2:~/ivy-tut/ex/ant-ivy-b8b77a2$ ant
Buildfile: build.xml

resolve:
[ivy:retrieve] impossible to define new type: class not found: org.apache.ivy.plugins.signer.bouncycastle.OpenPGPSignatureGenerator in [] nor Ivy classloader
[ivy:retrieve] :: Apache Ivy 2.3.0-local-20131108183505 - 20131108183505 :: http://ant.apache.org/ivy/ ::
[ivy:retrieve] :: loading settings :: url = jar:file:/usr/share/java/ivy-2.3.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[ivy:retrieve] :: resolving dependencies :: org.apache#hello-ivy;working@ukstbuild2.uk.nds.com
[ivy:retrieve]  confs: [default]
[ivy:retrieve]  found commons-lang#commons-lang;2.0 in public
[ivy:retrieve]  found commons-cli#commons-cli;1.0 in public
[ivy:retrieve]  found commons-logging#commons-logging;1.0 in public
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-lang/commons-lang/2.0/commons-lang-2.0-javadoc.jar ...
[ivy:retrieve] ............................................
[ivy:retrieve] ................................................................. (467kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-lang#commons-lang;2.0!commons-lang.jar(javadoc) (2552ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-lang/commons-lang/2.0/commons-lang-2.0-sources.jar ...
[ivy:retrieve] ......................... (245kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-lang#commons-lang;2.0!commons-lang.jar(source) (762ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-lang/commons-lang/2.0/commons-lang-2.0.jar ...
[ivy:retrieve] ............... (165kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-lang#commons-lang;2.0!commons-lang.jar (31756ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-cli/commons-cli/1.0/commons-cli-1.0-javadoc.jar ...
[ivy:retrieve] ......... (92kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-cli#commons-cli;1.0!commons-cli.jar(javadoc) (585ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-cli/commons-cli/1.0/commons-cli-1.0.jar ...
[ivy:retrieve] .. (29kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-cli#commons-cli;1.0!commons-cli.jar (769ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-cli/commons-cli/1.0/commons-cli-1.0-sources.jar ...
[ivy:retrieve] ..... (48kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-cli#commons-cli;1.0!commons-cli.jar(source) (318ms)
[ivy:retrieve] downloading http://repo1.maven.org/maven2/commons-logging/commons-logging/1.0/commons-logging-1.0.jar ...
[ivy:retrieve] .. (21kB)
[ivy:retrieve] .. (0kB)
[ivy:retrieve]  [SUCCESSFUL ] commons-logging#commons-logging;1.0!commons-logging.jar (506ms)
[ivy:retrieve] :: resolution report :: resolve 1852ms :: artifacts dl 37272ms
[ivy:retrieve]  :: evicted modules:
[ivy:retrieve]  commons-lang#commons-lang;1.0 by [commons-lang#commons-lang;2.0] in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   4   |   3   |   3   |   1   ||   7   |   7   |
        ---------------------------------------------------------------------
[ivy:retrieve] :: retrieving :: org.apache#hello-ivy
[ivy:retrieve]  confs: [default]
[ivy:retrieve]  7 artifacts copied, 0 already retrieved (1069kB/19ms)

run:
    [mkdir] Created dir: /home/NDS-UK/kyoupark/ivy-tut/ex/ant-ivy-b8b77a2/build
    [javac] Compiling 1 source file to /home/NDS-UK/kyoupark/ivy-tut/ex/ant-ivy-b8b77a2/build
     [java] standard message : hello ivy !
     [java] capitalized by org.apache.commons.lang.WordUtils : Hello Ivy !

BUILD SUCCESSFUL
Total time: 40 seconds
kyoupark@ukstbuild2:~/ivy-tut/ex/ant-ivy-b8b77a2$


What happened ?

Without any settings, Ivy retrieves files from the maven 2 repository. That's
what happened here.  

The resolve task has found the commons-lang and commons-cli modules in the
maven 2 repository, identified that commons-cli depends on commons-logging and
so resolved it as a transitive dependency. Then Ivy has downloaded all
corresponding `artifacts` in its cache (by default in your user home, in a
    .ivy2/cache directory). Finally, the retrieve task copies the resolved
jars from the ivy cache to the default library directory of the project: the
lib dir (you can change this easily by setting the pattern attribute on the
    retrieve task).

You might say that the task took a long time just to write out a "Hello Ivy!"
message. But remember that a lot of time was spent downloading the required
files from the web. Let's try to run it again:

kyoupark@ukstbuild2:~/ivy-tut/ex/ant-ivy-b8b77a2$ ant
Buildfile: build.xml

resolve:
[ivy:retrieve] impossible to define new type: class not found: org.apache.ivy.plugins.signer.bouncycastle.OpenPGPSignatureGenerator in [] nor Ivy classloader
[ivy:retrieve] :: Apache Ivy 2.3.0-local-20131108183505 - 20131108183505 :: http://ant.apache.org/ivy/ ::
[ivy:retrieve] :: loading settings :: url = jar:file:/usr/share/java/ivy-2.3.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[ivy:retrieve] :: resolving dependencies :: org.apache#hello-ivy;working@ukstbuild2.uk.nds.com
[ivy:retrieve]  confs: [default]
[ivy:retrieve]  found commons-lang#commons-lang;2.0 in public
[ivy:retrieve]  found commons-cli#commons-cli;1.0 in public
[ivy:retrieve]  found commons-logging#commons-logging;1.0 in public
[ivy:retrieve] :: resolution report :: resolve 208ms :: artifacts dl 15ms
[ivy:retrieve]  :: evicted modules:
[ivy:retrieve]  commons-lang#commons-lang;1.0 by [commons-lang#commons-lang;2.0] in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   4   |   0   |   0   |   1   ||   7   |   0   |
        ---------------------------------------------------------------------
[ivy:retrieve] :: retrieving :: org.apache#hello-ivy
[ivy:retrieve]  confs: [default]
[ivy:retrieve]  0 artifacts copied, 7 already retrieved (0kB/8ms)

run:
     [java] standard message : hello ivy !
     [java] capitalized by org.apache.commons.lang.WordUtils : Hello Ivy !

BUILD SUCCESSFUL
Total time: 1 second


Great! The cache was used, so no download was needed and the build was
instantaneous.

And now, if you want to generate a report detailing all the dependencies of
your module, you can call the report target, and check the generated file in
the build directory. You should obtain something looking like this.

As you can see, using Ivy to resolve dependencies stored in the maven 2
repository is extremely easy. Now you can go on with the next tutorials to
learn more about how to use module configurations which is a very powerful Ivy
specific feature. Other tutorials are also available where you will learn how
to use Ivy settings to leverage a possibly complex enterprise repository. It
may also be a good time to start reading the reference documentation, and
especially the introduction material which gives a good overview of Ivy. The
best practices page is also a must read to start thinking about how to use
Ant+Ivy to build a clean and robust build system. 


<ex>
<?xml version="1.0" encoding="UTF-8"?>
<ivy-module version="2.0">
  <info organisation="platforms" module="bin_BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01"/>
       <dependencies>
<dependency org="platforms" name="BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01" rev="BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01_16.9.2-h1.19p1_SI-9085">
<artifact name="BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01" ext="gz" type="gz"/>
        </dependency>
        </dependencies>
</ivy-module>


<term>
An artifact is a single file ready for delivery with the publication of a
module revision, as a product of development.

Compressed package formats are often preferred because they are easier to
manage, transfer and store. For the same reasons, only one or a few artifacts
per module are commonly used. However, artifacts can be of any file type and
any number of them can be declared in a single module.

In the Java world, common artifacts are Java archives or JAR files. In many
cases, each revision of a module publishes only one artifact (like
    jakarta-log4j-1.2.6.tar.gz, for instance), but some of them publish many
artifacts dependending on the use of the module (like apache-ant binary and
    source distributions in zip, gz and bz2 package formats, for instance).

Examples: ant-1.7.0-bin.zip, apache-ant-1.7.0-src.tar.gz


Type of an artifact

The artifact type is a category of a particular kind of artifact specimen. It
is a classification based on the intended purpose of an artifact or why it is
provided, not a category of packaging format or how the artifact is delivered.

Although the type of an artifact may (rather accidentally) imply its file
format, they are two different concepts. The artifact file name extension is
more closely associated with its format. For example, in the case of Java
archives the artifact type "jar" indicates that it is indeed a Java archive as
per the JAR File specification. The file name extension happens to be "jar" as
well. On the other hand, with source code distributions, the artifact type may
be "source" while the file name extensions vary from "tar.gz", "zip", "java",
   "c", or "xml" to pretty much anything. So, the type of an artifact is
     basically an abstract functional category to explain its purpose, while
     the artifact file name extension is a more concrete technical indication
     of its format and, of course, naming.

Defining appropriate artifact types for a module is up to its development
organisation. Common choices may include: "jar", "binary", "bin", "rc", "exe",
"dll", "source", "src", "config", "conf", "cfg", "doc", "api", "spec",
"manual", "man", "data", "var", "resource", "res", "sql", "schema", "deploy",
"install", "setup", "distrib", "distro", "distr", "dist", "bundle", etc.

Module descriptors are not really artifacts, but they are comparable to an
artifact type, i.e. "descriptor" (an ivy file or a Maven POM).

Electronic signatures or digests are not really artifacts themselves, but can
be found with them in repositories. They also are comparable to an artifact
type, i.e. "digest" (md5 or sha1).  

Artifact file name extension 

In some cases the artifact type already implies its file name extension, but
not always. More generic types may include several different file formats,
e.g.  documentation can contain tarballs, zip packages or any common document
  formats.

Examples: zip, tar, tar.gz, rar, jar, war, ear, txt, doc, xml, html


# ============================================================================
#{ gdb
={============================================================================
*kt_linux_tool_300* gdb-term and links

{inferior}
GDB represents the state of each program execution with an object called an
'inferior'. 


{target}
info files shows your active targets.

{reference}
http://www.gnu.org/software/gdb/
https://sourceware.org/gdb/current/onlinedocs/gdb/
http://visualgdb.com/gdbreference/commands/sharedlibrary 


={============================================================================
*kt_linux_tool_300* gdb-check-built with debug symbols

note:
file command do not show difference between nodebug and debug binary.

{readelf}

<on-nodebug-build>

note: see no debug sections

$ readelf -S nickelmediad 
There are 37 section headers, starting at offset 0xe4bfc:

$ readelf -S
Section Headers:
  [Nr] Name              Type            Addr Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            0000 000000 000000 00      0   0  0
  [ 1] .interp           PROGBITS        0040 000194 000014 00   A  0   0  1
  [ 2] .reginfo          MIPS_REGINFO    0040 0001a8 000018 18   A  0   0  4
  [ 3] .dynamic          DYNAMIC         0040 0001c0 000188 08   A  6   0  4
  [ 4] .hash             HASH            0040 000348 000998 04   A  5   0  4
  [ 5] .dynsym           DYNSYM          0040 000ce0 0015d0 10   A  6   1  4
  [ 6] .dynstr           STRTAB          0040 0022b0 003d20 00   A  0   0  1
  [ 7] .gnu.version      VERSYM          0040 005fd0 0002ba 02   A  5   0  2
  [ 8] .gnu.version_r    VERNEED         0040 00628c 000080 00   A  6   2  4
  [ 9] .rel.dyn          REL             0040 00630c 000238 08   A  5   0  4
  [10] .rel.plt          REL             0040 006bfc 0003b0 08   A  5  12  4
  [11] .init             PROGBITS        0040 006fac 000048 00  AX  0   0  4
  [12] .plt              PROGBITS        0040 007000 000784 00  AX  0   0 32
  [13] .text             PROGBITS        0040 007790 0aa1f0 00  AX  0   0 16
  [14] .MIPS.stubs       PROGBITS        004b 0b1980 000670 00  AX  0   0  4
  [15] .fini             PROGBITS        004b 0b1ff0 000038 00  AX  0   0  4
  [16] .rodata           PROGBITS        004b 0b2030 009a78 00   A  0   0 16
  [17] .eh_frame_hdr     PROGBITS        004b 0bbaa8 001afc 00   A  0   0  4
  [18] .eh_frame         PROGBITS        004c 0bdbc0 009068 00  WA  0   0  4
  [19] .gcc_except_table PROGBITS        004d 0c6c28 011914 00  WA  0   0  4
  [20] .ctors            PROGBITS        004e 0d853c 000024 00  WA  0   0  4
  [21] .dtors            PROGBITS        004e 0d8560 000008 00  WA  0   0  4
  [22] .jcr              PROGBITS        004e 0d8568 000004 00  WA  0   0  4
  [23] .data.rel.ro      PROGBITS        004e 0d8570 001a8c 00  WA  0   0  8
  [24] .data             PROGBITS        004e 0da000 0003d0 00  WA  0   0 16
  [25] .rld_map          PROGBITS        004e 0da3d0 000004 00  WA  0   0  4
  [26] .got.plt          PROGBITS        004e 0da3d4 0001e0 00  WA  0   0  4
  [27] .got              PROGBITS        004e 0da5c0 00079c 04 WAp  0   0 16
  [28] .sdata            PROGBITS        004e 0dad5c 000004 00 WAp  0   0  4
  [29] .bss              NOBITS          004e 0dad60 001270 00  WA  0   0 16
  [30] .comment          PROGBITS        0000 0dad60 00018c 00      0   0  1
  [31] .gnu.attributes   LOOS+ffffff5    0000 0daeec 000010 00      0   0  1
  [32] .mdebug.abi32     PROGBITS        0000 0daefc 000000 00      0   0  1
  [33] .pdr              PROGBITS        0000 0daefc 009bc0 00      0   0  4
  [34] .shstrtab         STRTAB          0000 0e4abc 00013e 00      0   0  1
  [35] .symtab           SYMTAB          0000 0e51c4 009b20 10     36 2134  4
  [36] .strtab           STRTAB          0000 0eece4 0457fd 00      0   0  1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings)
  I (info), L (link order), G (group), T (TLS), E (exclude), x (unknown)
  O (extra OS processing required) o (OS specific), p (processor specific)

<on-debug-build>
There are 46 section headers, starting at offset 0x6e857c:

Section Headers:
  [Nr] Name              Type            Addr Off    Size   ES Flg Lk Inf Al
  [ 0]                   NULL            0000 000000 000000 00      0   0  0
  [ 1] .interp           PROGBITS        0040 000194 000014 00   A  0   0  1
  [ 2] .reginfo          MIPS_REGINFO    0040 0001a8 000018 18   A  0   0  4
  [ 3] .dynamic          DYNAMIC         0040 0001c0 000188 08   A  6   0  4
  [ 4] .hash             HASH            0040 000348 001814 04   A  5   0  4
  [ 5] .dynsym           DYNSYM          0040 001b5c 003fa0 10   A  6   1  4
  [ 6] .dynstr           STRTAB          0040 005afc 0121b8 00   A  0   0  1
  [ 7] .gnu.version      VERSYM          0041 017cb4 0007f4 02   A  5   0  2
  [ 8] .gnu.version_r    VERNEED         0041 0184a8 000060 00   A  6   2  4
  [ 9] .rel.dyn          REL             0041 018508 000328 08   A  5   0  4
  [10] .rel.plt          REL             0041 019000 0003f8 08   A  5  12  4
  [11] .init             PROGBITS        0041 0193f8 000048 00  AX  0   0  4
  [12] .plt              PROGBITS        0041 019440 000814 00  AX  0   0 32
  [13] .text             PROGBITS        0041 019c60 187120 00  AX  0   0 16
  [14] .MIPS.stubs       PROGBITS        005a 1a0d80 000620 00  AX  0   0  4
  [15] .fini             PROGBITS        005a 1a13a0 000038 00  AX  0   0  4
  [16] .rodata           PROGBITS        005a 1a13e0 017438 00   A  0   0 16
  [17] .eh_frame_hdr     PROGBITS        005b 1b8818 009114 00   A  0   0  4
  [18] .eh_frame         PROGBITS        005d 1c1bb0 02c034 00  WA  0   0  4
  [19] .gcc_except_table PROGBITS        005f 1edbe4 0147bc 00  WA  0   0  4
  [20] .ctors            PROGBITS        0061 2023a0 000024 00  WA  0   0  4
  [21] .dtors            PROGBITS        0061 2023c4 000008 00  WA  0   0  4
  [22] .jcr              PROGBITS        0061 2023cc 000004 00  WA  0   0  4
  [23] .data.rel.ro      PROGBITS        0061 2023d0 001c2c 00  WA  0   0  8
  [24] .data             PROGBITS        0061 204000 0003d0 00  WA  0   0 16
  [25] .rld_map          PROGBITS        0061 2043d0 000004 00  WA  0   0  4
  [26] .got.plt          PROGBITS        0061 2043d4 000204 00  WA  0   0  4
  [27] .got              PROGBITS        0061 2045e0 002580 04 WAp  0   0 16
  [28] .sdata            PROGBITS        0061 206b60 000004 00 WAp  0   0  4
  [29] .bss              NOBITS          0061 206b64 001270 00  WA  0   0 16
  [30] .comment          PROGBITS        0000 206b64 00018c 00      0   0  1
  [31] .debug_aranges    MIPS_DWARF      0000 206cf0 008908 00      0   0  1 { ~
  [32] .debug_pubnames   MIPS_DWARF      0000 20f5f8 0c6fea 00      0   0  1
  [33] .debug_info       MIPS_DWARF      0000 2d65e2 1a07f7 00      0   0  1
  [34] .debug_abbrev     MIPS_DWARF      0000 476dd9 006741 00      0   0  1
  [35] .debug_line       MIPS_DWARF      0000 47d51a 039d68 00      0   0  1
  [36] .debug_frame      MIPS_DWARF      0000 4b7284 03f620 00      0   0  4
  [37] .debug_str        MIPS_DWARF      0000 4f68a4 14d3fc 01  MS  0   0  1
  [38] .debug_loc        MIPS_DWARF      0000 643ca0 05326e 00      0   0  1
  [39] .debug_ranges     MIPS_DWARF      0000 696f0e 013188 00      0   0  1 } ~
  [40] .gnu.attributes   LOOS+ffffff5    0000 6aa096 000010 00      0   0  1
  [41] .mdebug.abi32     PROGBITS        0000 6aa0a6 000000 00      0   0  1
  [42] .pdr              PROGBITS        0000 6aa0a8 03e320 00      0   0  4
  [43] .shstrtab         STRTAB          0000 6e83c8 0001b4 00      0   0  1
  [44] .symtab           SYMTAB          0000 6e8cac 020680 10     45 7279  4
  [45] .strtab           STRTAB          0000 70932c 10ffac 00      0   0  1


{readelf-dump}
note: This is better since there will be 'no' output on release version.

Also this command line shows no output on nodebug version but shows all symbols
on debug version. 

$ readelf --debug-dump=decodedline nickelmediad 

readelf -w

-w[lLiaprmfFsoRt]
--debug-dump[=rawline,=decodedline,=info,=abbrev,=pubnames,=aranges,=macro,
  =frames,=frames-interp,=str,=loc,=Ranges,=pubtypes,=trace_info,
  =trace_abbrev,=trace_aranges,=gdb_index]

   Displays the contents of the debug sections in the file, if any are
   present.  If one of the optional letters or words follows the switch then
   only data found in those specific sections will be dumped.

   Note that there is no single letter option to display the content of trace
   sections or .gdb_index.

   Note: the =decodedline option will display the interpreted contents of a
   .debug_line section whereas the =rawline option dumps the contents in a raw
   format.

   Note: the =frames-interp option will display the interpreted contents of a
   .debug_frame section whereas the =frames option dumps the contents in a raw
   format.

   Note: the output from the =info option can also be affected by the options
   --dwarf-depth and --dwarf-start.


{use-gdb}
Used gdb on host.

for nodebug version.

# gdb /bin/nickelmediad
...
Reading symbols from /bin/nickelmediad...(no debugging symbols found)...done. ~
(gdb) 

for debug version.

# gdb /bin/nickelmediad
...
Reading symbols from /lds/Nickel.System.DBusServer/.libs/nickelmediad...done.
(gdb)


={============================================================================
*kt_linux_tool_300* gdb-sample session

1 A Sample gdb Session

You can use this manual at your leisure to read all about gdb. However, a
handful of commands are enough to get started using the debugger. This chapter
illustrates those commands.

One of the preliminary versions of gnu m4 (a generic macro processor) exhibits
the following bug: sometimes, when we change its quote strings from the default,
the commands used to capture one macro definition within another stop working.
  In the following short m4 session, we define a macro foo which expands to
  0000; we then use the m4 built-in defn to define bar as the same thing.
  However, when we change the open quote string to <QUOTE> and the close quote
  string to <UNQUOTE>, the same procedure fails to define a new synonym baz:

     $ cd gnu/m4
     $ ./m4
     define(foo,0000)
     
     foo
     0000
     define(bar,defn(`foo'))
     
     bar
     0000
     changequote(<QUOTE>,<UNQUOTE>)
     
     define(baz,defn(<QUOTE>foo<UNQUOTE>))
     baz
     Ctrl-d
     m4: End of input: 0: fatal error: EOF in string

Let us use gdb to try to see what is going on.

     $ gdb m4
     
     
     (gdb)

gdb reads only enough symbol data to know where to find the rest when needed; as
a result, the first prompt comes up very quickly. We now tell gdb to use a
narrower display width than usual, so that examples fit in this manual.

     (gdb) set width 70

We need to see how the m4 built-in changequote works. Having looked at the
source, we know the relevant subroutine is m4_changequote, so we set a
breakpoint there with the gdb break command.

     (gdb) break m4_changequote
     Breakpoint 1 at 0x62f4: file builtin.c, line 879.

Using the run command, we start m4 running under gdb control; as long as control
does not reach the m4_changequote subroutine, the program runs as usual:

     (gdb) run
     Starting program: /work/Editorial/gdb/gnu/m4/m4
     define(foo,0000)
     
     foo
     0000


<gdb-call-function>

To trigger the breakpoint, we 'call' changequote. gdb suspends execution of m4,
displaying information about the context where it stops.

     changequote(<QUOTE>,<UNQUOTE>)
     
     Breakpoint 1, m4_changequote (argc=3, argv=0x33c70)
         at builtin.c:879
     879         if (bad_argc(TOKEN_DATA_TEXT(argv[0]),argc,1,3))

Now we use the command n (next) to advance execution to the next line of the
current function.

     (gdb) n
     882         set_quotes((argc >= 2) ? TOKEN_DATA_TEXT(argv[1])\
      : nil,

set_quotes looks like a promising subroutine. We can go into it by using the
command s (step) instead of next. step goes to the next line to be executed in
any subroutine, so it steps into set_quotes.

     (gdb) s
     set_quotes (lq=0x34c78 "<QUOTE>", rq=0x34c88 "<UNQUOTE>")
         at input.c:530
     530         if (lquote != def_lquote)

The display that shows the subroutine where m4 is now suspended (and its
  arguments) is called a stack frame display. It shows a summary of the stack.
We can use the backtrace command (which can also be spelled bt), to see where we
are in the stack as a whole: the backtrace command displays a stack frame for
each active subroutine.

     (gdb) bt
     #0  set_quotes (lq=0x34c78 "<QUOTE>", rq=0x34c88 "<UNQUOTE>")
         at input.c:530
     #1  0x6344 in m4_changequote (argc=3, argv=0x33c70)
         at builtin.c:882
     #2  0x8174 in expand_macro (sym=0x33320) at macro.c:242
     #3  0x7a88 in expand_token (obs=0x0, t=209696, td=0xf7fffa30)
         at macro.c:71
     #4  0x79dc in expand_input () at macro.c:40
     #5  0x2930 in main (argc=0, argv=0xf7fffb20) at m4.c:195

We step through a few more lines to see what happens. The first two times, we
can use ‘s’; the next two times we use n to avoid falling into the xstrdup
subroutine.

     (gdb) s
     0x3b5c  532         if (rquote != def_rquote)
     (gdb) s
     0x3b80  535         lquote = (lq == nil || *lq == '\0') ?  \
     def_lquote : xstrdup(lq);
     (gdb) n
     536         rquote = (rq == nil || *rq == '\0') ? def_rquote\
      : xstrdup(rq);
     (gdb) n
     538         len_lquote = strlen(rquote);

note: looks odd since "len_lquote (left) = strlen(rquote) (right)

The last line displayed looks a little odd; we can examine the variables lquote
  and rquote to see if they are in fact the new left and right quotes we
  specified. We use the command p (print) to see their values.

     (gdb) p lquote
     $1 = 0x35d40 "<QUOTE>"
     (gdb) p rquote
     $2 = 0x35d50 "<UNQUOTE>"

lquote and rquote are indeed the new left and right quotes. To look at some
context, we can display ten lines of source surrounding the current line with
the l (list) command.

     (gdb) l
     533             xfree(rquote);
     534
     535         lquote = (lq == nil || *lq == '\0') ? def_lquote\
      : xstrdup (lq);
     536         rquote = (rq == nil || *rq == '\0') ? def_rquote\
      : xstrdup (rq);
     537
     538         len_lquote = strlen(rquote);
     539         len_rquote = strlen(lquote);
     540     }
     541
     542     void

Let us step past the two lines that set len_lquote and len_rquote, and then
examine the values of those variables.

     (gdb) n
     539         len_rquote = strlen(lquote);
     (gdb) n
     540     }
     (gdb) p len_lquote
     $3 = 9
     (gdb) p len_rquote
     $4 = 7

That certainly looks 'wrong', assuming len_lquote and len_rquote are meant to be
the lengths of lquote and rquote respectively. We can 'set' them to better
values using the p command, since it can print the value of any expression—and
that expression can include subroutine calls and assignments.

<gdb-set-var>

     (gdb) p len_lquote=strlen(lquote)
     $5 = 7
     (gdb) p len_rquote=strlen(rquote)
     $6 = 9

Is that enough to fix the problem of using the new quotes with the m4 built-in
defn? We can allow m4 to continue executing with the c (continue) command, and
then try the example that caused trouble initially:

     (gdb) c
     Continuing.
     
     define(baz,defn(<QUOTE>foo<UNQUOTE>))
     
     baz
     0000

Success! The new quotes now work just as well as the default ones. The problem
seems to have been just the two typos defining the wrong lengths. We allow m4
exit by giving it an EOF as input:

     Ctrl-d
     Program exited normally.

The message ‘Program exited normally.’ is from gdb; it indicates m4 has finished
executing. We can end our gdb session with the gdb quit command.

     (gdb) quit


={============================================================================
*kt_linux_tool_300* gdb-start

2 Getting In and Out of gdb

2.1 Invoking gdb

The most usual way to start gdb is with one argument, specifying an executable
program:

     gdb program

note
Since it reads any arguments other than options as see the first argument as
equivalent to the '-se' option and the second argument as equivalent to the
'-c'/'-p' option followed by that argument:

"gdb program" is the same as "gdb -se program"

-se file
Read symbol table from file 'file' and use it as the executable file.


You can also start with both an executable program and a core file specified:

     gdb program core

You can, instead, specify a process ID as a second argument, if you want to
debug a running process:

     gdb program 1234
     gdb -p 1234 program 
     gdb -p $(pidof w3cEngine) /opt/zinc-trunk/bin/w3cEngine

would 'attach' gdb to process 1234 (unless you also have a file named 1234; gdb
        does check for a core file first).


<gdb-args-options>

Taking advantage of the second command-line argument requires a fairly complete
operating system; when you use gdb as a remote debugger attached to a bare
board, there may not be any notion of “process”, and there is often no way to
get a core dump. gdb will warn you if it is unable to attach or to read core
dumps.

You can optionally have gdb pass any arguments after the executable file 'to'
the inferior using --args. This option 'stops' option processing.

     gdb --args gcc -O2 -c foo.c

This will cause gdb to debug gcc, and to set gcc's command-line arguments (see
    Arguments) to '-O2 -c foo.c'.


<ex>
gdb --args /opt/zinc-trunk/oss/bin/gst-launch-1.0 $@
gdb --eval-command=run --args ${env_vars[@]} $prefix/bin/nickelmediad -b "Zinc.MediaProxy"


You can run gdb without printing the front material, which describes gdb's
non-warranty, by specifying --silent (or -q/--quiet):

     gdb --silent

You can further control how gdb starts up by using command-line options. gdb
itself can remind you of the options available.

Type

     gdb -help

to display all available options and briefly describe their use (‘gdb -h’ is a
    shorter equivalent).

All options and command line arguments you give are processed in sequential
order. The order makes a difference when the ‘-x’ option is used.


={============================================================================
*kt_linux_tool_300* gdb-start: options

Many options have both long and short forms; both are shown in the following
list. gdb also recognizes the long forms if you truncate them, so long as enough
of the option is present to be unambiguous. 

If you prefer, you can flag option arguments with "--" rather than "-", though
we illustrate the more usual convention. 


<gdb-eval-option>

-eval-command command
-ex command

Execute a 'single' gdb command. This option may be used multiple times to call
multiple commands. It may also be interleaved with '-command' as required.

gdb -eval-command=run --args /opt/zinc-trunk/oss/bin/gst-launch-1.0 $@


<gdb-command-file>

note: works for target

-command file
-x file
    Execute commands from file file. The contents of this file is evaluated
    exactly as the 'source' command would. See 23.1.3 Command Files

// not works for target version.
//
// -init-command file
// -ix file
//     Execute commands from file file 'before' loading the inferior (but after
//         loading gdbinit files). See Startup.

-init-eval-command command
-iex command
    Execute a single gdb command 'before' loading the inferior (but after
        loading gdbinit files). See Startup.


<gdb-set-search>

-directory directory
-d directory
    Add directory to the path to search for source and script files. 


={============================================================================
*kt_linux_tool_300* gdb-start: repeatedly run a test in GDB, until it fails

HOWTO repeatedly run a test in GDB, until it fails

Those of you grappling with intermittent failures may find this useful. I hit a
test that was intermittently aborting in x86 debug build, due to use of a
"singular iterator". It was particularly awkward to reproduce in gdb, so I
wanted to script it such that gdb would only prompt in the event of failure, and
otherwise quit and allow the next iteration.

Here's what I ended-up with:

while gdb --eval-command=start --eval-command="b abort" \
  --eval-command=continue --eval-command=quit \
    --args ./dbusoutputmanagertest; do echo "OK"; done


={============================================================================
*kt_linux_tool_300* gdb-start: what do during startup

2.1.3 What gdb Does During Startup

Here's the description of what gdb does during session startup:

    Sets up the command interpreter as specified by the command line (see
        interpreter).  
    
    Reads the system-wide init file (if --with-system-gdbinit was used when
        building gdb; see System-wide configuration and settings) and executes
    all the commands in that file.

    Reads the init file (if any) in your home directory and executes all the
    commands in that file.

    Executes commands and command files specified by the ‘-iex’ and ‘-ix’
    options in their specified order. note: Usually you should use the '-ex' and
    '-x' options instead, but this way you can apply settings before gdb init
    files get executed and before inferior gets loaded.  
    
    Processes command line options and operands.

    Reads and executes the commands from init file (if any) in the current
    working directory as long as ‘set auto-load local-gdbinit’ is set to ‘on’
    (see Init File in the Current Directory). 
    
    note: This is only done if the current directory is different from your home
    directory. Thus, you can have more than one init file, one generic in your
    home directory, and another, specific to the program you are debugging, in
    the directory where you invoke gdb.  
  
    If the command line specified a program to debug, or a process to attach to,
    or a core file, gdb loads any auto-loaded scripts provided for the program or
    for its loaded shared libraries. See Auto-loading.

    Executes commands and command files specified by the ‘-ex’ and ‘-x’ options
      in their specified order.

    note: command-history
    Reads the command history recorded in the history file. See Command History,
          for more details about the command history and the files where gdb
            records it. 


={============================================================================
*kt_linux_tool_300* gdb-start: init file

Init files use the same syntax as command files and are processed by gdb in the
same way. The init file in your home directory can set options (such as 'set
    complaints') that affect subsequent processing of command line options and
operands. Init files are not executed if you use the ‘-nx’ option (see Choosing
    Modes).

To display the list of init files loaded by gdb at startup, you can use gdb
--help.

The gdb init files are normally called .gdbinit. 

Q: use HOME varaible?


// host version

$ gdb --help
...
At startup, GDB reads the following init files and executes their commands:
   * system-wide init file: /etc/gdb/gdbinit

For more information, type "help" from within GDB, or consult the
...

// target version

$ gdb --help
...
At startup, GDB reads the following init files and executes their commands:

For more information, type "help" from within GDB, or consult the
...

// target version when have gdbinit in the root dir. Show the same when cd into
// other directory and run gdb there.

At startup, GDB reads the following init files and executes their commands:
   * user-specific init file: //.gdbinit

For more information, type "help" from within GDB, or consult the


={============================================================================
*kt_linux_tool_300* gdb-start: init file: ex

{example-one}
can define user func that have commands to run

$ more .gdbinit
set history save on
set history filename ./.gdb_history
set output-radix 16

define connect
    handle SIG32 nostop noprint pass
    handle SIG33 nostop noprint pass
#    b CTL_SimpleZapperTestStep
#    b CTL_ChannelZapping_FullStbTearDown
#               b readSectionFilterDataAndWriteToFile
    b sectionFilterTask
                b CTL_SectionFilter_Engine.c:2126
                b CTL_SectionFilter_Engine.c:2146
    directory components/FOSH/FUSIONOS_XTV_TESTS/xtv_test/src/
                i b
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define init_task
   set $t=&init_task
   printf "task name \"%s\", pid %05d \n", $t->comm, t->pid
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define find_task
  # Addresses greater than _end: kernel data...
  # ...user passed in an address
  if ((unsigned)$arg0 > (unsigned)&_end)
    set $t=(struct task_struct *)$arg0
  else
    # User entered a numeric PID
    # Walk the task list to find it
    set $t=&init_task
    if (init_task.pid != (unsigned)$arg0)
      find_next_task $t
      while (&init_task!=$t && $t->pid != (unsigned)$arg0)
        find_next_task $t
      end
      if ($t == &init_task)
        printf "Couldn't find task; using init_task\n"
      end
    end
  end
  printf "Task \"%s\":\n", $t->comm
end


Reads and executes the commands from init file (if any) in the current working
directory. This is only done if the current directory is different from your
home directory. Thus, you can have more than one init file, one generic in your
home directory, and another, specific to the program you are debugging, in the
directory where you invoke gdb.


={============================================================================
*kt_linux_tool_300* gdb-start: specify files

{file-command}
You may want to specify executable and core dump file names. 

<why-needed>
Occasionally it is necessary to change to a different file during a gdb session.
Or you may run gdb and forget to specify a file you want to use. 

Or you are debugging a remote target via gdbserver. In these situations the gdb
commands to specify new files are useful.

file filename

Use filename as the program to be debugged. Use the file command to get 'both'
symbol table and program to run from the same file. 

file 

file with no argument makes gdb discard any information it has on both
executable file and the symbol table.

note: any real case when useful to load seperately symbol file or executable?

core-file [filename]
core 

Specify the whereabouts of a core dump file to be used as the "contents of
memory". core files contain only 'some' parts of the address space of the
process that generated them; gdb can access the executable file itself for other
parts.  

core-file 

with no argument specifies that no core file is to be used.


{info-files}
info files
info target

info files and info target are synonymous; both print the current target,
     including the names of the executable and core dump files currently in use
     by gdb, and the files from which symbols were loaded. The command "help
     target" lists all possible targets rather than current ones.


={============================================================================
*kt_linux_tool_300* gdb-start: running under gdb

4 Running Programs Under gdb

When you run a program under gdb, you must first generate debugging information
when you compile it.

If you are doing native debugging, you may redirect your program's input and
output, debug an already running process, or kill a child process. 

4.1 Compiling for Debugging

In order to debug a program effectively, you need to generate debugging
information when you compile it. This debugging information is stored in the
object file; it describes the data type of each variable or function and the
correspondence between source line numbers and addresses in the executable code.

To request debugging information, specify the ‘-g’ option when you run the
compiler.

gcc, the gnu C/C++ compiler, supports ‘-g’ with or without ‘-O’, making it
possible to debug optimized code. We recommend that you always use ‘-g’ whenever
you compile a program. You may think your program is correct, but there is no
sense in pushing your luck. For more information, see Optimized Code.

<macro-expansion>
gdb knows about preprocessor macros and can show you their expansion (see
    Macros). Most compilers do not include information about preprocessor macros
in the debugging information if you specify the -g flag alone. Version 3.1 and
later of gcc, the gnu C compiler, provides macro information if you are using
the DWARF debugging format, and specify the option -g3.

See Options for Debugging Your Program or GCC, for more information on gcc
options affecting debug information.

You will have the best debugging experience if you use the latest version of the
DWARF debugging format that your compiler supports. DWARF is currently the most
expressive and best supported debugging format in gdb. 


4.2 Starting your Program

run
r 

Use the `run` command to start your program under gdb. You must first specify
the program name with an argument to gdb (see Getting In and Out of gdb), or by
using the `file` or exec-file command (see Commands to Specify Files). 

If you are running your program in an execution environment that supports
processes, run 'creates' an 'inferior' process and makes that process run your
program.

then use `continue` to run your program. You may need load first (see load).

note: the load command is for remote debugging.


{information-to-inferior}
The execution of a program is affected by certain information it receives from
its 'superior'. gdb provides ways to specify this information, which you must do
'before' starting your program. 

You can change it after starting your program, but such changes only affect your
program the 'next' time you start it.

This information may be divided into four categories: 

arguments, environment, working directory, standard input and output.

<arguments> 4.3 Your Program's Arguments

The arguments to your program can be specified by the arguments of the run
command. They are passed to a 'shell', which expands wildcard characters and
performs redirection of I/O, and thence to your program. 

note: gdb runs your program via a shell

run with no arguments uses the same arguments used by the previous run, or those
set by the set args command.

set args 

Specify the arguments to be used the 'next' time your program is run. If set
args has no arguments, run executes your program with no arguments. Once you
have run your program with arguments, using `set args` before the next run is
the only way to run it again without arguments.

show args 

Show the arguments to give your program when it is started.

note: don't need to care about \"..." as below.

(gdb) set args -b Zinc.MediaProxy 
(gdb) show args
Argument list to give program being debugged when it is started 
  is "-b Zinc.MediaProxy".


<environment> 4.4 Your Program's Environment

Usually you set up environment variables with the shell and they are inherited
by all the other programs you run. When debugging, it can be useful to try
running your program with a modified environment 'without' having to start gdb
over again.

show paths

Display the list of search paths for executables (the PATH environment
    variable).

show environment [varname]

Print the value of environment variable varname to be given to your program when
it starts. If you do not supply varname, print the names and values of 'all'
environment variables to be given to your program. You can abbreviate
environment as env.

set environment varname [=value]

Set environment variable varname to value. The value changes for your program
(and the shell gdb uses to launch it), not for gdb itself. The value parameter
is optional; if it is eliminated, the variable is set to a null value.

unset environment varname

Remove variable varname from the environment to be passed to your program.


<wd> 4.5 Your Program's Working Directory

Each time you start your program with run, it inherits its working directory
from the current working directory of gdb. The gdb working directory is
initially whatever it inherited from its 'parent' process, typically the shell,
          but you can specify a new working directory in gdb with the cd
          command.

The gdb working directory also serves as a 'default' for the 'commands' that
specify files for gdb to operate on. See Commands to Specify Files.

cd [directory]
    Set the gdb working directory to directory. If not given, directory uses
    '~'.

pwd
    Print the gdb working directory. 

note:
It is generally impossible to find the current working directory of the process
being debugged (since a program can change its directory during its run). If you
work on a system where gdb is configured with the /proc support, you can use the
`info proc` command (see SVR4 Process Information) to find out the current
working directory of the debuggee. 


<stdio> 4.6 Your Program's Input and Output

note: Can use this to get output in sandboxed debuggee?

Your program normally uses the 'same' device for standard input and standard
output as gdb is using.  You can redirect input and output in the `run` command
line, or you can use the tty command to set a different device for your program.
See Section 4.6 [Your Program's Input and Output], page 32.

note: Warning: While input and output redirection work, you cannot use pipes to
pass the output of the program you are debugging to another program; if you
attempt this, gdb is likely to wind up debugging the wrong program.

By default, the program you run under gdb does input and output to the same
terminal that gdb uses. gdb switches the terminal to its own terminal modes to
interact with you, but it records the terminal modes your program was using and
switches back to them when you continue running your program.

run > outfile

starts your program, diverting its output to the file 'outfile'.

Another way to specify where your program should do input and output is with the
tty command.

An explicit redirection in run overrides the tty command's effect on the
input/output device, but 'not' its effect on the controlling terminal.

When you use the tty command or redirect input in the run command, only the
input for your program is affected. The input for gdb still comes from your
terminal. tty is an alias for set inferior-tty.

set inferior-tty /dev/ttyb

Set the tty for the program being debugged to /dev/ttyb.

show inferior-tty

Show the current tty for the program being debugged.


When you issue the `run` command, your program begins to 'execute' immediately.
See Stopping and Continuing, for discussion of how to arrange for your program
to stop. Once your program has stopped, you may call functions in your program,
   using the `print` or `call` commands. See Examining Data.

If the modification time of your symbol file has changed since the last time gdb
read its symbols, gdb discards its symbol table, and reads it again. When it
does this, gdb tries to retain your current breakpoints. 


start 

The name of the main procedure can vary from language to language. With C or
C++, the main procedure name is always main, but other languages such as Ada do
not require a specific name for their main procedure. 

The debugger provides a convenient way to start the execution of the program and
to 'stop' at the beginning of the main procedure, depending on the language
used. The `start` command does the equivalent of setting a temporary breakpoint
at the beginning of the main procedure and then invoking the run command.

Specify the arguments to give to your program as arguments to the 'start'
command.

note: Sams as with run, but stop as at the main.


={============================================================================
*kt_linux_tool_300* gdb-start: running under gdb: case

<1>
gst-run.sh souphttpsrc location="http://www.bbc.co.uk/mediaselector/playlists/hls/hdtv/ak/bbc1.m3u8" ! hlsdemux ! tsnexusbin

# gst-run.sh
#!/bin/bash

export LD_LIBRARY_PATH=/lib:/usr/local/lib:/opt/zinc-trunk/oss/lib/gstreamer-1.0:/opt/zinc-trunk/oss/lib:/opt/zinc-trunk/oss/lib/python2.6:$LD_LIBRARY_PATH
export GST_PLUGIN_PATH=/opt/zinc-trunk/oss/lib/gstreamer-1.0
export LD_PRELOAD=/usr/local/lib/libdirectfb.so:/usr/local/lib/libdirect.so:/usr/local/lib/libinit.so

gdb --eval-command=run --args /opt/zinc-trunk/oss/bin/gst-launch-1.0 $@

<2>
gdb --eval-command=run --args ${env_vars[@]} /bin/nickelmediad -b "Zinc.MediaProxy"


={============================================================================
*kt_linux_tool_300* gdb-attach-debug already running process

4.7 Debugging an Already-running Process

attach process-id

This command attaches to a running process—one that was started outside gdb.

info files 

shows your active targets.

note: You must also have permission to send the process a signal.

The first thing gdb does after arranging to debug the specified process is to
'stop' it. You can examine and modify an attached process with all the gdb
commands that are ordinarily available when you start processes with `run`. You
can insert breakpoints; you can step and continue; you can modify storage. If
you would rather the process continue running, you may use the continue
'command' after attaching gdb to the process.

detach 

When you have finished debugging the attached process, you can use the detach
command to release it from gdb control. Detaching the process continues its
execution. After the detach command, that process and gdb become completely
independent once more, and you are ready to attach another process or start one
with run.

$ gdb gst-launch-1.0 11476


={============================================================================
*kt_linux_tool_300* gdb-thread: debug with multiple threads

4.10 Debugging Programs with Multiple Threads

note: These facilities are not yet available on every gdb configuration where
the operating system supports threads. If your gdb does not support threads,
these commands have no effect. For example, a system without thread support
  shows no output from 'info threads', and always rejects the thread command,
like this:

(gdb) info threads
(gdb) thread 1
Thread ID 1 not known. Use the "info threads" command to
see the IDs of currently known threads.


<current-thread>
One thread in particular is always the focus of debugging. This thread is called
the current thread. Debugging commands show program information from the
perspective of the current thread.


For debugging purposes, gdb associates its own thread number

info threads [id...]

Display a summary of all threads currently in your program. Optional argument
id... is one or more thread ids separated by spaces, and means to print
information only about the specified thread or threads. gdb displays for each
thread (in this order):

1. the thread number assigned by gdb
2. the target system's thread identifier (systag)
3. the thread's name, if one is known. A thread can either be named by the user
(see thread name, below), or, in some cases, by the program itself.
4. the current stack frame summary for that thread

An asterisk ‘*’ to the left of the gdb thread number indicates the current
thread.

For example,

(gdb) info threads
  Id Target Id        Frame
  3 process 35 thread 27 0x34e5 in sigpause ()
  2 process 35 thread 23 0x34e5 in sigpause ()
* 1 process 35 thread 13 main (argc=1, argv=0x7ffffff8)
at threadtest.c:68


thread threadno

Make thread number threadno the current thread.

thread apply [threadno | all] command

The thread apply command allows you to apply the named command to one or more
threads. Specify the numbers of the threads that you want affected with the
command argument threadno. It can be a single thread number, one of the numbers
shown in the first field of the 'info threads' display; or it could be a range
of thread numbers, as in 2-4. To apply a command to all threads, type thread
apply all command.


={============================================================================
*kt_linux_tool_300* gdb-thread: tid on ps and gdb

[root@HUMAX /]# ps -eAL -o 'tid command,wchan' | grep -E 'w3'
 7239 /opt/zinc-trunk/bin/w3cEngi fusion_core_wq_wait
 7269 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7270 /opt/zinc-trunk/bin/w3cEngi fusion_core_wq_wait
 7271 /opt/zinc-trunk/bin/w3cEngi BKNI_WaitForEvent_tagged
 7272 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7273 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7274 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7275 /opt/zinc-trunk/bin/w3cEngi poll_schedule_timeout
 7276 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7277 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7278 /opt/zinc-trunk/bin/w3cEngi poll_schedule_timeout
 7279 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7280 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7285 /opt/zinc-trunk/bin/w3cEngi poll_schedule_timeout
 7286 /opt/zinc-trunk/bin/w3cEngi poll_schedule_timeout
 7288 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7290 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7291 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7292 /opt/zinc-trunk/bin/w3cEngi futex_wait_queue_me
 7303 grep -E w3                  pipe_wait


(gdb) i th
  19 Thread 0x2fc71520 (LWP 7269)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  18 Thread 0x3067e520 (LWP 7270)  0x2df6c5bc in read () from /lib/libpthread.so.0
  17 Thread 0x4068e520 (LWP 7271)  0x2df8e11c in ioctl () from /lib/libc.so.0
  16 Thread 0x4069e520 (LWP 7272)  0x2df68560 in pthread_cond_timedwait () from /lib/libpthread.so.0
  15 Thread 0x406ae520 (LWP 7273)  0x2df68560 in pthread_cond_timedwait () from /lib/libpthread.so.0
  14 Thread 0x40f2a520 (LWP 7274)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  13 Thread 0x41a71520 (LWP 7275)  0x2df8ed04 in poll () from /lib/libc.so.0
  12 Thread 0x42341520 (LWP 7276)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  11 Thread 0x42b41520 (LWP 7277)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  10 Thread 0x43341520 (LWP 7278)  0x2df8ed04 in poll () from /lib/libc.so.0
  9 Thread 0x43b41520 (LWP 7279)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  8 Thread 0x44341520 (LWP 7280)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  7 Thread 0x453ab520 (LWP 7285)  0x2df8ed04 in poll () from /lib/libc.so.0
  6 Thread 0x45f77520 (LWP 7286)  0x2df8ed04 in poll () from /lib/libc.so.0
  5 Thread 0x47213520 (LWP 7288)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  4 Thread 0x48935520 (LWP 7290)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  3 Thread 0x49135520 (LWP 7291)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
  2 Thread 0x49f17520 (LWP 7292)  0x2df68904 in pthread_cond_wait () from /lib/libpthread.so.0
* 1 Thread 0x2aab0000 (LWP 7239)  0x2df8e11c in ioctl () from /lib/libc.so.0


={============================================================================
*kt_linux_tool_300* gdb-log

2.4 Logging Output

You may want to save the output of gdb commands to a file. There are several
commands to control gdb's logging.

set logging on
    Enable logging.

set logging off
    Disable logging.

set logging file file
    Change the name of the current logfile. The default logfile is gdb.txt.

set logging overwrite [on|off]
    By default, gdb will append to the logfile. Set overwrite if you want set
    logging on to 'overwrite' the logfile instead.

set logging redirect [on|off]
    By default, gdb output will go to both the terminal and the logfile. Set
    redirect if you want output to go 'only' to the log file.

show logging
    Show the current values of the logging settings. 


={============================================================================
*kt_linux_tool_300* gdb-shell

2.3 Shell Commands

If you need to execute occasional shell commands during your debugging session,
there is no need to leave or suspend gdb; you can just use the shell command. 

shell command-string
!command-string

Invoke a standard shell to execute command-string. Note that no space is needed
between ! and command-string.


={============================================================================
*kt_linux_tool_301* gdb-cpp:

15.4.1.7 gdb Features for C++

Some gdb commands are particularly useful with C++, and some are designed
specifically for use with C++. Here is a summary:

breakpoint menus
    When you want a breakpoint in a function whose name is 'overloaded', gdb has
    the capability to display a menu of possible breakpoint locations to help
    you specify which function definition you want. See Ambiguous Expressions.


rbreak regex
    Setting breakpoints using regular expressions is helpful for setting
    breakpoints on overloaded functions that are not members of any special
    classes. See Setting Breakpoints.

catch throw
catch rethrow
catch catch
    Debug C++ exception handling using these commands. See Setting Catchpoints.


ptype typename
    Print inheritance relationships as well as other information for type
    typename. See Examining the Symbol Table.

info vtbl expression.
    The info vtbl command can be used to display the virtual method tables of
    the object computed by expression. This shows one entry per virtual table.
    there may be multiple virtual tables when multiple inheritance is in use.

demangle name
    Demangle name. See Symbols, for a more complete description of the demangle
    command.


set print demangle
show print demangle
set print asm-demangle
show print asm-demangle
    Control whether C++ symbols display in their source form, both when
    displaying code as C++ source and when displaying disassemblies. See Print
    Settings.

set print object
show print object
    Choose whether to print derived (actual) or declared types of objects. See
    Print Settings.

set print vtbl
show print vtbl
    Control the format for printing virtual function tables. See Print Settings.
    (The vtbl commands do not work on programs compiled with the HP ANSI C++
     compiler (aCC).)


set overload-resolution on
    Enable overload resolution for C++ expression evaluation. The default is on.
    For overloaded functions, gdb evaluates the arguments and searches for a
    function whose signature matches the argument types, using the standard C++
    conversion rules (see C++ Expressions, for details). If it cannot find a
    match, it emits a message.

set overload-resolution off
    Disable overload resolution for C++ expression evaluation. For overloaded
    functions that are not class member functions, gdb chooses the first
    function of the specified name that it finds in the symbol table, whether or
    not its arguments are of the correct type. For overloaded functions that are
    class member functions, gdb searches for a function whose signature exactly
    matches the argument types.


show overload-resolution
    Show the current setting of overload resolution.

Overloaded symbol names
    You can specify a particular definition of an overloaded symbol, using the
    same notation that is used to declare such symbols in C++: type
    symbol(types) rather than just symbol. You can also use the gdb command-line
    word completion facilities to list the available choices, or to finish the
    type list for you. See Command Completion, for details on how to do this. 


={============================================================================
*kt_linux_tool_300* gdb-general and help

3 gdb Commands

{test-abbreviation}
You can test abbreviations by using them as arguments to the help command.

(gdb) help s
Step program until it reaches a different source line.
Argument N means do this N times (or till program stops for another reason).


{return}
A blank line as input to gdb (typing just RET) means to 'repeat' the previous
command. Certain commands (for example, run) will not repeat this way.

The list and x commands, when you repeat them with RET, construct new arguments
rather than repeating exactly as typed. This permits easy scanning of source or
memory.


{comment}
Any text from a # to the end of the line is a comment; it does nothing. This is
useful mainly in command files


{interrupt}
An interrupt (often Ctrl-c) does not exit from gdb, but rather terminates the
action of any gdb command that is in progress and returns to gdb command prompt.
It is safe to type the interrupt character at any time because gdb does not
allow it to take effect until a time when it is safe.

What if the program is running but you forgot to set breakpoints? You can hit
CTRL-C and that'll stop the program wherever it happens to be and return you to
a "(gdb)" prompt. At that point, you could set up a proper breakpoint somewhere
and continue to that breakpoint.


{help}
help class

Using one of the general help classes as an argument, you can get a list of the
individual commands in that class.

help command

With a command name as help argument, gdb displays a short paragraph on how to
use that command.

apropos args

The apropos command searches through all of the gdb commands, and their
documentation, for the regular expression specified in args. It prints out all
matches found.


={============================================================================
*kt_linux_tool_300* gdb-info

To inquire about the state of your program, or the state of gdb itself.

{info}
This command (abbreviated i) is for describing the state of your program. For
example, you can show the arguments passed to a function with "info args", list
the registers currently in use with "info registers", or list the breakpoints
you have set with "info breakpoints". You can get a complete list of the info
sub-commands with help info.

info args
info registers
info breakpoints

{show}
In contrast to info, show is for describing the state of gdb itself. You can
change most of the things you can show, by using the related command set; 

for example, you can control what number system is used for displays with set
radix, or simply inquire which is currently in use with show radix. To display
all the settable parameters and their current values, you can use show with no
arguments; you may also use info set. Both commands produce the same display.


={============================================================================
*kt_linux_tool_300* gdb-trace: call trace

When see call traces using bt, it does not show:

1. function calls which is completed along the call tree.
2. function calls which do "sleep()" call in an atempt to see if call traces
up to this point when attaching gdb.

Q: how to put some code to provide a point to use gdb?


={============================================================================
*kt_linux_tool_300* gdb-trace: get a call trace promatically

In running gst-launch, see the call traces made automatically as shown.

Caught SIGSEGV
#0  0x7753cd1c in __lll_lock_wait () from /lib/libpthread.so.0
#1  0x775446fc in pthread_mutex_lock () from /lib/libpthread.so.0
#2  0x772c4fcc in g_mutex_lock () from /opt/zinc-trunk/oss/lib/libglib-2.0.so.0
#3  0x75b6f2ac in gst_nexus_mgr_set_rate (mgr=0x48e0b0, settings=0x47ff18, res=0x0, rate=1) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexusmgr.c:1097
#4  0x75b74aa0 in gst_nexus_sink_play_locked (sink=0x47fd18) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexussink.c:1531
#5  gst_nexus_sink_change_state_locked (transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING, sink=0x47fd18) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexussink.c:1354
#6  gst_nexus_sink_change_state (element=<optimized out>, transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexussink.c:1326
#7  0x77464ed8 in gst_element_change_state (element=0x47fd18, transition=<optimized out>) at gstelement.c:2602
#8  0x77465aac in gst_element_set_state_func (element=0x47fd18, state=<optimized out>) at gstelement.c:2558
#9  0x7743a954 in gst_bin_element_set_state (next=GST_STATE_PLAYING, current=GST_STATE_PAUSED, start_time=0, base_time=592515640814054, element=0x47fd18, bin=0x489078) at gstbin.c:2328
#10 gst_bin_change_state_func (element=0x489078, transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING) at gstbin.c:2665
#11 0x77490e24 in gst_pipeline_change_state (element=0x489078, transition=GST_STATE_CHANGE_PAUSED_TO_PLAYING) at gstpipeline.c:474
#12 0x77464ed8 in gst_element_change_state (element=0x489078, transition=<optimized out>) at gstelement.c:2602
#13 0x77465aac in gst_element_set_state_func (element=0x489078, state=<optimized out>) at gstelement.c:2558
#14 0x00404c5c in main (argc=28, argv=0x7f9c2c94) at gst-launch.c:1095
Spinning.  Please run 'gdb gst-launch-1.0 15054' to continue debugging, Ctrl-C
to quit, or Ctrl-\ to dump core.


gstreamer-1.5.0/tools/gst-launch.c

static void
fault_handler_sighandler (int signum)
{
  fault_restore ();

  /* printf is used instead of g_print(), since it's less likely to
   * deadlock */
  switch (signum) {
    case SIGSEGV:
      fprintf (stderr, "Caught SIGSEGV\n");
      break;
    case SIGQUIT:
      if (!quiet)
        printf ("Caught SIGQUIT\n");
      break;
    default:
      fprintf (stderr, "signo:  %d\n", signum);
      break;
  }

  fault_spin ();
}

static void
fault_spin (void)
{
  int spinning = TRUE;

  glib_on_error_halt = FALSE;
  g_on_error_stack_trace ("gst-launch-" GST_API_VERSION);

  wait (NULL);

  /* FIXME how do we know if we were run by libtool? */
  fprintf (stderr,
      "Spinning.  Please run 'gdb gst-launch-" GST_API_VERSION " %d' to "
      "continue debugging, Ctrl-C to quit, or Ctrl-\\ to dump core.\n",
      (gint) getpid ());
  while (spinning)
    g_usleep (1000000);
}

static void
fault_restore (void)
{
  struct sigaction action;

  memset (&action, 0, sizeof (action));
  action.sa_handler = SIG_DFL;

  sigaction (SIGSEGV, &action, NULL);
  sigaction (SIGQUIT, &action, NULL);
}

static void
fault_setup (void)
{
  struct sigaction action;

  memset (&action, 0, sizeof (action));
  action.sa_handler = fault_handler_sighandler;

  sigaction (SIGSEGV, &action, NULL);
  sigaction (SIGQUIT, &action, NULL);
}

int
main (int argc, char *argv[])
{
  ...

#ifdef G_OS_UNIX
    fault_setup ();
#endif

}


={============================================================================
*kt_linux_tool_300* gdb-sources, source path, and substitute

9 Examining Source Files

gdb can print parts of your program's source, since the debugging information
recorded in the program tells gdb what source files were used to build it.

To print lines from a source file, use the `list` command (abbreviated l). By
default, ten lines are printed.

list linenum
Print lines centered around line number linenum in the current source file.

list function
Print lines centered around the beginning of function function.

list 
Print more lines. If the last lines printed were printed with a list command,
this prints lines following the last lines printed; however, if the last line
  printed was a solitary line printed as part of displaying a stack frame (see
      Chapter 8 [Examining the Stack], page 91), this prints lines centered
  around that line.

note:
(gdb) i sources
Source files for which symbols have been read in:


{source-path}
9.5 Specifying Source Directories

Executable programs sometimes do not record the directories of the source files
from which they were compiled, just the names. Even when they do, the
directories could be moved between the compilation and your debugging session.
gdb has a list of directories to search for source files; this is called the
source path.

Each time gdb wants a source file, it tries all the directories in the list, in
the 'order' they are present in the list, until it finds a file with the desired
name.

For example, suppose an executable references the file
/usr/src/foo-1.0/lib/foo.c, and our source path is /mnt/cross. The file is first
looked up literally; if this fails, /mnt/cross/usr/src/foo-1.0/lib/foo.c is
tried; if this fails, /mnt/cross/foo.c is opened; if this fails, an error
message is printed.

Plain file names, relative file names with leading directories, file names
containing dots, etc. are all treated as described above.

Note that the executable search path is not used to locate the source files.

When you start gdb, its source path includes only `cdir` and `cwd`, in that
order. To add other directories, use the `directory` command.

The search path is used to find 'both' program source files and gdb script files
(read using the `-command` option and `source` command).


{substitution}
A substitution rule specifies how to rewrite source directories stored in the
program's debug information in case the sources were moved to a different
directory between compilation and debugging.

gdb does a simple string replacement of from with to at the start of the
directory part of the source file name, and uses that result instead of the
original file name to look up the sources.

a rule is applied only if the from part of the directory name 'ends' at a
directory separator and only at the 'beginning' of the directory name,

‘/mnt/cross’ will be applied to ‘/usr/source/foo-1.0’ but not to
‘/usr/sourceware/foo-2.0’ and not be applied to ‘/root/usr/source/baz.c’ either.

In many cases, you can achieve the same result using the directory command.
However, set substitute-path can be more efficient in the case where the sources
are organized in a complex tree with multiple subdirectories.

set substitute-path from to

Define a source path substitution rule, and add it at the end of the current
list of existing substitution rules. If a rule with the same from was already
defined, then the old rule is also deleted.

For example, if the file ‘/foo/bar/baz.c’ was moved to ‘/mnt/cross/baz.c’, then
the command

(gdb) set substitute-path /usr/src /mnt/cross

will tell gdb to replace ‘/usr/src’ with ‘/mnt/cross’, which will allow gdb to
find the file ‘baz.c’ even though it was moved.

show substitute-path [path]

If a path is specified, then print the source path substitution rule which would
rewrite that path, if any. If no path is specified, then print all existing
source path substitution rules.


<to-reset>
If your source path is cluttered with directories that are no longer of
interest, gdb may sometimes cause confusion by finding the wrong versions of
source. You can correct the situation as follows:

1. Use directory with no argument to reset the source path to its default value.
2. Use directory with suitable arguments to reinstall the directories you want
in the source path.  You can add all the directories in one command.


<todo>
You can configure a default source path substitution rule by configuring gdb
with the ‘--with-relocated-sources=dir’ option. The dir should be the name of a
directory under gdb’s configured prefix (set with ‘--prefix’ or
    ‘--exec-prefix’), and directory names in debug information under dir will be
adjusted automatically if the installed gdb is moved to a new location. This is
useful if gdb, libraries or executables with debug information and corresponding
source code are being moved together.


={============================================================================
*kt_linux_tool_300* gdb-exem-memory

10.6 Examining Memory

You can use the `command x` (for "examine") to examine memory in any of several
formats, independently of your program's data types.

x/nfu addr
x addr
x
    Use the x command to examine memory. 

n, f, and u are all `optional parameters` that specify how much memory to
display and how to format it; addr is an expression giving the address where
you want to start displaying memory. If you use defaults for nfu, you need not
type the slash ‘/’. Several commands set convenient defaults for addr.

n, the repeat count
    The repeat count is a decimal integer; the default is 1. It specifies how
    much memory (counting by units u) to display.

f, the display format
    The display format is one of the formats used by `print` (‘x’, ‘d’, ‘u’, ‘o’,
        ‘t’, ‘a’, ‘c’, ‘f’, ‘s’), 

    and in addition 'i' (for machine instructions). 

    The 'default' is 'x' (hexadecimal) initially. The default changes each time
    you use either x or print.

u, the unit size
    The unit size is any of

    b Bytes.
    h Halfwords (two bytes).
    w Words (four bytes). This is the initial 'default'.
    g Giant words (eight bytes). 

    Each time you specify a unit size with x, that size becomes the default unit
    the next time you use x. 
    
    For the ‘i’ format, the unit size is ignored and is normally not written. 
    
    For the ‘s’ format, the unit size defaults to ‘b’, unless it is explicitly
    given. Use x /hs to display 16-bit char strings and x /ws to display 32-bit
    strings. The next use of x /s will again display 8-bit strings. Note that
    the results depend on the programming language of the current compilation
    unit. If the language is C, the ‘s’ modifier will use the UTF-16 encoding
    while ‘w’ will use UTF-32. The encoding is set by the programming language
      and cannot be altered.

addr, starting display address

    addr is the address where you want gdb to begin displaying memory. The
    expression need not have a pointer value (though it may); it is always
    interpreted as an integer address of a byte of memory. See Expressions, for
    more information on expressions. 
    
    The 'default' for addr is usually just after the last address examined—but
    several other commands also set the default address: info breakpoints (to
        the address of the last breakpoint listed), info line (to the starting
          address of a line), and print (if you use it to display a value from
            memory). 

For example, ‘x/3uh 0x54320’ is a request to display three halfwords (h) of
memory, formatted as unsigned decimal integers (‘u’), starting at address
0x54320. ‘x/4xw $sp’ prints the four words (‘w’) of memory above the stack
pointer (here, ‘$sp’; see Registers) in hexadecimal (‘x’).

Since the letters indicating unit sizes are all distinct from the letters
specifying output formats, you do not have to remember whether unit size or
format comes first; either order works. The output specifications ‘4xw’ and
‘4wx’ mean exactly the same thing. (However, the count n must come first; ‘wx4’
    does not work.)

Even though the unit size u is ignored for the formats ‘s’ and ‘i’, you might
still want to use a count n; for example, ‘3i’ specifies that you want to see
three machine instructions, including any operands. For convenience, especially
when used with the display command, the ‘i’ format also prints branch delay slot
instructions, if any, beyond the count specified, which immediately follow the
last instruction that is within the count. The command disassemble gives an
alternative way of inspecting machine instructions; see Source and Machine Code.

All the defaults for the arguments to x are designed to make it easy to continue
scanning memory with minimal specifications each time you use x. For example,
after you have inspected three machine instructions with ‘x/3i addr’, you can
  inspect the next seven with just ‘x/7’. If you use <RET> to repeat the x
  command, the repeat count n is used again; the other arguments default as for
  successive uses of x.

When examining machine instructions, the instruction at current program counter
is shown with a => marker. For example:

     (gdb) x/5i $pc-6
        0x804837f <main+11>: mov    %esp,%ebp
        0x8048381 <main+13>: push   %ecx
        0x8048382 <main+14>: sub    $0x4,%esp
     => 0x8048385 <main+17>: movl   $0x8048460,(%esp)
        0x804838c <main+24>: call   0x80482d4 <puts@plt>

The addresses and contents printed by the x command are 'not' saved in the value
'history' because there is often too much of them and they would get in the way.
Instead, gdb makes these values available for subsequent use in expressions as
values of the convenience variables $_ and $__. After an x command, the last
address examined is available for use in expressions in the convenience variable
$_. The contents of that address, as examined, are available in the convenience
variable $__.

If the x command has a repeat count, the address and contents saved are from the
last memory unit printed; this is not the same as the last address printed if
several units were printed on the last line of output.

Most targets have an addressable memory unit size of 8 bits. This means that to
each memory address are associated 8 bits of data. Some targets, however, have
other addressable memory unit sizes. Within gdb and this document, the term
addressable memory unit (or memory unit for short) is used when explicitly
referring to a chunk of data of that size. The word byte is used to refer to a
chunk of data of 8 bits, regardless of the addressable memory unit size of the
target. For most systems, addressable memory unit is a synonym of byte.

When you are debugging a program running on a remote target machine (see Remote
    Debugging), you may wish to verify the program's image in the remote
machine's memory against the executable file you downloaded to the target. Or,
on any target, you may want to check whether the program has corrupted its own
  read-only sections. The compare-sections command is provided for such
  situations.

<ex>
--eval-command 'x/100a $sp' \
--eval-command 'x/50i $pc' \


={============================================================================
*kt_linux_tool_300* gdb-exem-print

10.5 Output Formats

By default, gdb prints a value according to its data type. Sometimes this is
not what you want. For example, you might want to print a number in hex, or a
pointer in decimal. Or you might want to view data in memory at a certain
address as a character string or as an instruction. To do these things,
specify an output format when you print a value.

The simplest use of output formats is to say how to print a value already
computed. This is done by starting the arguments of the print command with a
slash and a format letter. The format letters supported are:

x
    Regard the bits of the value as an integer, and print the integer in
    hexadecimal.

d
    Print as integer in signed decimal.

u
    Print as integer in unsigned decimal.

o
    Print as integer in octal.

t
    Print as integer in binary. The letter ‘t’ stands for “two”. 1

a
    Print as an address, both absolute in hexadecimal and as an offset from
    the `nearest preceding symbol` You can use this format used `to discover`
    where (in what function) an unknown address is located:

              (gdb) p/a 0x54320
              $3 = 0x54320 <_initialize_vx+396>

    The command `info symbol 0x54320` yields similar results. See info symbol.

c
    Regard as an integer and print it as a character constant. This prints
    both the numerical value and its character representation. The character
    representation is replaced with the octal escape ‘\nnn’ for characters
    outside the 7-bit ascii range.

    Without this format, gdb displays char, unsigned char, and signed char
    data as character constants. Single-byte members of vectors are displayed
    as integer data.

f
    Regard the bits of the value as a floating point number and print using
    typical floating point syntax.

s
    Regard as a string, if possible. With this format, pointers to single-byte
    data are displayed as null-terminated strings and arrays of single-byte
    data are displayed as fixed-length strings. Other values are displayed in
    their natural types.

    Without this format, gdb displays pointers to and arrays of char, unsigned
    char, and signed char as strings. Single-byte members of a vector are
    displayed as an integer array.

z
    Like ‘x’ formatting, the value is treated as an integer and printed as
    hexadecimal, but leading zeros are printed to pad the value to the size of
    the integer type.

r
    Print using the ‘raw’ formatting. By default, gdb will use a Python-based
    pretty-printer, if one is available (see Pretty Printing). This typically
    results in a higher-level display of the value's contents. The ‘r’ format
    bypasses any Python pretty-printer which might exist. 

For example, to print the program counter in hex (see Registers), type

     p/x $pc

Note that no space is required before the slash; this is because command names
in gdb cannot contain a slash.

To reprint the last value in the value history with a different format, you
can use the print command with just a format and no expression. For example,
‘p/x’ reprints the last value in hex. 


(gdb) printf "%d\n", i
40
(gdb) printf "%08X\n", i
00000028

print i
Print the value of variable i.

print *p
Print the contents of memory pointed to by p, where p is a pointer variable.

(gdb) whatis pszSection
type = uint32_t *
(gdb) print *pszSection

print x.field
Check the different members of a structure.

print x
Check all the members of a structure, assuming x is a structure.

print y-field
y is a pointer to a structure.

print array[i]
Print the i'th element of array.

print array
Print all the elements of array.

(gdb) print /x block1->magic
$5 = 0xabeaa5b3

(gdb) print /x block1 
$9 = 0x1150f4c

(gdb) print /x *block1
$8 = {magic = 0xabeaa5b3, size = 0x28, line = 0x0, owner = 0x0, header = 0x21, data = {free = {previous = 0x8, next = 0x115106c}, 
    userData = {0x0}}}

# db_contexts_array is a global var
(gdb) p db_contexts_array
(gdb) p db_contexts_array[-1]


<gdb-disp>
to inspect over the course of the run

(gdb) [disp]lay var
(gdb) [undisp] disp_num
(gdb) info disp 	" to list disps


{gdb-set}
can 'set' them to better values using the p command, since it can print the
value of any expression that expression can include subroutine calls and
assignments. set new value and continue to see if it fixes the bug.

(gdb) p len lquote=strlen(lquote)
$5 = 7
(gdb) p len rquote=strlen(rquote)
$6 = 9
(gdb) c
Continuing.


={============================================================================
*kt_linux_tool_300* gdb-exam: between address and code

9.6 Source and Machine Code

You can use the command "info line" to map source lines to program addresses
(and vice versa), and the command disassemble to display a range of addresses as
machine instructions.

info line linespec

Print the starting and ending addresses of the compiled code for source line
linespec. You can specify source lines in any of the ways.

For example, we can use info line to discover the location of the object code
for the first line of 'function' m4_changequote:

(gdb) info line m4_changequote
Line 895 of "builtin.c" starts at pc 0x634c and ends at 0x6350.

We can also inquire (using *addr as the form for linespec) what source line
covers a particular address:

(gdb) info line *0x63ff
Line 926 of "builtin.c" starts at pc 0x63e4 and ends at 0x6404.


note: changes pc?

After info line, the default address for the x command is 'changed' to the
starting address of the line, so that 'x/i' is sufficient to begin examining the
machine code

disassemble
disassemble /m
disassemble /r

This specialized command dumps a range of memory as machine instructions. It can
also print 'mixed' source+disassembly by specifying the /m modifier and print
the raw instructions in hex as well as in symbolic form by specifying the /r. 

The default memory range is the function surrounding the "program counter" of
the selected frame.  A single argument to this command is a program counter
value; gdb dumps the function surrounding this value. 
    
When two arguments are given, they should be separated by a comma, possibly
surrounded by whitespace. The arguments specify a range of addresses to dump, in
one of two forms: 

start,end 

the addresses from start (inclusive) to end (exclusive)

The argument(s) can be any expression yielding a numeric value, such as 0x32c4,
    &main+10 or $pc-8.

<address>
Addresses 'cannot' be specified as a linespec (see Section 9.2 [Specify
    Location]). So, for example, if you want to disassemble function bar in file
‘foo.c’, you must type ‘disassemble ’foo.c’::bar’ and not ‘disassemble
foo.c:bar’.


set disassemble-next-line
show disassemble-next-line

Control whether or not gdb will disassemble the next source line or instruction
when execution stops. If ON, gdb will display disassembly of the next source
line when execution of the program being debugged stops. This is in addition to
displaying the source line itself, which gdb always does if possible. 

If the next source line cannot be displayed for some reason (e.g., if gdb cannot
    find the source file, or there's no line info in the debug info), gdb will
display disassembly of the next instruction instead of showing the next source
line. If AUTO, gdb will display disassembly of next instruction only if the
source line cannot be displayed. This setting causes gdb to display some
feedback when you step through a function with no line info or whose source file
is unavailable.  The default is OFF, which means never display the disassembly
of the next line or instruction.


<shared-libraries>
For programs that were dynamically linked and use shared libraries, instructions
that call functions or branch to locations in the shared libraries might show a
seemingly bogus location; it's actually a location of the relocation table. On
some architectures, gdb might be able to resolve these to actual function names.

note: might? how about linux?


={============================================================================
*kt_linux_tool_300* gdb-exam: symbols

16 Examining the Symbol Table

To allow gdb to recognize 'foo.c' as a single symbol, enclose it in single
quotes; for example,

p ’foo.c’::x


{symbol-and-address}

info address <symbol>

Print the address of a symbol. Describe where the data for symbol is stored. For
a register variable, this says which register it is kept in. For a non-register
local variable, this prints the stack-frame offset at which the variable is
always stored.

info symbol <addr>

Print the name of a symbol which is stored at the address addr. If no symbol is
stored exactly at addr, gdb prints the nearest symbol and an offset from it:

(gdb) info symbol 0x54320
_initialize_vx + 396 in section .text

This is the 'opposite' of the info address command. You can use it to find out
the name of a variable or a function given its address.

<shared-library>
For dynamically linked executables, the name of executable or shared library
containing the symbol is also printed:

(gdb) info symbol 0x400225
_start + 5 in section .text of /tmp/a.out
(gdb) info symbol 0x2aaaac2811cf
__read_nocancel + 6 in section .text of /usr/lib64/libc.so.6


{demangle}
demangle [-l language] [--] name

Demangle name. If language is provided it is the name of the language to
demangle name in. Otherwise name is demangled in the current language.
The ‘--’ option specifies the end of options, and is useful when name begins
with a dash.
The parameter demangle-style specifies how to interpret the kind of mangling
used. See Section 10.8 [Print Settings], page 121.


{whatis}
whatis[/flags] [arg]

Print the data 'type' of arg, which can be either an expression or a name of a
data type. With no argument, print the data type of $, the last value in the
value history. If arg is an expression (see Section 10.1 [Expressions], page
    109), it is not actually evaluated, and any side-effecting operations (such
      as assignments or function calls) inside it do not take place. 

If arg is a variable or an expression, whatis prints its literal type as it is
used in the source code. If the type was defined using a typedef, whatis will
not print the data type underlying the typedef. If the type of the variable or
the expression is a compound data type, such as struct or class, whatis never
prints their fields or methods. It just prints the struct/class name a.k.a.  its
tag. 

If you want to see the members of such a compound data type, use ptype. If arg
is a type name that was defined using typedef, whatis unrolls only one level of
that typedef. Unrolling means that whatis will show the underlying type used in
the typedef declaration of arg. However, if that underlying type is also a
typedef, whatis will not unroll it.

(gdb) whatis ppkSectionData
type = const uint8_t **

flags can be used to modify how the type is displayed. Available flags are:

r 
Display in "raw" form. Normally, gdb substitutes template parameters and
typedefs defined in a class when printing the class’ members. The /r flag
disables this.

m 
Do not print methods defined in the class. M Print methods defined in the class.
This is the 'default', but the flag exists in case you change the default with
set print type methods.

t 
Do not print typedefs defined in the class. Note that this controls whether the
typedef definition itself is printed, not whether typedef names are substituted
when printing other types.

T 
Print typedefs defined in the class. This is the default, but the flag exists in
case you change the default with set print type typedefs.


{ptype}
ptype[/flags] [arg]

ptype accepts the same arguments as whatis, but prints a 'detailed' description
of the type, instead of just the name of the type.

Contrary to whatis, ptype 'always' 'unrolls' any typedefs in its argument
declaration, whether the argument is a variable, expression, or a data type.


typedef double real_t;
struct complex { real_t real; double imag; };

typedef struct complex complex_t;

complex_t var;
real_t *real_pointer_var;

(gdb) whatis var
type = complex_t

(gdb) ptype var
type = struct complex {
    real_t real;
    double imag;
}

(gdb) whatis complex_t
type = struct complex

(gdb) whatis struct complex
type = struct complex

(gdb) ptype struct complex
type = struct complex {
    real_t real;
    double imag;
}

(gdb) whatis real_pointer_var
type = real_t *

(gdb) ptype real_pointer_var
type = double *


{info-scope}
info scope location

List all the variables local to a particular scope. This command accepts a
location argument; a function name, a source line, or an address preceded by a
'*', and prints all the variables local to the scope defined by that location.

For example:

(gdb) info scope command_line_handler
Scope for command_line_handler:
Symbol rl is an argument at stack/frame offset 8, length 4.
Symbol linebuffer is in static storage at address 0x150a18, length 4.
Symbol linelength is in static storage at address 0x150a1c, length 4.
Symbol p is a local variable in register $esi, length 4.
Symbol p1 is a local variable in register $ebx, length 4.
Symbol nline is a local variable in register $edx, length 4.
Symbol repeat is a local variable at frame offset -8, length 4.

This command is especially useful for determining what data to collect during a
trace experiment


{info-sources}
info source

Show information about the current source file

info sources

Print the names of 'all' source files in your program for which organized into
two lists: files whose symbols have already been read, and files whose symbols
'will' be read when needed.


{info-variables}
info variables

Print the names and data types of all variables that are defined 'outside' of
functions i.e. excluding local variables.


={============================================================================
*kt_linux_tool_300* gdb-exam: symbols loading

note:
not supported in 7.2

set print symbol-loading
set print symbol-loading full
set print symbol-loading brief
set print symbol-loading off
show print symbol-loading

The set print symbol-loading command allows you to control the printing of
messages when gdb loads symbol information. By default a message is printed for
the executable and one for each shared library, and normally this is what you
want. 
    
However, when debugging apps with large numbers of shared libraries these
messages can be annoying.  When set to 'brief' a message is printed for each
executable, and when gdb loads a collection of shared libraries at once it will
only print one message regardless of the number of shared libraries.


={============================================================================
*kt_linux_tool_300* gdb-exam: slib when with -g and without -g

note no -g on shared

  $ gcc -fpic -shared -o foo.so foo.c
  $ gcc -o main main.c ./foo.so -g
  $ gdb main

note: pending

  (gdb) b foo
  Function "foo" not defined.
  Make breakpoint pending on future shared library load? (y or [n]) y
  Breakpoint 1 (foo) pending.

  (gdb) r
  Starting program: /home/bhushan/RD/Shared_Lib_Debug/main
  Reading symbols from shared object read from target memory...done.
  Loaded system supplied DSO at 0x470000
  Breakpoint 2 at 0xdb7493

note: pending resolved

  Pending breakpoint "foo" resolved
  Breakpoint 2, 0x00db7493 in foo () from ./foo.so

  (gdb) s
  Single stepping until exit from function foo,
  which has no line number information.
  main () at main.c:7
  7 printf("inside main i = %d\n", i);

  (gdb) s
  inside main i = 4
  8 return 0;


NOW if you build the shared libarary using -g option

  $ gcc -fpic -shared -o foo.so foo.c -g
  $ gcc -o main main.c ./foo.so -g
  $ gdb main

  (gdb) b foo

  Function "foo" not defined.
  Make breakpoint pending on future shared library load? (y or [n]) y
  Breakpoint 1 (foo) pending.

  (gdb) r
  Starting program: /home/bhushan/RD/Shared_Lib_Debug/main
  Reading symbols from shared object read from target memory...done.
  Loaded system supplied DSO at 0x470000
  Breakpoint 2 at 0x1c5493: file foo.c, line 5.
  Pending breakpoint "foo" resolved

  Breakpoint 2, foo () at foo.c:5
  5 return 2*2;

  (gdb) s
  7 }

  (gdb) s
  main () at main.c:7
  7 printf("inside main i = %d\n", i);

  (gdb) s
  inside main i = 4
  8 return 0;

  U can see the differences in bold lines.


={============================================================================
*kt_linux_tool_300* gdb-exam: core when with -g and without -g

1. Seems that core file are the same since the size is the same and show the
same gdb result when use with both debug and release executable.

-rw-------   1 kpark kpark 397312 Sep  1 14:11 7842.core
-rw-------   1 kpark kpark 397312 Sep  1 14:15 7960.core
-rwxr-xr-x   1 kpark kpark  27974 Sep  1 13:40 a-g.out*
-rwxr-xr-x   1 kpark kpark  10350 Sep  1 14:15 a-r.out*

<debug> with -g option

$ gdb a-g.out 7960.core
...
Reading symbols from /home/kpark/work/a-g.out...done.

warning: core file may not match specified executable file.
[New LWP 7960]

warning: Can't read pathname for load map: Input/output error.
Core was generated by `./a.out'.
Program terminated with signal 11, Segmentation fault.
#0  0x0000000000400a1a in main () at tdebug.cpp:16
16	    cout << "anode's next is " << anode.next->val << endl;;

(gdb) l
11	
12	int main()
13	{
14	    node anode = { NULL, 0 };
15	
16	    cout << "anode's next is " << anode.next->val << endl;;
17	
18	    cout << "anode {" << anode.next << ", " << anode.val << "}" << endl;
19	}

(gdb) bt
#0  0x0000000000400a1a in main () at tdebug.cpp:16
(gdb) 


<release>
$ gdb a.out 7960.core
...
Reading symbols from /home/kpark/work/a.out...(no debugging symbols found)...done.
Illegal process-id: 7960.core.
[New LWP 7960]

warning: Can't read pathname for load map: Input/output error.
Core was generated by `./a.out'.
Program terminated with signal 11, Segmentation fault.
#0  0x0000000000400a1a in main ()

(gdb) l
No symbol table is loaded.  Use the "file" command.

(gdb) bt
#0  0x0000000000400a1a in main ()

2. readelf output above are true for debug and release build.

<ex>
(gdb) bt
#0  0x779dfd1c in __lll_lock_wait () from /lib/libpthread.so.0
#1  0x779e75cc in pthread_mutex_lock () from /lib/libpthread.so.0
#2  0x742993a8 in nickel::system::GstMediaRouter::getPosition() const () from 
  /opt/zinc-trunk/lib/libNickelSystemGStreamer.so

#3  0x766add10 in boost::_bi::bind_t<zinc::Future<Zinc::Media::Position>,
  boost::_mfi::cmf0<zinc::Future<Zinc::Media::Position>,
  Zinc::Media::MediaRouterAsync>, boost::_bi::list1<boost::arg<1> >
    >::result_type
    nickel::system::ProxyMediaRouter::deferForwardOrDefault<boost::_bi::bind_t<zinc::Future<Zinc::Media::Position>,
  boost::_mfi::cmf0<zinc::Future<Zinc::Media::Position>,
  Zinc::Media::MediaRouterAsync>, boost::_bi::list1<boost::arg<1> > >
    >(boost::_bi::bind_t<zinc::Future<Zinc::Media::Position>,
        boost::_mfi::cmf0<zinc::Future<Zinc::Media::Position>,
        Zinc::Media::MediaRouterAsync>, boost::_bi::list1<boost::arg<1> > >,
        nickel::system::returned_future_type<boost::_bi::bind_t<zinc::Future<Zinc::Media::Position>,
        boost::_mfi::cmf0<zinc::Future<Zinc::Media::Position>,
        Zinc::Media::MediaRouterAsync>, boost::_bi::list1<boost::arg<1> > >
        >::type const&) const () from
    /opt/zinc-trunk/lib/libNickelSystemProxy.so

#4  0x7667fe8c in nickel::system::ProxyMediaRouter::getPosition() const () 
    from /opt/zinc-trunk/lib/libNickelSystemProxy.so
#5  0x0044926c in Zinc::Media::(anonymous namespace)::getPosition_stub
    (Zinc::Media::MediaRouterAsync&, DBus::Connection::WeakRef&, DBus::CallMessage const&) ()
#6  0x0045d3ec in Zinc::Media::MediaRouterAsyncToDBus::call_method_async(DBus::CallMessage const&) ()
#7  0x0045d784 in Zinc::Media::MediaRouterAsyncToDBus::handle_message(DBus::Message const&) ()
#8  0x004634d0 in zinc::binding::dbus::
    RefCountedAdaptorDecorator<Zinc::Media::MediaRouterAsyncToDBus>::operator()(DBus::Connection&, DBus::Message&) ()

#9  0x00463678 in DBus::detail::AdaptorWrapper<zinc::binding::dbus::
    RefCountedAdaptorDecorator<Zinc::Media::MediaRouterAsyncToDBus> >::call(DBusConnection*, DBusMessage*, void*) ()

#10 0x777d316c in _dbus_object_tree_dispatch_and_unlock () from /opt/zinc-trunk/oss/lib/libdbus-1.so.3
#11 0x777b4944 in dbus_connection_dispatch () from /opt/zinc-trunk/oss/lib/libdbus-1.so.3
#12 0x778337f8 in DBus::drain_pending_messages(DBus::Connection::Private*) () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#13 0x777b3ea8 in _dbus_connection_update_dispatch_status_and_unlock () from /opt/zinc-trunk/oss/lib/libdbus-1.so.3
#14 0x777adef4 in _dbus_connection_handle_watch () from /opt/zinc-trunk/oss/lib/libdbus-1.so.3
#15 0x777e66dc in dbus_watch_handle () from /opt/zinc-trunk/oss/lib/libdbus-1.so.3
#16 0x77846484 in DBus::Watch::handle(int) () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#17 0x77851f6c in DBus::DefaultWatch::do_callback() () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#18 0x77852444 in DBus::DefaultMainLoop::processCommands() () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#19 0x77854030 in DBus::DefaultMainLoop::dispatch(unsigned int) () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#20 0x77855704 in DBus::BusDispatcher::run() () from /opt/zinc-trunk/lib/libdbus-c++-1.so.0
#21 0x778bd9bc in zinc::binding::dbus::MainLoop::run() () from /opt/zinc-trunk/lib/libZincDbusBindingRuntime.so.0
#22 0x00409674 in main ()

<ex>
(gdb) bt
#0  0x76bab5fc in DRV_AUDIO_Init () from /usr/local/lib/libcvms.so.1
#1  0x76ba6798 in DI_Init () from /usr/local/lib/libcvms.so.1
#2  0x769348c8 in CV_Media_Initialize () from /usr/local/lib/libcvms.so.1
#3  0x773ea5f8 in bronze::system::HumaxMediaStartup::HumaxMediaStartup() () from /opt/zinc/lib/libBronzeSystemHumax.so
#4  0x77454188 in ?? () from /opt/zinc/lib/libBronzeSystemHumax.so
#5  0x0040bfdc in ?? ()
#6  0x0040f7f4 in ?? ()
#7  0x77adb404 in DBus::DefaultMainLoop::processCommands() () from /opt/zinc/lib/libdbus-c++-1.so.0
#8  0x77adcff0 in DBus::DefaultMainLoop::dispatch(unsigned int) () from /opt/zinc/lib/libdbus-c++-1.so.0
#9  0x77ade6c4 in DBus::BusDispatcher::run() () from /opt/zinc/lib/libdbus-c++-1.so.0
#10 0x0040b0d8 in ?? ()
#11 0x776906d4 in __uClibc_main () from /lib/libc.so.0
#12 0x0040a008 in __start ()


={============================================================================
*kt_linux_tool_300* gdb-exam: step when with -g and without -g

1. $ ../install/bin/g++4.9.2 -std=c++0x t_templ.cpp

Do not step into and continue to the end.

Reading symbols from a.out...(no debugging symbols found)...done.
(gdb) start
Temporary breakpoint 1 at 0x400d0a
Starting program: /home/kpark/gcc-build/work/a.out 

Temporary breakpoint 1, 0x0000000000400d0a in main ()
(gdb) s
Single stepping until exit from function main,
which has no line number information.
---
this is main
---
__libc_start_main (main=0x400d06 <main>, argc=1, argv=0x7fffffffdcf8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffdce8) at libc-start.c:321
321	libc-start.c: No such file or directory.
(gdb) 


2. $ ../install/bin/g++4.9.2 -g -std=c++0x t_templ.cpp

Step into the the executable but not into the stl since stl is not debug
version.

Reading symbols from a.out...done.
(gdb) start
Temporary breakpoint 1 at 0x400d0f: file t_templ.cpp, line 21.
Starting program: /home/kpark/gcc-build/work/a.out 

Temporary breakpoint 1, main () at t_templ.cpp:21
21	  cout << "---" << endl;
(gdb) next
---
23	  My mime;
(gdb) 
26	  debug_rep(str);
(gdb) step
debug_rep<std::string> (t="this is string") at t_templ.cpp:14
14	    ostringstream ret;
(gdb) 
15	    ret << t;
(gdb) step                      // note: do not step into
16	    return ret.str();
(gdb) 
17	}


3. $ ../install/bin/g++4.9.2 -g -std=c++0x -D_GLIBCXX_DEBUG t_templ.cpp 

Reading symbols from a.out...done.
(gdb) start
Temporary breakpoint 1 at 0x401643: file t_templ.cpp, line 21.
Starting program: /home/kpark/gcc-build/work/a.out 

Temporary breakpoint 1, main () at t_templ.cpp:21
21	  cout << "---" << endl;

(gdb) 
26	  debug_rep(str);
(gdb) s
debug_rep<std::string> (t="this is string") at t_templ.cpp:14
14	    ostringstream ret;
(gdb) 
15	    ret << t;
(gdb) 
std::operator<< <char, std::char_traits<char>, std::allocator<char> > (__os=..., __str="this is string") at /home/kpark/gcc-build/install/include/c++/4.9.2/bits/basic_string.h:2777
2777	      return __ostream_insert(__os, __str.data(), __str.size());
(gdb) 


={============================================================================
*kt_linux_tool_300* gdb-slib: load, search

18 gdb Files

Normally, GDB will load the shared library symbols automatically. You can
control this behavior using set auto-solib-add command.

set auto-solib-add mode

If mode is on, symbols from all shared object libraries will be loaded
'automatically' when the inferior 'begins' execution, you 'attach' to an
independently started inferior, or when the dynamic linker informs gdb that a
new library has been loaded. If mode is off, symbols must be loaded manually,
using the sharedlibrary command. The 'default' value is on.

<selective-load>
If your program uses lots of shared libraries with debug info that takes large
amounts of memory, you can 'decrease' the gdb memory footprint by preventing it
from automatically loading the symbols from shared libraries. To that end, type
set auto-solib-add off before running the inferior, then load each library whose
debug symbols you do need with sharedlibrary regexp, where regexp is a regular
expression that matches the libraries whose symbols you want to be loaded.

show auto-solib-add

Display the current autoloading mode.


{info-share-and-share}
info share regex
info sharedlibrary regex

'print' the names of the shared libraries which are currently loaded that match
regex. If regex is omitted then print 'all' shared libraries that are loaded.

<ex>
(gdb) i sharedlibrary 
From        To          Syms Read   Shared Object Library
0x773da470  0x77429860  Yes (*)     /opt/zinc/oss/lib/libboost_program_options.so.1.57.0
0x773a99b0  0x773a9b60  Yes (*)     /opt/zinc-trunk/lib/libNickelSystemDbus.so.0
0x77380880  0x77392520  Yes (*)     /opt/zinc-trunk/lib/libNickelSystemAPI.so.0
0x77303b70  0x77350790  Yes (*)     /opt/zinc-trunk/lib/libZincDbusBindingRuntime.so.0
0x7729f170  0x772c3580  Yes (*)     /opt/zinc-trunk/lib/libdbus-c++-1.so.0
0x7720d950  0x77272f20  Yes (*)     /opt/zinc/oss/lib/libdbus-1.so.3
0x7717c940  0x771e6240  Yes (*)     /opt/zinc-trunk/lib/libZincCommon.so.0
0x77151a60  0x77152ed0  Yes (*)     /lib/libdl.so.0
0x77128740  0x7713b9f0  Yes (*)     /opt/zinc/oss/lib/libboost_thread.so.1.57.0
0x770b7950  0x770fb360  Yes (*)     /opt/zinc/oss/lib/liblog4cplus-1.0.so.4
0x770859e0  0x7708e260  Yes (*)     /opt/zinc/oss/lib/libboost_date_time.so.1.57.0
0x77055380  0x770697b0  Yes (*)     /opt/zinc/oss/lib/libboost_filesystem.so.1.57.0
0x7703dd60  0x7703ee30  Yes (*)     /opt/zinc/oss/lib/libboost_system.so.1.57.0
0x770286f0  0x7702b220  Yes (*)     /opt/zinc/oss/lib/libboost_chrono.so.1.57.0
0x76f9d780  0x77001aa0  Yes (*)     /lib/libstdc++.so.6
0x76f3f250  0x76f482f0  Yes (*)     /lib/libm.so.0
0x76f13680  0x76f2c760  Yes (*)     /lib/libgcc_s.so.1
0x76ee5f10  0x76ef2da0  Yes (*)     /lib/libpthread.so.0
0x76e33fd0  0x76e86bb0  Yes (*)     /lib/libc.so.0
0x76e15d20  0x76e174f0  Yes (*)     /lib/librt.so.0
0x76ded730  0x76dfe710  Yes (*)     /opt/zinc/lib/libNickelSystemAPI.so.0
0x76d70b40  0x76dbd760  Yes (*)     /opt/zinc/lib/libZincDbusBindingRuntime.so.0
0x76d0b130  0x76d2f540  Yes (*)     /opt/zinc/lib/libdbus-c++-1.so.0
0x76c6d950  0x76cd7420  Yes (*)     /opt/zinc/lib/libZincCommon.so.0
0x76c33560  0x76c418b0  Yes (*)     /opt/zinc/oss/lib/libz.so.1
0x76b7f6b0  0x76c157e0  Yes (*)     /opt/zinc/oss/lib/libsqlite3.so.0
0x77448eb0  0x7744e6f0  Yes (*)     /lib/ld-uClibc.so.0
0x76b01ac0  0x76b4f5e0  Yes         /opt/zinc-trunk/lib/libNickelSystemProxy.so
0x76ae3c60  0x76ae7690  Yes (*)     /opt/zinc/lib/libZincHttpServer.so.0
0x76aba0b0  0x76acdf10  Yes (*)     /opt/zinc/lib/libZincHttpClient.so.0
...


sharedlibrary regex
share regex

'load' shared object library symbols for files matching a Unix regular
expression.  As with files loaded automatically, it only loads shared libraries
required by your program for a core file or 'after' typing run. If regex is
omitted 'all' shared libraries required by your program are loaded.

note: 
Can specify library name which are required by a program but 'not' a filename
with a path to force a loading of it. That means cannot load a libaray until a
program needs it.

nosharedlibrary

'unload' all shared object library symbols. This discards all symbols that have
been loaded from all shared libraries. Symbols from shared libraries that were
loaded by explicit user requests are 'not' discarded.


{support-shared-library-remote} from 18 gdb Files
Shared libraries are also supported in many cross or remote debugging
configurations. gdb needs to have access to the target's libraries; this can be
accomplished either by providing copies of the libraries "on the host system",
             or by asking gdb to automatically retrieve the libraries from the
             target. 

If copies of the target libraries are provided, they need to be the same as the
target libraries, although the copies on the 'target' 'can' be 'stripped' as
long as the copies on the host are not.

For remote debugging, you need to tell gdb where the target libraries are, so
that it can load the correct copies. otherwise, it may try to load the host's
libraries. gdb has two variables to specify the search directories for target
libraries.


<sysroot>
set sysroot path

Use path as the system root 'for' the program being debugged. Any 'absolute'
shared library paths will be 'prefixed' with path; many runtime loaders store
the absolute paths to the shared library in the target program's memory. 

If you use set sysroot to find shared libraries, they need to be laid out in the
'same' way that they are on the target, with e.g. a '/lib' and '/usr/lib'
hierarchy under path.

The set solib-absolute-prefix command is an 'alias' for set sysroot.

show sysroot

Display the current shared library prefix.

note: said that this command forces to 'reload' shared libraries after changes
debuggee using file command.

(gdb) set sysroot /


<solib-search-path>
set solib-search-path path

If this variable is set, path is a colon-separated list of directories to search
for shared libraries. solib-search-path is used after sysroot fails to locate
the library, or if the path to the library is relative instead of absolute. 

note:
If you want to use solib-search-path instead of 'sysroot', be sure to set
sysroot to a nonexistent directory to prevent gdb from finding your host's
libraries.

sysroot is preferred; setting it to a nonexistent directory may interfere with
automatic loading of shared library symbols.

show solib-search-path

Display the current shared library search path.


{when-need-manual}
However, in some cases (e.g. when debugging with gdbserver and having
    incompatible symbols or using old Android toolchains) GDB will not load the
symbols automatically. In this case you can use the info sharedlibrary command
to list the loaded shared libraries and the sharedlibrary command to force the
symbols to be loaded. 

<check-solib-search-path>
If GDB does not automatically load debugging symbols for your library when
debugging with gdbserver, please check the search path using the set
solib-search-path command.


={============================================================================
*kt_linux_tool_300* gdb-slib: case

In this example we will disable shared library loading using the set
auto-solib-add command, then run the application, list the source files and load
the symbols manually:

  (gdb) set auto-solib-add off
  (gdb) break main
  Breakpoint 1 at 0x80484ed: file main.cpp, line 7.
  (gdb) run
  Starting program: /home/testuser/libtest/testApp

  Breakpoint 1, main () at main.cpp:7
  7 printf("In main()\n");

  (gdb) info sources
  Source files for which symbols have been read in:

  /home/testuser/libtest/main.cpp

  Source files for which symbols will be read in on demand:

  (gdb) info sharedlibrary
  From To Syms Read Shared Object Library
  0xb7fde820 0xb7ff6b9f No /lib/ld-linux.so.2
  0xb7fd83a0 0xb7fd84c8 No /home/testuser/libtest/libTest.so
  0xb7e30f10 0xb7f655cc No /lib/i386-linux-gnu/libc.so.6

  (gdb) sharedlibrary libTest
  Reading symbols from /home/testuser/libtest/libTest.so...done.
  Loaded symbols for /home/testuser/libtest/libTest.so

  (gdb) info sources
  Source files for which symbols have been read in:

  /home/testuser/libtest/main.cpp

  Source files for which symbols will be read in on demand:

  /home/testuser/libtest/lib.cpp    // note. see added source file

  (gdb) break lib.cpp:5
  Breakpoint 2 at 0xb7fd846e: file lib.cpp, line 5.

  (gdb) continue
  Continuing.
  In main()

  Breakpoint 2, func () at lib.cpp:5
  5 printf("In func()\n");


={============================================================================
*kt_linux_tool_300* gdb-case: debugging with strace and gdb in sandbox

Getting a core dump from the sandboxed application

First of all, bad things happen and sometimes we need to find out what went
wrong. Because the sandbox is intended to be robust, we do not want to leave any
back-door to get into with the debugger or tracing tools. Obviously back-door
hooks are allowed for debug builds but sometimes getting a debug build is
inconvenient.

In this short article I want to focus on a rapid approach to get to the point as
quickly as possible. Please bear in mind that this is more a bunch of advise
rather than a copy-and-paste approach. A bit of warning: if you get frustrated
very early with questions like "where do I get this" or "where do I get that",
then have a break with a cup of tea, ask people around etc. If you're still
  frustrated after that then probably this article is not for you.

note:
The first thing to try is to get the core dump and then possibly use strace
and/or GDB. By GDB I mean here a 'native' build of GDB rather than gdbserver as
debugging with the latter one gets really awkward with a "heavy" Stagecraft
process.

Note that these tools take control over the application process and affect
process relations, i.e.  they get in the middle of the application process and
its parent. This has implications wherever process relations are important, e.g.
no remote control input will be directed to the application process and the
application process will not be allowed to write into the screen buffer.

Getting the core dump

This is the easiest thing to do and requires the least effort. Obviously you
need a seg-fault. So if you don't have a seg-fault handy, then this is not for
you. You need to get a grip and carry on.

You need to apply all below before starting the babysitterd (C13) (uimanagerd
    before C13).

The instructions below must be applied in the same console which the babysitterd
is started from.  Note that it won't work when starting the babysitterd over the
D-BUS as the D-BUS daemon will not be aware of the new settings until restarted.

In this situation start the babysitterd from the command line.

# tell the kernel where to store the core file (it's going to be from within the
# sandbox, so bear with me)

echo /opt/adobe/stagecraft/data/core > /proc/sys/kernel/core_pattern

# tell the kernel we don't mind ending with a large core file (use your
# imagination to get the size you like)

ulimit -c 10000000000

# in case of a crash in any *setuid* executable, additionally you need to do the
# following:

echo 1 > /proc/sys/fs/suid_dumpable
chmod +r <setuid executable>

# start the babysitterd, launch the UIME and app and make it crash

Note that currently the only location where the sandboxed application can write
to, is its private /tmp which is not persistent. The other one is its private
"copy" of /opt/adobe/stagecraft/data tree. Obviously the choice it to create the
core file in /opt/adobe/stagecraft/data.

Where to look for:

private writeable area mapped as /opt/adobe/stagecraft/data:
$PREFIX/var/applications/data/air/<app CRID sha256>/stagecraft-data/

private log files:
$PREFIX/var/applications/data/air/<app CRID sha256>/log/


* Attaching to the application process

Unless the investigation is related to the application startup, it is better and
easier to attach to the process. 

Strace

# get the stagecraft process(es) PID(s)
/opt/zinc/oss/bin/busybox pgrep stagecraft

# pick the interesting process PID and strace it
strace -ff -o /tmp/stagecraft.log -s 128 -p <PID>

Please note that some of the Stagecraft threads are quite "intensive" loops
producing a lot of useless strace output. This is important to get every thread
output into a separate file (-ff option).  

GDB

As a prerequisite, you need to get MIPSEL build of GDB and a corresponding
version of thread libraries.

# override the default thread libraries
mount --bind <MY_PATH_TO_GDB_PTHREAD>/usr/lib /usr/lib
mount --bind <MY_PATH_TO_GDB_PTHREAD>/lib/libthread_db-0.9.29.so /lib/libthread_db-0.9.29.so
mount --bind <MY_PATH_TO_GDB_PTHREAD>/lib/libpthread-0.9.29.so /lib/libpthread.so.0

# add <MY_PATH_TO_GDB_PTHREAD> to $PREFIX/lib/sandbox/application.conf somewhere
# under [directories]

# launch the app

# get the stagecraft process(es) PID(s)
/opt/zinc/oss/bin/busybox pgrep stagecraft

# pick the interesting process PID and attach the debugger to it
<MY_PATH_TO_GDB>/gdb -p <PID> /opt/stagecraft-2.0/bin/stagecraft


* Starting the application process with a diagnostic tool

Strace

The easiest way to use strace is to edit $PREFIX/bin/runStagecraft2.sh somewhere
at the end:

# this is the original line
LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib:/opt/zinc-trunk/lib:/usr/local/lib" \
LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
exec "${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"

# this is the same line with strace
exec strace -ff -o /opt/adobe/stagecraft/data/trace.log -s 128 \
-E LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib:/opt/zinc-trunk/lib:/usr/local/lib" \
-E LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
"${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"

You will find strace log files here: $PREFIX/var/applications/data/air/<app CRID
sha256>/stagecraft-data/.  

GDB

The prerequisites are the same as in "Attaching to the application process". You
can use GDB_STAGECRAFT variable in $PREFIX/bin/runStagecraft2.sh. As setting it
in the environment would require re-starting UIME, I prefer to edit
runStagecraft2.sh instead:

+GDB_STAGECRAFT=1
 if [ -z "${GDB_STAGECRAFT}" ]; then
 	LD_LIBRARY_PATH="/lib:/opt/zinc-trunk/oss/lib:/opt/zinc-trunk/lib:/usr/local/lib" \
 	LD_PRELOAD="${LD_PRELOAD}:${PRELOAD_LIB}" \
 	exec "${STAGECRAFT}" ${NEW_FLASHVARS} ${NEWARGS} ${AIRCMDLINE:+ --aircmdline "${AIRCMDLINE}"} "${SWF}"
 else
-	GDB="gdb"
+	GDB="<MY_PATH_TO_GDB>/gdb"
 	ORG_LD_LIBRARY_PATH="${LD_LIBRARY_PATH}"
 	LD_LIBRARY_PATH="/usr/lib:/usr/local/lib:/opt/zinc-trunk/oss/lib" exec ${GDB} \

The other important thing here is to start the babysitterd in foreground as at
some point you will want GDB interactive session. You will also need to add
<MY_PATH_TO_GDB> to $PREFIX/lib/sandbox/application.conf somewhere under
[directories].

Remember about permissions


={============================================================================
*kt_linux_tool_300* gdb-remote:

20 Debugging Remote Programs

Some remote targets allow gdb to access program files over the same connection
used to communicate with gdb. With such a target, if the remote program is
unstripped, the only command you need is target remote. 
    
Otherwise, start up gdb using the name of the local unstripped copy of your
program as the first argument, or use the file command.


{host-side}
The target remote command establishes a connection to the target. Its arguments
indicate which medium to use:

target remote tcp:host:port

Debug using a TCP connection to port on host. The host may be either a host name
or a numeric IP address; port must be a decimal number. For example, to connect
to port 2828 on a terminal server named manyfarms:

target remote manyfarms:2828

detach

When you have finished debugging the remote program, you can use the detach
command to release it from gdb control. Detaching from the target normally
resumes its execution, but the results will depend on your particular remote
stub. After the detach command, gdb is free to connect to another target.

disconnect

The disconnect command behaves like detach, except that the target is gener-
ally not resumed. It will wait for gdb (this instance or another one) to connect
and continue debugging. After the disconnect command, gdb is again free to
connect to another target.

note:
First make sure you have the necessary symbol files. Load symbols for your
application using the file command before you connect. Use set sysroot to locate
target libraries unless your gdb was compiled with the correct sysroot using
--with-sysroot.

The symbol file and target libraries must exactly match the executable and
libraries on the target, with one exception: the files on the host system should
not be stripped, even if the files on the target system are. Mismatched or
missing files will lead to confusing results during debugging. On gnu/Linux
targets, mismatched or missing files may also prevent gdbserver from debugging
multi-threaded programs.

note:
The remote targets are always running. If you get an error message like this one
below then use continue to run your program. You may need load first.

The "remote" target does not support "run".  Try "help target" or "continue".  

note:
"Cannot access memory at address 0x0" warning, This happens when run gdb client
in case use to debug a applicaiton using a shared library. Thought that gdb is
not working but gdb works as usual.


{once-connected}
Once the connection has been established, you can use all the usual commands to
examine and change data. The remote program is already running; you can use step
and continue, and you do not need to use run.


{target-side}
<why-gdbserver>
gdbserver is sometimes useful nevertheless, because it is a much 'smaller'
program than gdb itself.  It is also easier to port than all of gdb, so you may
be able to get started more quickly on a new system by using gdbserver. Finally,
   if you develop code for real-time systems, you may find that the tradeoffs
   involved in real-time operation make it more convenient to do as much
   development work as possible on another system, for example by
   cross-compiling. You can use gdbserver to make a similar choice for
   debugging.

<no-symbol>
Run gdbserver on the target system. You need a copy of the program you want to
debug, including any libraries it requires. gdbserver does 'not' need your
program's symbol table, so you can strip the program if necessary to save space.
gdb on the host system does all the symbol handling.

<server-run>
target> gdbserver comm program [ args ... ]

target> gdbserver host:2345 emacs foo.txt

The host:2345 argument means that we are expecting to see a TCP connection from
host to local TCP port 2345. (note Currently, the host part is ignored.) You can
choose any number you want for the port number as long as it does not conflict
with any existing TCP ports on the target system. This same port number must be
used in the host GDBs target remote command.

<attach>
target> gdbserver --attach comm pid

$ gdbserver --attach 172.20.33.215:12345 1368


={============================================================================
*kt_linux_tool_300* gdb-remote: args

<arguments>
20.3.1.4 Other Command-Line Arguments for gdbserver

--debug

Instruct gdbserver to display extra status information about the debugging
process. This option is intended for gdbserver development and for bug reports
to the developers.

--remote-debug

Instruct gdbserver to display remote protocol debug output. This option is
intended for gdbserver development and for bug reports to the developers.

<symbols>
First make sure you have the necessary symbol files. Load symbols for your
application using the file command before you connect. Use set sysroot to locate
target libraries (unless your GDB was compiled with the correct sysroot using
        --with-sysroot). Q: <sysroot>?

The symbol file and target libraries must exactly match the executable and
libraries on the target with one exception: the files on the host system should
not be stripped, even if the files on the target system are. Mismatched or
missing files will lead to confusing results during debugging. On GNU/Linux
targets, mismatched or missing files may also prevent gdbserver from debugging
multi-threaded programs. 


{gdb-client}
target remote host:port

monitor cmd

This command allows you to send arbitrary commands directly to the remote
monitor. Since GDB doesn't care about the commands it sends like this, this
command is the way to extend GDBâyou can add new commands that only the
external monitor will understand and implement. 
	

20.3.3 Monitor Commands for gdbserver

During a GDB session using gdbserver, you can use the monitor command to send
special requests to gdbserver. Here are the available commands.

monitor help
List the available monitor commands.

monitor set debug 0
monitor set debug 1
Disable or enable general debugging messages.

monitor set remote-debug 0
monitor set remote-debug 1
Disable or enable specific debugging messages associated with the remote
protocol (see Remote Protocol).  

monitor set debug-format option1[,option2,...]

Specify additional text to add to debugging messages. Possible options are:

none Turn off all extra information in debugging output. 

all Turn on all extra information in debugging output. 

timestamps Include a timestamp in each line of debugging output. 

Options are processed in order. Thus, for example, if none appears last then no
additional information is added to debugging output. 

monitor exit

Tell gdbserver to exit immediately. This command should be followed by
disconnect to close the debugging session. gdbserver will detach from any
attached processes and kill any processes it created. Use monitor exit to
terminate gdbserver at the end of a multi-process mode debug session.


={============================================================================
*kt_linux_tool_300* gdb-remote: remote or local?

{on-remote}
* Use remote debugging when debug it from main out of sandbox
* Not work when use release version
* 172.20.33.215 is host machine

<target>
gdbserver 172.20.33.215:12345 /opt/zinc-trunk/bin/nickelmediad -b Zinc.MediaProxy

// console logs comes here

<host>
$ ./mips-linux-uclibc-gdb 

(gdb) file /home/kpark/builds/yv-media-next/humax.1000/zinc-install-root
  /debug/humax-dtr_t1000/opt/zinc-trunk/bin/nickelmediad

// when use release version
//
// (gdb) file /home/kpark/builds-src-dev/topic-master/humax.2100/\
//    zinc-install-root/release/humax-dtr_t2100/opt/zinc-trunk/bin/nickelmediad
// Reading symbols from /home/kpark/builds-src-dev/topic-master/humax.2100/\
//  zinc-install-root/release/humax-dtr_t2100/opt/zinc-trunk/bin/nickelmediad\
//  ...(no debugging symbols found)...done.

(gdb) target remote 172.20.32.34:12345

(gdb) set substitute-path /home/kpark/builds/_virtual_/humax.1000 /home/kpark/src-dev

(gdb) set sysroot /home/kpark/builds/yv-media-next/humax.1000/zinc-install-root/debug/humax-dtr_t1000/opt/zinc-trunk/lib

(gdb) b MediaDaemon.cpp:187
Breakpoint 1 at 0x41d238: file
/home/kpark/builds/_virtual_/humax.1000/DEVARCH/Nickel/
  Nickel.System.DBusServer/src/MediaDaemon.cpp,
    line 187.

(gdb) c
Continuing.
Error while mapping shared library sections:
/usr/local/lib/libdirectfb.so: No such file or directory.
...
/lib/ld-uClibc.so.0: No such file or directory.

Breakpoint 1, parseArgs (this=0x7f839f28, argc=3, argv=0x7f83a184) at 
  /home/kpark/builds/_virtual_/humax.1000/DEVARCH/Nickel/
  Nickel.System.DBusServer/src/MediaDaemon.cpp:189
189		std::string defaultConfigFilePath = 
  NS_ZINC::PackageDataFinder().find("media-daemon.plugin-config");
(gdb) list


// {scenario-two} 
// Use remote when debug stating from main. NOT OK. However, when code has a call
// to load shared library using dl_open then gdb do not picks up the library and
// hence do not hit a break point in that shared library. Tried sysroot and
// solib-search-path. 


{on-target} 
To solve the above, use local approach. Copy sources from host to target and set
substitute. This works since can trace routine calling code to load shared
library.

$ gdb --args /opt/zinc-bin/bin/nickelmediad -b Zinc.MediaProxy 
(gdb) set substitute-path 
  /home/kpark/builds/_virtual_/humax.1000/DEVARCH/Nickel /root
(gdb) b ProxySystemFactory.cpp:82
(gdb) run


{on-target-attach}
Still cannot use remote since still use dl_open depending on selection via dbus.
So must use local approach.

$ dbussenddaemon &
$ sleep 1

// this is necessary to make it run in background
$ /opt/zinc-trunk/bin/nickelmediad -b Zinc.MediaProxy > /var/tmp/x.log 2>&1 &

// to check PID and if necessary, kill the previous one.
$ pgrep -l nickelmediad

$ tail /var/tmp/x.log 
$ gdb -p 1846 /opt/zinc-trunk/bin/nickelmediad 

// gdb starts to load libraries
// do not need to run file since it's attached
// 172.20.33.215:/home/kpark/src-dev/DEVARCH/ on /mnt/tmp type nfs

(gdb) set substitute-path 
  /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/ /mnt/tmp
(gdb) b ProxyMediaRouter.cpp:448


note: when use release version

(gdb) b ProxyMediaRouter.cpp:448
No source file named ProxyMediaRouter.cpp.
Make breakpoint pending on future shared library load? (y or [n]) y
Breakpoint 1 (ProxyMediaRouter.cpp:448) pending.

note: when use debug build and gdb is able to set break, shows this:

Breakpoint 1 at 0x76b0c5e4: file
/home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Nickel/\
 Nickel.System.Proxy/src/ProxyMediaRouter.cpp,
line 448.


// From another ssh session, run dbus commands:

MR=`dbus-send 2>/dev/null --session --print-reply --type=method_call \
--dest='Zinc.DBusSendDaemon' \
/Zinc/Media/MediaRouterFactory \
Zinc.Media.MediaRouterFactory.createMediaRouter \
string:Zinc.MediaProxy 2>/dev/null |grep "/Zinc/Media/MediaRouters/" |cut -d'"' -f2`

dbus-send --session --print-reply --type=method_call --dest='Zinc.MediaProxy' $MR \
Zinc.Media.MediaRouter.setSource \
string:http://dash.bidi.int.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd \
int32:0


// then gdb hits a break point.

note: hit the break but a source is not found due to "set substitute-path" error

Breakpoint 1, nickel::system::ProxyMediaRouter::setSource (this=<optimized out>
    , mediaLocator_in=<optimized out>, reason_in=<optimized out>)
    at 
    /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Nickel/
      Nickel.System.Proxy/src/ProxyMediaRouter.cpp:448

448	/home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Nickel/
  Nickel.System.Proxy/src/ProxyMediaRouter.cpp: No such file or directory. ~
	in /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Nickel/
    Nickel.System.Proxy/src/ProxyMediaRouter.cpp
(gdb)

note: hit the break and found a source

Breakpoint 1, nickel::system::ProxyMediaRouter::setSource (this=<optimized out>
    , mediaLocator_in=<optimized out>, reason_in=<optimized out>)
    at 
    /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Nickel/
      Nickel.System.Proxy/src/ProxyMediaRouter.cpp:448

448	NS_ZINC::Future< void > ProxyMediaRouter::setSource
  (const std::string& mediaLocator_in, const SetSourceReason::Enum reason_in) {
(gdb)


note:
Before setting a break, file should be visible to gdb such as nfs mounted.


{scenario-five}
This is a case which have no direct loading of shared library and hence remote
approach is used.

<target>
$ gdbserver 172.20.33.215:12345 /opt/zinc-trunk/bin/nickelmediad -b Zinc.MediaProxy

<host>

// to see the default setting
(gdb)  show auto-solib-add             
Autoloading of shared library symbols is on.

// shall load file first before setting a breakpoint
(gdb) file /home/kit/tizen/tv-viewer/tv-viewer
Reading symbols from /home/kit/tizen/tv-viewer/tv-viewer...done.

(gdb) target remote 106.1.11.219:2345
Remote debugging using 106.1.11.219:2345
warning: Unable to find dynamic linker breakpoint function.
GDB will be unable to debug shared library initialisers
and track explicitly loaded dynamic code.
0xb63da7c0 in ?? ()

// to set a breakpoint in the gdbint; otherwise, gdb set no since no input from the user
(gdb) set breakpoint pending on

// this is a warning at this moment
(gdb) b main
Cannot access memory at address 0x0
Breakpoint 1 at 0x29842716: file /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1/src/core/AppMain.cpp, line 33.

// no shared library sicne the application do not started yet.
(gdb) i sharedlibrary
No shared libraries loaded at this time.

// see warnings on shared library now.
// set solib-search-path before and seems to have only one path as set solib-search-path /home/kit/mheg-port-ug
(gdb) c
Continuing.
warning: `/usr/lib/libicui18n.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicuuc.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicudata.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libfribidi.so.0': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicule.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: Could not load shared library symbols for 184 libraries, e.g. /usr/lib/libsys-assert.so.
Use the "info sharedlibrary" command to see the complete listing.
Do you need "set solib-search-path" or "set sysroot"?

Breakpoint 1, main (argc=-1237321556, argv=0xb63f4a30)
    at /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1/src/core/AppMain.cpp:33
33           _ERR("%s","Failed to create app!");

(gdb)
Continuing.

// see that even after running, not loaded full libraries. want to debug libug-mhegUG-efl.so
(gdb) i sharedlibrary
From        To          Syms Read   Shared Object Library
                        No          /usr/lib/libsys-assert.so
                        ...
                        No          /usr/lib/libsqlite3.so.0
0xb55efb24  0xb572307c  Yes (*)     /usr/lib/libicui18n.so.48
0xb55154e4  0xb55dfe7c  Yes (*)     /usr/lib/libicuuc.so.48
0xb43e4258  0xb43e4350  Yes (*)     /usr/lib/libicudata.so.48
                        No          /usr/lib/libavoc.so
                        ...
(*): Shared library is missing debugging information.

(gdb) c
Continuing.

(gdb) CTRL-C
Program received signal SIGINT, Interrupt.
0xb3db296c in ?? ()
warning: Could not load shared library symbols for 21 libraries, e.g. /usr/lib/ecore/immodules/libisf-imf-module.so.
Use the "info sharedlibrary" command to see the complete listing.
Do you need "set solib-search-path" or "set sysroot"?

// see that the wanted library is loaded
(gdb) i sharedlibrary
From        To          Syms Read   Shared Object Library
                        ...
0xab8d96c0  0xab9a2d78  No          /home/kit/mheg-port-ug/libug-mhegUG-efl.so
                        ..
(*): Shared library is missing debugging information.

// note:
// HOWEVER, the problem is that the library is loaded but 'not' the symblos and files. Can check to
// see if that is the case by running "i sources" to see files.

// note to fource to load again and check with i sources
(gdb) sharedlibrary mheg
Reading symbols from /home/kit/mheg-port-ug/libug-mhegUG-efl.so...done.
Loaded symbols for /home/kit/mheg-port-ug/libug-mhegUG-efl.so
Cannot access memory at address 0xb

// here can see the result of this substitude command:
// set substitute-path /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1 /home/kit/tizen/tv-viewer
(gdb) i sources
Source files for which symbols have been read in:

/home/kit/tizen/tv-viewer/src/core/AppMain.cpp, /usr/include/c++/4.5.3/new,
...

Source files for which symbols will be read in on demand:
...
/home/kit/mheg-port-ug/mh5eng/mh5e_token.c, /home/kit/mheg-port-ug/mh5eng/mh5b_program.c,

// now set a breakpoint in the loaded library
(gdb) b _key_pressed(char const*)
Cannot access memory at address 0xb
Breakpoint 2 at 0xab8d9afa: file /home/abuild/rpmbuild/BUILD/ug-mheg-0.2/main/Main.cpp, line 144.

(gdb) c

# hit breakpoint

(gdb) list

// note that if do not run this command fast enough, it seems that gdb fails to run a session. means
// not hitting a breakpoint.

(gdb) c
Continuing.


={============================================================================
*kt_linux_tool_300* gdb-remote: frontend: cgdb

It is how to change cgdb to use cross tool gdb.

http://www.programdevelop.com/4527764/

The GDB code CGDB calls in the path: /VARIOUS/util/src/fork_util.c by function invoke_debugger in

int invoke_debugger( 
            const char *path,  
            int argc, char *argv[],  
            int *in, int *out,  
            int choice, char *filename)  
{ 
    pid_t pid;     
    //GDBGDB?arm-linux-gdb 
    const char * const GDB               = "arm-linux-gdb"; 
}

./configure --prefix=/usr/local/ --program-suffix=arm-linux
make
sudo make install

kit@kit-vb:~/mheg-port$ ls /usr/local/bin/cgdb*
/usr/local/bin/cgdb  /usr/local/bin/cgdbarm-linux
kit@kit-vb:~/mheg-port$

http://cgdb.github.io/


{config}
/.cgdb/cgdbrc
==
:set winspilt=top_big
:set arrowstyle=long
:map <F6> :continue<CR>
:map <F7> :finish<CR>
:map <F8> :step<CR>
:map <F9> :next<CR>
==


={============================================================================
*kt_linux_tool_300* gdb-remote: frontend: eclipse

Debugging with Eclipse and gdb

I have recently tried to debug some elements with Eclipse. It's quite
straightforward. 

Steps to start debugging:

1. Right click on a project to be debugged and choose "Debug As" -> "Debug
Configurations...".

It should create new debug configuration under C/C++ Application branch named
same as project itself. If it is not created then it is possible to create it
with "New launch configuration" (button in upper left corner).

2. Select executable

In the Main tab it is required to provide a path to executable file which will
be launched ("Browse...") so need to know which executable we're going to debug
(e. g. some test).

3. Set environment

In most cases D-BUS demon is required. In Environment tab need to add variable
DBUS_SESSION_BUS_ADDRESS set to proper value (assuming that D-BUS daemon is
    started, e. g.
    unix:path=/tmp/dbus-7a0FdfCMQ8,guid=02115674e4f1464d822c1e700000230a).

4. Start debugging

Eclipse automatically stops on the first line in main() so there's no need to
worry about first breakpoint. Depending on settings in Eclipse it might ask for
switching to Debug perspective.

It is also possible to attach to the running process. In case of test
application running under Stagecraft some hack is required: need to hold
application, attach to it and resume it. It's a matter of experience for
choosing proper point of suspension. Using Lead project as an example I added
following line at the beginning of LeadAirPlugin::registerBindings() :

for ( bool bDbg = true; bDbg; ) {}

Rebuild:

make Lead.AIR.Client.API-clean
make Lead.AIR.Client.API

Run application:

$PREFIX/bin/runStagecraft2.sh --forceembeddedvectorfonts
/home/zinc/Zinc/source/Canvas/elements.trunk/Lead/Lead.ClientApiTests/as3/getTunedChannel/bin/YoureWatching.swf

Now need to attach to stagecraft application as follows:

1. Right click on particular project and choose  Debug As" -> "Debug
Configurations...".

In C/C++ Attach to Application branch create new configuration (name it whatever
    you want).

2. Select stagecraft executable: /opt/stagecraft-2.0/bin/stagecraft

3. Start debugging

New window with a list of processes will occur. Just start typing 'stagecraft'
and select proper process (it should be only one if you're not running any other
    Stagecraft applications) and choose "OK". In debug perspective take a look
into stack and find registerBindings() in one of threads (possibly the last
    one). Double-click it and the hack-line should appear in the source code
editor. In the upper right corner of Debug perspective there's a set of tabbed
windows. Select Variables then select 'bDbg' variable by clicking on its 'Value'
column and change it to false. Now you can step through the code.

This method is very simple but requires rebuilding project whenever holding
application is required or not. A better approach might be inserting this sort
of code somewhere:

for ( bool bDbg = getenv("GDB_HOLD_STAGECRAFT"); bDbg; ) {}  

This hack works also with optimized code ('bDbg' variable is set conditionally
    so compiler cannot optimize any code afterwards). It requires looking into
  assembly and finding out where the program execution should continue after the
  hack-loop. Example below is based on a simple few-liner application I've made
  for this purpose (test.cpp, I've used "KRK_DBG" environment variable):

#include <cstdlib>
#include <iostream>
 
int main() {
    for ( bool bDbg = getenv("KRK_DBG"); bDbg; ) {}
 
    std::cout << "haha" << std::endl;
    return 0;
}
 

And the session:

> g++ -O3 -o test test.cpp
> export KRK_DBG=1
> ./test &
[1] 1258

> gdb -p 1258
... some GDB introduction stuff ...

(gdb) set disassembly-flavor intel

(gdb) x/2i $pc
0x8048815 <main+245>:	lea    esi,[esi+0x0]
0x8048818 <main+248>:	jmp    0x8048815 <main+245>

(gdb) x/20i main
0x8048720 <main>:	lea    ecx,[esp+0x4]
0x8048724 <main+4>:	and    esp,0xfffffff0
0x8048727 <main+7>:	push   DWORD PTR [ecx-0x4]
0x804872a <main+10>:	push   ebp
0x804872b <main+11>:	mov    ebp,esp
0x804872d <main+13>:	push   edi
0x804872e <main+14>:	push   esi
0x804872f <main+15>:	push   ebx
0x8048730 <main+16>:	push   ecx
0x8048731 <main+17>:	sub    esp,0x118
0x8048737 <main+23>:	mov    DWORD PTR [esp],0x80488e4
0x804873e <main+30>:	call   0x80485a8 <getenv@plt>
0x8048743 <main+35>:	test   eax,eax
0x8048745 <main+37>:	jne    0x8048815 <main+245>
0x804874b <main+43>:	mov    DWORD PTR [esp+0x8],0x4
0x8048753 <main+51>:	mov    DWORD PTR [esp+0x4],0x80488ec
0x804875b <main+59>:	mov    DWORD PTR [esp],0x8049b20
0x8048762 <main+66>:	call   0x8048618 <_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_i@plt>
0x8048767 <main+71>:	mov    eax,ds:0x8049b20
0x804876c <main+76>:	mov    eax,DWORD PTR [eax-0xc]
(gdb) set $pc=0x804874b
(gdb) cont
Continuing.
haha

Program exited normally.
(gdb)


Stepping through chain of calls.

Assume this part of code:
class A {
   void someMethod() const {
      // (3)
 
   }
 
} ;
 
const A& getA() {
   static A a;
   return a;
} // (2)
 
void f() {
   getA().someMethod(); // (1)
   // (4)
 
}

Stepping in (F5) at point (1) will follow getA() body which might be stepped
over (F6) but it is important to step in (F5) at point (2) to get into
A::someMethod() at point (3). It's confusing but stepping over (F6) at point (2)
will bring the debugger into point (4).

Some observations:

    Keeping 'Variables' tab opened causes gdb to ignore breakpoints or crashes
    gdb session in some situations.

    Sometimes breakpoints are not re-installed (i. e. after restarting debug
        session).

    Stepping through chain of calls is not intuitive (see the example above).

All of these might be caused rather by MI bugs (gdb interface).

It is also possible to debug one or more processes simultaneously in Eclipse.
There is no problem to debug full conversation of Flash application (stagecraft
    -> Flash -> C++ bindings actually) with service by attaching to stagecraft
and particular daemon.

Obviously it is also possible to do it with pure GDB (command line) and requires
starting another instance of GDB what Eclipse as a matter of fact does.


={============================================================================
*kt_linux_tool_300* gdb-core: man

$ man 5 core

CORE(5)

NAME
       core - core dump file

DESCRIPTION
       The  default action of certain signals is to cause a process to terminate
       and produce a core dump file, a disk file containing an image of the
       process's memory at the time of termination.  This image can be used in a
       debugger (e.g., gdb(1)) to inspect the state of the program at the time
       that it terminated.  A list of the signals which cause a process to dump
       core can be found in signal(7).

       A process can set its soft RLIMIT_CORE resource limit to place an upper
       limit on the size of the core dump file that will be produced if it
       receives a "core dump" signal; see getrlimit(2) for details.

       There are various circumstances in which a core dump file is 'not'
       produced:
       ...

   Naming of core dump files
       By  default, a core dump file is named core, but the
       /proc/sys/kernel/core_pattern file (since Linux 2.6 and 2.4.21) can be
       set to define a template that is used to name core dump files.  The
       template can contain % specifiers which are substituted by the following
       values when a core file is created:

           %h  hostname (same as nodename returned by uname(2))

           %p  PID of dumped process, as seen in the PID namespace in which the
               process resides
           
           %t  time of dump, expressed as seconds since the Epoch, 1970-01-01
               00:00:00 +0000 (UTC)

           %e  executable filename (without path prefix)

   Piping core dumps to a program
       Since kernel 2.6.19, Linux supports an alternate syntax for the
       /proc/sys/kernel/core_pattern file. If the first character of this file
       is a pipe symbol (|), then the remainder of the line is interpreted as a
       'program' to be executed. Instead of being written to a disk file, the
       core dump is given as standard input to the program. Note the following
       points:

       *  The program must be specified using an 'absolute' pathname (or a
          pathname relative to the root directory, /), and must immediately
          follow the '|' character.

       *  The process created to run the program runs as user and group 'root'.

       *  Command-line arguments can be supplied to the program (since Linux
          2.6.24), delimited by white space (up to a total line length of 128
          bytes).

       *  The command-line arguments can include any of the % specifiers listed
          above. For example, to pass the PID of the process that is being
          dumped, specify %p in an argument.

   Controlling which mappings are written to the core dump
       Since kernel 2.6.23, the Linux-specific /proc/PID/coredump_filter file
       can be used to control which memory segments are written to the core dump
       file in the event that a core dump is performed for the process with the
       corresponding process ID.

       The  value  in  the file is a bit mask of memory mapping types (see
               mmap(2)). If a bit is set in the mask, then memory mappings of
       the corresponding type are dumped; otherwise they are not dumped. The
       bits in this file have the following meanings:

           bit 0  Dump anonymous private mappings.
           bit 1  Dump anonymous shared mappings.
           bit 2  Dump file-backed private mappings.
           bit 3  Dump file-backed shared mappings.
           bit 4 (since Linux 2.6.24)
                  Dump ELF headers.
           bit 5 (since Linux 2.6.28)
                  Dump private huge pages.
           bit 6 (since Linux 2.6.28)
                  Dump shared huge pages.

       By default, the following bits are set: 0, 1, 4 (if the
               CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS kernel configuration option
               is enabled), and 5.  The value of this file is displayed in
       hexadecimal. (The default value is thus displayed as 33.)

       <ex>
        [root@HUMAX /]# cat /proc/1339/coredump_filter 
        00000023

           543210
        '0b100011'


={============================================================================
*kt_linux_tool_300* gdb-core-setting

{core-pattern}
$ cat /proc/sys/kernel/core_pattern 
core

<syntax>
# set core dump location and format
echo '/tmp/%p.COR' >/proc/sys/kernel/core_pattern

<system-wide>
The changes done before are only applicable until the next reboot. In order to
make the change in all future reboots, you will need to add the following in
/etc/sysctl.conf

# own core file pattern...
kernel.core_pattern=/tmp/cores/core.%e.%p.%h.%t

sysctl.conf is the file controlling every configuration under /proc/sys

Just wanted to say that there is no need to edit the file manually. simply run
the sysctl command, which does the stuff

<things-to-check>
  * write permissions in the directory 
  * ulimit -c unlimited
  * should be debug version and statically linked. If instead of the callstack
    you see only "??", it was not compiled using static linkage

mkdir -p /tmp/cores
chmod a+rwx /tmp/cores
echo "/tmp/cores/core.%e.%p.%h.%t" > /proc/sys/kernel/core_pattern
 

<ulimit>
Do not use "ulimit" bash command to check if ulimit is unlimited and think that
ulimit setting for core is done since "If no option is given, then -f is
assumed." from bash manual.

// -f
//    The maximum size of files written by the shell and its children.
// -a
//    All current limits are reported.

$ ulimit
unlimited

note: okay as shows unlimited. really?

$ ulimit -a
core file size          (blocks, -c) 0             note: not set!
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128235
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128235
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited


$ ulimit -c unlimited
$ ulimit
unlimited
$ ulimit -a 
core file size          (blocks, -c) unlimited     note: now okay
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128235
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128235
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited


={============================================================================
*kt_linux_tool_300* gdb-core-setting-run-commands when makes core

echo "|/bin/bash /path/to/core-dump /mnt/hd1/ %p \
    [ disk_space_threshold_in_bytes ]" > /proc/sys/kernel/core_pattern

note: Use one like this and otherwise get an error from the script
note: Also, should set big enough value 

root     root      175.4M Feb 18 11:09 core.6135

echo "|/bin/bash /usr/local/bin/core-dump /mnt/hd1/ %p 10000000" > /proc/sys/kernel/core_pattern


[root@HUMAX sandbox]# cat /proc/sys/kernel/core_pattern 
|/bin/bash /usr/local/bin/core-dump /mnt/hd1/ %p

10:14:38 ~/source$ cat ./setup-xxx/scripts/core-dump
#!/bin/bash

##
# This is meant to be used to:
# - save the core dump piped to STDIN,
# - capture process memory map (/proc/pid/maps)
# - process the core dump (if gdb is installed)
#   to produce a text file with the backtrace
#   already demangled and info about threads.
# See core (5) man page for more information.
#
# This script is meant to be executed by the Kernel if
# /proc/sys/kernel/core_pattern references it.
#
# Usage:
#         ulimit -c unlimited
#         echo "|/bin/bash /path/to/core-dump /mnt/hd1/ %p \
#           [ disk_space_threshold_in_bytes ]" > /proc/sys/kernel/core_pattern
##

gdb_path=/opt/zinc/oss/debugtools/bin/gdb


##
# Defining custom reporting routines (instead of using the ones from
# common-functions) to minimize the dependency chain, since this script will be
# called by the kernel.
##
blit() {
    local severity="${1:-INFO}" && shift
    echo "[$(date +"%F %T")] $severity: $*"
}

error() {
    blit "ERROR" "$*"
}

die() {
    blit "DIE" "$*"
    exit 1
}

info() {
    blit "INFO" "$*"
}

##
# Produce a text file from the core dump with the backtrace
# already demangled and info about threads...
##
process_core_dump() {
    local core_dump="$1"
    local core_maps="$2"
    local output_file="$3"

    # note: gets a exec name from a map file
    local first_maps_line=$(head -1 "$core_maps")
    # Assume the executable is under /opt
    local executable="/opt/${first_maps_line#*/opt/}"

    [[ -e "$executable" ]] || {
        error "Failed to extract executable from '$first_maps_line'." 2> "$output_file"
        return
    }

    $gdb_path --batch \
              --eval-command="info threads" \
              --eval-command="echo ------------\n" \    // note: can add this
              --eval-command "bt" \
              --eval-command "thread apply all bt" \
              --eval-command 'info sharedlibrary' \
              --eval-command 'info registers' \
              --eval-command 'info locals' \
              --eval-command 'info args' \
              --eval-command 'x/100a $sp' \
              --eval-command 'x/50i $pc' \
              "$executable" "$core_dump" &> "$output_file"
}


##
# gets the available free space in the directory designated for core files
##
get_available_space() {
    declare -a fs_info=( $(stat -f -c "%a %s" "$core_output_path") )
    echo $(( $(IFS="*"; echo "${fs_info[*]}") ))
}


##
# Find the pid (which is the extension) of the oldest log entries
##
get_pid_of_oldest_logs() {
    local oldest_file=$(find "$core_output_path" -type f -exec stat -c "%Y %n" \
            {} \; | sort | head -n 1 | cut -d ' ' -f 2)
    echo "${oldest_file##*.}"
}


##
# Removes the oldest files in the core_output_path directory
##
do_cleanup() {
    local oldest_pid=$(get_pid_of_oldest_logs)
    rm -fv "$core_output_path"/*."$oldest_pid"
}

#
# Count number of entries in the core file directory
#
count_files() {
    echo $(find "$core_output_path" -type f | wc -l)
}


# note:
# echo "|/bin/bash /path/to/core-dump /mnt/hd1/ %p \
#   [ disk_space_threshold_in_bytes ]" > /proc/sys/kernel/core_pattern

core_output_path=$1
pid=$2
threshold=${3:-0}

# redirect stdout/stderr to the log file
exec 1>"${core_output_path}/core-log.$pid" 2>&1

[[ $# -eq 3 ]] || die "Script is not meant to be run manually!"


##
# Perform cleanup to reclaim some space if required
##
while true; do
    files_count=$(count_files)

    # break the loop if the available space is above the threshold
    [[ "$(get_available_space)" -lt "$threshold" ]] || break

    # break the loop if there are no files left to remove
    [[ "$files_count" -eq 0 ]] && break

    info "Performing oldest core files cleanup to free some space ..."
    do_cleanup

    # sanity check: break the loop if the cleanup didn't remove anything
    [[ "$files_count" -eq "$(count_files)" ]] && break
done


##
# Quit if free space is still bellow the threshold
##
[[ "$(get_available_space)" -lt "$threshold" ]] &&
    die "Core dump won't be created. Space usage limit exceeded!"

cat "/proc/$pid/maps" > "${core_output_path}/core-maps.$pid"

# save core file from stdin
dd of="${core_output_path}/core.$pid" bs=1M

[[ -x "$gdb_path" ]] &&
    process_core_dump \
        "${core_output_path}/core.$pid" \
        "${core_output_path}/core-maps.$pid" \
        "${core_output_path}/core-text.$pid"

# Unfortunately, this takes too much time (at least on t1000) and can result in
# a broken archive. Needs to be disabled.
#gzip "${core_output_path}/core.$pid"
info "Core file obtained successfully."


-rw-r--r--    1 root     root         161 Feb 18 10:37 core-log.5120
-rw-r--r--    1 root     root       46.7K Feb 18 10:37 core-maps.5120
-rw-r--r--    1 root     root       23.9K Feb 18 10:37 core-text.5120
-rw-r--r--    1 root     root      176.6M Feb 18 10:37 core.5120

<ex>
[root@HUMAX /]# more core-text.5339 
[New LWP 5339]
[New LWP 5404]
[New LWP 5405]
[New LWP 5403]
[New LWP 5406]
[New LWP 5408]
[New LWP 5407]
[New LWP 5423]
[New LWP 5420]
[New LWP 5409]
[New LWP 5410]
[New LWP 5419]
[New LWP 5411]
[New LWP 5414]
[New LWP 5413]
[New LWP 5412]
[New LWP 5422]
[New LWP 5425]
[Thread debugging using libthread_db enabled]
Core was generated by `/opt/zinc-trunk/bin/w3cEngine -debug -cache /app-data/client-cache -cache-size'.
Program terminated with signal 11, Segmentation fault.
#0  0x5447955c in vanadium::VanadiumMediaPlayer::currentTime() const () from /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0
  Id   Target Id         Frame 
  18   Thread 0x4ee7b510 (LWP 5425) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  17   Thread 0x53f1e510 (LWP 5422) 0x73a59284 in ?? () from /lib/libc.so.0
  16   Thread 0x5853c510 (LWP 5412) 0x73a59284 in ?? () from /lib/libc.so.0
  15   Thread 0x57d3c510 (LWP 5413) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  14   Thread 0x5753c510 (LWP 5414) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  13   Thread 0x58d3c510 (LWP 5411) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  12   Thread 0x564e4510 (LWP 5419) 0x73a59284 in ?? () from /lib/libc.so.0
  11   Thread 0x5953c510 (LWP 5410) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  10   Thread 0x59fee510 (LWP 5409) 0x73a59284 in ?? () from /lib/libc.so.0
  9    Thread 0x558e2510 (LWP 5420) 0x73a59284 in ?? () from /lib/libc.so.0
  8    Thread 0x5358c510 (LWP 5423) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  7    Thread 0x5abb3510 (LWP 5407) 0x73b0af84 in pthread_cond_timedwait () from /lib/libpthread.so.0
  6    Thread 0x5ab44510 (LWP 5408) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  5    Thread 0x5abc3510 (LWP 5406) 0x73b0af84 in pthread_cond_timedwait () from /lib/libpthread.so.0
  4    Thread 0x720c9510 (LWP 5403) 0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
  3    Thread 0x70ed3510 (LWP 5405) 0x73a58640 in ioctl () from /lib/libc.so.0
  2    Thread 0x716d3510 (LWP 5404) 0x73aa6820 in read () from /lib/libc.so.0
* 1    Thread 0x773f4320 (LWP 5339) 0x5447955c in vanadium::VanadiumMediaPlayer::currentTime() const () from /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0
#0  0x5447955c in vanadium::VanadiumMediaPlayer::currentTime() const () from /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0
#1  0x756e1fdc in ?? () from /opt/zinc-trunk/oss/lib/libQt5WebKit.so.5
warning: GDB can't find the start of the function at 0x756e1fda.

    GDB is unable to find the start of the function at 0x756e1fda
and thus can't determine the size of that function's stack frame.
This means that GDB may be unable to access that stack frame, or
the frames below it.
    This problem is most likely caused by an invalid program counter or
stack pointer.
    However, if you think GDB should simply search farther back
from 0x756e1fda for code which looks like the beginning of a
function, you can increase the range of the search using the `set
heuristic-fence-post' command.

Thread 18 (Thread 0x4ee7b510 (LWP 5425)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x763df9bc in zinc::run(boost::asio::io_service*) () from /opt/zinc-trunk/lib/libZincCommon.so.0
#2  0x76351a8c in thread_proxy () from /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
#3  0x73b0f738 in start_thread () from /lib/libpthread.so.0
#4  0x73b08030 in __thread_start () from /lib/libpthread.so.0
Backtrace stopped: frame did not save the PC

Thread 17 (Thread 0x53f1e510 (LWP 5422)):
#0  0x73a59284 in ?? () from /lib/libc.so.0
warning: GDB can't find the start of the function at 0x73a59284.

Thread 16 (Thread 0x5853c510 (LWP 5412)):
#0  0x73a59284 in ?? () from /lib/libc.so.0
warning: GDB can't find the start of the function at 0x73a59284.

Thread 15 (Thread 0x57d3c510 (LWP 5413)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x763df9bc in zinc::run(boost::asio::io_service*) () from /opt/zinc-trunk/lib/libZincCommon.so.0
#2  0x76351a8c in thread_proxy () from /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
#3  0x73b0f738 in start_thread () from /lib/libpthread.so.0
#4  0x73b08030 in __thread_start () from /lib/libpthread.so.0
Backtrace stopped: frame did not save the PC

Thread 14 (Thread 0x5753c510 (LWP 5414)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x75b3bd78 in ?? () from /opt/zinc-trunk/oss/lib/libQt5WebKit.so.5
warning: GDB can't find the start of the function at 0x75b3bd76.

Thread 13 (Thread 0x58d3c510 (LWP 5411)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x763df9bc in zinc::run(boost::asio::io_service*) () from /opt/zinc-trunk/lib/libZincCommon.so.0
#2  0x76351a8c in thread_proxy () from /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
#3  0x73b0f738 in start_thread () from /lib/libpthread.so.0
#4  0x73b08030 in __thread_start () from /lib/libpthread.so.0
Backtrace stopped: frame did not save the PC

Thread 12 (Thread 0x564e4510 (LWP 5419)):
#0  0x73a59284 in ?? () from /lib/libc.so.0
warning: GDB can't find the start of the function at 0x73a59284.

Thread 11 (Thread 0x5953c510 (LWP 5410)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x763df9bc in zinc::run(boost::asio::io_service*) () from /opt/zinc-trunk/lib/libZincCommon.so.0
#2  0x76351a8c in thread_proxy () from /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
#3  0x73b0f738 in start_thread () from /lib/libpthread.so.0
#4  0x73b08030 in __thread_start () from /lib/libpthread.so.0
Backtrace stopped: frame did not save the PC

Thread 10 (Thread 0x59fee510 (LWP 5409)):
#0  0x73a59284 in ?? () from /lib/libc.so.0
warning: GDB can't find the start of the function at 0x73a59284.

Thread 9 (Thread 0x558e2510 (LWP 5420)):
#0  0x73a59284 in ?? () from /lib/libc.so.0
warning: GDB can't find the start of the function at 0x73a59284.

Thread 8 (Thread 0x5358c510 (LWP 5423)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x763bdd94 in zinc::run_catching_exceptions(boost::asio::io_service*) () from /opt/zinc-trunk/lib/libZincCommon.so.0
#2  0x763c0b7c in boost::detail::thread_data<zinc::detail::WorkerFunction<boost::function<void ()> > >::run() () from /opt/zinc-trunk/lib/libZincCommon.so.0
#3  0x76351a8c in thread_proxy () from /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
#4  0x73b0f738 in start_thread () from /lib/libpthread.so.0
#5  0x73b08030 in __thread_start () from /lib/libpthread.so.0
Backtrace stopped: frame did not save the PC

Thread 7 (Thread 0x5abb3510 (LWP 5407)):
#0  0x73b0af84 in pthread_cond_timedwait () from /lib/libpthread.so.0
#1  0x72f33488 in BKNI_WaitForEvent () from /usr/local/lib/libnexus.so
#2  0x72e91a18 in ?? () from /usr/local/lib/libnexus.so
warning: GDB can't find the start of the function at 0x72e91a16.

Thread 6 (Thread 0x5ab44510 (LWP 5408)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x773386f0 in ?? () from /usr/local/lib/libdirectfb.so
warning: GDB can't find the start of the function at 0x773386ee.

Thread 5 (Thread 0x5abc3510 (LWP 5406)):
#0  0x73b0af84 in pthread_cond_timedwait () from /lib/libpthread.so.0
#1  0x72f33488 in BKNI_WaitForEvent () from /usr/local/lib/libnexus.so
#2  0x72e91a18 in ?? () from /usr/local/lib/libnexus.so
warning: GDB can't find the start of the function at 0x72e91a16.

Thread 4 (Thread 0x720c9510 (LWP 5403)):
#0  0x73b0b518 in pthread_cond_wait () from /lib/libpthread.so.0
#1  0x75e7c964 in ?? () from /opt/zinc-trunk/oss/lib/libQt5WebKit.so.5
warning: GDB can't find the start of the function at 0x75e7c962.

Thread 3 (Thread 0x70ed3510 (LWP 5405)):
#0  0x73a58640 in ioctl () from /lib/libc.so.0
#1  0x72e575b0 in ?? () from /usr/local/lib/libnexus.so
warning: GDB can't find the start of the function at 0x72e575ae.

Thread 2 (Thread 0x716d3510 (LWP 5404)):
#0  0x73aa6820 in read () from /lib/libc.so.0
#1  0x73aa6804 in read () from /lib/libc.so.0
Backtrace stopped: previous frame identical to this frame (corrupt stack?)

Thread 1 (Thread 0x773f4320 (LWP 5339)):
#0  0x5447955c in vanadium::VanadiumMediaPlayer::currentTime() const () from /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0
#1  0x756e1fdc in ?? () from /opt/zinc-trunk/oss/lib/libQt5WebKit.so.5
warning: GDB can't find the start of the function at 0x756e1fda.
From        To          Syms Read   Shared Object Library
0x772cfd70  0x773b9ee0  Yes (*)     /usr/local/lib/libdirectfb.so
0x77297d60  0x772a7f70  Yes (*)     /usr/local/lib/libdirect.so
0x7727bf90  0x772836f0  Yes (*)     /usr/local/lib/libinit.so
0x771f0780  0x77254aa0  Yes (*)     /lib/libstdc++.so.6
0x7714e9e0  0x7718f3f0  Yes (*)     /opt/zinc-trunk/lib/libCobaltSystemAPI.so.0
0x77127ae0  0x771283e0  Yes (*)     /opt/zinc-trunk/lib/libHeliumSystemAPI.so.0
0x7708b810  0x770e5870  Yes (*)     /opt/zinc-trunk/oss/lib/libsoup-2.4.so.1
0x76eb6890  0x770378f0  Yes         /opt/zinc-trunk/oss/lib/libgio-2.0.so.0
0x76e389d0  0x76e7bad0  Yes         /opt/zinc-trunk/oss/lib/libgobject-2.0.so.0
0x76caef50  0x76dc8e10  Yes         /opt/zinc-trunk/oss/lib/libglib-2.0.so.0
0x76c83010  0x76c89f30  Yes (*)     /opt/zinc-trunk/oss/lib/libintl.so.8
0x76c628f0  0x76c6c4b0  Yes (*)     /opt/zinc-trunk/lib/libMercurySystemAPI.so.0
0x76c3dea0  0x76c44c60  Yes (*)     /opt/zinc-trunk/lib/libNeonSystemAPI.so.0
0x76c1cd60  0x76c206c0  Yes (*)     /opt/zinc-trunk/lib/libNickelTunerSystemAPI.so.0
0x76c04bc0  0x76c06d20  Yes (*)     /opt/zinc-trunk/lib/libTitaniumApplicationContainerAPI.so.0
0x76bce750  0x76be8aa0  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5WebKitWidgets.so.5
0x76ba22e0  0x76ba2c80  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5PrintSupport.so.5
0x766869e0  0x76ada3f0  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5Widgets.so.5
0x765b8770  0x765b8920  Yes (*)     /opt/zinc-trunk/lib/libVanadiumSystemDbus.so.0
0x765a64c0  0x765a6bf0  Yes (*)     /opt/zinc-trunk/lib/libVanadiumSystemAPI.so.0
0x76535b70  0x76582790  Yes (*)     /opt/zinc-trunk/lib/libZincDbusBindingRuntime.so.0
0x764d1170  0x764f5580  Yes (*)     /opt/zinc-trunk/lib/libdbus-c++-1.so.0
0x764678a0  0x764a5330  Yes (*)     /opt/zinc-trunk/oss/lib/libdbus-1.so.3
0x76436750  0x7644d4b0  Yes (*)     /opt/zinc-trunk/lib/libZincJsCoreBindingRuntime.so.0
0x763a1940  0x7640b240  Yes (*)     /opt/zinc-trunk/lib/libZincCommon.so.0
0x76376a60  0x76377ed0  Yes (*)     /lib/libdl.so.0
0x7634d740  0x763609f0  Yes (*)     /opt/zinc-trunk/oss/lib/libboost_thread.so.1.57.0
0x762ddba0  0x76321200  Yes (*)     /opt/zinc-trunk/oss/lib/liblog4cplus-1.0.so.4
0x762ab9e0  0x762b4260  Yes (*)     /opt/zinc-trunk/oss/lib/libboost_date_time.so.1.57.0
0x7627c380  0x762907b0  Yes (*)     /opt/zinc-trunk/oss/lib/libboost_filesystem.so.1.57.0
0x76264d60  0x76265e30  Yes (*)     /opt/zinc-trunk/oss/lib/libboost_system.so.1.57.0
0x7624f6f0  0x76252220  Yes (*)     /opt/zinc-trunk/oss/lib/libboost_chrono.so.1.57.0
0x74768620  0x75eb2650  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5WebKit.so.5
0x74290a70  0x74568890  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5Gui.so.5
0x740e6c40  0x741f6970  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5Network.so.5
0x73c27fb0  0x73f48050  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5Core.so.5
0x73b7e810  0x73baea20  Yes (*)     /usr/local/lib/libjpeg.so.8
0x73b61250  0x73b6a2f0  Yes (*)     /lib/libm.so.0
0x73b35680  0x73b4e760  Yes (*)     /lib/libgcc_s.so.1
0x73b07f10  0x73b14da0  Yes (*)     /lib/libpthread.so.0
0x73a55fd0  0x73aa8bb0  Yes (*)     /lib/libc.so.0
0x73a1c560  0x73a366d0  Yes (*)     /usr/local/lib/libz.so.1
0x739f5d00  0x73a084a0  Yes (*)     /usr/local/lib/libfusion-1.4.so.17
0x739d0560  0x739de8b0  Yes         /opt/zinc-trunk/oss/lib/libz.so.1
0x72e554d0  0x72f36830  Yes (*)     /usr/local/lib/libnexus.so
0x72cdab00  0x72debd20  Yes (*)     /opt/zinc-trunk/oss/lib/libxml2.so.2
0x72c0d6b0  0x72ca37e0  Yes         /opt/zinc-trunk/oss/lib/libsqlite3.so.0
0x72bf30a0  0x72bf79d0  Yes         /opt/zinc-trunk/oss/lib/libffi.so.5
0x72bdfb70  0x72be1ad0  Yes (*)     /opt/zinc-trunk/oss/lib/libgmodule-2.0.so.0
0x72b9dd30  0x72bc6bc0  Yes (*)     /opt/zinc-trunk/oss/lib/libxslt.so.1
0x72b7ceb0  0x72b85070  Yes         /opt/zinc-trunk/oss/lib/libgstapp-1.0.so.0
0x72b49a20  0x72b61500  Yes         /opt/zinc-trunk/oss/lib/libgstpbutils-1.0.so.0
0x72ad4060  0x72b1fb90  Yes         /opt/zinc-trunk/oss/lib/libgstvideo-1.0.so.0
0x72a721e0  0x72aac8e0  Yes         /opt/zinc-trunk/oss/lib/libgstaudio-1.0.so.0
0x729fc8f0  0x72a4c760  Yes         /opt/zinc-trunk/oss/lib/libgstbase-1.0.so.0
0x728e0f20  0x729b2e90  Yes         /opt/zinc-trunk/oss/lib/libgstreamer-1.0.so.0
0x7287fc70  0x728b0f80  Yes (*)     /opt/zinc-trunk/oss/lib/libQt5Sql.so.5
0x72860d20  0x728624f0  Yes (*)     /lib/librt.so.0
0x7282b1c0  0x7284b500  Yes (*)     /usr/local/lib/libpng15.so.15
0x72663a70  0x727d0d10  Yes (*)     /opt/zinc-trunk/oss/lib/libicui18n.so.51
0x72495040  0x72578e20  Yes (*)     /opt/zinc-trunk/oss/lib/libicuuc.so.51
                        Yes (*)     /opt/zinc-trunk/oss/lib/libicudata.so.51
0x72111630  0x72111900  Yes (*)     /opt/zinc-trunk/oss/lib/libgthread-2.0.so.0
0x720cf190  0x720f34a0  Yes         /opt/zinc-trunk/oss/lib/libgsttag-1.0.so.0
0x773e0eb0  0x773e66f0  Yes (*)     /lib/ld-uClibc.so.0
0x716ffa60  0x7176c3c0  Yes (*)     /opt/zinc-trunk/oss/plugins/platforms/libqdirectfb.so
0x716d6800  0x716e2470  Yes (*)     /usr/local/lib/directfb-1.4-17-pure/systems/libdirectfb_bcmnexus_sys.so
0x5ab8a2a0  0x5ab92c60  Yes (*)     /usr/local/lib/directfb-1.4-17-pure/gfxdrivers/libdirectfb_bcmnexus_gfx.so
0x5ab6f740  0x5ab77a30  Yes (*)     /usr/local/lib/directfb-1.4-17-pure/wm/libdirectfbwm_sawman.so
0x5ab495c0  0x5ab5ab70  Yes (*)     /usr/local/lib/libsawman-1.5.so.0
0x5a1d22d0  0x5a2f1180  Yes         /opt/zinc-trunk/oss/lib/libcrypto.so.1.0.0
0x5a13edc0  0x5a17b970  Yes (*)     /opt/zinc-trunk/oss/lib/libssl.so.1.0.0
0x5a1171f0  0x5a11f870  Yes (*)     /opt/zinc-trunk/lib/engines/libyvSSLClientAuth.so
0x5a0f9d00  0x5a102450  Yes (*)     /opt/zinc-trunk/lib/libTitaniumTlsSelectorAPI.so.0
0x5a0b6090  0x5a0db500  Yes (*)     /opt/zinc-trunk/lib/libTitaniumUtils.so.0
0x5a07be60  0x5a096a00  Yes (*)     /opt/zinc-trunk/lib/libTitaniumTlsSelectorProduction.so
0x5a041140  0x5a05bcd0  Yes (*)     /opt/zinc-trunk/lib/libCopperSystemAPI.so.0
0x5a016ea0  0x5a023860  Yes (*)     /opt/zinc-trunk/lib/engines/libskb-yv.so
0x59ff5ab0  0x5a00c5b0  Yes (*)     /opt/zinc-trunk/lib/libTitaniumSystemKeysDbusClient.so.0
0x597b46b0  0x597d8280  Yes (*)     /opt/zinc-trunk/lib/libTitaniumApplicationContainerProduction.so
0x59798cb0  0x5979c6e0  Yes         /opt/zinc-trunk/lib/libZincHttpServer.so.0
0x5976f0f0  0x59782f50  Yes         /opt/zinc-trunk/lib/libZincHttpClient.so.0
0x59752d90  0x59758e30  Yes         /opt/zinc-trunk/lib/libZincHttpCurl.so.0
0x59724750  0x5973c860  Yes         /opt/zinc-trunk/lib/libZincHttpCommon.so.0
0x5970bbd0  0x5970f6f0  Yes         /opt/zinc-trunk/lib/libZincHttpMIMETypes.so.0
0x596a3c50  0x596ee010  Yes         /opt/zinc-trunk/oss/lib/libcurl.so.4
0x59682c30  0x5968d1a0  Yes         /opt/zinc-trunk/oss/lib/libmongoose.so.0
0x596405d0  0x59669050  Yes (*)     /opt/zinc-trunk/oss/lib/libcppunit-1.12.so.1
0x59606a30  0x5961aa50  Yes (*)     /opt/zinc-trunk/oss/lib/libgmock.so.0
0x595b2cc0  0x595e28d0  Yes (*)     /opt/zinc-trunk/oss/lib/libgtest.so.0
0x5955c010  0x59585390  Yes (*)     /opt/zinc-trunk/lib/libCopperSystemDbusClient.so
0x5953e1d0  0x5953efe0  Yes (*)     /opt/zinc-trunk/lib/libCopperSystemDbus.so.0
0x5679eb70  0x567a8ff0  Yes (*)     /opt/zinc-trunk/lib/jsplugins/libVanadiumJsCoreClientApi.so
0x56783cf0  0x56786500  Yes (*)     /opt/zinc-trunk/lib/libVanadiumClientApi.so.0
0x56759cd0  0x5676c4d0  Yes (*)     /opt/zinc-trunk/lib/libVanadiumClientSystem.so
0x5673e830  0x56743af0  Yes (*)     /opt/zinc-trunk/lib/libCopperAnnouncementSystemAPI.so.0
0x5671f900  0x56726950  Yes (*)     /opt/zinc-trunk/lib/jsplugins/libSodiumJsCoreClientApi.so
0x56708f00  0x56709df0  Yes (*)     /opt/zinc-trunk/lib/libSodiumClientAPI.so.0
0x566bed50  0x566ed9a0  Yes (*)     /opt/zinc-trunk/lib/libSodiumCommon.so.0
0x566a39e0  0x566a41d0  Yes (*)     /opt/zinc-trunk/lib/libSodiumSystemAPI.so.0
0x565e1310  0x56679990  Yes (*)     /opt/zinc-trunk/lib/libSodiumClientSystem.so
0x56572cf0  0x565b5390  Yes (*)     /opt/zinc-trunk/lib/libNickelTunerSystemDbusClient.so.0
0x5651bf20  0x56545bb0  Yes (*)     /opt/zinc-trunk/lib/libUraniumClientAPI.so.0
0x564e5d70  0x564e6300  Yes (*)     /opt/zinc-trunk/lib/libUraniumCommon.so.0
0x55cca1a0  0x55cd2190  Yes (*)     /opt/zinc-trunk/lib/jsplugins/libHeliumJsCoreClientApi.so
0x55cb33e0  0x55cb3bc0  Yes (*)     /opt/zinc-trunk/lib/libHeliumClientApi.so.0
0x55ca30e0  0x55cae550  Yes (*)     /opt/zinc-trunk/lib/libHeliumClientSystem.so
0x55c5fe90  0x55c85940  Yes (*)     /opt/zinc-trunk/lib/jsplugins/libNickelJsCoreClientApi.so
0x55c0fff0  0x55c31700  Yes (*)     /opt/zinc-trunk/lib/libNickelClientApi.so.0
0x55b78be0  0x55bd4140  Yes (*)     /opt/zinc-trunk/lib/libNickelClientSystem.so
0x55b42880  0x55b54520  Yes (*)     /opt/zinc-trunk/lib/libNickelSystemAPI.so.0
0x55b12f20  0x55b20420  Yes (*)     /opt/zinc-trunk/lib/libNickelAudioFeedbackDbusClient.so.0
0x55ae6180  0x55af8090  Yes (*)     /opt/zinc-trunk/lib/libHeliumSystemDbusClient.so
0x55acf760  0x55acf910  Yes (*)     /opt/zinc-trunk/lib/libHeliumSystemDbus.so.0
0x559d9640  0x55a788b0  Yes (*)     /opt/zinc-trunk/lib/libNickelSystemDbusClient.so
0x559aa9b0  0x559aab60  Yes (*)     /opt/zinc-trunk/lib/libNickelSystemDbus.so.0
0x5598cc80  0x559a36a0  Yes (*)     /opt/zinc-trunk/lib/libSodiumSystemDbusClient.so
0x559741a0  0x55974fb0  Yes (*)     /opt/zinc-trunk/lib/libSodiumSystemDbus.so.0
0x558fda60  0x55911bf0  Yes (*)     /opt/zinc-trunk/lib/libCopperAnnouncementSystemDbusClient.so
0x558e41b0  0x558e4fc0  Yes (*)     /opt/zinc-trunk/lib/libCopperAnnouncementSystemDbus.so.0
0x548d2320  0x548d2330  Yes (*)     /lib/libresolv.so
0x540049d0  0x54042a10  Yes (*)     /opt/zinc-trunk/lib/plugins/webkit/libVanadiumOipfPlugin.so.0.0.0
0x5451f810  0x5452c290  Yes (*)     /opt/zinc-trunk/lib/libVanadiumOipfPluginAPI.so.0
0x544ad280  0x544de660  Yes         /opt/zinc-trunk/lib/libVanadiumOipfPluginProduction.so

// This is a so in question which put some code to crash and see no debug info
// in it.
0x5446c670  0x5448c130  Yes (*)     /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0

0x53f3cc60  0x53facdb0  Yes (*)     /opt/zinc-trunk/lib/libNeonSystemDbusClient.so
0x536e12a0  0x53703cb0  Yes (*)     /opt/zinc-trunk/lib/libMercurySystemDbusClient.so
0x536c31a0  0x536c3fb0  Yes (*)     /opt/zinc-trunk/lib/libMercurySystemDbus.so.0
0x5368a560  0x536a9390  Yes (*)     /opt/zinc-trunk/lib/libMercuryCommon.so.0
0x535d9f80  0x5364a790  Yes (*)     /opt/zinc-trunk/lib/libCobaltSystemDbusClient.so
0x535b07d0  0x535b0980  Yes (*)     /opt/zinc-trunk/lib/libCobaltSystemDbus.so.0
0x535903e0  0x5359dc40  Yes (*)     /opt/zinc-trunk/lib/libCobaltCommon.so.0
(*): Shared library is missing debugging information.
          zero       at       v0       v1       a0       a1       a2       a3
 R0   00000000 00000001 00000064 00000001 77273d5c ffffffff 00000001 00000000 
            t0       t1       t2       t3       t4       t5       t6       t7
 R8   00000000 00000051 00000000 00000000 7fde3c38 00000001 73b03584 3031203e 
            s0       s1       s2       s3       s4       s5       s6       s7
 R16  004ea9b0 7fde3f6c 7fde3ef8 7642d7dc 54485000 7726f15c 7726e520 7726f124 
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00000000 771fc768 00000000 00000000 544ac170 7fde3ec0 7726f138 54479c54 
            sr       lo       hi      bad    cause       pc
      00008b13 00000000 00000000 00000000 0080000c 5447955c 
           fsr      fir
      02000074 00000000 
No symbol table info available.
No symbol table info available.
0x7fde3ec0:	0x0	0x7180f700	0x7622d5a0	0xb3dd98
0x7fde3ed0:	0x10d	0x71812a68	0x544ac170	0x7fde3eac
0x7fde3ee0:	0x1	0xffffffff	0xb33784	0xb7cea8
0x7fde3ef0:	0x4faa9000	0xb5c68c	0xb27724	0x0
0x7fde3f00:	0x7620a7e0	0xb7cea8	0xb7cea8	0x568dffe0
0x7fde3f10:	0xb7cea8	0x1	0x0	0x7558
0x7fde3f20:	0x3613c0	0xb2e800	0x0	0x7633f5c8 <_ZTVN9log4cplus6LoggerE+8>
0x7fde3f30:	0x4eb0d8	0xb33784	0x5448c8e8	0xfd
0x7fde3f40:	0x7726f15c	0x7726f2c0 <_ZTVSt15basic_streambufIcSt11char_traitsIcEE+8>	0xb0f44d	0xb0f44d
0x7fde3f50:	0xb0f44d	0xb0f44c	0xb0f47d	0xb0f64c
0x7fde3f60:	0x77273d5c	0x10	0xb0f44c	0x7726dbf8 <_ZTVSt8ios_base+8>
0x7fde3f70:	0x6	0x0	0x1002	0x0
0x7fde3f80:	0x0	0x0	0x0	0x0
0x7fde3f90:	0x0	0x0	0x0	0x0
0x7fde3fa0:	0x0	0x0	0x0	0x0
0x7fde3fb0:	0x0	0x0	0x0	0x0
0x7fde3fc0:	0x0	0x0	0x0	0x0
0x7fde3fd0:	0x8	0x7fde3f90	0x77273d5c	0x0
0x7fde3fe0:	0x0	0x7fde3f44	0x772730a4	0x7727334c
0x7fde3ff0:	0x77273344	0x4e1008c8	0x7fde3ef4	0x7fde3f44
0x7fde4000:	0x7fde3f60	0x7726f1e0 <_ZTVSt15basic_stringbufIcSt11char_traitsIcESaIcEE+8>	0x7726f2c0 <_ZTVSt15basic_streambufIcSt11char_traitsIcEE+8>	0x4ea9b0
0x7fde4010:	0x7726f170	0x73cf4384	0xb486b8	0x7605c000
0x7fde4020:	0x4acc50	0x7fde4984	0x1	0x0
0x7fde4030:	0x73ee1508	0x17fc9974	0x2	0x756e1fdc
0x7fde4040:	0x74cec000	0x74cf39c4 <_ZSt13__adjust_heapIN7WebCore17TimerHeapIteratorEiPNS0_9TimerBaseENS0_25TimerHeapLessThanFunctionEEvT_T0_S6_T1_T2_+340>	0x7622d5a0	0x12b6006a
=> 0x5447955c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+696>:	sw	v0,0(zero)
   0x54479560 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+700>:	lw	a0,0(s3)
   0x54479564 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+704>:	beqz	a0,0x54479da0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+2812>
   0x54479568 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+708>:	lw	t9,-32128(gp)
   0x5447956c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+712>:	lw	t9,-32296(gp)
   0x54479570 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+716>:	jalr	t9
   0x54479574 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+720>:	move	a1,zero
   0x54479578 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+724>:	bnez	v0,0x54479914 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+1648>
   0x5447957c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+728>:	lw	gp,24(sp)
   0x54479580 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+732>:	lw	v0,92(sp)
   0x54479584 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+736>:	mtc1	v0,$f0
   0x54479588 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+740>:	lw	v0,-32708(gp)
   0x5447958c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+744>:	cvt.s.w	$f1,$f0
   0x54479590 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+748>:	lwc1	$f0,-28112(v0)
   0x54479594 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+752>:	b	0x544794a8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+516>
   0x54479598 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+756>:	div.s	$f0,$f1,$f0
   0x5447959c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+760>:	lw	v0,116(sp)
   0x544795a0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+764>:	addiu	s0,sp,64
   0x544795a4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+768>:	lw	a1,-12(v0)
   0x544795a8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+772>:	lw	v0,-31588(gp)
   0x544795ac <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+776>:	lw	t9,-31824(gp)
   0x544795b0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+780>:	addiu	v0,v0,12
   0x544795b4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+784>:	sw	v0,64(sp)
   0x544795b8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+788>:	move	a0,s0
   0x544795bc <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+792>:	addiu	a1,a1,7
   0x544795c0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+796>:	jalr	t9
   0x544795c4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+800>:	lw	s7,104(sp)
   0x544795c8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+804>:	lw	gp,24(sp)
   0x544795cc <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+808>:	lw	a1,-32728(gp)
   0x544795d0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+812>:	lw	t9,-31972(gp)
   0x544795d4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+816>:	move	a0,s0
   0x544795d8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+820>:	addiu	a1,a1,31076
   0x544795dc <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+824>:	jalr	t9
   0x544795e0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+828>:	li	a2,7
   0x544795e4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+832>:	lw	gp,24(sp)
   0x544795e8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+836>:	lw	t9,-31940(gp)
   0x544795ec <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+840>:	move	a0,s0
   0x544795f0 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+844>:	jalr	t9
   0x544795f4 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+848>:	move	a1,s5
   0x544795f8 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+852>:	lw	gp,24(sp)
   0x544795fc <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+856>:	lw	v0,124(sp)
   0x54479600 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+860>:	lw	a3,120(sp)
   0x54479604 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+864>:	lw	t9,-31744(gp)
   0x54479608 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+868>:	sw	v0,16(sp)
   0x5447960c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+872>:	move	a0,s6
   0x54479610 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+876>:	move	a1,s7
   0x54479614 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+880>:	jalr	t9
   0x54479618 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+884>:	move	a2,s0
   0x5447961c <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+888>:	lw	gp,24(sp)
   0x54479620 <_ZNK8vanadium19VanadiumMediaPlayer11currentTimeEv+892>:	lw	t9,-32180(gp)
[root@HUMAX /]# 


When build with -g and this is summarized output. Shows exact line which
caues null pointer access.

[Thread debugging using libthread_db enabled]
Core was generated by `/opt/zinc-trunk/bin/w3cEngine -debug -cache /app-data/client-cache -cache-size'.

Program terminated with signal 11, Segmentation fault.

#0  0x5443955c in vanadium::VanadiumMediaPlayer::currentTime (this=<optimized out>) at /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Vanadium/Vanadium.WebKit.VideoElement/src/VanadiumMediaPlayer.cpp:271

note: can show source if have sources?

271	/home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Vanadium/Vanadium.WebKit.VideoElement/src/VanadiumMediaPlayer.cpp: No such file or directory.
	in /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Vanadium/Vanadium.WebKit.VideoElement/src/VanadiumMediaPlayer.cpp
  Id   Target Id         Frame 

...

* 1    Thread 0x77273320 (LWP 5552) 0x5443955c in vanadium::VanadiumMediaPlayer::currentTime (this=<optimized out>) at /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Vanadium/Vanadium.WebKit.VideoElement/src/VanadiumMediaPlayer.cpp:271
#0  0x5443955c in vanadium::VanadiumMediaPlayer::currentTime (this=<optimized out>) at /home/kpark/builds-src-dev/_virtual_/humax.2100/DEVARCH/Vanadium/Vanadium.WebKit.VideoElement/src/VanadiumMediaPlayer.cpp:271


note: now has debugging info

0x5442c670  0x5444c130  Yes         /opt/zinc-trunk/lib/libVanadiumWebKitVideoElement.so.0

=> 0x5443955c <vanadium::VanadiumMediaPlayer::currentTime() const+696>:	sw	v0,0(zero)


={============================================================================
*kt_linux_tool_300* gdb-core-code

#include <stdio.h>

int main()
{
    char *p = (char*)0;
    p[0] = 1;
    return 0;
}


={============================================================================
*kt_linux_tool_300* gdb-core-force-core-signal

kill -s SIGSEGV 113
kill -s 11 113
kill -11 `pidof w3cEngine`
kill -s 11 `pidof w3cEngine`

note:
kill -9 do not create a core.

export P=`ps -a | grep APP_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;
export P=`ps -a | grep MW_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;

SIG_KERNEL_COREDUMP_MASK (.../kernel/signal.c) defines sianals to create a core.

SIGSEGV(segmentation fault)

#define SIG_KERNEL_COREDUMP_MASK (\
        M(SIGQUIT)   |  M(SIGILL)    |  M(SIGTRAP)   |  M(SIGABRT)   | \
        M(SIGFPE)    |  M(SIGSEGV)   |  M(SIGBUS)    |  M(SIGSYS)    | \
        M(SIGXCPU)   |  M(SIGXFSZ)   |  M_SIGEMT                     )


={============================================================================
*kt_linux_tool_300* gdb-core-force-core-command

10.19 How to Produce a Core File from Your Program

A core file or core dump is a file that records the memory image of a running
process and its process status (register values etc.). Its primary use is
post-mortem debugging of a program that crashed while it ran outside a debugger.
A program that crashes automatically produces a core file, unless this feature
is disabled by the user. 

See Section 18.1 [Files], page 233, for information on invoking gdb in the
post-mortem debugging mode.

Occasionally, you may wish to produce a core file of the program you are
debugging in order to preserve a snapshot of its state. gdb has a special
command for that.

generate-core-file [file]
gcore [file]

Produce a core dump of the inferior process. The optional argument file
specifies the file name where to put the core dump. If not specified, the file
name defaults to ‘core.pid’, where pid is the inferior process ID.  Note that
this command is implemented only for some systems (as of this writing,
        gnu/Linux, FreeBSD, Solaris, and S390). 

On gnu/Linux, this command can take into account the value of the file
'/proc/pid/coredump_filter' when generating the core dump (see [set
        use-coredump-filter], page 144).


={============================================================================
*kt_linux_tool_300* gdb-core: stack frame

frame n 

Select frame number n. Recall that frame zero is the innermost (currently
    executing) frame, frame one is the frame that called the innermost one, and
so on. The highest-numbered frame is the one for main.


={============================================================================
*kt_linux_tool_300* gdb-core-analysis-on-mips

export LD_LIBRARY_PATH=/home/NDS-UK/parkkt/bins
mips-gdb
set solib-absolute-prefix /junk
set solib-search-path 
  /parkkt/com.nds.darwin.debugsupport/debug_libs/uClibc-nptl-0.9.29-20070423
file APP_Process 
core 272.COR
thread apply all bt full
bt

(gdb) f n

{1}
(gdb) bt
#0 memset () at libc/string/mips/memset.S:132
...

(gdb) i r
	zero     at       v0         v1       a0         a1       a2       a3
R0	00000000 fffffffc [00000000] ffffffff [00000008] 00000000 00000004 00022000
	t0 t1 t2 t3 t4 t5 t6 t7
R8	00000004 00000000 ffffffff 000000c2 00000000 00000004 00a52630 00000000
	s0 s1 s2 s3 s4 s5 s6 s7
R16 00020000 00000000 00022004 2c57b174 00000004 2c70d4d0 2d9dcd30 00be6f70
	t8 t9 k0 k1 gp sp s8 ra
R24 00befcc8 2ab25160 00000000 00000000 00bf7e60 2d9dc858 2d9dcbd0 00993fdc
	sr       lo       hi       bad        cause    pc
	00008413 00000000 00000000 [00000000] 0080000c [2ab251b4]
	fsr fir
	00001004 00000000

// void *memset(void *s, int c, size_t n);

(gdb) disassemble $pc
Dump of assembler code for function memset:
0x2ab25160 <memset+0>: slti t1,a2,8
0x2ab25164 <memset+4>: bnez t1,0x2ab251d4 <memset+116>
0x2ab25168 <memset+8>: move v0,a0 ~
0x2ab2516c <memset+12>: beqz a1,0x2ab25184 <memset+36>
0x2ab25170 <memset+16>: andi a1,a1,0xff
0x2ab25174 <memset+20>: sll t0,a1,0x8
0x2ab25178 <memset+24>: or a1,a1,t0
0x2ab2517c <memset+28>: sll t0,a1,0x10
0x2ab25180 <memset+32>: or a1,a1,t0
0x2ab25184 <memset+36>: negu t0,a0 # negu d, s; d = -s;
0x2ab25188 <memset+40>: andi t0,t0,0x3
0x2ab2518c <memset+44>: beqz t0,0x2ab2519c <memset+60>
0x2ab25190 <memset+48>: subu a2,a2,t0
0x2ab25194 <memset+52>: swl a1,0(a0) # swl t, addr; Store word left/right
0x2ab25198 <memset+56>: addu a0,a0,t0
0x2ab2519c <memset+60>: andi t0,a2,0x7
0x2ab251a0 <memset+64>: beq t0,a2,0x2ab251c0 <memset+96>
0x2ab251a4 <memset+68>: subu a3,a2,t0
0x2ab251a8 <memset+72>: addu a3,a3,a0
0x2ab251ac <memset+76>: move a2,t0
0x2ab251b0 <memset+80>: addiu a0,a0,8 ~
0x2ab251b4 <memset+84>: sw a1,-8(a0) ~
0x2ab251b8 <memset+88>: bne a0,a3,0x2ab251b0 <memset+80>

note: 
This shows that a0 which is the first argument of memset fuction and is NULL.
Hence SIGSEGV. *sigseg*


{2}
(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc38, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x007f09c8 in MHWMemAllocStatic (pool=0x131bc38, size=64) at mem_static.c:63
#2  0x007e6854 in MEMMAN_API_AllocStaticP (pool=0x131bc38, size=41) at memman_st.c:350
#3  0x00419328 in DIAG_JAVA_GetJavaString (env=0x2d7056c0, 
        l_java_string=0x2c4b3358, l_byte_array=0x2d705698, 
        l_len=0x2d70569c) at ../src/natdiag.c:63
#4  0x004197fc in DIAG_JAVA_GetJavaString (env=0x131bc38) at ../src/natdiag.c:341
#5  0x0041a0f8 in Java_com_nds_fusion_diagimpl_DiagImpl_natLogInfo (
        THIS=<value optimized out>, jClass=0x40, jInt=1, 
        jString=0x0) at sunnatdiag.c:177

[New process 124]
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
271     mem_blockpool.c: No such file or directory.
        in mem_blockpool.c

(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x72617469 in ?? ()
Backtrace stopped: previous frame inner to this frame (corrupt stack?)

(gdb) i r
          zero       at       v0       v1       a0       a1       a2       a3
 R0   00000000 00000005 00000004 00000000 0131bc40 00000040 00000001 00000000 
            t0       t1       t2       t3       t4       t5       t6       t7
 R8   0000000c 00000001 ffffffff ffffffff 00000000 00000000 00d7f220 00000200 
            s0       s1       s2       s3       s4       s5       s6       s7
 R16  00000000 0131bc40 0131bc40 00000040 00000001 00000000 00000001 2d705b00 
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00f0c360 00b62d98 00000000 00000000 00f14570 2d7055a0 2d7057d8 007f09c8 
            sr       lo       hi      bad    cause       pc
      00008413 000efdcb 00000005 00000018 00800008 007eb73c 
           fsr      fir
      00001004 00000000 

(gdb) disassemble $pc

// uint32_t MHWMemCheckBank(MEMMAN_API_MemoryPoolHandle * mpool, 
//    uint32_t size, uint32_t mem_nb_bank)
// {
//     MemWholeMemory *bank = mpool->s_WholeMem;
// }

Dump of assembler code for function MHWMemCheckBank:
0x007eb6b8 <MHWMemCheckBank+0>: addiu   sp,sp,-48
0x007eb6bc <MHWMemCheckBank+4>: sw      s4,40(sp)
0x007eb6c0 <MHWMemCheckBank+8>: sw      s3,36(sp)
0x007eb6c4 <MHWMemCheckBank+12>:        sw      s2,32(sp)
0x007eb6c8 <MHWMemCheckBank+16>:        sw      ra,44(sp)
0x007eb6cc <MHWMemCheckBank+20>:        sw      s1,28(sp)
0x007eb6d0 <MHWMemCheckBank+24>:        sw      s0,24(sp)
0x007eb6d4 <MHWMemCheckBank+28>:        lw      s0,40(a0)
0x007eb6d8 <MHWMemCheckBank+32>:        lw      v1,36(a0)
0x007eb6dc <MHWMemCheckBank+36>:        lui     v0,0x4
0x007eb6e0 <MHWMemCheckBank+40>:        lw      a3,108(a0)
0x007eb6e4 <MHWMemCheckBank+44>:        and     v1,v1,v0
0x007eb6e8 <MHWMemCheckBank+48>:        addiu   t0,s0,12
0x007eb6ec <MHWMemCheckBank+52>:        addiu   v0,s0,4
0x007eb6f0 <MHWMemCheckBank+56>:        move    s2,a0
0x007eb6f4 <MHWMemCheckBank+60>:        movn    t0,v0,v1
0x007eb6f8 <MHWMemCheckBank+64>:        move    s3,a1
0x007eb6fc <MHWMemCheckBank+68>:        beqz    a3,0x7eb73c <MHWMemCheckBank+132>
0x007eb700 <MHWMemCheckBank+72>:        move    s4,a2
0x007eb704 <MHWMemCheckBank+76>:        lw      a1,0(t0)
0x007eb708 <MHWMemCheckBank+80>:        lw      v1,24(s0)
0x007eb70c <MHWMemCheckBank+84>:        lw      v0,104(a0)
0x007eb710 <MHWMemCheckBank+88>:        li      a2,100
0x007eb714 <MHWMemCheckBank+92>:        mul     v1,v1,a2
0x007eb718 <MHWMemCheckBank+96>:        mul     v0,a1,v0
0x007eb71c <MHWMemCheckBank+100>:       sltu    v0,v1,v0
0x007eb720 <MHWMemCheckBank+104>:       beqz    v0,0x7eb73c <MHWMemCheckBank+132>
0x007eb724 <MHWMemCheckBank+108>:       nop
0x007eb728 <MHWMemCheckBank+112>:       divu    zero,v1,a1
0x007eb72c <MHWMemCheckBank+116>:       teq     a1,zero,0x7
0x007eb730 <MHWMemCheckBank+120>:       mflo    a1
0x007eb734 <MHWMemCheckBank+124>:       jal     0x7efa14  <MEMMAN_SHL_Notify>
0x007eb738 <MHWMemCheckBank+128>:       subu    a1,a2,a1
0x007eb73c <MHWMemCheckBank+132>:       lw      v0,24(s0) ~
...
---Type <return> to continue, or q <return> to quit---

(gdb) info locals
ind_bank = <value optimized out>
bank = (MemWholeMemory *) 0x0           // note: "bank = mpool->sWholeMem;"
pool_max = (uint32_t *) 0xc

(gdb) x/16wx 0x0131bc40 	// value of a0
0x131bc40:      0x0131f408      0x2ab97ca0      0x00000000      0x00000000
0x131bc50:      0x00000000      0x00000000      0x00000000      0x00000000
0x131bc60:      0xffffffff      0x00000000      0x00000000      0x41445054
0x131bc70:      0x5f444941      0x47000000      0x00000000      0x00000000

note:
This shows that structure passed in a0 has some NULLs and means that this pool
was already destoried. When see destory func, it sets:

poolHandle->s_WholeMem=NULL;


{3}
When a crash in a kernel module happens, you should see output like the
following on the serial or in the dmesg buffer (just run the dmesg command to
    see it). 

// (gdb) i r
//           zero       at       v0       v1       a0       a1       a2       a3
//  R0   00000000 00000005 00000004 00000000 0131bc40 00000040 00000001 00000000 
//             t0       t1       t2       t3       t4       t5       t6       t7
//  R8   0000000c 00000001 ffffffff ffffffff 00000000 00000000 00d7f220 00000200 
//             s0       s1       s2       s3       s4       s5       s6       s7
//  R16  00000000 0131bc40 0131bc40 00000040 00000001 00000000 00000001 2d705b00 
//             t8       t9       k0       k1       gp       sp       s8       ra
//  R24  00f0c360 00b62d98 00000000 00000000 00f14570 2d7055a0 2d7057d8 007f09c8 
//             sr       lo       hi      bad    cause       pc
//       00008413 000efdcb 00000005 00000018 00800008 007eb73c 
//            fsr      fir
//       00001004 00000000 

<4>Unhandled kernel unaligned access[#1]:
<4>Cpu 0
<4>$ 0   : 00000000 10008400 <f7ffdfdf> 80070000                    // note: `v0`
<4>$ 4   : c06e27c0 000ee208 8123a000 898d2680
<4>$ 8   : 00000000 7edbffff ffdeff7f fffb7fff
<4>$12   : fdf7fed7 000ee247 00000001 00000001
<4>$16   : 898d2680 00000000 00000001 00000001
<4>$20   : 898d2680 c06e2800 898d2680 00000001
<4>$24   : 00000001 c0504bcc                  
<4>$28   : 89cd2000 89cd3830 000ee208 c050cbe4
<4>Hi    : 00000128
<4>Lo    : 003e5708
<4>epc   : c050cc54 XHddLowIO+0x124/0x3d8 [xtvfs]     Tainted: P    // note `pc`
<4>ra    : c050cbe4 XHddLowIO+0xb4/0x3d8 [xtvfs]
<4>Status: 10008403    KERNEL EXL IE 
<4>Cause : 00800010
<4>BadVA : f7ffe073
<4>PrId  : 0002a044
<4>Modules linked in: xtvfs mhxnvramfs callisto_periph callisto_tuner callisto
<4>Process mount (pid: 404, threadinfo=89cd2000, task=89a249e8)
<4>Stack : 00000001 000ee08d 00000000 c04e523c c05394c4 00000000 00000000 f7ffdfdf
<4>        c06e2800 003e5708 00000001 000ee208 00000000 c04e523c c05394c4 00000000
<4>        c053950c c050cf50 00808000 c050d21c c06e2800 8567bc00 00000000 003e5708
<4>        c04d7f78 c04d7f58 ff7fffdf 000edf25 00000001 00000001 c067e200 c04d804c
<4>        c05394c4 89cd3950 00000000 c04e523c 000ee208 c06e2800 00000001 00000000
<4>        ...
<4>Call Trace:
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]
<4>[<c050cf50>] XHddLowRead+0x1c/0x28 [xtvfs]
<4>[<c04d7f78>] root_dir_devio_read+0x58/0x12c [xtvfs]
<4>[<c04d809c>] root_dir_devio_lock_read+0x50/0x84 [xtvfs]
<4>[<c04d8430>] RootDirCpyClusterReadBuffer+0xec/0x180 [xtvfs]
<4>[<c04d90cc>] RootDirCpyCheckCreate+0xf0/0x1214 [xtvfs]
<4>[<c04da344>] RootDirCpyInit+0x154/0x200 [xtvfs]
<4>[<c04d2944>] pc_ppart_init+0x10bc/0x12a8 [xtvfs]
<4>[<c04fb178>] XTVFS_CheckPpartInit+0x38/0x32c [xtvfs]
<4>[<c04fc684>] InitPpart+0x238/0x540 [xtvfs]
<4>[<c04fca28>] XTVFS_Mount+0x9c/0x490 [xtvfs]
<4>[<c050c8c8>] xtvfs_read_super+0x1e0/0x370 [xtvfs]
<4>[<c050bb80>] xtvfs_fill_super+0x18/0x48 [xtvfs]
<4>[<8007a6b0>] get_sb_bdev+0x114/0x194
<4>[<c050bb5c>] xtvfs_get_sb+0x2c/0x38 [xtvfs]
<4>[<80079f2c>] vfs_kern_mount+0x68/0xc4
<4>[<80079fe4>] do_kern_mount+0x4c/0x7c
<4>[<80094f10>] do_mount+0x5a8/0x614
<4>[<80095010>] sys_mount+0x94/0xec
<4>[<8000e5f0>] stack_done+0x20/0x3c
<4>
<4>
<4>Code: 00008821  8fa2001c  3c038007 <8c440094> 24631824  0060f809  8c46000c  ae020000  27de0001


Unhandled kernel unaligned access 

An unaligned access is a type of crash. Unlike a segmentation fault, where a
process tries to read memory that is not in its memory map, and unaligned access
is an attempt to read or write an address that is not on a word boundry. On 32
bit CPUs this means an address not divisble by 4. Often this will generate an
exception and some software will handle the access by reading adjacent words and
piecing things together. But in our case the exception is unhandled. 

*mips-register*
This output shows the value of the registers. We are on a MIPS CPU and many of
the registers have defined uses. This
document(http://msdn.microsoft.com/en-us/library/aa448706.aspx) describes them.

For example, $4 to $7 are used to store the first 4 words of function arguments
when calling a function. In assembler these registers are referred to as a0 to
a3. You can't know this in advance, you have to read the MIPS documentation to
find it out.  

epc c050cc54 XHddLowIO+0x124/0x3d8 [xtvfs] 

This is the `exception-program-counter`. It shows the address that the exception
occurred at, and that this is 0x124 bytes into the function XHddLowIO in the
module xtvfs.ko. 


Getting the disassembly

Given a kernel module like xtvfs.ko, it is possible to see the disassembled code
using the objdump -D command. Since we have a mips module, we use the
cross-compiler from the Clearcase Fusion view, so our command will look
something like: 

mips-linux-uclibc-objdump -D xtvfs.ko

We can then look for the function where we crashed, which is XHddLowIO from the
epc trace above. It starts like this: 

00045b30 <XHddLowIO>:
   45b30:       27bdffb8        addiu   sp,sp,-72
   45b34:       3c020002        lui     v0,0x2
   45b38:       afb50034        sw      s5,52(sp)
   45b3c:       345500d0        ori     s5,v0,0xd0
   45b40:       3c020000        lui     v0,0x0
   45b44:       afb7003c        sw      s7,60(sp)

From the call trace we can calculate the address offset in use. Recall: 

<4>epc   : c050cc54 XHddLowIO+0x124/0x3d8 [xtvfs]     Tainted: P    // note `pc`
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]

Virtual         
0xxxxxx         00045b30
    +0x124          +0x124
c050cc54        00045c54

So c050cc54 - 0x124 - 45b30 = offset = c04c7000 (the start of loadded in memory
    for this xtvfs.ko). The crash happened at c050cc54 which will appear as
c050cc54 - c04c7000 = 45c54 in the disassembly. That code looks like this: 

or 45b30+0x124 = 45c54


   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)
   45c50:       3c030000        lui     v1,0x0
 {45c54}:       8c440094        lw      a0,148(v0)
   45c58:       24630000        addiu   v1,v1,0
   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

So we have crashed executing an lw instruction. 


Using the relocation table

Given a kernel module like xtvfs.ko, it is possible to see the offsets of
functions (the relocation table) using the objdump -r command. Since we have a
mips module, we use the cross-compiler from the Clearcase Fusion view, so our
command will look something like: 

mips-linux-uclibc-objdump -r xtvfs.ko > xtvfs_relocations


The output near to our crash address of 45c54 is a table like this: 

 00045c20 R_MIPS_26         .text
 00045c2c R_MIPS_26         .text
 00045c44 R_MIPS_26         .text
 00045c50 R_MIPS_HI16       __getblk
 00045c58 R_MIPS_LO16       __getblk
 00045c78 R_MIPS_HI16       printk
 00045c80 R_MIPS_LO16       printk


Because we are using load time
relocation(http://eli.thegreenplace.net/2011/08/25/load-time-relocation-of-shared-libraries/)
of shared libraries, this table tells the operating system how to replace
addresses in the code. 

The first column is the address in the code, the second column is the type of
relocation to do, and the third column is the address to relocate. So the code
at address 45c50 and 45c58 will get 'overwritten' with the address of __getblk.
That makes the disassembly of the code near our crash look like this: 

   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)

   45c50:       3c030000        lui     v1,__getblk     // note: overwritten
   45c54:       8c440094        lw      a0,148(v0) ~
   45c58:       24630000        addiu   v1,v1,0

   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

Understanding __getblk

At 45c5c there is a jump instruction to v1 which has been loaded with the
address of __getblk. But we crashed immediately before that.

    "So it seems we crashed while 'preparing' to call __getblk." 

So it would help to understand this function. We can look it up in the kernel
code: http://lxr.free-electrons.com/source/fs/buffer.c?v=2.6.30;a=mips#L1363

1362 /*
1363  * __getblk will locate (and, if necessary, create) the buffer_head
1364  * which corresponds to the passed block_device, block and size. The
1365  * returned buffer has its reference count incremented.
1366  *
1367  * __getblk() cannot fail - it just keeps trying.  If you pass it an
1368  * illegal block number, __getblk() will happily return a buffer_head
1369  * which represents the non-existent block.  Very weird.
1370  *
1371  * __getblk() will lock up the machine if grow_dev_page's try_to_free_buffers()
1372  * attempt is failing.  FIXME, perhaps?
1373  */
1374 struct buffer_head *
1375 __getblk(struct block_device *bdev, sector_t block, unsigned size)
1376 {
1377         struct buffer_head *bh = __find_get_block(bdev, block, size);
1378 
1379         might_sleep();
1380         if (bh == NULL)
1381                 bh = __getblk_slow(bdev, block, size);
1382         return bh;
1383 }
1384 EXPORT_SYMBOL(__getblk);

We should also notice that it can be called via inline function sb_blk : 

287 static inline struct buffer_head *
288 sb_getblk(struct super_block *sb, sector_t block)
289 {
290         return __getblk(sb->s_bdev, block, sb->s_blocksize);
291 }


Understanding where in our C code the crash happened

Now we can tell exactly where in our C code the crash happened. We know we were
in the function XHddLowIO from the call trace and now we know we were calling
__getblk or sb_getblk. In XHddLowIO in the XTVFS code we can see: 


/* allocate sector buffers */
for(i = 0; i < cnt; i++){
    bh_array[i] = sb_getblk(sb,  block++);                          // note
    if(!bh_array[i]){ /* no sufficient buffers */
        printk("\n BH_ArrayXHddLowIO: bh = 0, i = %d !!!!!", i);
        if(0 == i){ /* no at all */
            return X_ERROR;
        }
        /* use what we have */
        cnt = i;
    }

So it is likely that the crash happened very close to this sb_getblk call. 


Understanding the lw instruction

Recall the instruction that crashed: 

   45c54:       8c440094        lw      a0,148(v0)

What does that notation mean? We can look up information about the MIPS
instructions: Description: A word is loaded into a register from the specified
address. 

Operation: $t = MEM[$s + offset]; advance_pc (4); Syntax: lw $t, offset($s) The
whole instruction means: load a0 with the address in v0 + 148. 


*mips-register*
Understanding the MIPS registers

In MIPS, registers tend to have special functions, such as return addresses or
function parameters. We can read about this online. 

`a0`, the register we are writing to, is the first of the "function argument
registers that hold the first four words of integer type arguments." So a0 is
the first argument to the function we are calling.  

`v0` is a "function result register" and is also called $2. So we know its value
from the original trace: 

<4>$ 0   : 00000000 10008400 f7ffdfdf 80070000

It is f7ffdfdf. Which is an *odd number*. Since we are trying to read from this
address and do arithmetic (add 148) with it, this would explain why we get an
unaligned access. 


Putting it all together

We are executing this line of C: 

bh_array[i] = sb_getblk(sb,  block++);

<conclusion>
Because sb_getblk is an inline function, it has been expanded by the compiler
into: __getblk(sb->s_bdev, block, sb->s_blocksize); So our crashing instruction
is adding 148 because 148 is the offset of s_bdev withing the sb struct. We hav
verify this by looking at struct super_block in the code.  However, sb has
somehow become and odd number, and THAT is our bug. 


{4}
(gdb) bt
#0  getPresentationTimeCached (timeFormat=GST_FORMAT_TIME, this=0x56b900) 
    at /r.cpp:1433

(gdb) disassemble $pc
Dump of assembler code for function nickel::system::GstMediaRouter::getPosition_locked(bool) const:
   0x73a1dac0 <+0>:	lui	gp,0x8
   0x73a1dac4 <+4>:	addiu	gp,gp,7440
   0x73a1dac8 <+8>:	addu	gp,gp,t9
   0x73a1dacc <+12>:	addiu	sp,sp,-328
   0x73a1dad0 <+16>:	sw	s7,316(sp)
   0x73a1dad4 <+20>:	lw	s7,-29712(gp)
   0x73a1dad8 <+24>:	sw	s3,300(sp)
   0x73a1dadc <+28>:	lw	s3,0(s7)
   0x73a1dae0 <+32>:	sw	gp,24(sp)
   0x73a1dae4 <+36>:	sw	s8,320(sp)
   0x73a1dae8 <+40>:	sw	s1,292(sp)
   0x73a1daec <+44>:	sw	s0,288(sp)
   0x73a1daf0 <+48>:	sw	ra,324(sp)
   0x73a1daf4 <+52>:	sw	s6,312(sp)
   0x73a1daf8 <+56>:	sw	s5,308(sp)
   0x73a1dafc <+60>:	sw	s4,304(sp)
   0x73a1db00 <+64>:	sw	s2,296(sp)
   0x73a1db04 <+68>:	move	s1,a0
   0x73a1db08 <+72>:	move	s0,a1
   0x73a1db0c <+76>:	beqz	s3,0x73a1df18 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+1112>
   0x73a1db10 <+80>:	andi	s8,a2,0xff
   0x73a1db14 <+84>:	lw	a1,-32724(gp)
   0x73a1db18 <+88>:	lw	t9,-30044(gp)
   0x73a1db1c <+92>:	addiu	s2,sp,36
   0x73a1db20 <+96>:	move	a0,s2
   0x73a1db24 <+100>:	addiu	a1,a1,27172
   0x73a1db28 <+104>:	jalr	t9
   0x73a1db2c <+108>:	addiu	a2,sp,32
   0x73a1db30 <+112>:	lw	gp,24(sp)
   0x73a1db34 <+116>:	addiu	v0,sp,64
---Type <return> to continue, or q <return> to quit---q


Use `disassemble` and  get the same result but don't need to work out
function address.


(gdb) x/10i $pc
=> 0x73a1de64 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+932>:	lw	v0,0(a0)
   0x73a1de68 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+936>:	lw	t9,44(v0)
   0x73a1de6c <nickel::system::GstMediaRouter::getPosition_locked(bool) const+940>:	jalr	t9
   0x73a1de70 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+944>:	li	a1,3
   0x73a1de74 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+948>:	lw	gp,24(sp)
   0x73a1de78 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+952>:	sw	v0,276(sp)
   0x73a1de7c <nickel::system::GstMediaRouter::getPosition_locked(bool) const+956>:	b	0x73a1dbac <nickel::system::GstMediaRouter::getPosition_locked(bool) const+236>
   0x73a1de80 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+960>:	move	s5,v1
   0x73a1de84 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+964>:	bne	s5,s2,0x73a1dc48 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+392>
   0x73a1de88 <nickel::system::GstMediaRouter::getPosition_locked(bool) const+968>:	move	v0,zero
(gdb) 

(gdb) p getPresentationTimeCached
$3 = {int64_t (const nickel::system::GstMediaRouter * const, GstFormat)} 0x73a1c7c4 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const>

(gdb) x/50i 0x73a1c7c4
   0x73a1c7c4 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const>:	lbu	v0,40(a0)
   0x73a1c7c8 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+4>:	beqz	v0,
   0x73a1c7dc <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+24>
   0x73a1c7cc <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+8>:	nop
   0x73a1c7d0 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+12>:	lw	v1,124(a0)
   0x73a1c7d4 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+16>:	jr	ra
   0x73a1c7d8 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+20>:	lw	v0,120(a0)
   0x73a1c7dc <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+24>:	lw	a0,144(a0)
   0x73a1c7e0 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+28>:	lw	v0,0(a0)
   0x73a1c7e4 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+32>:	lw	t9,44(v0)
   0x73a1c7e8 <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+36>:	jr	t9
   0x73a1c7ec <nickel::system::GstMediaRouter::getPresentationTimeCached(GstFormat) const+40>:	nop
   0x73a1c7f0 <nickel::system::GstMediaRouter::getControlCapabilities_async() const>:	lui	gp,0x8
   0x73a1c7f4 <nickel::system::GstMediaRouter::getControlCapabilities_async() const+4>:	addiu	gp,gp,12256


(gdb) p getPosition_locked
$6 = {const Zinc::Media::Position (const nickel::system::GstMediaRouter * const, bool)} 0x73a1dac0 <nickel::system::GstMediaRouter::getPosition_locked(bool) const>

(gdb) x/500i 0x73a1dac0

   0x73a1db7c <nickel::system::GstMediaRouter::getPosition_locked(bool) const+188>:	bal	0x73a1d60c <nickel::system::(anonymous namespace)::getBufferPosition(RefObj<_GstElement> const&, bool)>

const Position GstMediaRouter::getPosition_locked(bool liveMode) const
{
    const std::pair<gint64, gint64> bufPos = getBufferPosition(pipeline,liveMode);
    int64_t presentationStreamTime = getPresentationTimeCached(GST_FORMAT_TIME);
    ...
}

Couldn't see getPresentationTimeCached in getPositon_locked disassemled code as
getBufferPosition does. Why?


<05>
(gdb) bt
#0  MHWMemElementIsFree (pool=0xf49060, theAllocator=0x0, theElement=0x1981fa8, theIndex=0x0, 
    isFree=0xfa0c98) at mem_allocator.c:648
#1  0x006672a0 in IS_HANDLE (ptr=0x1981fa8, pool=0xf49060) at mem_moveable.c:731
#2  0x006666b8 in MHWMemFreeStatic (pool=0xf49060, staticBlock=0x19a5a20) at mem_static.c:771
#3  0x0066371c in MEMMAN_API_FreeStaticP (pool=0xf49060, ptr=0x19a5a20) at memman_st.c:667
#4  0x0060be08 in FDM_SVR_OPTV_TASK_P_LoadModuleNotifierCb (msg_ctx=0x19a5a20, 
    answer=<optimized out>, user_tag=<optimized out>, status=<optimized out>)
    at fdm_svr_optv_processor.c:448
...

(gdb) i r
          zero       at       v0       v1       a0       a1       a2       a3
 R0   00000000 10008400 00fa0c98 00000000 00f49060 00000000 01981fa8 00000000 
            t0       t1       t2       t3       t4       t5       t6       t7
 R8   0199d3c8 0199d3c8 ffffffff ffffffff 00fa0978 00000001 00000000 00000000 
            s0       s1       s2       s3       s4       s5       s6       s7
 R16  019a5a20 00f2f288 019a5a18 00f49060 00fa7e54 0060bd68 00000001 0000001f 
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00c9eb58 2aada010 00000000 00000000 2aae5010 00fa0c58 0051ff93 006672a0 
            sr       lo       hi      bad    cause       pc
      00008413 00000001 00000000 00000008 00800008 00664a10 
           fsr      fir
      00000000 00000000 

(gdb) disassemble $pc
Dump of assembler code for function MHWMemElementIsFree:
   0x00664a04 <+0>:	addiu	sp,sp,-32
   0x00664a08 <+4>:	sw	s0,24(sp)
   0x00664a0c <+8>:	sw	ra,28(sp)
=> 0x00664a10 <+12>:	lw	v1,8(a1)
   0x00664a14 <+16>:	lw	t4,16(a1)
   0x00664a18 <+20>:	lw	t0,12(a1)
   0x00664a1c <+24>:	mul	t5,t4,v1
   0x00664a20 <+28>:	mul	t3,t0,v1
   0x00664a24 <+32>:	lw	s0,48(sp)
   0x00664a28 <+36>:	lw	a0,0(a1)
   0x00664a2c <+40>:	move	t1,a2
   0x00664a30 <+44>:	j	0x664a44 <MHWMemElementIsFree+64>


(gdb) f 0
#0  MHWMemElementIsFree (pool=0xf49060, theAllocator=0x0, theElement=0x1981fa8, theIndex=0x0, 
    isFree=0xfa0c98) at mem_allocator.c:648
warning: Source file is more recent than executable.
648	    frameSize = theAllocator->elemNumber * elemSize;


646:    frame = theAllocator->firstFrame;
647:    elemSize = theAllocator->elemSize;
648:    frameSize = theAllocator->elemNumber * elemSize;
649:    nextFrameSize = theAllocator->numberIncrement * elemSize;

note that the line number of source do not exactly match to gdb.


={============================================================================
*kt_linux_tool_300* gdb-step

5.2 Continuing and Stepping

until
u
    Continue running until a source line past the current line, in the current
    stack frame, is reached. This command is used to avoid single stepping
    through a loop more than once. It is like the next command, except that when
    until encounters a jump, it automatically continues execution until the
    program counter is greater than the address of the jump.

    This means that when you reach the end of a loop after single stepping
    though it, until makes your program continue execution until it exits the
    loop. In contrast, a next command at the end of a loop simply steps back to
    the beginning of the loop, which forces you to step through the next
    iteration.

    until always stops your program if it attempts to exit the current stack
    frame.

    until may produce somewhat counterintuitive results if the order of machine
    code does not match the order of the source lines. For example, in the
    following excerpt from a debugging session, the f (frame) command shows that
    execution is stopped at line 206; yet when we use until, we get to line 195:

              (gdb) f
              #0  main (argc=4, argv=0xf7fffae8) at m4.c:206
              206                 expand_input();
              (gdb) until
              195             for ( ; argc > 0; NEXTARG) {

    This happened because, for execution efficiency, the compiler had generated
        code for the loop closure test at the end, rather than the start, of the
        loop - even though the test in a C for-loop is written before the body
        of the loop. The until command appeared to step back to the beginning of
        the loop when it advanced to this expression; however, it has not really
        gone to an earlier statement - not "in terms of" the actual machine
        code.

    until with no argument works by means of single instruction stepping, and
    hence is slower than until with an argument.

until location
u location
    Continue running your program until either the specified location is
    reached, or the current stack frame returns. The location is any of the
    forms described in Specify Location. This form of the command uses temporary
    breakpoints, and hence is quicker than until without an argument. The
    specified location is actually reached 'only' if it is in the current frame.
    This implies that until can be used to skip over recursive function
    invocations. For instance in the code below, if the current location is line
    96, issuing until 99 will execute the program up to line 99 in the same
    invocation of factorial, i.e., after the inner invocations have returned.

              94	int factorial (int value)
              95	{
              96	    if (value > 1) {
              97            value *= factorial (value - 1);
              98	    }
              99	    return (value);
              100     }


advance location
    Continue running the program up to the given location. An argument is
    required, which should be of one of the forms described in Specify Location.
    Execution will also stop upon exit from the current stack frame. This
    command is similar to until, but advance will not skip over recursive
    function calls, and the target location does 'not' have to be in the same
    frame as the current one. 

step, s
    Continue running your program until control reaches a different source line,
             then stop it and return control to gdb.

    Warning: If you use the step command while control is within a function that
    was compiled without debugging information, execution proceeds until control
    reaches a function that does have debugging information. Likewise, it will
    'not' step into a function which is compiled without debugging information.
    To step through functions without debugging information, use the `stepi`
    command, described below. 

    The step command only stops at the first instruction of a source line. This
    prevents the multiple stops that could otherwise occur in switch statements,
    for loops, etc. step continues to stop if a function that has debugging
        information is called within the line. In other words, step steps inside
            any functions called within the line.

    Also, the step command 'only' enters a function if there is line number
    information for the function. Otherwise it acts like the next command. This
    avoids problems when using cc -gl on MIPS machines. Previously, step entered
    subroutines if there was any debugging information about the routine. 

stepi
stepi arg
si
    Execute one machine instruction, then stop and return to the debugger.

    It is often useful to do "display/i $pc" when stepping by machine
    instructions. This makes gdb automatically display the next instruction to
    be executed, each time your program stops. See Automatic Display.

    An argument is a repeat count, as in step. 

    note:
    `stepi` shows more than when use "set step-mode on; step".

nexti
nexti arg
ni
    Execute one machine instruction, but if it is a function call, proceed until
    the function returns.

    An argument is a repeat count, as in next. 


<ex>
The "at lineno" means the line to be run next.

(gdb) n
main () at sample.c:28
28	    printf("ends main\n");
(gdb)

<ex>
This shows something puzzling.

template<typename T, typename A1>
void expose_impl(DBus::Connection conn, const char* path,
        boost::shared_ptr<T> obj, A1 a1) {
    using namespace std;

// note: the first break
46:	expose(conn, path, obj, a1);
}

template<typename T, typename A1>
void DBusService::expose(const char* path, boost::shared_ptr<T> obj, A1 a1) {
    using namespace std;
145:	detail::expose_impl(conn, path, obj, a1);
      objects.push_back(path);
}


// note: the first break
(gdb) bt
#0  expose_impl<Zinc::Media::MediaRouterFactoryAsync, boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > (
    a1=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
    obj=<error reading variable: access outside bounds of object referenced via synthetic pointer>, conn=<incomplete type>, path=<optimized out>)
    at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:46

#1  expose<Zinc::Media::MediaRouterFactoryAsync, boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > (
    a1=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
    obj=<error reading variable: access outside bounds of object referenced via synthetic pointer>, this=0x7fff6510, path=<optimized out>)
    at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:145

#2  (anonymous namespace)::MediaDaemon::start (this=0x7fff64e4, argc=<optimized out>, argv=<optimized out>)
    at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Nickel/Nickel.System.DBusServer/src/MediaDaemon.cpp:123
#3  0x77e57404 in DBus::DefaultMainLoop::processCommands() () from /opt/zinc/lib/libdbus-c++-1.so.0
#4  0x77e58ff0 in DBus::DefaultMainLoop::dispatch(unsigned int) () from /opt/zinc/lib/libdbus-c++-1.so.0
#5  0x77e5a6c4 in DBus::BusDispatcher::run() () from /opt/zinc/lib/libdbus-c++-1.so.0
#6  0x77ec24ec in zinc::binding::dbus::MainLoop::run (this=0x7fff64c8)
    at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Zinc/Zinc.DBus.BindingRuntime/src/dbus/MainLoop.cpp:70
#7  0x00408150 in main (argc=1, argv=0x7fff6864) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Nickel/Nickel.System.DBusServer/src/MediaDaemon.cpp:234


// note: when do n(next) the line 46 has no effect and goes back to where
// expose_impl gets called.

(gdb) n
expose<Zinc::Media::MediaRouterFactoryAsync, boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > (
    a1=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
    obj=<error reading variable: access outside bounds of object referenced via synthetic pointer>, this=0x7fff6510, path=<optimized out>)
    at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:145
145		detail::expose_impl(conn, path, obj, a1);


={============================================================================
*kt_linux_tool_300* gdb-print-gdb-set


={============================================================================
*kt_linux_tool_309* gdb-break

5.1 Breakpoints, Watchpoints, and Catchpoints

A breakpoint makes your program stop whenever a certain point in the program is
reached. For each breakpoint, you can add conditions to control in finer detail
whether your program stops. You can set breakpoints with the break command and
its variants (see Setting Breakpoints), to specify the place where your program
should stop 'by' line number, function name or exact address in the program. 

On some systems, you can set breakpoints in shared libraries before the
executable is run. 

A watchpoint is a special breakpoint that stops your program when the value of
an expression changes. The expression may be a value of a variable, or it could
involve values of one or more variables combined by operators, such as 'a + b'.
This is sometimes called 'data' breakpoints. You must use a different command to
set watchpoints, but aside from that, you can manage a watchpoint like any other
breakpoint: you enable, disable, and delete both breakpoints and watchpoints
using the same commands.

You can arrange to have values from your program displayed automatically
whenever gdb stops at a breakpoint. See Automatic Display.

A catchpoint is another special breakpoint that stops your program when a
certain kind of 'event' occurs, such as the throwing of a C++ exception or the
loading of a library. As with watchpoints, you use a different command to set a
catchpoint, but aside from that, you can manage a catchpoint like any other
breakpoint. (To stop when your program receives a signal, use the handle
    command; see Signals.)

gdb assigns a number to each breakpoint, watchpoint, or catchpoint when you
create it; these numbers are successive integers starting with one. In many of
the commands for controlling various features of breakpoints you use the
breakpoint number to say which breakpoint you want to change. Each breakpoint
may be enabled or disabled; if disabled, it has no effect on your program until
you enable it again.

Some gdb commands accept a range of breakpoints on which to operate. A
breakpoint range is either a single breakpoint number, like '5', or two such
numbers, in increasing order, separated by a hyphen, like '5-7'. When a
breakpoint range is given to a command, all breakpoints in that range are
operated on. 


={============================================================================
*kt_linux_tool_309* gdb-break-set-location

5.1.1 Setting Breakpoints

Breakpoints are set with the break command (abbreviated b).

break location
    Set a breakpoint at the given location, which can specify a function name, a
    line number, or an address of an instruction. The breakpoint will stop your
    program just before it executes any of the code in the specified location.

    When using source languages that permit overloading of symbols, such as C++,
    a function name may refer to more than one possible place to break. See
      Ambiguous Expressions, for a discussion of that situation.

    It is also possible to insert a breakpoint that will stop the program only
    if a specific 'thread' (see Thread-Specific Breakpoints).


9.2 Specifying a Location

Several gdb commands accept arguments that specify a location of your program's
code. Since gdb is a source-level debugger, a location usually specifies some
line in the source code; for that reason, locations are also known as linespecs.

9.2.1 Linespec Locations

A linespec is a colon-separated list of source location parameters such as file
name, function name, etc. Here are all the different ways of specifying a
linespec:

linenum
    Specifies the line number linenum of the current source file.
-offset
+offset
    Specifies the line offset lines before or after the current line. For the
    list command, the current line is the last one printed; for the breakpoint
    commands, this is the line at which execution stopped in the currently
    selected stack frame (see Frames, for a description of stack frames.) When
    used as the second of the two linespecs in a list command, this specifies
    the line offset lines up or down from the first linespec.

filename:linenum
    Specifies the line linenum in the source file filename. If filename is a
    relative file name, then it will match any source file name with the same
    trailing components. For example, if filename is ‘gcc/expr.c’, then it will
    match source file name of /build/trunk/gcc/expr.c, but not
    /build/trunk/libcpp/expr.c or /build/trunk/gcc/x-expr.c.

function
    Specifies the line that begins the body of the function function. For
    example, in C, this is the line with the open brace.

function:label
    Specifies the line where label appears in function.

filename:function
    Specifies the line that begins the body of the function in the file
    filename. You only need the file name with a function name to avoid
    ambiguity when there are identically named functions in different source
    files.

label
    Specifies the line at which the label named label appears in the function
    corresponding to the currently selected stack frame. If there is no current
    selected stack frame (for instance, if the inferior is not running), then
    gdb will not search for a label. 


9.2.2 Explicit Locations

Explicit locations allow the user to directly specify the source location's
parameters using option-value pairs.

Explicit locations are useful when several functions, labels, or file names have
the 'same' name (base name for files) in the program's sources. In these cases,
    explicit locations point to the source line you meant more accurately and
      unambiguously. Also, using explicit locations might be faster in large
      programs.

For example, the linespec 'foo:bar' may refer to a function bar defined in the
file named foo or the label bar in a function named foo. gdb must search either
the file system or the symbol table to know.

The list of valid explicit location options is summarized in the following
table:

-source filename
    The value specifies the source file name. To differentiate between files
    with the same base name, prepend as many directories as is necessary to
    uniquely identify the desired file, e.g., foo/bar/baz.c. Otherwise gdb will
    use the first file it finds with the given base name. This option requires
    the use of either -function or -line.

-function function
    The value specifies the name of a function. Operations on function locations
    unmodified by other options (such as -label or -line) refer to the line that
    begins the body of the function. In C, for example, this is the line with
    the open brace.

-label label
    The value specifies the name of a label. When the function name is not
    specified, the label is searched in the function of the currently selected
    stack frame.

-line number
    The value specifies a line offset for the location. The offset may either be
    absolute (-line 3) or relative (-line +3), depending on the command. When
    specified without any other options, the line offset is relative to the
    current line. 

Explicit location options may be abbreviated by omitting any non-unique trailing
characters from the option name, e.g., break -s main.c -li 3. 


9.2.3 Address Locations


={============================================================================
*kt_linux_tool_309* gdb-break-set-location-in-cpp

<1>
(gdb) break lib.cpp:5
Breakpoint 2 at 0xb7fd846e: file lib.cpp, line 5.


<namespace>
ProxyMediaRouter.cpp
448: NS_ZINC::Future< void > ProxyMediaRouter::setSource(
    const std::string& mediaLocator_in, const SetSourceReason::Enum reason_in)

(gdb) b ProxyMediaRouter.cpp:448                            // okay

(gdb) b ProxyMediaRouter.cpp:ProxyMediaRouter::setSource    // not work

note: why?

$ nm --defined-only --demangle Nickel.System.Proxy/src/.libs/
    libNickelSystemProxy.so.0.0.0 | ag setSource

000135e4 t nickel::system::ProxyMediaRouter::setSource(
    std::string const&, Zinc::Media::SetSourceReason::Enum)

(gdb) b ProxyMediaRouter.cpp:nickel::system::ProxyMediaRouter::setSource  // okay


<completion>
Press the TAB key whenever you want gdb to fill out the rest of a word; command,
function name or a field in a structure,

Sometimes the string you need, while logically a "word", may contain parentheses
  or other characters that gdb normally excludes from its notion of a word. To
  permit word completion to work in this situation, you may enclose words in ’
  (single quote marks) in gdb commands.

The most likely situation is C++ function overloading:

(gdb) b ’bubble( <tab>
bubble(double,double) bubble(int,int)
(gdb) b ’bubble(

note: "nickel::system" is namespace  

(gdb) b nickel::system::ProxyMediaRouter::setS
setSink(std::string const&)
setSink(std::string const&)::__PRETTY_FUNCTION__
setSource(std::string const&, Zinc::Media::SetSourceReason::Enum)
setSourceComplete(std::string const&, boost::shared_ptr<Zinc::Media::MediaRouterAsync>, boost::shared_ptr<Zinc::Media::MediaRouterFactoryAsync>)
setSourceFailed(zinc::ErrorCode const&)
setSubtitleTrack(int, std::string const&)
(gdb) b nickel::system::ProxyMediaRouter::setS


<template>
template <typename T> void coin(T v);
template<typename T> class Foo;

(gdb) b Foo<int>::bar(int)
Breakpoint 2 at 0x804871d: file main.cpp, line 16.
(gdb) b void coin<int>(int)
Breakpoint 1 at 0x804872a: file main.cpp, line 6.


<read>
Setting Breakpoint in Templates:

Templates can be much harder to set breakpoints in because we have to specify
the exact prototype for the fully-defined template. We as programmers are used
to the compiler handling the template type stuff for us, so it can be difficult
to guess the correct type.

For example, assume we have some abstract class BarAbstract that uses the
template Foo<> to make it concrete. Now assume we’re really clever and we want
to hide this from the users of our class. We could use a typedef to hide the
true type:

typedef Foo<BarAbstract> Bar;

Now, all the user needs to do is instantiate Bar without a thought to the Foo<>
    template.

int DoSomething()
{
Bar b;
return b.Baz();
}

So far so good? Okay, now how the heck do you set a breakpoint in the Baz()
    method of class Bar?

The quick answer: use objdump, c++filt, and grep to find the complete definition
that GDB will need.

$ objdump -t libMyLib.so | c++filt | grep ‘BarAbstract.*Baz’
0000d2d6 w F .text 0000000a     MY_PLUGIN_A::Foo<MY_PLUGIN_A::BarAbstract>::Baz()

Now, simply copy-n-paste the full method definition to GDB when setting the breakpoint:

(gdb) b MY_PLUGIN_A::Foo<MY_PLUGIN_A::BarAbstract>::Baz()
Breakpoint 6 at 0×8048890: file Source/Bar.cpp, line 355


$ man c++filt

NAME
       c++filt - Demangle C++ and Java symbols.


={============================================================================
*kt_linux_tool_309* gdb-break-set

5.1.1 Setting Breakpoints

tbreak args
    Set a breakpoint enabled only for 'one' stop. The args are the same as for
    the break command, and the breakpoint is set in the same way, but the
    breakpoint is automatically deleted after the first time your program stops
    there. See Disabling Breakpoints. 


break ... if cond
    Set a breakpoint with condition cond; evaluate the expression cond each time
    the breakpoint is reached, and stop only if the value is nonzero—that is, if
    cond evaluates as true. ‘...’ stands for one of the possible arguments
    described above (or no argument) specifying where to break. See Break
    Conditions, for more information on breakpoint conditions. 


<ex>
gpointer
g_object_ref (gpointer _object)
{
  ...
}

break g_object_ref if _object == 0xcafebabe
break g_object_unref if _object == 0xcafebabe


gdb normally implements breakpoints by replacing the program code at the
breakpoint address with a special instruction, which, when executed, given
control to the debugger.  By default, the program code is so modified only when
the program is resumed. As soon as the program stops, gdb restores the original
instructions. This behaviour guards against leaving breakpoints inserted in the
target should gdb abrubptly disconnect. However, with slow remote targets,
inserting and removing breakpoint can reduce the performance. This behavior can
  be controlled with the following commands::

set breakpoint always-inserted off
    All breakpoints, including newly added by the user, are inserted in the
    target only when the target is resumed. All breakpoints are removed from the
    target when it stops. This is the 'default' mode.

set breakpoint always-inserted on
    Causes all breakpoints to be inserted in the target at all times. If the
    user adds a new breakpoint, or changes an existing breakpoint, the
    breakpoints in the target are updated immediately. A breakpoint is removed
    from the target only when breakpoint itself is deleted. 


gdb handles conditional breakpoints by evaluating these conditions when a
breakpoint breaks. If the condition is true, then the process being debugged
stops, otherwise the process is resumed.

If the target supports evaluating conditions on its end, gdb may download the
breakpoint, together with its conditions, to it.

This feature can be controlled via the following commands:

set breakpoint condition-evaluation auto
    This is the default mode. If the target supports evaluating breakpoint
    conditions on its end, gdb will download breakpoint conditions to the target
    (limitations mentioned previously apply). If the target does not support
    breakpoint condition evaluation, then gdb will fallback to evaluating all
    these conditions on the host's side. 


={============================================================================
*kt_linux_tool_309* gdb-break: conditional ex

There are times when simply breaking on every call to a function isn't what you
want. Sometimes you want to be more 'selective'. For example, with the problem
I'm debugging right now, I'm interested in when close() is called on a
particular file descriptor. I don't want to break on every call to close(),
since it is called often.

GDB supports conditional breaking, but it wasn't immediately clear to me how I
should:

     Access the function parameters - I'm interested in the first parameter to
     the function (but I do 'not' know the parameter name).

    Construct the conditional expression part of the break command.

Reading the Function Parameters

I found a way to do this, although it is machine specific (intel in this case),
and requires knowledge of the machine stack layout. I found a reasonable
    reference for Intel here:
    http://www.tenouk.com/Bufferoverflowc/Bufferoverflow3.html

You don't need to understand every last detail. It's enough to remember that the
parameters can be found at a 2-word positive offset from $ebp. So in the 32-bit
case, the first parameter can be found at $ebp+8.

Say I break on this function:

void foo(int i, int j) {
    // stuff
}

And I want to inspect the first and second parameters (i & j). I can get the
value of the first parameter like this:

x/d $ebp+8

Similarly, I can get the second parameter like this:

x/d $ebp+12

since sizeof(int) == 4 in this case.

Conditional break

Armed with this knowledge, let's say I want to set a breakpoint on foo, but only
break when the value of the first parameter is 12345. How do I construct the
expression? Well, it turns out that the conditional syntax is just like a C 'if'
statement. So I do:

b foo if (*(int*)($ebp+8)==12345)

And if I want to put a constraint on the value of the second parameter:

b foo if (*(int*)($ebp+8)==12345 && *(int*)($ebp+12)==5678)


={============================================================================
*kt_linux_tool_309* gdb-break: info. get you quickly where

info breakpoints [n...]
info break [n...]

Print a table of all breakpoints, watchpoints, and catchpoints set and not
deleted. Optional argument n means print information only about the specified
breakpoint(s) (or watchpoint(s) or catchpoint(s)). For each breakpoint,
following columns are printed:


note: tip

(gdb) info break 

displays a count of the number of times the breakpoint has been hit. This is
especially useful in conjunction with the `ignore` command. You can ignore a
large number of breakpoint hits, look at the breakpoint info to see how many
times the breakpoint was hit, and then run again, ignoring one less than that
number. This will get you 'quickly' to the last hit of that breakpoint.


note: on cpp

It is possible that a breakpoint corresponds to several locations in your
program. Examples of this situation are:

    Multiple functions in the program may have the same name.

    For a C++ constructor, the gcc compiler generates several instances of the
    function body, used in different cases.

    For a C++ template function, a given line in the function can correspond to
    any number of instantiations.

    For an inlined function, a given source line can correspond to several
    places where that function is inlined. 


In all those cases, gdb will insert a breakpoint at all the relevant locations.

A breakpoint with multiple 'locations' is displayed in the breakpoint table
using several rows: 

one header row, followed by one row for each breakpoint location.  The header
row has ‘<MULTIPLE>’ in the address column. The rows for individual locations
contain the actual addresses for locations, and show the functions to which
those locations belong. The number column for a location is of the form
breakpoint-number.'location'-number.

For example:

     Num     Type           Disp Enb  Address    What
     1       breakpoint     keep y    <MULTIPLE>
             stop only if i==1
             breakpoint already hit 1 time
     1.1                         y    0x080486a2 in void foo<int>() at t.cc:8
     1.2                         y    0x080486ca in void foo<double>() at t.cc:8

Each location can be individually enabled or disabled by passing
breakpoint-number.location-number as argument to the enable and disable
commands. 

Note that you 'cannot' delete the individual locations from the list, you can
only delete the entire list of locations that belong to their parent breakpoint
(with the delete num command, where num is the number of the parent breakpoint,
 1 in the above example). Disabling or enabling the parent breakpoint (see
     Disabling) affects all of the locations that belong to that breakpoint. 


={============================================================================
*kt_linux_tool_309* gdb-break: ignore

5.1.6 Break Conditions

A special case of a breakpoint condition is to stop 'only' when the breakpoint
has been reached a certain number of times. This is so useful that there is a
special way to do it, using the ignore count of the breakpoint. 

Every breakpoint has an ignore count, which is an integer. Most of the time, the
ignore count is zero, and therefore has no effect. But if your program reaches a
breakpoint whose ignore count is positive, then instead of stopping, it just
decrements the ignore count by one and continues. As a result, if the ignore
count value is n, the breakpoint does not stop the next n times your program
reaches it.

ignore bnum count

Set the ignore count of breakpoint number bnum to count. The next count times
the breakpoint is reached, your program's execution does not stop; other than to
decrement the ignore count, gdb takes no action. 

To make the breakpoint stop the next time it is reached, specify a count of
zero. When you use `continue` to resume execution of your program from a
breakpoint, you can specify an ignore count directly as an argument to continue,
rather than using ignore. See Section 5.2 [Continuing and Stepping], page 68.

If a breakpoint has a positive ignore count and a condition, the condition is
not checked. Once the ignore count reaches zero, gdb resumes checking the
condition. You could achieve the effect of the ignore count with a condition
such as ‘$foo-- <= 0’ using a debugger convenience variable that is decremented
each time. See Section 10.11 [Convenience Variables], page 132.

Ignore counts apply to breakpoints, watchpoints, and catchpoints.


={============================================================================
*kt_linux_tool_309* gdb-break: set in shared

5.1.1 Setting Breakpoints

<pending-breakpoint>

Shared libraries can be loaded and unloaded explicitly, and possibly repeatedly,
as the program is executed.  To support this use case, gdb updates breakpoint
  locations 'whenever' any shared library is loaded or unloaded.  Typically, you
  would set a breakpoint in a shared library at the 'beginning' of your
  debugging session, when the library is not loaded, and when the symbols from
  the library are not available. When you try to set breakpoint, gdb will ask
  you if you want to set a so called 'pending' breakpoint-breakpoint whose
  address is not yet resolved.  After the program is run, whenever a new shared
  library is loaded, gdb reevaluates all the breakpoints. When a newly loaded
  shared library contains the symbol or line referred to by some pending
  breakpoint, that breakpoint is resolved and becomes an ordinary breakpoint.
  When a library is unloaded, all breakpoints that refer to its symbols or
  source lines become pending again.

This logic works for breakpoints with multiple locations, too. For example, if
you have a breakpoint in a C++ template function, and a newly loaded shared
library has an instantiation of that template, a new location is added to the
list of locations for the breakpoint.

Except for having unresolved address, pending breakpoints do not differ from
regular breakpoints.  You can set conditions or commands, enable and disable
them and perform other breakpoint operations.

set breakpoint pending auto

This is the 'default' behavior. When gdb cannot find the breakpoint location, it
queries you whether a pending breakpoint should be created.

set breakpoint pending on

This indicates that an unrecognized breakpoint location should automatically
result in a pending
breakpoint being created.


={============================================================================
*kt_linux_tool_309* gdb-break: set watch

5.1.2 Setting Watchpoints

You can use a watchpoint to stop execution whenever the 'value' of an
'expression' changes, 'without' having to predict a particular place where this
may happen. (This is sometimes called a data breakpoint.) The expression may be
as simple as the value of a single variable, or as complex as many variables
combined by operators. Examples include:

    A reference to the value of a single variable.

    An address cast to an appropriate data type. For example, ‘*(int
        *)0x12345678’ will watch a 4-byte region at the specified address
    (assuming an int occupies 4 bytes).

    An arbitrarily complex expression, such as ‘a*b + c/d’. The expression can
    use any operators valid in the program's native language (see Languages). 

You can set a watchpoint on an expression even if the expression can not be
evaluated 'yet'. For instance, you can set a watchpoint on ‘*global_ptr’ before
‘global_ptr’ is initialized. gdb will stop when your program sets ‘global_ptr’
and the expression produces a valid value. If the expression becomes valid in
some other way than changing a variable (e.g. if the memory pointed to by
    ‘*global_ptr’ becomes readable as the result of a malloc call), gdb may not
stop until the next time the expression changes.

Depending on your system, watchpoints may be implemented in software or
hardware. gdb does 'software' watchpointing by single-stepping your program and
testing the variable's value each time, which is hundreds of times slower than
normal execution. (But this may still be worth it, to catch errors where you
    have no clue what part of your program is the culprit.)

watch [-l|-location] expr [thread threadnum] [mask maskvalue]

    Set a watchpoint for an expression. gdb will break when the expression expr
    is written into by the program and its value 'changes'. The simplest (and
        the most popular) use of this command is to watch the value of a
    'single' variable:

              (gdb) watch foo

    If the command includes a [thread threadnum] argument, gdb breaks only when
    the thread identified by threadnum changes the value of expr. If any other
    threads change the value of expr, gdb will not break. Note that watchpoints
    restricted to a single thread in this way only work with Hardware
    Watchpoints.

    Ordinarily a watchpoint respects the scope of variables in expr (see below).
    The -location argument tells gdb to instead watch the memory referred to by
    expr. In this case, gdb will evaluate expr, take the address of the result,
and watch the memory at that address. The type of the result is used to
  determine the size of the watched memory. If the expression's result does not
  have an address, then gdb will print an error.

    The [mask maskvalue] argument allows creation of 'masked' watchpoints, if the
    current architecture supports this feature (e.g., PowerPC Embedded
        architecture, see PowerPC Embedded.) A masked watchpoint specifies a
    mask in addition to an address to watch. The mask specifies that some 'bits'
    of an address (the bits which are reset in the mask) should be ignored when
    matching the address accessed by the inferior against the watchpoint
    address. Thus, a masked watchpoint watches many addresses
    simultaneously—those addresses whose unmasked bits are identical to the
    unmasked bits in the watchpoint address. The mask argument implies
    -location. Examples:

              (gdb) watch foo mask 0xffff00ff
              (gdb) watch *0xdeadbeef mask 0xffffff00


rwatch [-l|-location] expr [thread threadnum] [mask maskvalue]
    Set a watchpoint that will break when the value of expr is read by the
    program.


awatch [-l|-location] expr [thread threadnum] [mask maskvalue]
    Set a watchpoint that will break when expr is either read from or written
    into by the program.


info watchpoints [n...]
    This command prints a list of watchpoints, using the same format as info
    break (see Set Breaks). 

If you watch for a change in a numerically entered address you need to
'dereference' it, as the address itself is just a constant number which will
never change. gdb refuses to create a watchpoint that watches a never-changing
value:

     (gdb) watch 0x600850
     Cannot watch constant value 0x600850.
     (gdb) watch *(int *) 0x600850
     Watchpoint 1: *(int *) 6293584

gdb sets a hardware watchpoint if 'possible'. Hardware watchpoints execute very
quickly, and the debugger reports a change in value at the exact instruction
where the change occurs. If gdb cannot set a hardware watchpoint, it sets a
software watchpoint, which executes more slowly and reports the change in value
at the next statement, not the instruction, after the change occurs.

Currently, the awatch and rwatch commands can only set hardware watchpoints,
because accesses to data that don't change the value of the watched expression
  cannot be detected without examining every instruction as it is being
  executed, and gdb does not do that currently. If gdb finds that it is unable
  to set a hardware breakpoint with the awatch or rwatch command, it will print
  a message like this:

     Expression cannot be implemented with read/access watchpoint.

Sometimes, gdb cannot set a hardware watchpoint because the data type of the
watched expression is wider than what a hardware watchpoint on the target
machine can handle. For example, some systems can only watch regions that are up
to 4 bytes wide; on such systems you cannot set hardware watchpoints for an
expression that yields a double-precision floating-point number (which is
    typically 8 bytes wide). As a work-around, it might be possible to break the
large region into a series of smaller ones and watch them with separate
watchpoints.

If you set too many hardware watchpoints, gdb might be unable to insert all of
them when you resume the execution of your program. Since the precise number of
active watchpoints is unknown until such time as the program is about to be
resumed, gdb might not be able to warn you about this when you set the
watchpoints, and the warning will be printed only when the program is resumed:

     Hardware watchpoint num: Could not insert watchpoint

If this happens, delete or disable some of the watchpoints.

Watching complex expressions that reference many variables can also exhaust the
resources available for hardware-assisted watchpoints. That's because gdb needs
to watch every variable in the expression with separately allocated resources.

If you call a function interactively using print or call, any watchpoints you
have set will be inactive until gdb reaches another kind of breakpoint or the
call completes.

gdb automatically deletes watchpoints that watch local (automatic) variables, or
expressions that involve such variables, when they go out of scope, that is,
when the execution leaves the block in which these variables were defined. In
  particular, when the program being debugged terminates, all local variables go
  out of scope, and so only watchpoints that watch global variables remain set.
  If you rerun the program, you will need to set all such watchpoints again. One
  way of doing that would be to set a code breakpoint at the entry to the main
  function and when it breaks, set all the watchpoints.

In multi-threaded programs, watchpoints will detect changes to the watched
expression 'from' every thread.

note:

Warning: In multi-threaded programs, software watchpoints have only limited
usefulness. If gdb creates a software watchpoint, it can only watch the value of
an expression in a single thread. If you are confident that the expression can
only change due to the current thread's activity (and if you are also confident
    that no other thread can become current), then you can use software
watchpoints as usual. However, gdb may not notice when a non-current thread's
activity changes the expression. Hardware watchpoints, in contrast, watch an
expression in all threads. 


={============================================================================
*kt_linux_tool_309* gdb-break: set catch

5.1.3 Setting Catchpoints

You can use catchpoints to cause the debugger to stop for certain kinds of
program events, such as C++ 'exceptions' or the loading of a shared library. Use
the catch command to set a catchpoint.

catch event
    Stop when event occurs. The event can be any of the following:

    throw [regexp]
    rethrow [regexp]
    catch [regexp]
        The throwing, re-throwing, or catching of a C++ exception.

        If regexp is given, then only exceptions whose type matches the regular
        expression will be caught.

        The convenience variable $_exception is available at an
        exception-related catchpoint, on some systems. This holds the exception
        being thrown.

        There are currently some limitations to C++ exception handling in gdb:

            note:
            The support for these commands is system-dependent. Currently, only
            systems using the ‘gnu-v3’ C++ ABI (see ABI) are supported.

            The regular expression feature and the $_exception convenience
            variable rely on the presence of some SDT probes in libstdc++. If
            these probes are not present, then these features cannot be used.
            These probes were first available in the GCC 4.8 release, but
            whether or not they are available in your GCC also depends on how it
            was built.
            
            The $_exception convenience variable is only valid at the
            instruction at which an exception-related catchpoint is set.

            When an exception-related catchpoint is hit, gdb stops at a location
            in the system library which implements runtime exception support for
            C++, usually libstdc++. You can use up (see Selection) to get to
            your code.

            If you call a function interactively, gdb normally returns control
            to you when the function has finished executing. If the call raises
            an exception, however, the call may bypass the mechanism that
            returns control to you and cause your program either to abort or to
            simply continue running until it hits a breakpoint, catches a signal
            that gdb is listening for, or exits. This is the case even if you
            set a catchpoint for the exception; catchpoints on exceptions are
            disabled within interactive calls. See Calling, for information on
            controlling this with set unwind-on-terminating-exception.

            You cannot raise an exception interactively.

            You cannot install an exception handler interactively. 

    exec
        A call to exec. This is currently only available for HP-UX and gnu/Linux.

    syscall
    syscall [name | number] ...
        A call to or return from a system call, a.k.a. syscall. A syscall is a
        mechanism for application programs to request a service from the
        operating system (OS) or one of the OS system services. gdb can catch
        some or all of the syscalls issued by the debuggee, and show the related
        information for each syscall. If no argument is specified, calls to and
        returns from all system calls will be caught.

        name can be any system call name that is valid for the underlying OS.
        you can use the gdb command-line completion facilities to list the
        available choices.

        You may also specify the system call numerically. A syscall's number is
        the value passed to the OS's syscall dispatcher to identify the
        requested service. When you specify the syscall by its name, gdb uses
        its database of syscalls to convert the name into the corresponding
        numeric code, but using the number directly may be useful if gdb's
        database does not have the complete list of syscalls on your system
        (e.g., because gdb lags behind the OS upgrades).

        The example below illustrates how this command works if you don't
        provide arguments to it:

                       (gdb) catch syscall
                       Catchpoint 1 (syscall)
                       (gdb) r
                       Starting program: /tmp/catch-syscall
                       
                       Catchpoint 1 (call to syscall 'close'), \
                       	   0xffffe424 in __kernel_vsyscall ()
                       (gdb) c
                       Continuing.
                       
                       Catchpoint 1 (returned from syscall 'close'), \
                       	0xffffe424 in __kernel_vsyscall ()
                       (gdb)

        Here is an example of catching a system call by name:

                       (gdb) catch syscall chroot
                       Catchpoint 1 (syscall 'chroot' [61])
                       (gdb) r
                       Starting program: /tmp/catch-syscall
                       
                       Catchpoint 1 (call to syscall 'chroot'), \
                       		   0xffffe424 in __kernel_vsyscall ()
                       (gdb) c
                       Continuing.
                       
                       Catchpoint 1 (returned from syscall 'chroot'), \
                       	0xffffe424 in __kernel_vsyscall ()
                       (gdb)

        An example of specifying a system call numerically. In the case below,
        the syscall number has a corresponding entry in the XML file, so gdb
          finds its name and prints it:

                       (gdb) catch syscall 252
                       Catchpoint 1 (syscall(s) 'exit_group')
                       (gdb) r
                       Starting program: /tmp/catch-syscall
                       
                       Catchpoint 1 (call to syscall 'exit_group'), \
                       		   0xffffe424 in __kernel_vsyscall ()
                       (gdb) c
                       Continuing.
                       
                       Program exited normally.
                       (gdb)

        // skipped

    fork
        A call to fork. This is currently only available for HP-UX and gnu/Linux.

    vfork
        A call to vfork. This is currently only available for HP-UX and gnu/Linux.

    load [regexp]
    unload [regexp]
        The loading or unloading of a shared library. If regexp is given, then
        the catchpoint will stop only if the regular expression matches one of
        the affected libraries.

    signal [signal... | ‘all’]
        The delivery of a signal.

        With no arguments, this catchpoint will catch any signal that is not
        used internally by gdb, specifically, all signals except ‘SIGTRAP’ and
        ‘SIGINT’.

        With the argument ‘all’, all signals, including those used by gdb, will
        be caught. This argument cannot be used with other signal names.

        Otherwise, the arguments are a list of signal names as given to handle
        (see Signals). Only signals specified in this list will be caught.

        One reason that catch signal can be more useful than handle is that you
        can attach commands and conditions to the catchpoint.

        When a signal is caught by a catchpoint, the signal's stop and print
        settings, as specified by handle, are ignored. However, whether the
        signal is still delivered to the inferior depends on the pass setting;
        this can be changed in the catchpoint's commands. 


tcatch event
    Set a catchpoint that is enabled only for 'one' stop. The catchpoint is
    automatically deleted after the first time the event is caught. 

Use the info break command to list the current catchpoints. 


={============================================================================
*kt_linux_tool_300* gdb-break: unset

5.1.4 Deleting Breakpoints

It is often necessary to eliminate a breakpoint, watchpoint, or catchpoint once
it has done its job and you no longer want your program to stop there. This is
called deleting the breakpoint. A breakpoint that has been deleted no longer
exists; it is forgotten.

With the clear command you can delete breakpoints according to where they are in
your program. With the delete command you can delete individual breakpoints,
watchpoints, or catchpoints by specifying their breakpoint numbers.

clear
    Delete any breakpoints at the next instruction to be executed in the
    selected stack frame (see Selecting a Frame). When the innermost frame is
    selected, this is a good way to delete a breakpoint where your program just
    stopped.  clear location Delete any breakpoints set at the specified
    location. See Specify Location, for the various forms of location; the most
    useful ones are listed below:

    clear function
    clear filename:function
        Delete any breakpoints set at entry to the named function.
    clear linenum
    clear filename:linenum
        Delete any breakpoints set at or within the code of the specified
        linenum of the specified filename. 


delete [breakpoints] [range...]
    Delete the breakpoints, watchpoints, or catchpoints of the breakpoint ranges
    specified as arguments. If 'no' argument is specified, delete 'all'
    breakpoints (gdb asks confirmation, unless you have set confirm off). You
    can abbreviate this command as d. 


={============================================================================
*kt_linux_tool_310* gdb-break: disable

5.1.5 Disabling breakpoints

This makes the breakpoint inoperative as if it had been deleted, but remembers
the information on the breakpoint so that you can enable it again later. 

A breakpoint, watchpoint, or catchpoint can have any of 'four' different
'states' of enablement:


    Enabled. The breakpoint stops your program. A breakpoint set with the break
    command starts out in this state.

    Disabled. The breakpoint has no effect on your program.

    Enabled 'once'. The breakpoint stops your program, but then becomes disabled.

    Enabled for a 'count'. The breakpoint stops your program for the next N
    times, then becomes disabled.

    Enabled for 'deletion'. The breakpoint stops your program, but immediately
    after it does so it is deleted permanently. A breakpoint set with the tbreak
    command starts out in this state. 


You can use the following commands to enable or disable breakpoints,
watchpoints, and catchpoints:

disable [breakpoints] [range...]
    Disable the specified breakpoints—or all breakpoints, if none are listed. A
    disabled breakpoint has no effect but is not forgotten. All options such as
    ignore-counts, conditions and commands are remembered in case the breakpoint
    is enabled again later. You may abbreviate disable as dis.


enable [breakpoints] [range...]
    Enable the specified breakpoints (or all defined breakpoints). They become
    effective once again in stopping your program.

enable [breakpoints] once range...
    Enable the specified breakpoints temporarily. gdb disables any of these
    breakpoints immediately after stopping your program.

enable [breakpoints] count count range...
    Enable the specified breakpoints temporarily. gdb records count with each of
    the specified breakpoints, and decrements a breakpoint's count when it is
    hit. When any count reaches 0, gdb disables that breakpoint. If a breakpoint
    has an ignore count (see Break Conditions), that will be decremented to 0
    before count is affected.

enable [breakpoints] delete range...
    Enable the specified breakpoints to work once, then die. gdb deletes any of
    these breakpoints as soon as your program stops there. Breakpoints set by
    the tbreak command start out in this state. 

Except for a breakpoint set with tbreak, breakpoints that you set are initially
enabled; subsequently, they become disabled or enabled only when you use one of
the commands above. (The command until can set and delete a breakpoint of its
    own, but it does not change the state of your other breakpoints; see
    Continuing and Stepping.) 


={============================================================================
*kt_linux_tool_310* gdb-break: run command on break

5.1.7 Breakpoint Command Lists

You can give any breakpoint (or watchpoint or catchpoint) a series of commands
to 'execute' when your program stops due to that breakpoint. For example, you
might want to print the values of certain expressions, or enable other
breakpoints.

For example, here is how you could use breakpoint commands to print the value of
x at entry to foo whenever x is positive.

  break foo if x>0
  commands
  silent
  printf "x is %d\n",x
  cont
  end

One application for breakpoint commands is to compensate for one bug so you can
test for another. Put a breakpoint just after the erroneous line of code, give
it a condition to detect the case in which something erroneous has been done,
and give it commands to assign correct values to any variables that need them.

End with the continue command so that your program does not stop, and start with
the silent command so that no output is produced. Here is an example:

  break 403
  commands
  silent
  set x = y + 4           // note <gdb-set>
  cont
  end


={============================================================================
*kt_linux_tool_310* gdb-break: save

5.1.9 How to save breakpoints to a file

To save breakpoint definitions to a file use the save breakpoints command. 

save breakpoints [filename]


={============================================================================
*kt_linux_tool_300* gdb-break: dprintf

The dynamic printf command dprintf 'combines' a breakpoint with formatted
printing of your program's data to give you the effect of inserting printf calls
into your program on-the-fly, 'without' having to recompile it.

<to-direct-to-programs-output>
In its most basic form, the output goes to the GDB console. However, you can set
the variable dprintf-style for alternate handling. For instance, you can ask to
format the output by calling 'your' program's printf function. This has the
advantage that the characters go to the program's 'output' device, so they can
recorded in redirects to files and so forth.

As an example, if you wanted dprintf output to go to a logfile that is a
standard I/O stream assigned to the variable mylog, you could do the following:

(gdb) set dprintf-style call
(gdb) set dprintf-function fprintf
(gdb) set dprintf-channel mylog
(gdb) dprintf 25,"at line 25, glob=%d\n",glob
Dprintf 1 at 0x123456: file main.c, line 25.
(gdb) info break
1 dprintf keep y 0x00123456 in main at main.c:25
call (void) fprintf (mylog,"at line 25, glob=%d\n",glob)
continue
(gdb)

note that the info break displays the dynamic printf commands as normal
breakpoint commands; you can thus easily see the effect of the variable
settings.

dprintf location,template,expression[,expression...]

Whenever execution reaches location, print the values of one or more expressions
under the control of the string template. To print several values, separate them
with commas.

set dprintf-style style

Set the dprintf output to be handled in one of several different styles
enumerated below. A change of style affects all existing dynamic printfs
immediately. (If you need individual control over the print commands, simply
    define normal breakpoints with explicitly-supplied command lists.) gdb
Handle the output using the gdb printf command.

'call' Handle the output by calling a function in your program (normally
    printf).  agent Have the remote debugging agent (such as gdbserver) handle
the output itself.  This style is only available for agents that support running
commands on the target.

set dprintf-function function

Set the function to call if the dprintf style is call. By default its value is
printf. You may set it to any expression. that gdb can evaluate to a function,
as per the call command.

set dprintf-channel channel

Set a channel for dprintf. If set to a non-empty value, gdb will evaluate it as
an expression and pass the result as a <first-argument> to the dprintf-function,
in the manner of fprintf and similar functions. Otherwise, the dprintf format
  string will be the first argument, in the manner of printf.


={============================================================================
*kt_linux_tool_400* gdb-stl-step-into-code

note:
This seems wrong since able to step into stl code without building gcc. Why
thought that needs to build gcc in the first place?

// To be able to step into stl code, need to have the debug version of C++ library
// and this is libstdc++6 in Debian Wheezy case.
// 
// https://packages.debian.org/wheezy/libstdc++6
// 
// (on pc vm, gcc-4.7)
// $ ldd a.out 
// 	linux-gate.so.1 =>  (0xb771f000)
// 	libstdc++.so.6 => /usr/lib/i386-linux-gnu/libstdc++.so.6 (0xb761a000)
// 	libm.so.6 => /lib/i386-linux-gnu/i686/cmov/libm.so.6 (0xb75f4000)
// 	libgcc_s.so.1 => /lib/i386-linux-gnu/libgcc_s.so.1 (0xb75d6000)
// 	libc.so.6 => /lib/i386-linux-gnu/i686/cmov/libc.so.6 (0xb7471000)
// 	/lib/ld-linux.so.2 (0xb7720000)
// 
// (on pc Debian Jessie, gcc-4.9)
// 09:26:35 ~/work$ ldd a.out 
// 	linux-vdso.so.1 (0x00007fff73fd5000)
// 	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fcba2103000)
// 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fcba1e02000)
// 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fcba1bec000)
// 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fcba1841000)
// 	/lib64/ld-linux-x86-64.so.2 (0x00007fcba240e000)
// 
// $ readelf --debug-dump=decodedline /usr/lib/i386-linux-gnu/libstdc++.so.6
// $ 
// 
// This shows that the release version.

a.out.rel : g++ -g -std=c++0x $1
a.out     : g++ -g -O0 -std=c++0x -D_GLIBCXX_DEBUG $1

$ ll a.out*
-rwxr-xr-x 1 keitee keitee  61920 Mar  7 23:27 a.out*
-rwxr-xr-x 1 keitee keitee  41069 Mar  7 23:22 a.out.rel*

$ ldd a.out
	linux-gate.so.1 =>  (0xb775d000)
	libstdc++.so.6 => /usr/lib/i386-linux-gnu/libstdc++.so.6 (0xb7658000)
	libm.so.6 => /lib/i386-linux-gnu/i686/cmov/libm.so.6 (0xb7632000)
	libgcc_s.so.1 => /lib/i386-linux-gnu/libgcc_s.so.1 (0xb7614000)
	libc.so.6 => /lib/i386-linux-gnu/i686/cmov/libc.so.6 (0xb74af000)
	/lib/ld-linux.so.2 (0xb775e000)

$ ldd a.out.rel 
	linux-gate.so.1 =>  (0xb7701000)
	libstdc++.so.6 => /usr/lib/i386-linux-gnu/libstdc++.so.6 (0xb75fc000)
	libm.so.6 => /lib/i386-linux-gnu/i686/cmov/libm.so.6 (0xb75d6000)
	libgcc_s.so.1 => /lib/i386-linux-gnu/libgcc_s.so.1 (0xb75b8000)
	libc.so.6 => /lib/i386-linux-gnu/i686/cmov/libc.so.6 (0xb7453000)
	/lib/ld-linux.so.2 (0xb7702000)

$ readelf --debug-dump=decodedline /usr/lib/i386-linux-gnu/libstdc++.so.6
$ 

Interestingly, the both case shows the same when do info sharedlibrary in gdb

(gdb) i sharedlibrary 
From                To                  Syms Read   Shared Object Library
0x00007ffff7b2c8b0  0x00007ffff7b94b1f  Yes (*)     /usr/lib/x86_64-linux-gnu/libstdc++.so.6
(*): Shared library is 'missing' debugging information.


={============================================================================
*kt_linux_tool_400* gdb-stl-debug-mode

https://gcc.gnu.org/onlinedocs/libstdc++/index.html
http://gcc.gnu.org/onlinedocs/libstdc++/manual/debug_mode_using.html#debug_mode.using.mode

Chapter 17. Debug Mode

Using

Using the Debug Mode

To use the libstdc++ debug mode, compile your application with the compiler flag
-D_GLIBCXX_DEBUG. Note that this flag changes the sizes and behavior of standard
class templates such as std::vector, and therefore you can only link code
compiled with debug mode and code compiled without debug mode if no
instantiation of a container is passed between the two translation units.

By default, error messages are formatted to fit on lines of about 78 characters.
The environment variable GLIBCXX_DEBUG_MESSAGE_LENGTH can be used to request a
different length.  

Using a Specific Debug Container

When it is not feasible to recompile your entire application, or only specific
containers need checking, debugging containers are available as GNU extensions.
These debugging containers are functionally equivalent to the standard drop-in
containers used in debug mode, but they are available in a separate namespace as
GNU extensions and may be used in programs compiled with either release mode or
with debug mode. The following table provides the names and headers of the
debugging containers:

Table 17.1. Debugging Containers

Container           Header    Debug container       Debug header
std::bitset         bitset    __gnu_debug::bitset   <debug/bitset>
std::deque          deque     __gnu_debug::deque    <debug/deque>
std::list           list      __gnu_debug::list     <debug/list>
std::map            map       __gnu_debug::map      <debug/map>
std::multimap       map       __gnu_debug::multimap <debug/map>
std::multiset       set       __gnu_debug::multiset <debug/set>
std::set            set       __gnu_debug::set      <debug/set>
...

In addition, when compiling in C++11 mode, these additional containers have
additional debug capability.

Table 17.2. Debugging Containers C++11
...


{debug-version}
/toolchain/mipsel-linux-uclibc/include/c++/4.4.5/debug$ ll
-rw-r--r--  1 root root 10042 Nov 15  2010 bitset
-rw-r--r--  1 root root  5250 Nov 15  2010 debug.h
-rw-r--r--  1 root root 12814 Nov 15  2010 deque
-rw-r--r--  1 root root 11235 Nov 15  2010 formatter.h
-rw-r--r--  1 root root 13046 Nov 15  2010 functions.h
-rw-r--r--  1 root root 16527 Nov 15  2010 list
-rw-r--r--  1 root root 10924 Nov 15  2010 macros.h
-rw-r--r--  1 root root  1328 Nov 15  2010 map
-rw-r--r--  1 root root 12014 Nov 15  2010 map.h
-rw-r--r--  1 root root 11874 Nov 15  2010 multimap.h
-rw-r--r--  1 root root 11665 Nov 15  2010 multiset.h
-rw-r--r--  1 root root  7900 Nov 15  2010 safe_base.h
-rw-r--r--  1 root root 21599 Nov 15  2010 safe_iterator.h
-rw-r--r--  1 root root  4412 Nov 15  2010 safe_iterator.tcc
-rw-r--r--  1 root root  6206 Nov 15  2010 safe_sequence.h
-rw-r--r--  1 root root  1322 Nov 15  2010 set
-rw-r--r--  1 root root 11592 Nov 15  2010 set.h
-rw-r--r--  1 root root 30299 Nov 15  2010 string
-rw-r--r--  1 root root 17180 Nov 15  2010 unordered_map
-rw-r--r--  1 root root 17013 Nov 15  2010 unordered_set
-rw-r--r--  1 root root 15055 Nov 15  2010 vector


{debug-release-mix}
Mixing debug and release code is bad practice. The problem is that the different
versions can depend on different fundamental parts of the C++ runtime library,
         such as how memory is allocated, structures for things like iterators
         might be different, extra code could be generated to perform operations
         (e.g. checked iterators).

It's the same as mixing library files built with any other different settings.
Imagine a case where a header file contains a structure that is used by both
application and library. The library is built with structure packing and
alignment set to one value and the application built with another. There are no
guarantees that passing the structure from the application into the library will
work since they could vary in size and member positions.

Is it possible to build your 3rd party libraries as DLLs? Assuming the interface
to any functions is cleaner and does not try to pass any STL objects you will be
able to mix a debug application with release DLLs without problems.


Q:
When this parameter is defined I want all my std::vectors to check their
boundarys when accesing an element, the way when using vector::at().

When the parameter is omitted I want all vectors to behave as if the normal []
operator is used, meaning no performance is "wasted" for boundary checking.

You can activate runtime iterator and bounds checking by compiling with
-D_GLIBCXX_DEBUG. Also note that random-access containers provide the always
bounds-checking at() operation in addition to operator [].


<safe-stl>
The STL is not required to protect you from yourself and the STL is as error
prone as pointers are in C. Thus, it is a good idea to use a "safe" STL, at
least during software development.

Cay S. Horstmann. Safe STL
http://www.horstmann.com/safestl.html

STLport
http://www.stlport.org/


={============================================================================
*kt_linux_tool_400* gdb-stl-pretty-printer-gdb-stl-views

https://sourceware.org/gdb/wiki/STLSupport

When you try to use GDB's "print" command to display the contents of a vector, a
stack, or any other GDB abstract data structure, you will get useless results.
Instead, download and install one of following tools to properly view the
contents of STL containers from within GDB. 

<ex>
GNU gdb (Debian 7.7.1+dfsg-5) 7.7.1

// Use -D_GLIBCXX_DEBUG

(gdb) p coll
$1 = std::__debug::vector of length 6, capacity 8 = {1, 2, 3, 4, 5, 6}

// when install gdb-stl-views

(gdb) pvector coll
elem[0]: $2 = 1
elem[1]: $3 = 2
elem[2]: $4 = 3
elem[3]: $5 = 4
elem[4]: $6 = 5
elem[5]: $7 = 6
Vector size = 6
Vector capacity = 8
Element type = std::__cxx1998::_Vector_base<int, std::allocator<int> >::pointer
(gdb) 


// Not use -D_GLIBCXX_DEBUG

(gdb) p coll
$1 = std::vector of length 6, capacity 8 = {1, 2, 3, 4, 5, 6}

(gdb) pvector coll
elem[0]: $2 = 1
elem[1]: $3 = 2
elem[2]: $4 = 3
elem[3]: $5 = 4
elem[4]: $6 = 5
elem[5]: $7 = 6
Vector size = 6
Vector capacity = 8
Element type = std::_Vector_base<int, std::allocator<int> >::pointer
(gdb) 

note: WHY p coll works without configuring any? Maybe:

GDB 7.0 will include support for writing pretty-printers in Python. This
feature, combined with the pretty-printers in the libstdc++ svn repository,
    yields the best way to visualize C++ containers. Some distros (Fedora 11+)
        ship all this code in a way that requires no configuration; in other
        cases, this email message explains how to set everything up. The main
        points have been redacted here: 

// gdb-man
10.9.1 Pretty-Printer Introduction

When gdb prints a value, it first sees if there is a pretty-printer registered
for the value. If there is then gdb invokes the pretty-printer to print the
    value. Otherwise the value is printed normally.

Pretty-printers are normally named. This makes them easy to manage. The 

info pretty-printer~

will list all the installed pretty-printers with their names. If a
pretty-printer can handle multiple data types, then its subprinters are the
printers for the individual data types. Each such subprinter has its own name.
The format of the name is printer-name;subprinter-name.

Pretty-printers are installed by registering them with gdb. Typically they are
automatically loaded and registered when the corresponding debug information is
loaded, thus making them available without having to do anything special.

There are three places where a pretty-printer can be registered.

    Pretty-printers registered globally are available when debugging all
    inferiors.

    Pretty-printers registered with a program space are available only when
    debugging that program. See Progspaces In Python, for more details on
    program spaces in Python.

    Pretty-printers registered with an objfile are loaded and unloaded with the
    corresponding objfile (e.g., shared library). See Objfiles In Python, for
    more details on objfiles in Python. 

<ex>
(gdb) info pretty-printer 
global pretty-printers:
  .*
    bound
  objfile /usr/lib/x86_64-linux-gnu/libstdc++.so.6 pretty-printers:
  libstdc++-v6
    __gnu_cxx::_Slist_iterator
    __gnu_cxx::__7::_Slist_iterator
    __gnu_cxx::__7::__normal_iterator
    __gnu_cxx::__7::slist
    __gnu_cxx::__normal_iterator
    __gnu_cxx::slist
    __gnu_debug::_Safe_iterator
    std::_Deque_const_iterator
    std::_Deque_iterator
    std::_List_const_iterator
    std::_List_iterator
    std::_Rb_tree_const_iterator
    std::_Rb_tree_iterator
    std::__7::_Deque_const_iterator
    std::__7::_Deque_iterator
    std::__7::_List_const_iterator
    std::__7::_List_iterator
    std::__7::_Rb_tree_const_iterator
    std::__7::_Rb_tree_iterator
    std::__7::basic_string
    std::__7::bitset
    std::__7::deque
    std::__7::forward_list
    std::__7::list
    std::__7::map
    std::__7::multimap
 ---Type <return> to continue, or q <return> to quit---


{gdb-stl-views}

<init-file>
#
#   STL GDB evaluators/views/utilities - 1.03
#
#   The new GDB commands:
#       are entirely non instrumental
#       do not depend on any "inline"(s) - e.g. size(), [], etc
#       are extremely tolerant to debugger settings
#
#   This file should be "included" in .gdbinit as following:
#   source stl-views.gdb or just paste it into your .gdbinit file
#
#   The following STL containers are currently supported:
#
#       std::vector<T> -- via pvector command
#       std::list<T> -- via plist or plist_member command
#       std::map<T,T> -- via pmap or pmap_member command
#       std::multimap<T,T> -- via pmap or pmap_member command
#       std::set<T> -- via pset command
#       std::multiset<T> -- via pset command
#       std::deque<T> -- via pdequeue command
#       std::stack<T> -- via pstack command
#       std::queue<T> -- via pqueue command
#       std::priority_queue<T> -- via ppqueue command
#       std::bitset<n> -- via pbitset command
#       std::string -- via pstring command
#       std::widestring -- via pwstring command
#
#   The end of this file contains (optional) C++ beautifiers
#   Make sure your debugger supports $argc
#
#   Simple GDB Macros writen by Dan Marinescu (H-PhD) - License GPL
#   Inspired by intial work of Tom Malnar,
#     Tony Novac (PhD) / Cornell / Stanford,
#     Gilad Mishne (PhD) and Many Many Others.
#   Contact: dan_c_marinescu@yahoo.com (Subject: STL)
#
#   Modified to work with g++ 4.3 by Anders Elton
#   Also added _member functions, that instead of printing the entire class in map, prints a member.

# support for pending breakpoints - you can now set a breakpoint into a shared library before the it was loaded.
set breakpoint pending on

#
# std::vector<>
#

define pvector
    if $argc == 0
        help pvector
    else
        set $size = $arg0._M_impl._M_finish - $arg0._M_impl._M_start
        set $capacity = $arg0._M_impl._M_end_of_storage - $arg0._M_impl._M_start
        set $size_max = $size - 1
    end
    if $argc == 1
        set $i = 0
        while $i < $size
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
    end
    if $argc == 2
        set $idx = $arg1
        if $idx < 0 || $idx > $size_max
            printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
        else
            printf "elem[%u]: ", $idx
            p *($arg0._M_impl._M_start + $idx)
        end
    end
    if $argc == 3
      set $start_idx = $arg1
      set $stop_idx = $arg2
      if $start_idx > $stop_idx
        set $tmp_idx = $start_idx
        set $start_idx = $stop_idx
        set $stop_idx = $tmp_idx
      end
      if $start_idx < 0 || $stop_idx < 0 || $start_idx > $size_max || $stop_idx > $size_max
        printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
      else
        set $i = $start_idx
        while $i <= $stop_idx
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
      end
    end
    if $argc > 0
        printf "Vector size = %u\n", $size
        printf "Vector capacity = %u\n", $capacity
        printf "Element "
        whatis $arg0._M_impl._M_start
    end
end

document pvector
    Prints std::vector<T> information.
    Syntax: pvector <vector> <idx1> <idx2>
    Note: idx, idx1 and idx2 must be in acceptable range [0..<vector>.size()-1].
    Examples:
    pvector v - Prints vector content, size, capacity and T typedef
    pvector v 0 - Prints element[idx] from vector
    pvector v 1 2 - Prints elements in range [idx1..idx2] from vector
end

#
# std::list<>
#

define plist
    if $argc == 0
        help plist
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 2
                printf "elem[%u]: ", $size
                p *($arg1*)($current + 1)
            end
            if $argc == 3
                if $size == $arg2
                    printf "elem[%u]: ", $size
                    p *($arg1*)($current + 1)
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist <variable_name> <element_type> to see the elements in the list.\n"
        end
    end
end

document plist
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist l - prints list size and definition
    plist l int - prints all elements and list size
    plist l int 2 - prints the third element in the list (if exists) and list size
end

define plist_member
    if $argc == 0
        help plist_member
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 3
                printf "elem[%u]: ", $size
                p (*($arg1*)($current + 1)).$arg2
            end
            if $argc == 4
                if $size == $arg3
                    printf "elem[%u]: ", $size
                    p (*($arg1*)($current + 1)).$arg2
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist_member <variable_name> <element_type> <member> to see the elements in the list.\n"
        end
    end
end

document plist_member
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist_member l int member - prints all elements and list size
    plist_member l int member 2 - prints the third element in the list (if exists) and list size
end


#
# std::map and std::multimap
#

define pmap
    if $argc == 0
        help pmap
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 3
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p *($arg1*)$value
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p *($arg2*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 4
            set $idx = $arg3
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p *($arg1*)$value
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p *($arg2*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        if $argc == 5
            set $idx1 = $arg3
            set $idx2 = $arg4
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                set $valueLeft = *($arg1*)$value
                set $valueRight = *($arg2*)($value + sizeof($arg1))
                if $valueLeft == $idx1 && $valueRight == $idx2
                    printf "elem[%u].left: ", $i
                    p $valueLeft
                    printf "elem[%u].right: ", $i
                    p $valueRight
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap m - prints map size and definition
    pmap m int int - prints all elements and map size
    pmap m int int 20 - prints the element(s) with left-value = 20 (if any) and map size
    pmap m int int 20 200 - prints the element(s) with left-value = 20 and right-value = 200 (if any) and map size
end


define pmap_member
    if $argc == 0
        help pmap_member
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 5
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p (*($arg1*)$value).$arg2
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p (*($arg3*)$value).$arg4
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 6
            set $idx = $arg5
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p (*($arg1*)$value).$arg2
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p (*($arg3*)$value).$arg4
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap_member
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap_member m class1 member1 class2 member2 - prints class1.member1 : class2.member2
    pmap_member m class1 member1 class2 member2 lvalue - prints class1.member1 : class2.member2 where class1 == lvalue
end


#
# std::set and std::multiset
#

define pset
    if $argc == 0
        help pset
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Set "
            whatis $tree
            printf "Use pset <variable_name> <element_type> to see the elements in the set.\n"
        end
        if $argc == 2
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u]: ", $i
                p *($arg1*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 3
            set $idx = $arg2
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u]: ", $i
                    p *($arg1*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Set size = %u\n", $tree_size
    end
end

document pset
    Prints std::set<T> or std::multiset<T> information. Works for std::multiset as well.
    Syntax: pset <set> <T> <val>: Prints set size, if T defined all elements or just element(s) having val
    Examples:
    pset s - prints set size and definition
    pset s int - prints all elements and the size of s
    pset s int 20 - prints the element(s) with value = 20 (if any) and the size of s
end



#
# std::dequeue
#

define pdequeue
    if $argc == 0
        help pdequeue
    else
        set $size = 0
        set $start_cur = $arg0._M_impl._M_start._M_cur
        set $start_last = $arg0._M_impl._M_start._M_last
        set $start_stop = $start_last
        while $start_cur != $start_stop
            p *$start_cur
            set $start_cur++
            set $size++
        end
        set $finish_first = $arg0._M_impl._M_finish._M_first
        set $finish_cur = $arg0._M_impl._M_finish._M_cur
        set $finish_last = $arg0._M_impl._M_finish._M_last
        if $finish_cur < $finish_last
            set $finish_stop = $finish_cur
        else
            set $finish_stop = $finish_last
        end
        while $finish_first != $finish_stop
            p *$finish_first
            set $finish_first++
            set $size++
        end
        printf "Dequeue size = %u\n", $size
    end
end

document pdequeue
    Prints std::dequeue<T> information.
    Syntax: pdequeue <dequeue>: Prints dequeue size, if T defined all elements
    Deque elements are listed "left to right" (left-most stands for front and right-most stands for back)
    Example:
    pdequeue d - prints all elements and size of d
end



#
# std::stack
#

define pstack
    if $argc == 0
        help pstack
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = $size - 1
        while $i >= 0
            p *($start_cur + $i)
            set $i--
        end
        printf "Stack size = %u\n", $size
    end
end

document pstack
    Prints std::stack<T> information.
    Syntax: pstack <stack>: Prints all elements and size of the stack
    Stack elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    pstack s - prints all elements and the size of s
end



#
# std::queue
#

define pqueue
    if $argc == 0
        help pqueue
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = 0
        while $i < $size
            p *($start_cur + $i)
            set $i++
        end
        printf "Queue size = %u\n", $size
    end
end

document pqueue
    Prints std::queue<T> information.
    Syntax: pqueue <queue>: Prints all elements and the size of the queue
    Queue elements are listed "top to bottom" (top-most element is the first to come on pop)
    Example:
    pqueue q - prints all elements and the size of q
end



#
# std::priority_queue
#

define ppqueue
    if $argc == 0
        help ppqueue
    else
        set $size = $arg0.c._M_impl._M_finish - $arg0.c._M_impl._M_start
        set $capacity = $arg0.c._M_impl._M_end_of_storage - $arg0.c._M_impl._M_start
        set $i = $size - 1
        while $i >= 0
            p *($arg0.c._M_impl._M_start + $i)
            set $i--
        end
        printf "Priority queue size = %u\n", $size
        printf "Priority queue capacity = %u\n", $capacity
    end
end

document ppqueue
    Prints std::priority_queue<T> information.
    Syntax: ppqueue <priority_queue>: Prints all elements, size and capacity of the priority_queue
    Priority_queue elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    ppqueue pq - prints all elements, size and capacity of pq
end



#
# std::bitset
#

define pbitset
    if $argc == 0
        help pbitset
    else
        p /t $arg0._M_w
    end
end

document pbitset
    Prints std::bitset<n> information.
    Syntax: pbitset <bitset>: Prints all bits in bitset
    Example:
    pbitset b - prints all bits in b
end



#
# std::string
#

define pstring
    if $argc == 0
        help pstring
    else
        printf "String \t\t\t= \"%s\"\n", $arg0._M_data()
        printf "String size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "String capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "String ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pstring
    Prints std::string information.
    Syntax: pstring <string>
    Example:
    pstring s - Prints content, size/length, capacity and ref-count of string s
end

#
# std::wstring
#

define pwstring
    if $argc == 0
        help pwstring
    else
        call printf("WString \t\t= \"%ls\"\n", $arg0._M_data())
        printf "WString size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "WString capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "WString ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pwstring
    Prints std::wstring information.
    Syntax: pwstring <wstring>
    Example:
    pwstring s - Prints content, size/length, capacity and ref-count of wstring s
end

#
# C++ related beautifiers (optional)
#

set print pretty on
set print object on
set print static-members on
set print vtbl on
set print demangle on
set demangle-style gnu-v3
set print sevenbit-strings off

set history filename ~/.gdb_history
set history save

# finally stop the silly "A debugging session is active." - question ... just quit both.
set confirm off


={============================================================================
*kt_linux_tool_400* gdb-stl: asm, registers, and colored 

https://github.com/gdbinit/Gdbinit/blob/master/gdbinit

<ex>
gdb$ next
------------------------------------------------------------------------[regs]
  RAX: 0x0000000000000001  RBX: 0x0000000000000000  RBP: 0x00007FFFFFFFDC10  RSP: 0x00007FFFFFFFDBD0  o d I t s z a p c 
  RDI: 0x00007FFFFFFFDBE0  RSI: 0x0000000000603064  RDX: 0x0000000000000018  RCX: 0x0000000000603064  RIP: 0x0000000000400CFD
  R8 : 0x0000000000000000  R9 : 0x0000000000603010  R10: 0x00007FFFFFFFD890  R11: 0x00007FFFF728B6E0  R12: 0x0000000000400BA0
  R13: 0x00007FFFFFFFDCF0  R14: 0x0000000000000000  R15: 0x0000000000000000
  CS: 0033  DS: 0000  ES: 0000  FS: 0000  GS: 0000  SS: 002B				
------------------------------------------------------------------------[code]
=> 0x400cfd <main()+103>:	mov    eax,DWORD PTR [rbp-0x14]
   0x400d00 <main()+106>:	movsxd rdx,eax
   0x400d03 <main()+109>:	lea    rax,[rbp-0x30]
   0x400d07 <main()+113>:	mov    rsi,rdx
   0x400d0a <main()+116>:	mov    rdi,rax
   0x400d0d <main()+119>:	call   0x400f0e <std::vector<int, std::allocator<int> >::operator[](unsigned long)>
   0x400d12 <main()+124>:	mov    eax,DWORD PTR [rax]
   0x400d14 <main()+126>:	mov    esi,eax
 ----------------------------------------------------------------------------
17	        cout << coll[i] << ':';
gdb$


# ============================================================================
#{
={============================================================================
|kt_linux_perf_001| perf-bootchart

http://www.bootchart.org/

Bootchart 2.0

I couldn't get bootchart2 to work, (no netlink taskstat interface).  However,
bootchart-0.9 works with only small tweaks. (See attached tarball.)


={============================================================================
|kt_linux_perf_001| perf-timeline

Using Frederico's Timeline Tools

These tools can be used to dig into the reasons for the time taken.

The tools used are strace and frederico's timeline tools from
http://people.gnome.org/~federico/news-2006-03.html#timeline-tools

Download requirements:

sudo yum install python-cairo
git clone git://gitorious.org/performance-scripts/mainline.git performance-scripts


# ============================================================================
#{
={============================================================================
*kt_linux_rmdb_001* sql-rollback

<how-rollback-works>
https://www.sqlite.org/atomiccommit.html

An important feature of `transactional databases` like SQLite is `atomic commit`

SQLite has the important property that transactions appear to be atomic even
if the transaction is interrupted by an operating system crash or power
failure.

This article describes the techniques used by SQLite to create the illusion of
atomic commit.

The information in this article applies only when SQLite is operating in
"rollback mode", or in other words when SQLite is not using a write-ahead log.


2. Hardware Assumptions

SQLite assumes that the operating system will buffer writes and that a write
request will return before data has actually been stored in the mass storage
device. SQLite further assumes that write operations will be reordered by the
operating system. For this reason, SQLite does a "flush" or "fsync" operation
at key points. SQLite assumes that the flush or fsync will not return until
all pending write operations for the file that is being flushed have
completed. We are told that the flush and fsync primitives are broken on some
versions of Windows and Linux. This is unfortunate. It opens SQLite up to the
possibility of database corruption following a power loss in the middle of a
commit. However, there is nothing that SQLite can do to test for or remedy the
situation. SQLite assumes that the operating system that it is running on
works as advertised. If that is not quite the case, well then hopefully you
will not lose power too often.


3. Single File Commit

We begin with an overview of the steps SQLite takes in order to perform an
atomic commit of a transaction against a single database file.

note: see these steps in time order.
note: make a journal file

3.5. Creating A Rollback Journal File

Prior to making any changes to the database file, SQLite first creates a
separate `rollback journal file` and writes into the rollback journal the
`original content of the database pages` that are to be altered. The idea behind
the rollback journal is that it contains all information needed to restore the
database back to its original state.

The rollback journal contains a small `header`  

When a new file is created, most desktop operating systems will not actually
write anything to disk. The new file is created in the operating systems disk
cache only. The file is not created on mass storage until sometime later, when
the operating system has a spare moment. This creates the impression to users
that I/O is happening much faster than is possible when doing real disk I/O.
We illustrate this idea in the diagram to the right by showing that the new
rollback journal appears in the operating system disk cache only and not on
the disk itself.


3.6. Changing Database Pages In User Space

After the original page content has been saved in the rollback journal, the
pages can `be modified in user memory` Each database connection has its own
private copy of user space, so the changes that are made in user space are
only visible to the database connection that is making the changes. Other
database connections still see the information in operating system disk cache
buffers which have not yet been changed. And so even though one process is
busy modifying the database, other processes can continue to read their own
copies of the original database content.


3.7. Flushing The Rollback Journal File To Mass Storage

The next step is to flush the content of the rollback journal file to
nonvolatile storage. As we will see later, this is a critical step in insuring
that the database can survive an unexpected power loss. This step also takes a
lot of time, since writing to nonvolatile storage is normally a slow
operation.

This step is usually more complicated than simply flushing the rollback
journal to the disk. 

note: change (main) db

3.9. Writing Changes To The Database File

Once an exclusive lock is held, we know that no other processes are reading
from the database file and it is safe to write changes into the database file.
Usually those changes only go as far as the operating systems disk cache and
do not make it all the way to mass storage.

3.10. 0 Flushing Changes To Mass Storage

Another flush must occur to make sure that all the database changes are
written into nonvolatile storage. This is a critical step to ensure that the
database will survive a power loss without damage. However, because of the
inherent slowness of writing to disk or flash memory, this step together with
the rollback journal file flush in section 3.7 above takes up most of the time
required to complete a transaction commit in SQLite.


note: reset a journal file since pcat use "reset" optimisation.

3.11. 1 Deleting The Rollback Journal

After the database changes are all safely on the mass storage device, the
rollback journal file is deleted. This is the instant where the transaction
commits. 

If a power failure or system crash occurs prior to this point, then recovery
processes to be described later make it appear as if no changes were ever made
to the database file. 

note: means a power failure before resetting the header then will see this in
the next reboot or next transaction. so will start "rollback" recovery process.

If a power failure or system crash occurs after the rollback journal is
deleted, then it appears as if all changes have been written to disk. Thus,
  SQLite gives the appearance of having made no changes to the database file
  or having made the complete set of changes to the database file `depending
  on whether or not the rollback journal file exists`

Deleting a file is not really an atomic operation, but it appears to be from
the point of view of a user process. A process is always able to ask the
operating system "does this file exist?" and the process will get back a yes
or no answer. After a power failure that occurs during a transaction commit,
   SQLite will ask the operating system whether or not the rollback journal
   file exists. If the answer is "yes" then the transaction is incomplete and
   is rolled back. If the answer is "no" then it means the transaction did
   commit.

The existence of a transaction depends on whether or not the rollback journal
file exists and the deletion of a file appears to be an atomic operation from
the point of view of a user-space process. Therefore, a transaction appears to
be an atomic operation.

The act of deleting a file is expensive on many systems. As an optimization,
    SQLite can be configured to truncate the journal file to zero bytes in
    length or `overwrite the journal file header with zeros` In either case,
    the resulting journal file is no longer capable of rolling back and so the
    transaction still commits. Truncating a file to zero length, like deleting
    a file, is assumed to be an atomic operation from the point of view of a
    user process. 
    
Overwriting the header of the journal with zeros is not atomic, but if any
part of `the header is malformed the journal will not roll back` Hence, one
can say that the commit occurs as soon as the header is sufficiently changed
to make it invalid. Typically this happens as soon as the first byte of the
header is zeroed.


4. Rollback

An atomic commit is supposed to happen instantaneously. But the processing
described above clearly takes a finite amount of time. Suppose the power to
the computer were cut part way through the commit operation described above.
In order to maintain the illusion that the changes were instantaneous, we have
to "rollback" any partial changes and restore the database to the state it was
in prior to the beginning of the transaction.  


note: hot journal means

4.2. Hot Rollback Journals

The first time that any SQLite process attempts to access the database file,
    it obtains a shared lock as described in section 3.2 above. But then it
    `notices` that there is a rollback journal file present. SQLite then checks
    to see if the rollback journal is a "hot journal". A hot journal is a
    rollback journal that needs to be played back in order to restore the
    database to a sane state.


4.4. Rolling Back Incomplete Changes

Once a process obtains an exclusive lock, it is permitted to write to the
database file. It then proceeds to read the original content of pages out of
the rollback journal and write that content back to where it came from in the
database file.


4.5. Deleting The Hot Journal

After all information in the rollback journal has been played back into the
database file (and flushed to disk in case we encounter yet another power
    failure), the hot rollback journal can be deleted.

As in section 3.11, the journal file might be truncated to zero length or its
header might be overwritten with zeros as an optimization on systems where
deleting a file is expensive. Either way, the journal is `no longer hot` after
this step.


# ============================================================================
#{
={============================================================================
*kt_linux_core_002* linux-c-lib

LPI. 2.5 File I/O Model

The stdio library

To perform file I/O, C programs typically employ I/O functions contained in
the standard C library. This set of functions, referred to as the stdio
library, includes fopen(), fclose(), scanf(), printf(), fgets(), fputs(), and
so on. The stdio functions are layered on top of the I/O system calls (open(),
    close(), read(), write(), and so on).


<system-call-is-expensive>
The syscalls(2) manual page lists the Linux system calls.

LPI. 3.1 System Calls

As an example of the overhead of making a system call, consider the getppid()
system call, which simply returns the process ID of the parent of the calling
process. On one of the author's x86-32 systems running Linux 2.6.25, 10
million calls to getppid() required approximately 2.2 seconds to complete.
This amounts to around 0.3 microseconds per call. By comparison, on the same
system, 10 million calls to a C function that simply returns an integer
required 0.11 seconds, or around one-twentieth of the time required for calls
to getppid(). Of course, most system calls have significantly more overhead
than getppid().


<libc>
LPI. 3.3 The Standard C Library; The GNU C Library (glibc)

Use ldd to find out the path and when run libc, it shows version.

$ ldd ~/bin/vim | grep libc
   libc.so.6 => /lib/i386-linux-gnu/i686/cmov/libc.so.6 (0xb6915000)

/lib/i386-linux-gnu/i686/cmov/libc.so.6 -> libc-2.19.so

note: How this shared object prints out messages when run?

$ /lib/i386-linux-gnu/i686/cmov/libc.so.6
GNU C Library (Debian GLIBC 2.19-18+deb8u4) stable release version 2.19, by Roland McGrath et al.
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.
There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE.
Compiled by GNU CC version 4.8.4.
Compiled on a Linux 3.16.7 system on 2016-02-29.
Available extensions:
	crypt add-on version 2.1 by Michael Glad and others
	GNU Libidn by Simon Josefsson
	Native POSIX Threads Library by Ulrich Drepper et al
	BIND-8.2.3-T5B
libc ABIs: UNIQUE IFUNC
For bug reporting instructions, please see:
<http://www.debian.org/Bugs/>.

<api>
Can use macro at compile time or use a call at runtime to check libc version:

#include <gnu/libc-version.h>
const char *gnu_get_libc_version(void);


={============================================================================
*kt_linux_core_004* linux-errno

LPI. 3.4 Handling Errors from System Calls and Library Functions


{system-call-error}
The section headed ERRORS in each manual page provides a list of possible
errno values that can be returned by each system call.

Usually, an error is indicated by a return of –1. When a system call fails, it
sets `the global integer variable errno` to a positive value that identifies
the specific error.

Thus, a system call can be checked with code such as the following:

cnt = read(fd, buf, numbytes);
if (cnt == -1) {
  if (errno == EINTR)
    fprintf(stderr, "read was interrupted by a signal\n");
  else {
    /* Some other error occurred */
  }
}

we should always first check if the function return value indicates an error,
   and `only then` examine errno to determine the cause of the error.


<exceptions>
A few system calls (e.g., getpriority()) can legitimately return -1 on
success. To determine whether an error occurs in such calls, we set errno to 0
before the call, and then check it afterward. If the call returns -1 and errno
is nonzero, an error occurred.


{library-call-error}
The various library functions return different data types and different values
to indicate failure. For our purposes, library functions can be divided into
the following `categories`:

1. Some library functions return error information in exactly the same way as
system calls: a -1 return value, with errno indicating the specific error. 

  An example of such a function is remove(), which removes a file (using the
      unlink() system call) or a directory (using the rmdir() system call).
  Errors from these functions can be diagnosed in the same way as errors from
  system calls.

2. Some library functions return a value 'other' than -1 on error, but
nevertheless set errno to indicate the specific error condition. 

  For example, fopen() returns a NULL pointer on error, and the setting of
  errno depends on which underlying system call failed. The perror() and
  strerror() functions can be used to diagnose these errors.

3. Other library functions don't use errno at all. The method for determining
the existence and cause of errors depends on the particular function and is
documented in the function's manual page. For these functions, it is a mistake
to use errno, perror(), or strerror() to diagnose errors.


<errno-h>
/SPK/ams-drx890-debug/build_mips/staging_dir/mips-linux-uclibc/include/asm/errno.h

/*
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1995, 1999, 2001, 2002 by Ralf Baechle
 */
#ifndef _ASM_ERRNO_H
#define _ASM_ERRNO_H

/*
 * These error numbers are intended to be MIPS ABI compatible
 */

#include <asm-generic/errno-base.h>

#define	ENOMSG		35	/* No message of desired type */
#define	EIDRM		36	/* Identifier removed */
#define	ECHRNG		37	/* Channel number out of range */
#define	EL2NSYNC	38	/* Level 2 not synchronized */
#define	EL3HLT		39	/* Level 3 halted */
#define	EL3RST		40	/* Level 3 reset */
#define	ELNRNG		41	/* Link number out of range */
#define	EUNATCH		42	/* Protocol driver not attached */
#define	ENOCSI		43	/* No CSI structure available */
#define	EL2HLT		44	/* Level 2 halted */
#define	EDEADLK		45	/* Resource deadlock would occur */
#define	ENOLCK		46	/* No record locks available */
#define	EBADE		50	/* Invalid exchange */
#define	EBADR		51	/* Invalid request descriptor */
#define	EXFULL		52	/* Exchange full */
#define	ENOANO		53	/* No anode */
#define	EBADRQC		54	/* Invalid request code */
#define	EBADSLT		55	/* Invalid slot */
#define	EDEADLOCK	56	/* File locking deadlock error */
#define	EBFONT		59	/* Bad font file format */
#define	ENOSTR		60	/* Device not a stream */
#define	ENODATA		61	/* No data available */
#define	ETIME		62	/* Timer expired */
#define	ENOSR		63	/* Out of streams resources */
#define	ENONET		64	/* Machine is not on the network */
#define	ENOPKG		65	/* Package not installed */
#define	EREMOTE		66	/* Object is remote */
#define	ENOLINK		67	/* Link has been severed */
#define	EADV		68	/* Advertise error */
#define	ESRMNT		69	/* Srmount error */
#define	ECOMM		70	/* Communication error on send */
#define	EPROTO		71	/* Protocol error */
#define	EDOTDOT		73	/* RFS specific error */
#define	EMULTIHOP	74	/* Multihop attempted */
#define	EBADMSG		77	/* Not a data message */
#define	ENAMETOOLONG	78	/* File name too long */
#define	EOVERFLOW	79	/* Value too large for defined data type */
#define	ENOTUNIQ	80	/* Name not unique on network */
#define	EBADFD		81	/* File descriptor in bad state */
#define	EREMCHG		82	/* Remote address changed */
#define	ELIBACC		83	/* Can not access a needed shared library */
#define	ELIBBAD		84	/* Accessing a corrupted shared library */
#define	ELIBSCN		85	/* .lib section in a.out corrupted */
#define	ELIBMAX		86	/* Attempting to link in too many shared libraries */
#define	ELIBEXEC	87	/* Cannot exec a shared library directly */
#define	EILSEQ		88	/* Illegal byte sequence */
#define	ENOSYS		89	/* Function not implemented */
#define	ELOOP		90	/* Too many symbolic links encountered */
#define	ERESTART	91	/* Interrupted system call should be restarted */
#define	ESTRPIPE	92	/* Streams pipe error */
#define	ENOTEMPTY	93	/* Directory not empty */
#define	EUSERS		94	/* Too many users */
#define	ENOTSOCK	95	/* Socket operation on non-socket */
#define	EDESTADDRREQ	96	/* Destination address required */
#define	EMSGSIZE	97	/* Message too long */
#define	EPROTOTYPE	98	/* Protocol wrong type for socket */
#define	ENOPROTOOPT	99	/* Protocol not available */
#define	EPROTONOSUPPORT	120	/* Protocol not supported */
#define	ESOCKTNOSUPPORT	121	/* Socket type not supported */
#define	EOPNOTSUPP	122	/* Operation not supported on transport endpoint */
#define	EPFNOSUPPORT	123	/* Protocol family not supported */
#define	EAFNOSUPPORT	124	/* Address family not supported by protocol */
#define	EADDRINUSE	125	/* Address already in use */
#define	EADDRNOTAVAIL	126	/* Cannot assign requested address */
#define	ENETDOWN	127	/* Network is down */
#define	ENETUNREACH	128	/* Network is unreachable */
#define	ENETRESET	129	/* Network dropped connection because of reset */
#define	ECONNABORTED	130	/* Software caused connection abort */
#define	ECONNRESET	131	/* Connection reset by peer */
#define	ENOBUFS		132	/* No buffer space available */
#define	EISCONN		133	/* Transport endpoint is already connected */
#define	ENOTCONN	134	/* Transport endpoint is not connected */
#define	EUCLEAN		135	/* Structure needs cleaning */
#define	ENOTNAM		137	/* Not a XENIX named type file */
#define	ENAVAIL		138	/* No XENIX semaphores available */
#define	EISNAM		139	/* Is a named type file */
#define	EREMOTEIO	140	/* Remote I/O error */
#define EINIT		141	/* Reserved */
#define EREMDEV		142	/* Error 142 */
#define	ESHUTDOWN	143	/* Cannot send after transport endpoint shutdown */
#define	ETOOMANYREFS	144	/* Too many references: cannot splice */
#define	ETIMEDOUT	145	/* Connection timed out */
#define	ECONNREFUSED	146	/* Connection refused */
#define	EHOSTDOWN	147	/* Host is down */
#define	EHOSTUNREACH	148	/* No route to host */
#define	EWOULDBLOCK	EAGAIN	/* Operation would block */
#define	EALREADY	149	/* Operation already in progress */
#define	EINPROGRESS	150	/* Operation now in progress */
#define	ESTALE		151	/* Stale NFS file handle */
#define ECANCELED	158	/* AIO operation canceled */

/*
 * These error are Linux extensions.
 */
#define ENOMEDIUM	159	/* No medium found */
#define EMEDIUMTYPE	160	/* Wrong medium type */
#define	ENOKEY		161	/* Required key not available */
#define	EKEYEXPIRED	162	/* Key has expired */
#define	EKEYREVOKED	163	/* Key has been revoked */
#define	EKEYREJECTED	164	/* Key was rejected by service */

/* for robust mutexes */
#define	EOWNERDEAD	165	/* Owner died */
#define	ENOTRECOVERABLE	166	/* State not recoverable */

#define EDQUOT		1133	/* Quota exceeded */

#endif /* _ASM_ERRNO_H */


={============================================================================
*kt_linux_core_005* linux-error handling codes, thread-errno

LPI. 3.5.2 Common Functions and Header Files

void errMsg(const char *format, ...);

  Prints a message on standard error. Its argument list is the same as for
    printf(), except that a terminating newline character is automatically
    appended to the output string. The errMsg() function prints the error text
    corresponding to the current value of errno-this consists of the error
    name, such as EPERM, plus the error description as returned by
    strerror()-followed by the formatted output specified in the argument
    list.

void errExit(const char *format, ...);

  Operates like errMsg(), but also terminates the program, either by calling
    exit() or, if the environment variable EF_DUMPCORE is defined with a
    nonempty string value, `by calling abort() to produce a core dump file`

void err_exit(const char *format, ...);

This is similar to errExit(), but differs in two respects:

  It doesn't flush standard output before printing the error message.

  It terminates the process by calling _exit() instead of exit(). This causes
  the process to terminate without flushing stdio buffers or invoking exit
  handlers.

The details of these differences in the operation of err_exit() will become
clearer where we describe the differences between _exit() and exit(), and
consider the treatment of stdio buffers and exit handlers in a child created
by fork(). 

For now, we simply note that err_exit() is especially useful if we write a
library function that creates a child process that needs to terminate because
of an error. This termination should occur without flushing the child's copy
of the parent's (i.e., the calling process's) stdio buffers and without
invoking exit handlers established by the parent.


<thread-errno>
void errExitEN(int errnum, const char *format, ...);

is the same as errExit(), except that instead of printing the error text
  corresponding to the current value of errno, it prints the text
  corresponding to the error number (thus, the EN suffix) given in the
  argument errnum.

Mainly, we use errExitEN() in programs that employ the POSIX threads API.
Since:

The traditional method of returning status from system calls and some library
functions is to return 0 on success and -1 on error, with errno being set to
indicate the error. The functions in the `pthreads API do things differently`
All pthreads functions return 0 on success or a positive value on failure. The
failure value is one of the same values that can be placed in errno by
traditional UNIX system calls. 

We could diagnose errors from the POSIX threads functions using code such as
the following:

errno = pthread_create(&thread, NULL, func, &arg);
if (errno != 0)
   errExit("pthread_create");

This approach is `inefficient` because errno is defined in threaded programs
  as a macro. Under the POSIX threads API, errno is redefined to be a function
  that returns a pointer to a thread-specific storage area (see Section 31.3

errExitEN() function allows us to write a more efficient equivalent of the
above code:

int s;
s = pthread_create(&thread, NULL, func, &arg);
if (s != 0)
   errExitEN(s, "pthread_create");


<code>
<lib/error_functions.h>

/*************************************************************************\
*                  Copyright (C) Michael Kerrisk, 2014.                   *
*                                                                         *
* This program is free software. You may use, modify, and redistribute it *
* under the terms of the GNU Lesser General Public License as published   *
* by the Free Software Foundation, either version 3 or (at your option)   *
* any later version. This program is distributed without any warranty.    *
* See the files COPYING.lgpl-v3 and COPYING.gpl-v3 for details.           *
\*************************************************************************/

/* Listing 3-2 */

/* error_functions.h

   Header file for error_functions.c.
*/
#ifndef ERROR_FUNCTIONS_H
#define ERROR_FUNCTIONS_H

/* Error diagnostic routines */

void errMsg(const char *format, ...);

#ifdef __GNUC__

    /* This macro stops 'gcc -Wall' complaining that "control reaches
       end of non-void function" if we use the following functions to
       terminate main() or some other non-void function. */

#define NORETURN __attribute__ ((__noreturn__))
#else
#define NORETURN
#endif

void errExit(const char *format, ...) NORETURN ;

void err_exit(const char *format, ...) NORETURN ;

void errExitEN(int errnum, const char *format, ...) NORETURN ;

void fatal(const char *format, ...) NORETURN ;

void usageErr(const char *format, ...) NORETURN ;

void cmdLineErr(const char *format, ...) NORETURN ;

#endif

<lib/error_functions.c>

/*************************************************************************\
*                  Copyright (C) Michael Kerrisk, 2014.                   *
*                                                                         *
* This program is free software. You may use, modify, and redistribute it *
* under the terms of the GNU Lesser General Public License as published   *
* by the Free Software Foundation, either version 3 or (at your option)   *
* any later version. This program is distributed without any warranty.    *
* See the files COPYING.lgpl-v3 and COPYING.gpl-v3 for details.           *
\*************************************************************************/

/* Listing 3-3 */

/* error_functions.c

   Some standard error handling routines used by various programs.
*/
#include <stdarg.h>
#include "error_functions.h"
#include "tlpi_hdr.h"
#include "ename.c.inc"          /* Defines ename and MAX_ENAME */

#ifdef __GNUC__                 /* Prevent 'gcc -Wall' complaining  */
__attribute__ ((__noreturn__))  /* if we call this function as last */
#endif                          /* statement in a non-void function */
static void
terminate(Boolean useExit3)
{
    char *s;

    /* Dump core if EF_DUMPCORE environment variable is defined and
       is a nonempty string; otherwise call exit(3) or _exit(2),
       depending on the value of 'useExit3'. */

    s = getenv("EF_DUMPCORE");

    if (s != NULL && *s != '\0')
        abort();
    else if (useExit3)
        exit(EXIT_FAILURE);
    else
        _exit(EXIT_FAILURE);
}

/* Diagnose 'errno' error by:

      * outputting a string containing the error name (if available
        in 'ename' array) corresponding to the value in 'err', along
        with the corresponding error message from strerror(), and

      * outputting the caller-supplied error message specified in
        'format' and 'ap'. */

static void
outputError(Boolean useErr, int err, Boolean flushStdout,
        const char *format, va_list ap)
{
#define BUF_SIZE 500
    char buf[BUF_SIZE], userMsg[BUF_SIZE], errText[BUF_SIZE];

    vsnprintf(userMsg, BUF_SIZE, format, ap);

    if (useErr)
        snprintf(errText, BUF_SIZE, " [%s %s]",
                (err > 0 && err <= MAX_ENAME) ?
                ename[err] : "?UNKNOWN?", strerror(err));
    else
        snprintf(errText, BUF_SIZE, ":");

    snprintf(buf, BUF_SIZE, "ERROR%s %s\n", errText, userMsg);

    if (flushStdout)
        fflush(stdout);       /* Flush any pending stdout */
    fputs(buf, stderr);
    fflush(stderr);           /* In case stderr is not line-buffered */
}

/* Display error message including 'errno' diagnostic, and
   return to caller */

void
errMsg(const char *format, ...)
{
    va_list argList;
    int savedErrno;

    savedErrno = errno;       /* In case we change it here */

    va_start(argList, format);
    outputError(TRUE, errno, TRUE, format, argList);
    va_end(argList);

    errno = savedErrno;
}

/* Display error message including 'errno' diagnostic, and
   terminate the process */

void
errExit(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errno, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Display error message including 'errno' diagnostic, and
   terminate the process by calling _exit().

   The relationship between this function and errExit() is analogous
   to that between _exit(2) and exit(3): unlike errExit(), this
   function does not flush stdout and calls _exit(2) to terminate the
   process (rather than exit(3), which would cause exit handlers to be
   invoked).

   These differences make this function especially useful in a library
   function that creates a child process that must then terminate
   because of an error: the child must terminate without flushing
   stdio buffers that were partially filled by the caller and without
   invoking exit handlers that were established by the caller. */

void
err_exit(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errno, FALSE, format, argList);
    va_end(argList);

    terminate(FALSE);
}

/* The following function does the same as errExit(), but expects
   the error number in 'errnum' */

void
errExitEN(int errnum, const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(TRUE, errnum, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Print an error message (without an 'errno' diagnostic) */

void
fatal(const char *format, ...)
{
    va_list argList;

    va_start(argList, format);
    outputError(FALSE, 0, TRUE, format, argList);
    va_end(argList);

    terminate(TRUE);
}

/* Print a command usage error message and terminate the process */

void
usageErr(const char *format, ...)
{
    va_list argList;

    fflush(stdout);           /* Flush any pending stdout */

    fprintf(stderr, "Usage: ");
    va_start(argList, format);
    vfprintf(stderr, format, argList);
    va_end(argList);

    fflush(stderr);           /* In case stderr is not line-buffered */
    exit(EXIT_FAILURE);
}

/* Diagnose an error in command-line arguments and
   terminate the process */

void
cmdLineErr(const char *format, ...)
{
    va_list argList;

    fflush(stdout);           /* Flush any pending stdout */

    fprintf(stderr, "Command-line usage error: ");
    va_start(argList, format);
    vfprintf(stderr, format, argList);
    va_end(argList);

    fflush(stderr);           /* In case stderr is not line-buffered */
    exit(EXIT_FAILURE);
}


={============================================================================
*kt_linux_core_006* linux-portability

LPI. 3.6 Portability Issues

<feature-test-macros>
Sometimes, when writing a portable application, we may want the various header
files to expose only the definitions (constants, function prototypes, and so
    on) that follow a particular standard.

The following feature test macros listed are glibc-specific:

_BSD_SOURCE

If defined (with any value), expose BSD definitions. Defining this macro also
defines _POSIX_C_SOURCE with the value 199506. Explicitly setting just this
macro causes BSD definitions to be favored in a few cases where standards
conflict.

_SVID_SOURCE

If defined (with any value), expose System V Interface Definition (SVID)
definitions.

_GNU_SOURCE

If defined (with any value), expose all of the definitions provided by setting
all of the preceding macros, as well as various GNU extensions.

If individual macros are defined, or the compiler is invoked in one of its
standard modes (e.g., cc –ansi or cc –std=c99), then only the requested
definitions are supplied.

The <features.h> header file and the `feature_test_macros(7)` manual page
provide further information on precisely what values are assigned to each of
the feature test macros.


<system-data-type>
Various implementation data types are represented using standard C types, for
example, process IDs, user IDs, and file offsets. Although it would be
possible to use the C fundamental types, `native type`, such as int and long
to declare variables storing such information, this reduces portability across
UNIX systems, for the following reasons:

  The sizes of these fundamental types vary across UNIX implementations (e.g.,
      a long may be 4 bytes on one system and 8 bytes on another), or
  sometimes even in different compilation environments on the same
  implementation.  Furthermore, different implementations may use different
  types to represent the same information. For example, a process ID might be
  an int on one system but a long on another.

  Even on a single UNIX implementation, the types used to represent
  information may differ between releases of the implementation. Notable
  examples on Linux are user and group IDs. On Linux 2.2 and earlier, these
  values were represented in 16 bits. On Linux 2.4 and later, they are 32-bit
  values.

To avoid such portability problems, SUSv3 specifies various standard system
data types, and requires an implementation to define and use these types
appropriately.

Each of these types is defined using the `C typedef feature` For example, the
pid_t data type is intended for representing process IDs, and on Linux/x86-32
this type is defined as follows:

typedef int pid_t;

Most of the standard system data types have names ending in _t. Many of them
  are declared in the header file <sys/types.h>, although a few are defined in
  other header files.

An application should employ these type definitions to portably declare the
variables it uses. For example, the following declaration would allow an
application to correctly represent process IDs on any SUSv3-conformant system:

pid_t mypid;

Table 3-1 lists some of the system data types we'll encounter in this book.

When discussing the data types in Table 3-1 in later chapters, we'll often
make statements that some type "is an integer type [specified by SUSv3]." This
means that SUSv3 requires the type to be defined as an integer, but doesn't
require that a particular `native integer type` (e.g., short, int, or long) be
used.


<printf-representation>
When printing values of one of the numeric system data types shown in Table
3-1 (e.g., pid_t and uid_t), we must be careful not to include a
representation dependency in the printf() call. A representation dependency
can occur because C’s argument promotion rules convert values of type short to
int, but leave values of type int and long unchanged. This means that,
  depending on the definition of the system data type, either an int or a long
    is passed in the printf() call. However, because printf() has no way to
    determine the types of its arguments at run time, the caller must
    explicitly provide this information using the %d or %ld format specifier.
    The problem is that simply coding one of these specifiers within the
    printf() call creates an implementation dependency. The usual solution is
    to use the %ld specifier and always cast the corresponding value to long,
  like so:

pid_t mypid;
mypid = getpid(); /* Returns process ID of calling process */
printf("My PID is %ld\n", (long) mypid);

We make one exception to the above technique. Because the off_t data type is
  the size of long long in some compilation environments, we cast off_t values
  to this type and use the %lld specifier, as described in Section 5.10.
  

<structures>
Each UNIX implementation specifies a range of standard structures that are
used in various system calls and library functions. As an example, consider
the sembuf structure, which is used to represent a semaphore operation to be
performed by the semop() system call:

struct sembuf {
  unsigned short sem_num; /* Semaphore number */
  short sem_op; /* Operation to be performed */
  short sem_flg; /* Operation flags */
};

Although SUSv3 specifies structures such as sembuf, it is important to realize
the following:

  `In general`, the order of field definitions within such structures is not
  specified.

  In some cases, extra implementation-specific fields may be included in such
  structures.

Consequently, it is not portable to use a structure initializer such as the
following:

struct sembuf s = { 3, -1, SEM_UNDO };

Although this initializer will work on Linux, it won't work on another
implementation where the fields in the sembuf structure are defined in a
different order. 

To portably initialize such structures, we must use explicit assignment
statements, as in the following:

struct sembuf s;
s.sem_num = 3;
s.sem_op = -1;
s.sem_flg = SEM_UNDO;

If we are using C99, then we can employ that language's new syntax for
structure initializers to write an equivalent initialization:

struct sembuf s = { .sem_num = 3, .sem_op = -1, .sem_flg = SEM_UNDO };

Considerations about the order of the members of standard structures also
apply if we want to write the contents of a standard structure to a file. To
do this portably, we can't simply do a binary write of the structure. Instead,
   the structure fields must be written individually (probably in text form)
  in a specified order.


={============================================================================
*kt_linux_core_006* linux-process-credential

LPI. 2.7 Processes

{real-and-effective}
The real user ID and group ID identify the user and group to which the process
belongs. A new process inherits these IDs from its parent. A login shell gets
its real user ID and real group ID from the corresponding fields in the system
password file.

Effective user ID and effective group ID: These two IDs (in conjunction with
    the supplementary group IDs discussed in a moment) are used in determining
the permissions that the process has `when accessing protected resources` such
as files and interprocess communication objects. Typically, the process's
effective IDs have the same values as the corresponding real IDs. Changing the
effective IDs is a mechanism that allows a process to assume the privileges of
another user or group, as described in a moment.

The effective user ID and group ID are used to 'determine' the permissions
granted to a process when it tries to perform various operations such as
system calls. 

For example, these identifiers determine the permissions granted to a process
when it accesses resources such as files and System V interprocess
communication (IPC) objects, which themselves have associated user and group
IDs determining to whom they belong. the effective user ID is also used by the
kernel to determine whether one process can send a signal to another.

A process whose effective user ID is 0 (the user ID of root) has all of the
privileges of the superuser. Such a process is referred to as a privileged
process. Certain system calls can be executed only by privileged processes.

<can-be-different>
Normally, the effective user and group IDs have the same values as the
corresponding real IDs, but there are two ways in which the effective IDs can
assume different values. One way is through the use of system calls that we
discuss in Section 9.7. 

The second way is through the execution of set-user-ID and set-group-ID
programs.

<set-user-ID-mechanism>
In addition, an executable file has two special permission bits: the
set-user-ID and set-group-ID bits. In fact, every file has these two
permission bits, but it is their use with executable files that interests us
here.

# chmod u+s prog  # Turn on set-user-ID permission bit
# chmod g+s prog  # Turn on set-group-ID permission bit

# ls -l prog
-rwsr-sr-x 1 root root 302585 Jun 26 15:05 prog


When a set-user-ID program is run, the kernel 'sets' the effective user ID of
the process to be the same as the user ID of the executable 'file'. Running a
set-group-ID program has an analogous effect for the effective group ID of the
process. Changing the effective user or group ID in this way gives a process
(in other words, the user executing the program) privileges it would not
normally have.

For example, if an executable file is owned by root (superuser) and has the
set-user-ID permission bit enabled, then the process gains superuser
privileges when that program is run.

<ex>
Examples of commonly used set-user-ID programs on Linux include: passwd(1),
         which changes a user's password; mount(8) and umount(8), which mount
         and unmount file systems; and su(1), which allows a user to run a
         shell under a different user ID.

$ ls -al /bin/mount
-rwsr-xr-x 1 root root 88744 Dec  9  2012 /bin/mount


={============================================================================
*kt_linux_core_007* /dev/null

`/dev/null` is a virtual device that always discards the data written to it.
When we want to eliminate the standard output or error of a shell command, we
can redirect it to this file. Reads from this device always return
end-of-file.


={============================================================================
*kt_linux_core_008* score: file ownership and permissions

Each file has an associated user ID and group ID that define the owner of the
file and the group to which it belongs. The ownership of a file is used to
determine the access rights available to users of the file.

For the purpose of accessing a file, the system divides users into three
categories: the owner of the file, users, group, and the rest of the world
(other). Three permission bits may be set for each of these categories of user.

The read permission allows the contents of the file to be read; write permission
allows modification of the contents of the file; and execute permission allows
execution of the file, which is either a program or a script to be processed by
some interpreter.

<search-permission>
These permissions may also be set on directories, although their meanings are
slightly different: read permission allows the contents of (i.e., the filenames
    in) the directory to be listed; write permission allows the contents of the
directory to be changed (i.e., filenames can be added, removed, and changed);
and execute (sometimes called search) permission allows access to files within
  the directory (subject to the permissions on the files themselves).

note: when there is no excute permission on a directory, cannot 'cd' in. this
causes a failure when do build and turns out that some dirs don't have execute
on dirs for a group and make system cannot cd in.


<mode-letter>

From chmod:

The letters rwxXst select file mode bits for the affected users: read (r), write
(w), execute (or search for directories) (x), execute/search only if the file is
a directory or already has execute permission  for  some  user (X),  set  user
or group ID on execution (s), restricted deletion flag or sticky bit (t).
Instead of one or more of these letters, you can specify exactly one of the
letters ugo: the permissions granted to the user who owns the file (u), the
permissions granted to other users who are members of the file's group (g), and
the permissions granted to users that are in neither of the two preceding
categories (o).


drwxrws--T    2 root     keys          4096 Oct 29 10:19 ./
drwxr-xr-x    3 root     root          4096 Oct 29 10:19 ../
-rw-rw-r-T    1 root     keys         63514 Oct 29 10:19 db0.storage


<sticky-bit>
http://www.linuxnix.com/sticky-bit-set-linux/

What is Sticky Bit?

Sticky Bit is mainly used on folders in order to avoid deletion of a folder and
its content by other users though they having write permissions on the folder
contents. If Sticky bit is enabled on a folder, the folder contents are deleted
by only owner who created them and the root user. No one else can delete other
users data in this folder(Where sticky bit is set). This is a security measure
to avoid deletion of critical folders and their content(sub-folders and files),
   though other users have full permissions.

Q: I am seeing "T" ie Capital s in the file permissions, what's that?

After setting Sticky Bit to a file/folder, if you see 'T' in the file permission
area that indicates the file/folder does not have executable permissions for
'all' users on that particular file/folder.


={============================================================================
*kt_linux_core_050* signal

{signal-is-notification}
A signal is a notification to a process that an event has occurred. Sometimes
described as `software-interrupts` and are analogous to hardware interrupts in
that they interrupt the normal flow of execution of a program.

{signal-and-kernel}
One process can (if it has suitable permissions) send a signal to another
process. Can be employed as a synchronization technique, or even as a primitive
form of interprocess communication (IPC). It is also possible for a process to
send a signal to itself. However, the usual source of many signals sent to a
process is the kernel.

{signal-symbolic-names}
Each signal is defined as a unique (small) integer, starting sequentially from
1. These integers are defined in <signal.h> with symbolic names of the form
SIGxxxx. Since the actual numbers used for each signal vary across
implementations, it is these symbolic names that are always used in programs.

note: 
$kill -l to see symbolic names.


{pending}
A signal is said to be generated by some event. Once generated, a signal is
later delivered to a process, which then takes some action in response to the
signal. Between the time it is generated and the time it is delivered, a signal
is said to be pending. Why pending since do not know when it is next scheduled
to run, or immediately if the process is already running


{signal-mask-per-process} {signal-block}
Sometimes, however, we need to ensure that a segment of code is 'not'
interrupted by the delivery of a signal. To do this, we can add a signal to the
process's `signal-mask`; a set of signals whose delivery is currently blocked.
If a signal is generated while it is blocked, it `remains-pending` until it is
later unblocked (removed from the signal mask).

If a process receives a signal that it is currently blocking, that signal is
added to the process's `set-of-pending-signals`.

That delivery of a signal is blocked during the execution of its handler (unless
    we specify the SA_NODEFER flag to sigaction()). If the signal is (again)
generated while the handler is executing, then it is marked as pending and later
delivered when the handler returns.

note:
Signals can be pending due to either blocked or its handling running.

<signal-is-queued-or-not>
`standard-signals` can't be queued; delivered only once. The set of pending
signals is only a mask; it indicates whether or not a signal has occurred, but
not how many times it has occurred. In other words, if the same signal is
generated multiple times while it is blocked, then it is recorded in the set of
pending signals, and later delivered, just once. One of the differences between
standard and realtime signals is that `realtime-signals` are queued.


{signal-handler}
Instead of accepting the default for a particular signal, a program can change
the action that occurs when the signal is delivered. Can be used to ignore
signals or to change the default. To change a default is usually referred to as
`installing` or establishing a signal handler. When a signal handler is invoked
in response to the delivery of a signal, we say that the signal has been handled
or, synonymously, caught.

<not-for-all-signals>
It isn't possible to set the disposition of a signal to terminate or dump core
unless one of these is the default disposition of the signal. The nearest we can
get to this is to install a handler for the signal that then calls either exit()
  or abort(). 
  
*coredump*
The abort() function generates a SIGABRT signal for the process, which causes it
to dump core and terminate.

note:
After all, can install signal handler only for signals which do exit() or
abort() by default?


{signal-reentrant}
Because a signal handler may asynchronously interrupt the execution of a program
at any point in time, the main program and the signal handler in effect form
`two-independent` (although not concurrent) threads of execution within the same
process.


A function is said to be `reentrant` if it can safely be simultaneously executed
by multiple threads of execution in the `same-process`. In this context, "safe"
means that the function achieves its expected result, regardless of the state of
execution of any other thread of execution.

A function may be `nonreentrant` if it updates global or static data structures. A
function that employs only local variables is guaranteed to be reentrant because
of race-condition. This book shows an example using crypt() in both main and
signal handler. This corrupts internal buffer which is statically allocated and
crypt uses when calls it with differnt parameter.

note:
`nonreentrant` means that function is not safe when gets called by multiple
threads. Using only local variables means reentrant. Why mentions race
condition?

Such possibilities are in fact rife within the standard C library. For example,
     we already noted in Section 7.1.3 that malloc() and free() maintain a
     linked list of freed memory blocks available for reallocation from the
     heap. If a call to malloc() in the main program is interrupted by a signal
     handler that also calls malloc(), then this linked list can be corrupted.
     For this reason, the malloc() family of functions, and other library
     functions that use them, are nonreentrant.

Even if we are not using nonreentrant library functions, reentrancy issues can
still be relevant. If a signal handler updates programmer-defined global data
structures that are also updated within the main program, then we can say that
the signal handler is nonreentrant with respect to the main program.

If a function is nonreentrant, then its manual page will normally provide an
explicit or implicit indication of this fact. In particular, watch out for
statements that the function uses or returns information in statically allocated
variables.


{async-signal-safe-function}
An `async-signal-safe` function is one that the implementation guarantees to be
safe when called from a signal handler. A function is async-signal-safe either
because it is reentrant or because it is not interruptible by a signal handler.

Table 21-1: Functions required to be async-signal-safe by POSIX.1-1990, SUSv2,
      and SUSv3

Real-world applications should avoid calling non-async-signal-safe functions
from signal handlers.


{signal-and-proc}
The Linux-specific /proc/PID/status file contains various bit-mask fields that
can be inspected to determine a process's treatment of signals. The bit masks
are displayed as hexadecimal numbers, with the least significant bit
representing signal 1, the next bit to the left representing signal 2, and so
on. 

<signals>
SigQ: 0/3941
SigPnd: 0000000000000000
ShdPnd: 0000000000000000
SigBlk: 0000000000000000
SigIgn: fffffffe57f0d8fc
SigCgt: 00000000280b2603

These fields are 

SigPnd (`per-thread` pending signals)
ShdPnd (`process-wide` pending signals since Linux 2.6)
SigBlk (blocked signals), SigIgn (ignored signals)
SigCgt (caught signals)

The difference between the SigPnd and ShdPnd fields will become clear when we
describe the handling of signals in multithreaded processes in Section 33.2.
The same information can also be obtained using various options to the ps(1)
  command.


{standard-and-realtime-signal}
Signals fall into two broad categories; standard and realtime. On Linux, the
standard signals are numbered from 1 to 31. We describe the standard signals in
this chapter. The other set of signals consists of the realtime signals.

<why-reliable-signal>
In early implementations, signals could be lost (i.e., not delivered to the
    target process) in certain circumstances. Furthermore, although facilities
were provided to block delivery of signals while critical code was executed, in
some circumstances, blocking was not reliable. These problems were remedied in
4.2BSD, which provided so-called reliable signals.

However, the Linux signal(7) manual page lists more than 31 signal names. The
excess names can be accounted for in a variety of ways. Some of the names are
simply synonyms for other names, and are defined for source compatibility with
other UNIX implementations. Other names are defined but unused.


={============================================================================
*kt_linux_core_051* signal-names

<SIGHUP> see process group for more.
When a terminal disconnect (hangup) occurs, this signal is sent to the
'controlling' process of the terminal. A second use of SIGHUP is with daemons
(e.g., init, httpd, and inetd). Many daemons are designed to respond to the
receipt of SIGHUP by reinitializing themselves and rereading their configuration
files. The system administrator triggers these actions by manually sending
SIGHUP to the daemon, either by using an explicit kill command or by executing a
program or script that does the same.


<SIGABRT> *coredump*
The abort() function (Section 21.2.2) generates a SIGABRT signal for the
process, which causes it to dump core and terminate.

<SIGINT>
When the user types the terminal interrupt character (usually Control-C), the
terminal driver sends this signal to the foreground process group. The default
action for this signal is to terminate the process.

<SIGPIPE> broken-pipe
This signal is generated when a process tries to write to a pipe, a FIFO, or a
socket for which there is no corresponding reader process. This normally occurs
because the reading process has closed its file descriptor for the IPC channel.
See Section 44.2 for further details.


<SIGSEGV> *sigseg* 11
This very popular signal is generated when a program makes an
`invalid-memory-reference`. A memory reference may be invalid because:

the referenced page doesn't exist e.g., it lies in an unmapped area somewhere
between the heap and the stack, 

the process tried to update a location in read-only memory e.g., the program
  text segment or a region of mapped memory marked read-only, 

the process tried to access a part of kernel memory while running in user mode
  (Section 2.1). 

In C, these events often result from dereferencing a pointer containing a bad
address (e.g., an uninitialized pointer) or passing an invalid argument in a
function call. 

<case>
Since there is no enough memory to create a thread stack due to memory leak,
coredump is made. `process-resource-case`

The name of this signal derives from the term segmentation violation.


<SIGKILL> 9
This is the `sure-kill-signal`. It can't be blocked, ignored, or caught by a
handler, and thus always terminates a process.


<SIGTERM> 15
This is the `default-signal` used for terminating a process and is the signal
sent by the 'kill' and 'killall' commands. 


<difference-between-sigkill-and-sigterm>
Users sometimes explicitly send the SIGKILL signal to a process using kill -KILL
or kill -9. 

However, this is generally a mistake. A well-designed application will have a
handler for SIGTERM that causes the application to exit 'gracefully', cleaning
up temporary files and releasing other resources beforehand. Killing a process
with SIGKILL `bypasses` the SIGTERM handler. Thus, we should always first
attempt to terminate a process using SIGTERM, and reserve SIGKILL as a last
resort for killing runaway processes that don't respond to SIGTERM. 

note: 
This means that if use sure kill, then can cause resource locked up and leaking.

What if there is no handler of SIGTERM? will be terminated by kernel anyway?
when tried to kill a simple application which do sleep and loop, it is
terminated when use kill, SIGTERM.

<SIGURG>
This signal is sent to a process to indicate the presence of out-of-band (also
    known as urgent) data on a socket

<SIGCHLD>
This signal is sent (by the kernel) to a parent process when one of its children
terminates: either by calling exit() or as a result of being killed by a signal.
It may also be sent to a process when one of its children is stopped or resumed
by a signal. By default, SIGCHLD is ignored.

<SIGUSR1>
This signal and SIGUSR2 are available for programmer-defined purposes. The
kernel never generates these signals for a process. Processes may use these
signals to notify one another of events or to synchronize with each other. In
early UNIX implementations, these were the only two signals that could be freely
used in applications. In fact, processes can send one another any signal, but
this has the potential for confusion if the kernel also generates one of the
signals for a process.  Modern UNIX implementations provide a large set of
realtime signals that are also available for programmer-defined purposes
(Section 22.8).


={============================================================================
*kt_linux_core_051* signal: example: use signal as synchronization

#include <signal.h>
#include "curr_time.h" /* Declaration of currTime() */
#include "tlpi_hdr.h"

#define SYNC_SIG SIGUSR1 /* Synchronization signal */

static void /* Signal handler - does nothing but return */
handler(int sig)
{
}

int
main(int argc, char *argv[])
{
  pid_t childPid;
  sigset_t blockMask, origMask, emptyMask;
  struct sigaction sa;

  setbuf(stdout, NULL); /* Disable buffering of stdout */

  // The sigemptyset() function initializes a signal set to contain no members. The sigfillset()
  // function initializes a set to contain all signals (including all realtime signals). After
  // initialization, individual signals can be added to a set using sigaddset() and removed using
  // sigdelset().

  sigemptyset(&blockMask);
  sigaddset(&blockMask, SYNC_SIG); /* Block signal */

  // We can use sigprocmask() to change the process signal mask, to retrieve the existing mask, or
  // both. The how argument determines the changes that sigprocmask() makes to the signal mask:

  if (sigprocmask(SIG_BLOCK, &blockMask, &origMask) == -1)
    errExit("sigprocmask");

  sigemptyset(&sa.sa_mask);
  sa.sa_flags = SA_RESTART;
  sa.sa_handler = handler;

  if (sigaction(SYNC_SIG, &sa, NULL) == -1)
    errExit("sigaction");

  switch (childPid = fork()) {
    case -1:
      errExit("fork");

    case 0: /* Child */

      /* Child does some required action here... */
      printf("[%s %ld] Child started - doing some work\n", currTime("%T"), (long) getpid());
      sleep(2); /* Simulate time spent doing some work */

      /* And then signals parent that it's done */
      printf("[%s %ld] Child about to signal parent\n", currTime("%T"), (long) getpid());

      if (kill(getppid(), SYNC_SIG) == -1)
        errExit("kill");

      /* Now child can do other things... */
      _exit(EXIT_SUCCESS);

    default: /* Parent */

      /* Parent may do some work here, and then waits for child to complete the required action */
      printf("[%s %ld] Parent about to wait for signal\n", currTime("%T"), (long) getpid());

      sigemptyset(&emptyMask);
      if (sigsuspend(&emptyMask) == -1 && errno != EINTR)
        errExit("sigsuspend");

      printf("[%s %ld] Parent got signal\n", currTime("%T"), (long) getpid());

      /* If required, return signal mask to its original state */
      if (sigprocmask(SIG_SETMASK, &origMask, NULL) == -1)
        errExit("sigprocmask");

      /* Parent carries on to do other things... */
      exit(EXIT_SUCCESS);
  }
}


={============================================================================
*kt_linux_core_052* signal: kill: checking for the existence of a process

Can send a signal to another process using the kill() system call, which is the analog of the kill
shell command.

#include <signal.h>

int kill(pid_t pid, int sig); Returns 0 on success, or -1 on error

If no process matches the specified pid, kill() fails and sets errno to ESRCH ("No such process"). A
process needs appropriate permissions to be able send a signal to another process.

If pid equals 0, the signal is sent to every process in the same process group as the calling
process, including the calling process itself. 

<signal-to-process-group>
If pid is less than -1, the signal is sent to all of the processes in the process group whose ID
equals the absolute value of pid. Sending a signal to all of the processes in a process group finds
particular use in shell job control.

<check-existence>
The kill() system call can serve another purpose. If the sig argument is specified as 0, the
so-called null signal, then no signal is sent. Instead, kill() merely performs error checking to see
if the process 'can' be signaled. 

Read another way, this means we can use the null signal to test if a process with a specific process
  ID exists. 

1. If sending a null signal fails with the error ESRCH, then we know the process doesn't exist. 
  
2. If the call fails with the error EPERM (meaning the process exists, but we don't have permission
    to send a signal to it) or 

3. succeeds (meaning we do have permission to send a signal to the process), then we know that the
process exists.

So this code do sleep while a process of pid is alive.

while (0 == kill(parent_pid, 0))
{
    sleep(1);
}


<but>
Verifying the existence of a particular process ID doesn't guarantee that a 'particular' program is
still running. Because the kernel recycles process IDs as processes are born and die, the same
process ID may, over time, refer to a different process.


={============================================================================
*kt_linux_core_052* signal-raise-call

RAISE(3)
Linux Programmer's Manual

NAME
       raise - send a signal to the caller

SYNOPSIS
       #include <signal.h>

       int raise(int sig);

DESCRIPTION
       The raise() function sends a signal to the calling process or thread.  In
       a single-threaded program it is equivalent to

           kill(getpid(), sig);

       In a multithreaded program it is equivalent to

           pthread_kill(pthread_self(), sig);

       If the signal causes a handler to be called, raise() will only return
         after the signal handler has returned.

RETURN VALUE
       raise() returns 0 on success, and nonzero for failure.

CONFORMING TO
       C89, C99, POSIX.1-2001.

NOTES
       Since version 2.3.3, glibc implements raise() by calling tgkill(2), if
       the kernel supports that system call.  Older glibc versions implemented
       raise() using kill(2).


/**
 * g_logv:
 * @log_domain: the log domain
 * @log_level: the log level
 * @format: the message format. See the printf() documentation
 * @args: the parameters to insert into the format string
 *
 * Logs an error or debugging message.
 *
 * If the log level has been set as fatal, the abort()
 * function is called to terminate the program.
 */
void
g_logv (const gchar   *log_domain,
  GLogLevelFlags log_level,
  const gchar   *format,
  va_list       args)
{
  ...
  _g_log_abort (!(test_level & G_LOG_FLAG_RECURSION));
  ...
}

static void
_g_log_abort (gboolean breakpoint)
{
  if (g_test_subprocess ())
    {
      /* If this is a test case subprocess then it probably caused
       * this error message on purpose, so just exit() rather than
       * abort()ing, to avoid triggering any system crash-reporting
       * daemon.
       */
      _exit (1);
    }

  if (breakpoint)
    G_BREAKPOINT ();
  else
    abort ();
}


#else   /* !__i386__ && !__alpha__ */
#  define G_BREAKPOINT()        G_STMT_START{ raise (SIGTRAP); }G_STMT_END
#endif  /* __i386__ */


={============================================================================
*kt_linux_core_100* process

LPI 6.

{process}
A process is an `abstract-entity`, defined by the kernel, to which system
resources are allocated in order to execute a program.


{process-id}
PID is integer type and the Linux limits PIDs to 32767. Once it has reached
32,767, the process ID counter is reset to 300, rather than 1. This is done
because many low-numbered process IDs are in permanent use by system processes
and daemons, and thus time would be wasted searching for an unused process ID in
this range. 

In Linux 2.4 and earlier, the process ID limit of 32,767 is defined by the
kernel constant PID_MAX.  With Linux 2.6, things change. While the default upper
limit for process IDs remains 32,767, this limit is adjustable via the value in
the Linux-specific /proc/sys/kernel/pid_max file (which is one greater than the
    maximum process ID). On 32-bit platforms, the maximum value for this file is
32,768, but on 64-bit platforms, it can be adjusted to any value up to 222
(approximately 4 million), making it possible to accommodate very large numbers
of processes.


={============================================================================
*kt_linux_core_101* process-creation

LPI 24.

{process-creation}
The wait(&status) system call has `two-purposes`. First, if a child of this
process has not yet terminated by calling exit(), then wait() suspends execution
of the process until one of its children has terminated. Second, the termination
status of the child is returned in the status argument of wait().

<fork-and-exec>
The UNIX approach is usually simpler and more elegant. Separating these two
steps makes the APIs simpler and allows a program a great degree of flexibility
in the actions it performs between the two steps. Moreover, it is often useful
to perform a fork() without a following exec().

Parent process running program A
      |
      A
      |
      child PID = fork(void);         Child process running program A. 
                                      0 = fork(void);
      |                                     |
Parent may perform other atcions here       A  
      |                                     | 
      wait(&status); // optional      Child may perform further actions here
                                            |
                                            execev( B, ... ); // optional
                                            |
                                            B
                                            |
                                      Execution of program B
                                            |
                                            exit(status);
                                            1. Child status passed to parent 
                                            and kernel restarts parent.
                                            2. Delivers SIGCHLD optionally.

<pid-of-child>
The PID of parent remains the same but child is not. Then when child pid is
assigned and what pid 0 check in the code is? 

The following idiom is sometimes employed when calling fork():

/* Used in parent after successful fork() to record PID of child */
pid_t childPid; 

switch (childPid = fork()) {
  case -1: /* fork() failed */
    /* Handle error */
  case 0: /* Child of successful fork() comes here */
    /* Perform actions specific to child */
  default: /* Parent comes here after successful fork() */
    /* Perform actions specific to parent */
}

note 
child pid is assigned immediately after fork and has the same name in ps. So pid
== 0 check is to distinguish execution path in the same code in parent.

$ pid (26337): before fork:  
pid (26337): in parent before sleep:  
pid (26338): in child before sleep:  

$ ps   
  PID TTY          TIME CMD
26131 pts/10   00:00:00 bash
26337 pts/10   00:00:00 a.out
26338 pts/10   00:00:00 a.out
26339 pts/10   00:00:00 ps


<sharing-between-parent-and-child>
The child's stack, data, and heap segments are initially exact duplicates of the
corresponding parts the parent's memory. The child receives duplicates of all of
the parent's file descriptors.


<race-condition-after-fork>
After a fork(), it is indeterminate which of the two is next scheduled to use
CPU. If we need to guarantee a particular order, we must use some kind of
synchronization technique.

<case-use-signal>
Avoiding Race Conditions by Synchronizing with Signals. Although, in practice,
         such coordination is more likely to be done using semaphores, file
         locks, or message passing. 


={============================================================================
*kt_linux_core_102* process-termination

LPI 25.

{two-ways}
A process may terminate in two general ways. One of these is abnormal
termination, caused by the delivery of a signal whose default action is to
terminate the process (with or without a core dump). Alternatively, a process
can terminate normally, using the _exit() system call.

#include <unistd.h>
void _exit(int status);


{exit-value}
This is exception to the rule that a function with a return type 'must' return a
value. If control reaches the end of main and there is no return, then the
compiler inserts a return of 0.

By convention, a termination status of 0 indicates that a process completed
successfully, and a `nonzero-value` indicates that the process terminated
unsuccessfully. 
  
There are no fixed rules about how nonzero status values are to be interpreted;
SUSv3 specifies two constants, EXIT_SUCCESS (0) and EXIT_FAILURE (1)

To make return value `machine-independent`, use EXIT_FAILURE and EXIT_SUCCESS
from cstdlib header which are preprocessor variables.

Performing an explicit return n is generally equivalent to calling exit(n) since
the run-time function that invokes main() uses the return value from main() in a
call to exit().

Performing a return without specifying a value, or falling off the end of the
main() function, also results in the caller of main() invoking exit(), but with
results that vary depending on the version of the C standard supported and the
compilation options employed:


{exit-call}
Programs generally don't call _exit() directly, but instead call the exit()
library function, which performs various actions before calling _exit().

note:
exit() is library function and _exit() is system call.

#include <stdlib.h>
void exit(int status);

The following actions are performed by exit():

1. Exit handlers (functions registered with atexit() and on_exit()) are called,
  in reverse order of their registration (Section 25.3).

2. The stdio stream buffers are flushed.

3. The _exit() system call is invoked, using the value supplied in status.

Unlike _exit(), which is UNIX-specific, exit() is defined as part of the
standard C library; that is, it is available with every C implementation.


{actions-on-termination}
During both normal and abnormal termination of a process, the following actions
occur:

1. Open file descriptors, directory streams (Section 18.8), message catalog descriptors
(see the catopen(3) and catgets(3) manual pages), and conversion descriptors
(see the iconv_open(3) manual page) are closed.

2. As a consequence of closing file descriptors, any file locks (Chapter 55) held by
this process are released.

<3>. Any attached System V shared memory segments are detached, and the shm_nattch counter
corresponding to each segment is decremented by one.

4. For each System V semaphore for which a semadj value has been set by the process,
that semadj value is added to the semaphore value. (Refer to Section 47.8.)

<5>. If this is the controlling process for a controlling terminal, then the SIGHUP signal is sent to
each process in the controlling terminal's foreground process group, and the terminal is
disassociated from the session.

6. Any POSIX named semaphores that are open in the calling process are closed as though sem_close()
  were called.

7. Any POSIX message queues that are open in the calling process are closed as though mq_close()
  were called.

8. If, as a consequence of this process exiting, a process group becomes orphaned and there are any
stopped processes in that group, then all processes in the group are sent a SIGHUP signal followed
by a SIGCONT signal. We consider this point further in Section 34.7.4.

9. Any memory locks established by this process using mlock() or mlockall() (Section 50.2) are
removed.

10. Any memory mappings established by this process using mmap() are unmapped.


{exit-handler}


{stdio-buffers-and-exit}

#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

int main(int argc, char** argv)
{
  printf("Hello world.\n");
  write( STDOUT_FILENO, "Ciao\n", 5);

  // setbuf( stdout, NULL );

  if( -1 == fork() )
  {
    fprintf( stderr, "fork() failed\n" );
    exit(EXIT_FAILURE);
  }

  exit(EXIT_SUCCESS);
}

$ gcc sample-two.c 
$ ./a.out 
Hello world.
Ciao

$ ./a.out > a
$ cat a
Ciao
Hello world.
Hello world.


<q> Why different output order and duplicates?

1. duplicates.
Recall that the stdio buffers are maintained in a process's user-space memory
(refer to 13.2).  Therefore, these buffers are duplicated in the child by
fork(). When standard output is directed to a terminal, it is `line-buffered` by
default, with the result that the newline-terminated string written by printf()
  appears 'immediately'. 

However, when standard output is directed to a file, it is `block-buffered` by
default. Thus the string written by printf() is still in the parent's stdio
buffer at the time of the fork(), and this string is duplicated in the child.
When the parent and the child later call exit(), they both flush their copies of
the stdio buffers, resulting in duplicate output.

To remedy this, can use setbuf() and fflush() to flush the stdio buffer prior to
a fork() call.  However, still have different order.

$ ./a.out 
Hello world.
Ciao

$ ./a.out > a
$ cat a
Ciao
Hello world.

2. the output order.
The write() transfers data directly to a kernel buffer, and this buffer is not
duplicated during a fork(). note: write is a system call.

The output of write() appears before that from printf() because the output of
write() is immediately transferred to the kernel buffer cache, while the output
from printf() is transferred only when the stdio buffers are flushed by the call
to exit().

note: In general, care is required when mixing stdio functions and system calls
to perform I/O on the same file, as described in Section 13.7.


={============================================================================
*kt_linux_core_103* process-monitor-child-process

LPI 26.  

<wait-system-call>
The wait() system call waits for one of the children of the calling process to
terminate and returns the termination status of that child in the buffer pointed
to by status.

#include <sys/wait.h>
pid_t wait(int *status);

If no (previously unwaited-for) child of the calling process has yet terminated,
   the call blocks until one of the children terminates. If a child has already
     terminated by the time of the call, wait() returns immediately.

As its function result, wait() returns the process ID of the child that has
terminated.

On error, wait() returns -1. One possible error is that the calling process has
no previously unwaited-for children, which is indicated by the errno value
ECHILD. This means that we can use the following loop to wait for all children
of the calling process to terminate:

while ((childPid = wait(NULL)) != -1)
  continue;
if (errno != ECHILD) /* An unexpected error... */
  errExit("wait");

<waitpid-system-call>
The wait() system call has a number of limitations, which waitpid() was designed
to address. See the reference for more details.

#include <sys/wait.h>
pid_t waitpid(pid_t pid, int *status, int options);


={============================================================================
*kt_linux_core_104* process-zombie *ex-interview*

2014.02 from google phone interview. 

The lifetimes of parent and child processes are usually not the same-either the
parent outlives the child or vice versa. This raises two questions: 

1. Who becomes the parent of an `orphaned-child`? 

Each process has a parent-the process that created it. If a child process
becomes orphaned because its "birth" parent terminates, then the child is
adopted by the `init-process`, and subsequent calls to getppid() in the child
return 1. See Section 26.2. The process 1, init, the ancestor of all processes.
The init process adopts the child and automatically performs a wait(), thus
removing the zombie process from the system.

2. What happens to a child that terminates before its parent has had a chance to
perform a wait()?

The point here is that, although the child has finished its work, the parent
should still be permitted to perform a wait() at some later time to determine
how the child terminated. The kernel deals with this situation by 'turning' the
child into a `zombie`. This means that most of the resources held by the child
are released back to the system to be reused by other processes. 

The only part of the process that remains is an entry in the kernel's process
table recording; among other things the child's process ID, termination status,
and resource usage statistics. Section 36.1.

A zombie process can't be killed by a signal, not even the (silver bullet)
SIGKILL. This ensures that the parent can always eventually perform a wait().
When the parent does perform a wait(), the kernel removes the zombie, since the
last remaining information about the child is no longer required.


<why-zombie-can-be-a-problem>
If a parent creates a child, but fails to perform a wait(), then an entry for
the zombie child will be maintained indefinitely in the kernel's process table.
If a large number of such zombie children are created, they will eventually fill
the kernel process table, 'preventing' the creation of new processes. 

note:
zombie-process is different from orphaned-process since parent is still alive
but fails to wait. So killing parent makes zombie orphaned process and then
adopted by init-process.

Since the zombies can't be killed by a signal, the only way to remove them from
the system is to kill their parent (or wait for it to exit), at which time the
zombies are adopted and waited on by init, and consequently removed from the
system.

note:
TODO to check code to see how to makes zombies.

$ ./make_zombie
Parent PID=1013
Child (PID=1014) exiting
  1013 pts/4 00:00:00 make_zombie Output from ps(1)
  1014 pts/4 00:00:00 make_zombie <defunct>
After sending SIGKILL to make_zombie (PID=1014):
  1013 pts/4 00:00:00 make_zombie Output from ps(1)
  1014 pts/4 00:00:00 make_zombie <defunct>

In the above output, we see that ps(1) displays the string <defunct> to indicate
a process in the zombie state.

A common way of reaping dead child processes is to establish a handler for the
SIGCHLD signal.


={============================================================================
*kt_linux_core_105* process-memory-layout

LPI 6. This is a layout in virtual memory.

<process-virtual-memory-address> figure 6.1
The ** areas represent invalid ranges in the process's virtual address space;
that is, areas for which page tables have 'not' been created.

High           +----------------+
                  ** kernel. mapped into process virtual memory but 
                             not accessible to program.
               +----------------+                  0xC0000000
                  argv, environ
               +----------------+
                  stack (grows down)
top of stack   +----------------+
                  ** unallocated
               +----------------+
                  heap (grows up)
               +----------------+ < &end
                  bss
               +----------------+ < &edata
                  inited data
               +----------------+ < &etext
                  text
               +----------------+
                  **
Low            +----------------+                  0x00000000

note: what's the kernel in this diagram?

/proc/kallsyms provides addresses of kernel symbols in this region. /proc/ksyms
in kernel 2.4 and earlier.

<text-segment> 
The text segment contains the machine-language instructions of the program run
by the process. The text segment is made `read-only` so that a process doesn't
accidentally modify its own instructions via a bad pointer value. Since many
processes may be running the same program, the text segment is made 'sharable'
so that a single copy of the program code can be mapped into the virtual address
space of all of the processes.

<initialized-data-segment>
The initialized data segment contains 'global' and 'static' variables that are
explicitly initialized. The values of these variables are read from the
executable file when the program is loaded into memory.

<uninitialized-data-segment> <bss>
The uninitialized data segment contains 'global' and 'static' variables that are
'not' explicitly initialized. Before starting the program, the system
initializes all memory in this segment to 0.  For historical reasons, this is
often called the bss segment, a name derived from an old assembler mnemonic for
"block started by symbol." 

The main reason for placing global and static variables that are initialized
into a separate segment from those that are uninitialized is that, when a
program is stored on disk, it is not necessary to allocate space for the
uninitialized data. Instead, the executable merely needs to record the location
and size required for the uninitialized data segment, and this space is
allocated by the program loader at 'runtime'.


<stack>
The stack is a dynamically growing and shrinking segment containing stack
frames. One stack frame is allocated for each currently called function. A frame
stores the function's local variables (so-called automatic variables),
       arguments, and return value. Stack frames are discussed in more detail in
         Section 6.5.

<heap>
The heap is an area from which memory (for variables) can be dynamically
allocated at run time. The top end of the heap is called the program break.

<segment-and-section>
Sometimes, the term section is used instead of segment, since section is more
consistent with the terminology used in the now ubiquitous ELF specification for
executable file formats.

<etext>
Three global symbols: etext, edata, and end. These symbols can be used from
within a program to obtain the addresses of the next byte past, respectively,
the end of the program text, the end of the initialized data segment, and the
  end of the uninitialized data segment.

<size-command>
The size(1) command displays the size of the text, initialized data, and
uninitialized data (bss) segments of a binary executable.

$ size a.out
   text	   data	    bss	    dec	    hex	filename
   3552	    356	    148	   4056	    fd8	a.out


={============================================================================
*kt_linux_core_106* process-virtual memory

{virtual-memory} locality-of-reference
The aim of this virtual memory is to make efficient use of both the CPU and RAM
(physical memory) by exploiting a property that is typical of most programs:
`locality-of-reference`. Most programs demonstrate two kinds of locality:

1. 'spatial' locality is the tendency of a program to reference memory addresses
that are 'near' those that were recently accessed (because of 'sequential'
    processing of instructions, and, sometimes, sequential processing of data
    structures).

2. 'temporal' locality is the tendency of a program to access the 'same' memory
addresses in the near future that it accessed in the recent past (because of
    loops).

The upshot of locality of reference is that it is possible to execute a program
while maintaining only 'part' of its address space in RAM.

<paging>
A virtual memory scheme splits the memory used by each program into small,
  fixed-size units called `pages`. Correspondingly, RAM is divided into a series
  of page frames of the same size. At any one time, only some of the pages of a
  program need to be resident in physical memory page frames; these pages form
  the so-called `resident-set`. Copies of the unused pages of a program are
  maintained in the `swap-area` - a reserved area of disk space used to
  supplement the computer's RAM - and loaded into physical memory only as
  required. 

<page-fault>
When a process references a page that is 'not' currently 'resident' in physical
memory, a `page-fault` occurs, at which point the kernel suspends execution of
the process while the page is loaded from disk into memory.

The kernel maintains a `page-table` for `each-process` (Figure 6-2). The page
table describes the location of each page in the process's virtual address space
(the set of all virtual memory pages available to the process). 

process virtual address space    page table        physical memory(RAM) page frames 
low   page 0                           4                    0
      page 1                           2                    1
      page 2                           7                    2
high  page 3                           0                    3

Each entry in the page table either 'indicates' the location of a virtual page
in RAM or indicates that it currently resides on disk.

Not all address ranges in the process's virtual address space require page-table
entries. Typically, large ranges of the potential virtual address space are
unused, so that it isn't necessary to maintain corresponding page-table entries.

If a process tries to access an address for which there is 'no' corresponding
page-table entry, it receives a SIGSEGV signal. *sigseg*

note:
page-fault is different from seg-fault *sigseg* since page-fault happens when
finds an entry in page table but not in the resident-set so raise page-fault to
signal that reads from swap-space. But seg-fault happens when no entry in page
table.


{valid-virtual-addresses}
A process's range of 'valid' virtual addresses can change over its lifetime, as
the kernel allocates and deallocates pages (and page-table entries) for the
process.

* as the stack grows downward beyond limits previously reached;

* when memory is allocated or deallocated on the heap, by raising the program
  break using brk(), sbrk(), or the malloc family of functions

* when System V shared memory regions are attached using shmat() and detached
  using shmdt()

* when memory mappings are created using mmap() and unmapped using munmap()


{advantages}

* Processes are `isolated` from one another and from the kernel, so that one
  process can't read or modify the memory of another process or the kernel. This
  is accomplished by having the page-table entries for each process point to
  distinct sets of physical pages in RAM (or in the swap area).

* Where appropriate, two or more processes can share memory. The kernel makes
  this possible by having page-table entries in different processes refer to the
  same pages of RAM. Memory sharing occurs in two common circumstances:

  - Multiple processes executing the same program can share a single (readonly)
  copy of the program code. This type of sharing is performed implicitly when
  multiple programs execute the same program file (or load the same shared
      library).

  - Processes can use the shmget() and mmap() system calls to explicitly request
  sharing of memory regions with other processes. This is done for the purpose
  of interprocess communication.

* The implementation of `memory-protection` schemes is facilitated; that is,
  pagetable entries can be marked to indicate that the contents of the
  corresponding page are readable, writable, executable, or some combination
  of these protections. Where multiple processes share pages of RAM, it is
  possible to specify that each process has different protections on the
  memory; for example, one process might have read-only access to a page,
  while another has read-write access.  
       
* Because only a part of a program needs to reside in memory, the program loads
  and runs faster.  Furthermore, the memory footprint (i.e., virtual size) of a
  process can exceed the capacity of RAM.

* One final advantage of virtual memory management is that since each process
  uses less RAM, more processes can simultaneously be held in RAM. This
  typically leads to better CPU utilization, since it increases the likelihood
  that, at any moment in time, there is at least one process that the CPU can
  execute.


{kernel-stack}
Sometimes, the term "user stack" is used to distinguish the stack we describe
here from the "kernel stack". The kernel stack is a `per-process` memory region
maintained in kernel memory that is used as the stack for execution of the
functions called internally during the execution of a `system-call`.  The kernel
can't employ the user stack for this purpose since it resides in unprotected
user memory.


{stack-frame}
Each (user) stack frame contains the following information:

* Function arguments and local variables: 

In C these are referred to as automatic variables, since they are automatically
created when a function is called. These variables also automatically disappear
when the function returns (since the stack frame disappears), and this forms the
primary semantic distinction between automatic and static (and global)
  variables: the latter have a permanent existence independent of the execution
  of functions.

* `call-linkage` information: 

Each function uses certain CPU registers, such as the program counter, which
points to the next machine-language instruction to be executed. Each time one
function calls another, a copy of these registers is saved in the called
function's stack frame so that when the function returns, the appropriate
register values can be restored for the calling function.

Referring to Listing 6-1, during the execution of the function square(), the
stack will contain frames as:

+----------------+
 frames for c run-time startup functions  note:
+----------------+
 frame for main
+----------------+
 frame for doCalc()
+----------------+
 frame for square() 
+----------------+      <- sp. grows down


={============================================================================
*kt_linux_core_107* linux-process-environment-list

LPI. 2.7 Processes

Each process has an environment list, which is a set of environment variables
that are maintained within the user-space memory of the process. Each element
of this list consists of a name and an associated value. When a new process is
created via fork(), it inherits a copy of its parent's environment. Thus, the
environment provides `a mechanism for a parent process to communicate`
information to a child process.  When a process replaces the program that it
is running using exec(), the new program either inherits the environment used
by the old program or receives a new environment specified as part of the
exec() call.


{in-process}
As shown in Figure 6-1, the argv and environ arrays, as well as the 'strings'
they initially point to, 'reside' in a single contiguous area of memory just
above the process stack.


{command-arguments}
The command-line arguments of any process can be read via the Linux-specific
/proc/PID/cmdline file, with each argument being terminated by a null byte. A
program can access its own command-line arguments via /proc/self/cmdline.


<name-value-pair>
Each process has an associated array of strings called the environment list,
     or simply the environment. Each of these strings is a definition of the
     form name=value.


<copied>
When a new process is created, it inherits a copy of its parent's environment.
This is a primitive but frequently used form of interprocess communication;
the environment provides a way to transfer information from a parent process
to its child(ren). Since the child gets a copy of its parent's environment at
the time it is created, this transfer of information is one-way and once-only.


{shell-export-command}
A value can be added to the environment using the export command. The above
commands permanently add a value to the shell's environment, and this
environment is then inherited by all child processes that the shell creates.
At any point, an environment variable can be removed with the unset command

$ export SHELL=/bin/bash

In the Bourne shell and its descendants, the following syntax can be used to
add values to the environment used to execute a single program, without
affecting the parent shell (and subsequent commands):

$ NAME=value program

This adds a definition to the environment of `just the child process`
executing the named program. If desired, multiple assignments (delimited by
    white space) can precede the program name.


{proc-pid-environ}
The environment list of any process can be examined via the Linux-specific /proc/PID/environ file,
    with each NAME=value pair being terminated by a null byte.


{get-set-env-call}
The getenv() function retrieves individual values from the 'process' environment.

#include <stdlib.h>

char *getenv(const char *name);

Returns pointer to (value) string, or NULL if no such variable

The setenv() function is an alternative to putenv() for adding a variable to the environment.

int setenv(const char *name, const char *value, int overwrite);
int unsetenv(const char *name);

Returns 0 on success, or -1 on error

int putenv(char *string);

Returns 0 on success, or nonzero on error

The string argument is a pointer to a string of the form name=value. After the putenv() call, this
string is part of the environment. In other words, rather than duplicating the string pointed to by
string, one of the elements of environ will be set to point to the same location as string.

The setenv() function 'creates' a new environment variable by allocating a memory buffer for a
string of the form name=value, and copying the strings pointed to by name and value into that
buffer. note: this is why setenv() is preferable to putenv() since putenv() do not allocate.

The setenv() function doesn't change the environment if the variable identified by name already
exists and overwrite has the value 0. If overwrite is nonzero, the environment is always changed.

The unsetenv() function removes the variable identified by name from the environment.
note: from man page, The unsetenv() function 'deletes' the variable name from the environment. If
name does not exist in the environment, then the function succeeds, and the environment is
unchanged.

On occasion, it is useful to erase the entire environment, and then rebuild it with selected values.
For example, we might do this in order to execute set-user-ID programs in a secure manner (Section
        38.8). We can erase the environment by assigning NULL to environ:

environ = NULL;

This is exactly the step performed by the clearenv() library function.

#define _BSD_SOURCE /* Or: #define _SVID_SOURCE */
#include <stdlib.h>

int clearenv(void)

Returns 0 on success, or a nonzero on error

<memory-leaks>
In some circumstances, the use of setenv() and clearenv() can lead to memory leaks in a program. We
noted above that setenv() allocates a memory buffer that is then made part of the environment. When
we call clearenv(), it doesn't free this buffer (it can't, since it doesn't know of the buffer's
        existence). A program that repeatedly employed these two functions would steadily leak
memory. 

In practice, this is unlikely to be a problem, because a program typically calls clearenv() just
once on startup, in order to remove all entries from the environment that it inherited from its
predecessor (i.e., the program that called exec() to start this program).


$ ./modify_env "GREET=Guten Tag" SHELL=/bin/bash BYE=Ciao
GREET=Guten Tag
SHELL=/bin/bash

$ ./modify_env SHELL=/bin/sh BYE=byebye
SHELL=/bin/sh
GREET=Hello world


#define _GNU_SOURCE /* To get various declarations from <stdlib.h> */
#include <stdlib.h>
#include "tlpi_hdr.h"

extern char **environ;

int main(int argc, char *argv[])
{
    int j;
    char **ep;

    // note: do not 'free' actually
    clearenv(); /* Erase entire environment */

    for (j = 1; j < argc; j++)
        if (putenv(argv[j]) != 0)
            errExit("putenv: %s", argv[j]);

    // note: when no GREET in arguments, this will create one but no call to free it?
    if (setenv("GREET", "Hello world", 0) == -1)
        errExit("setenv");

    // note: when BYE is from argv, how dose this free it if unsetenv() do delete it?
    unsetenv("BYE");

    for (ep = environ; *ep != NULL; ep++)
        puts(*ep);

    exit(EXIT_SUCCESS);
}


<inter-process-program>
Sometimes, it is useful for a process to modify its environment. One reason is to make a change that
is visible in all child processes subsequently created by that process.  Another possibility is that
we want to set a variable that is visible to a new program to be loaded into the memory of this
process (“execed”). In this sense, the environment is not just a form of interprocess communication,
        but also a method of interprogram communication.


={============================================================================
*kt_linux_core_107* process: group

LPI 34.

{why-group-and-session}
A process group is a collection of 'related' processes, and a session is a collection of 'related'
process groups. Process groups and sessions are abstractions defined to 'support' two uses:

1. shell job control, which allows interactive users to run commands in the foreground or in the
background. note: So need to know fine details when write a shell.

2. send a signal to a group.


{process-group} also known as job
A process group is a set of one or more processes sharing the same process group identifier (PGID).
A process group ID is a number of the same type (pid_t) as a process ID. A process group has a
process group 'leader', which is the process that 'creates' the group and whose process ID becomes
the process group ID of the group. A new process inherits its parent's process group ID.

note: the group leader has PID == PGID.

A process group has a lifetime, which is the period of time beginning when the leader creates the
group and ending when the last member process leaves the group.


{process-session}
A session is a collection of process groups. A process's session membership is determined by its
session identifier (SID), which, like the process group ID, is a number of type pid_t. A session
'leader' is the process that 'creates' a new session and whose process ID becomes the session ID. A
new process inherits its parent's session ID.

note: the session leader has PID == PGID == SID. That is group and session leader.


{controlling-process-and-terminal}
All of the processes in a session 'share' a single controlling terminal. The controlling terminal is
established when the 'session' leader first 'opens' a terminal device, /dev/tty. A terminal may be
the controlling terminal of at most 'one' session. Opening the controlling terminal also causes the
session leader to become the 'controlling' process for the terminal.

<forground-process-group>
At any point in time, one of the process groups in a session is the foreground process group for the
terminal, and the others are background process groups. 'only' processes in the foreground process
group can read input from the controlling terminal. note: process'es'

<signal-forground-group>
When the user types one of the signal-generating terminal characters on the controlling terminal, a
signal is sent to 'all' members of the 'foreground' process group. These characters are the
interrupt character (usually Control-C), which generates SIGINT; the quit character (usually
    Control-\), which generates SIGQUIT; and the suspend character (usually Control-Z), which
    generates SIGTSTP.


{use-other-case} 
Process groups occasionally find uses in areas other than job control, since they have two useful
properties: a parent process can wait on any of its children in a particular process group (Section
  26.1.2), and a signal can be sent to all of the members of a process group.

note: Use kill system call and see <signal-to-process-group> for detail.


{use-job-control} login-shell
Why session and group? Sessions and process groups were defined to support 'shell' job control

For an interactive login, the controlling terminal is the one on which the user logs in. The login
shell becomes the session leader and the controlling process for the terminal, and is also made the
'sole' member of its own process group. 

'each' job(a simple command or pipeline of commands) started from the shell results in the creation
of one or more processes, and the shell places all of these processes in a 'new' process group.

$ echo $$                              " Display the PID of the shell
400
$ find / 2> /dev/null | wc -l &        " Creates 2 processes in background group
[1] 659
$ sort < longlist | uniq -c            " Creates 2 processes in foreground group

<---------------------------- session 400 ----------------------------------->
bash (session/group leader)      find (group leader)        sort(group leader)
   PID  = 400                       PID  = 658                 PID  = 660
   PPID = 399                       PPID = <400>               PPID = <400>
   PGID = 400                       PGID = 658                 PGID = 660
   SID  = 400                       SID  = 400                 SID  = 400

                                 wc                         uniq
                                    PID  = 659                 PID  = 661
                                    PPID = <400>               PPID = <400>
                                    PGID = 658                 PGID = 660
                                    SID  = 400                 SID  = 400

<-------------------------->    <-------------------->    <------------------>
process group 400                process group 658          process group 660
'controlling' process
background process group         background group           forground group

<controlling-terminal>
Forground PGID    = 660
Controlling SID   = 400


{group}
<calls>
The setpgid() system call changes the process group of the process whose process ID is pid to the
value specified in pgid. Put simply, change pgid of a process with pid.

#include <unistd.h>
pid_t getpgrp(void);
Always successfully returns process group ID of calling process

int setpgid(pid_t pid, pid_t pgid);
Returns 0 on success, or -1 on error

If pid is specified as 0, the 'calling' process's process group ID is changed. If pgid is specified
as 0, then the process group ID of the process specified by pid is made the same as its process ID.
Thus, the following setpgid() calls are equivalent:

setpgid(0, 0);
setpgid(getpid(), 0);
setpgid(getpid(), getpid());

note: Put simply, this call is to change pgid of a process with the pid. However, when pid is 0, it
is for the calling process.

If the pid and pgid arguments specify the 'same' process, then a 'new' process group is created, and
the specified process is made the 'leader' of the new group. The typical callers of setpgid() and
setsid() are programs such as the shell 

If the two arguments specify different values, then setpgid() is being used to move a process
between process groups.

<group-creation>
The setpgid(0,0) is a way to create a new group and a group leader once a process is created such as
fork as shown example below.

However, from LPI 34-1 code:

childPid = fork();
switch (childPid) 
{
  case -1: /* fork() failed */
  /* Handle error */

  case 0: /* Child */
    if (setpgid(0, pipelinePgid) == -1)
    /* Handle error */
    /* Child carries on to exec the required program */

   default: /* Parent */
      if (setpgid(childPid, pipelinePgid) == -1 && errno != EACCES)
        /* Handle error */
        /* Parent carries on to do other things */
}

and 

Each job(a simple command or pipeline of commands) started from the shell results in the creation of
one or more processes, and the shell places all of these processes in a 'new' process group.

note: Q? 'not' clear when and how a group is created? The <example> seems to make more sense.

note: The above code is to show "How a job-control shell sets the process group ID of a child
process" because the scheduling of the parent and child is indeterminate after a fork(). 

Therefore, job-control shells are programmed so that the parent and the child process both call
setpgid() to change the childâs process group ID to the same value immediately after a fork(), and
the parent ignores any occurrence of the EACCES error on the setpgid() call.

So seems that the group creation is done before doing this.

<restrictions>
1. The pid argument may specify only the calling process or one of its children. Violation of this
rule results in the error ESRCH.

2. A process may 'not' change the process group ID of one of its children 'after' that child has
performed an exec(). Violation of this rule results in the error EACCES. The rationale for this
constraint is that it could confuse a program if its process group ID were changed after it had
commenced.

This restriction affects the programming of job-control shells, which have the following
requirements:

<sent-by-shell>
All of the processes in a job (i.e., a command or a pipeline) must be placed in a single process
group. This step permits the 'shell' to use killpg() (or, kill() with a negative pid argument) to
simultaneously send job-control signals to 'all' of the members of the process group. Naturally this
step must be carried out before any job-control signals are sent.

killpg - send signal to a process group

#include <signal.h>
int killpg(int pgrp, int sig);

<example>
The thing is that when a shell runs this line, creates a child process to run this application and
in this application, it make a new background group and make it itself a group leader.

note: In the first time, scripts runs "else" and set PPID with the pid that runs this script

#!/bin/bash

# to clean up A/V resources when a process dies, especially on a crash.
#
this_script=$0
prefix=@prefix@
parent_pid=${NEXUS_INSPECT_PARENT_PID:-}

if [ -n "${parent_pid}" ];
then
        echo "Going to watch pid: ${parent_pid}"
        while kill -0 "${parent_pid}" &>/dev/null;
        do
                usleep 500
        done
        echo "Pid ${parent_pid} has died. Going to cleanup."
        ${prefix}/bin/nexus-inspect -r -p "${parent_pid}"
else
        [ -x "${prefix}/bin/nexus-inspect" ] && \
            NEXUS_INSPECT_PARENT_PID=$$ \
               ${prefix}/bin/setpgid-and-exec bash "${this_script}" &
        exec "${@}"
fi


/*
 * The purpose of this simple program is to launch a program in a new process
 * group and have it become the group leader.
 *
 * This is useful in situations where the launched program fork()s and we want
 * to be sure that when we kill it, all of it's children are also killed.
 */

note: Q? does this mean that if kill group leader then all children will be killed automatically? 

#include <unistd.h>
#include <error.h>
#include <stdio.h>

int main(int argc, char* argv[]) {

  if (argc < 2) {
    fprintf(stderr, "Usage: %s <program> [ <arg> ... ]", argv[0]);
    exit(1);
  }

  /* Create new group and become group leader */
  if (setpgid(0, 0)) {
    perror("setpgid failed!");
    exit(2);
  }

  /* Execute the command */
  if (execvp(argv[1], (char*const*)(argv+1)) != 0) {
    perror("exec failed!");
    exit(3);
  }
}


{session}
<calls>
If the calling process is 'not' a process group leader, setsid() creates a 'new' session.

#include <unistd.h>
pid_t setsid(void);

Returns session ID of new session, or (pid_t) -1 on error

The setsid() system call creates a new session as follows:

1. The calling process becomes the leader of a new session, and is made the leader of a new process
group within that session. The calling processâs process group ID and session ID are set to the same
value as its process ID. note: this is a login shell example.

2. The calling process has 'no' controlling terminal. Any previously existing connection to a
controlling terminal is broken. note: Upon creation, a session has no controlling terminal

<restriction>
If the calling process is a process group leader, setsid() fails with the error EPERM. The simplest
way of ensuring that this doesn't happen is to perform a fork() and have the parent exit while the
child carries on to call setsid(). Since the child inherits its parent's process group ID and
receives its own unique process ID, it can't be a process group leader.

The restriction that group leader cannot call setsid() is necessary because, without it, the process
group leader would be able to place itself in another (new) session,

<example>
Shows the use of setsid() to create a new session. To check that it no longer has a controlling
terminal, this program attempts to open the special file /dev/ tty

$ ps -p $$ -o 'pid pgid sid command'      # $$ is PID of shell
PID PGID SID COMMAND
12243 12243 12243                         # bash PID, PGID, and SID of shell

$ ./t_setsid
$ PID=12352, PGID=12352, SID=12352
ERROR [ENXIO Device not configured] open /dev/tty

As can be seen from the output, the process successfully places itself in a new process group within
a new session. Has no controlling terminal, the open() call fails.

int
main(int argc, char *argv[])
{
  if (fork() != 0) /* Exit if parent, or on error */
    _exit(EXIT_SUCCESS);

  if (setsid() == -1)
    errExit("setsid");

  printf("PID=%ld, PGID=%ld, SID=%ld\n", (long) getpid(),
      (long) getpgrp(), (long) getsid(0));

  if (open("/dev/tty", O_RDWR) == -1)
    errExit("open /dev/tty");

  exit(EXIT_SUCCESS);
}

note: From this example, shell is not a group leader and this shows how a shell creates a new
session.


{SIGHUP-signal}
SIGHUP
When a terminal disconnect (hangup) occurs, this signal is sent to the controlling process of the
terminal. A second use of SIGHUP is with daemons (e.g., init, httpd, and inetd). Many daemons are
designed to respond to the receipt of SIGHUP by reinitializing themselves and rereading their
configuration files. The system administrator triggers these actions by manually sending SIGHUP to
the daemon, either by using an explicit kill command or by executing a program or script that does
the same.

The default action of SIGHUP is to terminate a process (by kernel). The delivery of SIGHUP to the
controlling process can set off a kind of chain reaction, resulting in the delivery of SIGHUP to
many other processes. 

This may occur in two ways:

First, in a login session, the shell is normally the controlling process for the terminal.  Most
shells are programmed so that, when run interactively, they establish a handler for SIGHUP. This
handler terminates the shell, but beforehand sends a SIGHUP signal to each of the process groups
(both foreground and background) created by the shell. 

note: sends a SIGHUP signal to only groups that the shell created. The below example shows that.

note: NOT understand this in 34.6.1 since if not handle SIGHUP then kernel will terminate all
anyway. So why install a hander and a shell send a signal? 

<because> Since only a foreground process can read input from the controlling terminal and receive
terminal generated signals. 

Second, if delivery of SIGHUP results in termination of a controlling process, then the kernel also
sends SIGHUP to all of the members of the 'foreground' process group of the controlling terminal.

note: SIGHUP is to terminate by default. So both terminates all children.

<example>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <signal.h>

static void
handler(int sig)
{
}

int main(int argc, char *argv[])
{
  pid_t childPid;
  struct sigaction sa;

  setbuf(stdout, NULL); /* Make stdout unbuffered */

  sigemptyset(&sa.sa_mask);
  sa.sa_flags = 0;
  sa.sa_handler = handler;

  if (sigaction(SIGHUP, &sa, NULL) == -1)
  {
    fprintf(stderr, "error: sigaction\n");
    exit(1);
  }

  childPid = fork();
  if (childPid == -1)
  {
    fprintf(stderr, "error: fork\n");
    exit(1);
  }

  if (childPid == 0 && argc > 1)
    if (setpgid(0, 0) == -1) /* Move to new process group */
    {
      fprintf(stderr, "error: setpgid\n");
      exit(1);
    }

  printf("PID=%ld; PPID=%ld; PGID=%ld; SID=%ld\n", (long) getpid(),
      (long) getppid(), (long) getpgrp(), (long) getsid(0));

  /* An unhandled SIGALRM ensures this process will die if nothing else terminates it */
  alarm(60);

  for(;;) {
    /* Wait for signals */
    pause();
    printf("%ld: caught SIGHUP\n", (long) getpid());
  }
}

The output on debian linux:

kpark@wll1p04345:~/work$ ps -o pgid $$
 PGID
 7523

kpark@wll1p04345:~/work$ echo $$
7523

kpark@wll1p04345:~/work$ ./a.out > samegroup.log 2>&1 &
[1] 16098
kpark@wll1p04345:~/work$ ./a.out x > diffgroup.log 2>&1  
[1]+  Alarm clock             ./a.out > samegroup.log 2>&1
Alarm clock

kpark@wll1p04345:~/work$ cat samegroup.log 
PID=16098; PPID=7523; PGID=16098; SID=7523         " shell, 7523 created a new process and group
PID=16099; PPID=16098; PGID=16098; SID=7523        " child

kpark@wll1p04345:~/work$ cat diffgroup.log         
PID=16231; PPID=7523; PGID=16231; SID=7523         " shell created a new process and group.
PID=16232; PPID=16231; PGID=16232; SID=7523        " child created a new group as well.

note: Unlike example output in 34.6.1, there are no output like:

$ cat samegroup.log
PID=5612; PPID=5611; PGID=5611; SID=5533  "child. This example shows that child runs first.
PID=5611; PPID=5533; PGID=5611; SID=5533  "parent
5611: caught SIGHUP
5612: caught SIGHUP

WHY NOT? The problem is the above application is not a shell and bash doc says:

The shell exits by default upon receipt of a SIGHUP. Before exiting, an interactive shell resends
the SIGHUP to all jobs, running or stopped. Stopped jobs are sent SIGCONT to ensure that they
receive the SIGHUP.

So when runs above commands, exit the shell by pressing C-D or close terminal. The output is:

kpark@wll1p04345:~/work$ cat samegroup.log 
PID=12104; PPID=7523; PGID=12104; SID=7523
PID=12105; PPID=12104; PGID=12104; SID=7523
12104: caught SIGHUP
12105: caught SIGHUP

kpark@wll1p04345:~/work$ cat diffgroup.log 
PID=15299; PPID=7523; PGID=15299; SID=7523
PID=15300; PPID=15299; PGID=15300; SID=7523
15299: caught SIGHUP
15299: caught SIGHUP


{session-leader-is-controlling-process}
As a consequence of establishing the connection to (i.e., opening) the controlling terminal, the
'session' 'leader' becomes the 'controlling' process for the terminal.

<disconnect>
The principal significance of being the controlling process is that the 'kernel' sends 'this'
process a SIGHUP signal if a terminal 'disconnect' occurs. note: say only SIGHUP.

<termination>
As for the termination of the controlling process, the following steps occur: The kernel sends a
SIGHUP signal (and a SIGCONT signal) to all members of the foreground process group, to inform them
of the loss of the controlling terminal.


{io-on-controlling-terminal}
The controlling terminal is inherited by the child of a fork() and preserved across an exec(). When
a session leader opens a controlling terminal, it becomes controlling process.

<terminal-driver>
To support job control, the terminal driver maintains a record of the foreground process group for
the controlling terminal. The terminal driver delivers job-control signals to the foreground job
when certain characters are typed. These signals either terminate or stop the foreground job.

<read>
The notion of the terminal's foreground job is also used to arbitrate terminal I/O requests. Only
processes in the foreground job may read from the controlling terminal. Background jobs are
prevented from reading by delivery of the SIGTTIN signal, whose default action is to stop the job. 

note: process'es'

SIGTTIN
When running under a job-control shell, the terminal driver sends this signal to a background
process 'group' when it attempts to read() from the terminal. This signal stops 'a' process by
default.

note: group but not process? when one of process in background group then get this signal.

<write>
If the terminal TOSTOP is set, then background jobs are also prevented from writing to the
controlling terminal by delivery of a SIGTTOU signal, whose default action is to stop the job.

SIGTTOU
This signal serves an analogous purpose to SIGTTIN, but for terminal output by background jobs. When
running under a job-control shell, if the TOSTOP (terminal output stop) option has been enabled for
the terminal (perhaps via the command stty tostop), the terminal driver sends SIGTTOU to a
background process group when it attempts to write() to the terminal (see Section 34.7.1).  This
signal stops a process by default.


={============================================================================
*kt_linux_core_108* process-daemon

LPI 37.

{characteritics}
A daemon is a process with the following characteristics:

1. It runs in the background and has 'no' controlling terminal. The lack of a controlling terminal
ensures that the kernel 'never' automatically generates any job-control or terminal-related signals
(such as SIGINT, SIGTSTP, and SIGHUP) for a daemon.

2. Many standard daemons run as privileged processes (i.e., effective user ID of 0),

3. It is a 'convention' (not universally observed) that daemons have names ending with the letter d.
On Linux, certain daemons are run as kernel threads. The code of such daemons is part of the kernel,
   and they are typically created during system startup. When listed using ps(1), the names of these
   daemons are surrounded by square brackets ([]). One example of a kernel thread is pdflush, which
   periodically flushes dirty pages (e.g., pages from the buffer cache) to disk.


{steps-to-create-daemon}

1. Perform a fork(), after which the parent exits and the child continues. As a consequence, the
daemon becomes a child of the init process. note: zombie.

This is done for two reasons:

@ Assuming the daemon was started from the command line, the parent's termination is noticed by the
shell, which then displays another shell prompt and leaves the child to 'continue' in the
background.

@ To guarantee not to be a process group leader, since it inherited its process group ID from its
parent and obtained its own unique process ID, which differs from the inherited process group ID.

2. The child process calls setsid() to start a new session and free itself of any association with a
controlling terminal.

note: From step 1, it is guaranteed not to be a group leader and this is necessary since only a
process which is not a group leader can create a new session and this has no controlling terminal.
See process: group: session.

3. If the daemon never opens any terminal devices thereafter, then we don't need to worry about the
daemon reacquiring a controlling terminal.

4.5. See 37.2

6. Close all open file descriptors that the daemon has inherited from its parent. A daemon may need
to keep certain inherited file descriptors open, so this step is optional, or open to variation.

This is done for a variety of reasons. Since the daemon has lost its controlling terminal and is
running in the background, it makes 'no' sense for the daemon to keep file descriptors 0, 1, and 2
open if these refer to the terminal. 

Furthermore, we can't 'unmount' any file systems on which the long-lived daemon holds files open.
And, as usual, we should close unused open file descriptors because file descriptors are a finite
resource.

7. After having closed file descriptors 0, 1, and 2, a daemon normally opens /dev/null and uses
dup2() (or similar) to make all those descriptors refer to this device.

note: this is why cannot see output when use sandbox?

This is done for two reasons:

@ It ensures that if the daemon calls library functions that perform I/O on these descriptors, those
functions won't unexpectedly fail. note: due to step 6.

@ It prevents the possibility that the daemon later opens a file using descriptor 1 or 2, which is
then written to-and thus corrupted-by a library function that expects to treat these descriptors as
standard output and standard error.

TODO: Once figure out the daemon implementation and compare it with listing 37.1.


{reinit-daemon}
The fact that many daemons should run continuously presents a couple of programming hurdles:

@ Typically, a daemon reads operational parameters from an associated configuration file on startup.
Sometimes, it is desirable to be able to change these parameters "on the fly," 'without' needing to
stop and restart the daemon.

@ Some daemons produce log files. If the daemon never closes the log file, then it may grow
endlessly, eventually clogging the file system. noted that even if we remove the last name of a
file, the file continues to exist as long as any process has it open. What we need is a way of
telling the daemon to close its log file and open a new file, so that we can rotate log files as
required.

The solution to both of these problems is to have the daemon establish a handler for SIGHUP, and
perform the required steps upon receipt of this signal.

Why is it possible? Since a daemon has no controlling terminal, the kernel never generates this
signal for a daemon. Therefore, daemons can use SIGHUP for the purpose described here.


{syslog}
note: no syslogd on a embedded system.

<why>
When writing a daemon, one problem we encounter is how to display error
messages. Since a daemon runs in the background, we can't display messages on
an associated terminal, as we would typically do with other programs. 

One possible alternative is to write messages to an application-specific log
file, as is done in the program in Listing 37-3. The main problem with this
approach is that it is difficult for a system administrator to manage multiple
application log files and monitor them all for error messages. 

The syslog facility was devised to address this problem which provides a
single, centralized logging facility that can be used to log messages by 'all'
applications on the system.


Figure 37-1: Overview of system logging 

printf uses syslog 2 and /proc/kmsg to log and klogd uses
syslog 3 to log in to /dev/log.

Kernel                            User process
  |                                 |
  | printk                          | syslog(3)
  | syslog(2), /proc/kmsg           |
  |                                 |
klogd                               |
  |                                 |
  | syslog(3)                       |
  |                                 |
  +---------------------------------+-------
                                |
                             /dev/log
                    Unix domain datagram socket
                                |
                            syslogd


<daemon-and-conf>
The syslog facility has two principal components: the syslogd daemon and the
syslog(3) library function.

The System Log daemon, `syslogd`, accepts log messages from two different
sources: a UNIX domain socket, /dev/log, which holds locally produced
messages, and (if enabled) an Internet domain socket (UDP port 514), which
holds messages sent across a TCP/IP network.

Each message processed by syslogd has a number of `attributes`, including a
`facility`, which specifies the `type of program` generating the message, and
a `level`, which specifies the `severity` (priority) of the message. The
syslogd daemon examines the facility and level of each message, and then
passes it along to any of several possible destinations according to the
dictates of an associated configuration file, /etc/syslog.conf. 

Possible destinations include a terminal or virtual console, a disk file, a
FIFO, one or more (or all) logged-in users, or a process (typically another
    syslogd daemon) on another system connected via a TCP/IP network. (Sending
      the message to a process on another system is useful for reducing
      administrative overhead by consolidating messages from multiple systems
      to a single location.) A single message may be sent to multiple
    destinations (or none at all), and messages with different combinations of
    facility and level can be targeted to different destinations or to
    different instances of destinations (i.e., different consoles, different
        disk files, and so on).


The `syslog(3)` library function can be used by any process to log a message.
This function, which we describe in detail in a moment, uses its supplied
arguments to construct a message in a standard format that is then placed on
the /dev/log socket for reading by syslogd.


<klogd>
An alternative source of the messages placed on /dev/log is the Kernel Log
daemon, `klogd`, which collects kernel log messages (produced by the kernel
    using its printk() function). 

These messages are collected using either of two equivalent Linux-specific
interfaces-the `/proc/kmsg` file and the syslog(2) 'system' call-and then
placed on /dev/log using the syslog(3) 'library' function.


Although `syslog(2)` and `syslog(3)` share the same name, they perform quite
different tasks. An interface to syslog(2) is provided in glibc under the name
klogctl(). Unless explicitly indicated otherwise, when we refer to syslog() in
this section, `we mean syslog(3)`


<syslog2>
SYSLOG(2)                  Linux Programmer’s Manual                 SYSLOG(2)

NAME
       syslog, klogctl - read and/or clear kernel message ring buffer; set console_loglevel

SYNOPSIS
       int syslog(int type, char *bufp, int len);
                       /* No wrapper provided in glibc */

       /* The glibc interface */
       #include <sys/klog.h>

       int klogctl(int type, char *bufp, int len);

DESCRIPTION
       If you need the C library function syslog() (which talks to
           syslogd(8)), then look at syslog(3).  
       
       The system call of this name is about controlling the kernel printk()
  buffer, and the glibc version is called klogctl().


   The kernel log buffer
       The kernel has a cyclic buffer of length LOG_BUF_LEN in which messages
       given as arguments to the kernel function printk() are stored
       (regardless of their loglevel).  In early ker- nels,  LOG_BUF_LEN  had
       the value 4096; from kernel 1.3.54, it was 8192; from kernel 2.1.113 it
       was 16384; since 2.4.23/2.6 the value is a kernel configuration option.
       In recent kernels the size can be queried with command type 10.

   The loglevel
       The  kernel  routine printk() will only print a message on the console,
       if it has a loglevel less than the value of the variable
       console_loglevel.  This variable initially has the value
       DEFAULT_CONSOLE_LOGLEVEL (7), but is set to 10 if the kernel command
       line contains the word "debug", and to 15 in case of a kernel fault
       (the 10 and 15 are just  silly,  and equivalent to 8).  This variable
       is set (to a value in the range 1-8) by the call syslog(8,dummy,value).
       The calls syslog(type,dummy,dummy) with type equal to 6 or 7, set it to
       1 (kernel panics only) or 7 (all except debugging messages),
       respectively.

       Every text line in a message has its own loglevel.  This level is
       DEFAULT_MESSAGE_LOGLEVEL - 1 (6) unless the line starts with <d> where
       d is a digit in the range  1-7,  in  which case the level is d.  The
       conventional meaning of the loglevel is defined in <linux/kernel.h> as
       follows:

       #define KERN_EMERG    "<0>"  /* system is unusable               */
       #define KERN_ALERT    "<1>"  /* action must be taken immediately */
       #define KERN_CRIT     "<2>"  /* critical conditions              */
       #define KERN_ERR      "<3>"  /* error conditions                 */
       #define KERN_WARNING  "<4>"  /* warning conditions               */
       #define KERN_NOTICE   "<5>"  /* normal but significant condition */
       #define KERN_INFO     "<6>"  /* informational                    */
       #define KERN_DEBUG    "<7>"  /* debug-level messages             */
       
       note:
       How to set level? Depends on words contained in a line.


<syslog3>
#include <syslog.h>

void openlog(const char *ident, int log_options, int facility);
void syslog(int priority, const char *format, ...);

<facility-and-level>
The priority argument is created by ORing together a facility value and a
level value. The `facility` indicates the general `category` of the application
logging the message, and is specified as one of the values listed in Table
37-1. If omitted, the facility defaults to the value specified in a previous
openlog() call, or to LOG_USER if that call was omitted.

       LOG_AUTH       security/authorization messages
       LOG_AUTHPRIV   security/authorization messages (private)
       ...

The `level` value indicates the `severity` of the message, and is specified as one
of the values in Table 37-2. All of the level values listed in this table
appear in SUSv3.

       LOG_EMERG      system is unusable
       LOG_ALERT      action must be taken immediately
       LOG_CRIT       critical conditions
       LOG_ERR        error conditions
       LOG_WARNING    warning conditions
       LOG_NOTICE     normal, but significant, condition
       LOG_INFO       informational message
       LOG_DEBUG      debug-level message

One difference from printf() is that the format string doesn't need to include
a terminating newline character. Also, the format string may include the
2-character sequence %m, which is replaced by the error string correspond- ing
to the current value of errno (i.e., the equivalent of strerror(errno)).


={============================================================================
*kt_linux_core_110* process-execution, exec call

LPI27

{execve-system-call}
Various library functions, all with names beginning with exec, are layered on
top of the execve() system call. Each of these functions provides a different
interface to the same functionality. The loading of a new program by any of
these calls is commonly referred to as an exec operation, or simply by the
notation exec().

#include <unistd.h>
int execve(const char *pathname, char *const argv[], char *const envp[]);

'never' returns on success; returns -1 on error

<attribute-that-remains>
After an execve(), the process ID of the process remains the same, because the
same process continues to exist.

<never-return>
Never need to check the return value from execve(); it will always be -1 if get
since the very fact that it returned informs us that an error occurred,

<ex>
note: See how envs are passed to.

$ ./t_execve ./envargs 
argv[0] = envargs 
argv[1] = hello world 
argv[2] = goodbye 
env: GREET=salut 
env: BYE=adieu 

// t_execve.c

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

int main(int argc, char** argv)
{
  char *argVec[10];
  char *envVec[] = { "GREET=salut", "BYE=adieu", NULL };

  if( argc != 2 || strcmp( argv[1], "--help" ) == 0 )
    printf("%s pathname\n", argv[0] );

  // get basename
  argVec[0] = strrchr( argv[1], '/' );
  if( argVec[0] != NULL )
    argVec[0]++;
  else
    argVec[0] = argv[1];

  // note: ternimate with NULL and argVec[] is passed as a whole to execed
  // application.

  argVec[1] = "hello world";
  argVec[2] = "goodbye";
  argVec[3] = NULL;

  execve( argv[1], argVec, envVec );

  printf("if we get here, someting wrong.\n" );
  exit(EXIT_FAILURE);
}

// envarg.c

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

extern char **environ;

int main(int argc, char** argv)
{
  int j;
  char **ep;

  for( j = 0; j < argc; j++ )
    printf("argv[%d] = %s \n", j, argv[j] );

  for( ep = environ; *ep != NULL; ep++ )
    printf("env: %s \n", *ep );

  exit(EXIT_SUCCESS);
}


{exec-library}
The library functions perfors an exec() and all are layered on top of execve().

#include <unistd.h>

int execle(const char *pathname, const char *arg, ...  
    /* , (char *) NULL, char *const envp[] */ );
int execlp(const char *filename, const char *arg, ...  
    /* , (char *) NULL */);
int execvp(const char *filename, char *const argv[]);
int execv(const char *pathname, char *const argv[]);
int execl(const char *pathname, const char *arg, ...  /* , (char *) NULL */);

None of the above returns on success; all return -1 on error

<path-filename>
execvp() allow the program to be specified using just a filename. The filename
is sought in the list of directories specified in the PATH environment variable.
The functions contain the letter p for PATH to indicate this.

The PATH variable is 'not' used if the filename contains a slash (/), in which
case it is treated as a relative or absolute pathname.

The PATH value for a login shell is set by system-wide and user-specific shell
startup scripts.  Since a child process inherits a copy of its parent's
environment variables, each process that the shell creates to execute a command
inherits a copy of the shell's PATH.

<env>
The execve() and execle() functions allow the programmer to 'explicitly' specify
the environment for the new program using envp, a NULL-terminated array of
pointers to character strings. The names of these functions end with the letter
e (for environment) to indicate this fact. All of the other exec() functions use
the caller's existing environment (i.e., the contents of environ) as the
environment for the new program.

<ex>
This is to two things: parent is to run actual application given and child is to
check if a parent is alive in a loop. If parent is died, do some action.

1. shell runs this line in a script. 
note: this cmd comes from outside to specify the application to run.

   exec $exec_wrapper "$cmd" args...
   
2. $exec_wrapper is an application, exec-then-cleanup-app, which do two things: 
   @ ppid from env is null and set ppid of shell. 
   @ call fork

   @ child sets env with ppid and 'exec' wrapper again. since env is set, child
   runs in a loop while checking if parent is died. If so, 'exec' cleanup
   application with ppid. 

   note: ppid comes from env and argv as well. argv's one is not used.

   @ parent 'exec' $cmd to run the given application such as a browser.

shell: exec(wrapper);

  // 'ppid' is null and set ppid with shell's pid
  wrapper: ppid = getpid();    

  wrapper: fork()
    -> child: setenv(PPID);
       child: exec(wrapper);

       // run 'wrapper' code again but with 'ppid' set this time before exec and
       // this is <inter-program> communication using envronment variable.

       // will have different pid from parent which comes from fork() but not
       // exec.

       wrapper: do watch
       wrapper: if parent is died, exec(cleanup);

    -> parent: exec(application);
       // has the same pid which is the shell's pid and run application code.


note: see how exec used to replace "program code(text)" to run. the shell,
  parent, replace itself with given application and the chain of exec in the
  child.

exec "$prefix/bin/exec-then-cleanup-app" "$prefix/bin/w3cEngine" \
$enable_webkit_remote_debugging_as_needed \
-cache "$app_data_dir/client-cache" \
-cache-size "$cacheSize" \
-jar "$app_data_dir/cookies.sqlite" \
-url "$url" \
-src "$sources"

// exec-then-cleanup-app

#include <unistd.h>
#include <signal.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <assert.h>
#include <sys/stat.h>
#include <sys/types.h>

void usage(const char* exec_name)
{
  fprintf(stderr, "Usage: %s cmd [args]\n\n"
                  "Run `cmd` passing it `args` and run inspect"
                  " when that completes\n", basename(exec_name));
}

int main(int argc, char** argv)
{
  pid_t pid;
  int opt = 0;

  // note: getenv returns NULL if an env is not set.

  char* parent_pid_str = getenv("NEXUS_INSPECT_PARENT_PID");
  char* cleanup_exe = PKG_BIN_DIR "/nexus-inspect";

  if (NULL == parent_pid_str && argc < 2)
  {
    usage(argv[0]);
    return 1;
  }

  if (NULL == parent_pid_str)
  {
    // Even if pids were unsigned 64-bit numbers, there could be a maximum
    // of 20 characters in it. We add another for the null terminator.
    char parent_pid[21];
    int ret = snprintf(parent_pid, sizeof(parent_pid), "%i", getpid());
    if (ret >= 20)
    {
      // If you're on a system with pids that are larger than unsigned
      // 64-bit integers, you should seriously consider why you're still
      // using nexus-inspect.
      fprintf(stderr, "Pids are longer than expected (ie %d characters)."
          " What's going on?", ret);
      return 2;
    }

    pid = fork();
    assert(!(pid < 0));
    if (0 == pid)
    {
      // Child.

      // Only launch the watcher if the cleanup app exists.
      struct stat sb;
      if (-1 == stat(cleanup_exe, &sb))
      {
        fprintf(stderr, "Cleanup executable (%s) doesn't exist, so not" 
            " spawning watcher\n", cleanup_exe);
        return 0;
      }

      // Create new group and become group leader.
      if (setpgid(0, 0)) {
        perror("setpgid failed!");
        return 3;
      }

      // "$prefix/bin/exec-then-cleanup-app" "-p" "$parent_pid"
      // args: /opt/zinc-trunk/bin/exec-then-cleanup-app -p 1479 

      // note: ppid via args is not used.

      const char* args[4] = { argv[0], "-p", parent_pid, NULL };

      // We set the pid in the environment. Doing it via environment rather than
      // arguments simplifies the code because we don't have to do any argument
      // parsing - note that we do accept arguments and options, but they are
      // *all* passed on to the exec'd process (see the parent branch, below).

      setenv("NEXUS_INSPECT_PARENT_PID", parent_pid, 1);
      execv(argv[0], (char*const*)args);
    }
    else
    {
      // Parent.
      if (execvp(argv[1], (char*const*)(argv+1)) != 0) {
        perror("exec failed!");
        return 4;;
      }
    }
  }
  else
  {
    // note: even if got ppid in number, have to use ppid string to pass over
    // cleanup_exe

    int parent_pid=atoi(parent_pid_str);
    fprintf(stderr, "Going to watch pid: %d\n", parent_pid);
    while (0 == kill(parent_pid, 0))
    {
      sleep(1);
    }
    fprintf(stderr, "Pid %d has died. Going to cleanup.\n", parent_pid);

    char* args[5] = { cleanup_exe, "-r", "-p", parent_pid_str, NULL };
    return execv(cleanup_exe, args);
  }

  return 0;
}


{pass-arguments}
To run "strace -e open /bin/ls" to see open system calls when ls runs, but was
not straightforward. Why?

./a.out /usr/bin/strace

#include <stdio.h>
#include <stdlib.h>

int main(int argc, char* argv[] )
{
    if( argc != 2 || strcmp( argv[1], "--help" ) == 0 )
    {
        printf("%s pathname\n", argv[0] );
        exit(EXIT_FAILURE);
    }

    // const char* args[] = {"ls", "-al", ".", NULL };
    // note: works fine

    // const char* args[] = {"strace", "-e open", "/bin/ls", NULL };
    // note: error when run "strace: invalid system call ` open'"

    const char* args[] = {"strace", "-e", "open", "/bin/ls", NULL };
    // note: works fine

    if( execvp( argv[1], args ) != 0 )
    {
        // if (execvp("/bin/strace", (char*const*)(args)) != 0) {
        perror("exec failed!");
        return 4;;
    }

    exit(EXIT_FAILURE);
}


={============================================================================
*kt_linux_core_110* process-resource

LPI36

36.2 Process Resource Limits

Each process has a set of resource limits that can be used to restrict the
amounts of various system resources that the process may consume. For example,
we may want to set resource limits on a process before execing an arbitrary
  program, if we are concerned that it may consume excessive resources. 

We can set the resource limits of the shell using the ulimit built-in command
(limit in the C shell). These limits are inherited by the processes that the
shell creates to execute user commands.

Since kernel 2.6.24, the Linux-specific `/proc/PID/limits` file can be used to
view all of the resource limits of any process. This file is owned by the real
user ID of the corresponding process and its permissions allow reading only by
that user ID (or by a privileged process).

$ cat /proc/3576/limits 
Limit                     Soft Limit           Hard Limit           Units     
Max cpu time              unlimited            unlimited            seconds   
Max file size             unlimited            unlimited            bytes     
Max data size             unlimited            unlimited            bytes     
Max stack size            8388608              unlimited            bytes     
Max core file size        0                    unlimited            bytes     
Max resident set          unlimited            unlimited            bytes     
Max processes             3941                 3941                 processes 
Max open files            1024                 4096                 files     
Max locked memory         65536                65536                bytes     
Max address space         unlimited            unlimited            bytes     
Max file locks            unlimited            unlimited            locks     
Max pending signals       3941                 3941                 signals   
Max msgqueue size         819200               819200               bytes     
Max nice priority         0                    0                    
Max realtime priority     0                    0                    
Max realtime timeout      unlimited            unlimited            us        


struct rlimit {
  rlim_t rlim_cur; /* Soft limit (actual process limit) */
  rlim_t rlim_max; /* Hard limit (ceiling for rlim_cur) */
};

These fields correspond to the two associated limits for a resource: the soft
(rlim_cur) and hard (rlim_max) limits. (The rlim_t data type is an integer
    type.) 

The soft limit governs the amount of the resource that may be consumed by the
process. A process can adjust the soft limit to any value from 0 up to the hard
limit. For most resources, the sole purpose of the hard limit is to provide this
'ceiling' for the soft limit.

In most cases, resource limits are enforced for both privileged and unprivileged
processes. They are inherited by child processes created via fork() and are
preserved across an exec().


Although a resource limit is a per-process attribute, in some cases, the limit
is measured against not just that process’s consumption of the corresponding
resource, but also against the sum of resources consumed by all processes with
the same real user ID. The RLIMIT_NPROC limit, which places a limit on the
number of processes that can be created, is a good example of the rationale for
this approach.

If not otherwise specified, then a resource limit is measured 'only' against the
process's own consumption of the resource.


In this section, we provide details on each of the resource limits available on
Linux, noting those that are Linux-specific.

RLIMIT_AS
The RLIMIT_AS limit specifies the maximum size for the process's virtual memory
(address space), in bytes. Attempts (brk(), sbrk(), mmap(), mremap(), and
    shmat()) to exceed this limit fail with the error ENOMEM. In practice, the
most common place where a program may hit this limit is in calls to functions in
the malloc package, which make use of sbrk() and mmap(). Upon encountering this
limit, stack growth can also fail with the consequences listed below for
RLIMIT_STACK.


RLIMIT_STACK
The RLIMIT_STACK limit specifies the maximum size of the process stack, in
bytes. Attempts to grow the stack beyond this limit result in the generation of
a `SIGSEGV` signal for the process. Since the stack is exhausted, the only way
to catch this signal is by establishing an alternate `signal-stack`, as
described in Section 21.3.

Since Linux 2.6.23, the RLIMIT_STACK limit also determines the amount of space
available for holding the process's command-line arguments and environment
variables. See the execve(2) manual page for details.


RLIM_INFINITY
What dose it really mean?


={============================================================================
*kt_linux_core_110* process-resource-case

The daemon crashes when it shows huge numbers in vm memory status and shows a
call trace in coredump. Believe that it has no memory to create a thread stack.
See RLIMIT_STACK and SIGSEGV. *sigseg*

VmPeak:	 1893244 kB
VmSize:	 1535856 kB

#0  0x77399418 in stack_list_add () from /lib/libpthread.so.0
#1  0x7739af1c in pthread_create () from /lib/libpthread.so.0
#2  0x773e1488 in direct_thread_create () from /usr/local/lib/libdirect.so
#3  0x76d6cb70 in fusion_enter () from /usr/local/lib/libfusion-1.4.so.17
#4  0x7742e05c in dfb_core_create () from /usr/local/lib/libdirectfb.so
#5  0x774100a8 in DirectFBCreate () from /usr/local/lib/libdirectfb.so

libpthread/nptl/pthread_create.c

int
__pthread_create_2_1 (
     pthread_t *newthread,
     const pthread_attr_t *attr,
     void *(*start_routine) (void *),
     void *arg)
{
  STACK_VARIABLES;

  const struct pthread_attr *iattr = (struct pthread_attr *) attr;
  if (iattr == NULL)
    /* Is this the best idea?  On NUMA machines this could mean
       accessing far-away memory.  */
    iattr = &default_attr;

  struct pthread *pd = NULL;
  int err = ALLOCATE_STACK (iattr, &pd);
  ...
}

static int
allocate_stack (const struct pthread_attr *attr, struct pthread **pdp,
		ALLOCATE_STACK_PARMS)
{
	  /* And add to the list of stacks in use.  */
	  stack_list_add (&pd->list, &stack_used);
}


={============================================================================
*kt_linux_core_050* process-function-reentrant

From LPI signal.

Because a signal handler may asynchronously interrupt the execution of a program
at any point in time, the main program and the signal handler in effect form
`two-independent` (although not concurrent) threads of execution within the same
process.


A function is said to be `reentrant` if it can safely be simultaneously executed
by multiple threads of execution in the `same-process`. In this context, "safe"
means that the function achieves its expected result, regardless of the state of
execution of any other thread of execution.

A function may be `nonreentrant` if it updates global or static data structures. A
function that employs only local variables is guaranteed to be reentrant because
of race-condition. This book shows an example using crypt() in both main and
signal handler. This corrupts internal buffer which is statically allocated and
crypt uses when calls it with differnt parameter.

note:
`nonreentrant` means that function is not safe when gets called by multiple
threads. Using only local variables means reentrant. Why mentions race
condition?

Such possibilities are in fact rife within the standard C library. For example,
     we already noted in Section 7.1.3 that malloc() and free() maintain a
     linked list of freed memory blocks available for reallocation from the
     heap. If a call to malloc() in the main program is interrupted by a signal
     handler that also calls malloc(), then this linked list can be corrupted.
     For this reason, the malloc() family of functions, and other library
     functions that use them, are nonreentrant.

Even if we are not using nonreentrant library functions, reentrancy issues can
still be relevant. If a signal handler updates programmer-defined global data
structures that are also updated within the main program, then we can say that
the signal handler is nonreentrant with respect to the main program.

If a function is nonreentrant, then its manual page will normally provide an
explicit or implicit indication of this fact. In particular, watch out for
statements that the function uses or returns information in statically allocated
variables.


={============================================================================
*kt_linux_core_150* thread-and-process
  
LPI 29

A process is an instance of an executing program and a thread is an instance of
an executing a task. In other words, a process is `processor-abstract`, an
abstract entity defined by kernel and with allocated resources in order to
execute a program.  

UNIX programs have a single thread of execution: the CPU processes instructions
for a single logical flow of execution through the program. In a multithreaded
  program, there are multiple, independent, concurrent logical
    `flows-of-execution` within the same process.

note:
A process is an instance of `processor-abstract` and a thread is an instance of
single flow of execution.

<on-linux>
Two virtualisation in Linux: processer and memory.

Linux don't distinguish a process(task) with a thread. The thread is just a
special process: 

* Linux kernel scheduler schedules based on 'thread'.
 
* Linux kernel has a double-linked list which has <thread_info> struct element
  which has thread_struct*.

* Thread is special since it shares resource (open files, pending signals,
  internal kernel data, process state, address space, text and data section)
  with others threads.

note:
A process is a single thread that don't share resources.

This is clear when see how to create a thread:

clone( CLONE_VM| CLONE_FS| CLONE_FILES| CLONE_SIGHAND, 0);

to create a process:

clone(SIGCHLD, 0);

Two usual steps to create a process:

fork();  // copy a child from a parent and actually use clone() call
exec*()  // load a new program text.


{thread-vs-process-model}

<PM>
* It is difficult to share information between processes. Since the parent and
  child don't share memory other than the read-only text segment, we must use
  IPC.

* Process creation with fork() is relatively expensive although 'copy-on-write'
  is used. The need to duplicate various process attributes such as page tables
  and file descriptor tables means that a fork() call is still time-consuming.

<TM>
* Sharing information between threads is easy and fast since it is just a matter
  of copying data into shared (global or heap) variables. However, needs
  syncronization.

* Thread creation is faster because many of the attributes that must be
  duplicated in a child created by fork() are `instead-shared` between threads
  such as page table.


<TM-disadvantages>
* Buggy thread can damage all of the threads in the process. less protection.
* More threads means more memory and `context-switching`.
 
* more efforts to ensure thread-safe
* each thread is competing for use of the finite virtual address space of a host process.
* desirable to avoid the use of signals in multi-threaded since requires careful designs.
// * should run the same program.


{shared-and-not-shared-between-threads}
From CH29 in {ref-LPI} and {ref-UNP}

<shared-attributes>
The attributes that are shared; in other words, global attributes to a process:

the same global memory, process code and most data;
process ID and parent process ID;
process group ID and session ID;
controlling terminal
process credentials (user and group IDs);

open file descriptors;
signal dispositions;
file system-related information: umask, current working directory, and root directory;
resource limits;

<not-shared-attributes> thread specific
thread ID (Section 29.5);
signal mask;
thread-specific data (Section 31.3);
alternate signal stack (sigaltstack());
the `errno` variable;
realtime 'scheduling' policy and priority (Sections 35.2 and 35.3);
capabilities (Linux-specific, described in Chapter 39); and
'stack' (local variables and function call linkage information).
'registers' including PC and SP


{multithreaded-vs-singlethreaded}
The mutiltithreaded means that a single process has multiple threads. The
singlethreaed means that a single process has a single thread. MT has less IPC
but prone to error because shares resources; less protection. ST has more IPC
but more protection. 

note: tradeoff between IPC and protection.


{user-and-kernel-stack}
The kernel stack is a per-process memory region maintained in kernel memory that
is used as the stack for execution of the functions called internally during the
execution of a system call.

Each (user) stack frame contains the following information:

* Function arguments and local variables
* Some registers. RA.

note: needs more about difference?


={============================================================================
*kt_linux_core_151* thread-sample

#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <pthread.h>

static void * threadFunc(void *arg)
{
    const char *s = (char *) arg;
    printf("%s", s);
    printf("sleeps for 30s\n");
    sleep(30);
    printf("wakes up\n");
    return (void *) strlen(s);
}

int main(int argc, char *argv[])
{
    pthread_t t1;
    void *res;
    int s;
    s = pthread_create(&t1, NULL, threadFunc, "Hello world\n");
    if (s != 0)
    {
        printf("pthread_create failed");
        exit(EXIT_FAILURE);
    }

    printf("Message from main()\n");

    s = pthread_join(t1, &res);
    if (s != 0)
    {
        printf("pthread_join failed");
        exit(EXIT_FAILURE);
    }

    printf("Thread returned %ld\n", (long) res);
    exit(EXIT_SUCCESS);
}


={============================================================================
*kt_linux_core_151* thread-source

POSIX.1 threads approved in 1995 is a POSIX standard for threads. The standard
defines an API for creating and manipulating threads. The two main Linux
threading implementations - LinuxThreads and Native POSIX Threads Library (NPTL)
- deviate from the standard.


{pthread-apis}
http://pubs.opengroup.org/onlinepubs/7990989799/xsh/pthread.h.html
http://pubs.opengroup.org/onlinepubs/7908799/xsh/pthread.h.html

Linux threading models compared: LinuxThreads and NPTL
http://www-128.ibm.com/developerworks/linux/library/l-threading.html?ca=dgr-lnxw07LinuxThreadsAndNPTL

From 2.6, NPTL(New Posix Threading Library) that supports futex(fast user space
mutex) and is part of glibc.


{source}
# from uclibc source

uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\pthread.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\rwlock.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\pthread.h
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\bits\pthreadtypes.h
(_pthread_rwlock_t)


={============================================================================
*kt_linux_core_152* thread-errno
  
LPI 29.2

{global-errno}
When a system call fails, it sets the 'global' integer variable errno to a
positive value that identifies the specific error. Including the <errno.h>
header file provides a declaration of errno, as well as a set of constants for
the various error numbers. All of these symbolic names commence with E.

man errno

EINVAL          Invalid argument (POSIX.1)

Usually, an error is indicated by a return of -1. Thus, a system call can be
checked with code such as the following:

fd = open(pathname, flags, mode); /* system call to open a file */

if (fd == -1) {
   /* Code to handle the error */
}

<print-errnos>
To print an error message based on the errno value. The perror() and strerror()
library functions are provided for this purpose.

#include <stdio.h>
void perror(const char *msg);

The perror() function prints the string pointed to by its msg argument, 'followed' by a message
corresponding to the current value of errno.

#include <string.h>
char *strerror(int errnum);


<when-to-check>
Successful system calls and library functions 'never' reset errno to 0, so this
variable may have a nonzero value as a consequence of an error from a previous
call. Furthermore, SUSv3 permits a successful function call to set errno to a
nonzero value (although few functions do this).

Therefore, when checking for an error, we should always first check if the
function return value indicates an error, and only then examine errno to
determine the cause of the error.


{thread-errno}
The traditional method of returning status from system calls and some library
functions is to return 0 on success and -1 on error, with errno being set to
indicate the error. 

The functions in the pthreads API do things differently. 

<why>
The errno is a global integer variable. However, this doesn't suffice for
threaded programs. If a thread made a call that returned an error in a
`global-errno` variable, then this would confuse other threads that might also
be making calls and checking errno. In other words, race conditions would
result.

<how>
Each thread has its own errno value. On Linux, a `thread-specific-errno` is
achieved in a similar manner to most other UNIX: errno is defined as a 'macro'
that expands into a function call returning a modifiable lvalue that is distinct
for each thread. Since the lvalue is modifiable, it is still possible to write
assignment statements of the form errno = value in threaded programs.

To summarize, the errno mechanism has been adapted for threads in a manner that
leaves error reporting unchanged from the traditional UNIX API.

note: 
After all, errno macro returns a per-thread errno. A program is required to
declare errno by including <errno.h>, which enables the implementation of a
per-thread errno.

<value>
All pthreads functions return 0 on success or a `positive-value` on 'failure'.
The failure value is one of the same values that can be placed in errno by
traditional UNIX system calls. 


{LPI-approach}
errExitEN() 
is the same as errExit(), except that instead of printing the error text corresponding to the
current value of errno, it prints the text corresponding to the error number (thus, the EN suffix)
given in the argument errnum.

Mainly, we use errExitEN() in programs that employ the POSIX threads API. Since:

The traditional method of returning status from system calls and some library functions is to return
0 on success and -1 on error, with errno being set to indicate the error. The functions in the
pthreads API do things differently. All pthreads functions return 0 on success or a positive value
on failure. The failure value is one of the same values that can be placed in errno by traditional
UNIX system calls. 

We could diagnose errors from the POSIX threads functions using code such as the following:

errno = pthread_create(&thread, NULL, func, &arg);
if (errno != 0)
   errExit("pthread_create");

However, this approach is 'inefficient' because errno is defined in threaded programs as a macro that
expands into a 'function' call that returns a modifiable lvalue. 

errExitEN() function allows us to write a more efficient equivalent of the above code:

int s;
s = pthread_create(&thread, NULL, func, &arg);
if (s != 0)
   errExitEN(s, "pthread_create");

note: errExitEN do 'not' use errno which is a function call and user provides it instead. 

TODO: needs to clarify further. 
<one>
From {ref-UNP}. Why is this? Because pthread funcs do not set <errno-var> and return errno instead.
This means that set manually errno before calling err_sys() for example. This util funcs handles
this:

To avoid cluttering the code with braces, use {comma-operator} to combine assignment and the call:

int n;
if(( n = pthread_mutex_lock( &ndone_mutex )) != 0 )
   errno = n, err_sys("pthread_mutex_lock error");

Or

void Pthread_mutex_lock(pthread_mutex_t* mptr)
{
  int n;

  if(( n = pthread_mutex_lock( mptr )) == 0 )
    return;

  errno = n;
  err_sys("pthread_mutex_lock error");
} 

From Appdix C in {ref-UNP}. The reason for our own error funcs is to handle error case with a single
line. See {pthread-util-func} for the use.

if( error condition )
   err_sys( printf format with any number of args );

instead of

if( error condition ) {
  char buff[200];
  snprintf( buff, sizeof(buff), printf format with any number of args );
  perror(buff);
  exit(1);
}


={============================================================================
*kt_linux_core_153* thread-compile

On Linux, programs that use the Pthreads API must be compiled with the cc -pthread option. The
effects of this option include the following:

1. The _REENTRANT preprocessor macro is defined. This causes the declarations of a few reentrant
functions to be exposed.

2. The program is linked with the libpthread library (the equivalent of -lpthread).

gcc -pthread sample.c


={============================================================================
*kt_linux_core_154* thread-create-and-exit

<pthread-create>
#include <pthread.h>

// return 0 if okay, positive Exxx on error which is different from most system calls. 
//
// If need multiple arguments to the function, must package them into a structure and then pass the
// address of this as the single argument to the start function.
//
// The tid argument points to a buffer of type pthread_t into which the unique identifier for
// this thread is 'copied' before pthread_create() returns. This identifier can be used in later
// Pthreads calls to refer to the thread. The arg should be in global or heap.
//
// If attr is NULL, uses default values.
//
// EAGAIN : when cannot create a new thread because exceeded the limit on the number of threads

int pthread_create( pthread_t *tid, const pthread_attr_t *attr, void *(*func)(void*), void *arg);


{pthread-exit}

// The execution of a thread terminates in one of the following ways: 
//
// o The thread's start function performs a return specifying a return value for the thread. This is
// equivalent to pthread_exit()  
// 
// o The thread calls pthread_exit() which can be called in any func called by start func.
//
// o The thread is canceled using pthread_cancel()
//
// * `any-of-the-threads` calls exit(), or the main thread performs a return (in
// the main() function), which causes 'all' threads in the process to terminate
// immediately.
// 
// @retval
//
// Specifies a return value that can be obtained in 'another' thread by calling
// pthread_join(). The value pointed to by retval should 'not' be located on the
// thread's stack, since the contents of that stack become undefined on thread
// termination. The same statement applies to the value given to a return
// statement in the thread's start function.
//
// <continue-after-main-thread-exit> 
// If the main thread calls pthread_exit() instead of calling exit() or
// performing a return, then the other threads continue to execute.

void pthread_exit(void *retval);


={============================================================================
*kt_linux_core_155* thread-join-and-detach

LPI 29

{pthread-join} 

// return 0 if okay, positive Exxx on error. 
//
// To 'wait' for a given thread to terminate. If that thread has already
// terminated, pthread_join() returns 'immediately'.
//
// If retval is a non-NULL pointer, then it receives a copy of the terminated
// thread's return value - that is, the value that was specified when the thread
// performed a return or called pthread_exit().

int pthread_join( pthread_t thread, void **retval );

<join-restriction>
Calling pthread_join() for a thread ID that has been previously joined can lead
to 'unpredictable' behavior; for example, it might instead join with a thread
created later that happened to reuse the same thread ID.


{pthread-detach}

int pthread_detach(pthread_t tid);

Thread is either 'joinable'(the 'default') or 'detached'. 

To make the specified thread detached and often used to detach itself or can
create detached thread when create it by using attr setting.

pthread_detach( pthread_self() );

By default, a thread is joinable, meaning that when it terminates, another
  thread can obtain its return status using pthread_join(). 

Sometimes, we don't care about the thread's return status; we simply want the
system to automatically clean up and remove the thread when it terminates. In
this case, can mark the thread as detached, by making a call to
pthread_detach().

Detaching a thread does 'not' make it 'immune' to a call to `exit()` in another
thread or a return in the main thread. In such an event, all threads in the
process are immediately terminated, 'regardless' of whether they are joinable or
detached. 
    
To put things another way, pthread_detach() simply controls what happens after a
thread terminates, not how or when it terminates.


{zombie-thread}
If a thread is not detached, then we must join with it using pthread_join(). If
not, then, when the thread terminates, it produces the thread 'equivalent of a
`zombie-process`. Aside from wasting system resources, if enough thread zombies
accumulate, we won't be able to create additional threads.


{what-join-do}
The task that pthread_join() performs for threads is similar to that performed
by waitpid() for processes. However, there are some notable differences:

1. Threads are 'peers'. 'any' thread in a process can use pthread_join() to join
with any other thread in the process. For example, if thread A creates thread B,
     which creates thread C, then it is possible for thread A to join with
     thread C, or vice versa. This differs from the 'hierarchical' relationship
     between processes.

When a parent process creates a child using fork(), it is the only process that
can wait() on that child. There is no such relationship between the thread that
calls pthread_create() and the resulting new thread.

2. There is no way of saying "join with any thread" (for processes, we can do
    this using the call waitpid(-1, &status, options)); nor is there a way to do
a nonblocking join (analogous to the waitpid() WNOHANG flag). There are ways to
achieve similar functionality using condition variables; we show an example in
Section 30.2.4.


={============================================================================
*kt_linux_core_156* thread-id

LPI 29

{tid}
Each thread within a process is uniquely identified by a thread ID. This ID is
returned to the caller of pthread_create(), and a thread can obtain its own ID
using pthread_self().

int pthread_equeal( pthread_t tid, pthread_t tid );

Returns nonzero value if t1 and t2 are equal, otherwise 0

pthread_t pthread_self(void);

<why-equal-function-needed> opaque-data
All pthread data type should be treated 'opaque' data. On Linux, pthread_t
happens to be defined as an unsigned long, but on other implementations, it
could be a pointer or a structure.

In NPTL, pthread_t is actually a pointer that has been cast to unsigned long.

Two problems:

1. Therefore, we can't 'portably' use code such as the following to display a
thread ID. Though it does work on many implementations, including Linux, and is
sometimes useful for debugging purposes:

pthread_t thr;
printf("Thread ID = %ld\n", (long) thr);

2. In the Linux threading implementations, thread IDs are unique across
processes. However, this is 'not' necessarily the case on other implementations


{posix-TID-and-kernel-tid}
POSIX thread IDs are 'not' the same as the thread IDs returned by the Linux
specific gettid() system call. POSIX thread IDs are assigned and maintained by
the threading implementation. The thread ID returned by gettid() is a number
(similar to a process ID) that is assigned by the kernel. 

Although each POSIX thread has a unique kernel thread ID in the Linux NPTL
implementation, an application generally doesn't need to know about the kernel
IDs and won't be portable if it depends on knowing them.


{tid-and-ps}

<pthread-self>
NAME
       pthread_self - obtain ID of the calling thread

NOTES
       POSIX.1  allows  an  implementation  wide  freedom  in  choosing  the
       type used to represent a thread ID; for example, representation using
       either an arithmetic type or a structure is permitted.  Therefore,
       variables of type pthread_t can't portably be compared using the C
       equality operator (==); use pthread_equal(3) instead.

       Thread identifiers should be considered opaque: any attempt to use a
       thread ID other than in pthreads calls is nonportable and can lead to
       unspecified results.

       Thread IDs are guaranteed to be unique only within a process.  A thread
       ID may be reused after a terminated thread has been joined, or a detached
       thread has terminated.

       note:
       The thread ID returned by pthread_self() is not the same thing as the
       kernel thread ID returned by a call to gettid(2).

<gettid>

SYNOPSIS
       #include <sys/types.h>

       pid_t gettid(void);

       Note: There is no glibc wrapper for this system call; see NOTES.

NOTES
       Glibc does not provide a wrapper for this system call; call it 'using'
       syscall(2).

       The thread ID returned by this call is not the same thing as a POSIX
       thread ID (i.e., the opaque value returned by pthread_self(3)).

<code>
    cout << "tid:" << syscall(SYS_gettid);


={============================================================================
*kt_linux_core_157* pthread thread id. LPI 29

/*  status must not point to an object that is local to the calling thread.
 *
 *  terminate two other ways:
 *
 *  o thread function returns. return value is the exit status of the thread. {Q} kernal handle and
 *  manage it?
 *
 *  o main thread function returns or any thread call exit/_exit, the process terminates
 *  immediately.
 */
void pthread_exit(void *status);

// A thread may be canceled by any other thread in the same process. For example, if multiple
// threads are started to work on a given task (say finding a record in a database) adn the first
// thread completes the task then cancels the other threds.
//
// To handle the possibility of being canceled, can install(push) and remove(pop) cleanup handlers.
// These handlers are called:
// a) when the thread is canceled by pthread_cancel
// b) when the thread voluntarily terminates (either by calling pthread_exit or returning from its
// thread)

int   pthread_cancel(pthread_t);
void  pthread_cleanup_push(void*), void *);
void  pthread_cleanup_pop(int);


{pthread-attribute} 
To override the default and normally take the detault using the attr arg as a NULL. Attributes are a
way to specify behavior that is different from the default. When a thread is created with
pthread_create or when a synchronization variable is initialized, an attribute object can be
specified. However the default atributes are usually sufficient for most applications. 

Note: Attributes are specified [only] at thread creation time; they cannot be altered while the thread
is being used. {Q} really?

Thus three functions are usually called in tandem when setting attribute

o Thread attibute intialisation 
pthread_attr_init() create a default pthread_attr_t tattr. The function pthread_attr_init() is used
to initialize object attributes to their default values. The storage is allocated by the thread
system during execution. note: once the thread has been creted, the attribute object is no longer
needed, and so is destoryed.

o Thread attribute value change (unless defaults appropriate) 
A variety of pthread_attr_*() functions are available to set individual attribute values for the
pthread_attr_t tattr structure. (see below).  

o Thread creation 
A call to pthread_create() with approriate attribute values set in a pthread_attr_t structure. 
 
<code>

The following code fragment should make this point clearer: 

#include <pthread.h> 

pthread_attr_t tattr; // note: can be a local var
pthread_t tid;
void *start_routine;
void arg
int ret;

ret = pthread_attr_init(&tattr);
ret = pthread_attr_*(&tattr,SOME_ATRIBUTE_VALUE_PARAMETER);
ret = pthread_create(&tid, &tattr, start_routine, arg);
ret = pthread_attr_destroy(&tattr);

In order to save space, code examples mainly focus on the attribute setting functions and the
intializing and creation functions are ommitted. These must of course be present in all actual code
fragtments. 

An attribute object is opaque, and cannot be directly modified by assignments. A set of functions is
provided to initialize, configure, and destroy each object type. Once an attribute is initialized
and configured, it has process-wide scope. The suggested method for using attributes is to configure
all required state specifications at one time in the early stages of program execution. The
appropriate attribute object can then be referred to as needed. Using attribute objects has two
primary advantages: 

First, it adds to code portability. Even though supported attributes might vary between
implementations, you need not modify function calls that create thread entities because the
attribute object is hidden from the interface. If the target port supports attributes that are not
found in the current port, provision must be made to manage the new attributes. This is an easy
porting task though, because attribute objects need only be initialized once in a well-defined
location. 

Second, state specification in an application is simplified. As an example, consider that several
sets of threads might exist within a process, each providing a separate service, and each with its
own state requirements. At some point in the early stages of the application, a thread attribute
object can be initialized for each set. All future thread creations will then refer to the attribute
object initialized for that type of thread. The initialization phase is simple and localized, and
any future modifications can be made quickly and reliably. Attribute objects require attention at
process exit time. When the object is initialized, memory is allocated for it. This memory must be
returned to the system. The pthreads standard provides function calls to destroy attribute objects. 


{pthread-example-in-FOSH}
/* Assert throughout that the POSIX calls worked. If not, HFL cannot be guaranteed to work. */
resPOSIX = pthread_attr_init(&threadAttrs);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* The stacksize attribute defines the minimum stack size (in bytes) allocated for the created
 * threads stack.
 *
 * int pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize);
 */
resPOSIX = pthread_attr_setstack(&threadAttrs, ptThreadInfo->pvStack,(size_t)ptThreadInfo->szStack);
/* If this assert is triggered, it might be because of not aligning address to a boundary of 8. */
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * To set the other scheduling policy: 
 */
resPOSIX = pthread_attr_setschedpolicy(&threadAttrs, SCHED_RR);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * The example to change prio:
 *
 * sched_param param;
 * param.sched_priority = 30;
 * ret = pthread_attr_setschedparam (&tattr, &param);
 * 
 * used to set/inquire a current thread's priority of scheduling.
 * 
 * int pthread_attr_setschedparam(pthread_attr_t *attr, const struct sched_param *param);
 * int pthread_attr_getschedparam(const pthread_attr_t *attr, struct sched_param *param);
 *
 * {Q} {pthread-schedule-at-runtime} is it possible as attr is only at creation?
 */
resPOSIX = pthread_attr_setschedparam(&threadAttrs, &tSchedParam);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);

resPOSIX = pthread_attr_setscope(&threadAttrs, ContentionScope);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* The pthread_attr_setinheritsched() function sets the scheduling parameter inheritance state
 * attribute in the specified attribute object. The thread's scheduling parameter inheritance state
 * determines whether scheduling parameters are explicitly specified in this attribute object, or if
 * scheduling attributes should be inherited from the creating thread. Valid settings for
 * inheritsched include:
 * 
 * PTHREAD_INHERIT_SCHED Scheduling parameters for the newly created thread are the same as those of
 * the creating thread.
 * 
 * PTHREAD_EXPLICIT_SCHED Scheduling parameters for the newly created thread are specified in the
 * thread attribute object.
 */
resPOSIX = pthread_attr_setinheritsched(&threadAttrs, PTHREAD_EXPLICIT_SCHED);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);

resPOSIX = pthread_create(&(ptThreadInfo->idThread), &threadAttrs, pfThreadMain, pvParam);


{pthread-stack} from LPI CH33
Each thread has its own stack whose size is fixed when the thread is created. On
Linux/x86-32, for all threads other than the main thread, the default size of
the per-thread stack is 2 MB. The main thread has a much larger space for stack
growth. Occasionally, it is useful to change the size of a thread's stack. 

The pthread_attr_setstacksize() function sets a thread attribute (Section 29.8)
  that determines the size of the stack in threads created using the thread
  attributes object. The related pthread_attr_setstack() function can be used to
  control both the size and the location of the stack, but setting the location
  of a stack can decrease application portability.

One reason to change the size of per-thread stacks is to allow for larger stacks
for threads that allocate large automatic variables or make nested function
calls of great depth (perhaps because of recursion).

Alternatively, an application may want to reduce the size of per-thread stacks
to allow for a greater number of threads within a process.

The minimum stack that can be employed on a particular architecture can be
determined by calling sysconf(_SC_THREAD_STACK_MIN). For the NPTL implementation
on Linux/x86-32, this call returns the value 16,384.


{nptl}
Linux threading models compared: LinuxThreads and NPTL
http://www-128.ibm.com/developerworks/linux/library/l-threading.html?ca=dgr-lnxw07LinuxThreadsAndNPTL

NPTL(new posix threading library) comes from kernel 2.6 and supports Futex(fast user space mutex).
It is part of glibc.

<how-to-check-nptl-version>
$ getconf GNU_LIBPTHREAD_VERSION
NPTL 2.15

<pid-nptl>
note: Q. In Linux, every thread has a PID and can see when use ps command but in NPTL, thread
group has one PID. is it true?


={============================================================================
*kt_linux_core_102* how to run three threads sequencially

From Cracking the coding interview, p425, 16.5:

Suppose we have following code:

public class Foo {
  public Foo() {...}
  public void first() {...}
  public void second() {...}
  public void third() {...}
}

The same instance of Foo will be passed to three different threads. ThreadA will call first, ThreadB
will call second, and ThreadC will call third. Design a mechanism to ensure that first is called
before second and second is called before third.

Using lock?

public class FooBad {
  public FooBad() {
    lock1 = new ReentrantLock();
    lock2 = new ReentrantLock();

    // locks all in a ctor
    lock1.lock(); lock2.lock(); 
  }
}

// already got lock1
public void first()
{
  ...
  lock1.unlock(); // finished first
}

// already got lock2
public void second()
{
  lock1.lock();   // wait first to finish
  lock1.unlock();
  ...
  lock2.unlock(); // finished second
}

public void third()
{
  lock2.lock();   // wait second to finish
  lock2.unlock();
  ...
}

This WON'T work in JAVA since a lock in JAVA is owned by the same thread which locked in.

http://docs.oracle.com/javase/6/docs/api/java/util/concurrent/locks/ReentrantLock.html
A ReentrantLock is owned by the thread last successfully locking, but not yet unlocking it. A thread
invoking lock will return, successfully acquiring the lock, when the lock is not owned by another
thread. The method will return immediately if the current thread already owns the lock. 

note: There's no such limitation in Linux and it's possible in the same thread group and if threads
in different group use lock, can use semaphore or lock on the shared memory.

The mutex has ownership as well and see {mutex-ownership}.


={============================================================================
*kt_linux_core_103* priority and schedule 

{priority-on-linux}

high           low
0     99 100   139

0  -99  : realtime priority. static and can set when create a process
100-139 : user priority. can use nice command which uses with value from -20 to +19. The default
nice value is 0. note: does it mean 120 is default? 100 is highest?


{realtime-and-latency}
The aim to distribute fairly CPU resource to all process on a system is not realtime approach.

<latency-components>

interrupt      ISR         ISR signals       user process
event          runs        user process      runs
 |             |              |                 |
---------------------------------------------------------> time
   interrupt      interrupt      scheduling
   latency        processing     latency

<no-hard-realtime>
No support in kernel. To use hard realtime, install patch from 
http://people.redhat.com/~mingo/realtime-preempt


{schedule}
Before Linux 2, kernel didn't support preemption which means that no other process can run in kernel
mode when one user process is already in the kernel mode until that is bloked or finishes its work.

<scheduler>
O(1) scheduler from Linux 2.5 and supports constant scheduling decision regardless of the number of
process.

<schedule-policy>
(from ~/include/linux/sched.h)
/*
 * Scheduling policies
 */

// normal user process. fairness
#define SCHED_NORMAL 0     

// realtime and run the highest priority. On the same priority, the first runs until it's blocked.
// So it is realtime without time slice.
#define SCHED_FIFO   1     

// realtime and run the highest priority. On the same priority, the first runs but in the time
// sliced. So it is realtime with time slice.
#define SCHED_RR     2

<default-policy>
SCHED_NORMAL(OTHER) is default.

To change the policy after a boot:
For example, if scheduling is modified before insmod-ing callisto BCM drivers, tasks inherit changed
scheduling. In that case last two task mods can be dropped. The list is quite aggressive as changing
policy of kthread affects all new kernel threads. Fine tuning would obviously have to be done along
with BCM.

(about ways to change from NORMAL to RR)
http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0182.html
http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0330.html


<code>
/* from sched.h.
#define MAX_USER_RT_PRIO   100
#define MAX_RT_PRIO        MAX_USER_RT_PRIO
*/

#define MY_RT_PRIORITY MAX_USER_RT_PRIO /* Highest possible */

int main(int argc, char **argv)
{
  ...
  int rc, old_scheduler_policy;
  struct sched_param my_params;
  ...

  /* Passing zero specifies caller's (our) policy */
  old_scheduler_policy = sched_getscheduler(0);
  my_params.sched_priority = MY_RT_PRIORITY;

  /* Passing zero specifies callers (our) pid */
  rc = sched_setscheduler(0, SCHED_RR, &my_params);
  if ( rc == -1 )
    handle_error();
  ...
}

(sched.c)
/**
 * sys_sched_get_priority_max - return maximum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the maximum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_max(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = MAX_USER_RT_PRIO-1;
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
                break;
        }
        return ret;
}

/**
 * sys_sched_get_priority_min - return minimum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the minimum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_min(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = 1;
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
        }
        return ret;
}


{pthread-schedule}
The POSIX draft standard specifies scheduling policy attributes of SCHED_FIFO (first-in-first-out),
SCHED_RR (round-robin), or SCHED_OTHER (an implementation-defined method). SCHED_FIFO and
SCHED_RR are optional in POSIX, and only are supported for real time bound threads


={============================================================================
*kt_linux_core_200* linux-io-model

LPI 4

<std-decs>
Or, more precisely, the program inherits copies of the shell's file
descriptors, and the shell normally operates with these three file descriptors
always open. (In an interactive shell, these three file descriptors normally
    refer to the terminal under which the shell is running.) 

If I/O redirections are specified on a command line, then the shell ensures
that the file descriptors are suitably modified before starting the program.


<proc-fdinfo>
Each process has its own set of file descriptors.

Since kernel 2.6.22, the Linux-specific files in the directory
`/proc/PID/fdinfo` can be read to obtain information about the file descriptors
of any process on the system. There is one file in this directory for each of
the process's open file descriptors, with a name that matches the number of
the descriptor. The pos field in this file shows the current file offset
(Section 4.7). The flags field is an octal number that shows the file access
mode flags and open file status flags.  (To decode this number, we need to
    look at the numeric values of these flags in the C library header files.)

# cat /proc/847/fdinfo/71
pos:	0
flags:	0200


<four-key-system-calls>

fd = open(pathname, flags, mode) 

opens the file identified by pathname, returning a file descriptor used to
refer to the open file in subsequent calls. If the file doesn't exist,
`open()` may create it, depending on the settings of the flags bit-mask
  argument. The flags argument also specifies whether the file is to be opened
  for reading, writing, or both. The mode argument specifies the permissions
    to be placed on the file if it is created by this call. If the open() call
      is not being used to create a file, this argument is ignored and can be
      omitted.


numread = read(fd, buffer, count) 

reads at 'most' count bytes from the open file referred to by fd and stores
them in buffer. The read() call returns the number of bytes 'actually' read.
If no further bytes could be read (i.e., end-of-file was encountered), read()
returns 0.  


numwritten = write(fd, buffer, count)

writes up to count bytes from buffer to the open file referred to by fd. The
write() call returns the number of bytes actually written, which may be less
than count.  


status = close(fd)

is called after all I/O has been completed, in order to release the file
descriptor fd and its associated kernel resources.


<ex-cp>
/* Transfer data until we encounter end of input or an error */
while ((numRead = read(inputFd, buf, BUF_SIZE)) > 0)
   if (write(outputFd, buf, numRead) != numRead)
      fatal("couldn't write whole buffer");

if (numRead == -1)
   errExit("read");


<universal-io-model>
We focus on I/O on disk files. However, much of the material covered here is
relevant for later chapters, since the same system calls are used for
performing I/O on all types of files, such as pipes and terminals.

All system calls for performing I/O refer to open files using a file
descriptor, a (usually small) nonnegative integer. File descriptors are used
to refer to 'all' types of open files, including pipes, FIFOs, sockets,
terminals, devices, and regular files. 

The concept of universality of I/O. This means that the same four system calls
- open(), read(), write(), and close() - are used to perform I/O on all types
of files, including devices such as terminals. 

$ ./copy test test.old           Copy a regular file
$ ./copy a.txt /dev/tty          Copy a regular file to this terminal
$ ./copy /dev/tty b.txt          Copy input from this terminal to a regular file
$ ./copy /dev/pts/16 /dev/tty    Copy input from another terminal

Universality of I/O is achieved by ensuring that each file system and device
driver implements the `same set of I/O system calls` Because details specific
to the file system or device are handled within the kernel, we can generally
ignore device-specific factors when writing application programs. When access
to specific features of a file system or device is required, a program can use
the catchall ioctl() system call (Section 4.8), which provides an interface to
features that fall outside the universal I/O model.


{open-call}

4.3 Opening a File: open()

#include <sys/stat.h>
#include <fcntl.h>

int open(const char *pathname, int flags, ... /* mode_t mode */);

Returns file descriptor on success, or –1 on error

/* Open existing file for reading */

fd = open("startup", O_RDONLY);
if (fd == -1)
errExit("open");

/* Open new or existing file for reading and writing, truncating to zero
 * bytes; file permissions read+write for owner, nothing for all others */

fd = open("myfile", O_RDWR | O_CREAT | O_TRUNC, S_IRUSR | S_IWUSR);
if (fd == -1)
errExit("open");

/* Open new or existing file for writing; writes should always append to end
 * of file */

fd = open("w.log", O_WRONLY | O_CREAT | O_TRUNC | O_APPEND, S_IRUSR | S_IWUSR);


<open-flags>
Table 4-3 are divided into the following groups:

File `access mode flags` 

These are the O_RDONLY, O_WRONLY, and O_RDWR flags described earlier. They can
be retrieved using the fcntl() F_GETFL operation (Section 5.3).

File `creation flags` 

These are the flags shown in the second part of Table 4-3. They control
various aspects of the behavior of the open() call, as well as options for
subsequent I/O operations. These flags can't be retrieved or changed.

Open file status flags

These are the remaining flags in Table 4-3. They can be retrieved and modified
using the fcntl() F_GETFL and F_SETFL operations (Section 5.3). These flags
are sometimes simply called the file status flags.


O_CLOEXEC (since Linux 2.6.23)

Enable the close-on-exec flag (FD_CLOEXEC) for the new file descriptor. We
describe the FD_CLOEXEC flag in Section 27.4. Using the O_CLOEXEC flag allows
a program to avoid additional fcntl() F_SETFD and F_SETFD operations to set
the close-on-exec flag. It is also necessary in multithreaded programs to
avoid the race conditions that could occur using the latter technique. These
races can occur when one thread opens a file descriptor and then tries to mark
it close-on-exec at the same time as another thread does a fork() and then an
exec() of an arbitrary program. (Suppose that the second thread manages to
    both fork() and exec() between the time the first thread opens the file
    descriptor and uses fcntl() to set the close-on-exec flag.) Such races
could result in open file descriptors being unintentionally passed to unsafe
programs. (We say more about race conditions in Section 5.1.)

O_CREAT

If the file doesn't already exist, it is created as a new, empty file. This
flag is effective even if the file is being opened only for reading. If we
specify O_CREAT, then we must supply a mode argument in the open() call;
otherwise, the permissions of the new file will be set to some random value
from the stack.


O_EXCL

This flag is used in conjunction with O_CREAT to indicate that if the file
already 'exists', it should not be opened; instead, open() should fail, with
errno set to EEXIST. In other words, this flag allows the caller to 'ensure'
that it is the process creating the file. 


<open-errors>

ENOENT

The specified file doesn’t exist, and O_CREAT was not specified, or O_CREAT
was specified, and one of the directories in pathname doesn’t exist or is a
symbolic link pointing to a nonexistent pathname (a dangling link).


{read-call}
#include <unistd.h>

ssize_t read(int fd, void *buffer, size_t count);

Returns number of bytes read, 0 on EOF, or -1 on error

The count argument specifies the maximum number of bytes to read. The size_t
data type is an unsigned integer type. The buffer argument supplies the
address of the memory buffer into which the input data is to be placed. This
buffer must be at least count bytes long.

A successful call to read() returns the number of bytes actually read, or 0 if
EOF is encountered.  On error, the usual -1 is returned. The ssize_t data type
is a signed integer type used to hold a byte count or a -1 error indication.  

A call to read() may read 'less' than the requested number of bytes. For a
regular file, the probable reason for this is that we were close to the end of
the file.


<not-null-terminated>
The output from read() is strange because read() doesn't place a terminating
null byte at the end of the string that printf() is being asked to print. A
moment's reflection leads us to realize that this must be so, since read() can
be used to read 'any' sequence of bytes from a file. 

In some cases, this input might be text, but in other cases, the input might
be binary integers or C structures in binary form. There is no way for read()
  to tell the difference, and so it can't attend to the C convention of null
  terminating character strings. If a terminating null byte is required at the
  end of the input buffer, we must put it there explicitly:


{write-call}
#include <unistd.h>

ssize_t write(int fd, void *buffer, size_t count);

Returns number of bytes written, or -1 on error


{ioctl-call}
The ioctl() system call is a general-purpose mechanism for performing file and
device operations that fall `outside the universal I/O model` described earlier
in this chapter.

#include <sys/ioctl.h>

int ioctl(int fd, int request, ... /* argp */);

Value returned on success depends on request, or -1 on error

The fd argument is an open file descriptor for the device or file upon which
the control operation specified by `request` is to be performed.

Device-specific header files define constants that can be passed in the
request argument.


={============================================================================
*kt_linux_core_201* linux-io-extended

LPI 5 File I/O: Futher Details

{automicity}
All system calls are executed atomically. By this, we mean that the kernel
guarantees that all of the steps in a system call are completed as a single
operation, without being interrupted by another process or thread.

<race-condition>
Atomicity is essential to the successful completion of some operations. In
particular, it allows us to avoid race conditions (sometimes known as race
    hazards). A race condition is a situation where the result produced by two
processes (or threads) operating on shared resources depends in an unexpected
way on the relative order in which the processes gain access to the CPU(s).


{two-cases-when-needs-automicity}
Two cases that shows race condition when do not use O_EXCL and O_APPEND flag.

<creation>
Creating a file exclusively

Using a single open() call that specifies the O_CREAT and O_EXCL flags
prevents this possibility by guaranteeing that the check and creation steps
are carried out as a single atomic uninterruptible operation.

See 5.1 for the problem description.

<appending>
When we have multiple processes appending data to the same file (e.g., a
    global log file).

if (lseek(fd, 0, SEEK_END) == -1)
   errExit("lseek");

if (write(fd, buf, len) != len)
   fatal("Partial/failed write");

Again, this is a race condition because the results depend on the order of
scheduling of the two processes. Avoiding this problem requires that the seek
to the next byte past the end of the file and the write operation happen
atomically. This is what opening a file with the `O_APPEND flag guarantees`

note: man 2 open shows

       O_APPEND
              The  file  is opened in append mode.  Before each write(2), the
              file offset is positioned at the end of the file, as if with
              lseek(2).  O_APPEND may lead to corrupted files on NFS
              filesystems if more than one process appends data to a file at
              once.  This is because NFS does not support appending to a file,
              so the client kernel  has  to  simulate  it,  which can't be
              done without a race condition.


{fcntl}

5.2 File Control Operations: fcntl()

#include <fcntl.h>

int fcntl(int fd, int cmd, ...);

Return on success depends on cmd, or -1 on error

One use of fcntl() is to retrieve or modify the access mode and open file
status flags of an open file.

Using fcntl() to modify open file status flags is particularly useful in the
following cases:

  The file was not opened by the calling program, so that it had no control
  over the flags used in the open() call. e.g., the file may be one of the
  three standard descriptors that are opened before the program is started.

  The file descriptor was obtained from a system call other than open().
  Examples of such system calls are pipe(), which creates a pipe and returns
  two file descriptors referring to either end of the pipe, and socket(),
  which creates a socket and returns a file descriptor referring to the
  socket.


{file-descriptors-and-open-files}

5.4 Relationship Between File Descriptors and Open Files

It is possible-and useful-to have multiple descriptors referring to the same
open file. These file descriptors may be open in the same process or in
different processes. Three data structures maintained by the 'kernel':

File descriptor   ->  Open file   ->    i-node table
    table               table
    fd x

1. the per-process `open file descriptor table`

  * a set of flags controlling the operation of the file descriptor (there is
    just one such flag, the close-on-exec flag, which we describe in Section
    27.4); and

  * a reference to the open file description.


2. the system-wide table of open file `descriptions`, `open file table`

An open file description stores all information relating to an open file,
including:

  * the current file offset (as updated by read() and write(), or explicitly
    modified using lseek());

  * status flags specified when opening the file (i.e., the flags argument to
    open());
 
  * the file access mode (read-only, write-only, or read-write, as specified
    in open());

  * settings relating to signal-driven I/O (Section 63.3); and

  * a reference to the i-node object for this file.


3. the file system `i-node table`

Each file system has a table of i-nodes for all files residing in the file
system. The i-node for each file includes the following information:

  * file type (e.g., regular file, socket, or FIFO) and permissions;
  * a pointer to a list of locks held on this file; and
  * various properties of the file, including its size and timestamps relating
    to different types of file operations.


<when-the-same-process-has-multiple-descriptors-for-the-same-file>
In process A, descriptors 1 and 20 both, fd 1 and fd 20, refer to the same
open file `description` (labeled 23). This situation may arise as a result of a
call to dup(), dup2(), or fcntl() (see Section 5.5).

<when-different-process-has-descriptor-for-the-same-file>
Descriptor 2 of process A and descriptor 2 of process B refer to a single open
file description (73). This scenario could occur after a call to fork() (i.e.,
    process A is the parent of process B, or vice versa), or if one process
passed an open descriptor to another process using a UNIX domain socket
(Section 61.13.3).

<when-different-process-has-the-same-inode>
Finally, we see that descriptor 0 of process A and descriptor 3 of process B
refer to different open file descriptions, but that these descriptions refer
to the same i-node table entry (1976)-in other words, to the same file. This
occurs because each process independently called `open()` for the same file. A
similar situation could occur if a single process opened `the same file twice`


{duplicating-descriptors}

5.5 Duplicating File Descriptors

The shell that we wish to have standard error (2) redirected to the same place
to which standard output (1) is being sent.

$ ./myscript > results.log 2>&1

The shell achieves the redirection of standard error `by duplicating` file
descriptor 2 so that it refers to the same open file description as file
descriptor 1.

Note that it is not sufficient for the shell simply to open the results.log
file twice since this makes two descriptors and descriptions: 

One reason for this is that the two file descriptors would not share a file
offset pointer, and hence could end up overwriting each other's output. (Since
    uses different description)

Another reason is that the file may not be a disk file. Consider the following
command, which sends standard error down the same pipe as standard output:
(Same problem of overwriting? not sure)

$ ./myscript 2>&1 | less


<dup-call>
The dup() call takes oldfd, an open file descriptor, and returns a new
descriptor that refers to the `same open file description` The new descriptor
is guaranteed to be `the lowest unused file descriptor`

#include <unistd.h>

int dup(int oldfd);
int dup2(int oldfd, int newfd);

Returns (new) file descriptor on success, or -1 on error

newfd = dup(1);

The dup2() system call makes a duplicate of the file descriptor given in oldfd
using the descriptor number 'supplied' in newfd. If the file descriptor
specified in newfd is already open, dup2() 'closes' it first. Any error that
occurs during this close is silently ignored; safer programming practice is to
explicitly close() newfd if it is open before the call to dup2().

If we wanted the duplicate to be descriptor 2, we could use the following
technique:

close(2);         /* Frees file descriptor 2 */
newfd = dup(1);   /* Should reuse file descriptor 2 */

We could simplify the preceding calls to close() and dup() to the following:

dup2(1, 2);


5.6 File I/O at a Specified Offset: pread() and pwrite()

The pread() and pwrite() system calls operate just like read() and write(),
    except that the file I/O is performed at the location specified by offset,
    rather than at the current file offset. The file offset is left unchanged
    by these calls.

These system calls can be particularly useful in multithreaded applications.
As we'll see in Chapter 29, all of the threads in a process share the same
file descriptor table. This means that the file offset for each open file is
global to all threads. Using pread() or pwrite(), multiple threads can
simultaneously perform I/O on the same file descriptor without being affected
by changes made to the file offset by other threads.


={============================================================================
*kt_linux_core_202* linux-io-non-blocking

LPI 5.9 Nonblocking I/O

The O_NONBLOCK flag when opening a file serves two purposes:

  If the file can't be opened immediately, then open() returns an error
  instead of blocking. One case where open() can block is with FIFOs (Section
      44.7).

  After a successful open(), `subsequent I/O operations` are also nonblocking.
  If an I/O system call can't complete immediately, then either a partial data
  transfer is performed or the system call fails with one of the errors EAGAIN
  or EWOULDBLOCK. Which error is returned depends on the system call. On
  Linux, as on many UNIX implementations, these two error constants are
  synonymous.

<nonblocking-pipe>
Nonblocking mode can be used with devices (e.g., terminals and
    pseudoterminals), pipes, FIFOs, and sockets. Because file descriptors for
pipes and sockets are not obtained using open(), we must enable this flag
using the fcntl() F_SETFL operation.

note: This is why use pipe2-call.

O_NONBLOCK `is generally ignored for regular files`, because `kernel buffer cache` 
ensures that `I/O on regular files does not block`, as described in Section
13.1. However, O_NONBLOCK does have an effect for regular files when mandatory
file locking is employed (Section 55.4). We say more about nonblocking I/O in
Section 44.9 and in Chapter 63.


={============================================================================
*kt_linux_core_202* linux-io-buffering

<kernel-buffer-cache>
13.1 Kernel Buffering of File I/O: The Buffer Cache

In the interests of speed and efficiency, I/O system calls (i.e., the kernel)
and the I/O functions of the standard C library (i.e., the stdio functions)
buffer data when operating on disk files.

When working with disk files, the `read()` and `write()` system calls don't
directly initiate disk access. Instead, they simply copy data between a
user-space buffer and a buffer in the kernel `buffer cache`

write(fd, "abc", 3);

At this point, write() returns. At some later point, the kernel writes
  (flushes) its buffer to the disk. (Hence, we say that the system call is not
      synchronized with the disk operation.) If, in the interim, another
  process attempts to read these bytes of the file, then the kernel
  automatically supplies the data from the buffer cache, rather than from (the
      outdated contents of) the file.

`The aim of this design` is to allow read() and write() to be fast, since they
don't need to wait on a (slow) disk operation. This design is also efficient,
  since it reduces the number of disk transfers that the kernel must perform.

The Linux kernel imposes `no fixed upper limit on the size of the buffer cache`
The kernel will allocate as many buffer cache pages as are required, limited
only by the amount of available physical memory and the demands for physical
memory for other purposes (e.g., holding the text and data pages required by
    running processes). If available memory is scarce, then the kernel flushes
some modified buffer cache pages to disk, in order to free those pages for
reuse.

Speaking more precisely, from kernel 2.4 onward, Linux no longer maintains a
separate buffer cache. Instead, file I/O buffers are included in the page
cache, which, for example, also contains pages from memory-mapped files.
Nevertheless, in the discussion in the main text, we use the term buffer
cache, since that term is historically common on UNIX implementations.


Effect of buffer size on I/O system call performance

The kernel performs the same number of disk accesses, regardless of whether we
perform 1000 writes of a single byte or a single write of a 1000 bytes.
However, the latter is preferable, since it requires a single system call,
  while the former requires 1000.

note: example 1. uses the program to copy a file.

The BUF_SIZE constant specifies how many bytes are transferred by each call to
  read() and write().)

Since the total amount of data transferred (and hence the number of disk
    operations) is the same for the various buffer sizes, what Table 13-1
illustrates is the overhead of making read() and write() calls.

In summary, if we are transferring a large amount of data to or from a file,
   then by buffering data in large blocks, and thus performing fewer system
     calls, we can greatly improve I/O performance.

note: example 2. use write() only.

However, we already saw that write() returns immediately after transferring
data from user space to the kernel buffer cache.

Since the RAM size on the test system (4 GB) far exceeds the size of the file
being copied (100 MB), we can assume that by the time the program completes,
the output file has not actually been written to disk.

This is because `no actual disk I/O is being performed` in the latter case. In
other words, the majority of the time required for the large buffer cases in
Table 13-1 is due to the disk reads.


13.2 Buffering in the stdio Library

<stdio-role-in-io>
Buffering of data into large blocks to reduce system calls is exactly what is
done by the C library I/O functions (e.g., fprintf(), fscanf(), fgets(),
    fputs(), fputc(), fgetc()) when operating on disk files. Thus, using the
stdio library relieves us of the task of buffering data for output with
write() or input via read().

The setvbuf() function controls the form of buffering employed by the stdio
library.

#include <stdio.h>

int setvbuf(`FILE *stream`, char *buf, int mode, size_t size);

Returns 0 on success, or nonzero on error

The setvbuf() call affects the behavior of `all subsequent stdio operations` on
the specified stream.

The buf and size arguments specify the buffer to be used for stream. These
arguments may be specified in two ways:

  If buf is NULL, then the stdio library `automatically allocates a buffer` for
  use with stream (unless we select unbuffered I/O, as described below). SUSv3
  permits, but does not require, an implementation to use size to determine
  the size for this buffer. In the glibc implementation, size is ignored in
  this case.

The mode argument specifies the type of buffering and has one of the following
values:

_IONBF

Don't buffer I/O. Each stdio library call results in an immediate write() or
read() system call. The buf and size arguments are ignored, and can be
specified as NULL and 0, respectively. This is the default for stderr, so that
error output is guaranteed to appear immediately.

_IOFBF

Employ fully buffered I/O. Data is read or written (via calls to read() or
    write()) in units equal to the size of the buffer. This mode is the
default for streams referring to disk files.


The setbuf() function is layered on top of setvbuf(), and performs a similar
task.

#include <stdio.h>

void setbuf(FILE *stream, char *buf);

Other than the fact that it doesn't return a function result, the call
  setbuf(fp, buf) is equivalent to:

setvbuf(fp, buf, (buf != NULL) ? _IOFBF: _IONBF, BUFSIZ);


Flushing a stdio buffer

Regardless of the current buffering mode, at any time, we can force the data
in a stdio output stream to be written (i.e., flushed to a kernel buffer via
    write()) using the fflush() library function. This function flushes the
output buffer for the specified stream.

#include <stdio.h>

int fflush(FILE *stream);

Returns 0 on success, EOF on error

If stream is NULL, fflush() flushes all stdio buffers. A stdio buffer is
`automatically flushed` when the corresponding stream is closed.


13.3 Controlling Kernel Buffering of File I/O

It is possible to force flushing of kernel buffers for output files.
Sometimes, this is necessary if an application (e.g., a database journaling
    process) must ensure that output really has been written to the disk (or
      at least to the disk’s hardware cache) before continuing.


System calls for controlling kernel buffering of file I/O

SUSv3 defines two different types of synchronized I/O completion. The
difference between the types involves the `metadata` ("data about data")
describing the file, which the kernel stores along with the data for a file.

The other type of synchronized I/O completion defined by SUSv3 is synchronized
I/O file integrity completion, which is a superset of synchronized I/O data
integrity completion. The difference with this mode of I/O completion is that
during a file update, all updated file metadata is transferred to disk, even
if it is not necessary for the operation of a subsequent read of the file
  data.

The fsync() system call causes `the buffered data and all metadata` associated
with the open file descriptor fd to be flushed to disk. Calling fsync() forces
the file to `the synchronized I/O file integrity completion` state.

#include <unistd.h>

int fsync(int fd);

Returns 0 on success, or 1 on error

An fsync() call returns only after the transfer to `the disk device` (or at
    least its cache) has completed.

note: here "disk device" means actual disk? seems a disk as below.

The sync() system call causes `all` kernel buffers containing updated file
information (i.e., data blocks, pointer blocks, metadata, and so on) to be
flushed to disk.

#include <unistd.h>

void sync(void);    // note on "void"

In the Linux implementation, sync() returns only after all data has been
  transferred to the disk device (or at least to its cache). However, SUSv3
  permits an implementation of sync() to simply schedule the I/O transfer and
  return before it has completed.


<pdflush>
A permanently running kernel thread ensures that modified kernel buffers are
flushed to disk if they are not explicitly synchronized within 30 seconds.
This is done to ensure that buffers don't remain unsynchronized with the
corresponding disk file (and thus vulnerable to loss in the event of a system
    crash) for long periods. 
  
In Linux 2.6, this task is performed by the `pdflush` kernel thread. (In Linux
    2.4, it is performed by the kupdated kernel thread.)

The file `/proc/sys/vm/dirty_expire_centisecs` specifies the age (in hundredths
    of a second) that a dirty buffer must reach before it is flushed by
pdflush. Additional files in the same directory control other aspects of the
operation of pdflush.

-sh-3.2# cat /proc/sys/vm/dirty_expire_centisecs
3000


<sync-io>
Specifying the O_SYNC flag when calling open() makes all subsequent output
synchronous:

fd = open(pathname, O_WRONLY | O_SYNC);

After this open() call, every write() to the file automatically flushes the
  file data and metadata to the disk (i.e., writes are performed according to
      `synchronized I/O file integrity completion`).


Performance impact of O_SYNC

As can be seen from the table, O_SYNC increases elapsed times enormouslyin the
1-byte buffer case, by a factor of more than 1000. Note also the large
differences between the elapsed and CPU times for writes with O_SYNC. This is
a consequence of the program being blocked while each buffer is actually
transferred to disk.


<disk-cache>
Modern disk drives have large internal caches, and by default, O_SYNC merely
causes data to be transferred `to the cache` If we disable caching on the disk
(using the command hdparm W0), then the performance impact of O_SYNC becomes
even more extreme. 

// hdparm
//
// For Get/Set options, a query without an optional parameter (e.g. -d) will
// query (get) the device state, and with a parameter (e.g., -d0) will set the
// device state. 
//
//      -W     Get/set the IDE/SATA drive´s write-caching feature.
//
//      --fibmap
//        When used, this must be the only flag given. It requires a file path
//        as a parameter, and will print out a list of the device extents
//        (sector ranges)
//        occupied by that file on disk. Sector numbers are given as absolute LBA
//        numbers, referenced from sector 0 of the physical device (*not* the
//        partition or filesystem). This information can then be used for a
//        variety of purposes, such as examining the degree of fragmenation of
//        larger files, or determining appropriate sectors to deliberately corrupt
//        during fault-injection testing procedures. 
//
// -sh-3.2# hdparm -W /dev/sda
// 
// /dev/sda:
//  write-caching =  1 (on)
//
// -sh-3.2# hdparm -W0 /dev/sda
// 
// /dev/sda:
//  setting drive write-caching to 0 (off)
//  write-caching =  0 (off)
//
// -sh-3.2# hdparm -W /dev/sda
// 
// /dev/sda:
//  write-caching =  0 (off)
// -sh-3.2#

In the 1-byte case, the elapsed time rises from 1030 seconds to around 16,000
seconds. In the 4096-byte case, the elapsed time rises from 0.34 seconds to 4
seconds.  

In summary, if we need to force flushing of kernel buffers, we should consider
whether we can design our application to use large write() buffer sizes or
make judicious use of occasional calls to fsync() or fdatasync(), instead of
using the O_SYNC flag when opening the file.


13.4 Summary of I/O Buffering

`Figure 13-1` provides an overview of the buffering employed (for output files)
by the stdio library and the kernel, along with the mechanisms for controlling
each type of buffering.


13.6 Bypassing the Buffer Cache: Direct I/O

note: The details described here are `Linux-specific` and are not standardized
by SUSv3.

Starting with kernel 2.4, Linux allows an application to bypass the buffer
cache when performing disk I/O, thus transferring data directly from user
space to a file or disk device. This is sometimes termed direct I/O or raw
I/O.

Direct I/O is sometimes misunderstood as being a means of obtaining fast I/O
performance. 

However, for most applications, using direct I/O can considerably degrade
performance. This is because the kernel applies `a number of optimizations` to
improve the performance of I/O done via the buffer cache, including performing
sequential read-ahead, performing I/O in clusters of disk blocks, and allowing
processes accessing the same file to share buffers in the cache. All of these
optimizations are lost when we use direct I/O.

Direct I/O is intended only for applications with specialized I/O
requirements. For example, database systems that perform their own caching and
I/O optimizations don’t need the kernel to consume CPU time and memory
performing the same tasks.

We can perform direct I/O either `on an individual file` or on a block device
(e.g., a disk). To do this, we specify the O_DIRECT flag when opening the file
or device with open().


<with-filesystem>
The O_DIRECT flag is effective since kernel 2.4.10. Not all Linux file systems
and kernel versions support the use of this flag. Most native file systems
support O_DIRECT, but many non-UNIX file systems (e.g., VFAT) do not. It may
be necessary to test the file system concerned (if a file system doesn’t
    support O_DIRECT, then open() fails with the error EINVAL) or read the
kernel source code to check for this support.

// skipped the rest since as it noted, not all file system supports it.


={============================================================================
*kt_linux_core_203* linux-io-ex

#include <sys/stat.h>
#include <fcntl.h>
#include <sys/wait.h>
#include "tlpi_hdr.h"

int
main(int argc, char *argv[])
{
  int fd, flags;
  char template[] = "/tmp/testXXXXXX";

  setbuf(stdout, NULL);    // {KT} Disable buffering of stdout

  fd = mkstemp(template);  // opens a temporary file
  if (fd == -1)
    errExit("mkstemp");

  printf("File offset before fork(): %lld\n", (long long) lseek(fd, 0, SEEK_CUR));

  flags = fcntl(fd, F_GETFL);    // get file flags
  if (flags == -1)
    errExit("fcntl - F_GETFL");
  
  printf("O_APPEND flag before fork() is: %s\n", (flags & O_APPEND) ? "on" : "off");

  switch (fork()) {
    case -1:
      errExit("fork");

    case 0: /* Child: change file offset and status flags */
      if (lseek(fd, 1000, SEEK_SET) == -1)
        errExit("lseek");

      flags = fcntl(fd, F_GETFL); /* Fetch current flags */
      if (flags == -1)
        errExit("fcntl - F_GETFL");

      flags |= O_APPEND; /* Turn O_APPEND on */
      if (fcntl(fd, F_SETFL, flags) == -1)
        errExit("fcntl - F_SETFL");
      _exit(EXIT_SUCCESS);

    default: /* Parent: can see file changes made by child */
      if (wait(NULL) == -1)
        errExit("wait"); /* Wait for child exit */

      printf("Child has exited\n");
      printf("File offset in parent: %lld\n", (long long) lseek(fd, 0, SEEK_CUR));

      flags = fcntl(fd, F_GETFL);
      if (flags == -1)
        errExit("fcntl - F_GETFL");

      printf("O_APPEND flag in parent is: %s\n", (flags & O_APPEND) ? "on" : "off");
      exit(EXIT_SUCCESS);
  }
}

For an explanation of why we cast the return value from lseek() to long long
in Listing 24-2, see Section 5.10.


={============================================================================
*kt_linux_core_203* linux-io-dev-fd

For each process, the kernel provides the special virtual directory /dev/fd.
This directory contains filenames of the form /dev/fd/n, where n is a number
corresponding to one of the open file descriptors for the process.

note: 
current process

/dev/fd is actually a symbolic link to the `Linux-specific` /proc/self/fd
directory. The latter directory is a `special case` of the Linux-specific
/proc/PID/fd directories, each of which contains symbolic links corresponding
to all of the files held open by a process.

$ ls -al /dev/fd
lrwxrwxrwx 1 root root 13 Nov 17 11:59 /dev/fd -> /proc/self/fd

$ ll /dev/fd/
total 0
dr-x------ 2 keitee keitee  0 May 10 22:22 ./
dr-xr-xr-x 8 keitee keitee  0 May 10 22:22 ../
lrwx------ 1 keitee keitee 64 May 10 22:22 0 -> /dev/pts/1
lrwx------ 1 keitee keitee 64 May 10 22:22 1 -> /dev/pts/1
lrwx------ 1 keitee keitee 64 May 10 22:22 2 -> /dev/pts/1
lr-x------ 1 keitee keitee 64 May 10 22:22 3 -> /proc/4122/fd/
$

<use>
Their most common use is in the shell. Many user-level commands take filename
arguments, and sometimes we would like to put them in a pipeline and have one
of the arguments be standard input or output instead. For this purpose, some
programs (e.g., diff, ed, tar, and comm) have evolved the hack of using an
argument consisting of a single hyphen (-) to mean "use standard input or
output (as appropriate) for this filename argument." Thus, to compare a file
list from ls against a previously built file list, we might write the
following:

$ ls | diff - oldfilelist

This approach has various problems. First, it requires specific interpretation
of the hyphen character on the part of each program, and many programs don't
perform such interpretation; they are written to work only with filename
arguments, and they have no means of specifying standard input or output as
the files with which they are to work. Second, some programs instead interpret
a single hyphen as a delimiter marking the end of command-line options.

Using /dev/fd eliminates these difficulties, allowing the specification of
standard input, output, and error as filename arguments to any program
requiring them. Thus, we can write the previous shell command as follows:

$ ls | diff /dev/fd/0 oldfilelist

As a convenience, the names /dev/stdin, /dev/stdout, and /dev/stderr are
provided as symbolic links to, respectively, /dev/fd/0, /dev/fd/1, and
/dev/fd/2.


={============================================================================
*kt_linux_core_203* linux-io-temp-file

5.12 Creating Temporary Files

Some programs need to create temporary files that are used only while the
program is running, and these files should be removed when the program
terminates.

Here, we describe two of these functions: mkstemp() and tmpfile().

The mkstemp() function generates a unique filename based on a template
supplied by the caller and opens the file, returning a file descriptor that
can be used with I/O system calls.

#include <stdlib.h>

int mkstemp(char *template);

Returns file descriptor on success, or 1 on error

The mkstemp() function creates the file with read and write permissions for
the file owner (and no permissions for other users), and opens it with the
O_EXCL flag, guaranteeing that the caller has exclusive access to the file.
Typically, a temporary file is unlinked (deleted) soon after it is opened,
  using the unlink() system call (Section 18.3).


The tmpfile() function creates a uniquely named temporary file that is opened
for reading and writing. (The file is opened with the O_EXCL flag to guard
    against the unlikely possibility that another process has already created
    a file with the same name.)

#include <stdio.h>

FILE *tmpfile(void);

Returns file pointer on success, or NULL on error

On success, tmpfile() returns a file stream that can be used with the stdio
library functions. The temporary file is automatically deleted when it is
closed. To do this, tmpfile() makes an internal call to unlink() to remove the
filename immediately after opening the file.


={============================================================================
*kt_linux_core_203* linux-io-fs

14.1 Device Special Files (Devices)

A device special file corresponds to a device on the system. Within the
kernel, each device type has a corresponding device driver, which handles all
I/O requests for the device. A device driver is a unit of kernel code that
implements a set of operations that (normally) correspond to input and output
actions on an associated piece of hardware. The API provided by device drivers
is fixed, and includes operations corresponding to the system calls open(),
close(), read(), write(), mmap(), and ioctl().  The fact that each device
  driver provides a consistent interface, hiding the differences in operation
  of individual devices, allows for universality of I/O (Section 4.2).

Devices can be divided into two types:

  * Character devices handle data on a character-by-character basis. Terminals
    and keyboards are examples of character devices.

  * Block devices handle data a block at a time. The size of a block depends
    on the type of device, but is typically some multiple of 512 bytes.
    Examples of block devices include disks and tape drives.

Device IDs

Each device file has a major ID number and a minor ID number. The major ID
identifies the general class of device, and is used by the kernel to look up
the appropriate driver for this type of device. The minor ID uniquely
identifies a particular device within a general class. The major and minor IDs
of a device file are displayed by the ls –l command.


14.2 Disks and Partitions

Disk drives

A hard disk drive is a mechanical device consisting of one or more `platters`
that rotate at high speed (of the order of thousands of revolutions per
    minute). Magnetically encoded information on the disk surface is retrieved
or modified by read/ write heads that move radially across the disk.
Physically, information on the disk surface is located on a set of concentric
circles called `tracks`. Tracks themselves are divided into a number of
`sectors`, each of which consists of a series of physical `blocks`.

Physical blocks are typically 512 bytes (or some multiple thereof) in size,
and represent the `smallest unit` of information that the drive can read or
  write.

Although modern disks are fast, reading and writing information on the disk
still takes significant time. The disk head must first move to the appropriate
track (seek time), then the drive must wait until the appropriate sector
rotates under the head (rotational latency), and finally the required blocks
must be transferred (transfer time). The total time required to carry out such
an operation is typically of the order of milliseconds. By comparison, modern
CPUs are capable of executing millions of instructions in this time.

Disk partitions

Each disk is divided into one or more (nonoverlapping) partitions. Each
partition is treated by the kernel as a separate device residing under the
/dev directory.


14.3 File Systems

A file system is an organized collection of regular files and directories. A
file system is created using the mkfs command. One of the strengths of Linux
is that it supports a wide variety of file systems.

The file-system types currently known by the kernel can be viewed in the
Linux-specific /proc/filesystems file.

File-system structure

The basic unit for allocating space in a file system is a `logical block`,
which is some multiple of `contiguous physical blocks` on the disk device on
  which the file system resides. For example, the logical block size on ext2
  is 1024, 2048, or 4096 bytes.  (The logical block size is specified as an
      argument of the mkfs(8) command used to build the file system.)

note:
From here, block refers to logical block.

| partition                                            |
| boot block | super block | i-node table | data block |

Figure 14-1: Layout of disk partitions and a file system

A file system contains the following parts:

  * Boot block: 
  This is always the first block in a file system. The boot block is not used
  by the file system; rather, it contains information used to boot the
  operating system. Although only one boot block is needed by the operating
  system, all file systems have a boot block (most of which are unused).

  * Superblock: 
  This is a single block, immediately following the boot block, which contains
  parameter information about the file system, including:

  the size of the i-node table;
  the size of `logical blocks` in this file system; and
  the size of the file system in logical blocks.


  * I-node table: 
  Each file or directory in the file system has a unique entry in the i-node
  table. This entry records various information about the file. 

  * Data blocks: 
  The great majority of space in a file system is used for the blocks of data
  that form the files and directories residing in the file system.


14.4 I-nodes

A file system's i-node table contains one i-node (short for index node) for
each file residing in the file system. I-nodes are identified numerically by
their sequential location in the i-node table. The i-node number (or simply
    i-number) of a file is the first field displayed by the ls –li command.
The information maintained in an i-node includes the following:

  * File type (e.g., regular file, directory, symbolic link, character device).
  * Owner (also referred to as the user ID or UID) for the file.
  * Group (also referred to as the group ID or GID) for the file.
  * Access permissions for three categories of user

  * Three timestamps: 
  time of last access to the file (shown by ls –lu), time of last modification
  of the file (the default time shown by ls –l), and time of last status
  change (last change to i-node information, shown by ls –lc).

  * Number of hard links to the file.
  * Size of the file in bytes.

  * Number of blocks actually allocated to the file
  measured in units of 512-byte blocks. There may not be a simple
  correspondence between this number and the size of the file in bytes, since
  a file can contain holes (Section 4.7), and thus require fewer allocated
  blocks than would be expected according to its nominal size in bytes.

  * Pointers to the data blocks of the file.

I-nodes and data block pointers in ext2

Like most UNIX file systems, the ext2 file system doesn't store the data
blocks of a file contiguously or even in sequential order. To locate the file
data blocks, the kernel maintains a set of pointers in the i-node. The system
used for doing this on the ext2 file system is shown in Figure 14-2.

note:
Removing the need to store the blocks of a file contiguously allows the file
system to use space in an efficient way. In particular, it reduces the
incidence of fragmentation of free disk space—the wastage created by the
existence of numerous pieces of noncontiguous free space, all of which are too
small to use.

Under ext2, each i-node contains 15 pointers. The first 12 of these pointers
(numbered 0 to 11 in Figure 14-2) point to the location in the file system of
the first 12 blocks of the file. 

The next pointer is a pointer to a block of pointers that give the locations
of the thirteenth and subsequent data blocks of the file. The number of
pointers in this block depends on the block size of the file system. Each
pointer requires 4 bytes, so there may be from 256 pointers (for a 1024-byte
    block size) to 1024 pointers (for a 4096-byte block size).

For even larger files, the fourteenth pointer (numbered 13 in the diagram) is
a `double` indirect pointer—it points to blocks of pointers that in turn point
to blocks of pointers that in turn point to data blocks of the file. And
should the need for a truly enormous file arise, there is a further level of
indirection: the last pointer in the i-node is a `triple`-indirect pointer.

designed to satisfy a number of requirements:

To begin with, it allows the i-node structure to be a fixed size, while at the
same time allowing for files of an arbitrary size. 

Additionally, it allows the file system to store the blocks of a file
noncontiguously, while also allowing the data to be accessed randomly via
lseek(); the kernel just needs to calculate which pointer(s) to follow. 

Finally, for small files, which form the overwhelming majority of files on
most systems, this scheme allows the file data blocks to be accessed rapidly
via the direct pointers of the i-node.


14.5 The Virtual File System (VFS)

If every program that worked with files needed to understand the specific
details of each file system, the task of writing programs that worked with all
of the different file systems would be nearly impossible. The virtual file
system (VFS, sometimes also referred to as the virtual file switch) is a
kernel feature that resolves this problem by creating an abstraction layer for
file-system operations

The ideas behind the VFS are straightforward:

  * The VFS defines a generic interface for file-system operations. All
    programs that work with files specify their operations in terms of this
    generic interface.

  * Each file system provides an implementation for the VFS interface.

Under this scheme, programs need to understand only the VFS interface and can
ignore details of individual file-system implementations.

The VFS interface includes operations corresponding to all of the usual system
calls for working with file systems and directories, such as 

open(), read(), write(), lseek(), close(), truncate(), stat(), mount(),
  umount(), mmap(), mkdir(), link(), unlink(), symlink(), and rename().


14.6 Journaling File Systems

The ext2 file system is a good example of a traditional UNIX file system, and
suffers from a classic limitation of such file systems: after a system crash,
a file-system consistency check (fsck) must be performed on reboot in order to
  ensure the integrity of the file system. 

This is necessary because, at the time of the system crash, a file update may
have been only partially completed, and the file-system metadata (directory
    entries, i-node information, and file data block pointers) may be in an
inconsistent state, so that the file system might be further damaged if these
inconsistencies are not repaired. 

A file-system consistency check ensures the consistency of the file-system
metadata. Where possible, repairs are performed; otherwise, information that
is not retrievable (possibly including file data) is discarded.

Journaling file systems eliminate the need for lengthy file-system consistency
checks after a system crash. A journaling file system logs (journals) all
metadata updates to a special on-disk journal file before they are actually
carried out. The updates are logged in groups of related metadata updates
(transactions). In the event of a system crash in the middle of a transaction,
on system reboot, the log can be used to rapidly redo any incomplete updates
  and bring the file system back to a consistent state.

note: see *sql-rollback*

<on-metadata>
Some journaling file systems ensure `only the consistency of file metadata`
Because they don't log file data, data may still be lost in the event of a
crash. The ext3, ext4, and Reiserfs file systems provide options for logging
data updates, but, depending on the workload, this may result in lower file
I/O performance. 

Two other file systems that provide journaling and a range of other advanced
features:

* The ext4 file system (http://ext4.wiki.kernel.org/) 
  is the successor to ext3. The first pieces of the implementation were added
  in kernel 2.6.19, and various features were added in later kernel versions.
  Among the planned (or already implemented) features for ext4 are extents
  (reservation of contiguous blocks of storage) and other allocation features
  that aim to reduce file fragmentation, online file-system defragmentation,
  faster file-system checking, and support for nanosecond timestamps.

* Btrfs (B-tree FS, usually pronounced “butter FS”; http://btrfs.wiki.kernel.org/) 
  is a new file system designed from the ground up to provide a range of
  modern features, including extents, writable snapshots (which provide
  functionality equivalent to metadata and data journaling), checksums on
  data and metadata, online file-system checking, online file-system
  defragmentation, space-efficient packing of small files, and space-efficient
  indexed directories. It was integrated into the kernel in version 2.6.29.


14.10 A Virtual Memory File System: tmpfs

Linux also supports the notion of virtual file systems that reside in memory.
There is, however, one important difference: file operations are much faster,
since no disk access is involved.

The tmpfs file system differs from other memory-based file systems in that it
is a virtual memory file system. This means that tmpfs uses not only RAM, but
also the swap space, if RAM is exhausted.

note:
The tmpfs file system is an optional Linux kernel component that is configured
via the CONFIG_TMPFS option.

Aside from use by user applications, tmpfs file systems also serve two special
purposes:

* An invisible tmpfs file system, mounted internally by the kernel, is used
  for implementing System V shared memory (Chapter 48) and shared anonymous
  memory mappings (Chapter 49).

* A tmpfs file system mounted at /dev/shm is used for the glibc implementation
  of POSIX shared memory and POSIX semaphores.


={============================================================================
*kt_linux_core_203* linux-io-fs-mount

14.7 Single Directory Hierarchy and Mount Points

With Linux 2.4.19 and later, things became more complicated. The kernel now
supports `per-process mount namespaces` This means that each process
potentially has its own set of file-system mount points, and thus may see a
different single directory hierarchy from other processes. We explain this
point further when we describe the CLONE_NEWNS flag in Section 28.2.1.


14.8 Mounting and Unmounting File Systems

Three files that contain information about the file systems that are currently
mounted or can be mounted:

  * /proc/mounts
  
  A list of the `currently mounted` file systems can be read from the
  Linux-specific /proc/mounts virtual file. /proc/mounts is an interface to
  kernel data structures, so it always contains `accurate information` about
  mounted file systems.

  note:
  With the arrival of the per-process mount namespace feature mentioned
  earlier, each process now has a /proc/PID/mounts file that lists the mount
  points constituting its mount namespace, and /proc/mounts is just a symbolic
  link to /proc/self/mounts.

  * /etc/mtab

  The mount(8) and umount(8) commands automatically maintain the file
  /etc/mtab, which contains information that is similar to that in
  /proc/mounts, but slightly more detailed. In particular, /etc/mtab includes
  file system–specific options given to mount(8), which are not shown in
  /proc/mounts. 

  However, because the `mount(2) and umount() system calls don't update`
  /etc/mtab, this file may be inaccurate if some application that mounts or
  unmounts devices fails to update it.

  * /etc/fstab

  maintained manually by the system administrator, contains descriptions of
  all of the available file systems on a system, and is used by the mount(8),
  umount(8), and fsck(8) commands.


Common format:

/dev/sda9 /boot ext3 rw 0 0

1. The name of the mounted device.

2. The mount point for the device.

3. The file-system type.

4. Mount flags. In the above example, rw indicates that the file system was
mounted read-write.

5. A number used to control the operation of file-system backups by dump(8).
This field and the next are used only in the /etc/fstab file; for /proc/mounts
and /etc/mtab, these fields are always 0.

6. A number used to control the order in which fsck(8) checks file systems at
system boot time.

The mountflags argument is a bit mask constructed by ORing (|) zero or more
of the flags shown in Table 14-1, which are described in more detail below.

// from mount(8)
       -o, --options opts
              Use the specified mount options.  The opts argument is a
              comma-separated list.  For example:

                     mount LABEL=mydisk -o noatime,nodev,nosuid

              For more details, see the FILESYSTEM-INDEPENDENT MOUNT OPTIONS
              and FILESYSTEM-SPECIFIC MOUNT OPTIONS sections.

FILESYSTEM-INDEPENDENT MOUNT OPTIONS

       dirsync
              All directory updates within the filesystem should be done
              synchronously.  This affects the following system calls: creat,
              link, unlink, symlink, mkdir, rmdir, mknod and rename.

       sync   
              All I/O to the filesystem should be done synchronously.  In the
              case of media with a limited number of write cycles (e.g. some
              flash drives), sync may cause life-cycle shortening.

Table 14-1: mountflags values for mount()

MS_DIRSYNC (since Linux 2.6)

Make directory updates synchronous. This is similar to the effect of the
open() O_SYNC flag (Section 13.3), but applies only to directory updates. The
MS_SYNCHRONOUS flag described below provides a superset of the functionality
of MS_DIRSYNC, ensuring that both file and directory updates are performed
synchronously. The MS_DIRSYNC flag allows an application to ensure that
directory updates (e.g., open(pathname, O_CREAT), rename(), link(), unlink(),
    symlink(), and mkdir()) are synchronized without incurring the expense of
synchronizing all file updates. The FS_DIRSYNC_FL flag (Section 15.5) serves a
similar purpose to MS_DIRSYNC, with the difference that FS_DIRSYNC_FL can be
applied to individual directories. In addition, on Linux, calling fsync() on a
file descriptor that refers to a directory provides a means of synchronizing
directory updates on a per-directory basis. (This Linux-specific fsync()
    behavior is not specified in SUSv3.)


MS_SYNCHRONOUS

Make all file and directory updates on this file system synchronous. (In the
    case of files, this is as though files were always opened with the open()
    O_SYNC flag.)

// sys/mount.h
// /* These are the `fs-independent mount-flags`: up to 16 flags are
//    supported  */
// enum
// {
//   MS_RDONLY = 1,                /* Mount read-only.  */
// #define MS_RDONLY       MS_RDONLY
//   MS_NOSUID = 2,                /* Ignore suid and sgid bits.  */
// #define MS_NOSUID       MS_NOSUID
//   MS_NODEV = 4,                 /* Disallow access to device special files.  */
// #define MS_NODEV        MS_NODEV
//   MS_NOEXEC = 8,                /* Disallow program execution.  */
// #define MS_NOEXEC       MS_NOEXEC
//   MS_SYNCHRONOUS = 16,          /* Writes are synced at once.  */


14.9 Advanced Mount Features

14.9.1 Mounting a File System at Multiple Mount Points

From kernel 2.4 onward, a file system can be mounted at multiple locations
within the file system. Because each of the mount points shows the same
subtree, changes made via one mount point are visible through the other(s),

It is useful to mount a file system at multiple points when describe bind
  mounts

<bind-mount>
A bind mount (created using the mount() MS_BIND flag) results in the directory
or file being visible in both locations. A bind mount is somewhat like a hard
link, but differs in two respects:

* A bind mount can `cross file-system` mount points (and even chroot jails).

* It is possible to make a bind mount `for a directory`

note:bind mount a file at another location

One example of when we might use a bind mount is in the creation of a chroot
jail (Section 18.12). Rather than replicating various standard directories
(such as /lib) in the jail, we can simply create bind mounts for these
directories (possibly mounted read-only) within the jail.


14.9.5 Recursive Bind Mounts


14.9.2 Stacking Multiple Mounts on the Same Mount Point

Since kernel 2.4, Linux allows multiple mounts to be stacked on a single mount
point. Each new mount hides the directory subtree previously visible at that
mount point. When the mount at the top of the stack is unmounted, the
previously hidden mount becomes visible once more

One use of mount stacking is to stack a new mount on an existing mount point
that is busy. Processes that hold file descriptors open, that are
chroot()-jailed, or that have current working directories within the old mount
point continue to operate under that mount, but processes making new accesses
to the mount point use the new mount.

Could employ mount stacking so that we don't need to care if /tmp is already
in use.


={============================================================================
*kt_linux_core_204* file attribute

LPI 15.

{calls}
The stat(), lstat(), and fstat() system calls retrieve information about a file, mostly drawn from
the file i-node.

man 2 stat

#include <sys/stat.h>
int stat(const char *pathname, struct stat *statbuf);
int lstat(const char *pathname, struct stat *statbuf);
int fstat(int fd, struct stat *statbuf);

All return 0 on success, or -1 on error

stat() returns information about a named file;

lstat() is similar to stat(), except that if the named file is a symbolic link, information about
the link itself is returned, rather than the file to which the link points; and

fstat() returns information about a file referred to by an open file descriptor.

<permission>
The stat() and lstat() system calls don't require permissions on the file itself. However, execute
(search) permission is required on 'all' of the parent directories specified in pathname. The fstat()
  system call always succeeds, if provided with a valid file descriptor. 

<use-to-check>
Shows that can use stat() to check if the file exist or not.

struct stat sb;
if (-1 == stat(cleanup_exe, &sb))
{
  fprintf(stderr, "executable (%s) doesn't exist\n", cleanup_exe);
  return 0;
}


={============================================================================
*kt_linux_core_200* ipc

CH43, Fig 43-1 in {ref-LPI} which says that the 'general' term IPC is often used to describe them all;
communication, signal, and synchronization.

communication - data transfer - byte stream  - pipe
                                             - fifo
                                             - stream socket

                              - message      - sys v message q
                                             - posix message q
                                             - datagram socket

                              - pseudoterminal

               - shared memory   - sys v shm
                                 - posix shm
                                 - memory mapping  - anonymous mapping
                                                   - mapped file

signal   - standard signal
         - realtime signal

synchronization   - semaphore - sys v
                  - posix     - named
                              - unnamed
                  - file lock - record lock
                              - file lock

                  - mutex
                  - condition variable

Signals: Although signals are intended primarily for other purposes, they can be used as a
synchronization technique in certain circumstances. More rarely, signals can be used as a
communication technique: the signal number itself is a form of information, and realtime signals can
be accompanied by associated data (an integer or a pointer).


{nonnetworked-ipc}
From {ref-UNP}. nonnetworked-ipc means that ipc for local and newtworked-ipc means that for remote
such as socket.


{categories-of-ipc}
From {ref-UNP}:

three-ways-to-share-between-processes

(1) Process Process       (2) Process Process          (3) Process <- shm -> Process
      |        |                 |       |      
Kernel                          shared info in kernel
      |        |
Filesystem


<persistence> which is lifetime of an objects
Define the persistence of any type of IPC as how long an object of that type remains in existence.

Process-persistence exists until last process with IPC open closes the object and kernel one exists
until kernel reboots or IPC objects is explicitly deleted.

process-persistence: pipe, fifo, mutex, condition-var, read-write-lock, ...
kernel-presistence : shm, named-semaphore, ...

Be careful when defining the presistence of ipc because it is not always as it seems: the data
within a pipe is maintained within the kernel, but pipes have process-persistence because after the
last process that has the pipe open for reading closes the pipe, the kernel discard all data and
remove the pipe.

From {ref-LPI}:

<data-transfer> 
1. requires two data transfers between user and kenel memory
2. synchronization between the reader and writer processes is automatic by kernel. if a reader
attempts to fetch data from data-transfer facility that has no data, then read will bock until some
write data to it.  
3. available to one which done read operation since read consumes data.

<byte-stream>
read and write is independent meaning read may read an arbitrary bytes. This models "file as a
sequence of bytes".

An application can also impose a message-oriented model on a byte-stream facility, by using
delimiter characters, fixed-length messages, or message headers that encode the length of the total
message message: each read reads a whole message. not possible to read part of a message and to read
multiple messages.


<shared-memory>
1. don't require system calls or data transfer between user and kenel. Hence shared memory
provide very fast communication. 
2. However it can be offset by the need to sync and semaphore is the usual method used with shared
memory.
3. avaible to all of the processes that share that memory

<file-desc-based>
facility using file descriptors like pipe, fifo, and sockets

The primary benefit of these techniques is that they allow an application to simultaneously monitor
multiple file descriptors to see whether I/O is possible on any of them.


{namespace}
Use name or identifier so that one process can create ipc object and other processes can specify
that same ipc object.

type                       name used to identify   handle used to refer to object
------------------------------------------------------------------------------------
pipe                       no name                 file descriptor
fifo                       pathname                ditto

UNIX domain socket         pathname                ditto
Internet domain socket     IP and port             ditto

posix message q            posix ipc pathname      mqd_t
posix named semaphore      ditto                   sem_t* (sem pointer)
posix unnamed semaphore    no name                 sem_t*
posix shared memory        posix ipc pathname      file descriptor 

anonymous mapping          no name                 none
memory mapped file         pathname                file descriptor 


{accessibility-and-persistence}
type                       accessibility                 persistence
------------------------------------------------------------------------------------
pipe                       only by related processes     process
fifo                       permission mask               ditto

UNIX domain socket         permission mask               ditto
Internet domain socket     by any processe               ditto

posix message q            permission mask               kernel
posix named semaphore      permission mask               kernel
posix unnamed semaphore    permission of underlying mem  depends
posix shared memory        permission mask               kernel

anonymous mapping          only by releated              process 
memory mapped file         permission mask               file system 

unix and network domain:

Of all of the IPC methods shown in Figure 43-1, only sockets permit processes to communicate over a
network. Sockets are generally used in one of two domains: the UNIX domain, which allows
communication between processes on the same system, and the Internet domain, which allows
communication between processes on different hosts connected via a TCP/IP network. Often, only minor
changes are required to convert a program that uses UNIX domain sockets into one that uses Internet
domain sockets, so an application that is built using UNIX domain sockets can be made
network-capable with relatively little effort.

<portability>
However, the POSIX IPC facilities (message queues, semaphores, and shared memory) are not quite as
widely available as their System V IPC counterparts, especially on older UNIX systems. An
implementation of POSIX message queues and 'full' support for POSIX semaphores have appeared on Linux
only in the 2.6.x kernel series. Therefore, from a portability point of view, System V IPC may be
preferable to POSIX IPC.

As of 06 Jan 2014, the latest stable kernel release is 3.12.6

note: Here, 'related' means related via fork(). In order for two processes to access the object, one
of them must create the object and then call fork(). As a consequence of the fork(), the child
process inherits a handle referring to the object, allowing both processes to share the object.

<performance>
In some circumstances, different IPC facilities may show notable differences in performance.
However, in later chapters, we generally refrain from making performance comparisons, for the
following reasons:

1) The performance of an IPC facility may not be a significant factor in the overall performance of
an application, and it may not be the only factor in determining the choice of an IPC facility.

2) The relative performance of the various IPC facilities may vary across UNIX implementations or
between different versions of the Linux kernel.

3) Most importantly, the performance of an IPC facility will vary depending on the precise manner
and environment in which it is used. Relevant factors include the size of the data units exchanged
in each IPC operation, the amount of unread data that may be outstanding on the IPC facility,
whether or not a process context switch is required for each unit of data exchanged, and other
load on the system.

If IPC performance is crucial, there is no substitute for application-specific benchmarks run under
an environment that matches the target system. To this end, it may be worth writing an 'abstract'
software layer that hides details of the IPC facility from the application and then testing
performance when different IPC facilities are substituted underneath the abstract layer.


={============================================================================
*kt_linux_core_201* ipc: system v

{interfaces}
A more significant reason for discussing the System V IPC mechanisms together is that their
programming interfaces share a number of common characteristics, so that many of the same concepts
apply to all of these mechanisms.


{key-and-identifier}
System V IPC keys are integer values represented using the data type key_t which is analogous to a
filename and get calls which is analogous to the open() system call used for files. The get calls
'translate' a key into the corresponding integer IPC identifier which is analogous to a file
descriptor.

<identifier>
There is, however, an important semantic difference. Whereas a file descriptor is a process
attribute, an IPC identifier is a property of the object itself and is visible 'system'-wide.

All processes accessing the same object use the same identifier. This means that if we know an IPC
object already exists, we can skip the get call, provided we have some other means of knowing the
identifier of the object.

<flag>
We specify the permissions to be placed on the new object as part of the final (flags) argument to
the get call, using the 'same' bit-mask constants as are used for files.

<key>
So, how do we provide a unique key? (LPI, 45.2)

One of three methods. Specify the IPC_PRIVATE constant as the key value to the get call when
creating the IPC object, which 'always' results in the creation of a 'new' IPC object that is
guaranteed to have a 'unique' key.


{ctl-calls}
A few are generic to all IPC mechanisms. An example of a generic control operation is IPC_RMID,
  which is used to delete an object.

<when-deleted>
For message queues and semaphores, deletion of the IPC object is immediate, and any information
contained within the object is destroyed, regardless of whether any other process is still using the
object.

For files, if we remove the last link to a file, then the file is actually removed only after all
open file descriptors referring to it have been closed. As with files, Deletion of shared memory
objects occurs differently.


{persistence}
System V IPC objects have 'kernel' persistence. Once created, an object continues
to exist until it is explicitly deleted or the system is shut down. 

Two disadvantages: 
1. system-imposed limit. If we fail to remove unused objects, we may eventually encounter
application errors as a result of reaching these limits.

2. When deleting a message queue or semaphore object, a multiprocess application may not be able to
easily determine which will be the last process requiring access to the object, and thus when the
object can be safely deleted. Not for shm.


{associated-data}
The kernel maintains an associated data structure for 'each' instance of a System V IPC object. 

The associated data structure for an IPC object is initialized when the object is created via the
appropriate get system call. Once the object has been 'created', a program can obtain a copy of this
data structure using the appropriate ctl system call, by specifying an operation type of IPC_STAT.

<ipc_perm>
As well as data specific to the type of IPC object, the associated data structure for all three IPC
mechanisms includes a substructure, ipc_perm, that holds information used to determine permissions
granted on the object

EACCES.

See LPI 45.3 for more about permission checks.

<bypass-permission-check>
The second user could bypass this check by specifying 0 for the second argument of the msgget()
  call, in which case an error would occur only when the program attempted an operation requiring
  write permission on the IPC object

msgget(key, 0);


{ipcs-command}
The ipcs command lists the System V IPC objects that currently exist on the system. The ipcrm
command is used to remove System IPC objects.

ipcs - provide information on ipc facilities

       ipcs [-asmq] [-tclup]

       Resources may be specified as follows:
       -m     shared memory segments
       -q     message queues
       -s     semaphore arrays
       -a     all (this is the default)

       The output format may be specified as follows:
       -t     time
       -p     pid
       -c     creator
       -l     limits
       -u     summary

$ ipcs -m -l

The status flags indicate whether the region has been locked into RAM to prevent swapping (Section
    48.7) and whether the region has been marked to be destroyed when all processes have detached
it.

<limitation>
On Linux, ipcs(1) displays information 'only' about IPC objects for which we have read permission,
   regardless of whether we own the objects.

<non-portable-means>
Linux provides two nonstandard methods of obtaining a list of all IPC objects on the system:

1. files within the /proc/sysvipc directory that list all IPC objects

/proc/sysvipc/msg
/proc/sysvipc/sem
/proc/sysvipc/shm
key         shmid    perms    size  cpid  lpid nattch uid   gid  cuid  cgid      atime      dtime      ctime
305450176   163844   666      24    1063  1395 4      504   504   504   504 1422517874 1422517866         33

note: see cpid and size

/proc/sysvipc
Subdirectory containing the pseudo-files msg, sem and shm. These files list the System V
Interprocess Communication (IPC) objects (respectively: message queues, semaphores, and shared
    memory) that currently exist on the system, providing similar information to that available via
ipcs(1).  These files have headers and are formatted (one IPC object per line) for easy
understanding. <svipc>(7) provides further background on the information shown by these files.

2. the use of Linux-specific ctl calls.

Unlike the ipcs command, these files always show all objects of the corresponding type, regardless
of whether read permission is available on the objects.


={============================================================================
*kt_linux_core_202* ipc: system v: shm

{segment}
Shared memory allows two or more processes to share the same region (usually referred to as a
segment) of physical memory. 


{good-and-bad}
Since a shared memory segment becomes part of a process's user-space memory, no kernel intervention
is required for IPC. note: means the fastest IPC.

On the other hand, the fact that IPC using shared memory is not mediated by the kernel means that,
   typically, some method of synchronization is required so that processes don't simultaneously
   access the shared memory. e.g., two processes performing simultaneous updates, or one process
   fetching data from the shared memory while another process is in the middle of updating it.


{interfaces}
<shmget>
To create a new shared memory segment or obtain the identifier of an existing segment. note: The
contents of a newly created shared memory segment are initialized to 0.

#include <sys/shm.h>

int shmget(key_t key, size_t size, int shmflg);

Returns shared memory segment 'identifier' on success, or -1 on error

<creator-and-size>
The 'shmflg' argument performs the same task as for the other IPC get calls, specifying the
permissions to be placed on a new shared memory segment or 'checked' against an existing segment. In
addition, zero or more of the fol- lowing flags can be ORed (|) in shmflg to control the operation
of shmget():

The 'kernel' allocates shared memory in multiples of the system page size, so size is effectively
rounded up to the next multiple of the system 'page' size. If we are using shmget() to obtain the
identifier of an existing segment, then size has 'no' effect on the segment, but it 'must' be less
than or equal to the size of the segment.

to-creat:

IPC_CREAT
If no segment with the specified key exists, create a new segment.

<when-get-errors>
The real case which got EINVAL when called shmget. The below is from man page.

EINVAL 

A new segment was to be created and size < SHMMIN or size > SHMMAX, or no new segment was to be
created, a segment with given key existed, but size is 'greater' than the size of that segment.

note: The problem was to ask shm which is 'greater' than the size of the segment. Interestingly, the
real size was 24 but /proc/PID/maps shows 4K since the pagesize is 4K. So careful to see mapping
info.

<shmat>
#include <sys/shm.h>

void *shmat(int shmid, const void *shmaddr, int shmflg);

Returns address at which shared memory is attached on success, or (void *) -1 on error

The shmaddr argument and the setting of the SHM_RND bit in the shmflg bit-mask argument control
how the segment is attached: See 48.3 for full options.

If shmaddr is NULL, then the segment is attached at a suitable address selected by the kernel. This
is the 'preferred' method of attaching a segment since specifying a non-NULL value for shmaddr is
not recommended, for the following reasons:

It reduces the portability of an application and the particular address will be already in use.

As its function result, shmat() returns the address at which the shared memory segment is attached.
Typically, we assign the return value from shmat() to a pointer to some programmer-defined
structure, in order to impose that structure on the segment. 

If SHM_RDONLY is not specified, the memory can be both read and modified.

note: Seen shmat(, , 0), that is shmflg is 0. what is it? See <bypass-permission-check>

<shmdt>
When a process no longer needs to access a shared memory segment, it can call shmdt() to detach the
segment from its virtual address space.

#include <sys/shm.h>

int shmdt(const void *shmaddr);

Returns 0 on success, or -1 on error

Detaching a shared memory segment is not the same as deleting it. Deletion is performed using the
shmctl() IPC_RMID operation

<child-and-exec>
A child created by fork() inherits its parent's attached shared memory segments. Thus, shared memory
provides an easy method of IPC between parent and child. During an exec(), all attached shared
memory segments are detached. Shared memory segments are also 'automatically' detached on process
'termination'.

<shmctl> 
#include <sys/shm.h>

int shmctl(int shmid, int cmd, struct shmid_ds *buf);

Returns 0 on success, or -1 on error

IPC_RMID <deletion>
Mark the shared memory segment and its associated shmid_ds data structure for deletion. If no
processes currently have the segment attached, deletion is immediate; otherwise, the segment is
removed only 'after' all processes have detached from it (i.e., when the value of the shm_nattch
    field in the shmid_ds data structure falls to 0). This is much closer to the situation with file
deletion.

shmctl(id, IPC_RMID, NULL);   note: buf must be NULL.

Only one process needs to perform this step.

IPC_STAT
Place a copy of the shmid_ds data structure associated with this shared memory segment in the buffer
pointed to by buf.

IPC_SET
Update selected fields of the shmid_ds data structure associated with this shared memory segment
using values in the buffer pointed to by buf.


{associated-data}
See LPI 48.8 for full details.

struct shmid_ds {
  struct ipc_perm shm_perm;   /* Ownership and permissions */
  size_t shm_segsz;           /* Size of segment in bytes */
  time_t shm_atime;           /* Time of last shmat() */
  time_t shm_dtime;           /* Time of last shmdt() */
  time_t shm_ctime;           /* Time of last change */
  pid_t shm_cpid;             /* PID of creator */
  pid_t shm_lpid;             /* PID of last shmat() / shmdt() */
  shmatt_t shm_nattch;        /* Number of currently attached processes */
};

shm_cpid
This field is set to the process ID of the process that created the segment using shmget().

shm_lpid
This field is set to 0 when the shared memory segment is created, and then set to the process ID of
the calling process on each successful shmat() or shmdt().

shm_nattch
This field counts the number of processes that currently have the segment attached. It is
initialized to 0 when the segment is 'created', and then incremented by each successful shmat() and
decremented by each successful shmdt(). 


{shm-in-virtual-memory}
From LPI 48.5. To check shm details from /proc/PID/maps.

The shared memory segments are attached starting at the virtual address 0x40000000 between heap and
stack. Mapped mappings and shared libraries are also placed in this area. The address 0x40000000 is
defined as the kernel constant TASK_UNMAPPED_BASE. Than can be changed.

$ ./svshm_create -p 102400       # size
9633796                          # shm id
$ ./svshm_create -p 3276800
9666565

$ ./svshm_attach 9633796:0 9666565:0
SHMLBA = 4096 (0x1000), PID = 9903
1: 9633796:0 ==> 0xb7f0d000      # attached at a address chosen by kernel
2: 9666565:0 ==> 0xb7bed000

$ cat /proc/9903/maps
...
b7bed000-b7f0d000 rw-s 00000000 00:09 9666565 /SYSV00000000 (deleted)
b7f0d000-b7f26000 rw-s 00000000 00:09 9633796 /SYSV00000000 (deleted)
...

<2> Two lines for the attached System V shared memory segments.

<columns>
1. A pair of hyphen-separated numbers indicating the virtual address range at which the memory
segment is mapped. The second of these numbers is the address of the next byte after the end of the
segment.

note: From this, can get the size. b7f0d000-b7bed000=0x320000. 3,276,800

2. Protection and flags for this memory segment. The first three letters indicate the protection of
the segment: read (r), write (w), and execute (x). A hyphen (-) in place of any of these letters
indicates that the corresponding protection is disabled. The final letter indicates the mapping flag
for the memory segment; it is either private (p) or shared (s). For an explanation of these flags,
    see the description of the MAP_PRIVATE and MAP_SHARED flags in Section 49.2. 
    
note: A System V shared memory segment is always marked shared.

3. The hexadecimal offset (in bytes) of the segment within the corresponding mapped file. The
meanings of this and the following two columns will become clearer when we describe the mmap()
system call in Chapter 49. 

note: For a System V shared memory segment, the offset is always 0.

4. The device number (major and minor IDs) of the device on which the corresponding mapped file is
located.

5. The i-node number of the mapped file, or, for System V shared memory segments, the 'identifier'
for the segment.

6. The filename or other identifying tag associated with this memory segment. For a System V shared
memory segment, this consists of the string SYSV concatenated with the shmget() key of the segment
(expressed in hexadecimal). In this example, SYSV is followed by zeros because we created the
segments using the key IPC_PRIVATE (which has the value 0). The string (deleted) that appears after
the SYSV field for a System V shared memory segment is an artifact of the implementation of shared
memory segments. Such segments are created as mapped files in an invisible tmpfs file system
(Section 14.10), and then later unlinked. Shared anonymous memory mappings are implemented in the
same manner. (We describe mapped files and shared anonymous memory mappings in Chapter 49.)

<example> 
Linux (none) 2.6.31-3.2 #2 SMP Wed Dec 10 02:53:42 EST 2014 mips GNU/Linux

cat /proc/sysvipc/shm

key         shmid    perms    size  cpid  lpid nattch uid   gid  cuid  cgid      atime      dtime      ctime
305450176   163844   666      24    1066  1470 4      504   504   504   504 1423064150 1423064135         33

root# ls -al /proc/1066/
lrwxrwxrwx    1 root     root             0 Jan  1 00:12 exe -> /opt/cds/bin/huaweidaemon

root# cat /proc/1066/maps | grep SYS      note: why twice?
2aab3000-2aab4000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)
2aab5000-2aab6000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)

root# cat /proc/1470/maps | grep SYS
2aab2000-2aab3000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)

The diff is 0x1000. 4096.

root# cat /proc/1523/smaps | grep -A 20 2fc37
2fc37000-2fc38000 rw-s 00000000 00:08 163844     /SYSV1234ccc0 (deleted)
Size:                  4 kB
Rss:                   4 kB
Pss:                   1 kB
Shared_Clean:          4 kB
Shared_Dirty:          0 kB
Private_Clean:         0 kB
Private_Dirty:         0 kB
Referenced:            4 kB
Swap:                  0 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB            <pagesize>


{shm-limits}
On Linux, some of the limits can be viewed or changed via files in the /proc file system. See LPI
48.9 for more details. 

Below is a list of the Linux shared memory limits. The system call affected by the limit and the
error that results if the limit is reached are noted in parentheses.

SHMMNI
This is a system-wide limit on the number of shared memory 'identifiers' (in other words, shared
    memory segments) that can be created. (shmget(), ENOSPC)

SHMMIN
This is the minimum size (in bytes) of a shared memory segment. This limit is defined with the value
1 (this can't be changed). However, the effective limit is the system page size. (shmget(), <EINVAL>)

SHMMAX
This is the maximum size (in bytes) of a shared memory segment. The practical upper limit for SHMMAX
depends on available RAM and swap space. (shmget(), <EINVAL>)

SHMALL
This is a system-wide limit on the total number of pages of shared memory. Most other UNIX
implementations don't provide this limit. The practical upper limit for SHMALL depends on available
RAM and swap space. (shmget(), ENOSPC)

<debian-linux>
keitee@debian-keitee:/proc/sys/kernel$ uname -a
Linux debian-keitee 3.2.0-4-486 #1 Debian 3.2.63-2+deb7u2 i686 GNU/Linux
keitee@debian-keitee:/proc/sys/kernel$ ls -al shm*
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmall
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmmax
-rw-r--r-- 1 root root 0 Jan 28 21:25 shmmni
-rw-r--r-- 1 root root 0 Jan 28 21:25 shm_rmid_forced
keitee@debian-keitee:/proc/sys/kernel$ cat shmall
2097152
keitee@debian-keitee:/proc/sys/kernel$ cat shmmax
33554432
keitee@debian-keitee:/proc/sys/kernel$ cat shmmni
4096
keitee@debian-keitee:/proc/sys/kernel$ cat shm_rmid_forced 
0

<source> ucLinux case
/include/linux/shm.h

SHMMAX: shm segment max size in byte,     131,072 (typical value),      33,554,432 (ucLinux)
SHMMIN: shm segment min size in byte,     1 (typical value),            1 (ucLinux)
SHMMNI: shm segment max num in system,    100 (typical value),          4,096 (ucLinux)
SHMSEG: shm segment max size in process,  6 (typical value),            4,096 (ucLinux)


{locking}
A shared memory segment can be locked into RAM, so that it is never swapped out. This provides a
performance benefit, since, once each page of the segment is memory-resident, an application is
guaranteed never to be delayed by a page fault when it accesses the page.

<not-all-support> TODO:
These operations are not specified by SUSv3, and they are not provided on all UNIX implementations.
In versions of Linux before 2.6.10, only privileged (CAP_IPC_LOCK) processes can lock a shared
memory segment into memory. Since Linux 2.6.10, an unprivileged process can lock and unlock a shared
memory segment if its effective user ID matches either the owner or the creator user ID of the
segment and (in the case of SHM_LOCK) the process has a sufficiently high RLIMIT_MEMLOCK resource
limit. See Section 50.2 for details.


={============================================================================
*kt_linux_core_203* ipc: server consideration

From LPI 45.4.

The consideration when IPC server is terminated prematurely.

Suppose a client engages in an extended dialogue with a server, with multiple IPC operations being
performed by each process (e.g., multiple messages exchanged, a sequence of semaphore operations, or
    multiple updates to shared memory). 

What happens if the server process 'crashes' or is deliberately halted and then restarted? 

At this point, it would make no sense to blindly reuse the existing IPC object created by the
previous server process, since the new server process has no knowledge of the historical information
associated with the current state of the IPC object. (For example, there may be a secondary request
    within a message queue that was sent by a client in response to an earlier message from the old
    server process.)

In such a scenario, the only option for the server may be to 'abandon' all existing
clients, 'delete' the IPC objects created by the previous server process, and create new
instances of the IPC objects. 

A newly started server handles the possibility that a previous instance of the server terminated
prematurely by first trying to create an IPC object by specifying both the IPC_CREAT and the
IPC_EXCL flags within the get call. If the get call fails because an object with the specified key
already exists, then the server assumes the object was created by an old server process; it
therefore uses the IPC_RMID ctl operation to delete the object, and once more performs a get call to
create the object. (This may be combined with other steps to ensure that another server process is
    not currently running, such as those described in Section 55.6.)

<example>
// return false if process is in slave mode(client)
// return true if process is in master mode(server)

// if no shm created already, then return true to create one.
// if shm is there, but no one use. so try to delete it and return true.
// if shm is there, but some use it, then return false.

// if no one use it then crash might happened so start it all over again.

static bool do_platform_init( size_t shMemSz )
{
  int    shmid, shmflag;
  struct shmid_ds shmds;

  shmflag = 0666;
  shmid = shmget( SHAREDMEM_KEY, shMemSz, shmflag );

  if ( shmid >= 0 )
  {
    if ( shmctl( shmid, IPC_STAT, &shmds ) < 0 )
    {
      perror( "shmctl - IPC_STAT failed" );
    }
    else
    {
      fprintf( stderr, "%s(): shmds.shm_nattch=%d\n",
               __FUNCTION__, (unsigned)shmds.shm_nattch );
      if ( 0 == shmds.shm_nattch && shmctl( shmid, IPC_RMID, (struct shmid_ds *)NULL ) < 0 )
      {
         fprintf( stderr, "shmctl - destroy failed\n" );
      }
    }
    return (0 == (unsigned)shmds.shm_nattch);
  }
  else
  {
    perror("shmget");
  }
  return true;
}


={============================================================================
*kt_linux_core_250* ipc: posix

One of the POSIX.1b developers' aims was to devise a set of IPC mechanisms that did not suffer the
deficiencies of the System V IPC facilities. These IPC mechanisms are collectively referred to as
POSIX IPC.

note: what deficiencies?


={============================================================================
*kt_linux_core_300* ipc: socket: LPI 56

Sockets are a method of IPC that allow data to be exchanged between applications, either on the same
host (computer) or on different hosts connected by a network.

A socket is created using the socket() system call, which returns a file descriptor used to refer to
the socket in subsequent system calls:

fd = socket(domain, type, protocol);

For all applications described in this book, protocol is 'always' specified as 0.


{communication-domain}
domain determines:

1. the method of identifying a socket (i.e., the format of a socket “address”)

2. the range of communication (i.e., either between applications on the same host or between
    applications on different hosts connected via a network).


OS supports at least the followings: AF stands for address family

Table 56-1: Socket domains

Domain   Communication     Communication           Address format          Address structure
         performed         between applications

AF_UNIX  within kernel     on same host            pathname                sockaddr_un

AF_INET  via IPv4          on hosts connected      32-bit IPv4 address +   sockaddr_in
                           via an IPv4 network     16-bit port number

AF_INET6 via IPv6          on hosts connected      128-bit IPv6 address +  sockaddr_in6
                           via an IPv6 network     16-bit port number


{type}
Every sockets implementation provides at least two types of sockets: stream and datagram.

Table 56-2: Socket types and their properties

Property                         Stream Socket     Datagram Socket 
Reliable delivery?               Y                 N
Message boundaries preserved?    N                 Y
Connection-oriented?             Y                 N

<stream-socket>
Stream sockets (SOCK_STREAM) provide a reliable, bidirectional, byte-stream communication channel.

These socket types are supported in both the UNIX(on same host) and the Internet domains(on
network).

1. Reliable means that we are guaranteed that either the transmitted data will arrive intact at the
receiving application, exactly as it was transmitted by the sender (assuming that neither the
    network link nor the receiver crashes), or that we'll receive notification of a probable failure
in transmission.

2. Bidirectional means that data may be transmitted in either direction between two sockets.

3. Byte-stream means that, as with pipes, there is no concept of message boundaries

Stream sockets operate in connected pairs so described as connection-'oriented'. A stream socket can
be connected to 'only' one peer.

<datagram-socket>
Datagram sockets (SOCK_DGRAM) allow data to be exchanged in the form of 'messages' called datagrams.

With datagram sockets, message boundaries are preserved, but data transmission is not reliable.
Messages may arrive out of order, be duplicated, or not arrive at all.

Datagram sockets are an example of the more generic concept of a 'connectionless' socket since
unlike a stream socket, a datagram socket doesn't need to be connected to another socket in order to
be used.

note:
In the Internet domain, datagram sockets employ the User Datagram Protocol (UDP), and stream sockets
(usually) employ the Transmission Control Protocol (TCP).  So often just use the terms UDP socket
and TCP socket, respectively.


{socket-io}
By default, these system calls 'block' if the I/O operation can't be completed immediately.
Nonblocking I/O is also possible, by using the fcntl() F_SETFL operation (Section 5.3) to enable the
O_NONBLOCK open file status flag.


{stream-example}

Passive socket (server)

  socket()

  bind()

  listen()

  accept()                                      Active socket(client)
      : blocks until client connects
   ...                                             socket()

      : resumes                  <-                connect()

+-------------------------------------------------------------+
| read()                         <-                write()    |
|                       : (possibly multiple) data            |
|                          transfers in either direction      |
| write()                        ->                read()     |
+-------------------------------------------------------------+
  close()                                          close()


<well-known-address>
Typically, we bind a server's socket to a well-known address - that is, a fixed address that is
known in advance to client applications that need to communicate with that server.


{syscalls}
#include <sys/socket.h>

<socket>
int socket(int domain, int type, int protocol);

Creates a new socket. Returns file descriptor on success, or -1 on error.
note: returns fd on success.

<bind>
int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

Binds a socket to an address. Returns 0 on success, or -1 on error

The sockfd argument is a file descriptor obtained from a previous call to socket(). 

<accept>
int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);

Returns file descriptor on success, or -1 on error

The accept() system call accepts an incoming connection on the listening stream socket referred to
by the file descriptor sockfd. If there are no pending connections when accept() is called, the call
blocks until a connection request arrives.

note: listening vs connection socket

The key point to understand about accept() is that it creates a 'new' socket, and it is this new
socket that is connected to the peer socket that performed the connect(). A file descriptor for the
connected socket is returned as the function result of the accept() call. The listening socket
(sockfd) remains open, and can be used to accept 'further' connections. A typical server application
creates one listening socket, binds it to a well-known address, and then handles all client requests
by accepting connections via that socket.

note: this is why accept() requires addr argument

The remaining arguments to accept() 'return' the address of the peer socket. The addr argument
points to a structure that is used to return the socket address. The type of this argument depends
on the socket domain

note: <value-result> argument is in-out argument which is also mentioned in man page.

The addrlen argument is a value-result argument. It points to an integer that, prior to the call,
    must be initialized to the size of the buffer pointed to by addr, 'so' that the kernel 'knows'
    how much space is available to return the socket address. Upon return from accept(), this
    integer is set to 'indicate' the number of bytes of data actually copied into the buffer.

If we are not interested in the address of the peer socket, then addr and addrlen should be
specified as NULL and 0, respectively. If desired, we can retrieve the peer’s address later using
the getpeername() system call, as described in Section 61.5.


{generic-address-structure}
The addr argument is a pointer to a structure specifying the address to which this socket is to be
bound. The type of structure passed in this argument 'depends' on the socket domain. The addrlen
argument specifies the size of the address structure.

For each socket domain, a 'different' structure type is defined to store a socket address. However,
    because system calls such as bind() are 'generic' to 'all' socket 'domains', they must be able
    to accept address structures of any type.

How?

In order to permit this, the sockets API defines a 'generic' address structure, struct sockaddr. The
only purpose for this type is to cast the various domain-specific address structures to a single
type for use as arguments in the socket system calls.

This structure serves as a template for all of the domain-specific address structures.

The value in the family field is 'sufficient' to determine the size and format of the address stored
in the remainder of the structure.

<sockaddr>
/* Structure describing a generic socket address.  */
struct sockaddr
{
  __SOCKADDR_COMMON (sa_);    /* Common data: address family and length.  */
  // note: that is sa_family_t sa_family
  char sa_data[14];           /* Address data.  */
};

#define  __SOCKADDR_COMMON(sa_prefix) \
  sa_family_t sa_prefix##family

<sockaddr_un>
/* Structure describing the address of an AF_LOCAL (aka AF_UNIX) socket.  */
struct sockaddr_un
{
  __SOCKADDR_COMMON (sun_);
  char sun_path[108];   /* Path name.  */          note: bigger size!
};

<sockaddr_in>
/* Structure describing an Internet (IP) socket address. */
#define __SOCK_SIZE__   16    /* sizeof(struct sockaddr) */
struct sockaddr_in {
  __kernel_sa_family_t  sin_family;    /* Address family */
  __be16                sin_port;      /* Port number    */    // 16 bit
  struct in_addr        sin_addr;      /* Internet address */  // 32 bit

  /* Pad to size of `struct sockaddr'. */
  unsigned char   __pad[__SOCK_SIZE__ - sizeof(short int) -
    sizeof(unsigned short int) - sizeof(struct in_addr)];
};

<sockaddr_in6>
struct sockaddr_in6 {
  unsigned short int    sin6_family;    /* AF_INET6 */
  __be16                sin6_port;      /* Transport layer port # */
  __be32                sin6_flowinfo;  /* IPv6 flow information */
  struct in6_addr       sin6_addr;      /* IPv6 address */
  __u32                 sin6_scope_id;  /* scope id (new in RFC2553) */
};


{code-example}

{
  const char *SOCKNAME = "/tmp/mysock";

  int sfd;
  struct sockaddr_un addr;

  sfd = socket(AF_UNIX, SOCK_STREAM, 0); /* Create socket */
  if (sfd == -1)
    errExit("socket");

  memset(&addr, 0, sizeof(struct sockaddr_un)); /* Clear structure */

  addr.sun_family = AF_UNIX; /* UNIX domain address */

  strncpy(addr.sun_path, SOCKNAME, sizeof(addr.sun_path) - 1);

  if (bind(sfd, (struct sockaddr *) &addr, sizeof(struct sockaddr_un)) == -1)
    errExit("bind");
}

// net/socket.c
//
/*
 * We move the socket address to kernel space before we call
 * the protocol layer (having also checked the address is ok).
 */
SYSCALL_DEFINE3(bind, int, fd, struct sockaddr __user *, umyaddr, int, addrlen)
{
  sock = sockfd_lookup_light(fd, &err, &fput_needed);
  if (sock) {
    err = move_addr_to_kernel(umyaddr, addrlen, &address);
    if (err >= 0) {
      err = security_socket_bind(sock,
          (struct sockaddr *)&address,
          addrlen);
      if (!err)
        err = sock->ops->bind(sock,
            (struct sockaddr *)
            &address, addrlen);
    }
    fput_light(sock->file, fput_needed);
  }
  return err;
}


// net/ipv4/af_inet.c
int inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
{
  // note: only used to check size
  if (addr_len < sizeof(struct sockaddr_in))
    goto out;
}


{
  struct sockaddr_storage claddr;

  addrlen = sizeof(struct sockaddr_storage);
  cfd = accept(lfd, (struct sockaddr *) &claddr, &addrlen);
  if (cfd == -1) {
    errMsg("accept");
    continue;
  }
}

/*
 * For accept, we attempt to create a new socket, set up the link
 * with the client, wake up the client, then return the new
 * connected fd. We collect the address of the connector in kernel
 * space and 'move' it to user at the very end. This is unclean because
 * we open the socket then return an error.
 *
 * 1003.1g adds the ability to recvmsg() to query connection pending
 * status to recvmsg. We need to add that support in a way thats
 * clean when we restucture accept also.
 */
SYSCALL_DEFINE4(accept4, int, fd, struct sockaddr __user *, upeer_sockaddr, int __user *, upeer_addrlen, int, flags)
{
  err = move_addr_to_user(&address, len, upeer_sockaddr, upeer_addrlen);
}


={============================================================================
*kt_linux_core_101* ipc-pipe

{pipe}
The pipe is `unnamed-fifo` and is an early form that can be used
`related-processes` such as parent and child. In other words, created using
fork() call. Linux supports uni-directional pipe or half-duplex so need 'two'
pipes for read and write.


<connect-two-process>

$ ls | wc -l

ls                          pipe                           wc
(stdout, fd 1)    ->        unidirectional byte stream  -> (stdin, fd 0)
                  write end                             read end
                  of pipe                               of pipe


<byte-stream-and-sequential> LPI 44.1
When we say that a pipe is a byte stream, we mean that there is no concept of 'messages' or message
boundaries when using a pipe. 

The process reading from a pipe can read 'blocks' of data of any size, regardless of the size of
blocks written by the writing process. Furthermore, the data passes through the pipe 'sequentially'
- bytes are read from a pipe in exactly the order they were written. It is not possible to randomly
access the data in a pipe using lseek().


<block-on-read>
Attempts to read from a pipe that is currently empty 'block' until at least one byte has been
written to the pipe. If the write end of a pipe is closed, then a process reading from the pipe will
see end-of-file (i.e., read() returns 0) once it has read all remaining data in the pipe.


<limited-capacity> <block-on-write> LPI 44
A pipe is simply a 'buffer' maintained in kernel memory. This buffer has a maximum capacity. Once a
pipe is full, further writes to the pipe 'block' until the reader removes some data from the pipe.


<use-large-buffer-size>
SUSv3 makes no requirement about the capacity of a pipe. In Linux kernels before 2.6.11, the pipe
capacity is the same as the system page size (e.g., 4096 bytes on x86-32); since Linux 2.6.11, the
pipe capacity is 65,536 bytes. Other UNIX implementations have different pipe capacities.

This means that pipe is kernel resource meaning that there is copy between kernel and process. In
theory, there is no reason why a pipe couldn't operate with smaller capacities, even with a
single-byte buffer. The reason for employing large buffer sizes is efficiency: each time a writer
fills the pipe, the kernel must perform a context switch to allow the reader to be scheduled so that
it can empty some data from the pipe. Employing a larger buffer size means that fewer context
switches are required.


<create-pipe-using-pipe-call>
#include <unistd.h>

int pipe(int filedes[2]);        Returns 0 on success, or -1 on error


<use-read-and-write>
As with any file descriptor, we can use the read() and write() system calls to perform I/O on the
pipe.


<set-up-pipe>
Parent gets two fds from a pipe() call which are fd[0] for read and fd[1] for write. Create a child
via fork() and the child process inherits copies of its parent's file descriptors. Close unused fds
to create a single channel between parent and child. note: 0 for read and 1 for write which are
fixed.

            parent                 ->                    parent           
                                             
fd[1] write       fd[0] read                 fd[1] write
                                             
   [         pipe       ]                       [         pipe        ]
                                             
            child                                        child
                                             
fd[0] write       fd[0] read                                fd[0] read


int filedes[2];

if (pipe(filedes) == -1) /* Create the pipe */
   errExit("pipe");

switch (fork()) { /* Create a child process */
 case -1:
   errExit("fork");

 case 0: /* Child */
   if (close(filedes[1]) == -1) // close unused 'write' end
     errExit("close");
   /* Child now reads from pipe */
   break;

 default: /* Parent */
   if (close(filedes[0]) == -1) // close unused 'read' end
     errExit("close");
   /* Parent now writes to pipe */
   break;
}


<needs-of-closing-for-reading-process>
If the reading process doesn't close the write end of the pipe, then, after the other process closes
its write descriptor, the reader won't see end-of-file, even after it has read all data from the
pipe. Instead, a read() would block waiting for data, because the kernel knows that there is still
at least one write descriptor open for the pipe. That this descriptor is held open by the reading
process itself is irrelevant; in theory, that process could still write to the pipe, even if it is
blocked trying to read. For example, the read() might be interrupted by a signal handler that writes
data to the pipe. (This is a realistic scenario, as we’ll see in Section 63.5.2.)


<broken-pipe>
When a process tries to write to a pipe for which no process has an open read descriptor, the kernel
sends the SIGPIPE signal to the writing process. By default, this signal kills a process. A process
can instead arrange to catch or ignore this signal, in which case the write() on the pipe fails with
the error EPIPE (broken pipe). Receiving the SIGPIPE signal or getting the EPIPE error is a useful
indication about the status of the pipe, and this is why unused read descriptors for the pipe should
be closed.


<needs-of-closing-for-writing-process>
If the writing process doesn't close the read end of the pipe, then, even after the other process
closes the read end of the pipe, the writing process will still be able to write to the pipe.
Eventually, the writing process will fill the pipe, and a further attempt to write will block
indefinitely.


<wary-of-deadlock>
If employing two pipes for bi-directional buffer, then we need to be wary of deadlocks that may
occur if both processes block while trying to read from empty pipes OR while trying to write to
pipes that are already full.


<pipe2-call>
Starting with kernel 2.6.27, Linux supports a new, nonstandard system call, pipe2(). This system
call performs the same task as pipe(), but supports an additional argument, flags, that can be used
to modify the behavior of the system call. Two flags are supported. The O_CLOEXEC flag causes the
kernel to enable the close-on-exec flag (FD_CLOEXEC) for the two new file descriptors. This flag is
useful for the same reasons as the open() O_CLOEXEC flag described in Section 4.3.1. The O_NONBLOCK
flag causes the kernel to mark both underlying open file descriptions as nonblocking, so that future
I/O operations will be nonblocking. This saves additional calls to fcntl() to achieve the same
result.


<pipe-for-unrelated-process>
There is an exception that pipes can be used to communicate only between related processes. Passing
a file descriptor over a UNIX domain socket (a technique that described in Section 61.13.3) makes it
possible to pass a file descriptor for a pipe to an unrelated process.


<second> to-create-pipe
popen() call which simplfies pipe creation, fork, reading/writing setting. However, need to set
problem to fork in the command line.

The C standard I/O library popen(3) makes it easy for the application programmer to open a pipe to
an external process.

#include <stdio.h>
FILE *popen(const char *command, const char *mode);
int pclose(FILE *stream);

The argument command must be a command that is acceptable to the UNIX shell. The second argument
mode must be the C string "r" for reading or "w" for writing.  No other combination, such as "w+",
is acceptable. 

(reading example)
FILE *p;
char cmd[1000];
/* argv[2] is fname */
sprintf(cmd,"grep 'Time has been updated to (Year:Month' %s | head -1",argv[2]);
p=popen(cmd,"r");
fgets(tmp,sizeof(tmp),p);
pclose(p);

After all, the reason that can use pipe between parent/child is that fds are shared.


={============================================================================
*kt_linux_core_102* ipc-pipe-check-setup

[root@HUMAX 856]# ll /proc/846/fd
...
lr-x------    1 root     root          64 May  6 14:35 68 -> socket:[22102]=
lrwx------    1 root     root          64 May  6 14:45 69 -> socket:[22104]=
l-wx------    1 root     root          64 May  6 14:35 7 -> /var/opt-zinc-var/log/copper.log
lrwx------    1 root     root          64 May  6 14:35 70 -> /var/tmp/zmp-log
lrwx------    1 root     root          64 May  6 14:45 71 -> /var/tmp/zmp-log
lr-x------    1 root     root          64 May  6 14:45 72 -> pipe:[22129]| // note:
lrwx------    1 root     root          64 May  6 14:35 8 -> /dev/fusion0
lrwx------    1 root     root          64 May  6 14:35 9 -> /dev/nexus_proxy

[root@HUMAX 856]# lsof | grep 22129
856	/opt/zinc/bin/bronzemediad.oem	pipe:[22129]
1067	/run/youview/jail/daemons/linearsourced-S3kWYA/opt/zinc-trunk/bin/linearsourced	pipe:[22129]

This shows 'two' ends of pipe.


={============================================================================
*kt_linux_core_103* ipc: fifo

{fifo}
To solve this, fifo was introduced and is called 'named'-pipe since has path name. Means that it
is created in the filesystem as a file. Use usual read and write call. Fifo is either read-only or
write-only.

1. create a fifo using mkfifo call.
2. open a fifo for read or write using open call

<fifo-create>
#include <sys/stat.h>
int mkfifo(const char *pathname, mode_t mode);

<fifo-sync>
Therefore, by default, opening a FIFO for reading (the open() O_RDONLY flag) blocks until another
process opens the FIFO for writing (the open() O_WRONLY flag). Conversely, opening the FIFO for
writing blocks until another process opens the FIFO for reading.  In other words, opening a FIFO
synchronizes the reading and writing processes.

<note-from-nds-fusion-ipc>
FIFOs have certain natural limitations, they are unidirectional, and fragment large message sizes
with no built in support for reassembling the fragments. Furthermore FIFOs are limited in number due
to system resources. The key are 'fragment' and 'limit'-in-number.

Due to the limited number of FIFOs it was determined that there would be one
control FIFO per server (to establish communication from clients) and one pair of unidirectional
FIFOs for every pair of server and client instances (note that there is one instance of a client in
every process that uses that client). This is illustrated below.

server                  client a
- control pipe          -> and <-

                        client b
                        -> and <-

note: KT. do not match with this pic?

To cope with fragmentation and the fact that there may be several interfaces being used on a single
client instance (and multiple components using that client instance in a single process) a protocol
was designed as described below. This protocol is used in both FIFO and TCP/IP IPC communication.


{summary}
When writes, if there is no reading process, broken-pipe. When reads, if there
is no writing process, blocked.


={============================================================================
*kt_linux_core_104* ipc: nonblocking read() and write() on pipes and fifo

LPI 44.10

Summarizes the operation of read() for pipes and FIFOs, and includes the effect of the O_NONBLOCK
flag.

<on-read>
The only difference between blocking and nonblocking reads occurs when no data is present and the
write end is open. In this case, a normal read() blocks, while a nonblocking read() fails with the
error EAGAIN.

Table 44-2: Semantics of 'read'ing n bytes from a pipe or FIFO containing p bytes

O_NONBLOCK  | Data bytes available in pipe or FIFO (p)
enabled?    | p = 0, write end open    | p = 0, write end closed  | p < n        | p >= n
----------------------------------------------------------------------------------------------- 
No          | block                    | return 0 (EOF)           | read p bytes | read n bytes
Yes         | fail (EAGAIN)            | return 0 (EOF)           | read p bytes | read n bytes

note: read(n) gets n or p


<on-write>
The impact of the O_NONBLOCK flag when writing to a pipe or FIFO is made complex by interactions
with the PIPE_BUF limit. The write() behavior is summarized in Table 44-3.

Table 44-3: Semantics of 'write'ing n bytes from a pipe or FIFO

O_NONBLOCK  | read end open
enabled?    | n <= PIFE_BUF
----------------------------------------------------------------------------------------------- 
No          | Atomically write n bytes; may block until sufficient data is read for write() to
            | be performed
Yes         | If sufficient space is available to immediately write n bytes, then write()
            | succeeds atomically; otherwise, it fails (EAGAIN)

note: if there is no space to write, then either blocks or EAGAIN.

O_NONBLOCK  | read end open
enabled?    | n > PIPE_BUF
----------------------------------------------------------------------------------------------- 
No          | Write n bytes; may block until sufficient data read for write() to complete; 
            | data may be interleaved with writes by other processes
Yes         | If there is sufficient space to immediately write some bytes, then write between 1 
            | and n bytes (which may be interleaved with data written by other processes);
            | otherwise, write() fails (EAGAIN)

            | read end closed
              SIGPIPE + EPIPE

The O_NONBLOCK flag causes a write() on a pipe or FIFO to fail (with the error EAGAIN) in any case
where data can't be transferred immediately. 

This means that if we are writing up to PIPE_BUF bytes, then the write() will fail if there is not
sufficient space in the pipe or FIFO, because the kernel can't complete the operation immediately
and can't perform a partial write, since that would break the requirement that writes of up to
PIPE_BUF bytes are 'atomic'.

When writing more than PIPE_BUF bytes at a time, a write is 'not' required to be atomic. For this
reason, write() transfers as many bytes as possible (a partial write) to fill up the pipe or FIFO.
In this case, the return value from write() is the number of bytes actually transferred, and the
'caller' 'must' retry later in order to write the remaining bytes. 

However, if the pipe or FIFO is full, so that not even one byte can be transferred, then write()
fails with the error EAGAIN.

Although blocks of data of any size can be written to a pipe, only writes that do not exceed
PIPE_BUF bytes are guaranteed to be atomic. 


={============================================================================
*kt_linux_core_105* ipc: which one to use

When performing interprocess synchronization, our choice of facility is typically determined by the
functional requirements. When coordinating access to a file, file record locking is usually the best
choice. Semaphores are often the better choice for coordinating access to other types of shared
resource.

{semaphores-versus-pthreads-mutexes}

<1>
Unlike mutex (this mean condition?), semaphore does not get lost when there is no waiting one.

<2>
POSIX semaphores and Pthreads mutexes can both be used to synchronize the actions of threads within
the same process, and their performance is [similar]. 

However, mutexes are usually preferable, because the [ownership] property of mutexes enforces good
structuring of code; only the thread that locks a mutex can unlock it. By contrast, one thread can
increment a semaphore that was decremented by another thread. This flexibility can lead to poorly
structured synchronization designs. For this reason, semaphores are sometimes referred to as the
"gotos" of concurrent programming.

There is one circumstance in which mutexes can't be used in a multithreaded application and
semaphores may therefore be preferable. Because it is async-signalsafe. See
{async-signal-safe-function}. The sem_post() function can be used from within a signal handler to
synchronize with another thread. This is not possible with mutexes, because the Pthreads functions
for operating on mutexes are not asyncsignal-safe. 

However, because it is usually preferable to deal with asynchronous signals by accepting them using
sigwaitinfo() (or similar), rather than using signal handlers (see Section 33.2.4), this advantage
of semaphores over mutexes is seldom required.

<3>
From 30.1.3. Peformance of mutex in ref-LPI. The problem with file locks and semaphores is that they
always require a system call for the lock and unlock operations, and each system call has a small
but appreciable, cost (Section 3.1). By contrast, mutexes are implemented using atomic
machine-language operations; performed on memory locations visible to all threads and require system
calls only in case of lock contention.


On Linux, mutexes are implemented using futexes (an acronym derived from fast user space mutexes),
   and lock contentions are dealt with using the futex() system call. We donât describe futexes in
   this book (they are not intended for direct use in user-space applications), but details can be
   found in [Drepper, 2004 (a)], which also describes how mutexes are implemented using futexes.
   [Franke et al., 2002] is a (now outdated) paper written by the developers of futexes, which
   describes the early futex implementation and looks at the performance gains derived from futexes.


# ============================================================================
#{
={============================================================================
*kt_linux_core_200* ipc: sync: semaphore

{what-is-semaphore}
note: This can be used for process or thread but it is expensive for thread.

System V semaphores are not used to transfer data between processes. Instead, they allow processes
to synchronize their actions. A semaphore is a 'kernel'-maintained 'integer' whose value is never
permitted to fall below 0. A process can decrease or increase the value of a semaphore.  If an
attempt is made to decrease the value of the semaphore below 0, then the kernel blocks the operation
until the semaphore's value increases to a level that permits the operation to be performed. 

The meaning of a semaphore is determined by the application. A process decrements a semaphore from
say, 1 to 0 in order to reserve exclusive access to some 'shared' resource, and after completing work
on the resource, increments the semaphore so that the shared resource is released for use by some
other process. The use of a binary semaphore-a semaphore whose value is limited to 0 or 1-is common.

However, an application that deals with multiple instances of a shared resource would employ a
semaphore whose maximum value equals the 'number' of shared resources. Linux provides both System V
semaphores and POSIX semaphores, which have essentially similar functionality.

1. Setting the semaphore to an absolute value;
2. Adding a number to the current value of the semaphore;            // give
3. Subtracting a number from the current value of the semaphore; and // take
4. Waiting for the semaphore value to be equal to 0.

The last two of these operations may cause the calling process to block. A semaphore has no meaning
in and of itself. Its meaning is determined only by the associations given to it by the processes
using the semaphore.

However, System V semaphores are rendered unusually complex by the fact that they are allocated in
groups called semaphore sets. So move to POSIX semaphore.


{posix-semaphore}
SUSv3 specifies two types of POSIX semaphores:

1. Named semaphores: This type of semaphore has a name. By calling sem_open() with the same name
'unrelated' processes can access the same semaphore.

2. Unnamed semaphores: This type of semaphore doesn't have a name; instead, it resides at an
agreed-upon location in 'memory'. Unnamed semaphores can be shared between processes or between a
group of 'threads'. When shared between processes, the semaphore must reside in a region of (System V,
POSIX, or mmap()) shared memory. When shared between threads, the semaphore may reside in an
area of memory shared by the threads (e.g., on the heap or in a global variable).

POSIX semaphores operate in a manner similar to System V semaphores; that is, a POSIX semaphore is
an integer whose value is not permitted to fall below 0. If a process attempts to decrease the value
of a semaphore below 0(take), then, depending on the function used, the call either blocks or fails
with an error indicating that the operation was not currently possible.

Some systems don't provide a full implementation of POSIX semaphores. A typical restriction is that
only unnamed thread-shared semaphores are supported. That was the situation on Linux 2.4; 

<linux-2-6-support>
With Linux 2.6 and a glibc that provides NPTL, a full implementation of POSIX semaphores is
available.

<named-semaphore>
To work with a named semaphore, we employ the following functions:

1. The sem_open() function opens or creates a semaphore, initializes the semaphore if it is created
by the call, and returns a handle for use in later calls.

#include <fcntl.h> /* Defines O_* constants */
#include <sys/stat.h> /* Defines mode constants */
#include <semaphore.h>

sem_t *sem_open(const char *name, int oflag, ...  /* mode_t mode, unsigned int value */ );

Returns pointer to semaphore on success, or SEM_FAILED on error

2. The sem_post(sem) and sem_wait(sem) functions respectively increment and decrement a semaphore's
value. give and take

3. The sem_getvalue() function retrieves a semaphore's current value.

4. The sem_close() function removes the calling processâs association with a semaphore that it
previously opened.

5. The sem_unlink() function removes a semaphore name and marks the semaphore for deletion when all
processes have closed it.

<named-semaphore-on-linux>
SUSv3 doesn't specify how named semaphores are to be implemented. On Linux, they are created as
small POSIX shared memory objects with names of the form sem.name, in a dedicated tmpfs file system
(Section 14.10) mounted under the directory /dev/shm. This file system has 'kernel'-persistence-the
semaphore objects that it contains will persist, even if no process currently has them open, but
they will be lost if the system is shut down.

<unnamed-semaphore>
The semaphore is made available to the processes or threads that use it by placing it in an area of
memory that they share. Operations on unnamed semaphores use the same functions; sem_wait(),
sem_post(), sem_getvalue(), and so on that are used to operate on named semaphores.

In addition, two further functions are required:

The sem_init() function initializes a semaphore and informs the system of whether the semaphore will
be shared between processes or between the threads of a single process.

The sem_destroy(sem) function destroys a semaphore. These functions should not be used with named
semaphores.

<when-useful-to-use-unnamed>
1. A semaphore that is shared between 'threads' doesn't need a name. Making an unnamed semaphore a
shared (global or heap) variable automatically makes it accessible to all threads.

2. A semaphore that is being shared between 'related' processes doesn't need a name. If a parent
process allocates an unnamed semaphore in a region of shared memory (e.g., a shared anonymous
mapping), then a child automatically inherits the mapping and thus the semaphore as part of the
operation of fork().

3. If we are building a dynamic data structure (e.g., a binary tree), each of whose items requires
an associated semaphore, then the simplest approach is to allocate an unnamed semaphore within each
item. Opening a named semaphore for each item would require us to design a convention for generating
a (unique) semaphore name for each item and to manage those names (e.g., unlinking them when they
are no longer required). note: real example? useful?


={============================================================================
*kt_linux_core_201* sync-mutex

LPI-30

The term critical section is used to refer to a section of 'code' that accesses
a shared resource and whose execution should be 'atomic'; that is, its execution
should not be interrupted by another thread that simultaneously accesses the
same shared resource.

In the below problem, two threads shares the same code. It is about critial
region but what is really protected is the 'data' being manipulated within the
critical region.


Problem
-------------------------------
The kind of problems that can occur when shared resources are not accessed
atomically.

This program creates two threads, each of which executes the same function. The
function executes a loop that repeatedly increments a global variable, glob, by
copying glob into the local variable loc.


Since loc is an automatic variable allocated on the per-thread stack, each
thread has its 'own' copy of this variable.

The vagaries of the kernel's CPU scheduling decisions:
The scheduler time slice for thread 1 expires, and thread 2 commences execution.

In complex programs, this 'nondeterministic' behavior means that such errors may
occur only rarely, be hard to reproduce, and therefore be difficult to find.

static int glob = 0;

staic void threadFunc()
{
  int loc, j;

  for(j=0; j < loops; j++)
  {
    loc = glob;
    loc++;
    glob = loc;
  }
}


Solution?
-------------------------------
It might seem that we could eliminate the problem by replacing the three statements
inside the for loop with a single statement. Looks atomic?

staic void threadFunc()
{
  int loc, j;

  for(j=0; j < loops; j++)
  {
    glob++; /* or: ++glob; */
  }
}

However, on many hardware architectures (e.g., RISC architectures), the compiler
would still need to convert this single statement into machine code whose steps
are equivalent to the three statements inside the loop. In other words, despite
its simple appearance, even a C increment operator may 'not' be atomic.


Mutex
-------------------------------
More generally, mutexes can be used to ensure atomic access to any shared
resource, but protecting shared variables is the most common use.


Mutex Ownership
-------------------------------
A mutex has two states: locked and unlocked. At any moment, *at-most* one thread
may hold the lock on a mutex. Attempting to lock a mutex that is already locked
either blocks or fails with an error depending on the method used to place the
lock.

When a thread locks a mutex, it becomes the 'owner' of that mutex. 'only' the
mutex owner can unlock the mutex. Because of this ownership property, the terms
'acquire' and 'release' are sometimes used synonymously for lock and unlock.


Mutex protocol
-------------------------------
Each thread employs the following protocol for accessing a resource:
1. lock the mutex for the shared resource;
2. access the shared resource; and
3. unlock the mutex.


Cooperative lock
-------------------------------
This means that mutex locking is advisory, rather than mandatory; nothing can
prevent one thread from manipulating the data without first obtaining the mutex.
For example, threads not participating a mutex circle can.


Statically Allocated Mutexes
-------------------------------
A mutex can either be allocated as a static variable or be created dynamically
at run time. Before it can be used, a mutex must always be initialized.

pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;


Dynamically Initializing a Mutex
-------------------------------
See LPI-30.1.5, 6


Locking and Unlocking a Mutex
-------------------------------
After initialization, a mutex is unlocked.

#include <pthread.h>

int pthread_mutex_lock(pthread_mutex_t *mutex);
int pthread_mutex_unlock(pthread_mutex_t *mutex);

Both return 0 on success, or a positive error number on error

If the mutex is currently locked by another thread, then pthread_mutex_lock()
  blocks 'until' the mutex is unlocked, at which point it locks the mutex and
  'returns'.

<indeterminate>
If more than one other thread is waiting to acquire the mutex unlocked by a call
to pthread_mutex_unlock(), it is 'indeterminate' which thread will succeed in
acquiring it.


Mutex Types
-------------------------------
o May not lock the same mutex twice. If do, two may happen for 'default'
type of mutex: mutex-'deadlock' or EDEADLK for error check type. On Linux,
     deadlock by default from {ref-LPI}. Why deadlock?  Because blocked trying
     to lock a mutex that it already owns.

*mutex-unlock-not-locked*
Unlocking a mutex that is not owned(locked) or that is locked by another thread
'undefined' result. note: ownership matters when unlock and semaphore do not
have ownership. Precisely what happens in each of these cases depends on the
type of the mutex.


<case-glib-beg>

// glib-2.40.0/gthread-posix.c
//
// The glib 2.40 changes unlock-not-locked which aborts from glib 2.0 which
// don't have check.

#if !defined(USE_NATIVE_MUTEX)

/**
 * g_mutex_unlock:
 * @mutex: a #GMutex
 *
 * Unlocks @mutex. If another thread is blocked in a g_mutex_lock()
 * call for @mutex, it will become unblocked and can lock @mutex itself.
 *
 * Calling g_mutex_unlock() on a mutex that is not locked by the
 * current thread leads to undefined behaviour.
 */
void
g_mutex_unlock (GMutex *mutex)
{
  gint status;

  if G_UNLIKELY ((status = pthread_mutex_unlock (g_mutex_get_impl (mutex))) != 0)
    g_thread_abort (status, "pthread_mutex_unlock");
}

#endif /* !defined(USE_NATIVE_MUTEX) */


#if defined(USE_NATIVE_MUTEX)

/* Our strategy for the mutex is pretty simple:
 *
 *  0: not in use
 *
 *  1: acquired by one thread only, no contention
 *
 *  > 1: contended
 *
 *
 * As such, attempting to acquire the lock should involve an increment.
 * If we find that the previous value was 0 then we can return
 * immediately.
 *
 * On unlock, we always store 0 to indicate that the lock is available.
 * If the value there was 1 before then we didn't have contention and
 * can return immediately.  If the value was something other than 1 then
 * we have the contended case and need to wake a waiter.
 *
 * If it was not 0 then there is another thread holding it and we must
 * wait.  We must always ensure that we mark a value >1 while we are
 * waiting in order to instruct the holder to do a wake operation on
 * unlock.
 */

void
g_mutex_unlock (GMutex *mutex)
{
  guint prev;

  prev = exchange_release (&mutex->i[0], 0);

  /* 1-> 0 and we're done.  Anything else and we need to signal... */
  if G_UNLIKELY (prev != 1)
    g_mutex_unlock_slowpath (mutex, prev);
}

static void __attribute__((noinline))
g_mutex_unlock_slowpath (GMutex *mutex,
                         guint   prev)
{
  /* We seem to get better code for the uncontended case by splitting
   * this out...
   */
  if G_UNLIKELY (prev == 0)
    {
      fprintf (stderr, "Attempt to unlock mutex that was not locked\n");
      abort ();
    }

  syscall (__NR_futex, &mutex->i[0], (gsize) FUTEX_WAKE_PRIVATE, (gsize) 1, NULL);
}

#endif

// callstack when aborted

#0  0x00007ffff5692107 in __GI_raise (sig=sig@entry=6)
    at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff56934e8 in __GI_abort () at abort.c:89
#2  0x00007ffff739bcea in g_mutex_unlock_slowpath ()
   from /release/debian-8-x86_64/oss/lib/libglib-2.0.so.0
#3  0x00007ffff739bdb6 in g_mutex_unlock ()
   from /release/debian-8-x86_64/oss/lib/libglib-2.0.so.0
#4  0x00007fffd8a94c3f in gst_hls_demux_switch_playlist ()
   from /release/debian-8-x86_64/oss/lib/gstreamer-1.0/libgsthlsdemux.so

<case-glib-end>


PTHREAD_MUTEX_NORMAL

*mutex-self-lock*
(Self-)deadlock detection is not provided for this type of mutex. If a thread
tries to lock a mutex that it has already locked, then 'deadlock' results.
Unlocking a mutex that is not locked or that is locked by another thread
produces 'undefined' results. 

On Linux, both of these operations 'succeed' for this mutex type and this is
PTHREAD_MUTEX_DEFAULT


PTHREAD_MUTEX_ERRORCHECK

All three of the above scenarios cause the relevant Pthreads function to return
an error than blocking or deadlock but 'slower' than a normal so debugging
purpose.


PTHREAD_MUTEX_RECURSIVE

A recursive mutex maintains the concept of a *lock-count* When a thread first
acquires the mutex, the lock count is set to 1. Each subsequent lock operation
by the same thread increments the lock count, and each unlock operation
decrements the count. The mutex is released (i.e., made available for other
    threads to acquire) only when the lock count falls to 0. 

When is it useful?

Still have ownership notion but there is no mutex deadlock or undefined result
as NOMAL type has.


Locking Variants
-------------------------------
Two variants of the pthread_mutex_lock() function which are much less frequently
used:

The pthread_mutex_trylock() function is the same as pthread_mutex_lock(), except
that if the mutex is currently locked, pthread_mutex_trylock() fails, returning
the error EBUSY.

The pthread_mutex_timedlock() function is the same as pthread_mutex_lock(),
    except that the caller can specify an additional argument, abstime, that
    places a limit on the time that the thread will sleep while waiting to
    acquire the mutex. If the time interval specified by its abstime argument
    expires without the caller becoming the owner of the mutex,
    pthread_mutex_timedlock() returns the error ETIMEDOUT.

note: Why less used?

In most well-designed applications, a thread should hold a mutex for only a
short time, so that other threads are not prevented from executing in parallel. 

A thread that uses pthread_mutex_trylock() to periodically poll the mutex to see
if it can be locked risks being starved of access to the mutex while other
queued threads are successively granted to the mutex via pthread_mutex_lock().


Performance of Mutexes
-------------------------------
This is relatively cheap and the performance impact of using a mutex is not
significant in most applications.

The problem with file locks and semaphores is that they always require a system
call for the lock and unlock operations, and each system call has a small, but
appreciable, cost (Section 3.1). By contrast, mutexes are implemented using
atomic machine-language operations (performed on memory locations visible to all
    threads) and require system calls only in case of lock contention.


On Linux, mutexes are implemented using futexes (derived from fast user space
    mutexes), and lock contentions are dealt with using the futex() system call.

We don't describe futexes in this book (they are not intended for direct use in
    user-space applications), but details can be found in [Drepper, 2004 (a)],
   which also describes how mutexes are implemented using futexes. [Franke et
   al., 2002] is a (now outdated) paper written by the developers of futexes,
   which describes the early futex implementation and looks at the performance
   gains derived from futexes.


={============================================================================
*kt_linux_core_202* sync: deadlock

Deadlocks. LPI-30.1.4
-------------------------------
A deadlock is a situation where more than one thread is locking the same set of
mutexes and will remain blocked indefinitely. Waiting for something never
happens. Unrequited love?

Thread A                            Thread B
1. pthread_mutex_lock(mutex1);      1. pthread_mutex_lock(mutex2);
2. pthread_mutex_lock(mutex2);      2. pthread_mutex_lock(mutex1);
blocks                              blocks


Deadlock Examples
-------------------------------
<1> from pipe
If employing this bidirectional communication using two pipes, then we need to
be wary of deadlocks that may occur if both processes block while trying to read
from empty pipes or while trying to write to pipes that are already full.

<2> from fifo.
use-different-fifo-for-read-and-write

Process X                              Process Y
1. Open FIFO A for reading(blocks)     1. Open FIFO A for reading(blocks)
2. Open FIFO B for writing             2. Open FIFO B for writing

When open a fifo to read which is not opend to write but there is no writing
process then reading process is blocked. Therefore, if parent and child opens
different fifos to read, deadlock happens.

use-same-fifo-for-read-and-write-but-the-same-order

Process X                              Process Y
1. Open FIFO A for reading(blocks)     1. Open FIFO B for reading(blocks)
2. Open FIFO B for writing             2. Open FIFO A for writing

The two processes shown in Figure 44-8 are deadlocked. Each process is blocked
waiting to open a FIFO for reading. This blocking would not happen if each
process could perform its second step (opening the other FIFO for writing). So
change order or use non-blocking call.

<3>
Message queues have a limited capacity. This has the potential to cause a couple of problems. One of
these is that multiple simultaneous clients could fill the message queue, resulting in a deadlock
situation, where no new client requests can be submitted and the server is blocked from writing any
responses.

<4> from mutex-deadlock
Locking. If try to lock what is already locked by self, two may happen for
'default' type of mutex: mutex-deadlock or EDEADLK. On Linux, deadlock by
default from {ref-LPI}. Why deadlock? Because blocked trying to lock a mutex
that it already owns. This could happen when exception is raised between lock
and unlock pair.

<5> from lock-free-queue
Used the timed_wait() instead of the simpler wait() to solve a possible deadlock
when Produce() is called between line A and line B in Listing One. Then wait()
    will miss the notify_one() call and have to wait for the next produced
    element to wake up. 'if' this element never comes (no more produced elements
            or if the Produce() call actually waits for Consume() to return),
    there's a deadlock.

<6> mutex-deadlock

Thead A             Thread B
lock(mutex1)        lock(mutex2)
lock(mutex1)        lock(mutex2)


Deadlock Avoid
-------------------------------

o Mutex hierarchy or order

The simplest way to avoid such deadlocks is to define a mutex hierarchy. When
threads can lock the same set of mutexes, they should always lock them in the
same order. Less flexible.

o Try and back off
An alternative strategy that is 'less' frequently used is "try, and then back
off." See above on why try_lock is less used. If try_lock fails, release 'all'
and try again later. This is less efficient than hierarchy approach but can be
more 'flexible' since no need rigid hierarchy.


={============================================================================
*kt_linux_core_202* sync-cond: why condition variable in UNP example

UNP-XX

Consumer and Producer: 01
-------------------------------
Multiple producer(writing) and one consumer(reading). Once writing finishs,
consumer get started. No sync between producer and consumer and only sync for
  producers. So use mutex. 

No sync for reading? NO for this example since reading starts after writing.
However, need to sync reading if want to have right result while writing
happens.  

<ex>

#include <stdio.h>
#include <pthread.h>
#include <sys/errno.h>

#define MAXNITEMS     1000000
#define MAXNTHREADS   100

#define min(a,b) ((a) < (b) ? (a) : (b))
#define max(a,b) ((a) > (b) ? (a) : (b))

void 
Pthread_create
(pthread_t* tid, const pthread_attr_t* attr, void *(*func)(void*), void*arg)
{
  int n;

  if(( n = pthread_create( tid, attr, func, arg )) == 0 )
    return;

  errno = n;
  fprintf( stderr, "pthread_create error(%d)", n );
}

// shared by all threads
int nitems;

struct {
  pthread_mutex_t mutex;
  int buff[MAXNITEMS];
  int nput;                     // next index to write
  int nval;                     // next val to write
} shared = { PTHREAD_MUTEX_INITIALIZER };

void *produce(void *), *consume(void *);

int main( int argc, char** argv )
{
  int i, nthreads, count[MAXNTHREADS]={0};
  pthread_t tid_produce[MAXNTHREADS]={0}, tid_consume;

  if( argc != 3 )
  {
    fprintf( stderr, "usuage: prodcons2 <#items> <#threads>\n");
    exit(1);
  }

  nitems = min( atoi( argv[1] ), MAXNITEMS );
  nthreads = min( atoi( argv[2] ), MAXNTHREADS );

  // start all producer threads
  for( i=0; i < nthreads; ++i )
  {
    count[i] = 0;
    Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
  }

  // wait for all the producer threads
  for( i=0; i < nthreads; ++i ) 
  {
    pthread_join( tid_produce[i], NULL );
    printf("tid[%d] count[%d] = %d\n", tid_produce[i], i, count[i] );
  }

  // start, then wait for the consumer thread
  Pthread_create(&tid_consume, NULL, consume, NULL );
  pthread_join(tid_consume, NULL );

  exit(0);
}

void* produce(void* arg)
{
  printf("run tid[%d] \n", pthread_self());

  for(;;) 
  {
    pthread_mutex_lock( &shared.mutex );

    // nitems is the max num of shared.buff. When buff is full, we are done.
    if( shared.nput >= nitems )
    {
      printf("done tid[%d] \n", pthread_self());
      pthread_mutex_unlock(&shared.mutex);
      return NULL;
    }

    shared.buff[ shared.nput ] = shared.nval;
    shared.nput++;
    shared.nval++;

    pthread_mutex_unlock( &shared.mutex );

    // inc(count[i]) is the num of items that this thread wrote. Why not in
    // the critical section? Because each thread has its own
    *((int*)arg) += 1;
  }
}

void* consume(void* arg)
{
  int i;

  // see use of global vars; nitems and shared.buff
  for( i=0; i < nitems; i++ )
  {
    if( shared.buff[i] != i )
      printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
  }

  printf("consume done\n" );

  return NULL;
}


$ ./a.out 1000000 5
run tid[-1252185280] 
run tid[-1243792576] 
run tid[-1235399872] 
run tid[-1227007168] 
run tid[-1218614464] 
done tid[-1252185280] 
done tid[-1243792576] 
done tid[-1227007168] 
done tid[-1218614464] 
tid[-1218614464] count[0] = 198088
tid[-1227007168] count[1] = 220562
done tid[-1235399872] 
tid[-1235399872] count[2] = 254805
tid[-1243792576] count[3] = 162778
tid[-1252185280] count[4] = 163767
consume done


Consumer and Producer: 02
-------------------------------
Now consumer runs at the same time and runs when there is data to read. All is
accessing the same data and are in the 'same' mutex group. Problem is that
when consumer has a lock and wakes up, consumer do polling or spinning to
check data.

note:
The consumer and producer has the equal opportunity to run since it is
'indeterminate' which thread will succeed in acquiring a lock.


<ex>
Uses shared mutex between producers and consumer. If items to read are ready
then returns. Otherwise, do loops. The changes from the previous is:

int main()
{
  // ...

  // start all producer threads
  for( i=0; i < nthreads; ++i )
  {
    count[i] = 0;
    Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
  }

  // do not waits for producers to finish off and starts consumer right away
  Pthread_create(&tid_consume, NULL, consume, NULL );

  // ...
}

void consume_wait(int i)
{
  for(;;) 
  {
    pthread_mutex_lock(&shared.mutex);

    if( i < shared.nput ) {
      pthread_mutex_unlock(&shared.mutex);

      // item is ready
      return; 
    }

    // item is not ready so continue waiting
    pthread_mutex_unlock(&shared.mutex);
  }
}

void* consume(void* arg)
{
  // reading index
  int i;

  // see use of global vars; nitems and shared.buff
  for( i=0; i < nitems; i++ )
  { >
    // polling until item i is ready
    consume_wait(i);

    if( shared.buff[i] != i )
      printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
  }

  printf("con: done\n" );

  return NULL;
}


Consumer and Producer: 03
-------------------------------
For producers, do not need to wait since when have a lock meaning that it is
time for it to do write what it need to do is to write blindly. However, for
consumers, it is not the same as producers since it have to chceck if items are
available to read to to its work.

That is why it has polling to check but the polling is a waste of cpu time. How
to solve? A mutex is for locking and a cond-var is for waiting. The mutex
provides mutual exclusion for accessing the shared variable, while the condition
variable is used to signal changes in the variable's state.

note:
-. The second example has reading and writing in the same mutex group and all
competes with others. So reading and writing cannot run at the same time.

-. This example uses 'two' mutex group: one for writing and one for reading. Why
two mutexes since there is only one reader? 

This allows two things:

1. Less contention and more chances to run since reading is not in the same
mutex group. 

2. Consumer wakes up when there are items to process and which do not waste a
running slot in polling. 

After all, codition variable is to reduce such waste of polling and blocking.

<ex>

// note: previously
// 
// // shared by all threads
// int nitems;
// 
// struct {
//   pthread_mutex_t mutex;
//   int buff[MAXNITEMS];
//   int nput;                     // next index to write
//   int nval;                     // next val to write
// } shared = { PTHREAD_MUTEX_INITIALIZER };

int nitems;
int buff[MAXNITEMS];

struct {
  pthread_mutex_t mutex;
  int nput;                     // next index to write
  int nval;                     // next val to write
} put = { PTHREAD_MUTEX_INITIALIZER };

struct {
  pthread_mutex_t mutex;
  pthread_cond_t cond;

  // note:
  int ready;

} nready = { PTHREAD_MUTEX_INITIALIZER, PTHREAD_COND_INITIALIZER };

void* produce(void* arg)
{
  printf("pro: run tid[%d] \n", pthread_self());

  for(;;) 
  {
    pthread_mutex_lock( &put.mutex );

    // buff is full, we are done.
    if( put.nput >= nitems )
    {
      printf("pro: no more. done tid[%d] \n", pthread_self());
      pthread_mutex_unlock(&put.mutex);
      return NULL;
    }

    buff[ put.nput ] = put.nval;
    put.nput++;
    put.nval++;

    pthread_mutex_unlock( &put.mutex );

    // note: changed to use cond-var 
    // {
    pthread_mutex_lock( &nready.mutex );

    // "ready" is 0 and means there is no items to read. Now there is one item
    // produced and signal cond-var. So "ready" is 'predicate' and the number of
    // items avaialbe to read ( number of produce - number of consume ).

    if( nready.ready == 0 )
      pthread_cond_signal( &nready.cond );

    nready.ready++;

    pthread_mutex_unlock( &nready.mutex );
    // }

    // inc(count[i]) is the num of items that this thread wrote. Why not in
    // the critical section? Because each thread has its own
    *((int*)arg) += 1;
  }
}

void* consume(void* arg)
{
  int i;

  // see use of global vars; nitems and shared.buff
  for( i=0; i < nitems; i++ )
  { >
    // receieve cond-var
    pthread_mutex_lock( &nready.mutex );

    // Always test the condition again when wakes up because *spurious-wakeups*
    // can occur. Unlock "nready.mutex" and wait. 

    while( nready.ready == 0 ) 
      pthread_cond_wait( &nready.cond, &nready.mutex);

    // When returns from pthread_cond_wait, "nready.mutex" is locked
    // automatically and there is a item to read.
    
    nready.ready--;

    pthread_mutex_unlock( &nready.mutex );

    if( buff[i] != i )
      printf("con: err: buff[%d] = %d\n", i, buff[i] );
  }

  printf("con: done\n" );

  return NULL;
}


Q: Is it possible to implement this using two mutex than using a mutex and a
cond-var? Yes, but more contention.

// Q: If change producer's handling of ready like this, is it better?
// 
// {
//     // note: changed to use cond-var 
//     // {
//     pthread_mutex_lock( &nready.mutex );
// 
//     // "ready" is 0 and means there is no items to read. Now there is one item
//     // produced and signal cond-var. So "ready" is the number of items
//     // avaialbe to read.
// 
//     if( nready.ready == 0 )
//     {
//       nready.ready++;
//       pthread_cond_signal( &nready.cond );
//     }
// 
//     pthread_mutex_unlock( &nready.mutex );
//     // }
// }
//
// NO since other writers cannot updates nready.ready even when done writing.

Q: Better if consumer reads all when has a lock? Yes, like a LPI example.


={============================================================================
*kt_linux_core_202* sync-cond: why condition variable in LPI example

<with-no-cond-var>

note: not a complete code since removed some to show points

/* LPI Chapter 30 */ threads/prod_no_condvar.c

/* prod_no_condvar.c

   A simple POSIX threads producer-consumer example that doesn't use a condition
   variable.
*/

static int avail = 0;

static void *
threadFunc(void *arg)
{
  // each thread has given the number of unit to produce from main argv

  for (j = 0; j < cnt; j++) {
    sleep(1);

    /* Code to produce a unit omitted */

    s = pthread_mutex_lock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_lock");

    avail++;        /* Let consumer know another unit is available */

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_unlock");
  }

  return NULL;
}

int
main(int argc, char *argv[])
{
  /* Create all threads */

  totRequired = 0;
  for (j = 1; j < argc; j++) {
    totRequired += atoi(argv[j]);

    s = pthread_create(&tid, NULL, threadFunc, argv[j]);
    if (s != 0)
      errExitEN(s, "pthread_create");
  }

  /* Use a polling loop to check for available units */

  numConsumed = 0;
  done = FALSE;

  for (;;) {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_lock");

    while (avail > 0) {             /* Consume all available units */

      /* Do something with produced unit */

      numConsumed ++;
      avail--;
      printf("T=%ld: numConsumed=%d\n", (long) (time(NULL) - t),
          numConsumed);

      done = numConsumed >= totRequired;
    }

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_unlock");

    if (done)
      break;

    /* Perhaps do other work here that does not require mutex lock */
  }

  exit(EXIT_SUCCESS);
}


<with-cond-var>

/* prod_condvar.c

   A simple POSIX threads producer-consumer example using a condition variable.
*/

static int avail = 0;

static void *
threadFunc(void *arg)
{
  // each thread has given the number of unit to produce from main argv

  for (j = 0; j < cnt; j++) {
    sleep(1);

    /* Code to produce a unit omitted */

    s = pthread_mutex_lock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_lock");

    avail++;        /* Let consumer know another unit is available */

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_unlock");

    s = pthread_cond_signal(&cond);         /* Wake sleeping consumer */
    if (s != 0)
      errExitEN(s, "pthread_cond_signal");
  }

  return NULL;
}

int
main(int argc, char *argv[])
{
  /* Create all threads */

  totRequired = 0;
  for (j = 1; j < argc; j++) {
    totRequired += atoi(argv[j]);

    s = pthread_create(&tid, NULL, threadFunc, argv[j]);
    if (s != 0)
      errExitEN(s, "pthread_create");
  }

  /* Loop to consume available units */

  numConsumed = 0;
  done = FALSE;

  for (;;) {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_lock");

    while (avail == 0) {            /* Wait for something to consume */
      s = pthread_cond_wait(&cond, &mtx);
      if (s != 0)
        errExitEN(s, "pthread_cond_wait");
    }

    /* At this point, 'mtx' is locked... */

    while (avail > 0) {             /* Consume all available units */

      /* Do something with produced unit */

      numConsumed ++;
      avail--;
      printf("T=%ld: numConsumed=%d\n", (long) (time(NULL) - t),
          numConsumed);

      done = numConsumed >= totRequired;
    }

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
      errExitEN(s, "pthread_mutex_unlock");

    if (done)
      break;

    /* Perhaps do other work here that does not require mutex lock */

  }

  exit(EXIT_SUCCESS);
}


={============================================================================
*kt_linux_core_202* sync-cond: compare two examples

<UNP>
- Two mutex. One for producer and the other for consumer and cond-var.

- Producers can produce in producer mutex and make a signal only when there was
no item for consumer to use. Producer is in consumer mutext for short time to
signal or update predicate.

- Single consumer reads as many item as it can. 

note: Looks that this approach would increase chances of running producer and
consumer concurrently. 

<LPI>
- One mutex for producer, consumer, and cond-var.

- Producers make a signal for each item produced. 

- Single consumer reads as many item as it can. 

note: Since consumer reads as many as it can, some signal gets lost and not
used.


={============================================================================
*kt_linux_core_202* sync-cond: condition variable 

<cond-wait>
// This is blocking on condition and ALWAYS has an associated mutex. Why?
// because both producer and consumer accesses to the shared state variable
// which is linked to that condition so should be synced. In other words, there
// is a natural association of a mutex with a condition varaible.
//
// pthread_cond_wait( &nready.cond, &nready.mutex );
//
// Do three things: unlock a mutex, block a calling thread until signaled, and
// relock mutex when signaled. 
//
// A condition variable holds no state information. It is simply a mechanism for
// communicating information about the application's state.

int pthread_cond_wait( pthread_cond_t *cptr, pthread_mutex_t* mptr);

<cond-can-be-lost>
// Guarantee that at least one of the blocked thread is woken up. If no thread
// is waiting, the signal is 'lost' since condition variable holds no state
// information. 
//
// Q: no cost for too many signals?
//

int pthread_cond_signal( pthread_cond_t *cptr );
int pthread_cond_broadcast( pthread_cond_t *cptr );
int pthread_cond_timedwait( ... );

The LPI said: 
  
Using pthread_cond_broadcast() always yields correct results (since all threads
    should be programmed to handle redundant and spurious wake-ups), but
pthread_cond_signal() can be more efficient. However, pthread_cond_signal()
should be used only if just 'one' of the waiting threads needs to be woken up to
handle the change in state of the shared variable, and it doesn't matter which
one of the waiting threads is woken up. This scenario typically applies when all
of the waiting threads are designed to perform the exactly same task. Given
these assumptions, pthread_cond_signal() can be more efficient than
pthread_cond_broadcast(), because it avoids the following possibility:


={============================================================================
*kt_linux_core_202* sync-cond: order, spurious wakeup

{the-order-of-call}

{LPI} uses different call order for producer as below because {UNP} said that
use _signal before unlock can cause {mutex-deadlock} or {lock-conflict}.
Therefore, POSIX recommends the followings although SUSv3 permits them to be
done in either order and {LPI} said this may yieid 'better' performance in
30.2.2:

<redundant-wakeup>

If the mutex is unlocked only after the condition variable is signaled, the
thread performing pthread_cond_wait() 'may' wake up while the mutex is still
locked, and then immediately go back to sleep again when it 'finds' that the
mutex is 'locked'. This results in two superfluous context switches. Some
implementations eliminate this problem by employing a technique called wait
morphing, which moves the signaled thread from the "condition variable wait
queue" to the "mutex wait queue" without performing a context switch 'if' the
mutex is locked.


<one> redundant wakeup

pthread_mutex_lock( &nready.mutex );

if( nready.ready == 0 )
   pthread_cond_signal( &nready.cond );

nready.ready++;

pthread_mutex_unlock( &nready.mutex );


<two>

pthread_mutex_lock( &nready.mutex );

nready.ready++;

pthread_mutex_unlock( &nready.mutex );

if( nready.ready == 0 )
 pthread_cond_signal( &nready.cond );


<ex> 
1st change: We should always set variable first and then signal the change to
other tasks, not the other way round. Although here it is done under locked
mutex, which makes it safe, however it is still worth following a good practice.

from:
    g_mutex_lock(&writer->mutex);
    {
        old_thread = writer->writer_thread;
        if (old_thread)
        {
            g_cond_signal(&writer->cond);
        }
        writer->writer_thread = NULL;
    }
    g_mutex_unlock(&writer->mutex);

to:
    g_mutex_lock(&writer->mutex);
    {
        old_thread = writer->writer_thread;
        if (old_thread)
        {
            writer->writer_thread = NULL;
            g_cond_signal(&writer->cond);
        }
    }
    g_mutex_unlock(&writer->mutex);

note: This is spurious wakeup since requires to check the predicate,
  "writer_thread".


{check-on-predicate} <spurious-wakeups>

Each condition variable has an associated 'predicate' involving one or more
shared variables. In this example, nready.ready == 0 is the predicate.

This demonstrates a general design 'principle': a pthread_cond_wait() call must
be governed by a while loop rather than an if statement. This is so because, on
return from pthread_cond_wait(), that is when 'signaled', there are 'no'
guarantees about the state of the predicate; therefore, we should immediately
'recheck' the predicate and resume sleeping if it is not in the desired state. 

So use while-loop always to test the condition again when wakes up because
spurious wakeups can occur for following reasons:

while( nready.ready == 0 )
	pthread_cond_wait( &nready.cond, &nready.mutex );

  * Other (consumer) threads may be woken up first. Perhaps several threads were
    waiting to acquire the mutex associated with the condition variable. Even if
    the thread that signaled the mutex set the predicate to the desired state,
    it is still possible that another thread might acquire the mutex first and
    change the state of the associated shared variable(s), and thus the state of
    the predicate.

    note: Likely when there are more readers

  * Designing for "loose" predicates may be simpler. Sometimes, it is easier to
    design applications based on condition variables that indicate 'possibility'
    rather than 'certainty'. In other words, signaling a condition variable would
    mean "there may be something" for the signaled thread to do rather than
    "there is something" to do. Using this approach, the condition variable can
    be signaled based on approximations of the predicate's state, and the
    signaled thread can ascertain if there really is something to do by
    rechecking the predicate.

  * Spurious wake-ups can occur. On some implementations, a thread waiting on a
    condition variable may be woken up even though no other thread actually
    signaled the condition variable. Such spurious wake-ups are a (rare)
  consequence of the techniques required for efficient implementation on some
  multiprocessor systems, and are explicitly 'permitted' by SUSv3.


note:
The redundant wakeup is to check on mutex and the spurious wakeup is to check on
predicate.


={============================================================================
*kt_linux_core_202* sync-cond: example

typedef struct
{
  GstNexusMgr* mgr;

  GstNexusCallbackId callback_id;

  int reason;

  /* The members below are specific to callback_id. */
  int playpump_num;
  EXUS_Callback delegate_callback;
  void* delegate_context;
} GstNexusMgrCallbackInfo;


typedef struct
{
  GstNexusMgrCallbackInfo audio_buffer_available;
  GstNexusMgrCallbackInfo video_buffer_available;
  GstNexusMgrCallbackInfo audio_first_pts;
  GstNexusMgrCallbackInfo video_first_pts;
  GstNexusMgrCallbackInfo audio_error;
  GstNexusMgrCallbackInfo video_error;
} GstNexusMgrCallbackContexts;


// callback from hardware

static void gst_nexus_mgr_generic_1st_level_cb(void* context, int reason)
{
  /* ctx points to a member of mgr->callback_contexts, which we can read
   * but not write from this thread.
   */
  const GstNexusMgrCallbackInfo* ctx = context;

  GstNexusMgrCallbackInfo* posted_cb = g_malloc(sizeof(GstNexusMgrCallbackInfo));
  memcpy(posted_cb, ctx, sizeof(GstNexusMgrCallbackInfo));
  posted_cb->reason = reason;

  gst_nexus_mgr_enqueue_callback(mgr, posted_cb);

  /* mgr->callback_cond is associated with mgr->resource_lock rather than
   * with mgr->pending_callbacks_mutex. We are not allowed to lock
   * mgr->resource_lock in 1st level callbacks. Fortunately, signalling
   * a condition variable doesn't require locking the corresponding mutex.
   */
  g_cond_signal(&mgr->callback_cond);
}


// note: follows the general principle. use while on the predicate which is to
// check if queue has items to process.

static gpointer mgr_callback_thread(gpointer context)
{
  GstNexusMgr* mgr = context;

  g_mutex_lock(&mgr->resource_lock);

  while(!mgr->callback_thread_exiting) {

    // deque cb item
    GstNexusMgrCallbackInfo* cb = mgr_dequeue_callback(mgr);

    // no item in the queue to process. wait and release a lock
    if (!cb){

      // <wait-on-cond>
      g_cond_wait(&mgr->callback_cond, &mgr->resource_lock);
      continue;
    }

    // use cb item
    mgr_2nd_level_cb_table[cb->callback_id](cb);
    g_free(cb);
  }

  g_mutex_unlock(&mgr->resource_lock);

  return NULL;
}


={============================================================================
*kt_linux_core_202* sync-cond: example

Two writer(source) threads which pushes data to each queue and the writer_thread
which is single reader and reads data from each queue and writes it to a
hardware buffer. Since there is a single reader, less concern of spurious
wakeups.

Uses two predicates.

gboolean pes_data_source_produce_data(PesDataSource* source, PesStream* data)
{
  g_mutex_lock(&writer->mutex);
  g_mutex_lock(&source->mutex);

  g_queue_push_tail(source->chunks, chunk);

  g_mutex_unlock(&source->mutex);
  g_cond_signal(&writer->cond);     // <writer-cond-writer-mutex>
  g_mutex_unlock(&writer->mutex);
}

gpointer writer_thread(gpointer context)
{
  ConcurrentPesWriter* writer = context;
  GThread* this_thread = g_thread_self();

  g_mutex_lock(&writer->mutex);

  // note: predicate two which is to check the shutdown.

  // concurrent_pes_writer_start() / _stop() will overwrite
  // writer->writer_thread with another value, which is how we notice we need to
  // shut down.

  while (this_thread == writer->writer_thread)
  {
    GQueue* queue = NULL;
    gint64 timeout;

    if(!g_mutex_trylock (writer->resource_lock)) {
      /* Might not be able to get the resource_lock because another
         thread already has the lock and is trying to shut down this
         writer_thread */
      goto nap_time;
    }

    // note: predicate one which is a queue check

    while ((queue = find_best_queue_for_consumption_locked(writer)))
    {
      // get a buffer
      rc = NEXUS_Playpump_GetBuffer(
          writer->playpump_handle, &buffer, &buffer_size);

      if (buffer_size == 0)
      {
        break;
      }

      // say a write is completed 
      rc = NEXUS_Playpump_WriteComplete(
          writer->playpump_handle, 0, bytes_just_written);
      if (NEXUS_SUCCESS != rc)
      {
        break;
      }
    } // end of write while ((queue = ...))

    g_mutex_unlock (writer->resource_lock);

nap_time:
    // <writer-cond-writer-mutex>
    /* sleep for a while, or until more data is added to a queue */
    g_cond_wait_until(&writer->cond, &writer->mutex, timeout);

  } // end of thread while

  g_mutex_unlock(&writer->mutex);

  return NULL;
}


={============================================================================
*kt_linux_core_202* sync-cond: example use single lock for api and callback

This uses a single mutex, managerMutex, to guard access to APIs and callbacks.

void XX::API1(...)
{
    boost::unique_lock<boost::mutex> statusLock(managerMutex);

    if(activePlayer)
    {
        // Suspend player and wait for StatusEvent saying MR is stopped
        activePlayer->suspend();
        managerCondition.wait(statusLock);
    }

    mediaRouter->addListener(dispatcher, shared_from_this());
}

void XX::API2(...)
{
    const boost::unique_lock<boost::mutex> statusLock(managerMutex);
    ...
}

void XX::CALLBACK(const EventValue::Enum statusEvent)
{
    if(statusEvent == StatusEventValue::stopped)
    {
        boost::unique_lock<boost::mutex> statusLock(managerMutex);
        managerCondition.notify_one();
    }
}


={============================================================================
*kt_linux_core_202* sync: between processes

As with semaphore, mutex and cond-var uses 'global' structure and if these are
shared in a shared memory, these can be used between processes. If not, it is
for threads in a process.

In {ref-LPI} p881, process-shared mutexes and condition variables. They are
'not' available on all UNIX systems, and so are not commonly employed for
process synchronization.

note: As said, if share mutex structure in shared memory, then what is
process-shared mutex?



{implicit-sync}
This is synchronization handled by kernel not by applicaion. Such as pipe and
message-q. For example,

grep pattern chapters.* | wc -l

Writing by producer and reading by consumer are handled by kernel. Does it mean
that grep and wc runs at the same time? not one after one.


==============================================================================
*kt_linux_core_220*	sync: read-write lock

From {ref-UNP} but no such a thing in {ref-LPI}, so may be old way but surely in posix but may be
different to this since this lock is before posix standard. See {ref-UNP} note.

To distinguish between obtaining the read-write lock for reading and for writing. The rules:

1. Any number of threads can hold a given read-write lock for reading as long as no threads holds
the the lock for writing.

2. A read-write lock can be allocated for writing only if no thread hold the lock for reading or
writing.

Stated another way, any threads can have read access to a data as long as no thread is modifying
that. A data can be modified only if no other thread is reading or modifying the data.

In application, the data is read more often than the data is modified, and these can benefit from
using read-write locks instead of mutex locks. 

can provide more concurrency than a plain mutex lock when the data is read more than it is written
and known as shared-exclusive locking since shared lock for reading and exclusive lock for writing.
Multiple readers and one writer problem or readers-writer locks.


{apis}

// to get read lock. blocks the calling if there are writers
int   pthread_rwlock_rdlock(pthread_rwlock_t *);

// to get write lock. blocks the calling if there are readers or writers
int   pthread_rwlock_wrlock(pthread_rwlock_t *);

int   pthread_rwlock_unlock(pthread_rwlock_t *);
int   pthread_rwlock_tryrdlock(pthread_rwlock_t *);
int   pthread_rwlock_trywrlock(pthread_rwlock_t *);

int   pthread_rwlock_init(pthread_rwlock_t *, const pthread_rwlockattr_t *);
int   pthread_rwlock_destroy(pthread_rwlock_t *);
int   pthread_rwlockattr_destroy(pthread_rwlockattr_t *);
int   pthread_rwlockattr_init(pthread_rwlockattr_t *);

// to share the lock between different processes
int   pthread_rwlockattr_setpshared(pthread_rwlockattr_t *, int); pthread_t
int   pthread_rwlockattr_getpshared(const pthread_rwlockattr_t *, int *);


{example-implementation}
\unpv22e.tar\unpv22e\my_rwlock_cancel\

Can be implemented using mutexes and condition variables. This is an implementation which gives
preference to waiting writers but there are other alternatives.

typedef struct {
	pthread_mutex_t 	rw_mutex;			// lock on this struct
	pthread_cond_t 	rw_condreaders;	// for reader threads waiting
	pthread_cond_t 	rw_condwriters;	// for writer threads waiting

	// [KT]
	// when struct is inited, set to RW_MAGIC and used by all functions to check that the caller is
	// passing a pointer to an initialized lock and set to 0 when the lock is destroyed.
	int 					rw_magic;
	int 					rw_nwaitreaders;
	int 					rw_nwaitwriters;

	// the current status of the read-write lock. only one of these can exist at a time: -1 indicates
	// a write lock, 0 is lock available, and an value greater than 0 menas that many read locks are
	// held.
	int 					rw_refcount; 
} pthread_rwlock_t;


#define RW_MAGIC 0x19283746

/* init and destroy */
int
pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		result;

	if (attr != NULL)
		return(EINVAL);		/* not supported */

	if ( (result = pthread_mutex_init(&rw->rw_mutex, NULL)) != 0)
		goto err1;
	if ( (result = pthread_cond_init(&rw->rw_condreaders, NULL)) != 0)
		goto err2;
	if ( (result = pthread_cond_init(&rw->rw_condwriters, NULL)) != 0)
		goto err3;
	rw->rw_nwaitreaders = 0;
	rw->rw_nwaitwriters = 0;
	rw->rw_refcount = 0;
	rw->rw_magic = RW_MAGIC;

	return(0);

err3:
	pthread_cond_destroy(&rw->rw_condreaders);
err2:
	pthread_mutex_destroy(&rw->rw_mutex);
err1:
	return(result);			/* an errno value */
}

void
Pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		n;

	if ( (n = pthread_rwlock_init(rw, attr)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_init error");
}

int
pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);
	if (rw->rw_refcount != 0 ||
		rw->rw_nwaitreaders != 0 || rw->rw_nwaitwriters != 0)
		return(EBUSY);

	pthread_mutex_destroy(&rw->rw_mutex);
	pthread_cond_destroy(&rw->rw_condreaders);
	pthread_cond_destroy(&rw->rw_condwriters);
	rw->rw_magic = 0;

	return(0);
}

void
Pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	int		n;

	if ( (n = pthread_rwlock_destroy(rw)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_destroy error");
}


// rdlock
// A problem exists in this function: if the calling thread blocks in the call to pthread_cond_wait
// and the thread is then canceled, the thread terminates while it holds the mutex lock, and the
// counter rw_nwaitreaders is wrong. The same problem exists in our implentation of
// pthred_rwlock_wrlock. Can correct these problem in {}

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// 4give preference to waiting writers. cannot get a read lock if a) rw_refcount < 0 (meaning
	// there is a writer holding the lock and b) if threads are waiting to get a write lock.
	// [KT] if. case that there are writers
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}

	// [KT] else. case that there are no writers
	if (result == 0)
		rw->rw_refcount++;		/* another reader has a read lock */

	pthread_mutex_unlock(&rw->rw_mutex);
	return (result);
}

/* tryrdlock */
int
pthread_rwlock_tryrdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0)
		result = EBUSY;			/* held by a writer or waiting writers */
	else
		rw->rw_refcount++;		/* increment count of reader locks */

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


/* wrlock */
int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// [KT] if there are other readers or writers
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}

	// [KT] else there are no readers and writers
	if (result == 0)
		rw->rw_refcount = -1;

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


// unlock. used by both reader and writer
int
pthread_rwlock_unlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount > 0)
		rw->rw_refcount--;			// releasing a reader
	else if (rw->rw_refcount == -1)
		rw->rw_refcount = 0;			// releasing a writer
	else
		err_dump("rw_refcount = %d", rw->rw_refcount); // cannot be since it is to unlock

	// 4give preference to waiting writers over waiting readers
	//
	// {Q} The ref-UNP says: notice that we do not grant any additional read locks as soon as a
	// writer is waiting; otherwise, a stream of continual read requests could block a waiting writer
	// forever. For this reason, we need two separate if tests and cannot write
	//
	// if( rw->rw_nwaitwriters > 0 && rw->rw_refcount == 0 )
	// ...
	// 
	// could also omit the test of rw->rw_refcount, but that can result in calls to
	// pthread_cond_signal when read locks are still allocated, which is less efficient.
	//
	if (rw->rw_nwaitwriters > 0) {
		if (rw->rw_refcount == 0)
			result = pthread_cond_signal(&rw->rw_condwriters);		// signal single writer
	} else if (rw->rw_nwaitreaders > 0)
		result = pthread_cond_broadcast(&rw->rw_condreaders);		// signal all readers

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


{example}

#include	"unpipc.h"
#include	"pthread_rwlock.h"

pthread_rwlock_t	rwlock = PTHREAD_RWLOCK_INITIALIZER;

void	 *thread1(void *), *thread2(void *);
pthread_t	tid1, tid2;

int
main(int argc, char **argv)
{
	void	*status;
	Pthread_rwlock_init(&rwlock, NULL);

	Set_concurrency(2);
	Pthread_create(&tid1, NULL, thread1, NULL);
	sleep(1);		/* let thread1() get the lock */
	Pthread_create(&tid2, NULL, thread2, NULL);

	Pthread_join(tid2, &status);
	if (status != PTHREAD_CANCELED)
		printf("thread2 status = %p\n", status);

	Pthread_join(tid1, &status);
	if (status != NULL)
		printf("thread1 status = %p\n", status);

	printf("rw_refcount = %d, rw_nwaitreaders = %d, rw_nwaitwriters = %d\n",
		   rwlock.rw_refcount, rwlock.rw_nwaitreaders,
		   rwlock.rw_nwaitwriters);
	Pthread_rwlock_destroy(&rwlock);
	/* 4returns EBUSY error if cancelled thread does not cleanup */

	exit(0);
}

void *
thread1(void *arg)
{
	Pthread_rwlock_rdlock(&rwlock);
	printf("thread1() got a read lock\n");
	sleep(3);		/* let thread2 block in pthread_rwlock_wrlock() */
	pthread_cancel(tid2);
	sleep(3);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

void *
thread2(void *arg)
{
	printf("thread2() trying to obtain a write lock\n");
	Pthread_rwlock_wrlock(&rwlock);

	// followings are never get executed since it gets canceled.
	printf("thread2() got a write lock\n");
	sleep(1);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

When run this, the program is hung. The occurred steps are:

1. the second trys to get write lock and blocks on pthread_cond_wait.
2. the first returns from slepp(3) and cancel the second.

3. when the second is canceled while it is blocked in a condition variable wait, the mutex is
reacquired before calling the first cleanup hander (even if not installed any handers, but the mutex
is still reacquired before the thread is canceled.) Therefore, when the secondis canceled, it holds
the mutex lock for the read-write lock.

4. the first calls pthread_rwlock_unlock, but it blocks forever in its call to pthread_mutex_lock
because the mutex is still locked by the first thread that was canceled. [KT] this means cancel
terminates a thread immediately and do not continue the rest in pthread_rwlock_wrlock. Hence still
locked. 

If remove the call to pthread_rwlock_unlock in the thread1 func, the main will print:

rw_refcount = 1, rw_nwaitreaders = 0, rw_nwaitwriters = 1
pthread_rwlock_destroy error: Device busy

The correctio is simple and this is addition to pthread_rwlock_rdlock:

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		pthread_cleanup_push( rwlock_cancelrdwait, (void*)rw); ~
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelrdwait */
static void
rwlock_cancelrdwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitreaders--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelrdwait */ 

int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		pthread_cleanup_push(rwlock_cancelwrwait, (void *) rw); ~
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelwrwait */
static void
rwlock_cancelwrwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitwriters--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelwrwait */


==============================================================================
*kt_linux_core_230*	sync: file-lock

File locks: File locks are a synchronization method explicitly designed to coordinate the actions of
multiple processes operating [on-the-same-file]. They can also be used to coordinate access to other
shared resources. File locks come in two flavors: read (shared) locks and write (exclusive) locks.
Any number of processes can hold a read lock on the same file (or region of a file). However, when
one process holds a write lock on a file (or file region), other processes are prevented from
holding either read or write locks on that file (or file region). Linux provides file-locking
facilities via the flock() and fcntl() system calls. The flock() system call provides a simple
locking mechanism, allowing processes to place a shared or an exclusive lock on an entire file.
Because of its limited functionality, flock() locking facility is rarely used nowadays. The fcntl()
  system call provides record locking, allowing processes to place multiple read and write locks on
  different regions of the same file.



==============================================================================
*kt_linux_core_240* 	sync: common problems when use threads

the big three of threading problems are deadlock, races and starvation.

The simplest deadlock condition is when there are two threads and thread A can't progress until
thread B finishes, while thread B can't progress until thread A finishes. This is usually because
both need the same two resources to progress, A has one and B has the other. Various symmetry
breaking algorithms can prevent this in the two thread or larger circle cases.

Races happen when one thread changes the state of some resource when another thread is not expecting
it (such as changing the contents of a memory location when another thread is part way through
reading, or writing to that memory). Locking methods are the key here. (Some lock free methods
and containers are also good choices for this. As are atomic operations, or transaction
based operations.)

Starvation happens when a thread needs a resource to proceed, but can't get it. The resource is
constantly tied up by other threads and the one that needs it can't get in. The scheduling algorithm
is the problem when this happens. Look at algorithms that assure access.


==============================================================================
*kt_linux_core_250*	sync: reentrant and thread-safe

{thread-safe}
A function is said to be thread-safe if it can safely be invoked by multiple threads at the same
time; put conversely, if a function is not thread-safe, then we canât call it from one thread while
it is being executed in another thread. The same example in {why-need-sync}.

<solution-one> serialization
There are various methods of rendering a function thread-safe. One way is to associate a mutex with
the function or perhaps with all of the functions in a library, if they all share the same global
variables, lock that mutex when the function is called, and unlock it when the mutex returns. This
approach has the virtue of simplicity. On the other hand, it means that only one thread at a time
can execute the function-we say that access to the function is [serialized]. 

The downside is that if the threads spend a significant amount of time executing this function, then
this serialization results in a [reduce-or-loss-of-concurrency], because the threads of a program
can no longer execute in parallel.

<solution-two> critial-section
A more sophisticated solution is to associate the mutex with a shared variable. We then determine
which parts of the function are critical sections that access the shared variable, and acquire and
release the mutex only during the execution of these critical sections. This allows multiple threads
to execute the function at the same time and to operate in parallel, except when more than one
thread needs to execute a critical section.


{non-thread-safe-functions}
To facilitate the development of threaded applications, all of the functions specified in SUSv3 are
required to be implemented in a thread-safe manner, except those listed in Table 31-1.

In ref-LPI, Table 31-1: Functions that SUSv3 does not require to be thread-safe


{reentrant-functions}
Although the use of critical sections to implement thread safety is a significant improvement over
the use of per-function mutexes, it is still somewhat inefficient because there is a cost to locking
and unlocking a mutex. A reentrant function achieves thread safety without the use of mutexes.

<how>
It does this by avoiding the use of global and static variables. Any information that must be
returned to the caller, or maintained between calls to the function, is stored in buffers allocated
by the caller. {Q} Although use a buffer from a caller, still seems to be a problem.

However, not all functions can be made reentrant. The usual reasons are the following:

o By their nature, some functions must access global data structures. The functions in the malloc
library provide a good example. These functions maintain a global linked list of free blocks on the
heap. The functions of the malloc library are made thread-safe through the use of mutexes.

o Some functions (defined before the invention of threads) have an interface that by definition is
nonreentrant, because they return pointers to storage statically allocated by the function, or they
employ static storage to maintain information between successive calls to the same (or a related)
function.

See {signal-reentrant} for more about reentrant.

[KT] In sum, reentrant is bigger notion than thread-safe.

<r_prefix>
For several of the functions that have nonreentrant interfaces, SUSv3 specifies reentrant
equivalents with names ending with the suffix _r. These functions require the caller to allocate a
buffer whose address is then passed to the function and used to return the result. This allows the
calling thread to use a local (stack) variable for the function result buffer.

For example, glibc provides crypt_r(), gethostbyname_r(), getservbyname_r(), getutent_r(),
getutid_r(), getutline_r(), and ptsname_r(). 

However, a portable application can't rely on these functions being present on other
implementations.


{code-reentrant-function}
The most efficient way of making a function thread-safe is to make it [reentrant]. All new library
functions should be implemented in this way. However, for an existing nonreentrant library function
(one that was perhaps designed before the use of threads became common), this approach usually
requires changing the functionâs interface, which means modifying all of the programs that use the
function. 

<why-thread-specific-data>
Thread-specific data is a technique for making an existing function thread-safe without
changing its interface. A function that uses thread-specific data may be slightly less efficient
than a reentrant function, but allows us to leave the programs that call the function unchanged.

<strerror-manpage>
STRERROR(3)               Linux Programmer's Manual              STRERROR(3)

NAME
       strerror, strerror_r - return string describing error number

SYNOPSIS

       #include <string.h>

       char *strerror(int errnum);

       int strerror_r(int errnum, char *buf, size_t buflen);
                   /* XSI-compliant */

       char *strerror_r(int errnum, char *buf, size_t buflen);
                   /* GNU-specific */

   Feature Test Macro Requirements for glibc (see feature_test_macros(7)):

       The XSI-compliant version of strerror_r() is provided if:
       (_POSIX_C_SOURCE >= 200112L || _XOPEN_SOURCE >= 600) && ! _GNU_SOURCE
       Otherwise, the GNU-specific version is provided.

DESCRIPTION

The strerror() function returns a pointer to a string that describes the error code passed in the
argument errnum, possibly using the LC_MESSAGES part of the current locale to select the appropriate
language. (For example, if errnum is EINVAL, the returned description will "Invalid argument".)
This string must not be modified by the application, but may be modified by a subsequent call to
strerror(). No library function, including perror(3), will modify this string.

The strerror_r() function is similar to strerror(), but is thread safe. This function is available
in two versions: an XSI-compliant version specified in POSIX.1-2001 (available since glibc 2.3.4,
but not POSIX-compliant until glibc 2.13), and a GNU-specific version (available since glibc
2.0). The XSI-compliant version is provided with the feature test macros settings shown in
the SYNOPSIS; otherwise the GNU-specific version is provided. If no feature test macros are
explicitly defined, then (since glibc 2.4) _POSIX_SOURCE is defined by default with the value
200112L, so that the XSI- compliant version of strerror_r() is provided by default.

The XSI-compliant strerror_r() is preferred for portable applications. It returns the error string
in the user-supplied buffer buf of length buflen.

The GNU-specific strerror_r() returns a pointer to a string containing the error message. This may
be either a pointer to a string that the function stores in buf, or a pointer to some (immutable)
static string (in which case buf is unused). If the function stores a string in buf, then at most
buflen bytes are stored (the string may be truncated if buflen is too small and errnum is
unknown). The string always includes a terminating null byte ('\0').

RETURN VALUE

The strerror() and the GNU-specific strerror_r() functions return the appropriate error description
string, or an "Unknown error nnn" message if the error number is unknown.

POSIX.1-2001 and POSIX.1-2008 require that a successful call to strerror() shall leave errno
unchanged, and note that, since no function return value is reserved to indicate an error, an
application that wishes to check for errors should initialize errno to zero before the call, and
then check errno after the call.

The XSI-compliant strerror_r() function returns 0 on success. On error, a (positive) error number is
returned (since glibc 2.13), or -1 is returned and errno is set to indicate the error (glibc
versions before 2.13).

ERRORS

EINVAL The value of errnum is not a valid error number.
ERANGE Insufficient storage was supplied to contain the error description string.

ATTRIBUTES

Multithreading (see pthreads(7))
The strerror() function is not thread-safe.
The strerror_r() function is thread-safe.

CONFORMING TO

strerror() is specified by POSIX.1-2001, C89, C99.  strerror_r() is specified by POSIX.1-2001.

The GNU-specific strerror_r() function is a nonstandard extension.

POSIX.1-2001 permits strerror() to set errno if the call encounters an error, but does not specify
what value should be returned as the function result in the event of an error. On some systems,
strerror() returns NULL if the error number is unknown.  On other systems, strerror() returns a
string something like "Error nnn occurred" and sets errno to EINVAL if the error number is
unknown. C99 and POSIX.1-2008 require the return value to be non-NULL.

SEE ALSO
err(3), errno(3), error(3), perror(3), strsignal(3)

COLOPHON

This page is part of release 3.61 of the Linux man-pages project.  A description of the project, and
information about reporting bugs, can be found at http://www.kernel.org/doc/man-pages/.

2013-06-21                      STRERROR(3)

<POSIX-1-2001>
Beginning in 1999, the IEEE, The Open Group, and the ISO/IEC Joint Technical Committee 1
collaborated in the Austin Common Standards Revision Group with the aim of revising and
consolidating the POSIX standards and the Single UNIX Specification. This resulted in the
ratification of POSIX 1003.1-2001, sometimes just called POSIX.1-2001, in December 2001
(subsequently approved as an ISO standard, ISO/IEC 9945:2002).

<thread-specific-data>
1. The function creates a key, which is the means of differentiating the thread-specific data item
used by this function from the thread-specific data items used by other functions. The key is
created by calling the pthread_key_create() function. Creating a key needs to be done only once,
when the first thread calls the function. For this purpose, pthread_once() is employed.
Creating a key doesnât allocate any blocks of thread-specific data.

2. The call to pthread_key_create() serves a second purpose: it allows the caller to specify the
address of the programmer-defined destructor function that is used to deallocate each of the storage
blocks allocated for this key (see the next step). When a thread that has thread-specific data
terminates, the Pthreads API automatically invokes the destructor, passing it a pointer to the data
block for this thread.

<typical-thread-specific-data-implementation>
This is a typical implementation (NPTL is typical). In this implementation, the pthread_key_t value
returned by pthread_key_create() is simply an index into the global array, which we label
pthread_keys

                  -----------------
pthread_keys[0]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
pthread_keys[1]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
pthread_keys[2]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
                  ...

int pthread_setspecific(pthread_key_t key, const void *value);

So key is allocated for each client which is a library function, strerror, in this case and assumes
that this is keys[1]. The data structure for value or buffer for each thread is:

All correspond to                   thread A
pthread_keys[1]    value of key1    tsd[0] | pointer |
                   for thread A  -> tsd[1] | pointer |  -> TSD buffer for strerror in thread A
                                    tsd[2] | pointer |                                         

                                    thread B
                   value of key1    tsd[0] | pointer |
                   for thread B  -> tsd[1] | pointer |  -> TSD buffer for strerror in thread B
                                    tsd[2] | pointer |                                        

                   ...

The TDS buffer is allocated in strerror for each thread and when each thread calls 

void *pthread_getspecific(pthread_key_t key);

The pthread library know the thread and use the key to pick up the entry and the buffer pointer.
When a thread is first created, all of its thread-specific data pointers are initialized to NULL.
This means that when our library function is called by a thread for the first time, it must begin by
using pthread_getspecific() to check whether the thread already has an associated value for key.

[KT] This means that when pthread_key_create is called, it allocates a entry in keys array and also
TSD array for each thread even if some thread is not using strerror.

<strerror-non-thread-safe-version>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#define MAX_ERROR_LEN 256 /* Maximum length of string
                             returned by strerror() */
static char buf[MAX_ERROR_LEN]; /* Statically allocated return buffer */
  char *
strerror(int err)
{
  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1);
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }
  return buf;
}

<user-code>
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#include "tlpi_hdr.h"

static void *
threadFunc(void *arg)
{
  char *str;
  printf("Other thread about to call strerror()\n");
  str = strerror(EPERM);
  printf("Other thread: str (%p) = %s\n", str, str);
  return NULL;
}

int
main(int argc, char *argv[])
{
  pthread_t t;
  int s;
  char *str;
  str = strerror(EINVAL);
  printf("Main thread has called strerror()\n");

  s = pthread_create(&t, NULL, threadFunc, NULL);
  if (s != 0)
    errExitEN(s, "pthread_create");

  s = pthread_join(t, NULL);
  if (s != 0)
    errExitEN(s, "pthread_join");

  printf("Main thread: str (%p) = %s\n", str, str);

  exit(EXIT_SUCCESS);
}

<strerror-thread-safe-version>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#include "tlpi_hdr.h"

static pthread_once_t once = PTHREAD_ONCE_INIT;
static pthread_key_t strerrorKey;

#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread
                             buffer returned by strerror() */

static void /* Free thread-specific data buffer */
q destructor(void *buf)
{
  free(buf);
}

static void /* One-time key creation function */
createKey(void) <2>
{
  int s;
  /* Allocate a unique thread-specific data key and save the address
     of the destructor for thread-specific data buffers */
  s = pthread_key_create(&strerrorKey, destructor); <1>
  if (s != 0)
    errExitEN(s, "pthread_key_create");
}

char *
strerror(int err)
{
  int s;
  char *buf;

  /* Make first caller allocate key for thread-specific data */
  s = pthread_once(&once, createKey); <2>
  if (s != 0)
    errExitEN(s, "pthread_once");

  buf = pthread_getspecific(strerrorKey); <3>
  if (buf == NULL) { /* If first call from this thread, allocate
                        buffer for thread, and save its location */
    buf = malloc(MAX_ERROR_LEN);
    if (buf == NULL)
      errExit("malloc");

    s = pthread_setspecific(strerrorKey, buf); <4>
    if (s != 0)
      errExitEN(s, "pthread_setspecific");
  }

  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); <4>
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }

  return buf;
}

<limitation>
SUSv3 requires that an implementation support at least 128 (_POSIX_THREAD_KEYS_MAX) keys. An
application can determine how many keys an implementation actually supports either via the
definition of PTHREAD_KEYS_MAX (defined in <limits.h>) or by calling sysconf(_SC_THREAD_KEYS_MAX).
Linux supports up to 1024 keys.

Even 128 keys should be more than sufficient for most applications. This is because each library
function should employ only a small number of keysâoften just one. If a function requires multiple
thread-specific data values, these can usually be placed in a single structure that has just one
associated thread-specific data key.


{thread-local-storage}
Like thread-specific data, thread-local storage provides persistent per-thread storage. This
feature is [nonstandard], but it is provided in the same or a similar form on many other UNIX
implementations.

Thread-local storage requires support from the kernel (provided in Linux 2.6), the Pthreads
implementation (provided in NPTL), and the C compiler (provided on x86-32 with gcc 3.3 and later).

The main advantage of thread-local storage is that it is much simpler to use than thread-specific
data. To create a thread-local variable, we simply include the __thread specifier in the declaration
of a global or static variable:

static __thread buf[MAX_ERROR_LEN];

Each thread has its own copy of the variables declared with this specifier. The variables
in a threadâs thread-local storage persist until the thread terminates, at which
time the storage is automatically deallocated.

The following points about the declaration and use of thread-local variables:

o The __thread keyword must immediately follow the static or extern keyword, if either of these is
specified in the variableâs declaration.

o The declaration of a thread-local variable can include an initializer, in the same manner as a
normal global or static variable declaration.

o The C address (&) operator can be used to obtain the address of a thread-local variable.

<revised-strerror-using-tls>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread
                             buffer returned by strerror() */
static __thread char buf[MAX_ERROR_LEN];
/* Thread-local return buffer */
char *
strerror(int err)
{
  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1);
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }
  return buf;
}


==============================================================================
*kt_linux_core_260*	sync: atomic operations

{atomic-operations}

For full articles:
http://www.informit.com/guides/content.aspx?g=cplusplus&seqNum=469

Atomicity

An atomic operation is a sequence of one or more machine instructions that are executed
sequentially, without interruption. By default, any sequence of two or more machine instructions
isn't atomic since the operating system may suspend the execution of the current sequence of
operations in favor of another task. If you want to ensure that a sequence of operations is atomic
you must use some form of locking or other types of synchronization. 

Without that, the only guarantee you have is that a single machine instruction is always atomic. the
CPU will not interrupt a single instruction in the middle. [KT] Not entirely true. 

We can conclude from that minimal guarantee that if you can prove that your compiler translates a
certain C++ statement into a single machine instruction, that C++ statement is naturally atomic
meaning, the programmer doesn't have to use explicit locking to enforce the atomic execution of that
statement.  

Which C++ Statements are Naturally Atomic?

Obviously, there are very few universal rules of thumb because each hardware architecture might
translate the same C++ statement differently. Many textbooks tell you that the unary ++ and --
operators, when applied to integral types and pointers, are guaranteed to be atomic. Historically,
when Dennis Ritchie and Brian Kernighan designed C, they added these operators to the language
because they wanted to take advantage of the fast INC (increment) assembly directive that many
machines supported. However, there is no guarantee in the C or C++ standards that these operators
shall be atomic. Ritchie and Kernighan were more concerned about speed rather than atomicity.

You shouldn't make assumptions about the atomicity of C++ statements without examining the output of
your compiler. In some cases, you might discover that what appears to be a single C++ statement is
in fact translated into a long and complex set of machine instructions. 


Epilog

The multithreading support of C++0x consists of a thread class as well as a standard atomics library
that guarantees the atomicity of logical and arithmetic operations. I will introduce the C++0x
atomics library in a separate column.


From C++11:

Data-dependency ordering: atomics and memory model 	N2664 	GCC 4.4
(memory_order_consume)


From StackOverflow:

The increment-memory machine instruction on an X86 is atomic only if you use it with a LOCK prefix.

x++ in C and C++ doesn't have atomic behavior. If you do unlocked increments, due to races in which
processor is reading and writing X, if two separate processors attempt an increment, you can end up
with just one increment or both being seen (the second processor may have read the initial value,
incremented it, and written it back after the first writes its results back).

I believe that C++11 offers atomic increments, and most vendor compilers have an idiomatic way to
cause an atomic increment of certain built-in integer types (typically int and long); see your
compiler reference manual.

If you want to increment a "large value" (say, a multiprecision integer), you need to do so with
using some standard locking mechanism such as a semaphore.

Note that you need to worry about atomic reads, too. On the x86, reading a 32 or 64 bit value
happens to be atomic if it is 64-bit word aligned. That won't be true of a "large value"; again
you'll need some standard lock.

{rmw-operations}
The RMW(read-modify-write) is:

<from-wikipedia>
A class of atomic operations such as test-and-set, fetch-and-add, and compare-and-swap which both
read a memory location and write a new value into it simultaneously, either with a completely new
value or some function of the previous value. These operations prevent race conditions in
multi-threaded applications. Typically they are used to implement mutexes or semaphores. These
atomic operations are also heavily used in non-blocking synchronization.

{atomic-non-atomic-operations}
http://preshing.com/20130618/atomic-vs-non-atomic-operations/

Much has already been written about atomic operations on the web, usually with a focus on atomic
read-modify-write (RMW) operations. However, those arenât the only kinds of atomic operations. There
are also atomic loads and stores, which are equally important. In this post, Iâll compare atomic
loads and stores to their non-atomic counterparts at both the processor level and the C/C++ language
level. Along the way, weâll clarify the C++11 concept of a âdata raceâ.

Automic operations: automic loads and stores + automic read-modify-write operations

An operation acting on shared memory is atomic if it completes in a single step relative to other
threads. When an atomic store is performed on a shared variable, no other thread can observe the
modification half-complete. When an atomic load is performed on a shared variable, it reads the
entire value as it appeared at a single moment in time. Non-atomic loads and stores do not make
those guarantees.

Without those guarantees, lock-free programming would be impossible, since you could never let
different threads manipulate a shared variable at the same time. We can formulate it as a rule:

Any time two threads operate on a shared variable concurrently, and one of those operations performs
a write, both threads must use atomic operations.

If you violate this rule, and either thread uses a non-atomic operation, youâll have what the C++11
standard refers to as a [data-race] (not to be confused with Javaâs concept of a data race, which is
different, or the more general race condition). [Q] what is the general race condition?

The C++11 standard doesnât tell you why data races are bad; only that if you have one, âundefined
behaviorâ will result (section 1.10.21). The real reason why such data races are bad is actually
quite simple: They result in [torn-reads] and [torn-writes].

A memory operation can be non-atomic because it uses multiple CPU instructions, non-atomic even when
using a single CPU instruction, or non-atomic because youâre writing portable code and you simply
canât make the assumption. Letâs look at a few examples.


<Non-Atomic Due to Multiple CPU Instructions>

Suppose you have a 64-bit global variable, initially zero.

uint64_t sharedValue = 0;

At some point, you assign a 64-bit value to this variable.

void storeValue()
{
    sharedValue = 0x100000002;
}

When you compile this function for 32-bit x86 using GCC, it generates the following machine code.

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	DWORD PTR sharedValue, 2
        mov	DWORD PTR sharedValue+4, 1
        ret
        ...

As you can see, the compiler implemented the 64-bit assignment using two separate machine
instructions. The first instruction sets the lower 32 bits to 0x00000002, and the second sets the
upper 32 bits to 0x00000001. Clearly, this assignment operation is not atomic. If sharedValue is
accessed concurrently by different threads, several things can now go wrong:

- If a thread calling storeValue is preempted between the two machine instructions, it will leave
the value of 0x0000000000000002 in memory â a torn write. At this point, if another thread reads
sharedValue, it will receive this completely bogus value which nobody intended to store.

- Even worse, if a thread is preempted between the two instructions, and another thread modifies
sharedValue before the first thread resumes, it will result in a permanently torn write: the upper
32 bits from one thread, the lower 32 bits from another.

- On multicore devices, it isnât even necessary to preempt one of the threads to have a torn write.
When a thread calls storeValue, any thread executing on a different core could read sharedValue at a
moment when only half the change is visible.

Reading concurrently from sharedValue brings its own set of problems:

uint64_t loadValue()
{
    return sharedValue;
}

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	eax, DWORD PTR sharedValue
        mov	edx, DWORD PTR sharedValue+4
        ret
        ...

Here too, the compiler has implemented the load operation using two machine instructions: The first
reads the lower 32 bits into eax, and the second reads the upper 32 bits into edx. In this case, if
a concurrent store to sharedValue becomes visible between the two instructions, it will result in a
torn read â even if the concurrent store was atomic.

These problems are not just theoretical. Mintomicâs test suite includes a test case called
test_load_store_64_fail, in which one thread stores a bunch of 64-bit values to a single variable
using a plain assignment operator, while another thread repeatedly performs a plain load from the
same variable, validating each result. On a multicore x86, this test fails consistently, as
expected.


<Non-Atomic in a single CPU Instructions>

A memory operation can be non-atomic even when performed by a single CPU instruction. For example,
  the ARMv7 instruction set includes the strd instruction, which stores the contents of two 32-bit
  source registers to a single 64-bit value in memory.

strd r0, r1, [r2]

On some ARMv7 processors, this instruction is not atomic. When the processor sees this instruction,
	it actually performs [two-separate-32-bit-stores] under the hood (section A3.5.3). Once again,
	another thread running on a separate core has the possibility of observing a torn write.
	Interestingly, a torn write is even possible on a single-core device: A system interrupt â say,
	for a scheduled thread context switch â can actually occur between the two internal 32-bit
	stores! In this case, when the thread resumes from the interrupt, it will restart the strd
	instruction all over again.

As another example, itâs well-known that on x86, a 32-bit mov instruction is atomic if the memory
operand is naturally aligned, but non-atomic otherwise. In other words, atomicity is [only]
guaranteed when the 32-bit integer is located at an address which is an exact multiple of 4.


<All C/C++ Operations Are Presumed Non-Atomic>

In C and C++, every operation is presumed non-atomic unless otherwise specified by the compiler or
hardware vendor â even plain 32-bit integer assignment.

uint32_t foo = 0;

void storeFoo()
{
    foo = 0x80286;
}

The language standards have nothing to say about atomicity in this case. Maybe integer assignment is
atomic, maybe it isnât. Since non-atomic operations donât make any guarantees, plain integer
assignment in C is non-atomic by definition.

In practice, we usually know more about our target platforms than that. For example, itâs common
knowledge that on all modern x86, x64, Itanium, SPARC, ARM and PowerPC processors, plain 32-bit
integer assignment is atomic [as-long-as] the target variable is naturally aligned. You can verify it
by consulting your processor manual and/or compiler documentation. In the games industry, I can tell
you that a lot of 32-bit integer assignments rely on this particular guarantee.

Nonetheless, when writing truly portable C and C++, thereâs a long-standing tradition of pretending
that we donât know anything more than what the language standards tell us. Portable C and C++ is
designed to run on every possible computing device past, present and imaginary. Personally, I like
to imagine a machine where memory can only be changed by mixing it up first:

On such a machine, you definitely wouldnât want to perform a concurrent read at the same time as a
plain assignment; you could end up reading a completely random value.

<CPP11>
In C++11, there is finally a way to perform truly portable atomic loads and stores: the C++11 atomic
library. Atomic loads and stores performed using the C++11 atomic library would even work on the
imaginary computer above - even if it means the C++11 atomic library must secretly [lock] a mutex to
make each operation atomic. Thereâs also the Mintomic library which I released last month, which
doesnât support as many platforms, but works on several older compilers, is hand-optimized and is
guaranteed to be lock-free.


Relaxed Atomic Operations

Letâs return to the original sharedValue example from earlier in this post. Weâll rewrite it using
Mintomic so that all operations are performed atomically on every platform Mintomic supports. First,
we must declare sharedValue as one of Mintomicâs atomic data types.

#include <mintomic/mintomic.h>

mint_atomic64_t sharedValue = { 0 };

The mint_atomic64_t type guarantees correct memory alignment for atomic access on each platform.
This is important because, for example, the GCC 4.2 compiler for ARM bundled with Xcode 3.2.5
doesnât guarantee that plain uint64_t will be 8-byte aligned.

In storeValue, instead of performing a plain, non-atomic assignment, we must call
mint_store_64_relaxed.

void storeValue()
{
    mint_store_64_relaxed(&sharedValue, 0x100000002);
}

Similarly, in loadValue, we call mint_load_64_relaxed.

uint64_t loadValue()
{
    return mint_load_64_relaxed(&sharedValue);
}

Using C++11âs terminology, these functions are now data race-free. When executing concurrently,
		there is absolutely no possibility of a torn read or write, whether the code runs on
		ARMv6/ARMv7 (Thumb or ARM mode), x86, x64 or PowerPC. If youâre curious how
		mint_load_64_relaxed and mint_store_64_relaxed actually work, both functions expand to an
		inline cmpxchg8b instruction on x86; for other platforms, consult Mintomicâs implementation.

Hereâs the exact same thing written in C++11 instead:

#include <atomic>

std::atomic<uint64_t> sharedValue(0);

void storeValue()
{
    sharedValue.store(0x100000002, std::memory_order_relaxed);
}

uint64_t loadValue()
{
    return sharedValue.load(std::memory_order_relaxed);
}

Youâll notice that both the Mintomic and C++11 examples use relaxed atomics, as evidenced by the
_relaxed suffix on various identifiers. The _relaxed suffix is a reminder that, just as with plain
loads and stores, no guarantees are made about memory ordering.

The only difference between a relaxed atomic load (or store) and a non-atomic load (or store) is
that relaxed atomics guarantee atomicity. No other difference is guaranteed.

In particular, it is still legal for the memory effects of a relaxed atomic operation to be
reordered with respect to instructions which follow or precede it in program order, either due to
compiler reordering or memory reordering on the processor itself. The compiler could even perform
optimizations on redundant relaxed atomic operations, just as with non-atomic operations. In all
cases, the operation remains atomic.

When manipulating shared memory concurrently, I think itâs good practice to always use Mintomic or
C++11 atomic library functions, even in cases where you know that a plain load or store would
already be atomic on your target platform. An atomic library function serves as a reminder that
elsewhere, the variable is the target of concurrent data access.

Hopefully, itâs now a bit more clear why the Worldâs Simplest Lock-Free Hash Table uses Mintomic
library functions to manipulate shared memory concurrently from different threads.


{lock-free-programming}
http://preshing.com/20120612/an-introduction-to-lock-free-programming/

An Introduction to Lock-Free Programming

Lock-free programming is a challenge, not just because of the complexity of the task itself, but
because of how difficult it can be to penetrate the subject in the first place.

I was fortunate in that my first introduction to lock-free (also known as lockless) programming was
Bruce Dawsonâs excellent and comprehensive white paper, Lockless Programming Considerations. And
like many, Iâve had the occasion to put Bruceâs advice into practice developing and debugging
lock-free code on platforms such as the Xbox 360.

Since then, a lot of good material has been written, ranging from abstract theory and proofs of
correctness to practical examples and hardware details. Iâll leave a list of references in the
footnotes. At times, the information in one source may appear orthogonal to other sources: For
instance, some material assumes sequential consistency, and thus sidesteps the memory ordering
issues which typically plague lock-free C/C++ code. The new C++11 atomic library standard throws
another wrench into the works, challenging the way many of us express lock-free algorithms.

In this post, Iâd like to re-introduce lock-free programming, first by defining it, then by
distilling most of the information down to a few key concepts. Iâll show how those concepts relate
to one another using flowcharts, then weâll dip our toes into the details a little bit. At a
minimum, any programmer who dives into lock-free programming should already understand how to write
correct multithreaded code using mutexes, and other high-level synchronization objects such as
semaphores and events.  

What Is It?

People often describe lock-free programming as programming without mutexes, which are also referred
to as [locks]. Thatâs true, but itâs only [part] of the story. The generally accepted definition, based
on academic literature, is a bit more broad. At its essence, lock-free is a property used to
describe some code, without saying too much about how that code was actually written.

Basically, if some part of your program satisfies the following conditions, then that part can
rightfully be considered lock-free. Conversely, if a given part of your code doesnât satisfy these
conditions, then that part is not lock-free.

<definition>
(This was a flow chart)

Are you programming with multiple threads? or interrupt, signal handlers, etc?
-> Yes

Do the threads access shared memeory?
-> Yes

Can the threads block each other? ie. is there some way to schedule the threads which would
'lock-up' indefinitely?
-> No

It is lock-free programming.

In this sense, the lock in lock-free [does not refer directly to mutexes], but rather to the
possibility of âlocking upâ the entire application in some way, whether itâs deadlock, livelock â or
even due to hypothetical thread scheduling decisions made by your worst enemy. That last point
sounds funny, but itâs key. Shared mutexes are ruled out trivially, because as soon as one thread
obtains the mutex, your worst enemy could simply never schedule that thread again. Of course, real
operating systems donât work that way - weâre merely defining terms.

Hereâs a simple example of an operation which contains no mutexes, but is still not lock-free.
Initially, X = 0. As an exercise for the reader, consider how two threads could be scheduled in a
way such that neither thread exits the loop.

while (X == 0)
{
    X = 1 - X;
}

Nobody expects a large application to be entirely lock-free. Typically, we identify a [specific-set]
of lock-free operations out of the whole codebase. For example, in a lock-free queue, there might be
a handful of lock-free operations such as push, pop, perhaps isEmpty, and so on.

Herlihy & Shavit, authors of The Art of Multiprocessor Programming, tend to express such operations
as class methods, and offer the following succinct definition of lock-free (see slide 150): 
	
"In an infinite execution, infinitely often some method call finishes." In other words, as long as
the program is able to keep calling those lock-free operations, the number of completed calls keeps
increasing, no matter what. It is algorithmically impossible for the system to lock up during those
operations. {Q} Didn't get that.

<why-use>
One important consequence of lock-free programming is that if you suspend a single thread, it will
never prevent other threads from making progress, as a group, through their own lock-free
operations. This hints at the value of lock-free programming when writing interrupt handlers and
real-time systems, where certain tasks must complete within a certain time limit, no matter what
state the rest of the program is in.

A final precision: Operations that are designed to block do not disqualify the algorithm. For
example, a queueâs pop operation may intentionally block when the queue is empty. The remaining
codepaths can still be considered lock-free.

<TODO> there is more in this page.


==============================================================================
*kt_linux_core_261*	sync: ref: locks aren't slow; lock contention is

<contention-and-frequency>
For example, this post measures the performance of a lock under heavy conditions: each thread must
hold the lock to do any work (high contention), and the lock is held for an extremely short interval
of time (high frequency)

But donât disregard locks yet. One good example of a place where locks perform admirably, in real
software, is when protecting the memory allocator. Doug Leaâs Malloc is a popular memory allocator
in video game development, but itâs single threaded, so we need to protect it using a lock. During
gameplay, itâs not uncommon to see multiple threads hammering the memory allocator, say around 15000
times per second. While loading, this figure can climb to 100000 times per second or more. Itâs not
a big problem, though. As youâll see, locks handle the workload like a champ.
{Q} what's this memory allocator?

Lock Contention Benchmark

In this test, we spawn a thread which generates random numbers, using a custom Mersenne Twister
implementation. For example, suppose we want to acquire the lock 15000 times per second, and keep it
held 50% of the time. (whole working time) 

This is code example to show 50% lock:

QueryPerformanceCounter(&start);
for (;;)
{
  // Do some work without holding the lock
  workunits = (int) (random.poissonInterval(averageUnlockedCount) + 0.5f);
  for (int i = 1; i < workunits; i++)
    random.integer();       // Do one work unit
  workDone += workunits;

  QueryPerformanceCounter(&end);
  elapsedTime = (end.QuadPart - start.QuadPart) * ooFreq;
  if (elapsedTime >= timeLimit)
    break;

  // Do some work while holding the lock
  EnterCriticalSection(&criticalSection);
  workunits = (int) (random.poissonInterval(averageLockedCount) + 0.5f);
  for (int i = 1; i < workunits; i++)
    random.integer();       // Do one work unit
  workDone += workunits;
  LeaveCriticalSection(&criticalSection);

  QueryPerformanceCounter(&end);
  elapsedTime = (end.QuadPart - start.QuadPart) * ooFreq;
  if (elapsedTime >= timeLimit)
    break;
}

Now suppose we launch two such threads, each running on a different core. Each thread will hold the
lock during 50% of the time when it can perform work, but if one thread tries to acquire the lock
while the other thread is holding it, it will be forced to wait. This is known as lock contention.

(Tested on dual core) When we run the above scenario, we find that each thread spends roughly 25% of
its time waiting, and 75% of its time doing actual work. Together, both threads achieve a net
performance of 1.5x compared to the single-threaded case.

I ran several variations of the test on a 2.66 GHz quad-core Xeon, from 1 thread, 2 threads, all the
way up to 4 threads, each running on its own core. I also varied the duration of the lock, from the
trivial case where the the lock is never held, all the way up to the maximum where each thread must
hold the lock for 100% of its workload. In all cases, the lock frequency remained constant â threads
acquired the lock 15000 times for each second of work performed.

<KT>
The graph shows that go up to 4x when 0% lock duration and down below 1x when 100% lock duration. 0%
means that there is no sharing between threads hence no lock is needed.

The results were interesting. For short lock durations, up to say 10%, the system achieved very high
parallelism. Not perfect parallelism, but close. Locks are fast!

To put the results in perspective, I analyzed the memory allocator lock in a multithreaded game
engine using this profiler. During gameplay, with 15000 locks per second coming from 3 threads, the
lock duration was in the neighborhood of just 2%. Thatâs well within the comfort zone on the left
side of the diagram.

These results also show that once the lock duration passes 90%, thereâs no point using multiple
threads anymore. A single thread performs better. Most surprising is the way the performance of 4
threads drops off a cliff around the 60% mark! This looked like an anomaly, so I re-ran the tests
several additional times, even trying a different testing order. The same behavior happened
consistently. My best hypothesis is that the experiment hits some kind of snag in the Windows
scheduler, but I didnât investigate further.

Lock Frequency Benchmark

Even a lightweight mutex has overhead. As my next post shows, a pair of lock/unlock operations on a
Windows Critical Section takes about 23.5 ns on the CPU used in these tests. Therefore, 15000 locks
per second is low enough that lock overhead does not significantly impact the results. But what
happens as we turn up the dial on lock frequency?

The algorithm offers very fine control over the amount of work performed between one lock and the
next, so I performed a new batch of tests using smaller amounts: from a very fine-grained 10 ns
between locks, all the way up to 31 Î¼s, which corresponds to roughly 32000 acquires per second. Each
test used exactly two threads:

As you might expect, for very high lock frequencies, the overhead of the lock itself begins to dwarf
the actual work being done. Several benchmarks youâll find online, including the one linked earlier
fall into the bottom-right corner of this chart. At such frequencies, youâre talking about some
seriously short lock times â on the scale of a few CPU instructions. The good news is that, when the
work between locks is that simple, a lock-free implementation is more likely to be feasible.

At the same time, the results show that locking up to 320000 times per second (3.1 Î¼s between
    successive locks) is not unreasonable. In game development, the memory allocator may flirt with
this frequency during load times. You can still achieve more than 1.5x parallelism if the lock
duration is short.

Weâve now seen a wide spectrum of lock performance: cases where it performs great, and cases where
the application slows to a crawl. Iâve argued that the lock around the memory allocator in a game
engine will often achieve excellent performance. Given this example from the real world, it cannot
be said that all locks are slow. Admittedly, itâs very easy to abuse locks, but one shouldnât live
in too much fear â any resulting bottlenecks will show up during careful profiling. When you
consider how reliable locks are, and the relative ease of understanding them (compared to lock-free
    techniques), locks are actually pretty awesome sometimes.

The goal of this post was to give locks a little respect where deserved - corrections are welcome. I
also realize that locks are used in a wide variety of industries and applications, and it may not
always be so easy to strike a good balance in lock performance. If youâve found that to be the case
in your own experience, I would love to hear from you in the comments.

{DN}
Then the bottomlien is that try to minimize the lock contention rather than try to find out which
one is fast. Of course, should use better one but the lock contention is far more important.


==============================================================================
*kt_linux_core_262*	sync: ref: always use a lightweight mutex {mutex-vs-semaphore}

http://preshing.com/20111124/always-use-a-lightweight-mutex/
Always Use a Lightweight Mutex

Have started wondering since seen this article which says that critical section is faster than mutex
in windows. This critical section is called as lightweight mutex which is equivalent to pthread
mutex in Linux. Does it mean that pthread mutex is faster than posix semaphore? Tried the same
approach that this article had and it seems not. Like ref-LPI, there is no big difference.

This is the result ran on the real Linux server machine which has multiple processors and as can
see, semaphore is slightly slower than using a mutex but not significant. This seems different from
what this article said.

<snippet>
Now, suppose you have a thread which acquires a Critical Section 100000 times per second, and there
are no other threads competing for the lock. Based on the above figures, you can expect to pay
between 0.2% and 0.6% in lock overhead. Not too bad! At lower frequencies, the overhead becomes
negligible.

<KT> Here shows the result in the graphic which is 58.7ns on Core 2 Duo and 23.5ns on Xeon for
windows critical section. Think that Core 2 means dual core and Xeon is single so means that there
are lock contention for Core 2 and not for Xeon. That's why he mean that there is 0.6% overhead for
locking. Then it's more about experimenting of mutl-core.

Naturally, Ubuntu 11.10 provides a lock implementation using the POSIX Threads API as well. Itâs
another lightweight mutex, based on a Linux-specific construct known as a futex. A pair of
pthread_mutex_lock/pthread_mutex_unlock calls takes about 66 ns on my Core 2 Duo. You can even share
this implementation between processes, but I didnât test that.

In my previous post, I argued against the misconception that locks are slow and provided some data
to support the argument. At this point, it should be clear that if you arenât using a lightweight
mutex, the entire argument goes out the window. Iâm fairly sure that the existence of heavy lock
implementations has only added to this misconception over the years.

Some of you old-timers may point out ancient platforms where a heavy lock was the only
implementation available, or when a semaphore had to be used for the job. But it seems all modern
platforms offer a lightweight mutex. And even if they didnât, you could write your own lightweight
mutex at the application level, even sharing it between processes, provided youâre willing to live
with certain caveats. Youâll find one example in my followup post, Roll Your Own Lightweight Mutex.


{when-with-no-threads}
keitee.park@magnum ~
$ ./sem
sem run
56:645
56:702   [57]
keitee.park@magnum ~
$ ./mtx
mtx run
3:710
3:773    [63]

keitee.park@magnum ~
$ ./sem
sem run
5:663
5:719    [56]
keitee.park@magnum ~
$ ./mtx
mtx run
6:894
6:957    [63]

keitee.park@magnum ~
$ ./sem
sem run
8:103
8:160    [57]
keitee.park@magnum ~
$ ./mtx
mtx run
9:294
9:356    [62]

<for-semaphore-example>
#include <stdio.h>
#include <time.h>
#include <fcntl.h>
#include <semaphore.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

  unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

int main()
{
  int s = 0, loop = 0;
  int flags, opt;
  mode_t perms;
  unsigned int value;
  sem_t *sem;

  flags = O_CREAT | O_EXCL;
  perms = S_IRUSR | S_IWUSR;

  fprintf( stderr, "sem run\n");

  // will create /dev/shm/sem.demo
  sem = sem_open("/demo", flags, perms, 1 );
  if(sem == SEM_FAILED)
    fprintf( stderr, "fail on sem_open");

  get_time_ms();

  for(loop = 0; loop < MAXLOOP; loop++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "fail on sem_wait");

    s = sem_post(sem);
    if (s != 0)
    fprintf(stderr, "fail on sem_post");
  }

  get_time_ms();

  s = sem_unlink("/demo");
  if(s != 0)
    fprintf(stderr, "fail on sem_unlink\n" );

}

<for-mutex-example>
#include <stdio.h>
#include <time.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

  unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;

int main()
{
  int s = 0, loop = 0;

  fprintf( stderr, "mtx run\n");

  get_time_ms();

  for(loop = 0; loop < MAXLOOP; loop++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "pthread_mutex_unlock");
  }

  get_time_ms();
}


{when-with-two-threads}
keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
14:946
15:370      [424]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
24:498
24:829      [331]
main: this is the end

keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
33:705
34:207      [502]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
35:993
36:460      [467]
main: this is the end

keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
2:673
3:125       [452]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
4:792
5:224       [432]
main: this is the end

<for-semaphore-example>
#include <stdio.h>
#include <time.h>
#include <fcntl.h>
#include <semaphore.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

	unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static sem_t *sem;
static unsigned int loopA;
static unsigned int loopB;

static void* threadFuncA( void *arg )
{
  int s = 0;

  for(loopA = 0; loopA < MAXLOOP; loopA++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "TA: fail on sem_wait\n");

    s = sem_post(sem);
    if (s != 0)
    fprintf( stderr, "TA: fail on sem_post\n");
  }
}

static void* threadFuncB( void *arg )
{
  int s = 0;

  for(loopB = 0; loopB < MAXLOOP; loopB++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "TB: fail on sem_wait\n");

    s = sem_post(sem);
    if (s != 0)
    fprintf( stderr, "TB: fail on sem_post\n");
  }
}

int main()
{
  int flags, opt;
  mode_t perms;
  pthread_t tA, tB;
  int s;

  flags = O_CREAT | O_EXCL;
  perms = S_IRUSR | S_IWUSR;

  // will create /dev/shm/sem.demo
  sem = sem_open("/demo", flags, perms, 1 );
  if(sem == SEM_FAILED)
    fprintf( stderr, "main: fail on sem_open\n");

  fprintf( stderr, "main: this is the second sem run\n");
  get_time_ms();

  s = pthread_create( &tA, NULL, threadFuncA, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(A)\n");

  s = pthread_create( &tB, NULL, threadFuncB, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(B)\n");

  s = pthread_join( tA, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(A)\n");

  s = pthread_join( tB, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(B)\n");

  get_time_ms();
  fprintf( stderr, "main: this is the end\n");

  s = sem_unlink("/demo");
  if(s != 0)
    fprintf(stderr, "main: fail on sem_unlink\n" );
}

<for-mutex-example>
#include <stdio.h>
#include <time.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

	unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;
static unsigned int loopA;
static unsigned int loopB;

static void* threadFuncA( void *arg )
{
  int s = 0;

  for(loopA = 0; loopA < MAXLOOP; loopA++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "TA: pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "TA: pthread_mutex_unlock");
  }
}

static void* threadFuncB( void *arg )
{
  int s = 0;

  for(loopB = 0; loopB < MAXLOOP; loopB++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "TB: pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "TB: pthread_mutex_unlock");
  }
}

int main()
{
  pthread_t tA, tB;
  int s;

  fprintf( stderr, "main: this is the second mtx run\n");
  get_time_ms();

  s = pthread_create( &tA, NULL, threadFuncA, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(A)\n");

  s = pthread_create( &tB, NULL, threadFuncB, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(B)\n");

  s = pthread_join( tA, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(A)\n");

  s = pthread_join( tB, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(B)\n");

  get_time_ms();
  fprintf( stderr, "main: this is the end\n");
}


={============================================================================
*kt_linux_core_263*	sync: ref: lock-free code: a false sense of security

{one}
http://www.drdobbs.com/cpp/lock-free-code-a-false-sense-of-security/210600279

Lock-Free Code: A False Sense of Security By Herb Sutter, September 08, 2008

Writing lock-free code can confound anyoneâeven expert programmers, as Herb shows this month.

Given that lock-based synchronization has serious problems [1], it can be tempting to think
lock-free code must be the answer. Sometimes that is true. In particular, it's useful to have
libraries provide hash tables and other handy types whose implementations are internally
synchronized using lock-free techniques, such as Java's ConcurrentHashMap, so that we can use those
types safely from multiple threads without external synchronization and without having to understand
the subtle lock-free implementation details.
{Q} Java's ConcurrentHashMap

<two-drawbacks-for-lock-free-code>
But replacing locks wholesale by writing your own lock-free code is not the answer. Lock-free code
has two major drawbacks. First, it's not broadly useful for solving typical problems-lots of basic
data structures, even doubly linked lists, still have no known lock-free implementations. Coming up
with a new or improved lock-free data structure will still earn you at least a published paper in a
refereed journal, and sometimes a degree.

Second, it's hard even for experts. It's easy to write lock-free code that appears to work, but it's
very difficult to write lock-free code that is correct and performs well. Even good magazines and
refereed journals have published a substantial amount of lock-free code that was actually broken in
subtle ways and needed correction.

To illustrate, let's dissect some peer-reviewed lock-free code that was published here in DDJ just
two months ago [2]. The author, Petru Marginean, has graciously allowed me to dissect it here so
that we can see what's wrong and why, what lessons we should learn, and how to write the code
correctly. That someone as knowledgable as Petru, who has published many good and solid articles,
can get this stuff wrong should be warning enough that lock-free coding requires great care.
<KT> See Lock-Free Queues in below.

A Limited Lock-Free Queue

<limitation-or-assumption-of-q>
Marginean's goal was to write a limited lock-free queue that can be used safely without internal or
external locking. To simplify the problem, the article imposed some significant restrictions,
including that the queue must only be used from two threads with specific roles: one Producer thread
that inserts into the queue, and one Consumer thread that removes items from the queue.

Marginean uses a nice technique that is designed to prevent conflicts between the writer and reader:

o The producer and consumer always work in separate parts of the underlying list, so that their work
won't conflict. At any given time, the first "unconsumed" item is the one after the one iHead refers
to, and the last (most recently added) "unconsumed" item is the one before the one iTail refers to.

o The consumer increments iHead to tell the producer that it has consumed another item in the queue.

o The producer increments iTail to tell the consumer that another item is now available in the
queue. <o> Only the producer thread ever actually modifies the queue.<o> That means the producer is
responsible, not only for adding into the queue, but also for removing consumed items. To maintain
separation between the producer and consumer and prevent them from doing work in adjacent nodes, the
producer won't clean up the most recently consumed item (the one referred to by iHead).

<KT> The consumer only changes iterator and do not modify list itself.

Q     ...   <- begin()
      ...
      ...
      [ ]   <- head, -> consumer, dummy. uses tail to check empty, publish head and consume.
      [ ]   <- first unconsumed item
      ...
      [ ]   <- last unconsumed item
      [ ]   <- tail, <- producer, end(). add, publish tail and uses head to trim unused. 
               end() and push_back()

The idea is reasonable; only the implementation is fatally flawed. Here's the original code, written
in C++ and using an STL doubly linked list<T> as the underlying data structure. I've reformatted the
code slightly for presentation, and added a few comments for readability: 

// Original code from [1] (broken without external locking)
//
template <typename T>
struct LockFreeQueue {
  private:
    std::list<T> list;
    typename std::list<T>::iterator iHead, iTail;

  public:
    LockFreeQueue() {
      list.push_back(T());        // add dummy separator
      iHead = list.begin();
      iTail = list.end();
    }

    // Produce is called on the producer thread only:

    void Produce(const T& t) {
      list.push_back(t);               // add the new item
      iTail = list.end();              // <publish> it
      list.erase(list.begin(), iHead); // trim unused nodes
    }

    // Consume is called on the consumer thread only:

    bool Consume(T& t) {
      typename std::list<T>::iterator iNext = iHead;
      ++iNext;
      if (iNext != iTail) {         // if queue is nonempty
        iHead = iNext;              // <publish> that we took an item
        t = *iHead;                 // copy it back to the caller
        return true;                // and report success
      }
      return false;                 // else report queue was empty
    }
};

<lock-free-variable> <two-key-property>
The fundamental reason that the code is broken is that it has race conditions on both would-be
lock-free variables, iHead and iTail. To avoid a race, a lock-free variable must have two key
properties that we need to watch for and guarantee: atomicity and ordering. These variables are
neither.

Atomicity

First, reads and writes of a lock-free variable must be atomic. For this reason, lock-free variables
are typically no larger than the machine's native word size, and are usually pointers (C++), object
references (Java, .NET), or integers. Trying to use an ordinary list<T>::iterator variable as a
lock-free shared variable isn't a good idea and can't reliably meet the atomicity requirement, as we
will see.

Let's consider the races on iHead and iTail in these lines from Produce and Consume:

void Produce(const T& t) {
  ...
  iTail = list.end();
  list.erase(list.begin(), iHead);
}
 
bool Consume(T& t) {
  ...
  if (iNext != iTail) {
    iHead = iNext;
  ...   
  }
}

If reads and writes of iHead and iTail are not atomic, then Produce could read a partly updated (and
therefore corrupt) iHead and try to dereference it, and Consume could read a corrupt iTail and
fall off the end of the queue. Marginean does note this requirement:

"Reading/writing list<T>::iterator is atomic on the machine upon which you run the application." [2]

Alas, atomicity is necessary but not sufficient (see next section), and not supported by
list<T>::iterator. First, in practice, many list<T>::iterator implementations I examined are [larger]
than the native machine/pointer size, which means that they can't be read or written with atomic
loads and stores on most architectures. Second, in practice, even if they were of an appropriate
size, you'd have to add other decorations to the variable to ensure atomicity, for example to
require that the variable be properly [aligned] in memory.

Finally, the code isn't valid ISO C++. The 1998 C++ Standard said nothing about concurrency, and so
provided no such guarantees at all. The upcoming second C++ standard that is now being finalized
C++0x, does include a memory model and thread support, and explicitly forbids it. In brief, C++0x
says that the answer to questions such as, "What do I need to do to use a list<T> mylist
thread-safely?" is "Same as any other object"âif you know that an object like mylist is shared, you
must externally synchronize access to it, including via iterators, by protecting all such uses with
locks, else you've written a race [3]. Note: Using C++0x's std::atomic<> is not an option for
list<T>::iterator, because atomic<T> requires T to be a bit-copyable type, and STL types and their
iterators aren't guaranteed to be that.

Ordering Problems in Produce

Second, reads and writes of a lock-free variable must occur in an expected order, which is nearly
always the exact order they appear in the program source code. But compilers, processors, and caches
love to optimize reads and writes, and will helpfully reorder, invent, and remove memory reads and
writes unless you prevent it from happening. 

<to-prevent-reordering>
The right prevention happens implicitly when you use mutex locks or ordered atomic variables; {Q}
why mutex prevent this? 

C++0x std::atomic, Java/.NET volatile; you can also do it explicitly, but with considerably more
effort, using ordered API calls e.g., Win32 InterlockedExchange or memory fences/barriers e.g.,
Linux mb.  Trying to write lock-free code without using any of these tools can't possibly work.

Consider again this code from Produce, and ignore that the assignment iTail isn't atomic as we look
for other problems:

list.push_back(t);  // A: add the new item
iTail = list.end(); // B: publish it

This is a classic publication race because lines A and B can be (partly or entirely) reordered. For
example, let's say that some of the writes to the T object's members are delayed until after the
write to iTail, which publishes that the new object is available; then the consumer thread can see a
partly assigned T object.

What is the minimum necessary fix? We might be tempted to write a memory barrier between the two
lines:

// Is this change enough?
list.push_back(t);  // A: add the new item
mb();               // full fence
iTail = list.end(); // B: publish it

Before reading on, think about it and see if you're convinced that this is (or isn't) right.

Have you thought about it? As a starter, here's one issue: Although list.end is probably unlikely to
perform writes, it's possible that it might, and those are side effects that need to be complete
before we publish iTail. 

The general issue is that you can't make assumptions about the side effects of library functions you
call, and you have to make sure they're fully performed before you publish the new state. 

So a slightly improved version might try to store the result of list.end into a local unshared
variable and assign it after the barrier:

// Better, but is it enough?
list.push_back(t);
tmp = list.end();
mb();                // full fence
iTail = tmp;

Unfortunately, this still isn't enough. Besides the fact that assigning to iTail isn't atomic and
that we still have a race on iTail in general, compilers and processors can also invent writes to
iTail that break this code. Let's consider write invention in the context of another problem area:
Consume.

Ordering Problems in Consume

Here's another reordering problem, this time from Consume:

++iNext;
if (iNext != iTail) {
  iHead = iNext;        // C
  t = *iHead;           // D

Note that Consume updates iHead to advertise that it has consumed another item before it actually
  reads the item's value. Is that a problem? We might think it's innocuous, because the producer
  always leaves the iHead item alone to stay at least one item away from the part of the list the
  consumer is using.

It turns out this code is broken regardless of which order we write lines C and D, because the
compiler or processor or cache can reorder either version in unfortunate ways. Consider what happens
if the consumer thread performs a consecutive two calls to Consume: The memory reads and writes
performed by those two calls could be reordered so that iHead is incremented twice before we copy
the two list nodes' values, and then we have a problem because the producer may try to remove
nodes the consumer is still using. [KT] Think that this is a problem besides odering and lock-free
variable.

Note: This doesn't mean the compiler or processor transformations are broken; they're not. Rather
the code is racy and has insufficient synchronization, and so it breaks the memory model guarantees
and makes such transformations possible and visible.

Reordering isn't the only issue. Another problem is that compilers and processors can invent writes,
so they could inject a transient value:

// Problematic compiler/processor transformation
if (iNext != iTail) {
  iHead = 0xDEADBEEF;
  iHead = iNext;
  t = *iHead;

Clearly, that would break the producer thread, which would read a bad value for iHead. More likely,
the compiler or processor might speculate that most of the time iNext != iTail:

// Another problematic transformation
//
__temp = iHead;
iHead = iNext;  // speculatively set to iNext

if (iNext == iTail) {   // note: inverted test!
  iHead = __temp;   // undo if we guessed wrong
} else {
  t = *iHead;

<invariant>
But now iHead could equal iTail, which breaks the essential invariant that iHead must never equal
iTail, on which the whole design depends.

Can we solve these problems by writing line D before C, then separating them with a full fence? Not
entirely: That will prevent most of the aforementioned optimizations, but it will not eliminate all
of the problematic invented writes. More is needed.

Next Steps

These are a sample of the concurrency problems in the original code. Marginean showed a good
algorithm, but the implementation is broken because it uses an inappropriate type and performs
insufficient synchronization/ordering. Fixing the code will require a rewrite, because we need to
change the data structure and the code to let us use proper ordered atomic lock-free variables. But
how? Next month, we'll consider a fixed version. Stay tuned.

Notes

[1] H. Sutter, "The Trouble With Locks," C/C++ Users Journal, March 2005.
(www.ddj.com/cpp/184401930).

[2] P. Marginean, "Lock-Free Queues," Dr. Dobb's Journal, July 2008. (www.ddj.com/208801974).

[3] B. Dawes, et al., "Thread-Safety in the Standard Library," ISO/IEC JTC1/SC22/WG21 N2669, June
2008. (www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2669.htm).


{two}
http://www.drdobbs.com/parallel/lock-free-queues/208801974?pgno=1
Lock-Free Queues By Petru Marginean, July 01, 2008

<note-begin>
This article as written assumes a sequentially consistent model. In particular, the code relies on
specific order of instructions in both Consumer and Producer methods. However, without inserting
proper memory barrier instructions, these instructions can be reordered with unpredictable results
(see, for example, the classic Double-Checked Locking problem).

Another issue is using the standard std::list<T>. While the article mentions that it is the
developer responsibility to check that the reading/writing std::list<T>::iterator is atomic, this
turns out to be too restrictive. While gcc/MSVC++2003 has 4-byte iterators, the MSVC++2005 has
8-byte iterators in Release Mode and 12-byte iterators in the Debug Mode.

The solution to prevent this is to use memory barriers/volatile variables. 
The downloadable code featured at the top of this article has fixed that issue. ~

Many thanks to Herb Sutter who signaled the issue and helped me fix the code. --P.M.
<note-end>

Queues can be useful in a variety of systems involving data-stream processing. Typically, you have a
data source producing dataârequests coming to a web server, market feeds, or digital telephony
packetsâat a variable pace, and you need to process the data as fast as possible so there are no
losses. To do this, you can push data into a queue using one thread and process it using a different
threadâa good utilization of resources on multicore processors. One thread inserts data into the
queue, and the other reads/deletes elements from the queue. Your main requirement is that a
high-rate data burst does not last longer than the system's ability to accumulate data while the
consumer thread handles it. [KT] This means no overlow? 

The queue you use has to be threadsafe to prevent race conditions when inserting/removing data from
multiple threads. For obvious reasons, it is necessary that the queue mutual exclusion mechanism add
as little overhead as possible.

In this article, I present a lock-free queue (the source code for the lockfreequeue class is
available online; see www.ddj.com/code/) in which one thread can write to the queue and another
read from itâat the same time without any locking. /0807/lockfreequeue/

To do this, the code implements these requirements:

<conditions>
o There is a single writer (Producer) and single reader (Consumer). When you have multiple producers
and consumers, you can still use this queue with some external locking. You cannot have multiple
producers writing at the same time (or multiple consumers consuming the data simultaneously), but
you can have one producer and one consumer (2x threads) accessing the queue at the same time
(Responsibility: developer).

o When inserting/erasing to/from an std::list<T>, the iterators for the existing elements must
remain valid (Responsibility: library implementor).

o <Only> one thread modifies the queue; the producer thread both adds/erases elements in the queue
(Responsibility: library implementor).

o Beside the underlying std::list<T> used as the container, the lock-free queue class also holds two
iterators pointing to the not-yet-consumed range of elements; each is modified by one thread and
read by the other (Responsibility: library implementor).

o Reading/writing list<T>::iterator is <atomic> on the machine upon which you run the application. If
they are not on your implementation of STL, you should check whether the raw pointer's operations
are atomic. You could easily replace the iterators to be mentioned shortly with raw pointers in the
code (Responsibility: machine). <KT> ?

<KT> Here big condition is that one producer and consumer model, only producer modify a list, and
updating iterators between threads are atomic.

Because I use Standard C++, the code is portable under the aforementioned "machine" assumption: 

template <typename T>
struct LockFreeQueue
{
    LockFreeQueue();
    void Produce(const T& t);
    bool Consume(T& t);

  private:
    typedef std::list<T> TList;
    TList list;
    typename TList::iterator iHead, iTail;   <KT> why not 'TList::iterator iHead'?
};

Considering how simple this code is, you might wonder how can it be threadsafe. The magic is due to
design, not implementation. Take a look at the implementation of the Produce() and Consume()
methods. The Produce() method looks like this: 

void Produce(const T& t)
{
  list.push_back(t);
  iTail = list.end();
  list.erase(list.begin(), iHead);
}

To understand how this works, mentally separate the data from LockFreeQueue<T> into two groups:

o The list and the iTail iterator, modified by the Produce() method (Producer thread).
o The iHead iterator, modified by the Consume() method (Consumer thread). 

<o>
Produce() is the [only] method that changes the list (adding new elements and erasing the consumed
elements), and it is essential that [only] one thread ever calls Produce()âit's the Producer
thread! The iterator (iTail) (only manipulated by the Producer thread) changes it only after a new
element is added to the list. This way, when the Consumer thread is reading the iTail element, the
new added element is ready to be used. The Consume() method tries to read all the elements between
iHead and iTail (excluding both ends). 
<o>

bool Consume(T& t)
{
  typename TList::iterator iNext = iHead;
  ++iNext;
  if (iNext != iTail)
  {
    iHead = iNext;
    t = *iHead;
    return true;
  }
  return false;
}

This method reads the elements, but doesn't remove them from the list. Nor does it access the list
directly, but through the iterators. They are guaranteed to be valid after std::list<T> is modified,
so no matter what the Producer thread does to the list, you are safe to use them.

The std::list<T> maintains an element (pointed to by iHead) that is considered already read. For
this algorithm to work even when the queue was just created, I add an empty T() element in the
constructor of the LockFreeQueue<T> (see Figure 1): 

LockFreeQueue()
{
  list.push_back(T());
  iHead = list.begin();
  iTail = list.end();
}

<discussion-when-queue-is-empty>
Consume() may fail to read an element (and return false). Unlike traditional lock-based queues, this
queue works fast when the queue is not empty, but needs an external locking or polling method to
wait for data. Sometimes you want to wait if there is no element available in the queue, and avoid
returning false. A naive approach to waiting is: 

T Consume()
{
  T tmp;
  while (!Consume(tmp))
    ;
  return tmp;
}

This Consume() method will likely heat up one of your CPUs red-hot to 100-percent use if there are
no elements in the queue. Nevertheless, this should have good performance when the queue is not
empty. However, if you think of it, a queue that's almost never empty is a sign of systemic trouble:
It means the consumer is unable to keep pace with the producer, and sooner or later, the system is
doomed to die of memory exhaustion. Call this approach NAIVE_POLLING. 

A friendlier Consume() function does some pooling and calls some sort of sleep() or yield() function
available on your system: 

T Consume(int wait_time = 1/*milliseconds*/)
{
  T tmp;
  while (!Consume(tmp))
  {
    Sleep(wait_time/*milliseconds*/);
  }
  return tmp;
}

The DoSleep() can be implemented using nanosleep() (POSIX) or Sleep() (Windows), or even better,
using boost::thread::sleep(), which abstracts away system-dependent nomenclature. Call this
approach SLEEP. Instead of simple polling, you can use more advanced techniques to signal the
Consumer thread that a new element is available. I illustrate this in Listing One using a
boost::condition variable.

#include <boost/thread.hpp>
#include <boost/thread/condition.hpp>
#include <boost/thread/xtime.hpp>
    
template <typename T>
struct WaitFreeQueue
{
    void Produce(const T& t)
    {
        queue.Produce(t);
        cond.notify_one();
    }
    bool Consume(T& t)
    {
        return queue.Consume(t);
    }
    T Consume(int wait_time = 1/*milliseconds*/)
    {
        T tmp;
        if (Consume(tmp))
            return tmp;

        // the queue is empty, try again (possible waiting...)
        boost::mutex::scoped_lock lock(mtx);
        while (!Consume(tmp))          // line A
        {
            boost::xtime t;
            boost::xtime_get(&t, boost::TIME_UTC);
            AddMilliseconds(t, wait_time);
            cond.timed_wait(lock, t);  // line B
        }
        return tmp;
    }
private:
    LockFreeQueue<T> queue;
    boost::condition cond;
    boost::mutex mtx;
};

I used the timed_wait() instead of the simpler wait() to solve a possible deadlock when Produce() is
called between line A and line B in Listing One. Then wait() will miss the notify_one() call and
have to wait for the next produced element to wake up. <If> this element never comes (no more produced
elements or if the Produce() call actually waits for Consume() to return), there's a deadlock.
Call this approach TIME_WAIT.

The lock is still wait-free as long as there are elements in the queue. In this case, the Consumer()
thread does no waiting and reads data as fast as possible (even with the Producer() that is
inserting new elements). Only when the queue is exhausted does locking occur. 
<KT> claims that it is lock-free as long as queue is not empty.

The Ping-Pong Test

To compare the three approaches (NAIVE_POLLING, SLEEP, and TIME_WAIT), I implemented a test called
"Ping-Pong" that is similar to the game of table tennis (the source code is available online). In
Figure 2, there are two identical queues between the threads T1 and T2. 

      queue 1
T1 ------------> T2
   <------------
      queue 2

You first load one of the queues with a number of "balls," then ask each thread to read from one
queue and write to the other. The result is a controlled infinite loop. By limiting the game to a
fixed number of reads/writes ("shots"), you get an understanding of how the queue behaves when
varying the waiting/sleep time and strategy and the number of "balls." The faster the game, the
better the performance. You should also check CPU usage to see how much of it is used for real work.

o "No ball" means "do nothing" (like two players waiting for the other to start). This gives you an
idea of how good the queues are when there is no dataâhow nervous the players are. Ideally, CPU
usage should be zero.
 
o "One ball" is like the real ping-pong game: Each player shoots and waits for the other to shoot.

o "Two (or more) balls" means both players could shoot at the same time, modulo collision and
waiting issues.


In a wait-free system, the more balls in the game, the better the performance gain compared to the
classic locking strategy. This is because wait-free is an optimistic concurrency control method
(works best when there is no contention), while classical lock-based concurrency control is
pessimistic (assumes contention happens and preemptively inserts locking).

Ready to play? Here is the Ping-Pong test command line:

$> ./pingpong [strategy] [timeout] [balls] [shots]

When you run the program, the tests show the results in the table shown in Figure 3:

o The best combination is the timed_wait() with a small wait time (1ms in the test for TIMED_WAIT).
It has a very fast response time and almost 0 percent CPU usage when the queue is empty.

o Even when the sleep time is 0 (usleep(0)), the worst seems to be the sleep() method, especially
when the queue is likely to be empty. (The number of shots in this case is 100-times smaller than
the other cases because of the long duration of the game.)

o The NO_WAIT strategy is [fast] but behaves worst when there are no balls (100-percent CPU usage to
do nothing). It has the best performance when there is a single ball.

Figure 4 presents a table with the results for a classic approach (see SafeQueue). These results
show that this queue is, on average, more than four-times slower than the LockFreeQueue. The
slowdown comes from the synchronization between threads. Both Produce() and Consume() have to wait
for each other to finish. CPU usage is almost 100 percent for this test (similar to the NO_WAIT
strategy, but not even close to its performance).

Final Considerations

The single-threaded code below shows the value of the list.size() when Producing/ Consuming
elements: 

LockFreeQueue<int> q;   // list.size() == 1
q.Produce(1);           // list.size() == 2
int i;
q.Consume(i);           // list.size() == still 2!;
                        // Consume() doesn't modify the list
q.Produce(i);           // list.size() == 2 again;

The size of the queue is 1 if Produce() was never called and greater than 1 if any element was
produced.

No matter how many times Consume() is called, the list's size will stay constant. It is Produce()
that is increasing the size (by 1); and if there were consumed elements, it will also delete them
from the queue. In a way, Produce() acts as a simple garbage collector. <o> The whole thread safety
comes from the fact that specific data is modified from single threads only. The synchronization
between threads is done using iterators (or pointers, whichever has atomic read/write operation on
your machine). <o> Also consider this code: 

usleep(1000);    // sleep 1 microsecond

On the face of it, this line of code makes a thread sleep for 1 microsecond, and
then continue. In reality, 1 microsecond is just a lower bound to the duration
of the call.

The man page for usleep() says, "The usleep() function suspends execution of the
calling process for (at least) usec microseconds. The sleep may be lengthened
slightly by any system activity or by the time spent processing the call or by
the granularity of system timers," or if you use the nanosleep() function.
"Therefore, nanosleep() always pauses for at least the specified time; however,
    it can take up to 10 ms longer than specified until the process becomes
    runnable again."

So if the process is not scheduled under a real-time policy, there's no
guarantee when your thread will be running again. I've done some tests and (to
        my surprise) there are situations when code such as: 

cond.timed_wait(lock, x);    // x = e.g. 1 millisecond

will actually wait for more than 1 second.


{three}
http://www.drdobbs.com/cpp/the-trouble-with-locks/184401930
The Trouble with Locks

By Herb Sutter, March 01, 2005

References

[1] Sutter, H. "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software," Dr.
Dobb's Journal, March 2005. Available online at http://www.gotw.ca/publications/concurrency-ddj.htm.

[2] Sutter, H. "The Concurrency Revolution," C/C++ Users Journal, February 2005. This is an
abbreviated version of [1].

[3] Alexandrescu, A. "Lock-Free Data Structures," C/C++ Users Journal, October 2004.

[4] Alexandrescu, A. and M. Michael. "Lock-Free Data Structures with Hazard Pointers," C/C++ Users
Journal, December 2004.

[5] http://blogs.msdn.com/cbrumme. 

Lock-based programming may not be the best approach to building large concurrent programs.

In my most recent articles [1, 2], I presented reasons why concurrency (for example, multithreading)
will be the next revolution in the way we develop software â a sea change of the same order as the
object-oriented revolution. I also stated that "the vast majority of programmers today don't grok
concurrency, just as the vast majority of programmers 15 years ago didn't yet grok objects."

In this column, I'd like to consider just one question that several people wrote to ask, namely: "Is
concurrency really that hard?" In particular, a few readers felt that lock-based programming is well
understood; it is, after all, the status quo mainstream solution to concurrency control.

So, "is concurrency really that hard?" My short answer is this:

o Lock-based programming, our status quo, is difficult for experts to get right. Worse, it is also
fundamentally flawed for building large programs. This article focuses exclusively on lock-based
programming just because there's so much to say in even a 50,000-foot overview that I ran out of
room.

o Lock-free programming is difficult for gurus to get right. I'll save this for another time, but if
you're interested, you should check out Andrei Alexandrescu's recent articles for a sampling of the
issues in lock-free programming and hazard pointers [3, 4]. (Aside: Yes, I'm implying Andrei is a
guru. I hope he doesn't mind my outing him in public like this. I don't think it was much of a
secret.) (More aside: The hazard pointer work shows in particular why, if you're writing
lock-free data structures, you really really want garbage collection. You can do it yourself
without garbage collection, but it's like working with knives that are sharp on both edges and
don't have handles. But that's another article. Specifically, it's Andrei's other article.)

Unfortunately, today's reality is that only thoughtful experts can write explicitly concurrent
programs that are correct and efficient. This is because today's programming models for concurrency
are subtle, intricate, and fraught with pitfalls that easily (and frequently) result in unforeseen
races (i.e., program corruption) deadlocks (i.e., program lockup) and performance cliffs (e.g.,
priority inversion, convoying, and sometimes complete loss of parallelism and/or even worse
performance than a single-threaded program). And even when a correct and efficient concurrent
program is written, it takes great care to maintain â it's usually brittle and difficult to maintain
correctly because current programming models set a very high bar of expertise required to reason
reliably about the operation of concurrent programs, so that apparently innocuous changes to a
working concurrent program can (and commonly do, in practice) render it entirely or intermittently
nonworking in unintended and unexpected ways. Because getting it right and keeping it right is so
difficult, at many major software companies there is a veritable priesthood of gurus who write and
maintain the core concurrent code.

Some people think I'm overstating this, so let me amplify. In this article, I'll focus on just the
narrow question of how to write a lock-based program correctly, meaning that it works (avoids data
corruption) and doesn't hang (avoids deadlock and livelock). That's pretty much the minimum
requirement to write a program that runs at all.

Question: Is the following code thread-safe? If it is, why is it safe? If it isn't always, under
what conditions is it thread-safe?

T Add( T& a, T& b ) {
  T result;
  // ... read a and b and set result to
  // proper values ...
  return result;
}

There are a lot of possibilities here. Let's consider some of the major ones.

Lock-Based Solutions?

Assume that reading a T object isn't an atomic operation. Then, if a and/or b are accessible from
another thread, we have a classic race condition: While we are reading the values of a and/or b,
some other thread might be changing those objects, resulting in blowing up the program; if you're
lucky, say, by causing the object to follow an internal pointer some other thread just deleted; or
reading corrupt values.

How would you solve that? Please stop and think about it before reading on...

Ready? Okay: Now please stop a little longer and think about your solution some more, and consider
whether it might have any further holes, before reading on...

Now that you've thought about it deeply, let's consider some alternatives.

Today's typical lock-based approach is to acquire locks so that uses of a and b on one thread won't
interleave. Typically, this is done by acquiring a lock on an explicit synchronization object [a
mutex, for instance] that covers both objects, or by acquiring locks on an implicit mutex associated
with the objects themselves. To acquire a lock that covers both objects, Add has to know what that
lock is, either because a and b and their lock are globals [but then why pass a and b as
parameters?] or because the caller acquires the lock outside of Add [which is usually preferable].
To acquire a lock on each object individually, we could write:

T SharedAdd( T& a, T& b ) {
  T result;
  lock locka( a );  // lock is a helper
  // whose constructor acquires a lock
  lock lockb( b );  // and whose
  // destructor releases the lock
  // ... read a and b and set result to
  //  proper values ...
  return result;
} // release the locks 


{four}
http://www.drdobbs.com/parallel/writing-lock-free-code-a-corrected-queue/210604448?pgno=1
Writing Lock-Free Code: A Corrected Queue

By Herb Sutter, September 29, 2008

Notes

[1] H. Sutter. âLock-Free Code: A False Sense of Securityâ (DDJ, September 2008).
(www.ddj.com/cpp/210600279).

[2] P. Marginean. "Lock-Free Queues" (DDJ, July 2008). (www.ddj.com/208801974).

[3] This is just like a canonical exception safety patternâdo all the work off to the side, then
commit to accept the new state using nonthrowing operations only. "Think in transactions" applies
everywhere, and should be ubiquitous in the way we write our code.

[4] Compare-and-swap (CAS) is the most widely available fundamental lock-free operation and so I'll
focus on it here. However, some systems instead provide the equivalently powerful
load-linked/store-conditional (LL/SC) instead.


As we saw last month [1], lock-free coding is hard even for experts. There, I dissected a published
lock-free queue implementation [2] and examined why the code was quite broken. This month, let's see
how to do it right.

Lock-Free Fundamentals

When writing lock-free code, always keep these essentials well in mind:

Key concepts. Think in transactions. Know who owns what data. Key tool. The ordered atomic variable. 

When writing a lock-free data structure, "to think in transactions" means to make sure that each
operation on the data structure is atomic, all-or-nothing with respect to other concurrent
operations on that same data. The typical coding pattern to use is to do work off to the side, then
"publish" each change to the shared data with a single atomic write or compare-and-swap. [3] Be sure
that concurrent writers don't interfere with each other or with concurrent readers, and pay special
attention to any operations that delete or remove data that a concurrent operation might still be
using.

Be highly aware of who owns what data at any given time; mistakes mean races where two threads think
they can proceed with conflicting work. You know who owns a given piece of shared data right now by
looking at the value of the ordered atomic variable that says who it is. To hand off ownership of
some data to another thread, do it at the end of a transaction with a single atomic operation that
means "now it's your's."

An ordered atomic variable is a "lock-free-safe" variable with the following properties that make it
safe to read and write across threads without any explicit locking:

Atomicity. Each individual read and write is guaranteed to be atomic with respect to all other reads
and writes of that variable. The variables typically fit into the machine's native word size, and so
are usually pointers (C++), object references (Java, .NET), or integers. Order. Each read and write
is guaranteed to be executed in source code order. Compilers, CPUs, and caches will respect it and
not try to optimize these operations the way they routinely distort reads and writes of ordinary
variables. Compare-and-swap (CAS) [4]. There is a special operation you can call using a syntax like
variable.compare_exchange( expectedValue, newValue ) that does the following as an atomic operation:
If variable currently has the value expectedValue, it sets the value to newValue and returns true;
else returns false. A common use is if(variable.compare_exchange(x,y)), which you should get in the
habit of reading as, "if I'm the one who gets to change variable from x to y." 

Ordered atomic variables are spelled in different ways on popular platforms and environments. For
example:

volatile in C#/.NET, as in volatile int. volatile or * Atomic* in Java, as in volatile int,
AtomicInteger. atomic<T> in C++0x, the forthcoming ISO C++ Standard, as in atomic<int>. 

In the code that follows, I'm going to highlight the key reads and writes of such a variable; these
variables should leap out of the screen at you, and you should get used to being very aware of every
time you touch one.

If you don't yet have ordered atomic variables yet on your language and platform, you can emulate
them by using ordinary but aligned variables whose reads and writes are guaranteed to be naturally
atomic, and enforce ordering by using either platform-specific ordered API calls (such as Win32's
InterlockedCompareExchange for compare-and-swap) or platform-specific explicit memory
fences/barriers (for example, Linux mb).


={============================================================================
*kt_linux_core_264*	conc: ref: the free lunch is over 
http://www.gotw.ca/publications/concurrency-ddj.htm

The Free Lunch Is Over
A Fundamental Turn Toward Concurrency in Software

By Herb Sutter

The biggest sea change in software development since the OO revolution is knocking at the door, and
its name is Concurrency.

This article appeared in Dr. Dobb's Journal, 30(3), March 2005. A much briefer version under the
title "The Concurrency Revolution" appeared in C/C++ Users Journal, 23(2), February 2005.

Update note: The CPU trends graph last updated August 2009 to include current data and show the
trend continues as predicted. The rest of this article including all text is still original as first
posted here in December 2004.

Your free lunch will soon be over. What can you do about it? What are you doing about it?

The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have
run out of room with most of their traditional approaches to boosting CPU performance. Instead of
driving clock speeds and straight-line instruction throughput ever higher, they are instead turning
en masse to hyperthreading and multicore architectures. Both of these features are already available
on chips today; in particular, multicore is available on current PowerPC and Sparc IV processors,
   and is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004 In-Stat/MDR Fall
   Processor Forum was multicore devices, as many companies showed new or updated multicore
   processors. Looking back, itâs not much of a stretch to call 2004 the year of multicore.

And that puts us at a fundamental turning point in software development, at least for the next few
years and for applications targeting general-purpose desktop computers and low-end servers (which
    happens to account for the vast bulk of the dollar value of software sold today). In this
article, Iâll describe the changing face of hardware, why it suddenly does matter to software, and
how specifically the concurrency revolution matters to you and is going to change the way you will
likely be writing software in the future.

Arguably, the free lunch has already been over for a year or two, only weâre just now noticing.  

<The Free Performance Lunch>

Thereâs an interesting phenomenon thatâs known as âAndy giveth, and Bill taketh away.â No matter how
fast processors get, software consistently finds new ways to eat up the extra speed. Make a CPU ten
times as fast, and software will usually find ten times as much to do (or, in some cases, will feel
    at liberty to do it ten times less efficiently). Most classes of applications have enjoyed free
and regular performance gains for several decades, even without releasing new versions or doing
anything special, because the CPU manufacturers (primarily) and memory and disk manufacturers
(secondarily) have reliably enabled ever-newer and ever-faster mainstream systems. Clock speed isnât
the only measure of performance, or even necessarily a good one, but itâs an instructive one: Weâre
used to seeing 500MHz CPUs give way to 1GHz CPUs give way to 2GHz CPUs, and so on. Today weâre in
the 3GHz range on mainstream computers.

The key question is: When will it end? After all, Mooreâs Law predicts exponential growth, and
clearly exponential growth canât continue forever before we reach hard physical limits; light isnât
getting any faster. The growth must eventually slow down and even end. (Caveat: Yes, Mooreâs Law
    applies principally to transistor densities, but the same kind of exponential growth has
    occurred in related areas such as clock speeds. Thereâs even faster growth in other spaces, most
    notably the data storage explosion, but that important trend belongs in a different article.)

If youâre a software developer, chances are that you have already been riding the âfree lunchâ wave
of desktop computer performance. Is your applicationâs performance borderline for some local
operations? âNot to worry,â the conventional (if suspect) wisdom goes; âtomorrowâs processors will
have even more throughput, and anyway todayâs applications are increasingly throttled by factors
other than CPU throughput and memory speed (e.g., theyâre often I/O-bound, network-bound,
    database-bound).â Right?

Right enough, in the past. But dead wrong for the foreseeable future.

The good news is that processors are going to continue to become more powerful. The bad news is
that, at least in the short term, the growth will come mostly in directions that do not take most
current applications along for their customary free ride.

Over the past 30 years, CPU designers have achieved performance gains in three main areas, the first
two of which focus on straight-line execution flow:

clock speed
execution optimization
cache

Increasing clock speed is about getting more cycles. Running the CPU faster more or less directly
means doing the same work faster.

Optimizing execution flow is about doing more work per cycle. Todayâs CPUs sport some more powerful
instructions, and they perform optimizations that range from the pedestrian to the exotic, including
pipelining, branch prediction, executing multiple instructions in the same clock cycle(s), and even
reordering the instruction stream for out-of-order execution. These techniques are all designed to
make the instructions flow better and/or execute faster, and to squeeze the most work out of each
clock cycle by reducing latency and maximizing the work accomplished per clock cycle.

Chip designers are under so much pressure to deliver ever-faster CPUs that theyâll risk changing the
meaning of your program, and possibly break it, in order to make it run faster

Brief aside on instruction reordering and memory models: Note that some of what I just called
âoptimizationsâ are actually far more than optimizations, in that they can change the meaning of
programs and cause visible effects that can break reasonable programmer expectations. This is
significant. CPU designers are generally sane and well-adjusted folks who normally wouldnât hurt a
fly, and wouldnât think of hurting your codeâ¦ normally. But in recent years they have been willing
to pursue aggressive optimizations just to wring yet more speed out of each cycle, even knowing full
well that these aggressive rearrangements could endanger the semantics of your code. Is this Mr.
Hyde making an appearance? Not at all. That willingness is simply a clear indicator of the extreme
pressure the chip designers face to deliver ever-faster CPUs; theyâre under so much pressure that
theyâll risk changing the meaning of your program, and possibly break it, in order to make it run
faster. Two noteworthy examples in this respect are write reordering and read reordering: Allowing a
processor to reorder write operations has consequences that are so surprising, and break so many
programmer expectations, that the feature generally has to be turned off because itâs too difficult
for programmers to reason correctly about the meaning of their programs in the presence of arbitrary
write reordering. Reordering read operations can also yield surprising visible effects, but that is
more commonly left enabled anyway because it isnât quite as hard on programmers, and the demands for
performance cause designers of operating systems and operating environments to compromise and choose
models that place a greater burden on programmers because that is viewed as a lesser evil than
giving up the optimization opportunities.

Finally, increasing the size of on-chip cache is about staying away from RAM. Main memory continues
to be so much slower than the CPU that it makes sense to put the data closer to the processorâand
you canât get much closer than being right on the die. On-die cache sizes have soared, and today
most major chip vendors will sell you CPUs that have 2MB and more of on-board L2 cache. (Of these
    three major historical approaches to boosting CPU performance, increasing cache is the only one
    that will continue in the near term. Iâll talk a little more about the importance of cache later
    on.)

Okay. So what does this mean?

A fundamentally important thing to recognize about this list is that all of these areas are
concurrency-agnostic. Speedups in any of these areas will directly lead to speedups in sequential
(nonparallel, single-threaded, single-process) applications, as well as applications that do make
use of concurrency. Thatâs important, because the vast majority of todayâs applications are
single-threaded, for good reasons that Iâll get into further below.

Of course, compilers have had to keep up; sometimes you need to recompile your application, and
target a specific minimum level of CPU, in order to benefit from new instructions (e.g., MMX, SSE)
  and some new CPU features and characteristics. But, by and large, even old applications have
  always run significantly fasterâeven without being recompiled to take advantage of all the new
  instructions and features offered by the latest CPUs.

That world was a nice place to be. Unfortunately, it has already disappeared.

<Obstacles, and Why You Donât Have 10GHz Today>

CPU performance growth as we have known it hit a wall two years ago. Most people have only recently
started to notice.

You can get similar graphs for other chips, but Iâm going to use Intel data here. Figure 1 graphs
the history of Intel chip introductions by clock speed and number of transistors. The number of
transistors continues to climb, at least for now. Clock speed, however, is a different story.

Around the beginning of 2003, youâll note a disturbing sharp turn in the previous trend toward
ever-faster CPU clock speeds. Iâve added lines to show the limit trends in maximum clock speed;
instead of continuing on the previous path, as indicated by the thin dotted line, there is a sharp
flattening. It has become harder and harder to exploit higher clock speeds due to not just one but
several physical issues, notably heat (too much of it and too hard to dissipate), power consumption
(too high), and current leakage problems.

Quick: Whatâs the clock speed on the CPU(s) in your current workstation? Are you running at 10GHz?
On Intel chips, we reached 2GHz a long time ago (August 2001), and according to CPU trends before
2003, now in early 2005 we should have the first 10GHz Pentium-family chips. A quick look around
shows that, well, actually, we donât. Whatâs more, such chips are not even on the horizonâwe have no
good idea at all about when we might see them appear.

Well, then, what about 4GHz? Weâre at 3.4GHz alreadyâsurely 4GHz canât be far away? Alas, even 4GHz
seems to be remote indeed. In mid-2004, as you probably know, Intel first delayed its planned
introduction of a 4GHz chip until 2005, and then in fall 2004 it officially abandoned its 4GHz plans
entirely. As of this writing, Intel is planning to ramp up a little further to 3.73GHz in early 2005
(already included in Figure 1 as the upper-right-most dot), but the clock race really is over, at
least for now; Intelâs and most processor vendorsâ future lies elsewhere as chip companies
aggressively pursue the same new multicore directions.

Weâll probably see 4GHz CPUs in our mainstream desktop machines someday, but it wonât be in 2005.
Sure, Intel has samples of their chips running at even higher speeds in the labâbut only by heroic
efforts, such as attaching hideously impractical quantities of cooling equipment. You wonât have
that kind of cooling hardware in your office any day soon, let alone on your lap while computing on
the plane.

<Myths and Realities: 2 x 3GHz < 6 GHz>

So a dual-core CPU that combines two 3GHz cores practically offers 6GHz of processing power. Right?

Wrong. Even having two threads running on two physical processors doesnât mean getting two times the
performance. Similarly, most multi-threaded applications wonât run twice as fast on a dual-core box.
They should run faster than on a single-core CPU; the performance gain just isnât linear, thatâs
all.

Why not? First, there is coordination overhead between the cores to ensure cache coherency (a
    consistent view of cache, and of main memory) and to perform other handshaking. Today, a two- or
four-processor machine isnât really two or four times as fast as a single CPU even for
multi-threaded applications. The problem remains essentially the same even when the CPUs in question
sit on the same die.

Second, unless the two cores are running different processes, or different threads of a single
process that are well-written to run independently and almost never wait for each other, they wonât
be well utilized. (Despite this, I will speculate that todayâs single-threaded applications as
    actually used in the field could actually see a performance boost for most users by going to a
    dual-core chip, not because the extra core is actually doing anything useful, but because it is
    running the adware and spyware that infest many usersâ systems and are otherwise slowing down
    the single CPU that user has today. I leave it up to you to decide whether adding a CPU to run
    your spyware is the best solution to that problem.)

If youâre running a single-threaded application, then the application can only make use of one core.
There should be some speedup as the operating system and the application can run on separate cores,
      but typically the OS isnât going to be maxing out the CPU anyway so one of the cores will be
      mostly idle. (Again, the spyware can share the OSâs core most of the time.) 
  

<TANSTAAFL: Mooreâs Law and the Next Generation(s)>

âThere ainât no such thing as a free lunch.â âR. A. Heinlein, The Moon Is a Harsh Mistress

Does this mean Mooreâs Law is over? Interestingly, the answer in general seems to be no. Of course,
     like all exponential progressions, Mooreâs Law must end someday, but it does not seem to be in
     danger for a few more years yet. Despite the wall that chip engineers have hit in juicing up
     raw clock cycles, transistor counts continue to explode and it seems CPUs will continue to
     follow Mooreâs Law-like throughput gains for some years to come.  The key difference, which is
     the heart of this article, is that the performance gains are going to be accomplished in
     fundamentally different ways for at least the next couple of processor generations. And most
     current applications will no longer benefit from the free ride without significant redesign.

For the near-term future, meaning for the next few years, the performance gains in new chips will be
fueled by three main approaches, only one of which is the same as in the past. The near-term future
performance growth drivers are:

hyperthreading
multicore
cache

Hyperthreading is about running two or more threads in parallel inside a single CPU. Hyperthreaded
CPUs are already available today, and they do allow some instructions to run in parallel. A limiting
factor, however, is that although a hyper-threaded CPU has some extra hardware including extra
registers, it still has just one cache, one integer math unit, one FPU, and in general just one each
of most basic CPU features. Hyperthreading is sometimes cited as offering a 5% to 15% performance
boost for reasonably well-written multi-threaded applications, or even as much as 40% under ideal
conditions for carefully written multi-threaded applications. Thatâs good, but itâs hardly double,
           and it doesnât help single-threaded applications.

Multicore is about running two or more actual CPUs on one chip. Some chips, including Sparc and
PowerPC, have multicore versions available already. The initial Intel and AMD designs, both due in
2005, vary in their level of integration but are functionally similar. AMDâs seems to have some
initial performance design advantages, such as better integration of support functions on the same
die, whereas Intelâs initial entry basically just glues together two Xeons on a single die. The
performance gains should initially be about the same as having a true dual-CPU system (only the
    system will be cheaper because the motherboard doesnât have to have two sockets and associated
    âglueâ chippery), which means something less than double the speed even in the ideal case, and
just like today it will boost reasonably well-written multi-threaded applications. Not
single-threaded ones.

Finally, on-die cache sizes can be expected to continue to grow, at least in the near term. Of these
three areas, only this one will broadly benefit most existing applications. The continuing growth in
on-die cache sizes is an incredibly important and highly applicable benefit for many applications,
  simply because space is speed. Accessing main memory is expensive, and you really donât want to
  touch RAM if you can help it. On todayâs systems, a cache miss that goes out to main memory often
  costs 10 to 50 times as much getting the information from the cache; this, incidentally, continues
  to surprise people because we all think of memory as fast, and it is fast compared to disks and
  networks, but not compared to on-board cache which runs at faster speeds. If an applicationâs
  working set fits into cache, weâre golden, and if it doesnât, weâre not. That is why increased
  cache sizes will save some existing applications and breathe life into them for a few more years
  without requiring significant redesign: As existing applications manipulate more and more data,
  and as they are incrementally updated to include more code for new features, performance-sensitive
  operations need to continue to fit into cache. As the Depression-era old-timers will be quick to
  remind you, âCache is king.â

(Aside: Hereâs an anecdote to demonstrate âspace is speedâ that recently hit my compiler team. The
 compiler uses the same source base for the 32-bit and 64-bit compilers; the code is just compiled
 as either a 32-bit process or a 64-bit one. The 64-bit compiler gained a great deal of baseline
 performance by running on a 64-bit CPU, principally because the 64-bit CPU had many more registers
 to work with and had other code performance features. All well and good. But what about data? Going
 to 64 bits didnât change the size of most of the data in memory, except that of course pointers in
 particular were now twice the size they were before. As it happens, our compiler uses pointers much
 more heavily in its internal data structures than most other kinds of applications ever would.
 Because pointers were now 8 bytes instead of 4 bytes, a pure data size increase, we saw a
 significant increase in the 64-bit compilerâs working set. That bigger working set caused a
 performance penalty that almost exactly offset the code execution performance increase weâd gained
 from going to the faster processor with more registers. As of this writing, the 64-bit compiler
 runs at the same speed as the 32-bit compiler, even though the source base is the same for both and
 the 64-bit processor offers better raw processing throughput. Space is speed.)

But cache is it. Hyperthreading and multicore CPUs will have nearly no impact on most current
applications.

So what does this change in the hardware mean for the way we write software? By now youâve probably
noticed the basic answer, so letâs consider it and its consequences.

<What This Means For Software: The Next Revolution>

In the 1990s, we learned to grok objects. The revolution in mainstream software development from
structured programming to object-oriented programming was the greatest such change in the past 20
years, and arguably in the past 30 years. There have been other changes, including the most recent
(and genuinely interesting) naissance of web services, but nothing that most of us have seen during
our careers has been as fundamental and as far-reaching a change in the way we write software as the
object revolution.

Until now.

Starting today, the performance lunch isnât free any more. Sure, there will continue to be generally
applicable performance gains that everyone can pick up, thanks mainly to cache size improvements.
But if you want your application to benefit from the continued exponential throughput advances in
new processors, it will need to be a well-written concurrent (usually multithreaded) application.
And thatâs easier said than done, because not all problems are inherently parallelizable and because
concurrent programming is hard.

I can hear the howls of protest: âConcurrency? Thatâs not news! People are already writing
concurrent applications.â Thatâs true. Of a small fraction of developers.

Remember that people have been doing object-oriented programming since at least the days of Simula
in the late 1960s. But OO didnât become a revolution, and dominant in the mainstream, until the
1990s. Why then? The reason the revolution happened was primarily that our industry was driven by
requirements to write larger and larger systems that solved larger and larger problems and exploited
the greater and greater CPU and storage resources that were becoming available. OOPâs strengths in
abstraction and dependency management made it a necessity for achieving large-scale software
development that is economical, reliable, and repeatable.

Concurrency is the next major revolution in how we write software

Similarly, weâve been doing concurrent programming since those same dark ages, writing coroutines
and monitors and similar jazzy stuff. And for the past decade or so weâve witnessed incrementally
more and more programmers writing concurrent (multi-threaded, multi-process) systems. But an actual
revolution marked by a major turning point toward concurrency has been slow to materialize. Today
the vast majority of applications are single-threaded, and for good reasons that Iâll summarize in
the next section.

By the way, on the matter of hype: People have always been quick to announce âthe next software
development revolution,â usually about their own brand-new technology. Donât believe it. New
technologies are often genuinely interesting and sometimes beneficial, but the biggest revolutions
in the way we write software generally come from technologies that have already been around for some
years and have already experienced gradual growth before they transition to explosive growth. This
is necessary: You can only base a software development revolution on a technology thatâs mature
enough to build on (including having solid vendor and tool support), and it generally takes any new
software technology at least seven years before itâs solid enough to be broadly usable without
performance cliffs and other gotchas. As a result, true software development revolutions like OO
happen around technologies that have already been undergoing refinement for years, often decades.
Even in Hollywood, most genuine âovernight successesâ have really been performing for many years
before their big break.

Concurrency is the next major revolution in how we write software. Different experts still have
different opinions on whether it will be bigger than OO, but that kind of conversation is best left
to pundits. For technologists, the interesting thing is that concurrency is of the same order as OO
both in the (expected) scale of the revolution and in the complexity and learning curve of the
technology.

<Benefits and Costs of Concurrency>

There are two major reasons for which concurrency, especially multithreading, is already used in
mainstream software. The first is to logically separate naturally independent control flows; for
example, in a database replication server I designed it was natural to put each replication session
on its own thread, because each session worked completely independently of any others that might be
active (as long as they werenât working on the same database row). The second and less common reason
to write concurrent code in the past has been for performance, either to scalably take advantage of
multiple physical CPUs or to easily take advantage of latency in other parts of the application; in
my database replication server, this factor applied as well and the separate threads were able to
scale well on multiple CPUs as our server handled more and more concurrent replication sessions with
many other servers.

There are, however, real costs to concurrency. Some of the obvious costs are actually relatively
unimportant. For example, yes, locks can be expensive to acquire, but when used judiciously and
properly you gain much more from the concurrent execution than you lose on the synchronization, if
you can find a sensible way to parallelize the operation and minimize or eliminate shared state.

Perhaps the second-greatest cost of concurrency is that not all applications are amenable to
parallelization. Iâll say more about this later on.

Probably the greatest cost of concurrency is that concurrency really is hard: The programming model,
         meaning the model in the programmerâs head that he needs to reason reliably about his
         program, is much harder than it is for sequential control flow.

Everybody who learns concurrency thinks they understand it, ends up finding mysterious races they
thought werenât possible, and discovers that they didnât actually understand it yet after all. As
the developer learns to reason about concurrency, they find that usually those races can be caught
by reasonable in-house testing, and they reach a new plateau of knowledge and comfort. What usually
doesnât get caught in testing, however, except in shops that understand why and how to do real
stress testing, is those latent concurrency bugs that surface only on true multiprocessor systems,
       where the threads arenât just being switched around on a single processor but where they
       really do execute truly simultaneously and thus expose new classes of errors. This is the
       next jolt for people who thought that surely now they know how to write concurrent code: Iâve
       come across many teams whose application worked fine even under heavy and extended stress
       testing, and ran perfectly at many customer sites, until the day that a customer actually had
       a real multiprocessor machine and then deeply mysterious races and corruptions started to
       manifest intermittently. In the context of todayâs CPU landscape, then, redesigning your
       application to run multithreaded on a multicore machine is a little like learning to swim by
       jumping into the deep endâgoing straight to the least forgiving, truly parallel environment
       that is most likely to expose the things you got wrong. Even when you have a team that can
       reliably write safe concurrent code, there are other pitfalls; for example, concurrent code
       that is completely safe but isnât any faster than it was on a single-core machine, typically
       because the threads arenât independent enough and share a dependency on a single resource
       which re-serializes the programâs execution. This stuff gets pretty subtle.

The vast majority of programmers today donât grok concurrency, just as the vast majority of
programmers 15 years ago didnât yet grok objects

Just as it is a leap for a structured programmer to learn OO (whatâs an object? whatâs a virtual
    function? how should I use inheritance? and beyond the âwhatsâ and âhows,â why are the correct
    design practices actually correct?), itâs a leap of about the same magnitude for a sequential
programmer to learn concurrency (whatâs a race? whatâs a deadlock? how can it come up, and how do I
    avoid it? what constructs actually serialize the program that I thought was parallel? how is the
    message queue my friend? and beyond the âwhatsâ and âhows,â why are the correct design practices
    actually correct?).

The vast majority of programmers today donât grok concurrency, just as the vast majority of
programmers 15 years ago didnât yet grok objects. But the concurrent programming model is learnable,
            particularly if we stick to message- and lock-based programming, and once grokked it
            isnât that much harder than OO and hopefully can become just as natural. Just be ready
            and allow for the investment in training and time, for you and for your team.

(I deliberately limit the above to message- and lock-based concurrent programming models. There is
 also lock-free programming, supported most directly at the language level in Java 5 and in at least
 one popular C++ compiler. But concurrent lock-free programming is known to be very much harder for
 programmers to understand and reason about than even concurrent lock-based programming. Most of the
 time, only systems and library writers should have to understand lock-free programming, although
 virtually everybody should be able to take advantage of the lock-free systems and libraries those
 people produce. Frankly, even lock-based programming is hazardous.)

<What It Means For Us>

Okay, back to what it means for us.

1. The clear primary consequence weâve already covered is that applications will increasingly need
to be concurrent if they want to fully exploit CPU throughput gains that have now started becoming
available and will continue to materialize over the next several years. For example, Intel is
talking about someday producing 100-core chips; a single-threaded application can exploit at most
1/100 of such a chipâs potential throughput. âOh, performance doesnât matter so much, computers just
keep getting fasterâ has always been a naÃ¯ve statement to be viewed with suspicion, and for the near
future it will almost always be simply wrong.

Applications will increasingly need to be concurrent if they want to fully exploit continuing
exponential CPU throughput gains

Efficiency and performance optimization will get more, not less, important

Now, not all applications (or, more precisely, important operations of an application) are amenable
to parallelization. True, some problems, such as compilation, are almost ideally parallelizable. But
others arenât; the usual counterexample here is that just because it takes one woman nine months to
produce a baby doesnât imply that nine women could produce one baby in one month. Youâve probably
come across that analogy before. But did you notice the problem with leaving the analogy at that?
Hereâs the trick question to ask the next person who uses it on you: Can you conclude from this that
the Human Baby Problem is inherently not amenable to parallelization? Usually people relating this
analogy err in quickly concluding that it demonstrates an inherently nonparallel problem, but thatâs
actually not necessarily correct at all. It is indeed an inherently nonparallel problem if the goal
is to produce one child. It is actually an ideally parallelizable problem if the goal is to produce
many children! Knowing the real goals can make all the difference. This basic goal-oriented
principle is something to keep in mind when considering whether and how to parallelize your
software.

2. Perhaps a less obvious consequence is that applications are likely to become increasingly
CPU-bound. Of course, not every application operation will be CPU-bound, and even those that will be
affected wonât become CPU-bound overnight if they arenât already, but we seem to have reached the
end of the âapplications are increasingly I/O-bound or network-bound or database-boundâ trend,
    because performance in those areas is still improving rapidly (gigabit WiFi, anyone?) while
    traditional CPU performance-enhancing techniques have maxed out. Consider: Weâre stopping in the
    3GHz range for now. Therefore single-threaded programs are likely not to get much faster any
    more for now except for benefits from further cache size growth (which is the main good news).
    Other gains are likely to be incremental and much smaller than weâve been used to seeing in the
    past, for example as chip designers find new ways to keep pipelines full and avoid stalls, which
    are areas where the low-hanging fruit has already been harvested. The demand for new application
    features is unlikely to abate, and even more so the demand to handle vastly growing quantities
    of application data is unlikely to stop accelerating. As we continue to demand that programs do
    more, they will increasingly often find that they run out of CPU to do it unless they can code
    for concurrency.

There are two ways to deal with this sea change toward concurrency. One is to redesign your
applications for concurrency, as above. The other is to be frugal, by writing code that is more
efficient and less wasteful. This leads to the third interesting consequence:

3. Efficiency and performance optimization will get more, not less, important. Those languages that
already lend themselves to heavy optimization will find new life; those that donât will need to find
ways to compete and become more efficient and optimizable. Expect long-term increased demand for
performance-oriented languages and systems.

4. Finally, programming languages and systems will increasingly be forced to deal well with
concurrency. The Java language has included support for concurrency since its beginning, although
mistakes were made that later had to be corrected over several releases in order to do concurrent
programming more correctly and efficiently. The C++ language has long been used to write heavy-duty
multithreaded systems well, but it has no standardized support for concurrency at all (the ISO C++
    standard doesnât even mention threads, and does so intentionally), and so typically the
concurrency is of necessity accomplished by using nonportable platform-specific concurrency features
and libraries. (Itâs also often incomplete; for example, static variables must be initialized only
    once, which typically requires that the compiler wrap them with a lock, but many C++
    implementations do not generate the lock.) Finally, there are a few concurrency standards,
    including pthreads and OpenMP, and some of these support implicit as well as explicit
    parallelization. Having the compiler look at your single-threaded program and automatically
    figure out how to parallelize it implicitly is fine and dandy, but those automatic
    transformation tools are limited and donât yield nearly the gains of explicit concurrency
    control that you code yourself. The mainstream state of the art revolves around lock-based
    programming, which is subtle and hazardous. We desperately need a higher-level programming model
    for concurrency than languages offer today; I'll have more to say about that soon.

<Conclusion>

If you havenât done so already, now is the time to take a hard look at the design of your
application, determine what operations are CPU-sensitive now or are likely to become so soon, and
identify how those places could benefit from concurrency. Now is also the time for you and your team
to grok concurrent programmingâs requirements, pitfalls, styles, and idioms.

A few rare classes of applications are naturally parallelizable, but most arenât. Even when you know
exactly where youâre CPU-bound, you may well find it difficult to figure out how to parallelize
those operations; all the most reason to start thinking about it now. Implicitly parallelizing
compilers can help a little, but donât expect much; they canât do nearly as good a job of
parallelizing your sequential program as you could do by turning it into an explicitly parallel and
threaded version.

Thanks to continued cache growth and probably a few more incremental straight-line control flow
optimizations, the free lunch will continue a little while longer; but starting today the buffet
will only be serving that one entrÃ©e and that one dessert. The filet mignon of throughput gains is
still on the menu, but now it costs extraâextra development effort, extra code complexity, and extra
testing effort. The good news is that for many classes of applications the extra effort will be
worthwhile, because concurrency will let them fully exploit the continuing exponential gains in
processor throughput.


={============================================================================
*kt_linux_core_265* sync: case: subtle race

This is to make sure that callback is called exactly once when there is an event
which calls code below. Thing is that if not remove it, it will keep adding
callbacks. So if there is callabck which is already in a queue, then remove the
new. The assumption was that the same events happens more but need to call
callback once asynchronously.

{
  // this add a callback to the glib main thread which is different from the
  // current thread.
  guint task_handle = g_idle_add( video_changed_message_callback, sink);

  // this set sink->task_handle atomic variable with new tast_handle when it
  // is 0 and return TRUE when it succeed.
  if (0 != g_atomic_int_compare_and_exchange(&sink->task_handle, 0,
        task_handle))
  {
    // There is already a task waiting to run, so we can cancel this one.
    g_source_remove(temporary_change_task_handle);
  }
}

The first problem is the wrong interpretation of the return value of
g_atomic_int_compare_and_exchange(). It doesn't return the old value of the
atomic variable. It returns TRUE if exchange took place and FALSE otherwise.
Because of that, we were proceeding to g_source_remove() when the atomic
variable went from zero to a non-zero value and it returns TRUE, which is wrong. 

This cause a race:

1. If the main thread managed to execute the newly added idle callback before
the background thread reached g_source_remove(), we were getting the following
error from g_source_remove():

GLib-CRITICAL **: Source ID 3 was not found when attempting to remove it

2. Otherwise, the idle callback wasn't executed at all!

3. When tried out, saw that both g_source_remove and callback works okay.

All in all, this is racy.

Even after changing the code above to:

{
  guint task_handle = g_idle_add( video_changed_message, sink);
  if (!g_atomic_int_compare_and_exchange(&sink->task_handle, 0,
        task_handle))
  {
    // There is already a task waiting to run, so we can cancel this one.
    g_source_remove(task_handle);
  }
}


There are still a subtle race condition. Consider the following sequence of
events:

1. One instance of this particular idle callback is queued. The atomic variable
   becomes non-zero.

2. Another instance is queued. Because the atomic variable is non-zero, we go
   inside the if.

3. Another thread (the one running GLib main loop) executes both of the queued
   callbacks.

4. The thread that entered the "if" finally reached g_source_remove(), which now
   acts on a non-existing source.

It turns out we don't have to execute the callback on the main thread, as all it
does is posting a message on GstBus, which does asynchronous delivery anyway. So
remove callback and do what it does in the current function.

Q: What if still needs callback approach?


={============================================================================
*kt_linux_core_266* sync: case: sync with no lock

  obj1                    obj2

    \                   /

  // callback called by external thread
  callback_func( object* obj, ... )
  {
    // access and set obj but no lock on objs
    deq_from_q_of_callbaks();
    run_callback();
  }

  ^
  |

  callback_to_hw()
  {
    enq_to_q_of_callbacks();
  }
  
  thread1             thread2

This callback is registered to two threads with different obj pointer. When gets
  called, picks up the obj and use it. But why no locks?

Since callback thread use queue, it's serialized and no need to have lock on
objects.


# ============================================================================
#{
==============================================================================
*kt_linux_core_290*	ref: concurrency in C++

{what-is-concurrency}
There is genuine or hardware concurrency or illusion of concurrency. What's new is that the
increased computing power of these machine comes not from running a single task faster but from
running multiple tasks in parallel.

<parallelism>
The task parallelism is to divide a single task into parts and run each in parallel, thus reducing
the total runtime. It can be complex since there may be many dependencies.

The data parallelism is that each thread performs the same operation on different parts of the data.
Good scalability; many hands make light work. There's a different focus in which more data can be
processed in the same amount of time.


{when-use-concurrency}
The use of concurrency is like any other optimisation strategy. Therefore it is only worth doing for
those performance critical parts of the application where there is the potential for mesurable gain.

{two-approaches-to-concurrency}
<use-multiple-process>
The downsides are 
o communication between processes.
o inherent overhead such as time to launch and resource in OS

The upsides are:
o safer code than threads
o can extend over a network

<use-multiple-thread>
The low overhead with launching and communicating between threads. So C++11 do not provide any
support for process and only support for threads.


{C++11}
o Allow writing portable multithreaded code without relying on platform-specific extensions.
o There is [abstraction penalty] compared to using the underlying low-level facilities directly.

<code-example>
#include <iostream>
#include <thread>

void hello()
{
  std::cout << "hello con world\n";
}

int main(int argc, char** argv)
{
  std::thread t(hello);
  t.join();
  // std::cout << "end of main" << std::endl;
}

If build and run like below:
kt@kt-ub-vb:~/work$ g++ -g -std=c++0x use-thread.cpp 
kt@kt-ub-vb:~/work$ ./a.out 
terminate called after throwing an instance of 'std::system_error'
  what():  Operation not permitted
Aborted (core dumped)

If build and run with pthread option, then works fine. This is known problem in g++.           
kt@kt-ub-vb:~/work$ g++ -g -std=c++0x -pthread use-thread.cpp 


==============================================================================
*kt_linux_core_291*	ref: concurrency in C++

{std-thread-basic}
<launch>
The std::thread works with any [callable] type; function object and lambda.

void do_some_work();
std::thread my_thread(do_some_work);

or

class background_task
{
  public:
    void operator() () const
    {
      do_something();
      do_something_else();
    }
};

background_task f;
std::thread my_thread(f);

Why callable? In this case, the supplied function object is copied into the storage belonging to the
created thread and invoked from there.

<join-and-detach>
If don't decide whether to join or to detach it before std::thread object is destroyed then program
is terminated since the std::thread destructor calls std::terminate(). Ture even in the presence of
exceptions.

The join() cleans up any storage associated with the thread so std::thread object isn't associated
with any thread. Once this, joinable() will return false.
{Q} what will happen when call join() twice on the same?

When detach a thread, make sure that the data accessed by the thread is vaild until the thread has
finished with it. For example, creat a thread within a function with thread function hold pointers
or reference to local varaibles. Bad idea and avoid this. 

void oops()
{
  int some_local_state = 0;
  T my_func(some_local_state);   // T(int&)
  std::thread my_thread(my_func);
  my_thread.detach();
}

<join-raii>
To avoid program being terminated when an exception is thrown, accidental lifetime problems, how?
Can use try and catch but verbose and easy to get it wrong. 

If you wan to do particular action for all possible exit path, whether normal or exceptional, use
raii.

class thread_guard
{
  std::thread& t;

  public:
  explicit thread_guard( std::thread& t_ ): t(t_) {}
  ~thread_guard()
  {
    if( t.joinable() )
    {
      t.join();
    }
  }

  thread_guard( thread_guard const& )=delete;
  thread_guard& operator=( thread_guard const&)=delete;
};

void oops()
{
  int some_local_state = 0;
  T my_func(some_local_state);         // T(int&)
  std::thread my_thread(my_func);
  thread_guard g(t);                   // <DN>

  do_something_in_current_thread();    // main thread continue to run
}


==============================================================================
*kt_linux_core_292*	ref: concurrency in C++

# ============================================================================
#{
==============================================================================
*kt_linux_core_300*	case: own semaphore and mutex class using pthred cond var

POSIX semaphore are system calls which means expensive. Is it possible to implement semaphore without it?

{class-semaphore}

This is for linux. When count is 0, waits and there is no upper limit. Also see that use one mutex
with many condition variables for semaphores.


{util-class}

Just to provide util funcs to all instances since these are static. Also SetPriority is not used.

class CThreadSelf
{
private:
	CThreadSelf(void) {}

public:
	// Returns the ID of the current executing thread.
	'static' int Id(void)
	{ return (int)pthread_self(); }

	'static' bool SetPriority(int priority);
	{
#if defined _LINUX

		  switch (priority)
		  {
			 case CThread::PRIORITY_HIGH: // [note] class type member
				 setpriority(PRIO_PROCESS, pthread_self(), -10); // [note] man setpriority
		       // param.sched_priority = 60;
				 break;
			 case CThread::PRIORITY_NORMAL:
				 setpriority(PRIO_PROCESS, pthread_self(), 0);
		       // param.sched_priority = 50;
				 break;
			 case CThread::PRIORITY_LOW:
		       // param.sched_priority = 40;
				 setpriority(PRIO_PROCESS, pthread_self(), 10);
				 break;
			 default:
				 return false;
		  }

#elif defined _WIN32
	}
};


{semaphore} [KT] the case uses containment(composition) to have implementation.

Use init count but no max count. 0 means to wait and other values means it is okay to get. Used as a
class memeber.

{Q} why need this? code says it calls sched_yield whenever sem count reaches 16.

#define	CONFIG_MAXIMUM_YIELD_COUNTER 16
static unsigned char semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;

pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER; [KT] this is global

struct PSemaphore
{
	pthread_cond_t  cond;
	int             count;
};

class Semaphore
{
	 private:
	 	PSemaphore* m_id;

	 public:
		Semaphore() { m_id = NULL; }
		virtual ~Semaphore() { assert( FlagCreate() == false); }

		bool Create(int count) // initial count
		{
			 pthread_mutex_lock(&mtx);

			 assert( FlagCreate() == false );

			 pthread_cond_t cond = PTHREAD_COND_INITIALIZER;

			 m_id = 'new' PSemaphore;	// new and m_id is not null
			 assert( m_id != NULL );
			 
			 m_id->cond = cond;  // [KT] is it okay as it is local variable?
			 m_id->count = count;

			 pthread_mutex_unlock(&mtx);

			 return m_id != NULL;
		}

		// return true when created
		bool FlagCreate() { return m_id != NULL; }

		virtual void Destory(void)
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  pthread_cond_destroy(&m_id->cond);
			  delete m_id;

			  m_id = NULL;

			  pthread_mutex_unlock(&mtx);
		}

		void Take()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  while (m_id->count <= 0)
			  {
				  pthread_cond_wait(&m_id->cond, &mtx);
			  }

			  m_id->count--;

			  pthread_mutex_unlock(&mtx);
		}

		void Give()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  m_id->count++;

			  pthread_cond_signal(&m_id->cond);

			  pthread_mutex_unlock(&mtx);

			  if (!semCounter--) {
				  sched_yield();
				  semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;
			  }
		}

		void Try(unsigned long msec = 0)
		{
			  if (msec == (unsigned long) INFINITY)
			  {
				  Take();

				  return true;
			  }

			  pthread_mutex_lock(&TimeMutex);
			  ASSERT(FlagCreate() == true);

			  struct timeval  now;
			  struct timespec timeout;
			  int             ret = 0;
			  bool            tf;


			  if (msec == 0)
			  {
				  if (m_id->count <= 0)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }
			  else
			  {
				  while ((m_id->count <= 0) && (ret != ETIMEDOUT))
				  {
					  gettimeofday(&now, NULL);
					  timeout.tv_sec  = now.tv_sec + msec / 1000;
					  timeout.tv_nsec = now.tv_usec + msec % 1000 * 1000;

					  while (timeout.tv_nsec > 1000000)
					  {
						  timeout.tv_nsec -= 1000000;
						  timeout.tv_sec++;
					  }

					  timeout.tv_nsec *= 1000;

					  ret = pthread_cond_timedwait(&m_id->cond, &TimeMutex, &timeout);
				  }

				  if (ret == ETIMEDOUT)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }

			  pthread_mutex_unlock(&TimeMutex);

			  return tf;
		}
};


{use-of-semaphore-one}

To make sure that an user can set prio once a thread is created.

class CThread
{
   PCSemaphore m_pidSync;

   Create()
   {
      m_pidSync.Create(0);
   }

   bool PCThread::SetPriority(int priority)
   {
      ASSERT(FlagCreate() == true);

      if (m_pid == -1)
      {
         m_pidSync.Take();
      }
      ...
   }

   inline void CThreadRun(CThread* thread)
   {
      thread->m_sync[0].Take();

      thread->m_pid = pthread_self();
      thread->m_pidSync.Give();

      thread->t_Main(); [KT] while loop on event get()

      thread->m_sync[1].Give();

      return;
   }
};


{mutex} 

CDerivedA: CMutex
 - thread   - Semaphore : uses global mutex

CDerivedB: CMutex
 - thread   - Semaphore : uses global mutex

Using sync always happens in the same thread. The case uses inheritance to have implementation. This
is based on the fact that mutex is a binary semaphore. Created with 1. Used to give the derived
class the lock/unlock feature to control interface access. That is, sync feature to class objects.
If need other use of sem, then have sem as a member.

class CMutex
{
	 private:
		  PCSemaphore 	m_sem; // [note] use of semaphore
		  int				m_tid;
		  int				m_count;

	 public:
	 	  // [note] no ctor?
	 	  virtual ~Mutex() { assert( FlagCreate() == false); }

		  bool Create(void)
		  {
				assert(FlagCreate() == false);

				if (m_sem.Create(1) == false) // [KT] create a sem
				{
					return false;
				}

				m_threadId = 0;

				return true;
		  }

		  virtual void Destroy(void)
		  {
				ASSERT(FlagCreate() == true);
				m_sem.Destroy();
		  }

		  bool FlagCreate(void) { return m_sem.FlagCreate(); }

		  void Lock(void)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;
				  return;
				}

				m_sem.Take();

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

		  }

		  bool Unlock(void);
		  {
				assert(FlagCreate() == true);

				if (m_threadId != CThreadSelf::Id())
				{
					return false;
				}

				m_count--;

				if (m_count > 0)
				{
				  return true;
				}

				m_threadId = 0;

				m_sem.Give();

				return true;
		  }

		  bool Try(unsigned long msec = 0)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;

				  return true;
				}

				if (m_sem.Try(msec) == false)
				{
				  return false;
				}

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

				return true;

		  }
};	

{cqueue}

struct PTEvent
{
	friend class PCQueue;
	friend class PCTask;

private:
	void* sync;

public:
	class PCHandler* receiver; //!< The Pointer to the handler that receives the event
	unsigned long    type;     //!< The event type

	//! Parameters of an event
	/*!
	 * The union of event parameters are 8-bytes long. That is, it can carry
	 * 2 long integers, 4 short integers, or 8 characters.
	 */
	union
	{
		long  l[2];
		short s[4];
		char  c[8];
	} param;
};

class CQueue : public CMutex
{
	private:
      PCSemaphore m_sem; // [KT] used to say that events are avaiable
		PTEvent     m_event[CONFIG_QUEUE_SIZE];

	bool CQueue::Create(void)
	{
		ASSERT(FlagCreate() == false);

		if (CMutex::Create() == false)
		{ return false; }

		if (m_sem.Create(0) == false)
		{
			CMutex::Destroy();
			return false;
		}

		m_in   = 0;		// [KT] this is {queue-contiguous-implementation} in *kt_dev*
		m_out  = 0;		// in(tail), out(head), size(count)
		m_size = 0;

		return true;
	}

	bool CQueue::Put(PTEvent* event, bool sync=false, bool priority=false)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		PCSemaphore sem;

		if (sync == false)
		{
			event->sync = NULL;
		}
		else
		{
			if (sem.Create(0) == false)
			{
				return false;
			}

			event->sync = &sem;
		}

		Lock();	// CMutex::Lock();

		int size;

		size = (priority == false) ? CONFIG_EMEGENCY_QUEUE_SIZE(16) : 0;
		size += m_size;

		if (size >= CONFIG_QUEUE_SIZE(256) )
		{
			Unlock();

			if (sync == true)
			{
				sem.Destroy();
			}

			PCDebug::Print("ERROR: Event queue full");

			return false;
		}

		if (priority == true)
		{
			m_out          = (m_out + CONFIG_QUEUE_SIZE - 1) % CONFIG_QUEUE_SIZE;
			m_event[m_out] = *event;
			m_size++;
		}
		// [KT] copy in event and inc count
		else
		{
			m_event[m_in] = *event;
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
			m_size++;
		}

		m_sem.Give();

      // [KT] As see how Get() uses m_sem, use m_sem to see if event are avaiable like a count or
      // length and then if there are call Lock() to lock access to this objects. May have some
      // performance gain from this.
		
      Unlock();	

		if (sync == true)
		{
			sem.Take();
			sem.Destroy();
		}

		return true;
	}

	bool PCQueue::Get(PTEvent* event, unsigned long msec = INFINITY)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		while (true)
		{
			if (m_sem.Try(msec) == false)
			{
				return false;
			}

			Lock();

			if (m_size == 0)
			{
				Unlock();

				continue;
			}

			*event = m_event[m_out];
			m_out  = (m_out + 1) % CONFIG_QUEUE_SIZE;
			m_size--;

			Unlock();

			return true;
		}
	}

};


{when-seek-one-specific}

{Q} This suggest that there are events for all(broadcast) and for specific ones. Move all event
between [head+1 ... tail-1] after tail. why? priority?

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver)
{ }

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver, unsigned long type)
{
	ASSERT(FlagCreate() == true);

	bool done = 0;

	Lock();

	unsigned long size = m_size;

	while (size-- != 0)
	{
		if (done == false && m_event[m_out].receiver == receiver && m_event[m_out].type == type)
		{
			*event = m_event[m_out];
			m_size--;
			done = true;
		}
		else
		{
			m_event[m_in] = m_event[m_out];
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
		}

		m_out = (m_out + 1) % CONFIG_QUEUE_SIZE;
	}

	if (done)
	{
		m_sem.Try();
	}

	Unlock();

	return done;
}

This q implementation uses q per thread. The different approach is to have q that threads share.
See *kt_linux_core_014* for msg q between threads


==============================================================================
*kt_linux_core_301*  case: use of mutex and thread class

This is case study using Mutex, Queue and Thread classes.

{CThread}

'inline' void CThreadRun(CThread* thread)
{
	thread->m_sync[0].Take();  // [KT] wait until signaled

#ifdef	_LINUX
	thread->m_pid = pthread_self();
	thread->m_pidSync.Give();
#endif

	thread->t_Main();

	thread->m_sync[1].Give();  // [KT] no use

	return;
}

'static' void* _Process(void* param)
{
	CThreadRun((CThread*)param);

	return NULL;
}

class CThread
{
private:

	int         m_id;
	int			m_pid;
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];

	friend void CThreadRun(CThread* thread); // [KT] inline friend

protected:

	virtual void t_Main(void) = 0;
	/*!< This function is the thread main virtual function.  As soon as a thread starts, this
	 * function is called.  When this function returns, the thread is terminated.
	 *
	 * You have to define this function when you define a new class that inherits CThread class.
	 */

public:

	//! Configuration constants
	enum PTConfigType
	{
		CONFIG_STACK_SIZE = 4096
	};

	enum PTPriorityType
	{
		PRIORITY_HIGH,
		PRIORITY_NORMAL,
		PRIORITY_LOW,
	};

	virtual ~CThread(void) { ASSERT(FlagCreate() == false); }

	bool Create(const char* name, unsigned long stackSize = CONFIG_STACK_SIZE);
	 {
		 ASSERT(FlagCreate() == false);

		 if (m_sync[0].Create(0) == false)
		 {
			 return false;
		 }

		 if (m_sync[1].Create(0) == false)
		 {
			 m_sync[0].Destroy();
			 return false;
		 }

#elif defined _LINUX

		 pthread_attr_t attr;
		 pthread_attr_init(&attr);
		 pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
		 pthread_attr_setstacksize(&attr, stackSize);
		 //struct sched_param schedParam;
		 //schedParam.sched_priority = 50;
		 //pthread_attr_getschedparam(&attr, &schedParam);
		 //pthread_attr_setschedpolicy(&attr, SCHED_RR);

		 m_pid = -1;
		 m_pidSync.Create(0);

		 if (pthread_create((pthread_t*)&m_id, &attr, _Process, this) != 0)
		 {
			 pthread_attr_destroy(&attr);
			 
			 ASSERT(!"CThread not created");

			 m_sync[0].Destroy();
			 m_sync[1].Destroy();

			 return false;
		 }

		 pthread_attr_destroy(&attr); 
		 
		 m_sync[0].Give();   // [KT] now a thread can run
		 return true;
	 }

	//! Check if the instance was created
	bool FlagCreate(void) { return m_sync[0].FlagCreate(); }
	//! Destroy the instance
	virtual void Destroy(void);

	//! Returns the ID of the CThread.
	int Id(void)
	{
		 ASSERT(FlagCreate() == true);
		 return m_id;
	}

	//! Set the priority value for the thread
	bool SetPriority(int priority);
};

The class hiarachy is:

class CThread
{
private:
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];
};

class CQueue : public CMutex

class CTask: private CThread, public CQueue, public CHandler
{
   private:
      void t_Main(void);

   protected:
      virtual bool t_Create(void);
};

class CSIEngineBase : public PCTask

class CSIVoiceEngine : public CSIEngineBase

class CSIEngineManager


<run>

CThread:
	CThreadRun: 
      thread->t_Main();	[pure-virtual] [main-start]

CTask:
              	t_Main()                 [main-end] {template-method} in kt_dev_txt
					{
				   	t_Create();                      [virtual] [create-start]
                 	while (ExecuteEvent() == true)   [thread-loop]
						//	virtual bool ExecuteEvent
						//	{
						//		 PTEvent event;
						//		 Get(&event, m_msec); [copy-in-event]
						//		 event.receiver->OnEvent(&event);
						//	}

					  	t_Destory();
					}

CSIEngineBase:
                                                 
CSIVoiceEngine:
                                                  t_Create() [create-end]
																     SendSelfEvent( OWN_EVENT_TYPE );

<event>

CHandler:
	protected:
	virtual bool t_OnEvent(const PTEvent* event) = 0;

	public:
	inline bool OnEvent
	{
      t_OnEvent() [pure-virtual] [event-start]
	}

CTask: public CHandler
	No OnEvent which means use CHandler one

	protectd:
	bool PCTask::t_OnEvent(const PTEvent* event)
	{
		has default basic event handling
	}

	static Send( PTEvent* event, bool sync = false, bool priority = false);
	  [to-other-task]
	  if ((sync == false) || (event->receiver->Task()->Id() != PCThreadSelf::Id()))
	  {
		 return event->receiver->Task()->Put(event, sync, priority);
		 {
			  In CQueue::Put, use copy-ctor of event structure to copy it into receiver's taks
			  m_event[].

			  m_event[i] *event;
		 }
	  }
	  [self] ends up with a func call
	  event->receiver->OnEvent(event);
		   
CSIEngineBase:
   public:
      bool SendSelfEvent(int nEventType)
      {
         PTEvent evt;
         evt.receiver = this; [event-to-self]
         evt.type = nEventType;
         Send(&evt);   // CTask::
      }

  protected:
      virtual bool t_OnEvent(const PTEvent* event)
      { return true; }

		virtual bool t_OnEvent( PTEvent* ) [event-end]
      {
		   PCTask::t_OnEvent(event); {hook-operation} in {template-method}
		   t_ProcessEvent(event); [virtual] [start]
      }

CSIVoiceEngine:
   SendSelfEvent( OWN_EVENT_TYPE );  // CSIEngineBase::

   virtual bool t_ProcessEvent(event);           [end]
   {
	   handle OWN_EVENT_TYPE;
   }


<create>

CThread:
   Create:
	  pthread_create

CTask:
	public:
   Create()
	  PCQueue::Create();
	  PCHandler::Create(this);
	  CThread::Create(stackSize);

CSIEngineBase: public CTask
	public:
   Create(const char* name)
      return CTask::Create(name);					[create]

CSIVoiceEngine: public CSIEngineBase
   no Create:

// {design-note}
// This can be a application manager which creates all applications and call Create() on them. This
// is platform wide and each application can override t_Create and init their own thing without
// knowing Create() calls made from outside.

class CSIEngineManager:
{
	 private:
	 CSIEngineBase* m_pVoiceEngine;

	 static CSIEngineManager* CSIEngineManager::GetInstance(void) {factory-method}
	 {
		 if(m_EngineManagerInstance == NULL)
		 {
			 m_EngineManagerInstance = 'new' CSIEngineManager;
		 }

		 return m_EngineManagerInstance;
	 }

	 CSIEngineManager::GetEngineInstance
	 {
		  m_pVoiceEngine =  'new' CSIVoiceEngine;
		  m_pVoiceEngine->Create("SIVoiceEngine"); // [create] and use inherited implementation
	 }
}

From review, t_ prefix means a primitive operation and to be overridden and all works are using the
most derived class object. Therefore, dreived version will be used for virtuals as shown
{template-method}


==============================================================================
*kt_linux_core_302*  case: analysis of 200 and 201 case

{how-this-work}

CUserClass instance:
                                       
CUserClass {
   // Eash CTask has a Q and thread
   CTask { CQueue, CHandler : CQueue { CMutex }
      CThread
         : pthread( staic _Process )
      }
}
  
static _Process(this) will run the derived class CTask t_Main which has a message loop. So this
_Process is a template code for all threads. 

Get(&event); in which all threads use the same code for thread routine

CUserB instance:
                                       
CUserB {
   CTask {
      CThread
         : pthread( staic _Process )
      }
}

Mutex class via inheritance:
Mutex #01     Mutex #02    Mutex #03     Mutex #04    Mutex #04  
(Semaphore)   (Semaphore)  (Semaphore)   (Semaphore)  (Semaphore)

Sem member in a class:
Sepmphore #01  Sepmphore #02 ...


Regarding q, each task has a q and other task can call put to insert a mesg to receiver's q.

{Q} Eash has its own pthread cond in Semaphore but all use a single global mutex for signaling. How
about performance? Is it better solution?


==============================================================================
*kt_linux_core_303*  case: msg q between threads

This uses stl q and sems to read, write and count(length) lock:


/** Maximum length of message queue name */
#define MQ_NAME_LENGTH 5

/** Queue Magic identifier Corresponds to ASCII QuEu*/
#define MQ_MAGIC 0x51754575

typedef struct PFMMessageQueueInfo_t_
{
    char                name[ MQ_NAME_LENGTH ];
    PCSemaphore         *readsem;
    PCMutex             *writeLock;
    PCMutex             *readLock;
    uint32_t            magic;
    std::queue<SPfmMessage>* container;
} PFMMessageQueueInfo_t;


extern "C"
{
///////////////////////////////////////////////////
// PFM Queue Create
///////////////////////////////////////////////////
HPfmQueue
pfmQueueCreate(const char* name, uint32_t max_size)
{
    PFMMessageQueueInfo_t *qptr;

    // Allocate queue controll structure
    qptr = (PFMMessageQueueInfo_t *)pfmMalloc( sizeof(PFMMessageQueueInfo_t) );
    if(!qptr)
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc\n");
        return PFM_NULL_HANDLE;
    }

    // create storage container
    qptr->container = new std::queue<SPfmMessage>();
    if (qptr->container == NULL)
    {
        fprintf(stderr,
            "pfmQueueCreate failed to alloc storage\n");
        pfmFree(qptr);
        return PFM_NULL_HANDLE;
    }

    // Create & initialize Read mutex for read serialization
    qptr->readLock = new PCMutex;
    if( qptr->readLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc read mutex\n");

        delete qptr->container;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    qptr->readLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init read mutex\n");

        delete qptr->container;
        delete qptr->readLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create & initialize Write mutex for write serialization
    qptr->writeLock = new PCMutex;
    if( qptr->writeLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }
    qptr->writeLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create and initialize read semaphore
    qptr->readsem = new PCSemaphore;
    if( qptr->readsem == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc semaphore\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    //Initialize read semaphore so it will "block" on try.
    qptr->readsem->Create(0);
    if( !qptr->readsem->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc sem\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        delete qptr->readsem;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Copy semaphore name
    if( !name )
    {
        char nameTmp[] = "SEM";
        PCString::Copy( qptr->name,nameTmp,MQ_NAME_LENGTH);
    }
    else
    {
        PCString::Copy( qptr->name,name,MQ_NAME_LENGTH);
    }

    qptr->name[ (MQ_NAME_LENGTH-1) ] = (char)NULL;
    qptr->magic = MQ_MAGIC;

    return (HPfmQueue)qptr;
}

///////////////////////////////////////////////////
// PFM Queue Destroy
///////////////////////////////////////////////////
pfmerr_t
pfmQueueDestroy(HPfmQueue h)
{
    // Validate input
    if( h == PFM_NULL_HANDLE)
    {
        pfmAssert( h != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)h;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Mark queue as invalid.
    qptr->magic = 0;

    // Release Reader (if exists) & Grab read/write locks.
    // This shall prevent "use while destruct" scenario.
    // Mutexes can be destroyed while locked.
    qptr->readsem->Give();
    qptr->readLock->Lock();
    qptr->writeLock->Lock();

    if( qptr->readsem->FlagCreate() )
    {
        qptr->readsem->Destroy();
    }
    if( qptr->readLock->FlagCreate() )
    {
        qptr->readLock->Destroy();
    }
    if( qptr->writeLock->FlagCreate() )
    {
        qptr->writeLock->Destroy();
    }

    delete qptr->readsem;
    delete qptr->readLock;
    delete qptr->writeLock;

    delete qptr->container;

    pfmFree( qptr );

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Receive
///////////////////////////////////////////////////
pfmerr_t
pfmQueueReceive(HPfmQueue q, uint32_t timeout_ms, SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    unsigned long readLockTickStart;
    unsigned long readLockTickEnd;
    unsigned long tickDiff;
    uint32_t    waitTime;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Convert PFM infinite timeout to Shadwo's infinite timeout
    switch( timeout_ms )
    {
        case PFM_WAIT_NONE:
            waitTime = 0;
            break;
        case PFM_WAIT_FOREVER:
            waitTime = INFINITY;
            break;
        default:
            waitTime = timeout_ms;
            break;
    }

    // Do thread serialized reading
    readLockTickStart = PCTime::Tick();
    if( !qptr->readLock->Try(waitTime) )
    {
        return ERR_TIMEDOUT;
    }

    // If waiting for data, do the tick calculation to
    // potentially reduce wait value
    if( waitTime )
    {
        readLockTickEnd = PCTime::Tick();
        tickDiff = pfmTickDiff( readLockTickStart, readLockTickEnd );

        // Check if we have time to hang on a samephore
        if( tickDiff > waitTime )
        {
            // We have waited longer on a mutex, so wait as short as possible
            waitTime = 0;
        }
        else
        {
            waitTime -= tickDiff;
        }
    }

    // Consume semaphore as it is signalled after every insertion
    qptr->readsem->Try(waitTime);

    // Check if there's data in the queue. This should only happen
    // on wait with timeout. Should not occour in infinite timeout
    // scenario
    if (qptr->container->empty())
    {
        pfmAssert(waitTime != (uint32_t)INFINITY );
        qptr->readLock->Unlock();
        return ERR_TIMEDOUT;
    }

    // read message
    *msg = qptr->container->front();
    qptr->container->pop();

    // This should be the last possible place where
    // a dying queue could be trapped (unlock fails).
    if( !qptr->readLock->Unlock() )
    {
        return ERR_SYS;
    }

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Send
///////////////////////////////////////////////////
pfmerr_t
pfmQueueSend(HPfmQueue q, const SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    pfmerr_t res;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Grab an read Lock
    qptr->writeLock->Lock();

    qptr->container->push(*msg);

    // Inform readers that there's data avaliable.
    qptr->readsem->Give();

    //Unlock queue
    if( !qptr->writeLock->Unlock() )
    {
        // This should handle a dying queue
        res = ERR_SYS;
    }
    else
    {
        res = ERR_OK;
    }

    return res;
}


={============================================================================
*kt_linux_gcc_400* gcc-libc-uclibc-and-glibc

$ nm libc.a | grep libc_malloc
5440:00005130 T __libc_malloc
5441:00000004 D __libc_malloc_initialized
5579:         U __libc_malloc_initialized

$ nm libuClibc-1.0.17.so | grep malloc
784:00054850 T __uc_malloc
785:000775e0 B __uc_malloc_failed
1513:00051820 T malloc


={============================================================================
*kt_linux_gcc_400* gcc-libc-uclibc

http://www.etalabs.net/compare_libcs.html
Comparison of C/POSIX standard library implementations for Linux


http://www.musl-libc.org/
musl New standard C library. musl is lightweight, fast, simple, free, and
  strives to be correct in the sense of standards-conformance and safety.

<uclibc>
http://elinux.org/Toolchains#C_library
uClibc is an alternate C library, which features a much smaller footprint.
This library can be an interesting alternative if flash space and/or memory
footprint is an issue. However, the space advantages gained using uClibc are
becoming less important as the price of memory & flash continues to drop. It
is still useful C library for embedded systems without MMU.

http://www.uclibc.org/
http://git.uclibc.org/uClibc/
15 May 2012, uClibc 0.9.33.2 Released
uClibc-0.9.33.2 was released today. 

$ wget --no-check-certificate https://www.uclibc.org/downloads/uClibc-0.9.33.2.tar.xz

1. 
    To configure uClibc, you can run:

            make menuconfig

2. To enable WORDEXP

#
# Big and Tall
#
UCLIBC_HAS_WORDEXP=y


<uclibc-ng>
http://www.uclibc-ng.org/
Latest Release
*NEWS* 1.0.18 (Codename Delirium Nocturnum) released 27.09.2016 *NEWS* 

uClibc-ng is a spin-off of uClibc (from Erik Andersen) from
http://www.uclibc.org. Our main goal is to provide regulary a stable and
tested release.

wget http://downloads.uclibc-ng.org/releases/1.0.17/uClibc-ng-1.0.17.tar.xz


={============================================================================
*kt_linux_gcc_400* gcc-libc-uclibc-ld

http://git.uclibc.org/uClibc/plain/extra/Configs/Config.in

config SUPPORT_LD_DEBUG
	bool "Build the shared library loader with debugging support"
	depends on HAVE_SHARED
	help
	  Answer Y here to enable all the extra code needed to debug the uClibc
	  native shared library loader.  The level of debugging noise that is
	  generated depends on the LD_DEBUG environment variable...  Just set
	  LD_DEBUG to something like: 'LD_DEBUG=token1,token2,..  prog' to
	  debug your application.  Diagnostic messages will then be printed to
	  the stderr.

	  For now these debugging tokens are available:
	    detail        provide more information for some options
	    move          display copy processing
	    symbols       display symbol table processing
	    reloc         display relocation processing; detail shows the
	                  relocation patch
	    nofixups      never fixes up jump relocations
	    bindings      displays the resolve processing (function calls);
	                  detail shows the relocation patch
	    all           Enable everything!

	  The additional environment variable:
	    LD_DEBUG_OUTPUT=file
	  redirects the diagnostics to an output file created using
	  the specified name and the process id as a suffix.

	  An excellent start is simply:
	    $ LD_DEBUG=binding,move,symbols,reloc,detail ./appname
	  or to log everything to a file named 'logfile', try this
	    $ LD_DEBUG=all LD_DEBUG_OUTPUT=logfile ./appname

	  If you are doing development and want to debug uClibc's shared library
	  loader, answer Y.  Mere mortals answer N.


={============================================================================
*kt_linux_gcc_400* gcc-libc-uclibc--fail to find symbol

Q: HOW to check if linker finds a symbols? 

<1>
Another unfortunate consequence of this situation, together with poor
implementation of uClibc dynamic linker [2], is that some libraries fail to
load. This can happen in a scenario when a C program loads a C library which
loads another C library that depends on some C++ libraries. uClibc dynamic
linker in some of such cases doesn't resolve symbols in the right order,
especially for global C++ objects with constructors!

[2] It's been proved many times that uClibc dynamic linker implementation
available on devices is broken as it doesn't resolve symbols properly when their
dependencies are complex enough. Possibly upstream implementation works better
but the one available from Broadcom is a mix of a very old version and some
random backported patches.

This libgstcencdec.so is a C library which depends on another C library:
libdrm.so. The latter one depends on several C++ libraries, with one being
SystemAPI.so. Because SystemAPI.so now depends on dbus-c++-1, the dynamic linker
tries to load it. But due some unfortunate library and symbol ordering, it gets
it wrong which results in a SIGSEGV while scanning for plug-ins:

Program received signal SIGSEGV, Segmentation fault.

[Switching to Thread 0x77ff4000 (LWP 1548)]
0x00000000 in ?? ()
(gdb) bt
#0 0x00000000 in ?? ()
#1 0x7661d180 in global constructors keyed to eventloop_integration.cpp () 
  from /opt/zinc-trunk/lib/libdbus-c++-1.so.0

#2 0x7661d558 in __do_global_ctors_aux () from 
  /opt/zinc-trunk/lib/libdbus-c++-1.so.0


<2>
During development, works okay but suddenly failed to start during code
management.

After all, found that there is one function which thought was deleted but was in
the code to be called. However, there is no function definition since it was
removed but only calls to that function remained.

How could that possible without linking error? Since it was so file and run nm:

U        is undefined meaning extern

This was undefined and supposed to be linked when loaded. So when run
application, failed to find symbol and crashes. 

root        1.1M Sep 18 15:16 core.gst-plugin-scan.8785.HUMAX.1442585817
root        1.1M Sep 18 15:16 core.gst-plugin-scan.8786.HUMAX.1442585817
root      171.9M Sep 18 15:17 core.multiqueue1:src.8775.HUMAX.1442585819


<3>
http://stackoverflow.com/questions/1617286/easy-check-for-unresolved-symbols-in-shared-libraries

Q: To check if there are undefined symbol at compile time than dynamic loading
time?

A: (not checked yet)

Check out the linker option -z defs / --no-undefined. When creating a shared
object, it will cause the link to fail if there are unresolved symbols.

If you are using gcc to invoke the linker, you'll use the compiler -Wl option to
pass the option to the linker:

gcc -shared ... -Wl,-z,defs

As an example, consider the following file:

#include <stdio.h>

void forgot_to_define(FILE *fp);

void doit(const char *filename)
{
    FILE *fp = fopen(filename, "r");
    if (fp != NULL)
    {
        forgot_to_define(fp);
        fclose(fp);
    }
}

Now, if you build that into a shared object, it will succeed:

> gcc -shared -fPIC -o libsilly.so silly.c && echo succeeded || echo failed
succeeded

But if you add -z defs, the link will fail and tell you about your missing symbol:

> 
gcc -shared -fPIC -o libsilly.so silly.c -Wl,-z,defs && echo succeeded || echo failed
/tmp/cccIwwbn.o: In function `doit':
silly.c:(.text+0x2c): undefined reference to `forgot_to_define'
collect2: ld returned 1 exit status
failed


={============================================================================
*kt_linux_gcc_400* gcc-option-directory-search

https://gcc.gnu.org/onlinedocs/gcc/Directory-Options.html#Directory-Options
These options specify directories to search for header files, for libraries
and for parts of the compiler: 

-Ldir
Add directory dir to the list of directories to be searched for -l. 
note: used when compile.

-Lsearchdir 
--library-path=searchdir 

Add path searchdir to the list of paths that ld will search for archive
libraries and ld control scripts. You may use this option any number of times.
The directories are searched in the order in which they are specified on the
command line. Directories specified on the command line are searched before
the default directories. All -L options apply to all -l options, regardless of
the order in which the options appear. 

If searchdir begins with =, then the = will be replaced by the sysroot prefix,
a path specified when the linker is configured. 

The default set of paths searched (without being specified with `-L') depends
on which emulation mode ld is using, and in some cases also on how it was
configured. See Environment. 

The paths can also be specified in a link script with the SEARCH_DIR command.
Directories specified this way are searched at the point in which the linker
script appears in the command line. 


={============================================================================
*kt_linux_gcc_400* gcc-option-link

http://gcc.gnu.org/onlinedocs/gcc/Link-Options.html#Link-Options


-llibrary
-l library
Search the library `named library` when `linking`. 

<order>
It makes a difference where in the command you write this option; the linker
searches and processes libraries and object files in the order they are
specified. Thus, 'foo.o -lz bar.o' searches library 'z' after file foo.o but
before bar.o. If bar.o refers to functions in 'z', those functions may not be
loaded.

The linker searches a standard list of directories for the library, which is
actually a file named liblibrary.a. The linker then uses this file as if it
had been specified precisely by name.

The directories searched include several standard system directories plus any
that you specify with -L.

Normally the files found this way are library files-archive files whose
members are object files. The linker handles an archive file by scanning
through it for members which define symbols that have so far been referenced
but not defined. But if the file that is found is an ordinary object file, it
is linked in the usual fashion. 

The only difference between using an -l option and specifying a file name is
that -l surrounds library with 'lib' and '.a' and searches several
directories.

-shared
Produce a shared object which can then be linked with other objects to form an
executable. Not all systems support this option. For predictable results, you
must also specify the same set of options used for compilation (-fpic, -fPIC,
    or model suboptions) when you specify this linker option.1 

https://gcc.gnu.org/onlinedocs/gcc/Code-Gen-Options.html#Code-Gen-Options

3.16 Options for Code Generation Conventions

-fpic
    Generate position-independent code (PIC) suitable for use in a shared
    library, `if supported for the target machine` Such code accesses all
    constant addresses through a global offset table (GOT). The dynamic loader
    resolves the GOT entries when the program starts (the dynamic loader is
        not part of GCC; it is part of the operating system). If the GOT size
    for the linked executable exceeds a machine-specific maximum size, you get
      an error message from the linker indicating that -fpic does not work; in
        that case, recompile with -fPIC instead. (These maximums are 8k on the
            SPARC, 28k on AArch64 and 32k on the m68k and RS/6000. The x86 has
            no such limit.)

    Position-independent code requires special support, and therefore works
    only on certain machines. For the x86, GCC supports PIC for System V but
    not for the Sun 386i. Code generated for the IBM RS/6000 is always
    position-independent.

    When this flag is set, the macros __pic__ and __PIC__ are defined to 1.

-fPIC
    If supported for the target machine, emit position-independent code,
       suitable for dynamic linking and avoiding any limit on the size of the
         global offset table. This option makes a difference on AArch64, m68k,
       PowerPC and SPARC.

    Position-independent code requires special support, and therefore works
    only on certain machines.

    When this flag is set, the macros __pic__ and __PIC__ are defined to 2.


-Wl,option
Pass option as an option to the linker. If option contains commas, it is split into multiple options
at the commas. You can use this syntax to pass an argument to the option. For example,
-Wl,-Map,output.map passes -Map output.map to the linker. When using the GNU linker, you can also
get the same effect with -Wl,-Map=output.map. 

-nostdlib
Do not use the standard system startup files or libraries when linking. No startup files and only
the libraries you specify are passed to the linker, and options specifying linkage of the system
libraries, such as -static-libgcc or -shared-libgcc, are ignored.

The compiler may generate calls to memcmp, memset, memcpy and memmove. These entries are usually
resolved by entries in libc. These entry points should be supplied through some other mechanism when
this option is specified.

One of the standard libraries bypassed by -nostdlib and -nodefaultlibs is libgcc.a, a library of
internal subroutines which GCC uses to overcome shortcomings of particular machines, or special
needs for some languages. (See Interfacing to GCC Output, for more discussion of libgcc.a.) In most
cases, you need libgcc.a even when you want to avoid other standard libraries. In other words, when
you specify -nostdlib or -nodefaultlibs you should usually specify -lgcc as well. This ensures that
you have no unresolved references to internal GCC library subroutines. (An example of such an
internal subroutine is '__main', used to ensure C++ constructors are called; see collect2.) 


={============================================================================
*kt_linux_gcc_400* gcc-options-debug

3.9 Options for Debugging Your Program

https://gcc.gnu.org/onlinedocs/gcc/Debugging-Options.html#Debugging-Options

GCC allows you to use -g with -O. The shortcuts taken by optimized code may
occasionally produce surprising results: some variables you declared may not
exist at all; flow of control may briefly move where you did not expect it; some
statements may not be executed because they compute constant results or their
values are already at hand; some statements may execute in different places
because they have been moved out of loops.

Nevertheless it proves possible to debug optimized output. This makes it
reasonable to use the optimizer for programs that might have bugs.

If you are not using some other optimization option, consider using -Og (see
        Optimize Options) with -g. With no -O option at all, some compiler
passes that collect information useful for debugging do not run at all, so that
-Og may result in a better debugging experience. 


-g
    Produce debugging information in the operating system's native format
    (stabs, COFF, XCOFF, or DWARF 2). GDB can work with this debugging
    information.

    On most systems that use stabs format, -g enables use of extra debugging
    information that only GDB can use; this extra information makes debugging
    work better in GDB but probably makes other debuggers crash or refuse to
    read the program. If you want to control for certain whether to generate the
    extra information, use -gstabs+, -gstabs, -gxcoff+, -gxcoff, or -gvms (see
            below).

-ggdb
    Produce debugging information for use by GDB. This means to use the most
    expressive format available (DWARF, stabs, or the native format if neither
            of those are supported), including GDB extensions if at all
    possible. 

-g`level`
-ggdb`level`
-gstabslevel
-gcofflevel
-gxcofflevel
-gvmslevel
    Request debugging information and also use level to specify how much
    information. The default level is 2.

    Level 0 produces no debug information at all. Thus, -g0 negates -g.

    Level 1 produces minimal information, enough for making backtraces in parts
    of the program that you don't plan to debug. This includes descriptions of
    functions and external variables, and line number tables, but no information
    about local variables.

    Level 3 includes extra information, such as all the macro definitions
    present in the program. Some debuggers support macro 'expansion' when you
    use -g3.

    -gdwarf does not accept a concatenated debug level, to avoid confusion with
    -gdwarf-level. Instead use an additional -glevel option to change the debug
    level for DWARF.


<ex>

#define CONST_VALUE (10)

// when no macro expansion

(gdb) p CONST_VALUE
No symbol "CONST_VALUE" in current context.

// when use either -ggdb3 or -g3

20	  cout << "value: " << CONST_VALUE << endl;
(gdb) n
value: 10
22	  string str("this is string");
(gdb) p CON
CONCAT       CONST_VALUE  
(gdb) p CONST_VALUE
$1 = 10


={============================================================================
*kt_linux_gcc_400* gcc-options-verbose

https://gcc.gnu.org/onlinedocs/gcc/Option-Index.html#Option-Index

-v

Print (on standard error output) the commands executed to run the stages of
compilation. Also print the version number of the compiler driver program and
of the preprocessor and the compiler proper. 

note: can see spec and include path

-###

Like -v except the commands are `not executed` and arguments are quoted unless
they contain only alphanumeric characters or ./-_. This is useful for shell
scripts to capture the driver-generated command lines. 


note: the -### flag to see the exact compilation steps and notice which
libraries get picked up, etc. (note that to see -lstdc++ a C++ source compiled
    with g++4.8 is needed instead).


-print-search-dirs 

note: to check path for lib


={============================================================================
*kt_linux_gcc_400* gcc-options-code-gen

-fpic or -fPIC

Generate position-independent code (PIC) suitable for use in a shared library,
if supported for the target machine. Such code accesses all constant addresses
  through a global offset table (GOT). The dynamic loader resolves the GOT
    entries when the program starts (the dynamic loader is not part of GCC; it
        is part of the operating system). If the GOT size for the linked
    executable exceeds a machine-specific maximum size, you get an error
    message from the linker indicating that -fpic does not work; in that case,
            recompile with -fPIC instead. (These maximums are 8k on the SPARC
                and 32k on the m68k and RS/6000. The 386 has no such limit.)

Position-independent code requires special support, and therefore works only
on certain machines.  For the 386, GCC supports PIC for System V but not for
the Sun 386i. Code generated for the IBM RS/6000 is always
position-independent.

When this flag is set, the macros __pic__ and __PIC__ are defined to 1. 


={============================================================================
*kt_linux_gcc_400* gcc-options-optimization

https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#Optimize-Options

3.10 Options That Control Optimization

These options control various sorts of optimizations.

Without any optimization option, the compiler's goal is to reduce the cost of
compilation and to make debugging produce the expected results. Statements are
independent: if you stop the program with a breakpoint between statements, you
can then assign a new value to any variable or change the program counter to
any other statement in the function and get exactly the results you expect
from the source code.

Turning on optimization flags makes the compiler attempt to improve the
performance and/or code size at the expense of compilation time and possibly
the ability to debug the program.

The compiler performs optimization based on the knowledge it has of the
program. Compiling multiple files at once to a single output file mode allows
the compiler to use information gained from all of the files when compiling
each of them.

Not all optimizations are controlled directly by a flag. Only optimizations
that have a flag are listed in this section.

Most optimizations are only enabled if an -O level is set on the command line.
Otherwise they are disabled, even if individual optimization flags are
specified.

Depending on the target and how GCC was configured, a slightly different set
of optimizations may be enabled at each -O level than those listed here. You
can invoke GCC with -Q --help=optimizers to find out the exact set of
optimizations that are enabled at each level. See Overall Options, for
examples. 

-O0
    Reduce compilation time and make debugging produce the expected results.
    This is the default. 

-fomit-frame-pointer

From LPI 41.

In addition, on some architectures, such as x86-32, the –fomit–frame–pointer
option should not be specified because this makes debugging impossible. On
some architectures, such as x86-64, this option is enabled by default since it
doesn't prevent debugging. For the same reason, executables and libraries
should not be stripped of debugging information using strip(1).

From gcc doc.

Don't keep the frame pointer in a register for functions that don't need one.
This avoids the instructions to save, set up and restore frame pointers; it
also makes an extra register available in many functions. It also makes
debugging impossible on some machines.

On some machines, such as the VAX, this flag has no effect, because the
standard calling sequence automatically handles the frame pointer and nothing
is saved by pretending it doesn't exist. The machine-description macro
FRAME_POINTER_REQUIRED controls whether a target machine supports this flag.
See Register Usage.

Enabled at levels -O, -O2, -O3, -Os. 


={============================================================================
*kt_linux_gcc_400* gcc-build-target-triplet

{triplet}
http://wiki.osdev.org/Target_Triplet

Target Triplets describe a platform on which code runs and are a core concept
in the GNU build system. They contain three fields: the name of the CPU
family/model, the vendor, and the operating system name. You can view the
unambiguous target triplet for your current system by running:

gcc -dumpmachine

<ex> pc vm
kyoupark@kit-debian:~/github$ gcc -dumpmachine
i586-linux-gnu

mips-linux-uclibc-gcc -dumpmachine
mips-linux-uclibc


Structure

Target triplets have this simple structure:

machine-vendor-operatingsystem

For instance, a FreeBSD system could be written as:

x86_64-unknown-freebsd

Notice how the vendor field is mostly irrelevant and is usually 'pc' for
32-bit x86 systems or 'unknown' or 'none' for other systems. The simple
three-field target triplet we have seen so far is unambiguous and easy to
parse. However, since the vendor field is mostly unused, the GNU build system
allows you to leave out the vendor field; the build system will automatically
insert a default vendor part when it disambiguates your target triplet. For
instance, this allows you to type:

x86_64-freebsd

The build system will then automatically deduce that the vendor is the default
(unknown) if it wishes to know the unambiguous target triplet. Note that
parsing target triplets are a bit more tricky, as sometimes the operating
system field `can be two fields`:

x86_64-unknown-linux-gnu

This gets a bit worse since the vendor field can be left out:

x86_64-linux-gnu

note: 
/home/kyoupark/si-logs/gcc-build/gcc-4.8.2/config.sub

This is most definitely ambiguous. Most autoconf-based packages ship with a
huge shell script called `config.sub` whose function is to disambiguate target
triplet using a long list of known CPUs and known operating systems. 


Rationale

Target triplets are intended to be systematic unambiguous platform names
(well, after disambiguation). They lets build systems understand exactly which
system the code will run on and allows enabling platform-specific features
automatically. In any compilation setting, there are usually three platforms
involved (which might be the same three ones):

    `Build Platform`: This is the platform on which the compilation tools are
    executed.

    `Host Platform`: This is the platform on which the code will eventually run.

    `Target Platform`: If this is a compiler, this is the platform that the
    compiler will generate code for. 

This means that up to three differently-targeted compilers might be in play
(if you are building a GCC on platform A, which will run on platform B, which
 produces executables for platform C). This problem is solved by simply
prefixing the compilation tools with the target triplet. When you build a
cross-compiler, the installed executables will be prefixed with the specified
target triplet:

i686-elf-gcc

This prevents the wrong compiler from being used (and prevents things from the
    build machine to leak onto the target machine) if build systems are
carefully to prefix all compilation tools with the target prefix. 


<from-autoconf>

16.7 Specifying the System Type
===============================

There may be some features `configure' cannot figure out automatically,
but needs to determine by `the type of machine the package will run on`

Usually, assuming the package is built to be run on the _same_
architectures, `configure' can figure that out, but if it prints a
message saying it cannot guess the machine type, give it the
`--build=TYPE' option.  TYPE can either be a short name for the system
type, such as `sun4', or a canonical name which has the form:

     CPU-COMPANY-SYSTEM

where SYSTEM can have one of these forms:

     OS
     KERNEL-OS

   See the file `config.sub` for the possible values of each field.  If
   `config.sub' isn't included in this package, then this package doesn't need
   to know the machine type.

   If you are _building_ compiler tools for cross-compiling, you should use
   the option `--target=TYPE` to select the type of system they will produce
   code for.

   If you want to _use_ a cross compiler, that generates code for a platform
   different from the build platform, you should specify the "host" platform
   (i.e., that on which the generated programs will eventually be run) with
   `--host=TYPE`.


<how-to-change-gcc-to-have-own-triplet>
Target Triplets for Operating Systems Development

For instance, if you develop your own operating system and modify GCC to add a
new target triplet, yours could be: http://wiki.osdev.org/OS_Specific_Toolchain


<in-configure>

case "${target}" in
  *-*-chorusos)
    ;;


<ex>
the `MIPSel` (little endian) 

$ ~/STB_SW/FUSIONOS_2/BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-uclibc-gcc -### x.c

Using built-in specs.
Target: mips-linux-uclibc
Configured with: ../gcc-4.2.0-20070124/configure
--prefix=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/
--build=mips-linux --host=mips-linux --target=mips-linux-uclibc
--with-build-sysroot=/usr/src/redhat/BUILD/build_uClibc
--enable-languages=c,c++ --disable-__cxa_atexit --enable-target-optspace
--with-gnu-ld --with-float=hard --enable-threads
--infodir=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/info
--mandir=/opt/toolchains/crosstools_hf-linux-2.6.18.0_gcc-4.2-10tsHound_uclibc-nptl-0.9.29-20070423_20140508/man
--with-arch=mips32 --disable-libmudflap --disable-nls --with-gnu-plts

Thread model: posix
gcc version 4.2.0 20070124 (prerelease) - BRCM 10tsHound-20140508
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../libexec/gcc/mips-linux-uclibc/4.2.0/cc1"
 "-quiet" "-iprefix"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/"
 "x.c" "-quiet" "-dumpbase" "x.c" "-march=mips32" "-mhard-float" "-mgnu-plts"
 "-mno-shared" "-minterlink-mips16" "-auxbase" "x" "-o" "/tmp/ccW3IwoX.s"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/bin/as"
 "-EB" "-no-mdebug" "-mabi=32" "-march=mips32" "-mno-shared" "-o"
 "/tmp/cck3ulQO.o" "/tmp/ccW3IwoX.s"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../libexec/gcc/mips-linux-uclibc/4.2.0/collect2"
 "--eh-frame-hdr" "-EB" "-dynamic-linker" "/lib/ld-uClibc.so.0"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib/crt1.o"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib/crti.o"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/crtbegin.o"
 "-L/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0"
 "-L/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc"
 "-L/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib"
 "/tmp/cck3ulQO.o" "-lgcc" "--as-needed" "-lgcc_s" "--no-as-needed" "-lc"
 "-lgcc" "--as-needed" "-lgcc_s" "--no-as-needed"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/crtend.o"
 "/home/NDS-UK/kyoupark/STB_SW/deps/SYSTEM_BIN_4/mips4k_gcc_x86_linux_hound_2/bin/../lib/gcc/mips-linux-uclibc/4.2.0/../../../../mips-linux-uclibc/lib/crtn.o"


<dumpspecs>
compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-uclibc-gcc -dumpspecs

note:
0 or 1

*cross_compile:
1

*version:
4.2.0

*multilib:
. ;

*multilib_defaults:
EB mips1 mabi=32

*asm_abi_default_spec:
-mabi=32


={============================================================================
*kt_linux_gcc_400* gcc-build-failed-cases

<1>
// note:
// DO NOT USE THIS method since fails to build gcc and there seems to be a
// problem in source distibution package.
//
// https://gcc.gnu.org/bugzilla/show_bug.cgi?id=48378
//
// Get gcc source from the distribution
// ------------------------------------
// 
// $ sudo apt-get source libstdc++6
// Reading package lists... Done
// Building dependency tree       
// Reading state information... Done
// Picking 'gcc-4.7' as source package instead of 'libstdc++6'
// ... (starts downloading)


<1>
http://eli.thegreenplace.net/2014/01/16/building-gcc-4-8-from-source-on-ubunu-12-04

// note:
// 4.8.2, 4.9.2, 4.9.4, all requires these:
// configure: error: Building GCC requires GMP 4.2+, MPFR 2.4.0+ and MPC 0.8.0+.
//
// However, depending on where you builds gcc, this passes without error and
// gets built. eg. 4.8.2 is configured on build server but not on debian VM.
//
// regardless of using:
// --with-gmp=/usr/local/lib --with-mpc=/usr/lib --with-mpfr=/usr/lib \
//
// // from configure.ac
// # The library versions listed in the error message below should match
// # the HARD-minimums enforced above.
//   if test x$have_gmp != xyes; then
//     AC_MSG_ERROR([Building GCC requires GMP 4.2+, MPFR 2.4.0+ and MPC 0.8.0+.
// Try the --with-gmp, --with-mpfr and/or --with-mpc options to specify
// their locations.  Source code for these libraries can be found at
// their respective hosting sites as well as at
// ftp://gcc.gnu.org/pub/gcc/infrastructure/.  See also
// http://gcc.gnu.org/install/prerequisites.html for additional info.  If
// you obtained GMP, MPFR and/or MPC from a vendor distribution package,
// make sure that you have installed both the libraries and the header
// files.  They may be located in separate packages.])
//   fi
// fi
//
// GMP is a free library for arbitrary precision arithmetic, operating on signed
// integers, rational numbers, and floating-point numbers. There is no practical
// limit to the precision except the ones implied by the available memory in the
// machine GMP runs on. GMP has a rich set of functions, and the functions have a
// regular interface.  
// 
// The main target applications for GMP are cryptography applications and
// research, Internet security applications, algebra systems, computational
// algebra research, etc. 
// 
// The MPFR library is a C library for multiple-precision floating-point
// computations with correct rounding. MPFR has continuously been supported by
// the INRIA and the current main authors come from the Caramba and AriC
// project-teams at Loria (Nancy, France) and LIP (Lyon, France) respectively;
// see more on the credit page. MPFR is based on the GMP multiple-precision
// library.
// 
// Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily
// high precision and correct rounding of the result. It extends the principles
// of the IEEE-754 standard for fixed precision real floating point numbers to
// complex numbers, providing well-defined semantics for every operation. At the
// same time, speed of operation at high precision is a major design goal. 
// 
// sudo apt-get install libgmp3-dev
// sudo apt-get install libmpfr-dev
// sudo apt-get install libmpc-dev

mkdir from-gnu
wget ftp://ftp.gnu.org/gnu/gcc/gcc-4.9.2/gcc-4.9.2.tar.bz2
tar xjvf gcc-4.9.2.tar.bz2 
mkdir build

// ../gcc-4.8.2/configure --disable-checking --enable-languages=c,c++ \
// --enable-multiarch --enable-shared --enable-threads=posix \
// --enable-libstdcxx-debug \
// --program-suffix=4.8.2 \
// --with-gmp=/usr/local/lib --with-mpc=/usr/lib --with-mpfr=/usr/lib \
// --without-included-gettext --with-system-zlib --with-tune=generic \
// --prefix=$HOME/gcc-build/install

// from ASAN how to build gcc
../gcc-4.8.2/configure --prefix=$HOME/toolchains --enable-languages=c,c++

make -j8
make install

$ cd /tmp
$ cat > test.c
int main() { return 42; }
$ $HOME/install/gcc-4.8.2/bin/gcc4.8 test.c
$ ./a.out; echo $?
42


<2> *gcc-build-mips*
note:
make fails for both 4.9.4 and 4.8.2 as well.

http://llvm.org/docs/GettingStarted.html#getting-a-modern-host-c-toolchain

wget https://ftp.gnu.org/gnu/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2

// wget https://ftp.gnu.org/gnu/gcc/gcc-4.8.2/gcc-4.8.2.tar.bz2.sig
// wget https://ftp.gnu.org/gnu/gnu-keyring.gpg
// signature_invalid=`gpg --verify --no-default-keyring --keyring ./gnu-keyring.gpg gcc-4.8.2.tar.bz2.sig`
// if [ $signature_invalid ]; then echo "Invalid signature" ; exit 1 ; fi

tar -xvjf gcc-4.8.2.tar.bz2
cd gcc-4.8.2

// downloads required packages
./contrib/download_prerequisites

cd ..
mkdir gcc-4.8.2-build
cd gcc-4.8.2-build
../gcc-4.8.2/configure --prefix=$HOME/gcc-bin --enable-languages=c,c++ --target=mips --program-suffix=4.9.4

make -j$(nproc)

checking for suffix of object files... configure: error: in `/home/NDS-UK/kyoupark/asn/gcc/gcc-4.9.2-build/mips-unknown-linux-gnu/libgcc':
configure: error: cannot compute suffix of object files: cannot compile
See `config.log' for more details.
make[1]: *** [configure-target-libgcc] Error 1
make[1]: Leaving directory `/home/NDS-UK/kyoupark/asn/gcc/gcc-4.9.2-build'
make: *** [all] Error 2

For more details, check out the excellent GCC wiki entry, where I got most of
this information from. (http://gcc.gnu.org/wiki/InstallingGCC)


<3>
https://www.linux-mips.org/wiki/Toolchains
If you're building for big-endian MIPS, your TARGET should be
mips-unknown-linux-gnu instead.

Have tried this target but failes on the same.


={============================================================================
*kt_linux_gcc_400* gcc-build-buildroot

https://www.uclibc.org/toolchains.html
https://buildroot.org/download.html

kyoupark@ukstbuild2:~/asn/broot-latest/buildroot-2016.08.1$ pwd
/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1

<wget-setting>
Connecting to cdn.kernel.org|151.101.36.69|:443... connected.
ERROR: certificate common name `k.ssl.fastly.net' doesn't match requested host name `cdn.kernel.org'.
To connect to cdn.kernel.org insecurely, use `--no-check-certificate'.

How to set?

You can add the necessary option for wget in the configuration.

Build options -> Commands -> Wget command.

Add what's recommended "--no-check-certificate" to the options already present
there. That will make it ignore all self-signed certificates.  The problem is
alioth.debian.org using a self-signed certificate instead of a proper one.
Regards.


{menuconfig}
this requires to have `apt-get install libncurses5-dev` if not installed.
saves configuration to: broot-latest/buildroot-2016.08.1/.config

make menuconfig


<build-output>
Buildroot output is stored in a single directory, output/. This directory
contains several subdirectories:

build/ 
where all the components are built (this includes tools needed by Buildroot on
    the host and packages compiled for the target). This directory contains
one subdirectory for each of these components.  

host/ 
contains the installation of tools compiled for the host that are needed for
the proper execution of Buildroot, including the cross-compilation toolchain.


note: host or target?
Host system differs from what is used by GNU configure, where the host is the
machine on which the application will run (which is usually the same as
target)


{toolchain-selection}
6.1.1 Internal toolchain backend

Once you have selected this backend, a number of options appear. The most
important ones allow to:

Change the version of the Linux kernel headers used to build the toolchain.
This item deserves a few explanations. In the process of building a
cross-compilation toolchain, `the C library is being built` 

This library provides the `interface` between userspace applications and the
Linux kernel. In order to know how to "talk" to the Linux kernel, the C
library needs to have access to the Linux kernel headers (i.e. the .h files
    from the kernel), which define the interface between userspace and the
kernel (system calls, data structures, etc.). 

Since this interface is `backward compatible`, the version of the Linux kernel
headers used to build your toolchain `do not need to match exactly`  the version
of the Linux kernel you intend to run on your embedded system. 

They only need to have a version `equal or older` to the version of the Linux
kernel you intend to run. If you use kernel headers that are more recent than
the Linux kernel you run on your embedded system, then the C library might be
using interfaces that are not provided by your Linux kernel.

It is worth noting that whenever one of those options is modified, then the
entire toolchain and system must be rebuilt. See Section 8.2.

Drawbacks of this backend:

Rebuilding the toolchain is needed when doing make clean, which takes time. If
you're trying to reduce your build time, consider using the External toolchain
backend.


// {attempt-to-use-old-linux}
// 
// <error>
// >>> linux-headers 2.6.18.8 Downloading
// --2016-09-23 14:27:50--  https://cdn.kernel.org/pub/linux/kernel/v2.6/linux-2.6.18.8.tar.xz
// Resolving cdn.kernel.org... 151.101.36.69
// 
// Build options -> Mirrors and Download locations 
// use https://cdn.kernel.org/pub
// 
// <error>
// // fixed by changing makefile
// // output/build/linux-headers-2.6.18/Makefile
// //
// // @896
// // headers_install: include/linux/version.h
// // #	$(Q)unifdef -Ux /dev/null
// 
// (cd /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18; PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" /usr/bin/make -j25 ARCH=mips HOSTCC="/usr/lib64/ccache/gcc" HOSTCFLAGS="" HOSTCXX="/usr/lib64/ccache/g++" INSTALL_HDR_PATH=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18/usr headers_install)
// make[1]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18'
//   CHK     include/linux/version.h
//   UPD     include/linux/version.h
// make[1]: unifdef: Command not found
// make[1]: *** [headers_install] Error 127
// make[1]: Leaving directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18'
// make: *** [/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18/.stamp_configured] Error 2
// 
// 
// <error>
// // fixed by changing version via menuconfig
// if ! support/scripts/check-kernel-headers.sh  /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/sysroot  4.7; then exit 1; fi
// Incorrect selection of kernel headers: expected 4.7.x, got 2.6.x
// make: *** [/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18/.stamp_staging_installed] Error 1
// 
// <error>
// >>> uclibc 1.0.17 Building
// /usr/bin/make -j25 -C /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/uclibc-1.0.17 \
//  ARCH="mips" \
//  CROSS_COMPILE="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2015.08.1/output/host/usr/bin/mips-buildroot-linux-uclibc-" UCLIBC_EXTRA_CFLAGS=" " \ 
//  HOSTCC="/usr/lib64/ccache/gcc" headers
// make[1]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/uclibc-1.0.17'
//   MKDIR libm/mips
//   MKDIR include/bits
//   MKDIR libubacktrace/mips
//   GEN include/bits/uClibc_config.h
//   LN include/semaphore.h
//   LN include/pthread.h
//   LN include/bits/libc-lock.h
//   LN include/bits/stdio-lock.h
//   LN include/bits/pthreadtypes.h
//   LN include/bits/semaphore.h
//   LN include/sgidefs.h
//   LN include/regdef.h
//   GEN include/bits/sysnum.h
// ERROR: Could not generate syscalls.
// Make sure that you have properly installed kernel headers.
// Your .config KERNEL_HEADERS="" was set to:
// /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18/usr/include/
// make[2]: *** [include/bits/sysnum.h] Error 1
// make[1]: *** [include/bits/uClibc_config.h] Error 2
// make[1]: Leaving directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/uclibc-1.0.17'
// make: *** [/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/uclibc-1.0.17/.stamp_built] Error 2
// 
// 
// errors from:
// 
// /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/uclibc-1.0.17/Makefile.in
// 
// 	$(Q)if grep -q __NR_ $@; then true; else \
// 		rm -f $@; \
// 		echo "ERROR: Could not generate syscalls."; \
// 		echo "Make sure that you have proper kernel headers."; \
// 		echo "Your .config in KERNEL_HEADERS=\"\" was set to:"; \
// 		echo "${KERNEL_HEADERS}"; \
// 		exit 1; \
// 	fi
// 
// 
// How to fix? BCM uses 0.9.29 from uclibc but not uclibc-ng and also uclibc-ng
// starts from 1.0.0.
// 
// kyoupark@ukstbuild2:~/asn/broot-latest/buildroot-2016.08.1/package/uclibc$ more uclibc.hash
// # From http://www.uclibc-ng.org/
// sha256  a2e7207634c19997e8b9f3e712182d80d42aaa85ce3462eff1a9bce812aaf354        uClibc-ng-1.0.17.tar.xz
// 
// How about 1.0.0 from uclib-ng since to avoid to know how to build uclibc.
// 
// $ cp -r uclibc/ uclibc-org
// $ change uclibc.hash to use 1.0.0
// $ change uclibc.mk
//   UCLIBC_VERSION = 1.0.0
//   UCLIBC_SOURCE = uClibc-ng-$(UCLIBC_VERSION).tar.xz
// $ delete all patches
// $ make clean
// $ make
// 
// SAME ERROR.
// 
// 
// HOW about uclibc?
// 
// UCLIBC_VERSION = 0.9.30.1
// UCLIBC_SOURCE = uClibc-$(UCLIBC_VERSION).tar.bz2
// UCLIBC_SITE = https://www.uclibc.org/downloads
// UCLIBC_LICENSE = LGPLv2.1+
// UCLIBC_LICENSE_FILES = COPYING.LIB
// UCLIBC_INSTALL_STAGING = YES
// 
// ERROR: Could not generate syscalls.
// Make sure that you have proper kernel headers.
// Your .config in KERNEL_HEADERS="" was set to:
// /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/linux-headers-2.6.18/usr/include/


{make-help}
8.1 make tips

This is a collection of tips that help you make the most of Buildroot.

Display all commands executed by make:
$ make V=1 <target>

Display the list of boards with a defconfig:
$ make list-defconfigs

Display `all available targets`:
$ make help

kyoupark@ukstbuild2:~/asn/broot-latest/buildroot-2016.08.1$ make help
Cleaning:
  clean                  - delete all files created by build
  distclean              - delete all non-source files (including .config)

Build:
  all                    - make world
  toolchain              - build toolchain

Configuration:
  menuconfig             - interactive curses-based configurator
  nconfig                - interactive ncurses-based configurator
  xconfig                - interactive Qt-based configurator
  gconfig                - interactive GTK-based configurator
  oldconfig              - resolve any unresolved symbols in .config
  silentoldconfig        - Same as oldconfig, but quietly, additionally update deps
  olddefconfig           - Same as silentoldconfig but sets new symbols to their default value
  randconfig             - New config with random answer to all options
  defconfig              - New config with default answer to all options
                             BR2_DEFCONFIG, if set, is used as input
  savedefconfig          - Save current config to BR2_DEFCONFIG (minimal config)
  allyesconfig           - New config where all options are accepted with yes
  allnoconfig            - New config where all options are answered with no
  randpackageconfig      - New config with random answer to package options
  allyespackageconfig    - New config where pkg options are accepted with yes
  allnopackageconfig     - New config where package options are answered with no

Package-specific:
  <pkg>                  - Build and install <pkg> and all its dependencies
  <pkg>-source           - Only download the source files for <pkg>
  <pkg>-extract          - Extract <pkg> sources
  <pkg>-patch            - Apply patches to <pkg>
  <pkg>-depends          - Build <pkg>'s dependencies
  <pkg>-configure        - Build <pkg> up to the configure step
  <pkg>-build            - Build <pkg> up to the build step
  <pkg>-graph-depends    - Generate a graph of <pkg>'s dependencies
  <pkg>-dirclean         - Remove <pkg> build directory
  <pkg>-reconfigure      - Restart the build from the configure step
  <pkg>-rebuild          - Restart the build from the build step

busybox:
  busybox-menuconfig     - Run BusyBox menuconfig

uclibc:
  `uclibc-menuconfig`      - Run uClibc menuconfig

Documentation:
  manual                 - build manual in all formats
  manual-html            - build manual in HTML
  manual-split-html      - build manual in split HTML
  manual-pdf             - build manual in PDF
  manual-text            - build manual in text
  manual-epub            - build manual in ePub
  graph-build            - generate graphs of the build times
  graph-depends          - generate graph of the dependency tree
  graph-size             - generate stats of the filesystem size
  list-defconfigs        - list all defconfigs (pre-configured minimal systems)

Miscellaneous:
  source                 - download all sources needed for offline-build
  source-check           - check selected packages for valid download URLs
  external-deps          - list external packages used
  legal-info             - generate info about license compliance

  make V=0|1             - 0 => quiet build (default), 1 => verbose build
  make O=dir             - Locate all output files in "dir", including .config

For further details, see README, generate the Buildroot manual, or consult
it on-line at http://buildroot.org/docs.html


{rebuild}

8.2 Understanding when a full rebuild is necessary

Buildroot does not attempt to detect what parts of the system should be
rebuilt when the system configuration is changed through make menuconfig, make
xconfig or one of the other configuration tools. In some cases, Buildroot
should rebuild the entire system, in some cases, only a specific subset of
packages. But detecting this in a completely reliable manner is very
difficult, and therefore the Buildroot developers have decided to simply not
attempt to do this.  

Instead, it is the responsibility of the user to know when a full rebuild is
necessary. As a hint, here are a few rules of thumb that can help you
understand how to work with Buildroot:


* When the toolchain configuration is changed, `a complete rebuild` generally is
  needed. Changing the toolchain configuration often involves changing the
  compiler version, the type of C library or its configuration, or some other
  fundamental configuration item, and these changes have an impact on the
  entire system.

Generally speaking, when you’re facing a build error and you’re unsure of the
potential consequences of the configuration changes you’ve made, do a full
rebuild.


<full-rebuild>
full rebuild is achieved by running:
$ make V=1 clean all


8.3 Understanding how to rebuild packages

The easiest way to rebuild a single package from scratch is to remove its
build directory in `output/build` Buildroot will then re-extract,
      re-configure, re-compile and re-install this package from scratch. You
      can ask buildroot to do this with the make `<package>-dirclean` command.

On the other hand, if you only want to restart the build process of a package
`from its compilation step`, you can run make `<package>-rebuild`, followed by
`make or make <package>` It will restart the compilation and installation of
the package, but not from scratch: it basically re-executes make and make
install inside the package, so it will only rebuild files that changed.

If you want to restart the build of a package `from its configuration step`,
   you can run `make <package>-reconfigure`, followed by make or make
   <package>. It will restart the configuration, compilation and installation
   of the package.

Internally, Buildroot creates so-called stamp files to keep track of which
build steps have been completed for each package. They are stored in the
package build directory, output/build/<package>-<version>/ and are named
.stamp_<stepname>.  The commands detailed above simply manipulate these stamp
files to force Buildroot to restart a specific set of steps of a package build
process.

Further details about package special make targets are explained in Section
8.12.5.


8.12.4 Location of downloaded packages

The various tarballs that are downloaded by Buildroot are all stored in
BR2_DL_DIR, which by default is the dl directory. If you want to keep a
complete version of Buildroot which is known to be working with the associated
tarballs, you can make a copy of this directory. This will allow you to
regenerate the toolchain and the target filesystem with exactly the same
versions.

The following line should be added to <~/.bashrc>.

export BR2_DL_DIR=<shared download location>

The download location can also be set in the .config file, with the BR2_DL_DIR
option. Unlike most options in the .config file, this value is overridden by
the BR2_DL_DIR environment variable.


{rebuild-package}

8.12.6 Using Buildroot during development

The normal operation of Buildroot is to download a tarball, extract it,
    configure, compile and install the software component found inside this
    tarball. 

The source code is extracted in output/build/<package>-<version>, which is a
temporary directory: whenever make clean is used, this directory is entirely
removed, and re-created at the next make invocation.

Even when a Git or Subversion repository is used as the input for the package
source code, Buildroot creates a tarball out of it, and then behaves as it
normally does with tarballs.

This behavior is well-suited when Buildroot is used mainly as an integration
tool, to build and integrate all the components of an embedded Linux system. 

However, if one uses Buildroot during the development of certain components of
the system, this behavior is not very convenient: 

  one would instead like to make a small change to the source code of one
  package, and be able to quickly rebuild the system with Buildroot.

Making changes directly in output/build/<package>-<version> `is not` an
appropriate solution, because this directory is removed on make clean.

Therefore, Buildroot provides a specific mechanism for this use case: the
<pkg>_OVERRIDE_SRCDIR mechanism. Buildroot reads an override file, which
allows the user to tell Buildroot the location of the source for certain
packages. By default this override file is named `local.mk` and located in the
top directory of the Buildroot source tree, but a different location can be
specified through the BR2_PACKAGE_OVERRIDE_FILE configuration option.


In this override file, Buildroot expects to find lines of the form:

<pkg1>_OVERRIDE_SRCDIR = /path/to/pkg1/sources
<pkg2>_OVERRIDE_SRCDIR = /path/to/pkg2/sources

For example:

LINUX_OVERRIDE_SRCDIR = /home/bob/linux/
BUSYBOX_OVERRIDE_SRCDIR = /home/bob/busybox/

When Buildroot finds that for a given package, an <pkg>_OVERRIDE_SRCDIR has
been defined, it will no longer attempt to download, extract and patch the
package. Instead, it will directly use the source code available in in the
specified directory and make clean will not touch this directory. This allows
to point Buildroot to your own directories, that can be managed by Git,
   Subversion, or any other version control system. To achieve this, Buildroot
   will use rsync to copy the source code of the component from the specified
   <pkg>_OVERRIDE_SRCDIR to output/build/<package>-custom/.

This mechanism is best used in conjunction with the make <pkg>-rebuild and
make <pkg>-reconfigure targets.

A make <pkg>-rebuild all sequence will rsync the source code from
<pkg>_OVERRIDE_SRCDIR to output/ build/<package>-custom (thanks to rsync, only
    the modified files are copied), and restart the build process of just this
package.

In the example of the linux package above, the developer can then make a
source code change in /home/bob/linux and then run:

make linux-rebuild all

and in a matter of seconds gets the updated Linux kernel image in
output/images. Similarly, a change can be made to the BusyBox source code in
/home/bob/busybox, and after:

make busybox-rebuild all

the root filesystem image in output/images contains the updated BusyBox.

<gcc-package-build>
why twice?

// not works
// broot-latest/buildroot-2016.08.1/package/gcc
// # gcc is a special package, not named gcc, but gcc-initial and
// # gcc-final, but patches are nonetheless stored in package/gcc in the
// # tree, and potentially in BR2_GLOBAL_PATCH_DIR directories as well.
// 
// works
// $ make V=1 host-gcc-initial-reconfigure

<initial>
>>> host-gcc-initial 4.9.4 Configuring
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build

note: where cwd is then?
ln -sf ../configure /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build/configure
(cd /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build && rm -rf config.cache; PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CXXFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib/pkgconfig:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CONFIG_SITE=/dev/null ./configure --prefix="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr" --sysconfdir="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/etc" --localstatedir="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/var" --enable-shared --disable-static --disable-gtk-doc --disable-gtk-doc-html --disable-doc --disable-docs --disable-documentation --disable-debug --with-xmlto=no --with-fop=no --disable-dependency-tracking  --target=mips-buildroot-linux-uclibc --with-sysroot=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/sysroot --disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib --with-gmp=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --with-mpfr=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --with-pkgversion="Buildroot 2016.08.1" --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath --disable-libsanitizer --enable-tls --disable-libmudflap --enable-threads --with-mpc=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --without-isl --without-cloog --with-float=soft --disable-decimal-float --with-arch="mips32" --with-abi="32" --enable-languages=c --disable-shared --without-headers --disable-threads --with-newlib --disable-largefile --disable-nls  )

configure: loading site script /dev/null
checking build system type... x86_64-pc-linux-gnu

>>> host-gcc-initial 4.9.4 Building
PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" PKG_CONFIG="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib/pkgconfig"  /usr/bin/make -j25 gcc_cv_libc_provides_ssp=no all-gcc all-target-libgcc -C /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build
make[1]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-initial-4.9.4/build'
mkdir -p -- ./libiberty


<final>
for p in   "http://ftpmirror.gnu.org"/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2; do \
		if test ! -e /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/`basename $p` ; then \
			echo ">>> host-gcc-final 4.9.4 Downloading" ; \
			break ; \
		fi ; \
	done
if test -n "" ; then case "" in file) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b cp -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- /gcc-4.9.4.tar.bz2 && exit ;; scp) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b scp -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- '/gcc-4.9.4.tar.bz2' && exit ;; *) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b wget -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- '/gcc-4.9.4.tar.bz2' && exit ;; esac ; fi ; if test "" = "y" ; then exit 1 ; fi ; if test -n ""http://ftpmirror.gnu.org"/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2" ; then case "http" in git) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b git -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2   -- "http://ftpmirror.gnu.org"/gcc/gcc-4.9.4 4.9.4 host-gcc-final-4.9.4 && exit ;; svn) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b svn -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2  -- "http://ftpmirror.gnu.org"/gcc/gcc-4.9.4 4.9.4 host-gcc-final-4.9.4 && exit ;; cvs) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b cvs -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2  -- ftpmirror.gnu.org/gcc/gcc-4.9.4 4.9.4 gcc-final host-gcc-final-4.9.4 && exit ;; bzr) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b bzr -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2  -- "http://ftpmirror.gnu.org"/gcc/gcc-4.9.4 4.9.4 host-gcc-final-4.9.4 && exit ;; file) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b cp -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- ftpmirror.gnu.org/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2 && exit ;; scp) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b scp -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- 'ftpmirror.gnu.org/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2' && exit ;; hg) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b hg -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2  -- "http://ftpmirror.gnu.org"/gcc/gcc-4.9.4 4.9.4 host-gcc-final-4.9.4 && exit ;; *) 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b wget -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- 'http://ftpmirror.gnu.org/gcc/gcc-4.9.4/gcc-4.9.4.tar.bz2' && exit ;; esac ; fi ; if test -n "http://sources.buildroot.net" ; then 	PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" BR2_DL_DIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl" BUILD_DIR=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build BR2_EXTERNAL=support/dummy-external support/download/dl-wrapper -b wget -o /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 -H package/gcc/gcc-final//gcc-final.hash  -- 'http://sources.buildroot.net/gcc-4.9.4.tar.bz2' && exit ; fi ; exit 1
gcc-4.9.4.tar.bz2: OK (sha512: 93abb78e16277454f41a8e9810f41f66c0fdffdc539a762ff6b67d3037f78db971378683fd2ebf707d1d51c059fad2161fe42d110c330027f40214b7db0f3efe)
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4
touch /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/.stamp_downloaded

>>> host-gcc-final 4.9.4 Extracting
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4
bzcat /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/dl/gcc-4.9.4.tar.bz2 | tar --strip-components=1 -C /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4 --exclude='libjava/*'  --exclude='libgo/*'  --exclude='gcc/testsuite/*'  --exclude='libstdc++-v3/testsuite/*'   -xf -
chmod -R +rw /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/libstdc++-v3/testsuite/
echo "all:" > /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/libstdc++-v3/testsuite/Makefile.in
echo "install:" >> /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/libstdc++-v3/testsuite/Makefile.in
touch /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/.stamp_extracted

>>> host-gcc-final 4.9.4 Configuring
mkdir -p /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build
ln -sf ../configure /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build/configure
(cd /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build && rm -rf config.cache; PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" AR="/usr/bin/ar" AS="/usr/bin/as" LD="/usr/bin/ld" NM="/usr/bin/nm" CC="/usr/lib64/ccache/gcc" GCC="/usr/lib64/ccache/gcc" CXX="/usr/lib64/ccache/g++" CPP="/usr/bin/cpp" OBJCOPY="/usr/bin/objcopy" RANLIB="/usr/bin/ranlib" CPPFLAGS="-I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" CXXFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 PKG_CONFIG="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib/pkgconfig:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/share/pkgconfig" INTLTOOL_PERL=/usr/bin/perl CFLAGS="-O2 -I/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/include" LDFLAGS="-L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/lib -L/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib -Wl,-rpath,/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib" MAKEINFO=missing CFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " CXXFLAGS_FOR_TARGET="-D_LARGEFILE_SOURCE -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64  -Os " 
./configure --prefix="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr" 
--sysconfdir="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/etc" --enable-static  
--target=mips-buildroot-linux-uclibc 
--with-sysroot=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/sysroot --disable-__cxa_atexit --with-gnu-ld --disable-libssp --disable-multilib --with-gmp=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --with-mpfr=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --with-pkgversion="Buildroot 2016.08.1" --with-bugurl="http://bugs.buildroot.net/" --disable-libquadmath 
--disable-libsanitizer --enable-tls --disable-libmudflap --enable-threads --with-mpc=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr --without-isl --without-cloog 
--with-float=soft --disable-decimal-float 
--with-arch="mips32" --with-abi="32" 
--enable-languages=c --with-build-time-tools=/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/mips-buildroot-linux-uclibc/bin --enable-shared --disable-libgomp  )
checking build system type... x86_64-pc-linux-gnu

>>> host-gcc-final 4.9.4 Building
PATH="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/sbin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin:/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/sbin:/home/NDS-UK/kyoupark/inst/bin:/home/NDS-UK/kyoupark/viminst/bin:/home/NDS-UK/kyoupark/github-bin:/shared/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/kerberos/bin:/opt/ecloud/i686_Linux/64/bin:/opt/rational/clearcase/bin:/usr/lib64/ccache:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/coverity:/home/NDS-UK/kyoupark/bin" PKG_CONFIG="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/bin/pkg-config" PKG_CONFIG_SYSROOT_DIR="/" PKG_CONFIG_LIBDIR="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/host/usr/lib/pkgconfig"  /usr/bin/make -j25 gcc_cv_libc_provides_ssp=no -C /home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build
make[1]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build'
make[2]: Entering directory `/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1/output/build/host-gcc-final-4.9.4/build'


<arch-abi>
--with-float=soft --disable-decimal-float 
--with-arch="mips32" --with-abi="32" 


// makefile (from `toolchain/toolchain-external/toolchain-external.mk', line 247)
TOOLCHAIN_EXTERNAL_CFLAGS = -march=$(CC_TARGET_ARCH_) -mabi=$(CC_TARGET_ABI_) -EB $(call qstrip,$(BR2_TARGET_OPTIMIZATION)) -msoft-float


BR2_GCC_TARGET_ARCH="mips32"
BR2_GCC_TARGET_ABI="32"


// buildroot-2016.08.1-mips-only/toolchain/toolchain-external/toolchain-external.mk
ifeq ($(call qstrip,$(BR2_GCC_TARGET_CPU_REVISION)),)
CC_TARGET_CPU_ := $(call qstrip,$(BR2_GCC_TARGET_CPU))
else
CC_TARGET_CPU_ := $(call qstrip,$(BR2_GCC_TARGET_CPU)-$(BR2_GCC_TARGET_CPU_REVISION))
endif

CC_TARGET_ARCH_ := $(call qstrip,$(BR2_GCC_TARGET_ARCH))
CC_TARGET_ABI_ := $(call qstrip,$(BR2_GCC_TARGET_ABI))
CC_TARGET_FPU_ := $(call qstrip,$(BR2_GCC_TARGET_FPU))
CC_TARGET_FLOAT_ABI_ := $(call qstrip,$(BR2_GCC_TARGET_FLOAT_ABI))
CC_TARGET_MODE_ := $(call qstrip,$(BR2_GCC_TARGET_MODE))

# march/mtune/floating point mode needs to be passed to the external toolchain
# to select the right multilib variant
ifneq ($(CC_TARGET_ARCH_),)
TOOLCHAIN_EXTERNAL_CFLAGS += -march=$(CC_TARGET_ARCH_)
TOOLCHAIN_EXTERNAL_TOOLCHAIN_WRAPPER_ARGS += -DBR_ARCH='"$(CC_TARGET_ARCH_)"'
endif
ifneq ($(CC_TARGET_CPU_),)
TOOLCHAIN_EXTERNAL_CFLAGS += -mcpu=$(CC_TARGET_CPU_)
TOOLCHAIN_EXTERNAL_TOOLCHAIN_WRAPPER_ARGS += -DBR_CPU='"$(CC_TARGET_CPU_)"'
endif
ifneq ($(CC_TARGET_ABI_),)
TOOLCHAIN_EXTERNAL_CFLAGS += -mabi=$(CC_TARGET_ABI_)
TOOLCHAIN_EXTERNAL_TOOLCHAIN_WRAPPER_ARGS += -DBR_ABI='"$(CC_TARGET_ABI_)"'
endif


={============================================================================
*kt_linux_gcc_400* gcc-build-old-kernel

1. use a specific(old) kernel version
wget --no-check-certificate https://www.kernel.org/pub/linux/kernel/v2.6/linux-2.6.19.tar.xz

2. 
INSTALL_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2-own

3. 
export PATH=$INSTALL_PATH/bin:$PATH

4.
../binutils-2.24/configure --target=mips-linux --prefix=${INSTALL_PATH} --with-sysroot --disable-nls --disable-multilib
make -j4
make install

5.
cd linux-2.6.19
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-linux headers_install

kyoupark@ukstbuild2:~/asn/gcc/linux-2.6.19$ make ARCH=mips INSTALL_HDR_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2-own/mips-linux headers_install
  CHK     include/linux/version.h
  UPD     include/linux/version.h
  HOSTCC  scripts/basic/fixdep
scripts/basic/fixdep.c: In function ‘traps’:
scripts/basic/fixdep.c:371: warning: dereferencing type-punned pointer will break strict-aliasing rules
scripts/basic/fixdep.c:373: warning: dereferencing type-punned pointer will break strict-aliasing rules
  HOSTCC  scripts/basic/docproc
  HOSTCC  scripts/unifdef
scripts/unifdef.c:209: error: conflicting types for ‘getline’
/usr/include/stdio.h:673: note: previous declaration of ‘getline’ was here
make[1]: *** [scripts/unifdef] Error 1
make: *** [headers_install] Error 2

<linux-kernel-header-fix>
To fix, chage getline to parseline in scripts/unifdef.c

6.
../gcc-4.8.2-clean/configure --prefix=${INSTALL_PATH} --target=mips-linux --enable-languages=c,c++ --disable-multilib --disable-nls
../gcc-4.8.2-clean/configure --prefix=${INSTALL_PATH} --target=mips-linux --enable-languages=c,c++ --disable-multilib --disable-nls --without-headers --with-newlib
make -j4 all-gcc
make install-gcc

7.
../glibc-2.20/configure --prefix=${INSTALL_PATH}/mips-linux \
--build=$MACHTYPE --host=mips-linux --target=mips-linux \
--with-headers=${INSTALL_PATH}/mips-linux/include \
--disable-multilib libc_cv_forced_unwind=yes

make install-bootstrap-headers=yes install-headers
make -j4 csu/subdir_lib

configure: error: GNU libc requires kernel header files from
Linux 2.6.32 or later to be installed before configuring.



={============================================================================
*kt_linux_gcc_400* gcc-build-options

https://gcc.gnu.org/install/configure.html

To get the debug library
https://gcc.gnu.org/onlinedocs/libstdc++/manual/debug.html

--enable-libstdcxx-debug

This sets the binary name such as $install/bin/gcc4.9.2
--program-suffix=4.9.2 \

This sets the installation directory
--prefix=$HOME/gcc-build/install


={============================================================================
*kt_linux_gcc_400* gcc-build-reference

http://preshing.com/20141119/how-to-build-a-gcc-cross-compiler/

note:
4.9.2 builds with exact version and settings in the blog.
4.8.2 builds with no changes on prerequsites.

Required Packages
=================

Starting with a clean Debian system, you must first install a few packages:

$ sudo apt-get install g++ make gawk

Everything else will be built from source. Create a new directory somewhere,
and download the following source packages. (If you’re following this guide at
    a later date, there will be more recent releases of each package
    available. Check for newer releases by pasting each URL into your browser
    without the filename. For example: http://ftpmirror.gnu.org/binutils/)

wget http://ftpmirror.gnu.org/binutils/binutils-2.24.tar.gz
$ wget http://ftpmirror.gnu.org/gcc/gcc-4.9.2/gcc-4.9.2.tar.gz

wget --no-check-certificate https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.17.2.tar.xz
wget --no-check-certificate http://ftpmirror.gnu.org/glibc/glibc-2.20.tar.xz
wget --no-check-certificate http://ftpmirror.gnu.org/mpfr/mpfr-3.1.2.tar.xz
wget --no-check-certificate http://ftpmirror.gnu.org/gmp/gmp-6.0.0a.tar.xz
wget --no-check-certificate http://ftpmirror.gnu.org/mpc/mpc-1.0.2.tar.gz

wget --no-check-certificate ftp://gcc.gnu.org/pub/gcc/infrastructure/isl-0.12.2.tar.bz2
wget --no-check-certificate ftp://gcc.gnu.org/pub/gcc/infrastructure/cloog-0.18.1.tar.gz

The first four packages  Binutils, GCC, the Linux kernel and Glibc  are the
main ones. We could have installed the next three packages in binary form
using our system’s package manager instead, but that tends to provide older
versions. The last two packages, ISL and CLooG, are `optional`, but they enable
a few more optimizations in the compiler we’re about to build.


How The Pieces Fit Together
===========================
By the time we’re finished, we will have built each of the following programs
and libraries. First, we’ll build the tools on the left, then we’ll use those
tools to build the programs and libraries on the right. We won’t actually
build the target system’s Linux kernel, but we `do need the kernel header files`
in order to build the target system’s standard C library.


Host Programs                             
(Instruction set: x64)                    

C cross-compiler
aarch64-linux-gcc built from GCC

C++ cross-compiler
aarch64-linux-gcc built from GCC

cross-assember, cross-linker
aarch64-linux-ld built from binutils
aarch64-linux-as built from binutils

      ||
      \/

Target
instruction set: aarch64

+---------------------------------+
|     a.out                       |
|       +-------------------------+
|       | standard C++ library    |
|       | libstdc++.so from GCC   |
+-------+-------------------------+
|   standard C library            |
|   libc.so from GCC              |
+---------------------------------+
|   Linux kernel                  |
+---------------------------------+

The compilers on the left will invoke the assembler & linker as part of their
job. All the other packages we downloaded, such as MPFR, GMP and MPC, will be
linked into the compilers themselves.

The diagram on the right represents a sample program, a.out, running on the
target OS, built using the cross compiler and linked with the target system's
standard C and C++ libraries. The standard C++ library makes calls to the
standard C library, and the C library makes direct system calls to the AArch64
Linux kernel.

Note that instead of using Glibc as the standard C library implementation, we
could have used Newlib, an alternative implementation. `Newlib` is a popular C
library implementation for embedded devices. Unlike Glibc, Newlib doesn’t
require a complete OS on the target system  just a thin hardware abstraction
layer called Libgloss. Newlib doesn’t have regular releases; instead, you’re
meant to pull the source directly from the Newlib CVS repository. One
limitation of Newlib is that currently, it doesn’t seem to support building
multithreaded programs for AArch64. That’s why I chose not to use it here.


Build Steps
===============================
Extract all the source packages.

$ for f in *.tar*; do tar xf $f; done

Create symbolic links from the GCC directory to some of the other directories.
These five packages are dependencies of GCC, and when the symbolic links are
present, GCC's build script will build them automatically.

$ cd gcc-4.9.2
ln -s ../mpfr-3.1.2 mpfr
ln -s ../gmp-6.0.0 gmp
ln -s ../mpc-1.0.2 mpc
// $ ln -s ../isl-0.12.2 isl
// $ ln -s ../cloog-0.18.1 cloog
$ cd ..


note:
https://gcc.gnu.org/install/download.html

Downloading GCC

If you also intend to build binutils (either to upgrade an existing
    installation or for use in place of the corresponding tools of your OS),
unpack the binutils distribution either in the same directory or a separate
  one. In the latter case, add symbolic links to any components of the
  binutils you intend to build alongside the compiler (bfd, binutils, gas,
      gprof, ld, opcodes, ...) to the directory containing the GCC sources.

Likewise the GMP, MPFR and MPC libraries can be automatically built together
with GCC. You may simply run the contrib/download_prerequisites script in the
GCC source directory to set up everything. Otherwise unpack the GMP, MPFR
and/or MPC source distributions in the directory containing the GCC sources
and rename their directories to gmp, mpfr and mpc, respectively (or use
    symbolic links with the same name). 

tar -xvjf gcc-4.8.2.tar.bz2
cd gcc-4.8.2
./contrib/download_prerequisites

// this script will download and set up sym links
//
// for 4.8.2
// MPFR=mpfr-2.4.2
// GMP=gmp-4.3.2
// MPC=mpc-0.8.1
//
// for 4.9.2
// MPFR=mpfr-2.4.2
// GMP=gmp-4.3.2
// MPC=mpc-0.8.1

Choose an installation directory, and make sure you have write permission to
it. In the steps that follow, I'll install the new toolchain to /opt/cross.

$ sudo mkdir -p /opt/cross
$ sudo chown jeff /opt/cross


1. PATH
===========
Throughout the entire build process, make sure the installation's bin
subdirectory is in your PATH environment variable. You can remove this
directory from your PATH later, but most of the build steps expect to find
aarch64-linux-gcc and other host tools via the PATH by default.

$ export PATH=/opt/cross/bin:$PATH

export PATH=~/asn/install-4.9.2/bin:$PATH
export PATH=~/asn/install-4.8.2/bin:$PATH


1. SYSTEM ROOT
===========
Pay particular attention to the stuff that gets installed under
`/opt/cross/aarch64-linux/`. This directory is considered the system root of an
imaginary AArch64 Linux target system. A self-hosted AArch64 Linux compiler
could, in theory, use all the headers and libraries placed here. Obviously,
none of the programs built for the host system, such as the cross-compiler
itself, will be installed to this directory.


1. Binutils: ${PREFIX}/bin
===========
This step builds and installs the cross-assembler, cross-linker, and other tools.

$ mkdir build-binutils
$ cd build-binutils
$ ../binutils-2.24/configure --prefix=/opt/cross --target=aarch64-linux --disable-multilib
$ make -j4
$ make install
$ cd ..

We've specified aarch64-linux as the target system type. Binutils's configure
script will recognize that this target is different from the machine we're
building on, and configure a cross-assembler and cross-linker as a result. 

The tools will be installed to /opt/cross/bin, their names prefixed by
aarch64-linux-.  

--disable-multilib means that we only want our Binutils installation to work
with programs and libraries using the AArch64 instruction set, and not any
related instruction sets such as AArch32.

../binutils-2.24/configure --target=mips-linux --prefix=/home/nds-uk/kyoupark/asn/install-4.9.2 --disable-nls --disable-multilib
../binutils-2.24/configure --target=mips-linux --prefix=/home/nds-uk/kyoupark/asn/install-4.8.2 --with-sysroot --disable-nls --disable-multilib

// ../binutils-2.24/configure --target=mipsel-linux --prefix=/home/nds-uk/kyoupark/asn/install-4.9.2 --with-sysroot --disable-nls --disable-multilib
//
// kyoupark@ukstbuild2:~/asn/install-4.9.2$
// drwxr-xr-x  2 kyoupark ccusers 4096 Nov 24 09:53 bin/
// drwxr-xr-x  4 kyoupark ccusers 4096 Nov 24 09:53 mipsel-linux/
// drwxr-xr-x  4 kyoupark ccusers 4096 Nov 24 09:53 share/
// kyoupark@ukstbuild2:~/asn/install-4.9.2$ ls bin/
// mipsel-linux-addr2line  mipsel-linux-as       mipsel-linux-elfedit  mipsel-linux-ld      mipsel-linux-nm       mipsel-linux-objdump  mipsel-linux-readelf  mipsel-linux-strings
// mipsel-linux-ar         mipsel-linux-c++filt  mipsel-linux-gprof    mipsel-linux-ld.bfd  mipsel-linux-objcopy  mipsel-linux-ranlib   mipsel-linux-size     mipsel-linux-strip

// ../binutils-2.24/configure --target=mipsel-linux-gnu --prefix=/home/nds-uk/kyoupark/asn/install --with-sysroot --disable-nls --disable-werror
//
// kyoupark@ukstbuild2:~/asn/install$
// drwxr-xr-x  2 kyoupark ccusers 4096 Nov 24 08:07 bin/
// drwxr-xr-x  4 kyoupark ccusers 4096 Nov 24 08:07 mipsel-linux-gnu/
// drwxr-xr-x  4 kyoupark ccusers 4096 Nov 24 08:07 share/
// kyoupark@ukstbuild2:~/asn/install$ ls bin/
// mipsel-linux-gnu-addr2line  mipsel-linux-gnu-c++filt  mipsel-linux-gnu-ld      mipsel-linux-gnu-objcopy  mipsel-linux-gnu-readelf  mipsel-linux-gnu-strip
// mipsel-linux-gnu-ar         mipsel-linux-gnu-elfedit  mipsel-linux-gnu-ld.bfd  mipsel-linux-gnu-objdump  mipsel-linux-gnu-size
// mipsel-linux-gnu-as         mipsel-linux-gnu-gprof    mipsel-linux-gnu-nm      mipsel-linux-gnu-ranlib   mipsel-linux-gnu-strings


note: failed to build 2.24 but build 2.25.1 on debian VM.
kyoupark@kit-debian:~/si-logs/gcc/build-binutil$ uname -a
Linux kit-debian 3.16.0-4-686-pae #1 SMP Debian 3.16.7-ckt25-2+deb8u3 (2016-07-02) i686 GNU/Linux

kyoupark@kit-debian:~/si-logs/gcc/build-binutil$ gcc --version
gcc (Debian 4.9.2-10) 4.9.2
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

note:
http://wiki.osdev.org/GCC_Cross-Compiler

--disable-nls tells binutils not to include native language support. This is
basically optional, but reduces dependencies and compile time. It will also
result in English-language diagnostics, which the people on the Forum
understand when you ask your questions. ;-)


<sysroot>

https://gcc.gnu.org/onlinedocs/gcc/Directory-Options.html#Directory-Options

--sysroot=`dir`
    Use dir as the logical root directory `for headers and libraries` 
    
    For example, if the compiler normally searches for headers in /usr/include
    and libraries in /usr/lib, it instead searches `dir`/usr/include and
    `dir`/usr/lib.

    If you use both this option and the -isysroot option, then the --sysroot
    option applies to libraries, but the -isysroot option applies to header
    files.

    The GNU linker (beginning with version 2.16) has the necessary support for
    this option. If your linker does not support this option, the header file
    aspect of --sysroot still works, but the library aspect does not. 


https://gcc.gnu.org/install/configure.html

Cross-Compiler-Specific Options

The following options only apply to building cross compilers.

--with-sysroot
--with-sysroot=dir
    Tells GCC to consider dir as the root of a tree that contains (a subset
        of) the root filesystem of the `target operating system` 
    
    `Target system headers, libraries and run-time object files` will be
    searched for in there. More specifically, this acts as if --sysroot=dir
    was added to the `default options` of the built compiler. 
    
    The specified directory is not copied into the install tree, unlike the
    options --with-headers and --with-libs that this option obsoletes. 
    
    The default value, in case --with-sysroot is not given an argument, is
    ${gcc_tooldir}/sys-root. If the specified directory is a subdirectory of
    ${exec_prefix}, then it will be found relative to the GCC binaries if the
    installation tree is moved.

    This option affects the system root for the compiler used to build target
    libraries (which runs on the build system) and the compiler newly
    installed with make install; it does not affect the compiler which is used
    to build GCC itself.

    If you specify the --with-native-system-header-dir=dirname option then the
    compiler will search that directory within dirname for native system
    headers rather than the default /usr/include. 

eg.,

../binutils-2.24/configure --target=mips-linux --prefix=/xx --with-sysroot=/xx/sysroot --disable-nls --disable-multilib

note: will not create sysroot directory.

--with-sysroot tells binutils to enable sysroot support in the cross-compiler
by pointing it to a default empty directory. By default the linker refuses to
use sysroots for no good technical reason, while gcc is able to handle both
cases at runtime. This will be useful later on. 

--prefix=xx
|
├── bin
│   ├── mips-linux-addr2line
│   ├── mips-linux-ar
│   ├── mips-linux-as
│   ├── mips-linux-c++filt
│   ├── mips-linux-elfedit
│   ├── mips-linux-gprof
│   ├── mips-linux-ld
│   ├── mips-linux-ld.bfd
│   ├── mips-linux-nm
│   ├── mips-linux-objcopy
│   ├── mips-linux-objdump
│   ├── mips-linux-ranlib
│   ├── mips-linux-readelf
│   ├── mips-linux-size
│   ├── mips-linux-strings
│   └── mips-linux-strip
│
├── mips-unknown-linux-uclibc
│   ├── bin
│   │   ├── ar
│   │   ├── as
│   │   ├── ld
│   │   ├── ld.bfd
│   │   ├── nm
│   │   ├── objcopy
│   │   ├── objdump
│   │   ├── ranlib
│   │   └── strip
│   └── lib
│       └── ldscripts
└── share
    ├── info
    └── man


2. Linux Kernel Headers: ${prefix}/arch/include 
=======================
This step installs the Linux kernel header files to ${INSTALL_HDR_PATH}/include,
which will ultimately allow programs built using our new toolchain to make
  system calls to the AArch64 kernel in the target environment.

$ cd linux-3.17.2
$ make ARCH=arm64 INSTALL_HDR_PATH=`/opt/cross/aarch64-linux` headers_install
$ cd ..

    We could even have done this before installing Binutils.

    The Linux kernel header files won't actually be used until step 6, when we
    build the standard C library, although the configure script in step 4
    expects them to be already installed.  
    
    Because the Linux kernel is a different open-source project from the
    others, it has a `different way of identifying the target` CPU architecture:
    ARCH=arm64


make ARCH=mips INSTALL_HDR_PATH=/home/nds-uk/kyoupark/asn/install-4.9.2/mips-linux headers_install
make ARCH=mips INSTALL_HDR_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2/`mips-linux` headers_install

note:

home/nds-uk/kyoupark/asn/install-4.8.2/mips-linux
├── include
│   ├── asm
│   ├── asm-generic
│   ├── drm
│   ├── linux
│   ├── mtd
│   ├── rdma
│   ├── scsi
│   ├── sound
│   ├── uapi
│   ├── video
│   └── xen


All of the remaining steps involve building GCC and Glibc. The trick is that
there are parts of GCC which depend on parts of Glibc already being built, and
vice versa. We can't build either package in a single step; we need to go back
and forth between the two packages and build their components in a way that
satisfies their dependencies.


<build-order>
GCC components          Glibc componenets
C/C++ compilers
                    ->  standard C library headers and starup files
                        stdio.h,... crti.o/libc.so
Compiler support    <-
library
libgcc.a/libgcc.so  ->  standard C library
                        libc.a/libc.so
standard C++ 
library             <-
libstdc++.a/libstdc++.so 


3. C/C++ Compilers (pass 1): ${prefix}/bin
==================
This step will build `GCC's C and C++ cross-compilers only`, and install them to
/opt/cross/bin. It won't invoke those compilers to build any libraries just
yet.

$ mkdir -p build-gcc
$ cd build-gcc
$ ../gcc-4.9.2/configure --prefix=/opt/cross --target=aarch64-linux --enable-languages=c,c++ --disable-multilib
$ make -j4 all-gcc
$ make install-gcc
$ cd ..

../gcc-4.9.2/configure --prefix=/home/nds-uk/kyoupark/asn/install-4.9.2 --target=mips-linux --enable-languages=c,c++ --disable-multilib
../gcc-4.8.2/configure --prefix=/home/nds-uk/kyoupark/asn/install-4.8.2 --target=mips-linux --enable-languages=c,c++ --disable-multilib

Because we've specified --target=aarch64-linux, the build script looks for the
Binutils cross-tools we built in step 1 with names prefixed by aarch64-linux-.
Likewise, the C/C++ compiler names will be prefixed by aarch64-linux-.
--enable-languages=c,c++ prevents other compilers in the GCC suite, such as
Fortran, Go or Java, from being built.

// note:
// must use "all-gcc" since when runs "make", build fails on:
// 
// shows error:
// 
// checking linker for .eh_frame personality relaxation... no
// *** This configuration requires the GNU assembler
// make[1]: *** [configure-gcc] Error 1
// make[1]: Leaving directory `/home/NDS-UK/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu'
// 
// // gcc/configure.ac
// 
// # Mips and HP-UX need the GNU assembler.
// # Linux on IA64 might be able to use the Intel assembler.
// 
// case "$target" in
//   mips*-*-* | *-*-hpux* )
//     if test x$gas_flag = xyes \
//     else
//       echo "*** This configuration requires the GNU assembler" >&2
//       exit 1
//     fi
//     ;;
// esac
// 
// try to add "--with-gnu-as" since this will set $gas_flag:
// 
// Checking multilib configuration for libgcc...
// mkdir -p -- mipsel-linux-gnu/libgcc
// `Configuring in mipsel-linux-gnu/libgcc`
// configure: creating cache ./config.cache
// checking build system type... x86_64-unknown-linux-gnu
// checking host system type... mipsel-unknown-linux-gnu
// checking for --enable-version-specific-runtime-libs... no
// checking for a BSD-compatible install... /usr/bin/install -c
// checking for gawk... gawk
// checking for mipsel-linux-gnu-ar... mipsel-linux-gnu-ar
// checking for mipsel-linux-gnu-lipo... mipsel-linux-gnu-lipo
// checking for mipsel-linux-gnu-nm... /home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu/./gcc/nm
// checking for mipsel-linux-gnu-ranlib... mipsel-linux-gnu-ranlib
// checking for mipsel-linux-gnu-strip... mipsel-linux-gnu-strip
// checking whether ln -s works... yes
// checking for mipsel-linux-gnu-gcc... /home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu/./gcc/xgcc -B/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu/./gcc/ -B/home/nds-uk/kyoupark/gcc-bin/mipsel-linux-gnu/bin/ -B/home/nds-uk/kyoupark/gcc-bin/mipsel-linux-gnu/lib/ -isystem /home/nds-uk/kyoupark/gcc-bin/mipsel-linux-gnu/include -isystem /home/nds-uk/kyoupark/gcc-bin/mipsel-linux-gnu/sys-include
// checking for suffix of object files... configure: error: in `/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu/mipsel-linux-gnu/libgcc':
// configure: error: cannot compute suffix of object files: cannot compile
// See `config.log' for more details.
// make[1]: *** [configure-target-libgcc] Error 1
// make[1]: Leaving directory `/home/NDS-UK/kyoupark/asn/gcc/gcc-4.8.2-build-mipsel-linux-gnu'
// make: *** [all] Error 2
// 
// try to use "--disable-miltilib" in order not to use multilib:
// try to use own binutil but fails on the same.
// 
// https://ftp.gnu.org/gnu/binutils/
// binutils-2.24.tar.bz2	2013-12-02 05:09 	22M	
// 
// http://wiki.osdev.org/Cross-Compiler_Successful_Builds
// 4.8.2 Yes 	Yes 
//       2.24, 2.23.2 	
// 
// ../binutils-2.24/configure --target=mipsel-linux-gnu --prefix=/home/nds-uk/kyoupark/asn/install --with-sysroot --disable-nls --disable-werror
// make
// make install
// 
// export PATH=~/asn/install/bin:$PATH

// note:
// cross gcc without C/C++ library
// http://wiki.osdev.org/GCC_Cross-Compiler
//
// build libgcc right after binutils is built without doing C/C++ libs?
// possible? 
//
// cd build-gcc
// ../gcc-x.y.z/configure --target=$TARGET --prefix="$PREFIX" --disable-nls --enable-languages=c,c++ --without-headers
// make all-gcc
// make all-target-libgcc
// make install-gcc
// make install-target-libgcc
//
// because:
//
// We build libgcc, a low-level support library that the compiler expects
// available at compile time. Linking against libgcc provides integer,
// floating point, decimal, stack unwinding (useful for exception handling)
// and other support functions. Note how we are not simply running make &&
// make install as that would build way too much, not all components of gcc
// are ready to target your unfinished operating system.
// 
// --disable-nls is the same as for binutils above.
// 
// --without-headers tells GCC not to rely on any C library (standard or runtime)
// being present for the target. 
//
// --without-headers
//  Tells GCC `not use any target headers` from a libc when building a cross
//  compiler. When crossing to GNU/Linux, you need the headers so GCC can
//  build the exception handling for libgcc. 
//
// Now you have a "naked" cross-compiler. It `does not have` access to a C
// library or C runtime yet, so you cannot use any of the standard includes or
// create runnable binaries. But it is quite `sufficient to compile the kernel`
//
// How this compiler is not able to compile normal C programs. The
// cross-compiler will spit errors whenever you want to #include any of the
// standard headers (except for a select few that actually are
// platform-independent, and generated by the compiler itself). This is quite
// correct - you don't have a standard library for the target system yet!
// 
// The C standard defines two different kinds of executing environments -
// "freestanding" and "hosted". While the definition might be rather fuzzy for
// the average application programmer, it is pretty clear-cut when you're
// doing OS development: A kernel is "freestanding", everything you do in user
// space is "hosted". A "freestanding" environment needs to provide only a
// `subset` of the C library: float.h, iso646.h, limits.h, stdalign.h, stdarg.h,
// stdbool.h, stddef.h, stdint.h and stdnoreturn.h (as of C11). All of these
// consist of typedef s and #define s "only", so you can implement them
// without a single .c file in sight. 
//
// note: 
// libgcc
// http://wiki.osdev.org/Libgcc

note: installs in ${prefix}/bin
-rwxr-xr-x 2 kyoupark ccusers 2259227 Dec  6 09:59 mips-unknown-linux-uclibc-c++*
-rwxr-xr-x 1 kyoupark ccusers 2253021 Dec  6 09:59 mips-unknown-linux-uclibc-cpp*
-rwxr-xr-x 2 kyoupark ccusers 2259227 Dec  6 09:59 mips-unknown-linux-uclibc-g++*
-rwxr-xr-x 2 kyoupark ccusers 2251225 Dec  6 09:59 mips-unknown-linux-uclibc-gcc*
-rwxr-xr-x 2 kyoupark ccusers 2251225 Dec  6 09:59 mips-unknown-linux-uclibc-gcc-4.8.2*
-rwxr-xr-x 1 kyoupark ccusers  110602 Dec  6 09:59 mips-unknown-linux-uclibc-gcc-ar*
-rwxr-xr-x 1 kyoupark ccusers  110546 Dec  6 09:59 mips-unknown-linux-uclibc-gcc-nm*
-rwxr-xr-x 1 kyoupark ccusers  110558 Dec  6 09:59 mips-unknown-linux-uclibc-gcc-ranlib*
-rwxr-xr-x 1 kyoupark ccusers 1241641 Dec  6 09:59 mips-unknown-linux-uclibc-gcov*


4. Standard C Library Headers and Startup Files: ${prefix}/arch/include, lib 
===============================================
note:
Before this, must have cross gcc, binuitl built, installed, and have it in the
PATH.

In this step, we install Glibc's `standard C library headers` to
/opt/cross/aarch64-linux/include. 

We also use the C compiler built in step 3 to compile the library's startup
files and install them to /opt/cross/aarch64-linux/lib. 

Finally, we create a couple of `dummy files`, libc.so and stubs.h, which are
expected in step 5, but which will be replaced in step 6.


$ mkdir -p build-glibc
$ cd build-glibc
$ ../glibc-2.20/configure --prefix=/opt/cross/aarch64-linux --build=$MACHTYPE --host=aarch64-linux --target=aarch64-linux --with-headers=/opt/cross/aarch64-linux/include --disable-multilib libc_cv_forced_unwind=yes
$ make install-bootstrap-headers=yes install-headers
$ make -j4 csu/subdir_lib
$ install csu/crt1.o csu/crti.o csu/crtn.o /opt/cross/aarch64-linux/lib
$ aarch64-linux-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o /opt/cross/aarch64-linux/lib/libc.so
$ touch /opt/cross/aarch64-linux/include/gnu/stubs.h
$ cd ..

--prefix=/opt/cross/aarch64-linux tells Glibc's configure script where it
should `install its headers and libraries` Note that it's different from the
usual --prefix.

Despite some contradictory information out there, Glibc's configure script
currently requires us to specify all three --build, --host and --target system
types.

$MACHTYPE is a predefined environment variable which describes the machine
running the build script. --build=$MACHTYPE is needed because in step 6, the
build script will compile some additional tools which run as part of the build
process itself.

// kyoupark@ukstbuild2:~/asn/gcc/glibc-2.20-build$ echo $MACHTYPE
// x86_64-redhat-linux-gnu

--host has `a different meaning` here than we've been using so far. In Glibc's
configure, both the --host and --target options are meant to describe the
system on which Glibc's libraries will ultimately run.

We install the C library's startup files, crt1.o, crti.o and crtn.o, to the
installation directory manually. There's doesn't seem to a make rule that does
this without having other side effects.

// ../glibc-2.20/configure --prefix=/home/nds-uk/kyoupark/asn/install-4.9.2/mips-linux 
// --build=x86_64-redhat-linux-gnu --host=mips-linux --target=mips-linux 
// --with-headers=/home/nds-uk/kyoupark/asn/install-4.9.2/mips-linux/include 
// --disable-multilib libc_cv_forced_unwind=yes

../glibc-2.20/configure --prefix=/home/nds-uk/kyoupark/asn/install-4.9.2/mips-linux \
--build=$MACHTYPE --host=mips-linux --target=mips-linux \
--with-headers=/home/nds-uk/kyoupark/asn/install-4.9.2/mips-linux/include \
--disable-multilib libc_cv_forced_unwind=yes

make install-bootstrap-headers=yes install-headers
make -j4 csu/subdir_lib

install csu/crt1.o csu/crti.o csu/crtn.o /home/nds-uk/kyoupark/asn/install-4.9.2/mips-linux/lib
mips-linux-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o /home/nds-uk/kyoupark/asn/install-4.9.2/mips-linux/lib/libc.so
touch /home/nds-uk/kyoupark/asn/install-4.9.2/mips-linux/include/gnu/stubs.h


../glibc-2.20/configure --prefix=/home/nds-uk/kyoupark/asn/install-4.8.2/mips-linux \
--build=$MACHTYPE --host=mips-linux --target=mips-linux \
--with-headers=/home/nds-uk/kyoupark/asn/install-4.8.2/mips-linux/include \
--disable-multilib libc_cv_forced_unwind=yes

make install-bootstrap-headers=yes install-headers
make -j4 csu/subdir_lib

note:
If miss out these, then cause libgcc in step 5 failed with errors around gcov
and pthred related.

install csu/crt1.o csu/crti.o csu/crtn.o /home/nds-uk/kyoupark/asn/install-4.8.2/mips-linux/lib
mips-linux-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o /home/nds-uk/kyoupark/asn/install-4.8.2/mips-linux/lib/libc.so
touch /home/nds-uk/kyoupark/asn/install-4.8.2/mips-linux/include/gnu/stubs.h


5. Compiler Support Library: 
===========================
This step `uses the cross-compilers` built in step 4 to build the compiler
support library. The compiler support library contains some C++ exception
handling boilerplate code, among other things. This library depends on the
startup files installed in step 4. The library itself is needed in step 6.

Unlike some other guides, we don't need to re-run GCC's configure. We're just
building additional targets in the same configuration.

$ cd build-gcc
$ make -j4 all-target-libgcc
$ make install-target-libgcc
$ cd ..

Two static libraries, libgcc.a and libgcc_eh.a, are installed to
/opt/cross/lib/gcc/aarch64-linux/4.9.2/.

A shared library, libgcc_s.so, is installed to /opt/cross/aarch64-linux/lib64.

cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc


6. Standard C Library (back to glibc)
=====================
In this step, we finish off the Glibc package, which builds the standard C
library and installs its files to /opt/cross/aarch64-linux/lib/. The static
library is named libc.a and the shared library is libc.so.

$ cd build-glibc
$ make -j4
$ make install
$ cd ..


7. Standard C++ Library (back to gcc)
=======================
Finally, we finish off the GCC package, which builds the standard C++ library
and installs it to /opt/cross/aarch64-linux/lib64/. It depends on the C
library built in step 6. The resulting static library is named libstdc++.a and
the shared library is libstdc++.so.

$ cd build-gcc
$ make -j5
$ make install
$ cd ..


8. Check
=======================
// from 
// BLD_AMS_BCM_MIPS4K_LNUX_DARWIN_01/platform_cfg/linux/compiler/mips4k_gcc_x86_linux_hound_2/bin/mips-linux-uclibc-gcc --version
// mips-linux-uclibc-gcc (GCC) 4.2.0 20070124 (prerelease) - BRCM 10tsHound-20140508
a.out: ELF 32-bit MSB executable, MIPS, MIPS32 version 1 (SYSV), dynamically linked (uses shared libs), not stripped

// from GCC built here. "2.6.32" comes from the build server but not from
// header used.
a.out: ELF 32-bit MSB executable, MIPS, MIPS-I version 1 (SYSV), dynamically linked (uses shared libs), \
for GNU/Linux 2.6.32, with unknown capability 0x41000000 = 0xf676e75, with unknown capability 0x10000 = 0x70401, not stripped

/home/nds-uk/kyoupark/a.out_from_gcc_482: ELF 32-bit MSB executable, MIPS, MIPS-I version 1 (SYSV), dynamically linked (uses shared libs), \
for GNU/Linux 2.6.32, with unknown capability 0x41000000 = 0xf676e75, with unknown capability 0x10000 = 0x70401, not stripped


={============================================================================
*kt_linux_gcc_400* gcc-build-asan

<enalbe-asan>

// gcc-4.8.2/configure.ac

# Disable libsanitizer on unsupported systems.
if test -d ${srcdir}/libsanitizer; then
  if test x$enable_libsanitizer = x; then
     AC_MSG_CHECKING([for libsanitizer support])

     # "test -n" true if the length of string is non-zero.

     if (srcdir=${srcdir}/libsanitizer; \
        . ${srcdir}/configure.tgt; \
        test -n "$UNSUPPORTED")
     then
         AC_MSG_RESULT([no])
         noconfigdirs="$noconfigdirs target-libsanitizer"
     else
         AC_MSG_RESULT([yes])
     fi
  fi
fi


// gcc-4.8.2/libsanitizer/configure.tgt

# Filter out unsupported systems.
case "${target}" in
  x86_64-*-linux* | i?86-*-linux*)
	if test x$ac_cv_sizeof_void_p = x8; then
		TSAN_SUPPORTED=yes
	fi
	;;
  powerpc*-*-linux*)
	;;
  sparc*-*-linux*)
	;;
  x86_64-*-darwin[1]* | i?86-*-darwin[1]*)
	TSAN_SUPPORTED=no
	;;
  *)
	UNSUPPORTED=1
	;;
esac

add this not to set(return) UNSUPPORTED as noted not to `filter out`:

  mips*-*-linux*)
	;;


0. Use either AC_CHECK or "--enable-libsanitizer".

configure:3202: checking for libsanitizer support
configure:3212: result: yes

../gcc-4.8.2/configure -C --prefix=/home/nds-uk/kyoupark/asn/install-4.8.2 --target=mips-linux --enable-languages=c,c++ --disable-multilib --enable-libsanitizer

# when use --enable-libsanitizer
# check on logs when runs "gcc/configure" to see which is excluded from.
*** This configuration is not supported in the following subdirectories:
     target-libitm gnattools target-libada target-libgfortran target-libgo target-libffi target-libbacktrace target-zlib target-libjava target-libobjc target-boehm-gc
    (Any other directories should still work fine.)


1. Run make in the step of the reference.

$ make
52:Configuring in ./fixincludes
160:Configuring in ./libiberty
654:Configuring in ./intl
775:Configuring in ./gmp
2199:Configuring in ./lto-plugin
2299:Configuring in ./gcc
2697:Configuring in ./mpfr
3536:Configuring in ./mpc
3877:Configuring in build-i686-pc-linux-gnu/libiberty
4343:Configuring in build-i686-pc-linux-gnu/fixincludes
4465:Configuring in ./zlib
4571:Configuring in ./libbacktrace
4722:Configuring in ./libcpp
4889:Configuring in ./libdecnumber
6168:Configuring in mips-linux/libgcc

$ make all-gcc
8:Configuring in ./gmp
1432:Configuring in ./lto-plugin
1515:Configuring in ./libiberty
1985:Configuring in ./intl
2106:Configuring in ./gcc
2499:Configuring in ./mpfr
3339:Configuring in ./mpc
3680:Configuring in build-x86_64-unknown-linux-gnu/libiberty
4132:Configuring in build-x86_64-unknown-linux-gnu/fixincludes
4254:Configuring in ./zlib
4360:Configuring in ./libbacktrace
4511:Configuring in ./libcpp
4678:Configuring in ./libdecnumber
4759:Configuring in ./fixincludes

When do "make" fails on libgcc and "make all-gcc" will not build "asan". 

The answer:

  @Maxim:
  Ah, no, you should do all 7 steps from the reference and just after that
  rebuild GCC like this:

  1) rm - rf *
  2) configure ... (from step 3)
  3) make - j12

You should not have problems with libgcc on that step (after you did all 7 steps from the reference).


2. do clean build as in the reference and back to step 3 and apply patches as
Maxim suggested. 

note: build outputs is in 
/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-mips/mips-linux

diff -up -r gcc-4.8.2-clean/gcc/config/mips/linux-common.h gcc-4.8.2/gcc/config/mips/linux-common.h
diff -up -r gcc-4.8.2-clean/gcc/config/mips/linux.h gcc-4.8.2/gcc/config/mips/linux.h

// Define TARGET_ASAN_SHADOW_OFFSET somewhere in gcc/config/mips/mips.c. You
// can see an example how to do this in gcc/config/arm/arm.c
diff -up -r gcc-4.8.2-clean/gcc/config/mips/mips.c gcc-4.8.2/gcc/config/mips/mips.c

// copy from 4.9.2 asan source which has mips support
diff -up -r gcc-4.8.2-clean/libsanitizer/asan/asan_linux.cc gcc-4.8.2/libsanitizer/asan/asan_linux.cc

// Add mips*-*-linux* (or whatever triplet you use) entry in
// libsanitizer/configure.tgt. This should enable libsanitizer build for MIPS.
diff -up -r gcc-4.8.2-clean/libsanitizer/configure.tgt gcc-4.8.2/libsanitizer/configure.tgt


../gcc-4.8.2/configure -C --prefix=/home/nds-uk/kyoupark/asn/install-4.8.2 --target=mips-linux --enable-languages=c,c++ --disable-multilib --enable-libsanitizer
make
make install


3. "-fstack-protector"

@Maxim:
Note that I've added  -fstack-protector flag to avoid "cc1: warning:
-fsanitize=address and -fsanitize=kernel-address are not supported for this
target" error on compilation step. This is weird, but for some reason GCC
folks use this flag to control FRAME_GROWS_DOWNWARD macro om MIPS.  GCC's ASan
doesn't support targets with FRAME_GROWS_DOWNWARD == 0 so I just added
-fstack-protector as a workaround.

$ mips-linux-gcc -fsanitize=address  <gcc_tree_location>/gcc/testsuite/c-c++-common/asan/heap-overflow-1.c -fstack-protector
$ qemu-mips -L $SYSROOT -R 0 ./a.out 

note: If misses out patch in mips.c then will get this error even if use
-fstack-protector.

$ ./bin/mips-linux-gcc-4.8.2 -fsanitize=address -Wl,-rpath=/home/NDS-UK/kyoupark/asn/install-4.8.2/mips-linux/lib ~/asn/x.c
/home/nds-uk/kyoupark/asn/x.c:1:0: warning: -fsanitize=address not supported for this target [enabled by default]
 #include <stdio.h>
 ^
$ ./bin/mips-linux-gcc-4.8.2 -fsanitize=address -Wl,-rpath=/home/NDS-UK/kyoupark/asn/install-4.8.2/mips-linux/lib ~/asn/x.c -fstack-protector
$


={============================================================================
*kt_linux_gcc_400* gcc-build-uclibc-build

Based on cross-ng

use a specific kernel version
wget --no-check-certificate https://www.kernel.org/pub/linux/kernel/v2.6/linux-2.6.19.tar.xz

INSTALL_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2-uc-two
export PATH=$INSTALL_PATH/bin:$PATH


1. binutil
===========

// [INFO ]  Installing binutils for host
// [DEBUG]    Entering '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-binutils-host-i686-build_pc-linux-gnu'
// [EXTRA]    Configuring binutils
// [DEBUG]    Extra config passed: '--enable-ld=yes --enable-gold=no --enable-plugins --with-pkgversion=crosstool-NG crosstool-ng-1.22.0 --disable-multilib --disable-nls'
// [DEBUG]    ==> Executing: 'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe '
//   'LDFLAGS= ' '/home/kyoupark/cross/work/.build/src/binutils-2.25.1/configure'
//     '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
//     '--target=mips-unknown-linux-uclibc'
//     '--prefix=/home/kyoupark/x-tools/mips-unknown-linux-uclibc'
//     '--disable-werror' '--enable-ld=yes' '--enable-gold=no' '--enable-plugins'
//     '--with-pkgversion=crosstool-NG crosstool-ng-1.22.0' '--disable-multilib'
//     '--disable-nls' '--with-float=soft'
//     '--with-sysroot=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot' 
// [EXTRA]    Installing binutils
// [DEBUG]    ==> Executing: '/usr/bin/make' 'install' 

../binutils-2.24/configure --target=mips-unknown-linux-uclibc --prefix=${INSTALL_PATH} --disable-nls --disable-multilib --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4
make install


2. kernel headers
===========
note: 
differnt from cross-ng since if do it after gcc-post-1 as cross-ng then see
build errors.

// [EXTRA]    Installing kernel headers
// [DEBUG]    ==> Executing: '/usr/bin/make' '-C'
//   '/home/kyoupark/cross/work/.build/src/linux-4.3'
//     'CROSS_COMPILE=mips-unknown-linux-uclibc-'
//     'O=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-kernel-headers'
//     'ARCH=mips'
//     'INSTALL_HDR_PATH=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr'
//     'V=0' 'headers_install' 
// 
// /usr/bin/make -C /home/kyoupark/cross/work/.build/src/linux-4.3 CROSS_COMPILE=mips-unknown-linux-uclibc- O=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-kernel-headers ARCH=mips INSTALL_HDR_PATH=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr V=0 headers_install
//
// note:
// install to ${INSTALL_HDR_PATH}/include
//
// kyoupark@kit-debian:~/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/include$ ll
// total 72
// drwxr-xr-x 14 kyoupark ccusers  4096 Dec  1 15:48 ./
// drwxr-xr-x  4 kyoupark ccusers  4096 Dec  1 15:48 ../
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 asm/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 asm-generic/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 drm/
// drwxr-xr-x 25 kyoupark ccusers 20480 Dec  1 15:48 linux/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 misc/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 mtd/
// drwxr-xr-x  3 kyoupark ccusers  4096 Dec  1 15:48 rdma/
// drwxr-xr-x  3 kyoupark ccusers  4096 Dec  1 15:48 scsi/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 sound/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 uapi/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 video/
// drwxr-xr-x  2 kyoupark ccusers  4096 Dec  1 15:48 xen/


cd linux-2.6.19
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr headers_install

cd linux-3.17.2
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr headers_install

// home/nds-uk/kyoupark/asn/install-4.8.2-own/mips-unknown-linux-uclibc/sysroot/usr


3. C/C++ Compilers (pass 1)
===========

// [INFO ]  Installing pass-1 core C gcc compiler
// [DEBUG]    Entering '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-cc-gcc-core-pass-1'
// [EXTRA]    Configuring core C gcc compiler

// note: this seems to do nothing
// [DEBUG]    Copying headers to install area of core C compiler
// [DEBUG]    ==> Executing: 'cp' '-a'
// '/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/include'
// '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools/mips-unknown-linux-uclibc/include' 
//
// [DEBUG]    ==> Executing: 'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc'
//   'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe ' 'LDFLAGS= '
//     '/home/kyoupark/cross/work/.build/src/gcc-5.2.0/configure'
//     '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
//     '--target=mips-unknown-linux-uclibc'
//     '--prefix=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-local-prefix=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
//     '--with-sysroot=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
//     '--with-newlib' '--enable-threads=no'  note: only in pass-1 
//     '--disable-shared'                     note: different from pass-2
//     '--with-pkgversion=crosstool-NG crosstool-ng-1.22.0' 
//     '--with-arch=mips1' '--with-abi=32' '--with-float=soft' '--enable-__cxa_atexit'
//     '--with-gmp=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-mpfr=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-mpc=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-isl=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-cloog=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-libelf=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--enable-lto' '--with-host-libstdcxx=-static-libgcc
//     -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' '--enable-target-optspace'
//     '--with-mips-plt' '--disable-libgomp' '--disable-libmudflap'
//     '--disable-libssp' '--disable-libquadmath' '--disable-libquadmath-support'
//     '--disable-nls' '--disable-multilib' '--enable-languages=c' 

../`gcc-4.8.2-clean`/configure --prefix=${INSTALL_PATH} --target=mips-unknown-linux-uclibc --enable-languages=c,c++ \
--disable-libmudflap --disable-multilib --disable-nls --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot

// ../gcc-4.8.2-clean/configure --prefix=${INSTALL_PATH} --target=mips-unknown-linux-uclibc --enable-languages=c,c++ --disable-multilib --disable-nls \
// --with-local-prefix=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot \
// --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot

make -j5 all-gcc
make install-gcc


4. Standard C Library Headers and Startup Files(uclibc)
===========

// note: untar to "build/src"
// 
// [INFO ]  =================================================================
// [INFO ]  Extracting and patching toolchain components
// [DEBUG]    Entering '/home/kyoupark/cross/work/.build/src'
// [EXTRA]    Extracting 'uClibc-ng-1.0.9'
// [DEBUG]    ==> Executing: 'mkdir' '-p' 'uClibc-ng-1.0.9' 
// [DEBUG]    ==> Executing: 'tar' '--strip-components=1' '-C' 'uClibc-ng-1.0.9' '-xv' '-f' '-' 
// 
//            /home/kyoupark/cross/work/.build/src/uClibc-ng-1.0.9
// 
// [INFO ]  =================================================================
// [INFO ]  Checking C library configuration
// [EXTRA]    Manage uClibc configuration
// [DEBUG]    ==> Executing: 'rm' '-f' '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/configs/uClibc.config' 
// [DEBUG]    ==> Executing: 'mkdir' '-p' '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/configs' 
// [DEBUG]    ==> Executing: 'cp'
//   '/home/kyoupark/cross/install/lib/crosstool-ng-1.22.0/contrib/uClibc-defconfigs/uClibc-ng.config'
//     '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/configs/uClibc.config' 
// 
// note: copy files from "build/src" to "build for header"
// 
// [INFO ]  =================================================================
// [INFO ]  Installing C library headers
// [EXTRA]    Copying sources to build dir
// [DEBUG]    ==> Executing: 'cp' '-av'
//   '/home/kyoupark/cross/work/.build/src/uClibc-ng-1.0.9'
//     '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/`build-libc-headers`' 

note: current directory '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc-headers'

// 
// note: after all, use "config" from "uClibc-ng.config"
// 
// [DEBUG]    ==> Executing: 'cp'
// '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/configs/uClibc.config'
//   '.config' 
//
// .config changes
// 
// from cross-ng
// KERNEL_HEADERS="/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/include"
//
// from BR
// TARGET_mips=y
// TARGET_ARCH="mips"
// FORCE_OPTIONS_FOR_ARCH=y
// CONFIG_MIPS_O32_ABI=y
// KERNEL_HEADERS="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1-mips-only/output/build/linux-headers-3.2.81/usr/include"
// CROSS_COMPILER_PREFIX="/home/NDS-UK/kyoupark/asn/broot-latest/buildroot-2016.08.1-mips-only/output/host/usr/bin/mips-buildroot-linux-uclibc-"
// 
// RUNTIME_PREFIX="/"
// DEVEL_PREFIX="/usr/"
// KERNEL_HEADERS="/home/nds-uk/kyoupark/asn/install-4.8.2-own/mips-unknown-linux-uclibc/sysroot/usr/include"
//
// [EXTRA]    Applying configuration
// [DEBUG]    ==> Executing: '/usr/bin/make'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'oldconfig' 
// 
// [ALL  ]    make[1]: Entering directory '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc-headers'
// [ALL  ]    * Restart config...
// [ALL  ]    *
// [ALL  ]    *
// [ALL  ]    * uClibc-ng 1.0.9 C Library Configuration
//
// 
// from INSTALL
//
// If you have an existing .config file, you can update this file
// using the
//
//       make oldconfig
//
// command, which will only ask you about new configuration options.

note: since binuitl is in PATH

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ oldconfig 

// kyoupark@ukstbuild2:~/asn/uclibc/uClibc-0.9.33.2-build$ make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ oldconfig
//   MKDIR include/config
//   HOSTCC-o extra/config/conf.o
//   GEN extra/config/zconf.tab.c
//   GEN extra/config/lex.zconf.c
//   GEN extra/config/zconf.hash.c
//   HOSTCC-o extra/config/zconf.tab.o
//   HOSTCC extra/config/conf
// #
// # configuration written to ./.config
// #

// note: build and install headers, startup, dummy c lib to "x-tools/sysroot"
// 
// [EXTRA]    Building headers
// [DEBUG]    ==> Executing: '/usr/bin/make'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'headers' 
//
// make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/ LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz headers

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ headers 

// [EXTRA]    Installing headers
// [DEBUG]    ==> Executing: '/usr/bin/make'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'install_headers' 
//
// [ALL  ]    make[1]: Entering directory '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc-headers'
// [ALL  ]      INSTALL include -> /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
// [ALL  ]    make[1]: Leaving directory '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc-headers'
// 
// note: this step really copies headers
// WHY "../sysroot/usr/include"? 
// since "install_headers.sh" uses:
// dstdir=${2:-`. ./.config 2>/dev/null && echo ${DEVEL_PREFIX}/include`}
//
// this is from .config
// DEVEL_PREFIX="/usr/"
//
// kyoupark@kit-debian:~/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc-headers$ make Q="echo " CROSS_COMPILE=mips-unknown-linux-uclibc- \
// UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/ LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz install_headers
// echo
// 
//   INSTALL include -> /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
// echo top_builddir=./ \
// ./extra/scripts/install_headers.sh \
// include /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
// top_builddir=./ ./extra/scripts/install_headers.sh include /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
//
// echo cd /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include && rm -f -r config generated internal cancel.h dl-osinfo.h jmpbuf-offsets.h jmpbuf-unwind.h hp-timing.h not-cancel.h _lfs_64.h bits/uClibc_arch_features.h bits/kernel_sigaction.h bits/kernel_stat.h bits/kernel_types.h bits/libc-lock.h bits/stdio-lock.h bits/syscalls.h bits/syscalls-common.h bits/uClibc_fpmax.h bits/uClibc_mutex.h bits/uClibc_pthread.h bits/uClibc_uintmaxtostr.h bits/uClibc_uwchar.h bits/uClibc_va_copy.h bits/sigcontextinfo.h bits/stackinfo.h atomic.h bits/atomic.h tls.h rpc/des_crypt.h rpc/key_prot.h rpc/rpc_des.h fenv.h bits/fenv.h bits/fenvinline.h fts.h libintl.h netinet/ip6.h netinet/icmp6.h execinfo.h iconv.h bits/uClibc_ctype.h rpc wordexp.h xlocale.h ustat.h sys/ustat.h bits/ustat.h
// cd /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
// echo cd /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include && rm -f -f wchar-stub.h
// cd /home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot//usr/include
//
// make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe \
// PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/ LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz install_headers
// 

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install_headers 

// 
// this is wrong
// kyoupark@ukstbuild2:~/asn/uclibc/uClibc-0.9.33.2-build$ make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install_headers
//   INSTALL include -> /home/nds-uk/kyoupark/asn/install-4.8.2-own/mips-unknown-linux-uclibc/sysroot//usr/mips-linux-uclibc/usr/include

// [EXTRA]    Building start files
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'STRIPTOOL=true' 'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz'
//     'lib/crt1.o' 'lib/crti.o' 'lib/crtn.o' 

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ STRIPTOOL=true lib/crt1.o lib/crti.o lib/crtn.o

//   MKDIR lib
//   AS lib/crt1.o
// libc/sysdeps/linux/mips/crt1.S: Assembler messages:
// libc/sysdeps/linux/mips/crt1.S:117: Warning: no .cprestore pseudo-op used in PIC code
//   AS lib/crti.o
//   AS lib/crtn.o

// [EXTRA]    Building dummy shared libs
// [DEBUG]    ==> Executing: 'mips-unknown-linux-uclibc-gcc' '-nostdlib'
//   '-nostartfiles' '-shared' '-x' 'c' '/dev/null' '-o' 'libdummy.so' 

mips-unknown-linux-uclibc-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o libdummy.so

// [EXTRA]    Installing start files
// [DEBUG]    ==> Executing: '/usr/bin/install' '-c' '-m' '0644' 'lib/crt1.o'
//   'lib/crti.o' 'lib/crtn.o'
//     '/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/lib' 

note: "lib" is created before in cross-ng?
mkdir -p ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0644 lib/crt1.o lib/crti.o lib/crtn.o ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib

// [EXTRA]    Installing dummy shared libs
// [DEBUG]    ==> Executing: '/usr/bin/install' '-c' '-m' '0755' 'libdummy.so'
//   '/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/lib/libc.so'

/usr/bin/install -c -m 0755 libdummy.so ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib/libc.so


5. Compiler Support Library (libgcc)
===========

[INFO ]  =================================================================
[INFO ]  Installing pass-2 core C gcc compiler
[DEBUG]    Entering '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-cc-gcc-core-pass-2'
[EXTRA]    Configuring core C gcc compiler
[DEBUG]    Copying headers to install area of core C compiler
[DEBUG]    ==> Executing: 'cp' '-a' '/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/usr/include' '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools/mips-unknown-linux-uclibc/include' 
[DEBUG]    Extra config passed: '--enable-shared --with-pkgversion=crosstool-NG crosstool-ng-1.22.0 --with-arch=mips1 --with-abi=32 --with-float=soft --enable-__cxa_atexit --with-gmp=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --with-mpfr=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --with-mpc=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --with-isl=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --with-cloog=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --with-libelf=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools --enable-lto --with-host-libstdcxx=-static-libgcc -Wl,-Bstatic,-lstdc++,-Bdynamic -lm --enable-target-optspace --with-mips-plt --disable-libgomp --disable-libmudflap --disable-libssp --disable-libquadmath --disable-libquadmath-support --disable-nls --disable-multilib'
[DEBUG]    ==> Executing: 'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc'
  'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe ' 'LDFLAGS= '
    '/home/kyoupark/cross/work/.build/src/gcc-5.2.0/configure'
    '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
    '--target=mips-unknown-linux-uclibc'
    '--prefix=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-local-prefix=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
    '--with-sysroot=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
    '--enable-shared'                       note: different from pass-1
    '--with-pkgversion=crosstool-NG crosstool-ng-1.22.0'
    '--with-arch=mips1' '--with-abi=32' '--with-float=soft' '--enable-__cxa_atexit'
    '--with-gmp=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-mpfr=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-mpc=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-isl=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-cloog=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--with-libelf=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
    '--enable-lto' '--with-host-libstdcxx=-static-libgcc
    -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' '--enable-target-optspace'
    '--with-mips-plt' '--disable-libgomp' '--disable-libmudflap'
    '--disable-libssp' '--disable-libquadmath' '--disable-libquadmath-support'
    '--disable-nls' '--disable-multilib' '--enable-languages=c' 
  
note:
must do "make install-target-libgcc". Otherwise, fails on next uclibc build.

cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc

kyoupark@ukstbuild2:~/asn/install-4.8.2-own/mips-unknown-linux-uclibc/lib$ ls -al
total 1552
drwxr-xr-x 3 kyoupark ccusers    4096 Dec  2 09:22 .
drwxr-xr-x 5 kyoupark ccusers    4096 Nov 29 16:02 ..
drwxr-xr-x 2 kyoupark ccusers    4096 Nov 29 13:35 ldscripts
lrwxrwxrwx 1 kyoupark ccusers      13 Dec  2 09:22 libgcc_s.so -> libgcc_s.so.1
-rw-r--r-- 1 kyoupark ccusers 1576912 Dec  2 09:22 libgcc_s.so.1


6. Standard C Library (back to uclibc)
===========

// note: copy files from "build/src" to "build for libc"
//
// [INFO ]  Installing C library
// [EXTRA]    Copying sources to build dir
// [DEBUG]    ==> Executing: 'cp' '-av'
//   '/home/kyoupark/cross/work/.build/src/uClibc-ng-1.0.9'
//     '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/build-libc' 

// note: same as "build-libc-headers"
//
// [DEBUG]    ==> Executing: 'cp'
// '/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/build/configs/uClibc.config'
//   '.config' 
// 
// [EXTRA]    Applying configuration
// [DEBUG]    ==> Executing: '/usr/bin/make'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'oldconfig' 

// [EXTRA]    Building C library
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j1'
//   'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//     'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//     'STRIPTOOL=true' 'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'pregen' 
// 
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l'
// 'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
//   'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
//   'STRIPTOOL=true' 'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'all' 
//
// [EXTRA]    Installing C library
// [DEBUG]    ==> Executing: '/usr/bin/make'
// 'CROSS_COMPILE=mips-unknown-linux-uclibc-' 'UCLIBC_EXTRA_CFLAGS=-pipe'
// 'PREFIX=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/'
// 'STRIPTOOL=true' 'LOCALE_DATA_FILENAME=uClibc-locale-030818.tgz' 'install' 

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ pregen
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ all

libc/libc_so.a(if_index.os): In function `__GI_if_nameindex':
if_index.c:(.text+0x328): undefined reference to `IFLA_RTA'
if_index.c:(.text+0x340): undefined reference to `IFLA_PAYLOAD'
collect2: error: ld returned 1 exit status
make: *** [lib/libc.so] Error 1

note:
Trid linux-3.17.2 instead and causes different error:

/home/nds-uk/kyoupark/asn/install-4.8.2-own/mips-unknown-linux-uclibc/sysroot/usr/include/linux/sysinfo.h:8:2: error: unknown type name '__kernel_long_t'
  __kernel_long_t uptime;  /* Seconds since boot */
  ^
make: *** [libc/inet/if_index.os] Error 1

note:
So tried uClibc-ng-1.0.17 instead with the same kernel and built! However,
   failed with linux-2.6.19 around the same code as uClibc case above.

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install


7. (back to gcc)
===========

// [INFO ]  =================================================================
// [INFO ]  Installing final gcc compiler
// [DEBUG]    ==> Executing: 'CC_FOR_BUILD=i686-build_pc-linux-gnu-gcc'
//   'CFLAGS=-O2 -g -pipe ' 'CXXFLAGS=-O2 -g -pipe ' 'LDFLAGS= '
//     'CFLAGS_FOR_TARGET= -EB -march=mips1 -mabi=32    -msoft-float '
//     'CXXFLAGS_FOR_TARGET= -EB -march=mips1 -mabi=32    -msoft-float '
//     'LDFLAGS_FOR_TARGET= -Wl,-EB '
//     '/home/kyoupark/cross/work/.build/src/gcc-5.2.0/configure'
//     '--build=i686-build_pc-linux-gnu' '--host=i686-build_pc-linux-gnu'
//     '--target=mips-unknown-linux-uclibc'
//     '--prefix=/home/kyoupark/x-tools/mips-unknown-linux-uclibc'
//     '--with-sysroot=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
//     '--enable-languages=c,c++' '--with-arch=mips1' '--with-abi=32'
//     '--with-float=soft' '--with-pkgversion=crosstool-NG crosstool-ng-1.22.0'
//     '--enable-__cxa_atexit' '--disable-libmudflap' '--disable-libgomp'
//     '--disable-libssp' '--disable-libquadmath' '--disable-libquadmath-support'
//     '--disable-libsanitizer'
//     '--with-gmp=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-mpfr=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-mpc=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-isl=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-cloog=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--with-libelf=/home/kyoupark/cross/work/.build/mips-unknown-linux-uclibc/buildtools'
//     '--enable-lto' '--with-host-libstdcxx=-static-libgcc
//     -Wl,-Bstatic,-lstdc++,-Bdynamic -lm' '--enable-threads=posix'
//     '--enable-target-optspace' '--enable-plugin' '--with-mips-plt'
//     '--disable-nls' '--disable-multilib'
//     '--with-local-prefix=/home/kyoupark/x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot'
//     '--enable-long-long' 
// 
// [EXTRA]    Building final gcc compiler
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l' 'all' 
// 
// [EXTRA]    Installing final gcc compiler
// [DEBUG]    ==> Executing: '/usr/bin/make' '-j2' '-l' 'install' 
// 
// [EXTRA]    Housekeeping for final gcc compiler
// [DEBUG]    Entering '/home/kyoupark/x-tools/mips-unknown-linux-uclibc'
// [DEBUG]    ==> Executing: 'ln' '-sfv' 'mips-unknown-linux-uclibc-gcc'
//   '/home/kyoupark/x-tools/mips-unknown-linux-uclibc/bin/mips-unknown-linux-uclibc-cc' 

make -j5
make install


<commands>
INSTALL_PATH=/home/nds-uk/kyoupark/asn/install-3.8.2-uc-two
export PATH=$INSTALL_PATH/bin:$PATH

../binutils-2.24/configure --target=mips-unknown-linux-uclibc --prefix=${INSTALL_PATH} --disable-nls --disable-multilib --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4
make install

cd linux-3.17.2
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr headers_install

../gcc-4.8.2-clean/configure --prefix=${INSTALL_PATH} --target=mips-unknown-linux-uclibc --enable-languages=c,c++ \
--disable-libmudflap --disable-multilib --disable-nls --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4 all-gcc
make install-gcc

cd uClibc-ng-1.0.17
DEVEL_PREFIX="/usr/"
KERNEL_HEADERS="/home/nds-uk/kyoupark/asn/install-4.8.2-uc-two/mips-unknown-linux-uclibc/sysroot/usr/include"
make clean
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install_headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ STRIPTOOL=true lib/crt1.o lib/crti.o lib/crtn.o
mips-unknown-linux-uclibc-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o libdummy.so

mkdir -p ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0644 lib/crt1.o lib/crti.o lib/crtn.o ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0755 libdummy.so ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib/libc.so

cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc

cd uClibc-ng-1.0.17
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ pregen
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ all
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install

cd build-gcc
make -j5
make install


={============================================================================
*kt_linux_gcc_400* gcc-build-uclibc-build-asan

/home/nds-uk/kyoupark/asn/gcc/gcc-4.8.2-build-uclibc-wasan

INSTALL_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan-wsys
export PATH=$INSTALL_PATH/bin:$PATH

../binutils-2.24/configure --target=mips-unknown-linux-uclibc --prefix=${INSTALL_PATH} --disable-nls --disable-multilib --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4
make install

cd linux-3.17.2
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr headers_install

# this gcc-pass-1 expects to see kernel headers from
# ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr. How?

../gcc-4.8.2-wasan/configure --prefix=${INSTALL_PATH} --target=mips-unknown-linux-uclibc --enable-languages=c,c++ \
--enable-libsanitizer --disable-libmudflap --disable-multilib --disable-nls --with-sysroot=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot
make -j4 all-gcc
make install-gcc

cd uClibc-ng-1.0.17
# change KERNEL_HEADERS in .config if intall path is changed
DEVEL_PREFIX="/usr/"
KERNEL_HEADERS="/home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/sysroot/usr/include"
make clean
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install_headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ STRIPTOOL=true lib/crt1.o lib/crti.o lib/crtn.o
mips-unknown-linux-uclibc-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o libdummy.so
mkdir -p ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0645 lib/crt1.o lib/crti.o lib/crtn.o ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0755 libdummy.so ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib/libc.so

cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ pregen
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ all
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/ install

cd build-gcc
make -j5
make install


/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/bin/ld: warning: libstdc++.so.6, needed by /home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/lib/libasan.so, not found (try using -rpath or -rpath-link)
/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/lib/libasan.so: undefined reference to `__libc_free'
/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/lib/libasan.so: undefined reference to `__libc_malloc'
collect2: error: ld returned 1 exit status

sanitizer_allocator.o:
00000000 r $LC0
00000054 r $LC1
0000007c r $LC2
         U _ZN11__sanitizer11CheckFailedEPKciS1_yy
00000074 T _ZN11__sanitizer12InternalFreeEPv
00000000 T _ZN11__sanitizer13InternalAllocEm
         U _ZN11__sanitizer17GetPageSizeCachedEv
00000124 T _ZN11__sanitizer17LowLevelAllocator8AllocateEm
0000024c T _ZN11__sanitizer27SetLowLevelAllocateCallbackEPFvmmE
00000264 T _ZN11__sanitizer35CallocShouldReturnNullDueToOverflowEmm
         U _ZN11__sanitizer9MmapOrDieEmPKc
00000000 b _ZN11__sanitizerL24low_level_alloc_callbackE
00000000 r _ZZN11__sanitizer17LowLevelAllocator8AllocateEmE12__FUNCTION__
         U __libc_free
         U __libc_malloc
         U _gp_disp

// sanitizer_allocator.cc
// FIXME: We should probably use more low-level allocator that would
// mmap some pages and split them into chunks to fulfill requests.
// #if defined(__linux__) && !defined(__ANDROID__)
// extern "C" void *__libc_malloc(__sanitizer::uptr size);
// extern "C" void __libc_free(void *ptr);
// # define LIBC_MALLOC __libc_malloc
// # define LIBC_FREE __libc_free
// #else  // __linux__ && !ANDROID
# include <stdlib.h>
# define LIBC_MALLOC malloc
# define LIBC_FREE free
// #endif  // __linux__ && !ANDROID

Fixes:
/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/lib/libasan.so: undefined reference to `__libc_free'
/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/lib/libasan.so: undefined reference to `__libc_malloc'


./bin/mips-unknown-linux-uclibc-gcc -fsanitize=address ~/uaf.c -fstack-protector -Wl,-rpath=/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/lib

Fixes:
/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/bin/ld: warning: libstdc++.so.6, needed by /home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/bin/../lib/gcc/mips-unknown-linux-uclibc/4.8.2/../../../../mips-unknown-linux-uclibc/lib/libasan.so, not found (try using -rpath or -rpath-link)

-sh-3.2# ./a.out_from_mips_asan
./a.out_from_mips_asan: can't load library 'libasan.so.0'

-rw-r--r-- 1 kyoupark ccusers 1639506 Dec  7 15:37 libasan.a
-rwxr-xr-x 1 kyoupark ccusers    1129 Dec  7 15:37 libasan.la*
lrwxrwxrwx 1 kyoupark ccusers      16 Dec  7 15:37 libasan.so -> libasan.so.0.0.0*
lrwxrwxrwx 1 kyoupark ccusers      16 Dec  7 15:37 libasan.so.0 -> libasan.so.0.0.0*
-rwxr-xr-x 1 kyoupark ccusers  984707 Dec  7 15:37 libasan.so.0.0.0*

-rw-r--r-- 1 kyoupark ccusers 53466 Dec  7 15:37 libssp.a
-rwxr-xr-x 1 kyoupark ccusers  1001 Dec  7 15:37 libssp.la*
-rw-r--r-- 1 kyoupark ccusers  2630 Dec  7 15:37 libssp_nonshared.a
-rwxr-xr-x 1 kyoupark ccusers   983 Dec  7 15:37 libssp_nonshared.la*
lrwxrwxrwx 1 kyoupark ccusers    15 Dec  7 15:37 libssp.so -> libssp.so.0.0.0*
lrwxrwxrwx 1 kyoupark ccusers    15 Dec  7 15:37 libssp.so.0 -> libssp.so.0.0.0*
-rwxr-xr-x 1 kyoupark ccusers 27038 Dec  7 15:37 libssp.so.0.0.0*

./a.out_from_mips_asan: can't load library 'libstdc++.so.6'

lrwxrwxrwx 1 kyoupark ccusers      19 Dec  7 15:37 libstdc++.so -> libstdc++.so.6.0.18*
lrwxrwxrwx 1 kyoupark ccusers      19 Dec  7 15:37 libstdc++.so.6 -> libstdc++.so.6.0.18*
-rwxr-xr-x 1 kyoupark ccusers 5410330 Dec  7 15:37 libstdc++.so.6.0.18*

./a.out_from_mips_asan: can't load library 'ld-uClibc.so.1'

lrwxrwxrwx 1 kyoupark ccusers     19 Dec  7 14:48 ld-uClibc.so.1 -> ld-uClibc-1.0.17.so*

<x>
kyoupark@ukstbuild2:~/asn/install-4.8.2-uclibc-asan/mips-unknown-linux-uclibc/lib$ ../../bin/mips-unknown-linux-uclibc-readelf -d libasan.so.0.0.0 | grep NEEDED
4: 0x00000001 (NEEDED)                     Shared library: [libpthread.so.0]
5: 0x00000001 (NEEDED)                     Shared library: [libdl.so.0]
6: 0x00000001 (NEEDED)                     Shared library: [libstdc++.so.6]
7: 0x00000001 (NEEDED)                     Shared library: [libm.so.0]
8: 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
9: 0x00000001 (NEEDED)                     Shared library: [ld-uClibc.so.1]
10: 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]


-sh-3.2# LD_LIBRARY_PATH=/mnt/tmp/si_logs/mips_libs ./a.out_from_mips_asan
Segmentation fault

(gdb) file a.out_from_mips_asan
warning: core file may not match specified executable file.
Reading symbols from /home/NDS-UK/kyoupark/a.out_from_mips_asan...Dwarf Error: wrong version in compilation unit header (is 4, should be 2) [in module /home/NDS-UK/kyoupark/a.out_from_mips_asan]


<static>
./bin/mips-unknown-linux-uclibc-gcc -fsanitize=address ~/uaf.c -fstack-protector -lasan -L./mips-unknown-linux-uclibc/lib/

-Wl,-rpath=/home/NDS-UK/kyoupark/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/lib


<without-sysroot>
INSTALL_PATH=/home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan-wosys
export PATH=$INSTALL_PATH/bin:$PATH

../binutils-2.24/configure --target=mips-unknown-linux-uclibc --prefix=${INSTALL_PATH} --disable-nls --disable-multilib
make -j4
make install

cd linux-3.17.2
make ARCH=mips INSTALL_HDR_PATH=${INSTALL_PATH}/mips-unknown-linux-uclibc headers_install

cd gcc-4.8.2-build-uclibc-wasan
../gcc-4.8.2-wasan/configure --prefix=${INSTALL_PATH} --target=mips-unknown-linux-uclibc --enable-languages=c,c++ \
--enable-libsanitizer --disable-libmudflap --disable-multilib --disable-nls 
make -j4 all-gcc
make install-gcc

# change KERNEL_HEADERS in .config if intall path is changed
DEVEL_PREFIX=""
KERNEL_HEADERS="/home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan-wosys/mips-unknown-linux-uclibc/include"
make clean
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc install_headers 
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc STRIPTOOL=true lib/crt1.o lib/crti.o lib/crtn.o
mips-unknown-linux-uclibc-gcc -nostdlib -nostartfiles -shared -x c /dev/null -o libdummy.so
mkdir -p ${INSTALL_PATH}/mips-unknown-linux-uclibc/sysroot/usr/lib
/usr/bin/install -c -m 0645 lib/crt1.o lib/crti.o lib/crtn.o ${INSTALL_PATH}/mips-unknown-linux-uclibc/lib
/usr/bin/install -c -m 0755 libdummy.so ${INSTALL_PATH}/mips-unknown-linux-uclibc/lib/libc.so

cd build-gcc
make -j4 all-target-libgcc
make install-target-libgcc

make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc pregen
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc all
make CROSS_COMPILE=mips-unknown-linux-uclibc- UCLIBC_EXTRA_CFLAGS=-pipe PREFIX=${INSTALL_PATH}/mips-unknown-linux-uclibc install

cd build-gcc
make -j5

// /home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan/mips-unknown-linux-uclibc/bin/ld: cannot find /lib/libc.so.1
// /home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan/mips-unknown-linux-uclibc/bin/ld: cannot find /lib/uclibc_nonshared.a
// /home/nds-uk/kyoupark/asn/install-4.8.2-uclibc-asan/mips-unknown-linux-uclibc/bin/ld: cannot find /lib/ld-uClibc.so.1
// collect2: error: ld returned 1 exit status
// make[2]: *** [libgcc_s.so] Error 1

make install


={============================================================================
*kt_linux_gcc_400* gcc-build-uclibc-xx

# from-mips-glibc-without-sysroot

kyoupark@ukstbuild2:~/asn/install-4.8.2-mips-glibc/mips-linux/lib$ ls
audit           libasan_preinit.o        libcidn-2.20.so   libgcc_s.so       libmudflap.a           libnss_compat.so       libnss_nisplus-2.20.so  librpcsvc.a          libstdc++.so
crt1.o          libasan.so               libcidn.so        libgcc_s.so.1     libmudflap.la          libnss_compat.so.2     libnss_nisplus.so       librt-2.20.so        libstdc++.so.6
crti.o          libasan.so.0             libcidn.so.1      libgomp.a         libmudflap.so          libnss_db-2.20.so      libnss_nisplus.so.2     librt.a              libstdc++.so.6.0.18
crtn.o          libasan.so.0.0.0         libc_nonshared.a  libgomp.la        libmudflap.so.0        libnss_db.so           libnss_nis.so           librt.so             libstdc++.so.6.0.18-gdb.py
gconv           libatomic.a              libcrypt-2.20.so  libgomp.so        libmudflap.so.0.0.0    libnss_db.so.2         libnss_nis.so.2         librt.so.1           libsupc++.a
gcrt1.o         libatomic.la             libcrypt.a        libgomp.so.1      libmudflapth.a         libnss_dns-2.20.so     libpcprofile.so         libSegFault.so       libsupc++.la
ld-2.20.so      libatomic.so             libcrypt.so       libgomp.so.1.0.0  libmudflapth.la        libnss_dns.so          libpthread-2.20.so      libssp.a             libthread_db-1.0.so
ldscripts       libatomic.so.1           libcrypt.so.1     libgomp.spec      libmudflapth.so        libnss_dns.so.2        libpthread.a            libssp.la            libthread_db.so
ld.so.1         libatomic.so.1.0.0       libc.so           libieee.a         libmudflapth.so.0      libnss_files-2.20.so   libpthread_nonshared.a  libssp_nonshared.a   libthread_db.so.1
libanl-2.20.so  libBrokenLocale-2.20.so  libc.so.6         libm-2.20.so      libmudflapth.so.0.0.0  libnss_files.so        libpthread.so           libssp_nonshared.la  libutil-2.20.so
libanl.a        libBrokenLocale.a        libdl-2.20.so     libm.a            libnsl-2.20.so         libnss_files.so.2      libpthread.so.0         libssp.so            libutil.a
libanl.so       libBrokenLocale.so       libdl.a           libmcheck.a       libnsl.a               libnss_hesiod-2.20.so  libresolv-2.20.so       libssp.so.0          libutil.so
libanl.so.1     libBrokenLocale.so.1     libdl.so          libmemusage.so    libnsl.so              libnss_hesiod.so       libresolv.a             libssp.so.0.0.0      libutil.so.1
libasan.a       libc-2.20.so             libdl.so.2        libm.so           libnsl.so.1            libnss_hesiod.so.2     libresolv.so            libstdc++.a          Mcrt1.o
libasan.la      libc.a                   libg.a            libm.so.6         libnss_compat-2.20.so  libnss_nis-2.20.so     libresolv.so.2          libstdc++.la         Scrt1.o


# from-mips-uclibc-with-sysroot

kyoupark@ukstbuild2:~/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/lib$ ls
ldscripts          libasan.so        libatomic.la        libgcc_s.so    libgomp.so        libssp.a             libssp.so        libstdc++.la         libstdc++.so.6.0.18-gdb.py
libasan.a          libasan.so.0      libatomic.so        libgcc_s.so.1  libgomp.so.1      libssp.la            libssp.so.0      libstdc++.so         libsupc++.a
libasan.la         libasan.so.0.0.0  libatomic.so.1      libgomp.a      libgomp.so.1.0.0  libssp_nonshared.a   libssp.so.0.0.0  libstdc++.so.6       libsupc++.la
libasan_preinit.o  libatomic.a       libatomic.so.1.0.0  libgomp.la     libgomp.spec      libssp_nonshared.la  libstdc++.a      libstdc++.so.6.0.18

kyoupark@ukstbuild2:~/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/sysroot/lib$ ls
ld-uClibc-1.0.17.so  libcrypt.so.0  libdl-1.0.17.so  libm.so.0         libnsl.so.1           libresolv-1.0.17.so  librt.so.0              libuClibc-1.0.17.so
ld-uClibc.so.0       libcrypt.so.1  libdl.so.0       libm.so.1         libpthread-1.0.17.so  libresolv.so.0       librt.so.1              libutil-1.0.17.so
ld-uClibc.so.1       libc.so.0      libdl.so.1       libnsl-1.0.17.so  libpthread.so.0       libresolv.so.1       libthread_db-1.0.17.so  libutil.so.0
libcrypt-1.0.17.so   libc.so.1      libm-1.0.17.so   libnsl.so.0       libpthread.so.1       librt-1.0.17.so      libthread_db.so.1       libutil.so.1

kyoupark@ukstbuild2:~/asn/install-4.8.2-uclibc-asan-wsys/mips-unknown-linux-uclibc/sysroot/usr/lib$ ls
crt1.o  libc_pic.a      libc.so      libm.a      libnsl_pic.a            libpthread_nonshared_pic.a  libresolv_pic.a  librt.so            libutil.a      uclibc_nonshared.a
crti.o  libcrypt.a      libdl.a      libm_pic.a  libnsl.so               libpthread_pic.a            libresolv.so     libthread_db.a      libutil_pic.a
crtn.o  libcrypt_pic.a  libdl_pic.a  libm.so     libpthread.a            libpthread.so               librt.a          libthread_db_pic.a  libutil.so
libc.a  libcrypt.so     libdl.so     libnsl.a    libpthread_nonshared.a  libresolv.a                 librt_pic.a      libthread_db.so     Scrt1.o


={============================================================================
*kt_linux_gcc_400* gcc-build-cross-ng

wget http://crosstool-ng.org/download/crosstool-ng/crosstool-ng-1.22.0.tar.bz2

tar xjf crosstool-ng-VERSION.tar.bz2
cd crosstool-ng-VERSION
./configure --prefix=/some/place
make
make install
export PATH="${PATH}:/some/place/bin"

Then, you are ready to use crosstool-NG.

create a place to work in, then list the existing samples (pre-configured
    toolchains that are known to build and work) to see if one can fit your
actual needs. Sample names are 4-part tuples, such as
arm-unknown-linux-gnueabi. In the following, we'll use that as a sample name;
adapt to your needs:

mkdir /a/directory/to/build/your/toolchain
cd /a/directory/to/build/your/toolchain
ct-ng help
ct-ng list-samples
ct-ng show-arm-unknown-linux-gnueabi

once you know what sample to use, configure ct-ng to use it:

ct-ng arm-unknown-linux-gnueabi

samples are configured to install in
"${HOME}/x-tools/arm-unknown-linux-gnueabi" by default. This should be OK for
a first time user, so you can now build your toolchain:

ct-ng build

finally, you can set access to your toolchain, and call your new
cross-compiler with :

export PATH="${PATH}:${HOME}/x-tools/arm-unknown-linux-gnueabi/bin"
arm-unknown-linux-gnueabi-gcc

If no sample really fits your needs:

choose the one closest to what you want (see above), and start building it
(see above, too) this ensures sure it is working for your machine, before
trying to do more advanced tests fine-tune the configuration, and re-run the
build, with:

ct-ng menuconfig
ct-ng build

Note 2: If you call ct-ng --help you will get help for make(2). This is
because ct-ng is in fact a make(2) script. There is no clean workaround for
this. 


<steps>
kyoupark@kit-debian:~/cross/work$ ct-ng list-steps
Available build steps, in order:
  - libc_check_config
  - companion_libs_for_build
  - binutils_for_build
  - companion_libs_for_host
  - binutils_for_host
  - cc_core_pass_1
  - kernel_headers
  - libc_start_files
  - cc_core_pass_2
  - libc
  - cc_for_build
  - cc_for_host
  - libc_post_cc
  - companion_libs_for_target
  - binutils_for_target
  - debug
  - test_suite
  - finish


Environment variables (see /home/kyoupark/cross/install/share/doc/crosstool-ng/crosstool-ng-1.22.0/0 - Table of content.txt):
  STOP=step          - Stop the build just after this step (list with list-steps)
  RESTART=step       - Restart the build just before this step (list with list-steps)

export PATH=/home/kyoupark/cross/install/bin:${PATH}

STOP=binutils_for_host ct-ng build
STOP=cc_core_pass_1 ct-ng build
STOP=libc_start_files V=2 ct-ng build
STOP=cc_core_pass_2 ct-ng build

// RESTART=cc_core_pass_2 ct-ng build
// 
// [ERROR]  You asked to restart a non-restartable build
// [ERROR]  This happened because you didn't set CT_DEBUG_CT_SAVE_STEPS
// [ERROR]  in the config options for the previous build, or the state
// [ERROR]  directory for the previous build was deleted.
// [ERROR]  I will stop here to avoid any carnage


<build-log>
kyoupark@kit-debian:~/cross/work$ ct-ng V=2 build
/home/kyoupark/cross/install/lib/crosstool-ng-1.22.0/scripts/crosstool-NG.sh
[INFO ]  Performing some trivial sanity checks
[INFO ]  Build started 20161129.124349
[INFO ]  Building environment variables
[WARN ]  Directory '/home/kyoupark/src' does not exist.
[WARN ]  Will not save downloaded tarballs to local storage.
[EXTRA]  Preparing working directories
[EXTRA]  Installing user-supplied crosstool-NG configuration
[EXTRA]  =================================================================
[EXTRA]  Dumping internal crosstool-NG configuration
[EXTRA]    Building a toolchain for:
[EXTRA]      build  = i686-pc-linux-gnu
[EXTRA]      host   = i686-pc-linux-gnu
[EXTRA]      target = mips-unknown-linux-uclibc
[EXTRA]  Dumping internal crosstool-NG configuration: done in 0.26s (at 00:02)
[INFO ]  =================================================================
[INFO ]  Retrieving needed toolchain components' tarballs
[INFO ]  Retrieving needed toolchain components' tarballs: done in 0.12s (at 00:02)

[INFO ]  =================================================================
[INFO ]  Extracting and patching toolchain components
[EXTRA]    Extracting 'uClibc-ng-1.0.9'
[INFO ]  Extracting and patching toolchain components: done in 0.12s (at 00:03)

[INFO ]  =================================================================
[INFO ]  Checking C library configuration
[EXTRA]    Manage uClibc configuration
[INFO ]  Checking C library configuration: done in 0.17s (at 00:03)
[INFO ]  =================================================================
[INFO ]  Installing binutils for host
[EXTRA]    Configuring binutils
[EXTRA]    Building binutils
[EXTRA]    Installing binutils
[INFO ]  Installing binutils for host: done in 175.28s (at 08:22)
[INFO ]  =================================================================
[INFO ]  Installing pass-1 core C gcc compiler
[EXTRA]    Configuring core C gcc compiler
[EXTRA]    Building gcc
[EXTRA]    Installing gcc
[EXTRA]    Housekeeping for final gcc compiler
[INFO ]  Installing pass-1 core C gcc compiler: done in 1045.75s (at 25:47)
[INFO ]  =================================================================
[INFO ]  Installing kernel headers
[EXTRA]    Installing kernel headers
[EXTRA]    Checking installed headers
[INFO ]  Installing kernel headers: done in 10.09s (at 25:57)
[INFO ]  =================================================================
[INFO ]  Installing C library headers
[EXTRA]    Copying sources to build dir
[EXTRA]    Applying configuration
[EXTRA]    Building headers
[EXTRA]    Installing headers
[EXTRA]    Building start files
[EXTRA]    Building dummy shared libs
[EXTRA]    Installing start files
[EXTRA]    Installing dummy shared libs
[INFO ]  Installing C library headers: done in 13.40s (at 26:11)
[INFO ]  =================================================================
[INFO ]  Installing pass-2 core C gcc compiler
[EXTRA]    Configuring core C gcc compiler
[EXTRA]    Building gcc
[EXTRA]    Installing gcc
[EXTRA]    Housekeeping for final gcc compiler
[INFO ]  Installing pass-2 core C gcc compiler: done in 1338.26s (at 48:29)
[INFO ]  =================================================================
[INFO ]  Installing C library
[EXTRA]    Copying sources to build dir
[EXTRA]    Applying configuration
[EXTRA]    Building C library
[EXTRA]    Installing C library
[INFO ]  Installing C library: done in 126.24s (at 50:35)
[INFO ]  =================================================================
[INFO ]  Installing final gcc compiler
[EXTRA]    Configuring final gcc compiler
[EXTRA]    Building final gcc compiler
[EXTRA]    Installing final gcc compiler
[EXTRA]    Housekeeping for final gcc compiler
[INFO ]  Installing final gcc compiler: done in 1458.59s (at 74:54)
[INFO ]  =================================================================


={============================================================================
*kt_linux_arch_001* arch-mips

QEMU is a generic open source processor and system emulator. It achieves good
performance from using a Just-in-time compilation. 

https://www.linux-mips.org/wiki/QEMU


={============================================================================
*kt_linux_core_400* lib-shared

LPI 41.

{static-library}
<create> ar-command
The archive also records various attributes of each of the component object
files, including file permissions, numeric user and group IDs, and last
modification time.

r (replace): 
  
Insert an object file into the archive, replacing any previous object file of
the same name. This is the standard method for creating and updating an archive.
Thus, we might build an archive with the following commands:

$ cc -g -c mod1.c mod2.c mod3.c
$ ar r libdemo.a mod1.o mod2.o mod3.o

t (table of contents): 
  
Display a table of contents of the archive. By default, this lists just the
names of the object files in the archive. By adding the v (verbose) modifier, we
additionally see all of the other attributes recorded in the archive for each
object file, as in the following example:

$ ar tv libdemo.a
rw-r--r-- 1000/100 1001016 Nov 15 12:26 2009 mod1.o
rw-r--r-- 1000/100 406668 Nov 15 12:21 2009 mod2.o
rw-r--r-- 1000/100 46672 Nov 15 12:21 2009 mod3.o

<staic-link>
Couple of ways in linking:

1. The first is to name the static library as part of the link command, as in
the following:

$ cc -g -c prog.c
$ cc -g -o prog prog.o libdemo.a

note: 'linktime' search
2. can place the library in one of the `standard directories` searched by the
linker such as /usr/lib, and then specify the library name; the filename of
the library without the lib prefix and .a suffix using the -l option:

$ cc -g -o prog prog.o -ldemo

3. If the library resides in a directory not normally searched by the linker,
  can specify that the linker should search this additional directory using
  the -L option:

$ cc -g -o prog prog.o -Lmylibdir -ldemo

<only-used>
Although a static library may contain many object modules, the linker includes
'only' those modules that the program 'requires'.


{downside-of-static}
1. Duplicates in disk and ram spce.

2. If a change is required perhaps a security or bug fix to an object module
in a static library, then all executables using that module must be relinked
in order to incorporate the change. This disadvantage is further compounded by
the fact that the system administrator needs to be aware of which applications
were linked against the library.


{what-is-shared}
Although the code of a shared library is shared among multiple processes, its
`variables are not` Each process that uses the library has its own copies of
the global and static variables that are defined within the library.


{further-advantages}
* Because overall program size is smaller, in some cases, programs can be
loaded into memory and started more 'quickly'. This point holds true only for
large shared libraries that are already in use by another program.

* Such changes can be carried out even while running programs are using an
existing version of the shared library.


{cost-of-shared}
* Shared libraries are more `complex than static libraries`, both at the
  conceptual level, and at the practical level of creating shared libraries
  and building the programs that use them.

* Shared libraries 'must' be compiled to use position-independent code, which
  has a performance 'overhead' on most architectures because it requires the
  use of an extra register 

* Symbol relocation must be performed at run time. During symbol relocation,
  references to each symbol (a variable or function) in a shared library need
  to be modified to correspond to the actual run-time location at which the
  symbol is placed in virtual memory. Take a little more time to execute.


{position-independent-code}
These changes allow the code to be located at any virtual address at run time.
This is necessary for shared libraries, since there is no way of knowing at link
time where the shared library code will be located in memory.

In order to determine whether an existing object file has been compiled with the
-fPIC option, can check for the presence of the name _GLOBAL_OFFSET_TABLE_ in
the object file's symbol table, using either of the following commands:

$ nm mod1.o | grep _GLOBAL_OFFSET_TABLE_
$ readelf -s mod1.o | grep _GLOBAL_OFFSET_TABLE_         // -s, --syms|--symbols

note: as shown above, this not necessarily means 'shared' object. It's only tell
-fPIC used.

<pic-vs-relocatable>
This is talking about position independent code, which is 'not' the same as
relocatable code. Relocatable code is code whose address may be assigned at
'linktime'. Position independent code is code whose address may be assigned at
'runtime'.

'all' object files are relocatable. For most targets, only object files compiled
with -fpic or -fPIC or -pie are position independent.

For more about relocatable code: http://www.airs.com/blog/archives/41


{create-shared}
$ gcc -g -c -fPIC -Wall mod1.c mod2.c mod3.c
$ gcc -g -shared -o libfoo.so mod1.o mod2.o mod3.o

or 

$ gcc -g -fPIC -Wall mod1.c mod2.c mod3.c -shared -o libfoo.so

Unlike static, it is not possible to add or remove individual object modules
from a previously built shared library. As with normal executables, the object
files within a shared library no longer maintain distinct identities.


{shared-vs-static} do-not-use-single-line

$ cat main.c 
#nclude <stdio.h>

extern void foo(void);

int main(void)
{
  foo();
  return 0;
}

$ cat foo.c 
#include <stdio.h>

void foo(void)
{
  printf("foo: this is foo...\n");
}

$ ls -alR 
drwxr-xr-x 2 kpark kpark 4096 Feb  5 09:34 one
drwxr-xr-x 2 kpark kpark 4096 Feb  5 09:37 two
drwxr-xr-x 2 kpark kpark 4096 Feb  5 09:46 thr

<used-seperated-step> recommended
./one:
-rw-r--r-- 1 kpark kpark   73 Feb  5 09:32 foo.c
-rw-r--r-- 1 kpark kpark 1528 Feb  5 09:33 foo.o
-rwxr-xr-x 1 kpark kpark 6407 Feb  5 09:33 libfoo.so
-rw-r--r-- 1 kpark kpark   82 Feb  5 09:32 main.c
-rwxr-xr-x 1 kpark kpark 7120 Feb  5 09:34 one


$ gcc -c -fpic foo.c 
$ gcc -shared -o libfoo.so foo.o
$ gcc -o one main.c libfoo.so 

$ ./one 
./one: error while loading shared libraries: libfoo.so: cannot open shared
  object file: No such file or directory

$ readelf -d one | grep NEEDED
:4: 0x0000000000000001 (NEEDED)             Shared library: [libfoo.so]
:5: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ file libfoo.so 
libfoo.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), 
  dynamically linked,
  BuildID[sha1]=0x7c5aa43368b2a98af48a1558d7e2c5a010d238b0, not stripped

$ readelf -s libfoo.so | grep _OFFSET_TA
(standard input):64:    43: 0000000000200978     0 OBJECT  LOCAL  DEFAULT  \
                   ABS _GLOBAL_OFFSET_TABLE_

note: no SONAME is involved here and use a realname since so do not have SONAME.

$ nm one | grep foo
(standard input):32:                 U foo


<used-single-step>
./two:
-rw-r--r-- 1 kpark kpark   73 Feb  5 09:32 foo.c
-rw-r--r-- 1 kpark kpark 1528 Feb  5 09:37 libfoo.so
-rw-r--r-- 1 kpark kpark   82 Feb  5 09:32 main.c
-rwxr-xr-x 1 kpark kpark 6872 Feb  5 09:37 two

$ gcc -c -fpic foo.c -shared -o libfoo.so
$ gcc -o two main.c libfoo.so

$ ./two 
foo: this is foo...

$ readelf -d two | grep NEEDED
4: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ readelf -s libfoo.so | grep _OFFSET_TA
(standard input):14:    10: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  \
                   UND _GLOBAL_OFFSET_TABLE_

$ file libfoo.so 
libfoo.so: ELF 64-bit LSB 'relocatable', x86-64, version 1 (SYSV), not stripped

note: what does it mean? same as static link? looks like "libfoo.so" is not
shared object.

$ nm two | grep foo
(standard input):32:000000000040051c T foo


<used-static-build>
./thr:
-rw-r--r-- 1 kpark kpark   73 Feb  5 09:45 foo.c
-rw-r--r-- 1 kpark kpark 1480 Feb  5 09:46 foo.o
-rw-r--r-- 1 kpark kpark   82 Feb  5 09:45 main.c
-rwxr-xr-x 1 kpark kpark 6872 Feb  5 09:46 thr

$ gcc -c foo.c -o foo.o 
$ gcc -o thr main.c foo.o 

$ ./thr 
foo: this is foo...

$ file foo.o 
foo.o: ELF 64-bit LSB 'relocatable', x86-64, version 1 (SYSV), not stripped

$ readelf -s foo.o | grep _OFFSET_TA
$ readelf -d thr | grep NEEDED
4: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ nm thr | grep foo
(standard input):32:000000000040051c T foo


={============================================================================
*kt_linux_core_400* lib-how-static-works

In a try to use DUMA which replace malloc calls, should I have to change all
sources which uses malloc calls to include "duma.h" and to recompile all?

How can be sure that duma_malloc is used in all static libraries used to build
a final binary?

// mod.c
// which do not include duma.h and not recompiled.

#include <stdio.h>

void func_from_mod()
{
    printf("func_from_mod: calls malloc and use it\n");
    char *p = (char*)malloc(10*sizeof(char));
    p[0] = 'm';
    p[1] = 'o';
    p[2] = 'd';
    p[3] = 0;
    printf("func_from_mod: p = %s\n", p );
    free(p);
    p[0] = 'm';
    printf("func_from_mod: return \n");
    return 0;
}


// testmain.c

#include <stdio.h>

func_from_mod();

See that replacing malloc calls with duma ones works when links with duma
library but not with a header inclusion.

$ nm mod.o
                 U free
0000000000000000 T func_from_mod
                 U malloc
                 U printf
                 U puts

See that malloc is `external undefined`


When use no duma, see `undefined but from glibc`

$ gcc -g testmain.c mod.host.o -o out_duma_host
$ nm out_duma_host | grep malloc
44:                 U malloc@@GLIBC_2.2.5


When include duma header, see compile errors:

$ gcc -g -DUSE_DUMA testmain.c mod.host.o -o out_duma_host

/testmain.c:33: undefined reference to `_duma_malloc'
/testmain.c:36: undefined reference to `_duma_free'


When not include duma header which causes to define global malloc in duma
library so see `defined` All malloc calls in the binary uses malloc from duma
by rebuilding one module and relinking the binary.

$ gcc -g testmain.c mod.host.o libduma.a.for.buildsvr -lpthread -o out_duma_host
$ nm out_duma_host | grep malloc
59:0000000000403451 T _duma_malloc
111:0000000000403e31 T malloc


See its working

=== DUMA: KIT: _duma_allocate
func_from_mod: p = mod
Segmentation fault (core dumped)


={============================================================================
*kt_linux_core_400* lib-dependancy

To see library dependancy of a application. This is a script file and shows a
`dynamic-library-dependancy`. If not, shows 'not a dynamic executable' message.

The simplest approach is to pick a binary that you consider is typical (e.g.
    /bin/ls and run ldd on it. One of the listed libraries should be libc -
    check its version number.

$ ldd /bin/ls
        libc.so.6 => /lib/libc.so.6 (0x4000e000)
        /lib/ld-linux.so.2 => /lib/ld-linux.so.2 (0x40000000)


<case>
The ldd is script. Although the binary uses shared library, ldd don't
understand it since it's cross compiled.

$ readelf -d nexus-inspect | grep NEED
(standard input):4: 0x00000001 (NEEDED)  Shared library: [libnexus.so]
(standard input):5: 0x00000001 (NEEDED)  Shared library: [libgcc_s.so.1]
(standard input):6: 0x00000001 (NEEDED)  Shared library: [libpthread.so.0]
(standard input):7: 0x00000001 (NEEDED)  Shared library: [libc.so.0]

$ ldd -v nexus-inspect
	not a dynamic executable


{shows-if-can-load-dependency}
This is result of target ldd version.

root# ldd /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so
checking sub-depends for 'not found' ~
checking sub-depends for '/usr/local/lib/libnexus.so'
checking sub-depends for '/opt/zinc/oss/lib/libgstbase-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgstmpegts-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgstreamer-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgobject-2.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libglib-2.0.so.0'
checking sub-depends for '/lib/libgcc_s.so.1'
checking sub-depends for '/lib/libpthread.so.0'
checking sub-depends for '/lib/libc.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgmodule-2.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libintl.so.8'
checking sub-depends for '/lib/libdl.so.0'
checking sub-depends for '/lib/libm.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libffi.so.5'
  libnexusMgr.so.0 => not found (0x00000000) ~
  libnexus.so => /usr/local/lib/libnexus.so (0x00000000)
  libgstbase-1.0.so.0 => /opt/zinc/oss/lib/libgstbase-1.0.so.0 (0x00000000)
  libgstmpegts-1.0.so.0 => /opt/zinc/oss/lib/libgstmpegts-1.0.so.0 (0x00000000)
  libgstreamer-1.0.so.0 => /opt/zinc/oss/lib/libgstreamer-1.0.so.0 (0x00000000)
  libgobject-2.0.so.0 => /opt/zinc/oss/lib/libgobject-2.0.so.0 (0x00000000)
  libglib-2.0.so.0 => /opt/zinc/oss/lib/libglib-2.0.so.0 (0x00000000)
  libgcc_s.so.1 => /lib/libgcc_s.so.1 (0x00000000)
  libpthread.so.0 => /lib/libpthread.so.0 (0x00000000)
  libc.so.0 => /lib/libc.so.0 (0x00000000)
  libgmodule-2.0.so.0 => /opt/zinc/oss/lib/libgmodule-2.0.so.0 (0x00000000)
  libintl.so.8 => /opt/zinc/oss/lib/libintl.so.8 (0x00000000)
  libdl.so.0 => /lib/libdl.so.0 (0x00000000)
  libm.so.0 => /lib/libm.so.0 (0x00000000)
  libffi.so.5 => /opt/zinc/oss/lib/libffi.so.5 (0x00000000)
  not a dynamic executable


{libc-case}
#include <stdio.h>

int main()
{
    printf("Hello, this is sample program.\n");
}

gcc sample.c


<on-debian-vm>
@kit-debian:~$ readelf -d a.out | grep NEEDED
4: 0x00000001 (NEEDED)                     Shared library: [libc.so.6]

@kit-debian:~$ ldd a.out 
	linux-gate.so.1 (0xb77b5000)
	libc.so.6 => /lib/i386-linux-gnu/i686/cmov/libc.so.6 (0xb75e9000)
	/lib/ld-linux.so.2 (0xb77b8000)


<on-mips-cross-gcc>
// same when use mips-unknown-linux-uclibc-readelf

@kit-debian:~$ readelf -d a.out | grep NEEDED
4: 0x00000001 (NEEDED)                     Shared library: [libc.so.1]

@kit-debian:~$ ldd a.out
	not a dynamic executable

note:
mips-unknown-linux-uclibc-ldd is a script and not part of binutil build.

@kit-debian:~$ ./x-tools/mips-unknown-linux-uclibc/bin/mips-unknown-linux-uclibc-ldd \
  --root ./x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/sysroot/ a.out 
        libc.so.1 => /lib/libc.so.1 (0xdeadbeef)
        ld-uClibc.so.1 => /lib/ld-uClibc.so.1 (0xdeadbeef)

// x-tools/mips-unknown-linux-uclibc/mips-unknown-linux-uclibc/lib
lrwxrwxrwx 1 kyoupark ccusers       18 Dec  2 10:17 libc.so.1 -> libuClibc-1.0.9.so*
-r-xr-xr-x 1 kyoupark ccusers   612272 Dec  2 10:17 libuClibc-1.0.9.so*


={============================================================================
*kt_linux_core_400* lib-dl ld-library-path

When an executable starts, the `dynamic-linker` loads 'all' of the shared
libraries in the program's dynamic dependency list. Sometimes, however, it can
be useful to load libraries at a 'later' time. For example, a plug-in is loaded
only when it is needed. This functionality is provided by an API to the dynamic
linker.

{dynamic-linking}
The dynamic linking, which is the task of resolving the embedded library name at
runtime. This task is performed by the dynamic linker (also called the dynamic
    linking loader or the runtime linker). The dynamic linker is 'itself' a
'shared' library, named /lib/ld-linux.so.2, which is employed by every ELF
executable that uses shared libraries.

<host>
$ ls -l /lib/ld-linux.so.2 
lrwxrwxrwx 1 root root 25 Oct 17 00:50 /lib/ld-linux.so.2 
  -> i386-linux-gnu/ld-2.13.so

<target>
# ls -al /lib/ld-linux.so.2 
24 Jan  1  1970 /lib/ld-linux.so.2 -> /lib/ld-uClibc-0.9.29.so

<target>
# ls -al /lib/ld-uClibc*
root     root         31760 Jul 31 11:10 /lib/ld-uClibc-0.9.32.1.so*
root     root            21 Aug  5 08:50 /lib/ld-uClibc.so 
  -> ld-uClibc-0.9.32.1.so*
root     root            21 Aug  5 08:50 /lib/ld-uClibc.so.0 
  -> ld-uClibc-0.9.32.1.so*

note:
Every program-including those that use shared libraries-goes through a
static-linking phase. At run time, a program that employs shared libraries
additionally undergoes dynamic linking.


{dynamic-linker}
note: From GCC doc, the dynamic loader is not part of GCC; it is part of the
operating system.

Two steps must occur that are not required for programs that use static
libraries:

$ gcc -c -fpic foo.c 
$ gcc -shared -o libfoo.so foo.o
$ gcc -o one main.c libfoo.so          <embedding-the-name-of-lib>

1. At 'linktime'. Since the executable file 'no' longer contains copies of the
object files that it requires, must have some mechanism for identifying the
shared library that it needs at runtime.

This is done by 'embedding' the name of the shared library inside the executable
during the link phase. DT_NEEDED tag in ELF.

$ readelf -d one | grep NEED
0x00000001 (NEEDED)  Shared library: [libfoo.so]    // note: "libfoo"
0x00000001 (NEEDED)  Shared library: [libc.so.6]

note: In order to embed the shared library in a executable that use so, requires
'so' file in linking.

2. At 'runtime', there must be some 'mechanism' for 'resolving' the embedded
library name-that is, for finding the shared library file corresponding to the
name specified in the executable file-and then loading the library into memory,
     if it is not already present.


{ld-library-path}
Some of these rules specify a set of 'standard' directories in which shared
libraries normally reside. To inform the dynamic linker that a shared library
resides in a nonstandard directory

If LD_LIBRARY_PATH is defined, then the dynamic linker searches for the shared
library in the directories it lists before looking in the standard library
directories.

note: Creates an environment variable definition 'within' the process executing
prog. Since if you don't export the changes to an environment variable, they
won't be inherited by the child processes. The loader and our test program
didn't inherit the changes we made.

$ LD_LIBRARY_PATH=. ./one 
foo: this is foo...

$ ./one 
./one: error while loading shared libraries: libfoo.so: cannot open shared 
  object file: No such file or directory

$ export LD_LIBRARY_PATH=.
$ ./one 
foo: this is foo...


{soname-and-realname}
In the above example, libfoo is realname. As with a realname, soname is used in
linking and is embedded in the executable, and used by the linker at runtime.

Why soname? The purpose of the soname is to provide a level of 'indirection'
that permits an executable to use, at runtime, a version of the shared library
that is different from but compatible with the library against which it was
linked.

$ gcc -c -fpic foo.c

$ gcc -shared -Wl,-soname,libbar.so -o libfoo.so foo.o      
note: shall no space between -soname

$ gcc -shared -Wl,-soname -Wl,libbar.so -o libfoo.so foo.o  

$ readelf -d libfoo.so | grep SONAME              note: DT_SONAME in ELF
(standard input):5: 0x000000000000000e (SONAME)   Library soname: [libbar.so]

$ gcc -o one main.c libfoo.so

$ readelf -d one | grep NEED 
:4: 0x0000000000000001 (NEEDED)             Shared library: [libbar.so]
:5: 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]

$ ./one 
./one: error while loading shared libraries: libbar.so: cannot open shared 
  object file: No such file or directory

$ LD_LIBRARY_PATH=. ./one
./one: error while loading shared libraries: libbar.so: cannot open shared 
  object file: No such file or directory

note: Why fails this time? So set soname in so file itself and executable embeds
it instead of realname. Now linker ld looks for libbar instead but not libfoo.

<indirection>
When using a soname, one further step is required: we must create a symbolic
link from the soname to the real name of the library. This symbolic link must be
created in one of the directories searched by the dynamic linker. Thus, we could
run our program as follows:

$ ln -s libfoo.so libbar.so 
$ ll
lrwxrwxrwx 1 kpark kpark    9 Feb  5 11:22 libbar.so -> libfoo.so*
$ ./one 
foo: this is foo...


{versioning}
<requirement>
The same calling interface and are semantically equivalent (they achieve
    identical results). Such differing but compatible versions are referred to
as 'minor' versions of a shared library.

Occasionally, however, it is necessary to create a new 'major' version of a
library-one that is incompatible with a previous version. At the same time, it
must still be possible to continue running programs that require the older
version of the library.

<realname>
libdemo.so.1.0.1
libdemo.so.1.0.2        Minor version, compatible with version 1.0.1
libdemo.so.2.0.0        New major version, incompatible with version 1.*

<soname> minor-independent link
The soname of the shared library includes the same 'major' version identifier as
its corresponding real library name, but excludes the minor version identifier.
Thus, the soname has the form libname.so.major-id.

Usually, the soname is created as a relative symbolic link in the directory that
contains the real name. The following are some examples of sonames, along with
the real names to which they might be symbolically linked:

libdemo.so.1   -> libdemo.so.1.0.2
libdemo.so.2   -> libdemo.so.2.0.0

Normally, the soname corresponding to each major library version points to the
most recent 'minor' version within the major version. This setup allows for the
correct versioning semantics during the runtime operation of shared libraries. 

Because the static-linking phase embeds a copy of the soname in the executable
which has major, and the soname symbolic link may subsequently be modified to
point to a newer (minor) version of the shared library, it is possible to ensure
that an executable loads the most up-to-date minor version of the library at
runtime.

Furthermore, since different major versions of a library have different sonames,
  they can happily coexist and be accessed by the programs that require them.

note: Due to this indirection, possible to meet versioning requirement by
changing a sym link. After all, soname is to have 'indirection' to minor
version.

<linkername> version-independent link
The linker name, which is used when 'linking' an executable against the shared
library. The linker name is a symbolic link containing just the library name and
thus has the form libname.so. The linker name allows us to construct
version-independent link commands that automatically operate with the correct
version of the shared library.

libdemo.so ->  libdemo.so.2


real name(minor)   <-   soname(major)         <-   linker name         
libdemo.so.1.0.1        libdemo.so.1               libdemo.so
(regular file)          (symbolic link)            (symbolic link)
lib name.so.maj.min     libname.so.maj
Object code for         
library modules


$ gcc -g -c -fPIC -Wall mod1.c mod2.c mod3.c

created a realname and soname

$ gcc -g -shared -Wl,-soname,libdemo.so.1 -o libdemo.so.1.0.1 mod1.o mod2.o mod3.o

$ ln -s libdemo.so.1.0.1 libdemo.so.1
$ ln -s libdemo.so.1 libdemo.so

$ ls -l libdemo.so* | awk '{print $1, $9, $10, $11}'
lrwxrwxrwx libdemo.so -> libdemo.so.1
lrwxrwxrwx libdemo.so.1 -> libdemo.so.1.0.1
-rwxr-xr-x libdemo.so.1.0.1

$ gcc -g -Wall -o prog prog.c -L. -ldemo        
$ gcc -g -Wall -o prog prog.c libdemo.so        
// as with exmple $ gcc -o one main.c libfoo.so

note: -ldemo or libdemo.so is linkername and use is in static linking stage.

$ LD_LIBRARY_PATH=. ./prog
Called mod1-x1
Called mod2-x2

So three cases:

o the case when linkername is realname. Means that no soname is used. 
o the case when linkername is soname. This is one-level indirection. 
o the case when linkername is different from soname. This is two-level
indirection.

note: Even when linkername is different from soname, executable 'always' embeds
'soname' and the linkername is a name to use linking stage that must be exist
before doing linking. Checked with the above one example. 

Using two names and two indirections enable us to change major and minor libs to
use. 

When major changes, changes linkname link: need to link again 

libdemo.so -> libdemo.so.1          =>    libdemo.so -> libdemo.so.2

When minor changes, change soname link: no need to link.

libdemo.so.1 -> libdemo.so.1.0.1    =>    libdemo.so.1 -> libdemo.so.1.0.1

note: The above is convention but not mandatory so can have followings:

17 Dec 29 13:08 liblog4c.so -> liblog4c.so.3.1.0*
17 Dec 29 13:08 liblog4c.so.3 -> liblog4c.so.3.1.0*


={============================================================================
*kt_linux_core_401* slib: --as-needed flag and link error

<manual>
https://sourceware.org/binutils/docs/ld/Options.html

--as-needed 
--no-as-needed

This option affects ELF DT_NEEDED tags for 'dynamic' libraries mentioned on the
command line 'after' the --as-needed option. 

<default>
Normally the linker will add a DT_NEEDED tag for each dynamic library mentioned
on the command line, 'regardless' of whether the library is actually needed or
not.

The --as-needed causes a DT_NEEDED tag to 'only' be emitted for a library that
"at that point in the link" satisfies a non-weak undefined symbol reference from
a regular object file or, if the library is not found in the DT_NEEDED lists of
other needed libraries, a non-weak undefined symbol reference from another
needed dynamic library. 

Object files or libraries appearing on the command line after the library in
question do not affect whether the library is seen as needed. This is similar to
the rules for extraction of object files from archives. --no-as-needed restores
the 'default' behaviour. 

<example>
$ gcc -c -fpic foo.c 
$ gcc -shared -o libfoo.so foo.o
$ gcc -o one main.c libfoo.so          <embedding-the-name-of-lib>

or 

$ gcc -L. -lfoo -o one main.c

$ nm libfoo.so | grep foo
(standard input):24:00000550 T foo

$ nm one | grep foo
(standard input):33:         U foo

$ readelf -d one | grep NEEDED
(standard input):4: 0x00000001 (NEEDED)   Shared library: [libfoo.so]
(standard input):5: 0x00000001 (NEEDED)   Shared library: [libc.so.6]


However, when tried:

$ gcc -Wl,--as-needed -L. -lfoo -o one main.c
/tmp/ccUshwwe.o: In function `main':
main.c:(.text+0x7): undefined reference to `foo'
collect2: error: ld returned 1 exit status

The followings are okay.

$ gcc -L. -lfoo -Wl,--as-needed -o one main.c
$ gcc -Wl,--as-needed -o one main.c libfoo.so

note: WHY this fails to link even if main really uses foo()?


<case>
There is an application which is said that it uses one shared library and this
library again uses the other shard library. 

The assumption is: application <- a.so <- b.so.

The problem happens when simply changes application source file to use functions
in a.so and statred to see "undefined symbols" of b.so from the linker. But
those symbols are not used in that application at all and the call that
applicaion uses don't have any reference to b.so as well. However other APIs of
a.so does have reference to b.so. Why is this?

When looked at application and library dependancies for okay case, there was no
reference to a.so library which has undefined symbols defined. Why was it okay
before?

Two problems:

1. Wrong assumption. For okay case, there was actually no reference between
application and a.so. That's why cannot see dependancies.

2. When build the new source, this code introduce a call to a.so and this call
has calls to the b.so. Now application need to know both a.so and b.so. Since
the application is the 'final' in the linking process and now see undefined
symbols which used in a.so but in the b.so. The problem is that b.so is dropped
to link of application due to as-needed option.

application          libMgr                  libFb
reference to Mgr  -> no reference to Fb
                     reference to Fb      ->


When fails:

-Wl,--as-needed -lMgr -lFb ...

/.../usr/local/lib/libMgr.so: undefined reference to `DirectFBSetOption' 
/.../usr/local/lib/libMgr.so: undefined reference to `DirectFBCreate' 
/.../usr/local/lib/libMgr.so: undefined reference to `DirectFBInit' 
  collect2: ld returned 1 exit status

When works:

-o application application.o
-lMgr -lFb ...
-Wl,--as-needed ...

So works fine when move --as-needed after necessary library.


<ex>

// main.c
#include <stdio.h>

extern void foo(void);

int main(void)
{
  // foo(5);
  foo();
  return 0;
}

// foo.c
#include <stdio.h>

extern void boo();

void foo()
{
  printf("foo: this is foo...\n");
  boo();
}

// boo.c
#include <stdio.h>

void boo()
{
  printf("boo: this is boo...\n");
}

$ gcc -c -fpic boo.c 
$ gcc -c -fpic foo.c
$ gcc -shared -o libfoo.so foo.o
$ gcc -shared -o libboo.so boo.o
$ gcc -o one main.c libfoo.so libboo.so

$ readelf -d one | ag NEEDED
 0x00000001 (NEEDED)                     Shared library: [libfoo.so]
 0x00000001 (NEEDED)                     Shared library: [libboo.so]
 0x00000001 (NEEDED)                     Shared library: [libc.so.6]

$ LD_LIBRARY_PATH=. ./one
foo: this is foo...
boo: this is boo...

$ gcc -o one main.c -Wl,--as-needed libfoo.so libboo.so
// same as $ gcc -o one main.c libfoo.so libboo.so

$ gcc -Wl,--as-needed libfoo.so libboo.so -o one main.c 
$ gcc -Wl,--as-needed -L. -lfoo -lboo -o one main.c 
/tmp/cc3ZjAgB.o: In function `main':
main.c:(.text+0x7): undefined reference to `foo'
collect2: error: ld returned 1 exit status

$ gcc -Wl,--as-needed -lfoo -lboo -o one main.c
/usr/bin/ld: cannot find -lfoo
/usr/bin/ld: cannot find -lboo
collect2: error: ld returned 1 exit status


<useful>
http://wiki.gentoo.org/wiki/Project:Quality_Assurance/As-needed#What_is_--as-needed.3F

What is --as-needed?

The --as-needed flag is passed to the GNU linker (GNU ld ). The flag tells the
linker to link in the produced binary only the libraries containing symbols
'actually' used by the binary itself. This binary can be either a final
executable or another library.

In theory, when linking something, only the needed libraries are passed to the
command line used to invoke the linker. But to workaround systems with broken
linkers or not using ELF format, many libraries declare some "dependencies" that
get pulled in while linking. A simple example can be found by looking at the
libraries declared as dependencies by gtk+ 2.0 :

libraries needed to link to gtk+ 2.0

$ pkg-config gtk+-2.0 --libs
-lgtk-x11-2.0 -lgdk-x11-2.0 -latk-1.0 -lgdk_pixbuf-2.0 -lm -lpangocairo-1.0 
-lpango-1.0 -lcairo -lgobject-2.0 -lgmodule-2.0 -ldl -lglib-2.0

If the application is just using functions from gtk+ 2.0, a simple link line
with -lgtk-x11-2.0 should make it build fine, but looking at which libraries are
needed and which are not from a package point of view is often an impossible
task. 

How can --as-needed be useful?

<improve-startup-time>
The use of the --as-needed flag allows the linker to avoid linking extra
libraries in a binary. This not only improves 'startup' times (as the loader does
    not have to load all the libraries for every step) but might avoid the full
initialization of things like KDE's KIO for a binary if it's not using the KIO
framework. 

More importantly, the use of --as-needed 'avoids' adding dependencies to a
binary that are prerequisites of one of its direct or indirect dependencies.
This is important because when a library changes SONAME after an ABI change, all
the binaries directly linking to it have to be 'rebuilt'. 

By linking only the libraries that are actually needed, the breakage due to an
ABI change is reduced. It is particularly useful when the ABI breakage happens
in a library used by some other high level library (like cairo , which is used
    directly by gtk+-2.0 , and gets linked indirectly in applications using the
    latter), as it prevents the rebuild of the final binaries and thus of the
packages carrying them.

It is also useful to check whether the dependencies stated by the documentation
are actually used by a package: it's not impossible that a package checks in a
configure script for some library, and then links to it, but without using it at
all because the code using it was removed or refactored or has not been written. 

<final-linking> only for executable but not library
Failure in final linking, undefined symbols

This is the most common error that happens while using --as-needed. It happens
during the final linking stage of an executable note: libraries don't create
problems, because they are allowed to have undefined symbols. The executable
linking stage dies because of an undefined symbol that is present in one of the
libraries fed to the command line. However, the library is not used by the
executable itself, thus it gets 'removed' by --as-needed.

This usually means that a library was not linked to another library, but was
using it, and then 'relying' on the final executable to link them together. This
behavior is also an extra encumbrance on developers using that library because
they have to check for the requirements.

The fix to this kind of problem is usually simple: just find which library
provides the symbols and which one is requiring them (the error message from the
    linker should contain the name of the latter). Then make sure that when the
library is linked from the source files it's also linked to the first. 


={============================================================================
*kt_linux_core_402* shared-lib-search and resolve

{search-rules}
note: >
LD_LIBRARY_PATH is broken and should 'not' be used if at all possible since
LD_LIBRARY_PATH is great for quick tests and for systems on which you don't have
admin privileges. However, as a downside, exporting the LD_LIBRARY_PATH variable
means it may cause problems with other programs if you don't reset it to its
previous state when you're done.


The dynamic linker searches for the shared library using the following rules:

1. If the executable has any directories listed in its DT_RPATH run-time library
path list (rpath) and the executable does not contain a DT_RUNPATH list, then
these directories are searched (in the order that they were supplied when
    linking the program).

2. If the LD_LIBRARY_PATH environment variable is defined, then each of the
colon-separated directories listed in its value is searched in turn. 

<security>
If the executable is a set-user-ID or set-group-ID program, then LD_LIBRARY_PATH
is ignored. This is a security measure to prevent users from tricking the
dynamic linker into loading a private version of a library with the same name as
a library required by the executable.

3. If the executable has any directories listed in its DT_RUNPATH run-time
library path list, then these directories are searched (in the order that they
    were supplied when linking the program).

4. The file /etc/ld.so.cache is checked to see if it contains an entry for the
library.

5. The directories /lib and /usr/lib are searched (in that order).

<rpath>
There is a third way: during the static editing phase, we can insert into the
executable a list of directories that should be searched at run time for shared
libraries. This is useful if we have libraries that reside in fixed locations
that are not among the standard locations searched by the dynamic linker. To do
this, we employ the -rpath linker option when creating an executable:

$ gcc -g -Wall -Wl,-rpath,/home/mtk/pdir -o prog prog.c libdemo.so

note: From ld linker docs:

2. Any directories specified by -rpath options. The difference between -rpath
and -rpath-link is that directories specified by -rpath options are included in
the executable and used at runtime, whereas the -rpath-link option is only
effective at link time. Searching -rpath in this way is only supported by native
linkers and cross linkers which have been configured with the --with-sysroot
option.


{runtime-resolution}
Suppose that a global symbol (i.e., a 'function' or variable) is defined in
multiple locations, such as in an executable and in a shared library, or in
'multiple' shared libraries. How is a reference to that symbol resolved?

         prog                                   libfoo.so
-----------------------------          -----------------------------
xyz() {                                xyz() {               
  printf("main-xyz\n");                  printf("foo-xyz\n");
}                                      }
                                       
main() {                               func() {
  func();    -->                         xyz();                
}                                      }       

$ gcc -g -c -fPIC -Wall -c foo.c
$ gcc -g -shared -o libfoo.so foo.o
$ gcc -g -o prog prog.c libfoo.so
$ LD_LIBRARY_PATH=. ./prog
main-xyz

See that the definition of xyz() in the main program overrides the one in the
shared library.

The following semantics apply:

o A definition of a global symbol in the 'main' program 'overrides' a definition
in a library.

<resolve-order>
o If a global symbol is defined in 'multiple' libraries, then a reference to
that symbol is bound to the 'first' definition found by scanning libraries in
the left-to-right 'order' in which they were listed on the static 'link' command
line.

<problem>
Although these semantics make the transition from static to shared libraries
relatively straightforward, they can cause some problems. The most significant
problem is that these semantics conflict with the model of a shared library as
implementing a 'self'-contained subsystem. 

By default, a shared library can 'not' guarantee that a reference to one of its
own global symbols will actually be bound to the library's definition of that
symbol.


={============================================================================
*kt_linux_core_403* slib-case-problem in name resolve. crash

03/07/2014. At samsung. 
When moves application which was a process and uses static link to the shared
library application to be used by other processes. The codes which works well
before starts to fail since crash happens when try to create a thread using
custom thread library. 

Problem of linking? Problem of the thread library when used in shared library
application? Somehow linker picks up the wrong libaray since a debugger shows
odd address when thread creation call is made and causes a crash? 

Tried various directions and spent many days. Eventually, found out that
PCThread::Create() is a problem and works fine when changes parameter orders
which is different from the call the application believes to uses. 

The problem was that the process loads a lot of shared library and one of those
has the same PCThread class in it but different signature. When our library make
a call, it picks it up from the other shared library in which has different
signature so crashes. Sovled when wraps PCThread class with a namespace.

note: So this was a problem of symbols between shared libraries.

<Q> so when say 'symbol' in runtime resolution, does it mean 'name' only or the
whole signature?

// foo.c

#include <stdio.h>

void foo()           <1>
void foo(int x)      <2>
{
  printf("foo: this is foo... %d \n", x );
}

// main.c

#include <stdio.h>

// extern void foo(void);
void foo()
{
  printf("main: this is foo...\n");
}

int main(void)
{
  foo();       <1>
  foo(5);      <2>
  return 0;
}

$ gcc -c -fpic foo.c
$ gcc -shared -o libfoo.so foo.o
$ gcc -o prog main.c libfoo.so 
$ LD_LIBRARY_PATH=. ./prog
main: this is foo...

$ gcc -c -fpic foo.c
$ gcc -shared -o libfoo.so foo.o
$ gcc -o prog main.c libfoo.so 
$ LD_LIBRARY_PATH=. ./prog
main: this is foo...

note: 
* Unlike the case-example, this is a problem between main and shared library.

* So always picks up the main version and suggest that only use 'name' in C.
  Given the real-case above, C++ may be different?

* If multiple shared library are used in a process, all constitute a single
  lookup space? NO. As noted above, symbols in main overrides others.


<solution> to the case-example

* As the case-example, can use namespace which effectively make a different
  symbol.

* As shown here, can use linker option, `-Bsymbolic` to ensure that the
  invocation of the same symbol 'in' the shared library actually called the
  version of the function defined 'within' the library.

  $ gcc -shared -Wl,-Bsymbolic -o libfoo.so foo.o

  This shows the same main's foo version.

note: this means that main version gets called for the above example but would
solve the case-example since it force to pick up the one in the same library.


={============================================================================
*kt_linux_core_404* slib: case problem in name resolve. pthread stub 

{description}
2015.05. YouView.
When uses a wrapper process which in short exec application given, hangs on
application launch and appears hang on this wrapper when looks at top and call
stack from gdb. This wrapper has other works but not relavent in this problem.

The odd thing is that it only happens on the different hardware platform with
the same wrapper source.

shell exec -> wrapper -> application      // fail and hang
shell exec -> application                 // okay

#0 0x2ab64fb0 in pthread_cond_init () from /lib/libc.so.0
#1 0x2ae12d50 in global constructors keyed to FutureContextBase.cpp () 
  from /opt/zinc-trunk/lib/libZincCommon.so.0
#2 0x2ae35ed4 in __do_global_ctors_aux () 
  from /opt/zinc-trunk/lib/libZincCommon.so.0
#3 0x2adcc59c in ?? () from /opt/zinc-trunk/lib/libZincCommon.so.0


Since known that pthread stub from glibc was used from call stack and found that
works okay when specify pthread library in LD_PRELOAD explicitly, looked at
library dependancy and found difference between two platforms:

for not working platform
: wrapper                    
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]

: libxx which is preloaded before application
 0x00000001 (NEEDED)                     Shared library: [libTitaniumUtils.so.0]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]

: application
 ...
 0x00000001 (NEEDED)                     Shared library: [libdirect-1.4.so.15]
 0x00000001 (NEEDED)                     Shared library: [libpthread.so.0]
 0x00000001 (NEEDED)                     Shared library: [libdl.so.0]
 ...

for working platforms

: libxx which is preloaded before application
 ...
 0x00000001 (NEEDED)                     Shared library: [libstdc++.so.6]
 0x00000001 (NEEDED)                     Shared library: [libm.so.0]
 0x00000001 (NEEDED)                     Shared library: [libpthread.so.0]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]


So "libxx" has libpthread for working case and thought this explains the problem
since the stub was used for not working case. While having problem to mess with
as-needed in the build system and libtool, eventually turned out that the
problem happens regardless of pthread so in libxx.


<ld-bug>
The vendor investigation is:

o dynamic loader for working platform is more powerful. for not working,
  ld-0.9.29.so, for working, ld-0.9.32.1.so

o libpthread in working platform is changed to meet the require of dynamic
loader.

When libpthread.so is not included in LD_PRELOAD and on working platform, 

__dl_runtime_resolve will finish the relocation, for example before invoking
pthread_cond_init, the symbol still belongs to libc, but we really invoking
pthread_cont_init, the pthread_cond_init funciton will be relocated to
libpthread.  this is implemented by __dl_runtime_resolve, of course
__dl_runtime_resolve is invoked by dynamic loader.

If open the log of dynamic loader, we can see this:

resolve function: pthread_cond_init
patched 0x77a313e0 ==> 0x77cbfa60 @ 0x77a6cef8 (reloacated)

However, we can't see this log for not working platform.


<conclusion>
This is a problem of ld which fails to relocate symbols. This is vendor's
conclusion but the same condition such as the same ld version and others works
okay on the other vendor's platform. Believe that it's uclibc bug and it is
fixed in the latest and the other vendor patched the old version.

int __pthread_cond_init (pthread_cond_t *cond, const pthread_condattr_t *cond_attr) 
{ 
if (__libc_pthread_functions.ptr___pthread_cond_init == ((void *)0)) 
	return 0; 

// {
// added by me to test if 
if (__libc_pthread_functions.ptr___pthread_cond_init == __pthread_cond_init)
	return 0;
// }

return __libc_pthread_functions.ptr___pthread_cond_init (cond, cond_attr); 
}


<glibc-stub>
http://stackoverflow.com/questions/21092601/is-pthread-in-glibc-so-implemented-by-weak-symbol-to-provide-pthread-stub-functi

I know there is pthread.so to provide the functions similar with pthread in
glibc.so somebody said pthread in glibc provide stub only and will be replace
when explicit linking to lpthread.o my question is how to support it? using
weak symbol or other tech?


Yes, glibc uses a stub implementation of various pthread functions, so that
single threaded programs do not have to waste cycles doing things like locking
and unlocking mutexes, and yet do not have to link to a different C library
(like what is done in the Microsoft world, for instance).

For instance, according to POSIX, every time you call fputc(ch, stream), there
is mutex lock and unlock. If you don't want that, you call fputc_unlocked. But
when you do that, you're using a POSIX extension related to threading; it's not
an appropriate workaround for programs that don't use POSIX or don't use the
threading API.

The overriding of the stub pthread functions with the real ones (in the dynamic
    glibc) is not based on weak symbols. The shared library mechanism makes it
possible to override non-weak definitions.


note:
Weak symbols are a mechanism which allows for symbol overriding under 'static'
linking.

If you want a source for the above statement, here it is:

"Note that a definition in a DSO being weak has no effects. Weak definitions
only play a role in static linking." [Ulrich Drepper, "How To Write Shared
Libraries"].  http://www.akkadia.org/drepper/dsohowto.pdf

If you run nm on the static glibc on your system (if you have one), libc.a, you
will note that functions like pthread_mutex_lock are marked weak. In the dynamic
version, libc.so.<whatetever>, the functions are not marked weak.

Note: you should use nm -D or nm --dynamic to look at the symbols in a shared
library. nm will not produce anything on a shared library that is stripped. If
it does, you're looking at the debug symbols, not the dynamic symbols.


<from-embedded-libc>
root# ls -al /lib/libc.so.0
lrwxrwxrwx    1 root     root            19 Jan  1  1970 
  /lib/libc.so.0 -> libuClibc-0.9.29.so

$ nm -D libc.so.0 | ag pthread
000574b0 T __libc_pthread_init
00056d1c T __pthread_attr_init_2_1
00056f58 T __pthread_cond_broadcast
00056f84 T __pthread_cond_destroy
00056fb0 T __pthread_cond_init
00056fdc T __pthread_cond_signal
00057034 T __pthread_cond_timedwait
00057008 T __pthread_cond_wait
0005730c T __pthread_exit
         w __pthread_initialize_minimal
         w __pthread_mutex_init
         w __pthread_mutex_lock
         w __pthread_mutex_trylock
         w __pthread_mutex_unlock
         w _pthread_cleanup_pop_restore
         w _pthread_cleanup_push_defer
00056cf0 T pthread_attr_destroy
00056d48 T pthread_attr_getdetachstate
00056da0 T pthread_attr_getinheritsched
00056df8 T pthread_attr_getschedparam
00056e50 T pthread_attr_getschedpolicy
00056ea8 T pthread_attr_getscope
00056d1c W pthread_attr_init
00056d74 T pthread_attr_setdetachstate
00056dcc T pthread_attr_setinheritsched
00056e24 T pthread_attr_setschedparam
00056e7c T pthread_attr_setschedpolicy
00056ed4 T pthread_attr_setscope
00056f58 W pthread_cond_broadcast
00056f84 W pthread_cond_destroy
00056fb0 W pthread_cond_init
00056fdc W pthread_cond_signal
00057034 W pthread_cond_timedwait
00057008 W pthread_cond_wait
00056f00 T pthread_condattr_destroy
00056f2c T pthread_condattr_init
00057060 T pthread_equal
0005708c T pthread_getschedparam
000570e4 T pthread_mutex_destroy
00057110 T pthread_mutex_init
00057168 T pthread_mutex_lock
0005713c T pthread_mutex_trylock
00057194 T pthread_mutex_unlock
000571ec T pthread_mutexattr_destroy
000571c0 T pthread_mutexattr_init
00057218 T pthread_mutexattr_settype
00057244 T pthread_self
00057270 T pthread_setcancelstate
0005729c T pthread_setcanceltype
000570b8 T pthread_setschedparam


={============================================================================
*kt_linux_core_405* slib-dl-api

{dl-apis} LPI42.1

<dlopen>
#include <dlfcn.h>

void *dlopen(const char *libfilename, int flags);

Returns library handle on success, or NULL on error

The dlopen() opens a shared library, returning a handle used by sub-sequent
calls. Loads the shared library named in libfilename into the calling process's
virtual address space and increments the count of open references to the
library.

If the shared library specified by libfilename contains 'dependencies' on other
shared libraries, dlopen() also automatically loads those libraries. This
procedure occurs recursively if necessary. We refer to the set of such loaded
libraries as this library's dependency tree.

RTLD_LAZY

Undefined function symbols in the library should be resolved only as the code is
executed. If a piece of code requiring a particular symbol is not executed, that
symbol is 'never' resolved. Lazy resolution is performed 'only' for function
references; references to variables are always resolved immediately. Specifying
the RTLD_LAZY flag provides behavior that corresponds to the normal operation of
the dynamic linker when loading the shared libraries identified in an
executable's dynamic dependency list.

It is also possible to include further values in flags. The following flags are
specified in SUSv3:

RTLD_GLOBAL

Symbols in this library and its dependency tree are made available for resolving
references in other libraries loaded by this process and also for lookups via
dlsym().


{dlerror}
If we receive an error return from dlopen() or one of the other functions in the
dlopen API, we can use dlerror() to obtain a pointer to a string that indicates
the cause of the error.

#include <dlfcn.h>
const char *dlerror(void);

Returns pointer to error-diagnostic string, or NULL if 'no' error has occurred
since previous call to dlerror()

The dlerror() function returns NULL if no error has occurred since the last call
to dlerror(). We'll see how this is useful in the next section.

<ex>
(void) dlerror(); /* Clear dlerror() */


{dlsym}
#include <dlfcn.h>

void *dlsym(void *handle, char *symbol);

Returns address of symbol, or NULL if symbol is not found

searches a library for a symbol (a string containing the name of a function or
variable) and returns its address.

The value of a symbol returned by dlsym() 'may' be NULL, which is
'indistinguishable' from the "symbol not found" return. 

In order to differentiate the two possibilities, we must call dlerror()
    beforehand (to make sure that any previously held error string is cleared)
    and then if, after the call to dlsym(), dlerror() returns a non-NULL value,
    we know that an error occurred.

<ex>

From dlsym(3):

    Since the value of the symbol could actually be NULL (so that a NULL return
            from dlsym() need not indicate an error), the correct way to test
    for an error is to call dlerror(3) to clear any old error conditions, then
    call dlsym(), and then call dlerror(3) again, saving its return value into a
    variable, and check whether this saved value is not NULL.


typedef bool (*BcmInit)(void);
BcmInit bcmInit = (BcmInit)dlsym(nexusSharedLibHandle, "BcmNexus_Platform_Init");
const char* err = dlerror();

// This assumes that when `dlsym` is NULL, `dlerror` should return some value so
// err should not be NULL. However, dlerror can return NULL when dlsym return
// NULL and this code runs to else-if and can crash since bcmInit() is a call.
//
if (!bcmInit && err) {
    NICKEL_ERROR("dlsym failed: " << err);
} else if (!bcmInit()) { 
    ...
}


Should be:

if (!bcmInit) {
    const char* const err = dlerror();
    ERROR("dlsym failed: " << (err ? err : "(unknown error)"));
} else if (!bcmInit()) {


{dlclose}
The dlclose() function closes a library previously opened by dlopen().


<to-link>
To build programs that use the dlopen API on Linux, we must specify the -ldl
option, in order to link against the libdl library.


{access-symbols-in-main}
Sometimes, it is desirable instead to have x() in shared library invoke an
implementation of y() in the main program. In order to do this, we must make the
(global-scope) symbols in the main program available to the dynamic linker, by
linking the program using the --export-dynamic linker option:

$ gcc -Wl,--export-dynamic main.c (plus further options and arguments)

Equivalently, we can write the following:

$ gcc -export-dynamic main.c

Using either of these options allows a dynamically loaded library to access
global symbols in the main program.


={============================================================================
*kt_linux_core_405* slib-visibility

{visibility}
A well-designed shared library should make visible only those symbols (functions
        and variables) that form part of its specified application binary
interface (ABI). The reasons for this are as follows:

* If the shared library designer accidentally exports unspecified interfaces,
  then authors of applications that use the library may choose to employ these
  interfaces. This creates a 'compatibility' problem for future upgrades of the
  shared library.

  The library developer expects to be able to change or remove any interfaces
  'other' than those in the documented ABI, while the library user expects to
  continue using the same interfaces (with the same semantics) that they
  currently employ.

* During run-time symbol resolution, any symbols that are exported by a shared
  library might interpose definitions that are provided in other shared
  libraries.

  note: this is a problem as <resolve-real-problem>?

* Exporting unnecessary symbols increases the size of the dynamic symbol table
  that must be loaded at runtime. note: also means 'faster' loading time?

All of these problems can be minimized or avoided altogether if the library
designer ensures that only the symbols required by the library's specified ABI
are exported. 

The following techniques can be used to control the export of symbols:

In a C program, we can use the static keyword to make a symbol private to a
source-code module, thus rendering it unavailable for binding by other object
files.

note:
As well as making a symbol private to a source-code module, the static keyword
also has a converse effect. If a symbol is marked as static, then all references
to the symbol 'in' the same source file will be bound to that definition of the
symbol. Consequently, these references won't be subject to run-time
interposition by definitions from other shared libraries in the manner described
in Section 41.12. 

This effect of the static keyword is similar to the `-Bsymbolic` linker option
described in Section 41.12, with the difference that the static keyword affects
a 'single' symbol within a single source file.

@ The GNU C complier, gcc, provides a compiler-specific attribute declaration
that performs a similar task to the static keyword:

void __attribute__ ((visibility("hidden")))
func(void) {
   /* Code */
}

note: see *kt_dev_gcc_110* gcc: attributes

Whereas the static keyword limits the visibility of a symbol to a single source
code file, the hidden attribute makes the symbol available across all source
code files that compose the shared library, but prevents it from being visible
outside the library.

As with the static keyword, the hidden attribute also has the converse effect of
preventing symbol interposition at runtime.

note: there are more ways in LPI 42.3.


={============================================================================
*kt_linux_core_406* slib-preload, debug and monitor ld

{ld-preload}
For testing purposes, it can sometimes be useful to 'selectively' override
functions and other symbols that would normally be found by the dynamic linker
using the search rules. 

To do this, we can define the environment variable LD_PRELOAD as a string
consisting of space-separated or colon-separated names of shared libraries that
should be loaded 'before' any other shared libraries. 

Since these libraries are loaded first, any functions they define will
automatically be used if required by the executable, thus overriding any other
functions of the same 'name' that the dynamic linker would otherwise have
searched for.

$ ./prog
Called mod1-x1 DEMO
Called mod2-x2 DEMO

$ LD_PRELOAD=libalt.so ./prog
Called mod1-x1 ALT      // x1 in libalt.so and changed this only, selectively.
Called mod2-x2 DEMO

<process-and-system-wide>
The LD_PRELOAD environment variable controls preloading on a <per-process>
basis. Alternatively, the file /etc/ld.so.preload, which lists libraries
separated by white space, can be used to perform the same task on a system-wide
basis. Libraries specified by LD_PRELOAD are loaded before those specified in
/etc/ld.so.preload. 

<security>
For security reasons, set-user-ID and set-group-ID programs ignore LD_PRELOAD.

<tip>
You can use the following command to test if the driver (the driver itself just
        dlopen() this lib) is happy to load:

LD_PRELOAD=/opt/zinc/lib/libyouviewrcu.so /bin/true

note:
Why "/bin/true"? This is a simple utility which returns 0(success) and to check
if libraries are loaded fine, run that with simple utility.


{ld-debug}
Sometimes, it is useful to monitor the operation of the dynamic linker in order to know, for
example, where it is searching for libraries. We can use the LD_DEBUG environment variable to do
this. By setting this variable to one (or more) of a set of standard keywords, we can obtain various
kinds of tracing information from the dynamic linker.

If we assign the value help to LD_DEBUG, the dynamic linker displays help information about
LD_DEBUG, and the specified command is not executed:

note: can be any command other 
$ LD_DEBUG=help ls  
Valid options for the LD_DEBUG environment variable are:

  libs        display library search paths
  reloc       display relocation processing
  files       display progress for input file
  symbols     display symbol table processing
  bindings    display information about symbol binding
  versions    display version dependencies
  all         all previous options combined
  statistics  display relocation statistics
  unused      determined unused DSOs
  help        display this help message and exit

To direct the debugging output into a file instead of standard output
a filename can be specified using the LD_DEBUG_OUTPUT environment variable.


$ LD_DEBUG=libs ls
     20177:	find library=libselinux.so.1 [0]; searching
     20177:	 search path=./tls/x86_64:./tls:./x86_64:.		(LD_LIBRARY_PATH)
     20177:	  trying file=./tls/x86_64/libselinux.so.1
     20177:	  trying file=./tls/libselinux.so.1
     20177:	  trying file=./x86_64/libselinux.so.1
     20177:	  trying file=./libselinux.so.1
     20177:	 search cache=/etc/ld.so.cache
     20177:	  trying file=/lib/x86_64-linux-gnu/libselinux.so.1
     20177:	
     20177:	find library=librt.so.1 [0]; searching
     20177:	 search path=./tls/x86_64:./tls:./x86_64:.		(LD_LIBRARY_PATH)
     20177:	  trying file=./tls/x86_64/librt.so.1
     20177:	  trying file=./tls/librt.so.1
     20177:	  trying file=./x86_64/librt.so.1
     20177:	  trying file=./librt.so.1
     ...
     20177:	
install-sh  ltmain.sh  Makefile.am  Makefile.in  missing  src $ 

note: ls command was executed. works on host but not on a target. use strace instead.

The PID value displayed at the start of each line and this is useful if we are monitoring several
processes (e.g., parent and child).

If desired, we can assign multiple options to LD_DEBUG by separating them with commas (no spaces
    should appear). 

<symbol-option>
The output of the symbols option (which traces symbol resolution by the dynamic linker) is
particularly voluminous.

LD_DEBUG is effective both for libraries implicitly loaded by the dynamic linker and for libraries
dynamically loaded by dlopen().

For security reasons, LD_DEBUG is (since glibc 2.2.5) ignored in set-user-ID and set- group-ID
programs.


={============================================================================
*kt_linux_core_407* shared library: further information

Further information

Various information related to static and shared libraries can be found in the ar(1), gcc(1), ld(1),
ldconfig(8), ld.so(8), dlopen(3), and objdump(1) manual pages and in the info documentation for ld
  and readelf. [Drepper, 2004 (b)] covers many of the finer details of writing shared libraries on
  Linux. Further useful information can also be found in David Wheeler's Program Library HOWTO,
which is online at the LDP web site, http://www.tldp.org/. 

The GNU shared library scheme has many similarities to that implemented in Solaris, and therefore it
is worth reading Sun¿s Linker and Libraries Guide (available at http://docs.sun.com/) for further
information and examples. [Levine, 2000] provides an introduction to the operation of static and
dynamic linkers.

Information about GNU Libtool, a tool that shields the programmer from the implementation-specific
details of building shared libraries, can be found online at http://www.gnu.org/software/libtool and
in [Vaughan et al., 2000].

The document Executable and Linking Format, from the Tools Interface Standards committee, provides
details on ELF. This document can be found online at http://refspecs.freestandards.org/elf/elf.pdf.
[Lu, 1995] also provides a lot of useful detail on ELF.


={============================================================================
*kt_linux_core_408* shared library: md5sum

{can-use-on-library-to-check-integrity}
The idea is that can use md5sum on a library to confirm that it uses the exact same compile and link
options. The assumption is that if both party has the same build configuration, the library made
from the same source 'must' have the same md5 checksum. If that is the case, can use the md5
checksum as a quick way to check if both party has and uses the same build configuration.

Is it true?

https://gcc.gnu.org/ml/gcc-help/2010-01/msg00082.html
beaugy.a@free.fr wrote:

    Hi all,
    So, to put it in a nutshell, all my generated objects file are
    identical on dev1 and dev2 and object files contained in my
    convenience libraries are all identical. The only difference
    remaining, before I generate my binary, resides in the generated
    convenience libraries which are not identical, but their contents
    are. So AFAK, this slight difference shall not make the difference. So
    "why does gcc output (MD5 checksum) differs when I build a binary
    using the project object files (*.o) or the project convenience
    libraries (*.a)?" and "what can I do to fix that?".

Your *.o files are proceeded in a different order when on the command line and on the .a archive,
     putting symbols on different addresses, so obviously different binaries are produced.

Even if you do:
gcc 1.o 2.o
And:
gcc 2.o 1.o

you get binaries with different md5.

<example>
This is example from "*kt_dev_gcc_103* gcc link and ld"

$ cat simplefunc.c
int func(int i) {
    return i + 21;
}

$ cat simplemain.c
int func(int);

int main(int argc, const char* argv[])
{
    return func(argc);
}

$ gcc -c simplefunc.c
$ gcc -c simplemain.c
$ gcc simplefunc.o simplemain.o
$ ./a.out ; echo $?
22

:~/work$ nm simplefunc.o
00000000 T func

:~/work$ nm simplemain.o 
         U func
00000000 T main

$ ar r libsimplefunc.a simplefunc.o    // ar rs to skip ranlib command.
$ ranlib libsimplefunc.a
$ gcc simplemain.o -L. -lsimplefunc
$ ./a.out ; echo $?
22

Now we have *.o and *.a files and run the same build and ar command without changes in source. Just
make o and a files.

<o-files-are-the-same>
keitee@debian-keitee:~/work$ ll simplefunc.*
-rw-r--r-- 1 keitee keitee 848 Jan 20 22:15 simplefunc.o
-rw-r--r-- 1 keitee keitee 848 Jan 20 22:06 simplefunc.o.old
keitee@debian-keitee:~/work$ diff simplefunc.o simplefunc.o.old 
keitee@debian-keitee:~/work$ 

<a-files-are-different>
keitee@debian-keitee:~/work$ ll libsimplefunc.a*
-rw-r--r-- 1 keitee keitee 990 Jan 20 22:14 libsimplefunc.a
-rw-r--r-- 1 keitee keitee 990 Jan 20 22:14 libsimplefunc.a.old
keitee@debian-keitee:~/work$ diff libsimplefunc.a libsimplefunc.a.old 
Binary files libsimplefunc.a and libsimplefunc.a.old differ
keitee@debian-keitee:~/work$ 

The md5 checksum result shows the same.

keitee@debian-keitee:~/work$ md5sum simplefunc.o*
3acb728c611c4c936320d64c1b360633  simplefunc.o
3acb728c611c4c936320d64c1b360633  simplefunc.o.old

keitee@debian-keitee:~/work$ md5sum libsimplefunc.a*
87663beba78feef15106bf156a8f6ef3  libsimplefunc.a
02a369c6a4476eea407bec53beaa7b95  libsimplefunc.a.old

So even when make a library from the same object at 'different' time, it will create different
library file.

<Q> However, when do the md5sum on library files created from project build at different time, the
libraries are the same on diff and md5. WHY?

Tried to make a shared library with the same code:

gcc -fpic -shared -o simple.so simplefunc.c
ar rs libsimple.so simple.so

Again, simple.so files are the same but libsimple.so are differ. This means that when run ar, will
have different output files. Then how were the libraries the same for the above case?

<A> The answer is that do not use ar when make a shared library and then will the same md5sum.

-rw-r--r-- 1 kpark kpark 1528 Feb  5 10:02 foo.o
-rw-r--r-- 1 kpark kpark 1528 Feb  5 09:33 foo.o.old
-rwxr-xr-x 1 kpark kpark 6407 Feb  5 10:03 libfoo.so*
-rwxr-xr-x 1 kpark kpark 6407 Feb  5 09:33 libfoo.so.old*

$ diff foo.o foo.o.old 
$ diff libfoo.so libfoo.so.old 
$ md5sum foo.o*
1f87ad103b677a3090707fee9daaea33  foo.o
1f87ad103b677a3090707fee9daaea33  foo.o.old

$ md5sum libfoo.so*
e64fd5c673979f09360178f938e6e1b7  libfoo.so
e64fd5c673979f09360178f938e6e1b7  libfoo.so.old


={============================================================================
*kt_linux_core_409* shared library: inspect dynamic sections

$ readelf -d libgstnexus.so.from.huawei.box 

Dynamic section at offset 0x16c contains 35 entries:
  Tag        Type                         Name/Value
 0x00000001 (NEEDED)                     Shared library: [libnexusMgr.so]
 0x00000001 (NEEDED)                     Shared library: [libnexus.so]
 0x00000001 (NEEDED)                     Shared library: [libgstbase-1.0.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgstmpegts-1.0.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgstreamer-1.0.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgobject-2.0.so.0]
 0x00000001 (NEEDED)                     Shared library: [libglib-2.0.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libpthread.so.0]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]
 0x0000000e (SONAME)                     Library soname: [libgstnexus.so.0]
 0x0000000f (RPATH)                      Library rpath: [/usr/local/lib]
 0x0000001d (RUNPATH)                    Library runpath: [/usr/local/lib]
 ...


$ objdump -p libgstnexus.so.from.huawei.box 

libgstnexus.so.from.huawei.box:     file format elf32-little

...

Dynamic Section:
  NEEDED               libnexusMgr.so
  NEEDED               libnexus.so
  NEEDED               libgstbase-1.0.so.0
  NEEDED               libgstmpegts-1.0.so.0
  NEEDED               libgstreamer-1.0.so.0
  NEEDED               libgobject-2.0.so.0
  NEEDED               libglib-2.0.so.0
  NEEDED               libgcc_s.so.1
  NEEDED               libpthread.so.0
  NEEDED               libc.so.0
  SONAME               libgstnexus.so.0
  RPATH                /usr/local/lib
  RUNPATH              /usr/local/lib

...


note: this is ldd on a target which is executable but ldd on host pc is scripts and do not show
details like this.

root# ldd libgstnexus.so
checking sub-depends for '/usr/local/lib/libnexusMgr.so'
checking sub-depends for '/usr/local/lib/libnexus.so'
checking sub-depends for '/opt/zinc/oss/lib/libgstbase-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgstmpegts-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgstreamer-1.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgobject-2.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libglib-2.0.so.0'
checking sub-depends for '/lib/libgcc_s.so.1'
checking sub-depends for '/lib/libpthread.so.0'
checking sub-depends for '/lib/libc.so.0'
checking sub-depends for '/lib/libstdc++.so.6'
checking sub-depends for '/lib/libm.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libgmodule-2.0.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libintl.so.8'
checking sub-depends for '/lib/libdl.so.0'
checking sub-depends for '/opt/zinc/oss/lib/libffi.so.5'
	libnexusMgr.so => /usr/local/lib/libnexusMgr.so (0x00000000)
	libnexus.so => /usr/local/lib/libnexus.so (0x00000000)
	libgstbase-1.0.so.0 => /opt/zinc/oss/lib/libgstbase-1.0.so.0 (0x00000000)
	libgstmpegts-1.0.so.0 => /opt/zinc/oss/lib/libgstmpegts-1.0.so.0 (0x00000000)
	libgstreamer-1.0.so.0 => /opt/zinc/oss/lib/libgstreamer-1.0.so.0 (0x00000000)
	libgobject-2.0.so.0 => /opt/zinc/oss/lib/libgobject-2.0.so.0 (0x00000000)
	libglib-2.0.so.0 => /opt/zinc/oss/lib/libglib-2.0.so.0 (0x00000000)
	libgcc_s.so.1 => /lib/libgcc_s.so.1 (0x00000000)
	libpthread.so.0 => /lib/libpthread.so.0 (0x00000000)
	libc.so.0 => /lib/libc.so.0 (0x00000000)
	libstdc++.so.6 => /lib/libstdc++.so.6 (0x00000000)
	libm.so.0 => /lib/libm.so.0 (0x00000000)
	libgmodule-2.0.so.0 => /opt/zinc/oss/lib/libgmodule-2.0.so.0 (0x00000000)
	libintl.so.8 => /opt/zinc/oss/lib/libintl.so.8 (0x00000000)
	libdl.so.0 => /lib/libdl.so.0 (0x00000000)
	libffi.so.5 => /opt/zinc/oss/lib/libffi.so.5 (0x00000000)
	not a dynamic executable
root# 


={============================================================================
*kt_linux_core_410* shared library: check libraries that  process uses

cat /proc/NNNN/maps | awk '{print $6}' | grep '\.so' | sort | uniq

or

can use ldd.


={============================================================================
*kt_linux_core_410* shared library: check libraries loading

LD_PRELOAD=/libyouviewrcushim.so /bin/true


={============================================================================
*kt_linux_core_411* slib: case problem in open failure

/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so:     
  file format elf32-tradlittlemips

Dynamic Section:
  NEEDED               libnexusMgr.so.0 ~
  NEEDED               libnexus.so
  NEEDED               libgstbase-1.0.so.0
  NEEDED               libgstmpegts-1.0.so.0
  NEEDED               libgstreamer-1.0.so.0
  NEEDED               libgobject-2.0.so.0
  NEEDED               libglib-2.0.so.0
  NEEDED               libgcc_s.so.1
  NEEDED               libpthread.so.0
  NEEDED               libc.so.0
  SONAME               libgstnexus.so.0
  RPATH                /usr/local/lib
  RUNPATH              /usr/local/lib

From strace log:

open("/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so", O_RDONLY) = 60

// see how search path works

open("/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/opt/zinc-trunk/oss/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/opt/zinc-trunk/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/usr/local/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)

open("/opt/zinc-trunk/oss/lib/gstreamer-1.0/libnexusMgr.so.0", O_RDONLY) = -1
ENOENT (No such file or directory)

open("/opt/zinc-trunk/devel/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/opt/zinc-trunk/tests/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/usr/local/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)
open("/usr/lib/libnexusMgr.so.0", O_RDONLY) = -1 ENOENT (No such file or directory)

// this is error on strerr

write(2, "\n(w3cEngine:1641): GStreamer-WARN"..., 134
(w3cEngine:1641): GStreamer-WARNING **: 
  Failed to load plugin '/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstnexus.so': 
  File not found
) = 134


note: Even though the real issue is that there is no Mgr.so which is the next
so, the error suggest that failed to open the starting so.


={============================================================================
*kt_linux_core_411* slib: points to enhance performance

Again, reiterate pros and cons:

{downside-of-static}
1. Duplicates in disk and ram spce.

2. If a change is required perhaps a security or bug fix to an object module in
a static library, then all executables using that module must be relinked in
order to incorporate the change. This disadvantage is further compounded by the
fact that the system administrator needs to be aware of which applications were
linked against the library.


{what-is-shared}
Although the code of a shared library is shared among multiple processes, its
variables are not. Each process that uses the library has its own copies of the
global and static variables that are defined within the library.


{further-advantages}
o Because overall program size is smaller, in some cases, programs can be loaded
into memory and started more 'quickly'. This point holds true only for large
shared libraries that are already in use by another program.

o Such changes can be carried out even while running programs are using an
existing version of the shared library.


{cost-of-shared}
o Shared libraries are more 'complex' than static libraries, both at the
conceptual level, and at the practical level of creating shared libraries and
building the programs that use them.

o Shared libraries 'must' be compiled to use position-independent code, which
has a performance 'overhead' on most architectures because it requires the use
of an extra register 

o Symbol relocation must be performed at run time. During symbol relocation,
  references to each symbol (a variable or function) in a shared library need to
  be modified to correspond to the actual run-time location at which the symbol
  is placed in virtual memory. Take a little more time to execute.


{points-to-think}

From YV real cases:

1. Reordering LD_LIBRARY_PATH

Quote {
From the visualisation, its fairly obvious the there are many (failing) repeated
attempts to find common libraries in /opt/zinc/lib /opt/zinc/oss/lib, before
finding them in there actual location in /lib

In the "before" and "after" visualisations, you can see the significant (0.5)
improvement for reordering of LD_LIBRARY_PATH.
Quote }

To see what is the best LD_LIBRARY_PATH ordering, use strace to show all the
calls to open that succeed and all the calls that failed during the application
startup. 

It seems that reordering the LD_LIBRARY_PATH brings an improvement of 0.2s. The
precision of the profiling has to be taken into account: 0.2s is not much bigger
than the precision of 0.1s.


2. Reduce the code size

Moreover, #Including <zinc-common/logger.h> causes object code bloat. A symbol
for a boost::shared_ptr<LoggerCache> is emitted for every translation unit that
includes the header. This means that the symbol is duplicated over 800 times in
our stack, this means that the stack size is bigger and therefore that it will
take more time to load the shared libraries.

It seems that removing the logger brings an improvement of 0.25s. The precision
of the profiling has to be taken into account: 0.25s is not much bigger than the
precision of 0.10s.


3. Removing unnecessary library dependencies

In order to reduce the number of dependencies, John mentioned a linker option
that allows to keep the dependencies that are really needed: --as-needed.
Normally, it would be as easy as adding this option to the global LDFLAGS and
recompiling the stack: export LDFLAGS="-Wl,--as-needed ${LDFLAGS}"

dynamic libraries mentioned on the command line after the --as-needed option
Unfortunately, the --as-needed option will only apply to the dynamic libraries
that appears after it.  And unfortunately, autotools reorders the parameters and
the --as-needed ends up at the end of the compilation parameters.

Fortunately, we are not the only one using autotools and there are some patches
for fixing that: See

Why –as–needed doesn't work as expected for your libraries on your autotools
project

http://sigquit.wordpress.com/2011/02/16/why-asneeded-doesnt-work-as-expected-for-your-libraries-on-your-autotools-project/

for more details and for links to patches.

Another issue is that this might require some Makefile changes for projects that
forget to list some dependencies and that happened to get them by transitive
dependencies.

We can finally see that this change brings an improvement of 0.32s. We have to
keep in mind that in this case, the box is not busy at all. Therefore, the gain
of 0.32s could be much higher in production when the performance are IO bound.


4. Reducing exported symbols

By applying the flag -fvisibility=hidden

This causes an improvement of 0.4s with sandboxing off and a bit less with
sandboxing on (no idea why?). This is a considerable improvement.

note: This figure comes from a situation when FLASH application loads lots of
libraries from the mw stack.

5. Removing script

As seen previously, a lot of time is spent in Python. It also seems to be a
reasonable idea to remove the python scripts from the launch chain as they did
not have a real justification.


={============================================================================
*kt_linux_core_412* lib-ld.conf lib-ld.cache

/etc/ld.so.cache

/etc/ld.so.cache is required to run D-BUS daemon as non-root. This is due to
the fact that LD_LIBRARY_PATH is ignored when it calls setuid
yv-daemon-sandbox (this is intended behaviour). The dynamic linker does not
resolve shared object dependencies properly in this situation. Using RPATH
could help with that but it is generally not recommended and it is broken in
uClibc.

In order to create ld.so.cache you may create /etc/ld.so.conf with the
following content:

/opt/zinc/oss/lib
/opt/zinc/lib
/usr/local/lib
/opt/stagecraft-2.0/bin

and run ldconfig. You can get ldconfig as part of uClibc build or download
MIPSEL rootfs from the uClibc site and extract it from there. Note that you
should generate /etc/ld.so.cache with every build as the list of the shared
libraries may differ.


={============================================================================
*kt_linux_core_412* slib: as-needed and _GLOBAL_OFFSET_TABLE_

The case is that shared so has liked with another shared library as:

mipsel-linux-gcc -shared sqlite3.o -Wl,--as-needed 
  -Wl,-soname -Wl,libsqlite3.so.0 
  -o .libs/libsqlite3.so

note: no -shared or -fPIC used to make the second shared so.

mipsel-linux-gcc -Wl,--as-needed 
-o .libs/sqlite3 shell.o ./.libs/libsqlite3.so -ldl -lpthread

$ readelf -d sqlite3

 0x00000001 (NEEDED)                     Shared library: [libsqlite3.so.0]
 0x00000001 (NEEDED)                     Shared library: [libgcc_s.so.1]
 0x00000001 (NEEDED)                     Shared library: [libc.so.0]

The problem is that on certain target, for the same package, the link to make
the second so fails with link error:

./.libs/libsqlite3.so:(.got+0x0): multiple definition of `_GLOBAL_OFFSET_TABLE_'
collect2: ld returned 1 exit status
make: *** [xxx] Error 1


Q1: Founds that links works okay without as-needed. Why?
Q2: Why the same commands does work on the other target?

A: The reason for that is that 'ld' seems to have a bug:
                       
https://sourceware.org/ml/binutils/2013-02/msg00159.html

Re: binutils 2.19.92 linker broke with --as-needed flag

On Mon, Feb 04, 2013 at 05:58:37PM -0800, Vincent Wen wrote:
> The linker broke when as-need flag is added.
> 
>  /bin/sh ../libtool --tag=CC --mode=link mipsel-linux-gcc -I../include -g
> -O2 -Wl,--as-needed -o test-example test.o ../lib/libTestGcc.la
> libtool: link: mipsel-linux-gcc -I../include -g -O2 -Wl,--as-needed -o
> .libs/test-example test.o ../lib/.libs/libTestGcc.so -Wl,-rpath
> -Wl,/usr/local/lib
> ../lib/.libs/libTestGcc.so:(.got+0x0): multiple definition of
> `_GLOBAL_OFFSET_TABLE_'

I think mips was broken before --as-needed.  mips seems to want a
dynamic _GLOBAL_OFFSET_TABLE_ symbol in shared libs, presumably for
use by ld.so.  However, a global symbol will be resolved by ld.so
according to the ELF rules which will result in the symbol being
resolved to the first definition seen in a breadth first search of the
application and its shared libraries.  That means the value in the
first shared lib.  So the value seen in other shared libs is wrong.
_GLOBAL_OFFSET_TABLE_ must resolve locally.

So _GLOBAL_OFFSET_TABLE_ must at least be STV_PROTECTED, and could be
STV_HIDDEN as you do in your patch.

> --- a/bfd/elfxx-mips.c    2013-02-01 03:26:00.000000000 -0800
> +++ b/bfd/elfxx-mips.c    2013-02-01 03:26:16.000000000 -0800
> @@ -4681,6 +4681,7 @@
>    h->non_elf = 0;
>    h->def_regular = 1;
>    h->type = STT_OBJECT;
> +  h->other = STV_HIDDEN;
>    elf_hash_table (info)->hgot = h;
> 
>    if (info->shared

The trouble with this is that making it STV_HIDDEN results in no
dynamic _GLOBAL_OFFSET_TABLE_ symbol at all, due to the following code
in bfd_elf_link_record_dynamic_symbol.

      /* XXX: The ABI draft says the linker must turn hidden and
	 internal symbols into STB_LOCAL symbols when producing the
	 DSO. However, if ld.so honors st_other in the dynamic table,
	 this would not be necessary.  */
      switch (ELF_ST_VISIBILITY (h->other))
	{
	case STV_INTERNAL:
	case STV_HIDDEN:
	  if (h->root.type != bfd_link_hash_undefined
	      && h->root.type != bfd_link_hash_undefweak)
	    {
	      h->forced_local = 1;
	      if (!elf_hash_table (info)->is_relocatable_executable)
		return TRUE;
	    }

Now that code is also wrong, I think.  The "return TRUE" should never
happen.

-- 
Alan Modra
Australia Development Lab, IBM


Found that:

GNU ld (GNU Binutils for Debian) 2.22   : Link fails on mips
GNU ld (GNU Binutils) 2.19.1            : Likk okay on mips


={============================================================================
*kt_linux_core_500* sandbox

There are occasional situations where static libraries may be appropriate. If the program is to be
run in an environment (perhaps a chroot jail, for example) where shared libraries are unavailable.


={============================================================================
*kt_linux_core_600* linux-time

LPI 10.

{two-kinds}
Within a program, we may be interested in two kinds of time

o Real time: This is the time as measured either from some standard point
(calendar time) or from some fixed point (typically the start) in the life of
a process (elapsed or `wall clock time`).  Obtaining the calendar time is
useful to programs that, for example, timestamp database records or files.
Measuring elapsed time is useful for a program that takes periodic actions or
makes regular measurements from some external input device.

o Process time: This is the amount of CPU time used by a process. Measuring
process time is useful for checking or optimizing the performance of a program
or algorithm.


{calender-time}
Regardless of geographic location, UNIX systems represent time internally as a
measure of seconds since the Epoch; that is, since midnight on the morning of
1 January 1970, Universal Coordinated Time (`UTC`, previously known as Greenwich
    Mean Time, or GMT). 

This is approximately the date when the UNIX system came into being. Calendar
time is stored in variables of type `time_t`, an integer type specified by
SUSv3.

note: UTC == GMT


{time-t-type} *struct-time-t*
On 32-bit Linux systems, time_t, which is a signed integer, can represent
dates in the range 13 December 1901 20:45:52 to 19 January 2038 03:14:07.
SUSv3 leaves the meaning of negative time_t values unspecified. 

Thus, many current 32-bit UNIX systems face a theoretical Year 2038 problem,
  which they may encounter before 2038, if they do calculations based on dates
  in the future.  This problem will be significantly alleviated by the fact
  that by 2038, probably all UNIX systems will have long become 64-bit and
  beyond. However, 32-bit embedded systems, which typically have a much longer
  lifespan than desktop hardware, may still be afflicted by the problem.
  Furthermore, the problem will remain for any legacy data and applications
  that maintain time in a 32-bit time_t format.

note: Appears that 32bit also has 64 bits time_t? This is taken from debian 32
bits in VM.

Linux debian-keitee 3.2.0-4-486 #1 Debian 3.2.63-2+deb7u2 i686 GNU/Linux

__STD_TYPE __TIME_T_TYPE __time_t;	/* Seconds since the Epoch.  */

__TIME_T_TYPE      50 i386-linux-gnu/bits/typesizes.h #define __TIME_T_TYPE	__SLONGWORD_TYPE

__SLONGWORD_TYPE  103 i386-linux-gnu/bits/types.h #define __SLONGWORD_TYPE	long int


{gettimeofday-time}
The gettimeofday() system call returns the `calendar time` in the buffer
pointed to by tv.

#include <sys/time.h>

int gettimeofday(struct timeval *tv, struct timezone *tz);

Returns 0 on success, or -1 on error

*struct-timeval*

struct timeval {
    time_t tv_sec;         /* Seconds since 00:00:00, 1 Jan 1970 UTC */
    suseconds_t tv_usec;   /* Additional `microseconds` (long int) */
};

The tz argument to gettimeofday() is a historical artifact. This argument is
now obsolete and should always be specified as NULL.


<time-call>
The time() system call returns the number of seconds since the Epoch. i.e.,
    the same value that gettimeofday() returns in the tv_sec field of its tv
    argument.

#include <time.h>

time_t time(time_t *timep);

Returns number of seconds since the Epoch,or (time_t) -1 on error

Since time() returns the same value in two ways, we often simply use the
following call without error checking:

t = time(NULL);

note:
The existence of time() as a system call is historical and now redundant; it
could be implemented as a library function that calls gettimeofday().


={============================================================================
*kt_linux_core_601* time: conversion

Shows the functions used to convert between `time_t` values and other time
formats, including printable representations. These functions shield us from
the complexity brought to such conversions by timezones, daylight saving time
(DST) regimes, and localization issues.

* affected by TZ env variable
+ affected by locale 

Kernel   -> time()               time_t               -> ctime()*    fixed-format string
         <- stime()                                                  Tue Feb 1 21:39:46 2011\n\0

         -> gettimeofday()       struct timeval
         <- settimeofday()

         -> gmtime()             struct tm            -> asctime()
         -> localtime()*         (broken down time)
         <- mktime()*            struct tm            -> strftime()*+   user-formatted,
                                                      <- strptime()+    localized string


{to-printable-form}
The ctime() function provides a simple method of converting a time_t value
into printable form. The ctime() function automatically accounts for local
timezone and DST settings when performing the conversion.

The returned string is statically allocated; future calls to ctime() will
overwrite it.

#include <time.h>

char *ctime(const time_t *timep);

Returns pointer to statically allocated string terminated by newline and \0 on
success, or NULL on error

<statically-allocated>
SUSv3 states that calls to any of the functions ctime(), gmtime(),
      localtime(), or asctime() may overwrite the statically allocated
      structure that is returned by any of the other functions. In other
      words, these functions may share single copies of the returned character
      array and tm structure, and this is done in some versions of glibc. If
      we need to maintain the returned information across multiple calls to
      these functions, we must save local copies.


{to-broken-down-from}
The gmtime() and localtime() functions convert a time_t value into a so-called
brokendown time. The broken-down time is placed in a statically allocated
structure whose address is returned as the function result.

#include <time.h>

struct tm *gmtime(const time_t *timep);
struct tm *localtime(const time_t *timep);

Both return a pointer to a statically allocated broken-down time structure on
success, or NULL on error

Unlike gmtime(), localtime() takes into account timezone and DST settings to
return a broken-down time corresponding to the system's local time.

*struct-tm*

struct tm {
    int tm_sec;      /* Seconds (0-60) */
    int tm_min;      /* Minutes (0-59) */
    int tm_hour;     /* Hours (0-23) */
    int tm_mday;     /* Day of the month (1-31) */
    int tm_mon;      /* Month (0-11) */
    int tm_year;     /* Year since 1900 */   note: 1900 but not 1970
    int tm_wday;     /* Day of the week (Sunday = 0)*/
    int tm_yday;     /* Day in the year (0-365; 1 Jan = 0)*/
    int tm_isdst;    /* Daylight saving time flag
                     > 0: DST is in effect;
                     = 0: DST is not effect;
                     < 0: DST information not available */
};


{between-broken-down-and-printable-form}
functions that convert a broken-down time to printable form, and vice versa.

<no-control-format>
#include <time.h>

char *asctime(const struct tm *timeptr);

Returns pointer to statically allocated string terminated by newline and \0 on
success, or NULL on error

By contrast with ctime(), local timezone settings have no effect on asctime(),
   since it is converting a broken-down time that is typically either already
   localized via localtime() or in UTC as returned by gmtime(). As with
   ctime(), we have no control over the format of the string produced by
   asctime().


<to-control-format>
The strftime() function provides us with more precise control when converting
a broken-down time into printable form. Given a broken-down time pointed to by
timeptr, strftime() places a corresponding null-terminated, date-plus-time
string in the buffer pointed to by outstr.

The string returned in outstr is formatted according to the specification in
format.

#include <time.h>

size_t strftime(char *outstr, size_t maxsize, const char *format, const struct
    tm *timeptr);

Returns number of bytes placed in outstr (excluding terminating null byte) on
success, or 0 on error


<example>
#include <time.h>
#include "curr_time.h" /* Declares function defined here */

#define BUF_SIZE 1000

// Return a string containing the current time formatted according to the
// specification in 'format' (see strftime(3) for specifiers). If 'format' is
// NULL, we use "%c" as a specifier (which gives the date and time as for
// ctime(3), but without the trailing newline). Returns NULL on error.
//
// %c Date and time                       Tue Feb 1 21:39:46 2011
// %T Time (same as %H:%M:%S)             21:39:46

char * currTime(const char *format)
{
  static char buf[BUF_SIZE]; /* Nonreentrant */
  time_t t;
  size_t s;
  struct tm *tm;
  t = time(NULL);
  tm = localtime(&t);
  if (tm == NULL)
    return NULL;
  s = strftime(buf, BUF_SIZE, (format != NULL) ? format : "%c", tm);
  return (s == 0) ? NULL : buf;
}

fprintf( stdout, "time: %s\n", currTime(NULL));
fprintf( stdout, "time: %s\n", currTime("%T"));

$ ./a.out 
time: Fri May  8 00:55:29 2015
time: 00:55:29


={============================================================================
*kt_linux_core_602* time: resolution, jiffies

10-9  1 nanosecond   ns    one billionth of one second
10-6  1 microsecond  us    one millionth of one second
10-3  1 millisecond  ms    one thousandth of one second

The accuracy of various time-related system calls described in this book is
limited to the resolution of the system software clock, which measures time in
units called jiffies.

The size of a jiffy is defined by the constant HZ within the kernel source code.
This is the unit in which the kernel allocates the CPU to processes under the
roundrobin time-sharing scheduling algorithm.

Because CPU speeds have greatly increased since Linux was first implemented, in
kernel 2.6.0, the rate of the software clock was raised to 1000 hertz on Linux/
x86-32. The advantages of a higher software clock rate are that timers can
operate with greater accuracy and time measurements can be made with greater
precision.

However, it isn't desirable to set the clock rate to arbitrarily high values,
because each clock interrupt consumes a small amount of CPU time, which is time
    that the CPU can't spend executing processes.

Debate among kernel developers eventually resulted in the software clock rate
becoming a configurable kernel option (under Processor type and features, Timer
        frequency). Since kernel 2.6.13, the clock rate can be set to 100, 250
(the default), or 1000 HZ, giving jiffy values of 10, 4, and 1 milliseconds,
respectively.


={============================================================================
*kt_linux_core_603* time: posix clock, realtime

LPI 23.5

POSIX clocks (originally defined in POSIX.1b) provide an API for accessing
clocks that measure time with `nanosecond precision` Nanosecond time values
are represented using the same `timespec structure`

The main system calls in the POSIX clocks API are `clock_gettime()`, which
retrieves the current value of a clock; clock_getres(), which returns the
resolution of a clock; and clock_settime(), which updates a clock.

note:
On Linux, programs using this API must be compiled with the `-lrt` option, in
order to link against the librt (realtime) library.


{clock-gettime}
The time value is returned in the timespec structure pointed to by tp.

Although the timespec structure affords nanosecond precision, the granularity
of the time value returned by clock_gettime() may be coarser than this. The
clock_getres() system call returns a pointer to a timespec structure
containing the resolution of the clock specified in clockid.

#define _POSIX_C_SOURCE 199309
#include <time.h>

int clock_gettime(clockid_t clockid, struct timespec *tp);
int clock_getres(clockid_t clockid, struct timespec *res);

Both return 0 on success, or -1 on error

<clockid>
The clockid_t data type is a type specified by SUSv3 for representing a clock
identifier and supports for POSIX.1b timers, as defined in
include/linux/time.h, are:

CLOCK_REALTIME                Settable system-wide real-time clock

The CLOCK_REALTIME clock is a system-wide clock that measures wall-clock time.
By contrast with the CLOCK_MONOTONIC clock, the setting of this clock can be
changed.

CLOCK_MONOTONIC               Nonsettable monotonic clock

SUSv3 specifies that the CLOCK_MONOTONIC clock measures time since some
“unspecified point in the past” that doesn’t change after system startup. This
clock is useful for applications that must not be affected by discontinuous
changes to the system clock (e.g., a manual change to the system time).  On
Linux, this clock measures the time since system startup.

CLOCK_PROCESS_CPUTIME_ID      Per-process CPU-time clock (since Linux 2.6.12)
CLOCK_THREAD_CPUTIME_ID       Per-thread CPU-time clock (since Linux 2.6.12)

The CLOCK_PROCESS_CPUTIME_ID clock measures the user and system CPU time
consumed by the calling process. The CLOCK_THREAD_CPUTIME_ID clock performs
the analogous task for an individual thread within a process.

note:
but `only CLOCK_REALTIME is mandatory` and widely supported on UNIX
implementations.

<linux>
Linux 2.6.28 adds a new clock type, CLOCK_MONOTONIC_RAW, to those listed in
Table 23-1. This is a nonsettable clock that is similar to CLOCK_MONOTONIC,
      but it gives access to a pure hardware-based time that is unaffected by
      NTP adjustments.  This nonstandard clock is intended for use in
      specialized clocksynchronization applications.

Linux 2.6.32 adds two more new clocks to those listed in Table 23-1:
CLOCK_REALTIME_COARSE and CLOCK_MONOTIC_COARSE. These clocks are similar to
CLOCK_REALTIME and CLOCK_MONOTONIC, but intended for applications that want to
obtain lower-resolution timestamps at minimal cost. These nonstandard clocks
don't cause any access to the hardware clock (which can be expensive for some
    hardware clock sources), and the resolution of the returned value is the
jiffy (Section 10.6).


={============================================================================
*kt_linux_core_604* time: ns timestamp

typedef uint64_t u64;

static u64 nsec() {
  struct timeval tv;
  if(gettimeofday(&tv, 0) < 0)
    return -1;
  return (u64)tv.tv_sec*1000*1000*1000 + tv.tv_usec*1000;
}

int main()
{
  be = nsec();
  sort(vec_byval.begin(), vec_byval.end(), compare_by_value);
  af = nsec();
  cout << "by value: diff(ns): " << af-be << endl;
  cout << "by value: diff(ms): " << (af-be)/(1000) << endl;
}


={============================================================================
*kt_linux_core_604* time: ms timestamp

If clock has 1024 HZ resolution, then

1/1024 = 0.000.9765625 (the result is 976562.5 nanoseconds.)
1/1000 = 0.001.
1/2048 = 0.000.48828125

The unit is wider or shorter depending on a resolution.

*struct-timespec*

struct timespec {
  time_t   tv_sec;        /* seconds */
  long     tv_nsec;       /* `nano` seconds */
};

The CLOCK_REALTIME clock measures the amount of time that has elapsed since
epoch. note: since uses time_t. The tv_nsec field specifies a nanoseconds
value. It must be a number in the range 0 to 999,999,999.


To get nanos in total

(tv_sec * 10+9) + tv_nsec nanoseconds

and the following gets time in ms.

1 ms = 1 sec * 10+3        // multiply
1 ms = 1 nsec * 10-6       // devide

So this is to get time in ms and us.

static uint32_t get_time_ms()
{
    struct timespec ts = {0, 0};
    clock_gettime(CLOCK_REALTIME, &ts);
    return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000));
}

static uint32_t time_get_us()
{
    struct	 timespec ts;
    xclock_gettime(CLOCK_REALTIME, &ts);
    return (uint32_t)((ts.tv_sec * 1000000) + (ts.tv_nsec / 10000));
}


<timpstamp-in-ms-example>
1. Since start from 0, this makes timestamp in HH:MM:SS:MS since started but not shows days.
2. Do math as below. So if wants to have timestamps with years and days then use other way.

hour = (thi)/3600000; 				# hour in ms
msec = (thi - (hour*3600000));	# ms remains 
minute = msec / 60000; 				# mins 
msec = msec - (minute * 60000);	# ms remains
sec = msec / 1000; 					# secs
msec = msec - (sec * 1000); 		# ms remains

/*
 * time in MIPS
 */
#include <stdio.h>
#include <unistd.h>
#include <linux/unistd.h>
#include <errno.h> 
#include <stdlib.h>
#include <string.h>
#include <time.h>

typedef unsigned int uint32_t;

// note: starts from 0
static uint32_t tstart = 0;

#if __mips__ /* optimization for mips */
static inline void xclock_gettime(unsigned int which_clock, struct timeval * tv);

#define _syscall_clock_gettimeX(type,name,atype,a,btype,b) \
type x##name(atype a, btype b) \
{ \
    register unsigned long __a0 asm("$4") = (unsigned long) a; \
    register unsigned long __a1 asm("$5") = (unsigned long) b; \
    register unsigned long __a3 asm("$7"); \
    unsigned long __v0; \
    \
    __asm__ volatile ( \
            ".set\tnoreorder\n\t" \
            "li\t$2, %4\t\t\t# " #name "\n\t" \
            "syscall\n\t" \
            "move\t%0, $2\n\t" \
            ".set\treorder" \
            : "=&r" (__v0), "=r" (__a3) \
            : "r" (__a0), "r" (__a1), "i" (__NR_##name) \
            : "$2", "$8", "$9", "$10", "$11", "$12", "$13", "$14", "$15", "$24", \
            "memory"); \
}

_syscall_clock_gettimeX(void, clock_gettime, unsigned int, which_clock, struct timeval *, tv);

#endif

static uint32_t time_get(void)
{
    struct timespec ts;
#if __mips__
    xclock_gettime(CLOCK_REALTIME, &ts);
    printf("call xclock_getttime: ts.tv_sec = %ld, ts.tv_nsec = %ld\n", ts.tv_sec, ts.tv_nsec);
#else
    clock_gettime(CLOCK_REALTIME, &ts);
#endif
    return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000)) - tstart;
}

void main(int argc, char* agrv[]) 
{
    int hour = 0, minute = 0, sec = 0, msec = 0;
    uint32_t tlo =0, thi = 0;

    tstart = time_get();

    thi = time_get();
    hour = (thi)/3600000;
    msec = (thi - (hour*3600000));
    minute = msec / 60000;
    msec = msec - (minute * 60000);
    sec = msec / 1000;
    msec = msec - (sec * 1000);
    printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

    sleep(2);

    thi = time_get();
    hour = (thi)/3600000;
    msec = (thi - (hour*3600000));
    minute = msec / 60000;
    msec = msec - (minute * 60000);
    sec = msec / 1000;
    msec = msec - (sec * 1000);
    printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

    return;
}

$ ./a.out 
call xclock_getttime: ts.tv_sec = 946693753, ts.tv_nsec = 932727000
call xclock_getttime: ts.tv_sec = 946693753, ts.tv_nsec = 932829000
thi:   0, time: 000:00:00.000

call xclock_getttime: ts.tv_sec = 946693755, ts.tv_nsec = 933026000
thi:2001, time: 000:00:02.001


#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <time.h>

typedef unsigned int uint32_t;

static uint32_t tstart = 0;

static time_get(void)
{
    struct timespec ts;
    clock_gettime(CLOCK_REALTIME, &ts);
    return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000)) - tstart;
}

static char* time_stamp(void)
{
    uint32_t tdiff = time_get();

    static char buf[100];
    int hour = 0, minute = 0, sec = 0, msec = 0;
    hour = (tdiff)/3600000;
    msec = (tdiff - (hour*3600000));
    minute = msec / 60000;
    msec = msec - (minute * 60000);
    sec = msec / 1000;
    msec = msec - (sec * 1000);
    sprintf( buf, "%.2d:%.2d:%.2d.%.3d", hour, minute, sec, msec);

    return buf;
}

int main(int argc, char** argv)
{
    tstart = time_get();

    fprintf( stdout, "time: %s\n", time_stamp() );

    int i, j;
    for(i = 0; i < 1000000000; i++)
    {
        j = 30;
        j = j++ % 10;
    }

    fprintf( stdout, "time: %s\n", time_stamp() );

    exit(EXIT_SUCCESS);
}


$ ./a.out 
time: 00:00:00.000
time: 00:00:04.648


={============================================================================
*kt_linux_core_605* time: us timestamp

#ifndef PERFCOUNTER_H
#define PERFCOUNTER_H

#include <string>

class PerfCounter
{
public:
    PerfCounter();
    ~PerfCounter();

    void snap(const char* name);

    std::string dump();

private:
    class DataRec;

    DataRec* head;
    DataRec* lastRec;
};

#endif


#include "PerfCounter.h"

#include <time.h>
#include <sstream>

#include <cstdint>

class PerfCounter::DataRec
{
public:
    DataRec() : next(NULL) {}

    struct timespec ts;
    const char* name;

    DataRec* next;
};

PerfCounter::PerfCounter()
{
    head = new DataRec;
    lastRec = head;
    head->name = "Start";
    clock_gettime(CLOCK_MONOTONIC,&(head->ts));
}

PerfCounter::~PerfCounter()
{
    //delete our records
    DataRec* toDel = head;
    while(toDel)
    {
        head = toDel->next;
        delete toDel;
        toDel = head;
    }
}

void PerfCounter::snap(const char* name)
{
    lastRec->next = new DataRec;
    lastRec = lastRec->next;
    lastRec->name = name;
    clock_gettime(CLOCK_MONOTONIC,&(lastRec->ts));
}

std::string PerfCounter::dump()
{
    snap("end");
    std::stringstream ss;

    DataRec* current = head;

    while(current && current->next)
    {
        //first calculate the number of us difference in the seconds part
        int64_t usecDiff = (current->next->ts.tv_sec - current->ts.tv_sec)*1000000;
        //now the nsec part
        usecDiff += (current->next->ts.tv_nsec - current->ts.tv_nsec)/1000;
        ss << current->name << " -> " << current->next->name << " took " << usecDiff << "us" << std::endl;
        current = current->next;
    }
    return ss.str();
}


#include "PerfCounter.h"

#include <iostream>
#include <sstream>
#include <boost/lexical_cast.hpp>

int main(int argc,char** argv)
{
    PerfCounter counter;
    
    for(int i=0;i<10000;++i)
    {
        int out;
        sscanf("42","%d",&out);
    }
    counter.snap("scanf int");

    for(int i=0;i<10000;++i)
    {
        int out;
        std::stringstream ss("42");
        ss >> out;
    }
    counter.snap("stringstream int");

    for(int i=0;i<10000;++i)
    {
        int out = boost::lexical_cast<int>("42");
    }
    counter.snap("boost::lexical_cast<int>");
    std::cout << counter.dump() << std::endl;
}


={============================================================================
*kt_linux_core_606* time: sleep

{usleep}

usleep - suspend execution for microsecond intervals

#include <unistd.h>

int usleep(useconds_t usec);

note: copied from other article.

Also consider this code: 

usleep(1000);    // sleep 1 microsecond

On the face of it, this line of code makes a thread sleep for 1 microsecond
and then continue. In reality, 1 microsecond is just a lower bound to the
duration of the call.

The man page for usleep() says, "The usleep() function suspends execution of
the calling process for (at least) usec microseconds. The sleep may be
lengthened slightly by any system activity or by the time spent processing the
call or by the granularity of system timers," or if you use the nanosleep()
function. "Therefore, nanosleep() always pauses for at least the specified
time; however, it can take up to 10 ms longer than specified until the process
becomes runnable again."

So if the process is not scheduled under a real-time policy, there's no
guarantee when your thread will be running again. I've done some tests and (to
        my surprise) there are situations when code such as: 

cond.timed_wait(lock, x);    // x = e.g. 1 millisecond

will actually wait for more than 1 second.


{msleep}

void msleep(size_t milliseconds)
{
    usleep(milliseconds * 1000);
}


={============================================================================
*kt_linux_core_700* dbus

{dbus}
http://www.freedesktop.org/wiki/Software/dbus/

What is D-Bus?

D-Bus is a message bus system, a simple way for applications to talk to one
another. In addition to interprocess communication, D-Bus helps coordinate
process lifecycle; it makes it simple and reliable to code a "single instance"
application or daemon, and to launch applications and daemons on demand when
their services are needed.

D-Bus is an Inter-Process Communication (IPC) and Remote Procedure Calling (RPC)
    mechanism specifically designed for efficient and easy-to-use communication
    between processes running on the 'same' machine.


{tutorial}
http://dbus.freedesktop.org/doc/dbus-tutorial.html

D-Bus is a system for interprocess communication (IPC). Architecturally, it has
several layers:

<libdbus> low-level-binding
A library, libdbus, that allows two applications to connect to each other and
exchange messages.

libdbus only supports one-to-one connections, just like a raw network socket.
However, rather than sending byte streams over the connection, you send
'messages'. Messages have a header identifying the kind of message, and a body
containing a data payload. libdbus also abstracts the exact transport used
(sockets vs. whatever else), and handles details such as authentication.

<bus-deamon>
A `message-bus-daemon-executable`, built on libdbus, that multiple applications
can connect to. The daemon can route messages from one application to zero or
more other applications.

The message bus daemon forms the hub of a wheel. Each spoke of the wheel is a
one-to-one connection to an application using libdbus. 

An application sends a message to the bus daemon over its spoke, and the bus
daemon forwards the message to other connected applications as appropriate.
Think of the daemon as a 'router'. 

// from a box, dbus_glib-0.100.2
dbus       671  0.0  0.3   3644  2056 ?        Ss   Apr13   2:45 /opt/zinc/oss/bin/dbus-daemon --fork --print-pid 4 --print-address 6 --session


<binding> high-level binding
'wrapper' libraries or 'bindings' based on particular application frameworks.
For example libdbus-glib and libdbus-qt. There are also bindings to languages
such as Python. These wrapper libraries are the API most people should use, as
they simplify the details of D-Bus programming.  libdbus is intended to be a
low-level backend for the higher level bindings. Much of the libdbus API is only
useful for binding implementation. 


<session-and-system>
D-Bus applications. D-Bus is designed for 'two' specific 'cases':

The bus daemon has 'multiple' instances on a typical computer. The first
instance is a machine-global singleton, that is, a system daemon similar to
sendmail or Apache. This instance has heavy security restrictions on what
messages it will accept, and is used for systemwide communication. The other
instances are created one per user login session. These instances allow
applications in the user's session to communicate with one another.

The systemwide and per-user daemons are separate. Normal within-session IPC does
not involve the systemwide message bus process and vice versa. 

1. session-bus. Communication between desktop applications in the 'same' desktop
'session'; to allow integration of the desktop session as a whole, and address
issues of process lifecycle when do desktop components start and stop running.

2. system-bus. Communication between the desktop session and the operating
system, where the operating system would typically include the kernel and any
system daemons or processes. 

For the within-desktop-session use case, the GNOME and KDE desktops have
significant previous experience with different IPC solutions such as CORBA and
DCOP. D-Bus is built 'on' that experience and carefully tailored to meet the
needs of these desktop projects in particular. D-Bus may or may not be
appropriate for other applications; the FAQ has some comparisons to 'other' IPC
systems.


{concept}
From concept diagram

bus daemon process
==================
 DbusConnection      ->          message dispatcher         <- DbusConnection
 Instance            <-          if(message is signal)      -> Instance
   |                               broadcast
   |                             else
   |                               find destination
   |                               named by message
   |
   |                             (destination table)
   |                 <-          Connection 1
   |                             Connection 2               ->
   |                             "The Window Manager"
   | socket                      ...
   | (bidirectional              
   | message system
   |

application process 1                                   application process 2
===================
 DbusConnection
 Instance

 incoming                   outgoing call
 | locate object via       /|\
 | object path              | marshal method call
 |                          | to message
 | bindings marshal         |
 | to method call           |
\|/                         |

 C/C++ object              Bindings proxy
 instance                  instance

note: incoming from address to method


<address>
Applications using D-Bus are either 'servers' or 'clients'. A server listens for
incoming connections; a client connects to a server. Once the connection is
established, it is a symmetric flow of messages; the client-server distinction
only matters when setting up the connection.

If you're using the bus daemon, as you probably are, your application will be a
client of the bus daemon. That is, the bus daemon listens for connections and
your application initiates a connection to the bus daemon.

A D-Bus address specifies where a server will listen, and where a client will
connect. For example the address unix:path=/tmp/abcdef specifies that the server
will listen on a UNIX domain socket at the path /tmp/abcdef and the client will
connect to that socket. An address can also specify TCP/IP sockets, or any other
'transport' defined in future iterations of the D-Bus specification. 


<busname> for application
When each application connects to the bus daemon, the daemon immediately
'assigns' it a 'name' called the unique connection name. A unique name begins
with a ':' (colon) character. These names are never reused during the lifetime
of the bus daemon - that is, you know a given name will always refer to the same
application.

An example of a unique name might be :34-907. The numbers after the colon have
no meaning other than their 'uniqueness'.

    "When a name is mapped to a particular application's connection, that
    application is said to own that name."

    note: Each connection = each application = each bus name

Applications may ask to own 'additional' well-known names. For example, you
could write a specification to define a name called com.mycompany.TextEditor.
Your definition could specify that to own this name, an application should have
an object at the path /com/mycompany/TextFileManager supporting the interface
org.freedesktop.FileHandler.

    "Applications could then send messages to this bus name, object, and
    interface to execute method calls."

You could think of the unique names as IP addresses, and the well-known names as
domain names. So com.mycompany.TextEditor might map to something like :34-907
just as mycompany.com maps to something like 192.168.0.5. 

note: can monitor if other application is live or not

Names have a 'second' important 'use', other than routing messages. They are
used to track 'lifecycle'. When an application exits (or crashes), its
connection to the message bus will be closed by the operating system kernel. The
message bus then sends out 'notification' messages telling remaining
applications that the application's names have lost their owner. By tracking
these notifications, your application can reliably monitor the lifetime of
'other' applications. 


<object-path>
Your programming framework probably defines what an "object" is like; usually
with a base class. For example: java.lang.Object, GObject, QObject, python's
base Object, or whatever. Let's call this a native object. 

The low-level D-Bus protocol, and corresponding libdbus API, does not care about
native objects.

However, low-level protocal provides a concept called an `object-path`. The idea
of an object path is that higher-level bindings can 'name' native object
'instances', and allow remote applications to refer to them. 

The object path looks like a filesystem path, for example an object could be
named /org/kde/kspread/sheets/3/cells/4/5. Human-readable paths are nice, but
you are free to create an object named /com/mycompany/c5yo817y0c1y1c5b if it
makes sense for your application.

Namespacing object paths is smart, by starting them with the components of a
domain name you own (e.g. /org/kde). This keeps different code modules in the
same process from stepping on one another's toes. 


<interface>
Each object supports one or more interfaces. Think of an interface as a named
group of methods and signals, just as it is in GLib or Qt or Java. Interfaces
define the type of an object instance.

DBus identifies interfaces with a simple 'namespaced' string, something like
org.freedesktop.Introspectable. Most bindings will map these interface names
directly to the appropriate programming language construct, for example to Java
interfaces or C++ pure virtual classes. 


<method-and-signal>
Each object has members; the two kinds of member are 'methods' and 'signals'.
Methods are operations that can be invoked on an object, with optional input
(aka arguments or "in parameters") and output (aka return values or "out
        parameters"). 

Signals are 'broadcasts' from the object to any interested observers of the
'object'; signals may contain a data payload.

Both methods and signals are referred to by name, such as "Frobate" or
"OnClicked".

note: outgoing


<proxies>
A proxy object is a convenient 'native' object created to 'represent' a 'remote'
object in another process. The low-level DBus API involves manually creating a
method call message, sending it, then manually receiving and processing the
method reply message. 

Higher-level bindings provide proxies as an alternative. Proxies look like a
normal native object; but when you invoke a method on the proxy object, the
binding converts it into a DBus method call message, waits for the reply
message, unpacks the return value, and returns it from the native method.

 In pseudocode, programming without proxies might look like this:

Message message = new Message("/remote/object/path", "MethodName", arg1, arg2);
Connection connection = getBusConnection();
connection.send(message);
Message reply = connection.waitForReply(message);
if (reply.isError()) {

} else {
Object returnValue = reply.getReturnValue();
}
        
Programming with proxies might look like this:

Proxy proxy = new Proxy(getBusConnection(), "/remote/object/path");
Object returnValue = proxy.MethodName(arg1, arg2);
        

{big-picture} to specify call
Pulling all these concepts together, to specify a particular method call on a
particular object instance, a number of nested components have to be named:

Address -> [Bus Name] -> Path -> Interface -> Method

dbus.expose(
    // <1> path "/Zinc/Media/MediaRouterFactory";
    ObjectPath::MEDIA_ROUTER_FACTORY, 
    // <2> obj
    factoryMediaRouter.createMediaRouterFactory(),
    // <3>
    boost::make_shared<NonInheritingAdaptorFactory<MediaRouter> > (
        NS_ZINC_DBUS_BINDING::RefCountedAdaptorFactory<MediaRouter>(*bnm), conn, 
        "/Zinc/Media/MediaRouters/")
           );

method call sender=:1.127 -> dest=Zinc.MediaProxy serial=44 
path=/Zinc/Media/MediaRouterFactory; 
interface=Zinc.Media.MediaRouterFactory; 
member=createMediaRouter

// :1.129 == Zinc.MediaProxy
method call sender=:1.127 -> dest=:1.129 serial=48 
path=/Zinc/Media/MediaRouters/0; 
interface=Zinc.Media.MediaRouter; 
member=setSource
   string "http://=1460458249&e=1460501449&h=b42724e238b5e67cbc99e24d435763f3"
   int32 1

dbus-send --print-reply --type=method_call --dest='Zinc.MediaProxy' /Zinc/Media/MediaRouters/0
Zinc.Media.MediaRouter.setSource 
string:http://dash.bidi.int.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd int32:0


{dbus-messages}
There are 4 message 'types':

1. `method-call-messages` ask to invoke a method on an object.  you send a
method call message, and receive either a method return message or an error
message in reply. 

2. `method-return-messages` return the results of invoking a method.

3. `error-messages` return an exception caused by invoking a method.

4. `signal-messages` are notifications that a given signal has been emitted that
an 'event' has occurred. You could also think of these as "event" messages. 

A method call maps very simply to messages: you send a method call message, and
receive either a method return message or an error message in reply.

Each message has a header, including fields, and a body, including arguments.
You can think of the header as the routing information for the message, and the
body as the payload. Header fields might include the sender bus name,
     destination bus name, method or signal name, and so forth. One of the
     header fields is a type signature describing the values found in the body.
     For example, the letter "i" means "32-bit integer" so the signature "ii"
     means the payload has two 32-bit integers. 


{calling-a-method} Behind the Scenes
A method call in DBus consists of two messages; a method call message sent from
process A to process B, and a matching method reply message sent from process B
to process A. Both the call and the reply messages are routed through the bus
daemon. The caller includes a different `serial-number` in each call message,
    and the reply message includes this number to allow the caller to 'match'
    replies to calls.

The call message will contain any arguments to the method. The reply message may
indicate an error, or may contain data returned by the method.

A method invocation in DBus happens as follows:

* The language binding may provide a proxy, such that invoking a method on an
  in-process object invokes a method on a remote object in another process. If
  so, the application calls a method on the proxy, and the proxy constructs a
  method call message to send to the remote process.

* For more low-level APIs, the application may construct a method call message
  itself, without using a proxy.

* In either case, the method call message contains: a bus name belonging to the
  remote process; the name of the method; the arguments to the method; an object
  path inside the remote process; and optionally the name of the interface that
  specifies the method.

* The method call message is sent to the bus daemon.

* The bus daemon looks at the destination bus name. If a process owns that name,
  the bus daemon forwards the method call to that process. Otherwise, the
  bus daemon creates an error message and sends it back as the reply to the
  method call message.

* The receiving process unpacks the method call message. In a simple low-level
  API situation, it may immediately run the method and send a method reply
  message to the bus daemon. When using a high-level binding API, the binding
  might examine the object path, interface, and method name, and convert the
  method call message into an invocation of a method on a native object sucn as
  java.lang.Object, QObject, etc., then convert the return value from the native
  method into a method, reply message.

* The bus daemon receives the method reply message and sends it to the process
  that made the method call.

* The process that made the method call looks at the method reply and makes use
  of any return values included in the reply. The reply may also indicate that
  an error occurred. When using a binding, the method reply message may be
  converted into the return value of of a proxy method, or into an exception. 

The bus daemon 'never' reorders messages. That is, if you send two method call
messages to the same recipient, they will be received in the order they were
sent. The recipient is not required to reply to the calls in order, however; for
example, it may process each method call in a separate thread, and return reply
messages in an undefined order depending on when the threads complete. Method
calls have a unique serial number used by the method caller to match reply
messages to call messages. 


{emitting-a-signal} Behind the Scenes
A signal in DBus consists of a single message, sent by one process to any number
of other processes.  That is, a signal is a `unidirectional-broadcast`. The
signal may contain arguments (a data payload), but because it is a broadcast, it
'never' has a "return value." Contrast this with a method call where the method
call message has a matching method reply message.

The emitter (aka sender) of a signal has no knowledge of the signal recipients.
Recipients register with the bus daemon to receive signals based on "match
rules" - these rules would typically include the sender and the signal name. The
bus daemon sends each signal only to recipients who have expressed interest in
that signal.

A signal in DBus happens as follows:

* A signal message is created and sent to the bus daemon. When using the
  low-level API this may be done manually, with certain bindings it may be done
  for you by the binding when a native object emits a native signal or event.

* The signal message contains the name of the interface that specifies the
  signal; the name of the signal; the bus name of the process sending the
  signal; and any arguments

* Any process on the message bus can register "match rules" indicating which
  signals it is interested in. The bus has a list of registered match rules.

* The bus daemon examines the signal and determines which processes are
  interested in it. It sends the signal message to these processes.

* Each process receiving the signal decides what to do with it; if using a
  binding, the binding may choose to emit a native signal on a proxy object. If
  using the low-level API, the process may just look at the signal sender and
  name and decide what to do based on that. 


={============================================================================
*kt_linux_core_700* dbus-libdbus

Reference Implementation (dbus-daemon and libdbus)

Released versions of D-Bus can be downloaded from the releases directory on
dbus.freedesktop.org and are available in all major Linux distributions. If in
doubt, use your distribution's packages.

note: Use dbus-1.6.4.tar.bz2

The current stable branch is D-Bus 1.10.x. This is the recommended version for
most purposes.

The current legacy branch is D-Bus 1.8.x. This is still supported, but only for
security fixes: only use these versions when upgrading from older stable
releases, or preparing security updates for frozen/stable distributions.

Older branches are unsupported and are unlikely to have any more releases, but
distributors who still provide security support for an older version are invited
to share backported patches via the older branches in the same git repository.

The current development branch is D-Bus 1.11.x, which will lead to a 1.12.x
stable branch in future.


<doc>
https://dbus.freedesktop.org/doc/api/html/group__DBusThreads.html

DBUS_EXPORT dbus_bool_t dbus_threads_init_default 	(void) 	

Initializes threads.

If this function is not called, the D-Bus library will not lock any data
structures. If it is called, D-Bus will do locking, at some cost in efficiency.

Since D-Bus 1.7 it is safe to call this function from any thread, any number of
times (but it must be called before any other libdbus API is used).

In D-Bus 1.6 or older, this function must be called in the main thread before
any other thread starts. As a result, it is not sufficient to call this function
in a library or plugin, unless the library or plugin imposes a similar
requirement on its callers.

dbus_shutdown() reverses the effects of this function when it resets all global
state in libdbus.

Returns
    TRUE on success, FALSE if not enough memory 

Definition at line 438 of file dbus-threads.c.

References dbus_threads_init(), and NULL.

Referenced by _dbus_cmutex_new_at_location(), _dbus_condvar_new(), and
_dbus_rmutex_new_at_location().


={============================================================================
*kt_linux_core_700* dbus-bindings

https://www.freedesktop.org/wiki/Software/DBusBindings/

// used dbus_python-0.83.1

dbus-python

dbus-python is a binding for libdbus, the reference implementation of D-Bus. For
compatibility reasons, its API involves a lot of type-guessing (despite
        "explicit is better than implicit" and "resist the temptation to
        guess").

Since version 1.0.0 it supports both Python 2 and 3.

    Recent release history

    Releases are always available from http://dbus.freedesktop.org/releases/dbus-python/

    API and other documentation are at http://dbus.freedesktop.org/doc/dbus-python/

    dbus-python is maintained in git: dbus-python git web

    For users with commit access: git clone
    git+ssh://git.freedesktop.org/git/dbus/dbus-python

    For anonymous read only access: git clone
    git://anongit.freedesktop.org/git/dbus/dbus-python

    Bugs are tracked in the freedesktop.org bugzilla: search for dbus-python
    bugs or file a dbus-python bug


C++

dbus-cpp was started almost three years ago to provide a C++ API for D-Bus, but
is unfortunately abandoned since then. For this reason ?PaoloDurante wrote a
pure C++ binding (dbus-c++) while working on the OpenWengo softphone.


={============================================================================
*kt_linux_core_700* dbus-case

// MediaDaemon.cpp


int main(int argc, char *argv[]) {
  int result = EXIT_SUCCESS;

  try {
    MainLoop mainloop(BusName::MEDIA);

    // note: create instance
    MediaDaemon daemon(mainloop);

    mainloop.post(boost::bind(&MediaDaemon::start, &daemon, argc, argv));
    result = mainloop.run();
  } catch (std::exception& e) {
    std::cerr << e.what() << endl;
    result = EXIT_FAILURE;
  }

  return result;
}


class MediaDaemon : boost::noncopyable
{
public:
    MediaBackendDaemon(MainLoop& mainloop);
    ~MediaBackendDaemon();
    void start();
    void stop();
private:

    MainLoop& mainloop;
    SignalReceiver sr;

    // note:
    DBusService dbus;

    NS_ZINC::InlineDispatcher futureDispatcher;
    boost::scoped_ptr<BusNameMonitor> bnm;

    std::string getUserAgentString();
};

MediaDaemon::MediaDaemon(MainLoop& mainloop):
    mainloop(mainloop),
    sr(mainloop, createExitSignalHandler(boost::bind(&MediaBackendDaemon::stop, this))),

    // note:
    dbus(mainloop)
{
}

void MediaDaemon::start()
{
    sr.start();
    dbus.start();

    DBus::Connection conn = dbus.getConnection();

    const bool dashCencEnable = convertToSync(createAuthoriser())->isFeatureEnabled("yv-mediarouter-dash");

    boost::shared_ptr<Zinc::Media::Backend::Factory> factory =
        createIPCSrcBinFactory(
            boost::shared_ptr<NS_ZINC::Dispatcher>(&futureDispatcher),
            createSinkBinFactory(
                boost::shared_ptr<NS_ZINC::Dispatcher>(&futureDispatcher),
                dashCencEnable));

    bnm.reset(new BusNameMonitor(conn));

    <ex>
    dbus.expose(OBJECT_PATH_FACTORY, factory,
        boost::make_shared<NonInheritingAdaptorFactory<Zinc::Media::Backend::SinkInstance> >(
            RefCountedAdaptorFactory<Zinc::Media::Backend::SinkInstance>(*bnm), conn, OBJECT_PATH_CONTROL));

    <ex> 
    dbus.expose(
            ObjectPath::MEDIA_ROUTER_FACTORY, 
            factory.createMediaRouterFactory(),
            boost::make_shared<NonInheritingAdaptorFactory<MediaRouter> >(
                NS_ZINC_DBUS_BINDING::RefCountedAdaptorFactory<MediaRouter>(*bnm), conn, "/Zinc/Media/MediaRouters/")
              );

    dbus.request_name(BUS_NAME);
}


<wrapper>
MainLoop
: DBusMainLoopDispatch (wraps BusDispatch)

    ->
        BusDispatch : Dispatch, DefaultMainLoop
            (DefaultMainLoop do the real work)

/////////////////////////////////////
#ifndef ZINC_DBUS_BINDING_MAINLOOP_H_
#define ZINC_DBUS_BINDING_MAINLOOP_H_

#include "macros.h"

#include <dbus-c++/connection.h>
#include <dbus-c++/dispatcher.h>
#include <dbus-c++/eventloop-integration.h>
#include <zinc-common/AsynchronousEventDispatcher.h>

#include <boost/make_shared.hpp>
#include <boost/noncopyable.hpp>
#include <boost/shared_ptr.hpp>

NS_ZINC_DBUS_BINDING_OPEN

/**
 *  MainLoop class wraps dispatchers and make those dispatchers have a unique
 *  interface to work in different situation.
 *
 *  It adapts a single dispatcher loop such that it can be used as any types of
 *  dispatcher. It worth to do further modifying on the dispatchers to have
 *  fewer dispatcher types for simplification
 *
 */

class ZINC_EXPORT MainLoop : boost::noncopyable {
public:
   // note: uses for debugging but not the real bus name
	explicit MainLoop(const std::string& daemonName);

	/**
	 * Post a command to be run by any thread currently running the dispatcher
	 * loop.
	 */
	void post(boost::function<void(void)> cb);

    /**
     * Break the dispatching loop, allowing graceful shutdown
     */
    void leave();

	/**
	 * Run the dispatcher loop.
	 */
	int run();

	/**
	 * Get different dispatchers maintained in mainloop
	 */
	boost::shared_ptr<NS_ZINC::Dispatcher> getZincDispatcher();
	boost::shared_ptr<DBus::Dispatcher> getDbusDispatcher();
	boost::shared_ptr<DBus::DefaultMainLoop> getDefaultMainLoop();

private:
	void initialization();
	boost::shared_ptr<NS_ZINC::Dispatcher> dispatcher;
	boost::shared_ptr<DBus::Dispatcher> dbusDispatcher;
	boost::shared_ptr<DBus::DefaultMainLoop> defaultMainloop;
	std::string name;
};

NS_ZINC_DBUS_BINDING_CLOSE

#endif /* MAINLOOP_H_ */


/////////////////////////////////////

#include "../../include/dbus/MainLoop.h"

#include <iostream>

#include <cstdlib>

namespace {
/**
 * Type conversion class to convert from DBus::DefaultMainLoop to NS_ZINC::Dispatcher
 */
class DBusMainLoopDispatcher: public NS_ZINC::Dispatcher {

public:
	explicit DBusMainLoopDispatcher(boost::shared_ptr<DBus::DefaultMainLoop> d_) :
			d(d_) {
	}

	void post(boost::function<void(void)> fn) {
		d->post_function(fn);
	}

	void onWorkAdded() {
		d->add_external_work();
	}

	void onWorkRemoved() {
		d->remove_external_work();
	}

	boost::shared_ptr<DBus::DefaultMainLoop> d;
};

} //anon namespace

NS_ZINC_DBUS_BINDING_OPEN

MainLoop::MainLoop(const std::string& daemonName) : name(daemonName) {
	initialization();
}


// note: 
// create dbus-dispatcher and dbus-main-loop
//
// 'DBusMainLoopDispatcher' is to translate between zinc dispatcher and dbus
// dispatcher.

void MainLoop::initialization(){

	boost::shared_ptr<DBus::BusDispatcher> busDispatcher = 
       boost::make_shared<DBus::BusDispatcher>();

   // DBus::Dispatcher = DBus::BusDispatcher;
	dbusDispatcher = busDispatcher;

	// DBus::DefaultMainLoop = DBus::BusDispatcher;
   // HOW?
	defaultMainloop = busDispatcher;

   // create wrapper(defaultMainloop);
	dispatcher = boost::make_shared<DBusMainLoopDispatcher>(defaultMainloop);
}

void MainLoop::post(boost::function<void(void)> cb) {
	dispatcher->post(cb);
}


void MainLoop::leave() {
    dbusDispatcher->leave();
}


int MainLoop::run() {
    int result = EXIT_FAILURE;
    try {
        // note: run
        dbusDispatcher->run();

        result = EXIT_SUCCESS;
    } catch (const std::exception& e) {
        std::cerr << name << " ERROR: " << e.what() << std::endl;
        throw;
    } catch (...) {
        std::cerr << name << " ERROR: Unknown\n";
        throw;
    }
    return result;
}

boost::shared_ptr<NS_ZINC::Dispatcher> MainLoop::getZincDispatcher() {
	return dispatcher;
}
boost::shared_ptr<DBus::Dispatcher> MainLoop::getDbusDispatcher() {
	return dbusDispatcher;
}
boost::shared_ptr<DBus::DefaultMainLoop> MainLoop::getDefaultMainLoop() {
	return defaultMainloop;
}
NS_ZINC_DBUS_BINDING_CLOSE


<dbus-bus-dispatcher>

// Zinc/Zinc.DBus-C++/include/dbus-c++/eventloop-integration.h
// Zinc/Zinc.DBus-C++/include/dbus-c++/dispatcher.h
// has class DXXAPI Dispatcher

namespace DBus {

/* 
 * Glue between the event loop and the DBus library
 */
class DXXAPI BusDispatcher 
	: public Dispatcher, public DefaultMainLoop
{
public:
	BusDispatcher(boost::shared_ptr<ErrorConverter> = DefaultErrorConverter::get());

	~BusDispatcher();

	bool running() { return _running; }

	virtual void enter();

	virtual void leave();

	virtual void start_iteration();

	virtual Timeout* add_timeout(Timeout::Internal *);

	virtual void rem_timeout(Timeout *);

	virtual Watch* add_watch(DBusWatch*);

	virtual void rem_watch(Watch *);

	void wakeup();

	bool isThisDispatchThread() const {
		return DefaultMainLoop::isThisDispatchThread();
	}

	bool shouldQueueCommand() const {
		return DefaultMainLoop::shouldQueueCommand();
	}
	std::size_t run();
	std::size_t poll();

protected:
	
   // note:
	void post_function(boost::function<void ()> command)
	{
		DefaultMainLoop::post_function(command);
	}

private:

	volatile bool _running;
};

}


// Zinc/Zinc.DBus-C++/src/eventloop-integration.cpp

BusDispatcher::BusDispatcher(boost::shared_ptr<ErrorConverter> ec)
	: Dispatcher()
	, DefaultMainLoop(ec)
	, _running(false)
{
    // note: call to libdbus
    dbus_threads_init_default();
}

BusDispatcher::~BusDispatcher()
{
}

std::size_t BusDispatcher::run()
{
    debug_log("Running dispatcher %p with EventLoop %p", (Dispatcher*) this, (DefaultMainLoop*) this);
    start_iteration();

    DefaultMainLoop::LoopScope scopeLoop(this);

    std::size_t n = 0;
    while (_running && (has_cmds() || has_timeouts() || has_watches()
                || has_external_work())) {
        n += dispatch(BLOCK_FOR_WATCHES | BLOCK_FOR_TIMEOUTS
                | BLOCK_FOR_EXTERNAL_WORK);
    }

    debug_log("Finished running dispatcher %p with EventLoop %p.  Ran %u handlers.",
            (Dispatcher*) this, (DefaultMainLoop*) this, (unsigned) n);
    return n;
}


// <dbus-busdispatcher>
// From Makefile. DO not use these.
// 
// #am__objects_4 =  \
// #	libdbus_c___1_la-glib-integration.lo
// #am__objects_5 =  \
// #	libdbus_c___1_la-ecore-integration.lo
// 
// Assumes that uses Glib since there are two options: Ecore and Glib.
// 
// // Defines ABC
// // Zinc/Zinc.DBus-C++/include/dbus-c++/dispatcher.h
// 
// class DXXAPI Dispatcher;
// 
// 
// // Zinc/Zinc.DBus-C++/include/dbus-c++/ecore-integration.h
// // Zinc/Zinc.DBus-C++/include/dbus-c++/glib-integration.h
// 
// namespace DBus {
// namespace Glib {
// 
// class DXXAPI BusDispatcher : public Dispatcher
// {
// public:
// 
// 	BusDispatcher();
// 	~BusDispatcher();
// 
// 	void attach(GMainContext *);
// 
// 	void enter() {}
// 
// 	void leave() {}
// 
// 	Timeout *add_timeout(Timeout::Internal *);
// 
// 	void rem_timeout(Timeout *);
// 
// 	Watch* add_watch(DBusWatch*);
// 
// 	void rem_watch(Watch *);
// 
// 	void set_priority(int priority);
// 
// private:
// 
// 	GMainContext *_ctx;
// 	int _priority;
// 	GSource *_source;
// };
// 
// } /* namespace Glib */
// } /* namespace DBus */
// 
// 
// // Zinc/Zinc.DBus-C++/src/glib-integration.cpp
// 
// Glib::BusDispatcher::BusDispatcher()
// : _ctx(NULL), _priority(G_PRIORITY_DEFAULT), _source(NULL)
// {
// }


<dbus-default-main-loop>

// Zinc/Zinc.DBus-C++/include/dbus-c++/eventloop.h

/*
 *
 *  D-Bus++ - C++ bindings for D-Bus
 *
 *  Copyright (C) 2005-2009  Paolo Durante <shackan@gmail.com>
 */

#ifndef __DBUSXX_EVENTLOOP_H
#define __DBUSXX_EVENTLOOP_H

namespace DBus {

/*
 * these Default *classes implement a very simple event loop which
 * is used here as the default main loop, if you want to hook
 * a different one use the Bus *classes in eventloop-integration.h
 * or the Glib::Bus *classes as a reference
 */

class DXXAPI DefaultMainLoop
{
public:

	DefaultMainLoop(boost::shared_ptr<ErrorConverter>);

	virtual ~DefaultMainLoop();

	void wakeup();

	bool isThisDispatchThread() const;
	bool shouldQueueCommand() const;

	void post_function(boost::function<void ()>);

	/**
	 * Register for a callback after the specified amount of time.
	 *
	 * @param interval_ms The amount of time to wait before calling the
	 *                    callback in milliseconds
	 * @param repeat      A value of false will cause the callback to be called
	 *                    exactly once.  i.e. the callback is canceled
	 *                    immediately after it is first called.
	 * @param callback    The callback functor to call after/every interval_ms
	 *                    milliseconds.
	 *
	 * @return A pointer to a DefaultTimeout object which can be used to cancel
	 *         the timeout.
	 */
	DefaultTimeout* add_timeout(int interval_ms, bool repeat, boost::function<void (DefaultTimeout&)> callback);

	/**
	 * Register for a callback when a file descriptor becomes ready for I/O
	 *
	 * @param fd      The file descriptor to watch
	 * @param flags   Flags describing under what circumstances the callback
	 *                should be called.  These are the same as what should be
	 *                passed to poll as the events member of the pollfd
	 *                structure.  This will typically be several of POLLIN,
	 *                POLLPRI, POLLOUT, POLLRDHUP, POLLERR, POLLHUP and
	 *                POLLNVAL.  See man 2 poll for more information.
	 * @param handler The callback functor to call when the fd becomes ready
	 *                for I/O
	 *
	 * @return A pointer to a DefaultWatch object which can be used to stop
	 *         watching the file descriptor.
	 */
	DefaultWatch* add_watch(int fd, int flags, boost::function<void (DefaultWatch&)> handler);

	/**
	 * The way of doing graceful exit of a dispatcher is by removing works from
	 * it and letting it exit after it's run out of works.
	 *
	 * In order to avoid the dispatcher run down before all the Dispatcher::work
	 * objects destroyed, it is necessary to maintain a ref count of the works.
	 *
	 * Working with the ref count, Flags of BLOCK_FOR_EXTERNAL_WORK can be set
	 * to decide whether polling in dispatcher should block(for some timeout)
	 * when there is any work has not been finished on the dispatcher.
	 *
	 */
	void add_external_work();

	void remove_external_work();

	/**
	 * Do as much work (call as many callbacks) as possible without blocking
	 * on any of the watchers or timeouts.
	 *
	 * @return The number of callbacks that were called.
	 */
	size_t poll();
protected:

	virtual unsigned dispatch(unsigned flags);

	void enteredLoop();

	void leftLoop();

	bool has_cmds();
	bool has_timeouts();
	bool has_watches();

	/**
	 * check whether there is any work has not been finished on the dispatcher
	 */
	bool has_external_work();

	/**
	 * This struct follows RAII(Resource Acquisition Is Initialization), in
	 * order to make sure that the leftloop is always called even there is an
	 * exception thrown.
	 *
	 */
	struct LoopScope {
		LoopScope(DefaultMainLoop* loop_)
		 : loop(loop_) {
			loop->enteredLoop();
		}
		~LoopScope(){
			loop->leftLoop();
		}
		DefaultMainLoop* loop;
	};
private:
	unsigned processCommands();

	boost::mutex _mutex_cmds;
	std::list<boost::function<void ()> > _cmds;

	boost::mutex _mutex_t;
	std::list<DefaultTimeout> _timeouts;

	boost::mutex _mutex_w;
	std::list<DefaultWatch> _watches;

	unsigned externalWorkCount;

	int wakeupReadFd;
	int wakeupWriteFd;

	bool enteredDispatch;
	pthread_t dispatchThread;

	boost::shared_ptr<ErrorConverter> errorConverter;
};

} /* namespace DBus */

#endif//__DBUSXX_EVENTLOOP_H

// Zinc/Zinc.DBus-C++/src/eventloop.cpp

void DefaultMainLoop::post_function(boost::function<void ()> fn)
{
	std::list<boost::function<void ()> > new_cmd;
	new_cmd.push_back(fn);
	boost::mutex::scoped_lock lock(_mutex_cmds);
	_cmds.splice(_cmds.end(), new_cmd);
	lock.unlock();
	wakeup();
}


={============================================================================
*kt_linux_core_700* dbus-case-service

// The line 46 has no effect. WHAT is that for?
//
// template<typename T, typename A1>
// void expose_impl(DBus::Connection conn, const char* path,
//         boost::shared_ptr<T> obj, A1 a1) {
//     using namespace std;
// 46:	expose(conn, path, obj, a1);
// }
// 
// template<typename T, typename A1>
// void DBusService::expose(const char* path, boost::shared_ptr<T> obj, A1 a1) {
//     using namespace std;
// 145:	detail::expose_impl(conn, path, obj, a1);
//       objects.push_back(path);
// }
// 
// (gdb) bt
// #0  expose_impl<Zinc::Media::MediaRouterFactoryAsync, 
// boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > 
// (
//     a1=<error reading variable: access outside bounds of object referenced 
//          via synthetic pointer>, 
//     obj=<error reading variable: access outside bounds of object referenced 
//          via synthetic pointer>, 
//     conn=<incomplete type>, 
//     path=<optimized out>
// )
//     at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:46
// 
// #1  expose<
//  Zinc::Media::MediaRouterFactoryAsync, 
//  boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > 
//  (
//     a1=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
//     obj=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
//     this=0x7fff6510, 
//     path=<optimized out>
//  )
//     at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:145
// 
// #2  (anonymous namespace)::MediaDaemon::start (this=0x7fff64e4, argc=<optimized out>, argv=<optimized out>)
//     at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Nickel/Nickel.System.DBusServer/src/MediaDaemon.cpp:123
// #3  0x77e57404 in DBus::DefaultMainLoop::processCommands() () from /opt/zinc/lib/libdbus-c++-1.so.0
// #4  0x77e58ff0 in DBus::DefaultMainLoop::dispatch(unsigned int) () from /opt/zinc/lib/libdbus-c++-1.so.0
// #5  0x77e5a6c4 in DBus::BusDispatcher::run() () from /opt/zinc/lib/libdbus-c++-1.so.0
// #6  0x77ec24ec in zinc::binding::dbus::MainLoop::run (this=0x7fff64c8)
//     at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Zinc/Zinc.DBus.BindingRuntime/src/dbus/MainLoop.cpp:70
// #7  0x00408150 in main (argc=1, argv=0x7fff6864) at /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Nickel/Nickel.System.DBusServer/src/MediaDaemon.cpp:234
// 
// 
// (gdb) n
// expose<
//  Zinc::Media::MediaRouterFactoryAsync, 
//  boost::shared_ptr<zinc::binding::dbus::NonInheritingAdaptorFactory<Zinc::Media::MediaRouterAsync> > > 
//  (
//     a1=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
//     obj=<error reading variable: access outside bounds of object referenced via synthetic pointer>, 
//     this=0x7fff6510, 
//     path=<optimized out>
//  )
//     at /opt/zinc-trunk/include/zinc-binding-runtime/dbus/DBusService.h:145
// 145		detail::expose_impl(conn, path, obj, a1);
//

/////////////////////////////////////
// Zinc.DBus.BindingRuntime/include/dbus/DBusService.h
/*
 * DBusService.h
 */

#ifndef ZINC_DBUS_BINDING_DBUSSERVICE_H_
#define ZINC_DBUS_BINDING_DBUSSERVICE_H_

#include "macros.h"
#include "MainLoop.h"
#include "MessageBusFactory.h"

#include <dbus-c++/connection.h>

#include <boost/make_shared.hpp>
#include <boost/shared_ptr.hpp>
#include <boost/foreach.hpp>

NS_ZINC_DBUS_BINDING_OPEN

/**
 * Those expose_impl(s) is to do with name-lookup.
 * Without this indirection, DBusService is unable to look outside class scope
 * to find additional expose methods as it already provides some
 */
namespace detail {

template<typename T>
void expose_impl(DBus::Connection conn, const char* path,
		boost::shared_ptr<T> obj) {
	using namespace std;
	expose(conn, path, obj);
}

// note:
template<typename T, typename A1>
void expose_impl(DBus::Connection conn, const char* path,
		boost::shared_ptr<T> obj, A1 a1) {
	using namespace std;

   // note: WHERE this leads to?
	expose(conn, path, obj, a1);
}

template<typename T, typename A1, typename A2>
void expose_impl(DBus::Connection conn, const char* path,
		boost::shared_ptr<T> obj, A1 a1, A2 a2) {
	using namespace std;
	expose(conn, path, obj, a1, a2);
}

} // detail namespace



/**
 * DBusService class is kind of wrapper of dbus, it maintains the connection
 * with bus, takes care of exposing and registering objects to the bus.
 *
 */
class ZINC_EXPORT DBusService {

public:
	/**
	 * Constructor
	 *
	 * @param mainloop is a wrapper of dispatchers
	 *
	 */
	DBusService(MainLoop& mainloop_);

	/**
	 * This method should be called when the dbus daemons start, in order to
	 * establish connection with dbus
	 */
	void start();

	/**
	 * Expose the given object on the bus
	 * By exposing this way the objects will be automatically unexposed when
	 * stop is called.
	 */
	template<typename T>
	void expose(const char* path, boost::shared_ptr<T> obj);

   // note:
	template<typename T, typename A1>
	void expose(const char* path, boost::shared_ptr<T> obj, A1 a1);

	template<typename T, typename A1, typename A2>
	void expose(const char* path, boost::shared_ptr<T> obj, A1 a1, A2 a2);

	/**
	 * The name will be automatically removed when stop() is called
	 */
	void request_name(const char* name);

	/**
	 * This method should be called when the dbus daemon ends, in order to close
	 * the connection
	 * with bus and also release names and unregister the object paths
	 */
	void stop();

	/**
	 * Unregister all object paths when the DBusService stops(the connection
	 * with bus stops)
	 */
	void onNamesReleased();

	/**
	 * Get connection that the DBusService maintains
	 */
	DBus::Connection getConnection();

private:
	DBus::Connection conn;
	std::vector<std::string> busNames;
	std::vector<std::string> objects;
	MainLoop& mainloop;

	/**
	 * Bus proxy used to access libdbus
	 */
	boost::shared_ptr<NS_ZINC_DBUS_BINDING::MessageBusAsync> bus_Async;
};

template<typename T>
void DBusService::expose(const char* path, boost::shared_ptr<T> obj) {
	using namespace std;
	detail::expose_impl(conn, path, obj);
	objects.push_back(path);
}

// note:
template<typename T, typename A1>
void DBusService::expose(const char* path, boost::shared_ptr<T> obj, A1 a1) {
	using namespace std;
	detail::expose_impl(conn, path, obj, a1);
	objects.push_back(path);
}

template<typename T, typename A1, typename A2>
void DBusService::expose(const char* path, boost::shared_ptr<T> obj, A1 a1,
		A2 a2) {
	using namespace std;
	detail::expose_impl(conn, path, obj, a1, a2);
	objects.push_back(path);
}

NS_ZINC_DBUS_BINDING_CLOSE

#endif /* DBUSSERVICE_H_ */


/////////////////////////////////////
/*
 * DBusService.cpp
 */

#include <zinc-common/macros.h>

#include "../../include/dbus/DBusService.h"
#include "../../include/dbus/MessageBus.h"
#include "../../include/dbus/DBusConnectionManager.h"
#include "../../include/dbus/detail/SingletonDBusErrorConverter.h"

#include <zinc-common/async/FutureBarrier.h>
#include <dbus-c++/connection.h>

NS_ZINC_DBUS_BINDING_OPEN

DBusService::DBusService(MainLoop& mainloop_) :
		mainloop(mainloop_) {
}

void DBusService::start() {
    // Zinc/Zinc.DBus-C++/include/dbus-c++/connection.h
    // class DXXAPI StandardConnectionFactory
    conn=DBus::StandardConnectionFactory(mainloop.getDbusDispatcher(), SingletonDBusErrorConverter::get()).connectToSessionBus();

    NS_ZINC_DBUS_BINDING::DBusConnectionManager::instance().setSessionConnection(conn);
    bus_Async = NS_ZINC_DBUS_BINDING::createMessageBusProxy(conn,mainloop.getZincDispatcher());
}

void DBusService::request_name(const char* name) {
	conn.request_name(name);
	busNames.push_back(name);
}

void DBusService::stop() {
	NS_ZINC::FutureBarrier bar(*mainloop.getZincDispatcher());
	BOOST_FOREACH(std::string name, busNames){
		bar.add(bus_Async->ReleaseName(name));
	}
	bar.setCallback(*mainloop.getZincDispatcher(),
			boost::bind(&DBusService::onNamesReleased, this));
}
void DBusService::onNamesReleased() {
	busNames.clear();
	BOOST_FOREACH(std::string path, objects){
		conn.unregister_object_path(path.c_str());
	}
	conn.set_callback_when_all_requests_replied(boost::bind(&DBus::Connection::close, _1));
}

DBus::Connection DBusService::getConnection(){
	return conn;
}

NS_ZINC_DBUS_BINDING_CLOSE


/////////////////////////////////////
/*
 * NonInheritingAdaptorFactory.h
 */

#ifndef ZINC_DBUS_BINDING_NONINHERITINGADAPTORFACTORY_H_
#define ZINC_DBUS_BINDING_NONINHERITINGADAPTORFACTORY_H_

#include "macros.h"

#include "PredestructDeleter.h"
#include "AdaptorFunctor.h"
#include "RefCountedAdaptorDecorator.h"
#include "InterfaceAdaptor.h"

#include <dbus-c++/types.h>
#include <dbus-c++/connection.h>

#include <boost/shared_ptr.hpp>
#include <boost/function.hpp>
#include <boost/foreach.hpp>

#include <string>
#include <sstream>

NS_ZINC_DBUS_BINDING_OPEN

class Connection;

/**
 * An implementation of the DBus-C++ AdaptorFactory concept.
 * 
 * WARNING: This is not thread-safe.  DBus internal locks must be exposed to
 *          make this thread-safe.
 */
template <class BaseT>
class NonInheritingAdaptorFactory : boost::noncopyable {
public:
    typedef AdaptorFunctor<typename NS_ZINC_DBUS_BINDING::adaptor<BaseT>::type> Adaptor;
    typedef RefCountedAdaptorDecorator<typename NS_ZINC_DBUS_BINDING::adaptor<BaseT>::type> RefCountedAdaptor;
    /**
     * Constructor
     * 
     * Factory should be a functor with the signature:
     *     std::string (boost::shared_ptr<BaseT> obj, boost::shared_ptr<Connection> connection, const std::string& basepath)
     * or equivalent.  The factory should expose the given object on the bus
     * generating a new unique path for that object and returning it.  When the
     * object drops off the bus the predestructor functor should be called.
     */
    template<class Factory>
    NonInheritingAdaptorFactory(Factory f, DBus::Connection connection_, const std::string& basePath_)
     : factory(f), connection(connection_), basePath(basePath_)
    {
    }

    /**
     * Expose the given object on the bus (if not already exposed) and return
     * the object path. 
     */
    std::string expose(boost::shared_ptr<BaseT> obj, const DBus::Message& msg = DBus::Message())
    {
        // First check if the object has yet been exposed:
        typedef std::pair<std::string, Adaptor*> AdaptorNamePair;
        std::vector<AdaptorNamePair> adaptors = connection.get_adaptors_of_type<Adaptor>();
        BOOST_FOREACH(AdaptorNamePair& i, adaptors) {
            if (i.second->getObject() == obj) {
                return i.first;
            }
        }
        typedef std::pair<std::string, RefCountedAdaptor*> RefCountedAdaptorNamePair;
        std::vector<RefCountedAdaptorNamePair> refcountedadaptors = connection.get_adaptors_of_type<RefCountedAdaptor>();
        BOOST_FOREACH(RefCountedAdaptorNamePair& i, refcountedadaptors) {
            if (i.second->getObject() == obj) {
                const char* dest = msg.destination();
                if (dest) {
                    i.second->getManager().addRef(connection, dest);
                }
                return i.first;
            }
        }
        return factory(obj, connection, basePath, msg);
    }
    /**
     * Return the object exposed at the given object path.  If one is not found
     * return NULL.  See caveat above.
     */
    boost::shared_ptr<BaseT> getFromPath(const std::string& path, const DBus::Message& = DBus::Message())
    {
        // This nested try, catch is in incredibly poor taste, sorry. -- Will
        // TODO: Make nicer
        try {
            try {
                return connection.get_adaptor_at_path<Adaptor>(path.c_str()).getObject();
            }
            catch (std::bad_cast&) {
                return connection.get_adaptor_at_path<RefCountedAdaptor>(path.c_str()).getObject();
            }
        }
        catch (DBus::Error&) {
            return boost::shared_ptr<BaseT>();
        }
    }
private:
    boost::function<std::string (boost::shared_ptr<BaseT>, DBus::Connection&, const std::string&, const DBus::Message&)> factory;
    DBus::Connection connection;
    const std::string basePath;
};

/**
 * A factory suitable to be provided to NonInheritingAdaptorFactory.  Should be
 * used for generated adaptors for objects that are to either stay on the bus
 * until teardown time or will be removed manually by calling into the
 * DBus::Connection (i.e. unrefcounted objects)
 */
template<class BaseT>
class ObjectAdaptorFactory
{
public:
    ObjectAdaptorFactory()
     : nextId(0)
    {
    }
    typedef std::string result_type;
    std::string operator() (boost::shared_ptr<BaseT> adaptee, DBus::Connection& conn, const std::string& root, const DBus::Message&)
    {
        std::stringstream ss;
        ss << root << nextId++;
        expose(conn, ss.str(), adaptee);
        return ss.str();
    }
private:
    int nextId;
};

/**
 * A factory suitable to be provided to NonInheritingAdaptorFactory.  Should be
 * used for generated adaptors for objects that are to stay on the bus for as
 * long as a remote process has a proxy for them.
 */
template<class BaseT>
class RefCountedAdaptorFactory
{
public:
    RefCountedAdaptorFactory(BusNameMonitor& bnm_)
     : nextId(0), bnm(bnm_)
    {
    }
    typedef std::string result_type;
    std::string operator() (boost::shared_ptr<BaseT> adaptee, DBus::Connection& conn, const std::string& root, const DBus::Message& msg)
    {
        const char* dest = msg.destination();
        if (dest == NULL) {
            throw std::runtime_error("RefCounted objects can only be created in calls or returns (i.e. messages that have a specific destination)");
        }
        std::stringstream ss;
        ss << root << nextId++;
        exposeRefCounted(conn, ss.str(), adaptee, bnm, dest);
        return ss.str();
    }
private:
    int nextId;
    BusNameMonitor& bnm;
};

NS_ZINC_DBUS_BINDING_CLOSE

#endif /* ZINC_DBUS_BINDING_NONINHERITINGADAPTORFACTORY_H_ */


={============================================================================
*kt_linux_core_701* dbus-introspection

{introspection}
D-Bus objects may support the interface org.freedesktop.DBus.Introspectable. This interface has one
method "Introspect" which takes no arguments and returns an XML string. The XML string describes the
interfaces, methods, and signals of the object. See the D-Bus specification for more details on this
introspection format. 


{signature-strings}
D-Bus uses a string-based type encoding mechanism called Signatures to describe the number and types
of arguments requried by methods and signals. Signatures are used for interface
declaration/documentation, data marshalling, and validity checking. Their string encoding uses a
simple, though expressive, format and a basic understanding of it is required for effective D-Bus
use. The table below lists the fundamental types and their encoding characters.


Character 	Code Data Type
y           8-bit unsigned integer
b           boolean value
n           16-bit signed integer
q           16-bit unsigned integer
i           32-bit signed integer
u           32-bit unsigned integer
x           64-bit signed integer
t           64-bit unsigned integer
d           double-precision floating point (IEEE 754)
s           UTF-8 string (no embedded nul characters)
o           D-Bus Object Path string
g           D-Bus Signature string
a           Array
(           Structure start
)           Structure end
v           Variant type (described below)
{           Dictionary/Map begin
}           Dictionary/Map end
h           Unix file descriptor

<example> xml has member and type defs as well
[17-02-2015 10:57:31.574183] signal sender=:1.0 -> dest=(null destination) serial=96 
path=/org/freedesktop/NetworkManager/Devices/0; interface=org.freedesktop.NetworkManager.Device; 
member=StateChanged

<node name="/" xmlns:tp="http://telepathy.freedesktop.org/wiki/DbusSpec#extensions-v0">
  <interface name="org.freedesktop.NetworkManager.Device">
    ...
    <signal name="StateChanged">
      <arg name="new_state" type="u" tp:type="NM_DEVICE_STATE">
        <tp:docstring>
          The new state of the device.
        </tp:docstring>
      </arg>
      <arg name="old_state" type="u" tp:type="NM_DEVICE_STATE">
        <tp:docstring>
          The previous state of the device.
        </tp:docstring>
      </arg>
      <arg name="reason" type="u" tp:type="NM_DEVICE_STATE_REASON">
        <tp:docstring>
          A reason for the state transition.
        </tp:docstring>
      </arg>
    </signal>

    <tp:enum name="NM_DEVICE_STATE" type="u">
      <tp:enumvalue suffix="UNKNOWN" value="0">
        <tp:docstring>
          The device is in an unknown state.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="UNMANAGED" value="1">
        <tp:docstring>
          The device is not managed by NetworkManager.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="UNAVAILABLE" value="2">
        <tp:docstring>
          The device cannot be used (carrier off, rfkill, etc).
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="DISCONNECTED" value="3">
        <tp:docstring>
          The device is not connected.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="PREPARE" value="4">
        <tp:docstring>
          The device is preparing to connect.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="CONFIG" value="5">
        <tp:docstring>
          The device is being configured.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="NEED_AUTH" value="6">
        <tp:docstring>
          The device is awaiting secrets necessary to continue connection.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="IP_CONFIG" value="7">
        <tp:docstring>
          The IP settings of the device are being requested and configured.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="ACTIVATED" value="8">
        <tp:docstring>
          The device is active.
        </tp:docstring>
      </tp:enumvalue>
      <tp:enumvalue suffix="FAILED" value="9">
        <tp:docstring>
          The device is in a failure state following an attempt to activate it.
        </tp:docstring>
      </tp:enumvalue>
    </tp:enum>
    ...

<example>

<yv:member type="a{ss}" name="shortTitle">

->

std::map< std::string, std::string > shortTitle;


<introspect>
dbus-send  --session --type=method_call --print-reply --dest='org.freedesktop.DBus' / org.freedesktop.DBus.Introspectable.Introspect

method return sender=org.freedesktop.DBus -> dest=:1.255 reply_serial=2
   string "<!DOCTYPE node PUBLIC "-//freedesktop//DTD D-BUS Object Introspection 1.0//EN"
"http://www.freedesktop.org/standards/dbus/1.0/introspect.dtd">
<node>
  <interface name="org.freedesktop.DBus"> ~
    <method name="Hello">
      <arg direction="out" type="s"/>
    </method>
    ...
    <method name="ListNames"> ~
      <arg direction="out" type="as"/>
    </method>
    ...
  </interface>
  <interface name="org.freedesktop.DBus.Introspectable"> ~
    <method name="Introspect">
      <arg direction="out" type="s"/>
    </method>
  </interface>
</node>
"


={============================================================================
*kt_linux_core_702* dbus: dbus-send tool

dbus-send, distributed with D-Bus, allows you to invoke methods on services from
the command line.

http://dbus.freedesktop.org/doc/dbus-send.1.html

dbus-send — Send a message to a message bus

Synopsis

dbus-send [ --system | --session | --address=ADDRESS ] [--dest=NAME] 
   [ --print-reply [=literal]] [--reply-timeout=MSEC] [--type=TYPE] 
   OBJECT_PATH INTERFACE.MEMBER [CONTENTS...]

DESCRIPTION

The dbus-send command is used to send a message to a D-Bus message bus. See
http://www.freedesktop.org/software/dbus/ for more information about the big
picture.

note: system vs session

There are two well-known message buses: the systemwide message bus (installed on
    many systems as the "messagebus" service) and the per-user-login-session
message bus (started each time a user logs in). 

The --system and --session options direct dbus-send to send messages to the
system or session buses respectively. note: If neither is specified, dbus-send
sends to the session bus.

Nearly all uses of dbus-send must provide the --dest argument which is the name
of a connection on the bus to send the message to. If --dest is omitted, no
destination is set.

note: message is either method or signal.

The object path and the name of the message to send must always be specified.
Following arguments, if any, are the message contents (message arguments). These
are given as type-specified values and may include containers (arrays, dicts,
        and variants) as described below.

<contents>   ::= <item> | <container> [ <item> | <container>...]
<item>       ::= <type>:<value>
<container>  ::= <array> | <dict> | <variant>
<array>      ::= array:<type>:<value>[,<value>...]
<dict>       ::= dict:<type>:<type>:<key>,<value>[,<key>,<value>...]
<variant>    ::= variant:<type>:<value>
<type>       ::= string | int16 | uint 16 | int32 | uint32 | int64 | uint64 
                        | double | byte | boolean | objpath

D-Bus supports more types than these, but dbus-send currently does not. Also,
dbus-send does not permit empty containers or nested containers (e.g. arrays of
        variants).

Here is an example invocation:

  dbus-send --dest=org.freedesktop.ExampleName               \
            /org/freedesktop/sample/object/name              \
            org.freedesktop.ExampleInterface.ExampleMethod   \
            int32:47 string:'hello world' double:65.32       \
            array:string:"1st item","next item","last item"  \
            dict:string:int32:"one",1,"two",2,"three",3      \
            variant:int32:-8                                 \
            objpath:/org/freedesktop/sample/object/name

  dbus-send --session --type=method_call --print-reply --dest=Zinc.MediaProxy2
  /Zinc/Media/MediaRouters/0 Zinc.Media.MediaRouter.getSourceInformation

Note that the interface is separated from a method or signal name by a dot,
though in the actual protocol the interface and the interface member are
    separate fields.  

OPTIONS

The following options are supported:

--dest=NAME
    Specify the name of the connection to receive the message.

--print-reply
    'block' for a reply to the message sent, and print any reply received in a
    human-readable form. It also means the message type (--type=) is
    method_call.

--print-reply=literal
    Block for a reply to the message sent, and print the body of the reply. If
    the reply is an object path or a string, it is printed literally, with no
    punctuation, escape characters etc.

--reply-timeout=MSEC
    Wait for a reply for up to MSEC milliseconds. The default is
    implementation-defined, typically 25 seconds.

--system
    Send to the system message bus.

--session
    Send to the session message bus. (This is the default.)

--address=ADDRESS
    Send to ADDRESS.

--type=TYPE
    Specify method_call or signal (defaults to "signal").


={============================================================================
*kt_linux_core_703* dbus: lsdbus

Get the list of activateable bus names
dbus-send  --session --type=method_call --print-reply --dest='org.freedesktop.DBus' \
/ org.freedesktop.DBus.ListActivatableNames

to find the owners of all dbus connections
dbus-send  --session --type=method_call --print-reply --dest='org.freedesktop.DBus' \
/ org.freedesktop.DBus.ListNames

note: org.freedesktop.Dbus means the dbus itself and ListNmaes to get the list of names on the bus

method return sender=org.freedesktop.DBus -> dest=:1.81 reply_serial=2
   array [
      string "org.freedesktop.DBus"
      string ":1.7"
      string ":1.8"
      string "Zinc.BabySitter"
      string "Zinc.Application"
      string "org.freedesktop.NetworkManager"
      string "Zinc.Crb"
      string "Zinc.OEMSystemTime"
      string ":1.81"
      string "Zinc.ApplicationPackages"
      string ":1.41"
      string "Zinc.UsageCollection"
      string ":1.42"
      string ":1.65"
      string ":1.21"
      string ":1.43"
      string ":1.66"
      string ":1.44"
      string ":1.67"
      string ":1.45"
      string "org.freedesktop.NetworkManagerSystemSettings"
      string ":1.46"
      string ":1.69"
      string "Zinc.Media"
      string ":1.47"
      string ":1.48"
      string "Zinc.OEMSystem"
      string ":1.29"
      string "Zinc.ContentAcquisition"
      string "Zinc.Broker"
      string "org.freedesktop.Avahi"
      string "Zinc.DeviceSoftware"
      string "Zinc.RemoteBooking"
      string "Zinc.LinearSource"
      string "Zinc.DeviceManager"
      string "Zinc.Tuner"
      string ":1.70"
      string ":1.71"
      string "Zinc.OEMSystemManager"
      string ":1.72"
      string ":1.73"
      string ":1.75"
      string "Zinc.MetadataProxy"
      string ":1.53"
      string ":1.76"
      string "Zinc.Reminders"
      string ":1.34"
      string "Zinc.Metadata"
      string "Zinc.System"
      string ":1.0"
      string ":1.58"
      string "Zinc.RemoteDiagnostics"
      string ":1.4"
      string ":1.5"
      string ":1.18"
      string ":1.19"
   ]


# lsdbus
793   :1.0                     /opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
793   org.freedesktop.NetworkManager/opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
793   org.freedesktop.NetworkManagerSystemSettings/opt/zinc/oss/sbin/NetworkManager --no-daemon --log-level=INFO        
831   :1.4                     /opt/zinc/bin/litaniumsystemmanagerd            
831   :1.5                     /opt/zinc/bin/litaniumsystemmanagerd            
831   Zinc.OEMSystemManager    /opt/zinc/bin/litaniumsystemmanagerd            
831   Zinc.System              /opt/zinc/bin/litaniumsystemmanagerd            
...


#cat /opt/zinc/devel/bin/lsdbus
#!/bin/sh -e

# Print a list of all dbus connections and the processes that own them.
# Taken from the wiki page:
# https://wiki.youview.co.uk/display/canvas/How+to+Introspect+DBus+from+the+Command+Line

# The format of the output is:
# pid | bus name | command

function ListBusNames {
    dbus-send --session --type=method_call --print-reply \
              --dest=org.freedesktop.DBus / \
              org.freedesktop.DBus.ListNames
}

function GetProcessID {
    dbus-send --session --type=method_call --print-reply \
              --dest=org.freedesktop.DBus / \
              org.freedesktop.DBus.GetConnectionUnixProcessID string:$1 2>/dev/null \
        | xargs -n1 | tail -1
}

for i in $(ListBusNames | grep string | cut -d'"' -f2)
do
    DCNAME=$i
    DCPID=$(GetProcessID $DCNAME)
    if [ -n "$DCPID" ]
    then
        DCPCMD=$(cat /proc/$DCPID/cmdline 2>/dev/null | xargs -0 echo) && \
               printf "%-6s%-25s%-40s%-8s%s\n" \
                  "$DCPID" "$DCNAME" "$DCPCMD"
    fi
done | sort -n


={============================================================================
*kt_linux_core_704* dbus: dbus-monitor tool

http://dbus.freedesktop.org/doc/dbus-monitor.1.html

Distributed with D-Bus, prints out traffic on the bus. You can filter the output by passing match
rules as arguments. 

dbus-monitor [ --system | --session | --address ADDRESS ] 
  [ --profile | --monitor ] [ watch expressions ]

<session>
There are two well-known message buses: the systemwide message bus; installed on
many systems as the "messagebus" service and the per-user-login-session message
bus; started each time a user logs in.  The --system and --session options
direct dbus-monitor to monitor the system or session buses respectively. 

note:
If neither is specified, dbus-monitor monitors the session bus.

<monitor>
dbus-monitor has two different output modes, the 'classic'-style monitoring mode
and profiling mode.  The profiling format is a compact format with a single line
per message and microsecond-resolution timing information. The --profile and
--monitor options select the profiling and monitoring output format
respectively. 

note: If neither is specified, dbus-monitor uses the monitoring output format.


<profile>
--profile
    Use the profiling output format.

[root@HUMAX /]# /opt/zinc-trunk/oss/bin/dbus-monitor --profile 
  "interface=Zinc.Media.MediaRouter,member=getSourceInformation" "type=method_call" "type=method_return"

sig	1442323921	119827	2	/org/freedesktop/DBus	org.freedesktop.DBus	NameAcquired
mc	1442323921	124985	3	:1.167	/org/freedesktop/DBus	org.freedesktop.DBus	AddMatch
mc	1442323921	128350	4	:1.167	/org/freedesktop/DBus	org.freedesktop.DBus	AddMatch
mc	1442323923	728265	1587	:1.72	/Zinc/Tuner/LinearPlaybackControl	Zinc.Tuner.LinearPlaybackControl	getSourceInformation
mc	1442323923	731820	1424	:1.80	/Zinc/Media/DefaultMediaRouter	Zinc.Media.MediaRouter	getSourceInformation
mr	1442323923	734371	772	1424	:1.80
mr	1442323923	735621	1425	1587	:1.72
mc	1442323927	846109	1588	:1.72	/Zinc/Broker/UnifiedEventRepository	Zinc.Broker.UnifiedEventRepository	getScheduleEvents
mc	1442323927	854912	3206	:1.22	/Zinc/Metadata/EventRepository	Zinc.Metadata.EventRepository	getScheduleEvents
mr	1442323927	868006	1366	3206	:1.22
mr	1442323927	884479	3207	1588	:1.72
mc	1442323927	890274	1589	:1.72	/Zinc/Broker/UnifiedEventRepository	Zinc.Broker.UnifiedEventRepository	getScheduleEvents
mc	1442323927	895714	3208	:1.22	/Zinc/Metadata/EventRepository	Zinc.Metadata.EventRepository	getScheduleEvents
mr	1442323927	901114	1367	3208	:1.22
mr	1442323927	906785	3209	1589	:1.72
mc	1442323927	910330	1590	:1.72	/Zinc/Broker/UnifiedEventRepository	Zinc.Broker.UnifiedEventRepository	getScheduleEvents


<watch-expr>
In order to get dbus-monitor to see the messages you are interested in, you
should specify a set of watch expressions as you would expect to be passed to
the dbus_bus_add_match function.

# display only method calls, returns and errors. No signals at all will be
# displayed but it does have the benefit of you not getting the PositionChange
# signals cluttering your display.

dbus-monitor "type=method_call" "type=method_return" "type=error"

dbus-monitor profile 
  "interface=Zinc.Application.ApplicationManager,member=launchApplication" \ 
  "interface=Zinc.Application.ApplicationManager,member=ApplicationLifecycleEvent"

dbus-monitor > /mnt/hd1/mylogs.log &
dbus-monitor | tee /mnt/hd1/mylogs.log &


# To launch Dbus-Monitor on your STB, and inspect MediaRouter activity, run the
# following command:

dbus-monitor "interface=Zinc.Media.MediaRouter,member=getSourceInformation"
  "type=method_call" "type=method_return"

<log>
[07-07-2015 07:45:04.848504] 
method call sender=:1.246 -> dest=Zinc.Media serial=2
path=/Zinc/Media/DefaultMediaRouter; interface=Zinc.Media.MediaRouter;
member=stop

[07-07-2015 07:45:04.864666] 
method return sender=:1.6 -> dest=:1.246 reply_serial=2

<>

// 1.247 becomes dbussenddaemon

[07-07-2015 07:45:05.164966] 
signal sender=org.freedesktop.DBus -> dest=(null destination) 
  serial=413 path=/org/freedesktop/DBus;
  interface=org.freedesktop.DBus; member=NameOwnerChanged string
  "Zinc.DBusSendDaemon" string "" string ":1.247"

[07-07-2015 07:45:05.165242] 
method call sender=:1.247 -> dest=org.freedesktop.DBus 
  serial=2 path=/org/freedesktop/DBus; 
  interface=org.freedesktop.DBus; member=RequestName
  string "Zinc.DBusSendDaemon"
  uint32 0

// dbussenddaemon fowards a call

[07-07-2015 07:45:07.926145] 
method call sender=:1.249 -> dest=Zinc.DBusSendDaemon serial=2
path=/Zinc/Media/MediaRouterFactory; interface=Zinc.Media.MediaRouterFactory;
  member=createMediaRouter string "Zinc.MediaProxy"

[07-07-2015 07:45:07.935001] 
method call sender=:1.247 -> dest=Zinc.MediaProxy serial=3
path=/Zinc/Media/MediaRouterFactory; interface=Zinc.Media.MediaRouterFactory;
  member=createMediaRouter


// fowards a reply

[07-07-2015 07:45:07.945512] 
method return sender=:1.231 -> dest=:1.247 reply_serial=3 object path
"/Zinc/Media/MediaRouters/1"

[07-07-2015 07:45:07.948415] 
method return sender=:1.247 -> dest=:1.249 reply_serial=2 object path
"/Zinc/Media/MediaRouters/1"


={============================================================================
*kt_linux_core_710* dbus: kdbus

{kdbus}
https://github.com/gregkh/presentation-kdbus
https://github.com/gregkh/kdbus


={============================================================================
*kt_linux_core_800* proc: /proc/mounts

The proc(5) manual page.

from stackoverflow:
The definitive list of mounted filesystems in in /proc/mounts.

If you have any form of containers on your system, /proc/mounts only lists the filesystems that are
in your present container. For example, in a chroot, /proc/mounts lists only the filesystems whose
mount point is within the chroot. (There are ways to escape the chroot, mind.)

There's also a list of mounted filesystems in /etc/mtab. This list is maintained by the mount and
umount commands. That means that if you don't use these commands (which is pretty rare), your action
(mount or unmount) won't be recorded. In practice, it's mostly in a chroot that you'll find
/etc/mtab files that differ wildly from the state of the system (also mounts performed in the chroot
    will be reflected in the chroot's /etc/mtab but not in the main /etc/mtab). 

Actions performed while /etc/mtab is on a read-only filesystem are also not recorded there. The
reason why you'd sometimes want to consult /etc/mtab in preference to or in addition to /proc/mounts
is that because it has access to the mount command line, it's sometimes able to present information
in a way that's easier to understand; for example you see mount options as requested (whereas
    /proc/mounts lists the mount and kernel defaults as well), and bind mounts appear as such in
/etc/mtab.


={============================================================================
*kt_linux_core_801* proc-status: /proc/PID/status

The parent of any process can be found by looking at the Ppid field provided in
the Linux-specific /proc/PID/status file.

Uid:  1024  1024  1024  1024
Gid:  1025  1025  1025  1025

The credentials of any process can be found by examining the Uid, Gid, and
Groups lines provided in the Linux-specific /proc/PID/status file. The Uid and
Gid lines list the identifiers in the order real, effective, saved set, and file
system.

FDSize:	32
Groups:	
VmPeak:	    2292 kB
VmSize:	    2288 kB
VmLck:	       0 kB
VmPin:	       0 kB
VmHWM:	     752 kB
VmRSS:	     752 kB
VmData:	     164 kB
VmStk:	     136 kB
VmExe:	      32 kB
VmLib:	    1904 kB
VmPTE:	      16 kB
VmSwap:	       0 kB
Threads:	1


              * VmPeak: Peak virtual memory size.

              * VmSize: Virtual memory size.

<signals>
SigQ: 0/3941
SigPnd: 0000000000000000
ShdPnd: 0000000000000000
SigBlk: 0000000000000000
SigIgn: fffffffe57f0d8fc
SigCgt: 00000000280b2603

CapInh:	0000000000000000
CapPrm:	ffffffffffffffff
CapEff:	ffffffffffffffff
CapBnd:	ffffffffffffffff
Cpus_allowed:	1
Cpus_allowed_list:	0
Mems_allowed:	1
Mems_allowed_list:	0
voluntary_ctxt_switches:	310
nonvoluntary_ctxt_switches:	127


={============================================================================
*kt_linux_core_802* proc-maps: /proc/PID/maps

Using the Linux-specific /proc/PID/maps file, we can see the location of the
shared memory segments and shared libraries mapped by a program.

From LPI 48.5.

$ cat /proc/9903/maps
08048000-0804a000 r-xp 00000000 08:05 5526989 /home/mtk/svshm_attach          <1>
0804a000-0804b000 r--p 00001000 08:05 5526989 /home/mtk/svshm_attach
0804b000-0804c000 rw-p 00002000 08:05 5526989 /home/mtk/svshm_attach
b7bed000-b7f0d000 rw-s 00000000 00:09 9666565 /SYSV00000000 (deleted)         <2>
b7f0d000-b7f26000 rw-s 00000000 00:09 9633796 /SYSV00000000 (deleted)
b7f26000-b7f27000 rw-p b7f26000 00:00 0
b7f27000-b8064000 r-xp 00000000 08:06 122031 /lib/libc-2.8.so                 <3>
b8064000-b8066000 r--p 0013d000 08:06 122031 /lib/libc-2.8.so
b8066000-b8067000 rw-p 0013f000 08:06 122031 /lib/libc-2.8.so
b8067000-b806b000 rw-p b8067000 00:00 0
b8082000-b8083000 rw-p b8082000 00:00 0
b8083000-b809e000 r-xp 00000000 08:06 122125 /lib/ld-2.8.so                   <4>
b809e000-b809f000 r--p 0001a000 08:06 122125 /lib/ld-2.8.so
b809f000-b80a0000 rw-p 0001b000 08:06 122125 /lib/ld-2.8.so
bfd8a000-bfda0000 rw-p bffea000 00:00 0 [stack]                               <5>
ffffe000-fffff000 r-xp 00000000 00:00 0 [vdso]                                <6>

<1> Three lines for the main program, shm_attach. These correspond to the text and data segments of
the program. The second of these lines is for a readonly page holding the string constants used by
the program.

<2> Two lines for the attached System V shared memory segments.

<3> Lines corresponding to the segments for two shared libraries. One of these is the standard C
library (libc-version.so). 

<4> The other is the dynamic linker (ld-version.so).

<5> A line labeled [stack]. This corresponds to the process stack.

<columns>
1. A pair of hyphen-separated numbers indicating the virtual address range at which the memory
segment is mapped. The second of these numbers is the address of the next byte after the end of the
segment.

note: From this, can get the size. b7f0d000-b7bed000=0x320000. 3,276,800

2. Protection and flags for this memory segment. The first three letters indicate the protection of
the segment: read (r), write (w), and execute (x). A hyphen (-) in place of any of these letters
indicates that the corresponding protection is disabled. The final letter indicates the mapping flag
for the memory segment; it is either private (p) or shared (s). For an explanation of these flags,
    see the description of the MAP_PRIVATE and MAP_SHARED flags in Section 49.2. 
    
note: A System V shared memory segment is always marked shared.

3. The hexadecimal offset (in bytes) of the segment within the corresponding mapped file. The
meanings of this and the following two columns will become clearer when we describe the mmap()
system call in Chapter 49. 

note: For a System V shared memory segment, the offset is always 0.

4. The device number (major and minor IDs) of the device on which the corresponding mapped file is
located.

5. The i-node number of the mapped file, or, for System V shared memory segments, the 'identifier'
for the segment.

6. The filename or other identifying tag associated with this memory segment. For a System V shared
memory segment, this consists of the string SYSV concatenated with the shmget() key of the segment
(expressed in hexadecimal). In this example, SYSV is followed by zeros because we created the
segments using the key IPC_PRIVATE (which has the value 0). The string (deleted) that appears after
the SYSV field for a System V shared memory segment is an artifact of the implementation of shared
memory segments. Such segments are created as mapped files in an invisible tmpfs file system
(Section 14.10), and then later unlinked. Shared anonymous memory mappings are implemented in the
same manner. (We describe mapped files and shared anonymous memory mappings in Chapter 49.)

<smaps>
Starting with kernel 2.6.14, Linux also provides the /proc/PID/smaps file, which exposes more
information about the memory consumption of each of a process's mappings. For further details, see
the proc(5) manual page.

The snippet from proc man and see man page for more.

/proc/[pid]/smaps (since Linux 2.6.14)

This file shows memory consumption for each of the process's mappings. (The pmap(1) command displays
    similar information, in a form that may be easier for parsing.) For each mapping there is a
series of lines such as the following:

00400000-0048a000 r-xp 00000000 fd:03 960637       /bin/bash
Size:                552 kB
Rss:                 460 kB
Pss:                 100 kB
Shared_Clean:        452 kB
Shared_Dirty:          0 kB
Private_Clean:         8 kB
Private_Dirty:         0 kB
Referenced:          460 kB
Anonymous:             0 kB
AnonHugePages:         0 kB
Swap:                  0 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB
Locked:                0 kB


={============================================================================
*kt_linux_core_802* proc-maps: check shared libraries that process uses

cat /proc/NNNN/maps | awk '{print $6}' | grep '\.so' | sort | uniq

or

can use ldd.



={============================================================================
*kt_linux_core_803* proc: /proc/PID/exe

The Linux-specific /proc/PID/exe file is a symbolic link containing the absolute pathname of the
executable file being run by the corresponding process.

$ ls -1l /proc/3510/exe
lrwxrwxrwx 1 keitee keitee 0 Feb  2 22:21 /proc/3510/exe -> /home/keitee/bin/vim

note: to check the path name of the process.


={============================================================================
*kt_linux_core_804* proc: /proc/PID/fd. which file does process open?

/proc/[pid]/fd

This is a subdirectory containing one entry for each file which the process has open, named by its
file descriptor, and which is a symbolic link to the actual file. Thus, 0 is standard input, 1
standard output, 2 standard error, etc.

In a multithreaded process, the contents of this directory are not available if the main thread has
already terminated (typically by calling pthread_exit(3)).

root       793  0.7  1.0  25528  4176 ?        Sl   16:04   0:00 /opt/zinc/oss/sbin/NetworkManager 
   --no-daemon --log-level=INFO

# ll /proc/793/fd
fd/     fdinfo/ 

# ll /proc/793/fd/
dr-x------    2 root     root           0 Feb 16 16:06 ./
dr-xr-xr-x    6 root     root           0 Jan  1  2000 ../
lr-x------    1 root     root          64 Feb 16 16:06 0 -> /dev/null
l-wx------    1 root     root          64 Feb 16 16:06 1 -> /var/tmp/NetworkManager.log
lrwx------    1 root     root          64 Feb 16 16:06 10 -> /dev/nexus_astm
lrwx------    1 root     root          64 Feb 16 16:06 11 -> /dev/nexus_display
lrwx------    1 root     root          64 Feb 16 16:06 12 -> /dev/nexus_graphics2d
lrwx------    1 root     root          64 Feb 16 16:06 13 -> /dev/nexus_surface
lrwx------    1 root     root          64 Feb 16 16:06 14 -> /dev/nexus_audio
lrwx------    1 root     root          64 Feb 16 16:06 15 -> /dev/nexus_video_decoder
lrwx------    1 root     root          64 Feb 16 16:06 16 -> /dev/nexus_transport
lrwx------    1 root     root          64 Feb 16 16:06 17 -> /dev/nexus_dma
lrwx------    1 root     root          64 Feb 16 16:06 18 -> /dev/nexus_security
lrwx------    1 root     root          64 Feb 16 16:06 19 -> /dev/nexus_spi
l-wx------    1 root     root          64 Feb 16 16:06 2 -> /var/tmp/NetworkManager.log
lrwx------    1 root     root          64 Feb 16 16:06 20 -> /dev/nexus_frontend
lrwx------    1 root     root          64 Feb 16 16:06 21 -> /dev/nexus_keypad
lrwx------    1 root     root          64 Feb 16 16:06 22 -> /dev/nexus_rfm
lrwx------    1 root     root          64 Feb 16 16:06 23 -> /dev/nexus_uhf_input
lrwx------    1 root     root          64 Feb 16 16:06 24 -> /dev/nexus_input_capture
lrwx------    1 root     root          64 Feb 16 16:06 25 -> /dev/nexus_ir_blaster
lrwx------    1 root     root          64 Feb 16 16:06 26 -> /dev/nexus_ir_input
lrwx------    1 root     root          64 Feb 16 16:06 27 -> /dev/nexus_led
lrwx------    1 root     root          64 Feb 16 16:06 28 -> /dev/nexus_gpio
lrwx------    1 root     root          64 Feb 16 16:06 29 -> /dev/nexus_i2c
lr-x------    1 root     root          64 Feb 16 16:06 3 -> pipe:[548]|
lrwx------    1 root     root          64 Feb 16 16:06 30 -> /dev/nexus_pwm
...


={============================================================================
*kt_linux_core_813* proc: /proc/sys/kernel, system resource limits

Various files under the /proc/sys/kernel directory can be used to view and modify these limits.

On Linux, the ipcs -l command can be used to list the limits on each of the IPC mechanisms. 


={============================================================================
*kt_linux_core_814* proc: /proc/stat

root# cat /proc/stat 
cpu  33343 0 15876 89567 4707 788 3710 0 0
cpu0 15661 0 8726 40043 3559 788 3698 0 0
cpu1 17681 0 7149 49524 1147 0 11 0 0
intr 3547815 0 125304 0 2 128777 0 1798 0 3886 2 7 0 0 71484 0 2 42852 0 1 5264 435754 0 5346 0 0 0 0 0 0 0 0 34415 47045 0 0 0 0 0 0 0 0 0 27337 0 0 0 0 0 7031 0 0 0 0 0 0 0 0 0 16826 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2181 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 65562 107793 0 0 0 0 0 2419144 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
ctxt 4296231
btime 1429628483
processes 1602
procs_running 4
procs_blocked 0
softirq 2964653 827836 1298645 1927 47517 27361 5147 32904 1321 721995

cpu  3357 0 4313 1362393
The amount of time, measured in units of USER_HZ (1/100ths of a second on most architectures, use sysconf(_SC_CLK_TCK) to obtain the right value), 

1. that  the  system spent  in  user  mode, 
2. user mode with low priority (nice), 
3. system mode, and 
4. the idle task, respectively.  

The last value should be USER_HZ times the second entry in the uptime pseudo-file.

In Linux 2.6 this line includes three additional columns: 

5. iowait - time waiting for I/O  to  complete  (since  2.5.41);  
6. irq  -  time  servicing  interrupts  (since 2.6.0-test4); 
7. softirq - time servicing softirqs (since 2.6.0-test4).

Since Linux 2.6.11, there is an eighth column, 
      
8. steal - stolen time, which is the time spent in other operating systems when running in a
    virtualized environment

Since Linux 2.6.24, there is a ninth column, 

9. guest, which is the time spent running a virtual CPU for guest operating systems under the
    control of the Linux kernel.


<example>
/**
 * Abstraction of proc/stats file format to hold the data and to
 *   allow calculation of the CPU utilisation based on it.
 */
struct ProcStats
{
    long user;   // Time spent executing user applications (user mode).
    long nice;   // Time spent executing user applications with low priority (nice).
    long system; // Time spent executing system calls (system mode).
    long idle;   // Idle time.
    long iowait; // Time waiting for I/O operations to complete.
    long irq;    // Time spent servicing interrupts.
    long softirq;// Time spent servicing soft-interrupts.

    ProcStats operator-(const ProcStats& other)
    {
        ProcStats res;
        res.user    = user   - other.user;
        res.nice    = nice   - other.nice;
        res.system  = system - other.system;
        res.idle    = idle   - other.idle;
        res.iowait  = iowait - other.iowait;
        res.irq     = irq    - other.irq;
        res.softirq = softirq- other.softirq;
        return res;
    }

    /**
     * Returns calculated total cpu utilisation in percentage.
     */
    double cpuUtilisation() const
    {
        double cpu_total = user + nice + system + irq + softirq;
        double cpu_idle  = idle + iowait;
        return cpu_total / (cpu_total + cpu_idle) * 100; // in percentage
    }
};

inline std::istream& operator>>(std::istream& in, ProcStats& st)
{
    std::string cpu;
    if (in >> cpu)
    {
        in >> st.user >> st.nice >> st.system >> st.idle;
        in >> st.iowait >> st.irq >> st.softirq;
    }
    return in;
}


/**
 * Objects of this type can read (repeatedly) cpu stats file and compare
 * it against values read previously.
 */
class CpuProcStatsReader
{
public:
    CpuProcStatsReader() :
        stat_filename("/proc/stat"),
        now_first(false)
    {
    }

    /**
     * Reads current value of /proc/stats and returns ProcStats with differences
     * with values read during last call to get().
     */
    ProcStats get()
    {
        std::string cpu;
        std::ifstream f(stat_filename.c_str());
        ProcStats& st = curr();
        f >> st;
        now_first = !now_first;
        return now_first ? stats.second - stats.first : stats.first - stats.second;
    }

    /**
     * Helper function (useful rather for testing) to setup a location of
     *   /proc/stat file.
     */
    void setupProcStatLocation(const std::string& filename)
    {
        stat_filename = filename;
    }

private:
    /**
     * Internal method to get the least recently used stats that is to be used/updated.
     */
    ProcStats& curr()
    {
        return now_first ? stats.first : stats.second;
    }

    std::pair<ProcStats, ProcStats> stats;
    std::string stat_filename;
    bool now_first;
};


={============================================================================
*kt_linux_core_814* proc: /proc/PID/cmdline, get process name

{
  printf( "Nexus resources are owned by PID %d\n", sharedMem->resource_pid);
  char name[512];
  sprintf(name, "cat /proc/%d/cmdline", sharedMem->resource_pid);
  system(name);
}


# ============================================================================
#{ SYSCALL
={============================================================================
*kt_linux_sysc_001* syscall list

What syscalls are valid depends on the OS. On GNU and Unix systems, you can find
the full list of valid syscall names on /usr/include/asm/unistd.h.


={============================================================================
*kt_linux_sysc_001* pause

pause() causes the calling process (or thread) to sleep until a signal is delivered that either
terminates the process or causes the invocation of a signal-catching function.


={============================================================================
*kt_linux_sysc_002* alarm

Since the final for loop of the program loops forever, this program uses alarm() to establish a
timer to deliver SIGALRM. The arrival of an unhandled SIGALRM signal guarantees process termination,
if the process is not other- wise terminated.

unsigned int alarm(unsigned int seconds);

alarm() arranges for a SIGALRM signal to be delivered to the calling process in seconds seconds.

int main(int argc, char *argv[])
{
  /* An unhandled SIGALRM ensures this process will die if nothing else terminates it */
  alarm(60);

  for(;;) {
    /* Wait for signals */
    pause();
    printf("%ld: caught SIGHUP\n", (long) getpid());
  }
}


={============================================================================
*kt_linux_sysc_003* getenv, setenv

<getenv>
#include <stdlib.h>

char *getenv(const char *name);

RETURN VALUE
The getenv() function returns a pointer to the value in the environment, or NULL
if there is no match.

note:
"no match" means when there is no env variable set so when "SAMPLE_ENV=" and it
has null but is set, getenv returns "\0" null string but not null pointer. So
need to use strcmp to check if env variable is set or unset since check on NULL
means that env not defined but not set. 


<setenv>
int setenv(const char *name, const char *value, int overwrite);

The setenv() function adds the variable name to the environment with the value
  value, if name does not already exist. If name does exist in the environment,
  then its value is changed to value 'if' overwrite is 'nonzero'; if overwrite
    is zero, then the value of name is not changed. This function makes copies
    of the strings pointed to by name and value (by contrast with putenv(3)).


# ============================================================================
#{ LINUX SYS ADMIN
# ============================================================================
#{
={============================================================================
*kt_linux_sete_001* set-resize-boot-partition

https://www.nonamehosts.com/blog/how-to-extend-ext4-root-partition-without-reboot/

note:
Works for Debian Jessy

How to extend ext4 root partition without reboot

Let’s say you want to extend existing disk on your already installed VM.
Usually to extend the existing ext4 partition where system is running you
would use some Live CD, to edit partition while it’s unmounted. However it’s
possible to extend the partition without booting from Live CD. Here are some
simple steps to do so:

Here we see that we have disk bigger than already existing partition:

root@test:~# df -h /
Filesystem Size Used Avail Use% Mounted on
/dev/vda1 24G 1008M 22G 5% /
 
root@test:~# fdisk -l /dev/vda
 
Disk /dev/vda: 53.7 GB, 53687091200 bytes
255 heads, 63 sectors/track, 6527 cylinders, total 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0009256d
 
 Device Boot Start End Blocks Id System
/dev/vda1 * 2048 50329215 25163584 83 Linux
/dev/vda2 50329216 52426367 1048576 82 Linux swap / Solaris

First of all you have to make sure to turn off swap:
root@test:~# swapoff -a
root@test:~# free -m
             total       used       free     shared    buffers     cached
Mem:           994        189        804          0         11        140
-/+ buffers/cache:         38        955
Swap:            0          0          0

#Make sure that you see zeroes in "Swap:" row

And now let’s remove swap partition and extend root partition. From previous
fdisk command we can see that our swap partition is second one (/dev/vda2).

In following steps we’re going to remove swap partition and root partition,
                           create root partition with new size (size should be
                               = [last sector]  [swap partition sector count])
                             and finally new swap partition with same size.

root@test:~# fdisk /dev/vda

Command (m for help): p

Disk /dev/vda: 53.7 GB, 53687091200 bytes
255 heads, 63 sectors/track, 6527 cylinders, total 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0009256d

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048    50329215    25163584   83  Linux
/dev/vda2        50329216    52426367     1048576   82  Linux swap / Solaris

Command (m for help): d
Partition number (1-4): 2

Command (m for help): d
Selected partition 1

Command (m for help): n
Partition type:
   p   primary (0 primary, 0 extended, 4 free)
   e   extended
Select (default p):
Using default response p
Partition number (1-4, default 1):
Using default value 1
First sector (2048-104857599, default 2048):
Using default value 2048
Last sector, +sectors or +size{K,M,G} (2048-104857599, default 104857599): +103809023

Command (m for help): n
Partition type:
   p   primary (1 primary, 0 extended, 3 free)
   e   extended
Select (default p):
Using default response p
Partition number (1-4, default 2):
Using default value 2
First sector (103811072-104857599, default 103811072):
Using default value 103811072
Last sector, +sectors or +size{K,M,G} (103811072-104857599, default 104857599):
Using default value 104857599

Command (m for help): p

Disk /dev/vda: 53.7 GB, 53687091200 bytes
255 heads, 63 sectors/track, 6527 cylinders, total 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0009256d

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1            2048   103811071    51904512   83  Linux
/dev/vda2       103811072   104857599      523264   83  Linux


Before writing changes to disk we have to set proper partition types and make
root partition bootable:

Command (m for help): t
Partition number (1-4): 1
Hex code (type L to list codes): 83

Command (m for help): t
Partition number (1-4): 2
Hex code (type L to list codes): 82
Changed system type of partition 2 to 82 (Linux swap / Solaris)

Command (m for help): a
Partition number (1-4): 1

Command (m for help): p

Disk /dev/vda: 53.7 GB, 53687091200 bytes
255 heads, 63 sectors/track, 6527 cylinders, total 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0009256d

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048   103811071    51904512   83  Linux
/dev/vda2       103811072   104857599      523264   82  Linux swap / Solaris
	

Now let’s write changes to disk and re-read partition table (to re-read
partition table you can also reboot the VM) so the system could see new
size:

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.

WARNING: Re-reading the partition table failed with error 16: Device or resource busy.
The kernel still uses the old table. The new table will be used at
the next reboot or after you run partprobe(8) or kpartx(8)
Syncing disks.
root@test:~# partprobe
root@test:~# fdisk -l

Disk /dev/vda: 53.7 GB, 53687091200 bytes
16 heads, 63 sectors/track, 104025 cylinders, total 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0009256d

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048   103811071    51904512   83  Linux
/dev/vda2       103811072   104857599      523264   82  Linux swap / Solaris
	

Now we can see that we have new partition table with resized partitions,
                           however root partition’s filesystem has to be
                             resized as well. It’s one simple step left:

root@test:~# resize2fs /dev/vda1
resize2fs 1.42.9 (4-Feb-2014)
Filesystem at /dev/vda1 is mounted on /; on-line resizing required
old_desc_blocks = 2, new_desc_blocks = 4
The filesystem on /dev/vda1 is now 12976128 blocks long.

root@test:~# df -h /
Filesystem      Size  Used Avail Use% Mounted on
/dev/vda1        49G 1009M   46G   3% /
	
Resize2fs did the trick and you can see that we have 49G partition now.


={============================================================================
*kt_linux_sete_002* set-bugzilla

On debian:

apt-get install apache2 mysql-server libappconfig-perl libdate-calc-perl\
libtemplate-perl libmime-perl build-essential libdatetime-timezone-perl\
libdatetime-perl libemail-sender-perl libemail-mime-perl\
libemail-mime-modifier-perl libdbi-perl libdbd-mysql-perl libcgi-pm-perl\
libmath-random-isaac-perl libmath-random-isaac-xs-perl apache2-mpm-prefork\
libapache2-mod-perl2 libapache2-mod-perl2-dev libchart-perl libxml-perl\
libxml-twig-perl perlmagick libgd-graph-perl libtemplate-plugin-gd-perl\
libsoap-lite-perl libhtml-scrubber-perl libjson-rpc-perl libtheschwartz-perl\
libtest-taint-perl libauthen-radius-perl libfile-slurp-perl\
libencode-detect-perl libmodule-build-perl libnet-ldap-perl libfile-which-perl\
libauthen-sasl-perl libtemplate-perl-doc libfile-mimeinfo-perl\
libhtml-formattext-withlinks-perl libgd-dev libmysqlclient-dev lynx-cur\
graphviz python-sphinx rst2pdf

note:
removed libdaemon-generic-perl since cannot find it for debian.


={============================================================================
*kt_linux_sete_003* set-samba

sudo service smbd start
sudo service smbd stop
sudo service smbd restart
sudo systemctl restart smbd.service

To see what are shared:

$ smbclient -L //106.1.8.6/
Enter keitee.park's password:
Domain=[SERILOCAL] OS=[Unix] Server=[Samba 3.6.3]

        Sharename       Type      Comment
        ---------       ----      -------
        IPC$            IPC       IPC Service (rockford server (Samba, Ubuntu))
        dsk1            Disk      Main Disk
Domain=[SERILOCAL] OS=[Unix] Server=[Samba 3.6.3]

        Server               Comment
        ---------            -------

        Workgroup            Master
        ---------            -------
        WORKGROUP


https://www.howtoforge.com/tutorial/debian-samba-server/

3.3 Anonymous share

You like to have a share were all users in your network can write to? Be
careful, this share is open to anyone in the network, so use this only in
local networks. Add an anonymous share like this:

/etc/samba/smb.conf

[anonymous]
   path = /home/shares/anonymous
   force group = users
   create mask = 0660
   directory mask = 0771
   browsable =yes
   writable = yes
   guest ok = yes

note:
If `path` is wrong then PC host see anonymous share but cannot access through
it.


<no-write>
However, the above setting has no write and tried others such as:

1. force user = username
2. username map
  [global]
    username map = /pathToMapFile/usermap.txt

Neither works and the easiest is:

 Re: Can read but not write to Samba Share with Win7

    [ShareName]
    path = /home/username/
    #read only = no
    writeable = yes
    browseable = yes

    Your share has two problems unless you did some other things that you
    didn't post.

    It's not allowing guest access so you needed to create a samba user for
    access.

    Even if you did create a samba user the only one that would allow write
    access is "username" because of the underlying Linux file permissions of
    the path.

    By default, your home directory has permissions of 755. The owner can read
    and write ( the "7" ) and everyone else can only read ( the "5"'s ).

    So you can set up a samba user for "username",

    Or change the permissions on the target to 777:


<list-users>
From the man page "pdbedit - manage the SAM database (Database of Samba
Users)"

sudo pdbedit -L -v

-L to list users. -v to be verbose.


={============================================================================
*kt_linux_sete_005* admin: check running services

sudo service --status-all


={============================================================================
*kt_linux_sete_005* admin: check nvidia version

$ cat /proc/driver/nvidia/version 
NVRM version: NVIDIA UNIX x86_64 Kernel Module  340.96  Sun Nov  8 22:33:28 PST 2015
GCC version:  gcc version 4.8.4 (Debian 4.8.4-1) 


={============================================================================
*kt_linux_sete_005* admin-gnome: check gnome version

$ gnome-session --version
gnome-session 3.14.0


={============================================================================
*kt_linux_sete_005* admin-gnome: shortcuts

{activate-button}
To access your windows and applications, click the Activities button, or just
move your mouse pointer to the top-left hot corner. 

You can also press the 'super' key on your keyboard. You can see your windows
and applications in the overview.  You can also just start typing to search your
applications, files, folders and the web. note: this is window key.


{tile-windows}
Super+Left     " to left
Super+Right    " to right
Super+Up       " to max
Supaer+Down    " back to original size


{lock-screen}
Super-L


{keyboard-shortcuts}
https://wiki.gnome.org/Design/OS/KeyboardShortcuts
https://help.gnome.org/users/gnome-help/stable/shell-keyboard-shortcuts.html.en

To change key maps:

System Setting -> Keyboard -> Shortcuts

={============================================================================
*kt_linux_sete_005* admin: firefox key shortcuts

Ctrl+D                     Add Bookmark
Ctrl+B or Ctrl+I           Bookmarks

Backspace or Alt+<-        Back
Shft+Backspace or Alt+->   Forward

Ctrl+W or Ctrl+F4          Close Tab
Ctrl+T                     New Tab
Ctrl+Tab or Ctrl+PageDown  Next Tab

F5 or Ctrl+R               Reload


={============================================================================
*kt_linux_sete_005* admin: tbird key shortcuts

https://support.mozilla.org/en-US/kb/keyboard-shortcuts

Tagging and marking your messages

Mark Message as Read/Unread     M
Mark Thread as Read             R


={============================================================================
*kt_linux_sete_006*	ubuntu: connect from windows remote desktop

sudo apt-get install xrdp

Then run windows remote desktop and connect using ip or hostname. That's it.

<q-and-a>
I use Ubuntu on my desktop. When I am away from my desktop, I would like to
access the session using my Windows 7 laptop. Currently, I am using xrdp to
connect, but it starts up a remote session. Is there any way to just use the
same desktop session? I want to be able to pick up where I left off on the
desktop.

http://askubuntu.com/questions/235905/use-xrdp-to-connect-to-desktop-session

// original when installs

/etc/xrdp/xrdp.ini

[xrdp1]
name=sesman-Xvnc
lib=libvnc.so
username=ask
password=ask
ip=127.0.0.1
port=-1

// this works as said in the above link but not for dual 

[xrdp1]
name=sesman-Xvnc
lib=libvnc.so
username=
password=ask
ip=127.0.0.1
port=5900


={============================================================================
*kt_linux_sete_008* ubuntu: change default application setting

Change a map between file type and default application in:

/usr/share/applications/defaults.list -> /etc/gnome/defaults.list


={============================================================================
*kt_linux_sete_101* gnome: workspace

https://help.gnome.org/users/gnome-help/stable/shell-workspaces.html.en

To add a workspace, drag and drop a window from an existing workspace onto the
empty workspace in the workspace selector. This workspace now contains the
window you have dropped, and a new empty workspace will appear below it.

default keymap:

Cult-Alt-Up       move to workspace above
Cult-Alt-Down     move to workspace below


={============================================================================
*kt_linux_sete_101* gnome: change wallpapers

http://fabhax.com/technology/change-wallpapers-in-gnome-3.4/


={============================================================================
*kt_linux_sete_105* web server

{nginx}

http://wiki.nginx.org/Main

$ sudo apt-get install nginx

Nginx has become one of the most important web servers over the last couple of
years. There's a reason for that. Instead of using the standard threaded- or
process-oriented architecture, it uses a scalable, event-driven (asynchronous)
  architecture. 

So not only is it incredibly light weight, it's highly scalable and memory usage
is far better suited for limited resource deployments. Nginx also handles simple
load balancing, fault tolerance, auto-indexing, virtual servers (both name- and
    IP-based), mod_rewrite, access control, and much more. Nginx can also serve
as a reverse proxy and an IMAP/POP3 proxy server.

Surprisingly, Nginx powers a few very high-profile sites, such as: Netflix,
  Hulu, Pinterest, Wordpress.com, and AirBnB.

Who is Nginx right for? The nice thing about this particular light weight HTTPD
daemon is that it doesn't perform like a lightweight server. Not only does it
run with minimal resources, it offers plenty of optional modules and addons. You
can find pre-built packages for Linux and BSD for easy installation. So if you
need a powerhouse server, in a lighter weight package, Nginx is the server for
you.

Nginx comes in at a 10 MB installation (versus the Apache 30 MB installation)
  and can give you up to a 35 percent performance increase (versus Apache).

<home>
$ ll /usr/share/nginx/www

<config>

/etc/nginx/sites-enabled/default 

server {
  #listen   80; ## listen for ipv4; this line is default and implied
  #listen   [::]:80 default_server ipv6only=on; ## listen for ipv6

  root /usr/share/nginx/www;
  index index.html index.htm;
  ...
}


<commands>

Use the following command:

# /etc/init.d/nginx restart
# /etc/init.d/nginx reload

# service nginx restart
# service nginx reload

However, recommend way is as follows. This should work with any Linux
distributions or Unix like operating systems:

# nginx -s reload
# /path/to/full/nginx -s reload


{proxy-pass}
http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass

Take a look at nginx's HttpProxyModule, which is where proxy_pass comes from.
The proxy_pass docs say:

This directive sets the address of the proxied server and the URI to which
location will be mapped.

So when you tell Nginx to proxy_pass, you're saying "Pass this request on to
this proxy URL".


<okay>
This is a configuration for local web server and want want to have is when there
are requests to the local server, it would redirect those to the actual server.

wget http://wll1p04345.dev.youview.co.uk/e/pseudolive/bbb/client_manifest.mpd;

To:

wget http://dash.bidi.int.bbc.co.uk/e/pseudolive/bbb/client_manifest.mpd;


location / {
# First attempt to serve request as file, then
# as directory, then fall back to displaying a 404.
# try_files $uri $uri/ /index.html;
# Uncomment to enable naxsi on this location
# include /etc/nginx/naxsi.rules
  proxy_pass http://dash.bidi.int.bbc.co.uk;
}

<not-okay>

location / {
# First attempt to serve request as file, then
# as directory, then fall back to displaying a 404.
try_files $uri $uri/ /index.html;

# Uncomment to enable naxsi on this location
# include /etc/nginx/naxsi.rules
  proxy_pass http://dash.bidi.int.bbc.co.uk;
}


={============================================================================
*kt_linux_sete_105* set: update adobe flash plugin

sudo update-flashplugin-nonfree --install


={============================================================================
*kt_linux_sete_200* which to install?

{pdf-viewer}
Okular


# ============================================================================
#{
={============================================================================
*kt_linux_ref_001* references

{ref-UNP}
Richard Stevens. Unix Network Programming Vol 2 Addision Wesley. 2nd Ed.
http://www.kohala.com/start/unpv22e/unpv22e.html

{ref-LPI}
The Linux Programming Interface: A Linux and UNIX System Programming Handbook 
Michael Kerrisk (Author) 

For sources, http://www.man7.org/tlpi/


-------------------------------------------------------------------------------
Copyright: see |ktkb|  vim:tw=100:ts=3:ft=help:norl:

