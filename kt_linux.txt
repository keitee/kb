*kt_linux*                                                           tw=100

/^[#=]{ 

aWed 18 Dec 2013 09:47:23 GMT

KT KB. Linux

|kt_linux_tool_001| bash history
|kt_linux_tool_002| cut
|kt_linux_tool_003| 
|kt_linux_tool_004| 
|kt_linux_tool_005| 
|kt_linux_tool_006|  gdb: run with gdb client and server/client {gdb-run-step}
|kt_linux_tool_007| 
|kt_linux_tool_008|  gdb: platform specific
|kt_linux_tool_009| 
|kt_linux_tool_010| 
|kt_linux_tool_011|  stdout and stderr
|kt_linux_tool_012|  to make a empty file
|kt_linux_tool_013|  to upgrade a kernel
|kt_linux_tool_014|  ssh and putty {scp}
|kt_linux_tool_015|  apt-xxx to get package
|kt_linux_tool_016|  check linux version
|kt_linux_tool_017|  redirection
|kt_linux_tool_018|  cmd: ls
|kt_linux_tool_019|  cmd: find
|kt_linux_tool_020|  cmd: time
|kt_linux_tool_021|  cmd: sort
|kt_linux_tool_022|  cmd: grep
|kt_linux_tool_023|  cmd: wc
|kt_linux_tool_024|  cmd: du
|kt_linux_tool_025|  cmd: ln
|kt_linux_tool_026|  cmd: rsync
|kt_linux_tool_027|  cmd: awk
|kt_linux_tool_028|  cmd: sed

|kt_linux_tool_140|  cmake
|kt_linux_tool_141|  cmake: mix of c and cpp build
|kt_linux_tool_142|  cmake: cflags
|kt_linux_tool_143|  cmake: includes {message-keyword}
|kt_linux_tool_144|  cmake: link group

#{ GDB
|kt_linux_tool_200| gdb: core dump setting
|kt_linux_tool_201| gdb: core dump analysis {frame-command}
|kt_linux_tool_202| gdb: kernel crash analysis
|kt_linux_tool_203| gdb: .gdbinit
|kt_linux_tool_204| gdb: debugging info, run and quit
|kt_linux_tool_205| gdb: commands
|kt_linux_tool_206| gdb: commands for stepping
|kt_linux_tool_207| gdb: multi thread
|kt_linux_tool_208| gdb: breakpoints
|kt_linux_tool_209| gdb: breakpoints: advanced
|kt_linux_tool_210| gdb: examine running
|kt_linux_tool_211| gdb: examine sources
|kt_linux_tool_248| gdb: symbols and files
|kt_linux_tool_249| gdb: shared library debugging
|kt_linux_tool_250| gdb: remote debugging

|kt_linux_tool_300| gdb: frontend tool: cgdb

|kt_linux_core_001|  check shared libraries that process uses 
|kt_linux_core_002|  
|kt_linux_core_003|  
|kt_linux_core_004| 
|kt_linux_core_005|  
|kt_linux_core_006|  

|kt_linux_core_100| thread vs. process {zombie-process}
|kt_linux_core_101| pthread {nptl}
|kt_linux_core_102| how to run three threads sequencially
|kt_linux_core_103| priority and schedule

|kt_linux_core_100| ipc
|kt_linux_core_101| ipc: pipe and fifo
|kt_linux_core_103| ipc: which one to use {semaphores-versus-pthreads-mutexes}

|kt_linux_core_110| ipc: dbus

|kt_linux_core_200| ipc: sync: semaphore 
|kt_linux_core_201| ipc: sync: pthread mutex and cond-var {race-condition} {deadlock-condition} {mutex-ownership} {mutex-types} 
|kt_linux_core_220| sync: read-write lock

|kt_linux_core_230|  sync: file-lock

|kt_linux_core_240|  sync: common problems when use threads {race-condition}

|kt_linux_core_250|  sync: reentrant and thread-safe {thread-specific-data} {thread-local-storage}
|kt_linux_core_260|  sync: atomic operations {lock-free-programming}
|kt_linux_core_261|  sync: ref: locks aren't slow; lock contention is
|kt_linux_core_262|  sync: ref: always use a lightweight mutex {mutex-vs-semaphore}
|kt_linux_core_263|  conc: ref: lock-free code: a false sense of security
|kt_linux_core_264|  conc: ref: the free lunch is over 

|kt_linux_core_290|  ref: concurrency in C++. ch01
|kt_linux_core_291|  ref: concurrency in C++. ch02 {std::thread}
|kt_linux_core_293|  ref: concurrency in C++. ch03

|kt_linux_core_300|  case: own semaphore and mutex class using pthread cond-var {cqueue}
|kt_linux_core_301|  case: use of mutex and thread class
|kt_linux_core_302|  case: analysis of 200 and 201 case
|kt_linux_core_303|  case: msg q between threads

|kt_linux_core_400|  signal
|kt_linux_core_401|  signal example: use signal as synchronization

|kt_linux_core_500|  file io
|kt_linux_core_600|  time
|kt_linux_core_601|  time: curTime
|kt_linux_core_602|  time: ms and us

|kt_linux_sete_001|  ubuntu: virtualbox
|kt_linux_sete_002|  ubuntu: workspace
|kt_linux_sete_003|  ubuntu: samba
|kt_linux_sete_004|  ubuntu: nfs
|kt_linux_sete_005|  ubuntu: check running services
|kt_linux_sete_006|  ubuntu: connect from windows remote desktop
|kt_linux_sete_007|  ubuntu: cpuinfo
|kt_linux_sete_008|  ubuntu: change default application setting

|kt_linux_refe_001|  references


# ============================================================================
#{
==============================================================================
*kt_linux_tool_001*	bash history

{settings}

# eliminate the continuous repeated entry across the whole history

export HISTCONTROL=erasedups

# run multiple commands from the history

fc [-e ename] [-lnr] [first] [last]
fc -s [pat=rep] [cmd]

Fix  Command.   In  the  first form, a range of commands from first to last is selected
from the history list.  First and last may be specified as a string (to locate  the  last
command  beginning  with  that string)  or  as  a  number (an index into the history list,
where a negative number is used as an offset from the current command number).

This is not quite what you want as it will launch an editor first, but that is probably a
good thing since it gives you a chance to double check that you have the correct commands
and even edit them using all the capabilities of your favorite editor. Once you save you
changes and exit the editor, the commands will be run.

e.g.

$ fc 100 120

{reverse-search-history}

http://www.gnu.org/software/bash/manual/html_node/Commands-For-History.html

reverse-search-history (C-r)

Search backward starting at the current line and moving 'up' through the history as necessary. This
is an incremental search.

Once you've found the command you have several options:

1. Run it verbatim  just press Enter
2. Cycle through other commands that match the letters you've typed  press Ctrl-R successively
3. Quit the search and back to the command line empty-handed  press Ctrl-G
4. Take one by pressing ESC and edit the command line.

To set vi mode for editing command line:

8.3.1 Readline Init File Syntax

There are only a few basic constructs allowed in the Readline init file. Blank lines are ignored.
Lines beginning with a `#' are comments. Lines beginning with a `$' indicate conditional constructs
(see section 8.3.2 Conditional Init Constructs). Other lines denote variable settings and key
bindings.

Variable Settings
You can modify the run-time behavior of Readline by altering the values of variables in Readline
using the set command within the init file. The syntax is simple:

    set variable value

Here, for example, is how to change from the default Emacs-like key binding to use vi line editing
commands:

    set editing-mode vi

To set binding to up/down key to history search:

# ~/.inputrc
"\e[A": history-search-backward
"\e[B": history-search-forward

or equivalently,

# ~/.bashrc
bind '"\e[A": history-search-backward'
bind '"\e[B": history-search-forward'

Normally, Up and Down are bound to the Readline functions previous-history and next-history
respectively. I prefer to bind PgUp/PgDn to these functions, instead of displacing the normal
operation of Up/Down.

# ~/.inputrc
"\e[5~": history-search-backward
"\e[6~": history-search-forward

After you modify ~/.inputrc, restart your shell or use Ctrl+X, Ctrl+R to tell it to re-read
~/.inputrc. By the way, if you're looking for relevant documentation: Bash uses The GNU Readline
Library for the shell prompt and history.


==============================================================================
*kt_linux_tool_002*	cut

# input file
WIFI_SSID="SKY0F227"

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d=`
# this lead to WIFI_SSID='"SKY0F227"' and cannot use it as a var

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d \"`
# this fixes the problem.


={============================================================================
*kt_linux_tool_003*	

={============================================================================
*kt_linux_tool_004*	

={============================================================================
*kt_linux_tool_005*

={============================================================================
*kt_linux_tool_006*


(gdb) run -v all
(gdb) sectionFilterTable[sfIdx]
(gdb) directory components/FOSH/FUSIONOS_XTV_TESTS/xtv_test/src/
(gdb) directory components/FOSH/FUSIONOS_TEST_SHARED/DSL/src/ (SDS_SectionFilter.c)

To continue to a specific location, use the 
(gdb) advance funcname

={============================================================================
*kt_linux_tool_007*

={============================================================================
*kt_linux_tool_008*	gdb: platform specific


In the gdb backtrace the function arguments will be displayed from the stack if they are
available. Otherwise, gdb displays the values of the argument registers and hopes for the
best. The arguments in the backtrace are mostly correct because gdb can often pull them
from the stack. When compiled -O0 functions always save their argument registers to the
stack, so it is only optimized code which can make debugging difficult in this way.

See mips-tdep.c from gdb source


={============================================================================
*kt_linux_tool_009*

={============================================================================
*kt_linux_tool_010*

==============================================================================
*kt_linux_tool_011*	stdout and stderr

On program startup, the integer file descriptors associated with the streams stdin,
stdout, and stderr  are  0, 1,  and  2,  respectively.

prog1 > /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log 2>&1
prog1 2>&1 | tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log


==============================================================================
*kt_linux_tool_012*	to make a empty file

can use 'touch' but when busybox do not support touch, can use following to make a empty
file or to reset a file.

cat /dev/null > file


==============================================================================
*kt_linux_tool_013*	to upgrade a kernel

1. build a kernel

2. copy built images to /boot
vmlinuz-2.6.9-22.ELsmp, System.map-2.6.9-22.ELsmp, initrd-2.6.9-22.ELsmp.img

3. change /etc/grub.conf that's multi-boot script to boot with built images.
# cat /etc/grub.conf (links to /boot/grub/grub.conf)


# grub.conf generated by anaconda
#
# Note that you do not have to rerun grub after making changes to this file
# NOTICE:  You do not have a /boot partition.  This means that
#          all kernel and initrd paths are relative to /, eg.
#          root (hd0,0)
#          kernel /boot/vmlinuz-version ro root=/dev/sda1
#          initrd /boot/initrd-version.img
#boot=/dev/sda
default=0
timeout=5
splashimage=(hd0,0)/boot/grub/splash.xpm.gz
hiddenmenu
title Red Hat Enterprise Linux ES (2.6.9-22.ELsmp)
        root (hd0,0)
        kernel /boot/vmlinuz-2.6.9-22.ELsmp ro root=LABEL=/ rhgb quiet
        initrd /boot/initrd-2.6.9-22.ELsmp.img
title Red Hat Enterprise Linux ES-up (2.6.9-22.EL)
        root (hd0,0)
        kernel /boot/vmlinuz-2.6.9-22.EL ro root=LABEL=/ rhgb quiet
        initrd /boot/initrd-2.6.9-22.EL.img


={============================================================================
*kt_linux_tool_014*	ssh and putty

{aim}
To switch hosts using key base instead of using password.

{ssh-keygen}
$ ssh-keygen <enter>

$ ls -al .ssh/
total 20
drwx------  2 parkkt ccusers 4096 Dec  9 13:24 .
drwxr-xr-x 15 parkkt ccusers 4096 Dec  9 12:47 ..
-rw-------  1 parkkt ccusers 1675 Dec  9 13:24 id_rsa
-rw-r--r--  1 parkkt ccusers  411 Dec  9 13:24 id_rsa.pub

The file we need to copy to the server is named id_dsa.pub. As you can see above, the file needed
exists. You may or may not have other files in ~/.ssh as I do. If the key doesn't exist, however,
you can make one as follows:

$cp id_rsa.pub authorized_keys
$scp -p ~/.ssh/authorized_keys ukstbuild3:.ssh/
(user@homebox ~ $ scp ~/.ssh/id_dsa.pub user@'servername':.ssh/authorized_keys)
(     -p      Preserves modification times, access times, and modes from the
             original file.)

This make one-way ssh connection which means the machine you are on is added the authorized_keys of
the server so can run scp from the machine to the server:

scp remote-server:{path}/filename .
scp filename remote-server:{path}/filename

note: if there is working ssh connection, can use tab key to get file completion when use scp.

note: ssh-copy is not working and key line in authorized_keys must be one line.


{to-check-match-pair}
$ ssh-keygen /?
  -y          Read private key file and print public key.

$ ssh-keygen -y -f id_rsa


{caution}
note:
If the file permissions are too open then ssh will not trust them, and will still prompt
you for your password. 

chmod 700 ~/.ssh
chmod 644 ~/.ssh/authorized_keys		# must check this as caused big grief when different
chmod 644 ~/.ssh/id_dsa_pub
chmod 644 ~/.ssh/known_hosts
chmod 600 ~/.ssh/id_dsa

{config}
When user name is different between servers, must have an entry in this file for servers to connect.

$ cat ~/.ssh/config
Host tizen
        Hostname 168.219.241.167
        IdentityFile ~/.ssh/id_rsa
        User keitee.park                 # this is username that can be different from real user.
        Port 29418

To debug ssh. note: shall use name on the command line
-v  # -vv
Verbose mode. Causes ssh to print debugging messages about its progress. This is helpful in
debugging connection, authentication, and configuration problems. Multiple -v options increase
the verbosity. The maximum is 3. 

$ ssh -vT tizen


{github-when-ssh-do-not-work}
Using SSH over the HTTPS port

Sometimes the administrator of a firewall will refuse to allow SSH connections entirely. If using
HTTPS cloning with credential caching is not an option, you can attempt to clone using an SSH
connection made over the HTTPS port. Most firewall rules should allow this, but proxy servers may
interfere. 

Testing

To test if SSH over the HTTPS port is possible, run this ssh command:

ssh -T -p 443 git@ssh.github.com
# Hi username! You've successfully authenticated, but GitHub does not provide shell access.

Make it so

If you are able to ssh to git@ssh.github.com over port 443, you can override your ssh settings to
force any connection to github.com to run though that server and port. To set this in your ssh
config, edit the file at ~/.ssh/config and add this section:

Host github.com
  Hostname ssh.github.com
  Port 443

You can test that this works by connecting to github.com:

ssh -T git@github.com
# Hi username! You've successfully authenticated, but GitHub does not provide shell access.


{putty-ssh-setup}
When use keys generated from putty.

o run puttygen to make key pairs. rsa or dsa
o get a pub key and save a pri key(ppk)
o run putty and set ssh key to use
  menu: connection: ssh: auth: private key file for auth: specify the path to a pri key.
o login to the host and add a pub key in the auth key list


{convert-rsa-key-to-putty-ppk}
To convert keys from linux machine to putty ppk and from ppk to lunux(opsnssh keys)

o run puttygen and menu: conversion: import key:
o save it as a pri key(ppk)


{xserver} gnome-terminal
I have just found that you can open new terminals using 'gnome-terminal'.  You can open multiple
windows and multiple tabs like this:

gnome-terminal --window --tab --window --tab --tab

# 
#
I stumbled across " VcXsrv Windows X Server" - that seems to be the well maintained and if you want
you can compiled yourself (not that you need to).

http://sourceforge.net/projects/vcxsrv/?source=directory

If you chose to use it, you need to run VcXsrc without OpenGL support but it works - see highlighted
text below ...

"C:\Program Files\VcXsrv\vcxsrv.exe" :0 -ac -terminate -lesspointer -multiwindow -clipboard -nowgl

Note: this server is better since it seems to have more fonts and shows gvim properly than xming
which is old xserver. It replace xming and seems to use the rest since use xlanuch. 'One window' do
not works but 'multiple windows' works.

http://alexcooper.co.uk/blog/2013/09/using-vcxsrv-putty-remote-x-windows/


==============================================================================
*kt_linux_tool_015*	apt-xxx to get package

To search package:
apt-cache search <program name>

To install a package:
sudo apt-get install tk8.5 

To remove a package:
sudo apt-get purge tk8.5 

{update-and-upgrade}

Note: The apt-get install command is recommended because it upgrades one or more already installed
packages without upgrading every package installed, whereas the apt-get upgrade command installs the
newest version of all currently installed packages. In additon, apt-get update command must be
executed before an upgrade to resynchronize the package index files.

{update-error}

When see:
Update Error: Require Installation Of Untrusted Packages

Run manually on console

sudo apt-get upgrade xxx


==============================================================================
*kt_linux_tool_016*	check linux version

{ubuntu}

$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=12.04
DISTRIB_CODENAME=precise
DISTRIB_DESCRIPTION="Ubuntu 12.04.1 LTS"


==============================================================================
*kt_linux_tool_017*	redirection

(for sh, borne shell, bash)
prog1 > /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log 2>&1
prog1 2>&1 | tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log


==============================================================================
*kt_linux_tool_018*	cmd: ls

{get-filename-only}

ls -1

{get-color}

For bash, copy /etc/DIR_COLORS into home as .dir_colors and edit it to change default values. Run
man dir_colors for help.


==============================================================================
*kt_linux_tool_019*	cmd: find

{find-mtime}
24 hours based so -mtime 3 means that 72 hours from the current time and -mtime -3 means 72 before.
To specify between 72 and 96, -mtime 3 and after 96, -mtime +3. If want to find file or dirs changed
most recently, use -mtime 0 or 1.


{find-to-get-the-number-of-files}
%find -type f | wc -l


{find-print0}

-print0

print the full file name on the standard output, followed by a null character (instead of the new-
line character that -print uses). This allows file names that contain newlines or other types of
white space to be correctly interpreted by programs that process the find output. This option
corresponds to the -0 option of xargs.

$ find . -name CMakeLists.txt -print0
./CMakeLists.txt./Source/CMakeLists.txt./Source/cmake/gtest/CMakeLists.txt./Source/WebKit2/CMakeLists.txt./Source/WebKit2/UIProcess/efl/po_tizen/CMakeLists.txt./Source/WebKit/CMakeLists.txt./Source/WebKit/efl/DefaultTheme/CMakeLists.txt./Source/JavaScriptCore/CMakeLists.txt./Source/JavaScriptCore/shell/CMakeLists.txt./Source/WebCore/CMakeLists.txt./Source/ThirdParty/gtest/CMakeLists.txt./Source/WTF/CMakeLists.txt./Source/WTF/wtf/CMakeLists.txt./Tools/WinCELauncher/CMakeLists.txt./Tools/tizen-webview-test/CMakeLists.txt./Tools/MiniBrowser/efl/CMakeLists.txt./Tools/CMakeLists.txt./Tools/TestWebKitAPI/CMakeLists.txt./Tools/EWebLauncher/CMakeLists.txt./Tools/EWebLauncher/ControlTheme/CMakeLists.txt./Tools/DumpRenderTree/TestNetscapePlugIn/CMakeLists.txt./Tools/DumpRenderTree/efl/CMakeLists.txt./Tools/WebKitTestRunner/CMakeLists.txtkeitee.park@rockford /home/tbernard/Git/vdTizen/webkit

{Q} when useful? To use the out as a single line?
Used for xargs


{find-exec}
{} where the filename will be inserted. Add \; at the end of the command to complete the required
syntax. note: there must be a space after {}

$ find . -name CMakeLists.txt -exec egrep PROJECT {} \;

To run dirtags script for each directory:

$ find * -type d -exec dirtags {} \;

{find-oring}

You can specify a logical "or" condition using -o:

find / \( -size +50 -o -mtime -3 \) -print
find /my/project/dir -name '*.c' -o -name '*.h'
find -name *.[ch]

This is from bash  expr: expr1 -or expr2 and this means expr1 -o expr2, but not POSIX compliant.

{find-ignore}

-path pattern

File name matches shell pattern pattern.  The metacharacters do not treat `/' or `.' specially; so,
for example,

find . -path "./sr*sc"

will print an entry for a directory called `./src/misc' (if one exists).  To ignore a  whole
directory tree,  use  -prune  rather  than  checking  every file in the tree.  For example, to skip
the directory `src/emacs' and all files and directories under it, and print the names of the other
files  found,  do something like this:

find . -path ./src/emacs -prune -o -print

Note  that the pattern match test applies to the whole file name, starting from one of the start
points named on the command line.  It would only make sense to use an absolute path name here if the
relevant start point is also an absolute path.  This means that this command will never match
anything:

find bar -path /foo/bar/myfile -print

The  predicate  -path is also supported by HP-UX find and will be in a forthcoming version of the
POSIX standard.

{find-sym}

-L     
Follow symbolic links.


==============================================================================
*kt_linux_tool_020*	cmd: time

kit@kit-vb:~/tizencore$ time -f "%E real,%U user,%S sys" ls -Fs
-f: command not found

real	0m0.143s
user	0m0.068s
sys	0m0.040s
kit@kit-vb:~/tizencore$ 

kit@kit-vb:~/tizencore$ /usr/bin/time -f "%E real,%U user,%S sys" ls -Fs
total 24
4 app-core/  4 appfw/  4 application/  4 app-service/  4 dlog/	4 README
0:00.00 real,0.00 user,0.00 sys
kit@kit-vb:~/tizencore$ 

Users of the bash shell need to use an explicit path in order to run the external time command and
not the shell builtin variant. On system where time is installed in /usr/bin, the first example
would become /usr/bin/time wc /etc/hosts


==============================================================================
*kt_linux_tool_021*	cmd: sort

See the unix power tool 22.6 for more. -u remove duplicates and sort against field [4,7].

sort -u -k 4,7

http://www.gnu.org/software/coreutils/manual/html_node/sort-invocation.html

--key=pos1[,pos2]

Specify a sort field that consists of the part of the line between pos1 and pos2 (or the end of the
line, if pos2 is omitted), inclusive.  Each pos has the form 'f[.c][opts]', where f is the number of
the field to use, and c is the number of the first character from the beginning of the field. Fields
and character positions are numbered starting with 1; a character position of zero in pos2 indicates
the field's last character. If '.c' is omitted from pos1, it defaults to 1 (the beginning of the
field); if omitted from pos2, it defaults to 0 (the end of the field). opts are ordering options,
allowing individual keys to be sorted according to different rules; see below for details. Keys can
span multiple fields.  Example: To sort on the second field, use --key=2,2 (-k 2,2). See below for
more notes on keys and more examples. See also the --debug option to help determine the part of the
line being used in the sort.

To sort
NDS: ^0946684946.752246 !ERROR -aem          < M:aem_list.c F:AEM_ListGetApplication L:01808 > Can't find
-1-- ----------2------- ---3-- -4--         -5 ------6----- ---------------7-------- ---8--- 9 -10--

sort -u -k 4,10 ndsfusion.test > ndsfusion.dic 


-t separator --field-separator=separator

Use character separator as the field separator when finding the sort keys in each line. By default,
fields are separated by the empty string between a non-blank character and a blank character. By
default a blank is a space or a tab, but the LC_CTYPE locale can change this.

That is, given the input line ' foo bar', sort breaks it into fields ' foo' and ' bar'. The field
separator is not considered to be part of either the field preceding or the field following, so with
'sort -t " "' the same input line has three fields: an empty field, 'foo', and 'bar'. However,
fields that extend to the end of the line, as -k 2, or fields consisting of a range, as -k 2,3,
retain the field separators present between the endpoints of the range.  To specify ASCII NUL as the
field separator, use the two-character string '\0', e.g., 'sort -t '\0''. 


mh5a_variable.c
mh5b_variable.c

sort -t _ -k 2,2

Use this to list out member functions from sources

egrep -r 'g_pAppWindow->' . | sort -t - -k 2,3 >> in.txt


==============================================================================
*kt_linux_tool_022*	cmd: grep

To find files which has the given string:

grep -nro LINUX ./

       -s, --no-messages
              Suppress error messages about nonexistent or unreadable files.   Portability  note:
       -o, --only-matching
              Print only the matched (non-empty) parts of a matching line, with each such part on
              a separate output line.
       -n, --line-number
              Prefix each line of output with the 1-based line number within its input file.  (-n
              is specified by POSIX.)
       -R, -r, --recursive
              Read  all  files  under  each  directory, recursively; this is equivalent to the -d
              recurse option.

To specify the target files:

grep -nr --include *.c __setup ./
grep -nr --include *.c [a-z]*_initcall ./

{when-see-binary-errors-on-text-file}

Even if that is a text file, grep say it is binary file then use -a option.

(http://www.gnu.org/software/grep/manual/html_node/Usage.html)

Why does grep report Binary file matches? If grep listed all matching lines from a binary file,
it would probably generate output that is not useful, and it might even muck up your display. So gnu
grep suppresses output from files that appear to be binary files. To force gnu grep to output lines
even from files that appear to be binary, use the '-a' or '--binary-files=text' option. To eliminate
the Binary file matches messages, use the '-I' or '--binary-files=without-match' option. 

To exclude binaries:

 -I     Process a binary file as if it did not contain matching data; this is equivalent to
		  the --binary-files=without-match option.


{q-option}

Used in the script.

-q, --quiet, --silent

Quiet; do not write anything to standard output. Exit immediately with zero status if any match is
found, even if an error was detected. Also see the -s or --no-messages option. 

echo xx | grep -q '.gz$' && echo ture
echo xx.gz | grep -q '.gz$' && echo ture
ture

The script example:

##:zcat if this is a .gz, cat otherwise
F=cat
echo "$1" | grep -q '\.gz$' && F=zcat
$F "$1"


{case-example}
		
# what it's trying to do is that search through the same errors in logs which was uploaded in
# Feburary and was under translation.  for example,
# translation/Zone9-Box5_Feb_18_09_38_42b3e5c9130e1d758acdc71bb9d12b2a		
#

ls translation/ | grep Feb | while read x; do pushd -n translation/$x; pwd; egrep --color -ano 'btreePageFromDbPage' .detailed_output; popd -n; done 

$ egrep --color -an 'btreePageFromDbPage' translation/*_Feb_*/.detailed_output
translation/PaulRiley_Work_Feb_18_09_42_25d1235e4ebe020a369a64d728c2ce9f/.detailed_output:8:parser_result=<p1>MW_Process crash in #0 btreePageFromDbPage (pDbPage=0x2b994bcc, pgno=67, pBt=0x2bb06da4) (built by hudson)</p1>
$ 

$ egrep --color -ano 'event manager POLL time exceeded threshold' translation/*_Feb_*/LOGlastrun_realtime
translation/000000002704_Feb_1_14_05_3a6f10bd9e8934088df33e9d991faa74/LOGlastrun_realtime:2100:event manager POLL time exceeded threshold


==============================================================================
*kt_linux_tool_023*	cmd: wc

-l, --lines
     print the newline counts

{how-to-count-lines-recursively}

find . -name '*.php' | xargs wc -l

# should be re-written using sh scripting
#!/bin/bash
echo "debug..."
find debug -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "dsm"
find dsm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "include"
find include -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "main"
find main -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

#
echo "mah"
find mah -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5dec"
find mh5dec -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5eng"
find mh5eng -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5gpi"
find mh5gpi -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mhv"
find mhv -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "pfm"
find pfm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l


==============================================================================
*kt_linux_tool_024*	cmd: du

' to print one level
du -h --max-depth=1

-h, --human-readable
              print sizes in human readable format (e.g., 1K 234M 2G)

' to show all and total
du -ach
du -sh


={============================================================================
*kt_linux_tool_025* cmd: ln

       -f, --force
              remove existing destination files

       -s, --symbolic
              make symbolic links instead of hard links

ln -sf input output. E.g., ln -sf linux-2.4.25-2.8 linux


={============================================================================
*kt_linux_tool_026* cmd: rsync

Want to copy all expect .git from source to destination

rsync -av --progress /home/kit/mheg-remote-git/mag_shared/ . --exclude .git

NAME
       rsync - a fast, versatile, remote (and local) file-copying tool


={============================================================================
*kt_linux_tool_027* cmd: awk

To make a filename from the date:

date: Fri Jul 25 05:30:12 BST 2014 -> log-Fri-Jul-25-...

script -f /home/kit/log/`date | awk '{print "log-"$1"-"$2"-"$3"-"$4}'`

To make a gateway from the ip address:
gateway=`echo $ip | awk 'BEGIN { FS="." } ; { print $1"."$2"."$3"."1 }'`


={============================================================================
*kt_linux_tool_028* cmd: sed

To find and replace string in multiple files.

$ egrep -lr --include *.c mhvSessionCancel .
./mh5eng/mh5a_application.c
kit@kit-vb:~/mheg-port$ egrep -lr --include *.c mhvSessionCancel . | xargs sed -i 's/mhvSessionCancel/mmhv_session_cancel/'

grep
       -l, --files-with-matches
              Suppress  normal  output;  instead  print  the name of each input file from which output
              would normally have been printed.  The scanning will stop on the first  match.   (-l  is
              specified by POSIX.)

sed
       -i[SUFFIX], --in-place[=SUFFIX]
              edit files in place (makes backup if extension supplied)


# ============================================================================
#{
={============================================================================
*kt_linux_tool_140*	cmake

<ref>
http://www.cmake.org/cmake/help/v2.8.9/cmake.html
http://stackoverflow.com/questions/6352123/multiple-directories-under-cmake

From:

./mheg
+-- CMakeLists.txt
+-- include
¦   +-- dbg.h
¦   +-- def.h
+-- main
¦   +-- main.c


To:

./mheg
+-- CMakeLists.txt
+-- include
¦   +-- dbg.h
¦   +-- def.h
+-- main
¦   +-- main.c
+-- mh5eng
¦   +-- CMakeLists.txt
¦   +-- sample.c
¦   +-- sample.h


The changes made to build:

../mheg/CMakeLists.txt

	 INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/mh5eng)

	 ADD_EXECUTABLE(${PROJECT_NAME}
		 main/main.c # uses sample.h
	 )

	 ADD_SUBDIRECTORY(mh5eng)

	 TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})

../mheg/mh5eng/CMakeLists.txt

	 SET(MH5ENG_HEADERS
		 sample.h
	 )

	 SET(MH5ENG_SOURCES
		 sample.c
	 )

	 ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES})

This create <libmh5eng.a> as a output


==============================================================================
*kt_linux_tool_141*	cmake: mix of c and cpp build

To compile the mix of c and cpp file, when try followings:

PROJECT(mhegproto C)

SET(MH5ENG_SOURCES
	xx.c
	mh5i_residentprogram_db.cpp
)

ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES} )

cmake do not compile cpp file although it is defined in the set variable. To make it built, must add
c++ build as below:

PROJECT(mhegproto C CXX)

From http://cmake.org/cmake/help/v2.8.8/cmake.html

project: Set a name for the entire project.

project(<projectname> [languageName1 languageName2 ... ] )

Sets the name of the project. Additionally this sets the variables <projectName>_BINARY_DIR and
<projectName>_SOURCE_DIR to the respective values.

Optionally you can specify which languages your project supports. Example languages are CXX (i.e.
C++), C, Fortran, etc. By default C and CXX are enabled. E.g. if you do not have a C++ compiler, you
can disable the check for it by explicitly listing the languages you want to support, e.g. C. By
using the special language "NONE" all checks for any language can be disabled. If a variable exists
called CMAKE_PROJECT_<projectName>_INCLUDE_FILE, the file pointed to by that variable will be
included as the last step of the project command.

note: by default? seems not.


==============================================================================
*kt_linux_tool_142*	cmake: cflags

To enable preprocessor, set -E as below.

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "-E ${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")


==============================================================================
*kt_linux_tool_143*	cmake: includes

Found that building a library in the subdir cannot find necessary includes.

root CMakeList.txt:

INCLUDE(FindPkgConfig)
pkg_check_modules(APPS_PKGS REQUIRED
	capi-appfw-application
	dlog
	edje
	elementary
	ecore-x
	evas
	utilX
	x11
	aul
	ail
)

ADD_EXECUTABLE(${PROJECT_NAME}
	main/main.c
	main/viewmgr.c
	main/view_main.c
)

ADD_SUBDIRECTORY(mh5eng)
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)

Here there is no problem to use headers from packages such as elementary but when add the same to
the one in /mh5eng then cannot find headers. When looked at flags used to build mh5eng, there is no
necessary -I. This is the same when add pkg_check_moudles in the mh5eng/CMakeList.txt. Thing is
build faild to update flags as expected.

The finding is that when move mh5eng then it builds without adding pkg_check_moudles in the
mh5eng/CMakeList.txt

...
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)
ADD_SUBDIRECTORY(mh5eng) [KT] moved to here

That suggests that where to put is important and not sure it is a GBS(git build system) or cmake
itself problem. 


Found that this error still happens when build cpp file and the solution is:

From:
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

To:
MESSAGE("KT >>>>>PKGS_LDFLAGS>>>>>" : ${APPS_PKGS_CFLAGS})
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")
SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${EXTRA_CFLAGS}")

Turns out that PKGS_CFLAGS will have necessary includes depending on pkg selection.


==============================================================================
*kt_linux_tool_144*	cmake: link group

To solve {cyclic-dependencies} in link, can use this:

From defining each: 
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
...

To use group:
TARGET_LINK_LIBRARIES(${PROJECT_NAME} -Wl,--start-group mhdebug pfm mh5eng mh5dec mhv mah
-Wl,--end-group ${APPS_PKGS_LDFLAGS})

 On 05/19/2011 11:11 AM, Anton Sibilev wrote:
> Hello!
> I'm wondering how I can use "--start-group archives --end-group"
> linker flags with "Unix Makefiles".
> May be somebody know the right way?

You might specify these flags immediately in TARGET_LINK_LIBRARIES():

CMAKE_MINIMUM_REQUIRED(VERSION 2.8 FATAL_ERROR)
PROJECT(LINKERGROUPS C)
SET(CMAKE_VERBOSE_MAKEFILE ON)
FILE(WRITE ${CMAKE_BINARY_DIR}/f.c "void f(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g1.c "void g1(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g2.c "void g2(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/h.c "void h(void){}\n")
ADD_LIBRARY(f STATIC f.c)
ADD_LIBRARY(g1 STATIC g1.c)
ADD_LIBRARY(g2 STATIC g2.c)
ADD_LIBRARY(h STATIC h.c)
FILE(WRITE ${CMAKE_BINARY_DIR}/main.c "int main(void){return 0;}\n")
ADD_EXECUTABLE(main main.c)
TARGET_LINK_LIBRARIES(main f -Wl,--start-group g1 g2 -Wl,--end-group h)

However, do you really need these flags? Refer to the target properties
[IMPORTED_]LINK_INTERFACE_MULTIPLICITY[_<CONFIG>] and the documentation
of TARGET_LINK_LIBRARIES().

Regards,

Michael

LINK_INTERFACE_MULTIPLICITY

Repetition count for STATIC libraries with cyclic dependencies.

When linking to a STATIC library target with cyclic dependencies the linker may need to scan more
than once through the archives in the strongly connected component of the dependency graph. CMake by
default constructs the link line so that the linker will scan through the component at least twice.
This property specifies the minimum number of scans if it is larger than the default. CMake uses the
largest value specified by any target in a component.


# ============================================================================
#{
={============================================================================
*kt_linux_tool_200*  gdb: core dump setting

{setting}
# default
-sh-3.2# cat /proc/sys/kernel/core_pattern 
core

<core-pattern>
# set core dump location and format
echo '/tmp/%p.COR' >/proc/sys/kernel/core_pattern

the following pattern elements in the core_pattern file:

%p: pid
%: '%' is dropped
%%: output one '%'
%u: uid
%g: gid
%s: signal number
%t: UNIX time of dump
%h: hostname
%e: executable filename
%: both are dropped

# configure it forever

The changes done before are only applicable until the next reboot. In order to make the change in
all future reboots, you will need to add the following in /etc/sysctl.conf

# own core file pattern...
kernel.core_pattern=/tmp/cores/core.%e.%p.%h.%t

sysctl.conf is the file controlling every configuration under /proc/sys

Just wanted to say that there is no need to edit the file manually. simply run the sysctl command,
which does the stuff

<conditions-to-check>
0. configured for cross platform

1. write permissions in the directory 

2. ulimit -c unlimited
this is shell command to set resource limit for a core.
-c     The maximum size of core files created 

3. compilation of the image: should be with debug symbols (not release version), and statically
compiled. In cygwin compilations, static link is enabled by default. In linux compilations, you
should modify platform.mk in your view.

If instead of the callstack you see only "??", it usually means the application wasn't
compiled with debug symbols (release version) or it was not compiled using static linkage

4. debug build
can use gdb without -g but need to map virtual address.

mkdir -p /tmp/cores
chmod a+rwx /tmp/cores
echo "/tmp/cores/core.%e.%p.%h.%t" > /proc/sys/kernel/core_pattern
 
<force-core>
ulimit -c unlimited

# SIGSEGV 	11 	Core 	Invalid memory reference 
# -s signal Specify the signal to send.  The signal may be given as a signal name or number.
 
kill -s SIGSEGV $$
kill -s 11 113

export P=`ps -a | grep APP_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;
export P=`ps -a | grep MW_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;

SIG_KERNEL_COREDUMP_MASK (.../kernel/signal.c) defines sianals to create a core.

SIGSEGV(segmentation fault)

#define SIG_KERNEL_COREDUMP_MASK (\
        M(SIGQUIT)   |  M(SIGILL)    |  M(SIGTRAP)   |  M(SIGABRT)   | \
        M(SIGFPE)    |  M(SIGSEGV)   |  M(SIGBUS)    |  M(SIGSYS)    | \
        M(SIGXCPU)   |  M(SIGXFSZ)   |  M_SIGEMT                     )


={============================================================================
*kt_linux_tool_201*  gdb: core dump analysis {frame-command}

export LD_LIBRARY_PATH=/home/NDS-UK/parkkt/bins
mips-gdb
set solib-absolute-prefix /junk
set solib-search-path /home/NDS-UK/parkkt/com.nds.darwin.debugsupport/debug_libs/uClibc-nptl-0.9.29-20070423
file APP_Process 
core 272.COR
thread apply all bt full
bt

(gdb) f n

<gdb-frame-command>
frame n 

Select frame number n. Recall that frame zero is the innermost (currently executing) frame, frame
one is the frame that called the innermost one, and so on. The highest-numbered frame is the one for
main.


# 01
#

(gdb) bt
#0 memset () at libc/string/mips/memset.S:132

(gdb) i r
	zero     at       v0         v1       a0         a1       a2       a3
R0	00000000 fffffffc [00000000] ffffffff [00000008] 00000000 00000004 00022000
	t0 t1 t2 t3 t4 t5 t6 t7
R8	00000004 00000000 ffffffff 000000c2 00000000 00000004 00a52630 00000000
	s0 s1 s2 s3 s4 s5 s6 s7
R16 00020000 00000000 00022004 2c57b174 00000004 2c70d4d0 2d9dcd30 00be6f70
	t8 t9 k0 k1 gp sp s8 ra
R24 00befcc8 2ab25160 00000000 00000000 00bf7e60 2d9dc858 2d9dcbd0 00993fdc
	sr       lo       hi       bad        cause    pc
	00008413 00000000 00000000 [00000000] 0080000c [2ab251b4]
	fsr fir
	00001004 00000000

# void *memset(void *s, int c, size_t n);

(gdb) disassemble $pc
Dump of assembler code for function memset:
0x2ab25160 <memset+0>: slti t1,a2,8
0x2ab25164 <memset+4>: bnez t1,0x2ab251d4 <memset+116>
-> 0x2ab25168 <memset+8>: move v0,a0
0x2ab2516c <memset+12>: beqz a1,0x2ab25184 <memset+36>
0x2ab25170 <memset+16>: andi a1,a1,0xff
0x2ab25174 <memset+20>: sll t0,a1,0x8
0x2ab25178 <memset+24>: or a1,a1,t0
0x2ab2517c <memset+28>: sll t0,a1,0x10
0x2ab25180 <memset+32>: or a1,a1,t0
0x2ab25184 <memset+36>: negu t0,a0 # negu d, s; d = -s;
0x2ab25188 <memset+40>: andi t0,t0,0x3
0x2ab2518c <memset+44>: beqz t0,0x2ab2519c <memset+60>
0x2ab25190 <memset+48>: subu a2,a2,t0
0x2ab25194 <memset+52>: swl a1,0(a0) # swl t, addr; Store word left/right
0x2ab25198 <memset+56>: addu a0,a0,t0
0x2ab2519c <memset+60>: andi t0,a2,0x7
0x2ab251a0 <memset+64>: beq t0,a2,0x2ab251c0 <memset+96>
0x2ab251a4 <memset+68>: subu a3,a2,t0
0x2ab251a8 <memset+72>: addu a3,a3,a0
0x2ab251ac <memset+76>: move a2,t0
-> 0x2ab251b0 <memset+80>: addiu a0,a0,8
-> 0x2ab251b4 <memset+84>: sw a1,-8(a0)
0x2ab251b8 <memset+88>: bne a0,a3,0x2ab251b0 <memset+80>

this shows that a0, first arg, was null. hence SEGFLT

# 02
#

(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc38, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x007f09c8 in MHWMemAllocStatic (pool=0x131bc38, size=64) at mem_static.c:63
#2  0x007e6854 in MEMMAN_API_AllocStaticP (pool=0x131bc38, size=41) at memman_st.c:350
#3  0x00419328 in DIAG_JAVA_GetJavaString (env=0x2d7056c0, l_java_string=0x2c4b3358, l_byte_array=0x2d705698, 
    l_len=0x2d70569c) at ../src/natdiag.c:63
#4  0x004197fc in DIAG_JAVA_GetJavaString (env=0x131bc38) at ../src/natdiag.c:341
#5  0x0041a0f8 in Java_com_nds_fusion_diagimpl_DiagImpl_natLogInfo (THIS=<value optimized out>, jClass=0x40, jInt=1, 
    jString=0x0) at sunnatdiag.c:177

[New process 124]
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
271     mem_blockpool.c: No such file or directory.
        in mem_blockpool.c

(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x72617469 in ?? ()
Backtrace stopped: previous frame inner to this frame (corrupt stack?)

(gdb) i r
          zero       at       v0       v1       a0       a1       a2       a3
 R0   00000000 00000005 00000004 00000000 0131bc40 00000040 00000001 00000000 
            t0       t1       t2       t3       t4       t5       t6       t7
 R8   0000000c 00000001 ffffffff ffffffff 00000000 00000000 00d7f220 00000200 
            s0       s1       s2       s3       s4       s5       s6       s7
 R16  00000000 0131bc40 0131bc40 00000040 00000001 00000000 00000001 2d705b00 
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00f0c360 00b62d98 00000000 00000000 00f14570 2d7055a0 2d7057d8 007f09c8 
            sr       lo       hi      bad    cause       pc
      00008413 000efdcb 00000005 00000018 00800008 007eb73c 
           fsr      fir
      00001004 00000000 

(gdb) disassemble $pc
# source
# uint32_t MHWMemCheckBank(MEMMAN_API_MemoryPoolHandle * mpool, uint32_t size, # uint32_t mem_nb_bank)
# {
#     MemWholeMemory *bank = mpool->s_WholeMem;
# }
#
Dump of assembler code for function MHWMemCheckBank:
0x007eb6b8 <MHWMemCheckBank+0>: addiu   sp,sp,-48
0x007eb6bc <MHWMemCheckBank+4>: sw      s4,40(sp)
0x007eb6c0 <MHWMemCheckBank+8>: sw      s3,36(sp)
0x007eb6c4 <MHWMemCheckBank+12>:        sw      s2,32(sp)
0x007eb6c8 <MHWMemCheckBank+16>:        sw      ra,44(sp)
0x007eb6cc <MHWMemCheckBank+20>:        sw      s1,28(sp)
0x007eb6d0 <MHWMemCheckBank+24>:        sw      s0,24(sp)
0x007eb6d4 <MHWMemCheckBank+28>:        lw      s0,40(a0)
0x007eb6d8 <MHWMemCheckBank+32>:        lw      v1,36(a0)
0x007eb6dc <MHWMemCheckBank+36>:        lui     v0,0x4
0x007eb6e0 <MHWMemCheckBank+40>:        lw      a3,108(a0)
0x007eb6e4 <MHWMemCheckBank+44>:        and     v1,v1,v0
0x007eb6e8 <MHWMemCheckBank+48>:        addiu   t0,s0,12
0x007eb6ec <MHWMemCheckBank+52>:        addiu   v0,s0,4
0x007eb6f0 <MHWMemCheckBank+56>:        move    s2,a0
0x007eb6f4 <MHWMemCheckBank+60>:        movn    t0,v0,v1
0x007eb6f8 <MHWMemCheckBank+64>:        move    s3,a1
0x007eb6fc <MHWMemCheckBank+68>:        beqz    a3,0x7eb73c <MHWMemCheckBank+132>
0x007eb700 <MHWMemCheckBank+72>:        move    s4,a2
0x007eb704 <MHWMemCheckBank+76>:        lw      a1,0(t0)
0x007eb708 <MHWMemCheckBank+80>:        lw      v1,24(s0)
0x007eb70c <MHWMemCheckBank+84>:        lw      v0,104(a0)
0x007eb710 <MHWMemCheckBank+88>:        li      a2,100
0x007eb714 <MHWMemCheckBank+92>:        mul     v1,v1,a2
0x007eb718 <MHWMemCheckBank+96>:        mul     v0,a1,v0
0x007eb71c <MHWMemCheckBank+100>:       sltu    v0,v1,v0
0x007eb720 <MHWMemCheckBank+104>:       beqz    v0,0x7eb73c <MHWMemCheckBank+132>
0x007eb724 <MHWMemCheckBank+108>:       nop
0x007eb728 <MHWMemCheckBank+112>:       divu    zero,v1,a1
0x007eb72c <MHWMemCheckBank+116>:       teq     a1,zero,0x7
0x007eb730 <MHWMemCheckBank+120>:       mflo    a1
0x007eb734 <MHWMemCheckBank+124>:       jal     0x7efa14 <MEMMAN_SHL_Notify>
0x007eb738 <MHWMemCheckBank+128>:       subu    a1,a2,a1
-> 0x007eb73c <MHWMemCheckBank+132>:       lw      v0,24(s0)
...
---Type <return> to continue, or q <return> to quit---

(gdb) info locals
ind_bank = <value optimized out>
bank = (MemWholeMemory *) 0x0		#
pool_max = (uint32_t *) 0xc

(gdb) x/16wx 0x0131bc40 	# value of a0
0x131bc40:      0x0131f408      0x2ab97ca0      0x00000000      0x00000000
0x131bc50:      0x00000000      0x00000000      0x00000000      0x00000000
0x131bc60:      0xffffffff      0x00000000      0x00000000      0x41445054
0x131bc70:      0x5f444941      0x47000000      0x00000000      0x00000000

this shows that structure passed on a0 has some NULLs and means that this pool was already
destoried. when see destory func, it sets poolHandle->s_WholeMem=NULL;


={============================================================================
*kt_linux_tool_202* gdb: kernel crash analysis {frame-command}

# 01
#

The crash dump When a crash in a kernel module happens, you should see output like the
following on the serial port or in the dmesg buffer (just run the dmesg command to see
it). 

<4>Unhandled kernel unaligned access[#1]:
<4>Cpu 0
<4>$ 0   : 00000000 10008400 {f7ffdfdf} 80070000	# {v0}
<4>$ 4   : c06e27c0 000ee208 8123a000 898d2680
<4>$ 8   : 00000000 7edbffff ffdeff7f fffb7fff
<4>$12   : fdf7fed7 000ee247 00000001 00000001
<4>$16   : 898d2680 00000000 00000001 00000001
<4>$20   : 898d2680 c06e2800 898d2680 00000001
<4>$24   : 00000001 c0504bcc                  
<4>$28   : 89cd2000 89cd3830 000ee208 c050cbe4
<4>Hi    : 00000128
<4>Lo    : 003e5708
<4>epc   : c050cc54 XHddLowIO+0x124/0x3d8 [xtvfs]     Tainted: P    # {pc} 
<4>ra    : c050cbe4 XHddLowIO+0xb4/0x3d8 [xtvfs]
<4>Status: 10008403    KERNEL EXL IE 
<4>Cause : 00800010
<4>BadVA : f7ffe073
<4>PrId  : 0002a044
<4>Modules linked in: xtvfs mhxnvramfs callisto_periph callisto_tuner callisto
<4>Process mount (pid: 404, threadinfo=89cd2000, task=89a249e8)
<4>Stack : 00000001 000ee08d 00000000 c04e523c c05394c4 00000000 00000000 f7ffdfdf
<4>        c06e2800 003e5708 00000001 000ee208 00000000 c04e523c c05394c4 00000000
<4>        c053950c c050cf50 00808000 c050d21c c06e2800 8567bc00 00000000 003e5708
<4>        c04d7f78 c04d7f58 ff7fffdf 000edf25 00000001 00000001 c067e200 c04d804c
<4>        c05394c4 89cd3950 00000000 c04e523c 000ee208 c06e2800 00000001 00000000
<4>        ...
<4>Call Trace:
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]
<4>[<c050cf50>] XHddLowRead+0x1c/0x28 [xtvfs]
<4>[<c04d7f78>] root_dir_devio_read+0x58/0x12c [xtvfs]
<4>[<c04d809c>] root_dir_devio_lock_read+0x50/0x84 [xtvfs]
<4>[<c04d8430>] RootDirCpyClusterReadBuffer+0xec/0x180 [xtvfs]
<4>[<c04d90cc>] RootDirCpyCheckCreate+0xf0/0x1214 [xtvfs]
<4>[<c04da344>] RootDirCpyInit+0x154/0x200 [xtvfs]
<4>[<c04d2944>] pc_ppart_init+0x10bc/0x12a8 [xtvfs]
<4>[<c04fb178>] XTVFS_CheckPpartInit+0x38/0x32c [xtvfs]
<4>[<c04fc684>] InitPpart+0x238/0x540 [xtvfs]
<4>[<c04fca28>] XTVFS_Mount+0x9c/0x490 [xtvfs]
<4>[<c050c8c8>] xtvfs_read_super+0x1e0/0x370 [xtvfs]
<4>[<c050bb80>] xtvfs_fill_super+0x18/0x48 [xtvfs]
<4>[<8007a6b0>] get_sb_bdev+0x114/0x194
<4>[<c050bb5c>] xtvfs_get_sb+0x2c/0x38 [xtvfs]
<4>[<80079f2c>] vfs_kern_mount+0x68/0xc4
<4>[<80079fe4>] do_kern_mount+0x4c/0x7c
<4>[<80094f10>] do_mount+0x5a8/0x614
<4>[<80095010>] sys_mount+0x94/0xec
<4>[<8000e5f0>] stack_done+0x20/0x3c
<4>
<4>
<4>Code: 00008821  8fa2001c  3c038007 <8c440094> 24631824  0060f809  8c46000c  ae020000  27de0001


Unhandled kernel unaligned access An unaligned access is a type of crash. Unlike a
segmentation fault, where a process tries to read memory that is not in its memory map,
and unaligned access is an attempt to read or write an address that is not on a word
boundry. On 32 bit CPUs this means an address not divisble by 4. Often this will generate
an exception and some software will handle the access by reading adjacent words and
piecing things together. But in our case the exception is unhandled. 


$0, $4, etc

This output shows the value of the registers. We are on a MIPS CPU and many of the
registers have defined uses. This
document(http://msdn.microsoft.com/en-us/library/aa448706.aspx) describes them. For
example, $4 to $7 are used to store the first 4 words of function arguments when calling a
function. In assembler these registers are referred to as a0 to a3. You can't know this in
advance, you have to read the MIPS documentation to find it out.  epc c050cc54
XHddLowIO+0x124/0x3d8 [xtvfs] This is the {exception-program-counter}. It shows the address
that the exception occurred at, and that this is 0x124 bytes into the function XHddLowIO
in the module xtvfs.ko. 


Getting the disassembly

Given a kernel module like xtvfs.ko, it is possible to see the disassembled code using the
objdump -D command. Since we have a mips module, we use the cross-compiler from the
Clearcase Fusion view, so our command will look something like: 

mips-linux-uclibc-objdump -D xtvfs.ko

We can then look for the function where we crashed, which is XHddLowIO from the epc trace
above. It starts like this: 

00045b30 <XHddLowIO>:
   45b30:       27bdffb8        addiu   sp,sp,-72
   45b34:       3c020002        lui     v0,0x2
   45b38:       afb50034        sw      s5,52(sp)
   45b3c:       345500d0        ori     s5,v0,0xd0
   45b40:       3c020000        lui     v0,0x0
   45b44:       afb7003c        sw      s7,60(sp)

From the call trace we can calculate the address offset in use. Recall: 
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]

So c050cc54 - 0x124 - 45b30 = offset = c04c7000 (the start of loadded in memory for this
xtvfs.ko). The crash happened at c050cc54 which will appear as c050cc54 - c04c7000 = 45c54
in the disassembly. That code looks like this: 

or 45b30+0x124 = 45c54


   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)
   45c50:       3c030000        lui     v1,0x0
 {45c54}:       8c440094        lw      a0,148(v0)
   45c58:       24630000        addiu   v1,v1,0
   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

So we have crashed executing an lw instruction. 


Using the relocation table

Given a kernel module like xtvfs.ko, it is possible to see the offsets of functions (the
relocation table) using the objdump -r command. Since we have a mips module, we use the
cross-compiler from the Clearcase Fusion view, so our command will look something like: 

mips-linux-uclibc-objdump -r xtvfs.ko > xtvfs_relocations


The output near to our crash address of 45c54 is a table like this: 

 00045c20 R_MIPS_26         .text
 00045c2c R_MIPS_26         .text
 00045c44 R_MIPS_26         .text
 00045c50 R_MIPS_HI16       __getblk
 00045c58 R_MIPS_LO16       __getblk
 00045c78 R_MIPS_HI16       printk
 00045c80 R_MIPS_LO16       printk


Because we are using load time
relocation(http://eli.thegreenplace.net/2011/08/25/load-time-relocation-of-shared-libraries/)
of shared libraries, this table tells the operating system how to replace addresses in the
code. The first column is the address in the code, the second column is the type of
relocation to do, and the third column is the address to relocate. So the code at address
45c50 and 45c58 will get overwritten with the address of __getblk. That makes the
disassembly of the code near our crash look like this: 

   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)
   45c50:       3c030000        lui     v1,__getblk
   45c54:       8c440094        lw      a0,148(v0)
   45c58:       24630000        addiu   v1,v1,0
   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

Understanding __getblk

At 45c5c there is a jump instruction to v1 which has been loaded with the address of
__getblk. But we crashed immediately before that.

	So it seems we crashed while preparing to call __getblk. 
	
So it would help to understand this function. We can look it up in the kernel code: 
http://lxr.free-electrons.com/source/fs/buffer.c?v=2.6.30;a=mips#L1363

1362 /*
1363  * __getblk will locate (and, if necessary, create) the buffer_head
1364  * which corresponds to the passed block_device, block and size. The
1365  * returned buffer has its reference count incremented.
1366  *
1367  * __getblk() cannot fail - it just keeps trying.  If you pass it an
1368  * illegal block number, __getblk() will happily return a buffer_head
1369  * which represents the non-existent block.  Very weird.
1370  *
1371  * __getblk() will lock up the machine if grow_dev_page's try_to_free_buffers()
1372  * attempt is failing.  FIXME, perhaps?
1373  */
1374 struct buffer_head *
1375 __getblk(struct block_device *bdev, sector_t block, unsigned size)
1376 {
1377         struct buffer_head *bh = __find_get_block(bdev, block, size);
1378 
1379         might_sleep();
1380         if (bh == NULL)
1381                 bh = __getblk_slow(bdev, block, size);
1382         return bh;
1383 }
1384 EXPORT_SYMBOL(__getblk);

We should also notice that it can be called via inline function sb_blk : 

287 static inline struct buffer_head *
288 sb_getblk(struct super_block *sb, sector_t block)
289 {
290         return __getblk(sb->s_bdev, block, sb->s_blocksize);
291 }


Understanding where in our C code the crash happened

Now we can tell exactly where in our C code the crash happened. We know we were in the
function XHddLowIO from the call trace and now we know we were calling __getblk or
sb_getblk. In XHddLowIO in the XTVFS code we can see: 


/* allocate sector buffers */
for(i = 0; i < cnt; i++){
    bh_array[i] = sb_getblk(sb,  block++);
    if(!bh_array[i]){ /* no sufficient buffers */
        printk("\n BH_ArrayXHddLowIO: bh = 0, i = %d !!!!!", i);
        if(0 == i){ /* no at all */
            return X_ERROR;
        }
        /* use what we have */
        cnt = i;
    }

So it is likely that the crash happened very close to this sb_getblk call. 


Understanding the lw instruction

Recall the instruction that crashed: 

   45c54:       8c440094        lw      a0,148(v0)

What does that notation mean? We can look up information about the MIPS instructions:
Description: A word is loaded into a register from the specified address.  Operation: $t =
MEM[$s + offset]; advance_pc (4); Syntax: lw $t, offset($s) The whole instruction means:
load a0 with the address in v0 + 148. 


Understanding the MIPS registers

In MIPS, registers tend to have special functions, such as return addresses or function
parameters. We can read about this online. 

a0, the register we are writing to, is the first of the "function argument registers that
hold the first four words of integer type arguments." So a0 is the first argument to the
function we are calling.  v0 is a "function result register" and is also called $2. So we
know its value from the original trace: 

<4>$ 0   : 00000000 10008400 f7ffdfdf 80070000

It is f7ffdfdf. Which is an *odd number*. Since we are trying to read from this address
and do arithmetic (add 148) with it, this would explain why we get an unaligned access. 


Putting it all together

We are executing this line of C: 

bh_array[i] = sb_getblk(sb,  block++);

Because sb_getblk is an inline function, it has been expanded by the compiler into:
__getblk(sb->s_bdev, block, sb->s_blocksize); So our crashing instruction is adding 148
because 148 is the offset of s_bdev withing the sb struct. We hav verify this by looking
at struct super_block in the code.  However, sb has somehow become and odd number, and
THAT is our bug. 


={============================================================================
*kt_linux_tool_203* gdb: .gdbinit

{example-one}
can define user func that have commands to run

$ more .gdbinit
set history save on
set history filename ./.gdb_history
set output-radix 16

define connect
    handle SIG32 nostop noprint pass
    handle SIG33 nostop noprint pass
#    b CTL_SimpleZapperTestStep
#    b CTL_ChannelZapping_FullStbTearDown
#               b readSectionFilterDataAndWriteToFile
    b sectionFilterTask
                b CTL_SectionFilter_Engine.c:2126
                b CTL_SectionFilter_Engine.c:2146
    directory components/FOSH/FUSIONOS_XTV_TESTS/xtv_test/src/
                i b
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define init_task
   set $t=&init_task
   printf "task name \"%s\", pid %05d \n", $t->comm, t->pid
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define find_task
  # Addresses greater than _end: kernel data...
  # ...user passed in an address
  if ((unsigned)$arg0 > (unsigned)&_end)
    set $t=(struct task_struct *)$arg0
  else
    # User entered a numeric PID
    # Walk the task list to find it
    set $t=&init_task
    if (init_task.pid != (unsigned)$arg0)
      find_next_task $t
      while (&init_task!=$t && $t->pid != (unsigned)$arg0)
        find_next_task $t
      end
      if ($t == &init_task)
        printf "Couldn't find task; using init_task\n"
      end
    end
  end
  printf "Task \"%s\":\n", $t->comm
end


Reads and executes the commands from init file (if any) in the current working directory. This is
only done if the current directory is different from your home directory. Thus, you can have more
than one init file, one generic in your home directory, and another, specific to the program you are
debugging, in the directory where you invoke gdb.


{gdbinit-for-c++}
#
#   STL GDB evaluators/views/utilities - 1.03
#
#   The new GDB commands:
#       are entirely non instrumental
#       do not depend on any "inline"(s) - e.g. size(), [], etc
#       are extremely tolerant to debugger settings
#
#   This file should be "included" in .gdbinit as following:
#   source stl-views.gdb or just paste it into your .gdbinit file
#
#   The following STL containers are currently supported:
#
#       std::vector<T> -- via pvector command
#       std::list<T> -- via plist or plist_member command
#       std::map<T,T> -- via pmap or pmap_member command
#       std::multimap<T,T> -- via pmap or pmap_member command
#       std::set<T> -- via pset command
#       std::multiset<T> -- via pset command
#       std::deque<T> -- via pdequeue command
#       std::stack<T> -- via pstack command
#       std::queue<T> -- via pqueue command
#       std::priority_queue<T> -- via ppqueue command
#       std::bitset<n> -- via pbitset command
#       std::string -- via pstring command
#       std::widestring -- via pwstring command
#
#   The end of this file contains (optional) C++ beautifiers
#   Make sure your debugger supports $argc
#
#   Simple GDB Macros writen by Dan Marinescu (H-PhD) - License GPL
#   Inspired by intial work of Tom Malnar,
#     Tony Novac (PhD) / Cornell / Stanford,
#     Gilad Mishne (PhD) and Many Many Others.
#   Contact: dan_c_marinescu@yahoo.com (Subject: STL)
#
#   Modified to work with g++ 4.3 by Anders Elton
#   Also added _member functions, that instead of printing the entire class in map, prints a member.

# support for pending breakpoints - you can now set a breakpoint into a shared library before the it was loaded.
set breakpoint pending on

#
# std::vector<>
#

define pvector
    if $argc == 0
        help pvector
    else
        set $size = $arg0._M_impl._M_finish - $arg0._M_impl._M_start
        set $capacity = $arg0._M_impl._M_end_of_storage - $arg0._M_impl._M_start
        set $size_max = $size - 1
    end
    if $argc == 1
        set $i = 0
        while $i < $size
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
    end
    if $argc == 2
        set $idx = $arg1
        if $idx < 0 || $idx > $size_max
            printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
        else
            printf "elem[%u]: ", $idx
            p *($arg0._M_impl._M_start + $idx)
        end
    end
    if $argc == 3
      set $start_idx = $arg1
      set $stop_idx = $arg2
      if $start_idx > $stop_idx
        set $tmp_idx = $start_idx
        set $start_idx = $stop_idx
        set $stop_idx = $tmp_idx
      end
      if $start_idx < 0 || $stop_idx < 0 || $start_idx > $size_max || $stop_idx > $size_max
        printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
      else
        set $i = $start_idx
        while $i <= $stop_idx
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
      end
    end
    if $argc > 0
        printf "Vector size = %u\n", $size
        printf "Vector capacity = %u\n", $capacity
        printf "Element "
        whatis $arg0._M_impl._M_start
    end
end

document pvector
    Prints std::vector<T> information.
    Syntax: pvector <vector> <idx1> <idx2>
    Note: idx, idx1 and idx2 must be in acceptable range [0..<vector>.size()-1].
    Examples:
    pvector v - Prints vector content, size, capacity and T typedef
    pvector v 0 - Prints element[idx] from vector
    pvector v 1 2 - Prints elements in range [idx1..idx2] from vector
end

#
# std::list<>
#

define plist
    if $argc == 0
        help plist
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 2
                printf "elem[%u]: ", $size
                p *($arg1*)($current + 1)
            end
            if $argc == 3
                if $size == $arg2
                    printf "elem[%u]: ", $size
                    p *($arg1*)($current + 1)
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist <variable_name> <element_type> to see the elements in the list.\n"
        end
    end
end

document plist
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist l - prints list size and definition
    plist l int - prints all elements and list size
    plist l int 2 - prints the third element in the list (if exists) and list size
end

define plist_member
    if $argc == 0
        help plist_member
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 3
                printf "elem[%u]: ", $size
                p (*($arg1*)($current + 1)).$arg2
            end
            if $argc == 4
                if $size == $arg3
                    printf "elem[%u]: ", $size
                    p (*($arg1*)($current + 1)).$arg2
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist_member <variable_name> <element_type> <member> to see the elements in the list.\n"
        end
    end
end

document plist_member
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist_member l int member - prints all elements and list size
    plist_member l int member 2 - prints the third element in the list (if exists) and list size
end


#
# std::map and std::multimap
#

define pmap
    if $argc == 0
        help pmap
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 3
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p *($arg1*)$value
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p *($arg2*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 4
            set $idx = $arg3
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p *($arg1*)$value
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p *($arg2*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        if $argc == 5
            set $idx1 = $arg3
            set $idx2 = $arg4
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                set $valueLeft = *($arg1*)$value
                set $valueRight = *($arg2*)($value + sizeof($arg1))
                if $valueLeft == $idx1 && $valueRight == $idx2
                    printf "elem[%u].left: ", $i
                    p $valueLeft
                    printf "elem[%u].right: ", $i
                    p $valueRight
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap m - prints map size and definition
    pmap m int int - prints all elements and map size
    pmap m int int 20 - prints the element(s) with left-value = 20 (if any) and map size
    pmap m int int 20 200 - prints the element(s) with left-value = 20 and right-value = 200 (if any) and map size
end


define pmap_member
    if $argc == 0
        help pmap_member
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 5
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p (*($arg1*)$value).$arg2
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p (*($arg3*)$value).$arg4
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 6
            set $idx = $arg5
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p (*($arg1*)$value).$arg2
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p (*($arg3*)$value).$arg4
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap_member
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap_member m class1 member1 class2 member2 - prints class1.member1 : class2.member2
    pmap_member m class1 member1 class2 member2 lvalue - prints class1.member1 : class2.member2 where class1 == lvalue
end


#
# std::set and std::multiset
#

define pset
    if $argc == 0
        help pset
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Set "
            whatis $tree
            printf "Use pset <variable_name> <element_type> to see the elements in the set.\n"
        end
        if $argc == 2
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u]: ", $i
                p *($arg1*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 3
            set $idx = $arg2
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u]: ", $i
                    p *($arg1*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Set size = %u\n", $tree_size
    end
end

document pset
    Prints std::set<T> or std::multiset<T> information. Works for std::multiset as well.
    Syntax: pset <set> <T> <val>: Prints set size, if T defined all elements or just element(s) having val
    Examples:
    pset s - prints set size and definition
    pset s int - prints all elements and the size of s
    pset s int 20 - prints the element(s) with value = 20 (if any) and the size of s
end



#
# std::dequeue
#

define pdequeue
    if $argc == 0
        help pdequeue
    else
        set $size = 0
        set $start_cur = $arg0._M_impl._M_start._M_cur
        set $start_last = $arg0._M_impl._M_start._M_last
        set $start_stop = $start_last
        while $start_cur != $start_stop
            p *$start_cur
            set $start_cur++
            set $size++
        end
        set $finish_first = $arg0._M_impl._M_finish._M_first
        set $finish_cur = $arg0._M_impl._M_finish._M_cur
        set $finish_last = $arg0._M_impl._M_finish._M_last
        if $finish_cur < $finish_last
            set $finish_stop = $finish_cur
        else
            set $finish_stop = $finish_last
        end
        while $finish_first != $finish_stop
            p *$finish_first
            set $finish_first++
            set $size++
        end
        printf "Dequeue size = %u\n", $size
    end
end

document pdequeue
    Prints std::dequeue<T> information.
    Syntax: pdequeue <dequeue>: Prints dequeue size, if T defined all elements
    Deque elements are listed "left to right" (left-most stands for front and right-most stands for back)
    Example:
    pdequeue d - prints all elements and size of d
end



#
# std::stack
#

define pstack
    if $argc == 0
        help pstack
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = $size - 1
        while $i >= 0
            p *($start_cur + $i)
            set $i--
        end
        printf "Stack size = %u\n", $size
    end
end

document pstack
    Prints std::stack<T> information.
    Syntax: pstack <stack>: Prints all elements and size of the stack
    Stack elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    pstack s - prints all elements and the size of s
end



#
# std::queue
#

define pqueue
    if $argc == 0
        help pqueue
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = 0
        while $i < $size
            p *($start_cur + $i)
            set $i++
        end
        printf "Queue size = %u\n", $size
    end
end

document pqueue
    Prints std::queue<T> information.
    Syntax: pqueue <queue>: Prints all elements and the size of the queue
    Queue elements are listed "top to bottom" (top-most element is the first to come on pop)
    Example:
    pqueue q - prints all elements and the size of q
end



#
# std::priority_queue
#

define ppqueue
    if $argc == 0
        help ppqueue
    else
        set $size = $arg0.c._M_impl._M_finish - $arg0.c._M_impl._M_start
        set $capacity = $arg0.c._M_impl._M_end_of_storage - $arg0.c._M_impl._M_start
        set $i = $size - 1
        while $i >= 0
            p *($arg0.c._M_impl._M_start + $i)
            set $i--
        end
        printf "Priority queue size = %u\n", $size
        printf "Priority queue capacity = %u\n", $capacity
    end
end

document ppqueue
    Prints std::priority_queue<T> information.
    Syntax: ppqueue <priority_queue>: Prints all elements, size and capacity of the priority_queue
    Priority_queue elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    ppqueue pq - prints all elements, size and capacity of pq
end



#
# std::bitset
#

define pbitset
    if $argc == 0
        help pbitset
    else
        p /t $arg0._M_w
    end
end

document pbitset
    Prints std::bitset<n> information.
    Syntax: pbitset <bitset>: Prints all bits in bitset
    Example:
    pbitset b - prints all bits in b
end



#
# std::string
#

define pstring
    if $argc == 0
        help pstring
    else
        printf "String \t\t\t= \"%s\"\n", $arg0._M_data()
        printf "String size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "String capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "String ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pstring
    Prints std::string information.
    Syntax: pstring <string>
    Example:
    pstring s - Prints content, size/length, capacity and ref-count of string s
end

#
# std::wstring
#

define pwstring
    if $argc == 0
        help pwstring
    else
        call printf("WString \t\t= \"%ls\"\n", $arg0._M_data())
        printf "WString size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "WString capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "WString ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pwstring
    Prints std::wstring information.
    Syntax: pwstring <wstring>
    Example:
    pwstring s - Prints content, size/length, capacity and ref-count of wstring s
end

#
# C++ related beautifiers (optional)
#

set print pretty on
set print object on
set print static-members on
set print vtbl on
set print demangle on
set demangle-style gnu-v3
set print sevenbit-strings off

set history filename ~/.gdb_history
set history save

# finally stop the silly "A debugging session is active." - question ... just quit both.
set confirm off

={============================================================================
*kt_linux_tool_204* gdb: source and docs

{gdb-site}
https://sourceware.org/gdb/

{gdb-doc}
# pdf is also available
https://sourceware.org/gdb/download/onlinedocs/gdb/index

# GDB Command Referencehtml
http://visualgdb.com/gdbreference/commands/sharedlibrary 


={============================================================================
*kt_linux_tool_204* gdb: debugging info, run and quit

{gdb-gcc} # debugging information
Programs that are to be shipped to your customers are compiled with optimizations, using the ‘-O’
compiler option. However, some compilers are unable to handle the ‘-g’ and ‘-O’ options together.
Using those compilers, you cannot generate optimized executables containing debugging information.

<gbd-debug-and-optimization>
gcc, the gnu C/C++ compiler, supports ‘-g’ with or without ‘-O’, making it possible to debug
optimized code. We recommend that you always use ‘-g’ whenever you compile a program. You may think
your program is correct, but there is no sense in pushing your luck. For more information, see
Chapter 11 [Optimized Code], page 149.

<gdb-macro>
gdb knows about preprocessor macros and can show you their expansion (see Chapter 12 [Macros], page
153). Most compilers do not include information about preprocessor macros in the debugging
information if you specify the ‘-g’ flag alone. Version 3.1 and later of gcc provides macro
information if you are using the DWARF debugging format, and specify the option ‘-g3’.  See Section
“Options for Debugging Your Program or GCC” in Using the gnu Compiler Collection (GCC), for more
information on gcc options affecting debug information.

<qdb-quit>
type quit or Ctrl-d to exit.

<gdb-interrupt>
An interrupt (often Ctrl-c) does not exit from gdb, but rather terminates the action of any gdb
command that is in progress and returns to gdb command level. It is safe to type the interrupt
character at any time because gdb does not allow it to take effect until a time when it is safe.

What if the program is running but you forgot to set breakpoints? You can hit CTRL-C and that'll
stop the program wherever it happens to be and return you to a "(gdb)" prompt. At that point, you
could set up a proper breakpoint somewhere and continue to that breakpoint.

<return-key>
One final shortcut is that just hitting RETURN will repeat the last command entered; this will save
you typing next over and over again.


={============================================================================
*kt_linux_tool_205* gdb: commands

{gdb-command-completion}
If you just want to see the list of alternatives in the first place, you can press M-? rather than
pressing TAB twice. M-? means META ?. You can type this either by holding down a key designated as
the META shift on your keyboard (if there is one) while typing ?, or as ESC followed by ?.

<gdb-command-completion-for-cpp-overloading>
Sometimes the string you need, while logically a “word”, may contain parentheses or other characters
that gdb normally excludes from its notion of a word. To permit word completion to work in this
situation, you may enclose words in ’ (single quote marks) in gdb commands.

The most likely situation where you might need this is in typing the name of a C++ function. This is
because C++ allows function overloading (multiple definitions of the same function, distinguished by
argument type). For example, when you want to set a breakpoint you may need to distinguish
whether you mean the version of name that takes an int parameter, name(int), or the version that
takes a float parameter, name(float). To use the word-completion facilities in this situation, type
a single quote ’ at the beginning of the function name. This alerts gdb that it may need to consider
more information than usual when you press TAB or M-? to request word completion: (gdb) b ’bubble(
M-?  bubble(double,double) bubble(int,int) (gdb) b ’bubble(

<gdb-command-completion-for-structure>
When completing in an expression which looks up a field in a structure, gdb also tries to limit
completions to the field names available in the type of the left-hand-side:

(gdb) p gdb_stdout.M-?
magic to_fputs to_rewind
to_data to_isatty to_write
to_delete to_put to_write_async_safe
to_flush to_read

{gdb-info}
info This command (abbreviated i) is for describing the state of your program. For example, you can
show the arguments passed to a function with info args, list the registers currently in use with
info registers, or list the breakpoints you have set with info breakpoints. You can get a complete
list of the info sub-commands with help info.

info program

Display information about the status of your program: whether it is running or not, what process it
is, and why it stopped.

{gdb-show}
show configuration
Display detailed information about the way gdb was configured when it was built. This displays the
optional arguments passed to the ‘configure’ script and also configuration parameters detected
automatically by configure. When reporting a gdb bug (see Chapter 31 [GDB Bugs], page 483), it is
important to include this information in your report.


={============================================================================
*kt_linux_tool_206* gdb: commands for stepping

{gdb-next-gdb-step}
n (next) to advance execution to the next line of the current function.

s (step) instead of next. step goes to the next line to be executed in any subroutine.
It shows a summary of the stack. can use the backtrace command (which can also be spelled bt), to
see where we are in the stack as a whole: the backtrace command displays a stack frame for each
active subroutine.

note. Warning: If you use the step command while control is within a function that was compiled
without debugging information, execution proceeds until control reaches a function that does have
debugging information. Likewise, it will not step into a function which is compiled without
debugging information. To step through functions without debugging information, use the stepi
command, described below.

To step for a single assembly instruction, use the (gdb) stepi

Also, the step command 'only' enters a function if there is line number information for the
function. Otherwise it acts like the next command.

{gdb-return-gdb-finish}
To resume execution at a different place, you can use return (see Section 17.4 [Returning from a
Function], page 221) to go back to the calling function; or jump (see Section 17.2 [Continuing
at a Different Address], page 220) to go to an arbitrary location in your program.

The finish Continue running until just after function in the selected stack frame returns.  Print
the returned value (if any). This command can be abbreviated as fin.  Contrast this with the return
command (see Section 17.4 [Returning from a Function], page 221). Q: this is only difference?

{gdb-print}
p (print) to see their values.

(gdb) p len lquote
$3 = 9
(gdb) p len rquote
$4 = 7

<gdb-set>
can 'set' them to better values using the p command, since it can print the value of any
expression—and that expression can include subroutine calls and assignments. set new value and
continue to see if it fixes the bug.

(gdb) p len lquote=strlen(lquote)
$5 = 7
(gdb) p len rquote=strlen(rquote)
$6 = 9
(gdb) c
Continuing.


={============================================================================
*kt_linux_tool_207* gdb: multi thread

{gdb-thread}
4.10 Debugging Programs with Multiple Threads

The precise semantics of threads differ from one operating system to another, but in general the
threads of a single program are akin to multiple processes—except that they share one address space
(that is, they can all examine and modify the same variables). On the other hand, each thread has
its own registers and execution stack, and perhaps private memory.

gdb provides these facilities for debugging multi-thread programs:

* automatic notification of new threads
* ‘thread threadno’, a command to switch among threads
* ‘info threads’, a command to inquire about existing threads
* ‘thread apply [threadno] [all] args’, a command to apply a command to a list of threads
* thread-specific breakpoints

<current-thread>
The gdb thread debugging facility allows you to observe all threads while your program runs—but
whenever gdb takes control, one thread in particular is always the focus of debugging. This thread
is called the current thread. Debugging commands show program information from the perspective of
the current thread.

(gdb) info threads
Id Target Id Frame
3 process 35 thread 27 0x34e5 in sigpause ()
2 process 35 thread 23 0x34e5 in sigpause ()
* 1 process 35 thread 13 main (argc=1, argv=0x7ffffff8)

{gdb-thread-apply}
thread apply [threadno | all] command

The thread apply command allows you to apply the named command to one or more threads. Specify the
numbers of the threads that you want affected with the command argument threadno. It can be a single
thread number, one of the numbers shown in the first field of the ‘info threads’ display; or it
could be a range of thread numbers, as in 2-4. To apply a command to all threads, type thread apply
all command.


={============================================================================
*kt_linux_tool_208* gdb: breakpoints

{gdb-breakpoint}
Breakpoints are set with the break command (abbreviated b).

gdb normally implements breakpoints by replacing the program code at the breakpoint address with a
special instruction, which, when executed, given control to the debugger.  By default, the program
code is so modified only when the program is resumed. As soon as the program stops, gdb restores the
original instructions. This behaviour guards against leaving breakpoints inserted in the target
should gdb abrubptly disconnect. However, with slow remote targets, inserting and removing
breakpoint can reduce the performance. This behavior can be controlled with the following commands::

<shared-and-multiple>
On some systems, you can set breakpoints in shared libraries before the executable is run. There is
a minor limitation on HP-UX systems: you must wait until the executable is run in order to set
breakpoints in shared library routines that are not called directly by the program (for example,
routines that are arguments in a pthread_create call).

It’s quite common to have a breakpoint inside a shared library. Shared libraries can be loaded and
unloaded explicitly, and possibly repeatedly, as the program is executed.  To support this use case,
gdb updates breakpoint locations whenever any shared library is loaded or unloaded.
Typically, you would set a breakpoint in a shared library at the beginning of your
debugging session, when the library is not loaded, and when the symbols from the library
are not available. When you try to set breakpoint, gdb will ask you if you want to set a so
called pending breakpoint—breakpoint whose address is not yet resolved.  After the program
is run, whenever a new shared library is loaded, gdb reevaluates all the breakpoints. When
a newly loaded shared library contains the symbol or line referred to by some pending
breakpoint, that breakpoint is resolved and becomes an ordinary breakpoint.  When a library
is unloaded, all breakpoints that refer to its symbols or source lines become pending
again.

This logic works for breakpoints with multiple locations, too. For example, if you have a breakpoint
in a C++ template function, and a newly loaded shared library has an instantiation of that template,
a new location is added to the list of locations for the breakpoint.

Except for having unresolved address, pending breakpoints do not differ from regular breakpoints.
You can set conditions or commands, enable and disable them and perform other breakpoint operations.

set breakpoint pending auto

This is the default behavior. When gdb cannot find the breakpoint location, it queries you whether a
pending breakpoint should be created.

set breakpoint pending on

This indicates that an unrecognized breakpoint location should automatically result in a pending
breakpoint being created.

A breakpoint with multiple locations is displayed in the breakpoint table using several rows—one
header row, followed by one row for each breakpoint location. The header row has ‘<MULTIPLE>’ in the
address column. The rows for individual locations contain the actual addresses for locations, and
show the functions to which those locations belong. The number column for a location is of the form
breakpoint-number.location-number.

For example:
Num   Type        Disp     Enb      Address     What
1     breakpoint  keep     y        <MULTIPLE>
      stop only if i==1
      breakpoint already hit 1 time
1.1                        y 0x080486a2 in void foo<int>() at t.cc:8
1.2                        y 0x080486ca in void foo<double>() at t.cc:8

Each location can be individually enabled or disabled by passing breakpointnumber.  location-number
as argument to the enable and disable commands.

<watchpoint>
A watchpoint is a special breakpoint that stops your program when the value of an expression
changes. The expression may be a value of a variable, or it could involve values of one or more
variables combined by operators, such as ‘a + b’. This is sometimes called data breakpoints. You
must use a different command to set watchpoints (see Section 5.1.2 [Setting Watchpoints], page 50),
but aside from that, you can manage a watchpoint like any other breakpoint: you enable,
disable, and delete both breakpoints and watchpoints using the same commands.

<catchpoint>
A catchpoint is another special breakpoint that stops your program when a certain kind of event
occurs, such as the throwing of a C++ exception or the loading of a library. As with watchpoints,
you use a different command to set a catchpoint (see Section 5.1.3 [Setting Catchpoints], page
53), but aside from that, you can manage a catchpoint like any other breakpoint. (To stop when
your program receives a signal, use the handle command; see Section 5.4 [Signals], page 70.)


{gdb-break-location}
Set a breakpoint at the given location, which can specify a function name, a line number, or an
address of an instruction. (See Section 9.2 [Specify Location], page 98, for a list of all the
possible ways to specify a location.) The breakpoint will stop your program just before it
executes any of the code in the specified location.

<overloading-and-thread>
When using source languages that permit overloading of symbols, such as C++, a function name may
refer to more than one possible place to break. See Section 10.2 [Ambiguous Expressions], page 110,
for a discussion of that situation.  It is also possible to insert a breakpoint that will stop
the program only if a specific thread (see Section 5.5.4 [Thread-Specific Breakpoints], page
76) or a specific task (see Section 15.4.9.6 [Ada Tasks], page 204) hits that breakpoint.

{gdb-break-cond}
break ... if cond

Set a breakpoint with condition cond; evaluate the expression cond each time the breakpoint is
reached, and stop only if the value is nonzero—that is, if cond evaluates as true. ‘...’ stands for
one of the possible arguments described above (or no argument) specifying where to break. See
Section 5.1.6 [Break Conditions], page 59, for more information on breakpoint conditions.

{gdb-break-tbreak}
tbreak args

Set a breakpoint enabled only for 'one' stop. args are the same as for the break command, and the
breakpoint is set in the same way, but the breakpoint is auto matically deleted after the first time
your program stops there. See Section 5.1.5 [Disabling Breakpoints], page 58.

<gdb-break-info>
info breakpoints [n...]
info break [n...]

Print a table of all breakpoints, watchpoints, and catchpoints set and not deleted. Optional
argument n means print information only about the specified breakpoint(s) (or watchpoint(s) or
catchpoint(s)). For each breakpoint, following columns are printed:

It is possible that a breakpoint corresponds to several locations in your program. Examples
of this situation are:

1) Multiple functions in the program may have the same name.

2) For a C++ constructor, the gcc compiler generates several instances of the function body, used in
different cases.

3) For a C++ template function, a given line in the function can correspond to any number of
instantiations.

4) For an inlined function, a given source line can correspond to several places where that function
is inlined.

In all those cases, gdb will insert a breakpoint at all the relevant locations.


{break-disabling}
5.1.5 Disabling breakpoints

This makes the breakpoint inoperative as if it had been deleted, but remembers the information on
the breakpoint so that you can enable it again later. 

Optionally specifying one or more breakpoint numbers as arguments. Use info break or info watch to
print a list of breakpoints, watchpoints, and catchpoints if you do not know which numbers to use.

A breakpoint, watchpoint, or catchpoint can have any of four different 'states' of enablement:

- Enabled. The breakpoint stops your program. A breakpoint set with the break command starts out in
this state. You may abbreviate disable as dis. 
- Disabled. The breakpoint has no effect on your program.
- Enabled once. The breakpoint stops your program, but then becomes disabled.
- Enabled for deletion. The breakpoint stops your program, but immediately after it does so it is
deleted permanently. A breakpoint set with the tbreak command starts out in this state. 

disable [breakpoints] [range...]
Disable the specified breakpoints-or 'all' breakpoints, if none are listed. A disabled breakpoint
has no effect but is not forgotten. All options such as ignore-counts, conditions and commands are
remembered in case the breakpoint is enabled again later. You may abbreviate disable as dis.

enable [breakpoints] [range...]
Enable the specified breakpoints (or 'all' defined breakpoints). They become effective once again in
stopping your program.


={============================================================================
*kt_linux_tool_209* gdb: breakpoints: advanced

{break-command-list}
5.1.7 Breakpoint Command Lists
You can give any breakpoint (or watchpoint or catchpoint) a series of commands to 'execute' when
your program stops due to that breakpoint. For example, you might want to print the values of
certain expressions, or enable other breakpoints.

For example, here is how you could use breakpoint commands to print the value of x at entry to foo
whenever x is positive.

break foo if x>0
commands
silent
printf "x is %d\n",x
cont
end

One application for breakpoint commands is to compensate for one bug so you can test for another.
Put a breakpoint just after the erroneous line of code, give it a condition to detect the case in
which something erroneous has been done, and give it commands to assign correct values to any
variables that need them. End with the continue command so that your program does not stop, and
start with the silent command so that no output is produced. Here is an example:

break 403
commands
silent
set x = y + 4           // note <gdb-set>
cont
end

{dynamic-printf}
The dynamic printf command dprintf combines a breakpoint with formatted printing of your program’s
data to give you the effect of inserting printf calls into your program on-the-fly, without having
to recompile it.

In its most basic form, the output goes to the GDB console. However, you can set the variable
dprintf-style for alternate handling. For instance, you can ask to format the output by calling your
program’s printf function. This has the advantage that the characters go to the program’s output
device, so they can recorded in redirects to files and so forth.

As an example, if you wanted dprintf output to go to a logfile that is a standard I/O stream
assigned to the variable mylog, you could do the following:

(gdb) set dprintf-style call
(gdb) set dprintf-function fprintf
(gdb) set dprintf-channel mylog
(gdb) dprintf 25,"at line 25, glob=%d\n",glob
Dprintf 1 at 0x123456: file main.c, line 25.
(gdb) info break
1 dprintf keep y 0x00123456 in main at main.c:25
call (void) fprintf (mylog,"at line 25, glob=%d\n",glob)
continue
(gdb)

note that the info break displays the dynamic printf commands as normal breakpoint commands; you can
thus easily see the effect of the variable settings.

dprintf location,template,expression[,expression...]

Whenever execution reaches location, print the values of one or more expressions under the control
of the string template. To print several values, separate them with commas.

set dprintf-style style

Set the dprintf output to be handled in one of several different styles enumerated below. A change
of style affects all existing dynamic printfs immediately. (If you need individual control over the
print commands, simply define normal breakpoints with explicitly-supplied command lists.) gdb
Handle the output using the gdb printf command.

'call' Handle the output by calling a function in your program (normally printf).  agent Have the
remote debugging agent (such as gdbserver) handle the output itself.  This style is only available
for agents that support running commands on the target.

set dprintf-function function

Set the function to call if the dprintf style is call. By default its value is printf. You may set
it to any expression. that gdb can evaluate to a function, as per the call command.

set dprintf-channel channel

Set a “channel” for dprintf. If set to a non-empty value, gdb will evaluate it as an expression and
pass the result as a <first-argument> to the dprintf-function, in the manner of fprintf and similar
functions. Otherwise, the dprintf format string will be the first argument, in the manner of printf.

{break-save}
5.1.9 How to save breakpoints to a file

To save breakpoint definitions to a file use the save breakpoints command.  save breakpoints
[filename]


={============================================================================
*kt_linux_tool_210* gdb: examining running

{whatis}
16 Examining the Symbol Table

whatis[/flags] [arg]

Print the data type of arg, which can be either an expression or a name of a data type. With no
argument, print the data type of $, the last value in the value history. If arg is an expression
(see Section 10.1 [Expressions], page 109), it is not actually evaluated, and any side-effecting
operations (such as assignments or function calls) inside it do not take place. 

If arg is a variable or an expression, whatis prints its literal type as it is used in the source
code. If the type was defined using a typedef, whatis will not print the data type underlying the
typedef. If the type of the variable or the expression is a compound data type, such as struct or
class, whatis never prints their fields or methods. It just prints the struct/class name (a.k.a.
    its tag). If you want to see the members of such a compound data type, use ptype.  If arg is a
type name that was defined using typedef, whatis unrolls only one level of that typedef. Unrolling
means that whatis will show the underlying

(gdb) whatis ppkSectionData
type = const uint8_t **


{gdb-print}
(gdb) printf "%d\n", i
40
(gdb) printf "%08X\n", i
00000028

print i
Print the value of variable i.

print *p
Print the contents of memory pointed to by p, where p is a pointer variable.

(gdb) whatis pszSection
type = uint32_t *
(gdb) print *pszSection

print x.field
Check the different members of a structure.

print x
Check all the members of a structure, assuming x is a structure.

print y-field
y is a pointer to a structure.

print array[i]
Print the i'th element of array.

print array
Print all the elements of array.

(gdb) print /x block1->magic
$5 = 0xabeaa5b3

(gdb) print /x block1 
$9 = 0x1150f4c

(gdb) print /x *block1
$8 = {magic = 0xabeaa5b3, size = 0x28, line = 0x0, owner = 0x0, header = 0x21, data = {free = {previous = 0x8, next = 0x115106c}, 
    userData = {0x0}}}

# db_contexts_array is a global var
(gdb) p db_contexts_array
(gdb) p db_contexts_array[-1]


<gdb-disp>
to inspect over the course of the run

(gdb) [disp]lay var
(gdb) [undisp] disp_num
(gdb) info disp 	" to list disps


={============================================================================
*kt_linux_tool_211* gdb: examine sources

<directory>
9.5 Specifying Source Directories

<list>
9 Examining Source Files

list 

Print more lines. If the last lines printed were printed with a list command, this prints lines
following the last lines printed; however, if the last line printed was a solitary line printed as
part of displaying a stack frame (see Chapter 8 [Examining the Stack], page 89), this prints lines
centered around that line.


={============================================================================
*kt_linux_tool_248* gdb: symbols and files

<info-sources>
16 Examining the Symbol Table

info sources

Print the names of all source files in your program for which there is debugging information,
organized into two lists: files whose symbols have already been read, and files whose symbols will
be read when needed.

<file> <core>
You may want to specify executable and core dump file names. The usual way to do this is at start-up
time, using the arguments to gdb’s start-up commands (see Chapter 2 [Getting In and Out of gdb],
page 11). 

Occasionally it is necessary to change to a different file during a gdb session. Or you may run gdb
and forget to specify a file you want to use. Or you are debugging a remote target via gdbserver
(see Section 20.3 [Using the gdbserver Program], page 247). In these situations the gdb commands to
specify new files are useful.

file filename

Use filename as the program to be debugged.

core-file [filename]
core 

Specify the whereabouts of a core dump file to be used as the “contents of memory”. Traditionally,
core files contain only some parts of the address space of the process that generated them; gdb can
access the executable file itself for other parts.  core-file with no argument specifies that no
core file is to be used.


<info-files>
info files
info target

info files and info target are synonymous; both print the current target (see Chapter 19 [Specifying
a Debugging Target], page 241), including the names of the executable and core dump files
currently in use by gdb, and the files from which symbols were loaded. The command help target lists
all possible targets rather than current ones.


<sysroot> <solib-absolute-prefix>
18 gdb Files

Shared libraries are also supported in many cross or remote debugging configurations.  gdb needs to
have access to the target’s libraries; this can be accomplished either by providing copies of the
libraries on the host system, or by asking gdb to automatically retrieve the libraries from the
target. If copies of the target libraries are provided, they need to be the same as the target
libraries, although the copies on the target can be stripped as long as the copies on the host are
not.

<note-this>
For remote debugging, you need to tell gdb where the target libraries are, so that it can load the
correct copies-'otherwise', it may try to load the 'host'’s libraries. gdb has two variables to
specify the search directories for target libraries.

set sysroot 'path'

Use path as the system root for the program being debugged. Any absolute shared library paths will
be prefixed with 'path'; many runtime loaders store the absolute paths to the shared library in the
target program’s memory. If you use set sysroot to find shared libraries, they need to be laid out
in the same way that they are on the target, with e.g. a ‘/lib’ and ‘/usr/lib’ hierarchy under
'path'.

If path starts with the sequence ‘remote:’, gdb will retrieve the target libraries from the remote
system. This is only supported when using a remote target that supports the remote get command (see
Section 20.2 [Sending files to a remote system], page 247). The part of path following the
initial ‘remote:’ (if present) is used as system root prefix on the remote file system.1

The set solib-absolute-prefix command is an alias for set sysroot.

You can set the default system root by using the configure-time ‘--with-sysroot’ option. If the
system root is inside gdb’s configured binary prefix (set with ‘--prefix’ or ‘--exec-prefix’), then
the default system root will be updated automatically if the installed gdb is moved to a new
location.

show sysroot
Display the current shared library prefix.

set solib-search-path path
If this variable is set, path is a colon-separated list of directories to search for shared
libraries. ‘solib-search-path’ is used after ‘sysroot’ fails to locate the library, or if the path
to the library is relative instead of absolute. 

If you want to use ‘solib-search-path’ instead of ‘sysroot’, be sure to set ‘sysroot’ to a
'nonexistent' directory to prevent gdb from finding your host’s libraries. ‘sysroot’ is preferred;
setting it to a nonexistent directory may interfere with automatic loading of shared library
symbols.

show solib-search-path
Display the current shared library search path.


={============================================================================
*kt_linux_tool_249* gdb: shared library debugging

{how-to-debug-shared-lib} from online

HOW TO DEBUG shared library using GDB

[bhushan@Shared_Lib_Debug]$ gcc -fpic -shared -o foo.so foo.c              // note no -g

[bhushan@Shared_Lib_Debug]$ gcc -o main main.c ./foo.so -g

[bhushan@Shared_Lib_Debug]$ gdb main
GNU gdb Red Hat Linux (6.3.0.0-1.21rh)
Copyright 2004 Free Software Foundation, Inc.
GDB is free software, covered by the GNU General Public License, and you are
welcome to change it and/or distribute copies of it under certain conditions.
Type "show copying" to see the conditions.
There is absolutely no warranty for GDB. Type "show warranty" for details.
This GDB was configured as "i386-redhat-linux-gnu"...Using host libthread_db library "/lib/libthread_db.so.1".

(gdb) b foo
Function "foo" not defined.
Make breakpoint pending on future shared library load? (y or [n]) y     // note
Breakpoint 1 (foo) pending.

(gdb) r

Starting program: /home/bhushan/RD/Shared_Lib_Debug/main
Reading symbols from shared object read from target memory...done.
Loaded system supplied DSO at 0x470000
Breakpoint 2 at 0xdb7493

Pending breakpoint "foo" resolved                                       // note
Breakpoint 2, 0x00db7493 in foo () from ./foo.so

(gdb) s

Single stepping until exit from function foo,
which has no line number information.
main () at main.c:7
7 printf("inside main i = %d\n", i);
(gdb) s
inside main i = 4
8 return 0;

NOW if you build the shared libarary using -g option
[bhushan@Shared_Lib_Debug]$ gcc -fpic -shared -o foo.so foo.c -g

[bhushan@Shared_Lib_Debug]$ gcc -o main main.c ./foo.so -g

[bhushan@Shared_Lib_Debug]$ gdb main
GNU gdb Red Hat Linux (6.3.0.0-1.21rh)
Copyright 2004 Free Software Foundation, Inc.
GDB is free software, covered by the GNU General Public License, and you are
welcome to change it and/or distribute copies of it under certain conditions.
Type "show copying" to see the conditions.
There is absolutely no warranty for GDB. Type "show warranty" for details.
This GDB was configured as "i386-redhat-linux-gnu"...Using host libthread_db library "/lib/libthread_db.so.1".


(gdb) b foo

Function "foo" not defined.

Make breakpoint pending on future shared library load? (y or [n]) y

Breakpoint 1 (foo) pending.

(gdb) r

Starting program: /home/bhushan/RD/Shared_Lib_Debug/main
Reading symbols from shared object read from target memory...done.
Loaded system supplied DSO at 0x470000
Breakpoint 2 at 0x1c5493: file foo.c, line 5.
Pending breakpoint "foo" resolved

Breakpoint 2, foo () at foo.c:5
5 return 2*2;

(gdb) s

7 }

(gdb) s

main () at main.c:7
7 printf("inside main i = %d\n", i);

(gdb) s

inside main i = 4

8 return 0;

(gdb)

U can see the differences in bold lines.

<sharedlibrary-command>
http://visualgdb.com/gdbreference/commands/sharedlibrary # GDB Command Reference
sharedlibrary command

Forces GDB to load symbols for the specified shared libraries or all loaded shared libraries.

Syntax
sharedlibrary
sharedlibrary [Library Name]
sharedlibrary [Regular Expression]
share [...]

Parameters

Library Name 
Specifies the library to load debugging symbols for.

Regular Expression 
When specified, GDB will load the symbols for all currently loaded libraries matching the specified expression.

Remarks

Normally, GDB will load the shared library symbols automatically. You can control this behavior
using set auto-solib-add command. <auto-solib-add>

<further-reference>
18 gdb Files

To control the automatic loading of shared library symbols, use the commands: 

set auto-solib-add mode

If mode is on, symbols from all shared object libraries will be loaded automatically when the
inferior begins execution, you attach to an independently started inferior, or when the dynamic
linker informs gdb that a new library has been loaded. If mode is off, symbols must be loaded
manually, using the sharedlibrary command. The 'default' value is on.

If your program uses lots of shared libraries with debug info that takes large amounts of memory,
you can decrease the gdb memory footprint by preventing it from automatically loading the symbols
from shared libraries. 

To that end, type set auto-solib-add off before running the inferior, then load each library whose
debug symbols you do need with sharedlibrary regexp, where regexp is a regular expression that
matches the libraries whose symbols you want to be loaded. To explicitly load shared library
symbols, use the 'sharedlibrary' command:

show auto-solib-add

Display the current autoloading mode.

<info-sharedlibrary>
info share regex
info sharedlibrary regex
Print the names of the shared libraries which are currently loaded that match regex. If regex is
omitted then print all shared libraries that are loaded.

sharedlibrary regex
share regex
Load shared object library symbols for files matching a Unix regular expression.  As with files
loaded automatically, it only loads shared libraries required by your program for a core file or
'after' typing run. If regex is omitted all shared libraries required by your program are loaded.

note. can specify library name which are required by a program but not a filename with a path to
force a loading of it. That means cannot load a libaray until a program needs it.

<continue-article>
However, in some cases (e.g. when debugging with gdbserver and having incompatible symbols or using
old Android toolchains) GDB will not load the symbols automatically. In this case you can use
the info sharedlibrary command to list the loaded shared libraries and the sharedlibrary command to
force the symbols to be loaded. 

If GDB does not automatically load debugging symbols for your library when debugging with gdbserver,
please check the search path using the set solib-search-path command.

Examples

In this example we will disable shared library loading using the set auto-solib-add command, then
run the application, list the source files and load the symbols manually:

(gdb) set auto-solib-add off
(gdb) break main
Breakpoint 1 at 0x80484ed: file main.cpp, line 7.
(gdb) run
Starting program: /home/testuser/libtest/testApp

Breakpoint 1, main () at main.cpp:7
7 printf("In main()\n");

(gdb) info sources      // <info-sources>
Source files for which symbols have been read in:

/home/testuser/libtest/main.cpp

Source files for which symbols will be read in on demand:

(gdb) info sharedlibrary
From To Syms Read Shared Object Library
0xb7fde820 0xb7ff6b9f No /lib/ld-linux.so.2
0xb7fd83a0 0xb7fd84c8 No /home/testuser/libtest/libTest.so
0xb7e30f10 0xb7f655cc No /lib/i386-linux-gnu/libc.so.6

(gdb) sharedlibrary libTest
Reading symbols from /home/testuser/libtest/libTest.so...done.
Loaded symbols for /home/testuser/libtest/libTest.so

(gdb) info sources
Source files for which symbols have been read in:

/home/testuser/libtest/main.cpp

Source files for which symbols will be read in on demand:

/home/testuser/libtest/lib.cpp                  // note. see added source file

(gdb) break lib.cpp:5
Breakpoint 2 at 0xb7fd846e: file lib.cpp, line 5.

(gdb) continue
Continuing.
In main()

Breakpoint 2, func () at lib.cpp:5
5 printf("In func()\n");


{real-examples}
# to see the default setting
(gdb)  show auto-solib-add
Autoloading of shared library symbols is on.

# note that shall load file first before setting a breakpoint
(gdb) file /home/kit/tizen/tv-viewer/tv-viewer
Reading symbols from /home/kit/tizen/tv-viewer/tv-viewer...done.

(gdb) target remote 106.1.11.219:2345
Remote debugging using 106.1.11.219:2345
warning: Unable to find dynamic linker breakpoint function.
GDB will be unable to debug shared library initialisers
and track explicitly loaded dynamic code.
0xb63da7c0 in ?? ()

# to set a breakpoint in the gdbint; otherwise, gdb set no since no input from the user
(gdb) set breakpoint pending on
(gdb)

# note that this is a warning at this moment
(gdb) b main
Cannot access memory at address 0x0
Breakpoint 1 at 0x29842716: file /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1/src/core/AppMain.cpp, line 33.

# no shared library sicne the application do not started yet.
(gdb) i sharedlibrary
No shared libraries loaded at this time.

# note that see warnings on shared library now.
# set solib-search-path before and seems to have only one path as set solib-search-path /home/kit/mheg-port-ug
(gdb) c
Continuing.
warning: `/usr/lib/libicui18n.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicuuc.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicudata.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libfribidi.so.0': Shared library architecture unknown is not compatible with target architecture arm.
warning: `/usr/lib/libicule.so.48': Shared library architecture unknown is not compatible with target architecture arm.
warning: Could not load shared library symbols for 184 libraries, e.g. /usr/lib/libsys-assert.so.
Use the "info sharedlibrary" command to see the complete listing.
Do you need "set solib-search-path" or "set sysroot"?

Breakpoint 1, main (argc=-1237321556, argv=0xb63f4a30)
    at /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1/src/core/AppMain.cpp:33
33           _ERR("%s","Failed to create app!");

(gdb)
Continuing.

# note that even after running, not loaded full libraries. want to debug libug-mhegUG-efl.so
(gdb) i sharedlibrary
From        To          Syms Read   Shared Object Library
                        No          /usr/lib/libsys-assert.so
                        ...
                        No          /usr/lib/libsqlite3.so.0
0xb55efb24  0xb572307c  Yes (*)     /usr/lib/libicui18n.so.48
0xb55154e4  0xb55dfe7c  Yes (*)     /usr/lib/libicuuc.so.48
0xb43e4258  0xb43e4350  Yes (*)     /usr/lib/libicudata.so.48
                        No          /usr/lib/libavoc.so
                        ...
(*): Shared library is missing debugging information.

(gdb) c
Continuing.

(gdb) CTRL-C
Program received signal SIGINT, Interrupt.
0xb3db296c in ?? ()
warning: Could not load shared library symbols for 21 libraries, e.g. /usr/lib/ecore/immodules/libisf-imf-module.so.
Use the "info sharedlibrary" command to see the complete listing.
Do you need "set solib-search-path" or "set sysroot"?

# note that the wanted library is not loaded
(gdb) i sharedlibrary
From        To          Syms Read   Shared Object Library
                        ...
0xab8d96c0  0xab9a2d78  No          /home/kit/mheg-port-ug/libug-mhegUG-efl.so
                        ..
(*): Shared library is missing debugging information.

# HOWEVER, the problem is that the library is loaded but not the symblos and files. can check to see
# if that is the case by running "i sources" to see files.

# note to fource to load again and check with i sources
(gdb) sharedlibrary mheg
Reading symbols from /home/kit/mheg-port-ug/libug-mhegUG-efl.so...done.
Loaded symbols for /home/kit/mheg-port-ug/libug-mhegUG-efl.so
Cannot access memory at address 0xb

# here can see the result of this command:
# set substitute-path /home/abuild/rpmbuild/BUILD/org.tizen.tv-viewer-0.2.1 /home/kit/tizen/tv-viewer
(gdb) i sources
Source files for which symbols have been read in:

/home/kit/tizen/tv-viewer/src/core/AppMain.cpp, /usr/include/c++/4.5.3/new,
...

Source files for which symbols will be read in on demand:
...
/home/kit/mheg-port-ug/mh5eng/mh5e_token.c, /home/kit/mheg-port-ug/mh5eng/mh5b_program.c,

# note that now set a breakpoint
(gdb) b _key_pressed(char const*)
Cannot access memory at address 0xb
Breakpoint 2 at 0xab8d9afa: file /home/abuild/rpmbuild/BUILD/ug-mheg-0.2/main/Main.cpp, line 144.

(gdb) c

# hit breakpoint

(gdb) list

# note that if do not run this command fast enough, it seems that gdb fails to run a session. means
# not hitting a breakpoint.

(gdb) c
Continuing.

Program received signal SIGINT, Interrupt.
0xb3db296c in ?? ()
(gdb) c
Continuing.
Cannot access memory at address 0xb
[New Thread 9633]

Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 9633]
0xb4124ab0 in ?? ()
(gdb) c
Continuing.
^C
^CInterrupted while waiting for the program.
Give up (and stop debugging it)? (y or n) n


={============================================================================
*kt_linux_tool_250* gdb: remote debugging

{cannot-access-memory-warning} "Cannot access memory at address 0×0" warning,
This happens when run gdb client in case use to debug a applicaiton using a shared library. Thought
that gdb is not working but seems not as below. 

<how-to-setup-eclipse>
http://hertaville.com/2013/01/11/remote-debugging/
Neil, Yes the remote path has to exist…I will amend to tutorial & explicitly make note of this. As
for the "Cannot access memory at address 0×0" warning, I also get it (see Fig15). I have no problems
however with stepping and using breakpoints. I do not have any serious speed issues when
simulating a simple program similar to the one in the Tutorial. I also had no problems when
debugging the GPIO examples posted on this site. Having said that I didn’t simulate a program
that calls external libraries such as libjpeg. Perhaps that is why debugging is slow in your
case ?

{gdb-term}
GDB represents the state of each program execution with an object called an 'inferior'. 

{gdb-server}
To use a TCP connection, you could say:

target> gdbserver host:2345 emacs foo.txt

This says pretty much the same thing as the last example, except that we are going to communicate
with the host GDB via TCP. The host:2345 argument means that we are expecting to see a TCP
connection from host to local TCP port 2345. (note Currently, the host part is ignored.) You can
choose any number you want for the port number as long as it does not conflict with any existing TCP
ports on the target system. This same port number must be used in the host GDBs target remote
command, which will be described shortly. Note that if you chose a port number that conflicts with
another service, gdbserver will print an error message and exit. 

# does make a difference?
sudo gdbserver 10.42.0.1:12345 HelloRPiWorld 

<arguments>
20.3.1.4 Other Command-Line Arguments for gdbserver

--debug

Instruct gdbserver to display extra status information about the debugging process. This option is
intended for gdbserver development and for bug reports to the developers.

--remote-debug

Instruct gdbserver to display remote protocol debug output. This option is intended for gdbserver
development and for bug reports to the developers.

<symbols>
First make sure you have the necessary symbol files. Load symbols for your application using the
file command before you connect. Use set sysroot to locate target libraries (unless your GDB was
compiled with the correct sysroot using --with-sysroot). Q: <sysroot>?

The symbol file and target libraries must exactly match the executable and libraries on the target
with one exception: the files on the host system should not be stripped, even if the files on the
target system are. Mismatched or missing files will lead to confusing results during debugging. On
GNU/Linux targets, mismatched or missing files may also prevent gdbserver from debugging
multi-threaded programs. 


{gdb-client}
target remote host:port

monitor cmd

This command allows you to send arbitrary commands directly to the remote monitor. Since GDB doesn’t
care about the commands it sends like this, this command is the way to extend GDB—you can add new
commands that only the external monitor will understand and implement. 
	

20.3.3 Monitor Commands for gdbserver

During a GDB session using gdbserver, you can use the monitor command to send special requests to
gdbserver. Here are the available commands.

monitor help
List the available monitor commands.

monitor set debug 0
monitor set debug 1
Disable or enable general debugging messages.

monitor set remote-debug 0
monitor set remote-debug 1
Disable or enable specific debugging messages associated with the remote protocol (see Remote
Protocol).  

monitor set debug-format option1[,option2,...]

Specify additional text to add to debugging messages. Possible options are:

none Turn off all extra information in debugging output. 

all Turn on all extra information in debugging output. 

timestamps Include a timestamp in each line of debugging output. 

Options are processed in order. Thus, for example, if none appears last then no additional
information is added to debugging output. 

monitor exit

Tell gdbserver to exit immediately. This command should be followed by disconnect to close the
debugging session. gdbserver will detach from any attached processes and kill any processes it
created. Use monitor exit to terminate gdbserver at the end of a multi-process mode debug session.


={============================================================================
*kt_linux_tool_251* gdb: local debugging

={============================================================================
*kt_linux_tool_300* gdb: frontend tool: cgdb

It is how to change cgdb to use cross tool gdb.

http://www.programdevelop.com/4527764/

The GDB code CGDB calls in the path: /VARIOUS/util/src/fork_util.c by function invoke_debugger in

int invoke_debugger( 
            const char *path,  
            int argc, char *argv[],  
            int *in, int *out,  
            int choice, char *filename)  
{ 
    pid_t pid;     
    //GDBGDB?arm-linux-gdb 
    const char * const GDB               = "arm-linux-gdb"; 
}

./configure --prefix=/usr/local/ --program-suffix=arm-linux
make
sudo make install

kit@kit-vb:~/mheg-port$ ls /usr/local/bin/cgdb*
/usr/local/bin/cgdb  /usr/local/bin/cgdbarm-linux
kit@kit-vb:~/mheg-port$

http://cgdb.github.io/


{config}
/.cgdb/cgdbrc
==
:set winspilt=top_big
:set arrowstyle=long
:map <F6> :continue<CR>
:map <F7> :finish<CR>
:map <F8> :step<CR>
:map <F9> :next<CR>
==


# ============================================================================
#{
={============================================================================
*kt_linux_core_001*  check shared libraries that process uses

cat /proc/NNNN/maps | awk '{print $6}' | grep '\.so' | sort | uniq

# ============================================================================
#{
={============================================================================
*kt_linux_core_100* thread vs process

A process is an instance of an executing program and a thread is an instance of an executing a task.
In other words, a process is processor abstract, an abstract entity defined by kernel and with
allocated resources in order to execute a program.  

UNIX programs have a single thread of execution: the CPU processes instructions for a single logical
flow of execution through the program. In a multithreaded program, there are multiple, independent,
concurrent logical flows of execution within the same process.

<key> In short, a process is an abstract(virtualisation) of a processor and a thread is an abstract
of single flow of execution.

<on-linux>
Two virtualisation in Linux: processer and memory.

Linux don't distinguish a process(task) with a thread. The thread is just a special process: 

1. Linux kernel scheduler schedules based on 'thread'.
2. Linux kernel has a double-linked list which has <thread_info> struct element which has
thread_struct*.
3. Thread is special since it shares resource (open files, pending signals, internal kernel data,
process state, address space, text and data section) with others threads.

In other words, process is a single thread that don't share resources.

This is clear when see how to create a thread:

clone( CLONE_VM| CLONE_FS| CLONE_FILES| CLONE_SIGHAND, 0);

to create a process:

clone(SIGCHLD, 0);

Two usual steps to create a process:

fork();  // copy a child from a parent and actually use clone() call
exec*()  // load a new program text.


{shared-and-not-shared-between-threads}
From CH29 in {ref-LPI} and {ref-UNP}

<shared>
The attributes that are shared; in other words, global attributes to a process:

the same global memory, process code and most data;
process ID and parent process ID;
process group ID and session ID;
process credentials (user and group IDs);

open file descriptors;
signal dispositions;
file system-related information: umask, current working directory, and root directory;
resource limits;

<not-shared>
The attributes for each thread:

thread ID (Section 29.5);
signal mask;
thread-specific data (Section 31.3);
alternate signal stack (sigaltstack());
the errno variable;
realtime 'scheduling' policy and priority (Sections 35.2 and 35.3);
capabilities (Linux-specific, described in Chapter 39); and
'stack' (local variables and function call linkage information).
'registers' including PC and SP


{multithreaded-vs-singlethreaded}
The mutiltithreaded means that a single process has multi threads, 'lightweight'-process. The
singlethreaed means that a single process has a single thread. MT has less IPC but prone to error
because shares resources; less protection. ST has more IPC but more protection. 

note: tradeoff between IPC and protection.


{why-thread}
From {ref-LPI}. Problems with fork:

1) The fork call is expensive because memory is copied from the parent to the child. Although
<copy-on-write> is used to avoid this, but still expensive.

2) Need IPC after fork between parent and child.

Thread help with these problems as it is 'lightweight'-process in terms of 'creation' cost:

Thread creation is faster because many of the attributes that must be duplicated in a child created
by fork() are instead shared between threads.

Sharing information between threads is easy and fast. It is just a matter of copying data into
shared (global or heap) variables.

However, has disadvantages:

1) more efforts to ensure thread-safe
2) buggy thread can damage all of the threads in the process. less protection.
3) each thread is competing for use of the finite virtual address space of a host process.
4) usually desirable to avoid the use of signals in multi-threaded programs since requires careful
designs.
5) should run the same program.
6) more threads, more memory and context switching.


{process}
<process-id>
From {ref-LPI} PID is integer type and the Linux limits PIDs to 32767. Once it has reached 32,767, 
the process ID counter is reset to 300, rather than 1. This is done because many low-numbered
process IDs are in permanent use by system processes and daemons, and thus time would be wasted
searching for an unused process ID in this range. 

In Linux 2.4 and earlier, the process ID limit of 32,767 is defined by the kernel constant PID_MAX.
With Linux 2.6, things change. While the default upper limit for process IDs remains 32,767, this
limit is adjustable via the value in the Linux-specific /proc/sys/kernel/pid_max file (which is one
greater than the maximum process ID). On 32-bit platforms, the maximum value for this file is
32,768, but on 64-bit platforms, it can be adjusted to any value up to 222 (approximately 4
million), making it possible to accommodate very large numbers of processes.


{memory-layout} 6 from LPI
This is a layout in virtual memory.

<text-segment> 
The text segment contains the machine-language instructions of the program run by the process. The
text segment is made 'read'-only so that a process doesn't accidentally modify its own instructions
via a bad pointer value. Since many processes may be running the same program, the text segment is
made 'sharable' so that a single copy of the program code can be mapped into the virtual address space
of all of the processes.

<initialized-data-segment>
The initialized data segment contains 'global' and 'static' variables that are explicitly
initialized.  The values of these variables are read from the executable file when the program is
loaded into memory.

<uninitialized-data-segment> <bss>
The uninitialized data segment contains 'global' and 'static' variables that are 'not' explicitly
initialized. Before starting the program, the system initializes all memory in this segment to 0.
For historical reasons, this is often called the bss segment, a name derived from an old assembler
mnemonic for “block started by symbol.” The main reason for placing global and static variables that
are initialized into a separate segment from those that are uninitialized is that, when a program is
stored on disk, it is not necessary to allocate space for the uninitialized data. Instead, the
executable merely needs to record the location and size required for the uninitialized data segment,
and this space is allocated by the program loader at run time.

<stack>
The stack is a dynamically growing and shrinking segment containing stack frames. One stack frame is
allocated for each currently called function. A frame stores the function's local variables
(so-called automatic variables), arguments, and return value. Stack frames are discussed in more
detail in Section 6.5.

<heap>
The heap is an area from which memory (for variables) can be dynamically allocated at run time. The
top end of the heap is called the program break.

<process-virtual-memory-address>

High           +----------------+
                  kernel. mapped into process virtual memory but not accessible to program.
               +----------------+
                  argv, environ
               +----------------+
                  stack (grows down)
top of stack   +----------------+
                  unallocated
               +----------------+
                  heap (grows up)
               +----------------+ < &end
                  bss
               +----------------+ < &edata
                  inited data
               +----------------+ < &etext
                  text
               +----------------+

Low            +----------------+

note: what's the kernel in this digram?


{virtual-memory} locality-of-reference
The aim of this virtual memory is to make efficient use of both the CPU and RAM (physical memory) by
exploiting a property that is typical of most programs: locality of reference. Most programs
demonstrate two kinds of locality:

1. 'spatial' locality is the tendency of a program to reference memory addresses that are 'near' those
that were recently accessed (because of 'sequential' processing of instructions, and, sometimes,
sequential processing of data structures).

2. 'temporal' locality is the tendency of a program to access the 'same' memory addresses in the near
future that it accessed in the recent past (because of loops).

The upshot of locality of reference is that it is possible to execute a program while maintaining
only 'part' of its address space in RAM.

<paging>
A virtual memory scheme splits the memory used by each program into small, fixed-size units called
pages. Correspondingly, RAM is divided into a series of page frames of the same size. At any one
time, only some of the pages of a program need to be resident in physical memory page frames; these
pages form the so-called resident set. Copies of the unused pages of a program are maintained in the
'swap' area-a reserved area of disk space used to supplement the computer's RAM-and loaded into
physical memory only as required. When a process references a page that is 'not' currently
'resident' in physical memory, a page 'fault' occurs, at which point the kernel suspends execution of
the process while the page is loaded from disk into memory.

The kernel maintains a page table for 'each'-process (Figure 6-2). The page table describes the
location of each page in the process's virtual address space (the set of all virtual memory pages
available to the process). 

process virtual address space    page table        physical memory(RAM) page frames 
   page 0                           4                    0
   page 1                           2                    1
   page 2                           7                    2
   page 3                           0                    3
\|/
increasing virtual address

Each entry in the page table either 'indicates' the location of a virtual page in RAM or indicates
that it currently resides on disk.

Not all address ranges in the process's virtual address space require page-table entries. Typically,
large ranges of the potential virtual address space are unused, so that it isn't necessary to
maintain corresponding page-table entries. If a process tries to access an address for which
there is 'no' corresponding page-table entry, it receives a SIGSEGV signal.


{user-and-kernel-stack}
The kernel stack is a per-process memory region maintained in kernel memory that is used as the
stack for execution of the functions called internally during the execution of a system call.

Each (user) stack frame contains the following information:

1. Function arguments and local variables
2. Some registers. RA.

note: needs more about difference?


{process-creation}
The wait(&status) system call has two purposes. First, if a child of this process has not yet
terminated by calling exit(), then wait() suspends execution of the process until one of its
children has terminated. Second, the termination status of the child is returned in the status
argument of wait().

Parent process running program A
      |
      A
      |
      child PID = fork(void);                   Child process running program A. 0 = fork(void);
      |                                               |
Parent may perform other atcions here                 A  
      |                                               | 
      wait(&status); // optional                Child may perform further actions here
                                                      |
                                                      execev( B, ... ); // optional
                                                      |
                                                      B
                                                      |
                                                Execution of program B
                                                      |
                                                      exit(status);
                                                      1> Child status passed to parent and kernel
                                                      restarts parent.
                                                      2> Delivers SIGCHLD optionally.

The following idiom is sometimes employed when calling fork():

pid_t childPid; /* Used in parent after successful fork() to record PID of child */

switch (childPid = fork()) {
  case -1: /* fork() failed */
    /* Handle error */
  case 0: /* Child of successful fork() comes here */
    /* Perform actions specific to child */
  default: /* Parent comes here after successful fork() */
    /* Perform actions specific to parent */
}

<sharing-between-parent-and-child>
The child's stack, data, and heap segments are initially exact duplicates of the corresponding parts
the parent's memory. The child receives duplicates of all of the parent’s file descriptors.

<race-condition-after-fork>
After a fork(), it is indeterminate which process-the parent or the child—next has access to the
CPU. If we need to guarantee a particular order, we must use some kind of synchronization technique.

<case-use-signal>
Avoiding Race Conditions by Synchronizing with Signals. Although, in practice, such coordination is
more likely to be done using semaphores, file locks, or message passing. See *kt_linux_core_401* for
code example.


{monitoring-child-process}
From #26 in ref-LPI. 
<wait-system-call>
The wait() system call waits for one of the children of the calling process to terminate and returns
the termination status of that child in the buffer pointed to by status.

#include <sys/wait.h>
pid_t wait(int *status);

If no (previously unwaited-for) child of the calling process has yet terminated, the call blocks
until one of the children terminates. If a child has already terminated by the time of the call,
wait() returns immediately.

As its function result, wait() returns the process ID of the child that has terminated.

On error, wait() returns -1. One possible error is that the calling process has no previously
unwaited-for children, which is indicated by the errno value ECHILD. This means that we can use the
following loop to wait for all children of the calling process to terminate:

while ((childPid = wait(NULL)) != -1)
  continue;
if (errno != ECHILD) /* An unexpected error... */
  errExit("wait");

<waitpid-system-call>
The wait() system call has a number of limitations, which waitpid() was designed to address. See the
reference for more details.

#include <sys/wait.h>
pid_t waitpid(pid_t pid, int *status, int options);


{orphan-process} {process-init} {zombie-process}
2014.02 from google phone interview. The lifetimes of parent and child processes are usually not the
same-either the parent outlives the child or vice versa. This raises two questions: 

1. Who becomes the parent of an orphaned child? Each process has a parent-the process that created
it. If a child process becomes orphaned because its "birth" parent terminates, then the child is
adopted by the 'init' process, and subsequent calls to getppid() in the child return 1. See Section
26.2. The process 1, init, the ancestor of all processes. The init process adopts the child and
automatically performs a wait(), thus removing the zombie process from the system.

note: first case when zombie is created
2. What happens to a child that terminates before its parent has had a chance to perform a wait()?
The point here is that, although the child has finished its work, the parent should still be
permitted to perform a wait() at some later time to determine how the child terminated. The kernel
deals with this situation by turning the child into a <zombie>. This means that most of the
resources held by the child are released back to the system to be reused by other processes. The
only part of the process that remains is an entry in the kernel's process table recording; among
other things the child's process ID, termination status, and resource usage statistics. Section
36.1.

A zombie process can't be killed by a signal, not even the (silver bullet) SIGKILL. This ensures
that the parent can always eventually perform a wait(). When the parent does perform a wait(), the
kernel removes the zombie, since the last remaining information about the child is no longer
required.

<why-zombie-can-be-a-problem> note: second case when zombie is created
If a parent creates a child, but fails to perform a wait(), then an entry for the zombie child will
be maintained indefinitely in the kernel's process table. If a large number of such zombie children
are created, they will eventually fill the kernel process table, 'preventing' the creation of new
processes. Since the zombies can’t be killed by a signal, the only way to remove them from the
system is to kill their parent (or wait for it to exit), at which time the zombies are adopted and
waited on by init, and consequently removed from the system.

$ ./make_zombie
Parent PID=1013
Child (PID=1014) exiting
  1013 pts/4 00:00:00 make_zombie Output from ps(1)
  1014 pts/4 00:00:00 make_zombie <defunct>
After sending SIGKILL to make_zombie (PID=1014):
  1013 pts/4 00:00:00 make_zombie Output from ps(1)
  1014 pts/4 00:00:00 make_zombie <defunct>

In the above output, we see that ps(1) displays the string <defunct> to indicate a process in the
zombie state.

A common way of reaping dead child processes is to establish a handler for the SIGCHLD signal.

<key> The zombie is created in two cases: child terminates early before parent calls wait and parent
fail to wait for child. The second case is a real problem. The problem is that kernel's process
table fills up and prevent creatring new processes. The zombies will be adopted by init process
eventually and can be removed by killing init process.


={============================================================================
*kt_linux_core_101* pthread

POSIX.1 threads approved in 1995 is a POSIX standard for threads. The standard defines an API for
creating and manipulating threads. So there is one posix standard but there are two implementation
like NPTL.


{pthread-apis}
http://pubs.opengroup.org/onlinepubs/7990989799/xsh/pthread.h.html
http://pubs.opengroup.org/onlinepubs/7908799/xsh/pthread.h.html

Linux threading models compared: LinuxThreads and NPTL
http://www-128.ibm.com/developerworks/linux/library/l-threading.html?ca=dgr-lnxw07LinuxThreadsAndNPTL

From 2.6, NPTL(New Posix Threading Library) that supports futex(fast user space mutex) and is part
of glibc.


{source}
# from uclibc source

uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\pthread.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\rwlock.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\pthread.h
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\bits\pthreadtypes.h
(_pthread_rwlock_t)


{pthread-errno}
The traditional method of returning status from system calls and some library functions is to return
0 on success and -1 on error, with errno being set to indicate the error. The functions in the
Pthreads API do things differently. All Pthreads functions return 0 on success or a positive value
on failure. The failure value is one of the same values that can be placed in errno by traditional
UNIX system calls. Two approaches:

<one>
From {ref-UNP}. Why is this? Because pthread funcs do not set <errno-var> and return errno instead.
This means that set manually errno before calling err_sys() for example. This util funcs handles
this:

To avoid cluttering the code with braces, use {comma-operator} to combine assignment and the call:

int n;
if(( n = pthread_mutex_lock( &ndone_mutex )) != 0 )
   errno = n, err_sys("pthread_mutex_lock error");

Or

void Pthread_mutex_lock(pthread_mutex_t* mptr)
{
  int n;

  if(( n = pthread_mutex_lock( mptr )) == 0 )
    return;

  errno = n;
  err_sys("pthread_mutex_lock error");
} 

From Appdix C in {ref-UNP}. The reason for our own error funcs is to handle error case with a single
line. See {pthread-util-func} for the use.

if( error condition )
   err_sys( printf format with any number of args );

instead of

if( error condition ) {
  char buff[200];
  snprintf( buff, sizeof(buff), printf format with any number of args );
  perror(buff);
  exit(1);
}

<two>
Because each reference to errno in a threaded program carries the overhead of a function call, our
example programs don't directly assign the return value of a Pthreads function to errno. note: KT so
no need to care about errno in pthread.

From CH03 in {ref-LPI}

(lib/tlpi_hdr.h)

#ifndef TLPI_HDR_H
#define TLPI_HDR_H /* Prevent accidental double inclusion */
#include <sys/types.h> /* Type definitions used by many programs */
#include <stdio.h> /* Standard I/O functions */
#include <stdlib.h> /* Prototypes of commonly used library functions,
plus EXIT_SUCCESS and EXIT_FAILURE constants */
#include <unistd.h> /* Prototypes for many system calls */
#include <errno.h> /* Declares errno and defines error constants */
#include <string.h> /* Commonly used string-handling functions */
#include "get_num.h" /* Declares our functions for handling numeric
arguments (getInt(), getLong()) */
#include "error_functions.h" /* Declares our error-handling functions */
typedef enum { FALSE, TRUE } Boolean;
#define min(m,n) ((m) < (n) ? (m) : (n))
#define max(m,n) ((m) > (n) ? (m) : (n))
#endif

TODO: need to summarize:


{pthread.h}
#include <pthread.h>

// return 0 if okay, positive Exxx on error which is different from most system calls. 
//
// If need multiple arguments to the function, must package them into a structure and then pass the
// address of this as the single argument to the start function.
//
// The tid argument points to a buffer of type pthread_t into which the unique identifier for
// this thread is copied before pthread_create() returns. This identifier can be used in later
// Pthreads calls to refer to the thread. The arg should be in global or heap.
//
// EAGAIN : when cannot create a new thread because exceeded the limit on the number of threads
//
int pthread_create( pthread_t *tid, const pthread_attr_t *attr, void *(*func)(void*), void *arg);

<join> <pthread-error-use>
// return 0 if okay, positive Exxx on error which is different from most system calls. See
// {errno-use}
//
// To wait for a given thread to terminate but no way to wait for any other threads. If that thread
// has already terminated, pthread_join() returns immediately.
//
// If status is not null, the return value from the thread(a pointer to some object) is stored.
//
// <circumbent-join-restriction>
// We noted earlier that pthread_join() can be used to join with only a specific thread. It
// provides no mechanism for joining with any terminated thread. We now show how a condition
// variable can be used to circumvent this restriction. The ref-LPI 30.2.4 shows code.

int pthread_join( pthread_t tid, void **status);


/* The execution of a thread terminates in one of the following ways: 
 *
 * o The thread's start function performs a return specifying a return value for the thread. This is
 * equivalent to pthread_exit()  
 * 
 * o The thread calls pthread_exit() which can be called in any func called by start func.
 *
 * o The thread is canceled using pthread_cancel()
 *
 * o note: Any of the threads calls exit(), or the main thread performs a return (in the main() function),
 * which causes 'all' threads in the process to terminate immediately.
 * 
 * {return-value}
 *
 * The retval argument specifies the return value for the thread. The value pointed to by retval
 * should not be located on the thread's stack, since the contents of that stack become undefined on
 * thread termination. (For example, that region of the process's virtual memory might be
 * immediately reused by the stack for a new thread.) The same statement applies to the value given
 * to a return statement in the thread's start function.
 *
 * {main-thread-exit}
 *
 * note: If the main thread calls pthread_exit() instead of calling exit() or performing a return, then
 * the other threads continue to execute.
 */
void pthread_exit(void *retval);


// {threadid}
// return tid of calling thread. note: pthread_t is structure and need more to print out and also
// implementation dependant. In NPTL, pthread_t is a pointer so treat it as opaque data and hence
// need pthread_equal(). 
//
// if (pthread_equal(tid, pthread_self()) 
//    printf("tid matches self\n");
//
// In 29.5 of {ref-LPI}, POSIX thread IDs are not the same as the thread IDs returned by the Linux
// specific gettid() system call. POSIX thread IDs are assigned and maintained by the threading
// implementation. The thread ID returned by gettid() is a number (similar to a process ID) that is
// assigned by the kernel.  Although each POSIX thread has a unique kernel thread ID in the Linux
// NPTL threading implementation, an application generally doesn't need to know about the kernel IDs
// (and won't be portable if it depends on knowing them).
//
//  return 0 if they are equal
//
int pthread_equeal( pthread_t tid, pthread_t tid );
pthread_t pthread_self(void);


/*  Thread is either 'joinable'(the default) or 'detached'. The detached thread is like a daemon
 *  process: when it terminates, all its resources are released, and cannot wait for it to
 *  terminate. This means cannot get return state. {Q} does it mean joinable thread possibly leaks
 *  resources when not joined? If a joinable thread termintes without pthread_join, it became a
 *  'zombie' process. Leak resources and may not able to create additional threads. 
 *
 *  {ref-UNP} Detaching a thread doesn't make it immune to a call to exit() in another thread or a
 *  return in the main thread. In such an event, all threads in the process are immediately
 *  terminated, regardless of whether they are joinable or detached. To put things another way,
 *  pthread_detach() simply controls what happens after a thread terminates, not how or when it
 *  terminates.
 *
 *  To make the specified thread detached and often used to detach itself. Can create detached
 *  thread when create it by using attr setting.
 *
 *  pthread_detach( pthread_self() );
 */
int pthread_detach(pthread_t tid);


/*  status must not point to an object that is local to the calling thread.
 *
 *  terminate two other ways:
 *
 *  o thread function returns. return value is the exit status of the thread. {Q} kernal handle and
 *  manage it?
 *
 *  o main thread function returns or any thread call exit/_exit, the process terminates
 *  immediately.
 */
void pthread_exit(void *status);

// A thread may be canceled by any other thread in the same process. For example, if multiple
// threads are started to work on a given task (say finding a record in a database) adn the first
// thread completes the task then cancels the other threds.
//
// To handle the possibility of being canceled, can install(push) and remove(pop) cleanup handlers.
// These handlers are called:
// a) when the thread is canceled by pthread_cancel
// b) when the thread voluntarily terminates (either by calling pthread_exit or returning from its
// thread)

int   pthread_cancel(pthread_t);
void  pthread_cleanup_push(void*), void *);
void  pthread_cleanup_pop(int);


{pthread-attribute} 
To override the default and normally take the detault using the attr arg as a NULL. Attributes are a
way to specify behavior that is different from the default. When a thread is created with
pthread_create or when a synchronization variable is initialized, an attribute object can be
specified. However the default atributes are usually sufficient for most applications. 

Note: Attributes are specified [only] at thread creation time; they cannot be altered while the thread
is being used. {Q} really?

Thus three functions are usually called in tandem when setting attribute

o Thread attibute intialisation 
pthread_attr_init() create a default pthread_attr_t tattr. The function pthread_attr_init() is used
to initialize object attributes to their default values. The storage is allocated by the thread
system during execution. note: once the thread has been creted, the attribute object is no longer
needed, and so is destoryed.

o Thread attribute value change (unless defaults appropriate) 
A variety of pthread_attr_*() functions are available to set individual attribute values for the
pthread_attr_t tattr structure. (see below).  

o Thread creation 
A call to pthread_create() with approriate attribute values set in a pthread_attr_t structure. 
 
<code>

The following code fragment should make this point clearer: 

#include <pthread.h> 

pthread_attr_t tattr; // note: can be a local var
pthread_t tid;
void *start_routine;
void arg
int ret;

ret = pthread_attr_init(&tattr);
ret = pthread_attr_*(&tattr,SOME_ATRIBUTE_VALUE_PARAMETER);
ret = pthread_create(&tid, &tattr, start_routine, arg);
ret = pthread_attr_destroy(&tattr);

In order to save space, code examples mainly focus on the attribute setting functions and the
intializing and creation functions are ommitted. These must of course be present in all actual code
fragtments. 

An attribute object is opaque, and cannot be directly modified by assignments. A set of functions is
provided to initialize, configure, and destroy each object type. Once an attribute is initialized
and configured, it has process-wide scope. The suggested method for using attributes is to configure
all required state specifications at one time in the early stages of program execution. The
appropriate attribute object can then be referred to as needed. Using attribute objects has two
primary advantages: 

First, it adds to code portability. Even though supported attributes might vary between
implementations, you need not modify function calls that create thread entities because the
attribute object is hidden from the interface. If the target port supports attributes that are not
found in the current port, provision must be made to manage the new attributes. This is an easy
porting task though, because attribute objects need only be initialized once in a well-defined
location. 

Second, state specification in an application is simplified. As an example, consider that several
sets of threads might exist within a process, each providing a separate service, and each with its
own state requirements. At some point in the early stages of the application, a thread attribute
object can be initialized for each set. All future thread creations will then refer to the attribute
object initialized for that type of thread. The initialization phase is simple and localized, and
any future modifications can be made quickly and reliably. Attribute objects require attention at
process exit time. When the object is initialized, memory is allocated for it. This memory must be
returned to the system. The pthreads standard provides function calls to destroy attribute objects. 


{pthread-example-in-FOSH}
/* Assert throughout that the POSIX calls worked. If not, HFL cannot be guaranteed to work. */
resPOSIX = pthread_attr_init(&threadAttrs);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* The stacksize attribute defines the minimum stack size (in bytes) allocated for the created
 * threads stack.
 *
 * int pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize);
 */
resPOSIX = pthread_attr_setstack(&threadAttrs, ptThreadInfo->pvStack,(size_t)ptThreadInfo->szStack);
/* If this assert is triggered, it might be because of not aligning address to a boundary of 8. */
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * To set the other scheduling policy: 
 */
resPOSIX = pthread_attr_setschedpolicy(&threadAttrs, SCHED_RR);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * The example to change prio:
 *
 * sched_param param;
 * param.sched_priority = 30;
 * ret = pthread_attr_setschedparam (&tattr, &param);
 * 
 * used to set/inquire a current thread's priority of scheduling.
 * 
 * int pthread_attr_setschedparam(pthread_attr_t *attr, const struct sched_param *param);
 * int pthread_attr_getschedparam(const pthread_attr_t *attr, struct sched_param *param);
 *
 * {Q} {pthread-schedule-at-runtime} is it possible as attr is only at creation?
 */
resPOSIX = pthread_attr_setschedparam(&threadAttrs, &tSchedParam);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);

resPOSIX = pthread_attr_setscope(&threadAttrs, ContentionScope);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* The pthread_attr_setinheritsched() function sets the scheduling parameter inheritance state
 * attribute in the specified attribute object. The thread's scheduling parameter inheritance state
 * determines whether scheduling parameters are explicitly specified in this attribute object, or if
 * scheduling attributes should be inherited from the creating thread. Valid settings for
 * inheritsched include:
 * 
 * PTHREAD_INHERIT_SCHED Scheduling parameters for the newly created thread are the same as those of
 * the creating thread.
 * 
 * PTHREAD_EXPLICIT_SCHED Scheduling parameters for the newly created thread are specified in the
 * thread attribute object.
 */
resPOSIX = pthread_attr_setinheritsched(&threadAttrs, PTHREAD_EXPLICIT_SCHED);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);

resPOSIX = pthread_create(&(ptThreadInfo->idThread), &threadAttrs, pfThreadMain, pvParam);


{pthread-stack} from CH33 in {ref-LPI}.
Each thread has its own stack whose size is fixed when the thread is created. On Linux/x86-32, for
all threads other than the main thread, the default size of the per-thread stack is 2 MB. The main
thread has a much larger space for stack growth. Occasionally, it is useful to change the size of a
thread's stack. 

The pthread_attr_setstacksize() function sets a thread attribute (Section 29.8) that determines the
size of the stack in threads created using the thread attributes object. The related
pthread_attr_setstack() function can be used to control both the size and the location of the stack,
but setting the location of a stack can decrease application portability.

One reason to change the size of per-thread stacks is to allow for larger stacks for threads that
allocate large automatic variables or make nested function calls of great depth (perhaps because of
recursion).

Alternatively, an application may want to reduce the size of per-thread stacks to allow for a
greater number of threads within a process.

The minimum stack that can be employed on a particular architecture can be determined by calling
sysconf(_SC_THREAD_STACK_MIN). For the NPTL implementation on Linux/x86-32, this call returns the
value 16,384.


{nptl}
Linux threading models compared: LinuxThreads and NPTL
http://www-128.ibm.com/developerworks/linux/library/l-threading.html?ca=dgr-lnxw07LinuxThreadsAndNPTL

NPTL(new posix threading library) comes from kernel 2.6 and supports Futex(fast user space mutex).
It is part of glibc.

<how-to-check-nptl-version>
$ getconf GNU_LIBPTHREAD_VERSION
NPTL 2.15

<pid-nptl>
note: Q. In Linux, every thread has a PID and can see when use ps command but in NPTL, thread
group has one PID. is it true?


{pthread-compile-link}
It is on GCC 4.6.3 and not -lpthread. This is said when run 'man' on pthread funcs. On Linux,
option. The effects of this option include the following:

o The _REENTRANT preprocessor macro is defined. This causes the declarations of a few reentrant
functions to be exposed.

o The program is linked with the libpthread library (the equivalent of -lpthread).

gcc -pthread sample.c


={============================================================================
*kt_linux_core_102* how to run three threads sequencially

From Cracking the coding interview, p425, 16.5:

Suppose we have following code:

public class Foo {
  public Foo() {...}
  public void first() {...}
  public void second() {...}
  public void third() {...}
}

The same instance of Foo will be passed to three different threads. ThreadA will call first, ThreadB
will call second, and ThreadC will call third. Design a mechanism to ensure that first is called
before second and second is called before third.

Using lock?

public class FooBad {
  public FooBad() {
    lock1 = new ReentrantLock();
    lock2 = new ReentrantLock();

    // locks all in a ctor
    lock1.lock(); lock2.lock(); 
  }
}

// already got lock1
public void first()
{
  ...
  lock1.unlock(); // finished first
}

// already got lock2
public void second()
{
  lock1.lock();   // wait first to finish
  lock1.unlock();
  ...
  lock2.unlock(); // finished second
}

public void third()
{
  lock2.lock();   // wait second to finish
  lock2.unlock();
  ...
}

This WON'T work in JAVA since a lock in JAVA is owned by the same thread which locked in.

http://docs.oracle.com/javase/6/docs/api/java/util/concurrent/locks/ReentrantLock.html
A ReentrantLock is owned by the thread last successfully locking, but not yet unlocking it. A thread
invoking lock will return, successfully acquiring the lock, when the lock is not owned by another
thread. The method will return immediately if the current thread already owns the lock. 

note: There's no such limitation in Linux and it's possible in the same thread group and if threads
in different group use lock, can use semaphore or lock on the shared memory.

The mutex has ownership as well and see {mutex-ownership}.


={============================================================================
*kt_linux_core_103* priority and schedule 

{priority-on-linux}

high           low
0     99 100   139

0  -99  : realtime priority. static and can set when create a process
100-139 : user priority. can use nice command which uses with value from -20 to +19. The default
nice value is 0. note: does it mean 120 is default? 100 is highest?


{realtime-and-latency}
The aim to distribute fairly CPU resource to all process on a system is not realtime approach.

<latency-components>

interrupt      ISR         ISR signals       user process
event          runs        user process      runs
 |             |              |                 |
---------------------------------------------------------> time
   interrupt      interrupt      scheduling
   latency        processing     latency

<no-hard-realtime>
No support in kernel. To use hard realtime, install patch from 
http://people.redhat.com/~mingo/realtime-preempt


{schedule}
Before Linux 2, kernel didn't support preemption which means that no other process can run in kernel
mode when one user process is already in the kernel mode until that is bloked or finishes its work.

<scheduler>
O(1) scheduler from Linux 2.5 and supports constant scheduling decision regardless of the number of
process.

<schedule-policy>
(from ~/include/linux/sched.h)
/*
 * Scheduling policies
 */

// normal user process. fairness
#define SCHED_NORMAL 0     

// realtime and run the highest priority. On the same priority, the first runs until it's blocked.
// So it is realtime without time slice.
#define SCHED_FIFO   1     

// realtime and run the highest priority. On the same priority, the first runs but in the time
// sliced. So it is realtime with time slice.
#define SCHED_RR     2

<default-policy>
SCHED_NORMAL(OTHER) is default.

To change the policy after a boot:
For example, if scheduling is modified before insmod-ing callisto BCM drivers, tasks inherit changed
scheduling. In that case last two task mods can be dropped. The list is quite aggressive as changing
policy of kthread affects all new kernel threads. Fine tuning would obviously have to be done along
with BCM.

(about ways to change from NORMAL to RR)
http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0182.html
http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0330.html


<code>
/* from sched.h.
#define MAX_USER_RT_PRIO   100
#define MAX_RT_PRIO        MAX_USER_RT_PRIO
*/

#define MY_RT_PRIORITY MAX_USER_RT_PRIO /* Highest possible */

int main(int argc, char **argv)
{
  ...
  int rc, old_scheduler_policy;
  struct sched_param my_params;
  ...

  /* Passing zero specifies caller's (our) policy */
  old_scheduler_policy = sched_getscheduler(0);
  my_params.sched_priority = MY_RT_PRIORITY;

  /* Passing zero specifies callers (our) pid */
  rc = sched_setscheduler(0, SCHED_RR, &my_params);
  if ( rc == -1 )
    handle_error();
  ...
}

(sched.c)
/**
 * sys_sched_get_priority_max - return maximum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the maximum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_max(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = MAX_USER_RT_PRIO-1;
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
                break;
        }
        return ret;
}

/**
 * sys_sched_get_priority_min - return minimum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the minimum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_min(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = 1;
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
        }
        return ret;
}


{pthread-schedule}
The POSIX draft standard specifies scheduling policy attributes of SCHED_FIFO (first-in-first-out),
SCHED_RR (round-robin), or SCHED_OTHER (an implementation-defined method). SCHED_FIFO and
SCHED_RR are optional in POSIX, and only are supported for real time bound threads


# ============================================================================
#{
={============================================================================
*kt_linux_core_100*  ipc

CH43, Fig 43-1 in {ref-LPI} which says that the 'general' term IPC is often used to describe them all;
communication, signal, and synchronization.

communication - data transfer - byte stream  - pipe
                                             - fifo
                                             - stream socket

                              - message      - sys v message q
                                             - posix message q
                                             - datagram socket

                              - pseudoterminal

               - shared memory   - sys v shm
                                 - posix shm
                                 - memory mapping  - anonymous mapping
                                                   - mapped file

signal   - standard signal
         - realtime signal

synchronization   - semaphore - sys v
                  - posix     - named
                              - unnamed
                  - file lock - record lock
                              - file lock

                  - mutex
                  - condition variable

Signals: Although signals are intended primarily for other purposes, they can be used as a
synchronization technique in certain circumstances. More rarely, signals can be used as a
communication technique: the signal number itself is a form of information, and realtime signals can
be accompanied by associated data (an integer or a pointer).

{nonnetworked-ipc}
From {ref-UNP}. nonnetworked-ipc means that ipc for local and newtworked-ipc means that for remote
such as socket.

{categories-of-ipc}

From {ref-UNP}:

three-ways-to-share-between-processes

(1) Process Process       (2) Process Process          (3) Process <- shm -> Process
      |        |                 |       |      
Kernel                          shared info in kernel
      |        |
Filesystem


<persistence> which is lifetime of an objects
Define the persistence of any type of IPC as how long an object of that type remains in existence.
Process-persistence exists until last process with IPC open closes the object and kernel one exists
until kernel reboots or IPC objects is explicitly deleted.

process-persistence: pipe, fifo, mutex, condition-var, read-write-lock, ...
kernel-presistence : shm, named-semaphore, ...

Be careful when defining the presistence of ipc because it is not always as it seems: the data
within a pipe is maintained within the kernel, but pipes have process-persistence because after the
last process that has the pipe open for reading closes the pipe, the kernel discard all data and
remove the pipe.

From {ref-LPI}:

<data-transfer> 
1. requires two data transfers between user and kenel memory
2. synchronization between the reader and writer processes is automatic by kernel. if a reader
attempts to fetch data from data-transfer facility that has no data, then read will bock until some
write data to it.  
3. available to one which done read operation since read consumes data.

<byte-stream>
read and write is independent meaning read may read an arbitrary bytes. This models "file as a
sequence of bytes".

An application can also impose a message-oriented model on a byte-stream facility, by using
delimiter characters, fixed-length messages, or message headers that encode the length of the total
message message: each read reads a whole message. not possible to read part of a message and to read
multiple messages.


<shared-memory>
1. don't require system calls or data transfer between user and kenel. Hence shared memory
provide very fast communication. 
2. However it can be offset by the need to sync and semaphore is the usual method used with shared
memory.
3. avaible to all of the processes that share that memory

<file-desc-based>
facility using file descriptors like pipe, fifo, and sockets

The primary benefit of these techniques is that they allow an application to simultaneously monitor
multiple file descriptors to see whether I/O is possible on any of them.


{namespace}
Use name or identifier so that one process can create ipc object and other processes can specify
that same ipc object.

type                       name used to identify   handle used to refer to object
------------------------------------------------------------------------------------
pipe                       no name                 file descriptor
fifo                       pathname                ditto

UNIX domain socket         pathname                ditto
Internet domain socket     IP and port             ditto

posix message q            posix ipc pathname      mqd_t
posix named semaphore      ditto                   sem_t* (sem pointer)
posix unnamed semaphore    no name                 sem_t*
posix shared memory        posix ipc pathname      file descriptor 

anonymous mapping          no name                 none
memory mapped file         pathname                file descriptor 


{accessibility-and-persistence}
type                       accessibility                 persistence
------------------------------------------------------------------------------------
pipe                       only by related processes     process
fifo                       permission mask               ditto

UNIX domain socket         permission mask               ditto
Internet domain socket     by any processe               ditto

posix message q            permission mask               kernel
posix named semaphore      permission mask               kernel
posix unnamed semaphore    permission of underlying mem  depends
posix shared memory        permission mask               kernel

anonymous mapping          only by releated              process 
memory mapped file         permission mask               file system 

unix and network domain:

Of all of the IPC methods shown in Figure 43-1, only sockets permit processes to communicate over a
network. Sockets are generally used in one of two domains: the UNIX domain, which allows
communication between processes on the same system, and the Internet domain, which allows
communication between processes on different hosts connected via a TCP/IP network. Often, only minor
changes are required to convert a program that uses UNIX domain sockets into one that uses Internet
domain sockets, so an application that is built using UNIX domain sockets can be made
network-capable with relatively little effort.

<portability>
However, the POSIX IPC facilities (message queues, semaphores, and shared memory) are not quite as
widely available as their System V IPC counterparts, especially on older UNIX systems. An
implementation of POSIX message queues and 'full' support for POSIX semaphores have appeared on Linux
only in the 2.6.x kernel series. Therefore, from a portability point of view, System V IPC may be
preferable to POSIX IPC.

As of 06 Jan 2014, the latest stable kernel release is 3.12.6

note: Here, 'related' means related via fork(). In order for two processes to access the object, one
of them must create the object and then call fork(). As a consequence of the fork(), the child
process inherits a handle referring to the object, allowing both processes to share the object.

<performance>
In some circumstances, different IPC facilities may show notable differences in performance.
However, in later chapters, we generally refrain from making performance comparisons, for the
following reasons:

1) The performance of an IPC facility may not be a significant factor in the overall performance of
an application, and it may not be the only factor in determining the choice of an IPC facility.

2) The relative performance of the various IPC facilities may vary across UNIX implementations or
between different versions of the Linux kernel.

3) Most importantly, the performance of an IPC facility will vary depending on the precise manner
and environment in which it is used. Relevant factors include the size of the data units exchanged
in each IPC operation, the amount of unread data that may be outstanding on the IPC facility,
whether or not a process context switch is required for each unit of data exchanged, and other
load on the system.

If IPC performance is crucial, there is no substitute for application-specific benchmarks run under
an environment that matches the target system. To this end, it may be worth writing an 'abstract'
software layer that hides details of the IPC facility from the application and then testing
performance when different IPC facilities are substituted underneath the abstract layer.


={============================================================================
*kt_linux_core_101*  ipc: pipe and fifo

{pipe}
The pipe is 'unnamed' fifo and is an early form that can be used 'related'-processes such as parent
and child. In other words, created using fork() call. Linux supports uni-directional pipe or
half-duplex so need 'two' pipes for read and write.

<limited-capacity> from LPI 44
A pipe is simply a buffer maintained in kernel memory. This buffer has a maximum capacity. Once a
pipe is full, further writes to the pipe block until the reader removes some data from the pipe.
This means that pipe is kernel resource meaning that there is copy between kernel and process. Also
there is <no-open-call>.

<first> to-create-pipe
Get two fds from a pipe() call. fd[0] for read and fd[1] for write. Create a child via fork() and
close unused fds to create a single channel between parent and child. note: 0 for read and 1 for
write which are fixed.

parent                        child
fd[1] write   pipe ->         fd[0] read
fd[0] read                    fd[1] write

int filedes[2];

if (pipe(filedes) == -1) /* Create the pipe */
   errExit("pipe");

switch (fork()) { /* Create a child process */
 case -1:
   errExit("fork");

 case 0: /* Child */
   if (close(filedes[1]) == -1) // close unused 'write' end
     errExit("close");
   /* Child now reads from pipe */
   break;

 default: /* Parent */
   if (close(filedes[0]) == -1) // close unused 'read' end
     errExit("close");
   /* Parent now writes to pipe */
   break;
}


<second> to-create-pipe
popen() call which simplfies pipe creation, fork, reading/writing setting. However, need to set
problem to fork in the command line.

The C standard I/O library popen(3) makes it easy for the application programmer to open a pipe to
an external process.

#include <stdio.h>
FILE *popen(const char *command, const char *mode);
int pclose(FILE *stream);

The argument command must be a command that is acceptable to the UNIX shell. The second argument
mode must be the C string "r" for reading or "w" for writing.  No other combination, such as "w+",
is acceptable. 

(reading example)
FILE *p;
char cmd[1000];
/* argv[2] is fname */
sprintf(cmd,"grep 'Time has been updated to (Year:Month' %s | head -1",argv[2]);
p=popen(cmd,"r");
fgets(tmp,sizeof(tmp),p);
pclose(p);

After all, the reason that can use pipe between parent/child is that fds are shared.

<broken-pipe>
When 'write' to fifo that is not opened to read. Menas there is no reading process or reading process
is killed because reading fd of pipe or file gets closed. SIGPIPE. Instead, there is no data to
read, empty, reading thread or process is blocked.


{fifo}
To solve this, fifo was introduced and is called 'named'-pipe since has path name. Means that it
is created in the filesystem as a file. Use usual read and write call. Fifo is either read-only or
write-only.

1. create a fifo using mkfifo call.
2. open a fifo for read or write using open call

<fifo-create>
#include <sys/stat.h>
int mkfifo(const char *pathname, mode_t mode);

<fifo-sync>
Therefore, by default, opening a FIFO for reading (the open() O_RDONLY flag) blocks until another
process opens the FIFO for writing (the open() O_WRONLY flag). Conversely, opening the FIFO for
writing blocks until another process opens the FIFO for reading.  In other words, opening a FIFO
synchronizes the reading and writing processes.

<note-from-nds-fusion-ipc>
FIFOs have certain natural limitations, they are unidirectional, and fragment large message sizes
with no built in support for reassembling the fragments. Furthermore FIFOs are limited in number due
to system resources. The key are 'fragment' and 'limit'-in-number.

Due to the limited number of FIFOs it was determined that there would be one
control FIFO per server (to establish communication from clients) and one pair of unidirectional
FIFOs for every pair of server and client instances (note that there is one instance of a client in
every process that uses that client). This is illustrated below.

server                  client a
- control pipe          -> and <-

                        client b
                        -> and <-

note: KT. do not match with this pic?

To cope with fragmentation and the fact that there may be several interfaces being used on a single
client instance (and multiple components using that client instance in a single process) a protocol
was designed as described below. This protocol is used in both FIFO and TCP/IP IPC communication.


{summary}
When writes, if there is no reading process, broken-pipe. When reads, if there
is no writing process, blocked.


={============================================================================
*kt_linux_core_103* ipc: which one to use

When performing interprocess synchronization, our choice of facility is typically determined by the
functional requirements. When coordinating access to a file, file record locking is usually the best
choice. Semaphores are often the better choice for coordinating access to other types of shared
resource.

{semaphores-versus-pthreads-mutexes}

<1>
Unlike mutex (this mean condition?), semaphore does not get lost when there is no waiting one.

<2>
POSIX semaphores and Pthreads mutexes can both be used to synchronize the actions of threads within
the same process, and their performance is [similar]. 

However, mutexes are usually preferable, because the [ownership] property of mutexes enforces good
structuring of code; only the thread that locks a mutex can unlock it. By contrast, one thread can
increment a semaphore that was decremented by another thread. This flexibility can lead to poorly
structured synchronization designs. For this reason, semaphores are sometimes referred to as the
"gotos" of concurrent programming.

There is one circumstance in which mutexes can't be used in a multithreaded application and
semaphores may therefore be preferable. Because it is async-signalsafe. See
{async-signal-safe-function}. The sem_post() function can be used from within a signal handler to
synchronize with another thread. This is not possible with mutexes, because the Pthreads functions
for operating on mutexes are not asyncsignal-safe. 

However, because it is usually preferable to deal with asynchronous signals by accepting them using
sigwaitinfo() (or similar), rather than using signal handlers (see Section 33.2.4), this advantage
of semaphores over mutexes is seldom required.

<3>
From 30.1.3. Peformance of mutex in ref-LPI. The problem with file locks and semaphores is that they
always require a system call for the lock and unlock operations, and each system call has a small
but appreciable, cost (Section 3.1). By contrast, mutexes are implemented using atomic
machine-language operations; performed on memory locations visible to all threads and require system
calls only in case of lock contention.


On Linux, mutexes are implemented using futexes (an acronym derived from fast user space mutexes),
   and lock contentions are dealt with using the futex() system call. We don’t describe futexes in
   this book (they are not intended for direct use in user-space applications), but details can be
   found in [Drepper, 2004 (a)], which also describes how mutexes are implemented using futexes.
   [Franke et al., 2002] is a (now outdated) paper written by the developers of futexes, which
   describes the early futex implementation and looks at the performance gains derived from futexes.


==============================================================================
*kt_linux_core_110*  ipc: dbus and kbus	

{dbus}

http://www.freedesktop.org/wiki/Software/dbus/

What is D-Bus?

D-Bus is a message bus system, a simple way for applications to talk to one another. In addition to
interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and
reliable to code a "single instance" application or daemon, and to launch applications and daemons
on demand when their services are needed.

D-Bus supplies both a system daemon (for events such as "new hardware device added" or "printer
queue changed") and a per-user-login-session daemon (for general IPC needs among user
applications). Also, the message bus is built on top of a general one-to-one message passing
framework, which can be used by any two apps to communicate directly (without going through the
message bus daemon). Currently the communicating applications are on one computer, or through
unencrypted TCP/IP suitable for use behind a firewall with shared NFS home directories. (Help wanted
with better remote transports - the transport mechanism is well-abstracted and extensible.)

The D-Bus low-level API reference implementation and protocol have been heavily tested in the real
world over several years, and are now "set in stone." Future changes will either be compatible or
versioned appropriately.

The low-level libdbus reference implementation has no required dependencies; the bus daemon's only
required dependency is an XML parser (expat). Higher-level bindings specific to particular
frameworks (Qt, GLib, Java, C#, Python, etc.) add more dependencies, but can make more assumptions
and are thus much simpler to use. The bindings evolve separately from the low-level libdbus, so some
are more mature and ABI-stable than others; check the docs for the binding you plan to use.

There are also some reimplementations of the D-Bus protocol for languages such as C#, Java, and
Ruby. These do not use the libdbus reference implementation.

It should be noted that the low-level implementation is not primarily designed for application
authors to use. Rather, it is a basis for binding authors and a reference for reimplementations. If
you are able to do so it is recommended that you use one of the higher level bindings or
implementations. A list of these can be found on the bindings page.

The list of projects using D-Bus is growing and they provide a wealth of examples of using the
various APIs to learn from.

D-Bus is very portable to any Linux or UNIX flavor, and a port to Windows is in progress.

D-Bus applications

There are many, many technologies in the world that have "Inter-process communication" or
"networking" in their stated purpose: CORBA, DCE, DCOM, DCOP, XML-RPC, SOAP, MBUS, Internet
Communications Engine (ICE), and probably hundreds more. Each of these is tailored for particular
kinds of application. 

D-Bus is designed for two specific cases:

Communication between desktop applications in the same desktop session; to allow integration of the
desktop session as a whole, and address issues of process lifecycle (when do desktop components
start and stop running).

Communication between the desktop session and the operating system, where the operating system would
typically include the kernel and any system daemons or processes. 

For the within-desktop-session use case, the GNOME and KDE desktops have significant previous
experience with different IPC solutions such as CORBA and DCOP. D-Bus is built on that experience
and carefully tailored to meet the needs of these desktop projects in particular. D-Bus may or may
not be appropriate for other applications; the FAQ has some comparisons to other IPC systems.

The problem solved by the systemwide or communication-with-the-OS case is explained well by the
following text from the Linux Hotplug project:

A gap in current Linux support is that policies with any sort of dynamic "interact with user"
component aren't currently supported. For example, that's often needed the first time a network
adapter or printer is connected, and to determine appropriate places to mount disk drives. It would
seem that such actions could be supported for any case where a responsible human can be identified:
single user workstations, or any system which is remotely administered.

This is a classic "remote sysadmin" problem, where in this case hotplugging needs to deliver an
event from one security domain (operating system kernel, in this case) to another (desktop for
logged-in user, or remote sysadmin). Any effective response must go the other way: the remote
domain taking some action that lets the kernel expose the desired device capabilities. (The action
can often be taken asynchronously, for example letting new hardware be idle until a meeting
finishes.) At this writing, Linux doesn't have widely adopted solutions to such problems.
However, the new D-Bus work may begin to solve that problem. 

D-Bus may happen to be useful for purposes other than the one it was designed for. Its general
properties that distinguish it from other forms of IPC are:

Binary protocol designed to be used asynchronously (similar in spirit to the X Window System protocol).

Stateful, reliable connections held open over time.

The message bus is a daemon, not a "swarm" or distributed architecture.

Many implementation and deployment issues are specified rather than left ambiguous/configurable/pluggable.

Semantics are similar to the existing DCOP system, allowing KDE to adopt it more easily.

Security features to support the systemwide mode of the message bus. 


{kdbus}
https://github.com/gregkh/presentation-kdbus
https://github.com/gregkh/kdbus


# ============================================================================
#{
={============================================================================
*kt_linux_core_200* ipc: sync: semaphore

{what-is-semaphore}
note: This can be used for process or thread but it is expensive for thread.

System V semaphores are not used to transfer data between processes. Instead, they allow processes
to synchronize their actions. A semaphore is a 'kernel'-maintained 'integer' whose value is never
permitted to fall below 0. A process can decrease or increase the value of a semaphore.  If an
attempt is made to decrease the value of the semaphore below 0, then the kernel blocks the operation
until the semaphore's value increases to a level that permits the operation to be performed. 

The meaning of a semaphore is determined by the application. A process decrements a semaphore from
say, 1 to 0 in order to reserve exclusive access to some 'shared' resource, and after completing work
on the resource, increments the semaphore so that the shared resource is released for use by some
other process. The use of a binary semaphore-a semaphore whose value is limited to 0 or 1-is common.

However, an application that deals with multiple instances of a shared resource would employ a
semaphore whose maximum value equals the 'number' of shared resources. Linux provides both System V
semaphores and POSIX semaphores, which have essentially similar functionality.

1. Setting the semaphore to an absolute value;
2. Adding a number to the current value of the semaphore;            // give
3. Subtracting a number from the current value of the semaphore; and // take
4. Waiting for the semaphore value to be equal to 0.

The last two of these operations may cause the calling process to block. A semaphore has no meaning
in and of itself. Its meaning is determined only by the associations given to it by the processes
using the semaphore.

However, System V semaphores are rendered unusually complex by the fact that they are allocated in
groups called semaphore sets. So move to POSIX semaphore.


{posix-semaphore}
SUSv3 specifies two types of POSIX semaphores:

1. Named semaphores: This type of semaphore has a name. By calling sem_open() with the same name
'unrelated' processes can access the same semaphore.

2. Unnamed semaphores: This type of semaphore doesn't have a name; instead, it resides at an
agreed-upon location in 'memory'. Unnamed semaphores can be shared between processes or between a
group of 'threads'. When shared between processes, the semaphore must reside in a region of (System V,
POSIX, or mmap()) shared memory. When shared between threads, the semaphore may reside in an
area of memory shared by the threads (e.g., on the heap or in a global variable).

POSIX semaphores operate in a manner similar to System V semaphores; that is, a POSIX semaphore is
an integer whose value is not permitted to fall below 0. If a process attempts to decrease the value
of a semaphore below 0(take), then, depending on the function used, the call either blocks or fails
with an error indicating that the operation was not currently possible.

Some systems don't provide a full implementation of POSIX semaphores. A typical restriction is that
only unnamed thread-shared semaphores are supported. That was the situation on Linux 2.4; 

<linux-2-6-support>
With Linux 2.6 and a glibc that provides NPTL, a full implementation of POSIX semaphores is
available.

<named-semaphore>
To work with a named semaphore, we employ the following functions:

1. The sem_open() function opens or creates a semaphore, initializes the semaphore if it is created
by the call, and returns a handle for use in later calls.

#include <fcntl.h> /* Defines O_* constants */
#include <sys/stat.h> /* Defines mode constants */
#include <semaphore.h>

sem_t *sem_open(const char *name, int oflag, ...  /* mode_t mode, unsigned int value */ );

Returns pointer to semaphore on success, or SEM_FAILED on error

2. The sem_post(sem) and sem_wait(sem) functions respectively increment and decrement a semaphore's
value. give and take

3. The sem_getvalue() function retrieves a semaphore's current value.

4. The sem_close() function removes the calling process’s association with a semaphore that it
previously opened.

5. The sem_unlink() function removes a semaphore name and marks the semaphore for deletion when all
processes have closed it.

<named-semaphore-on-linux>
SUSv3 doesn't specify how named semaphores are to be implemented. On Linux, they are created as
small POSIX shared memory objects with names of the form sem.name, in a dedicated tmpfs file system
(Section 14.10) mounted under the directory /dev/shm. This file system has 'kernel'-persistence-the
semaphore objects that it contains will persist, even if no process currently has them open, but
they will be lost if the system is shut down.

<unnamed-semaphore>
The semaphore is made available to the processes or threads that use it by placing it in an area of
memory that they share. Operations on unnamed semaphores use the same functions; sem_wait(),
sem_post(), sem_getvalue(), and so on that are used to operate on named semaphores.

In addition, two further functions are required:

The sem_init() function initializes a semaphore and informs the system of whether the semaphore will
be shared between processes or between the threads of a single process.

The sem_destroy(sem) function destroys a semaphore. These functions should not be used with named
semaphores.

<when-useful-to-use-unnamed>
1. A semaphore that is shared between 'threads' doesn't need a name. Making an unnamed semaphore a
shared (global or heap) variable automatically makes it accessible to all threads.

2. A semaphore that is being shared between 'related' processes doesn't need a name. If a parent
process allocates an unnamed semaphore in a region of shared memory (e.g., a shared anonymous
mapping), then a child automatically inherits the mapping and thus the semaphore as part of the
operation of fork().

3. If we are building a dynamic data structure (e.g., a binary tree), each of whose items requires
an associated semaphore, then the simplest approach is to allocate an unnamed semaphore within each
item. Opening a named semaphore for each item would require us to design a convention for generating
a (unique) semaphore name for each item and to manage those names (e.g., unlinking them when they
are no longer required). note: real example? useful?


={============================================================================
*kt_linux_core_201* ipc: sync: pthread mutex and cond var

From #07 in {ref-UNP} and #30 in {ref-LPI}

This is to sync for data or critial region. Although talking of critial region, what is really
protected is the 'data' being manipulated within the critical region. This is posix.1 standard.

As with semaphore, mutex and cond-var uses 'global' structure and if these are shared in a shared
memory, these can be used between processes. If not, it is for threads in a process.

More generally, mutexes can be used to ensure atomic access to any shared resource, but protecting
shared variables is the most common use.

In {ref-LPI} p881, process-shared mutexes and condition variables. They are 'not' available on all
UNIX systems, and so are not commonly employed for process synchronization.

note: As said, if share mutex structure in shared memory, then what is process-shared mutex?


{why-need-sync}
Because of 'race' condition. Like {ref-LPI}, when run two threads without sync, task-switching
happens based on time-slot given and yield 'non''determistic' result.

threadFunc()
{
  for(j=0; j < loops; j++)
  {
    local = global;
    local++;
    global = local;
  }
}

If we change this like below, do not need to sync because inc looks atomic?

threadFunc()
{
  for(j=0; j < loops; j++)
  {
    global++;
  }
}

NO because this single inc operator may result in 'multiple' machine code which are equivalent to the
before. See *kt_linux_core_107* for more about atomic and race-condition.


{ownership} mutex-deadlock
A mutex has two states: locked and unlocked. At any moment, at most one thread may hold the lock on
a mutex. Attempting to lock a mutex that is already locked either blocks or fails with an error
depending on the method used to place the lock.

When a thread locks a mutex, it becomes the 'owner' of that mutex. Only the mutex owner can 'unlock' the
mutex. This property 'improves' the structure of code that uses mutexes and also allows for some
optimizations in the implementation of mutexes.

1. Locking. If try to lock what is already locked by self, two may happen for 'default' type of
mutex: mutex-'deadlock' or EDEADLK for error check type. On Linux, deadlock by default from
{ref-LPI}. Why deadlock?  Because blocked trying to lock a mutex that it already owns.

2. Unlocking. Unlocking a mutex that is not locked or that is locked by another thread 'undefined'
result.

note: ownership matters when unlock and semaphore do not have ownership.


{cooperative-lock}
This means that mutex locking is advisory, rather than mandatory; nothing can prevent one thread
from manipulating the data without first obtaining the mutex. For example, threads not participating
a mutex circle can.

Each thread employs the following protocol for accessing a resource:
1. lock the mutex for the shared resource;
2. access the shared resource; and
3. unlock the mutex.


{implicit-sync}
This is synchronization handled by kernel not by applicaion. Such as pipe and message-q. For
example,

grep pattern chapters.* | wc -l

Writing by producer and reading by consumer are handled by kernel. Does it mean that grep and wc
runs at the same time? not one after one.


{mutex-apis} {explicit-sync}
Unlike semaphore and file locks, mutex only requires system call for lock contention since on Linux,
mutexes are implemented using futexes (an acronym derived from fast user space mutexes), and lock
contentions are dealt with using the futex() system call. So less expensive. from 30.1.3 in
{ref-LPI}

If it is 'statically' allocated for default attributes, should use:

pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;

If it is dymamically allocated such as malloc or shared-mem, should use. When? mutex can be part of
structure or object on the heap.

1. The mutex was dynamically allocated on the heap. For example, suppose that we create a
dynamically allocated linked list of structures, and each structure in the list includes a
pthread_mutex_t field that holds a mutex that is used to protect access to that structure.

2. The mutex is an automatic variable allocated on the stack.

3. We want to initialize a statically allocated mutex with attributes other than the defaults.

int pthread_mutex_init(pthread_mutex_t* mutex, const pthread_mutexattr_t* attr);
int pthread_mutex_destory(pthread_mutex_t* mutex);


This is blocking call.

int pthread_mutex_lock( pthread_mutex_t *mptr); 
int pthread_mutex_unlock( pthread_mutex_t *mptr);


This is non-blocking and returns EBUSY if the mutex is already locked. If the time interval
specified by its abstime argument expires without the caller becoming the owner of the mutex
pthread_mutex_timedlock() returns the error ETIMEDOUT.

These are less used because need to poll and risks being starved. Well designed one can avoid this.
In most well-designed applications, a thread should hold a mutex for only a short time, so that
other threads are not prevented from executing in parallel.

int pthread_mutex_trylock( pthread_mutex_t *mptr);
int pthread_mutex_timedlock( pthread_mutex_t *mptr);


{deadlock-condition}
A deadlock is a situation waiting for something never happens or where two or more process are
blocked because each is waiting on the other process(es) to complete some action. Unrequited love?

<1> from pipe
If employing this bidirectional communication using two pipes, then we need to be wary of deadlocks
that may occur if both processes block while trying to read from empty pipes or while trying to
write to pipes that are already full.

<2> from fifo.
use-different-fifo-for-read-and-write

Process X                              Process Y
1. Open FIFO A for reading(blocks)     1. Open FIFO A for reading(blocks)
2. Open FIFO B for writing             2. Open FIFO B for writing

When open a fifo to read which is not opend to write but there is no writing process then reading
process is blocked. Therefore, if parent and child opens different fifos to read, deadlock happens.

use-same-fifo-for-read-and-write-but-the-same-order

Process X                              Process Y
1. Open FIFO A for reading(blocks)     1. Open FIFO B for reading(blocks)
2. Open FIFO B for writing             2. Open FIFO A for writing

The two processes shown in Figure 44-8 are deadlocked. Each process is blocked waiting to open a
FIFO for reading. This blocking would not happen if each process could perform its second step
(opening the other FIFO for writing). So change order or use non-blocking call.

<3>
Message queues have a limited capacity. This has the potential to cause a couple of problems. One of
these is that multiple simultaneous clients could fill the message queue, resulting in a deadlock
situation, where no new client requests can be submitted and the server is blocked from writing any
responses.

<4> from mutex-deadlock
Locking. If try to lock what is already locked by self, two may happen for 'default' type of mutex:
mutex-deadlock or EDEADLK. On Linux, deadlock by default from {ref-LPI}. Why deadlock? Because
blocked trying to lock a mutex that it already owns.

<4> from lock-free-queue
Used the timed_wait() instead of the simpler wait() to solve a possible deadlock when Produce() is
called between line A and line B in Listing One. Then wait() will miss the notify_one() call and
have to wait for the next produced element to wake up. 'if' this element never comes (no more
produced elements or if the Produce() call actually waits for Consume() to return), there's a
deadlock.

<5> from {ref-LPI}, deadlock happens that is the same as mutex-deadlock

Thead A             Thread B
lock(mutex1)        lock(mutex2)
lock(mutex2)        lock(mutex1)

Both tries to lock the mutex that the other thread has already locked. Both threads will remain
blocked indefinitely.


{to-avoid-deadklock}
1. mutex-hierarchy-or-order
Less flexible. Use locks in the same set of mutexs in the same order.

2. try-and-back-off
Flexible. If try_lock fails, release all and try again later. This is less efficient than hierarchy
approach but can be more 'flexible' since no need rigid hierarchy.


{mutex-types} from {ref-LPI} 30.1.7:
PTHREAD_MUTEX_NORMAL
// no deadlock detection. On Linux, this is PTHREAD_MUTEX_DEFAULT

PTHREAD_MUTEX_ERRORCHECK
// reports errors when fails than blocking or deadlock but 'slower' than a normal so debugging
// purpose.

PTHREAD_MUTEX_RECURSIVE
// A recursive mutex maintains the concept of a lock-'count'. When a thread first acquires the mutex,
// the lock count is set to 1. Each subsequent lock operation by the same thread increments the lock
// count, and each unlock operation decrements the count. The mutex is released (i.e., made
// available for other threads to acquire) only when the lock count falls to 0. 
//
// When is it useful?
//
// Unlocking an unlocked mutex fails, as does unlocking a mutex that is currently locked by another
// thread. 
//
// note: Still have ownership notion but there is no mutex deadlock or undefined result as NOMAL
// type has.


{consumer-producer-example-one}
From {ref-UNP}. Multiple producer(writing) and one consumer(reading). Once writing finishs, consumer
get started. No need to sync between producer and consumer. Only sync between producers. So use
mutex and no cond var. 

No need to have a sync for reading? NO. Need to have reading in sync if want to have right result.
For this example, reaading starts after writing. Hence no need.

<code-example>

#include <stdio.h>
#include <pthread.h>
#include <sys/errno.h>

#define MAXNITEMS 	1000000
#define MAXNTHREADS	100

#define	min(a,b)	((a) < (b) ? (a) : (b))
#define	max(a,b)	((a) > (b) ? (a) : (b))

//
void Pthread_create(pthread_t* tid, const pthread_attr_t* attr, void *(*func)(void*), void*arg)
{
	int n;

	if(( n = pthread_create( tid, attr, func, arg )) == 0 )
		return;
	
	errno = n;
	fprintf( stderr, "pthread_create error(%d)", n );
}

// shared by all threads
int nitems;

struct {
	pthread_mutex_t mutex;
	int				buff[MAXNITEMS];
	int				nput;				// next index to write
	int				nval;				// next val to write
} shared = { PTHREAD_MUTEX_INITIALIZER };

void *produce(void *), *consume(void *);

int main( int argc, char** argv )
{
	int i, nthreads, count[MAXNTHREADS]={0};
	pthread_t tid_produce[MAXNTHREADS]={0}, tid_consume;

	if( argc != 3 )
	{
		fprintf( stderr, "usuage: prodcons2 <#items> <#threads>\n");
		exit(1);
	}

	nitems = min( atoi( argv[1] ), MAXNITEMS );
	nthreads = min( atoi( argv[2] ), MAXNTHREADS );

	// start all producer threads
	for( i=0; i < nthreads; ++i )
	{
		count[i] = 0;
		Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
	}

	// wait for all the producer threads
	for( i=0; i < nthreads; ++i ) 
	{
		pthread_join( tid_produce[i], NULL );
		printf("tid[%d] count[%d] = %d\n", tid_produce[i], i, count[i] );
	}

	// start, then wait for the consumer thread
	Pthread_create(&tid_consume, NULL, consume, NULL );
	pthread_join(tid_consume, NULL );

	exit(0);
}

void* produce(void* arg)
{
	printf("run tid[%d] \n", pthread_self());

	for(;;) 
	{
		pthread_mutex_lock( &shared.mutex );
		if( shared.nput >= nitems )  // buff is full, we are done.
		{
			printf("done tid[%d] \n", pthread_self());
			pthread_mutex_unlock(&shared.mutex);
			return NULL;
		}

		shared.buff[ shared.nput ] = shared.nval;
		shared.nput++;
		shared.nval++;

		pthread_mutex_unlock( &shared.mutex );

		// inc of the count is not part of the CR because each thread has its own
		*((int*)arg) += 1;
	}
}

void* consume(void* arg)
{
	int i;

	// see use of global vars; nitems and shared.buff
	for( i=0; i < nitems; i++ )
	{
		if( shared.buff[i] != i )
			printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
	}

	printf("consume done\n" );

	return NULL;
}


kit@kit-VirtualBox:~/work$ ./a.out 1000000 5
run tid[-1252185280] 
run tid[-1243792576] 
run tid[-1235399872] 
run tid[-1227007168] 
run tid[-1218614464] 
done tid[-1252185280] 
done tid[-1243792576] 
done tid[-1227007168] 
done tid[-1218614464] 
tid[-1218614464] count[0] = 198088
tid[-1227007168] count[1] = 220562
done tid[-1235399872] 
tid[-1235399872] count[2] = 254805
tid[-1243792576] count[3] = 162778
tid[-1252185280] count[4] = 163767
consume done


{consumer-producer-example-two} 

Now consumer runs at the same time and runs when there is data to read. All is accessing the same
data and are in the same mutex group. Problem is that consumer do busy loop to check data, <polling>
or <spinning> 

<code-example>

The changes from the previous is:

int main()
{
	...

	// start all producer threads
	for( i=0; i < nthreads; ++i )
	{
		count[i] = 0;
		Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
	}

	// moved here. start, then wait for the consumer thread
	Pthread_create(&tid_consume, NULL, consume, NULL );

	...
}

Uses shared mutex between producers and consumer. If items to read are ready then call returns.
Otherwise, do loops. 

void consume_wait(int i)
{
  for(;;) {
    pthread_mutex_lock(&shared.mutex);
    if( i < shared.nput ) {
      pthread_mutex_unlock(&shared.mutex);
      return; /* item is ready */
    }
    pthread_mutex_unlock(&shared.mutex);
  }
}

void* consume(void* arg)
{
	int i;

	// see use of global vars; nitems and shared.buff
	for( i=0; i < nitems; i++ )
	{ >
		// polling until item is ready
		consume_wait(i);
<
		if( shared.buff[i] != i )
			printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
	}

	printf("con: done\n" );

	return NULL;
}


{consumer-producer-example-three} {cond-var}

The polling is a waste of cpu time. How to solve? A mutex is for locking and a cond-var is for
waiting. The mutex provides mutual exclusion for accessing the shared variable, while the condition
variable is used to signal changes in the variable’s state.

<cond-wait>
// This is blocking on condition and ALWAYS has an associated mutex. Why? because both producer and
// consumer accesses to the shared state variable which is linked to that condition so should be
// synced. In other words, there is a natural association of a muext with a condition varaible.
//
// pthread_cond_wait( &nready.cond, &nready.mutex );
//
// Do three things: unlock a mutex, block a calling thread until signaled, and relock mutex when
// signaled. 
//
// A condition variable holds no state information. It is simply a mechanism for communicating
// information about the application's state.

int pthread_cond_wait( pthread_cond_t *cptr, pthread_mutex_t* mptr);

<cond-can-be-lost>
// Guaranteede that at least one of the blocked thread is woken up. If no thread is waiting, the
// signal is lost since condition variable holds no state information.

int pthread_cond_signal( pthread_cond_t *cptr );
int pthread_cond_broadcast( pthread_cond_t *cptr );
int pthread_cond_timedwait( ... );

The {ref-LPI} said that _signal is for the case where all waiting threads do the same task since it
do not care which should woken up and _broadcast is for case where waiting threads do different task.
Real examples?

<code-example>

{Q} Is it possible to implement this using two mutex than using a muext and a cond-var?

//
int nitems;
int buff[MAXNITEMS];

// shared by all threads
struct {
	pthread_mutex_t mutex;
	int				nput;				// next index to write
	int				nval;				// next val to write
} put = { PTHREAD_MUTEX_INITIALIZER };

struct {
	pthread_mutex_t mutex;
	pthread_cond_t cond;
	int ready;						// [KT] this is state information linked to condition.
} nready = { PTHREAD_MUTEX_INITIALIZER, PTHREAD_COND_INITIALIZER };

void* produce(void* arg)
{
	printf("pro: run tid[%d] \n", pthread_self());

	for(;;) 
	{
		pthread_mutex_lock( &put.mutex );
		if( put.nput >= nitems )  // buff is full, we are done.
		{
			printf("pro: no more. done tid[%d] \n", pthread_self());
			pthread_mutex_unlock(&put.mutex);
			return NULL;
		}

		buff[ put.nput ] = put.nval;
		put.nput++;
		put.nval++;

		pthread_mutex_unlock( &put.mutex );
>
		// { signal cond-var saying some has been written
		pthread_mutex_lock( &nready.mutex );
		if( nready.ready == 0 )
			pthread_cond_signal( &nready.cond );

		nready.ready++;

		pthread_mutex_unlock( &nready.mutex );
		// }
		
		// inc of the count is not part of the CR because each thread has its own
		*((int*)arg) += 1;
	}
}

void* consume(void* arg)
{
	int i;

	// see use of global vars; nitems and shared.buff
	for( i=0; i < nitems; i++ )
	{ >
		// receieve cond-var
		pthread_mutex_lock( &nready.mutex );

		// Always test the condition again when wakes up because [spurious-wakeups] can occur.
		// Unlock and wait. When returns from pthread_cond_wait, lock in again.
		while( nready.ready == 0 )
			pthread_cond_wait( &nready.cond, &nready.mutex );

		nready.ready--;

		pthread_mutex_unlock( &nready.mutex );
<
		if( buff[i] != i )
			printf("con: err: buff[%d] = %d\n", i, buff[i] );
	}

	printf("con: done\n" );

	return NULL;
}


{the-order-of-call}

{ref-LPI} uses different call order for producer as below because {ref-UNP} said that use _signal
before unlock can cause {mutex-deadlock} or {lock-conflict}. Therefore, POSIX recommends the
followings and {ref-LPI} said this may yieid better performance in 30.2.2:

<approach-one>
pthread_mutex_lock( &nready.mutex );
if( nready.ready == 0 )
   pthread_cond_signal( &nready.cond );

nready.ready++;

pthread_mutex_unlock( &nready.mutex );

<approach-two>
pthread_mutex_lock( &nready.mutex );

nready.ready++;

pthread_mutex_unlock( &nready.mutex );

if( nready.ready == 0 )
 pthread_cond_signal( &nready.cond );


{check-on-predicate}

Each condition variable has an associated predicate involving one or more shared variables. In this
example, nready.ready == 0 is the predicate.

This demonstrates a general design principle: a pthread_cond_wait() call must be governed by a while
loop rather than an if statement. This is so because, on return from pthread_cond_wait(), there are
no guarantees about the state of the predicate; therefore, we should immediately recheck the
predicate and resume sleeping if it is not in the desired state. [KT] When calls cond_wait, the
associated lock is released so no gurantee that this predicate is the same when wakes up.

So use [while-loop] always to test the condition again when wakes up because spurious wakeups can
occur for following reasons:

while( nready.ready == 0 )
	pthread_cond_wait( &nready.cond, &nready.mutex );

1> Other threads may be woken up first. Perhaps several threads were waiting to acquire the mutex
associated with the condition variable. Even if the thread that signaled the mutex set the predicate
to the desired state, it is still possible that another thread might acquire the mutex first and
change the state of the associated shared variable(s), and thus the state of the predicate.

2> Designing for "loose" predicates may be simpler. Sometimes, it is easier to design applications
based on condition variables that indicate possibility rather than certainty. In other words,
signaling a condition variable would mean "there may be something" for the signaled thread to do
rather than "there is something" to do. Using this approach, the condition variable can be signaled
based on approximations of the predicate’s state, and the signaled thread can ascertain if there
really is something to do by rechecking the predicate.

3> Spurious wake-ups can occur. On some implementations, a thread waiting on a condition variable
may be woken up even though no other thread actually signaled the condition variable. Such spurious
wake-ups are a (rare) consequence of the techniques required for efficient implementation on some
multiprocessor systems, and are explicitly permitted by SUSv3.


==============================================================================
*kt_linux_core_220*	sync: read-write lock

From {ref-UNP} but no such a thing in {ref-LPI}, so may be old way but surely in posix but may be
different to this since this lock is before posix standard. See {ref-UNP} note.

To distinguish between obtaining the read-write lock for reading and for writing. The rules:

1. Any number of threads can hold a given read-write lock for reading as long as no threads holds
the the lock for writing.

2. A read-write lock can be allocated for writing only if no thread hold the lock for reading or
writing.

Stated another way, any threads can have read access to a data as long as no thread is modifying
that. A data can be modified only if no other thread is reading or modifying the data.

In application, the data is read more often than the data is modified, and these can benefit from
using read-write locks instead of mutex locks. 

can provide more concurrency than a plain mutex lock when the data is read more than it is written
and known as shared-exclusive locking since shared lock for reading and exclusive lock for writing.
Multiple readers and one writer problem or readers-writer locks.


{apis}

// to get read lock. blocks the calling if there are writers
int   pthread_rwlock_rdlock(pthread_rwlock_t *);

// to get write lock. blocks the calling if there are readers or writers
int   pthread_rwlock_wrlock(pthread_rwlock_t *);

int   pthread_rwlock_unlock(pthread_rwlock_t *);
int   pthread_rwlock_tryrdlock(pthread_rwlock_t *);
int   pthread_rwlock_trywrlock(pthread_rwlock_t *);

int   pthread_rwlock_init(pthread_rwlock_t *, const pthread_rwlockattr_t *);
int   pthread_rwlock_destroy(pthread_rwlock_t *);
int   pthread_rwlockattr_destroy(pthread_rwlockattr_t *);
int   pthread_rwlockattr_init(pthread_rwlockattr_t *);

// to share the lock between different processes
int   pthread_rwlockattr_setpshared(pthread_rwlockattr_t *, int); pthread_t
int   pthread_rwlockattr_getpshared(const pthread_rwlockattr_t *, int *);


{example-implementation}
\unpv22e.tar\unpv22e\my_rwlock_cancel\

Can be implemented using mutexes and condition variables. This is an implementation which gives
preference to waiting writers but there are other alternatives.

typedef struct {
	pthread_mutex_t 	rw_mutex;			// lock on this struct
	pthread_cond_t 	rw_condreaders;	// for reader threads waiting
	pthread_cond_t 	rw_condwriters;	// for writer threads waiting

	// [KT]
	// when struct is inited, set to RW_MAGIC and used by all functions to check that the caller is
	// passing a pointer to an initialized lock and set to 0 when the lock is destroyed.
	int 					rw_magic;
	int 					rw_nwaitreaders;
	int 					rw_nwaitwriters;

	// the current status of the read-write lock. only one of these can exist at a time: -1 indicates
	// a write lock, 0 is lock available, and an value greater than 0 menas that many read locks are
	// held.
	int 					rw_refcount; 
} pthread_rwlock_t;


#define RW_MAGIC 0x19283746

/* init and destroy */
int
pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		result;

	if (attr != NULL)
		return(EINVAL);		/* not supported */

	if ( (result = pthread_mutex_init(&rw->rw_mutex, NULL)) != 0)
		goto err1;
	if ( (result = pthread_cond_init(&rw->rw_condreaders, NULL)) != 0)
		goto err2;
	if ( (result = pthread_cond_init(&rw->rw_condwriters, NULL)) != 0)
		goto err3;
	rw->rw_nwaitreaders = 0;
	rw->rw_nwaitwriters = 0;
	rw->rw_refcount = 0;
	rw->rw_magic = RW_MAGIC;

	return(0);

err3:
	pthread_cond_destroy(&rw->rw_condreaders);
err2:
	pthread_mutex_destroy(&rw->rw_mutex);
err1:
	return(result);			/* an errno value */
}

void
Pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		n;

	if ( (n = pthread_rwlock_init(rw, attr)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_init error");
}

int
pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);
	if (rw->rw_refcount != 0 ||
		rw->rw_nwaitreaders != 0 || rw->rw_nwaitwriters != 0)
		return(EBUSY);

	pthread_mutex_destroy(&rw->rw_mutex);
	pthread_cond_destroy(&rw->rw_condreaders);
	pthread_cond_destroy(&rw->rw_condwriters);
	rw->rw_magic = 0;

	return(0);
}

void
Pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	int		n;

	if ( (n = pthread_rwlock_destroy(rw)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_destroy error");
}


// rdlock
// A problem exists in this function: if the calling thread blocks in the call to pthread_cond_wait
// and the thread is then canceled, the thread terminates while it holds the mutex lock, and the
// counter rw_nwaitreaders is wrong. The same problem exists in our implentation of
// pthred_rwlock_wrlock. Can correct these problem in {}

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// 4give preference to waiting writers. cannot get a read lock if a) rw_refcount < 0 (meaning
	// there is a writer holding the lock and b) if threads are waiting to get a write lock.
	// [KT] if. case that there are writers
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}

	// [KT] else. case that there are no writers
	if (result == 0)
		rw->rw_refcount++;		/* another reader has a read lock */

	pthread_mutex_unlock(&rw->rw_mutex);
	return (result);
}

/* tryrdlock */
int
pthread_rwlock_tryrdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0)
		result = EBUSY;			/* held by a writer or waiting writers */
	else
		rw->rw_refcount++;		/* increment count of reader locks */

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


/* wrlock */
int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// [KT] if there are other readers or writers
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}

	// [KT] else there are no readers and writers
	if (result == 0)
		rw->rw_refcount = -1;

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


// unlock. used by both reader and writer
int
pthread_rwlock_unlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount > 0)
		rw->rw_refcount--;			// releasing a reader
	else if (rw->rw_refcount == -1)
		rw->rw_refcount = 0;			// releasing a writer
	else
		err_dump("rw_refcount = %d", rw->rw_refcount); // cannot be since it is to unlock

	// 4give preference to waiting writers over waiting readers
	//
	// {Q} The ref-UNP says: notice that we do not grant any additional read locks as soon as a
	// writer is waiting; otherwise, a stream of continual read requests could block a waiting writer
	// forever. For this reason, we need two separate if tests and cannot write
	//
	// if( rw->rw_nwaitwriters > 0 && rw->rw_refcount == 0 )
	// ...
	// 
	// could also omit the test of rw->rw_refcount, but that can result in calls to
	// pthread_cond_signal when read locks are still allocated, which is less efficient.
	//
	if (rw->rw_nwaitwriters > 0) {
		if (rw->rw_refcount == 0)
			result = pthread_cond_signal(&rw->rw_condwriters);		// signal single writer
	} else if (rw->rw_nwaitreaders > 0)
		result = pthread_cond_broadcast(&rw->rw_condreaders);		// signal all readers

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


{example}

#include	"unpipc.h"
#include	"pthread_rwlock.h"

pthread_rwlock_t	rwlock = PTHREAD_RWLOCK_INITIALIZER;

void	 *thread1(void *), *thread2(void *);
pthread_t	tid1, tid2;

int
main(int argc, char **argv)
{
	void	*status;
	Pthread_rwlock_init(&rwlock, NULL);

	Set_concurrency(2);
	Pthread_create(&tid1, NULL, thread1, NULL);
	sleep(1);		/* let thread1() get the lock */
	Pthread_create(&tid2, NULL, thread2, NULL);

	Pthread_join(tid2, &status);
	if (status != PTHREAD_CANCELED)
		printf("thread2 status = %p\n", status);

	Pthread_join(tid1, &status);
	if (status != NULL)
		printf("thread1 status = %p\n", status);

	printf("rw_refcount = %d, rw_nwaitreaders = %d, rw_nwaitwriters = %d\n",
		   rwlock.rw_refcount, rwlock.rw_nwaitreaders,
		   rwlock.rw_nwaitwriters);
	Pthread_rwlock_destroy(&rwlock);
	/* 4returns EBUSY error if cancelled thread does not cleanup */

	exit(0);
}

void *
thread1(void *arg)
{
	Pthread_rwlock_rdlock(&rwlock);
	printf("thread1() got a read lock\n");
	sleep(3);		/* let thread2 block in pthread_rwlock_wrlock() */
	pthread_cancel(tid2);
	sleep(3);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

void *
thread2(void *arg)
{
	printf("thread2() trying to obtain a write lock\n");
	Pthread_rwlock_wrlock(&rwlock);

	// followings are never get executed since it gets canceled.
	printf("thread2() got a write lock\n");
	sleep(1);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

When run this, the program is hung. The occurred steps are:

1. the second trys to get write lock and blocks on pthread_cond_wait.
2. the first returns from slepp(3) and cancel the second.

3. when the second is canceled while it is blocked in a condition variable wait, the mutex is
reacquired before calling the first cleanup hander (even if not installed any handers, but the mutex
is still reacquired before the thread is canceled.) Therefore, when the secondis canceled, it holds
the mutex lock for the read-write lock.

4. the first calls pthread_rwlock_unlock, but it blocks forever in its call to pthread_mutex_lock
because the mutex is still locked by the first thread that was canceled. [KT] this means cancel
terminates a thread immediately and do not continue the rest in pthread_rwlock_wrlock. Hence still
locked. 

If remove the call to pthread_rwlock_unlock in the thread1 func, the main will print:

rw_refcount = 1, rw_nwaitreaders = 0, rw_nwaitwriters = 1
pthread_rwlock_destroy error: Device busy

The correctio is simple and this is addition to pthread_rwlock_rdlock:

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		pthread_cleanup_push( rwlock_cancelrdwait, (void*)rw); ~
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelrdwait */
static void
rwlock_cancelrdwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitreaders--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelrdwait */ 

int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		pthread_cleanup_push(rwlock_cancelwrwait, (void *) rw); ~
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelwrwait */
static void
rwlock_cancelwrwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitwriters--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelwrwait */


==============================================================================
*kt_linux_core_230*	sync: file-lock

File locks: File locks are a synchronization method explicitly designed to coordinate the actions of
multiple processes operating [on-the-same-file]. They can also be used to coordinate access to other
shared resources. File locks come in two flavors: read (shared) locks and write (exclusive) locks.
Any number of processes can hold a read lock on the same file (or region of a file). However, when
one process holds a write lock on a file (or file region), other processes are prevented from
holding either read or write locks on that file (or file region). Linux provides file-locking
facilities via the flock() and fcntl() system calls. The flock() system call provides a simple
locking mechanism, allowing processes to place a shared or an exclusive lock on an entire file.
Because of its limited functionality, flock() locking facility is rarely used nowadays. The fcntl()
  system call provides record locking, allowing processes to place multiple read and write locks on
  different regions of the same file.



==============================================================================
*kt_linux_core_240* 	sync: common problems when use threads

the big three of threading problems are deadlock, races and starvation.

The simplest deadlock condition is when there are two threads and thread A can't progress until
thread B finishes, while thread B can't progress until thread A finishes. This is usually because
both need the same two resources to progress, A has one and B has the other. Various symmetry
breaking algorithms can prevent this in the two thread or larger circle cases.

Races happen when one thread changes the state of some resource when another thread is not expecting
it (such as changing the contents of a memory location when another thread is part way through
reading, or writing to that memory). Locking methods are the key here. (Some lock free methods
and containers are also good choices for this. As are atomic operations, or transaction
based operations.)

Starvation happens when a thread needs a resource to proceed, but can't get it. The resource is
constantly tied up by other threads and the one that needs it can't get in. The scheduling algorithm
is the problem when this happens. Look at algorithms that assure access.


==============================================================================
*kt_linux_core_250*	sync: reentrant and thread-safe

{thread-safe}
A function is said to be thread-safe if it can safely be invoked by multiple threads at the same
time; put conversely, if a function is not thread-safe, then we can’t call it from one thread while
it is being executed in another thread. The same example in {why-need-sync}.

<solution-one> serialization
There are various methods of rendering a function thread-safe. One way is to associate a mutex with
the function or perhaps with all of the functions in a library, if they all share the same global
variables, lock that mutex when the function is called, and unlock it when the mutex returns. This
approach has the virtue of simplicity. On the other hand, it means that only one thread at a time
can execute the function-we say that access to the function is [serialized]. 

The downside is that if the threads spend a significant amount of time executing this function, then
this serialization results in a [reduce-or-loss-of-concurrency], because the threads of a program
can no longer execute in parallel.

<solution-two> critial-section
A more sophisticated solution is to associate the mutex with a shared variable. We then determine
which parts of the function are critical sections that access the shared variable, and acquire and
release the mutex only during the execution of these critical sections. This allows multiple threads
to execute the function at the same time and to operate in parallel, except when more than one
thread needs to execute a critical section.


{non-thread-safe-functions}
To facilitate the development of threaded applications, all of the functions specified in SUSv3 are
required to be implemented in a thread-safe manner, except those listed in Table 31-1.

In ref-LPI, Table 31-1: Functions that SUSv3 does not require to be thread-safe


{reentrant-functions}
Although the use of critical sections to implement thread safety is a significant improvement over
the use of per-function mutexes, it is still somewhat inefficient because there is a cost to locking
and unlocking a mutex. A reentrant function achieves thread safety without the use of mutexes.

<how>
It does this by avoiding the use of global and static variables. Any information that must be
returned to the caller, or maintained between calls to the function, is stored in buffers allocated
by the caller. {Q} Although use a buffer from a caller, still seems to be a problem.

However, not all functions can be made reentrant. The usual reasons are the following:

o By their nature, some functions must access global data structures. The functions in the malloc
library provide a good example. These functions maintain a global linked list of free blocks on the
heap. The functions of the malloc library are made thread-safe through the use of mutexes.

o Some functions (defined before the invention of threads) have an interface that by definition is
nonreentrant, because they return pointers to storage statically allocated by the function, or they
employ static storage to maintain information between successive calls to the same (or a related)
function.

See {signal-reentrant} for more about reentrant.

[KT] In sum, reentrant is bigger notion than thread-safe.

<r_prefix>
For several of the functions that have nonreentrant interfaces, SUSv3 specifies reentrant
equivalents with names ending with the suffix _r. These functions require the caller to allocate a
buffer whose address is then passed to the function and used to return the result. This allows the
calling thread to use a local (stack) variable for the function result buffer.

For example, glibc provides crypt_r(), gethostbyname_r(), getservbyname_r(), getutent_r(),
getutid_r(), getutline_r(), and ptsname_r(). 

However, a portable application can't rely on these functions being present on other
implementations.


{code-reentrant-function}
The most efficient way of making a function thread-safe is to make it [reentrant]. All new library
functions should be implemented in this way. However, for an existing nonreentrant library function
(one that was perhaps designed before the use of threads became common), this approach usually
requires changing the function’s interface, which means modifying all of the programs that use the
function. 

<why-thread-specific-data>
Thread-specific data is a technique for making an existing function thread-safe without
changing its interface. A function that uses thread-specific data may be slightly less efficient
than a reentrant function, but allows us to leave the programs that call the function unchanged.

<strerror-manpage>
STRERROR(3)               Linux Programmer's Manual              STRERROR(3)

NAME
       strerror, strerror_r - return string describing error number

SYNOPSIS

       #include <string.h>

       char *strerror(int errnum);

       int strerror_r(int errnum, char *buf, size_t buflen);
                   /* XSI-compliant */

       char *strerror_r(int errnum, char *buf, size_t buflen);
                   /* GNU-specific */

   Feature Test Macro Requirements for glibc (see feature_test_macros(7)):

       The XSI-compliant version of strerror_r() is provided if:
       (_POSIX_C_SOURCE >= 200112L || _XOPEN_SOURCE >= 600) && ! _GNU_SOURCE
       Otherwise, the GNU-specific version is provided.

DESCRIPTION

The strerror() function returns a pointer to a string that describes the error code passed in the
argument errnum, possibly using the LC_MESSAGES part of the current locale to select the appropriate
language. (For example, if errnum is EINVAL, the returned description will "Invalid argument".)
This string must not be modified by the application, but may be modified by a subsequent call to
strerror(). No library function, including perror(3), will modify this string.

The strerror_r() function is similar to strerror(), but is thread safe. This function is available
in two versions: an XSI-compliant version specified in POSIX.1-2001 (available since glibc 2.3.4,
but not POSIX-compliant until glibc 2.13), and a GNU-specific version (available since glibc
2.0). The XSI-compliant version is provided with the feature test macros settings shown in
the SYNOPSIS; otherwise the GNU-specific version is provided. If no feature test macros are
explicitly defined, then (since glibc 2.4) _POSIX_SOURCE is defined by default with the value
200112L, so that the XSI- compliant version of strerror_r() is provided by default.

The XSI-compliant strerror_r() is preferred for portable applications. It returns the error string
in the user-supplied buffer buf of length buflen.

The GNU-specific strerror_r() returns a pointer to a string containing the error message. This may
be either a pointer to a string that the function stores in buf, or a pointer to some (immutable)
static string (in which case buf is unused). If the function stores a string in buf, then at most
buflen bytes are stored (the string may be truncated if buflen is too small and errnum is
unknown). The string always includes a terminating null byte ('\0').

RETURN VALUE

The strerror() and the GNU-specific strerror_r() functions return the appropriate error description
string, or an "Unknown error nnn" message if the error number is unknown.

POSIX.1-2001 and POSIX.1-2008 require that a successful call to strerror() shall leave errno
unchanged, and note that, since no function return value is reserved to indicate an error, an
application that wishes to check for errors should initialize errno to zero before the call, and
then check errno after the call.

The XSI-compliant strerror_r() function returns 0 on success. On error, a (positive) error number is
returned (since glibc 2.13), or -1 is returned and errno is set to indicate the error (glibc
versions before 2.13).

ERRORS

EINVAL The value of errnum is not a valid error number.
ERANGE Insufficient storage was supplied to contain the error description string.

ATTRIBUTES

Multithreading (see pthreads(7))
The strerror() function is not thread-safe.
The strerror_r() function is thread-safe.

CONFORMING TO

strerror() is specified by POSIX.1-2001, C89, C99.  strerror_r() is specified by POSIX.1-2001.

The GNU-specific strerror_r() function is a nonstandard extension.

POSIX.1-2001 permits strerror() to set errno if the call encounters an error, but does not specify
what value should be returned as the function result in the event of an error. On some systems,
strerror() returns NULL if the error number is unknown.  On other systems, strerror() returns a
string something like "Error nnn occurred" and sets errno to EINVAL if the error number is
unknown. C99 and POSIX.1-2008 require the return value to be non-NULL.

SEE ALSO
err(3), errno(3), error(3), perror(3), strsignal(3)

COLOPHON

This page is part of release 3.61 of the Linux man-pages project.  A description of the project, and
information about reporting bugs, can be found at http://www.kernel.org/doc/man-pages/.

2013-06-21                      STRERROR(3)

<POSIX-1-2001>
Beginning in 1999, the IEEE, The Open Group, and the ISO/IEC Joint Technical Committee 1
collaborated in the Austin Common Standards Revision Group with the aim of revising and
consolidating the POSIX standards and the Single UNIX Specification. This resulted in the
ratification of POSIX 1003.1-2001, sometimes just called POSIX.1-2001, in December 2001
(subsequently approved as an ISO standard, ISO/IEC 9945:2002).

<thread-specific-data>
1. The function creates a key, which is the means of differentiating the thread-specific data item
used by this function from the thread-specific data items used by other functions. The key is
created by calling the pthread_key_create() function. Creating a key needs to be done only once,
when the first thread calls the function. For this purpose, pthread_once() is employed.
Creating a key doesn’t allocate any blocks of thread-specific data.

2. The call to pthread_key_create() serves a second purpose: it allows the caller to specify the
address of the programmer-defined destructor function that is used to deallocate each of the storage
blocks allocated for this key (see the next step). When a thread that has thread-specific data
terminates, the Pthreads API automatically invokes the destructor, passing it a pointer to the data
block for this thread.

<typical-thread-specific-data-implementation>
This is a typical implementation (NPTL is typical). In this implementation, the pthread_key_t value
returned by pthread_key_create() is simply an index into the global array, which we label
pthread_keys

                  -----------------
pthread_keys[0]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
pthread_keys[1]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
pthread_keys[2]   | 'in use' flag |
                  | dest pointer  |
                  -----------------
                  ...

int pthread_setspecific(pthread_key_t key, const void *value);

So key is allocated for each client which is a library function, strerror, in this case and assumes
that this is keys[1]. The data structure for value or buffer for each thread is:

All correspond to                   thread A
pthread_keys[1]    value of key1    tsd[0] | pointer |
                   for thread A  -> tsd[1] | pointer |  -> TSD buffer for strerror in thread A
                                    tsd[2] | pointer |                                         

                                    thread B
                   value of key1    tsd[0] | pointer |
                   for thread B  -> tsd[1] | pointer |  -> TSD buffer for strerror in thread B
                                    tsd[2] | pointer |                                        

                   ...

The TDS buffer is allocated in strerror for each thread and when each thread calls 

void *pthread_getspecific(pthread_key_t key);

The pthread library know the thread and use the key to pick up the entry and the buffer pointer.
When a thread is first created, all of its thread-specific data pointers are initialized to NULL.
This means that when our library function is called by a thread for the first time, it must begin by
using pthread_getspecific() to check whether the thread already has an associated value for key.

[KT] This means that when pthread_key_create is called, it allocates a entry in keys array and also
TSD array for each thread even if some thread is not using strerror.

<strerror-non-thread-safe-version>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#define MAX_ERROR_LEN 256 /* Maximum length of string
                             returned by strerror() */
static char buf[MAX_ERROR_LEN]; /* Statically allocated return buffer */
  char *
strerror(int err)
{
  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1);
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }
  return buf;
}

<user-code>
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#include "tlpi_hdr.h"

static void *
threadFunc(void *arg)
{
  char *str;
  printf("Other thread about to call strerror()\n");
  str = strerror(EPERM);
  printf("Other thread: str (%p) = %s\n", str, str);
  return NULL;
}

int
main(int argc, char *argv[])
{
  pthread_t t;
  int s;
  char *str;
  str = strerror(EINVAL);
  printf("Main thread has called strerror()\n");

  s = pthread_create(&t, NULL, threadFunc, NULL);
  if (s != 0)
    errExitEN(s, "pthread_create");

  s = pthread_join(t, NULL);
  if (s != 0)
    errExitEN(s, "pthread_join");

  printf("Main thread: str (%p) = %s\n", str, str);

  exit(EXIT_SUCCESS);
}

<strerror-thread-safe-version>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#include "tlpi_hdr.h"

static pthread_once_t once = PTHREAD_ONCE_INIT;
static pthread_key_t strerrorKey;

#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread
                             buffer returned by strerror() */

static void /* Free thread-specific data buffer */
q destructor(void *buf)
{
  free(buf);
}

static void /* One-time key creation function */
createKey(void) <2>
{
  int s;
  /* Allocate a unique thread-specific data key and save the address
     of the destructor for thread-specific data buffers */
  s = pthread_key_create(&strerrorKey, destructor); <1>
  if (s != 0)
    errExitEN(s, "pthread_key_create");
}

char *
strerror(int err)
{
  int s;
  char *buf;

  /* Make first caller allocate key for thread-specific data */
  s = pthread_once(&once, createKey); <2>
  if (s != 0)
    errExitEN(s, "pthread_once");

  buf = pthread_getspecific(strerrorKey); <3>
  if (buf == NULL) { /* If first call from this thread, allocate
                        buffer for thread, and save its location */
    buf = malloc(MAX_ERROR_LEN);
    if (buf == NULL)
      errExit("malloc");

    s = pthread_setspecific(strerrorKey, buf); <4>
    if (s != 0)
      errExitEN(s, "pthread_setspecific");
  }

  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); <4>
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }

  return buf;
}

<limitation>
SUSv3 requires that an implementation support at least 128 (_POSIX_THREAD_KEYS_MAX) keys. An
application can determine how many keys an implementation actually supports either via the
definition of PTHREAD_KEYS_MAX (defined in <limits.h>) or by calling sysconf(_SC_THREAD_KEYS_MAX).
Linux supports up to 1024 keys.

Even 128 keys should be more than sufficient for most applications. This is because each library
function should employ only a small number of keys—often just one. If a function requires multiple
thread-specific data values, these can usually be placed in a single structure that has just one
associated thread-specific data key.


{thread-local-storage}
Like thread-specific data, thread-local storage provides persistent per-thread storage. This
feature is [nonstandard], but it is provided in the same or a similar form on many other UNIX
implementations.

Thread-local storage requires support from the kernel (provided in Linux 2.6), the Pthreads
implementation (provided in NPTL), and the C compiler (provided on x86-32 with gcc 3.3 and later).

The main advantage of thread-local storage is that it is much simpler to use than thread-specific
data. To create a thread-local variable, we simply include the __thread specifier in the declaration
of a global or static variable:

static __thread buf[MAX_ERROR_LEN];

Each thread has its own copy of the variables declared with this specifier. The variables
in a thread’s thread-local storage persist until the thread terminates, at which
time the storage is automatically deallocated.

The following points about the declaration and use of thread-local variables:

o The __thread keyword must immediately follow the static or extern keyword, if either of these is
specified in the variable’s declaration.

o The declaration of a thread-local variable can include an initializer, in the same manner as a
normal global or static variable declaration.

o The C address (&) operator can be used to obtain the address of a thread-local variable.

<revised-strerror-using-tls>
#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist'
                       declarations from <stdio.h> */
#include <stdio.h>
#include <string.h> /* Get declaration of strerror() */
#include <pthread.h>
#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread
                             buffer returned by strerror() */
static __thread char buf[MAX_ERROR_LEN];
/* Thread-local return buffer */
char *
strerror(int err)
{
  if (err < 0 || err >= _sys_nerr || _sys_errlist[err] == NULL) {
    snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err);
  } else {
    strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1);
    buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */
  }
  return buf;
}


==============================================================================
*kt_linux_core_260*	sync: atomic operations

{atomic-operations}

For full articles:
http://www.informit.com/guides/content.aspx?g=cplusplus&seqNum=469

Atomicity

An atomic operation is a sequence of one or more machine instructions that are executed
sequentially, without interruption. By default, any sequence of two or more machine instructions
isn't atomic since the operating system may suspend the execution of the current sequence of
operations in favor of another task. If you want to ensure that a sequence of operations is atomic
you must use some form of locking or other types of synchronization. 

Without that, the only guarantee you have is that a single machine instruction is always atomic. the
CPU will not interrupt a single instruction in the middle. [KT] Not entirely true. 

We can conclude from that minimal guarantee that if you can prove that your compiler translates a
certain C++ statement into a single machine instruction, that C++ statement is naturally atomic
meaning, the programmer doesn't have to use explicit locking to enforce the atomic execution of that
statement.  

Which C++ Statements are Naturally Atomic?

Obviously, there are very few universal rules of thumb because each hardware architecture might
translate the same C++ statement differently. Many textbooks tell you that the unary ++ and --
operators, when applied to integral types and pointers, are guaranteed to be atomic. Historically,
when Dennis Ritchie and Brian Kernighan designed C, they added these operators to the language
because they wanted to take advantage of the fast INC (increment) assembly directive that many
machines supported. However, there is no guarantee in the C or C++ standards that these operators
shall be atomic. Ritchie and Kernighan were more concerned about speed rather than atomicity.

You shouldn't make assumptions about the atomicity of C++ statements without examining the output of
your compiler. In some cases, you might discover that what appears to be a single C++ statement is
in fact translated into a long and complex set of machine instructions. 


Epilog

The multithreading support of C++0x consists of a thread class as well as a standard atomics library
that guarantees the atomicity of logical and arithmetic operations. I will introduce the C++0x
atomics library in a separate column.


From C++11:

Data-dependency ordering: atomics and memory model 	N2664 	GCC 4.4
(memory_order_consume)


From StackOverflow:

The increment-memory machine instruction on an X86 is atomic only if you use it with a LOCK prefix.

x++ in C and C++ doesn't have atomic behavior. If you do unlocked increments, due to races in which
processor is reading and writing X, if two separate processors attempt an increment, you can end up
with just one increment or both being seen (the second processor may have read the initial value,
incremented it, and written it back after the first writes its results back).

I believe that C++11 offers atomic increments, and most vendor compilers have an idiomatic way to
cause an atomic increment of certain built-in integer types (typically int and long); see your
compiler reference manual.

If you want to increment a "large value" (say, a multiprecision integer), you need to do so with
using some standard locking mechanism such as a semaphore.

Note that you need to worry about atomic reads, too. On the x86, reading a 32 or 64 bit value
happens to be atomic if it is 64-bit word aligned. That won't be true of a "large value"; again
you'll need some standard lock.

{rmw-operations}
The RMW(read-modify-write) is:

<from-wikipedia>
A class of atomic operations such as test-and-set, fetch-and-add, and compare-and-swap which both
read a memory location and write a new value into it simultaneously, either with a completely new
value or some function of the previous value. These operations prevent race conditions in
multi-threaded applications. Typically they are used to implement mutexes or semaphores. These
atomic operations are also heavily used in non-blocking synchronization.

{atomic-non-atomic-operations}
http://preshing.com/20130618/atomic-vs-non-atomic-operations/

Much has already been written about atomic operations on the web, usually with a focus on atomic
read-modify-write (RMW) operations. However, those aren’t the only kinds of atomic operations. There
are also atomic loads and stores, which are equally important. In this post, I’ll compare atomic
loads and stores to their non-atomic counterparts at both the processor level and the C/C++ language
level. Along the way, we’ll clarify the C++11 concept of a “data race”.

Automic operations: automic loads and stores + automic read-modify-write operations

An operation acting on shared memory is atomic if it completes in a single step relative to other
threads. When an atomic store is performed on a shared variable, no other thread can observe the
modification half-complete. When an atomic load is performed on a shared variable, it reads the
entire value as it appeared at a single moment in time. Non-atomic loads and stores do not make
those guarantees.

Without those guarantees, lock-free programming would be impossible, since you could never let
different threads manipulate a shared variable at the same time. We can formulate it as a rule:

Any time two threads operate on a shared variable concurrently, and one of those operations performs
a write, both threads must use atomic operations.

If you violate this rule, and either thread uses a non-atomic operation, you’ll have what the C++11
standard refers to as a [data-race] (not to be confused with Java’s concept of a data race, which is
different, or the more general race condition). [Q] what is the general race condition?

The C++11 standard doesn’t tell you why data races are bad; only that if you have one, “undefined
behavior” will result (section 1.10.21). The real reason why such data races are bad is actually
quite simple: They result in [torn-reads] and [torn-writes].

A memory operation can be non-atomic because it uses multiple CPU instructions, non-atomic even when
using a single CPU instruction, or non-atomic because you’re writing portable code and you simply
can’t make the assumption. Let’s look at a few examples.


<Non-Atomic Due to Multiple CPU Instructions>

Suppose you have a 64-bit global variable, initially zero.

uint64_t sharedValue = 0;

At some point, you assign a 64-bit value to this variable.

void storeValue()
{
    sharedValue = 0x100000002;
}

When you compile this function for 32-bit x86 using GCC, it generates the following machine code.

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	DWORD PTR sharedValue, 2
        mov	DWORD PTR sharedValue+4, 1
        ret
        ...

As you can see, the compiler implemented the 64-bit assignment using two separate machine
instructions. The first instruction sets the lower 32 bits to 0x00000002, and the second sets the
upper 32 bits to 0x00000001. Clearly, this assignment operation is not atomic. If sharedValue is
accessed concurrently by different threads, several things can now go wrong:

- If a thread calling storeValue is preempted between the two machine instructions, it will leave
the value of 0x0000000000000002 in memory – a torn write. At this point, if another thread reads
sharedValue, it will receive this completely bogus value which nobody intended to store.

- Even worse, if a thread is preempted between the two instructions, and another thread modifies
sharedValue before the first thread resumes, it will result in a permanently torn write: the upper
32 bits from one thread, the lower 32 bits from another.

- On multicore devices, it isn’t even necessary to preempt one of the threads to have a torn write.
When a thread calls storeValue, any thread executing on a different core could read sharedValue at a
moment when only half the change is visible.

Reading concurrently from sharedValue brings its own set of problems:

uint64_t loadValue()
{
    return sharedValue;
}

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	eax, DWORD PTR sharedValue
        mov	edx, DWORD PTR sharedValue+4
        ret
        ...

Here too, the compiler has implemented the load operation using two machine instructions: The first
reads the lower 32 bits into eax, and the second reads the upper 32 bits into edx. In this case, if
a concurrent store to sharedValue becomes visible between the two instructions, it will result in a
torn read – even if the concurrent store was atomic.

These problems are not just theoretical. Mintomic’s test suite includes a test case called
test_load_store_64_fail, in which one thread stores a bunch of 64-bit values to a single variable
using a plain assignment operator, while another thread repeatedly performs a plain load from the
same variable, validating each result. On a multicore x86, this test fails consistently, as
expected.


<Non-Atomic in a single CPU Instructions>

A memory operation can be non-atomic even when performed by a single CPU instruction. For example,
  the ARMv7 instruction set includes the strd instruction, which stores the contents of two 32-bit
  source registers to a single 64-bit value in memory.

strd r0, r1, [r2]

On some ARMv7 processors, this instruction is not atomic. When the processor sees this instruction,
	it actually performs [two-separate-32-bit-stores] under the hood (section A3.5.3). Once again,
	another thread running on a separate core has the possibility of observing a torn write.
	Interestingly, a torn write is even possible on a single-core device: A system interrupt – say,
	for a scheduled thread context switch – can actually occur between the two internal 32-bit
	stores! In this case, when the thread resumes from the interrupt, it will restart the strd
	instruction all over again.

As another example, it’s well-known that on x86, a 32-bit mov instruction is atomic if the memory
operand is naturally aligned, but non-atomic otherwise. In other words, atomicity is [only]
guaranteed when the 32-bit integer is located at an address which is an exact multiple of 4.


<All C/C++ Operations Are Presumed Non-Atomic>

In C and C++, every operation is presumed non-atomic unless otherwise specified by the compiler or
hardware vendor – even plain 32-bit integer assignment.

uint32_t foo = 0;

void storeFoo()
{
    foo = 0x80286;
}

The language standards have nothing to say about atomicity in this case. Maybe integer assignment is
atomic, maybe it isn’t. Since non-atomic operations don’t make any guarantees, plain integer
assignment in C is non-atomic by definition.

In practice, we usually know more about our target platforms than that. For example, it’s common
knowledge that on all modern x86, x64, Itanium, SPARC, ARM and PowerPC processors, plain 32-bit
integer assignment is atomic [as-long-as] the target variable is naturally aligned. You can verify it
by consulting your processor manual and/or compiler documentation. In the games industry, I can tell
you that a lot of 32-bit integer assignments rely on this particular guarantee.

Nonetheless, when writing truly portable C and C++, there’s a long-standing tradition of pretending
that we don’t know anything more than what the language standards tell us. Portable C and C++ is
designed to run on every possible computing device past, present and imaginary. Personally, I like
to imagine a machine where memory can only be changed by mixing it up first:

On such a machine, you definitely wouldn’t want to perform a concurrent read at the same time as a
plain assignment; you could end up reading a completely random value.

<CPP11>
In C++11, there is finally a way to perform truly portable atomic loads and stores: the C++11 atomic
library. Atomic loads and stores performed using the C++11 atomic library would even work on the
imaginary computer above - even if it means the C++11 atomic library must secretly [lock] a mutex to
make each operation atomic. There’s also the Mintomic library which I released last month, which
doesn’t support as many platforms, but works on several older compilers, is hand-optimized and is
guaranteed to be lock-free.


Relaxed Atomic Operations

Let’s return to the original sharedValue example from earlier in this post. We’ll rewrite it using
Mintomic so that all operations are performed atomically on every platform Mintomic supports. First,
we must declare sharedValue as one of Mintomic’s atomic data types.

#include <mintomic/mintomic.h>

mint_atomic64_t sharedValue = { 0 };

The mint_atomic64_t type guarantees correct memory alignment for atomic access on each platform.
This is important because, for example, the GCC 4.2 compiler for ARM bundled with Xcode 3.2.5
doesn’t guarantee that plain uint64_t will be 8-byte aligned.

In storeValue, instead of performing a plain, non-atomic assignment, we must call
mint_store_64_relaxed.

void storeValue()
{
    mint_store_64_relaxed(&sharedValue, 0x100000002);
}

Similarly, in loadValue, we call mint_load_64_relaxed.

uint64_t loadValue()
{
    return mint_load_64_relaxed(&sharedValue);
}

Using C++11’s terminology, these functions are now data race-free. When executing concurrently,
		there is absolutely no possibility of a torn read or write, whether the code runs on
		ARMv6/ARMv7 (Thumb or ARM mode), x86, x64 or PowerPC. If you’re curious how
		mint_load_64_relaxed and mint_store_64_relaxed actually work, both functions expand to an
		inline cmpxchg8b instruction on x86; for other platforms, consult Mintomic’s implementation.

Here’s the exact same thing written in C++11 instead:

#include <atomic>

std::atomic<uint64_t> sharedValue(0);

void storeValue()
{
    sharedValue.store(0x100000002, std::memory_order_relaxed);
}

uint64_t loadValue()
{
    return sharedValue.load(std::memory_order_relaxed);
}

You’ll notice that both the Mintomic and C++11 examples use relaxed atomics, as evidenced by the
_relaxed suffix on various identifiers. The _relaxed suffix is a reminder that, just as with plain
loads and stores, no guarantees are made about memory ordering.

The only difference between a relaxed atomic load (or store) and a non-atomic load (or store) is
that relaxed atomics guarantee atomicity. No other difference is guaranteed.

In particular, it is still legal for the memory effects of a relaxed atomic operation to be
reordered with respect to instructions which follow or precede it in program order, either due to
compiler reordering or memory reordering on the processor itself. The compiler could even perform
optimizations on redundant relaxed atomic operations, just as with non-atomic operations. In all
cases, the operation remains atomic.

When manipulating shared memory concurrently, I think it’s good practice to always use Mintomic or
C++11 atomic library functions, even in cases where you know that a plain load or store would
already be atomic on your target platform. An atomic library function serves as a reminder that
elsewhere, the variable is the target of concurrent data access.

Hopefully, it’s now a bit more clear why the World’s Simplest Lock-Free Hash Table uses Mintomic
library functions to manipulate shared memory concurrently from different threads.


{lock-free-programming}
http://preshing.com/20120612/an-introduction-to-lock-free-programming/

An Introduction to Lock-Free Programming

Lock-free programming is a challenge, not just because of the complexity of the task itself, but
because of how difficult it can be to penetrate the subject in the first place.

I was fortunate in that my first introduction to lock-free (also known as lockless) programming was
Bruce Dawson’s excellent and comprehensive white paper, Lockless Programming Considerations. And
like many, I’ve had the occasion to put Bruce’s advice into practice developing and debugging
lock-free code on platforms such as the Xbox 360.

Since then, a lot of good material has been written, ranging from abstract theory and proofs of
correctness to practical examples and hardware details. I’ll leave a list of references in the
footnotes. At times, the information in one source may appear orthogonal to other sources: For
instance, some material assumes sequential consistency, and thus sidesteps the memory ordering
issues which typically plague lock-free C/C++ code. The new C++11 atomic library standard throws
another wrench into the works, challenging the way many of us express lock-free algorithms.

In this post, I’d like to re-introduce lock-free programming, first by defining it, then by
distilling most of the information down to a few key concepts. I’ll show how those concepts relate
to one another using flowcharts, then we’ll dip our toes into the details a little bit. At a
minimum, any programmer who dives into lock-free programming should already understand how to write
correct multithreaded code using mutexes, and other high-level synchronization objects such as
semaphores and events.  

What Is It?

People often describe lock-free programming as programming without mutexes, which are also referred
to as [locks]. That’s true, but it’s only [part] of the story. The generally accepted definition, based
on academic literature, is a bit more broad. At its essence, lock-free is a property used to
describe some code, without saying too much about how that code was actually written.

Basically, if some part of your program satisfies the following conditions, then that part can
rightfully be considered lock-free. Conversely, if a given part of your code doesn’t satisfy these
conditions, then that part is not lock-free.

<definition>
(This was a flow chart)

Are you programming with multiple threads? or interrupt, signal handlers, etc?
-> Yes

Do the threads access shared memeory?
-> Yes

Can the threads block each other? ie. is there some way to schedule the threads which would
'lock-up' indefinitely?
-> No

It is lock-free programming.

In this sense, the lock in lock-free [does not refer directly to mutexes], but rather to the
possibility of “locking up” the entire application in some way, whether it’s deadlock, livelock – or
even due to hypothetical thread scheduling decisions made by your worst enemy. That last point
sounds funny, but it’s key. Shared mutexes are ruled out trivially, because as soon as one thread
obtains the mutex, your worst enemy could simply never schedule that thread again. Of course, real
operating systems don’t work that way - we’re merely defining terms.

Here’s a simple example of an operation which contains no mutexes, but is still not lock-free.
Initially, X = 0. As an exercise for the reader, consider how two threads could be scheduled in a
way such that neither thread exits the loop.

while (X == 0)
{
    X = 1 - X;
}

Nobody expects a large application to be entirely lock-free. Typically, we identify a [specific-set]
of lock-free operations out of the whole codebase. For example, in a lock-free queue, there might be
a handful of lock-free operations such as push, pop, perhaps isEmpty, and so on.

Herlihy & Shavit, authors of The Art of Multiprocessor Programming, tend to express such operations
as class methods, and offer the following succinct definition of lock-free (see slide 150): 
	
"In an infinite execution, infinitely often some method call finishes." In other words, as long as
the program is able to keep calling those lock-free operations, the number of completed calls keeps
increasing, no matter what. It is algorithmically impossible for the system to lock up during those
operations. {Q} Didn't get that.

<why-use>
One important consequence of lock-free programming is that if you suspend a single thread, it will
never prevent other threads from making progress, as a group, through their own lock-free
operations. This hints at the value of lock-free programming when writing interrupt handlers and
real-time systems, where certain tasks must complete within a certain time limit, no matter what
state the rest of the program is in.

A final precision: Operations that are designed to block do not disqualify the algorithm. For
example, a queue’s pop operation may intentionally block when the queue is empty. The remaining
codepaths can still be considered lock-free.

<TODO> there is more in this page.


==============================================================================
*kt_linux_core_261*	sync: ref: locks aren't slow; lock contention is

<contention-and-frequency>
For example, this post measures the performance of a lock under heavy conditions: each thread must
hold the lock to do any work (high contention), and the lock is held for an extremely short interval
of time (high frequency)

But don’t disregard locks yet. One good example of a place where locks perform admirably, in real
software, is when protecting the memory allocator. Doug Lea’s Malloc is a popular memory allocator
in video game development, but it’s single threaded, so we need to protect it using a lock. During
gameplay, it’s not uncommon to see multiple threads hammering the memory allocator, say around 15000
times per second. While loading, this figure can climb to 100000 times per second or more. It’s not
a big problem, though. As you’ll see, locks handle the workload like a champ.
{Q} what's this memory allocator?

Lock Contention Benchmark

In this test, we spawn a thread which generates random numbers, using a custom Mersenne Twister
implementation. For example, suppose we want to acquire the lock 15000 times per second, and keep it
held 50% of the time. (whole working time) 

This is code example to show 50% lock:

QueryPerformanceCounter(&start);
for (;;)
{
  // Do some work without holding the lock
  workunits = (int) (random.poissonInterval(averageUnlockedCount) + 0.5f);
  for (int i = 1; i < workunits; i++)
    random.integer();       // Do one work unit
  workDone += workunits;

  QueryPerformanceCounter(&end);
  elapsedTime = (end.QuadPart - start.QuadPart) * ooFreq;
  if (elapsedTime >= timeLimit)
    break;

  // Do some work while holding the lock
  EnterCriticalSection(&criticalSection);
  workunits = (int) (random.poissonInterval(averageLockedCount) + 0.5f);
  for (int i = 1; i < workunits; i++)
    random.integer();       // Do one work unit
  workDone += workunits;
  LeaveCriticalSection(&criticalSection);

  QueryPerformanceCounter(&end);
  elapsedTime = (end.QuadPart - start.QuadPart) * ooFreq;
  if (elapsedTime >= timeLimit)
    break;
}

Now suppose we launch two such threads, each running on a different core. Each thread will hold the
lock during 50% of the time when it can perform work, but if one thread tries to acquire the lock
while the other thread is holding it, it will be forced to wait. This is known as lock contention.

(Tested on dual core) When we run the above scenario, we find that each thread spends roughly 25% of
its time waiting, and 75% of its time doing actual work. Together, both threads achieve a net
performance of 1.5x compared to the single-threaded case.

I ran several variations of the test on a 2.66 GHz quad-core Xeon, from 1 thread, 2 threads, all the
way up to 4 threads, each running on its own core. I also varied the duration of the lock, from the
trivial case where the the lock is never held, all the way up to the maximum where each thread must
hold the lock for 100% of its workload. In all cases, the lock frequency remained constant – threads
acquired the lock 15000 times for each second of work performed.

<KT>
The graph shows that go up to 4x when 0% lock duration and down below 1x when 100% lock duration. 0%
means that there is no sharing between threads hence no lock is needed.

The results were interesting. For short lock durations, up to say 10%, the system achieved very high
parallelism. Not perfect parallelism, but close. Locks are fast!

To put the results in perspective, I analyzed the memory allocator lock in a multithreaded game
engine using this profiler. During gameplay, with 15000 locks per second coming from 3 threads, the
lock duration was in the neighborhood of just 2%. That’s well within the comfort zone on the left
side of the diagram.

These results also show that once the lock duration passes 90%, there’s no point using multiple
threads anymore. A single thread performs better. Most surprising is the way the performance of 4
threads drops off a cliff around the 60% mark! This looked like an anomaly, so I re-ran the tests
several additional times, even trying a different testing order. The same behavior happened
consistently. My best hypothesis is that the experiment hits some kind of snag in the Windows
scheduler, but I didn’t investigate further.

Lock Frequency Benchmark

Even a lightweight mutex has overhead. As my next post shows, a pair of lock/unlock operations on a
Windows Critical Section takes about 23.5 ns on the CPU used in these tests. Therefore, 15000 locks
per second is low enough that lock overhead does not significantly impact the results. But what
happens as we turn up the dial on lock frequency?

The algorithm offers very fine control over the amount of work performed between one lock and the
next, so I performed a new batch of tests using smaller amounts: from a very fine-grained 10 ns
between locks, all the way up to 31 μs, which corresponds to roughly 32000 acquires per second. Each
test used exactly two threads:

As you might expect, for very high lock frequencies, the overhead of the lock itself begins to dwarf
the actual work being done. Several benchmarks you’ll find online, including the one linked earlier
fall into the bottom-right corner of this chart. At such frequencies, you’re talking about some
seriously short lock times – on the scale of a few CPU instructions. The good news is that, when the
work between locks is that simple, a lock-free implementation is more likely to be feasible.

At the same time, the results show that locking up to 320000 times per second (3.1 μs between
    successive locks) is not unreasonable. In game development, the memory allocator may flirt with
this frequency during load times. You can still achieve more than 1.5x parallelism if the lock
duration is short.

We’ve now seen a wide spectrum of lock performance: cases where it performs great, and cases where
the application slows to a crawl. I’ve argued that the lock around the memory allocator in a game
engine will often achieve excellent performance. Given this example from the real world, it cannot
be said that all locks are slow. Admittedly, it’s very easy to abuse locks, but one shouldn’t live
in too much fear – any resulting bottlenecks will show up during careful profiling. When you
consider how reliable locks are, and the relative ease of understanding them (compared to lock-free
    techniques), locks are actually pretty awesome sometimes.

The goal of this post was to give locks a little respect where deserved - corrections are welcome. I
also realize that locks are used in a wide variety of industries and applications, and it may not
always be so easy to strike a good balance in lock performance. If you’ve found that to be the case
in your own experience, I would love to hear from you in the comments.

{DN}
Then the bottomlien is that try to minimize the lock contention rather than try to find out which
one is fast. Of course, should use better one but the lock contention is far more important.


==============================================================================
*kt_linux_core_262*	sync: ref: always use a lightweight mutex {mutex-vs-semaphore}

http://preshing.com/20111124/always-use-a-lightweight-mutex/
Always Use a Lightweight Mutex

Have started wondering since seen this article which says that critical section is faster than mutex
in windows. This critical section is called as lightweight mutex which is equivalent to pthread
mutex in Linux. Does it mean that pthread mutex is faster than posix semaphore? Tried the same
approach that this article had and it seems not. Like ref-LPI, there is no big difference.

This is the result ran on the real Linux server machine which has multiple processors and as can
see, semaphore is slightly slower than using a mutex but not significant. This seems different from
what this article said.

<snippet>
Now, suppose you have a thread which acquires a Critical Section 100000 times per second, and there
are no other threads competing for the lock. Based on the above figures, you can expect to pay
between 0.2% and 0.6% in lock overhead. Not too bad! At lower frequencies, the overhead becomes
negligible.

<KT> Here shows the result in the graphic which is 58.7ns on Core 2 Duo and 23.5ns on Xeon for
windows critical section. Think that Core 2 means dual core and Xeon is single so means that there
are lock contention for Core 2 and not for Xeon. That's why he mean that there is 0.6% overhead for
locking. Then it's more about experimenting of mutl-core.

Naturally, Ubuntu 11.10 provides a lock implementation using the POSIX Threads API as well. It’s
another lightweight mutex, based on a Linux-specific construct known as a futex. A pair of
pthread_mutex_lock/pthread_mutex_unlock calls takes about 66 ns on my Core 2 Duo. You can even share
this implementation between processes, but I didn’t test that.

In my previous post, I argued against the misconception that locks are slow and provided some data
to support the argument. At this point, it should be clear that if you aren’t using a lightweight
mutex, the entire argument goes out the window. I’m fairly sure that the existence of heavy lock
implementations has only added to this misconception over the years.

Some of you old-timers may point out ancient platforms where a heavy lock was the only
implementation available, or when a semaphore had to be used for the job. But it seems all modern
platforms offer a lightweight mutex. And even if they didn’t, you could write your own lightweight
mutex at the application level, even sharing it between processes, provided you’re willing to live
with certain caveats. You’ll find one example in my followup post, Roll Your Own Lightweight Mutex.


{when-with-no-threads}
keitee.park@magnum ~
$ ./sem
sem run
56:645
56:702   [57]
keitee.park@magnum ~
$ ./mtx
mtx run
3:710
3:773    [63]

keitee.park@magnum ~
$ ./sem
sem run
5:663
5:719    [56]
keitee.park@magnum ~
$ ./mtx
mtx run
6:894
6:957    [63]

keitee.park@magnum ~
$ ./sem
sem run
8:103
8:160    [57]
keitee.park@magnum ~
$ ./mtx
mtx run
9:294
9:356    [62]

<for-semaphore-example>
#include <stdio.h>
#include <time.h>
#include <fcntl.h>
#include <semaphore.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

  unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

int main()
{
  int s = 0, loop = 0;
  int flags, opt;
  mode_t perms;
  unsigned int value;
  sem_t *sem;

  flags = O_CREAT | O_EXCL;
  perms = S_IRUSR | S_IWUSR;

  fprintf( stderr, "sem run\n");

  // will create /dev/shm/sem.demo
  sem = sem_open("/demo", flags, perms, 1 );
  if(sem == SEM_FAILED)
    fprintf( stderr, "fail on sem_open");

  get_time_ms();

  for(loop = 0; loop < MAXLOOP; loop++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "fail on sem_wait");

    s = sem_post(sem);
    if (s != 0)
    fprintf(stderr, "fail on sem_post");
  }

  get_time_ms();

  s = sem_unlink("/demo");
  if(s != 0)
    fprintf(stderr, "fail on sem_unlink\n" );

}

<for-mutex-example>
#include <stdio.h>
#include <time.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

  unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;

int main()
{
  int s = 0, loop = 0;

  fprintf( stderr, "mtx run\n");

  get_time_ms();

  for(loop = 0; loop < MAXLOOP; loop++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "pthread_mutex_unlock");
  }

  get_time_ms();
}


{when-with-two-threads}
keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
14:946
15:370      [424]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
24:498
24:829      [331]
main: this is the end

keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
33:705
34:207      [502]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
35:993
36:460      [467]
main: this is the end

keitee.park@magnum ~
$ ./sem_two 
main: this is the second sem run
2:673
3:125       [452]
main: this is the end
keitee.park@magnum ~
$ ./mtx_two 
main: this is the second mtx run
4:792
5:224       [432]
main: this is the end

<for-semaphore-example>
#include <stdio.h>
#include <time.h>
#include <fcntl.h>
#include <semaphore.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

	unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static sem_t *sem;
static unsigned int loopA;
static unsigned int loopB;

static void* threadFuncA( void *arg )
{
  int s = 0;

  for(loopA = 0; loopA < MAXLOOP; loopA++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "TA: fail on sem_wait\n");

    s = sem_post(sem);
    if (s != 0)
    fprintf( stderr, "TA: fail on sem_post\n");
  }
}

static void* threadFuncB( void *arg )
{
  int s = 0;

  for(loopB = 0; loopB < MAXLOOP; loopB++)
  {
    s = sem_wait(sem);
    if (s != 0)
    fprintf( stderr, "TB: fail on sem_wait\n");

    s = sem_post(sem);
    if (s != 0)
    fprintf( stderr, "TB: fail on sem_post\n");
  }
}

int main()
{
  int flags, opt;
  mode_t perms;
  pthread_t tA, tB;
  int s;

  flags = O_CREAT | O_EXCL;
  perms = S_IRUSR | S_IWUSR;

  // will create /dev/shm/sem.demo
  sem = sem_open("/demo", flags, perms, 1 );
  if(sem == SEM_FAILED)
    fprintf( stderr, "main: fail on sem_open\n");

  fprintf( stderr, "main: this is the second sem run\n");
  get_time_ms();

  s = pthread_create( &tA, NULL, threadFuncA, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(A)\n");

  s = pthread_create( &tB, NULL, threadFuncB, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(B)\n");

  s = pthread_join( tA, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(A)\n");

  s = pthread_join( tB, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(B)\n");

  get_time_ms();
  fprintf( stderr, "main: this is the end\n");

  s = sem_unlink("/demo");
  if(s != 0)
    fprintf(stderr, "main: fail on sem_unlink\n" );
}

<for-mutex-example>
#include <stdio.h>
#include <time.h>
#include <pthread.h>

static unsigned int get_time_ms()
{
  struct timespec time = {0, 0};
  unsigned int today = 0U;

	unsigned int hour = 0, minute = 0, sec = 0, msec = 0;

  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  today = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  today += time.tv_nsec / 1000000;

  //
  hour = (today)/3600000; 				// hour in ms
  msec = (today - (hour*3600000));	// ms remains 
  minute = msec / 60000; 				// mins 
  msec = msec - (minute * 60000);	// ms remains
  sec = msec / 1000; 					// secs
  msec = msec - (sec * 1000); 		// ms remains

  fprintf(stderr, "%d:%d\n", sec, msec ); 

  return msec;
}

#define MAXLOOP 1000000

static pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;
static unsigned int loopA;
static unsigned int loopB;

static void* threadFuncA( void *arg )
{
  int s = 0;

  for(loopA = 0; loopA < MAXLOOP; loopA++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "TA: pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "TA: pthread_mutex_unlock");
  }
}

static void* threadFuncB( void *arg )
{
  int s = 0;

  for(loopB = 0; loopB < MAXLOOP; loopB++)
  {
    s = pthread_mutex_lock(&mtx);
    if (s != 0)
    fprintf( stderr, "TB: pthread_mutex_lock");

    s = pthread_mutex_unlock(&mtx);
    if (s != 0)
    fprintf(stderr, "TB: pthread_mutex_unlock");
  }
}

int main()
{
  pthread_t tA, tB;
  int s;

  fprintf( stderr, "main: this is the second mtx run\n");
  get_time_ms();

  s = pthread_create( &tA, NULL, threadFuncA, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(A)\n");

  s = pthread_create( &tB, NULL, threadFuncB, NULL );
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_create(B)\n");

  s = pthread_join( tA, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(A)\n");

  s = pthread_join( tB, NULL);
  if (s != 0)
  fprintf( stderr, "main: fail on pthread_join(B)\n");

  get_time_ms();
  fprintf( stderr, "main: this is the end\n");
}


==============================================================================
*kt_linux_core_263*	sync: ref: lock-free code: a false sense of security

{one}
http://www.drdobbs.com/cpp/lock-free-code-a-false-sense-of-security/210600279

Lock-Free Code: A False Sense of Security By Herb Sutter, September 08, 2008

Writing lock-free code can confound anyone—even expert programmers, as Herb shows this month.

Given that lock-based synchronization has serious problems [1], it can be tempting to think
lock-free code must be the answer. Sometimes that is true. In particular, it's useful to have
libraries provide hash tables and other handy types whose implementations are internally
synchronized using lock-free techniques, such as Java's ConcurrentHashMap, so that we can use those
types safely from multiple threads without external synchronization and without having to understand
the subtle lock-free implementation details.
{Q} Java's ConcurrentHashMap

<two-drawbacks-for-lock-free-code>
But replacing locks wholesale by writing your own lock-free code is not the answer. Lock-free code
has two major drawbacks. First, it's not broadly useful for solving typical problems-lots of basic
data structures, even doubly linked lists, still have no known lock-free implementations. Coming up
with a new or improved lock-free data structure will still earn you at least a published paper in a
refereed journal, and sometimes a degree.

Second, it's hard even for experts. It's easy to write lock-free code that appears to work, but it's
very difficult to write lock-free code that is correct and performs well. Even good magazines and
refereed journals have published a substantial amount of lock-free code that was actually broken in
subtle ways and needed correction.

To illustrate, let's dissect some peer-reviewed lock-free code that was published here in DDJ just
two months ago [2]. The author, Petru Marginean, has graciously allowed me to dissect it here so
that we can see what's wrong and why, what lessons we should learn, and how to write the code
correctly. That someone as knowledgable as Petru, who has published many good and solid articles,
can get this stuff wrong should be warning enough that lock-free coding requires great care.
<KT> See Lock-Free Queues in below.

A Limited Lock-Free Queue

<limitation-or-assumption-of-q>
Marginean's goal was to write a limited lock-free queue that can be used safely without internal or
external locking. To simplify the problem, the article imposed some significant restrictions,
including that the queue must only be used from two threads with specific roles: one Producer thread
that inserts into the queue, and one Consumer thread that removes items from the queue.

Marginean uses a nice technique that is designed to prevent conflicts between the writer and reader:

o The producer and consumer always work in separate parts of the underlying list, so that their work
won't conflict. At any given time, the first "unconsumed" item is the one after the one iHead refers
to, and the last (most recently added) "unconsumed" item is the one before the one iTail refers to.

o The consumer increments iHead to tell the producer that it has consumed another item in the queue.

o The producer increments iTail to tell the consumer that another item is now available in the
queue. <o> Only the producer thread ever actually modifies the queue.<o> That means the producer is
responsible, not only for adding into the queue, but also for removing consumed items. To maintain
separation between the producer and consumer and prevent them from doing work in adjacent nodes, the
producer won't clean up the most recently consumed item (the one referred to by iHead).

<KT> The consumer only changes iterator and do not modify list itself.

Q     ...   <- begin()
      ...
      ...
      [ ]   <- head, -> consumer, dummy. uses tail to check empty, publish head and consume.
      [ ]   <- first unconsumed item
      ...
      [ ]   <- last unconsumed item
      [ ]   <- tail, <- producer, end(). add, publish tail and uses head to trim unused. 
               end() and push_back()

The idea is reasonable; only the implementation is fatally flawed. Here's the original code, written
in C++ and using an STL doubly linked list<T> as the underlying data structure. I've reformatted the
code slightly for presentation, and added a few comments for readability: 

// Original code from [1] (broken without external locking)
//
template <typename T>
struct LockFreeQueue {
  private:
    std::list<T> list;
    typename std::list<T>::iterator iHead, iTail;

  public:
    LockFreeQueue() {
      list.push_back(T());        // add dummy separator
      iHead = list.begin();
      iTail = list.end();
    }

    // Produce is called on the producer thread only:

    void Produce(const T& t) {
      list.push_back(t);               // add the new item
      iTail = list.end();              // <publish> it
      list.erase(list.begin(), iHead); // trim unused nodes
    }

    // Consume is called on the consumer thread only:

    bool Consume(T& t) {
      typename std::list<T>::iterator iNext = iHead;
      ++iNext;
      if (iNext != iTail) {         // if queue is nonempty
        iHead = iNext;              // <publish> that we took an item
        t = *iHead;                 // copy it back to the caller
        return true;                // and report success
      }
      return false;                 // else report queue was empty
    }
};

<lock-free-variable> <two-key-property>
The fundamental reason that the code is broken is that it has race conditions on both would-be
lock-free variables, iHead and iTail. To avoid a race, a lock-free variable must have two key
properties that we need to watch for and guarantee: atomicity and ordering. These variables are
neither.

Atomicity

First, reads and writes of a lock-free variable must be atomic. For this reason, lock-free variables
are typically no larger than the machine's native word size, and are usually pointers (C++), object
references (Java, .NET), or integers. Trying to use an ordinary list<T>::iterator variable as a
lock-free shared variable isn't a good idea and can't reliably meet the atomicity requirement, as we
will see.

Let's consider the races on iHead and iTail in these lines from Produce and Consume:

void Produce(const T& t) {
  ...
  iTail = list.end();
  list.erase(list.begin(), iHead);
}
 
bool Consume(T& t) {
  ...
  if (iNext != iTail) {
    iHead = iNext;
  ...   
  }
}

If reads and writes of iHead and iTail are not atomic, then Produce could read a partly updated (and
therefore corrupt) iHead and try to dereference it, and Consume could read a corrupt iTail and
fall off the end of the queue. Marginean does note this requirement:

"Reading/writing list<T>::iterator is atomic on the machine upon which you run the application." [2]

Alas, atomicity is necessary but not sufficient (see next section), and not supported by
list<T>::iterator. First, in practice, many list<T>::iterator implementations I examined are [larger]
than the native machine/pointer size, which means that they can't be read or written with atomic
loads and stores on most architectures. Second, in practice, even if they were of an appropriate
size, you'd have to add other decorations to the variable to ensure atomicity, for example to
require that the variable be properly [aligned] in memory.

Finally, the code isn't valid ISO C++. The 1998 C++ Standard said nothing about concurrency, and so
provided no such guarantees at all. The upcoming second C++ standard that is now being finalized
C++0x, does include a memory model and thread support, and explicitly forbids it. In brief, C++0x
says that the answer to questions such as, "What do I need to do to use a list<T> mylist
thread-safely?" is "Same as any other object"—if you know that an object like mylist is shared, you
must externally synchronize access to it, including via iterators, by protecting all such uses with
locks, else you've written a race [3]. Note: Using C++0x's std::atomic<> is not an option for
list<T>::iterator, because atomic<T> requires T to be a bit-copyable type, and STL types and their
iterators aren't guaranteed to be that.

Ordering Problems in Produce

Second, reads and writes of a lock-free variable must occur in an expected order, which is nearly
always the exact order they appear in the program source code. But compilers, processors, and caches
love to optimize reads and writes, and will helpfully reorder, invent, and remove memory reads and
writes unless you prevent it from happening. 

<to-prevent-reordering>
The right prevention happens implicitly when you use mutex locks or ordered atomic variables; {Q}
why mutex prevent this? 

C++0x std::atomic, Java/.NET volatile; you can also do it explicitly, but with considerably more
effort, using ordered API calls e.g., Win32 InterlockedExchange or memory fences/barriers e.g.,
Linux mb.  Trying to write lock-free code without using any of these tools can't possibly work.

Consider again this code from Produce, and ignore that the assignment iTail isn't atomic as we look
for other problems:

list.push_back(t);  // A: add the new item
iTail = list.end(); // B: publish it

This is a classic publication race because lines A and B can be (partly or entirely) reordered. For
example, let's say that some of the writes to the T object's members are delayed until after the
write to iTail, which publishes that the new object is available; then the consumer thread can see a
partly assigned T object.

What is the minimum necessary fix? We might be tempted to write a memory barrier between the two
lines:

// Is this change enough?
list.push_back(t);  // A: add the new item
mb();               // full fence
iTail = list.end(); // B: publish it

Before reading on, think about it and see if you're convinced that this is (or isn't) right.

Have you thought about it? As a starter, here's one issue: Although list.end is probably unlikely to
perform writes, it's possible that it might, and those are side effects that need to be complete
before we publish iTail. 

The general issue is that you can't make assumptions about the side effects of library functions you
call, and you have to make sure they're fully performed before you publish the new state. 

So a slightly improved version might try to store the result of list.end into a local unshared
variable and assign it after the barrier:

// Better, but is it enough?
list.push_back(t);
tmp = list.end();
mb();                // full fence
iTail = tmp;

Unfortunately, this still isn't enough. Besides the fact that assigning to iTail isn't atomic and
that we still have a race on iTail in general, compilers and processors can also invent writes to
iTail that break this code. Let's consider write invention in the context of another problem area:
Consume.

Ordering Problems in Consume

Here's another reordering problem, this time from Consume:

++iNext;
if (iNext != iTail) {
  iHead = iNext;        // C
  t = *iHead;           // D

Note that Consume updates iHead to advertise that it has consumed another item before it actually
  reads the item's value. Is that a problem? We might think it's innocuous, because the producer
  always leaves the iHead item alone to stay at least one item away from the part of the list the
  consumer is using.

It turns out this code is broken regardless of which order we write lines C and D, because the
compiler or processor or cache can reorder either version in unfortunate ways. Consider what happens
if the consumer thread performs a consecutive two calls to Consume: The memory reads and writes
performed by those two calls could be reordered so that iHead is incremented twice before we copy
the two list nodes' values, and then we have a problem because the producer may try to remove
nodes the consumer is still using. [KT] Think that this is a problem besides odering and lock-free
variable.

Note: This doesn't mean the compiler or processor transformations are broken; they're not. Rather
the code is racy and has insufficient synchronization, and so it breaks the memory model guarantees
and makes such transformations possible and visible.

Reordering isn't the only issue. Another problem is that compilers and processors can invent writes,
so they could inject a transient value:

// Problematic compiler/processor transformation
if (iNext != iTail) {
  iHead = 0xDEADBEEF;
  iHead = iNext;
  t = *iHead;

Clearly, that would break the producer thread, which would read a bad value for iHead. More likely,
the compiler or processor might speculate that most of the time iNext != iTail:

// Another problematic transformation
//
__temp = iHead;
iHead = iNext;  // speculatively set to iNext

if (iNext == iTail) {   // note: inverted test!
  iHead = __temp;   // undo if we guessed wrong
} else {
  t = *iHead;

<invariant>
But now iHead could equal iTail, which breaks the essential invariant that iHead must never equal
iTail, on which the whole design depends.

Can we solve these problems by writing line D before C, then separating them with a full fence? Not
entirely: That will prevent most of the aforementioned optimizations, but it will not eliminate all
of the problematic invented writes. More is needed.

Next Steps

These are a sample of the concurrency problems in the original code. Marginean showed a good
algorithm, but the implementation is broken because it uses an inappropriate type and performs
insufficient synchronization/ordering. Fixing the code will require a rewrite, because we need to
change the data structure and the code to let us use proper ordered atomic lock-free variables. But
how? Next month, we'll consider a fixed version. Stay tuned.

Notes

[1] H. Sutter, "The Trouble With Locks," C/C++ Users Journal, March 2005.
(www.ddj.com/cpp/184401930).

[2] P. Marginean, "Lock-Free Queues," Dr. Dobb's Journal, July 2008. (www.ddj.com/208801974).

[3] B. Dawes, et al., "Thread-Safety in the Standard Library," ISO/IEC JTC1/SC22/WG21 N2669, June
2008. (www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2669.htm).


{two}
http://www.drdobbs.com/parallel/lock-free-queues/208801974?pgno=1
Lock-Free Queues By Petru Marginean, July 01, 2008

<note-begin>
This article as written assumes a sequentially consistent model. In particular, the code relies on
specific order of instructions in both Consumer and Producer methods. However, without inserting
proper memory barrier instructions, these instructions can be reordered with unpredictable results
(see, for example, the classic Double-Checked Locking problem).

Another issue is using the standard std::list<T>. While the article mentions that it is the
developer responsibility to check that the reading/writing std::list<T>::iterator is atomic, this
turns out to be too restrictive. While gcc/MSVC++2003 has 4-byte iterators, the MSVC++2005 has
8-byte iterators in Release Mode and 12-byte iterators in the Debug Mode.

The solution to prevent this is to use memory barriers/volatile variables. 
The downloadable code featured at the top of this article has fixed that issue. ~

Many thanks to Herb Sutter who signaled the issue and helped me fix the code. --P.M.
<note-end>

Queues can be useful in a variety of systems involving data-stream processing. Typically, you have a
data source producing data—requests coming to a web server, market feeds, or digital telephony
packets—at a variable pace, and you need to process the data as fast as possible so there are no
losses. To do this, you can push data into a queue using one thread and process it using a different
thread—a good utilization of resources on multicore processors. One thread inserts data into the
queue, and the other reads/deletes elements from the queue. Your main requirement is that a
high-rate data burst does not last longer than the system's ability to accumulate data while the
consumer thread handles it. [KT] This means no overlow? 

The queue you use has to be threadsafe to prevent race conditions when inserting/removing data from
multiple threads. For obvious reasons, it is necessary that the queue mutual exclusion mechanism add
as little overhead as possible.

In this article, I present a lock-free queue (the source code for the lockfreequeue class is
available online; see www.ddj.com/code/) in which one thread can write to the queue and another
read from it—at the same time without any locking. /0807/lockfreequeue/

To do this, the code implements these requirements:

<conditions>
o There is a single writer (Producer) and single reader (Consumer). When you have multiple producers
and consumers, you can still use this queue with some external locking. You cannot have multiple
producers writing at the same time (or multiple consumers consuming the data simultaneously), but
you can have one producer and one consumer (2x threads) accessing the queue at the same time
(Responsibility: developer).

o When inserting/erasing to/from an std::list<T>, the iterators for the existing elements must
remain valid (Responsibility: library implementor).

o <Only> one thread modifies the queue; the producer thread both adds/erases elements in the queue
(Responsibility: library implementor).

o Beside the underlying std::list<T> used as the container, the lock-free queue class also holds two
iterators pointing to the not-yet-consumed range of elements; each is modified by one thread and
read by the other (Responsibility: library implementor).

o Reading/writing list<T>::iterator is <atomic> on the machine upon which you run the application. If
they are not on your implementation of STL, you should check whether the raw pointer's operations
are atomic. You could easily replace the iterators to be mentioned shortly with raw pointers in the
code (Responsibility: machine). <KT> ?

<KT> Here big condition is that one producer and consumer model, only producer modify a list, and
updating iterators between threads are atomic.

Because I use Standard C++, the code is portable under the aforementioned "machine" assumption: 

template <typename T>
struct LockFreeQueue
{
    LockFreeQueue();
    void Produce(const T& t);
    bool Consume(T& t);

  private:
    typedef std::list<T> TList;
    TList list;
    typename TList::iterator iHead, iTail;   <KT> why not 'TList::iterator iHead'?
};

Considering how simple this code is, you might wonder how can it be threadsafe. The magic is due to
design, not implementation. Take a look at the implementation of the Produce() and Consume()
methods. The Produce() method looks like this: 

void Produce(const T& t)
{
  list.push_back(t);
  iTail = list.end();
  list.erase(list.begin(), iHead);
}

To understand how this works, mentally separate the data from LockFreeQueue<T> into two groups:

o The list and the iTail iterator, modified by the Produce() method (Producer thread).
o The iHead iterator, modified by the Consume() method (Consumer thread). 

<o>
Produce() is the [only] method that changes the list (adding new elements and erasing the consumed
elements), and it is essential that [only] one thread ever calls Produce()—it's the Producer
thread! The iterator (iTail) (only manipulated by the Producer thread) changes it only after a new
element is added to the list. This way, when the Consumer thread is reading the iTail element, the
new added element is ready to be used. The Consume() method tries to read all the elements between
iHead and iTail (excluding both ends). 
<o>

bool Consume(T& t)
{
  typename TList::iterator iNext = iHead;
  ++iNext;
  if (iNext != iTail)
  {
    iHead = iNext;
    t = *iHead;
    return true;
  }
  return false;
}

This method reads the elements, but doesn't remove them from the list. Nor does it access the list
directly, but through the iterators. They are guaranteed to be valid after std::list<T> is modified,
so no matter what the Producer thread does to the list, you are safe to use them.

The std::list<T> maintains an element (pointed to by iHead) that is considered already read. For
this algorithm to work even when the queue was just created, I add an empty T() element in the
constructor of the LockFreeQueue<T> (see Figure 1): 

LockFreeQueue()
{
  list.push_back(T());
  iHead = list.begin();
  iTail = list.end();
}

<discussion-when-queue-is-empty>
Consume() may fail to read an element (and return false). Unlike traditional lock-based queues, this
queue works fast when the queue is not empty, but needs an external locking or polling method to
wait for data. Sometimes you want to wait if there is no element available in the queue, and avoid
returning false. A naive approach to waiting is: 

T Consume()
{
  T tmp;
  while (!Consume(tmp))
    ;
  return tmp;
}

This Consume() method will likely heat up one of your CPUs red-hot to 100-percent use if there are
no elements in the queue. Nevertheless, this should have good performance when the queue is not
empty. However, if you think of it, a queue that's almost never empty is a sign of systemic trouble:
It means the consumer is unable to keep pace with the producer, and sooner or later, the system is
doomed to die of memory exhaustion. Call this approach NAIVE_POLLING. 

A friendlier Consume() function does some pooling and calls some sort of sleep() or yield() function
available on your system: 

T Consume(int wait_time = 1/*milliseconds*/)
{
  T tmp;
  while (!Consume(tmp))
  {
    Sleep(wait_time/*milliseconds*/);
  }
  return tmp;
}

The DoSleep() can be implemented using nanosleep() (POSIX) or Sleep() (Windows), or even better,
using boost::thread::sleep(), which abstracts away system-dependent nomenclature. Call this
approach SLEEP. Instead of simple polling, you can use more advanced techniques to signal the
Consumer thread that a new element is available. I illustrate this in Listing One using a
boost::condition variable.

#include <boost/thread.hpp>
#include <boost/thread/condition.hpp>
#include <boost/thread/xtime.hpp>
    
template <typename T>
struct WaitFreeQueue
{
    void Produce(const T& t)
    {
        queue.Produce(t);
        cond.notify_one();
    }
    bool Consume(T& t)
    {
        return queue.Consume(t);
    }
    T Consume(int wait_time = 1/*milliseconds*/)
    {
        T tmp;
        if (Consume(tmp))
            return tmp;

        // the queue is empty, try again (possible waiting...)
        boost::mutex::scoped_lock lock(mtx);
        while (!Consume(tmp))          // line A
        {
            boost::xtime t;
            boost::xtime_get(&t, boost::TIME_UTC);
            AddMilliseconds(t, wait_time);
            cond.timed_wait(lock, t);  // line B
        }
        return tmp;
    }
private:
    LockFreeQueue<T> queue;
    boost::condition cond;
    boost::mutex mtx;
};

I used the timed_wait() instead of the simpler wait() to solve a possible deadlock when Produce() is
called between line A and line B in Listing One. Then wait() will miss the notify_one() call and
have to wait for the next produced element to wake up. <If> this element never comes (no more produced
elements or if the Produce() call actually waits for Consume() to return), there's a deadlock.
Call this approach TIME_WAIT.

The lock is still wait-free as long as there are elements in the queue. In this case, the Consumer()
thread does no waiting and reads data as fast as possible (even with the Producer() that is
inserting new elements). Only when the queue is exhausted does locking occur. 
<KT> claims that it is lock-free as long as queue is not empty.

The Ping-Pong Test

To compare the three approaches (NAIVE_POLLING, SLEEP, and TIME_WAIT), I implemented a test called
"Ping-Pong" that is similar to the game of table tennis (the source code is available online). In
Figure 2, there are two identical queues between the threads T1 and T2. 

      queue 1
T1 ------------> T2
   <------------
      queue 2

You first load one of the queues with a number of "balls," then ask each thread to read from one
queue and write to the other. The result is a controlled infinite loop. By limiting the game to a
fixed number of reads/writes ("shots"), you get an understanding of how the queue behaves when
varying the waiting/sleep time and strategy and the number of "balls." The faster the game, the
better the performance. You should also check CPU usage to see how much of it is used for real work.

o "No ball" means "do nothing" (like two players waiting for the other to start). This gives you an
idea of how good the queues are when there is no data—how nervous the players are. Ideally, CPU
usage should be zero.
 
o "One ball" is like the real ping-pong game: Each player shoots and waits for the other to shoot.

o "Two (or more) balls" means both players could shoot at the same time, modulo collision and
waiting issues.


In a wait-free system, the more balls in the game, the better the performance gain compared to the
classic locking strategy. This is because wait-free is an optimistic concurrency control method
(works best when there is no contention), while classical lock-based concurrency control is
pessimistic (assumes contention happens and preemptively inserts locking).

Ready to play? Here is the Ping-Pong test command line:

$> ./pingpong [strategy] [timeout] [balls] [shots]

When you run the program, the tests show the results in the table shown in Figure 3:

o The best combination is the timed_wait() with a small wait time (1ms in the test for TIMED_WAIT).
It has a very fast response time and almost 0 percent CPU usage when the queue is empty.

o Even when the sleep time is 0 (usleep(0)), the worst seems to be the sleep() method, especially
when the queue is likely to be empty. (The number of shots in this case is 100-times smaller than
the other cases because of the long duration of the game.)

o The NO_WAIT strategy is [fast] but behaves worst when there are no balls (100-percent CPU usage to
do nothing). It has the best performance when there is a single ball.

Figure 4 presents a table with the results for a classic approach (see SafeQueue). These results
show that this queue is, on average, more than four-times slower than the LockFreeQueue. The
slowdown comes from the synchronization between threads. Both Produce() and Consume() have to wait
for each other to finish. CPU usage is almost 100 percent for this test (similar to the NO_WAIT
strategy, but not even close to its performance).

Final Considerations

The single-threaded code below shows the value of the list.size() when Producing/ Consuming
elements: 

LockFreeQueue<int> q;   // list.size() == 1
q.Produce(1);           // list.size() == 2
int i;
q.Consume(i);           // list.size() == still 2!;
                        // Consume() doesn't modify the list
q.Produce(i);           // list.size() == 2 again;

The size of the queue is 1 if Produce() was never called and greater than 1 if any element was
produced.

No matter how many times Consume() is called, the list's size will stay constant. It is Produce()
that is increasing the size (by 1); and if there were consumed elements, it will also delete them
from the queue. In a way, Produce() acts as a simple garbage collector. <o> The whole thread safety
comes from the fact that specific data is modified from single threads only. The synchronization
between threads is done using iterators (or pointers, whichever has atomic read/write operation on
your machine). <o> Also consider this code: 

usleep(1000);    // sleep 1 microsecond

On the face of it, this line of code makes a thread sleep for 1 microsecond, and then continue. In
reality, 1 microsecond is just a lower bound to the duration of the call.

The man page for usleep() says, "The usleep() function suspends execution of the calling process for
(at least) usec microseconds. The sleep may be lengthened slightly by any system activity or by the
time spent processing the call or by the granularity of system timers," or if you use the
nanosleep() function. "Therefore, nanosleep() always pauses for at least the specified time;
however, it can take up to 10 ms longer than specified until the process becomes runnable again."

So if the process is not scheduled under a real-time policy, there's no guarantee when your thread
will be running again. I've done some tests and (to my surprise) there are situations when code such
as: 

cond.timed_wait(lock, x);    // x = e.g. 1 millisecond

will actually wait for more than 1 second.


{three}
http://www.drdobbs.com/cpp/the-trouble-with-locks/184401930
The Trouble with Locks

By Herb Sutter, March 01, 2005

References

[1] Sutter, H. "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software," Dr.
Dobb's Journal, March 2005. Available online at http://www.gotw.ca/publications/concurrency-ddj.htm.

[2] Sutter, H. "The Concurrency Revolution," C/C++ Users Journal, February 2005. This is an
abbreviated version of [1].

[3] Alexandrescu, A. "Lock-Free Data Structures," C/C++ Users Journal, October 2004.

[4] Alexandrescu, A. and M. Michael. "Lock-Free Data Structures with Hazard Pointers," C/C++ Users
Journal, December 2004.

[5] http://blogs.msdn.com/cbrumme. 

Lock-based programming may not be the best approach to building large concurrent programs.

In my most recent articles [1, 2], I presented reasons why concurrency (for example, multithreading)
will be the next revolution in the way we develop software — a sea change of the same order as the
object-oriented revolution. I also stated that "the vast majority of programmers today don't grok
concurrency, just as the vast majority of programmers 15 years ago didn't yet grok objects."

In this column, I'd like to consider just one question that several people wrote to ask, namely: "Is
concurrency really that hard?" In particular, a few readers felt that lock-based programming is well
understood; it is, after all, the status quo mainstream solution to concurrency control.

So, "is concurrency really that hard?" My short answer is this:

o Lock-based programming, our status quo, is difficult for experts to get right. Worse, it is also
fundamentally flawed for building large programs. This article focuses exclusively on lock-based
programming just because there's so much to say in even a 50,000-foot overview that I ran out of
room.

o Lock-free programming is difficult for gurus to get right. I'll save this for another time, but if
you're interested, you should check out Andrei Alexandrescu's recent articles for a sampling of the
issues in lock-free programming and hazard pointers [3, 4]. (Aside: Yes, I'm implying Andrei is a
guru. I hope he doesn't mind my outing him in public like this. I don't think it was much of a
secret.) (More aside: The hazard pointer work shows in particular why, if you're writing
lock-free data structures, you really really want garbage collection. You can do it yourself
without garbage collection, but it's like working with knives that are sharp on both edges and
don't have handles. But that's another article. Specifically, it's Andrei's other article.)

Unfortunately, today's reality is that only thoughtful experts can write explicitly concurrent
programs that are correct and efficient. This is because today's programming models for concurrency
are subtle, intricate, and fraught with pitfalls that easily (and frequently) result in unforeseen
races (i.e., program corruption) deadlocks (i.e., program lockup) and performance cliffs (e.g.,
priority inversion, convoying, and sometimes complete loss of parallelism and/or even worse
performance than a single-threaded program). And even when a correct and efficient concurrent
program is written, it takes great care to maintain — it's usually brittle and difficult to maintain
correctly because current programming models set a very high bar of expertise required to reason
reliably about the operation of concurrent programs, so that apparently innocuous changes to a
working concurrent program can (and commonly do, in practice) render it entirely or intermittently
nonworking in unintended and unexpected ways. Because getting it right and keeping it right is so
difficult, at many major software companies there is a veritable priesthood of gurus who write and
maintain the core concurrent code.

Some people think I'm overstating this, so let me amplify. In this article, I'll focus on just the
narrow question of how to write a lock-based program correctly, meaning that it works (avoids data
corruption) and doesn't hang (avoids deadlock and livelock). That's pretty much the minimum
requirement to write a program that runs at all.

Question: Is the following code thread-safe? If it is, why is it safe? If it isn't always, under
what conditions is it thread-safe?

T Add( T& a, T& b ) {
  T result;
  // ... read a and b and set result to
  // proper values ...
  return result;
}

There are a lot of possibilities here. Let's consider some of the major ones.

Lock-Based Solutions?

Assume that reading a T object isn't an atomic operation. Then, if a and/or b are accessible from
another thread, we have a classic race condition: While we are reading the values of a and/or b,
some other thread might be changing those objects, resulting in blowing up the program; if you're
lucky, say, by causing the object to follow an internal pointer some other thread just deleted; or
reading corrupt values.

How would you solve that? Please stop and think about it before reading on...

Ready? Okay: Now please stop a little longer and think about your solution some more, and consider
whether it might have any further holes, before reading on...

Now that you've thought about it deeply, let's consider some alternatives.

Today's typical lock-based approach is to acquire locks so that uses of a and b on one thread won't
interleave. Typically, this is done by acquiring a lock on an explicit synchronization object [a
mutex, for instance] that covers both objects, or by acquiring locks on an implicit mutex associated
with the objects themselves. To acquire a lock that covers both objects, Add has to know what that
lock is, either because a and b and their lock are globals [but then why pass a and b as
parameters?] or because the caller acquires the lock outside of Add [which is usually preferable].
To acquire a lock on each object individually, we could write:

T SharedAdd( T& a, T& b ) {
  T result;
  lock locka( a );  // lock is a helper
  // whose constructor acquires a lock
  lock lockb( b );  // and whose
  // destructor releases the lock
  // ... read a and b and set result to
  //  proper values ...
  return result;
} // release the locks 


{four}
http://www.drdobbs.com/parallel/writing-lock-free-code-a-corrected-queue/210604448?pgno=1
Writing Lock-Free Code: A Corrected Queue

By Herb Sutter, September 29, 2008

Notes

[1] H. Sutter. “Lock-Free Code: A False Sense of Security” (DDJ, September 2008).
(www.ddj.com/cpp/210600279).

[2] P. Marginean. "Lock-Free Queues" (DDJ, July 2008). (www.ddj.com/208801974).

[3] This is just like a canonical exception safety pattern—do all the work off to the side, then
commit to accept the new state using nonthrowing operations only. "Think in transactions" applies
everywhere, and should be ubiquitous in the way we write our code.

[4] Compare-and-swap (CAS) is the most widely available fundamental lock-free operation and so I'll
focus on it here. However, some systems instead provide the equivalently powerful
load-linked/store-conditional (LL/SC) instead.


As we saw last month [1], lock-free coding is hard even for experts. There, I dissected a published
lock-free queue implementation [2] and examined why the code was quite broken. This month, let's see
how to do it right.

Lock-Free Fundamentals

When writing lock-free code, always keep these essentials well in mind:

Key concepts. Think in transactions. Know who owns what data. Key tool. The ordered atomic variable. 

When writing a lock-free data structure, "to think in transactions" means to make sure that each
operation on the data structure is atomic, all-or-nothing with respect to other concurrent
operations on that same data. The typical coding pattern to use is to do work off to the side, then
"publish" each change to the shared data with a single atomic write or compare-and-swap. [3] Be sure
that concurrent writers don't interfere with each other or with concurrent readers, and pay special
attention to any operations that delete or remove data that a concurrent operation might still be
using.

Be highly aware of who owns what data at any given time; mistakes mean races where two threads think
they can proceed with conflicting work. You know who owns a given piece of shared data right now by
looking at the value of the ordered atomic variable that says who it is. To hand off ownership of
some data to another thread, do it at the end of a transaction with a single atomic operation that
means "now it's your's."

An ordered atomic variable is a "lock-free-safe" variable with the following properties that make it
safe to read and write across threads without any explicit locking:

Atomicity. Each individual read and write is guaranteed to be atomic with respect to all other reads
and writes of that variable. The variables typically fit into the machine's native word size, and so
are usually pointers (C++), object references (Java, .NET), or integers. Order. Each read and write
is guaranteed to be executed in source code order. Compilers, CPUs, and caches will respect it and
not try to optimize these operations the way they routinely distort reads and writes of ordinary
variables. Compare-and-swap (CAS) [4]. There is a special operation you can call using a syntax like
variable.compare_exchange( expectedValue, newValue ) that does the following as an atomic operation:
If variable currently has the value expectedValue, it sets the value to newValue and returns true;
else returns false. A common use is if(variable.compare_exchange(x,y)), which you should get in the
habit of reading as, "if I'm the one who gets to change variable from x to y." 

Ordered atomic variables are spelled in different ways on popular platforms and environments. For
example:

volatile in C#/.NET, as in volatile int. volatile or * Atomic* in Java, as in volatile int,
AtomicInteger. atomic<T> in C++0x, the forthcoming ISO C++ Standard, as in atomic<int>. 

In the code that follows, I'm going to highlight the key reads and writes of such a variable; these
variables should leap out of the screen at you, and you should get used to being very aware of every
time you touch one.

If you don't yet have ordered atomic variables yet on your language and platform, you can emulate
them by using ordinary but aligned variables whose reads and writes are guaranteed to be naturally
atomic, and enforce ordering by using either platform-specific ordered API calls (such as Win32's
InterlockedCompareExchange for compare-and-swap) or platform-specific explicit memory
fences/barriers (for example, Linux mb).


==============================================================================
*kt_linux_core_264*	conc: ref: the free lunch is over 
http://www.gotw.ca/publications/concurrency-ddj.htm

The Free Lunch Is Over
A Fundamental Turn Toward Concurrency in Software

By Herb Sutter

The biggest sea change in software development since the OO revolution is knocking at the door, and
its name is Concurrency.

This article appeared in Dr. Dobb's Journal, 30(3), March 2005. A much briefer version under the
title "The Concurrency Revolution" appeared in C/C++ Users Journal, 23(2), February 2005.

Update note: The CPU trends graph last updated August 2009 to include current data and show the
trend continues as predicted. The rest of this article including all text is still original as first
posted here in December 2004.

Your free lunch will soon be over. What can you do about it? What are you doing about it?

The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have
run out of room with most of their traditional approaches to boosting CPU performance. Instead of
driving clock speeds and straight-line instruction throughput ever higher, they are instead turning
en masse to hyperthreading and multicore architectures. Both of these features are already available
on chips today; in particular, multicore is available on current PowerPC and Sparc IV processors,
   and is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004 In-Stat/MDR Fall
   Processor Forum was multicore devices, as many companies showed new or updated multicore
   processors. Looking back, it’s not much of a stretch to call 2004 the year of multicore.

And that puts us at a fundamental turning point in software development, at least for the next few
years and for applications targeting general-purpose desktop computers and low-end servers (which
    happens to account for the vast bulk of the dollar value of software sold today). In this
article, I’ll describe the changing face of hardware, why it suddenly does matter to software, and
how specifically the concurrency revolution matters to you and is going to change the way you will
likely be writing software in the future.

Arguably, the free lunch has already been over for a year or two, only we’re just now noticing.  

<The Free Performance Lunch>

There’s an interesting phenomenon that’s known as “Andy giveth, and Bill taketh away.” No matter how
fast processors get, software consistently finds new ways to eat up the extra speed. Make a CPU ten
times as fast, and software will usually find ten times as much to do (or, in some cases, will feel
    at liberty to do it ten times less efficiently). Most classes of applications have enjoyed free
and regular performance gains for several decades, even without releasing new versions or doing
anything special, because the CPU manufacturers (primarily) and memory and disk manufacturers
(secondarily) have reliably enabled ever-newer and ever-faster mainstream systems. Clock speed isn’t
the only measure of performance, or even necessarily a good one, but it’s an instructive one: We’re
used to seeing 500MHz CPUs give way to 1GHz CPUs give way to 2GHz CPUs, and so on. Today we’re in
the 3GHz range on mainstream computers.

The key question is: When will it end? After all, Moore’s Law predicts exponential growth, and
clearly exponential growth can’t continue forever before we reach hard physical limits; light isn’t
getting any faster. The growth must eventually slow down and even end. (Caveat: Yes, Moore’s Law
    applies principally to transistor densities, but the same kind of exponential growth has
    occurred in related areas such as clock speeds. There’s even faster growth in other spaces, most
    notably the data storage explosion, but that important trend belongs in a different article.)

If you’re a software developer, chances are that you have already been riding the “free lunch” wave
of desktop computer performance. Is your application’s performance borderline for some local
operations? “Not to worry,” the conventional (if suspect) wisdom goes; “tomorrow’s processors will
have even more throughput, and anyway today’s applications are increasingly throttled by factors
other than CPU throughput and memory speed (e.g., they’re often I/O-bound, network-bound,
    database-bound).” Right?

Right enough, in the past. But dead wrong for the foreseeable future.

The good news is that processors are going to continue to become more powerful. The bad news is
that, at least in the short term, the growth will come mostly in directions that do not take most
current applications along for their customary free ride.

Over the past 30 years, CPU designers have achieved performance gains in three main areas, the first
two of which focus on straight-line execution flow:

clock speed
execution optimization
cache

Increasing clock speed is about getting more cycles. Running the CPU faster more or less directly
means doing the same work faster.

Optimizing execution flow is about doing more work per cycle. Today’s CPUs sport some more powerful
instructions, and they perform optimizations that range from the pedestrian to the exotic, including
pipelining, branch prediction, executing multiple instructions in the same clock cycle(s), and even
reordering the instruction stream for out-of-order execution. These techniques are all designed to
make the instructions flow better and/or execute faster, and to squeeze the most work out of each
clock cycle by reducing latency and maximizing the work accomplished per clock cycle.

Chip designers are under so much pressure to deliver ever-faster CPUs that they’ll risk changing the
meaning of your program, and possibly break it, in order to make it run faster

Brief aside on instruction reordering and memory models: Note that some of what I just called
“optimizations” are actually far more than optimizations, in that they can change the meaning of
programs and cause visible effects that can break reasonable programmer expectations. This is
significant. CPU designers are generally sane and well-adjusted folks who normally wouldn’t hurt a
fly, and wouldn’t think of hurting your code… normally. But in recent years they have been willing
to pursue aggressive optimizations just to wring yet more speed out of each cycle, even knowing full
well that these aggressive rearrangements could endanger the semantics of your code. Is this Mr.
Hyde making an appearance? Not at all. That willingness is simply a clear indicator of the extreme
pressure the chip designers face to deliver ever-faster CPUs; they’re under so much pressure that
they’ll risk changing the meaning of your program, and possibly break it, in order to make it run
faster. Two noteworthy examples in this respect are write reordering and read reordering: Allowing a
processor to reorder write operations has consequences that are so surprising, and break so many
programmer expectations, that the feature generally has to be turned off because it’s too difficult
for programmers to reason correctly about the meaning of their programs in the presence of arbitrary
write reordering. Reordering read operations can also yield surprising visible effects, but that is
more commonly left enabled anyway because it isn’t quite as hard on programmers, and the demands for
performance cause designers of operating systems and operating environments to compromise and choose
models that place a greater burden on programmers because that is viewed as a lesser evil than
giving up the optimization opportunities.

Finally, increasing the size of on-chip cache is about staying away from RAM. Main memory continues
to be so much slower than the CPU that it makes sense to put the data closer to the processor—and
you can’t get much closer than being right on the die. On-die cache sizes have soared, and today
most major chip vendors will sell you CPUs that have 2MB and more of on-board L2 cache. (Of these
    three major historical approaches to boosting CPU performance, increasing cache is the only one
    that will continue in the near term. I’ll talk a little more about the importance of cache later
    on.)

Okay. So what does this mean?

A fundamentally important thing to recognize about this list is that all of these areas are
concurrency-agnostic. Speedups in any of these areas will directly lead to speedups in sequential
(nonparallel, single-threaded, single-process) applications, as well as applications that do make
use of concurrency. That’s important, because the vast majority of today’s applications are
single-threaded, for good reasons that I’ll get into further below.

Of course, compilers have had to keep up; sometimes you need to recompile your application, and
target a specific minimum level of CPU, in order to benefit from new instructions (e.g., MMX, SSE)
  and some new CPU features and characteristics. But, by and large, even old applications have
  always run significantly faster—even without being recompiled to take advantage of all the new
  instructions and features offered by the latest CPUs.

That world was a nice place to be. Unfortunately, it has already disappeared.

<Obstacles, and Why You Don’t Have 10GHz Today>

CPU performance growth as we have known it hit a wall two years ago. Most people have only recently
started to notice.

You can get similar graphs for other chips, but I’m going to use Intel data here. Figure 1 graphs
the history of Intel chip introductions by clock speed and number of transistors. The number of
transistors continues to climb, at least for now. Clock speed, however, is a different story.

Around the beginning of 2003, you’ll note a disturbing sharp turn in the previous trend toward
ever-faster CPU clock speeds. I’ve added lines to show the limit trends in maximum clock speed;
instead of continuing on the previous path, as indicated by the thin dotted line, there is a sharp
flattening. It has become harder and harder to exploit higher clock speeds due to not just one but
several physical issues, notably heat (too much of it and too hard to dissipate), power consumption
(too high), and current leakage problems.

Quick: What’s the clock speed on the CPU(s) in your current workstation? Are you running at 10GHz?
On Intel chips, we reached 2GHz a long time ago (August 2001), and according to CPU trends before
2003, now in early 2005 we should have the first 10GHz Pentium-family chips. A quick look around
shows that, well, actually, we don’t. What’s more, such chips are not even on the horizon—we have no
good idea at all about when we might see them appear.

Well, then, what about 4GHz? We’re at 3.4GHz already—surely 4GHz can’t be far away? Alas, even 4GHz
seems to be remote indeed. In mid-2004, as you probably know, Intel first delayed its planned
introduction of a 4GHz chip until 2005, and then in fall 2004 it officially abandoned its 4GHz plans
entirely. As of this writing, Intel is planning to ramp up a little further to 3.73GHz in early 2005
(already included in Figure 1 as the upper-right-most dot), but the clock race really is over, at
least for now; Intel’s and most processor vendors’ future lies elsewhere as chip companies
aggressively pursue the same new multicore directions.

We’ll probably see 4GHz CPUs in our mainstream desktop machines someday, but it won’t be in 2005.
Sure, Intel has samples of their chips running at even higher speeds in the lab—but only by heroic
efforts, such as attaching hideously impractical quantities of cooling equipment. You won’t have
that kind of cooling hardware in your office any day soon, let alone on your lap while computing on
the plane.

<Myths and Realities: 2 x 3GHz < 6 GHz>

So a dual-core CPU that combines two 3GHz cores practically offers 6GHz of processing power. Right?

Wrong. Even having two threads running on two physical processors doesn’t mean getting two times the
performance. Similarly, most multi-threaded applications won’t run twice as fast on a dual-core box.
They should run faster than on a single-core CPU; the performance gain just isn’t linear, that’s
all.

Why not? First, there is coordination overhead between the cores to ensure cache coherency (a
    consistent view of cache, and of main memory) and to perform other handshaking. Today, a two- or
four-processor machine isn’t really two or four times as fast as a single CPU even for
multi-threaded applications. The problem remains essentially the same even when the CPUs in question
sit on the same die.

Second, unless the two cores are running different processes, or different threads of a single
process that are well-written to run independently and almost never wait for each other, they won’t
be well utilized. (Despite this, I will speculate that today’s single-threaded applications as
    actually used in the field could actually see a performance boost for most users by going to a
    dual-core chip, not because the extra core is actually doing anything useful, but because it is
    running the adware and spyware that infest many users’ systems and are otherwise slowing down
    the single CPU that user has today. I leave it up to you to decide whether adding a CPU to run
    your spyware is the best solution to that problem.)

If you’re running a single-threaded application, then the application can only make use of one core.
There should be some speedup as the operating system and the application can run on separate cores,
      but typically the OS isn’t going to be maxing out the CPU anyway so one of the cores will be
      mostly idle. (Again, the spyware can share the OS’s core most of the time.) 
  

<TANSTAAFL: Moore’s Law and the Next Generation(s)>

“There ain’t no such thing as a free lunch.” —R. A. Heinlein, The Moon Is a Harsh Mistress

Does this mean Moore’s Law is over? Interestingly, the answer in general seems to be no. Of course,
     like all exponential progressions, Moore’s Law must end someday, but it does not seem to be in
     danger for a few more years yet. Despite the wall that chip engineers have hit in juicing up
     raw clock cycles, transistor counts continue to explode and it seems CPUs will continue to
     follow Moore’s Law-like throughput gains for some years to come.  The key difference, which is
     the heart of this article, is that the performance gains are going to be accomplished in
     fundamentally different ways for at least the next couple of processor generations. And most
     current applications will no longer benefit from the free ride without significant redesign.

For the near-term future, meaning for the next few years, the performance gains in new chips will be
fueled by three main approaches, only one of which is the same as in the past. The near-term future
performance growth drivers are:

hyperthreading
multicore
cache

Hyperthreading is about running two or more threads in parallel inside a single CPU. Hyperthreaded
CPUs are already available today, and they do allow some instructions to run in parallel. A limiting
factor, however, is that although a hyper-threaded CPU has some extra hardware including extra
registers, it still has just one cache, one integer math unit, one FPU, and in general just one each
of most basic CPU features. Hyperthreading is sometimes cited as offering a 5% to 15% performance
boost for reasonably well-written multi-threaded applications, or even as much as 40% under ideal
conditions for carefully written multi-threaded applications. That’s good, but it’s hardly double,
           and it doesn’t help single-threaded applications.

Multicore is about running two or more actual CPUs on one chip. Some chips, including Sparc and
PowerPC, have multicore versions available already. The initial Intel and AMD designs, both due in
2005, vary in their level of integration but are functionally similar. AMD’s seems to have some
initial performance design advantages, such as better integration of support functions on the same
die, whereas Intel’s initial entry basically just glues together two Xeons on a single die. The
performance gains should initially be about the same as having a true dual-CPU system (only the
    system will be cheaper because the motherboard doesn’t have to have two sockets and associated
    “glue” chippery), which means something less than double the speed even in the ideal case, and
just like today it will boost reasonably well-written multi-threaded applications. Not
single-threaded ones.

Finally, on-die cache sizes can be expected to continue to grow, at least in the near term. Of these
three areas, only this one will broadly benefit most existing applications. The continuing growth in
on-die cache sizes is an incredibly important and highly applicable benefit for many applications,
  simply because space is speed. Accessing main memory is expensive, and you really don’t want to
  touch RAM if you can help it. On today’s systems, a cache miss that goes out to main memory often
  costs 10 to 50 times as much getting the information from the cache; this, incidentally, continues
  to surprise people because we all think of memory as fast, and it is fast compared to disks and
  networks, but not compared to on-board cache which runs at faster speeds. If an application’s
  working set fits into cache, we’re golden, and if it doesn’t, we’re not. That is why increased
  cache sizes will save some existing applications and breathe life into them for a few more years
  without requiring significant redesign: As existing applications manipulate more and more data,
  and as they are incrementally updated to include more code for new features, performance-sensitive
  operations need to continue to fit into cache. As the Depression-era old-timers will be quick to
  remind you, “Cache is king.”

(Aside: Here’s an anecdote to demonstrate “space is speed” that recently hit my compiler team. The
 compiler uses the same source base for the 32-bit and 64-bit compilers; the code is just compiled
 as either a 32-bit process or a 64-bit one. The 64-bit compiler gained a great deal of baseline
 performance by running on a 64-bit CPU, principally because the 64-bit CPU had many more registers
 to work with and had other code performance features. All well and good. But what about data? Going
 to 64 bits didn’t change the size of most of the data in memory, except that of course pointers in
 particular were now twice the size they were before. As it happens, our compiler uses pointers much
 more heavily in its internal data structures than most other kinds of applications ever would.
 Because pointers were now 8 bytes instead of 4 bytes, a pure data size increase, we saw a
 significant increase in the 64-bit compiler’s working set. That bigger working set caused a
 performance penalty that almost exactly offset the code execution performance increase we’d gained
 from going to the faster processor with more registers. As of this writing, the 64-bit compiler
 runs at the same speed as the 32-bit compiler, even though the source base is the same for both and
 the 64-bit processor offers better raw processing throughput. Space is speed.)

But cache is it. Hyperthreading and multicore CPUs will have nearly no impact on most current
applications.

So what does this change in the hardware mean for the way we write software? By now you’ve probably
noticed the basic answer, so let’s consider it and its consequences.

<What This Means For Software: The Next Revolution>

In the 1990s, we learned to grok objects. The revolution in mainstream software development from
structured programming to object-oriented programming was the greatest such change in the past 20
years, and arguably in the past 30 years. There have been other changes, including the most recent
(and genuinely interesting) naissance of web services, but nothing that most of us have seen during
our careers has been as fundamental and as far-reaching a change in the way we write software as the
object revolution.

Until now.

Starting today, the performance lunch isn’t free any more. Sure, there will continue to be generally
applicable performance gains that everyone can pick up, thanks mainly to cache size improvements.
But if you want your application to benefit from the continued exponential throughput advances in
new processors, it will need to be a well-written concurrent (usually multithreaded) application.
And that’s easier said than done, because not all problems are inherently parallelizable and because
concurrent programming is hard.

I can hear the howls of protest: “Concurrency? That’s not news! People are already writing
concurrent applications.” That’s true. Of a small fraction of developers.

Remember that people have been doing object-oriented programming since at least the days of Simula
in the late 1960s. But OO didn’t become a revolution, and dominant in the mainstream, until the
1990s. Why then? The reason the revolution happened was primarily that our industry was driven by
requirements to write larger and larger systems that solved larger and larger problems and exploited
the greater and greater CPU and storage resources that were becoming available. OOP’s strengths in
abstraction and dependency management made it a necessity for achieving large-scale software
development that is economical, reliable, and repeatable.

Concurrency is the next major revolution in how we write software

Similarly, we’ve been doing concurrent programming since those same dark ages, writing coroutines
and monitors and similar jazzy stuff. And for the past decade or so we’ve witnessed incrementally
more and more programmers writing concurrent (multi-threaded, multi-process) systems. But an actual
revolution marked by a major turning point toward concurrency has been slow to materialize. Today
the vast majority of applications are single-threaded, and for good reasons that I’ll summarize in
the next section.

By the way, on the matter of hype: People have always been quick to announce “the next software
development revolution,” usually about their own brand-new technology. Don’t believe it. New
technologies are often genuinely interesting and sometimes beneficial, but the biggest revolutions
in the way we write software generally come from technologies that have already been around for some
years and have already experienced gradual growth before they transition to explosive growth. This
is necessary: You can only base a software development revolution on a technology that’s mature
enough to build on (including having solid vendor and tool support), and it generally takes any new
software technology at least seven years before it’s solid enough to be broadly usable without
performance cliffs and other gotchas. As a result, true software development revolutions like OO
happen around technologies that have already been undergoing refinement for years, often decades.
Even in Hollywood, most genuine “overnight successes” have really been performing for many years
before their big break.

Concurrency is the next major revolution in how we write software. Different experts still have
different opinions on whether it will be bigger than OO, but that kind of conversation is best left
to pundits. For technologists, the interesting thing is that concurrency is of the same order as OO
both in the (expected) scale of the revolution and in the complexity and learning curve of the
technology.

<Benefits and Costs of Concurrency>

There are two major reasons for which concurrency, especially multithreading, is already used in
mainstream software. The first is to logically separate naturally independent control flows; for
example, in a database replication server I designed it was natural to put each replication session
on its own thread, because each session worked completely independently of any others that might be
active (as long as they weren’t working on the same database row). The second and less common reason
to write concurrent code in the past has been for performance, either to scalably take advantage of
multiple physical CPUs or to easily take advantage of latency in other parts of the application; in
my database replication server, this factor applied as well and the separate threads were able to
scale well on multiple CPUs as our server handled more and more concurrent replication sessions with
many other servers.

There are, however, real costs to concurrency. Some of the obvious costs are actually relatively
unimportant. For example, yes, locks can be expensive to acquire, but when used judiciously and
properly you gain much more from the concurrent execution than you lose on the synchronization, if
you can find a sensible way to parallelize the operation and minimize or eliminate shared state.

Perhaps the second-greatest cost of concurrency is that not all applications are amenable to
parallelization. I’ll say more about this later on.

Probably the greatest cost of concurrency is that concurrency really is hard: The programming model,
         meaning the model in the programmer’s head that he needs to reason reliably about his
         program, is much harder than it is for sequential control flow.

Everybody who learns concurrency thinks they understand it, ends up finding mysterious races they
thought weren’t possible, and discovers that they didn’t actually understand it yet after all. As
the developer learns to reason about concurrency, they find that usually those races can be caught
by reasonable in-house testing, and they reach a new plateau of knowledge and comfort. What usually
doesn’t get caught in testing, however, except in shops that understand why and how to do real
stress testing, is those latent concurrency bugs that surface only on true multiprocessor systems,
       where the threads aren’t just being switched around on a single processor but where they
       really do execute truly simultaneously and thus expose new classes of errors. This is the
       next jolt for people who thought that surely now they know how to write concurrent code: I’ve
       come across many teams whose application worked fine even under heavy and extended stress
       testing, and ran perfectly at many customer sites, until the day that a customer actually had
       a real multiprocessor machine and then deeply mysterious races and corruptions started to
       manifest intermittently. In the context of today’s CPU landscape, then, redesigning your
       application to run multithreaded on a multicore machine is a little like learning to swim by
       jumping into the deep end—going straight to the least forgiving, truly parallel environment
       that is most likely to expose the things you got wrong. Even when you have a team that can
       reliably write safe concurrent code, there are other pitfalls; for example, concurrent code
       that is completely safe but isn’t any faster than it was on a single-core machine, typically
       because the threads aren’t independent enough and share a dependency on a single resource
       which re-serializes the program’s execution. This stuff gets pretty subtle.

The vast majority of programmers today don’t grok concurrency, just as the vast majority of
programmers 15 years ago didn’t yet grok objects

Just as it is a leap for a structured programmer to learn OO (what’s an object? what’s a virtual
    function? how should I use inheritance? and beyond the “whats” and “hows,” why are the correct
    design practices actually correct?), it’s a leap of about the same magnitude for a sequential
programmer to learn concurrency (what’s a race? what’s a deadlock? how can it come up, and how do I
    avoid it? what constructs actually serialize the program that I thought was parallel? how is the
    message queue my friend? and beyond the “whats” and “hows,” why are the correct design practices
    actually correct?).

The vast majority of programmers today don’t grok concurrency, just as the vast majority of
programmers 15 years ago didn’t yet grok objects. But the concurrent programming model is learnable,
            particularly if we stick to message- and lock-based programming, and once grokked it
            isn’t that much harder than OO and hopefully can become just as natural. Just be ready
            and allow for the investment in training and time, for you and for your team.

(I deliberately limit the above to message- and lock-based concurrent programming models. There is
 also lock-free programming, supported most directly at the language level in Java 5 and in at least
 one popular C++ compiler. But concurrent lock-free programming is known to be very much harder for
 programmers to understand and reason about than even concurrent lock-based programming. Most of the
 time, only systems and library writers should have to understand lock-free programming, although
 virtually everybody should be able to take advantage of the lock-free systems and libraries those
 people produce. Frankly, even lock-based programming is hazardous.)

<What It Means For Us>

Okay, back to what it means for us.

1. The clear primary consequence we’ve already covered is that applications will increasingly need
to be concurrent if they want to fully exploit CPU throughput gains that have now started becoming
available and will continue to materialize over the next several years. For example, Intel is
talking about someday producing 100-core chips; a single-threaded application can exploit at most
1/100 of such a chip’s potential throughput. “Oh, performance doesn’t matter so much, computers just
keep getting faster” has always been a naïve statement to be viewed with suspicion, and for the near
future it will almost always be simply wrong.

Applications will increasingly need to be concurrent if they want to fully exploit continuing
exponential CPU throughput gains

Efficiency and performance optimization will get more, not less, important

Now, not all applications (or, more precisely, important operations of an application) are amenable
to parallelization. True, some problems, such as compilation, are almost ideally parallelizable. But
others aren’t; the usual counterexample here is that just because it takes one woman nine months to
produce a baby doesn’t imply that nine women could produce one baby in one month. You’ve probably
come across that analogy before. But did you notice the problem with leaving the analogy at that?
Here’s the trick question to ask the next person who uses it on you: Can you conclude from this that
the Human Baby Problem is inherently not amenable to parallelization? Usually people relating this
analogy err in quickly concluding that it demonstrates an inherently nonparallel problem, but that’s
actually not necessarily correct at all. It is indeed an inherently nonparallel problem if the goal
is to produce one child. It is actually an ideally parallelizable problem if the goal is to produce
many children! Knowing the real goals can make all the difference. This basic goal-oriented
principle is something to keep in mind when considering whether and how to parallelize your
software.

2. Perhaps a less obvious consequence is that applications are likely to become increasingly
CPU-bound. Of course, not every application operation will be CPU-bound, and even those that will be
affected won’t become CPU-bound overnight if they aren’t already, but we seem to have reached the
end of the “applications are increasingly I/O-bound or network-bound or database-bound” trend,
    because performance in those areas is still improving rapidly (gigabit WiFi, anyone?) while
    traditional CPU performance-enhancing techniques have maxed out. Consider: We’re stopping in the
    3GHz range for now. Therefore single-threaded programs are likely not to get much faster any
    more for now except for benefits from further cache size growth (which is the main good news).
    Other gains are likely to be incremental and much smaller than we’ve been used to seeing in the
    past, for example as chip designers find new ways to keep pipelines full and avoid stalls, which
    are areas where the low-hanging fruit has already been harvested. The demand for new application
    features is unlikely to abate, and even more so the demand to handle vastly growing quantities
    of application data is unlikely to stop accelerating. As we continue to demand that programs do
    more, they will increasingly often find that they run out of CPU to do it unless they can code
    for concurrency.

There are two ways to deal with this sea change toward concurrency. One is to redesign your
applications for concurrency, as above. The other is to be frugal, by writing code that is more
efficient and less wasteful. This leads to the third interesting consequence:

3. Efficiency and performance optimization will get more, not less, important. Those languages that
already lend themselves to heavy optimization will find new life; those that don’t will need to find
ways to compete and become more efficient and optimizable. Expect long-term increased demand for
performance-oriented languages and systems.

4. Finally, programming languages and systems will increasingly be forced to deal well with
concurrency. The Java language has included support for concurrency since its beginning, although
mistakes were made that later had to be corrected over several releases in order to do concurrent
programming more correctly and efficiently. The C++ language has long been used to write heavy-duty
multithreaded systems well, but it has no standardized support for concurrency at all (the ISO C++
    standard doesn’t even mention threads, and does so intentionally), and so typically the
concurrency is of necessity accomplished by using nonportable platform-specific concurrency features
and libraries. (It’s also often incomplete; for example, static variables must be initialized only
    once, which typically requires that the compiler wrap them with a lock, but many C++
    implementations do not generate the lock.) Finally, there are a few concurrency standards,
    including pthreads and OpenMP, and some of these support implicit as well as explicit
    parallelization. Having the compiler look at your single-threaded program and automatically
    figure out how to parallelize it implicitly is fine and dandy, but those automatic
    transformation tools are limited and don’t yield nearly the gains of explicit concurrency
    control that you code yourself. The mainstream state of the art revolves around lock-based
    programming, which is subtle and hazardous. We desperately need a higher-level programming model
    for concurrency than languages offer today; I'll have more to say about that soon.

<Conclusion>

If you haven’t done so already, now is the time to take a hard look at the design of your
application, determine what operations are CPU-sensitive now or are likely to become so soon, and
identify how those places could benefit from concurrency. Now is also the time for you and your team
to grok concurrent programming’s requirements, pitfalls, styles, and idioms.

A few rare classes of applications are naturally parallelizable, but most aren’t. Even when you know
exactly where you’re CPU-bound, you may well find it difficult to figure out how to parallelize
those operations; all the most reason to start thinking about it now. Implicitly parallelizing
compilers can help a little, but don’t expect much; they can’t do nearly as good a job of
parallelizing your sequential program as you could do by turning it into an explicitly parallel and
threaded version.

Thanks to continued cache growth and probably a few more incremental straight-line control flow
optimizations, the free lunch will continue a little while longer; but starting today the buffet
will only be serving that one entrée and that one dessert. The filet mignon of throughput gains is
still on the menu, but now it costs extra—extra development effort, extra code complexity, and extra
testing effort. The good news is that for many classes of applications the extra effort will be
worthwhile, because concurrency will let them fully exploit the continuing exponential gains in
processor throughput.


# ============================================================================
#{
==============================================================================
*kt_linux_core_290*	ref: concurrency in C++

{what-is-concurrency}
There is genuine or hardware concurrency or illusion of concurrency. What's new is that the
increased computing power of these machine comes not from running a single task faster but from
running multiple tasks in parallel.

<parallelism>
The task parallelism is to divide a single task into parts and run each in parallel, thus reducing
the total runtime. It can be complex since there may be many dependencies.

The data parallelism is that each thread performs the same operation on different parts of the data.
Good scalability; many hands make light work. There's a different focus in which more data can be
processed in the same amount of time.


{when-use-concurrency}
The use of concurrency is like any other optimisation strategy. Therefore it is only worth doing for
those performance critical parts of the application where there is the potential for mesurable gain.

{two-approaches-to-concurrency}
<use-multiple-process>
The downsides are 
o communication between processes.
o inherent overhead such as time to launch and resource in OS

The upsides are:
o safer code than threads
o can extend over a network

<use-multiple-thread>
The low overhead with launching and communicating between threads. So C++11 do not provide any
support for process and only support for threads.


{C++11}
o Allow writing portable multithreaded code without relying on platform-specific extensions.
o There is [abstraction penalty] compared to using the underlying low-level facilities directly.

<code-example>
#include <iostream>
#include <thread>

void hello()
{
  std::cout << "hello con world\n";
}

int main(int argc, char** argv)
{
  std::thread t(hello);
  t.join();
  // std::cout << "end of main" << std::endl;
}

If build and run like below:
kt@kt-ub-vb:~/work$ g++ -g -std=c++0x use-thread.cpp 
kt@kt-ub-vb:~/work$ ./a.out 
terminate called after throwing an instance of 'std::system_error'
  what():  Operation not permitted
Aborted (core dumped)

If build and run with pthread option, then works fine. This is known problem in g++.           
kt@kt-ub-vb:~/work$ g++ -g -std=c++0x -pthread use-thread.cpp 


==============================================================================
*kt_linux_core_291*	ref: concurrency in C++

{std-thread-basic}
<launch>
The std::thread works with any [callable] type; function object and lambda.

void do_some_work();
std::thread my_thread(do_some_work);

or

class background_task
{
  public:
    void operator() () const
    {
      do_something();
      do_something_else();
    }
};

background_task f;
std::thread my_thread(f);

Why callable? In this case, the supplied function object is copied into the storage belonging to the
created thread and invoked from there.

<join-and-detach>
If don't decide whether to join or to detach it before std::thread object is destroyed then program
is terminated since the std::thread destructor calls std::terminate(). Ture even in the presence of
exceptions.

The join() cleans up any storage associated with the thread so std::thread object isn't associated
with any thread. Once this, joinable() will return false.
{Q} what will happen when call join() twice on the same?

When detach a thread, make sure that the data accessed by the thread is vaild until the thread has
finished with it. For example, creat a thread within a function with thread function hold pointers
or reference to local varaibles. Bad idea and avoid this. 

void oops()
{
  int some_local_state = 0;
  T my_func(some_local_state);   // T(int&)
  std::thread my_thread(my_func);
  my_thread.detach();
}

<join-raii>
To avoid program being terminated when an exception is thrown, accidental lifetime problems, how?
Can use try and catch but verbose and easy to get it wrong. 

If you wan to do particular action for all possible exit path, whether normal or exceptional, use
raii.

class thread_guard
{
  std::thread& t;

  public:
  explicit thread_guard( std::thread& t_ ): t(t_) {}
  ~thread_guard()
  {
    if( t.joinable() )
    {
      t.join();
    }
  }

  thread_guard( thread_guard const& )=delete;
  thread_guard& operator=( thread_guard const&)=delete;
};

void oops()
{
  int some_local_state = 0;
  T my_func(some_local_state);         // T(int&)
  std::thread my_thread(my_func);
  thread_guard g(t);                   // <DN>

  do_something_in_current_thread();    // main thread continue to run
}


==============================================================================
*kt_linux_core_292*	ref: concurrency in C++

# ============================================================================
#{
==============================================================================
*kt_linux_core_300*	case: own semaphore and mutex class using pthred cond var

POSIX semaphore are system calls which means expensive. Is it possible to implement semaphore without it?

{class-semaphore}

This is for linux. When count is 0, waits and there is no upper limit. Also see that use one mutex
with many condition variables for semaphores.


{util-class}

Just to provide util funcs to all instances since these are static. Also SetPriority is not used.

class CThreadSelf
{
private:
	CThreadSelf(void) {}

public:
	// Returns the ID of the current executing thread.
	'static' int Id(void)
	{ return (int)pthread_self(); }

	'static' bool SetPriority(int priority);
	{
#if defined _LINUX

		  switch (priority)
		  {
			 case CThread::PRIORITY_HIGH: // [note] class type member
				 setpriority(PRIO_PROCESS, pthread_self(), -10); // [note] man setpriority
		       // param.sched_priority = 60;
				 break;
			 case CThread::PRIORITY_NORMAL:
				 setpriority(PRIO_PROCESS, pthread_self(), 0);
		       // param.sched_priority = 50;
				 break;
			 case CThread::PRIORITY_LOW:
		       // param.sched_priority = 40;
				 setpriority(PRIO_PROCESS, pthread_self(), 10);
				 break;
			 default:
				 return false;
		  }

#elif defined _WIN32
	}
};


{semaphore} [KT] the case uses containment(composition) to have implementation.

Use init count but no max count. 0 means to wait and other values means it is okay to get. Used as a
class memeber.

{Q} why need this? code says it calls sched_yield whenever sem count reaches 16.

#define	CONFIG_MAXIMUM_YIELD_COUNTER 16
static unsigned char semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;

pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER; [KT] this is global

struct PSemaphore
{
	pthread_cond_t  cond;
	int             count;
};

class Semaphore
{
	 private:
	 	PSemaphore* m_id;

	 public:
		Semaphore() { m_id = NULL; }
		virtual ~Semaphore() { assert( FlagCreate() == false); }

		bool Create(int count) // initial count
		{
			 pthread_mutex_lock(&mtx);

			 assert( FlagCreate() == false );

			 pthread_cond_t cond = PTHREAD_COND_INITIALIZER;

			 m_id = 'new' PSemaphore;	// new and m_id is not null
			 assert( m_id != NULL );
			 
			 m_id->cond = cond;  // [KT] is it okay as it is local variable?
			 m_id->count = count;

			 pthread_mutex_unlock(&mtx);

			 return m_id != NULL;
		}

		// return true when created
		bool FlagCreate() { return m_id != NULL; }

		virtual void Destory(void)
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  pthread_cond_destroy(&m_id->cond);
			  delete m_id;

			  m_id = NULL;

			  pthread_mutex_unlock(&mtx);
		}

		void Take()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  while (m_id->count <= 0)
			  {
				  pthread_cond_wait(&m_id->cond, &mtx);
			  }

			  m_id->count--;

			  pthread_mutex_unlock(&mtx);
		}

		void Give()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  m_id->count++;

			  pthread_cond_signal(&m_id->cond);

			  pthread_mutex_unlock(&mtx);

			  if (!semCounter--) {
				  sched_yield();
				  semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;
			  }
		}

		void Try(unsigned long msec = 0)
		{
			  if (msec == (unsigned long) INFINITY)
			  {
				  Take();

				  return true;
			  }

			  pthread_mutex_lock(&TimeMutex);
			  ASSERT(FlagCreate() == true);

			  struct timeval  now;
			  struct timespec timeout;
			  int             ret = 0;
			  bool            tf;


			  if (msec == 0)
			  {
				  if (m_id->count <= 0)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }
			  else
			  {
				  while ((m_id->count <= 0) && (ret != ETIMEDOUT))
				  {
					  gettimeofday(&now, NULL);
					  timeout.tv_sec  = now.tv_sec + msec / 1000;
					  timeout.tv_nsec = now.tv_usec + msec % 1000 * 1000;

					  while (timeout.tv_nsec > 1000000)
					  {
						  timeout.tv_nsec -= 1000000;
						  timeout.tv_sec++;
					  }

					  timeout.tv_nsec *= 1000;

					  ret = pthread_cond_timedwait(&m_id->cond, &TimeMutex, &timeout);
				  }

				  if (ret == ETIMEDOUT)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }

			  pthread_mutex_unlock(&TimeMutex);

			  return tf;
		}
};


{use-of-semaphore-one}

To make sure that an user can set prio once a thread is created.

class CThread
{
   PCSemaphore m_pidSync;

   Create()
   {
      m_pidSync.Create(0);
   }

   bool PCThread::SetPriority(int priority)
   {
      ASSERT(FlagCreate() == true);

      if (m_pid == -1)
      {
         m_pidSync.Take();
      }
      ...
   }

   inline void CThreadRun(CThread* thread)
   {
      thread->m_sync[0].Take();

      thread->m_pid = pthread_self();
      thread->m_pidSync.Give();

      thread->t_Main(); [KT] while loop on event get()

      thread->m_sync[1].Give();

      return;
   }
};


{mutex} 

CDerivedA: CMutex
 - thread   - Semaphore : uses global mutex

CDerivedB: CMutex
 - thread   - Semaphore : uses global mutex

Using sync always happens in the same thread. The case uses inheritance to have implementation. This
is based on the fact that mutex is a binary semaphore. Created with 1. Used to give the derived
class the lock/unlock feature to control interface access. That is, sync feature to class objects.
If need other use of sem, then have sem as a member.

class CMutex
{
	 private:
		  PCSemaphore 	m_sem; // [note] use of semaphore
		  int				m_tid;
		  int				m_count;

	 public:
	 	  // [note] no ctor?
	 	  virtual ~Mutex() { assert( FlagCreate() == false); }

		  bool Create(void)
		  {
				assert(FlagCreate() == false);

				if (m_sem.Create(1) == false) // [KT] create a sem
				{
					return false;
				}

				m_threadId = 0;

				return true;
		  }

		  virtual void Destroy(void)
		  {
				ASSERT(FlagCreate() == true);
				m_sem.Destroy();
		  }

		  bool FlagCreate(void) { return m_sem.FlagCreate(); }

		  void Lock(void)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;
				  return;
				}

				m_sem.Take();

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

		  }

		  bool Unlock(void);
		  {
				assert(FlagCreate() == true);

				if (m_threadId != CThreadSelf::Id())
				{
					return false;
				}

				m_count--;

				if (m_count > 0)
				{
				  return true;
				}

				m_threadId = 0;

				m_sem.Give();

				return true;
		  }

		  bool Try(unsigned long msec = 0)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;

				  return true;
				}

				if (m_sem.Try(msec) == false)
				{
				  return false;
				}

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

				return true;

		  }
};	

{cqueue}

struct PTEvent
{
	friend class PCQueue;
	friend class PCTask;

private:
	void* sync;

public:
	class PCHandler* receiver; //!< The Pointer to the handler that receives the event
	unsigned long    type;     //!< The event type

	//! Parameters of an event
	/*!
	 * The union of event parameters are 8-bytes long. That is, it can carry
	 * 2 long integers, 4 short integers, or 8 characters.
	 */
	union
	{
		long  l[2];
		short s[4];
		char  c[8];
	} param;
};

class CQueue : public CMutex
{
	private:
      PCSemaphore m_sem; // [KT] used to say that events are avaiable
		PTEvent     m_event[CONFIG_QUEUE_SIZE];

	bool CQueue::Create(void)
	{
		ASSERT(FlagCreate() == false);

		if (CMutex::Create() == false)
		{ return false; }

		if (m_sem.Create(0) == false)
		{
			CMutex::Destroy();
			return false;
		}

		m_in   = 0;		// [KT] this is {queue-contiguous-implementation} in *kt_dev*
		m_out  = 0;		// in(tail), out(head), size(count)
		m_size = 0;

		return true;
	}

	bool CQueue::Put(PTEvent* event, bool sync=false, bool priority=false)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		PCSemaphore sem;

		if (sync == false)
		{
			event->sync = NULL;
		}
		else
		{
			if (sem.Create(0) == false)
			{
				return false;
			}

			event->sync = &sem;
		}

		Lock();	// CMutex::Lock();

		int size;

		size = (priority == false) ? CONFIG_EMEGENCY_QUEUE_SIZE(16) : 0;
		size += m_size;

		if (size >= CONFIG_QUEUE_SIZE(256) )
		{
			Unlock();

			if (sync == true)
			{
				sem.Destroy();
			}

			PCDebug::Print("ERROR: Event queue full");

			return false;
		}

		if (priority == true)
		{
			m_out          = (m_out + CONFIG_QUEUE_SIZE - 1) % CONFIG_QUEUE_SIZE;
			m_event[m_out] = *event;
			m_size++;
		}
		// [KT] copy in event and inc count
		else
		{
			m_event[m_in] = *event;
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
			m_size++;
		}

		m_sem.Give();

      // [KT] As see how Get() uses m_sem, use m_sem to see if event are avaiable like a count or
      // length and then if there are call Lock() to lock access to this objects. May have some
      // performance gain from this.
		
      Unlock();	

		if (sync == true)
		{
			sem.Take();
			sem.Destroy();
		}

		return true;
	}

	bool PCQueue::Get(PTEvent* event, unsigned long msec = INFINITY)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		while (true)
		{
			if (m_sem.Try(msec) == false)
			{
				return false;
			}

			Lock();

			if (m_size == 0)
			{
				Unlock();

				continue;
			}

			*event = m_event[m_out];
			m_out  = (m_out + 1) % CONFIG_QUEUE_SIZE;
			m_size--;

			Unlock();

			return true;
		}
	}

};


{when-seek-one-specific}

{Q} This suggest that there are events for all(broadcast) and for specific ones. Move all event
between [head+1 ... tail-1] after tail. why? priority?

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver)
{ }

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver, unsigned long type)
{
	ASSERT(FlagCreate() == true);

	bool done = 0;

	Lock();

	unsigned long size = m_size;

	while (size-- != 0)
	{
		if (done == false && m_event[m_out].receiver == receiver && m_event[m_out].type == type)
		{
			*event = m_event[m_out];
			m_size--;
			done = true;
		}
		else
		{
			m_event[m_in] = m_event[m_out];
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
		}

		m_out = (m_out + 1) % CONFIG_QUEUE_SIZE;
	}

	if (done)
	{
		m_sem.Try();
	}

	Unlock();

	return done;
}

This q implementation uses q per thread. The different approach is to have q that threads share.
See *kt_linux_core_014* for msg q between threads


==============================================================================
*kt_linux_core_301*  case: use of mutex and thread class

This is case study using Mutex, Queue and Thread classes.

{CThread}

'inline' void CThreadRun(CThread* thread)
{
	thread->m_sync[0].Take();  // [KT] wait until signaled

#ifdef	_LINUX
	thread->m_pid = pthread_self();
	thread->m_pidSync.Give();
#endif

	thread->t_Main();

	thread->m_sync[1].Give();  // [KT] no use

	return;
}

'static' void* _Process(void* param)
{
	CThreadRun((CThread*)param);

	return NULL;
}

class CThread
{
private:

	int         m_id;
	int			m_pid;
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];

	friend void CThreadRun(CThread* thread); // [KT] inline friend

protected:

	virtual void t_Main(void) = 0;
	/*!< This function is the thread main virtual function.  As soon as a thread starts, this
	 * function is called.  When this function returns, the thread is terminated.
	 *
	 * You have to define this function when you define a new class that inherits CThread class.
	 */

public:

	//! Configuration constants
	enum PTConfigType
	{
		CONFIG_STACK_SIZE = 4096
	};

	enum PTPriorityType
	{
		PRIORITY_HIGH,
		PRIORITY_NORMAL,
		PRIORITY_LOW,
	};

	virtual ~CThread(void) { ASSERT(FlagCreate() == false); }

	bool Create(const char* name, unsigned long stackSize = CONFIG_STACK_SIZE);
	 {
		 ASSERT(FlagCreate() == false);

		 if (m_sync[0].Create(0) == false)
		 {
			 return false;
		 }

		 if (m_sync[1].Create(0) == false)
		 {
			 m_sync[0].Destroy();
			 return false;
		 }

#elif defined _LINUX

		 pthread_attr_t attr;
		 pthread_attr_init(&attr);
		 pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
		 pthread_attr_setstacksize(&attr, stackSize);
		 //struct sched_param schedParam;
		 //schedParam.sched_priority = 50;
		 //pthread_attr_getschedparam(&attr, &schedParam);
		 //pthread_attr_setschedpolicy(&attr, SCHED_RR);

		 m_pid = -1;
		 m_pidSync.Create(0);

		 if (pthread_create((pthread_t*)&m_id, &attr, _Process, this) != 0)
		 {
			 pthread_attr_destroy(&attr);
			 
			 ASSERT(!"CThread not created");

			 m_sync[0].Destroy();
			 m_sync[1].Destroy();

			 return false;
		 }

		 pthread_attr_destroy(&attr); 
		 
		 m_sync[0].Give();   // [KT] now a thread can run
		 return true;
	 }

	//! Check if the instance was created
	bool FlagCreate(void) { return m_sync[0].FlagCreate(); }
	//! Destroy the instance
	virtual void Destroy(void);

	//! Returns the ID of the CThread.
	int Id(void)
	{
		 ASSERT(FlagCreate() == true);
		 return m_id;
	}

	//! Set the priority value for the thread
	bool SetPriority(int priority);
};

The class hiarachy is:

class CThread
{
private:
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];
};

class CQueue : public CMutex

class CTask: private CThread, public CQueue, public CHandler
{
   private:
      void t_Main(void);

   protected:
      virtual bool t_Create(void);
};

class CSIEngineBase : public PCTask

class CSIVoiceEngine : public CSIEngineBase

class CSIEngineManager


<run>

CThread:
	CThreadRun: 
      thread->t_Main();	[pure-virtual] [main-start]

CTask:
              	t_Main()                 [main-end] {template-method} in kt_dev_txt
					{
				   	t_Create();                      [virtual] [create-start]
                 	while (ExecuteEvent() == true)   [thread-loop]
						//	virtual bool ExecuteEvent
						//	{
						//		 PTEvent event;
						//		 Get(&event, m_msec); [copy-in-event]
						//		 event.receiver->OnEvent(&event);
						//	}

					  	t_Destory();
					}

CSIEngineBase:
                                                 
CSIVoiceEngine:
                                                  t_Create() [create-end]
																     SendSelfEvent( OWN_EVENT_TYPE );

<event>

CHandler:
	protected:
	virtual bool t_OnEvent(const PTEvent* event) = 0;

	public:
	inline bool OnEvent
	{
      t_OnEvent() [pure-virtual] [event-start]
	}

CTask: public CHandler
	No OnEvent which means use CHandler one

	protectd:
	bool PCTask::t_OnEvent(const PTEvent* event)
	{
		has default basic event handling
	}

	static Send( PTEvent* event, bool sync = false, bool priority = false);
	  [to-other-task]
	  if ((sync == false) || (event->receiver->Task()->Id() != PCThreadSelf::Id()))
	  {
		 return event->receiver->Task()->Put(event, sync, priority);
		 {
			  In CQueue::Put, use copy-ctor of event structure to copy it into receiver's taks
			  m_event[].

			  m_event[i] *event;
		 }
	  }
	  [self] ends up with a func call
	  event->receiver->OnEvent(event);
		   
CSIEngineBase:
   public:
      bool SendSelfEvent(int nEventType)
      {
         PTEvent evt;
         evt.receiver = this; [event-to-self]
         evt.type = nEventType;
         Send(&evt);   // CTask::
      }

  protected:
      virtual bool t_OnEvent(const PTEvent* event)
      { return true; }

		virtual bool t_OnEvent( PTEvent* ) [event-end]
      {
		   PCTask::t_OnEvent(event); {hook-operation} in {template-method}
		   t_ProcessEvent(event); [virtual] [start]
      }

CSIVoiceEngine:
   SendSelfEvent( OWN_EVENT_TYPE );  // CSIEngineBase::

   virtual bool t_ProcessEvent(event);           [end]
   {
	   handle OWN_EVENT_TYPE;
   }


<create>

CThread:
   Create:
	  pthread_create

CTask:
	public:
   Create()
	  PCQueue::Create();
	  PCHandler::Create(this);
	  CThread::Create(stackSize);

CSIEngineBase: public CTask
	public:
   Create(const char* name)
      return CTask::Create(name);					[create]

CSIVoiceEngine: public CSIEngineBase
   no Create:

// {design-note}
// This can be a application manager which creates all applications and call Create() on them. This
// is platform wide and each application can override t_Create and init their own thing without
// knowing Create() calls made from outside.

class CSIEngineManager:
{
	 private:
	 CSIEngineBase* m_pVoiceEngine;

	 static CSIEngineManager* CSIEngineManager::GetInstance(void) {factory-method}
	 {
		 if(m_EngineManagerInstance == NULL)
		 {
			 m_EngineManagerInstance = 'new' CSIEngineManager;
		 }

		 return m_EngineManagerInstance;
	 }

	 CSIEngineManager::GetEngineInstance
	 {
		  m_pVoiceEngine =  'new' CSIVoiceEngine;
		  m_pVoiceEngine->Create("SIVoiceEngine"); // [create] and use inherited implementation
	 }
}

From review, t_ prefix means a primitive operation and to be overridden and all works are using the
most derived class object. Therefore, dreived version will be used for virtuals as shown
{template-method}


==============================================================================
*kt_linux_core_302*  case: analysis of 200 and 201 case

{how-this-work}

CUserClass instance:
                                       
CUserClass {
   // Eash CTask has a Q and thread
   CTask { CQueue, CHandler : CQueue { CMutex }
      CThread
         : pthread( staic _Process )
      }
}
  
static _Process(this) will run the derived class CTask t_Main which has a message loop. So this
_Process is a template code for all threads. 

Get(&event); in which all threads use the same code for thread routine

CUserB instance:
                                       
CUserB {
   CTask {
      CThread
         : pthread( staic _Process )
      }
}

Mutex class via inheritance:
Mutex #01     Mutex #02    Mutex #03     Mutex #04    Mutex #04  
(Semaphore)   (Semaphore)  (Semaphore)   (Semaphore)  (Semaphore)

Sem member in a class:
Sepmphore #01  Sepmphore #02 ...


Regarding q, each task has a q and other task can call put to insert a mesg to receiver's q.

{Q} Eash has its own pthread cond in Semaphore but all use a single global mutex for signaling. How
about performance? Is it better solution?


==============================================================================
*kt_linux_core_303*  case: msg q between threads

This uses stl q and sems to read, write and count(length) lock:


/** Maximum length of message queue name */
#define MQ_NAME_LENGTH 5

/** Queue Magic identifier Corresponds to ASCII QuEu*/
#define MQ_MAGIC 0x51754575

typedef struct PFMMessageQueueInfo_t_
{
    char                name[ MQ_NAME_LENGTH ];
    PCSemaphore         *readsem;
    PCMutex             *writeLock;
    PCMutex             *readLock;
    uint32_t            magic;
    std::queue<SPfmMessage>* container;
} PFMMessageQueueInfo_t;


extern "C"
{
///////////////////////////////////////////////////
// PFM Queue Create
///////////////////////////////////////////////////
HPfmQueue
pfmQueueCreate(const char* name, uint32_t max_size)
{
    PFMMessageQueueInfo_t *qptr;

    // Allocate queue controll structure
    qptr = (PFMMessageQueueInfo_t *)pfmMalloc( sizeof(PFMMessageQueueInfo_t) );
    if(!qptr)
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc\n");
        return PFM_NULL_HANDLE;
    }

    // create storage container
    qptr->container = new std::queue<SPfmMessage>();
    if (qptr->container == NULL)
    {
        fprintf(stderr,
            "pfmQueueCreate failed to alloc storage\n");
        pfmFree(qptr);
        return PFM_NULL_HANDLE;
    }

    // Create & initialize Read mutex for read serialization
    qptr->readLock = new PCMutex;
    if( qptr->readLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc read mutex\n");

        delete qptr->container;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    qptr->readLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init read mutex\n");

        delete qptr->container;
        delete qptr->readLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create & initialize Write mutex for write serialization
    qptr->writeLock = new PCMutex;
    if( qptr->writeLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }
    qptr->writeLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create and initialize read semaphore
    qptr->readsem = new PCSemaphore;
    if( qptr->readsem == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc semaphore\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    //Initialize read semaphore so it will "block" on try.
    qptr->readsem->Create(0);
    if( !qptr->readsem->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc sem\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        delete qptr->readsem;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Copy semaphore name
    if( !name )
    {
        char nameTmp[] = "SEM";
        PCString::Copy( qptr->name,nameTmp,MQ_NAME_LENGTH);
    }
    else
    {
        PCString::Copy( qptr->name,name,MQ_NAME_LENGTH);
    }

    qptr->name[ (MQ_NAME_LENGTH-1) ] = (char)NULL;
    qptr->magic = MQ_MAGIC;

    return (HPfmQueue)qptr;
}

///////////////////////////////////////////////////
// PFM Queue Destroy
///////////////////////////////////////////////////
pfmerr_t
pfmQueueDestroy(HPfmQueue h)
{
    // Validate input
    if( h == PFM_NULL_HANDLE)
    {
        pfmAssert( h != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)h;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Mark queue as invalid.
    qptr->magic = 0;

    // Release Reader (if exists) & Grab read/write locks.
    // This shall prevent "use while destruct" scenario.
    // Mutexes can be destroyed while locked.
    qptr->readsem->Give();
    qptr->readLock->Lock();
    qptr->writeLock->Lock();

    if( qptr->readsem->FlagCreate() )
    {
        qptr->readsem->Destroy();
    }
    if( qptr->readLock->FlagCreate() )
    {
        qptr->readLock->Destroy();
    }
    if( qptr->writeLock->FlagCreate() )
    {
        qptr->writeLock->Destroy();
    }

    delete qptr->readsem;
    delete qptr->readLock;
    delete qptr->writeLock;

    delete qptr->container;

    pfmFree( qptr );

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Receive
///////////////////////////////////////////////////
pfmerr_t
pfmQueueReceive(HPfmQueue q, uint32_t timeout_ms, SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    unsigned long readLockTickStart;
    unsigned long readLockTickEnd;
    unsigned long tickDiff;
    uint32_t    waitTime;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Convert PFM infinite timeout to Shadwo's infinite timeout
    switch( timeout_ms )
    {
        case PFM_WAIT_NONE:
            waitTime = 0;
            break;
        case PFM_WAIT_FOREVER:
            waitTime = INFINITY;
            break;
        default:
            waitTime = timeout_ms;
            break;
    }

    // Do thread serialized reading
    readLockTickStart = PCTime::Tick();
    if( !qptr->readLock->Try(waitTime) )
    {
        return ERR_TIMEDOUT;
    }

    // If waiting for data, do the tick calculation to
    // potentially reduce wait value
    if( waitTime )
    {
        readLockTickEnd = PCTime::Tick();
        tickDiff = pfmTickDiff( readLockTickStart, readLockTickEnd );

        // Check if we have time to hang on a samephore
        if( tickDiff > waitTime )
        {
            // We have waited longer on a mutex, so wait as short as possible
            waitTime = 0;
        }
        else
        {
            waitTime -= tickDiff;
        }
    }

    // Consume semaphore as it is signalled after every insertion
    qptr->readsem->Try(waitTime);

    // Check if there's data in the queue. This should only happen
    // on wait with timeout. Should not occour in infinite timeout
    // scenario
    if (qptr->container->empty())
    {
        pfmAssert(waitTime != (uint32_t)INFINITY );
        qptr->readLock->Unlock();
        return ERR_TIMEDOUT;
    }

    // read message
    *msg = qptr->container->front();
    qptr->container->pop();

    // This should be the last possible place where
    // a dying queue could be trapped (unlock fails).
    if( !qptr->readLock->Unlock() )
    {
        return ERR_SYS;
    }

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Send
///////////////////////////////////////////////////
pfmerr_t
pfmQueueSend(HPfmQueue q, const SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    pfmerr_t res;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Grab an read Lock
    qptr->writeLock->Lock();

    qptr->container->push(*msg);

    // Inform readers that there's data avaliable.
    qptr->readsem->Give();

    //Unlock queue
    if( !qptr->writeLock->Unlock() )
    {
        // This should handle a dying queue
        res = ERR_SYS;
    }
    else
    {
        res = ERR_OK;
    }

    return res;
}


# ============================================================================
#{
==============================================================================
*kt_linux_core_400*  signal

{signal-is-notification}
A signal is a notification to a process that an event has occurred. Sometimes described as software
interrupts and are analogous to hardware interrupts in that they interrupt the normal flow of
execution of a program.

{signal-and-kernel}
One process can (if it has suitable permissions) send a signal to another process. Can be employed
as a synchronization technique, or even as a primitive form of interprocess communication (IPC). It
is also possible for a process to send a signal to itself. However, the usual source of many signals
sent to a process is the kernel.

{signal-symbolic-names}
Each signal is defined as a unique (small) integer, starting sequentially from 1. These integers are
defined in <signal.h> with symbolic names of the form SIGxxxx. Since the actual numbers used for
each signal vary across implementations, it is these symbolic names that are always used in
programs.

{pending}
A signal is said to be generated by some event. Once generated, a signal is later delivered to a
process, which then takes some action in response to the signal. Between the time it is generated
and the time it is delivered, a signal is said to be pending.

{signal-mask-per-process} {signal-block}
Sometimes, however, we need to ensure that a segment of code is not interrupted by the delivery of a
signal. To do this, we can add a signal to the process's signal mask-a set of signals whose delivery
is currently blocked. If a signal is generated while it is blocked, it remains pending until it is
later unblocked (removed from the signal mask).

If a process receives a signal that it is currently blocking, that signal is added to the process’s
set of pending signals.

That delivery of a signal is blocked during the execution of its handler (unless we specify the
SA_NODEFER flag to sigaction()). If the signal is (again) generated while the handler is
executing, then it is marked as pending and later delivered when the handler returns.

<signal-is-queued-or-not>
Standard signals can't be queued; delivered only once. The set of pending signals is only a mask; it
indicates whether or not a signal has occurred, but not how many times it has occurred. In other
words, if the same signal is generated multiple times while it is blocked, then it is recorded in
the set of pending signals, and later delivered, just once. One of the differences between standard
and realtime signals is that realtime signals are queued.

{signal-handler}
Instead of accepting the default for a particular signal, a program can change the action that
occurs when the signal is delivered. Can be used to ignore signals or to change the default. To
change a default is usually referred to as installing or establishing a signal handler. When a
signal handler is invoked in response to the delivery of a signal, we say that the signal has been
handled or, synonymously, caught.

{signal-reentrant} {async-signal-safe-function}
Because a signal handler may asynchronously interrupt the execution of a program at any point in
time, the main program and the signal handler in effect form two independent (although not
concurrent) threads of execution within the same process.

A function is said to be reentrant if it can safely be simultaneously executed by multiple threads
of execution in the same process. In this context, "safe" means that the function achieves its
expected result, regardless of the state of execution of any other thread of execution.

A function may be nonreentrant if it updates global or static data structures. A function that
employs only local variables is guaranteed to be reentrant because of race-condition. This book
shows an example using crypt() in both main and signal handler. This corrupts internal buffer which
is statically allocated and crypt uses when calls it with differnt parameter.

{Q} In short, it is reentrant if it do not cause race-condition. Even if it uses only local
variables, it can still cause race-condition. Doesn't it?

Such possibilities are in fact rife within the standard C library. For example, we already noted in
Section 7.1.3 that malloc() and free() maintain a linked list of freed memory blocks available for
reallocation from the heap. If a call to malloc() in the main program is interrupted by a signal
handler that also calls malloc(), then this linked list can be corrupted. For this reason, the
malloc() family of functions, and other library functions that use them, are nonreentrant.

Even if we are not using nonreentrant library functions, reentrancy issues can still be relevant. If
a signal handler updates programmer-defined global data structures that are also updated within the
main program, then we can say that the signal handler is nonreentrant with respect to the main
program.

If a function is nonreentrant, then its manual page will normally provide an explicit or implicit
indication of this fact. In particular, watch out for statements that the function uses or returns
information in statically allocated variables.

{async-signal-safe-function}
An async-signal-safe function is one that the implementation guarantees to be safe when called from
a signal handler. A function is async-signal-safe either because it is reentrant or because it is
not interruptible by a signal handler.

Table 21-1: Functions required to be async-signal-safe by POSIX.1-1990, SUSv2, and SUSv3

Real-world applications should avoid calling non-async-signal-safe functions from signal handlers.

{signal-and-proc}
The Linux-specific /proc/PID/status file contains various bit-mask fields that can be inspected to
determine a process’s treatment of signals. The bit masks are displayed as hexadecimal numbers, with
the least significant bit representing signal 1, the next bit to the left representing signal 2, and
so on. 

These fields are SigPnd (per-thread pending signals), ShdPnd (process-wide pending signals since
Linux 2.6), SigBlk (blocked signals), SigIgn (ignored signals), and SigCgt (caught signals).
(The difference between the SigPnd and ShdPnd fields will become clear when we describe the handling
 of signals in multithreaded processes in Section 33.2.) The same information can also be obtained
using various options to the ps(1) command.

{standard-and-realtime-signal}
Signals fall into two broad categories; standard and realtime. On Linux, the standard signals are
numbered from 1 to 31. We describe the standard signals in this chapter. The other set of signals
consists of the realtime signals.

<why-reliable-signal>
In early implementations, signals could be lost (i.e., not delivered to the target process) in
certain circumstances. Furthermore, although facilities were provided to block delivery of signals
while critical code was executed, in some circumstances, blocking was not reliable. These problems
were remedied in 4.2BSD, which provided so-called reliable signals.

However, the Linux signal(7) manual page lists more than 31 signal names. The excess names can be
accounted for in a variety of ways. Some of the names are simply synonyms for other names, and are
defined for source compatibility with other UNIX implementations. Other names are defined but
unused.

{signals}

<SIGABRT>
The abort() function (Section 21.2.2) generates a SIGABRT signal for the process, which causes it to
dump core and terminate.

<SIGINT>
When the user types the terminal interrupt character (usually Control-C), the terminal driver sends
this signal to the foreground process group. The default action for this signal is to terminate the
process.

<SIGPIPE>
This signal is generated when a process tries to write to a pipe, a FIFO, or a socket for which
there is no corresponding reader process. This normally occurs because the reading process has
closed its file descriptor for the IPC channel. See Section 44.2 for further details.

<SIGSEGV>
This very popular signal is generated when a program makes an invalid memory reference. A memory
reference may be invalid because the referenced page doesn’t exist e.g., it lies in an unmapped area
somewhere between the heap and the stack, the process tried to update a location in read-only memory
e.g., the program text segment or a region of mapped memory marked read-only, or the process tried
to access a part of kernel memory while running in user mode (Section 2.1). In C, these events often
result from dereferencing a pointer containing a bad address (e.g., an uninitialized pointer) or
passing an invalid argument in a function call. The name of this signal derives from the term
segmentation violation.

<SIGKILL>
This is the sure kill signal. It can’t be blocked, ignored, or caught by a handler, and thus always
terminates a process.

<SIGTERM>
This is the standard signal used for terminating a process and is the [default] signal sent by the
kill and killall commands. Users sometimes explicitly send the SIGKILL signal to a process using
kill -KILL or kill -9. 

<why-should-use-sigterm>
However, this is generally a mistake. A well-designed application will have a handler for SIGTERM
that causes the application to exit gracefully, cleaning up temporary files and releasing other
resources beforehand. Killing a process with SIGKILL bypasses the SIGTERM handler.  Thus, we should
always first attempt to terminate a process using SIGTERM, and reserve SIGKILL as a last resort for
killing runaway processes that don’t respond to SIGTERM.

<SIGURG>
This signal is sent to a process to indicate the presence of out-of-band (also known as urgent) data
on a socket

<SIGCHLD>
This signal is sent (by the kernel) to a parent process when one of its children terminates: either
by calling exit() or as a result of being killed by a signal. It may also be sent to a process when
one of its children is stopped or resumed by a signal. By default, SIGCHLD is ignored.

<SIGUSR1>
This signal and SIGUSR2 are available for programmer-defined purposes. The kernel never generates
these signals for a process. Processes may use these signals to notify one another of events or to
synchronize with each other. In early UNIX implementations, these were the only two signals that
could be freely used in applications. In fact, processes can send one another any signal, but this
has the potential for confusion if the kernel also generates one of the signals for a process.
Modern UNIX implementations provide a large set of realtime signals that are also available for
programmer-defined purposes (Section 22.8).


==============================================================================
*kt_linux_core_401*  signal example: use signal as synchronization


#include <signal.h>
#include "curr_time.h" /* Declaration of currTime() */
#include "tlpi_hdr.h"

#define SYNC_SIG SIGUSR1 /* Synchronization signal */

static void /* Signal handler - does nothing but return */
handler(int sig)
{
}

int
main(int argc, char *argv[])
{
  pid_t childPid;
  sigset_t blockMask, origMask, emptyMask;
  struct sigaction sa;

  setbuf(stdout, NULL); /* Disable buffering of stdout */

  // The sigemptyset() function initializes a signal set to contain no members. The sigfillset()
  // function initializes a set to contain all signals (including all realtime signals). After
  // initialization, individual signals can be added to a set using sigaddset() and removed using
  // sigdelset().

  sigemptyset(&blockMask);
  sigaddset(&blockMask, SYNC_SIG); /* Block signal */

  // We can use sigprocmask() to change the process signal mask, to retrieve the existing mask, or
  // both. The how argument determines the changes that sigprocmask() makes to the signal mask:

  if (sigprocmask(SIG_BLOCK, &blockMask, &origMask) == -1)
    errExit("sigprocmask");

  sigemptyset(&sa.sa_mask);
  sa.sa_flags = SA_RESTART;
  sa.sa_handler = handler;

  if (sigaction(SYNC_SIG, &sa, NULL) == -1)
    errExit("sigaction");

  switch (childPid = fork()) {
    case -1:
      errExit("fork");

    case 0: /* Child */

      /* Child does some required action here... */
      printf("[%s %ld] Child started - doing some work\n", currTime("%T"), (long) getpid());
      sleep(2); /* Simulate time spent doing some work */

      /* And then signals parent that it's done */
      printf("[%s %ld] Child about to signal parent\n", currTime("%T"), (long) getpid());

      if (kill(getppid(), SYNC_SIG) == -1)
        errExit("kill");

      /* Now child can do other things... */
      _exit(EXIT_SUCCESS);

    default: /* Parent */

      /* Parent may do some work here, and then waits for child to complete the required action */
      printf("[%s %ld] Parent about to wait for signal\n", currTime("%T"), (long) getpid());

      sigemptyset(&emptyMask);
      if (sigsuspend(&emptyMask) == -1 && errno != EINTR)
        errExit("sigsuspend");

      printf("[%s %ld] Parent got signal\n", currTime("%T"), (long) getpid());

      /* If required, return signal mask to its original state */
      if (sigprocmask(SIG_SETMASK, &origMask, NULL) == -1)
        errExit("sigprocmask");

      /* Parent carries on to do other things... */
      exit(EXIT_SUCCESS);
  }
}


# ============================================================================
#{
==============================================================================
*kt_linux_core_500* file io

#include <sys/stat.h>
#include <fcntl.h>
#include <sys/wait.h>
#include "tlpi_hdr.h"

int
main(int argc, char *argv[])
{
  int fd, flags;
  char template[] = "/tmp/testXXXXXX";

  setbuf(stdout, NULL);    // {KT} Disable buffering of stdout

  fd = mkstemp(template);  // opens a temporary file
  if (fd == -1)
    errExit("mkstemp");

  printf("File offset before fork(): %lld\n", (long long) lseek(fd, 0, SEEK_CUR));

  flags = fcntl(fd, F_GETFL);    // get file flags
  if (flags == -1)
    errExit("fcntl - F_GETFL");
  
  printf("O_APPEND flag before fork() is: %s\n", (flags & O_APPEND) ? "on" : "off");

  switch (fork()) {
    case -1:
      errExit("fork");

    case 0: /* Child: change file offset and status flags */
      if (lseek(fd, 1000, SEEK_SET) == -1)
        errExit("lseek");

      flags = fcntl(fd, F_GETFL); /* Fetch current flags */
      if (flags == -1)
        errExit("fcntl - F_GETFL");

      flags |= O_APPEND; /* Turn O_APPEND on */
      if (fcntl(fd, F_SETFL, flags) == -1)
        errExit("fcntl - F_SETFL");
      _exit(EXIT_SUCCESS);

    default: /* Parent: can see file changes made by child */
      if (wait(NULL) == -1)
        errExit("wait"); /* Wait for child exit */

      printf("Child has exited\n");
      printf("File offset in parent: %lld\n", (long long) lseek(fd, 0, SEEK_CUR));

      flags = fcntl(fd, F_GETFL);
      if (flags == -1)
        errExit("fcntl - F_GETFL");

      printf("O_APPEND flag in parent is: %s\n", (flags & O_APPEND) ? "on" : "off");
      exit(EXIT_SUCCESS);
  }
}

For an explanation of why we cast the return value from lseek() to long long in Listing 24-2, see
Section 5.10.


={============================================================================
*kt_linux_core_600*  time

{abstime}
The abstime is the system time - the number of seconds and nanoseconds past Jan. 1, 1970, UTC. The
advantage in using the abstime, instead of a delta, is if the function prematurely returns ( perhaps
because of a caught signal ): the function can be called again without having to change the timespec
structure. 


={============================================================================
*kt_linux_core_601*  time: currTime

{implementation}
#include <time.h>
#include "curr_time.h" /* Declares function defined here */

#define BUF_SIZE 1000

// Return a string containing the current time formatted according to the specification in 'format'
// (see strftime(3) for specifiers). If 'format' is NULL, we use "%c" as a specifier (which gives
// the date and time as for ctime(3), but without the trailing newline). Returns NULL on error.

char * currTime(const char *format)
{
  static char buf[BUF_SIZE]; /* Nonreentrant */
  time_t t;
  size_t s;
  struct tm *tm;
  t = time(NULL);
  tm = localtime(&t);
  if (tm == NULL)
    return NULL;
  s = strftime(buf, BUF_SIZE, (format != NULL) ? format : "%c", tm);
  return (s == 0) ? NULL : buf;
}

This '%T' returns '09:34:55' format.

From CTIME(3)                   Linux Programmer's Manual                  CTIME(3)

struct tm *localtime(const time_t *timep);

The ctime(), gmtime() and localtime() functions all take an argument of data  type  time_t which
represents calendar time.  When interpreted as an absolute time value, it represents the  number  of
seconds  elapsed since the Epoch, 1970-01-01 00:00:00 +0000 (UTC).

Broken-down  time  is  stored  in  the structure tm which is defined in <time.h> as follows:

  struct tm {
      int tm_sec;         /* seconds */
      int tm_min;         /* minutes */
      int tm_hour;        /* hours */
      int tm_mday;        /* day of the month */
      int tm_mon;         /* month */
      int tm_year;        /* year */
      int tm_wday;        /* day of the week */
      int tm_yday;        /* day in the year */
      int tm_isdst;       /* daylight saving time */
  };

The localtime() function converts the calendar time timep to broken-down time representation,
    expressed relative to the user's specified timezone. The function acts as if it called tzset(3)
    and sets the external variables tzname with information about the current timezone, timezone
    with the difference between Coordinated Universal Time (UTC) and local standard time in seconds,
    and daylight to a nonzero value if daylight savings time rules apply during some part of the
    year. The return value points to a statically allocated struct which might be overwritten by
    subsequent calls to any of the date and time  functions.


={============================================================================
*kt_linux_core_602*  time: ms and us

{how-to-get-ms-and-us}
<gettimeofday>  # method 01
To get ns(nano) from us(micro). 

#include <sys/time.h>

int gettimeofday(struct timeval *tv, struct timezone *tz);

The  functions  gettimeofday()  and  settimeofday() can get and set the time as well as a timezone.
The tv argument is a  struct  timeval  (as specified in <sys/time.h>):

  struct timeval {
    time_t      tv_sec;     /* seconds */
    suseconds_t tv_usec;    /* microseconds */
  };

and gives the number of seconds and microseconds since the Epoch (see time(2)). The use of the
timezone structure is obsolete; the tz argument should normally be specified as NULL.

#include <iostream>
#include <vector>
#include <sys/time.h>

using namespace std;

typedef uint64_t u64;

// return nano secs
static u64 nsec() {
  struct timeval tv;
  if(gettimeofday(&tv, 0) < 0)
    return -1;
  return (u64)tv.tv_sec*1000*1000*1000 + tv.tv_usec*1000;
}

int main()
{
  std::vector<int> ivec;

  uint64_t be = nsec();

  ivec.push_back(1);
  ivec.push_back(2);
  ivec.push_back(3);
  ivec.push_back(4);

  uint64_t af = nsec();

  cout << "diff(ns): " << af-be << endl;
  cout << "diff(ms): " << (af-be)/1000 << endl;
}

$ ./a.out 
diff(ns): 59000
diff(ms): 59

<clock_gettime> # method 02
NAME
       clock_gettime - Return the current timespec value of tp for the specified clock

SYNOPSIS
       long sys_clock_gettime (clockid_t which_clock, struct timespec *tp);

DESCRIPTION
       clock_gettime  returns  the  current timespec value of tp for the specific clock,
       which_clock.  The values that clockid_t currently supports for POSIX.1b timers, as defined in
       include/linux/time.h, are:

       CLOCK_REALTIME
              Systemwide realtime clock.

       CLOCK_MONOTONIC
              Represents monotonic time. Cannot be set.

       CLOCK_PROCESS_CPUTIME_ID
              High resolution per-process timer.

       CLOCK_THREAD_CPUTIME_ID
              Thread-specific timer.

If clock has 1024 hz resolution, then

1/1024 = 0.000.9765625 (the result is 976562.5 nanoseconds.)
1/1000 = 0.001.
1/2048 = 0.000.48828125

The unit is wider or shorter depending on a resolution.

struct timespec {
  time_t   tv_sec;        /* seconds */
  long     tv_nsec;       /* nanoseconds */     // note nano
};

The CLOCK_REALTIME clock measures the amount of time that has elapsed 'since' 00:00:00 January 1,
1970 Greenwich Mean Time (GMT)

int clock_gettime(clockid_t clock_id, struct timespec *tp); A return value of 0 shall
indicate that the call succeeded.

10-9 	1 nanosecond 	ns 	One billionth of one second
10-6 	1 microsecond 	µs 	One millionth of one second
10-3 	1 millisecond 	ms 	One thousandth of one second

This struct increase 'sec' when 'nsec' nano sec passes. so to get the total nano:

(tv_sec * 10+9) + tv_nsec nanoseconds


the following gets time in ms.

1 ms = 1 sec * 10+3
1 ms = 1 nsec * 10-6

Therfore, this is to get time in ms.

static uint32_t get_time()
{
  struct timespec time = {0, 0};
  uint32_t msec = 0U;
  clock_gettime(CLOCK_REALTIME, &time);

  // 86400 is secs for a day. so care only about secs but not day and hours and convert to ms

  msec = (time.tv_sec % 86400) * 1000;

  // get ms from nano
  msec += time.tv_nsec / 1000000;
  return msec;
}

// get time in µs 
//
static uint32_t time_get_us(void)
{
	 struct	 timespec ts;
	 xclock_gettime(CLOCK_REALTIME, &ts);
	 return (uint32_t)((ts.tv_sec * 1000000) + (ts.tv_nsec / 10000));
}

// start time
//
static void time_init(void)
{
	tstart = time_get();
}

// get diff between 'start' and 'now' in ms
//
static uint32_t time_get(void)
{
    struct	 timespec ts;
#if __mips__
    xclock_gettime(CLOCK_REALTIME, &ts);
#else
	clock_gettime(CLOCK_REALTIME, &ts);
#endif
	return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000)) - tstart;
}

// usuage
//
thi = time_get();

hour = (thi)/3600000; 				# hour in ms
msec = (thi - (hour*3600000));	# ms remains 
minute = msec / 60000; 				# mins 
msec = msec - (minute * 60000);	# ms remains
sec = msec / 1000; 					# secs
msec = msec - (sec * 1000); 		# ms remains

fprintf(of, "start check at: %.3d:%.2d:%.2d.%.3d\n", hour, minute, sec, msec);


/*
 * time in MIPS
 */
#include<stdio.h>
#include<unistd.h>
#include<linux/unistd.h>
#include<errno.h> 
#include <stdlib.h>
#include <string.h>
#include <time.h>

typedef unsigned int uint32_t;
static uint32_t tstart = 0;

#if __mips__ /* optimization for mips */
static inline void xclock_gettime(unsigned int which_clock, struct timeval * tv);
	
#define _syscall_clock_gettimeX(type,name,atype,a,btype,b) \
type x##name(atype a, btype b) \
{ \
	register unsigned long __a0 asm("$4") = (unsigned long) a; \
	register unsigned long __a1 asm("$5") = (unsigned long) b; \
	register unsigned long __a3 asm("$7"); \
	unsigned long __v0; \
	\
	__asm__ volatile ( \
	".set\tnoreorder\n\t" \
	"li\t$2, %4\t\t\t# " #name "\n\t" \
	"syscall\n\t" \
	"move\t%0, $2\n\t" \
	".set\treorder" \
	: "=&r" (__v0), "=r" (__a3) \
	: "r" (__a0), "r" (__a1), "i" (__NR_##name) \
	: "$2", "$8", "$9", "$10", "$11", "$12", "$13", "$14", "$15", "$24", \
	  "memory"); \
}

_syscall_clock_gettimeX(void, clock_gettime, unsigned int, which_clock, struct timeval *, tv);

#endif

static uint32_t time_get(void)
{
    struct	 timespec ts;
#if __mips__
    xclock_gettime(CLOCK_REALTIME, &ts);
	printf("call xclock_getttime: ts.tv_sec = %ld, ts.tv_nsec = %ld\n", ts.tv_sec, ts.tv_nsec);
#else
	clock_gettime(CLOCK_REALTIME, &ts);
#endif
	return (uint32_t)((ts.tv_sec * 1000) + (ts.tv_nsec / 1000000)) - tstart;
}

void main(int argc, char* agrv[]) 
{
	int hour = 0, minute = 0, sec = 0, msec = 0;
	uint32_t tlo =0, thi = 0;

	tstart = time_get();

	thi = time_get();
	hour = (thi)/3600000;
	msec = (thi - (hour*3600000));
	minute = msec / 60000;
	msec = msec - (minute * 60000);
	sec = msec / 1000;
	msec = msec - (sec * 1000);
	printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

	sleep(2);

	thi = time_get();
	hour = (thi)/3600000;
	msec = (thi - (hour*3600000));
	minute = msec / 60000;
	msec = msec - (minute * 60000);
	sec = msec / 1000;
	msec = msec - (sec * 1000);
	printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

	thi = 0xFFFFFFFF;
	hour = (thi)/3600000;
	msec = (thi - (hour*3600000));
	minute = msec / 60000;
	msec = msec - (minute * 60000);
	sec = msec / 1000;
	msec = msec - (sec * 1000);
	printf("thi:%ld, time: %.3d:%.2d:%.2d.%.3d\n", thi, hour, minute, sec, msec);  

	return;
}

-sh-3.2# ./a.out 
call xclock_getttime: ts.tv_sec = 946693753, ts.tv_nsec = 932727000
call xclock_getttime: ts.tv_sec = 946693753, ts.tv_nsec = 932829000
thi:   0, time: 000:00:00.000

call xclock_getttime: ts.tv_sec = 946693755, ts.tv_nsec = 933026000
thi:2001, time: 000:00:02.001

thi:  -1, time: 1193:02:47.295


{mktime} # localtime
time_t is a sec used in timespec

char *ctime(const time_t *timep);

The call ctime(t) is equivalent to asctime(localtime(t)). It converts the calendar time t
into a null-terminated string of the form

"Wed Jun 30 21:49:08 1993\n"

struct tm *localtime(const time_t *timep);

The localtime() function converts the calendar time timep to broken-down time
representation, expressed relative to the user's specified timezone.

The ctime(), gmtime() and localtime() functions all take an argument of data type time_t
which represents calendar time. When interpreted as an absolute time value, it represents
the number of seconds elapsed since the Epoch, 1970-01-01 00:00:00 +0000 (UTC).

time_t mktime(struct tm *tm);

The mktime() function converts a broken-down time structure, expressed as local time, to
calendar time representation.  The asctime() and mktime() functions both take an argument
representing broken-down time which is a representation separated into year, month, day,
etc.

Broken-down time is stored in the structure tm which is defined in <time.h> as follows:
this is the broken time is the local time based on time zone.

struct tm {
	int tm_sec;         /* seconds */
	int tm_min;         /* minutes */
	int tm_hour;        /* hours */
	int tm_mday;        /* day of the month */
	int tm_mon;         /* month */
	int tm_year;        /* year */
	int tm_wday;        /* day of the week */
	int tm_yday;        /* day in the year */
	int tm_isdst;       /* daylight saving time */
};

The members of the tm structure are:
tm_year 	The number of years since 1900.
tm_mon		The number of months since January, in the range 0 to 11.


to convert local time to UTC

t3=2011:12:15:13:5:5
sscanf(t3,"%d:%d:%d:%d:%d:%d",&year,&month,&day,&hrs,&mins,&secs);
mytime.tm_year=year - 1900;
mytime.tm_mon=month-1;
mytime.tm_mday=day;
mytime.tm_hour=hrs;
mytime.tm_min=mins;
mytime.tm_sec=secs;
mktime(&mytime)


# ============================================================================
#{
={============================================================================
*kt_linux_sete_001* ubuntu: virtualbox

{graphic-issue}
When see a problem with NVIDIA, need to install a driver for ubuntu manually:

http://askubuntu.com/questions/141606/how-to-fix-the-system-is-running-in-low-graphics-mode-error

In short, get console and do:

sudo apt-get install nvidia-current

Also, install the guest addition:

Click on Install Guest Additions from the Devices menu and all will be done automatically.


{sharing-between-os}
http://www.virtualbox.org/manual/ch04.html#sharedfolders

# set sharing folder on the host using virtual box menu and reboot
#
select "Shared folders" from the "Devices" menu, or click on the folder icon on the status bar in
the bottom right corner.

# shared folder is
#
/media/sf_myfiles 

# add group permission when see access error when reading shared folder
#
Access to auto-mounted shared folders is only granted to the user group vboxsf, which is created by
the VirtualBox Guest Additions installer. Hence guest users have to be member of that group to have
read/write access or to have read-only access in case the folder is not mapped writable.

sudo usermod -a -G vboxsf {username}


{sharing-using-samba}
<1> Have samba installation, setting, and user. This is simple and general and see as an example.
http://www.sitepoint.com/ubuntu-12-04-lts-precise-pangolin-file-sharing-with-samba/

sudo apt-get install samba samba-common system-config-samba winbind

change winbind setting

For samba server settings: Workgroup. (this is domain) This field should be the same value as that
used by your Windows Workgroupi.e if your WIndows Users are members of the 'Home' workgroup, type
'Home' in this field.  

For samba users: use windows user name.


<2> By default, VB uses Host-only networking and this means host(windows) cannot see guest. To
enable for host to see guest, change to 'bridged network' as shown in:

6.4. Bridged networking
http://www.virtualbox.org/manual/ch06.html

So change it in setting menu of VB and restart VM. Get IP and can access guest from host windows.

note: must have two network adaptors: NAT for internet and bridged for samba between host and guest.
But cannot ssh to other host. seems firewall problem and when back to host only network, ssh works
but not sharing as host only net gives private net ip.


==============================================================================
*kt_linux_sete_002*	ubuntu: workspace {shortcuts}

https://help.ubuntu.com/community/KeyboardShortcuts

{workspaces}
Known as virtual desktops. You can switch between workspaces with hotkeys by pressing Ctrl-Alt-arrow
key. 

To move apps to other workspaces, press Ctrl-Alt-Shift-arrow key.

{shortcuts}
C-A-T		" open a terminal

{compiz}
To have window like win management: win arrow keys
http://www.howtoforge.com/install-compiz-on-the-unity-desktop-on-ubuntu-12.04-precise-pangolin


==============================================================================
*kt_linux_sete_003*	ubuntu: samba

sudo service smbd start
sudo service smbd stop
sudo service smbd restart


To see what are shared:

$ smbclient -L //106.1.8.6/
Enter keitee.park's password:
Domain=[SERILOCAL] OS=[Unix] Server=[Samba 3.6.3]

        Sharename       Type      Comment
        ---------       ----      -------
        IPC$            IPC       IPC Service (rockford server (Samba, Ubuntu))
        dsk1            Disk      Main Disk
Domain=[SERILOCAL] OS=[Unix] Server=[Samba 3.6.3]

        Server               Comment
        ---------            -------

        Workgroup            Master
        ---------            -------
        WORKGROUP


==============================================================================
*kt_linux_sete_004*	ubuntu: nfs

sudo apt-get install nfs-kernel-server

You can configure the directories to be exported by adding them to the /etc/exports file. For
example:

/ubuntu  *(ro,sync,no_root_squash)
/home    *(rw,sync,no_root_squash)

You can replace * with one of the hostname formats. Make the hostname declaration as specific as
possible so unwanted systems cannot access the NFS mount.

To start the NFS server, you can run the following command at a terminal prompt:
sudo /etc/init.d/nfs-kernel-server start


==============================================================================
*kt_linux_sete_005*	ubuntu: check running services

sudo service --status-all


==============================================================================
*kt_linux_sete_006*	ubuntu: connect from windows remote desktop

sudo apt-get install xrdp

Then run windows remote desktop and connect using ip or hostname. That's it.


={============================================================================
*kt_linux_sete_007*	ubuntu: cpuinfo

cat /proc/cpuinfo 


={============================================================================
*kt_linux_sete_008*  ubuntu: change default application setting

Change a map between file type and default application in:

/usr/share/applications/defaults.list -> /etc/gnome/defaults.list


# ============================================================================
#{
={============================================================================
*kt_linux_ref_001* references

{ref-UNP}
Richard Stevens. Unix Network Programming Vol 2 Addision Wesley. 2nd Ed.
http://www.kohala.com/start/unpv22e/unpv22e.html

{ref-LPI}
The Linux Programming Interface: A Linux and UNIX System Programming Handbook 
Michael Kerrisk (Author) 

For sources, http://www.man7.org/tlpi/


-------------------------------------------------------------------------------
Copyright: see |ktkb|  vim:tw=100:ts=3:ft=help:norl:

