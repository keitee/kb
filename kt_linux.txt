*kt_linux*		tw=100

aWed 18 Dec 2013 09:47:23 GMT

KT KB. Linux

|kt_linux_tool_001|	bash history
|kt_linux_tool_002|	cut
|kt_linux_tool_003|	gdb: core file setting
|kt_linux_tool_004|	gdb: conditions to check
|kt_linux_tool_005|	gdb: signals for core
|kt_linux_tool_006|	gdb: run with gdb client and server/client {gdb-run-step}
|kt_linux_tool_007|	gdb: .gdbinit
|kt_linux_tool_008|	gdb: platform specific
|kt_linux_tool_009|	gdb: core analysis example 
|kt_linux_tool_010|	crash dump example 
|kt_linux_tool_011|	stdout and stderr
|kt_linux_tool_012|	to make a empty file
|kt_linux_tool_013|	to upgrade a kernel
|kt_linux_tool_014|	ssh and putty {scp}
|kt_linux_tool_015|	apt-xxx to get package
|kt_linux_tool_016|	check linux version
|kt_linux_tool_017|	redirection
|kt_linux_tool_018|	cmd: ls
|kt_linux_tool_019|	cmd: find
|kt_linux_tool_020|	cmd: time
|kt_linux_tool_021|	cmd: sort
|kt_linux_tool_022|	cmd: grep
|kt_linux_tool_023|	cmd: wc
|kt_linux_tool_024|	cmd: du
|kt_linux_tool_025|	cmd: ln

|kt_linux_tool_140|	cmake
|kt_linux_tool_141|	cmake: mix of c and cpp build
|kt_linux_tool_142|	cmake: cflags
|kt_linux_tool_143|	cmake: includes {message-keyword}
|kt_linux_tool_144|	cmake: link group

|kt_linux_core_001|	info at runtime
|kt_linux_core_002|	linux prio and scheduling
|kt_linux_core_003|	change shed policy after a boot
|kt_linux_core_004|	prio inversion
|kt_linux_core_005|	posix 
|kt_linux_core_006|	errno

|kt_linux_core_100|	sync: thread vs. process {zombie-process}
|kt_linux_core_101|	sync: pthread {nptl}
|kt_linux_core_102|	sync: how to run three threads sequencially
|kt_linux_core_103|	sync: mutex and cond-var {race-condition} {deadlock-condition} {mutex-ownership} {mutex-types} 
|kt_linux_core_104|	sync: read-write lock
|kt_linux_core_105|	sync: semaphore
|kt_linux_core_106|	sync: common problems when use threads {race-condition}
|kt_linux_core_107|	sync: atomic operations 

|kt_linux_core_200|	case: own semaphore and mutex class using pthread cond-var {cqueue}
|kt_linux_core_201|	case: use of mutex and thread class
|kt_linux_core_202|	case: analysis of 200 and 201 case
|kt_linux_core_203|	case: msg q between threads

|kt_linux_core_300|	ipc
|kt_linux_core_301|	ipc: pipe and fifo	
|kt_linux_core_310|	ipc: dbus
|kt_linux_core_031|	time

|kt_linux_core_400|	signal

|kt_linux_set_001|	ubuntu: virtualbox
|kt_linux_set_002|	ubuntu: workspace
|kt_linux_set_003|	ubuntu: samba
|kt_linux_set_004|	ubuntu: nfs
|kt_linux_set_005|	ubuntu: check running services
|kt_linux_set_006|	ubuntu: connect from windows remote desktop

|kt_linux_ref_001|	references



# ============================================================================
#{


==============================================================================
*kt_linux_tool_001*	bash history

{settings}

# eliminate the continuous repeated entry across the whole history

export HISTCONTROL=erasedups

# run multiple commands from the history

fc [-e ename] [-lnr] [first] [last]
fc -s [pat=rep] [cmd]

Fix  Command.   In  the  first form, a range of commands from first to last is selected
from the history list.  First and last may be specified as a string (to locate  the  last
command  beginning  with  that string)  or  as  a  number (an index into the history list,
where a negative number is used as an offset from the current command number).

This is not quite what you want as it will launch an editor first, but that is probably a
good thing since it gives you a chance to double check that you have the correct commands
and even edit them using all the capabilities of your favorite editor. Once you save you
changes and exit the editor, the commands will be run.

e.g.

$ fc 100 120

{reverse-search-history}

http://www.gnu.org/software/bash/manual/html_node/Commands-For-History.html

reverse-search-history (C-r)

Search backward starting at the current line and moving 'up' through the history as necessary. This
is an incremental search.

Once you've found the command you have several options:

1. Run it verbatim  just press Enter
2. Cycle through other commands that match the letters you've typed  press Ctrl-R successively
3. Quit the search and back to the command line empty-handed  press Ctrl-G
4. Take one by pressing ESC and edit the command line.

To set vi mode for editing command line:

8.3.1 Readline Init File Syntax

There are only a few basic constructs allowed in the Readline init file. Blank lines are ignored.
Lines beginning with a `#' are comments. Lines beginning with a `$' indicate conditional constructs
(see section 8.3.2 Conditional Init Constructs). Other lines denote variable settings and key
bindings.

Variable Settings
You can modify the run-time behavior of Readline by altering the values of variables in Readline
using the set command within the init file. The syntax is simple:

    set variable value

Here, for example, is how to change from the default Emacs-like key binding to use vi line editing
commands:

    set editing-mode vi

To set binding to up/down key to history search:

# ~/.inputrc
"\e[A": history-search-backward
"\e[B": history-search-forward

or equivalently,

# ~/.bashrc
bind '"\e[A": history-search-backward'
bind '"\e[B": history-search-forward'

Normally, Up and Down are bound to the Readline functions previous-history and next-history
respectively. I prefer to bind PgUp/PgDn to these functions, instead of displacing the normal
operation of Up/Down.

# ~/.inputrc
"\e[5~": history-search-backward
"\e[6~": history-search-forward

After you modify ~/.inputrc, restart your shell or use Ctrl+X, Ctrl+R to tell it to re-read
~/.inputrc. By the way, if you're looking for relevant documentation: Bash uses The GNU Readline
Library for the shell prompt and history.


==============================================================================
*kt_linux_tool_002*	cut

# input file
WIFI_SSID="SKY0F227"

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d=`
# this lead to WIFI_SSID='"SKY0F227"' and cannot use it as a var

WIFI_SSID=`grep '\<WIFI_SSID\>' {input} | cut -f 2 -d \"`
# this fixes the problem.


==============================================================================
*kt_linux_tool_003*	gdb: core file setting

# default

-sh-3.2# cat /proc/sys/kernel/core_pattern 
core


# set core dump location and format

echo '/tmp/%p.COR' >/proc/sys/kernel/core_pattern

mkdir -p /tmp/cores
chmod a+rwx /tmp/cores
echo "/tmp/cores/core.%e.%p.%h.%t" > /proc/sys/kernel/core_pattern


# force a core

ulimit -c unlimited

# SIGSEGV 	11 	Core 	Invalid memory reference 
# -s signal Specify the signal to send.  The signal may be given as a signal name or number.
 
kill -s SIGSEGV $$
kill -s 11 113

export P=`ps -a | grep APP_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;export P=`ps -a | grep MW_Process | cut -d' ' -f3`;echo $P;kill -s 11 $P;


# core

/tmp/cores/core.bash.8539.drehbahn-mbp.1236975953


the following pattern elements in the core_pattern file:

%p: pid
%: '%' is dropped
%%: output one '%'
%u: uid
%g: gid
%s: signal number
%t: UNIX time of dump
%h: hostname
%e: executable filename
%: both are dropped

# configure it forever

The changes done before are only applicable until the next reboot. In order to make the
change in all future reboots, you will need to add the following in /etc/sysctl.conf:

# own core file pattern...
kernel.core_pattern=/tmp/cores/core.%e.%p.%h.%t

sysctl.conf is the file controlling every configuration under /proc/sys

just wanted to say that there is no need to edit the file manually. simply run the sysctl
command, which does the stuff


==============================================================================
*kt_linux_tool_004*	gdb: conditions to check

conditions to check:

0. configured for cross platform

1. write permissions in the directory 

2. ulimit -c unlimited
this is shell command to set resource limit for a core.
-c     The maximum size of core files created 

3. compilation of the image: should be with debug symbols (not release version), and
statically compiled.  In cygwin compilations, static link is enabled by default.  In linux
compilations, you should modify platform.mk in your view.

If instead of the callstack you see only "??", it usually means the application wasn't
compiled with debug symbols (release version) or it was not compiled using static linkage

4. debug build
can use gdb without -g but need to map virtual address.


==============================================================================
*kt_linux_tool_005*	gdb: signals for core

SIG_KERNEL_COREDUMP_MASK (.../kernel/signal.c) defines sianals to create a core.

SIGSEGV(segmentation fault)

#define SIG_KERNEL_COREDUMP_MASK (\
        M(SIGQUIT)   |  M(SIGILL)    |  M(SIGTRAP)   |  M(SIGABRT)   | \
        M(SIGFPE)    |  M(SIGSEGV)   |  M(SIGBUS)    |  M(SIGSYS)    | \
        M(SIGXCPU)   |  M(SIGXFSZ)   |  M_SIGEMT                     )


==============================================================================
*kt_linux_tool_006*	gdb: run with gdb client and server/client

{gdb-run-step}

to use curses gui mode
$ gdb -tui a.out

# ./gdb ./FusionOS_Harmonizer_2.3.2.eng.sf.noam
GNU gdb 6.7.1
Copyright (C) 2007 Free Software Foundation, Inc.

This GDB was configured as "arm-linux-uclibcgnueabi"...
Using host libthread_db library "/lib/libthread_db.so.1".

(gdb) backtrace

(gdb) list

(gdb) whatis ppkSectionData
type = const uint8_t **

(gdb) whatis pszSection
type = uint32_t *

(gdb) print *pszSection

(gdb) run
This will start running the FOSH app.

(gdb) b CTL_SimpleZapperTestStep		# (b)reak
(gdb) break CTL_ChannelZapping_FullStbTearDown
(gdb) break readSectionFilterDataAndWriteToFile
(gdb) break SDS_SectionFilter_mReadSection
(gdb) run -v all
(gdb) sectionFilterTable[sfIdx]
(gdb) directory components/FOSH/FUSIONOS_XTV_TESTS/xtv_test/src/
(gdb) directory components/FOSH/FUSIONOS_TEST_SHARED/DSL/src/ (SDS_SectionFilter.c)

<breakpoint>

[b]reak main	Break at the beginning of the main() function
b 5				Break at line 5 of the current file
b hello.c:5		Break at line 5 of hello.c
i[nfo] b 		info breakpoints

clear main
delete num_break
enale num_break
disable num_break

[r]un

<step-command>

(gdb) [n]ext steps over a function call 
(gdb) [s]tep steps in a function 

What if the program is running but you forgot to set breakpoints? You can hit CTRL-C and that'll
stop the program wherever it happens to be and return you to a "(gdb)" prompt. At that point, you
could set up a proper breakpoint somewhere and continue to that breakpoint.

(gdb) [c]ontinue

One final shortcut is that just hitting RETURN will repeat the last command entered; this will save
you typing next over and over again.

To exit the current function and return to the calling function, 
(gdb) finish

To step for a single assembly instruction, use the 
(gdb) stepi

To continue to a specific location, use the 
(gdb) advance funcname


<watch>

to inspect over the course of the run

(gdb) [disp]lay var
(gdb) [undisp] disp_num
(gdb) info disp 	" to list disps

<print>

(gdb) printf "%d\n", i
40
(gdb) printf "%08X\n", i
00000028

print i
Print the value of variable i.

print *p
Print the contents of memory pointed to by p, where p is a pointer variable.

print x.field
Check the different members of a structure.

print x
Check all the members of a structure, assuming x is a structure.

print y-field
y is a pointer to a structure.

print array[i]
Print the i'th element of array.

print array
Print all the elements of array.

(gdb) print /x block1->magic
$5 = 0xabeaa5b3

(gdb) print /x block1 
$9 = 0x1150f4c

(gdb) print /x *block1
$8 = {magic = 0xabeaa5b3, size = 0x28, line = 0x0, owner = 0x0, header = 0x21, data = {free = {previous = 0x8, next = 0x115106c}, 
    userData = {0x0}}}

# db_contexts_array is a global var
(gdb) p db_contexts_array
(gdb) p db_contexts_array[-1]


{server-client}

run gdbserver on a target and run client on a PC that use sorce and resource.

(Server Side)

# gdbserver 172.18.200.224:2001 ./FusionOS_Harmonizer_2.9.0.final
Process ./FusionOS_Harmonizer_2.9.0.final created; pid = 366
Listening on port 2001
Remote debugging from host 172.18.150.161
You are registered as Observer0

172.18.200.224 is a target IP.


(Client Side)

[parkkt@ixion gdb dump]$ ./gdb-client
GNU gdb 6.7.1
Copyright (C) 2007 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "--host=i386-redhat-linux --target=arm-linux-uclibcgnueabi".
(gdb) 
(gdb) target remote 172.18.200.224:2001
Remote debugging using 172.18.200.224:2001
[New Thread 419]
0x2faad590 in ?? ()
(gdb) continue
Continuing.


==============================================================================
*kt_linux_tool_007*	gdb: .gdbinit

can define user func that have commands to run

$ more .gdbinit
set history save on
set history filename ./.gdb_history
set output-radix 16

define connect
    handle SIG32 nostop noprint pass
    handle SIG33 nostop noprint pass
#    b CTL_SimpleZapperTestStep
#    b CTL_ChannelZapping_FullStbTearDown
#               b readSectionFilterDataAndWriteToFile
    b sectionFilterTask
                b CTL_SectionFilter_Engine.c:2126
                b CTL_SectionFilter_Engine.c:2146
    directory components/FOSH/FUSIONOS_XTV_TESTS/xtv_test/src/
                i b
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define init_task
   set $t=&init_task
   printf "task name \"%s\", pid %05d \n", $t->comm, t->pid
end

# Helper function to find a task given a PID or the
# address of a task_struct.
# The result is set into $t
define find_task
  # Addresses greater than _end: kernel data...
  # ...user passed in an address
  if ((unsigned)$arg0 > (unsigned)&_end)
    set $t=(struct task_struct *)$arg0
  else
    # User entered a numeric PID
    # Walk the task list to find it
    set $t=&init_task
    if (init_task.pid != (unsigned)$arg0)
      find_next_task $t
      while (&init_task!=$t && $t->pid != (unsigned)$arg0)
        find_next_task $t
      end
      if ($t == &init_task)
        printf "Couldn't find task; using init_task\n"
      end
    end
  end
  printf "Task \"%s\":\n", $t->comm
end


Reads and executes the commands from init file (if any) in the current working directory.
This is only done if the current directory is different from your home directory.  Thus,
you can have more than one init file, one generic in your home directory, and another,
specific to the program you are debugging, in the directory where you invoke gdb.

{gdbinit-for-c++}

#
#   STL GDB evaluators/views/utilities - 1.03
#
#   The new GDB commands:
#       are entirely non instrumental
#       do not depend on any "inline"(s) - e.g. size(), [], etc
#       are extremely tolerant to debugger settings
#
#   This file should be "included" in .gdbinit as following:
#   source stl-views.gdb or just paste it into your .gdbinit file
#
#   The following STL containers are currently supported:
#
#       std::vector<T> -- via pvector command
#       std::list<T> -- via plist or plist_member command
#       std::map<T,T> -- via pmap or pmap_member command
#       std::multimap<T,T> -- via pmap or pmap_member command
#       std::set<T> -- via pset command
#       std::multiset<T> -- via pset command
#       std::deque<T> -- via pdequeue command
#       std::stack<T> -- via pstack command
#       std::queue<T> -- via pqueue command
#       std::priority_queue<T> -- via ppqueue command
#       std::bitset<n> -- via pbitset command
#       std::string -- via pstring command
#       std::widestring -- via pwstring command
#
#   The end of this file contains (optional) C++ beautifiers
#   Make sure your debugger supports $argc
#
#   Simple GDB Macros writen by Dan Marinescu (H-PhD) - License GPL
#   Inspired by intial work of Tom Malnar,
#     Tony Novac (PhD) / Cornell / Stanford,
#     Gilad Mishne (PhD) and Many Many Others.
#   Contact: dan_c_marinescu@yahoo.com (Subject: STL)
#
#   Modified to work with g++ 4.3 by Anders Elton
#   Also added _member functions, that instead of printing the entire class in map, prints a member.

# support for pending breakpoints - you can now set a breakpoint into a shared library before the it was loaded.
set breakpoint pending on

#
# std::vector<>
#

define pvector
    if $argc == 0
        help pvector
    else
        set $size = $arg0._M_impl._M_finish - $arg0._M_impl._M_start
        set $capacity = $arg0._M_impl._M_end_of_storage - $arg0._M_impl._M_start
        set $size_max = $size - 1
    end
    if $argc == 1
        set $i = 0
        while $i < $size
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
    end
    if $argc == 2
        set $idx = $arg1
        if $idx < 0 || $idx > $size_max
            printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
        else
            printf "elem[%u]: ", $idx
            p *($arg0._M_impl._M_start + $idx)
        end
    end
    if $argc == 3
      set $start_idx = $arg1
      set $stop_idx = $arg2
      if $start_idx > $stop_idx
        set $tmp_idx = $start_idx
        set $start_idx = $stop_idx
        set $stop_idx = $tmp_idx
      end
      if $start_idx < 0 || $stop_idx < 0 || $start_idx > $size_max || $stop_idx > $size_max
        printf "idx1, idx2 are not in acceptable range: [0..%u].\n", $size_max
      else
        set $i = $start_idx
        while $i <= $stop_idx
            printf "elem[%u]: ", $i
            p *($arg0._M_impl._M_start + $i)
            set $i++
        end
      end
    end
    if $argc > 0
        printf "Vector size = %u\n", $size
        printf "Vector capacity = %u\n", $capacity
        printf "Element "
        whatis $arg0._M_impl._M_start
    end
end

document pvector
    Prints std::vector<T> information.
    Syntax: pvector <vector> <idx1> <idx2>
    Note: idx, idx1 and idx2 must be in acceptable range [0..<vector>.size()-1].
    Examples:
    pvector v - Prints vector content, size, capacity and T typedef
    pvector v 0 - Prints element[idx] from vector
    pvector v 1 2 - Prints elements in range [idx1..idx2] from vector
end

#
# std::list<>
#

define plist
    if $argc == 0
        help plist
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 2
                printf "elem[%u]: ", $size
                p *($arg1*)($current + 1)
            end
            if $argc == 3
                if $size == $arg2
                    printf "elem[%u]: ", $size
                    p *($arg1*)($current + 1)
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist <variable_name> <element_type> to see the elements in the list.\n"
        end
    end
end

document plist
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist l - prints list size and definition
    plist l int - prints all elements and list size
    plist l int 2 - prints the third element in the list (if exists) and list size
end

define plist_member
    if $argc == 0
        help plist_member
    else
        set $head = &$arg0._M_impl._M_node
        set $current = $arg0._M_impl._M_node._M_next
        set $size = 0
        while $current != $head
            if $argc == 3
                printf "elem[%u]: ", $size
                p (*($arg1*)($current + 1)).$arg2
            end
            if $argc == 4
                if $size == $arg3
                    printf "elem[%u]: ", $size
                    p (*($arg1*)($current + 1)).$arg2
                end
            end
            set $current = $current._M_next
            set $size++
        end
        printf "List size = %u \n", $size
        if $argc == 1
            printf "List "
            whatis $arg0
            printf "Use plist_member <variable_name> <element_type> <member> to see the elements in the list.\n"
        end
    end
end

document plist_member
    Prints std::list<T> information.
    Syntax: plist <list> <T> <idx>: Prints list size, if T defined all elements or just element at idx
    Examples:
    plist_member l int member - prints all elements and list size
    plist_member l int member 2 - prints the third element in the list (if exists) and list size
end


#
# std::map and std::multimap
#

define pmap
    if $argc == 0
        help pmap
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 3
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p *($arg1*)$value
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p *($arg2*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 4
            set $idx = $arg3
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p *($arg1*)$value
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p *($arg2*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        if $argc == 5
            set $idx1 = $arg3
            set $idx2 = $arg4
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                set $valueLeft = *($arg1*)$value
                set $valueRight = *($arg2*)($value + sizeof($arg1))
                if $valueLeft == $idx1 && $valueRight == $idx2
                    printf "elem[%u].left: ", $i
                    p $valueLeft
                    printf "elem[%u].right: ", $i
                    p $valueRight
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap m - prints map size and definition
    pmap m int int - prints all elements and map size
    pmap m int int 20 - prints the element(s) with left-value = 20 (if any) and map size
    pmap m int int 20 200 - prints the element(s) with left-value = 20 and right-value = 200 (if any) and map size
end


define pmap_member
    if $argc == 0
        help pmap_member
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Map "
            whatis $tree
            printf "Use pmap <variable_name> <left_element_type> <right_element_type> to see the elements in the map.\n"
        end
        if $argc == 5
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u].left: ", $i
                p (*($arg1*)$value).$arg2
                set $value = $value + sizeof($arg1)
                printf "elem[%u].right: ", $i
                p (*($arg3*)$value).$arg4
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 6
            set $idx = $arg5
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u].left: ", $i
                    p (*($arg1*)$value).$arg2
                    set $value = $value + sizeof($arg1)
                    printf "elem[%u].right: ", $i
                    p (*($arg3*)$value).$arg4
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Map size = %u\n", $tree_size
    end
end

document pmap_member
    Prints std::map<TLeft and TRight> or std::multimap<TLeft and TRight> information. Works for std::multimap as well.
    Syntax: pmap <map> <TtypeLeft> <TypeRight> <valLeft> <valRight>: Prints map size, if T defined all elements or just element(s) with val(s)
    Examples:
    pmap_member m class1 member1 class2 member2 - prints class1.member1 : class2.member2
    pmap_member m class1 member1 class2 member2 lvalue - prints class1.member1 : class2.member2 where class1 == lvalue
end


#
# std::set and std::multiset
#

define pset
    if $argc == 0
        help pset
    else
        set $tree = $arg0
        set $i = 0
        set $node = $tree._M_t._M_impl._M_header._M_left
        set $end = $tree._M_t._M_impl._M_header
        set $tree_size = $tree._M_t._M_impl._M_node_count
        if $argc == 1
            printf "Set "
            whatis $tree
            printf "Use pset <variable_name> <element_type> to see the elements in the set.\n"
        end
        if $argc == 2
            while $i < $tree_size
                set $value = (void *)($node + 1)
                printf "elem[%u]: ", $i
                p *($arg1*)$value
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
        end
        if $argc == 3
            set $idx = $arg2
            set $ElementsFound = 0
            while $i < $tree_size
                set $value = (void *)($node + 1)
                if *($arg1*)$value == $idx
                    printf "elem[%u]: ", $i
                    p *($arg1*)$value
                    set $ElementsFound++
                end
                if $node._M_right != 0
                    set $node = $node._M_right
                    while $node._M_left != 0
                        set $node = $node._M_left
                    end
                else
                    set $tmp_node = $node._M_parent
                    while $node == $tmp_node._M_right
                        set $node = $tmp_node
                        set $tmp_node = $tmp_node._M_parent
                    end
                    if $node._M_right != $tmp_node
                        set $node = $tmp_node
                    end
                end
                set $i++
            end
            printf "Number of elements found = %u\n", $ElementsFound
        end
        printf "Set size = %u\n", $tree_size
    end
end

document pset
    Prints std::set<T> or std::multiset<T> information. Works for std::multiset as well.
    Syntax: pset <set> <T> <val>: Prints set size, if T defined all elements or just element(s) having val
    Examples:
    pset s - prints set size and definition
    pset s int - prints all elements and the size of s
    pset s int 20 - prints the element(s) with value = 20 (if any) and the size of s
end



#
# std::dequeue
#

define pdequeue
    if $argc == 0
        help pdequeue
    else
        set $size = 0
        set $start_cur = $arg0._M_impl._M_start._M_cur
        set $start_last = $arg0._M_impl._M_start._M_last
        set $start_stop = $start_last
        while $start_cur != $start_stop
            p *$start_cur
            set $start_cur++
            set $size++
        end
        set $finish_first = $arg0._M_impl._M_finish._M_first
        set $finish_cur = $arg0._M_impl._M_finish._M_cur
        set $finish_last = $arg0._M_impl._M_finish._M_last
        if $finish_cur < $finish_last
            set $finish_stop = $finish_cur
        else
            set $finish_stop = $finish_last
        end
        while $finish_first != $finish_stop
            p *$finish_first
            set $finish_first++
            set $size++
        end
        printf "Dequeue size = %u\n", $size
    end
end

document pdequeue
    Prints std::dequeue<T> information.
    Syntax: pdequeue <dequeue>: Prints dequeue size, if T defined all elements
    Deque elements are listed "left to right" (left-most stands for front and right-most stands for back)
    Example:
    pdequeue d - prints all elements and size of d
end



#
# std::stack
#

define pstack
    if $argc == 0
        help pstack
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = $size - 1
        while $i >= 0
            p *($start_cur + $i)
            set $i--
        end
        printf "Stack size = %u\n", $size
    end
end

document pstack
    Prints std::stack<T> information.
    Syntax: pstack <stack>: Prints all elements and size of the stack
    Stack elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    pstack s - prints all elements and the size of s
end



#
# std::queue
#

define pqueue
    if $argc == 0
        help pqueue
    else
        set $start_cur = $arg0.c._M_impl._M_start._M_cur
        set $finish_cur = $arg0.c._M_impl._M_finish._M_cur
        set $size = $finish_cur - $start_cur
        set $i = 0
        while $i < $size
            p *($start_cur + $i)
            set $i++
        end
        printf "Queue size = %u\n", $size
    end
end

document pqueue
    Prints std::queue<T> information.
    Syntax: pqueue <queue>: Prints all elements and the size of the queue
    Queue elements are listed "top to bottom" (top-most element is the first to come on pop)
    Example:
    pqueue q - prints all elements and the size of q
end



#
# std::priority_queue
#

define ppqueue
    if $argc == 0
        help ppqueue
    else
        set $size = $arg0.c._M_impl._M_finish - $arg0.c._M_impl._M_start
        set $capacity = $arg0.c._M_impl._M_end_of_storage - $arg0.c._M_impl._M_start
        set $i = $size - 1
        while $i >= 0
            p *($arg0.c._M_impl._M_start + $i)
            set $i--
        end
        printf "Priority queue size = %u\n", $size
        printf "Priority queue capacity = %u\n", $capacity
    end
end

document ppqueue
    Prints std::priority_queue<T> information.
    Syntax: ppqueue <priority_queue>: Prints all elements, size and capacity of the priority_queue
    Priority_queue elements are listed "top to buttom" (top-most element is the first to come on pop)
    Example:
    ppqueue pq - prints all elements, size and capacity of pq
end



#
# std::bitset
#

define pbitset
    if $argc == 0
        help pbitset
    else
        p /t $arg0._M_w
    end
end

document pbitset
    Prints std::bitset<n> information.
    Syntax: pbitset <bitset>: Prints all bits in bitset
    Example:
    pbitset b - prints all bits in b
end



#
# std::string
#

define pstring
    if $argc == 0
        help pstring
    else
        printf "String \t\t\t= \"%s\"\n", $arg0._M_data()
        printf "String size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "String capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "String ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pstring
    Prints std::string information.
    Syntax: pstring <string>
    Example:
    pstring s - Prints content, size/length, capacity and ref-count of string s
end

#
# std::wstring
#

define pwstring
    if $argc == 0
        help pwstring
    else
        call printf("WString \t\t= \"%ls\"\n", $arg0._M_data())
        printf "WString size/length \t= %u\n", $arg0._M_rep()._M_length
        printf "WString capacity \t= %u\n", $arg0._M_rep()._M_capacity
        printf "WString ref-count \t= %d\n", $arg0._M_rep()._M_refcount
    end
end

document pwstring
    Prints std::wstring information.
    Syntax: pwstring <wstring>
    Example:
    pwstring s - Prints content, size/length, capacity and ref-count of wstring s
end

#
# C++ related beautifiers (optional)
#

set print pretty on
set print object on
set print static-members on
set print vtbl on
set print demangle on
set demangle-style gnu-v3
set print sevenbit-strings off

set history filename ~/.gdb_history
set history save

# finally stop the silly "A debugging session is active." - question ... just quit both.
set confirm off


==============================================================================
*kt_linux_tool_008*	gdb: platform specific


In the gdb backtrace the function arguments will be displayed from the stack if they are
available. Otherwise, gdb displays the values of the argument registers and hopes for the
best. The arguments in the backtrace are mostly correct because gdb can often pull them
from the stack. When compiled -O0 functions always save their argument registers to the
stack, so it is only optimized code which can make debugging difficult in this way.

See mips-tdep.c from gdb source


==============================================================================
*kt_linux_tool_009*	gdb: core analysis example 

export LD_LIBRARY_PATH=/home/NDS-UK/parkkt/bins
mips-gdb
set solib-absolute-prefix /junk
set solib-search-path /home/NDS-UK/parkkt/com.nds.darwin.debugsupport/debug_libs/uClibc-nptl-0.9.29-20070423
file APP_Process 
core 272.COR
thread apply all bt full
bt

(gdb) f n

frame n 

Select frame number n. Recall that frame zero is the innermost (currently executing)
frame, frame one is the frame that called the innermost one, and so on. The
highest-numbered frame is the one for main.


# 01
#

(gdb) bt
#0 memset () at libc/string/mips/memset.S:132

(gdb) i r
	zero     at       v0         v1       a0         a1       a2       a3
R0	00000000 fffffffc [00000000] ffffffff [00000008] 00000000 00000004 00022000
	t0 t1 t2 t3 t4 t5 t6 t7
R8	00000004 00000000 ffffffff 000000c2 00000000 00000004 00a52630 00000000
	s0 s1 s2 s3 s4 s5 s6 s7
R16 00020000 00000000 00022004 2c57b174 00000004 2c70d4d0 2d9dcd30 00be6f70
	t8 t9 k0 k1 gp sp s8 ra
R24 00befcc8 2ab25160 00000000 00000000 00bf7e60 2d9dc858 2d9dcbd0 00993fdc
	sr       lo       hi       bad        cause    pc
	00008413 00000000 00000000 [00000000] 0080000c [2ab251b4]
	fsr fir
	00001004 00000000

# void *memset(void *s, int c, size_t n);

(gdb) disassemble $pc
Dump of assembler code for function memset:
0x2ab25160 <memset+0>: slti t1,a2,8
0x2ab25164 <memset+4>: bnez t1,0x2ab251d4 <memset+116>
-> 0x2ab25168 <memset+8>: move v0,a0
0x2ab2516c <memset+12>: beqz a1,0x2ab25184 <memset+36>
0x2ab25170 <memset+16>: andi a1,a1,0xff
0x2ab25174 <memset+20>: sll t0,a1,0x8
0x2ab25178 <memset+24>: or a1,a1,t0
0x2ab2517c <memset+28>: sll t0,a1,0x10
0x2ab25180 <memset+32>: or a1,a1,t0
0x2ab25184 <memset+36>: negu t0,a0 # negu d, s; d = -s;
0x2ab25188 <memset+40>: andi t0,t0,0x3
0x2ab2518c <memset+44>: beqz t0,0x2ab2519c <memset+60>
0x2ab25190 <memset+48>: subu a2,a2,t0
0x2ab25194 <memset+52>: swl a1,0(a0) # swl t, addr; Store word left/right
0x2ab25198 <memset+56>: addu a0,a0,t0
0x2ab2519c <memset+60>: andi t0,a2,0x7
0x2ab251a0 <memset+64>: beq t0,a2,0x2ab251c0 <memset+96>
0x2ab251a4 <memset+68>: subu a3,a2,t0
0x2ab251a8 <memset+72>: addu a3,a3,a0
0x2ab251ac <memset+76>: move a2,t0
-> 0x2ab251b0 <memset+80>: addiu a0,a0,8
-> 0x2ab251b4 <memset+84>: sw a1,-8(a0)
0x2ab251b8 <memset+88>: bne a0,a3,0x2ab251b0 <memset+80>

this shows that a0, first arg, was null. hence SEGFLT



# 02
#

(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc38, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x007f09c8 in MHWMemAllocStatic (pool=0x131bc38, size=64) at mem_static.c:63
#2  0x007e6854 in MEMMAN_API_AllocStaticP (pool=0x131bc38, size=41) at memman_st.c:350
#3  0x00419328 in DIAG_JAVA_GetJavaString (env=0x2d7056c0, l_java_string=0x2c4b3358, l_byte_array=0x2d705698, 
    l_len=0x2d70569c) at ../src/natdiag.c:63
#4  0x004197fc in DIAG_JAVA_GetJavaString (env=0x131bc38) at ../src/natdiag.c:341
#5  0x0041a0f8 in Java_com_nds_fusion_diagimpl_DiagImpl_natLogInfo (THIS=<value optimized out>, jClass=0x40, jInt=1, 
    jString=0x0) at sunnatdiag.c:177

[New process 124]
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
271     mem_blockpool.c: No such file or directory.
        in mem_blockpool.c

(gdb) bt
#0  MHWMemCheckBank (mpool=0x131bc40, size=64, mem_nb_bank=1) at mem_blockpool.c:271
#1  0x72617469 in ?? ()
Backtrace stopped: previous frame inner to this frame (corrupt stack?)

(gdb) i r
          zero       at       v0       v1       a0       a1       a2       a3
 R0   00000000 00000005 00000004 00000000 0131bc40 00000040 00000001 00000000 
            t0       t1       t2       t3       t4       t5       t6       t7
 R8   0000000c 00000001 ffffffff ffffffff 00000000 00000000 00d7f220 00000200 
            s0       s1       s2       s3       s4       s5       s6       s7
 R16  00000000 0131bc40 0131bc40 00000040 00000001 00000000 00000001 2d705b00 
            t8       t9       k0       k1       gp       sp       s8       ra
 R24  00f0c360 00b62d98 00000000 00000000 00f14570 2d7055a0 2d7057d8 007f09c8 
            sr       lo       hi      bad    cause       pc
      00008413 000efdcb 00000005 00000018 00800008 007eb73c 
           fsr      fir
      00001004 00000000 

(gdb) disassemble $pc
# source
# uint32_t MHWMemCheckBank(MEMMAN_API_MemoryPoolHandle * mpool, uint32_t size, # uint32_t mem_nb_bank)
# {
#     MemWholeMemory *bank = mpool->s_WholeMem;
# }
#
Dump of assembler code for function MHWMemCheckBank:
0x007eb6b8 <MHWMemCheckBank+0>: addiu   sp,sp,-48
0x007eb6bc <MHWMemCheckBank+4>: sw      s4,40(sp)
0x007eb6c0 <MHWMemCheckBank+8>: sw      s3,36(sp)
0x007eb6c4 <MHWMemCheckBank+12>:        sw      s2,32(sp)
0x007eb6c8 <MHWMemCheckBank+16>:        sw      ra,44(sp)
0x007eb6cc <MHWMemCheckBank+20>:        sw      s1,28(sp)
0x007eb6d0 <MHWMemCheckBank+24>:        sw      s0,24(sp)
0x007eb6d4 <MHWMemCheckBank+28>:        lw      s0,40(a0)
0x007eb6d8 <MHWMemCheckBank+32>:        lw      v1,36(a0)
0x007eb6dc <MHWMemCheckBank+36>:        lui     v0,0x4
0x007eb6e0 <MHWMemCheckBank+40>:        lw      a3,108(a0)
0x007eb6e4 <MHWMemCheckBank+44>:        and     v1,v1,v0
0x007eb6e8 <MHWMemCheckBank+48>:        addiu   t0,s0,12
0x007eb6ec <MHWMemCheckBank+52>:        addiu   v0,s0,4
0x007eb6f0 <MHWMemCheckBank+56>:        move    s2,a0
0x007eb6f4 <MHWMemCheckBank+60>:        movn    t0,v0,v1
0x007eb6f8 <MHWMemCheckBank+64>:        move    s3,a1
0x007eb6fc <MHWMemCheckBank+68>:        beqz    a3,0x7eb73c <MHWMemCheckBank+132>
0x007eb700 <MHWMemCheckBank+72>:        move    s4,a2
0x007eb704 <MHWMemCheckBank+76>:        lw      a1,0(t0)
0x007eb708 <MHWMemCheckBank+80>:        lw      v1,24(s0)
0x007eb70c <MHWMemCheckBank+84>:        lw      v0,104(a0)
0x007eb710 <MHWMemCheckBank+88>:        li      a2,100
0x007eb714 <MHWMemCheckBank+92>:        mul     v1,v1,a2
0x007eb718 <MHWMemCheckBank+96>:        mul     v0,a1,v0
0x007eb71c <MHWMemCheckBank+100>:       sltu    v0,v1,v0
0x007eb720 <MHWMemCheckBank+104>:       beqz    v0,0x7eb73c <MHWMemCheckBank+132>
0x007eb724 <MHWMemCheckBank+108>:       nop
0x007eb728 <MHWMemCheckBank+112>:       divu    zero,v1,a1
0x007eb72c <MHWMemCheckBank+116>:       teq     a1,zero,0x7
0x007eb730 <MHWMemCheckBank+120>:       mflo    a1
0x007eb734 <MHWMemCheckBank+124>:       jal     0x7efa14 <MEMMAN_SHL_Notify>
0x007eb738 <MHWMemCheckBank+128>:       subu    a1,a2,a1
-> 0x007eb73c <MHWMemCheckBank+132>:       lw      v0,24(s0)
...
---Type <return> to continue, or q <return> to quit---

(gdb) info locals
ind_bank = <value optimized out>
bank = (MemWholeMemory *) 0x0		#
pool_max = (uint32_t *) 0xc

(gdb) x/16wx 0x0131bc40 	# value of a0
0x131bc40:      0x0131f408      0x2ab97ca0      0x00000000      0x00000000
0x131bc50:      0x00000000      0x00000000      0x00000000      0x00000000
0x131bc60:      0xffffffff      0x00000000      0x00000000      0x41445054
0x131bc70:      0x5f444941      0x47000000      0x00000000      0x00000000

this shows that structure passed on a0 has some NULLs and means that this pool was already
destoried. when see destory func, it sets poolHandle->s_WholeMem=NULL;


==============================================================================
*kt_linux_tool_010*	crash dump example 


# 01
#

The crash dump When a crash in a kernel module happens, you should see output like the
following on the serial port or in the dmesg buffer (just run the dmesg command to see
it). 

<4>Unhandled kernel unaligned access[#1]:
<4>Cpu 0
<4>$ 0   : 00000000 10008400 {f7ffdfdf} 80070000	# {v0}
<4>$ 4   : c06e27c0 000ee208 8123a000 898d2680
<4>$ 8   : 00000000 7edbffff ffdeff7f fffb7fff
<4>$12   : fdf7fed7 000ee247 00000001 00000001
<4>$16   : 898d2680 00000000 00000001 00000001
<4>$20   : 898d2680 c06e2800 898d2680 00000001
<4>$24   : 00000001 c0504bcc                  
<4>$28   : 89cd2000 89cd3830 000ee208 c050cbe4
<4>Hi    : 00000128
<4>Lo    : 003e5708
<4>epc   : c050cc54 XHddLowIO+0x124/0x3d8 [xtvfs]     Tainted: P    # {pc} 
<4>ra    : c050cbe4 XHddLowIO+0xb4/0x3d8 [xtvfs]
<4>Status: 10008403    KERNEL EXL IE 
<4>Cause : 00800010
<4>BadVA : f7ffe073
<4>PrId  : 0002a044
<4>Modules linked in: xtvfs mhxnvramfs callisto_periph callisto_tuner callisto
<4>Process mount (pid: 404, threadinfo=89cd2000, task=89a249e8)
<4>Stack : 00000001 000ee08d 00000000 c04e523c c05394c4 00000000 00000000 f7ffdfdf
<4>        c06e2800 003e5708 00000001 000ee208 00000000 c04e523c c05394c4 00000000
<4>        c053950c c050cf50 00808000 c050d21c c06e2800 8567bc00 00000000 003e5708
<4>        c04d7f78 c04d7f58 ff7fffdf 000edf25 00000001 00000001 c067e200 c04d804c
<4>        c05394c4 89cd3950 00000000 c04e523c 000ee208 c06e2800 00000001 00000000
<4>        ...
<4>Call Trace:
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]
<4>[<c050cf50>] XHddLowRead+0x1c/0x28 [xtvfs]
<4>[<c04d7f78>] root_dir_devio_read+0x58/0x12c [xtvfs]
<4>[<c04d809c>] root_dir_devio_lock_read+0x50/0x84 [xtvfs]
<4>[<c04d8430>] RootDirCpyClusterReadBuffer+0xec/0x180 [xtvfs]
<4>[<c04d90cc>] RootDirCpyCheckCreate+0xf0/0x1214 [xtvfs]
<4>[<c04da344>] RootDirCpyInit+0x154/0x200 [xtvfs]
<4>[<c04d2944>] pc_ppart_init+0x10bc/0x12a8 [xtvfs]
<4>[<c04fb178>] XTVFS_CheckPpartInit+0x38/0x32c [xtvfs]
<4>[<c04fc684>] InitPpart+0x238/0x540 [xtvfs]
<4>[<c04fca28>] XTVFS_Mount+0x9c/0x490 [xtvfs]
<4>[<c050c8c8>] xtvfs_read_super+0x1e0/0x370 [xtvfs]
<4>[<c050bb80>] xtvfs_fill_super+0x18/0x48 [xtvfs]
<4>[<8007a6b0>] get_sb_bdev+0x114/0x194
<4>[<c050bb5c>] xtvfs_get_sb+0x2c/0x38 [xtvfs]
<4>[<80079f2c>] vfs_kern_mount+0x68/0xc4
<4>[<80079fe4>] do_kern_mount+0x4c/0x7c
<4>[<80094f10>] do_mount+0x5a8/0x614
<4>[<80095010>] sys_mount+0x94/0xec
<4>[<8000e5f0>] stack_done+0x20/0x3c
<4>
<4>
<4>Code: 00008821  8fa2001c  3c038007 <8c440094> 24631824  0060f809  8c46000c  ae020000  27de0001


Unhandled kernel unaligned access An unaligned access is a type of crash. Unlike a
segmentation fault, where a process tries to read memory that is not in its memory map,
and unaligned access is an attempt to read or write an address that is not on a word
boundry. On 32 bit CPUs this means an address not divisble by 4. Often this will generate
an exception and some software will handle the access by reading adjacent words and
piecing things together. But in our case the exception is unhandled. 


$0, $4, etc

This output shows the value of the registers. We are on a MIPS CPU and many of the
registers have defined uses. This
document(http://msdn.microsoft.com/en-us/library/aa448706.aspx) describes them. For
example, $4 to $7 are used to store the first 4 words of function arguments when calling a
function. In assembler these registers are referred to as a0 to a3. You can't know this in
advance, you have to read the MIPS documentation to find it out.  epc c050cc54
XHddLowIO+0x124/0x3d8 [xtvfs] This is the {exception-program-counter}. It shows the address
that the exception occurred at, and that this is 0x124 bytes into the function XHddLowIO
in the module xtvfs.ko. 


Getting the disassembly

Given a kernel module like xtvfs.ko, it is possible to see the disassembled code using the
objdump -D command. Since we have a mips module, we use the cross-compiler from the
Clearcase Fusion view, so our command will look something like: 

mips-linux-uclibc-objdump -D xtvfs.ko

We can then look for the function where we crashed, which is XHddLowIO from the epc trace
above. It starts like this: 

00045b30 <XHddLowIO>:
   45b30:       27bdffb8        addiu   sp,sp,-72
   45b34:       3c020002        lui     v0,0x2
   45b38:       afb50034        sw      s5,52(sp)
   45b3c:       345500d0        ori     s5,v0,0xd0
   45b40:       3c020000        lui     v0,0x0
   45b44:       afb7003c        sw      s7,60(sp)

From the call trace we can calculate the address offset in use. Recall: 
<4>[<c050cc54>] XHddLowIO+0x124/0x3d8 [xtvfs]

So c050cc54 - 0x124 - 45b30 = offset = c04c7000 (the start of loadded in memory for this
xtvfs.ko). The crash happened at c050cc54 which will appear as c050cc54 - c04c7000 = 45c54
in the disassembly. That code looks like this: 

or 45b30+0x124 = 45c54


   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)
   45c50:       3c030000        lui     v1,0x0
 {45c54}:       8c440094        lw      a0,148(v0)
   45c58:       24630000        addiu   v1,v1,0
   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

So we have crashed executing an lw instruction. 


Using the relocation table

Given a kernel module like xtvfs.ko, it is possible to see the offsets of functions (the
relocation table) using the objdump -r command. Since we have a mips module, we use the
cross-compiler from the Clearcase Fusion view, so our command will look something like: 

mips-linux-uclibc-objdump -r xtvfs.ko > xtvfs_relocations


The output near to our crash address of 45c54 is a table like this: 

 00045c20 R_MIPS_26         .text
 00045c2c R_MIPS_26         .text
 00045c44 R_MIPS_26         .text
 00045c50 R_MIPS_HI16       __getblk
 00045c58 R_MIPS_LO16       __getblk
 00045c78 R_MIPS_HI16       printk
 00045c80 R_MIPS_LO16       printk


Because we are using load time
relocation(http://eli.thegreenplace.net/2011/08/25/load-time-relocation-of-shared-libraries/)
of shared libraries, this table tells the operating system how to replace addresses in the
code. The first column is the address in the code, the second column is the type of
relocation to do, and the third column is the address to relocate. So the code at address
45c50 and 45c58 will get overwritten with the address of __getblk. That makes the
disassembly of the code near our crash look like this: 

   45c44:       08011728        j       45ca0 <XHddLowIO+0x170>
   45c48:       00008821        move    s1,zero
   45c4c:       8fa2001c        lw      v0,28(sp)
   45c50:       3c030000        lui     v1,__getblk
   45c54:       8c440094        lw      a0,148(v0)
   45c58:       24630000        addiu   v1,v1,0
   45c5c:       0060f809        jalr    v1
   45c60:       8c46000c        lw      a2,12(v0)
   45c64:       ae020000        sw      v0,0(s0)

Understanding __getblk

At 45c5c there is a jump instruction to v1 which has been loaded with the address of
__getblk. But we crashed immediately before that.

	So it seems we crashed while preparing to call __getblk. 
	
So it would help to understand this function. We can look it up in the kernel code: 
http://lxr.free-electrons.com/source/fs/buffer.c?v=2.6.30;a=mips#L1363

1362 /*
1363  * __getblk will locate (and, if necessary, create) the buffer_head
1364  * which corresponds to the passed block_device, block and size. The
1365  * returned buffer has its reference count incremented.
1366  *
1367  * __getblk() cannot fail - it just keeps trying.  If you pass it an
1368  * illegal block number, __getblk() will happily return a buffer_head
1369  * which represents the non-existent block.  Very weird.
1370  *
1371  * __getblk() will lock up the machine if grow_dev_page's try_to_free_buffers()
1372  * attempt is failing.  FIXME, perhaps?
1373  */
1374 struct buffer_head *
1375 __getblk(struct block_device *bdev, sector_t block, unsigned size)
1376 {
1377         struct buffer_head *bh = __find_get_block(bdev, block, size);
1378 
1379         might_sleep();
1380         if (bh == NULL)
1381                 bh = __getblk_slow(bdev, block, size);
1382         return bh;
1383 }
1384 EXPORT_SYMBOL(__getblk);

We should also notice that it can be called via inline function sb_blk : 

287 static inline struct buffer_head *
288 sb_getblk(struct super_block *sb, sector_t block)
289 {
290         return __getblk(sb->s_bdev, block, sb->s_blocksize);
291 }


Understanding where in our C code the crash happened

Now we can tell exactly where in our C code the crash happened. We know we were in the
function XHddLowIO from the call trace and now we know we were calling __getblk or
sb_getblk. In XHddLowIO in the XTVFS code we can see: 


/* allocate sector buffers */
for(i = 0; i < cnt; i++){
    bh_array[i] = sb_getblk(sb,  block++);
    if(!bh_array[i]){ /* no sufficient buffers */
        printk("\n BH_ArrayXHddLowIO: bh = 0, i = %d !!!!!", i);
        if(0 == i){ /* no at all */
            return X_ERROR;
        }
        /* use what we have */
        cnt = i;
    }

So it is likely that the crash happened very close to this sb_getblk call. 


Understanding the lw instruction

Recall the instruction that crashed: 

   45c54:       8c440094        lw      a0,148(v0)

What does that notation mean? We can look up information about the MIPS instructions:
Description: A word is loaded into a register from the specified address.  Operation: $t =
MEM[$s + offset]; advance_pc (4); Syntax: lw $t, offset($s) The whole instruction means:
load a0 with the address in v0 + 148. 


Understanding the MIPS registers

In MIPS, registers tend to have special functions, such as return addresses or function
parameters. We can read about this online. 

a0, the register we are writing to, is the first of the "function argument registers that
hold the first four words of integer type arguments." So a0 is the first argument to the
function we are calling.  v0 is a "function result register" and is also called $2. So we
know its value from the original trace: 

<4>$ 0   : 00000000 10008400 f7ffdfdf 80070000

It is f7ffdfdf. Which is an *odd number*. Since we are trying to read from this address
and do arithmetic (add 148) with it, this would explain why we get an unaligned access. 


Putting it all together

We are executing this line of C: 

bh_array[i] = sb_getblk(sb,  block++);

Because sb_getblk is an inline function, it has been expanded by the compiler into:
__getblk(sb->s_bdev, block, sb->s_blocksize); So our crashing instruction is adding 148
because 148 is the offset of s_bdev withing the sb struct. We hav verify this by looking
at struct super_block in the code.  However, sb has somehow become and odd number, and
THAT is our bug. 


==============================================================================
*kt_linux_tool_011*	stdout and stderr

On program startup, the integer file descriptors associated with the streams stdin,
stdout, and stderr  are  0, 1,  and  2,  respectively.

prog1 > /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log 2>&1
prog1 2>&1 | tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log


==============================================================================
*kt_linux_tool_012*	to make a empty file

can use 'touch' but when busybox do not support touch, can use following to make a empty
file or to reset a file.

cat /dev/null > file


==============================================================================
*kt_linux_tool_013*	to upgrade a kernel

1. build a kernel

2. copy built images to /boot
vmlinuz-2.6.9-22.ELsmp, System.map-2.6.9-22.ELsmp, initrd-2.6.9-22.ELsmp.img

3. change /etc/grub.conf that's multi-boot script to boot with built images.
# cat /etc/grub.conf (links to /boot/grub/grub.conf)


# grub.conf generated by anaconda
#
# Note that you do not have to rerun grub after making changes to this file
# NOTICE:  You do not have a /boot partition.  This means that
#          all kernel and initrd paths are relative to /, eg.
#          root (hd0,0)
#          kernel /boot/vmlinuz-version ro root=/dev/sda1
#          initrd /boot/initrd-version.img
#boot=/dev/sda
default=0
timeout=5
splashimage=(hd0,0)/boot/grub/splash.xpm.gz
hiddenmenu
title Red Hat Enterprise Linux ES (2.6.9-22.ELsmp)
        root (hd0,0)
        kernel /boot/vmlinuz-2.6.9-22.ELsmp ro root=LABEL=/ rhgb quiet
        initrd /boot/initrd-2.6.9-22.ELsmp.img
title Red Hat Enterprise Linux ES-up (2.6.9-22.EL)
        root (hd0,0)
        kernel /boot/vmlinuz-2.6.9-22.EL ro root=LABEL=/ rhgb quiet
        initrd /boot/initrd-2.6.9-22.EL.img


==============================================================================
*kt_linux_tool_014*	ssh and putty

{aim}
To switch hosts using key base instead of using password.

{ssh-keygen}

$ ssh-keygen <enter>

$ ls -al .ssh/
total 20
drwx------  2 parkkt ccusers 4096 Dec  9 13:24 .
drwxr-xr-x 15 parkkt ccusers 4096 Dec  9 12:47 ..
-rw-------  1 parkkt ccusers 1675 Dec  9 13:24 id_rsa
-rw-r--r--  1 parkkt ccusers  411 Dec  9 13:24 id_rsa.pub

The file we need to copy to the server is named id_dsa.pub. As you can see above, the file needed
exists. You may or may not have other files in ~/.ssh as I do. If the key doesn't exist, however,
you can make one as follows:

$cp id_rsa.pub authorized_keys
$scp -p ~/.ssh/authorized_keys ukstbuild3:.ssh/
(user@homebox ~ $ scp ~/.ssh/id_dsa.pub user@'servername':.ssh/authorized_keys)
(     -p      Preserves modification times, access times, and modes from the
             original file.)

This make one-way ssh connection which means the machine you are on is added the authorized_keys of
the server so can run scp from the machine to the server:

scp remote-server:{path}/filename .
scp filename remote-server:{path}/filename

note: if there is working ssh connection, can use tab key to get file completion when use scp.

note: ssh-copy is not working and key line in authorized_keys must be one line.


{to-check-match-pair}

$ ssh-keygen /?
  -y          Read private key file and print public key.

$ ssh-keygen -y -f id_rsa


{caution}

note:
If the file permissions are too open then ssh will not trust them, and will still prompt
you for your password. 

chmod 700 ~/.ssh
chmod 644 ~/.ssh/authorized_keys		# must check this as caused big grief when different
chmod 644 ~/.ssh/id_dsa_pub
chmod 644 ~/.ssh/known_hosts
chmod 600 ~/.ssh/id_dsa

{config}

When user name is different between servers, must have an entry in this file for servers to connect.

$ cat ~/.ssh/config
Host tizen
        Hostname 168.219.241.167
        IdentityFile ~/.ssh/id_rsa
        User keitee.park                 # this is username that can be different from real user.
        Port 29418

To debug ssh. note: shall use name on the command line
-v  # -vv
Verbose mode. Causes ssh to print debugging messages about its progress. This is helpful in
debugging connection, authentication, and configuration problems. Multiple -v options increase
the verbosity. The maximum is 3. 

$ ssh -vT tizen


{github-when-ssh-do-not-work}

Using SSH over the HTTPS port

Sometimes the administrator of a firewall will refuse to allow SSH connections entirely. If using
HTTPS cloning with credential caching is not an option, you can attempt to clone using an SSH
connection made over the HTTPS port. Most firewall rules should allow this, but proxy servers may
interfere. 

Testing

To test if SSH over the HTTPS port is possible, run this ssh command:

ssh -T -p 443 git@ssh.github.com
# Hi username! You've successfully authenticated, but GitHub does not provide shell access.

Make it so

If you are able to ssh to git@ssh.github.com over port 443, you can override your ssh settings to
force any connection to github.com to run though that server and port. To set this in your ssh
config, edit the file at ~/.ssh/config and add this section:

Host github.com
  Hostname ssh.github.com
  Port 443

You can test that this works by connecting to github.com:

ssh -T git@github.com
# Hi username! You've successfully authenticated, but GitHub does not provide shell access.


{putty-ssh-setup}

When use keys generated from putty.

o run puttygen to make key pairs. rsa or dsa
o get a pub key and save a pri key(ppk)
o run putty and set ssh key to use
  menu: connection: ssh: auth: private key file for auth: specify the path to a pri key.
o login to the host and add a pub key in the auth key list


{convert-rsa-key-to-putty-ppk}

To convert keys from linux machine to putty ppk and from ppk to lunux(opsnssh keys)

o run puttygen and menu: conversion: import key:
o save it as a pri key(ppk)


==============================================================================
*kt_linux_tool_015*	apt-xxx to get package

To search package:
apt-cache search <program name>

To install a package:
sudo apt-get install tk8.5 

To remove a package:
sudo apt-get purge tk8.5 

{update-and-upgrade}

Note: The apt-get install command is recommended because it upgrades one or more already installed
packages without upgrading every package installed, whereas the apt-get upgrade command installs the
newest version of all currently installed packages. In additon, apt-get update command must be
executed before an upgrade to resynchronize the package index files.

{update-error}

When see:
Update Error: Require Installation Of Untrusted Packages

Run manually on console

sudo apt-get upgrade xxx


==============================================================================
*kt_linux_tool_016*	check linux version

{ubuntu}

$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=12.04
DISTRIB_CODENAME=precise
DISTRIB_DESCRIPTION="Ubuntu 12.04.1 LTS"


==============================================================================
*kt_linux_tool_017*	redirection

(for sh, borne shell, bash)
prog1 > /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log 2>&1
prog1 2>&1 | tee /home/NDS-UK/parkkt/fosh_build_home/FUSIONOS/log


==============================================================================
*kt_linux_tool_018*	cmd: ls

{get-filename-only}

ls -1

{get-color}

For bash, copy /etc/DIR_COLORS into home as .dir_colors and edit it to change default values. Run
man dir_colors for help.


==============================================================================
*kt_linux_tool_019*	cmd: find

{find-mtime}

24 hours based so -mtime 3 means that 72 hours from the current time and -mtime -3 means 72 before.
To specify between 72 and 96, -mtime 3 and after 96, -mtime +3. If want to find file or dirs changed
most recently, use -mtime 0 or 1.


{find-to-get-the-number-of-files}

%find -type f | wc -l

{find-print0}

-print0

print the full file name on the standard output, followed by a null character (instead of the new-
line character that -print uses). This allows file names that contain newlines or other types of
white space to be correctly interpreted by programs that process the find output. This option
corresponds to the -0 option of xargs.

$ find . -name CMakeLists.txt -print0
./CMakeLists.txt./Source/CMakeLists.txt./Source/cmake/gtest/CMakeLists.txt./Source/WebKit2/CMakeLists.txt./Source/WebKit2/UIProcess/efl/po_tizen/CMakeLists.txt./Source/WebKit/CMakeLists.txt./Source/WebKit/efl/DefaultTheme/CMakeLists.txt./Source/JavaScriptCore/CMakeLists.txt./Source/JavaScriptCore/shell/CMakeLists.txt./Source/WebCore/CMakeLists.txt./Source/ThirdParty/gtest/CMakeLists.txt./Source/WTF/CMakeLists.txt./Source/WTF/wtf/CMakeLists.txt./Tools/WinCELauncher/CMakeLists.txt./Tools/tizen-webview-test/CMakeLists.txt./Tools/MiniBrowser/efl/CMakeLists.txt./Tools/CMakeLists.txt./Tools/TestWebKitAPI/CMakeLists.txt./Tools/EWebLauncher/CMakeLists.txt./Tools/EWebLauncher/ControlTheme/CMakeLists.txt./Tools/DumpRenderTree/TestNetscapePlugIn/CMakeLists.txt./Tools/DumpRenderTree/efl/CMakeLists.txt./Tools/WebKitTestRunner/CMakeLists.txtkeitee.park@rockford /home/tbernard/Git/vdTizen/webkit

{Q} when useful? To use the out as a single line?
Used for xargs

{find-exec}

{} where the filename will be inserted. Add \; at the end of the command to complete the required
syntax. note: there must be a space after {}

$ find . -name CMakeLists.txt -exec egrep PROJECT {} \;

To run dirtags script for each directory:

$ find * -type d -exec dirtags {} \;

{find-oring}

You can specify a logical "or" condition using -o:

find / \( -size +50 -o -mtime -3 \) -print
find /my/project/dir -name '*.c' -o -name '*.h'
find -name *.[ch]

This is from bash  expr: expr1 -or expr2 and this means expr1 -o expr2, but not POSIX compliant.

{find-ignore}

-path pattern

File name matches shell pattern pattern.  The metacharacters do not treat `/' or `.' specially; so,
for example,

find . -path "./sr*sc"

will print an entry for a directory called `./src/misc' (if one exists).  To ignore a  whole
directory tree,  use  -prune  rather  than  checking  every file in the tree.  For example, to skip
the directory `src/emacs' and all files and directories under it, and print the names of the other
files  found,  do something like this:

find . -path ./src/emacs -prune -o -print

Note  that the pattern match test applies to the whole file name, starting from one of the start
points named on the command line.  It would only make sense to use an absolute path name here if the
relevant start point is also an absolute path.  This means that this command will never match
anything:

find bar -path /foo/bar/myfile -print

The  predicate  -path is also supported by HP-UX find and will be in a forthcoming version of the
POSIX standard.

{find-sym}

-L     
Follow symbolic links.


==============================================================================
*kt_linux_tool_020*	cmd: time

kit@kit-vb:~/tizencore$ time -f "%E real,%U user,%S sys" ls -Fs
-f: command not found

real	0m0.143s
user	0m0.068s
sys	0m0.040s
kit@kit-vb:~/tizencore$ 

kit@kit-vb:~/tizencore$ /usr/bin/time -f "%E real,%U user,%S sys" ls -Fs
total 24
4 app-core/  4 appfw/  4 application/  4 app-service/  4 dlog/	4 README
0:00.00 real,0.00 user,0.00 sys
kit@kit-vb:~/tizencore$ 

Users of the bash shell need to use an explicit path in order to run the external time command and
not the shell builtin variant. On system where time is installed in /usr/bin, the first example
would become /usr/bin/time wc /etc/hosts


==============================================================================
*kt_linux_tool_021*	cmd: sort

See the unix power tool 22.6 for more. -u remove duplicates and sort against field [4,7].

sort -u -k 4,7

http://www.gnu.org/software/coreutils/manual/html_node/sort-invocation.html

--key=pos1[,pos2]

Specify a sort field that consists of the part of the line between pos1 and pos2 (or the end of the
line, if pos2 is omitted), inclusive.  Each pos has the form 'f[.c][opts]', where f is the number of
the field to use, and c is the number of the first character from the beginning of the field. Fields
and character positions are numbered starting with 1; a character position of zero in pos2 indicates
the field's last character. If '.c' is omitted from pos1, it defaults to 1 (the beginning of the
field); if omitted from pos2, it defaults to 0 (the end of the field). opts are ordering options,
allowing individual keys to be sorted according to different rules; see below for details. Keys can
span multiple fields.  Example: To sort on the second field, use --key=2,2 (-k 2,2). See below for
more notes on keys and more examples. See also the --debug option to help determine the part of the
line being used in the sort.

To sort
NDS: ^0946684946.752246 !ERROR -aem          < M:aem_list.c F:AEM_ListGetApplication L:01808 > Can't find
-1-- ----------2------- ---3-- -4--         -5 ------6----- ---------------7-------- ---8--- 9 -10--

sort -u -k 4,10 ndsfusion.test > ndsfusion.dic 


-t separator --field-separator=separator

Use character separator as the field separator when finding the sort keys in each line. By default,
fields are separated by the empty string between a non-blank character and a blank character. By
default a blank is a space or a tab, but the LC_CTYPE locale can change this.

That is, given the input line ' foo bar', sort breaks it into fields ' foo' and ' bar'. The field
separator is not considered to be part of either the field preceding or the field following, so with
'sort -t " "' the same input line has three fields: an empty field, 'foo', and 'bar'. However,
fields that extend to the end of the line, as -k 2, or fields consisting of a range, as -k 2,3,
retain the field separators present between the endpoints of the range.  To specify ASCII NUL as the
field separator, use the two-character string '\0', e.g., 'sort -t '\0''. 


mh5a_variable.c
mh5b_variable.c

sort -t _ -k 2,2


==============================================================================
*kt_linux_tool_022*	cmd: grep

To find files which has the given string:

grep -nro LINUX ./

       -s, --no-messages
              Suppress error messages about nonexistent or unreadable files.   Portability  note:
       -o, --only-matching
              Print only the matched (non-empty) parts of a matching line, with each such part on
              a separate output line.
       -n, --line-number
              Prefix each line of output with the 1-based line number within its input file.  (-n
              is specified by POSIX.)
       -R, -r, --recursive
              Read  all  files  under  each  directory, recursively; this is equivalent to the -d
              recurse option.

To specify the target files:

grep -nr --include *.c __setup ./
grep -nr --include *.c [a-z]*_initcall ./

{when-see-binary-errors-on-text-file}

Even if that is a text file, grep say it is binary file then use -a option.

(http://www.gnu.org/software/grep/manual/html_node/Usage.html)

Why does grep report Binary file matches? If grep listed all matching lines from a binary file,
it would probably generate output that is not useful, and it might even muck up your display. So gnu
grep suppresses output from files that appear to be binary files. To force gnu grep to output lines
even from files that appear to be binary, use the '-a' or '--binary-files=text' option. To eliminate
the Binary file matches messages, use the '-I' or '--binary-files=without-match' option. 

To exclude binaries:

 -I     Process a binary file as if it did not contain matching data; this is equivalent to
		  the --binary-files=without-match option.


{q-option}

Used in the script.

-q, --quiet, --silent

Quiet; do not write anything to standard output. Exit immediately with zero status if any match is
found, even if an error was detected. Also see the -s or --no-messages option. 

echo xx | grep -q '.gz$' && echo ture
echo xx.gz | grep -q '.gz$' && echo ture
ture

The script example:

##:zcat if this is a .gz, cat otherwise
F=cat
echo "$1" | grep -q '\.gz$' && F=zcat
$F "$1"


{case-example}
		
# what it's trying to do is that search through the same errors in logs which was uploaded in
# Feburary and was under translation.  for example,
# translation/Zone9-Box5_Feb_18_09_38_42b3e5c9130e1d758acdc71bb9d12b2a		
#

ls translation/ | grep Feb | while read x; do pushd -n translation/$x; pwd; egrep --color -ano 'btreePageFromDbPage' .detailed_output; popd -n; done 

$ egrep --color -an 'btreePageFromDbPage' translation/*_Feb_*/.detailed_output
translation/PaulRiley_Work_Feb_18_09_42_25d1235e4ebe020a369a64d728c2ce9f/.detailed_output:8:parser_result=<p1>MW_Process crash in #0 btreePageFromDbPage (pDbPage=0x2b994bcc, pgno=67, pBt=0x2bb06da4) (built by hudson)</p1>
$ 

$ egrep --color -ano 'event manager POLL time exceeded threshold' translation/*_Feb_*/LOGlastrun_realtime
translation/000000002704_Feb_1_14_05_3a6f10bd9e8934088df33e9d991faa74/LOGlastrun_realtime:2100:event manager POLL time exceeded threshold


==============================================================================
*kt_linux_tool_023*	cmd: wc

-l, --lines
     print the newline counts

{how-to-count-lines-recursively}

find . -name '*.php' | xargs wc -l

# should be re-written using sh scripting
#!/bin/bash
echo "debug..."
find debug -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "dsm"
find dsm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "include"
find include -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "main"
find main -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

#
echo "mah"
find mah -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5dec"
find mh5dec -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5eng"
find mh5eng -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mh5gpi"
find mh5gpi -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "mhv"
find mhv -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l

echo "pfm"
find pfm -name '*.[ch]' -o -name '*.[ch]pp' | xargs wc -l


==============================================================================
*kt_linux_tool_024*	cmd: du

' to print one level
du -h --max-depth=1

-h, --human-readable
              print sizes in human readable format (e.g., 1K 234M 2G)

' to show all and total
du -ach
du -sh


==============================================================================
*kt_linux_tool_025*	cmd: ln

       -f, --force
              remove existing destination files

       -s, --symbolic
              make symbolic links instead of hard links

ln -sf input output. E.g., ln -sf linux-2.4.25-2.8 linux


# ============================================================================
#{

==============================================================================
*kt_linux_tool_140*	cmake

<ref>
http://www.cmake.org/cmake/help/v2.8.9/cmake.html
http://stackoverflow.com/questions/6352123/multiple-directories-under-cmake

From:

./mheg
+-- CMakeLists.txt
+-- include
¦   +-- dbg.h
¦   +-- def.h
+-- main
¦   +-- main.c


To:

./mheg
+-- CMakeLists.txt
+-- include
¦   +-- dbg.h
¦   +-- def.h
+-- main
¦   +-- main.c
+-- mh5eng
¦   +-- CMakeLists.txt
¦   +-- sample.c
¦   +-- sample.h


The changes made to build:

../mheg/CMakeLists.txt

	 INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/mh5eng)

	 ADD_EXECUTABLE(${PROJECT_NAME}
		 main/main.c # uses sample.h
	 )

	 ADD_SUBDIRECTORY(mh5eng)

	 TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})

../mheg/mh5eng/CMakeLists.txt

	 SET(MH5ENG_HEADERS
		 sample.h
	 )

	 SET(MH5ENG_SOURCES
		 sample.c
	 )

	 ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES})

This create <libmh5eng.a> as a output


==============================================================================
*kt_linux_tool_141*	cmake: mix of c and cpp build

To compile the mix of c and cpp file, when try followings:

PROJECT(mhegproto C)

SET(MH5ENG_SOURCES
	xx.c
	mh5i_residentprogram_db.cpp
)

ADD_LIBRARY(mh5eng ${MH5ENG_HEADERS} ${MH5ENG_SOURCES} )

cmake do not compile cpp file although it is defined in the set variable. To make it built, must add
c++ build as below:

PROJECT(mhegproto C CXX)

From http://cmake.org/cmake/help/v2.8.8/cmake.html

project: Set a name for the entire project.

project(<projectname> [languageName1 languageName2 ... ] )

Sets the name of the project. Additionally this sets the variables <projectName>_BINARY_DIR and
<projectName>_SOURCE_DIR to the respective values.

Optionally you can specify which languages your project supports. Example languages are CXX (i.e.
C++), C, Fortran, etc. By default C and CXX are enabled. E.g. if you do not have a C++ compiler, you
can disable the check for it by explicitly listing the languages you want to support, e.g. C. By
using the special language "NONE" all checks for any language can be disabled. If a variable exists
called CMAKE_PROJECT_<projectName>_INCLUDE_FILE, the file pointed to by that variable will be
included as the last step of the project command.

note: by default? seems not.


==============================================================================
*kt_linux_tool_142*	cmake: cflags

To enable preprocessor, set -E as below.

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "-E ${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")


==============================================================================
*kt_linux_tool_143*	cmake: includes

Found that building a library in the subdir cannot find necessary includes.

root CMakeList.txt:

INCLUDE(FindPkgConfig)
pkg_check_modules(APPS_PKGS REQUIRED
	capi-appfw-application
	dlog
	edje
	elementary
	ecore-x
	evas
	utilX
	x11
	aul
	ail
)

ADD_EXECUTABLE(${PROJECT_NAME}
	main/main.c
	main/viewmgr.c
	main/view_main.c
)

ADD_SUBDIRECTORY(mh5eng)
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)

Here there is no problem to use headers from packages such as elementary but when add the same to
the one in /mh5eng then cannot find headers. When looked at flags used to build mh5eng, there is no
necessary -I. This is the same when add pkg_check_moudles in the mh5eng/CMakeList.txt. Thing is
build faild to update flags as expected.

The finding is that when move mh5eng then it builds without adding pkg_check_moudles in the
mh5eng/CMakeList.txt

...
ADD_SUBDIRECTORY(mh5dec)
ADD_SUBDIRECTORY(mah)
ADD_SUBDIRECTORY(mhv)
ADD_SUBDIRECTORY(pfm)

FOREACH (flag ${APPS_PKGS_CFLAGS})
    SET(EXTRA_CFLAGS "${EXTRA_CFLAGS} ${flag}")
ENDFOREACH(flag)
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5eng ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mh5dec ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mhv ${APPS_PKGS_LDFLAGS})
TARGET_LINK_LIBRARIES(${PROJECT_NAME} pfm ${APPS_PKGS_LDFLAGS})

CONFIGURE_FILE(${PACKAGE_NAME}.xml.in ${PACKAGE_NAME}.xml)

# Install 
INSTALL(TARGETS ${PROJECT_NAME} DESTINATION ${BINDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${PACKAGE_NAME}.xml DESTINATION ${MANIFESTDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/dummy.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_selected.png DESTINATION ${IMAGEDIR})
INSTALL(FILES ${CMAKE_SOURCE_DIR}/images/apps_wallpaper.png DESTINATION ${IMAGEDIR})

ADD_SUBDIRECTORY(data)
ADD_SUBDIRECTORY(mh5eng) [KT] moved to here

That suggests that where to put is important and not sure it is a GBS(git build system) or cmake
itself problem. 


Found that this error still happens when build cpp file and the solution is:

From:
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")

To:
MESSAGE("KT >>>>>PKGS_LDFLAGS>>>>>" : ${APPS_PKGS_CFLAGS})
SET(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${EXTRA_CFLAGS}")
SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${EXTRA_CFLAGS}")

Turns out that PKGS_CFLAGS will have necessary includes depending on pkg selection.


==============================================================================
*kt_linux_tool_144*	cmake: link group

To solve {cyclic-dependencies} in link, can use this:

From defining each: 
TARGET_LINK_LIBRARIES(${PROJECT_NAME} mah ${APPS_PKGS_LDFLAGS})
...

To use group:
TARGET_LINK_LIBRARIES(${PROJECT_NAME} -Wl,--start-group mhdebug pfm mh5eng mh5dec mhv mah
-Wl,--end-group ${APPS_PKGS_LDFLAGS})

 On 05/19/2011 11:11 AM, Anton Sibilev wrote:
> Hello!
> I'm wondering how I can use "--start-group archives --end-group"
> linker flags with "Unix Makefiles".
> May be somebody know the right way?

You might specify these flags immediately in TARGET_LINK_LIBRARIES():

CMAKE_MINIMUM_REQUIRED(VERSION 2.8 FATAL_ERROR)
PROJECT(LINKERGROUPS C)
SET(CMAKE_VERBOSE_MAKEFILE ON)
FILE(WRITE ${CMAKE_BINARY_DIR}/f.c "void f(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g1.c "void g1(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/g2.c "void g2(void){}\n")
FILE(WRITE ${CMAKE_BINARY_DIR}/h.c "void h(void){}\n")
ADD_LIBRARY(f STATIC f.c)
ADD_LIBRARY(g1 STATIC g1.c)
ADD_LIBRARY(g2 STATIC g2.c)
ADD_LIBRARY(h STATIC h.c)
FILE(WRITE ${CMAKE_BINARY_DIR}/main.c "int main(void){return 0;}\n")
ADD_EXECUTABLE(main main.c)
TARGET_LINK_LIBRARIES(main f -Wl,--start-group g1 g2 -Wl,--end-group h)

However, do you really need these flags? Refer to the target properties
[IMPORTED_]LINK_INTERFACE_MULTIPLICITY[_<CONFIG>] and the documentation
of TARGET_LINK_LIBRARIES().

Regards,

Michael

LINK_INTERFACE_MULTIPLICITY

Repetition count for STATIC libraries with cyclic dependencies.

When linking to a STATIC library target with cyclic dependencies the linker may need to scan more
than once through the archives in the strongly connected component of the dependency graph. CMake by
default constructs the link line so that the linker will scan through the component at least twice.
This property specifies the minimum number of scans if it is larger than the default. CMake uses the
largest value specified by any target in a component.


# ============================================================================
#{

==============================================================================
*kt_linux_core_001*	info at runtime 

# 
syn_retries=`cat /proc/sys/net/ipv4/tcp_syn_retries`
mtu_probing=`cat /proc/sys/net/ipv4/tcp_mtu_probing`

# can be either 'up' and 'unknown'
grep -q up /sys/class/net/eth0/operstate


==============================================================================
*kt_linux_core_002*	linux prio and scheduling


{prio-and-sched}

Lo   Hi Hi     Lo
0 .. 99 100 .. 139

0   .. 99  : RT prio which is static and is used at creation.
100 .. 139 : user prio. nice call can use -20 - +19.

(from ~/include/linux/sched.h)
/*
 * Scheduling policies
 */
#define SCHED_NORMAL		0	# SCHED_OTHER. default
#define SCHED_FIFO		1	# RT without time slice, until it's blocked.
#define SCHED_RR			2	# RT with time slice, (NDS use). RR in the same prio. fair usage.


(Embedded Linux Primer: A Practical, Real-World Approach By Christopher Hallinan)
RT is against to the aim that share CPU sharely with all process in a system.

Fig 17-1. Latency components

Interrupt event       ISR runs      ISR signals RT process      RT process runs
      t0                 t1                    t2                     t3        
		| int latency      | int processing       | sched latency       |
		| int to process latency                                        |


{order-one-sched}

From 2.5. gurantee a constant time for a scheduling decision regardless of the number of process.
O(1) sched. 


{rt-process-creation}

#include <sched.h>

#define MY_RT_PRIORITY MAX_USER_RT_PRIO /* Highest possible */

/* from sched.h 
#define MAX_USER_RT_PRIO	100
#define MAX_RT_PRIO		MAX_USER_RT_PRIO
*/

int main(int argc, char **argv)
{
      ...
      int rc, old_scheduler_policy;
      struct sched_param my_params;
      ...

      /* Passing zero specifies caller's (our) policy */
      old_scheduler_policy = sched_getscheduler(0);
      my_params.sched_priority = MY_RT_PRIORITY;
		
      /* Passing zero specifies callers (our) pid */
      rc = sched_setscheduler(0, SCHED_RR, &my_params);
      if ( rc == -1 )
           handle_error();
      ...
}

(sched.c)
/**
 * sys_sched_get_priority_max - return maximum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the maximum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_max(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = MAX_USER_RT_PRIO-1; # {99}
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
                break;
        }
        return ret;
}

/**
 * sys_sched_get_priority_min - return minimum RT priority.
 * @policy: scheduling class.
 *
 * this syscall returns the minimum rt_priority that can be used
 * by a given scheduling class.
 */
asmlinkage long sys_sched_get_priority_min(int policy)
{
        int ret = -EINVAL;

        switch (policy) {
        case SCHED_FIFO:
        case SCHED_RR:
                ret = 1;
                break;
        case SCHED_NORMAL:
        case SCHED_BATCH:
                ret = 0;
        }
        return ret;
}


{kernel-preemption}

Linux 1.x didn't support it. Kernel preemption means that a user process goes to kernel mode when
calls a system call and there's no scheduling until it's blocked or finishes its job.  That's is
'preemption' is supported in kernel mode. 


==============================================================================
*kt_linux_core_003*	change shed policy after a boot

For example, if scheduling is modified before insmod-ing callisto BCM drivers, tasks
inherit changed scheduling. In that case last two task mods can be dropped. The list is
quite aggressive as changing policy of kthread affects all new kernel threads. Fine tuning
would obviously have to be done along with BCM.

for chaning OTHER SCHED to RR SCHED

http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0182.html
http://www.ussg.iu.edu/hypermail/linux/kernel/0411.1/0330.html


==============================================================================
*kt_linux_core_004*	prio inversion

(http://www.embeddedheaven.com/priority-inversion-2.htm) There are special quote really famous
in the field of real time computing "What Really happen on Mars". In the 1997, the Mar
Pathfinder landing in the Mar surface .During collecting information,is suddenly reset and lost
data. The expert team of Microsoft was gather quickly, one solution was quickly uploaded to the
Robot.The problem was traced to Priority Inversion.So What exactly Priory Inversion is ? or
What is Priority Inversion?.

1.Preemptive Kernel In the Real Time Operating System (RTOS) to ensure the rapid response and
time deterministic the RTOS has the main distinguish with General Purpose OS is the Pre-emptive
kernel.RTOS let's the higher priority task interrupt the lower priority task that's
running.When the higher priority task completed the lower priority task resume and continue
it's works.  And that is the Pre-emptive property a reason lead to Priority Inversion.

2.Multitasking In multitasking OS,  there are 4 common states of tasks.The Ready means the task
is running, the Delayed mean task waiting , for eg, waiting for resource or waiting for higher
priority task finished then the lower priority task resume it's works.The suspended for e.g,
the task was created and it enter the suspended state before come to ready state.Finally , the
pended(blocked) mean the task was halt or stopped. Each tasks can move from one state to
another states that's depend on the Scheduler of CPU.

3.Priority Inversion
The conflict of sharing resource and preemptive kernel are the reason lead to the priority
inversion.There are 2 type of of priority inversion that that are Bounded and Unbounded
Inversion. We will examine step by step the process of using share resource  then explain the
Priority Inversion.

3.1 Identify Problem
In the following example, there are 2 threads A and B running concurrently. They are trying to
modify the sharing resource (global value X)  by increase the value of X. Then the question is
what is the value of X at the final? and how to determine the value of X at an specific time?.

Then the solution: at the first time , each thread will try to lock the resource and only one
thread success.The owning resource-thread will using resource while the others waiting for
resource will be released.When the owning resource-thread release ( unlock ) one of the others
locked the resource and continue running.

Bounded Inversion
There is one low priority task (LPT) running and it locked the resource.The high priority  task
(HPT) appear. The LPT will be preemptive by the  HPT, that is the property of preemptive
kernel. But the LPT not release the resource, the HPT  require resource for it's executing.In
this situation, the scheduler have to put the HPT in waiting state and LPT in runing state
untill the LPT release ( unlock) the resource, then the HPT can continue running

Note. LPT is holding a resource that HPT is waiting for. hence a problem.

3.3 Unbounded Inversion
The problem becomes worst if during the time that the HPT waiting ,LPT is running and LPT
locking the resource, that's appear the medium priority task(MPT). MPT not request resource,
and it's will preemptive the LPT. The LPT have to waiting until MPT finished, and continues
running then release the resource to HPT. But we don't know exactly when the MPT finished and
MPT can run forever.The situation now, the LPT waiting for MPT finished and cannot unlock
resource for HPT, the HPT waiting fot resource from LPT, the MPT running unknown.

3.4 Deadlock and Chain of Nested Resource Lock
The Unbound inversion can lead to Chain of Nested Resource Lock and Deadlock.These thing will
make the CPU waiting forever and lost data.

# nested resouce locks

t1 :                     waiting resource a
t2 : has resource a      waiting resource b
t3 : has resource b      waiting resource c
t4 : has resource c

4.Avoid Priority Inversion
two approach to solve priority inversion. priority ceiling and priority inheritance. most of
RTOS using the second methods -Priority inheritance to solve Inversion Priority Problem.


4.2 Priority Inheritance Method
Uses dynamic priority adjustments
When a low-priority task acquires a shared resource, the task continues running at its original
priority level. If a high-priority task requests ownership of the shared resource, the low-priority task is
{hoisted} above the requesting task. The low-priority task can then continue executing until it
releases the resource. Once the resource is released, the task is dropped back to its original low-priority level,
permitting the high-priority task to use the resource it has just acquired.

Note. PI solves 'unbounded' case and PI allows LPT to finish its use of resource.


(http://www.dvhart.com/content/blog/dvhart/linux/priority_inversion_avoidance_for_enterprise_real_time_linux_systems)

And finally, while best-practices suggest restricting the use of shared resources protected by
these protocols to real-time threads (those of scheduling policy SCHED_FIFO or SCHED_RR), these
complex systems will at times include a SCHED_OTHER thread in the list of lock contenders. The
SUSv3 is not specific on the case of a SCHED_OTHER thread trying to take a mutex of either
PTHREAD_PRIO_PROTECT or PTHREAD_PRIO_INHERIT policy, while the Solaris man page explicitly
state the behavior is unspecified.

In the case of priority inheritance however, the priority boosting is handled within the
kernel, which has the ability to boost a SCHED_OTHER thread's effective priority without
changing its scheduling policy. The current Linux kernel (2.6.24.4) explicitly supports the use
of priority inheritance by SCHED_OTHER threads.

Since static analysis of very complex applications is impractical and the flexibility to use
SCHED_OTHER threads in combination with SCHED_FIFO and SCHED_RR threads is important for
Enterprise Real-Time Systems, priority inheritance becomes the best choice for these systems.

Note: doesn't mean that inversion deosn't happen in RR and recommend that don't share resource
beteen RR and OTHER threads.


==============================================================================
*kt_linux_core_005* posix	

not an interface bwt OS and a driver. this is c programming interface(system call),
commands and IPC that OS provides to APP.

http://pubs.opengroup.org/onlinepubs/9699919799/


==============================================================================
*kt_linux_core_006* errno

{errno-use}

This is global var which is set to positive values and function returns -1 to indicate errors.
Usually defiend in <sys/errno.h>. With multiple threads, provides per-thread errno and done by
having hash define in compile such as -D_REENTRANT. See {pthread-compile-link}

This global errno can cause race condition. Therefore, in threaded programs, each thread has its own
errno value. On Linux, a thread-specific errno is achieved in a similar manner to most other UNIX
implementations: errno is defined as a macro that expands into a function call returning a
modifiable lvalue that is distinct for each thread.


{errno-values}

For BRCM case, errno.h includes following file and starts numbering after following range.

./include/asm-generic/errno-base.h:28:#define   ENOTTY          25      /* Not a typewriter */

#define	EPERM		 1	/* Operation not permitted */
#define	ENOENT		 2	/* No such file or directory */
#define	ESRCH		 3	/* No such process */
#define	EINTR		 4	/* Interrupted system call */
#define	EIO		 5	/* I/O error */
#define	ENXIO		 6	/* No such device or address */
#define	E2BIG		 7	/* Argument list too long */
#define	ENOEXEC		 8	/* Exec format error */
#define	EBADF		 9	/* Bad file number */
#define	ECHILD		10	/* No child processes */
#define	EAGAIN		11	/* Try again */
#define	ENOMEM		12	/* Out of memory */
#define	EACCES		13	/* Permission denied */
#define	EFAULT		14	/* Bad address */
#define	ENOTBLK		15	/* Block device required */
#define	EBUSY		16	/* Device or resource busy */
#define	EEXIST		17	/* File exists */
#define	EXDEV		18	/* Cross-device link */
#define	ENODEV		19	/* No such device */
#define	ENOTDIR		20	/* Not a directory */
#define	EISDIR		21	/* Is a directory */
#define	EINVAL		22	/* Invalid argument */
#define	ENFILE		23	/* File table overflow */
#define	EMFILE		24	/* Too many open files */
#define	ENOTTY		25	/* Not a typewriter */
#define	ETXTBSY		26	/* Text file busy */
#define	EFBIG		27	/* File too large */
#define	ENOSPC		28	/* No space left on device */
#define	ESPIPE		29	/* Illegal seek */
#define	EROFS		30	/* Read-only file system */
#define	EMLINK		31	/* Too many links */
#define	EPIPE		32	/* Broken pipe */
#define	EDOM		33	/* Math argument out of domain of func */
#define	ERANGE		34	/* Math result not representable */


==============================================================================
*kt_linux_core_100* thread vs. process	

{multithreaded-vs-singlethreaded}

The mutiltithreaded means that a single process has multi threads, <lightweight-process>. The
singlethreaed means that a single process has a single thread. MT has less IPC but prone to error
because shares resources; less protection. ST has more IPC but more protection.

<why-thread>

From {ref-002}. Problems with fork:

1) The fork call is expensive because memory is copied from the parent to the child. Although
<copy-on-write> is used to avoid this, but still expensive.

2) Need IPC after fork between parent and child.

Thread help with these problems as it is <lightweight-process> in terms of creation cost:

Thread creation is faster because many of the attributes that must be duplicated in a child created
by fork() are instead shared between threads.

Sharing information between threads is easy and fast. It is just a matter of copying data into
shared (global or heap) variables.

However, has disadvantages:

1) more efforts to ensure thread-safe
2) buggy thread can damage all of the threads in the process. less protection.
3) each thread is competing for use of the finite virtual address space of a host process.
4) usually desirable to avoid the use of signals in multi-threaded programs since requires careful
designs.
5) should run the same program.
6) more threads, more memory and context switching.

{how-about-linux}

A process is a program in action and Linux kernel scheduler schedules based on 'thread'. {Q} KT.
This is a perspective from scheduler but not from sharing resources. In Linux, kernel has a double
linked list which has thread_info element. (have task_struct* in this struct) 
	
Two virtualisation in Linux: processer and memory.

Linux don't distinguish a process(task) with a thread. thread is just a special process. Linux
kernel has a double-linked list which has <thread_info> struct element(link to thread_struct).

why a thread is special is that it shares resource( open files, pending signals, internal kernel
data, process state, address space, text and data section) with others.

to create a thread:

clone( CLONE_VM| CLONE_FS| CLONE_FILES| CLONE_SIGHAND, 0);

to create a process:

clone(SIGCHLD, 0);

Two usual steps to create a process:

fork();	# copy a child from a parent and actually use clone() call
exec*()	# load a new program text.


{shared-and-not-shared-between-threads}

Threads do not shares:
From CH29 in {ref-002} and {ref-001}

The attributes that are shared; in other words, global attributes to a process:
 
the same global memory, process code and most data;
process ID and parent process ID;
process group ID and session ID;
process credentials (user and group IDs);
open file descriptors;
signal dispositions;
file system-related information: umask, current working directory, and root directory;
resource limits;

The attributes for each thread:
thread ID (Section 29.5);
signal mask;
thread-specific data (Section 31.3);
alternate signal stack (sigaltstack());
the errno variable;
realtime scheduling policy and priority (Sections 35.2 and 35.3);
capabilities (Linux-specific, described in Chapter 39); and
stack (local variables and function call linkage information).
registers including PC and SP


{zombie-process}

2014.02 from google phone interview:


==============================================================================
*kt_linux_core_101* pthread

POSIX.1 threads approved in 1995 is a POSIX standard for threads. The standard defines an API for
creating and manipulating threads. So there is one posix standard but there are two implementation
like NPTL. 

{pthread-apis}

http://pubs.opengroup.org/onlinepubs/7990989799/xsh/pthread.h.html
http://pubs.opengroup.org/onlinepubs/7908799/xsh/pthread.h.html

Linux threading models compared: LinuxThreads and NPTL
http://www-128.ibm.com/developerworks/linux/library/l-threading.html?ca=dgr-lnxw07LinuxThreadsAndNPTL

From 2.6, NPTL(New Posix Threading Library) that supports futex(fast user space mutex) and is part
of glibc.


{source}

# from uclibc source

uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\pthread.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\rwlock.c
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\pthread.h
uClibc-0.9.30.1\uClibc-0.9.30.1\libpthread\linuxthreads\sysdeps\pthread\bits\pthreadtypes.h
(_pthread_rwlock_t)


{pthread-errno}

The traditional method of returning status from system calls and some library functions is to return
0 on success and -1 on error, with errno being set to indicate the error. The functions in the
Pthreads API do things differently. All Pthreads functions return 0 on success or a positive value
on failure. The failure value is one of the same values that can be placed in errno by traditional
UNIX system calls. Two approaches:

<one>

From {ref-001}. Why is this? Because pthread funcs do not set <errno-var> and return errno instead.
This means that set manually errno before calling err_sys() for example. This util funcs handles
this:

To avoid cluttering the code with braces, use {comma-operator} to combine assignment and the call:

int n;
if(( n = pthread_mutex_lock( &ndone_mutex )) != 0 )
	 errno = n, err_sys("pthread_mutex_lock error");

Or

void Pthread_mutex_lock(pthread_mutex_t* mptr)
{
	int n;

	if(( n = pthread_mutex_lock( mptr )) == 0 )
		 return;

	errno = n;
	err_sys("pthread_mutex_lock error");
} 

From Appdix C in {ref-001}. The reason for our own error funcs is to handle error case with a single
line. See {pthread-util-func} for the use.

if( error condition )
	 err_sys( printf format with any number of args );

instead of

if( error condition ) {
	 char buff[200];
	 snprintf( buff, sizeof(buff), printf format with any number of args );
	 perror(buff);
	 exit(1);
}

<two>

Because each reference to errno in a threaded program carries the overhead of a function call, our
example programs don't directly assign the return value of a Pthreads function to errno. note: KT so
no need to care about errno in pthread.

From CH03 in {ref-002}

(lib/tlpi_hdr.h)

#ifndef TLPI_HDR_H
#define TLPI_HDR_H /* Prevent accidental double inclusion */
#include <sys/types.h> /* Type definitions used by many programs */
#include <stdio.h> /* Standard I/O functions */
#include <stdlib.h> /* Prototypes of commonly used library functions,
plus EXIT_SUCCESS and EXIT_FAILURE constants */
#include <unistd.h> /* Prototypes for many system calls */
#include <errno.h> /* Declares errno and defines error constants */
#include <string.h> /* Commonly used string-handling functions */
#include "get_num.h" /* Declares our functions for handling numeric
arguments (getInt(), getLong()) */
#include "error_functions.h" /* Declares our error-handling functions */
typedef enum { FALSE, TRUE } Boolean;
#define min(m,n) ((m) < (n) ? (m) : (n))
#define max(m,n) ((m) > (n) ? (m) : (n))
#endif

TODO: need to summarize:


{pthread.h}

#include <pthread.h>

//  return 0 if okay, positive Exxx on error which is different from most system calls. 
//
//  If need multiple arguments to the function, must package them into a structure and then pass the
//  address of this as the single argument to the start function.
//
//  arg should be in global or heap.
//
//  EAGAIN : when cannot create a new thread because exceeded the limit on the number of threads
//
int pthread_create( pthread_t *tid, const pthread_attr_t *attr, void *(*func)(void*), void *arg);


/*  return 0 if okay, positive Exxx on error which is different from most system calls. See
 *  {errno-use}
 *
 *  To wait for a given thread to terminate but no way to wait for any other threads. If that
 *  thread has already terminated, pthread_join() returns immediately.
 *
 *  If status is nonnull, the return value from the thread(a pointer to some object) is stored.
 */
int pthread_join( pthread_t tid, void **status);


/* The execution of a thread terminates in one of the following ways: 
 *
 * o The thread's start function performs a return specifying a return value for the thread. This is
 * equivalent to pthread_exit()  
 * 
 * o The thread calls pthread_exit() which can be called in any func called by start func.
 *
 * o The thread is canceled using pthread_cancel()
 *
 * o Any of the threads calls exit(), or the main thread performs a return (in the main() function),
 * which causes all threads in the process to terminate immediately.
 * 
 * {return-value}
 *
 * The retval argument specifies the return value for the thread. The value pointed to by retval
 * should not be located on the thread's stack, since the contents of that stack become undefined on
 * thread termination. (For example, that region of the process's virtual memory might be
 * immediately reused by the stack for a new thread.) The same statement applies to the value given
 * to a return statement in the thread's start function.
 *
 * {main-thread-exit}
 *
 * If the main thread calls pthread_exit() instead of calling exit() or performing a return, then
 * the other threads continue to execute.
 */
void pthread_exit(void *retval);


// {threadid}
// return tid of calling thread. note: pthread_t is structure and need more to print out and also
// implementation dependant. In NPTL, pthread_t is a pointer so treat it as opaque data and hence
// need pthread_equal(). 
//
// if (pthread_equal(tid, pthread_self()) 
// 	printf("tid matches self\n");
//
// In 29.5 of {ref-002}, POSIX thread IDs are not the same as the thread IDs returned by the Linux
// specific gettid() system call. POSIX thread IDs are assigned and maintained by the threading
// implementation. The thread ID returned by gettid() is a number (similar to a process ID) that is
// assigned by the kernel.  Although each POSIX thread has a unique kernel thread ID in the Linux
// NPTL threading implementation, an application generally doesn't need to know about the kernel IDs
// (and won't be portable if it depends on knowing them).
//
//  return 0 if they are equal
//
int pthread_equeal( pthread_t tid, pthread_t tid );
pthread_t pthread_self(void);


/*  Thread is either [joinable](the default) or [detached]. The detached thread is like a daemon
 *  process: when it terminates, all its resources are released, and cannot wait for it to
 *  terminate. This means cannot get return state. {Q} does it mean joinable thread possibly leaks
 *  resources when not joined? If a joinable thread termintes without pthread_join, it became a
 *  zombie process. Leak resources and may not able to create additional threads. 
 *
 *  {ref-001} Detaching a thread doesn't make it immune to a call to exit() in another thread or a
 *  return in the main thread. In such an event, all threads in the process are immediately
 *  terminated, regardless of whether they are joinable or detached. To put things another way,
 *  pthread_detach() simply controls what happens after a thread terminates, not how or when it
 *  terminates.
 *
 *  To make the specified thread detached and often used to detach itself. Can create detached
 *  thread when create it by using attr setting.
 *
 *  pthread_detach( pthread_self() );
 */
int pthread_detach(pthread_t tid);


/*  status must not point to an object that is local to the calling thread.
 *
 *  terminate two other ways:
 *
 *  o thread function returns. return value is the exit status of the thread. {Q} kernal handle and
 *  manage it?
 *
 *  o main thread function returns or any thread call exit/_exit, the process terminates
 *  immediately.
 */
void pthread_exit(void *status);

// A thread may be canceled by any other thread in the same process. For example, if multiple
// threads are started to work on a given task (say finding a record in a database) adn the first
// thread completes the task then cancels the other threds.
//
// To handle the possibility of being canceled, can install(push) and remove(pop) cleanup handlers.
// These handlers are called:
// a) when the thread is canceled by pthread_cancel
// b) when the thread voluntarily terminates (either by calling pthread_exit or returning from its
// thread)

int   pthread_cancel(pthread_t);
void  pthread_cleanup_push(void*), void *);
void  pthread_cleanup_pop(int);


{pthread-attribute} 

To override the default and normally take the detault using the attr arg as a NULL. Attributes are a
way to specify behavior that is different from the default. When a thread is created with
pthread_create or when a synchronization variable is initialized, an attribute object can be
specified. However the default atributes are usually sufficient for most applications. 

Note: Attributes are specified [only] at thread creation time; they cannot be altered while the thread
is being used. {Q} really?

Thus three functions are usually called in tandem when setting attribute

o Thread attibute intialisation 
pthread_attr_init() create a default pthread_attr_t tattr. The function pthread_attr_init() is used
to initialize object attributes to their default values. The storage is allocated by the thread
system during execution. note: once the thread has been creted, the attribute object is no longer
needed, and so is destoryed.

o Thread attribute value change (unless defaults appropriate) 
A variety of pthread_attr_*() functions are available to set individual attribute values for the
pthread_attr_t tattr structure. (see below).  

o Thread creation 
A call to pthread_create() with approriate attribute values set in a pthread_attr_t structure. 
 
<code>

The following code fragment should make this point clearer: 

#include <pthread.h> 

pthread_attr_t tattr; // note: can be a local var
pthread_t tid;
void *start_routine;
void arg
int ret;

ret = pthread_attr_init(&tattr);
ret = pthread_attr_*(&tattr,SOME_ATRIBUTE_VALUE_PARAMETER);
ret = pthread_create(&tid, &tattr, start_routine, arg);
ret = pthread_attr_destroy(&tattr);

In order to save space, code examples mainly focus on the attribute setting functions and the
intializing and creation functions are ommitted. These must of course be present in all actual code
fragtments. 

An attribute object is opaque, and cannot be directly modified by assignments. A set of functions is
provided to initialize, configure, and destroy each object type. Once an attribute is initialized
and configured, it has process-wide scope. The suggested method for using attributes is to configure
all required state specifications at one time in the early stages of program execution. The
appropriate attribute object can then be referred to as needed. Using attribute objects has two
primary advantages: 

First, it adds to code portability. Even though supported attributes might vary between
implementations, you need not modify function calls that create thread entities because the
attribute object is hidden from the interface. If the target port supports attributes that are not
found in the current port, provision must be made to manage the new attributes. This is an easy
porting task though, because attribute objects need only be initialized once in a well-defined
location. 

Second, state specification in an application is simplified. As an example, consider that several
sets of threads might exist within a process, each providing a separate service, and each with its
own state requirements. At some point in the early stages of the application, a thread attribute
object can be initialized for each set. All future thread creations will then refer to the attribute
object initialized for that type of thread. The initialization phase is simple and localized, and
any future modifications can be made quickly and reliably. Attribute objects require attention at
process exit time. When the object is initialized, memory is allocated for it. This memory must be
returned to the system. The pthreads standard provides function calls to destroy attribute objects. 


{pthread-example-in-FOSH}

/* Assert throughout that the POSIX calls worked. If not, HFL cannot be guaranteed to work. */
resPOSIX = pthread_attr_init(&threadAttrs);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* The stacksize attribute defines the minimum stack size (in bytes) allocated for the created
 * threads stack.
 *
 * int pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize);
 */
resPOSIX = pthread_attr_setstack(&threadAttrs, ptThreadInfo->pvStack,(size_t)ptThreadInfo->szStack);
/* If this assert is triggered, it might be because of not aligning address to a boundary of 8. */
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * To set the other scheduling policy: 
 */
resPOSIX = pthread_attr_setschedpolicy(&threadAttrs, SCHED_RR);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* {pthread-schedule-at-creation}
 * The example to change prio:
 *
 * sched_param param;
 * param.sched_priority = 30;
 * ret = pthread_attr_setschedparam (&tattr, &param);
 * 
 * used to set/inquire a current thread's priority of scheduling.
 * 
 * int pthread_attr_setschedparam(pthread_attr_t *attr, const struct sched_param *param);
 * int pthread_attr_getschedparam(const pthread_attr_t *attr, struct sched_param *param);
 *
 * {Q} {pthread-schedule-at-runtime} is it possible as attr is only at creation?
 */
resPOSIX = pthread_attr_setschedparam(&threadAttrs, &tSchedParam);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);
	
resPOSIX = pthread_attr_setscope(&threadAttrs, ContentionScope);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);


/* The pthread_attr_setinheritsched() function sets the scheduling parameter inheritance state
 * attribute in the specified attribute object. The thread's scheduling parameter inheritance state
 * determines whether scheduling parameters are explicitly specified in this attribute object, or if
 * scheduling attributes should be inherited from the creating thread. Valid settings for
 * inheritsched include:
 * 
 * PTHREAD_INHERIT_SCHED Scheduling parameters for the newly created thread are the same as those of
 * the creating thread.
 * 
 * PTHREAD_EXPLICIT_SCHED Scheduling parameters for the newly created thread are specified in the
 * thread attribute object.
 */
resPOSIX = pthread_attr_setinheritsched(&threadAttrs, PTHREAD_EXPLICIT_SCHED);
HFL_DEBUGMGR_ASSERT(resPOSIX == 0);

resPOSIX = pthread_create(&(ptThreadInfo->idThread), &threadAttrs, pfThreadMain, pvParam);
    

{pthread-stack}

From CH33 in {ref-002}.

Each thread has its own stack whose size is fixed when the thread is created. On Linux/x86-32, for
all threads other than the main thread, the default size of the per-thread stack is 2 MB. The main
thread has a much larger space for stack growth. Occasionally, it is useful to change the size of a
thread's stack. 

The pthread_attr_setstacksize() function sets a thread attribute (Section 29.8) that determines the
size of the stack in threads created using the thread attributes object. The related
pthread_attr_setstack() function can be used to control both the size and the location of the stack,
but setting the location of a stack can decrease application portability.

One reason to change the size of per-thread stacks is to allow for larger stacks for threads that
allocate large automatic variables or make nested function calls of great depth (perhaps because of
recursion).

Alternatively, an application may want to reduce the size of per-thread stacks to allow for a
greater number of threads within a process.

The minimum stack that can be employed on a particular architecture can be determined by calling
sysconf(_SC_THREAD_STACK_MIN). For the NPTL implementation on Linux/x86-32, this call returns the
value 16,384.


{nptl}

Linux threading models compared: LinuxThreads and NPTL
http://www-128.ibm.com/developerworks/linux/library/l-threading.html?ca=dgr-lnxw07LinuxThreadsAndNPTL

NPTL(new posix threading library) comes from kernel 2.6 and supports Futex(fast user space mutex).
It is part of glibc.


{how-to-check-nptl-version}

$ getconf GNU_LIBPTHREAD_VERSION
NPTL 2.15


{pid-nptl}

note: Q. In Linux, every thread has a PID and can see when use ps command but in NPTL, thread
group has one PID. is it true?


{pthread-compile-link}

It is on GCC 4.6.3 and not -lpthread. This is said when run 'man' on pthread funcs. On Linux,
option. The effects of this option include the following:

o The _REENTRANT preprocessor macro is defined. This causes the declarations of a few reentrant
functions to be exposed.

o The program is linked with the libpthread library (the equivalent of -lpthread).

gcc -pthread sample.c


==============================================================================
*kt_linux_core_102*	sync: how to run three threads sequencially

From Cracking the coding interview, p425, 16.5:

Suppose we have following code:

public class Foo {
	public Foo() {...}
	public void first() {...}
	public void second() {...}
	public void third() {...}
}

The same instance of Foo will be passed to three different threads. ThreadA will call first, ThreadB
will call second, and ThreadC will call third. Design a mechanism to ensure that first is called
before second and second is called before third.

Using lock?

public class FooBad {
	public FooBad() {
		lock1 = new ReentrantLock();
		lock2 = new ReentrantLock();
		
		// locks all in a ctor.	
		lock1.lock(); lock2.lock(); 
	}
}

public void first()
{
	...
	lock1.unlock();	// finished first
}

public void second()
{
	lock1.lock();		// wait first to finish
	lock1.unlock();
	...
	lock2.unlock();	// finished second
}

public void third()
{
	lock2.lock();		// wait second to finish
	lock2.unlock();
	...
}

This WON'T work in JAVA since a lock in JAVA is owned by the same thread which locked in.

http://docs.oracle.com/javase/6/docs/api/java/util/concurrent/locks/ReentrantLock.html
A ReentrantLock is owned by the thread last successfully locking, but not yet unlocking it. A thread
invoking lock will return, successfully acquiring the lock, when the lock is not owned by another
thread. The method will return immediately if the current thread already owns the lock. 

 [KT] There's no such limitation in Linux and it's possible in the same thread group and if threads
 in different group use lock, can use semaphore or lock on the shared memory.

 The mutex has ownership as well and see {mutex-ownership}.


==============================================================================
*kt_linux_core_103*	sync: mutex and cond var

From CH07 in {ref-001}. 

This is to sync for data or critial region. Although talking of critial region, what is really
protected is [the-data] being manipulated within the critical region. This is posix.1 standard. mutex
and cond-var uses global structure so if these are shared in a shared memory, these can be used
between processes. If not, for threads within a process.

In {ref-002} p881, process-shared mutexes and condition variables. They are not available on all
UNIX systems, and so are not commonly employed for process synchronization.

{Q} The phrase "no difference between process and thread." If so why is there limitation on IPC and
sync? Think that meant from kernel scheduler point of view but not sharing.


{cooperative-lock}

This means that nothing can prevent one thread from manipulating the data without first obtaining
the mutex. For example, thread not participating a mutex circle.


{implicit-sync}

This is synchronization handled by kernel not by applicaion. Such as pipe and message-q. For
example,

grep pattern chapters.* | wc -l

Writing by producer and reading by consumer are handled by kernel. Does it mean that grep and wc
runs at the same time? not one after one.


{why-need-sync}

Because of <race-condition>. Like {ref-002}, when run two threads without sync, task-switching
happens based on time-slot given and yield <non-determistic> result.

threadFunc()
{
	 for(j=0; j < loops; j++)
	 {
		  local = global;
		  local++;
		  global = local;
	 }
}

If we change this like below, do not need to sync because inc looks atomic?

threadFunc()
{
	 for(j=0; j < loops; j++)
	 {
		  global++;
	 }
}

NO because this single inc operator may result in multiple machine code which are equivalent to the
before. See *kt_linux_core_107* for more about atomic and race-condition.


{mutex-apis} {explicit-sync}

Unlike semaphore and file locks, mutex only requires system call for lock contention. So not that
expensive. On Linux, mutex is implemented by futex which is not intended for direct use in user
space applicaion. From ch30 in {ref-002}

// If it is [statically] allocated for default attributes, should use:

pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;


// If it is dymamically allocated such as malloc or shared-mem, should use. When? mutex can be part
// of structure or object on the heap.

int pthread_mutex_init(pthread_mutex_t* mutex, const pthread_mutexattr_t* attr);
int pthread_mutex_destory(pthread_mutex_t* mutex);


// {mutex-ownership} mutex has ownership means the only owner can unlock the mutex. 
//
// 1) Locking. If try to lock what is already locked, two may happen for default type of mutex:
// {mutex-deadlock} or EDEADLK. On Linux, deadlock by default from {ref-002}.
//
// 2) Unlocking. Unlocking a mutex that is not locked or that is locked by another thread,
// {undefined-result}
//
// This is blocking.

int pthread_mutex_lock( pthread_mutex_t *mptr); 
int pthread_mutex_unlock( pthread_mutex_t *mptr);


// This is non-blocking and EBUSY if the mutex is already locked.
//
// These are less used because need to poll and risks being starved. Well designed one can avoid
// this.

int pthread_mutex_trylock( pthread_mutex_t *mptr);
int pthread_mutex_timedlock( pthread_mutex_t *mptr);


{deadlock-condition}

From {ref-002}, deadlock happens:

Thead A             Thread B
lock(mutex1)        lock(mutex2)
lock(mutex2)        lock(mutex1)

Two approach to avoid:

o <mutex-hierarchy-or-order> and less flexible. Use locks in the same set of mutexs in [the same
order].

o <try-and-back-off> and flexible. If try_lock fails, release all and try again later. This is less
efficient than hierarchy approach but can be more [flexible] since no need rigid hierarchy.


{mutex-types}

From {ref-002}:

// no deadlock detection. On Linux, this is PTHREAD_MUTEX_DEFAULT
PTHREAD_MUTEX_NORMAL

// supports error checking for <mutex-deadlock> locking case but slower than a normal.
PTHREAD_MUTEX_ERRORCHECK

// A recursive mutex maintains the concept of a [lock-count]. When a thread first acquires the mutex,
// the lock count is set to 1. Each subsequent lock operation by the same thread increments the lock
// count, and each unlock operation decrements the count. The mutex is released (i.e., made
// available for other threads to acquire) only when the lock count falls to 0. 
//
// {Q} when is it useful?
//
// Unlocking an unlocked mutex fails, as does unlocking a mutex that is currently locked by another
// thread. In other words, there is no <mutex-deadlock> for unlocking case but personally prefer
// normal lock with careful design than using recursive lock.
//
PTHREAD_MUTEX_RECURSIVE


{consumer-producer-example-one}

From {ref-001}. Multiple producer(writing) and one consumer(reading). Once writing finishs, consumer
get started. No need to sync between producer and consumer. Only sync between producers. So use
mutex and no cond var. 

No need to have a sync for reading? NO. Need to have reading in sync if want to have right result.
For this example, reaading starts after writing. Hence no need.

<code-example>

#include <stdio.h>
#include <pthread.h>
#include <sys/errno.h>

#define MAXNITEMS 	1000000
#define MAXNTHREADS	100

#define	min(a,b)	((a) < (b) ? (a) : (b))
#define	max(a,b)	((a) > (b) ? (a) : (b))

//
void Pthread_create(pthread_t* tid, const pthread_attr_t* attr, void *(*func)(void*), void*arg)
{
	int n;

	if(( n = pthread_create( tid, attr, func, arg )) == 0 )
		return;
	
	errno = n;
	fprintf( stderr, "pthread_create error(%d)", n );
}

// shared by all threads
int nitems;

struct {
	pthread_mutex_t mutex;
	int				buff[MAXNITEMS];
	int				nput;				// next index to write
	int				nval;				// next val to write
} shared = { PTHREAD_MUTEX_INITIALIZER };

void *produce(void *), *consume(void *);

int main( int argc, char** argv )
{
	int i, nthreads, count[MAXNTHREADS]={0};
	pthread_t tid_produce[MAXNTHREADS]={0}, tid_consume;

	if( argc != 3 )
	{
		fprintf( stderr, "usuage: prodcons2 <#items> <#threads>\n");
		exit(1);
	}

	nitems = min( atoi( argv[1] ), MAXNITEMS );
	nthreads = min( atoi( argv[2] ), MAXNTHREADS );

	// start all producer threads
	for( i=0; i < nthreads; ++i )
	{
		count[i] = 0;
		Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
	}

	// wait for all the producer threads
	for( i=0; i < nthreads; ++i ) 
	{
		pthread_join( tid_produce[i], NULL );
		printf("tid[%d] count[%d] = %d\n", tid_produce[i], i, count[i] );
	}

	// start, then wait for the consumer thread
	Pthread_create(&tid_consume, NULL, consume, NULL );
	pthread_join(tid_consume, NULL );

	exit(0);
}

void* produce(void* arg)
{
	printf("run tid[%d] \n", pthread_self());

	for(;;) 
	{
		pthread_mutex_lock( &shared.mutex );
		if( shared.nput >= nitems )  // buff is full, we are done.
		{
			printf("done tid[%d] \n", pthread_self());
			pthread_mutex_unlock(&shared.mutex);
			return NULL;
		}

		shared.buff[ shared.nput ] = shared.nval;
		shared.nput++;
		shared.nval++;

		pthread_mutex_unlock( &shared.mutex );

		// inc of the count is not part of the CR because each thread has its own
		*((int*)arg) += 1;
	}
}

void* consume(void* arg)
{
	int i;

	// see use of global vars; nitems and shared.buff
	for( i=0; i < nitems; i++ )
	{
		if( shared.buff[i] != i )
			printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
	}

	printf("consume done\n" );

	return NULL;
}


kit@kit-VirtualBox:~/work$ ./a.out 1000000 5
run tid[-1252185280] 
run tid[-1243792576] 
run tid[-1235399872] 
run tid[-1227007168] 
run tid[-1218614464] 
done tid[-1252185280] 
done tid[-1243792576] 
done tid[-1227007168] 
done tid[-1218614464] 
tid[-1218614464] count[0] = 198088
tid[-1227007168] count[1] = 220562
done tid[-1235399872] 
tid[-1235399872] count[2] = 254805
tid[-1243792576] count[3] = 162778
tid[-1252185280] count[4] = 163767
consume done


{consumer-producer-example-two} 

Now consumer runs at the same time and runs when there is data to read. All is accessing the same
data and are in the same mutex group. Problem is that consumer do busy loop to check data, <polling>
or <spinning> 

<code-example>

The changes from the previous is:

int main()
{
	...

	// start all producer threads
	for( i=0; i < nthreads; ++i )
	{
		count[i] = 0;
		Pthread_create( &tid_produce[i], NULL, produce, &count[i] );
	}

	// moved here. start, then wait for the consumer thread
	Pthread_create(&tid_consume, NULL, consume, NULL );

	...
}

Uses shared mutex between producers and consumer. If items to read are ready then call returns.
Otherwise, do loops. 

void consume_wait(int i)
{
  for(;;) {
    pthread_mutex_lock(&shared.mutex);
    if( i < shared.nput ) {
      pthread_mutex_unlock(&shared.mutex);
      return; /* item is ready */
    }
    pthread_mutex_unlock(&shared.mutex);
  }
}

void* consume(void* arg)
{
	int i;

	// see use of global vars; nitems and shared.buff
	for( i=0; i < nitems; i++ )
	{ >
		// polling until item is ready
		consume_wait(i);
<
		if( shared.buff[i] != i )
			printf("con: err: buff[%d] = %d\n", i, shared.buff[i] );
	}

	printf("con: done\n" );

	return NULL;
}


{consumer-producer-example-three} {cond-var}

The polling is a waste of cpu time. How to solve? A mutex is for locking and a cond-var is for
waiting.

// This is blocking on condition and ALWAYS has an associated mutex. Why? because both producer
// and consumer accesses to the shared state variable so should be synced. In other words, there is
// a natural association of a muext with a condition varaible.
//
// Do three things: unlock a mutex, block a calling thread until signaled, and relock mutex when
// signaled.

int pthread_cond_wait( pthread_cond_t *cptr, pthread_mutex_t* mptr);


// Guaranteede that at least one of the blocked thread is woken up. If no thread is waiting, the
// signal is lost since condition variable holds no state information.

int pthread_cond_signal( pthread_cond_t *cptr );
int pthread_cond_broadcast( pthread_cond_t *cptr );
int pthread_cond_timedwait( ... );

{Q} {ref-002} said that _signal is for the case where all waiting threads do the same task and
_broadcast is for case where waiting threads do different task. Real examples?


<code-example>

{Q} Is it possible to implement this using two mutex than using a muext and a cond-var?

//
int nitems;
int buff[MAXNITEMS];

// shared by all threads
struct {
	pthread_mutex_t mutex;
	int				nput;				// next index to write
	int				nval;				// next val to write
} put = { PTHREAD_MUTEX_INITIALIZER };

struct {
	pthread_mutex_t mutex;
	pthread_cond_t cond;
	int ready;						// [KT] this is state information linked to condition.
} nready = { PTHREAD_MUTEX_INITIALIZER, PTHREAD_COND_INITIALIZER };

void* produce(void* arg)
{
	printf("pro: run tid[%d] \n", pthread_self());

	for(;;) 
	{
		pthread_mutex_lock( &put.mutex );
		if( put.nput >= nitems )  // buff is full, we are done.
		{
			printf("pro: no more. done tid[%d] \n", pthread_self());
			pthread_mutex_unlock(&put.mutex);
			return NULL;
		}

		buff[ put.nput ] = put.nval;
		put.nput++;
		put.nval++;

		pthread_mutex_unlock( &put.mutex );
>
		// { signal cond-var saying some has been written
		pthread_mutex_lock( &nready.mutex );
		if( nready.ready == 0 )
			pthread_cond_signal( &nready.cond );

		nready.ready++;

		pthread_mutex_unlock( &nready.mutex );
		// }
		
		// inc of the count is not part of the CR because each thread has its own
		*((int*)arg) += 1;
	}
}

void* consume(void* arg)
{
	int i;

	// see use of global vars; nitems and shared.buff
	for( i=0; i < nitems; i++ )
	{ >
		// receieve cond-var
		pthread_mutex_lock( &nready.mutex );

		// Always test the condition again when wakes up because spurious wakeups can occur.
		// Unlock and wait. When returns from pthread_cond_wait, lock in again.
		while( nready.ready == 0 )
			pthread_cond_wait( &nready.cond, &nready.mutex );

		nready.ready--;

		pthread_mutex_unlock( &nready.mutex );
<
		if( buff[i] != i )
			printf("con: err: buff[%d] = %d\n", i, buff[i] );
	}

	printf("con: done\n" );

	return NULL;
}


{when-to-call-signal}

{ref-002} uses different order for producer as below because {ref-001} said that use _signal before
unlock can cause {mutex-deadlock} or {lock-conflict}. Therefore, POSIX recommends the followings and
{ref-002} said this may yieid better performance:

pthread_mutex_lock( &nready.mutex );

nready.ready++;

pthread_mutex_unlock( &nready.mutex );

if( nready.ready == 0 )
 pthread_cond_signal( &nready.cond );


{more-on-cond-wait}

The cond-var has associated with mutex and shared var which do dynamically binds both:

pthread_cond_wait( &nready.cond, &nready.mutex );

This func do: unlock mutex, block the calling thread, and when signaled, lock mutex.


{check-on-predicate}

Must use while on checks as:

o may multiple consumers
o spurious wakeups

// always test the condition again when wakes up because spurious wakeups can occur.
while( nready.ready == 0 )
	pthread_cond_wait( &nready.cond, &nready.mutex );


==============================================================================
*kt_linux_core_104*	sync: read-write lock

From {ref-001} but no such a thing in {ref-002}, so may be old way but surely in posix but may be
different to this since this lock is before posix standard. See {ref-001} note.

To distinguish between obtaining the read-write lock for reading and for writing. The rules:

1. Any number of threads can hold a given read-write lock for reading as long as no threads holds
the the lock for writing.

2. A read-write lock can be allocated for writing only if no thread hold the lock for reading or
writing.

Stated another way, any threads can have read access to a data as long as no thread is modifying
that. A data can be modified only if no other thread is reading or modifying the data.

In application, the data is read more often than the data is modified, and these can benefit from
using read-write locks instead of mutex locks. 

can provide more concurrency than a plain mutex lock when the data is read more than it is written
and known as shared-exclusive locking since shared lock for reading and exclusive lock for writing.
Multiple readers and one writer problem or readers-writer locks.


{apis}

// to get read lock. blocks the calling if there are writers
int   pthread_rwlock_rdlock(pthread_rwlock_t *);

// to get write lock. blocks the calling if there are readers or writers
int   pthread_rwlock_wrlock(pthread_rwlock_t *);

int   pthread_rwlock_unlock(pthread_rwlock_t *);
int   pthread_rwlock_tryrdlock(pthread_rwlock_t *);
int   pthread_rwlock_trywrlock(pthread_rwlock_t *);

int   pthread_rwlock_init(pthread_rwlock_t *, const pthread_rwlockattr_t *);
int   pthread_rwlock_destroy(pthread_rwlock_t *);
int   pthread_rwlockattr_destroy(pthread_rwlockattr_t *);
int   pthread_rwlockattr_init(pthread_rwlockattr_t *);

// to share the lock between different processes
int   pthread_rwlockattr_setpshared(pthread_rwlockattr_t *, int); pthread_t
int   pthread_rwlockattr_getpshared(const pthread_rwlockattr_t *, int *);


{example-implementation}
\unpv22e.tar\unpv22e\my_rwlock_cancel\

Can be implemented using mutexes and condition variables. This is an implementation which gives
preference to waiting writers but there are other alternatives.

typedef struct {
	pthread_mutex_t 	rw_mutex;			// lock on this struct
	pthread_cond_t 	rw_condreaders;	// for reader threads waiting
	pthread_cond_t 	rw_condwriters;	// for writer threads waiting

	// [KT]
	// when struct is inited, set to RW_MAGIC and used by all functions to check that the caller is
	// passing a pointer to an initialized lock and set to 0 when the lock is destroyed.
	int 					rw_magic;
	int 					rw_nwaitreaders;
	int 					rw_nwaitwriters;

	// the current status of the read-write lock. only one of these can exist at a time: -1 indicates
	// a write lock, 0 is lock available, and an value greater than 0 menas that many read locks are
	// held.
	int 					rw_refcount; 
} pthread_rwlock_t;


#define RW_MAGIC 0x19283746

/* init and destroy */
int
pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		result;

	if (attr != NULL)
		return(EINVAL);		/* not supported */

	if ( (result = pthread_mutex_init(&rw->rw_mutex, NULL)) != 0)
		goto err1;
	if ( (result = pthread_cond_init(&rw->rw_condreaders, NULL)) != 0)
		goto err2;
	if ( (result = pthread_cond_init(&rw->rw_condwriters, NULL)) != 0)
		goto err3;
	rw->rw_nwaitreaders = 0;
	rw->rw_nwaitwriters = 0;
	rw->rw_refcount = 0;
	rw->rw_magic = RW_MAGIC;

	return(0);

err3:
	pthread_cond_destroy(&rw->rw_condreaders);
err2:
	pthread_mutex_destroy(&rw->rw_mutex);
err1:
	return(result);			/* an errno value */
}

void
Pthread_rwlock_init(pthread_rwlock_t *rw, pthread_rwlockattr_t *attr)
{
	int		n;

	if ( (n = pthread_rwlock_init(rw, attr)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_init error");
}

int
pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);
	if (rw->rw_refcount != 0 ||
		rw->rw_nwaitreaders != 0 || rw->rw_nwaitwriters != 0)
		return(EBUSY);

	pthread_mutex_destroy(&rw->rw_mutex);
	pthread_cond_destroy(&rw->rw_condreaders);
	pthread_cond_destroy(&rw->rw_condwriters);
	rw->rw_magic = 0;

	return(0);
}

void
Pthread_rwlock_destroy(pthread_rwlock_t *rw)
{
	int		n;

	if ( (n = pthread_rwlock_destroy(rw)) == 0)
		return;
	errno = n;
	err_sys("pthread_rwlock_destroy error");
}


// rdlock
// A problem exists in this function: if the calling thread blocks in the call to pthread_cond_wait
// and the thread is then canceled, the thread terminates while it holds the mutex lock, and the
// counter rw_nwaitreaders is wrong. The same problem exists in our implentation of
// pthred_rwlock_wrlock. Can correct these problem in {}

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// 4give preference to waiting writers. cannot get a read lock if a) rw_refcount < 0 (meaning
	// there is a writer holding the lock and b) if threads are waiting to get a write lock.
	// [KT] if. case that there are writers
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}

	// [KT] else. case that there are no writers
	if (result == 0)
		rw->rw_refcount++;		/* another reader has a read lock */

	pthread_mutex_unlock(&rw->rw_mutex);
	return (result);
}

/* tryrdlock */
int
pthread_rwlock_tryrdlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0)
		result = EBUSY;			/* held by a writer or waiting writers */
	else
		rw->rw_refcount++;		/* increment count of reader locks */

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


/* wrlock */
int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	// [KT] if there are other readers or writers
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}

	// [KT] else there are no readers and writers
	if (result == 0)
		rw->rw_refcount = -1;

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


// unlock. used by both reader and writer
int
pthread_rwlock_unlock(pthread_rwlock_t *rw)
{
	int		result;

	if (rw->rw_magic != RW_MAGIC)
		return(EINVAL);

	if ( (result = pthread_mutex_lock(&rw->rw_mutex)) != 0)
		return(result);

	if (rw->rw_refcount > 0)
		rw->rw_refcount--;			// releasing a reader
	else if (rw->rw_refcount == -1)
		rw->rw_refcount = 0;			// releasing a writer
	else
		err_dump("rw_refcount = %d", rw->rw_refcount); // cannot be since it is to unlock

	// 4give preference to waiting writers over waiting readers
	//
	// {Q} The ref-001 says: notice that we do not grant any additional read locks as soon as a
	// writer is waiting; otherwise, a stream of continual read requests could block a waiting writer
	// forever. For this reason, we need two separate if tests and cannot write
	//
	// if( rw->rw_nwaitwriters > 0 && rw->rw_refcount == 0 )
	// ...
	// 
	// could also omit the test of rw->rw_refcount, but that can result in calls to
	// pthread_cond_signal when read locks are still allocated, which is less efficient.
	//
	if (rw->rw_nwaitwriters > 0) {
		if (rw->rw_refcount == 0)
			result = pthread_cond_signal(&rw->rw_condwriters);		// signal single writer
	} else if (rw->rw_nwaitreaders > 0)
		result = pthread_cond_broadcast(&rw->rw_condreaders);		// signal all readers

	pthread_mutex_unlock(&rw->rw_mutex);
	return(result);
}


{example}

#include	"unpipc.h"
#include	"pthread_rwlock.h"

pthread_rwlock_t	rwlock = PTHREAD_RWLOCK_INITIALIZER;

void	 *thread1(void *), *thread2(void *);
pthread_t	tid1, tid2;

int
main(int argc, char **argv)
{
	void	*status;
	Pthread_rwlock_init(&rwlock, NULL);

	Set_concurrency(2);
	Pthread_create(&tid1, NULL, thread1, NULL);
	sleep(1);		/* let thread1() get the lock */
	Pthread_create(&tid2, NULL, thread2, NULL);

	Pthread_join(tid2, &status);
	if (status != PTHREAD_CANCELED)
		printf("thread2 status = %p\n", status);

	Pthread_join(tid1, &status);
	if (status != NULL)
		printf("thread1 status = %p\n", status);

	printf("rw_refcount = %d, rw_nwaitreaders = %d, rw_nwaitwriters = %d\n",
		   rwlock.rw_refcount, rwlock.rw_nwaitreaders,
		   rwlock.rw_nwaitwriters);
	Pthread_rwlock_destroy(&rwlock);
	/* 4returns EBUSY error if cancelled thread does not cleanup */

	exit(0);
}

void *
thread1(void *arg)
{
	Pthread_rwlock_rdlock(&rwlock);
	printf("thread1() got a read lock\n");
	sleep(3);		/* let thread2 block in pthread_rwlock_wrlock() */
	pthread_cancel(tid2);
	sleep(3);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

void *
thread2(void *arg)
{
	printf("thread2() trying to obtain a write lock\n");
	Pthread_rwlock_wrlock(&rwlock);

	// followings are never get executed since it gets canceled.
	printf("thread2() got a write lock\n");
	sleep(1);
	Pthread_rwlock_unlock(&rwlock);
	return(NULL);
}

When run this, the program is hung. The occurred steps are:

1. the second trys to get write lock and blocks on pthread_cond_wait.
2. the first returns from slepp(3) and cancel the second.

3. when the second is canceled while it is blocked in a condition variable wait, the mutex is
reacquired before calling the first cleanup hander (even if not installed any handers, but the mutex
is still reacquired before the thread is canceled.) Therefore, when the secondis canceled, it holds
the mutex lock for the read-write lock.

4. the first calls pthread_rwlock_unlock, but it blocks forever in its call to pthread_mutex_lock
because the mutex is still locked by the first thread that was canceled. [KT] this means cancel
terminates a thread immediately and do not continue the rest in pthread_rwlock_wrlock. Hence still
locked. 

If remove the call to pthread_rwlock_unlock in the thread1 func, the main will print:

rw_refcount = 1, rw_nwaitreaders = 0, rw_nwaitwriters = 1
pthread_rwlock_destroy error: Device busy

The correctio is simple and this is addition to pthread_rwlock_rdlock:

int
pthread_rwlock_rdlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount < 0 || rw->rw_nwaitwriters > 0) {
		rw->rw_nwaitreaders++;
		pthread_cleanup_push( rwlock_cancelrdwait, (void*)rw); ~
		result = pthread_cond_wait(&rw->rw_condreaders, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitreaders--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelrdwait */
static void
rwlock_cancelrdwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitreaders--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelrdwait */ 

int
pthread_rwlock_wrlock(pthread_rwlock_t *rw)
{
	...
	while (rw->rw_refcount != 0) {
		rw->rw_nwaitwriters++;
		pthread_cleanup_push(rwlock_cancelwrwait, (void *) rw); ~
		result = pthread_cond_wait(&rw->rw_condwriters, &rw->rw_mutex);
		pthread_cleanup_pop(0); ~
		rw->rw_nwaitwriters--;
		if (result != 0)
			break;
	}
	...
}

/* include rwlock_cancelwrwait */
static void
rwlock_cancelwrwait(void *arg)
{
	pthread_rwlock_t	*rw;

	rw = arg;
	rw->rw_nwaitwriters--;
	pthread_mutex_unlock(&rw->rw_mutex);
}
/* end rwlock_cancelwrwait */


==============================================================================
*kt_linux_core_105*	sync: semaphore

Semaphores: A semaphore is a kernel-maintained integer whose value is never permitted to fall below
0. A process can decrease or increase the value of a semaphore. If an attempt is made to decrease
the value of the semaphore below 0, then the kernel blocks the operation until the semaphore's value
increases to a level that permits the operation to be performed. 

The meaning of a semaphore is determined by the application. A process decrements a semaphore (from,
say, 1 to 0) in order to reserve exclusive access to some shared resource, and after
completing work on the resource, increments the semaphore so that the shared resource is released
for use by some other process. The use of a binary semaphore-a semaphore whose value is limited to 0
or 1-is common. However, an application that deals with multiple instances of a shared resource
would employ a semaphore whose maximum value equals the number of shared resources. Linux provides
both System V semaphores and POSIX semaphores, which have essentially similar functionality.

Unlike mutex, semaphore does not get lost when there is no waiting one.

File locks: File locks are a synchronization method explicitly designed to coordinate the actions of
multiple processes operating [on-the-same-file]. They can also be used to coordinate access to other
shared resources. File locks come in two flavors: read (shared) locks and write (exclusive) locks.
Any number of processes can hold a read lock on the same file (or region of a file). However, when
one process holds a write lock on a file (or file region), other processes are prevented from
holding either read or write locks on that file (or file region). Linux provides file-locking
facilities via the flock() and fcntl() system calls. The flock() system call provides a simple
locking mechanism, allowing processes to place a shared or an exclusive lock on an entire file.
Because of its limited functionality, flock() locking facility is rarely used nowadays. The fcntl()
system call provides record locking, allowing processes to place multiple read and write locks on
different regions of the same file.

When performing interprocess synchronization, our choice of facility is typically determined by the
functional requirements. When coordinating access to a file, file record locking is usually the best
choice. Semaphores are often the better choice for coordinating access to other types of shared
resource.


==============================================================================
*kt_linux_core_106* 	sync: common problems when use threads

the big three of threading problems are deadlock, races and starvation.

The simplest deadlock condition is when there are two threads and thread A can't progress until
thread B finishes, while thread B can't progress until thread A finishes. This is usually because
both need the same two resources to progress, A has one and B has the other. Various symmetry
breaking algorithms can prevent this in the two thread or larger circle cases.

Races happen when one thread changes the state of some resource when another thread is not expecting
it (such as changing the contents of a memory location when another thread is part way through
reading, or writing to that memory). Locking methods are the key here. (Some lock free methods
and containers are also good choices for this. As are atomic operations, or transaction
based operations.)

Starvation happens when a thread needs a resource to proceed, but can't get it. The resource is
constantly tied up by other threads and the one that needs it can't get in. The scheduling algorithm
is the problem when this happens. Look at algorithms that assure access.


==============================================================================
*kt_linux_core_107*	sync: atomic operations

{atomic-operations}

For full articles:
http://www.informit.com/guides/content.aspx?g=cplusplus&seqNum=469

Atomicity

An atomic operation is a sequence of one or more machine instructions that are executed
sequentially, without interruption. By default, any sequence of two or more machine instructions
isn't atomic since the operating system may suspend the execution of the current sequence of
operations in favor of another task. If you want to ensure that a sequence of operations is atomic
you must use some form of locking or other types of synchronization. 

Without that, the only guarantee you have is that a single machine instruction is always atomic. the
CPU will not interrupt a single instruction in the middle. [KT] Not entirely true. 

We can conclude from that minimal guarantee that if you can prove that your compiler translates a
certain C++ statement into a single machine instruction, that C++ statement is naturally atomic
meaning, the programmer doesn't have to use explicit locking to enforce the atomic execution of that
statement.  

Which C++ Statements are Naturally Atomic?

Obviously, there are very few universal rules of thumb because each hardware architecture might
translate the same C++ statement differently. Many textbooks tell you that the unary ++ and --
operators, when applied to integral types and pointers, are guaranteed to be atomic. Historically,
when Dennis Ritchie and Brian Kernighan designed C, they added these operators to the language
because they wanted to take advantage of the fast INC (increment) assembly directive that many
machines supported. However, there is no guarantee in the C or C++ standards that these operators
shall be atomic. Ritchie and Kernighan were more concerned about speed rather than atomicity.

You shouldn't make assumptions about the atomicity of C++ statements without examining the output of
your compiler. In some cases, you might discover that what appears to be a single C++ statement is
in fact translated into a long and complex set of machine instructions. 


Epilog

The multithreading support of C++0x consists of a thread class as well as a standard atomics library
that guarantees the atomicity of logical and arithmetic operations. I will introduce the C++0x
atomics library in a separate column.


From C++11:

Data-dependency ordering: atomics and memory model 	N2664 	GCC 4.4
(memory_order_consume)


From StackOverflow:

The increment-memory machine instruction on an X86 is atomic only if you use it with a LOCK prefix.

x++ in C and C++ doesn't have atomic behavior. If you do unlocked increments, due to races in which
processor is reading and writing X, if two separate processors attempt an increment, you can end up
with just one increment or both being seen (the second processor may have read the initial value,
incremented it, and written it back after the first writes its results back).

I believe that C++11 offers atomic increments, and most vendor compilers have an idiomatic way to
cause an atomic increment of certain built-in integer types (typically int and long); see your
compiler reference manual.

If you want to increment a "large value" (say, a multiprecision integer), you need to do so with
using some standard locking mechanism such as a semaphore.

Note that you need to worry about atomic reads, too. On the x86, reading a 32 or 64 bit value
happens to be atomic if it is 64-bit word aligned. That won't be true of a "large value"; again
you'll need some standard lock.


{atomic-non-atomic-operations}
http://preshing.com/20130618/atomic-vs-non-atomic-operations/

Much has already been written about atomic operations on the web, usually with a focus on atomic
read-modify-write (RMW) operations. However, those aren’t the only kinds of atomic operations. There
are also atomic loads and stores, which are equally important. In this post, I’ll compare atomic
loads and stores to their non-atomic counterparts at both the processor level and the C/C++ language
level. Along the way, we’ll clarify the C++11 concept of a “data race”.

Automic operations: automic loads and stores + automic read-modify-write operations

An operation acting on shared memory is atomic if it completes in a single step relative to other
threads. When an atomic store is performed on a shared variable, no other thread can observe the
modification half-complete. When an atomic load is performed on a shared variable, it reads the
entire value as it appeared at a single moment in time. Non-atomic loads and stores do not make
those guarantees.

Without those guarantees, lock-free programming would be impossible, since you could never let
different threads manipulate a shared variable at the same time. We can formulate it as a rule:

Any time two threads operate on a shared variable concurrently, and one of those operations performs
a write, both threads must use atomic operations.

If you violate this rule, and either thread uses a non-atomic operation, you’ll have what the C++11
standard refers to as a [data-race] (not to be confused with Java’s concept of a data race, which is
different, or the more general race condition). [Q] what is the general race condition?

The C++11 standard doesn’t tell you why data races are bad; only that if you have one, “undefined
behavior” will result (section 1.10.21). The real reason why such data races are bad is actually
quite simple: They result in [torn-reads] and [torn-writes].

A memory operation can be non-atomic because it uses multiple CPU instructions, non-atomic even when
using a single CPU instruction, or non-atomic because you’re writing portable code and you simply
can’t make the assumption. Let’s look at a few examples.


<Non-Atomic Due to Multiple CPU Instructions>

Suppose you have a 64-bit global variable, initially zero.

uint64_t sharedValue = 0;

At some point, you assign a 64-bit value to this variable.

void storeValue()
{
    sharedValue = 0x100000002;
}

When you compile this function for 32-bit x86 using GCC, it generates the following machine code.

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	DWORD PTR sharedValue, 2
        mov	DWORD PTR sharedValue+4, 1
        ret
        ...

As you can see, the compiler implemented the 64-bit assignment using two separate machine
instructions. The first instruction sets the lower 32 bits to 0x00000002, and the second sets the
upper 32 bits to 0x00000001. Clearly, this assignment operation is not atomic. If sharedValue is
accessed concurrently by different threads, several things can now go wrong:

- If a thread calling storeValue is preempted between the two machine instructions, it will leave
the value of 0x0000000000000002 in memory – a torn write. At this point, if another thread reads
sharedValue, it will receive this completely bogus value which nobody intended to store.

- Even worse, if a thread is preempted between the two instructions, and another thread modifies
sharedValue before the first thread resumes, it will result in a permanently torn write: the upper
32 bits from one thread, the lower 32 bits from another.

- On multicore devices, it isn’t even necessary to preempt one of the threads to have a torn write.
When a thread calls storeValue, any thread executing on a different core could read sharedValue at a
moment when only half the change is visible.

Reading concurrently from sharedValue brings its own set of problems:

uint64_t loadValue()
{
    return sharedValue;
}

$ gcc -O2 -S -masm=intel test.c
$ cat test.s
        ...
        mov	eax, DWORD PTR sharedValue
        mov	edx, DWORD PTR sharedValue+4
        ret
        ...

Here too, the compiler has implemented the load operation using two machine instructions: The first
reads the lower 32 bits into eax, and the second reads the upper 32 bits into edx. In this case, if
a concurrent store to sharedValue becomes visible between the two instructions, it will result in a
torn read – even if the concurrent store was atomic.

These problems are not just theoretical. Mintomic’s test suite includes a test case called
test_load_store_64_fail, in which one thread stores a bunch of 64-bit values to a single variable
using a plain assignment operator, while another thread repeatedly performs a plain load from the
same variable, validating each result. On a multicore x86, this test fails consistently, as
expected.


<Non-Atomic in a single CPU Instructions>

A memory operation can be non-atomic even when performed by a single CPU instruction. For example,
  the ARMv7 instruction set includes the strd instruction, which stores the contents of two 32-bit
  source registers to a single 64-bit value in memory.

strd r0, r1, [r2]

On some ARMv7 processors, this instruction is not atomic. When the processor sees this instruction,
	it actually performs [two-separate-32-bit-stores] under the hood (section A3.5.3). Once again,
	another thread running on a separate core has the possibility of observing a torn write.
	Interestingly, a torn write is even possible on a single-core device: A system interrupt – say,
	for a scheduled thread context switch – can actually occur between the two internal 32-bit
	stores! In this case, when the thread resumes from the interrupt, it will restart the strd
	instruction all over again.

As another example, it’s well-known that on x86, a 32-bit mov instruction is atomic if the memory
operand is naturally aligned, but non-atomic otherwise. In other words, atomicity is [only]
guaranteed when the 32-bit integer is located at an address which is an exact multiple of 4.


<All C/C++ Operations Are Presumed Non-Atomic>

In C and C++, every operation is presumed non-atomic unless otherwise specified by the compiler or
hardware vendor – even plain 32-bit integer assignment.

uint32_t foo = 0;

void storeFoo()
{
    foo = 0x80286;
}

The language standards have nothing to say about atomicity in this case. Maybe integer assignment is
atomic, maybe it isn’t. Since non-atomic operations don’t make any guarantees, plain integer
assignment in C is non-atomic by definition.

In practice, we usually know more about our target platforms than that. For example, it’s common
knowledge that on all modern x86, x64, Itanium, SPARC, ARM and PowerPC processors, plain 32-bit
integer assignment is atomic [as-long-as] the target variable is naturally aligned. You can verify it
by consulting your processor manual and/or compiler documentation. In the games industry, I can tell
you that a lot of 32-bit integer assignments rely on this particular guarantee.

Nonetheless, when writing truly portable C and C++, there’s a long-standing tradition of pretending
that we don’t know anything more than what the language standards tell us. Portable C and C++ is
designed to run on every possible computing device past, present and imaginary. Personally, I like
to imagine a machine where memory can only be changed by mixing it up first:

On such a machine, you definitely wouldn’t want to perform a concurrent read at the same time as a
plain assignment; you could end up reading a completely random value.

In C++11, there is finally a way to perform truly portable atomic loads and stores: the C++11 atomic
library. Atomic loads and stores performed using the C++11 atomic library would even work on the
imaginary computer above – even if it means the C++11 atomic library must secretly [lock] a mutex to
make each operation atomic. There’s also the Mintomic library which I released last month, which
doesn’t support as many platforms, but works on several older compilers, is hand-optimized and is
guaranteed to be lock-free.


Relaxed Atomic Operations

Let’s return to the original sharedValue example from earlier in this post. We’ll rewrite it using
Mintomic so that all operations are performed atomically on every platform Mintomic supports. First,
we must declare sharedValue as one of Mintomic’s atomic data types.

#include <mintomic/mintomic.h>

mint_atomic64_t sharedValue = { 0 };

The mint_atomic64_t type guarantees correct memory alignment for atomic access on each platform.
This is important because, for example, the GCC 4.2 compiler for ARM bundled with Xcode 3.2.5
doesn’t guarantee that plain uint64_t will be 8-byte aligned.

In storeValue, instead of performing a plain, non-atomic assignment, we must call
mint_store_64_relaxed.

void storeValue()
{
    mint_store_64_relaxed(&sharedValue, 0x100000002);
}

Similarly, in loadValue, we call mint_load_64_relaxed.

uint64_t loadValue()
{
    return mint_load_64_relaxed(&sharedValue);
}

Using C++11’s terminology, these functions are now data race-free. When executing concurrently,
		there is absolutely no possibility of a torn read or write, whether the code runs on
		ARMv6/ARMv7 (Thumb or ARM mode), x86, x64 or PowerPC. If you’re curious how
		mint_load_64_relaxed and mint_store_64_relaxed actually work, both functions expand to an
		inline cmpxchg8b instruction on x86; for other platforms, consult Mintomic’s implementation.

Here’s the exact same thing written in C++11 instead:

#include <atomic>

std::atomic<uint64_t> sharedValue(0);

void storeValue()
{
    sharedValue.store(0x100000002, std::memory_order_relaxed);
}

uint64_t loadValue()
{
    return sharedValue.load(std::memory_order_relaxed);
}

You’ll notice that both the Mintomic and C++11 examples use relaxed atomics, as evidenced by the
_relaxed suffix on various identifiers. The _relaxed suffix is a reminder that, just as with plain
loads and stores, no guarantees are made about memory ordering.

The only difference between a relaxed atomic load (or store) and a non-atomic load (or store) is
that relaxed atomics guarantee atomicity. No other difference is guaranteed.

In particular, it is still legal for the memory effects of a relaxed atomic operation to be
reordered with respect to instructions which follow or precede it in program order, either due to
compiler reordering or memory reordering on the processor itself. The compiler could even perform
optimizations on redundant relaxed atomic operations, just as with non-atomic operations. In all
cases, the operation remains atomic.

When manipulating shared memory concurrently, I think it’s good practice to always use Mintomic or
C++11 atomic library functions, even in cases where you know that a plain load or store would
already be atomic on your target platform. An atomic library function serves as a reminder that
elsewhere, the variable is the target of concurrent data access.

Hopefully, it’s now a bit more clear why the World’s Simplest Lock-Free Hash Table uses Mintomic
library functions to manipulate shared memory concurrently from different threads.


{lock-free-programming}
http://preshing.com/20120612/an-introduction-to-lock-free-programming/

An Introduction to Lock-Free Programming

Lock-free programming is a challenge, not just because of the complexity of the task itself, but
because of how difficult it can be to penetrate the subject in the first place.

I was fortunate in that my first introduction to lock-free (also known as lockless) programming was
Bruce Dawson’s excellent and comprehensive white paper, Lockless Programming Considerations. And
like many, I’ve had the occasion to put Bruce’s advice into practice developing and debugging
lock-free code on platforms such as the Xbox 360.

Since then, a lot of good material has been written, ranging from abstract theory and proofs of
correctness to practical examples and hardware details. I’ll leave a list of references in the
footnotes. At times, the information in one source may appear orthogonal to other sources: For
instance, some material assumes sequential consistency, and thus sidesteps the memory ordering
issues which typically plague lock-free C/C++ code. The new C++11 atomic library standard throws
another wrench into the works, challenging the way many of us express lock-free algorithms.

In this post, I’d like to re-introduce lock-free programming, first by defining it, then by
distilling most of the information down to a few key concepts. I’ll show how those concepts relate
to one another using flowcharts, then we’ll dip our toes into the details a little bit. At a
minimum, any programmer who dives into lock-free programming should already understand how to write
correct multithreaded code using mutexes, and other high-level synchronization objects such as
semaphores and events.  

What Is It?

People often describe lock-free programming as programming without mutexes, which are also referred
to as locks. That’s true, but it’s only part of the story. The generally accepted definition, based
on academic literature, is a bit more broad. At its essence, lock-free is a property used to
describe some code, without saying too much about how that code was actually written.

Basically, if some part of your program satisfies the following conditions, then that part can
rightfully be considered lock-free. Conversely, if a given part of your code doesn’t satisfy these
conditions, then that part is not lock-free.

<definition>

(This was a flow chart)

Are you programming with multiple threads? or interrupt, signal handlers, etc?
-> Yes

Do the threads access shared memeory?
-> Yes

Can the threads block each other? ie. is there some way to schedule the threads which would
'lock-up' indefinitely?
-> No

It is lock-free programming.

In this sense, the lock in lock-free does not refer directly to mutexes, but rather to the
possibility of “locking up” the entire application in some way, whether it’s deadlock, livelock – or
even due to hypothetical thread scheduling decisions made by your worst enemy. That last point
sounds funny, but it’s key. Shared mutexes are ruled out trivially, because as soon as one thread
obtains the mutex, your worst enemy could simply never schedule that thread again. Of course, real
operating systems don’t work that way – we’re merely defining terms.

Here’s a simple example of an operation which contains no mutexes, but is still not lock-free.
Initially, X = 0. As an exercise for the reader, consider how two threads could be scheduled in a
way such that neither thread exits the loop.

while (X == 0)
{
    X = 1 - X;
}

Nobody expects a large application to be entirely lock-free. Typically, we identify a specific set
of lock-free operations out of the whole codebase. For example, in a lock-free queue, there might be
a handful of lock-free operations such as push, pop, perhaps isEmpty, and so on.

Herlihy & Shavit, authors of The Art of Multiprocessor Programming, tend to express such operations
as class methods, and offer the following succinct definition of lock-free (see slide 150): 
	
"In an infinite execution, infinitely often some method call finishes." In other words, as long as
the program is able to keep calling those lock-free operations, the number of completed calls keeps
increasing, no matter what.  It is algorithmically impossible for the system to lock up during those
operations.

<why-use>

One important consequence of lock-free programming is that if you suspend a single thread, it will
never prevent other threads from making progress, as a group, through their own lock-free
operations. This hints at the value of lock-free programming when writing interrupt handlers and
real-time systems, where certain tasks must complete within a certain time limit, no matter what
state the rest of the program is in.

A final precision: Operations that are designed to block do not disqualify the algorithm. For
example, a queue’s pop operation may intentionally block when the queue is empty. The remaining
codepaths can still be considered lock-free.

<TODO> there is more in this page.


==============================================================================
*kt_linux_core_200*	case: own semaphore and mutex class using pthred cond var

POSIX semaphore are system calls which means expensive. Is it possible to implement semaphore without it?

{class-semaphore}

This is for linux. When count is 0, waits and there is no upper limit. Also see that use one mutex
with many condition variables for semaphores.


{util-class}

Just to provide util funcs to all instances since these are static. Also SetPriority is not used.

class CThreadSelf
{
private:
	CThreadSelf(void) {}

public:
	// Returns the ID of the current executing thread.
	'static' int Id(void)
	{ return (int)pthread_self(); }

	'static' bool SetPriority(int priority);
	{
#if defined _LINUX

		  switch (priority)
		  {
			 case CThread::PRIORITY_HIGH: // [note] class type member
				 setpriority(PRIO_PROCESS, pthread_self(), -10); // [note] man setpriority
		       // param.sched_priority = 60;
				 break;
			 case CThread::PRIORITY_NORMAL:
				 setpriority(PRIO_PROCESS, pthread_self(), 0);
		       // param.sched_priority = 50;
				 break;
			 case CThread::PRIORITY_LOW:
		       // param.sched_priority = 40;
				 setpriority(PRIO_PROCESS, pthread_self(), 10);
				 break;
			 default:
				 return false;
		  }

#elif defined _WIN32
	}
};


{semaphore} [KT] the case uses containment(composition) to have implementation.

Use init count but no max count. 0 means to wait and other values means it is okay to get. Used as a
class memeber.

{Q} why need this? code says it calls sched_yield whenever sem count reaches 16.

#define	CONFIG_MAXIMUM_YIELD_COUNTER 16
static unsigned char semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;

pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER; [KT] this is global

struct PSemaphore
{
	pthread_cond_t  cond;
	int             count;
};

class Semaphore
{
	 private:
	 	PSemaphore* m_id;

	 public:
		Semaphore() { m_id = NULL; }
		virtual ~Semaphore() { assert( FlagCreate() == false); }

		bool Create(int count) // initial count
		{
			 pthread_mutex_lock(&mtx);

			 assert( FlagCreate() == false );

			 pthread_cond_t cond = PTHREAD_COND_INITIALIZER;

			 m_id = 'new' PSemaphore;	// new and m_id is not null
			 assert( m_id != NULL );
			 
			 m_id->cond = cond;  // [KT] is it okay as it is local variable?
			 m_id->count = count;

			 pthread_mutex_unlock(&mtx);

			 return m_id != NULL;
		}

		// return true when created
		bool FlagCreate() { return m_id != NULL; }

		virtual void Destory(void)
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  pthread_cond_destroy(&m_id->cond);
			  delete m_id;

			  m_id = NULL;

			  pthread_mutex_unlock(&mtx);
		}

		void Take()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  while (m_id->count <= 0)
			  {
				  pthread_cond_wait(&m_id->cond, &mtx);
			  }

			  m_id->count--;

			  pthread_mutex_unlock(&mtx);
		}

		void Give()
		{
			  pthread_mutex_lock(&mtx);

			  assert(FlagCreate() == true);

			  m_id->count++;

			  pthread_cond_signal(&m_id->cond);

			  pthread_mutex_unlock(&mtx);

			  if (!semCounter--) {
				  sched_yield();
				  semCounter = CONFIG_MAXIMUM_YIELD_COUNTER;
			  }
		}

		void Try(unsigned long msec = 0)
		{
			  if (msec == (unsigned long) INFINITY)
			  {
				  Take();

				  return true;
			  }

			  pthread_mutex_lock(&TimeMutex);
			  ASSERT(FlagCreate() == true);

			  struct timeval  now;
			  struct timespec timeout;
			  int             ret = 0;
			  bool            tf;


			  if (msec == 0)
			  {
				  if (m_id->count <= 0)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }
			  else
			  {
				  while ((m_id->count <= 0) && (ret != ETIMEDOUT))
				  {
					  gettimeofday(&now, NULL);
					  timeout.tv_sec  = now.tv_sec + msec / 1000;
					  timeout.tv_nsec = now.tv_usec + msec % 1000 * 1000;

					  while (timeout.tv_nsec > 1000000)
					  {
						  timeout.tv_nsec -= 1000000;
						  timeout.tv_sec++;
					  }

					  timeout.tv_nsec *= 1000;

					  ret = pthread_cond_timedwait(&m_id->cond, &TimeMutex, &timeout);
				  }

				  if (ret == ETIMEDOUT)
				  {
					  tf = false;
				  }
				  else
				  {
					  tf = true;

					  m_id->count--;
				  }
			  }

			  pthread_mutex_unlock(&TimeMutex);

			  return tf;
		}
};


{use-of-semaphore-one}

To make sure that an user can set prio once a thread is created.

class CThread
{
   PCSemaphore m_pidSync;

   Create()
   {
      m_pidSync.Create(0);
   }

   bool PCThread::SetPriority(int priority)
   {
      ASSERT(FlagCreate() == true);

      if (m_pid == -1)
      {
         m_pidSync.Take();
      }
      ...
   }

   inline void CThreadRun(CThread* thread)
   {
      thread->m_sync[0].Take();

      thread->m_pid = pthread_self();
      thread->m_pidSync.Give();

      thread->t_Main(); [KT] while loop on event get()

      thread->m_sync[1].Give();

      return;
   }
};


{mutex} 

CDerivedA: CMutex
 - thread   - Semaphore : uses global mutex

CDerivedB: CMutex
 - thread   - Semaphore : uses global mutex

Using sync always happens in the same thread. The case uses inheritance to have implementation. This
is based on the fact that mutex is a binary semaphore. Created with 1. Used to give the derived
class the lock/unlock feature to control interface access. That is, sync feature to class objects.
If need other use of sem, then have sem as a member.

class CMutex
{
	 private:
		  PCSemaphore 	m_sem; // [note] use of semaphore
		  int				m_tid;
		  int				m_count;

	 public:
	 	  // [note] no ctor?
	 	  virtual ~Mutex() { assert( FlagCreate() == false); }

		  bool Create(void)
		  {
				assert(FlagCreate() == false);

				if (m_sem.Create(1) == false) // [KT] create a sem
				{
					return false;
				}

				m_threadId = 0;

				return true;
		  }

		  virtual void Destroy(void)
		  {
				ASSERT(FlagCreate() == true);
				m_sem.Destroy();
		  }

		  bool FlagCreate(void) { return m_sem.FlagCreate(); }

		  void Lock(void)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;
				  return;
				}

				m_sem.Take();

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

		  }

		  bool Unlock(void);
		  {
				assert(FlagCreate() == true);

				if (m_threadId != CThreadSelf::Id())
				{
					return false;
				}

				m_count--;

				if (m_count > 0)
				{
				  return true;
				}

				m_threadId = 0;

				m_sem.Give();

				return true;
		  }

		  bool Try(unsigned long msec = 0)
		  {
				assert(FlagCreate() == true);

				if (m_threadId == CThreadSelf::Id())
				{
				  m_count++;

				  return true;
				}

				if (m_sem.Try(msec) == false)
				{
				  return false;
				}

				m_threadId = CThreadSelf::Id();
				m_count    = 1;

				return true;

		  }
};	

{cqueue}

struct PTEvent
{
	friend class PCQueue;
	friend class PCTask;

private:
	void* sync;

public:
	class PCHandler* receiver; //!< The Pointer to the handler that receives the event
	unsigned long    type;     //!< The event type

	//! Parameters of an event
	/*!
	 * The union of event parameters are 8-bytes long. That is, it can carry
	 * 2 long integers, 4 short integers, or 8 characters.
	 */
	union
	{
		long  l[2];
		short s[4];
		char  c[8];
	} param;
};

class CQueue : public CMutex
{
	private:
      PCSemaphore m_sem; // [KT] used to say that events are avaiable
		PTEvent     m_event[CONFIG_QUEUE_SIZE];

	bool CQueue::Create(void)
	{
		ASSERT(FlagCreate() == false);

		if (CMutex::Create() == false)
		{ return false; }

		if (m_sem.Create(0) == false)
		{
			CMutex::Destroy();
			return false;
		}

		m_in   = 0;		// [KT] this is {queue-contiguous-implementation} in *kt_dev*
		m_out  = 0;		// in(tail), out(head), size(count)
		m_size = 0;

		return true;
	}

	bool CQueue::Put(PTEvent* event, bool sync=false, bool priority=false)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		PCSemaphore sem;

		if (sync == false)
		{
			event->sync = NULL;
		}
		else
		{
			if (sem.Create(0) == false)
			{
				return false;
			}

			event->sync = &sem;
		}

		Lock();	// CMutex::Lock();

		int size;

		size = (priority == false) ? CONFIG_EMEGENCY_QUEUE_SIZE(16) : 0;
		size += m_size;

		if (size >= CONFIG_QUEUE_SIZE(256) )
		{
			Unlock();

			if (sync == true)
			{
				sem.Destroy();
			}

			PCDebug::Print("ERROR: Event queue full");

			return false;
		}

		if (priority == true)
		{
			m_out          = (m_out + CONFIG_QUEUE_SIZE - 1) % CONFIG_QUEUE_SIZE;
			m_event[m_out] = *event;
			m_size++;
		}
		// [KT] copy in event and inc count
		else
		{
			m_event[m_in] = *event;
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
			m_size++;
		}

		m_sem.Give();

      // [KT] As see how Get() uses m_sem, use m_sem to see if event are avaiable like a count or
      // length and then if there are call Lock() to lock access to this objects. May have some
      // performance gain from this.
		
      Unlock();	

		if (sync == true)
		{
			sem.Take();
			sem.Destroy();
		}

		return true;
	}

	bool PCQueue::Get(PTEvent* event, unsigned long msec = INFINITY)
	{
		ASSERT(FlagCreate() == true);
		ASSERT(event != NULL);

		while (true)
		{
			if (m_sem.Try(msec) == false)
			{
				return false;
			}

			Lock();

			if (m_size == 0)
			{
				Unlock();

				continue;
			}

			*event = m_event[m_out];
			m_out  = (m_out + 1) % CONFIG_QUEUE_SIZE;
			m_size--;

			Unlock();

			return true;
		}
	}

};


{when-seek-one-specific}

{Q} This suggest that there are events for all(broadcast) and for specific ones. Move all event
between [head+1 ... tail-1] after tail. why? priority?

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver)
{ }

bool PCQueue::Get(PTEvent* event, const PCHandler* receiver, unsigned long type)
{
	ASSERT(FlagCreate() == true);

	bool done = 0;

	Lock();

	unsigned long size = m_size;

	while (size-- != 0)
	{
		if (done == false && m_event[m_out].receiver == receiver && m_event[m_out].type == type)
		{
			*event = m_event[m_out];
			m_size--;
			done = true;
		}
		else
		{
			m_event[m_in] = m_event[m_out];
			m_in          = (m_in + 1) % CONFIG_QUEUE_SIZE;
		}

		m_out = (m_out + 1) % CONFIG_QUEUE_SIZE;
	}

	if (done)
	{
		m_sem.Try();
	}

	Unlock();

	return done;
}

This q implementation uses q per thread. The different approach is to have q that threads share.
See *kt_linux_core_014* for msg q between threads


==============================================================================
*kt_linux_core_201*  case: use of mutex and thread class

This is case study using Mutex, Queue and Thread classes.

{CThread}

'inline' void CThreadRun(CThread* thread)
{
	thread->m_sync[0].Take();  // [KT] wait until signaled

#ifdef	_LINUX
	thread->m_pid = pthread_self();
	thread->m_pidSync.Give();
#endif

	thread->t_Main();

	thread->m_sync[1].Give();  // [KT] no use

	return;
}

'static' void* _Process(void* param)
{
	CThreadRun((CThread*)param);

	return NULL;
}

class CThread
{
private:

	int         m_id;
	int			m_pid;
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];

	friend void CThreadRun(CThread* thread); // [KT] inline friend

protected:

	virtual void t_Main(void) = 0;
	/*!< This function is the thread main virtual function.  As soon as a thread starts, this
	 * function is called.  When this function returns, the thread is terminated.
	 *
	 * You have to define this function when you define a new class that inherits CThread class.
	 */

public:

	//! Configuration constants
	enum PTConfigType
	{
		CONFIG_STACK_SIZE = 4096
	};

	enum PTPriorityType
	{
		PRIORITY_HIGH,
		PRIORITY_NORMAL,
		PRIORITY_LOW,
	};

	virtual ~CThread(void) { ASSERT(FlagCreate() == false); }

	bool Create(const char* name, unsigned long stackSize = CONFIG_STACK_SIZE);
	 {
		 ASSERT(FlagCreate() == false);

		 if (m_sync[0].Create(0) == false)
		 {
			 return false;
		 }

		 if (m_sync[1].Create(0) == false)
		 {
			 m_sync[0].Destroy();
			 return false;
		 }

#elif defined _LINUX

		 pthread_attr_t attr;
		 pthread_attr_init(&attr);
		 pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
		 pthread_attr_setstacksize(&attr, stackSize);
		 //struct sched_param schedParam;
		 //schedParam.sched_priority = 50;
		 //pthread_attr_getschedparam(&attr, &schedParam);
		 //pthread_attr_setschedpolicy(&attr, SCHED_RR);

		 m_pid = -1;
		 m_pidSync.Create(0);

		 if (pthread_create((pthread_t*)&m_id, &attr, _Process, this) != 0)
		 {
			 pthread_attr_destroy(&attr);
			 
			 ASSERT(!"CThread not created");

			 m_sync[0].Destroy();
			 m_sync[1].Destroy();

			 return false;
		 }

		 pthread_attr_destroy(&attr); 
		 
		 m_sync[0].Give();   // [KT] now a thread can run
		 return true;
	 }

	//! Check if the instance was created
	bool FlagCreate(void) { return m_sync[0].FlagCreate(); }
	//! Destroy the instance
	virtual void Destroy(void);

	//! Returns the ID of the CThread.
	int Id(void)
	{
		 ASSERT(FlagCreate() == true);
		 return m_id;
	}

	//! Set the priority value for the thread
	bool SetPriority(int priority);
};

The class hiarachy is:

class CThread
{
private:
	PCSemaphore	m_pidSync;
	PCSemaphore m_sync[2];
};

class CQueue : public CMutex

class CTask: private CThread, public CQueue, public CHandler
{
   private:
      void t_Main(void);

   protected:
      virtual bool t_Create(void);
};

class CSIEngineBase : public PCTask

class CSIVoiceEngine : public CSIEngineBase

class CSIEngineManager


<run>

CThread:
	CThreadRun: 
      thread->t_Main();	[pure-virtual] [main-start]

CTask:
              	t_Main()                 [main-end] {template-method} in kt_dev_txt
					{
				   	t_Create();                      [virtual] [create-start]
                 	while (ExecuteEvent() == true)   [thread-loop]
						//	virtual bool ExecuteEvent
						//	{
						//		 PTEvent event;
						//		 Get(&event, m_msec); [copy-in-event]
						//		 event.receiver->OnEvent(&event);
						//	}

					  	t_Destory();
					}

CSIEngineBase:
                                                 
CSIVoiceEngine:
                                                  t_Create() [create-end]
																     SendSelfEvent( OWN_EVENT_TYPE );

<event>

CHandler:
	protected:
	virtual bool t_OnEvent(const PTEvent* event) = 0;

	public:
	inline bool OnEvent
	{
      t_OnEvent() [pure-virtual] [event-start]
	}

CTask: public CHandler
	No OnEvent which means use CHandler one

	protectd:
	bool PCTask::t_OnEvent(const PTEvent* event)
	{
		has default basic event handling
	}

	static Send( PTEvent* event, bool sync = false, bool priority = false);
	  [to-other-task]
	  if ((sync == false) || (event->receiver->Task()->Id() != PCThreadSelf::Id()))
	  {
		 return event->receiver->Task()->Put(event, sync, priority);
		 {
			  In CQueue::Put, use copy-ctor of event structure to copy it into receiver's taks
			  m_event[].

			  m_event[i] *event;
		 }
	  }
	  [self] ends up with a func call
	  event->receiver->OnEvent(event);
		   
CSIEngineBase:
   public:
      bool SendSelfEvent(int nEventType)
      {
         PTEvent evt;
         evt.receiver = this; [event-to-self]
         evt.type = nEventType;
         Send(&evt);   // CTask::
      }

  protected:
      virtual bool t_OnEvent(const PTEvent* event)
      { return true; }

		virtual bool t_OnEvent( PTEvent* ) [event-end]
      {
		   PCTask::t_OnEvent(event); {hook-operation} in {template-method}
		   t_ProcessEvent(event); [virtual] [start]
      }

CSIVoiceEngine:
   SendSelfEvent( OWN_EVENT_TYPE );  // CSIEngineBase::

   virtual bool t_ProcessEvent(event);           [end]
   {
	   handle OWN_EVENT_TYPE;
   }


<create>

CThread:
   Create:
	  pthread_create

CTask:
	public:
   Create()
	  PCQueue::Create();
	  PCHandler::Create(this);
	  CThread::Create(stackSize);

CSIEngineBase: public CTask
	public:
   Create(const char* name)
      return CTask::Create(name);					[create]

CSIVoiceEngine: public CSIEngineBase
   no Create:

// {design-note}
// This can be a application manager which creates all applications and call Create() on them. This
// is platform wide and each application can override t_Create and init their own thing without
// knowing Create() calls made from outside.

class CSIEngineManager:
{
	 private:
	 CSIEngineBase* m_pVoiceEngine;

	 static CSIEngineManager* CSIEngineManager::GetInstance(void) {factory-method}
	 {
		 if(m_EngineManagerInstance == NULL)
		 {
			 m_EngineManagerInstance = 'new' CSIEngineManager;
		 }

		 return m_EngineManagerInstance;
	 }

	 CSIEngineManager::GetEngineInstance
	 {
		  m_pVoiceEngine =  'new' CSIVoiceEngine;
		  m_pVoiceEngine->Create("SIVoiceEngine"); // [create] and use inherited implementation
	 }
}

From review, t_ prefix means a primitive operation and to be overridden and all works are using the
most derived class object. Therefore, dreived version will be used for virtuals as shown
{template-method}


==============================================================================
*kt_linux_core_202*  case: analysis of 200 and 201 case

{how-this-work}

CUserClass instance:
                                       
CUserClass {
   // Eash CTask has a Q and thread
   CTask { CQueue, CHandler : CQueue { CMutex }
      CThread
         : pthread( staic _Process )
      }
}
  
static _Process(this) will run the derived class CTask t_Main which has a message loop. So this
_Process is a template code for all threads. 

Get(&event); in which all threads use the same code for thread routine

CUserB instance:
                                       
CUserB {
   CTask {
      CThread
         : pthread( staic _Process )
      }
}

Mutex class via inheritance:
Mutex #01     Mutex #02    Mutex #03     Mutex #04    Mutex #04  
(Semaphore)   (Semaphore)  (Semaphore)   (Semaphore)  (Semaphore)

Sem member in a class:
Sepmphore #01  Sepmphore #02 ...


Regarding q, each task has a q and other task can call put to insert a mesg to receiver's q.

{Q} Eash has its own pthread cond in Semaphore but all use a single global mutex for signaling. How
about performance? Is it better solution?


==============================================================================
*kt_linux_core_203*  case: msg q between threads

This uses stl q and sems to read, write and count(length) lock:


/** Maximum length of message queue name */
#define MQ_NAME_LENGTH 5

/** Queue Magic identifier Corresponds to ASCII QuEu*/
#define MQ_MAGIC 0x51754575

typedef struct PFMMessageQueueInfo_t_
{
    char                name[ MQ_NAME_LENGTH ];
    PCSemaphore         *readsem;
    PCMutex             *writeLock;
    PCMutex             *readLock;
    uint32_t            magic;
    std::queue<SPfmMessage>* container;
} PFMMessageQueueInfo_t;


extern "C"
{
///////////////////////////////////////////////////
// PFM Queue Create
///////////////////////////////////////////////////
HPfmQueue
pfmQueueCreate(const char* name, uint32_t max_size)
{
    PFMMessageQueueInfo_t *qptr;

    // Allocate queue controll structure
    qptr = (PFMMessageQueueInfo_t *)pfmMalloc( sizeof(PFMMessageQueueInfo_t) );
    if(!qptr)
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc\n");
        return PFM_NULL_HANDLE;
    }

    // create storage container
    qptr->container = new std::queue<SPfmMessage>();
    if (qptr->container == NULL)
    {
        fprintf(stderr,
            "pfmQueueCreate failed to alloc storage\n");
        pfmFree(qptr);
        return PFM_NULL_HANDLE;
    }

    // Create & initialize Read mutex for read serialization
    qptr->readLock = new PCMutex;
    if( qptr->readLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc read mutex\n");

        delete qptr->container;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    qptr->readLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init read mutex\n");

        delete qptr->container;
        delete qptr->readLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create & initialize Write mutex for write serialization
    qptr->writeLock = new PCMutex;
    if( qptr->writeLock == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }
    qptr->writeLock->Create();
    if( !qptr->readLock->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to init write mutex\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;
        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Create and initialize read semaphore
    qptr->readsem = new PCSemaphore;
    if( qptr->readsem == 0 )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc semaphore\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    //Initialize read semaphore so it will "block" on try.
    qptr->readsem->Create(0);
    if( !qptr->readsem->FlagCreate() )
    {
        fprintf(stderr,  "pfmQueueCreate failed to alloc sem\n");

        delete qptr->container;
        qptr->readLock->Destroy();
        qptr->writeLock->Destroy();
        delete qptr->readLock;
        delete qptr->writeLock;

        delete qptr->readsem;

        pfmFree( qptr );

        return PFM_NULL_HANDLE;
    }

    // Copy semaphore name
    if( !name )
    {
        char nameTmp[] = "SEM";
        PCString::Copy( qptr->name,nameTmp,MQ_NAME_LENGTH);
    }
    else
    {
        PCString::Copy( qptr->name,name,MQ_NAME_LENGTH);
    }

    qptr->name[ (MQ_NAME_LENGTH-1) ] = (char)NULL;
    qptr->magic = MQ_MAGIC;

    return (HPfmQueue)qptr;
}

///////////////////////////////////////////////////
// PFM Queue Destroy
///////////////////////////////////////////////////
pfmerr_t
pfmQueueDestroy(HPfmQueue h)
{
    // Validate input
    if( h == PFM_NULL_HANDLE)
    {
        pfmAssert( h != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)h;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Mark queue as invalid.
    qptr->magic = 0;

    // Release Reader (if exists) & Grab read/write locks.
    // This shall prevent "use while destruct" scenario.
    // Mutexes can be destroyed while locked.
    qptr->readsem->Give();
    qptr->readLock->Lock();
    qptr->writeLock->Lock();

    if( qptr->readsem->FlagCreate() )
    {
        qptr->readsem->Destroy();
    }
    if( qptr->readLock->FlagCreate() )
    {
        qptr->readLock->Destroy();
    }
    if( qptr->writeLock->FlagCreate() )
    {
        qptr->writeLock->Destroy();
    }

    delete qptr->readsem;
    delete qptr->readLock;
    delete qptr->writeLock;

    delete qptr->container;

    pfmFree( qptr );

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Receive
///////////////////////////////////////////////////
pfmerr_t
pfmQueueReceive(HPfmQueue q, uint32_t timeout_ms, SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    unsigned long readLockTickStart;
    unsigned long readLockTickEnd;
    unsigned long tickDiff;
    uint32_t    waitTime;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Convert PFM infinite timeout to Shadwo's infinite timeout
    switch( timeout_ms )
    {
        case PFM_WAIT_NONE:
            waitTime = 0;
            break;
        case PFM_WAIT_FOREVER:
            waitTime = INFINITY;
            break;
        default:
            waitTime = timeout_ms;
            break;
    }

    // Do thread serialized reading
    readLockTickStart = PCTime::Tick();
    if( !qptr->readLock->Try(waitTime) )
    {
        return ERR_TIMEDOUT;
    }

    // If waiting for data, do the tick calculation to
    // potentially reduce wait value
    if( waitTime )
    {
        readLockTickEnd = PCTime::Tick();
        tickDiff = pfmTickDiff( readLockTickStart, readLockTickEnd );

        // Check if we have time to hang on a samephore
        if( tickDiff > waitTime )
        {
            // We have waited longer on a mutex, so wait as short as possible
            waitTime = 0;
        }
        else
        {
            waitTime -= tickDiff;
        }
    }

    // Consume semaphore as it is signalled after every insertion
    qptr->readsem->Try(waitTime);

    // Check if there's data in the queue. This should only happen
    // on wait with timeout. Should not occour in infinite timeout
    // scenario
    if (qptr->container->empty())
    {
        pfmAssert(waitTime != (uint32_t)INFINITY );
        qptr->readLock->Unlock();
        return ERR_TIMEDOUT;
    }

    // read message
    *msg = qptr->container->front();
    qptr->container->pop();

    // This should be the last possible place where
    // a dying queue could be trapped (unlock fails).
    if( !qptr->readLock->Unlock() )
    {
        return ERR_SYS;
    }

    return ERR_OK;
}

///////////////////////////////////////////////////
// PFM Queue Send
///////////////////////////////////////////////////
pfmerr_t
pfmQueueSend(HPfmQueue q, const SPfmMessage *msg)
{
    // Validate input
    if( q == PFM_NULL_HANDLE)
    {
        pfmAssert( q != PFM_NULL_HANDLE );
        return ERR_INV;
    }

    PFMMessageQueueInfo_t   *qptr = (PFMMessageQueueInfo_t*)q;
    pfmerr_t res;

    if( qptr->magic != MQ_MAGIC )
    {
        pfmAssert( qptr->magic == MQ_MAGIC );
        return ERR_INV;
    }

    // Grab an read Lock
    qptr->writeLock->Lock();

    qptr->container->push(*msg);

    // Inform readers that there's data avaliable.
    qptr->readsem->Give();

    //Unlock queue
    if( !qptr->writeLock->Unlock() )
    {
        // This should handle a dying queue
        res = ERR_SYS;
    }
    else
    {
        res = ERR_OK;
    }

    return res;
}
}


==============================================================================
*kt_linux_core_300*  ipc

CH43, Fig 43-1 in {ref-002} which says that the general term IPC is often used to describe them all;
communication, signal, and synchronization.

communication - data transfer - byte stream 	- pipe
															- fifo
															- stream socket

										- message 		- sys v message q
															- posix message q
															- datagram socket

										- pseudoterminal

					- shared memory 	- sys v shm
											- posix shm
											- memory mapping 	- anonymous mapping
																	- mapped file

signal 			- standard signal
					- realtime signal

synchronization- semaphore 	- sys v
										- posix 	- named
													- unnamed
					- file lock 	- record lock
										- file lock

					- mutex	
					- condition variable

Signals: Although signals are intended primarily for other purposes, they can be used as a
synchronization technique in certain circumstances. More rarely, signals can be used as a
communication technique: the signal number itself is a form of information, and realtime signals can
be accompanied by associated data (an integer or a pointer).


==============================================================================
*kt_linux_core_301*  ipc: pipe and fifo	

{nonnetworked-ipc}
From {ref-001}. nonnetworked-ipc means that ipc for local and newtworked-ipc means that for remote
such as socket.

{categories-of-ipc}

From {ref-001}:

three-ways-to-share-between-processes

(1) Process Process       (2) Process Process          (3) Process <- shm -> Process
      |        |                 |       |      
Kernel                          shared info in kernel
      |        |
Filesystem


<persistence> which is lifetime of an objects

Define the persistence of any type of IPC as how long an object of that type remains in existence.
Process-persistence exists until last process with IPC open closes the object and kernel one exists
until kernel reboots or IPC objects is explicitly deleted.

process-persistence: pipe, fifo, mutex, condition-var, read-write-lock, ...
kernel-presistence : shm, named-semaphore, ...

Be careful when defining the presistence of ipc because it is not always as it seems: the date
within a pipe is maintained within the kernel, but pipes have process-persistence because after the
last process that has the pipe open for reading closes the pipe, the kernel discard all data and
remove the pipe.

From {ref-002}:

Data-transfer which 
1) requires two data transfers between user and kenel memory
2) synchronization between the reader and writer processes is automatic by kernel. if a reader
attempts to fetch data from data-transfer facility that has no data, then read will bock until some
write data to it.  
3) available to one which done read operation since read consumes data.

byte stream: read and write is independent meaning read may read an arbitrary bytes. This models
"file as a sequence of bytes".

An application can also impose a message-oriented model on a byte-stream facility, by using
delimiter characters, fixed-length messages, or message headers that encode the length of the total
message message: each read reads a whole message. not possible to read part of a message and to read
multiple messages.


shared memory which 
1) don't require system calls or data transfer between user and kenel. Hence shared memory
provide very fast communication. 
2) However it can be offset by the need to sync and semaphore is the usual method used with shared
memory.
3) avaible to all of the processes that share that memory

facility using file descriptors like pipe, fifo, and sockets

The primary benefit of these techniques is that they allow an application to simultaneously monitor
multiple file descriptors to see whether I/O is possible on any of them.


{name-space}

Use name or identifier so that one process can create ipc object and other processes can specify
that same ipc object.

type 								name used to identify 		handle used to refer to object
------------------------------------------------------------------------------------
pipe 								no name 							file descriptor
fifo 								pathname 						ditto

UNIX domain socket 			pathname 						ditto
Internet domain socket 		IP and port 					ditto

posix message q 				posix ipc pathname 			mqd_t
posix named semaphore 		ditto								sem_t* (sem pointer)
posix unnamed semaphore 	no name 							sem_t*
posix shared memory 			posix ipc pathname 			file descriptor 

anonymous mapping 			no name 							none
memory mapped file 			pathname 						file descriptor 


{accessibility-and-persistence}

type 								accessibility 					persistence
------------------------------------------------------------------------------------
pipe 								only by related processes 	process
fifo 								permission mask 				ditto

UNIX domain socket 			permission mask 				ditto
Internet domain socket 		by any processe 				ditto

posix message q 				ppermission mask 				kernel
posix named semaphore 		dpermission mask 				kernel
posix unnamed semaphore 	permission of underlying mem depends
posix shared memory 			permission mask 				kernel

anonymous mapping 			only by releated				process 
memory mapped file 			permission mask 				file system 

unix and network domain:

Of all of the IPC methods shown in Figure 43-1, only sockets permit processes to communicate over a
network. Sockets are generally used in one of two domains: the UNIX domain, which allows
communication between processes on the same system, and the Internet domain, which allows
communication between processes on different hosts connected via a TCP/IP network. Often, only minor
changes are required to convert a program that uses UNIX domain sockets into one that uses Internet
domain sockets, so an application that is built using UNIX domain sockets can be made
network-capable with relatively little effort.

<portability>

However, the POSIX IPC facilities (message queues, semaphores, and shared memory) are not quite as
widely available as their System V IPC counterparts, especially on older UNIX systems. An
implementation of POSIX message queues and full support for POSIX semaphores have appeared on Linux
only in the 2.6.x kernel series. Therefore, from a portability point of view, System V IPC may be
preferable to POSIX IPC.

As of 06 Jan 2014, the latest stable kernel release is 3.12.6

releated:

Here, related means related via fork(). In order for two processes to access the object, one of them
must create the object and then call fork(). As a consequence of the fork(), the child process
inherits a handle referring to the object, allowing both processes to share the object.

<performance>

In some circumstances, different IPC facilities may show notable differences in performance.
However, in later chapters, we generally refrain from making performance comparisons, for the
following reasons:

1) The performance of an IPC facility may not be a significant factor in the overall performance of
an application, and it may not be the only factor in determining the choice of an IPC facility.

2) The relative performance of the various IPC facilities may vary across UNIX implementations or
between different versions of the Linux kernel.

3) Most importantly, the performance of an IPC facility will vary depending on the precise manner
and environment in which it is used. Relevant factors include the size of the data units exchanged
in each IPC operation, the amount of unread data that may be outstanding on the IPC facility,
whether or not a process context switch is required for each unit of data exchanged, and other
load on the system.

If IPC performance is crucial, there is no substitute for application-specific benchmarks run under
an environment that matches the target system. To this end, it may be worth writing an [abstract]
software layer that hides details of the IPC facility from the application and then testing
performance when different IPC facilities are substituted underneath the abstract layer.


{pipe}

The pipe is unnamed fifo and is an early form that can be used [related-processes] such as parent
and child. In other words, created using fork() call.

Linux supports uni-directional pipe or half-duplex so need two pipes for read and write.


{first-way::to-create-pipe}

Get two fds from a pipe() call. fd[0] for read and fd[1] for write. Create a child via fork(). Then
there is read-write channel between parent and child.

parent                      child
fd[1] write   pipe ->       fd[0] read
fd[2] read    pipe <-       fd[1] write

When close fd[1] in parent and fd[0] in child, only have one-way pipe. 

note: Pipe is kernel resource meaning that there is copy between kernel and process. Also there is
<no-open-call>.


{second-way::to-create-pipe}

popen() call which simplfies pipe creation, fork, reading/writing setting. However, need to set
problem to fork in the command line.

The C standard I/O library popen(3) makes it easy for the application programmer to open a pipe to
an external process.

#include <stdio.h>
FILE *popen(const char *command, const char *mode);
int pclose(FILE *stream);

The argument command must be a command that is acceptable to the UNIX shell. The second argument
mode must be the C string "r" for reading or "w" for writing.  No other combination, such as "w+",
is acceptable. 

(reading example)
FILE *p;
char cmd[1000];
/* argv[2] is fname */
sprintf(cmd,"grep 'Time has been updated to (Year:Month' %s | head -1",argv[2]);
p=popen(cmd,"r");
fgets(tmp,sizeof(tmp),p);
pclose(p);

note: After all, the reason that can use pipe between parent/child is that fds are shared.


{broken-pipe}

When write to fifo that is not opened to read. menas there is no reading process or reading process is
killed because reading fd of pipe or file gets closed. SIGPIPE. Instead, there is no date to read,
empty, reading thread or process is blocked.


{fifo}

To solve this, fifo was introduced and is called named pipe since has path name. means that it
is created in the filesystem as a file. Use usual read and write call. Fifo is either read-only or
write-only. mkfifo() call.

note: caution. When open a fifo to read which is not opend to write, calling process is blocked.
meaning there is no writing process. Therfore, if parent and child opens different fifos to read,
deadlock happens.


{from-nds-fusion-ipc}

FIFOs have certain natural limitations, they are unidirectional, and fragment large message sizes
with no built in support for reassembling the fragments. Furthermore FIFOs are limited in number due
to system resources. <fragment> and <limit-in-number>

Due to the limited number of FIFOs it was determined that there would be one
control FIFO per server (to establish communication from clients) and one pair of unidirectional
FIFOs for every pair of server and client instances (note that there is one instance of a client in
every process that uses that client). This is illustrated below.

server                  client a
- control pipe          -> and <-

								client b
							   -> and <-

note: KT. do not match with this pic?

To cope with fragmentation and the fact that there may be several interfaces being used on a single
client instance (and multiple components using that client instance in a single process) a protocol
was designed as described below. This protocol is used in both FIFO and TCP/IP IPC communication.


{summary}

If there is no reading process, broken-pipe. If there is no writing process, blocked.


==============================================================================
*kt_linux_core_310*  ipc: dbus and kbus	

{dbus}

http://www.freedesktop.org/wiki/Software/dbus/

What is D-Bus?

D-Bus is a message bus system, a simple way for applications to talk to one another. In addition to
interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and
reliable to code a "single instance" application or daemon, and to launch applications and daemons
on demand when their services are needed.

D-Bus supplies both a system daemon (for events such as "new hardware device added" or "printer
queue changed") and a per-user-login-session daemon (for general IPC needs among user
applications). Also, the message bus is built on top of a general one-to-one message passing
framework, which can be used by any two apps to communicate directly (without going through the
message bus daemon). Currently the communicating applications are on one computer, or through
unencrypted TCP/IP suitable for use behind a firewall with shared NFS home directories. (Help wanted
with better remote transports - the transport mechanism is well-abstracted and extensible.)

The D-Bus low-level API reference implementation and protocol have been heavily tested in the real
world over several years, and are now "set in stone." Future changes will either be compatible or
versioned appropriately.

The low-level libdbus reference implementation has no required dependencies; the bus daemon's only
required dependency is an XML parser (expat). Higher-level bindings specific to particular
frameworks (Qt, GLib, Java, C#, Python, etc.) add more dependencies, but can make more assumptions
and are thus much simpler to use. The bindings evolve separately from the low-level libdbus, so some
are more mature and ABI-stable than others; check the docs for the binding you plan to use.

There are also some reimplementations of the D-Bus protocol for languages such as C#, Java, and
Ruby. These do not use the libdbus reference implementation.

It should be noted that the low-level implementation is not primarily designed for application
authors to use. Rather, it is a basis for binding authors and a reference for reimplementations. If
you are able to do so it is recommended that you use one of the higher level bindings or
implementations. A list of these can be found on the bindings page.

The list of projects using D-Bus is growing and they provide a wealth of examples of using the
various APIs to learn from.

D-Bus is very portable to any Linux or UNIX flavor, and a port to Windows is in progress.

D-Bus applications

There are many, many technologies in the world that have "Inter-process communication" or
"networking" in their stated purpose: CORBA, DCE, DCOM, DCOP, XML-RPC, SOAP, MBUS, Internet
Communications Engine (ICE), and probably hundreds more. Each of these is tailored for particular
kinds of application. 

D-Bus is designed for two specific cases:

Communication between desktop applications in the same desktop session; to allow integration of the
desktop session as a whole, and address issues of process lifecycle (when do desktop components
start and stop running).

Communication between the desktop session and the operating system, where the operating system would
typically include the kernel and any system daemons or processes. 

For the within-desktop-session use case, the GNOME and KDE desktops have significant previous
experience with different IPC solutions such as CORBA and DCOP. D-Bus is built on that experience
and carefully tailored to meet the needs of these desktop projects in particular. D-Bus may or may
not be appropriate for other applications; the FAQ has some comparisons to other IPC systems.

The problem solved by the systemwide or communication-with-the-OS case is explained well by the
following text from the Linux Hotplug project:

A gap in current Linux support is that policies with any sort of dynamic "interact with user"
component aren't currently supported. For example, that's often needed the first time a network
adapter or printer is connected, and to determine appropriate places to mount disk drives. It would
seem that such actions could be supported for any case where a responsible human can be identified:
single user workstations, or any system which is remotely administered.

This is a classic "remote sysadmin" problem, where in this case hotplugging needs to deliver an
event from one security domain (operating system kernel, in this case) to another (desktop for
logged-in user, or remote sysadmin). Any effective response must go the other way: the remote
domain taking some action that lets the kernel expose the desired device capabilities. (The action
can often be taken asynchronously, for example letting new hardware be idle until a meeting
finishes.) At this writing, Linux doesn't have widely adopted solutions to such problems.
However, the new D-Bus work may begin to solve that problem. 

D-Bus may happen to be useful for purposes other than the one it was designed for. Its general
properties that distinguish it from other forms of IPC are:

Binary protocol designed to be used asynchronously (similar in spirit to the X Window System protocol).

Stateful, reliable connections held open over time.

The message bus is a daemon, not a "swarm" or distributed architecture.

Many implementation and deployment issues are specified rather than left ambiguous/configurable/pluggable.

Semantics are similar to the existing DCOP system, allowing KDE to adopt it more easily.

Security features to support the systemwide mode of the message bus. 


{kdbus}
https://github.com/gregkh/presentation-kdbus
https://github.com/gregkh/kdbus


==============================================================================
*kt_linux_core_031*  time

{abstime}

The abstime is the system time - the number of seconds and nanoseconds past Jan. 1, 1970, UTC. The
advantage in using the abstime, instead of a delta, is if the function prematurely returns ( perhaps
because of a caught signal ): the function can be called again without having to change the timespec
structure. 


==============================================================================
*kt_linux_core_400*  signal

{signal-is-notification}
A signal is a notification to a process that an event has occurred. Sometimes described as software
interrupts and are analogous to hardware interrupts in that they interrupt the normal flow of
execution of a program.

{signal-and-kernel}
One process can (if it has suitable permissions) send a signal to another process. Can be employed
as a synchronization technique, or even as a primitive form of interprocess communication (IPC). It
is also possible for a process to send a signal to itself. However, the usual source of many signals
sent to a process is the kernel.

{signal-symbolic-names}
Each signal is defined as a unique (small) integer, starting sequentially from 1. These integers are
defined in <signal.h> with symbolic names of the form SIGxxxx. Since the actual numbers used for
each signal vary across implementations, it is these symbolic names that are always used in
programs.

{pending}
A signal is said to be generated by some event. Once generated, a signal is later delivered to a
process, which then takes some action in response to the signal. Between the time it is generated
and the time it is delivered, a signal is said to be pending.

{signal-mask-per-process}
Sometimes, however, we need to ensure that a segment of code is not interrupted by the delivery of a
signal. To do this, we can add a signal to the process’s signal mask—a set of signals whose delivery
is currently blocked. If a signal is generated while it is blocked, it remains pending until it is
later unblocked (removed from the signal mask).

If a process receives a signal that it is currently blocking, that signal is added to the process’s
set of pending signals.

<signal-is-queued-or-not>
Standard signals can’t be queued; delivered only once. The set of pending signals is only a mask; it
indicates whether or not a signal has occurred, but not how many times it has occurred. In other
words, if the same signal is generated multiple times while it is blocked, then it is recorded in
the set of pending signals, and later delivered, just once. One of the differences between standard
and realtime signals is that realtime signals are queued.

{signal-handler}
Instead of accepting the default for a particular signal, a program can change the action that
occurs when the signal is delivered. Can be used to ignore signals or to change the default. To
change a default is usually referred to as installing or establishing a signal handler. When a
signal handler is invoked in response to the delivery of a signal, we say that the signal has been
handled or, synonymously, caught.

{signal-and-proc}
The Linux-specific /proc/PID/status file contains various bit-mask fields that can be inspected to
determine a process’s treatment of signals. The bit masks are displayed as hexadecimal numbers, with
the least significant bit representing signal 1, the next bit to the left representing signal 2, and
so on. 

These fields are SigPnd (per-thread pending signals), ShdPnd (process-wide pending signals since
Linux 2.6), SigBlk (blocked signals), SigIgn (ignored signals), and SigCgt (caught signals).
(The difference between the SigPnd and ShdPnd fields will become clear when we describe the handling
 of signals in multithreaded processes in Section 33.2.) The same information can also be obtained
using various options to the ps(1) command.

{standard-and-realtime-signal}
Signals fall into two broad categories; standard and realtime. On Linux, the standard signals are
numbered from 1 to 31. We describe the standard signals in this chapter. The other set of signals
consists of the realtime signals.

<why-reliable-signal>
In early implementations, signals could be lost (i.e., not delivered to the target process) in
certain circumstances. Furthermore, although facilities were provided to block delivery of signals
while critical code was executed, in some circumstances, blocking was not reliable. These problems
were remedied in 4.2BSD, which provided so-called reliable signals.

However, the Linux signal(7) manual page lists more than 31 signal names. The excess names can be
accounted for in a variety of ways. Some of the names are simply synonyms for other names, and are
defined for source compatibility with other UNIX implementations. Other names are defined but
unused.

{signals}

<SIGABRT>
The abort() function (Section 21.2.2) generates a SIGABRT signal for the process, which causes it to
dump core and terminate.

<SIGINT>
When the user types the terminal interrupt character (usually Control-C), the terminal driver sends
this signal to the foreground process group. The default action for this signal is to terminate the
process.

<SIGPIPE>
This signal is generated when a process tries to write to a pipe, a FIFO, or a socket for which
there is no corresponding reader process. This normally occurs because the reading process has
closed its file descriptor for the IPC channel. See Section 44.2 for further details.

<SIGSEGV>
This very popular signal is generated when a program makes an invalid memory reference. A memory
reference may be invalid because the referenced page doesn’t exist e.g., it lies in an unmapped area
somewhere between the heap and the stack, the process tried to update a location in read-only memory
e.g., the program text segment or a region of mapped memory marked read-only, or the process tried
to access a part of kernel memory while running in user mode (Section 2.1). In C, these events often
result from dereferencing a pointer containing a bad address (e.g., an uninitialized pointer) or
passing an invalid argument in a function call. The name of this signal derives from the term
segmentation violation.

<SIGKILL>
This is the sure kill signal. It can’t be blocked, ignored, or caught by a handler, and thus always
terminates a process.

<SIGTERM>
This is the standard signal used for terminating a process and is the [default] signal sent by the
kill and killall commands. Users sometimes explicitly send the SIGKILL signal to a process using
kill –KILL or kill –9. However, this is generally a mistake. A well-designed application will have a
handler for SIGTERM that causes the application to exit gracefully, cleaning up temporary files and
releasing other resources beforehand. Killing a process with SIGKILL bypasses the SIGTERM handler.
Thus, we should always first attempt to terminate a process using SIGTERM, and reserve SIGKILL as a
last resort for killing runaway processes that don’t respond to SIGTERM.

<SIGURG>
This signal is sent to a process to indicate the presence of out-of-band (also known as urgent) data
on a socket


# ============================================================================
#{

==============================================================================
*kt_linux_set_001* ubuntu: virtualbox

{graphic-issue}

When see a problem with NVIDIA, need to install a driver for ubuntu manually:

http://askubuntu.com/questions/141606/how-to-fix-the-system-is-running-in-low-graphics-mode-error

In short, get console and do:

sudo apt-get install nvidia-current

Also, install the guest addition:

Click on Install Guest Additions from the Devices menu and all will be done automatically.


{sharing-between-os}

http://www.virtualbox.org/manual/ch04.html#sharedfolders

# set sharing folder on the host using virtual box menu and reboot
#
select "Shared folders" from the "Devices" menu, or click on the folder icon on the status bar in
the bottom right corner.

# shared folder is
#
/media/sf_myfiles 

# add group permission when see access error when reading shared folder
#
Access to auto-mounted shared folders is only granted to the user group vboxsf, which is created by
the VirtualBox Guest Additions installer. Hence guest users have to be member of that group to have
read/write access or to have read-only access in case the folder is not mapped writable.

sudo usermod -a -G vboxsf {username}


{sharing-using-samba}

<1> Have samba installation, setting, and user. This is simple and general and see as an example.
http://www.sitepoint.com/ubuntu-12-04-lts-precise-pangolin-file-sharing-with-samba/

sudo apt-get install samba samba-common system-config-samba winbind

change winbind setting

For samba server settings: Workgroup. (this is domain) This field should be the same value as that
used by your Windows Workgroupi.e if your WIndows Users are members of the 'Home' workgroup, type
'Home' in this field.  

For samba users: use windows user name.


<2> By default, VB uses Host-only networking and this means host(windows) cannot see guest. To
enable for host to see guest, change to 'bridged network' as shown in:

6.4. Bridged networking
http://www.virtualbox.org/manual/ch06.html

So change it in setting menu of VB and restart VM. Get IP and can access guest from host windows.

note: must have two network adaptors: NAT for internet and bridged for samba between host and guest.
But cannot ssh to other host. seems firewall problem and when back to host only network, ssh works
but not sharing as host only net gives private net ip.


==============================================================================
*kt_linux_set_002*	ubuntu: workspace {shortcuts}

https://help.ubuntu.com/community/KeyboardShortcuts

{workspaces}

Known as virtual desktops. You can switch between workspaces with hotkeys by pressing Ctrl-Alt-arrow
key. 

To move apps to other workspaces, press Ctrl-Alt-Shift-arrow key.

{shortcuts}

C-A-T		" open a terminal

{compiz}

To have window like win management: win arrow keys

http://www.howtoforge.com/install-compiz-on-the-unity-desktop-on-ubuntu-12.04-precise-pangolin


==============================================================================
*kt_linux_set_003*	ubuntu: samba

sudo service smbd start
sudo service smbd stop
sudo service smbd restart


To see what are shared:

$ smbclient -L //106.1.8.6/
Enter keitee.park's password:
Domain=[SERILOCAL] OS=[Unix] Server=[Samba 3.6.3]

        Sharename       Type      Comment
        ---------       ----      -------
        IPC$            IPC       IPC Service (rockford server (Samba, Ubuntu))
        dsk1            Disk      Main Disk
Domain=[SERILOCAL] OS=[Unix] Server=[Samba 3.6.3]

        Server               Comment
        ---------            -------

        Workgroup            Master
        ---------            -------
        WORKGROUP


==============================================================================
*kt_linux_set_004*	ubuntu: nfs

sudo apt-get install nfs-kernel-server

You can configure the directories to be exported by adding them to the /etc/exports file. For
example:

/ubuntu  *(ro,sync,no_root_squash)
/home    *(rw,sync,no_root_squash)

You can replace * with one of the hostname formats. Make the hostname declaration as specific as
possible so unwanted systems cannot access the NFS mount.

To start the NFS server, you can run the following command at a terminal prompt:
sudo /etc/init.d/nfs-kernel-server start

==============================================================================
*kt_linux_set_005*	ubuntu: check running services

sudo service --status-all

==============================================================================
*kt_linux_set_006*	ubuntu: connect from windows remote desktop

sudo apt-get install xrdp

Then run windows remote desktop and connect using ip or hostname. That's it.


# ============================================================================
#{

==============================================================================
*kt_linux_ref_001* references

{ref-001}
Richard Stevens. Unix Network Programming Vol 2 Addision Wesley. 2nd Ed.
http://www.kohala.com/start/unpv22e/unpv22e.html

{ref-002}
The Linux Programming Interface: A Linux and UNIX System Programming Handbook 
Michael Kerrisk (Author) 

For sources, http://www.man7.org/tlpi/


-------------------------------------------------------------------------------
Copyright: see |ktkb|  vim:tw=100:ts=3:ft=help:norl:

