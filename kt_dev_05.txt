*kt_dev_05*                                                                tw=100

KT KB. DEVELOPMENT. BROADCAST.

/^[#=]{
Use #{ for a group and ={ for a item

|kt_dev_bcast_001| OIPF
|kt_dev_bcast_002| CRB
|kt_dev_bcast_003| pdl

|kt_dev_bcast_100| mpeg: time sync
|kt_dev_bcast_101| mpeg: ts view tool
|kt_dev_bcast_102| mpeg: locator
|kt_dev_bcast_103| mpeg: es

|kt_dev_bcast_200| mpeg: streaming: terms
|kt_dev_bcast_201| mpeg: streaming: dash
|kt_dev_bcast_202| mpeg: streaming: mpd file

|kt_dev_bcast_300| gst: git web
|kt_dev_bcast_300| gst:
|kt_dev_bcast_302| gst: timer and query
|kt_dev_bcast_303| gst: registry
|kt_dev_bcast_304| gst: queue, branch
|kt_dev_bcast_305| gst: buffers and interact with a pipeline
|kt_dev_bcast_301| gst: base elements
|kt_dev_bcast_302| gst: elements: uridecodebin
|kt_dev_bcast_302| gst: elements: h264parse
*kt_dev_bcast_302* gst: elements: tsparse

|kt_dev_bcast_310| gst: debug
|kt_dev_bcast_311| gst: gst-launch
|kt_dev_bcast_311| gst: gst-inspect
|kt_dev_bcast_312| gst: sdk, tutorials
|kt_dev_bcast_313| gst: core apis reference

|kt_dev_bcast_319| gst: mime and types
|kt_dev_bcast_320| gst: plugin writer's guide

*kt_dev_bcast_330* gst: element property 
*kt_dev_bcast_330* gst: man: 01: introduction
*kt_dev_bcast_331* gst: man: 03: foundations
*kt_dev_bcast_332* gst: man: 04: initialize gst
*kt_dev_bcast_333* gst: man: 05: elements. state
*kt_dev_bcast_334* gst: man: 06: bins
*kt_dev_bcast_335* gst: man: 07: bus
*kt_dev_bcast_336* gst: man: 08: pad and capability
*kt_dev_bcast_330* gst: man: 14: clocks and synchronization
*kt_dev_bcast_330* gst: man: 15: buffering
|kt_dev_bcast_330| gst: man: 18: autoplug and typefind

*kt_dev_bcast_400* gst: gst time and mpeg time
|kt_dev_bcast_400| gst: preroll
|kt_dev_bcast_400| gst: ref: gstcap

*kt_dev_bcast_500* gst: man: gstcap
*kt_dev_bcast_500* gst: gstbasesink


# ============================================================================
#{
={============================================================================
*kt_dev_bcast_001* OIPF, DAE

As of 16 June 2014, the Open IPTV Forum has transferred its technical activities to the HbbTV
Association.

http://www.oipf.tv/specifications/

The Open IPTV Forum¿s (OIPF) Declarative Application Environment (DAE), which offers a browser
environment to network applications, is briefly reviewed. It is being implemented in many retail TVs
by major manufacturers. 


={============================================================================
*kt_dev_bcast_002* CRB

connected red button (CRB)


={============================================================================
*kt_dev_bcast_003* PDL

What is Progressive Download

* Download of Movies and TV program content to STB over the IP connection.
* VOD-like experience without allocating VOD bandwidths in the system.
* Supports variable available bandwidth – over Internet.
* Content servers (HTTP) much cheaper than VOD real time servers.
* Progressive Download is where content viewing occurs while content is still
being downloaded.
* STB will only download if it has enough disk space available for the download
content file.


PDL vs. VOD

VOD:
* Content is streamed over the IP connection. Trick modes are controlled by the
server according to the user request.

* Expensive VOD servers are required.
* Network Quality of Service is required.
* No local disk needed.

PDL:
* The content downloaded over the IP connection into the local disk and played
back like any other XTV content.

* No need of expensive VOD servers.
* No need of network Quality of Service.
* Local disk required.

PDL content is stored on the Content Distribution Network (CDN) in Network File
Format (NFF), a file format defined by NDS specifically to support PDL and
related applications. NFF defines a structure for content files including a
Table of Contents (TOC) and containers for content in MPEG TS format, and
optionally indexing and related metadata.


# ============================================================================
#{
={============================================================================
*kt_dev_bcast_100* mpeg: time sync

{master-clock}
STC(system time clock). The mpeg-2 encoder contains 27 MHz oscillator and 33 bits counter, called
the STC. STC is a 33 bits value driven by 90 KHz clock, obtained by dividing the 27 MHz by 300. It
belongs to a particular 'program' and is the master clock of the video and audio encoders for that
program.


{timing-model} 
At the input of the encoder, Point A, the time of occurrence of an input video picture or audio
block (and of the appearance of its coded version at the zero-delay encoder output) is noted by
sampling the STC. A constant quantity equal to the sum of encoder and decoder buffer 'delays' is
added, creating a Presentation Time Stamp (PTS), which is then inserted in the 'first' of the
packet(s) representing that picture or audio block, at Point B in the diagram.


<dts-and-pts>
Also entered into the bitstream under certain conditions is a Decode Time Stamp (DTS), which
represents the time at which the data should be taken instantaneously from the decoder buffer and
decoded. Since the System Target Decoder delay is zero, the DTS and PTS are identical except in the
case of picture reordering for B pictures. The DTS is only used where it is needed because of
reordering. Whenever DTS is used, PTS is also coded.

PTS (or DTS) is entered in the bitstream at intervals not exceeding 700 mS. ATSC further constrains
PTS (or DTS) to be inserted at the beginning of each coded picture ( access unit ). 

note: the both are generally the same, can decide whether to use PTS only or both in PES header.


<pcr-and-scr>
in addition, the output of the 'encoder' buffer (Point C) is time stamped with System Time Clock
(STC) values, called Program Clock Reference (PCR) if the stamp is at the 'transport' packet level,
    or System Clock Reference (SCR) at the PES level. PCR time stamps are required to occur at
    maximum 100 mS 'intervals'. SCR time stamps are required to occur at maximum 700ms intervals.

note: The PTS is stamped at encoder end and the PCR is stamped at packetizer end.

note: MPEG says there should be 10 PCR a second at least and DVB says 25 a second.


<mpeg-spec-says>
D.0.2 Audio and Video Presentation Synchronization

A PTS indicates the time that the PU(Presentation Unit) which results from
decoding the AU(Access Unit) which is associated with the PTS should be
presented to the user. 

So AU is coded stream and PU is decoded stream.

Since PTS and DTS values are not required for every AAU and VAU, the decoder may
choose to interpolate values which are not coded. PTS values are required with
intervals not exceeding 700ms in each elementary audio and video stream. These
time intervals are measured in presentation time, that is, in the same context
as the values of the fields, not in terms of the times that the fields are
transmitted and received. 


<dash-example>
Usually, the audio ES chunk comes first with later DTS and the video comes later
with sooner DTS. This varies to make sure there is no big difference between
them.

The difference between audio chunks : 21ms
The difference between video chunks : 40ms
The difference between video and audio chunks : 808ms

Shows that DTS and PTS are differenct for some chunks and believe that it is
owing to reordering. 

// from MPEG spec. So when I and P is too far from each other.
Since the audio and video elementary stream decoders are instantaneous in the
STD, the decoding time and presentation time are identical in most cases; the
only exception occurs with video pictures which have undergone re-ordering
within the coded bit stream, i.e. I and P pictures in the case of non-low-delay
video sequences. 


{clock-sync} clock-recovery, sync-decoders
The Program Clock Reference (PCR) and/or the System Clock Reference (SCR) are used to synchronize
the decoder STC with the encoder STC. (See Decoder STC Synchronization )

PCR is a clock recovery mechanism for MPEG programs. When a program is encoded, a 27 MHz STC drives
the encoding process. When the program is decoded (or multiplexed), the decoding process must be
driven by a clock which is locked to the encoder's STC. The decoder uses the PCR to regenerate a
local 27 MHz clock.

When a program is inserted into the TS (packetized), 27 MHz timestamp is inserted - PCR. At the
decoder end, it uses a Voltage Controlled Oscillator to generate 27 MHz clock. When PCR is received
via PCR PID in a program, it is compared to a local counter which is driven by the VCXO to ensure
that the 27 MHz clock is locked to the PCR. Get a diff and 'adjust' local clock.

The filtered difference (times a proportionality constant) is the control voltage for a crystal VCO.
This loop stabilizes with the correct frequency, but with an offset in STC that is proportional to
the offset in frequency between the encoder 27 Mhz oscillator and decoder 27 MHz oscillator
free-running frequency. This implies that the decoder should have a slightly larger buffer to
absorb the offset timing.

The PCR field is 42 bit field in the adaptation field of the TS. The PCR field consists of a 9 bit
part that increments at a 27 MHz rate and a 33 bit part that increments at a 90 KHz rate.

Not every TS packet containing the specific PID necessarily includes a PCR value. It is sufficient
to insert a value into a TS packet every 40/100 ms. For this reason, the PCR value is transmitted in
an optional field of the extendable header (adaptation field) in the TS packet.


pcr base: 33 bits          pcr extension: 9 bits    : 42 bits
0 to 2^33-1                0 to 299
90 KHz                     27 MHz
<---------- PTS --------->
<--------------------- PCR ---------------->

note:

Say when use 8 bits:

   7     6     5     4     3     2     1     0
   2^7   2^6   2^5   2^4   2^3   2^2   2^1   2^0

So the max is 2^8-1 and use 8 bits. Likewise, the max is 2^33-1 and use 33 bits. Needs one bit more
in addition to 32 bits type.

8589934591 (2^33-1) / 90K = 95443.717677778 sec 
                            95443.717677778 sec / 3600 = 26.5 hours

There is to be no problem longer than this and so enough to use it as PTS

<in-cdi>
Typedef struct
{
    Uint32_t high; // use LSB and if MSB is 1, invalid PTS
    Uint32_t low;
} PTS;

00:09:55:361 [pid=521,tid=18821664] IOCTL(48="clocksync0", CLOCK_SYNC_GET_VALUE)
PARAM =  (*debug_ptr_ClockValue) = {
   .high = 0
   .low = 108107180
 }

KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458528462]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458528462]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458532062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458928062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1459828062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1460728062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1460810862]

As can see, use 32th bit in the high.


{av-sync} sync-frames
This means that PTS is always higher than the current PCR. The difference between PCR and PTS
represents the data dwell time in the receiver and is thus closely related to the buffer size of the
receiver. According to MPEG-2, the dwell time must not exceed one second.


{problems-caused-by-pcr}
If decoding is too fast (pcr is faster) the buffer memory if the receiver might run empty because
the receiver wants to process the data faster than it arrives at the receiver. In the second case,
    the buffer might overflow because data is processed slower than it arrives at the receiver.


{pcr-example}
pcr base : 0x02B2E37AF  = 724449199
pcr ext  : 0x009B       = 155
pcr      : (724449199*300 + 155)/27 MHz = 8049.435550s = 2:14:09.435550


{pts-example-from-pes-header}
5 byte (40bits)
     0x23 :      0x9A :      0x0F :      0x08 :      0x19
0010 0011 : 1001 1010 : 0000 1111 : 0000 1000 : 0001 1001
        X                       X                       X

Remove the first 4 bits and each maker bit

     001    1001 1010   0000 111    0000 1000   0001 100

This becomes 33 bits:
     001100110100000111000010000001100 (33 bits)
     0-0110-0110-1000-0011-1000-0100-0000-1100 (0x6683840C)

Converts it to decimal, and / 90K

     19109.945022222222222222222222222 sec

5 hours and 18 minuts is 19080 sec. 19109-19080 = 29. So 5:18:29.945.


{pts-from-a-stream}
Frame #        PTS (hex)        PTS (dec)        Diff
1                29042690        688137872                
2                29042d98        688139672        1800    << Frame 1s interpolated..so this is expected.
3                29043ba8        688143272        3600    -> 0.04s = 40ms
4                290449b8        688146872        3600

Each 'frame' has a Presentation Time Stamp (PTS) note: can put PTS for each frame and GOP?

I can't see any drift more than 20 milli seconds, which is a field period. Where ever you see
INTERPOLTAED values, the PTS are 20 milli seconds ahead from the expected value, which shows that
this PTS was read from the next vsync period.

For example,for frame number 32,

PTS in 1st run is 0x2905d378  CODED
PTS is 2nd run is 0x2905da80 INTERP
PTS in 3rd run is 0x2905d378 CODED

The difference between 1st and 2nd run is 20 milli seonds. Similarly in all other cases the drift is
+/-20milli sec which is with in a frame period of 40 msec. This should not create issue of this
sort.


<interpolated-pts>
There is no relation between gop boundaries and interpolation. If a PTS is not available for a
frame/field, Display Manager (Software responsible for comparing PTS of frames and STC) willl
generate a PTS based on the previous coded PTS value so that the PTS-STC compariosn is more
accurate.


{lip-sync-problem}
The lip sync problem has nothing to do with the spec and is implementation issue. This happens when
do not check PTS from a decoder.


{clock-source}
Use CDI term. For live, clock source is "Clock Filter", set PCR PID, get PCR. This CF is attached to
"Clock Sync" device and CS is do clock recovery.

For playback, clock source is decoder, that is uses the first video or audio PTS, set CS with PTS
and do "free-running". That means there is no clock recovery (clock sync) for playback.

When playback a different program, discontinuity happens and set new PTS to CS and do free run.


{discontinuity}
Under special circumstances, the PCR may contain an unavoidable discontinuity, which may be caused
by a switchover from one decoder to another in the transmitter during program emission (contents are
        obtained from another source). PCR discontinuity of this type must be marked in the program
by means of discontinuity_indicator in the adaptation field.


={============================================================================
*kt_dev_bcast_101* mpeg: ts view tool

http://dvbsnoop.sourceforge.net/

// PID is 0x200, gives the filename, set ts format, show 10 items.
$ dvbsnoop.exe 0x200 -if FOSH_Stream20.TS -s ts -n 10

// -tssubdecode shows PES decoding and -ph 0 don't show data dump
// note: to see PES, should give big counts
$ dvbsnoop.exe 0x200 -ph 0 -if FOSH_Stream20.TS -s ts -tssubdecode -n 10000 > log.txt

        program_clock_reference:
            baseH: 0 (0x00)
            baseL: 224777 (0x00036e09)
            reserved: 63 (0x3f)
            extension: 79 (0x004f)
             ==> program_clock_reference: 67433179 (0x0404f2db)  [= PCR-Timestamp: 0:00:02.497525]

            PTS: 
               Fixed: 2 (0x02)
               PTS:
                  bit[32..30]: 0 (0x00)
                  marker_bit: 1 (0x01)
                  bit[29..15]: 7 (0x0007)
                  marker_bit: 1 (0x01)
                  bit[14..0]: 1579 (0x062b)
                  marker_bit: 1 (0x01)
                   ==> PTS: 230955 (0x0003862b)  [= 90 kHz-Timestamp: 0:00:02.5661]


={============================================================================
*kt_dev_bcast_102* mpeg: locator

ETSI TS 102 812 V1.3.1 (2012-05)

14 System integration aspects
14.1 Namespace mapping (DVB Locator)


={============================================================================
*kt_dev_bcast_103* mpeg: pes

{ES}
sequence layer    : SH | Sequence(GOP) | SH | GOP | SH | GOP | ...
                         *
SH {picture width, picture height, aspect ratio, bit rate, picture rate }

GOP layer         : | I B B P B B P B B ... B P |
                        *
GOP { GOP header, FH, frame 1, FH, frame N }                    

FH { frame type, ..., extension start code, frame structure }

Picture layer     : slice | slice ...
                            *
Slice { SH(Slice Header), macro blocks 1 to N }

Slice layer       : MB MB MB ...

Macro block layer :

Block layer       :

The elementary stream is a 'continual' stream of encoded video frames. Though
all the data required to reconstuct frames exists here. No timing information or
systems data is contained. Thats the job of the MPEG-2 multiplexer


<pictures-or-frames>
I(Intra-coded)-picture 
P(Predictive-coded)-picture
B(Bi-directional Predictive-coded)-picture

I-frames: contain full picture information
P-frames: predicted from past I or P frames
B-frames: use past and future I or P frames
Transmit I frames every 12 frames or so.

<picture-order>
Pictures are coded and decoded in a different order than they are displayed.
This is due to bidirectional prediction for B pictures. See example below which
illustrates reordering for a 12 picture long GOP.

Source order and encoder input order:
I(1) B(2) B(3) P(4) B(5) B(6) P(7) B(8) B(9) P(10) B(11) B(12) I(13)
 
Encoding order and order in the coded bitstream: DTS
I(1) P(4) B(2) B(3) P(7) B(5) B(6) P(10) B(8) B(9) I(13) B(11) B(12)

In order for a decoder to reconstruct a B-frame from the preceding I and
following P frames, both these must arrive first. So the order of frame
transmission must be different to the order they appear on the tv screen.

Decoder output order and display order (same as input): PTS
I(1) B(2) B(3) P(4) B(5) B(6) P(7) B(8) B(9) P(10) B(11) B(12) I(13)


{PES}
The PES packets can be of variable length, typically upto 64 kbytes, but they
can be longer.

| start | stream | PES           | optional   | stuffing | PES data |
  code    id       packet length   PES header 

One of the most important parts of the structure, are the PTS and DTS, these
allow the decoder to reconstruct the video stream from the I, B, P frames sent
by the encoder.


https://www.uic.edu/classes/ece/ece434/chapter_file/chapter7.htm

The format of the PES header is defined by the stream ID (SID) used to identify
the type of ES. The PES packet length (PESPL) indicates the number of bytes in
the PES packet. The scrambling mode is represented by the scrambling control
(SC). The PES header data length (PESHDL) indicates the number of bytes in the
optional PES header (OPESH) fields, as well as stuffing bytes (SB) used to
satisfy the communication network requirements.

The PES header contains timestamps to allow for synchronization by the decoder.
Two different timestamps are used: presentation timestamp (PTS) and decoding
timestamp (DTS). The PTS specifies the time at which the access unit should be
removed from the decoder buffer and presented. The DTS represents the time at
which the access unit must be decoded. The DTS is optional and it is only used
if the decoding time differs from the presentation time.[2]

The elementary stream clock reference (ESCR) indicates the intended time of
arrival of the packet at the system target decoder (STD). The rate at which the
STD receives the PES is indicated by the elementary stream rate (ESR). Error
checking is provided by the PES cyclic redundancy check (PESCRC).

note: is it a way to reconstruct ES from PES?

The pack header field (PHF) is a PS pack header. The program packet sequence
counter (PPSC) indicates the number of system streams. The STD buffer size is
specified by the P-STD buffer (PSTDB) field.

Optional Fields 2
PESPD Packetized Elementary Stream Private Data
PHF   Pack Header Field
PPSC  Program Packet Sequence Counter
PSTDB P-STD Buffer
PESEF Packetized Elementary Stream Extension Field


{packetization}

VES |        I    | B | B |   P   | B | B |   P    | B | B | ...
AES | frame 1 | frame 2 | ...

    |        I    | B | B |   P   | B | frame 1  | B |   P    | B | B | ...
PES <-packet-><-packet-><-packet    -> <-packet-> <-packet-><-packet->

note: PES has audio and video? Maybe muxed PES stream from VPES and APES?

note: How construct ES from PES? Use time?

TS

<pes-usuage-for-video>
Except for ATSC, there is not defined mapping between an encoded picture and a
PES packet, nor is there any alignment requirement between PES and pictures

ATSC defines PES per Picture

Tandberg (and some others) use a PES per GOP (additional picture timestamps are
        inferred as required)

A few others use a fixed size PES; the timestamp then applies to the first
picture that starts in the PES packet, and is not always present

PTS (Presentation Time Stamp) and DTS (Decode Time Stamp), when present, are
used to synchronise streams, and control buffer usage

<from-brcm-7401>

note: PES parser

The data transport processor is an MPEG-2/DIRECTV transport stream message/PES
parser and demultiplexer. It can simultaneously process 256 PID filters via 128
PID channels in up to five independent external transport stream inputs and two
internal playback channels, with decryption for all 128 PID channels. 

It supports message/PES parsing for 128 PID channels with storage to 128
external DRAM buffers, and it provides 512 4-byte generic section filters that
can be cascaded to provide effectively longer filters (up to 64-bytes or 128
        filters of 16-bytes each). 

The data transport module provides two sets of a two-channel remux output. 
    
The data transport module has a RAVE (record, audio and video interface engine)
    function, which can be configured to support eight record channels for PVR
    functionality and six AV channels to interface audio and video decoders.

* PES packet extraction for up to 128 PID channels.
* Supports parsing of Transport/PES data to ES and generate CDB/ ITBs for
audio/video decoders.


FUNCTIONAL OVERVIEW

The Data Transport Processor is an MPEG-2/DIRECTV transport stream message/PES
parser and demultiplexer. It is capable of simultaneously processing 256 PIDs
via 128 PID channels in up to six independent transport streams using the six
available parsers. These six streams are selected from five external serial
transport stream inputs, and four internal playback channels. The data transport
supports decryption for up to 128 PID channels in the six streams. All 128 PID
channels can be used by RAVE (record, audio and video interface engine), PCR
processors, message filter as well as for output via the high-speed transport or
remux module.

The data transport supports up to 128 PID channels for message or generic PES
processing and storage in up to 128 external DRAM message buffers. There are 512
4-byte generic filters supported for processing of MPEG/DVB sections or DIRECTV
messages. A special addressing mode filter is included for up to 32 PID channels
(PID channels 0-31), which filters MPEG and private stream messages.

The data transport module supports RAVE (record, audio and video interface
        engine) function, which can supports up to eight record channels with up
to total 12 SCDs (configured 1-12 per record channel) and six AV channels to
interface to the audio/video decoders.

The data transport also provides two PCR recovery blocks and one serial STC
broadcast block for transmitting the STC to the decoders

Input Band 
Refers to the five external transport stream inputs supported by this design
(IB0-4).

Parser Band 
Refers to the transport streams that are selected as inputs to the five front
end parsers or two playback channels to two playback parsers

PES Parser

The PES parser delineates PES packets and sends them to the message buffers. Any
number of the PID channels 0-127 can be enabled for PES processing. When a
complete PES packet is received, a data available interrupt is generated by the
message buffer manager. 

The PES parser checks for PES packet lengths and generates length error
interrupts. PES Padding streams (i.e., PES messages with stream_id of 0xBE) are
removed by default, or optionally retained. When stored to memory, padding bytes
(0x55) are optionally added at the end of each PES packet to word align to
32-bit boundaries in the message buffers. During data transport playback, the
padding bytes are removed.

The PES Parser checks for PES packet length and can generate length error
interrupts if enabled. It uses the payload_unit_start_indicator bit in the
transport packet to detect the beginning of the PES packet. A length error is
generated whenever the end of a PES packet does not coincide with the end of a
transport packet or the payload_unit start_indicator is received prior to the
end of the current PES packet. Only PID channels 0 to 127 can be routed to PES
parser.


{TS}
PUSI: Payload Unit Start Indicator which indicate if TS packet has PES header


={============================================================================
*kt_dev_bcast_200* mpeg: streaming: terms

o RTMP
Adobe's RTMP-based Dynamic Streaming uses Adobe's proprietary Real Time Messaging Protocol (RTMP),

o HLS
Apple HTTP Live Streaming (HLS).

What is MPEG DASH?
http://www.streamingmedia.com/Articles/Editorial/What-Is-.../What-is-MPEG-DASH-79041.aspx


={============================================================================
*kt_dev_bcast_201* mpeg: streaming: dash

MPEG DASH (Dynamic Adaptive Streaming over HTTP) is a developing ISO Standard
(ISO/IEC 23009-1)

Adaptive streaming involves producing several instances of a live or on-demand
source file and making them available to various clients depending upon their
delivery bandwidth and CPU processing power. By monitoring CPU utilization
and/or buffer status, adaptive streaming technologies can change streams when
necessary to ensure continuous playback or to improve the experience.


{media-presentation-description-data-model}

http://www.streamingmedia.com/Articles/Editorial/What-Is-.../What-is-MPEG-DASH-79041.aspx

Figure 1. The Media Presentation Data Model. Taken from MPEG-DASH presentation
at Streaming Media West, 2011.

For DASH, the actual A/V streams are called the Media Presentation, while the
manifest file is called the Media Presentation Description.

The media presentation, <mpd> defines the video sequence with one or more
consecutive <periods> that break up the video from start to finish. Each period
contains multiple <adaptation-sets> that contain the content that comprises the
audio/video experience. This content can be muxed, in which case there might be
one adaptation set, or represented in elementary streams, as shown in Figure 1,
    enabling features like multiple language support for audio. 

Each adaptation set contains multiple <representations>, each a single stream in
the adaptive streaming experience. In the figure, Representation 1 is
640x480@500Kbps, while Representation 2 is 640x480@250Kbps.

Each representation is divided into <media-segments>, essentially the chunks of
data that all HTTP-based adaptive streaming technologies use. Data chunks can be
presented in discrete files, as in HLS, or as byte ranges in a single media
file. Presentation in a single file helps improve file administration and
caching efficiency as compared to chunked technologies that can create hundreds
of thousands of files for a single audio/video event.

note: there are five components in the model.

<MPD>
The DASH manifest file, called the Media Presentation Description, is an XML
file that identifies the various content components and the location of all
alternative streams. This enables the DASH player to identify and start playback
of the initial segments, switch between representations as necessary to adapt to
changing CPU and buffer status, and change adaptation sets to respond to user
input, like enabling/disabling subtitles or changing languages.


{features}
Other attributes of DASH include:

o DASH is codec-independent, and will work with H.264, WebM and other codecs

o DASH supports both the ISO Base Media File Format (essentially the MP4 format)
    and MPEG-2 transport streams

o DASH does not specify a DRM method but supports all DRM techniques specified
in ISO/IEC 23001-7: Common Encryption

o DASH supports trick modes for seeking, fast forwards and rewind

o DASH supports advertising insertion


{spec}
http://mpeg.chiariglione.org/


={============================================================================
*kt_dev_bcast_202* mpeg: streaming: mpd file

<?xml version="1.0" encoding="UTF-8"?>
<MPD type="dynamic" xmlns="urn:mpeg:dash:schema:mpd:2011" profiles="urn:mpeg:dash:profile:isoff-live:2011,urn:dvb:dash:profile:dvbdash:2014" minBufferTime="PT1.11S" minimumUpdatePeriod="PT1H" timeShiftBufferDepth="PT35M" availabilityStartTime="2014-08-06T11:00:00Z">
<!-- MPEG DASH ISO BMFF test stream with avc3 -->
<!-- BBC Research & Development -->
<!-- For more information see http://rdmedia.bbc.co.uk -->
<ProgramInformation>
	<Title>Adaptive Bitrate Test Stream from BBC Research and Development - Full stream with separate initialisation segments</Title>
	<Source>BBC Research and Development</Source>
</ProgramInformation>
<UTCTiming schemeIdUri="urn:mpeg:dash:utc:http-xsdate:2014" value="http://time.akamai.com/?iso"/>
<Period start="PT0S">
	<AdaptationSet startWithSAP="2" segmentAlignment="true" id="1" scanType="progressive" mimeType="video/mp4" contentType="video">
		<Role schemeIdUri="urn:mpeg:dash:role:2011" value="main"/>
		<SegmentTemplate startNumber="1" timescale="1000" duration="3840" media="$RepresentationID$/$Number$.m4s" initialization="$RepresentationID$/IS.mp4"/>
		<Representation id="V1" codecs="avc3.4d4015" height="288" width="512" bandwidth="356296" />
		<Representation id="V2" codecs="avc3.4d401e" height="396" width="704" bandwidth="619088" />
		<Representation id="V3" codecs="avc3.64001f" height="504" width="896" bandwidth="1330608" />
		<Representation id="V4" codecs="avc3.640020" height="720" width="1280" bandwidth="2501216" />
		<Representation id="V5" codecs="avc3.640028" height="1080" width="1920" bandwidth="4487408" />
	</AdaptationSet>
	<AdaptationSet startWithSAP="2" segmentAlignment="true" id="2" audioSamplingRate="48000" lang="eng" mimeType="audio/mp4" contentType="audio">
		<AudioChannelConfiguration schemeIdUri="urn:mpeg:dash:23003:3:audio_channel_configuration:2011" value="2"/>
		<Role schemeIdUri="urn:mpeg:dash:role:2011" value="main"/>
		<SegmentTemplate startNumber="1" timescale="1000" duration="3840" media="$RepresentationID$/$Number$.m4s" initialization="$RepresentationID$/IS.mp4"/>
		<Representation id="A1" codecs="mp4a.40.2" bandwidth="95792" />
	</AdaptationSet>
</Period>
</MPD>


={============================================================================
*kt_dev_bcast_300* gst: git web

http://cgit.freedesktop.org/gstreamer/

http://cgit.freedesktop.org/gstreamer/gstreamer/tree/docs/design


={============================================================================
*kt_dev_bcast_300* gst

http://docs.gstreamer.com/display/GstSDK/Basic+tutorials

{pipeline}

<element-and-pipeline>
GStreamer is a framework designed to handle multimedia flows. Media travels from
the "source" elements (the producers), down to the "sink" elements (the
        consumers), passing through a series of intermediate elements performing
all kinds of tasks. The set of all the interconnected elements is called a
"pipeline".

<branch>
If a container embeds multiple streams (one video and two audio tracks, for
        example), the demuxer will separate them and expose them through
different output ports. In this way, different branches can be created in the
pipeline, dealing with different types of data.

<downstream>
The basic construction block of GStreamer are the elements, which process the
data as it flows downstream from the source elements (the producers) to the sink
elements (the consumers), passing through filter elements.


<pad>
The 'ports' through which GStreamer elements communicate with each other are
called pads (GstPad).  There exists sink pads, through which data enters an
element, and source pads, through which data exits an element.

A demuxer contains one sink pad, through which the muxed data arrives, and
multiple source pads, one for each stream found in the container:

demux
+===================+
|[sink]     [audio] |
|           [video] |
+===================+


<signal>
GSignals are a crucial point in GStreamer. They allow you to be notified by
means of a callback when something interesting has happened. Signals are
identified by a name, and each GObject has its own signals.

/* Connect to the pad-added signal */
g_signal_connect (data.source, "pad-added", G_CALLBACK (pad_added_handler), &data);

In this line, we are attaching to the "pad-added" signal of our source
uridecodebin element. To do so, we use g_signal_connect() and provide the
callback function to be used (pad_added_handler) and a data pointer. GStreamer
does nothing with this data pointer, it just forwards it to the callback so we
can share information with it. In this case, we pass a pointer to the CustomData
structure we built specially for this purpose.

When our source element finally has enough information to start producing data,
     it will create source pads, and trigger the "pad-added" signal. At this
     point our callback will be called:


{build-pipeline}

<automatic>
Showed how to build a pipeline automatically. 

/* Initialize GStreamer */
gst_init (&argc, &argv);

/* Build the pipeline */
pipeline = gst_parse_launch ("playbin2 uri=http://docs.gstreamer.com/media/sintel_trailer-480p.webm", NULL);

/* Start playing */
gst_element_set_state (pipeline, GST_STATE_PLAYING);

<gst-parse-launch>
In GStreamer you usually build the pipeline by manually assembling the individual elements, but,
   when the pipeline is easy enough, and you do not need any advanced features, you can take the
   shortcut: gst_parse_launch().

This function takes a textual representation of a pipeline and turns it into an actual pipeline,
     which is very handy. In fact, this function is so handy there is a tool built completely around
     it which you will get very acquainted with (see Basic tutorial 10: GStreamer tools to learn
             about gst-launch and the gst-launch syntax).

gst_parse_bin_from_description ()

GstElement *        gst_parse_bin_from_description      (const gchar *bin_description,
                                                         gboolean ghost_unlinked_pads,
                                                         GError **err);

This is a convenience wrapper around gst_parse_launch() to create a GstBin from
a gst-launch-style pipeline description. See gst_parse_launch() and the
gst-launch man page for details about the syntax. Ghost pads on the bin for
unlinked source or sink pads within the bin can automatically be created (but
        only a maximum of one ghost pad for each direction will be created; if
        you expect multiple unlinked source pads or multiple unlinked sink pads
        and want them all ghosted, you will have to create the ghost pads
        yourself).

bin_description :
	command line describing the bin

ghost_unlinked_pads :
	whether to automatically create ghost pads for unlinked source or sink pads within the bin

err :
	where to store the error message in case of an error, or NULL

Returns :
	a newly-created bin, or NULL if an error occurred. [transfer full]

Since 0.10.3

<manually>
Now we are going to build a pipeline manually by instantiating each element and
linking them all together. 

1. create elements

/* Create the elements */
source = gst_element_factory_make ("videotestsrc", "source");
sink = gst_element_factory_make ("autovideosink", "sink");


GstElement *        gst_element_factory_make            (const gchar *factoryname,
                                                         const gchar *name);

Create a new element of the type defined by the given element factory. note: If
name is NULL, then the element will receive a guaranteed unique name, consisting
of the element factory name and a number. If name is given, it will be given the
name supplied.

factoryname : a named factory to instantiate

name : name of new element, or NULL to automatically create a unique name.

Returns : new GstElement or NULL if unable to create element.


2. create pipeline
All elements in GStreamer must typically be contained inside a pipeline before
they can be used, because it takes care of some clocking and messaging
functions.

/* Create the empty pipeline */
pipeline = gst_pipeline_new ("test-pipeline");

/* Build the pipeline */
gst_bin_add_many (GST_BIN (pipeline), source, sink, NULL);
if (gst_element_link (source, sink) != TRUE) {
    g_printerr ("Elements could not be linked.\n");
    gst_object_unref (pipeline);
    return -1;
}

A pipeline is a particular 'type' of bin, which is the element used to contain
other elements.  Therefore all methods which apply to bins also apply to
pipelines. In our case, we call gst_bin_add_many() to add the elements to the
pipeline (mind the cast). This function accepts a list of elements to be added,
         ending with NULL.

These elements, however, are not 'linked' with each other yet. For this, we need
to use gst_element_link().


4. start

/* Start playing */
ret = gst_element_set_state (pipeline, GST_STATE_PLAYING);
if (ret == GST_STATE_CHANGE_FAILURE) {
    g_printerr ("Unable to set the pipeline to the playing state.\n");
    gst_object_unref (pipeline);
    return -1;
}


<error-handling>
5. check errors

/* Wait until error or EOS */
bus = gst_element_get_bus (pipeline);
msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE, GST_MESSAGE_ERROR | GST_MESSAGE_EOS);
   
/* Parse message */
if (msg != NULL) {
  GError *err;
  gchar *debug_info;
   
  switch (GST_MESSAGE_TYPE (msg)) {
    case GST_MESSAGE_ERROR:
      gst_message_parse_error (msg, &err, &debug_info);
      g_printerr ("Error received from element %s: %s\n", GST_OBJECT_NAME (msg->src), err->message);
      g_printerr ("Debugging information: %s\n", debug_info ? debug_info : "none");
      g_clear_error (&err);
      g_free (debug_info);
      break;
    case GST_MESSAGE_EOS:
      g_print ("End-Of-Stream reached.\n");
      break;
    default:
      /* We should not reach here because we only asked for ERRORs and EOS */
      g_printerr ("Unexpected message received.\n");
      break;
  }
  gst_message_unref (msg);
}

Can have while on message:

/* Listen to the bus */
bus = gst_element_get_bus (data.pipeline);
do {
    msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE,
            GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR | GST_MESSAGE_EOS);
    ...
} while (!terminate);


{bus-and-message}
GStreamer bus is the object responsible for delivering to the application the GstMessages generated
by the elements, in order and to the application thread. This last point is important, because the
actual streaming of media is done in another thread than the application.

Messages can be extracted from the bus synchronously with gst_bus_timed_pop_filtered() and its
siblings, or asynchronously, using signals (shown in the next tutorial). Your application should
always keep an eye on the bus to be notified of errors and other playback-related issues.


{bin}
GstBin — Base class and element that can contain other elements

Object Hierarchy

  GObject
   +----GstObject
         +----GstElement
               +----GstBin *
                     +----GstPipeline

Description

GstBin is an element that can contain other GstElement, allowing them to be managed as a group. Pads
from the child elements can be ghosted to the bin, see GstGhostPad. This makes the bin look like any
other elements and enables creation of higher-level abstraction elements.

A new GstBin is created with gst_bin_new(). Use a GstPipeline instead if you want to create a
toplevel bin because a normal bin doesn't have a bus or handle clock distribution of its own.

After the bin has been created you will typically add elements to it with gst_bin_add(). You can
remove elements with gst_bin_remove().

An element can be retrieved from a bin with gst_bin_get_by_name(), using the elements name.
gst_bin_get_by_name_recurse_up() is mainly used for internal purposes and will query the parent bins
when the element is not found in the current bin.

An iterator of elements in a bin can be retrieved with gst_bin_iterate_elements(). Various other
iterators exist to retrieve the elements in a bin.

gst_object_unref() is used to drop your reference to the bin.

The "element-added" signal is fired whenever a new element is added to the bin. Likewise the
"element-removed" signal is fired whenever an element is removed from the bin. 


={============================================================================
*kt_dev_bcast_302* gst: timer and query

Here we modify this function to periodically wake up and query the pipeline
for the stream position, so we can print it on screen. This is similar to what
    a media player would do, updating the User Interface on a periodic basis.


{gstquery}
GstQuery is a mechanism that allows asking an element or pad for a piece of
information.

msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE,
        GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR | GST_MESSAGE_EOS);

Previously we did not provide a timeout to gst_bus_timed_pop_filtered(), meaning
that it didn't return until a message was received. 

Now we use a timeout of 100 milliseconds, so, if no message is received, 10
times per second the function will return with a NULL instead of a GstMessage.
We are going to use this to update our “UI”. Note that the timeout period is
specified in nanoseconds, so usage of the GST_SECOND or GST_MSECOND macros is
highly recommended.

msg = gst_bus_timed_pop_filtered (bus, 100 * GST_MSECOND,
        GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR 
        | GST_MESSAGE_EOS | GST_MESSAGE_DURATION);


This leads to code that can do something on every timeout.

/* Parse message */
if (msg != NULL) {
    handle_message (&data, msg);
} else {
    /* We got no message, this means the timeout expired */
    if (data.playing) {

         /* We get here approximately 10 times per second, a good enough
          * refresh rate for our UI. We are going to print on screen the
          * current media position, which we can learn be querying the
          * pipeline.
          */
    } 
}

gst_element_query_position() is helper function and hides the management of the
query object and directly provides us with the result.


={============================================================================
*kt_dev_bcast_303* gst: registry

gst-init can use a gst-registry cache that can store information about existing
plugins. This greatly improves performance, as the discovery of plugins can be
only done when gst-init() is ran for the first time.  We configure this cache
using runBrowser.sh script, and if it does not exist, with the use of
gst-inspect tool we force it to be populted.

Unfortunately, gstreamer performs a limited validation of this cache, checking
only for it's version. This leads to potential problems, when plugins are
changed / renamed. Such a problem was discovered by CANTST-16517 Error 02100
when playing iPlayer programmes.  Between two affected releases name of one of
the gst-plugins has changed (DEVARCH-9248), namely: libgstfragmented became
ibgsthlsdemux.

It appears that gst plugin (gstregistry.bin) was not automatically rebuilt by
the gstreamer following the software upgrade, and non-existing plugins were
refered to in attempt to build the playback pipeline.

Ideally, the gstreamer could invalidate such a cache in such case, and it would
have been re-built automatically to match the changes performed during the
upgrade. As a quick solution, as part of DEVARCH-9643, runBrowser.sh script was
updated to reflect that there were changes by using a new name for the cachefile
(gstregistry.bin) appending a suffix with a version and deleting all of the
previous versions if a cache-file of the latest version did not exist. This,
however, would require updates to release validation process to make sure the
    version was increased whenever there was a need for it.

This change should make it less error-prone and fully automated.

# enable gstreamer plugin registy
GST_INSPECT_CMD="${GST_INSPECT_CMD:-$prefix/oss/bin/gst-inspect-1.0}"
gst_registry_file="$app_data_dir/gstregistry.bin"
if [ ! -s "${gst_registry_file}" ]; then
    GST_REGISTRY="$gst_registry_file" $GST_INSPECT_CMD &> /dev/null
fi


http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/gst-running.html

GST_REGISTRY, GST_REGISTRY_1_0.  

Set this environment variable to make GStreamer use a different file for the
plugin cache / registry than the default one. This is useful when operating in a
separate environment which should not affect the default cache in the user's
home directory. 


={============================================================================
*kt_dev_bcast_304* gst: queue, branch

Basic tutorial 7: Multithreading and Pad Availability

GStreamer is a multithreaded framework. This means that, internally, it creates
and destroys threads as it needs them, for example, to decouple streaming from
the application thread. 

Moreover, 'plugins' are also free to create threads for their own processing,
for example, a video decoder could create 4 threads to take full advantage of a
    CPU with 4 cores.


{queue-and-branch}
An application can specify explicitly that a 'branch' (a part of the pipeline)
runs on a different thread (for example, to have the audio and video decoders
        executing simultaneously).

This is accomplished using the queue element, which works as follows. The sink
pad just enqueues data and returns control. On a different thread, data is
dequeued and pushed downstream. This element is also used for 'buffering', as
seen later in the streaming tutorials. The size of the queue can be controlled
through properties.

app source(thread1)    tee                     queue(thread2)       audio 
+=================+    +=================+     +=================+  resample
|           [src] | -> |[sink]     [src] | ->  |[sink]     [src] |  [sink] ->
|                 |    |           [src] |     |                 |
+=================+    +=================+     +=================+

                                               queue(thread3)       video 
                                               +=================+  convert
                                               |[sink]     [src] |  [sink] ->
                                               |                 |
                                               +=================+ 

As seen in the picture, queues create a new thread, so this pipeline runs in 3
threads. Pipelines with more than one sink usually need to be multithreaded,
    because, to be 'synchronized', sinks usually block execution until all
    other sinks are ready, and they cannot get ready if there is only one
    thread, being blocked by the first sink.


{pad-link}
note: Linking elements is one thing and linking pads is the other once linking
element is done.

/* Create the elements */
audio_source = gst_element_factory_make ("audiotestsrc", "audio_source");
tee = gst_element_factory_make ("tee", "tee");
audio_queue = gst_element_factory_make ("queue", "audio_queue");
video_queue = gst_element_factory_make ("queue", "video_queue");

/* Manually link the Tee, which has "request" pads */

<audio-branch>
// request 'request pad' from tee for audio path
tee_src_pad_template = 
   gst_element_class_get_pad_template (GST_ELEMENT_GET_CLASS (tee), "src%d");

tee_audio_pad = 
   gst_element_request_pad (tee, tee_src_pad_template, NULL, NULL);

g_print ("Obtained request pad %s for audio branch.\n", 
        gst_pad_get_name (tee_audio_pad));

// GstPad* gst_element_get_static_pad (GstElement *element, const gchar *name);
//
// Retrieves a pad from element by name. This version only retrieves
// already-existing (i.e.  'static') pads. 

// get a pad from audio_queue element
queue_audio_pad = gst_element_get_static_pad (audio_queue, "sink");

<video-branch>
// request 'request pad' from tee for video path
tee_video_pad = 
   gst_element_request_pad (tee, tee_src_pad_template, NULL, NULL);
g_print ("Obtained request pad %s for video branch.\n", 
        gst_pad_get_name (tee_video_pad));

// get a pad from video_queue element
queue_video_pad = gst_element_get_static_pad (video_queue, "sink");

if (gst_pad_link (tee_audio_pad, queue_audio_pad) != GST_PAD_LINK_OK ||
  gst_pad_link (tee_video_pad, queue_video_pad) != GST_PAD_LINK_OK) {
  g_printerr ("Tee could not be linked.\n");
  gst_object_unref (pipeline);
  return -1;
}

// GstPadLinkReturn
// gst_pad_link_full (GstPad *srcpad,
//                    GstPad *sinkpad,
//                    GstPadLinkCheck flags);
//
// Links the source pad and the sink pad.

This variant of gst_pad_link provides a more granular control on the checks
being done when linking. While providing some considerable speedups the caller
of this method must be aware that wrong usage of those flags can cause severe
issues. Refer to the documentation of GstPadLinkCheck for more information.


<element>
http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-plugins/html/gstreamer-plugins-queue.html

// to give a delay
! queue min-threshold-time=2000000000

// to flush data more quickly
! queue max-size-bytes=10000

The “max-size-bytes” property

“max-size-bytes”           guint

Max. amount of data in the queue (bytes, 0=disable).

Flags: Read / Write

note: Default value: 10485760


gst-inspect-1.0 queue

Factory Details:
  Rank                     none (0)
  Long-name                Queue
  Klass                    Generic
  Description              Simple data queue
  Author                   Erik Walthinsen <omega@cse.ogi.edu>

Plugin Details:
  Name                     coreelements
  Description              GStreamer core elements
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstcoreelements.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gstreamer
  Source release date      2015-08-03 14:00 (UTC)
  Binary package           GStreamer git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstQueue

Pad Templates:
  SRC template: 'src'
    Availability: Always
    Capabilities:
      ANY

  SINK template: 'sink'
    Availability: Always
    Capabilities:
      ANY


Element Flags:
  no flags set

Element Implementation:
  Has change_state() function: gst_element_change_state_func

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SINK: 'sink'
    Pad Template: 'sink'
  SRC: 'src'
    Pad Template: 'src'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "queue0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  current-level-buffers: Current number of buffers in the queue
                        flags: readable
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  current-level-bytes : Current amount of data in the queue (bytes)
                        flags: readable
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  current-level-time  : Current amount of data in the queue (in ns)
                        flags: readable
                        Unsigned Integer64. Range: 0 - 18446744073709551615 Default: 0 
  max-size-buffers    : Max. number of buffers in the queue (0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer. Range: 0 - 4294967295 Default: 200 
  max-size-bytes      : Max. amount of data in the queue (bytes, 0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer. Range: 0 - 4294967295 Default: 10485760 
  max-size-time       : Max. amount of data in the queue (in ns, 0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer64. Range: 0 - 18446744073709551615 Default: 1000000000 
  min-threshold-buffers: Min. number of buffers in the queue to allow reading (0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  min-threshold-bytes : Min. amount of data in the queue to allow reading (bytes, 0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  min-threshold-time  : Min. amount of data in the queue to allow reading (in ns, 0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer64. Range: 0 - 18446744073709551615 Default: 0 
  leaky               : Where the queue leaks, if at all
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Enum "GstQueueLeaky" Default: 0, "no"
                           (0): no               - Not Leaky
                           (1): upstream         - Leaky on upstream (new buffers)
                           (2): downstream       - Leaky on downstream (old buffers)
  silent              : Don't emit queue signals
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Boolean. Default: false
  flush-on-eos        : Discard all data in the queue when an EOS event is received
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Boolean. Default: false

Element Signals:
  "underrun" :  void user_function (GstElement* object,
                                    gpointer user_data);
  "running" :  void user_function (GstElement* object,
                                   gpointer user_data);
  "overrun" :  void user_function (GstElement* object,
                                   gpointer user_data);
  "pushing" :  void user_function (GstElement* object,
                                   gpointer user_data);


={============================================================================
*kt_dev_bcast_305* gst: buffers and interact with a pipeline

Basic tutorial 8: Short-cutting the pipeline

Data 'travels' through a GStreamer pipeline in chunks called buffers. Since this example produces
and consumes data, we need to know about GstBuffers.

Source Pads produce buffers, that are consumed by Sink Pads; GStreamer takes these buffers and
passes them from element to element.

A buffer simply represents a piece of data, do not assume that all buffers will have the same size,
or represent the same amount of time. Neither should you assume that if a single buffer enters an
    element, a single buffer will come out. Elements are free to do with the received buffers as
    they please.

Every buffer has an attached 'GstCaps' structure that describes the kind of media contained in the
buffer. Also, buffers have an attached 'time-stamp' and 'duration', that describe in which moment
the content of the buffer should be rendered or displayed. Time stamping is a very complex and
delicate subject, but this simplified vision should suffice for now.

As an example, a filesrc (a GStreamer element that reads files) produces buffers with the "ANY" caps
and no time-stamping information. After demuxing (see Basic tutorial 3: Dynamic pipelines) buffers
can have some specific caps, for example “video/x-h264”. After decoding, each buffer will contain a
single video frame with raw caps (for example, “video/x-raw-yuv”) and very precise time stamps
indicating when should that frame be displayed.


#define CHUNK_SIZE 1024   /* Amount of bytes we are sending in each buffer */
#define SAMPLE_RATE 44100 /* Samples per second we are sending */
#define AUDIO_CAPS "audio/x-raw-int,channels=1,rate=%d,signed=(boolean)true,width=16,depth=16,endianness=BYTE_ORDER"


/* Configure appsrc */
audio_caps_text = g_strdup_printf (AUDIO_CAPS, SAMPLE_RATE);
audio_caps = gst_caps_from_string (audio_caps_text);
g_object_set (data.app_source, "caps", audio_caps, NULL);
g_signal_connect (data.app_source, "need-data", G_CALLBACK (start_feed), &data);
g_signal_connect (data.app_source, "enough-data", G_CALLBACK (stop_feed), &data);

The first property that needs to be set on the appsrc is caps. It specifies the kind of data that
    the element is going to produce, so GStreamer can check if linking with downstream elements is
    possible 

We then 'connect' to the need-data and enough-data signals. These are fired 'by' appsrc when its
internal queue of data is running low or almost full, respectively. We will use these signals to
start and stop (respectively) our signal generation process.


/* Configure appsink */
g_object_set (data.app_sink, "emit-signals", TRUE, "caps", audio_caps, NULL);
g_signal_connect (data.app_sink, "new-buffer", G_CALLBACK (new_buffer), &data);
gst_caps_unref (audio_caps);
g_free (audio_caps_text);

Regarding the appsink configuration, we connect to the new-buffer signal, which is emitted every
    time the sink 'receives' a buffer. Also, the signal emission needs to be enabled through the
    emit-signals property, because, by default, it is disabled.

note: why not to set emit-signals for appsrc given that it is disabled by default?


/* Start playing the pipeline */
gst_element_set_state (data.pipeline, GST_STATE_PLAYING);
   

/* Create a GLib Main Loop and set it to run */
data.main_loop = g_main_loop_new (NULL, FALSE);
g_main_loop_run (data.main_loop);


<pushing>
/* This method is called by the idle GSource in the mainloop, to feed CHUNK_SIZE bytes into appsrc.
 * The ide handler is added to the mainloop when appsrc requests us to start sending data (need-data signal)
 * and is removed when appsrc has enough data (enough-data signal).
 */
static gboolean push_data (CustomData *data) {
  GstBuffer *buffer;
  GstFlowReturn ret;
  int i;
  gint16 *raw;
  gint num_samples = CHUNK_SIZE / 2; /* Because each sample is 16 bits */
  gfloat freq;
   
  /* Create a new empty buffer */                  note: alloc
  buffer = gst_buffer_new_and_alloc (CHUNK_SIZE);
   
  /* Set its timestamp and duration */             note: set and timestamp buffer and duration means size
  GST_BUFFER_TIMESTAMP (buffer) = gst_util_uint64_scale (data->num_samples, GST_SECOND, SAMPLE_RATE);
  GST_BUFFER_DURATION (buffer) = gst_util_uint64_scale (CHUNK_SIZE, GST_SECOND, SAMPLE_RATE);
   
  /* Generate some psychodelic waveforms */
  raw = (gint16 *)GST_BUFFER_DATA (buffer);        note: gst_buffer_'data'
  data->c += data->d;
  data->d -= data->c / 1000;
  freq = 1100 + 1000 * data->d;
  for (i = 0; i < num_samples; i++) {
    data->a += data->b;
    data->b -= data->a / freq;
    raw[i] = (gint16)(500 * data->a);
  }
  data->num_samples += num_samples;
   
  /* Push the buffer into the appsrc */ note:
  g_signal_emit_by_name (data->app_source, "push-buffer", buffer, &ret);
   
  /* Free the buffer now that we are done with it */ note:
  gst_buffer_unref (buffer);
   
  if (ret != GST_FLOW_OK) {
    /* We got some error, stop sending data */
    return FALSE;
  }
   
  return TRUE;
}

Once we have the buffer ready, we pass it to appsrc with the push-buffer action signal (see
        information box at the end of Playback tutorial 1: Playbin2 usage), and then
gst_buffer_unref() it since we no longer need it.

note: why need to free if a buffer travels down in a pipeline?
	

<action-signal>
This rather unintuitive way of retrieving the tag list is called an Action Signal. Action signals
are emitted by the application to a specific element, which then performs an action and returns a
result. They behave like a "dynamic function call", in which methods of a class are identified by
their name (the signal's name) instead of their memory address. These signals are listed In the
documentation along with the regular signals, and are tagged “Action”. See playbin2, for example.

// appsrc signals
Signals
  "end-of-stream"                                  : Action
  "enough-data"                                    : Run Last
  "need-data"                                      : Run Last
  "push-buffer"                                    : Action
  "seek-data"                                      : Run Last


/* This signal callback triggers when appsrc needs data. Here, we add an idle handler
 * to the mainloop to start pushing data into the appsrc */
static void start_feed (GstElement *source, guint size, CustomData *data) {
  if (data->sourceid == 0) {
    g_print ("Start feeding\n");
    data->sourceid = g_idle_add ((GSourceFunc) push_data, data);
  }
}

This function is called when the internal queue of appsrc is about to starve (run out of data). The
only thing we do here is register a GLib idle function with g_idle_add() that feeds data to appsrc
until it is full again. "A GLib idle function" is a method that GLib will call from its main loop
whenever it is 'idle', this is, when it has no higher-priority tasks to perform. It requires a GLib
GMainLoop to be instantiated and running, obviously.

This is only one of the multiple approaches that appsrc allows. In particular, buffers do not need
to be fed into appsrc from the main thread using GLib, and you do not need to use the need-data and
enough-data signals to synchronize with appsrc (although this is allegedly the most convenient).


/* The appsink has received a buffer */
static void new_buffer (GstElement *sink, CustomData *data) {
  GstBuffer *buffer;
   
  /* Retrieve the buffer */
  g_signal_emit_by_name (sink, "pull-buffer", &buffer);
  if (buffer) {
    /* The only thing we do in this example is print a * to indicate a received buffer */
    g_print ("*");
    gst_buffer_unref (buffer);
  }
}

// appsink
Signals
  "eos"                                            : Run Last
  "new-buffer"                                     : Run Last
  "new-preroll"                                    : Run Last
  "pull-buffer"                                    : Action
  "pull-preroll"                                   : Action
  "new-buffer-list"                                : Run Last
  "pull-buffer-list"                               : Action

Finally, this is the function that gets called when the appsink receives a buffer. We use the
pull-buffer action signal to 'retrieve' the buffer and then just print some indicator on the screen.

We can retrieve the data pointer using the GST_BUFFER_DATA macro and the data size using the
GST_BUFFER_SIZE macro in GstBuffer. Remember that this buffer does 'not' have to match the buffer
that we produced in the push_data function, any element in the path could have altered the buffers
in any way.

note: no copying. what gst_buffer_unref() actually do?


={============================================================================
*kt_dev_bcast_306* gst: 


={============================================================================
*kt_dev_bcast_301* gst: base elements

gst-plugins-base Elements
http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-good-plugins/html/

http://docs.gstreamer.com/display/GstSDK/Basic+tutorial+14%3A+Handy+elements

http://gstreamer.freedesktop.org/documentation/

{uridecodebin}
will internally instantiate all the necessary elements (sources, demuxers and decoders) to turn a
URI into raw audio and/or video streams. It does half the work that playbin2 does. Since it contains
demuxers, its source pads are not initially available and we will need to link to them on the fly.

This element decodes data from a URI into raw media. It selects a source element that can handle the
given URI scheme and connects it to a decodebin2 element. Like a demuxer, so it offers as many
source pads as streams are found in the media.

gst-launch-0.10 uridecodebin uri=http://docs.gstreamer.com/media/sintel_trailer-480p.webm ! ffmpegcolorspace ! autovideosink
gst-launch-0.10 uridecodebin uri=http://docs.gstreamer.com/media/sintel_trailer-480p.webm ! audioconvert ! autoaudiosink

Decodes data from a URI into raw media. It selects a source element that can handle the given "uri" scheme and connects it to a decodebin2. 


{decodebin2}
http://www.freedesktop.org/software/gstreamer-sdk/data/docs/2012.5/gst-plugins-base-plugins-0.10/gst-plugins-base-plugins-decodebin2.html#decodebin2
GstBin that auto-magically constructs a decoding pipeline using available decoders and demuxers via
auto-plugging.

decodebin2 is considered stable now and replaces the old decodebin element. uridecodebin uses
decodebin2 internally and is often more convenient to use, as it creates a suitable source element
as well. 

appsrc, appsink

The element used to inject application data into a GStreamer pipeline is appsrc, and its
counterpart, used to extract GStreamer data back to the application is appsink.


{decodebin}
decodebin — Autoplug and decode to raw media

[root@HUMAX /]# gst-inspect-1.0 decodebin
Factory Details:
  Rank                     none (0)
  Long-name                Decoder Bin
  Klass                    Generic/Bin/Decoder
  Description              Autoplug and decode to raw media
  Author                   Edward Hervey <edward.hervey@collabora.co.uk>, Sebastian DrÃ¶ge <sebastian.droege@collabora.co.uk>

Plugin Details:
  Name                     playback
  Description              various playback elements
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstplayback.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gst-plugins-base
  Source release date      2015-06-02 10:11 (UTC)
  Binary package           GStreamer Base Plug-ins git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstBin
                         +----GstDecodeBin

Implemented Interfaces:
  GstChildProxy

Pad Templates:
  SINK template: 'sink'
    Availability: Always
    Capabilities:
      ANY

  SRC template: 'src_%u'
    Availability: Sometimes
    Capabilities:
      ANY


Element Flags:
  no flags set

Bin Flags:
  no flags set

Element Implementation:
  Has change_state() function: gst_decode_bin_change_state

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SINK: 'sink'
    Pad Template: 'sink'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "decodebin0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  async-handling      : The bin will handle Asynchronous state changes
                        flags: readable, writable
                        Boolean. Default: false
  message-forward     : Forwards all children messages
                        flags: readable, writable
                        Boolean. Default: false
  caps                : The caps on which to stop decoding.
                        flags: readable, writable
                           video/x-raw(ANY)
                           audio/x-raw(ANY)
                           text/x-raw
                           subpicture/x-dvd
                           subpicture/x-dvb
                           subpicture/x-pgs

  subtitle-encoding   : Encoding to assume if input subtitles are not in UTF-8 encoding. If not set, the GST_SUBTITLE_ENCODING environment variable will be checked for an encoding to use. If that is not set either, ISO-8859-15 will be assumed.
                        flags: readable, writable
                        String. Default: null
  sink-caps           : The caps of the input data. (NULL = use typefind element)
                        flags: readable, writable
                        Caps (NULL)
  use-buffering       : Emit GST_MESSAGE_BUFFERING based on low-/high-percent thresholds
                        flags: readable, writable
                        Boolean. Default: false
  low-percent         : Low threshold for buffering to start
                        flags: readable, writable
                        Integer. Range: 0 - 100 Default: 10 
  high-percent        : High threshold for buffering to finish
                        flags: readable, writable
                        Integer. Range: 0 - 100 Default: 99 
  max-size-bytes      : Max. amount of bytes in the queue (0=automatic)
                        flags: readable, writable
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  max-size-buffers    : Max. number of buffers in the queue (0=automatic)
                        flags: readable, writable
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  max-size-time       : Max. amount of data in the queue (in ns, 0=automatic)
                        flags: readable, writable
                        Unsigned Integer64. Range: 0 - 18446744073709551615 Default: 0 
  post-stream-topology: Post stream-topology messages
                        flags: readable, writable
                        Boolean. Default: false
  expose-all-streams  : Expose all streams, including those of unknown type or that don't match the 'caps' property
                        flags: readable, writable
                        Boolean. Default: true
  connection-speed    : Network connection speed in kbps (0 = unknown)
                        flags: readable, writable
                        Unsigned Integer64. Range: 0 - 18446744073709551 Default: 0 

Element Signals:
  "pad-added" :  void user_function (GstElement* object,
                                     GstPad* arg0,
                                     gpointer user_data);
  "pad-removed" :  void user_function (GstElement* object,
                                       GstPad* arg0,
                                       gpointer user_data);
  "no-more-pads" :  void user_function (GstElement* object,
                                        gpointer user_data);
  "unknown-type" :  void user_function (GstElement* object,
                                        GstPad* arg0,
                                        GstCaps* arg1,
                                        gpointer user_data);
  "autoplug-continue" :  gboolean user_function (GstElement* object,
                                                 GstPad* arg0,
                                                 GstCaps* arg1,
                                                 gpointer user_data);
  "autoplug-factories" :  GValueArray * user_function (GstElement* object,
                                                       GstPad* arg0,
                                                       GstCaps* arg1,
                                                       gpointer user_data);
  "autoplug-sort" :  GValueArray * user_function (GstElement* object,
                                                  GstPad* arg0,
                                                  GstCaps* arg1,
                                                  GValueArray* arg2,
                                                  gpointer user_data);
  "autoplug-select" :  GstAutoplugSelectResult user_function (GstElement* object,
                                                              GstPad* arg0,
                                                              GstCaps* arg1,
                                                              GstElementFactory* arg2,
                                                              gpointer user_data);
  "autoplug-query" :  gboolean user_function (GstElement* object,
                                              GstPad* arg0,
                                              GstElement* arg1,
                                              GstQuery* arg2,
                                              gpointer user_data);
  "drained" :  void user_function (GstElement* object,
                                   gpointer user_data);

Children:
  typefind


={============================================================================
*kt_dev_bcast_302* gst: elements: uridecodebin

[root@HUMAX /]# gst-inspect-1.0 uridecodebin
Factory Details:
  Rank                     none (0)
  Long-name                URI Decoder
  Klass                    Generic/Bin/Decoder
  Description              Autoplug and decode an URI to raw media
  Author                   Wim Taymans <wim.taymans@gmail.com>

Plugin Details:
  Name                     playback
  Description              various playback elements
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstplayback.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gst-plugins-base
  Source release date      2015-06-02 10:11 (UTC)
  Binary package           GStreamer Base Plug-ins git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstBin
                         +----GstURIDecodeBin

Implemented Interfaces:
  GstChildProxy

Pad Templates:
  SRC template: 'src_%u'
    Availability: Sometimes
    Capabilities:
      ANY


Element Flags:
  no flags set

Bin Flags:
  no flags set

Element Implementation:
  Has change_state() function: gst_uri_decode_bin_change_state

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  none

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "uridecodebin0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  async-handling      : The bin will handle Asynchronous state changes
                        flags: readable, writable
                        Boolean. Default: false
  message-forward     : Forwards all children messages
                        flags: readable, writable
                        Boolean. Default: false
  uri                 : URI to decode
                        flags: readable, writable
                        String. Default: null
  source              : Source object used
                        flags: readable
                        Object of type "GstElement"
  connection-speed    : Network connection speed in kbps (0 = unknown)
                        flags: readable, writable
                        Unsigned Integer64. Range: 0 - 18446744073709551 Default: 0 
  caps                : The caps on which to stop decoding. (NULL = default)
                        flags: readable, writable
                           video/x-raw(ANY)
                           audio/x-raw(ANY)
                           text/x-raw
                           subpicture/x-dvd
                           subpicture/x-dvb
                           subpicture/x-pgs

  subtitle-encoding   : Encoding to assume if input subtitles are not in UTF-8 encoding. If not set, the GST_SUBTITLE_ENCODING environment variable will be checked for an encoding to use. If
 that is not set either, ISO-8859-15 will be assumed.
                        flags: readable, writable
                        String. Default: null
  buffer-size         : Buffer size when buffering streams (-1 default value)
                        flags: readable, writable
                        Integer. Range: -1 - 2147483647 Default: -1 
  buffer-duration     : Buffer duration when buffering streams (-1 default value)
                        flags: readable, writable
                        Integer64. Range: -1 - 9223372036854775807 Default: -1 
  download            : Attempt download buffering when buffering network streams
                        flags: readable, writable
                        Boolean. Default: false
  use-buffering       : Perform buffering on demuxed/parsed media
                        flags: readable, writable
                        Boolean. Default: false
  expose-all-streams  : Expose all streams, including those of unknown type or that don't match the 'caps' property
                        flags: readable, writable
                        Boolean. Default: true
  ring-buffer-max-size: Max. amount of data in the ring buffer (bytes, 0 = ring buffer disabled)
                        flags: readable, writable
                        Unsigned Integer64. Range: 0 - 4294967295 Default: 0 

Element Signals:
  "pad-added" :  void user_function (GstElement* object,
                                     GstPad* arg0,
                                     gpointer user_data);
  "pad-removed" :  void user_function (GstElement* object,
                                       GstPad* arg0,
                                       gpointer user_data);
  "no-more-pads" :  void user_function (GstElement* object,
                                        gpointer user_data);
  "unknown-type" :  void user_function (GstElement* object,
                                        GstPad* arg0,
                                        GstCaps* arg1,
                                        gpointer user_data);
  "autoplug-continue" :  gboolean user_function (GstElement* object,
                                                 GstPad* arg0,
                                                 GstCaps* arg1,
                                                 gpointer user_data);
  "autoplug-factories" :  GValueArray * user_function (GstElement* object,
                                                       GstPad* arg0,
                                                       GstCaps* arg1,
                                                       gpointer user_data);
  "autoplug-sort" :  GValueArray * user_function (GstElement* object,
                                                  GstPad* arg0,
                                                  GstCaps* arg1,
                                                  GValueArray* arg2,
                                                  gpointer user_data);
  "autoplug-select" :  GstAutoplugSelectResult user_function (GstElement* object,
                                                              GstPad* arg0,
                                                              GstCaps* arg1,
                                                              GstElementFactory* arg2,
                                                              gpointer user_data);
  "autoplug-query" :  gboolean user_function (GstElement* object,
                                              GstPad* arg0,
                                              GstElement* arg1,
                                              GstQuery* arg2,
                                              gpointer user_data);
  "drained" :  void user_function (GstElement* object,
                                   gpointer user_data);
  "source-setup" :  void user_function (GstElement* object,
                                        GstElement* arg0,
                                        gpointer user_data);


={============================================================================
*kt_dev_bcast_302* gst: elements: h264parse

$ gst-inspect-1.0 h264parse   
Factory Details:
  Rank                     primary + 1 (257)
  Long-name                H.264 parser
  Klass                    Codec/Parser/Converter/Video
  Description              Parses H.264 streams
  Author                   Mark Nauwelaerts <mark.nauwelaerts@collabora.co.uk>

Plugin Details:
  Name                     videoparsersbad
  Description              videoparsers
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstvideoparsersbad.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gst-plugins-bad
  Source release date      2015-08-03 14:19 (UTC)
  Binary package           GStreamer Bad Plug-ins git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstBaseParse
                         +----GstH264Parse

Pad Templates:
  SRC template: 'src'
    Availability: Always
    Capabilities:
      video/x-h264
                 parsed: true
          stream-format: { avc, avc3, byte-stream }
              alignment: { au, nal }

  SINK template: 'sink'
    Availability: Always
    Capabilities:
      video/x-h264


Element Flags:
  no flags set

Element Implementation:
  Has change_state() function: gst_base_parse_change_state

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SINK: 'sink'
    Pad Template: 'sink'
  SRC: 'src'
    Pad Template: 'src'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "h264parse0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  disable-passthrough : Force processing (disables passthrough)
                        flags: readable, writable
                        Boolean. Default: false
  config-interval     : Send SPS and PPS Insertion Interval in seconds 
  (sprop parameter sets will be multiplexed in the data stream when detected.) (0 = disabled)
                        flags: readable, writable
                        Unsigned Integer. Range: 0 - 3600 Default: 0 


={============================================================================
*kt_dev_bcast_302* gst: elements: mpegtsmux

gst-inspect-1.0 mpegtsmux

Factory Details:
  Rank                     primary (256)
  Long-name                MPEG Transport Stream Muxer
  Klass                    Codec/Muxer
  Description              Multiplexes media streams into an MPEG Transport Stream
  Author                   Fluendo <contact@fluendo.com>

Plugin Details:
  Name                     mpegtsmux
  Description              MPEG-TS muxer
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstmpegtsmux.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gst-plugins-bad
  Source release date      2015-08-03 14:19 (UTC)
  Binary package           GStreamer Bad Plug-ins git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----MpegTsMux

Pad Templates:
  SRC template: 'src'
    Availability: Always
    Capabilities:
      video/mpegts
           systemstream: true
             packetsize: { 188, 192 }

  SINK template: 'sink_%d'
    Availability: On request
      Has request_new_pad() function: 0x76e8ca70
    Capabilities:
      video/mpeg
                 parsed: true
            mpegversion: { 1, 2, 4 }
           systemstream: false
      video/x-dirac
      video/x-h264
          stream-format: byte-stream
              alignment: { au, nal }
      audio/mpeg
                 parsed: true
            mpegversion: { 1, 2 }
      audio/mpeg
                 framed: true
            mpegversion: 4
          stream-format: { raw, adts }
      audio/x-lpcm
                  width: { 16, 20, 24 }
                   rate: { 48000, 96000 }
               channels: [ 1, 8 ]
          dynamic_range: [ 0, 255 ]
               emphasis: { false, true }
                   mute: { false, true }
      audio/x-ac3
                 framed: true
      audio/x-dts
                 framed: true
      subpicture/x-dvb
      application/x-teletext


Element Flags:
  no flags set

Element Implementation:
  Has change_state() function: 0x76e8c804

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SRC: 'src'
    Pad Template: 'src'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "mpegtsmux0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  prog-map            : A GstStructure specifies the mapping from elementary streams to programs
                        flags: readable, writable
                        Boxed pointer of type "GstStructure"
  m2ts-mode           : Set to TRUE to output Blu-Ray disc format with 192 byte packets. FALSE for standard TS format with 188 byte packets.
                        flags: readable, writable
                        Boolean. Default: false
  pat-interval        : Set the interval (in ticks of the 90kHz clock) for writing out the PAT table
                        flags: readable, writable
                        Unsigned Integer. Range: 1 - 4294967295 Default: 9000 
  pmt-interval        : Set the interval (in ticks of the 90kHz clock) for writing out the PMT table
                        flags: readable, writable
                        Unsigned Integer. Range: 1 - 4294967295 Default: 9000 
  alignment           : Number of packets per buffer (padded with dummy packets on EOS) (-1 = auto, 0 = all available packets)
                        flags: readable, writable
                        Integer. Range: -1 - 2147483647 Default: -1 
  si-interval         : Set the interval (in ticks of the 90kHz clock) for writing out the ServiceInformation tables
                        flags: readable, writable
                        Unsigned Integer. Range: 1 - 4294967295 Default: 9000 


={============================================================================
*kt_dev_bcast_302* gst: elements: tsparse

$gst-inspect-1.0 tsparse   
Factory Details:
  Rank                     none (0)
  Long-name                MPEG transport stream parser
  Klass                    Codec/Parser
  Description              Parses MPEG2 transport streams
  Author                   Alessandro Decina <alessandro@nnva.org>, 
                           Zaheer Abbas Merali <zaheerabbas at merali dot org>

Plugin Details:
  Name                     mpegtsdemux
  Description              MPEG TS demuxer
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstmpegtsdemux.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gst-plugins-bad
  Source release date      2015-08-03 14:19 (UTC)
  Binary package           GStreamer Bad Plug-ins git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----MpegTSBase
                         +----MpegTSParse2

Pad Templates:
  SINK template: 'sink'
    Availability: Always
    Capabilities:
      video/mpegts
           systemstream: true

  SRC template: 'src'
    Availability: Always
    Capabilities:
      video/mpegts
           systemstream: true

  SRC template: 'program_%u'
    Availability: On request
      Has request_new_pad() function: 0x772c67c4
    Capabilities:
      video/mpegts
           systemstream: true


Element Flags:
  no flags set

Element Implementation:
  Has change_state() function: 0x772c0864

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SINK: 'sink'
    Pad Template: 'sink'
  SRC: 'src'
    Pad Template: 'src'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "mpegtsparse2-0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  parse-private-sections: Parse private sections
                        flags: readable, writable
                        Boolean. Default: false
  set-timestamps      : If set, timestamps will be set on the output buffers using PCRs and smoothed over the smoothing-latency period
                        flags: readable, writable
                        Boolean. Default: false
  smoothing-latency   : Additional latency in microseconds for smoothing jitter in input timestamps on live capture
                        flags: readable, writable
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  pcr-pid             : Set the PID to use for PCR values (-1 for auto)
                        flags: readable, writable
                        Integer. Range: -1 - 2147483647 Default: -1 


={============================================================================
*kt_dev_bcast_310* gst: debug

http://docs.gstreamer.com/display/GstSDK/Basic+tutorial+11%3A+Debugging+tools

Basic tutorial 11: Debugging tools

0	none	No debug information is output.

1	ERROR
Logs all fatal errors. These are errors that do not allow the core or elements to perform the
requested action. The application can still recover if programmed to handle the conditions that
triggered the error.

2	WARNING
Logs all warnings. Typically these are non-fatal, but user-visible problems are expected to happen.

3	INFO
Logs all informational messages. These are typically used for events in the system that only happen
once, or are important and rare enough to be logged at this level.

4	DEBUG
Logs all debug messages. These are general debug messages for events that happen only a limited
number of times during an object's lifetime; these include setup, teardown, change of parameters,
...

5	LOG
Logs all log messages. These are messages for events that happen repeatedly during an object's
lifetime; these include streaming and steady-state conditions.

To enable debug output, set the GST_DEBUG environment variable to the desired debug level. All
levels below that will also be shown (i.e., if you set GST_DEBUG=2, you will get both ERROR and
        WARNING messages).

Furthermore, each plugin or part of the GStreamer defines its own category, so you can specify a
debug level for each individual category. For example, GST_DEBUG=2,audiotestsrc:5, will use Debug
Level 5 for the audiotestsrc element, and 2 for all the others.

The '*' wildcard is also available. For example GST_DEBUG=2,audio*:5 will use Debug Level 5 for all
categories starting with the word audio. GST_DEBUG=*:2 is equivalent to GST_DEBUG=2.

note:
Use gst-launch-0.10 --gst-debug-help to obtain the list of all registered categories. Bear in mind
that each plugin registers its own categories, so, when installing or removing plugins, this list
can change.


[root@HUMAX /]# gst-launch-1.0 --gst-debug-help
<VQEC-3-VQEC_ERROR> : CNAME: no mac addr found using defined input_ifname
<VQEC-3-VQEC_ERROR> : error loading configuration file
Channel summary: 0 out of 0 channels passed the validation

name                  level    description
---------------------+--------+--------------------------------
GST_BUFFER            0   no description
GST_BUFFER_LIST       0   no description
GST_BUS               0   no description
GST_CALL_TRACE        0   no description
GST_CAPS              0   no description
GST_CLOCK             0   no description
GST_CONTEXT           0   no description
GST_DATAFLOW          0   dataflow inside pads
GST_DEBUG             0   debugging subsystem
GST_ELEMENT_FACTORY   0   element factories keep information about installed elements
GST_ELEMENT_PADS      0   no description
GST_ERROR_SYSTEM      0   no description
GST_EVENT             0   no description
GST_INIT              0   no description
GST_LOCKING           0   locking
GST_MEMORY            0   memory
GST_MESSAGE           0   no description
GST_META              0   meta
GST_NEGOTIATION       0   no description
...

<envs>
You can send the gstreamer debug output to a file, rather than to the console by adding the
following two lines to the runBrowser.sh script:

GST_DEBUG_NO_COLOR=1 \
GST_DEBUG_FILE=/app-data/gst-log.txt \
GST_DEBUG=5


<logline>
The content of each line in the debug output is:

0:00:00.868050000  1592   09F62420 WARN   filesrc gstfilesrc.c:1044:gst_file_src_start:<filesrc0> 
                                          error: No such file "non-existing-file.webm"

0:00:00.868050000	Time stamp in HH:MM:SS.sssssssss format since the start of the program

1592	Process ID from which the message was issued. Useful when your problem involves multiple processes

09F62420	Thread ID from which the message was issued. Useful when your problem involves multiple threads

filesrc	Debug Category of the message

<filesrc0>	Name of the object that issued the message. It can be an element, a Pad, or something
else. Useful when you have multiple elements of the same kind and need to distinguish among them.

Naming your elements with the name property will make this debug output more readable (otherwise,
        GStreamer assigns each new element a unique name).


<in-source>
To do so, use the GST_ERROR(), GST_WARNING(), GST_INFO(), GST_LOG() and GST_DEBUG() macros. They
accept the same parameters as printf, and they use the default category (default will be shown as
        the Debug category in the output log).

To change the category to something more meaningful, add these two lines at the top of your code:

GST_DEBUG_CATEGORY_STATIC (my_category);
#define GST_CAT_DEFAULT my_category

And then this one after you have initialized GStreamer with gst_init():

GST_DEBUG_CATEGORY_INIT (my_category, "my category", 0, "This is my very own");

This registers a new category (this is, for the duration of your application: it is not stored in
        any file), and sets it as the default category for your code. See the documentation for
    GST_DEBUG_CATEGORY_INIT().


<graph>
Getting pipeline graphs

For those cases where your pipeline starts to grow too large and you lose track of what is connected
with what, GStreamer has the capability to output graph files. These are .dot files, readable with
free programs like GraphViz, that describe the topology of your pipeline, along with the caps
negotiated in each link.

This is also very handy when using all-in-one elements like playbin2  or uridecodebin, which
instantiate several elements inside them. Use the .dot files to learn what pipeline they have
created inside (and learn a bit of GStreamer along the way).

To obtain .dot files, simply set the GST_DEBUG_DUMP_DOT_DIR environment variable to point to the
folder where you want the files to be placed. gst-launch will create a .dot file at each state
change, so you can see the evolution of the caps negotiation. Unset the variable to disable this
facility. From within your application, you can use the GST_DEBUG_BIN_TO_DOT_FILE() and
GST_DEBUG_BIN_TO_DOT_FILE_WITH_TS() macros to generate .dot files at your convenience.

dot -Tjpeg gst-launch.PLAYING_PAUSED.dot -o gst-launch.PLAYING_PAUSED.jpg


={============================================================================
*kt_dev_bcast_311* gst: gst-launch

Basic tutorial 10: GStreamer tools
http://docs.gstreamer.com/display/GstSDK/Basic+tutorial+10%3A+GStreamer+tools

The command line for gst-launch consists of a list of options followed by a
PIPELINE-DESCRIPTION.  Some simplified instructions are given next, see the
complete documentation at the reference page for gst-launch.

In simple form, a PIPELINE-DESCRIPTION is a list of element types separated by
exclamation marks (!). Go ahead and type in the following command:

gst-launch-0.10 videotestsrc ! ffmpegcolorspace ! autovideosink


$ ./gst-launch-0.10 --help-all
Usage:
  gst-launch-0.10 [OPTION?] PIPELINE-DESCRIPTION

Help Options:
  -h, --help                        Show help options
  --help-all                        Show all help options
  --help-gst                        Show GStreamer Options

GStreamer Options
  --gst-version                     Print the GStreamer version
  --gst-fatal-warnings              Make all warnings fatal
  --gst-debug-help                  Print available debug categories and exit

  --gst-debug-level=LEVEL           Default debug level from 1 (only error) to 
                                    5 (anything) or 0 for no output

  --gst-debug=LIST                  Comma-separated list of category_name:level 
                                    pairs to set specific levels for the 
                                    individual categories. 
                                    Example: GST_AUTOPLUG:5,GST_ELEMENT_*:3

  --gst-debug-no-color              Disable coloured debugging output
  --gst-debug-disable               Disable debugging
  --gst-plugin-spew                 Enable verbose plugin loading diagnostics
  --gst-plugin-path=PATHS           Colon-separated paths containing plugins
  --gst-plugin-load=PLUGINS         Comma-separated list of plugins to preload 
                                    in addition to the list stored in environment 
                                    variable GST_PLUGIN_PATH
  --gst-disable-segtrap             Disable trapping of segmentation faults 
                                    during plugin loading
  --gst-disable-registry-update     Disable updating the registry
  --gst-disable-registry-fork       Disable spawning a helper process while 
                                    scanning the registry

Application Options:
  -t, --tags                        Output tags (also known as metadata)
  -v, --verbose                     Output status information and property notifications
  -q, --quiet                       Do not print any progress information
  -m, --messages                    Output messages
  -X, --exclude=TYPE1,TYPE2,...     Do not output status information of TYPE
  -o, --output=FILE                 Save xml representation of pipeline to FILE and exit
  -f, --no-fault                    Do not install a fault handler
  --no-sigusr-handler               Do not install signal handlers for SIGUSR1 and SIGUSR2
  -T, --trace                       Print alloc trace (if enabled at compile time)
  -e, --eos-on-shutdown             Force EOS on sources before shutting the pipeline down
  -i, --index                       Gather and print index statistics
  --version                         Print version information and exit


{set-properties}
Properties may be appended to elements, in the form property=value (multiple
        properties can be specified, separated by spaces). Use the gst-inspect
tool (explained next) to find out the available properties for an element.

gst-launch-0.10 videotestsrc pattern=11 ! ffmpegcolorspace ! autovideosink


{name-elements-named-pad}
Elements can be named using the name property, in this way complex pipelines
involving branches can be created. Names allow linking to elements created
previously in the description, and are indispensable to use elements with
multiple output pads, like demuxers or tees, for example.

Named elements are referred to using their name followed by a dot.

gst-launch-0.10 \
   videotestsrc ! ffmpegcolorspace ! tee name=t ! queue ! autovideosink t. ! queue ! autovideosink
                                                                                     ^^
   videotestsrc ! ffmpegcolorspace ! tee ! queue ! autovideosink 
                                     tee ! queue ! autovideosink

You should see two video windows, showing the same sample video pattern.

The tee is named simply 't' (using the name property) and then linked to a
queue and an autovideosink. The same tee is referred to using 't.' (mind the
        dot) and then linked to a second queue and a second autovideosink.


{cap-filters}
When an element has more than one output pad, it might happen that the link to
the next element is ambiguous: the next element may have more than one
compatible input pad, or its input pad may be compatible with the Pad Caps of
all the output pads. In these cases GStreamer will link using the first pad
that is available, which pretty much amounts to saying that GStreamer will
choose one output pad at random.

Consider the following pipeline:

gst-launch-0.10 
   souphttpsrc location=http://docs.gstreamer.com/media/sintel_trailer-480p.webm 
   ! matroskademux ! filesink location=test

This is the same media file and demuxer as in the previous example. The input
Pad Caps of filesink are ANY, meaning that it can accept any kind of media.
Which one of the two output pads of matroskademux will be linked against the
filesink? video_00 or audio_00? You cannot know.

You can remove this ambiguity, though, by using named pads, as in the previous
sub-section, or by using Caps Filters:

gst-launch-0.10 
   souphttpsrc location=http://docs.gstreamer.com/media/sintel_trailer-480p.webm 
   ! matroskademux ! video/x-vp8 ! matroskamux ! filesink location=sintel_video.mkv

A Caps Filter behaves like a pass-through element which does nothing and only
accepts media with the given Caps, effectively resolving the ambiguity. In
this example, between matroskademux and matroskamux we added a video/x-vp8
Caps Filter to specify that we are interested in the output pad of
matroskademux which can produce this kind of video.


{example}
gst-launch-1.0 
   souphttpsrc location=http://test-media.youview.co.uk/ondemand/bbb/avc3/1/2drm_manifest.mpd 
   ! dashdemux name=dash dash. 
   ! video/quicktime 
   ! qtdemux 
   ! queue 
   ! cencdec sas-url=https://ms3.youview.co.uk/s/Big+Buck+Bunny+DASH+2#http://test-media.youview.co.uk/ondemand/bbb/avc3/1/2drm_manifest.mpd 
   ! h264parse 
   ! video/x-h264,stream-format=byte-stream 
   ! nexussink dash. 
   ! audio/x-m4a '!' qtdemux '!' cencdec '!' aacparse '!' nexussink

   souphttpsrc 
   ! dashdemux name=dash dash. ! video/quicktime ! qtdemux ! queue ! cencdec ! h264parse ! video/x-h264,stream-format=byte-stream ! nexussink dash. 
                         dash. ! audio/x-m4a ! qtdemux ! cencdec ! aacparse ! nexussink

gst-launch-1.0 
   souphttpsrc location=http://test-media.youview.co.uk/ondemand/bbb/avc3/1/2drm_manifest.mpd 
   ! dashdemux name=dash dash. 
   ! video/quicktime 
   ! qtdemux 
   ! cencdec sas-url=https://ms3.youview.co.uk/s/Big+Buck+Bunny+DASH+2 
   ! h264parse 
   ! mpegtsmux name=m 
   ! tsnexusbin dash. 
   ! audio/x-m4a ! qtdemux ! cencdec ! aacparse ! m.

   souphttpsrc 
   ! dashdemux name=dash dash. ! video/quicktime ! qtdemux ! cencdec ! h264parse ! mpegtsmux name=m ! tsnexusbin dash. 
                         dash. ! audio/x-m4a ! qtdemux ! cencdec ! aacparse      ! mpegtsmux


={============================================================================
*kt_dev_bcast_311* gst: gst-inspect

This tool has three modes of operation:

o Without arguments, it lists 'all' available elements types, this is, the types you can use to
instantiate new elements.

o With a file name as an argument, it treats the file as a GStreamer plugin, tries to open it, and
lists all the elements described inside.

o With a GStreamer 'element' name as an argument, it lists all information regarding that element.

The most relevant sections are:

o Pad Templates (line 25): This lists all the kinds of Pads this element can have, along with their
                           capabilities. This is where you look to find out if an element can link
                           with another one. In this case, it has only one sink pad template,
                           accepting only video/x-vp8 (encoded video data in VP8 format) and only
                           one source pad template, producing video/x-raw-yuv (decoded video data).

o Element Properties (line 70): This lists the properties of the element, along with their type and
                                accepted values.


{gst-discoverer}
This tool is a wrapper around the GstDiscoverer object shown in Basic tutorial 9: Media information
gathering. It accepts a URI from the command line and prints all information regarding the media
that GStreamer can extract. It is useful to find out what container and codecs have been used to
produce the media, and therefore what elements you need to put in a pipeline to play it.

Use gst-discoverer --help to obtain the list of available options, which basically control the
amount of verbosity of the output.


={============================================================================
*kt_dev_bcast_312* gst: sdk and tutorials

http://docs.gstreamer.com/display/GstSDK/Installing+on+Linux


={============================================================================
*kt_dev_bcast_313* gst: core apis reference

http://www.freedesktop.org/software/gstreamer-sdk/data/docs/2012.5/gstreamer-0.10/libgstreamer.html

guint               gst_bus_add_watch                   (GstBus *bus,
                                                         GstBusFunc func,
                                                         gpointer user_data);

Adds a bus watch to the default main context with the default priority (G_PRIORITY_DEFAULT). Since
    0.10.33 it is also possible to use a non-default main context set up using
    g_main_context_push_thread_default() (before one had to create a bus watch source and attach it
            to the desired main context 'manually').

This function is used to receive asynchronous messages in the main loop. There can only be a single
bus watch per bus, you must remove it before you can set a new one.

The watch can be removed using g_source_remove() or by returning FALSE from func.


={============================================================================
*kt_dev_bcast_319* gst: mime and types

GStreamer already supports many basic media types. Following is a table of a few of the basic types
used for buffers in GStreamer. The table contains the name ("media type") and a description of the
type, the properties associated with the type, and the meaning of each property. A full list of
supported types is included in List of Defined Types.

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/pwg/html/section-types-definitions.html

note: no subtitles yet.

from gstreamer-1.5.0/docs/random/mimetypes

MIME types in GStreamer

What is a MIME type ?
=====================

A MIME type is a combination of two (short) strings (words)---the content type
and the content subtype. Content types are broad categories used for describing
almost all types of files: video, audio, text, and application are common
content types. The subtype further breaks the content type down into a more
specific type description, for example 'application/ogg', 'audio/raw',
'video/mpeg', or 'text/plain'.

So the content type and subtype make up a pair that describes the type of
information contained in a file. In multimedia processing, MIME types are used
to describe the type of information carried by a media stream. In GStreamer, we
use MIME types in the same way, to identify the types of information that are
allowed to pass between GStreamer elements. The MIME type is part of a GstCaps
object that describes a media stream. Besides a MIME type, a GstCaps object also
contains a name and some stream properties (GstProps, which hold combinations of
key/value pairs).

An example of a MIME type is 'video/mpeg'. A corresponding GstCaps could be
created using code:

GstCaps *caps = gst_caps_new_simple ("video/mpeg",
				     "width",  G_TYPE_INT, 384,
				     "height", G_TYPE_INT, 288,
				     NULL);

MIME types and their corresponding properties are of major importance in
GStreamer for uniquely identifying media streams. Therefore, we define them
per media type. All GStreamer plugins should keep to this definition.

Official MIME media types are assigned by the IANA. Current assignments are at
http://www.iana.org/assignments/media-types/.

The problems
============

Some streams may have MIME types or GstCaps that do not fully describe the
stream. In most cases, this is not a problem, though. For example, if a stream
contains Ogg/Vorbis data (which is of type 'application/ogg'), we don't need to
know the samplerate of the raw audio stream, since we can't play the encoded
audio anyway. The samplerate is, however, important for raw audio, so a decoder
would need to retrieve the samplerate from the Ogg/Vorbis stream headers (the
headers are part of the bytestream) in order to pass it on in the GstCaps that
belongs to the decoded audio (which becomes a type like 'audio/raw'). However,
other plugins might want to know such properties, even for compressed streams.
One such example is an AVI muxer, which does want to know the samplerate of an
audio stream, even when it is compressed.

Another problem is that many media types can be defined in multiple ways. For
example, MJPEG video can be defined as 'video/jpeg', 'video/mjpeg',
'image/jpeg', 'video/x-msvideo' with a compression of (fourcc) MJPG, etc.
None of these is really official, since there isn't an official mimetype
for encoded MJPEG video.

The main focus of this document is to 'propose' a standardized set of MIME types
and properties that will be used by the GStreamer plugins.

Different types of streams
==========================

There are several types of media streams. The most important distinction will be
container formats, audio codecs and video codecs. Container formats are
bytestreams that contain one or more substreams inside it, and don't provide any
direct media data itself. Examples are Quicktime, AVI or MPEG System Stream.
They mostly contain of a set of headers that define the media streams that are
packed inside the container, along with the media data itself.

Video codecs and audio codecs describe encoded audio or video data. Examples are
MPEG-1 video, DivX video, MPEG-1 layer 3 (MP3) audio or Ogg/Vorbis audio.
Actually, Ogg is a container format too (for Vorbis audio), but these are
usually used in conjunction with each other.

Finally, there are the somewhat obvious (but not commonly encountered as files)
raw data formats.

Container formats
-----------------

1 - AVI (Microsoft RIFF/AVI)
    MIME type: video/x-msvideo
    Properties:
    Parser: avidemux, ffdemux_avi
    Formatter: avimux

2 - Quicktime (Apple)
    MIME type: video/quicktime
    Properties:
    Parser: qtdemux
    Formatter:

3 - MPEG (MPEG LA)
    MIME type: video/mpeg
    Properties: 'systemstream' = TRUE (BOOLEAN)
    Parser: mpegdemux, ffdemux_mpeg (PS), ffdemux_mpegts (TS), dvddemux
    Formatter: mplex

4 - ASF (Microsoft)
    MIME type: video/x-ms-asf
    Properties:
    Parser: asfdemux, ffdemux_asf
    Formatter: asfmux

5 - WAV (Microsoft RIFF/WAV)
    MIME type: audio/x-wav
    Properties:
    Parser: wavparse, ffdemux_wav
    Formatter: wavenc

6 - RealMedia (Real)
    MIME type: application/vnd.rn-realmedia
    Properties: 'systemstream' = TRUE (BOOLEAN)
    Parser: rmdemux, ffdemux_rm
    Formatter:

7 - DV (Digital Video)
    MIME type: video/x-dv
    Properties: 'systemstream' = TRUE (BOOLEAN)
    Parser: gst1394, ffdemux_dv
    Formatter:

8 - Ogg (Xiph)
    MIME type: application/ogg
    Properties:
    Parser: oggdemux
    Formatter: oggmux

9 - Matroska
    MIME type: video/x-mkv
    Properties:
    Parser: matroskademux, ffdemux_matroska
    Formatter: matroskamux

10 - Shockwave (Macromedia)
     MIME type: application/x-shockwave-flash
     Properties:
     Parser: swfdec, ffdemux_swf
     Formatter:

11 - AU audio (Sun)
     MIME type: audio/x-au
     Properties:
     Parser: auparse, ffdemux_au
     Formatter:

12 - Mod audio
     MIME type: audio/x-mod
     Properties:
     Parser: modplug, mikmod
     Formatter:

13 - FLX video
     MIME type: video/x-fli
     Properties:
     Parser: flxdec
     Formatter:

14 - Monkeyaudio
     MIME type: application/x-ape
     Properties:
     Parser:
     Formatter:

15 - AIFF audio
     MIME type: audio/x-aiff
     Properties:
     Parser:
     Formatter:

16 - SID audio
     MIME type: audio/x-sid
     Properties:
     Parser: siddec
     Formatter:

Please note that we try to keep these MIME types as similar as possible to the
MIME types used as standards in Gnome (Gnome-VFS/Nautilus) and KDE
(Konqueror). Both will (in future) stick to a shared-mime-info database that
is hosted on freedesktop.org, and bases itself on IANA.

Also, there is a very thin line between audio codecs and audio containers
(take mp3 vs. sid, etc.). This is just a per-case thing right now and needs to
be documented further.

Video codecs
------------

For convenience, the fourcc codes used in the AVI container format will be
listed along with the MIME type and optional properties.

Optional properties for all video formats are the following:

width = 1 - MAXINT (INT)
height = 1 - MAXINT (INT)
pixel_width = 1 - MAXINT (INT, with pixel_height forms aspect ratio)
pixel_height = 1 - MAXINT (INT, with pixel_width forms aspect ratio)
framerate = 0 - MAXFLOAT (FLOAT)

1 - MPEG-1, -2 and -4 video (ISO/LA MPEG)
    MIME type: video/mpeg
    Properties: systemstream = FALSE (BOOLEAN)
                mpegversion = 1/2/4 (INT)
    Known fourccs: MPEG, MPGI
    Encoder: mpeg1enc, mpeg2enc
    Decoder: mpeg1dec, mpeg2dec, mpeg2subt

2 - DivX 3.x, 4.x and 5.x video (divx.com)
    MIME type: video/x-divx
    Properties:
    Optional properties: divxversion = 3/4/5 (INT)
    Known fourccs: DIV3, DIV4, DIV5, DIVX, DX50, DIVX, divx
    Encoder: divxenc
    Decoder: divxdec, ffdec_mpeg4

3 - Microsoft MPEG 4.1, 4.2 and 4.3
    MIME type: video/x-msmpeg
    Properties:
    Optional properties: msmpegversion = 41/42/43 (INT)
    Known fourccs: MPG4, MP42, MP43
    Encoder: ffenc_msmpeg4, ffenc_msmpeg4v1, ffenc_msmpeg4v2
    Decoder: ffdec_msmpeg4, ffdec_msmpeg4v1, ffdec_msmpeg4v2

4 - Motion-JPEG (official and extended)
    MIME type: video/x-jpeg
    Properties:
    Known fourccs: MJPG (YUY2 MJPEG), JPEG (any), PIXL (Pinnacle/Miro), VIXL
    Encoder: jpegenc
    Decoder: jpegdec, ffdec_mjpeg

5 - Sorensen (Quicktime - SVQ1/SVQ3)
    MIME types: video/x-svq
    Properties: svqversion = 1/3 (INT)
    Encoder:
    Decoder: ffdec_svq1, ffdec_svq3

6 - H263 and related codecs
    MIME type: video/x-h263
    Properties:
    Known fourccs: H263/h263, i263, L263, M263/m263, s263, x263, VDOW, VIVO
    Encoder: ffenc_h263, ffenc_h263p
    Decoder: ffdec_h263, ffdec_h263i

7 - RealVideo (Real)
    MIME type: video/x-pn-realvideo
    Properties: rmversion = "1"/"2"/"3"/"4" (INT)
    Known fourccs: RV10, RV20, RV30, RV40
    Encoder: ffenc_rv10
    Decoder: ffdec_rv10, ffdec_rv20

8 - Digital Video (DV)
    MIME type: video/x-dv
    Properties: systemstream = FALSE (BOOLEAN)
    Known fourccs: DVSD/dvsd (SDTV), dvhd (HDTV), dvsl (SDTV LongPlay)
    Encoder: ffenc_dvvideo
    Decoder: dvdec, ffdec_dvvideo

9 - Windows Media Video 1, 2 and 3 (WMV)
    MIME type: video/x-wmv
    Properties: wmvversion = 1/2/3 (INT)
    Encoder: ffenc_wmv1, ffenc_wmv2, none
    Decoder: ffdec_wmv1, ffdec_wmv2, none

10 - XviD (xvid.org)
     MIME type: video/x-xvid
     Properties:
     Known fourccs: xvid, XVID
     Encoder: xvidenc
     Decoder: xviddec, ffdec_mpeg4

11 - 3IVX (3ivx.org)
     MIME type: video/x-3ivx
     Properties:
     Known fourccs: 3IV0, 3IV1, 3IV2
     Encoder:
     Decoder:

12 - Ogg/Tarkin (Xiph)
     MIME type: video/x-tarkin
     Properties:
     Encoder:
     Decoder:

13 - VP3
     MIME type: video/x-vp3
     Properties:
     Encoder:
     Decoder: ffdec_vp3

14 - Ogg/Theora (Xiph, VP3-like)
     MIME type: video/x-theora
     Properties:
     Encoder: theoraenc
     Decoder: theoradec, ffdec_theora
     This is the raw stream that comes out of an ogg file.

15 - Huffyuv
     MIME type: video/x-huffyuv
     Properties:
     Known fourccs: HFYU
     Encoder:
     Decoder: ffdec_hfyu

16 - FF Video 1 (FFMPEG)
     MIME type: video/x-ffv
     Properties: ffvversion = 1 (INT)
     Encoder:
     Decoder: ffdec_ffv1

17 - H264
     MIME type: video/x-h264
     Properties:
     Known fourccs: VSSH
     Encoder:
     Decoder: ffdec_h264

18 - Indeo 3 (Intel)
     MIME type: video/x-indeo
     Properties: indeoversion = 3 (INT)
     Encoder:
     Decoder: ffdec_indeo3

19 - Portable Network Graphics (PNG)
     MIME type: video/x-png
     Properties:
     Encoder: pngenc
     Decoder: pngdec, gdkpixbufdec

20 - Cinepak
     MIME type: video/x-cinepak
     Properties:
     Encoder:
     Decoder: ffdec_cinepak

TODO: subsampling information for YUV?

TODO: colorspace identifications for MJPEG? How?

TODO: how to distinguish MJPEG-A/B (Quicktime) and lossless JPEG?

TODO: divx4/divx5/xvid/3ivx/mpeg-4 - how to make them overlap? (all
      ISO MPEG-4 compatible)

3c) Audio Codecs
----------------
For convenience, the two-byte hexcodes (as used for identification in AVI files)
are also given.

Properties for all audio formats include the following:

rate = 1 - MAXINT (INT, sampling rate)
channels = 1 - MAXINT (INT, number of audio channels)

1 - Alaw Raw Audio
    MIME type: audio/x-alaw
    Properties:
    Encoder: alawenc
    Decoder: alawdec

2 - Mulaw Raw Audio
    MIME type: audio/x-mulaw
    Properties:
    Encoder: mulawenc
    Decoder: mulawdec

3 - MPEG-1 layer 1/2/3 audio
    MIME type: audio/mpeg
    Properties: mpegversion = 1 (INT)
                layer = 1/2/3 (INT)
    Encoder: lame, ffdec_mp3
    Decoder: mad

4 - Ogg/Vorbis
    MIME type: audio/x-vorbis
    Encoder: rawvorbisenc (vorbisenc does rawvorbisenc+oggmux)
    Decoder: vorbisdec

5 - Windows Media Audio 1, 2 and 3 (WMA)
    MIME type: audio/x-wma
    Properties: wmaversion = 1/2/3 (INT)
    Encoder:
    Decoder: ffdec_wmav1, ffdec_wmav2, none

6 - AC3
    MIME type: audio/x-ac3
    Properties:
    Encoder: ffenc_ac3
    Decoder: a52dec, ac3parse

7 - FLAC (Free Lossless Audio Codec)
    MIME type: audio/x-flac
    Properties:
    Encoder: flacenc
    Decoder: flacdec, ffdec_flac

8 - MACE 3/6 (Quicktime audio)
    MIME type: audio/x-mace
    Properties: maceversion = 3/6 (INT)
    Encoder:
    Decoder: ffdec_mace3, ffdec_mace6

9 - MPEG-4 AAC
    MIME type: audio/mpeg
    Properties: mpegversion = 4 (INT)
    Encoder: faac
    Decoder: faad

10 - (IMA) ADPCM (Quicktime/WAV/Microsoft/4XM)
     MIME type: audio/x-adpcm
     Properties: layout = "quicktime"/"wav"/"microsoft"/"4xm"/"g721"/"g722"/"g723_3"/"g723_5" (STRING)
     Encoder: ffenc_adpcm_ima_[qt/wav/dk3/dk4/ws/smjpeg], ffenc_adpcm_[ms/4xm/xa/adx/ea]
     Decoder: ffdec_adpcm_ima_[qt/wav/dk3/dk4/ws/smjpeg], ffdec_adpcm_[ms/4xm/xa/adx/ea]

     Note: The difference between each of these four PCM formats is the number
           of samples packed together per channel. For WAV, for example, each
           sample is 4 bit, and 8 samples are packed together per channel in the
           bytestream. For the others, refer to technical documentation. We
           probably want to distinguish these differently, but I don't know how,
           yet.

11 - RealAudio (Real)
     MIME type: audio/x-pn-realaudio
     Properties: raversion ="1"/"2" (INT)
     Known fourccs: 14_4, 28_8
     Encoder:
     Decoder: ffdec_real_144 / ffdec_real_288

12 - DV Audio
     MIME type: audio/x-dv
     Properties:
     Encoder:
     Decoder:

13 - GSM Audio
     MIME type: audio/x-gsm
     Properties:
     Encoder: gsmenc, rtpgsmenc
     Decoder: gsmdec, rtpgsmparse

14 - Speex audio
     MIME type: audio/x-speex
     Properties:
     Encoder: speexenc
     Decoder: speexdec

15 - QDM2
     MIME type: audio/x-qdm2
     Properties:

16 - Sony ATRAC4 (detected inside realmedia and wave/avi streams, nothing to decode it yet)
     MIME type: audio/x-vnd.sony.atrac3
     Properties:
     Encoder:
     Decoder:

17 - Ensoniq PARIS audio
     MIME type: audio/x-paris
     Properties:
     Encoder:
     Decoder:

18 - Amiga IFF / SVX8 / SV16 audio
     MIME type: audio/x-svx
     Properties:
     Encoder:
     Decoder:

19 - Sphere NIST audio
     MIME type: audio/x-nist
     Properties:
     Encoder:
     Decoder:

20 - Sound Blaster VOC audio
     MIME type: audio/x-voc
     Properties:
     Encoder:
     Decoder:

21 - Berkeley/IRCAM/CARL audio
     MIME type: audio/x-ircam
     Properties:
     Encoder:
     Decoder:

22 - Sonic Foundry's 64 bit RIFF/WAV
     MIME type: audio/x-w64
     Properties:
     Encoder:
     Decoder:

TODO: adpcm/dv needs confirmation from someone with knowledge...

Raw formats
-----------

Raw formats contain unencoded, raw media information. These are rather rare from
an end user point of view since raw media files have historically been
prohibitively large ... hence the multitude of encoding formats.

Raw video formats require the following common properties, in addition to
format-specific properties:

width = 1 - MAXINT (INT)
height = 1 - MAXINT (INT)

1 - Raw Video (YUV/YCbCr)
    MIME type: video/x-raw-yuv
    Properties: 'format' = 'XXXX' (fourcc)
    Known fourccs: YUY2, I420, Y41P, YVYU, UYVY, etc.
    Properties:

    Some raw video formats have implicit alignment rules. We should discuss this
    more. Also, some formats have multiple fourccs (e.g. IYUV/I420 or
    YUY2/YUYV). For each of these, we only use one (e.g. I420 and YUY2).

    Currently recognized formats:

    YUY2: packed, Y-U-Y-V order, U/V hor 2x subsampled (YUV-4:2:2, 16 bpp)
    YVYU: packed, Y-V-Y-U order, U/V hor 2x subsampled (YUV-4:2:2, 16 bpp)
    UYVY: packed, U-Y-V-Y order, U/V hor 2x subsampled (YUV-4:2:2, 16 bpp)
    Y41P: packed, UYVYUYVYYYYY order, U/V hor 4x subsampled (YUV-4:1:1, 12 bpp)
    IUY2: packed, U-Y-V order, not subsampled (YUV-1:1:1, 24 bpp)

    Y42B: planar, Y-U-V order, U/V hor 2x subsampled (YUV-4:2:2, 16 bpp)
    YV12: planar, Y-V-U order, U/V hor+ver 2x subsampled (YUV-4:2:0, 12 bpp)
    I420: planar, Y-U-V order, U/V hor+ver 2x subsampled (YUV-4:2:0, 12 bpp)
    Y41B: planar, Y-U-V order, U/V hor 4x subsampled (YUV-4:1:1, 12bpp)
    YUV9: planar, Y-U-V order, U/V hor+ver 4x subsampled (YUV-4:1:0, 9bpp)
    YVU9: planar, Y-V-U order, U/V hor+ver 4x subsampled (YUV-4:1:0, 9bpp)

    Y800: one-plane (Y-only, YUV-4:0:0, 8bpp)

    See http://www.fourcc.org/ for more information.

    Note: YUV-4:4:4 (both planar and packed, in multiple orders) are missing.

2 - Raw video (RGB)
    MIME type: video/x-raw-rgb
    Properties: endianness = 1234/4321 (INT) <- use G_LITTLE_ENDIAN/G_BIG_ENDIAN
                depth = 15/16/24 (INT, color depth)
                bpp = 16/24/32 (INT, bits used to store each pixel)
                red_mask = bitmask (0x..) (INT)
                green_mask = bitmask (0x..) (INT)
                blue_mask = bitmask (0x..) (INT)

    24 and 32 bit RGB should always be specified as big endian, since any little
    endian format can be transformed into big endian by rearranging the color
    masks. 15 and 16 bit formats should generally have the same byte order as
    the CPU.

    Color masks are interpreted by loading 'bpp' number of bits using the given
    'endianness', and masking and shifting by each color mask. Loading a 24-bit
    value cannot be done directly, but one can perform an equivalent operation.

    Examples:
               msb .. lsb
      - memory: RRRRRRRR GGGGGGGG BBBBBBBB RRRRRRRR GGGGGGGG ...
                bpp        = 24
                depth      = 24
                endianness = 4321 (G_BIG_ENDIAN)
                red_mask   = 0xff0000
                green_mask = 0x00ff00
                blue_mask  = 0x0000ff

      - memory: xRRRRRGG GGGBBBBB xRRRRRGG GGGBBBBB xRRRRRGG ...
                bpp        = 16
                depth      = 15
                endianness = 4321 (G_BIG_ENDIAN)
                red_mask   = 0x7c00
                green_mask = 0x03e0
                blue_mask  = 0x003f

      - memory: GGGBBBBB xRRRRRGG GGGBBBBB xRRRRRGG GGGBBBBB ...
                bpp        = 16
                depth      = 15
                endianness = 1234 (G_LITTLE_ENDIAN)
                red_mask   = 0x7c00
                green_mask = 0x03e0
                blue_mask  = 0x003f

The raw audio formats require the following common properties, in addition to
format-specific properties:

rate = 1 - MAXINT (INT, sampling rate)
channels = 1 - MAXINT (INT, number of audio channels)
endianness = 1234/4321 (INT) <- use G_LITTLE_ENDIAN/G_BIG_ENDIAN/G_BYTE_ORDER

3 - Raw audio (integer format)
    MIME type: audio/x-raw-int
    properties: width = 8/16/24/32 (INT, bits used to store each sample)
                depth = 8 - 32 (INT, bits actually used per sample)
                signed = TRUE/FALSE (BOOLEAN)

4 - Raw audio (floating point format)
    MIME type: audio/x-raw-float
    Properties: width = 32/64 (INT)
                buffer-frames: number of audio frames per buffer, 0=undefined

Plugin Guidelines
=================

So, a short bit on what plugins should do. Above, I've stated that audio
properties like 'channels' and 'rate' or video properties like 'width' and
'height' are all optional. This doesn't mean you can just simply omit them and
everything will still work!

An example is the best way to explain all this. AVI needs the width, height,
rate and channels for the AVI header. So if these properties are missing, the
avimux element cannot properly create the AVI header. On the other hand, MPEG
doesn't have such properties in its header, so the mpegdemux element would need
to parse the separate streams in order to find them out. We don't want that
either, because a plugin only does one job. So normally, mpegdemux and avimux
wouldn't allow transcoding. To solve this problem, there are stream parser
elements (such as mpegaudioparse, ac3parse and mpeg1videoparse).

Conclusions to draw from here: a plugin gives info it can provide as seen from
its own task/job. If it can't, other elements might still need it and a stream
parser needs to be written if it doesn't already exist.

On properties that can be described by one of these (properties such as 'width',
'height', 'fps', etc.): they're forbidden and should be handled using filtered
caps.

Status of this document
=======================

Not all plugins strictly follow these guidelines yet, but these are the official
types. Plugins not following these specs either use extensions that should be
documented, or are buggy (and should be fixed).

Blame Ronald Bultje <rbultje@ronald.bitfreak.net> aka BBB for any mistakes in
this document.


={============================================================================
*kt_dev_bcast_320* gst: plugin writer's guide

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/pwg/html/index.html

GStreamer Plugin Writer's Guide (1.5.0.1)

<pluggable>
Its main advantages are that the pluggable components can be mixed and matched into arbitrary
pipelines so that it's possible to write a full-fledged video or audio editing application.

The framework is based on plugins that will provide the various codec and other functionality. The
plugins can be linked and arranged in a pipeline. This pipeline defines the flow of the data.

The GStreamer core function is to provide a framework for plugins, data flow, synchronization and
media type handling/negotiation. It also provides an API to write applications using the various
plugins. 


Preliminary Reading

This guide assumes that you are somewhat familiar with the basic workings of GStreamer. For a gentle
introduction to programming concepts in GStreamer, you may wish to read the GStreamer Application
Development Manual first. Also check out the other documentation available on the GStreamer web
site.

In order to understand this manual, you will need to have a basic understanding of the C language.
Since GStreamer adheres to the GObject programming model, this guide also assumes that you
understand the basics of GObject programming. You may also want to have a look at Eric Harlow's book
Developing Linux Applications with GTK+ and GDK. 


CH2.

<element-and-plugin>
Just writing a new element is not entirely enough, however: You will need to encapsulate your
element in a plugin to enable GStreamer to use it. 

A plugin is essentially a loadable block of code, usually called a shared object file or a
dynamically linked library. A single plugin may contain the implementation of several elements, or
just a single one. For simplicity, this guide concentrates primarily on plugins containing one
element. 

A filter is an important type of element that processes a stream of data. Producers and consumers of
data are called source and sink elements, respectively. Bin elements contain other elements. One
type of bin is responsible for synchronization of the elements that they contain so that data flows
smoothly. 

Another type of bin, called 'autoplugger' elements, automatically add other elements to the bin and
links them together so that they act as a filter between two arbitrary stream types.


={============================================================================
*kt_dev_bcast_321* gst: plugin writer's guide: plugin template

Chapter 3. Constructing the Boilerplate

The first step is to check out a copy of the gst-template git module to get an important tool and
the source code template for a basic GStreamer plugin.

$ git clone git://anongit.freedesktop.org/gstreamer/gst-template.git

The following commands create the MyFilter plugin based on the plugin template and put the output
files in the gst-template/gst-plugin/src directory: 

$ cd gst-template/gst-plugin/src
$ ../tools/make_element myfilter


<element-metadata>
The element details are registered with the plugin during the _class_init () function, which is part
of the GObject system. The _class_init () function should be set for this GObject in the function
where you register the type with GLib.

static void
gst_myfilter_class_init (GstmyfilterClass * klass)
{
  GObjectClass *gobject_class;
  GstElementClass *gstelement_class;

  gobject_class = (GObjectClass *) klass;
  gstelement_class = (GstElementClass *) klass;

  ...

  gst_element_class_set_details_simple(gstelement_class,
    "myfilter",
    "FIXME:Generic",
    "FIXME:Generic Template Element",
    " <<user@hostname.org>>");

}


<constructor-functions>
Each element has two functions which are used for construction of an element. The _class_init()
    function, which is used to initialise the class only once (specifying what signals, arguments
            and virtual functions the class has and setting up global state); 

and the _init() function, which is used to initialise a specific instance of this type. 


<plugin-init>
This is a special function, which is called as soon as the plugin is loaded, and should return TRUE
or FALSE depending on whether it loaded initialized any dependencies correctly. Also, in this
function, any supported element type in the plugin should be registered.

/* entry point to initialize the plug-in
 * initialize the plug-in itself
 * register the element factories and other features
 */
static gboolean
myfilter_init (GstPlugin * myfilter)
{
  /* debug category for fltering log messages
   *
   * exchange the string 'Template myfilter' with your description
   */
  GST_DEBUG_CATEGORY_INIT (gst_myfilter_debug, "myfilter",
      0, "Template myfilter");

  return gst_element_register (myfilter, "myfilter", GST_RANK_NONE,
      GST_TYPE_MYFILTER);
}

/* PACKAGE: this is usually set by autotools depending on some _INIT macro
 * in configure.ac and then written into and defined in config.h, but we can
 * just set it ourselves here in case someone doesn't use autotools to
 * compile this code. GST_PLUGIN_DEFINE needs PACKAGE to be defined.
 */
#ifndef PACKAGE
#define PACKAGE "myfirstmyfilter"
#endif

/* gstreamer looks for this structure to register myfilters
 *
 * exchange the string 'Template myfilter' with your myfilter description
 */
GST_PLUGIN_DEFINE (
    GST_VERSION_MAJOR,
    GST_VERSION_MINOR,
    myfilter,
    "Template myfilter",
    myfilter_init,
    VERSION,
    "LGPL",
    "GStreamer",
    "http://gstreamer.net/"
)


Chapter 4. Specifying the pads

<chain-function>
In the element _init () function, you create the pad from the pad template that has been registered
with the element class in the _class_init () function. After creating the pad, you have to set a
_chain () function pointer that will 'receive' and process the input data on the sinkpad. You can
optionally also set an _event () function pointer and a _query () function pointer.

static void
gst_myfilter_init (Gstmyfilter * filter)
{
  filter->sinkpad = gst_pad_new_from_static_template (&sink_factory, "sink");
  gst_pad_set_event_function (filter->sinkpad,
                              GST_DEBUG_FUNCPTR(gst_myfilter_sink_event));
  gst_pad_set_chain_function (filter->sinkpad,
                              GST_DEBUG_FUNCPTR(gst_myfilter_chain));
  GST_PAD_SET_PROXY_CAPS (filter->sinkpad);
  gst_element_add_pad (GST_ELEMENT (filter), filter->sinkpad);

  filter->srcpad = gst_pad_new_from_static_template (&src_factory, "src");
  GST_PAD_SET_PROXY_CAPS (filter->srcpad);
  gst_element_add_pad (GST_ELEMENT (filter), filter->srcpad);

  filter->silent = FALSE;
}

/* chain function
 * this function does the actual processing
 */
static GstFlowReturn
gst_myfilter_chain (GstPad * pad, GstObject * parent, GstBuffer * buf)
{
  Gstmyfilter *filter;

  filter = GST_MYFILTER (parent);

  if (filter->silent == FALSE)
    g_print ("I'm plugged, therefore I'm in.\n");

  /* just push out the incoming buffer without touching it */
  return gst_pad_push (filter->srcpad, buf);
}

Remember, however, that buffers are not always writeable.

In more advanced elements (the ones that do event processing), you may want to additionally specify
an event handling function, which will be called when stream-events are sent (such as caps,
        end-of-stream, newsegment, tags, etc.). 


Chapter 6. The event function

The event function notifies you of special events that happen in the datastream (such as caps,
        end-of-stream, newsegment, tags, etc.). Events can 'travel' both upstream and downstream, so
you can receive them on sink pads as well as source pads.

Below follows a very simple event function that we install on the sink pad of our element. 

static gboolean
gst_my_filter_sink_event (GstPad *pad, GstObject *parent, GstEvent  *event)
{
  gboolean ret;
  GstMyFilter *filter = GST_MY_FILTER (parent);

  switch (GST_EVENT_TYPE (event)) {
    case GST_EVENT_CAPS:
      /* we should handle the format here */

      /* push the event downstream */
      ret = gst_pad_push_event (filter->srcpad, event);
      break;
    case GST_EVENT_EOS:
      /* end-of-stream, we should close down all stream leftovers here */
      gst_my_filter_stop_processing (filter);

      ret = gst_pad_event_default (pad, parent, event);
      break;
    default:
      /* just call the default handler */
      ret = gst_pad_event_default (pad, parent, event);
      break;
  }
  return ret;
}

It is a good idea to call the default event handler gst_pad_event_default () for unknown events.

'depending' on the event type, the default handler will forward the event or simply unref it. The CAPS
event is by default not forwarded so we need to do this in the event handler ourselves. 


Chapter 7. The query function

Through the query function, your element will receive queries that it has to reply to. These are
queries like position, duration but also about the supported formats and scheduling modes your
element supports. Queries can 'travel' both upstream and downstream, so you can receive them on sink
pads as well as source pads. 

As with event, it is a good idea to call the default query handler gst_pad_query_default () for
unknown queries. Depending on the query type, the default handler will forward the query or simply
unref it. 


Chapter 8. What are states?

Table of Contents

A state describes whether the element instance is initialized, whether it is ready to transfer data
and whether it is currently handling data. There are four states defined in GStreamer:

note: state is per element

which will from now on be referred to simply as "NULL", "READY", "PAUSED" and "PLAYING".

GST_STATE_NULL 
is the default state of an element. In this state, it has not allocated any runtime resources, it
has not loaded any runtime libraries and it can obviously not handle data.

GST_STATE_READY 
is the next state that an element can be in. In the READY state, an element has all default
resources (runtime-libraries, runtime-memory) allocated. However, it has not yet allocated or
defined anything that is stream-specific. 

When going from NULL to READY state (GST_STATE_CHANGE_NULL_TO_READY), an element should allocate any
'non'-stream-specific resources and should load runtime-loadable libraries (if any). When going the
other way around (from READY to NULL, GST_STATE_CHANGE_READY_TO_NULL), an element should unload
these libraries and free all allocated resources. Examples of such resources are hardware devices.
Note that files are generally streams, and these should thus be considered as stream-specific
resources; therefore, they should not be allocated in this state.

GST_STATE_PAUSED 
is the state in which an element is ready to accept and handle data. For most
elements this state is the same as PLAYING. The only exception to this rule are
sink elements. Sink elements only accept one single buffer of data and then
block. At this point the pipeline is 'prerolled' and ready to render data
immediately.

GST_STATE_PLAYING 
is the highest state that an element can be in. For most elements this state is exactly the same as
PAUSED, they accept and process events and buffers with data. Only sink elements need to
differentiate between PAUSED and PLAYING state. In PLAYING state, sink elements actually render
incoming data, e.g. output audio to a sound card or render video pictures to an image sink.


<managing-state>
If at all possible, your element should derive from one of the new base classes (Pre-made base
        classes). There are ready-made general purpose base classes for different types of sources,
   sinks and filter/transformation elements. In addition to those, specialised base classes exist
   for audio and video elements and others.

If you use a base class, you will rarely have to handle state changes yourself. All you have to do
is override the base class's start() and stop() virtual functions (might be called differently
        depending on the base class) and the base class will take care of everything for you.

If, however, you do not derive from a ready-made base class, but from GstElement or some other class
not built on top of a base class, you will most likely have to implement your own state change
function to be notified of state changes. 

note: This is definitively necessary if your plugin is a demuxer or a muxer, as there are no base
classes for muxers or demuxers yet. 


={============================================================================
*kt_dev_bcast_330* gst: element property 

Basic tutorial 2: GStreamer concepts

Most GStreamer elements have 'customizable' properties: named attributes that can be modified to
change the element's behavior (writable properties) or inquired to find out about the element's
internal state (readable properties).

/* Modify the source's properties */
g_object_set (source, "pattern", 0, NULL);

Properties are read from with g_object_get() and written to with g_object_set().

g_object_set() accepts a NULL-terminated 'list' of property-name, property-value pairs, so multiple
properties can be changed in one go. 

note: this is about element but not a pad and seems that's becuase it is GObject.

GStreamer elements are all a particular kind of GObject, which is the entity offering property
facilities: This is why the property handling methods have the g_ prefix.

The line of code above changes the "pattern" property of videotestsrc, which controls the type of
test video the element outputs. Try different values!

basic-tutorial-15.c
53:  g_object_set(timeline, "loop", TRUE, NULL);
74:  g_object_set (sink, "texture", texture, NULL);
77:  g_object_set (pipeline, "video-sink", sink, NULL);

basic-tutorial-2.c
33:  g_object_set (source, "pattern", 0, NULL);

basic-tutorial-3.c
47:  g_object_set (data.source, "uri", "http://docs.gstreamer.com/media/sintel_trailer-480p.webm", NULL);

basic-tutorial-4.c
40:  g_object_set (data.playbin2, "uri", "http://docs.gstreamer.com/media/sintel_trailer-480p.webm", NULL);

basic-tutorial-5.c
348:  g_object_set (data.playbin2, "uri", "http://docs.gstreamer.com/media/sintel_trailer-480p.webm", NULL);

basic-tutorial-7.c
37:  g_object_set (audio_source, "freq", 215.0f, NULL);
38:  g_object_set (visual, "shader", 0, "style", 1, NULL);

basic-tutorial-8.c

  /* Configure wavescope */
  g_object_set (data.visual, "shader", 0, "style", 0, NULL);
   
  /* Configure appsrc */
  audio_caps_text = g_strdup_printf (AUDIO_CAPS, SAMPLE_RATE);
  audio_caps = gst_caps_from_string (audio_caps_text);
  g_object_set (data.app_source, "caps", audio_caps, NULL);
  g_signal_connect (data.app_source, "need-data", G_CALLBACK (start_feed), &data);
  g_signal_connect (data.app_source, "enough-data", G_CALLBACK (stop_feed), &data);
   
  /* Configure appsink */
  g_object_set (data.app_sink, "emit-signals", TRUE, "caps", audio_caps, NULL);
  g_signal_connect (data.app_sink, "new-buffer", G_CALLBACK (new_buffer), &data);
  gst_caps_unref (audio_caps);
  g_free (audio_caps_text);


{example}

[root@HUMAX /]# gst-inspect-1.0 uridecodebin

Pad Templates:
  SRC template: 'src_%u'
    Availability: Sometimes
    Capabilities:
      ANY

Element Properties:
  caps                : The caps on which to stop decoding. (NULL = default)
                        flags: readable, writable
                           video/x-raw(ANY)
                           audio/x-raw(ANY)
                           text/x-raw
                           subpicture/x-dvd
                           subpicture/x-dvb
                           subpicture/x-pgs


={============================================================================
*kt_dev_bcast_330* gst: man: 01: introduction

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/manual/html/index.html

GStreamer is an extremely powerful and versatile framework for creating
streaming media applications.  Many of the virtues of the GStreamer framework
come from its modularity: GStreamer can seamlessly incorporate new plugin
modules. But because modularity and power often come at a cost of greater
complexity, writing new applications is not always easy.


2. Preliminary reading

In order to understand this manual, you need to have a basic understanding of
the C language.  Since GStreamer adheres to the GObject programming model, this
guide also assumes that you understand the basics of GObject
(http://library.gnome.org/devel/gobject/stable/) and glib
(http://library.gnome.org/devel/glib/stable/) programming. Especially, 


* GObject instantiation 
* GObject properties (set/get) 
* GObject casting 
* GObject referecing/dereferencing 
* glib memory management 
* glib signals and callbacks 
* glib main loop


1. What is GStreamer?

Specifically, GStreamer core provides

* an API for multimedia applications
* a plugin architecture
* a pipeline architecture
* a mechanism for media type handling/negotiation
* a mechanism for synchronization
* over 250 plug-ins providing more than 1000 elements
* a set of tools

GStreamer plug-ins could be classified into

* protocols handling
* sources: for audio and video (involves protocol plugins)

GStreamer is packaged into

* gstreamer: the core package
* gst-plugins-base: an essential exemplary set of elements
* gst-plugins-good: a set of good-quality plug-ins under LGPL
* gst-plugins-ugly: a set of good-quality plug-ins that might pose
distribution problems
* gst-plugins-bad: a set of plug-ins that need more quality
* gst-libav: a set of plug-ins that wrap libav for decoding and encoding


See Figure 1-1. Gstreamer overview


2. Design principles

2.1. Clean and powerful

GStreamer provides a clean interface to:

* The application programmer who wants to build a media pipeline. The
programmer can use an extensive set of powerful tools to create media
pipelines without writing a single line of code.  Performing complex media
manipulations becomes very easy.

* The plugin programmer. Plugin programmers are provided a clean and simple
API to create self-contained plugins. 

2.2. Object oriented

GStreamer uses the mechanism of signals and object properties.

All objects can be queried at runtime for their various properties and
capabilities.

2.5. High performance

High performance is obtained by:

* extremely light-weight links between plugins. Data can travel the pipeline
with minimal overhead. Data passing between plugins only involves a pointer
dereference in a typical pipeline.

* providing a mechanism to directly work on the target memory. A plugin can
for example directly write to the X server's shared memory space. Buffers can
    also point to arbitrary memory, such as a sound card's internal hardware
        buffer.

* using a plugin registry with the specifications of the plugins so that the
plugin loading can be delayed until the plugin is actually used.


={============================================================================
*kt_dev_bcast_331* gst: man: 03: foundations

3.2. Pads

Pads are element's input and output, where you can connect other elements.
They are used to negotiate links and data flow between elements in GStreamer.
A pad can be viewed as a "plug" or "port" on an element where links may be
made with other elements, and through which data can flow to or from those
elements. Pads have specific data handling capabilities: a pad can restrict
the type of data that flows through it. Links are only allowed between two
pads when the allowed data types of the two pads are compatible. Data types
are negotiated between pads using a process called caps negotiation. Data
types are described as a GstCaps.

For the most part, all data in GStreamer flows one way through a link between
elements. Data flows out of one element through one or more source pads, and
elements accept incoming data through one or more sink pads. Source and sink
elements have only source and sink pads, respectively. 

Data usually means buffers (described by the GstBuffer object) and events
(described by the GstEvent)


3.3. Bins and pipelines

A pipeline is a top-level 'bin'. It provides a bus for the application and
manages the synchronization for its children. As you set it to PAUSED or
PLAYING state, data flow will start and media processing will take place. Once
started, pipelines will run in a separate 'thread' until you stop them or the
end of the data stream is reached.


3.4. Communication

GStreamer provides several mechanisms for communication and data exchange
between the application and the pipeline.

* buffers are objects for passing streaming data "between elements" in the
pipeline. Buffers always travel from sources to sinks (downstream).

* events are objects sent between elements or "from the application to
elements". Events can travel upstream and downstream. Downstream events can be
synchronised to the data flow.

* messages are objects posted by elements "on the pipeline's message bus",
where they will be held for collection by the application. Messages can be
    intercepted synchronously from the streaming thread context of the element
    posting the message, but are usually handled asynchronously by the
    application from the application's main thread. 

Messages are used to 'transmit' information such as errors, tags, state
changes, buffering state, redirects etc. "from elements to the application" in
a thread-safe way.

* queries allow applications to request information such as duration or
current playback position from the pipeline. Queries are always answered
synchronously. Elements can also use queries to request information from their
peer elements (such as the file size or duration). They can be used both ways
within a pipeline, but upstream queries are more common.

Figure 3-2. GStreamer pipeline with different communication flows

note: events is from app to elements and messages is from elements to app.


={============================================================================
*kt_dev_bcast_332* gst: man: 04: initialize gst

4.1. Simple initialization

Before the GStreamer libraries can be used, gst_init has to be called from the
'main' application.  This call will perform the necessary initialization of
the library as well as parse the GStreamer-specific command line options.


={============================================================================
*kt_dev_bcast_333* gst: man: 05: elements

The most important object in GStreamer for the application programmer is the
GstElement object. An element is the basic building block for a media
pipeline. All the different high-level components you will use are derived
from GstElement. Every decoder, encoder, demuxer, video or audio output is in
fact a GstElement

5.1. What are elements?

For the application programmer, elements are best visualized as black boxes.
On the one end, you might put something in, the element does something with it
and something else comes out at the other side. For a decoder element, for
example, you'd put in encoded data, and the element would output decoded data.


5.1.1. Source elements

Source elements generate data for use by a pipeline, for example reading from
disk or from a sound card. We always draw a source pad to the right of the
element. 

Source elements do not accept data, they only generate data. A source pad can
only generate data.


5.1.2. Filters, convertors, demuxers, muxers and codecs

Filters and filter-like elements have both input and outputs pads. They
operate on data that they receive on their input (sink) pads, and will provide
data on their output (source) pads. Examples of such elements are a volume
element (filter), a video scaler (convertor), an Ogg demuxer or a Vorbis
decoder.

Filter-like elements can have any number of source or sink pads. A video
demuxer, for example, would have one sink pad and several (1-N) source pads,
one for each elementary stream contained in the container format. Decoders, on
    the other hand, will only have one source and sink pads.

+====================+
|[sink]     [source] |
+====================+

+===================+
|[sink]     [audio] |
|           [video] |
+===================+


5.1.3. Sink elements

Sink elements are 'end' points in a media pipeline. They accept data but do not
produce anything. Disk writing, soundcard playback, and video output would all
be implemented by sink elements.

+===================+
|[sink]             |
+===================+


5.2. Creating a GstElement

The simplest way to create an element is to use gst_element_factory_make().
This function takes a factory name and an element name for the newly created
element. The name of the element is something you can use later on to look up
the element in a bin, for example. The name will also be used in debug output.
You can pass NULL as the name argument to get a unique, default name.

note: reference count

When you don't need the element anymore, you need to unref it using
gst_object_unref(). This decreases the reference count for the element by 1.
An element has a refcount of 1 when it gets created. An element gets destroyed
completely when the refcount is decreased to 0.  

The following example 1 shows how to create an element named source from the
element factory named fakesrc. It checks if the creation succeeded. After
checking, it unrefs the element.

/* create element */
element = gst_element_factory_make ("fakesrc", "source");
if (!element) {
    g_print ("Failed to create element of type ’fakesrc’\n");
    return -1;
}

gst_object_unref (GST_OBJECT (element));


gst_element_factory_make is actually a shorthand for a combination of two
functions.


5.3. Using an element as a GObject

A GstElement can have several properties which are implemented using standard
GObject properties. The usual GObject methods to query, set and get property
values and GParamSpecs are therefore supported.

Every GstElement 'inherits' at least one property from its parent GstObject:
the "name" property. This is the name you provide to the functions
gst_element_factory_make ()

int main (int argc, char *argv[])
{
    GstElement *element;
    gchar *name;

    /* init GStreamer */
    gst_init (&argc, &argv);

    /* create element */
    element = gst_element_factory_make ("fakesrc", "source");

    /* get name */
    g_object_get (G_OBJECT (element), "name", &name, NULL);
    g_print ("The name of the element is ’%s’.\n", name);
    g_free (name);

    gst_object_unref (GST_OBJECT (element));

    return 0;
}

note: signal and properties

A GstElement also provides various GObject signals that can be used as a
flexible callback mechanism. Here, too, you can use gst-inspect to see which
signals a specific element supports. Together, signals and properties are the
most basic way in which elements and applications interact.


5.4. More about element factories

In the previous section, we briefly introduced the GstElementFactory object
already as a way to create instances of an element. Element factories are the
basic types retrieved from the GStreamer 'registry', they describe all plugins
and elements that GStreamer can create. This means that element factories are
useful for automated element instancing, such as what autopluggers do, and for
creating lists of available elements.

Tools like gst-inspect will provide some generic information about an element,
      such as the person that wrote the plugin, a descriptive name (and a
              shortname), a rank and a category. 
      
The category can be used to get the type of the element that can be created
using this element factory. Examples of categories include Codec/Decoder/Video
(video decoder), Codec/Encoder/Video (video encoder), Source/Video (a video
        generator), Sink/Video (a video output), and all these exist for audio
as well, of course. 

gst-inspect will give a list of all factories, and gst-inspect <factory-name>
will list all of the above information, and a lot more.


5.4.2. Finding out what pads an element can contain

Perhaps the most powerful feature of element factories is that they contain a
full description of the pads that the element can generate, and the
capabilities of those pads such as what types of media can stream over those
pads, without actually having to load those plugins into memory. This can be
used to provide a codec selection list for encoders, or it can be used for
autoplugging purposes for media players. All current GStreamer-based media
players and autopluggers work this way.


5.5. Linking elements

By linking a source element with zero or more filter-like elements and finally
a sink element, you set up a media pipeline. Data will flow through the
elements. This is the basic concept of media handling in GStreamer.

// source -> filter -> sink

main (int argc, char *argv[])
{
    GstElement *pipeline;
    GstElement *source, *filter, *sink;

    /* init */
    gst_init (&argc, &argv);

    /* create pipeline */
    pipeline = gst_pipeline_new ("my-pipeline");

    /* create elements */
    source = gst_element_factory_make ("fakesrc", "source");
    filter = gst_element_factory_make ("identity", "filter");
    sink = gst_element_factory_make ("fakesink", "sink");

    /* must add elements to pipeline before linking them */
    gst_bin_add_many (GST_BIN (pipeline), source, filter, sink, NULL);

    /* link */
    if (!gst_element_link_many (source, filter, sink, NULL)) {
        g_warning ("Failed to link elements!");
    }

    [..]
}

note: 
you cannot directly link elements that are not in the same bin or pipeline; if
you want to link elements or pads at different hierarchy levels, you will need
to use ghost pads. more about ghost pads later, see Section 8.4.


5.6. Element States

After being created, an element will not actually perform any actions yet. You
need to change elements state to make it do something. GStreamer knows four
element states, each with a very specific meaning.  Those four states are:

* GST_STATE_NULL 
default state. No resources are allocated in this state, so, transitioning to
it will 'free' all resources. The element must be in this state when its
refcount reaches 0 and it is freed.

* GST_STATE_READY 
an element has allocated all of its global resources, that is, resources that
can be kept within streams. You can think about opening devices, allocating
buffers and so on. However, the stream is not opened in this state, so the
stream positions is automatically zero. If a stream was previously opened, it
should be closed in this state, and position, properties and such should be
reset.

* GST_STATE_PAUSED
an element has opened the stream, but is 'not' actively 'processing' it. An
element is allowed to modify a stream's position, read and process data and
such to 'prepare' for playback as soon as state is changed to PLAYING, but it
is not allowed to play the data which would make the clock run. In summary,
   PAUSED is the same as PLAYING but 'without' a running clock.  
    
Elements going into the PAUSED state should prepare themselves for moving over
to the PLAYING state as soon as possible. Video or audio outputs would, for
example, wait for data to arrive and queue it so they can play it right after
the state change. Also, video sinks can already play the first frame (since
        this does not affect the clock yet). Autopluggers could use this same
state transition to already plug together a pipeline. 

Most other elements, such as codecs or filters, do not need to explicitly do
anything in this state, however.

* GST_STATE_PLAYING
an element does exactly the same as in the PAUSED state, except that the clock
now runs.

You can change the state of an element using the function
gst_element_set_state(). If you set an element to another state, GStreamer
will internally traverse all intermediate states. So if you set an element
from NULL to PLAYING, GStreamer will internally set the element to READY and
PAUSED in between.

note: only for playing?

You can only move between adjacent ones, this is, you can't go from NULL to
PLAYING, you have to go through the intermediate READY and PAUSED states. If
you set the pipeline to PLAYING, though, GStreamer will make the intermediate
transitions for you.

note: see when threads start

When moved to GST_STATE_PLAYING, pipelines will process data automatically.
They do not need to be iterated in any form. Internally, GStreamer will start
threads that take this task on to them. GStreamer will also take care of
switching messages from the pipeline’s thread into the application’s own
thread, by using a GstBus. 


When you set a bin or pipeline to a certain target state, it will usually
propagate the state change to all elements within the bin or pipeline
automatically, so it’s usually only necessary to set the state of the
top-level pipeline to start up the pipeline or shut it down. 

note: see when needs manually to set state

However, when adding elements dynamically to an already-running pipeline, e.g.
from within a "pad-added" signal callback, you need to set it to the desired
target state yourself using gst_element_set_state () or
gst_element_sync_state_with_parent ().


={============================================================================
*kt_dev_bcast_334* gst: man: 06: bins

A bin is a container element. You can add elements to a bin. Since a bin is an
element 'itself', a bin can be handled in the same way as any other element.
Therefore, the whole previous chapter (Elements) applies to bins as well.


6.1. What are bins

Bins allow you to 'combine' a group of linked elements into one 'logical'
element. You do not deal with the individual elements anymore but with just
one element, the bin. We will see that this is extremely powerful when you are
going to construct complex pipelines since it allows you to break up the
pipeline in smaller chunks.

The bin will also 'manage' the elements contained in it. It will perform state
changes on the elements as well as collect and forward bus messages.


There is one 'specialized' type of bin available to the GStreamer programmer:

A pipeline: a generic container that manages the synchronization and bus
messages of the contained elements. The toplevel bin has to be a pipeline,
every application thus needs at least one of these.

note: pipeline is a special bin.


6.2. Creating a bin

Bins are created in the same way that other elements are created, i.e. using
an element factory.  There are also convenience functions available
(gst_bin_new () and gst_pipeline_new ()). To add elements to a bin or remove
elements from a bin, you can use gst_bin_add () and gst_bin_remove ().  Note
that the bin that you add an element to will take ownership of that element. 

If you destroy the bin, the element will be dereferenced with it. If you
remove an element from a bin, it will be dereferenced automatically.


6.4. Bins manage states of their children

Bins manage the state of all elements contained in them. If you set a bin (or
        a pipeline, which is a special top-level type of bin) to a certain
target state using gst_element_set_state(), it will make sure all elements
contained within it will also be set to this state. This means it's usually
only necessary to set the state of the top-level pipeline to start up the
pipeline or shut it down.

The bin will perform the state changes on all its children from the sink
element to the source element.  This ensures that the downstream element is
ready to receive data when the upstream element is brought to PAUSED or
PLAYING. Similarly when shutting down, the sink elements will be set to READY
or NULL first, which will cause the upstream elements to receive a FLUSHING
error and stop the streaming threads before the elements are set to the READY
or NULL state.

Note, however, that if elements are added to a bin or pipeline that's already
running, e.g. from within a "pad-added" signal callback, its state will not
automatically be brought in line with the current state or target state of the
bin or pipeline it was added to. Instead, you have to need to set it to the
desired target state yourself using gst_element_set_state () or
gst_element_sync_state_with_parent () when adding elements to an
already-running pipeline.


={============================================================================
*kt_dev_bcast_335* gst: man: 07: bus

A bus is a simple system that takes care of forwarding messages from the
streaming threads to an application in its own thread context. The advantage of
a bus is that an application does not need to be thread-aware in order to use
GStreamer, even though GStreamer itself is heavily threaded.

Every pipeline contains a bus by default, so applications do not need to create
a bus or anything. The only thing applications should do is set a message
handler on a bus, which is similar to a signal handler to an object. When the
mainloop is running, the bus will periodically be checked for new messages, and
the callback will be called when any message is available.

note: every pipeline has a bus.


7.1. How to use a bus

There are two different ways to use a bus:

* Run a GLib/Gtk+ main loop or iterate the default GLib main context yourself
regularly and attach some kind of watch to the bus. 

This way the GLib main loop will check the bus for new messages and notify you
whenever there are messages. Typically you would use gst_bus_add_watch () or
gst_bus_add_signal_watch () in this case.

To use a bus, attach a message handler to the bus of a pipeline using
gst_bus_add_watch (). This handler will be called whenever the pipeline emits a
message to the bus. In this handler, check the signal type (see next section)
and do something accordingly. The return value of the handler should be TRUE to
keep the handler attached to the bus, return FALSE to remove it. 


<example>

#include <gst/gst.h>

static GMainLoop *loop;

static gboolean my_bus_callback (GstBus *bus, GstMessage *message, gpointer data)
{
    g_print ("Got %s message\n", GST_MESSAGE_TYPE_NAME (message));

    switch (GST_MESSAGE_TYPE (message)) 
    {

        case GST_MESSAGE_ERROR: {
                                    GError *err;
                                    gchar *debug;
                                    gst_message_parse_error (message, &err, &debug);
                                    g_print ("Error: %s\n", err->message);
                                    g_error_free (err);
                                    g_free (debug);
                                    g_main_loop_quit (loop);
                                    break;
                                }
        case GST_MESSAGE_EOS:
                                /* end-of-stream */
                                g_main_loop_quit (loop);
                                break;
        default:
                                /* unhandled message */
                                break;
    }

    /* we want to be notified again the next time there is a message
     * on the bus, so returning TRUE (FALSE means we want to stop watching
     * for messages on the bus and our callback should not be called again)
     */
    return TRUE;
}

gint main (gint argc, gchar *argv[])
{
    GstElement *pipeline;
    GstBus *bus;
    guint bus_watch_id;

    /* init */
    gst_init (&argc, &argv);

    /* create pipeline, add handler */
    pipeline = gst_pipeline_new ("my_pipeline");

    /* adds a watch for new message on our pipeline's message bus to
     * the default GLib main context, which is the main context that our
     * GLib main loop is attached to below
     */
    bus = gst_pipeline_get_bus (GST_PIPELINE (pipeline));
    bus_watch_id = gst_bus_add_watch (bus, my_bus_callback, NULL);
    gst_object_unref (bus);

    /*...*/

    /* create a mainloop that runs/iterates the default GLib main context
     * (context NULL), in other words: makes the context check if anything
     * it watches for has happened. When a message has been posted on the
     * bus, the default main context will automatically call our
     * my_bus_callback() function to notify us of that message.
     * The main loop will be run until someone calls g_main_loop_quit()
     */
    loop = g_main_loop_new (NULL, FALSE);
    g_main_loop_run (loop);

    /* clean up */
    gst_element_set_state (pipeline, GST_STATE_NULL);
    gst_object_unref (pipeline);
    g_source_remove (bus_watch_id);
    g_main_loop_unref (loop);

    return 0;
}


* Check for messages on the bus yourself. 

This can be done using gst_bus_peek () and/or gst_bus_poll ().


7.2. Message types

GStreamer has a few pre-defined message types that can be passed over the bus.
The messages are extensible, however. Plug-ins can define additional messages,
and applications can decide to either have specific code for those or ignore
    them. All applications are strongly recommended to at least handle error
    messages by providing visual feedback to the user.

All messages have a message source, type and timestamp. The message source can
be used to see which element emitted the message. For some messages, for
example, only the ones emitted by the top-level pipeline will be interesting to
most applications (e.g. for state-change notifications). Below is a list of all
messages and a short explanation of what they do and how to parse
message-specific content.

note: what's the top-level pipeline?


* Error, warning and information notifications: 
those are used by elements if a message should be shown to the user about the
state of the pipeline. Error messages are fatal and terminate the data-passing.
The error should be repaired to resume pipeline activity. Warnings are not
fatal, but imply a problem nevertheless. Information messages are for
non-problem notifications. All those messages contain a GError with the main
error type and message, and optionally a debug string. Both can be extracted
using gst_message_parse_error () , _parse_warning () and _parse_info (). Both
error and debug strings should be freed after use.


* End-of-stream notification: 
this is emitted when the stream has ended. The state of the pipeline will not
change, but further media handling will stall. Applications can use this to skip
to the next song in their playlist. After end-of-stream, it is also possible to
seek back in the stream. Playback will then continue automatically. This message
has no specific arguments.


* Tags: 
emitted when metadata was found in the stream. This can be emitted multiple
times for a pipeline (e.g. once for descriptive metadata such as artist name or
        song title, and another one for stream-information, such as samplerate
        and bitrate). Applications should cache metadata internally.
gst_message_parse_tag () should be used to parse the taglist, which should be
gst_tag_list_unref () ’ed when no longer needed.


* State-changes: 
emitted after a successful state change. gst_message_parse_state_changed () can
be used to parse the old and new state of this transition.  


* Buffering: 
emitted during caching of network-streams. One can manually extract the progress
(in percent) from the message by extracting the “buffer-percent” property from
the structure returned by gst_message_get_structure (). See also Chapter 15.


* Element messages: 
these are special messages that are unique to certain elements and usually
represent additional features. The element's documentation should mention in
detail which element messages a particular element may send. As an example, the
qtdemux QuickTime demuxer element may send a redirect element message on certain
occasions if the stream contains a redirect instruction.

* Application-specific messages: 
any information on those can be extracted by getting the message structure (see
        above) and reading its fields. Usually these messages can safely be
ignored. Application messages are primarily meant for internal use in
applications in case the application needs to marshal information from some
thread into the main thread. This is particularly useful when the application is
making use of element signals (as those signals will be emitted in the context
        of the streaming thread).


<want-to-message-from-pipeline>
Every element puts messages on the bus regarding its current state, so we
filter them out and only listen to messages coming from the pipeline.

case GST_MESSAGE_STATE_CHANGED:
  /* We are only interested in state-changed messages from the pipeline */
  if (GST_MESSAGE_SRC (msg) == GST_OBJECT (data.pipeline)) {
    GstState old_state, new_state, pending_state;
    gst_message_parse_state_changed (msg, &old_state, &new_state, &pending_state);
    g_print ("Pipeline state changed from %s to %s:\n",
        gst_element_state_get_name (old_state), gst_element_state_get_name (new_state));
  }
  break;


={============================================================================
*kt_dev_bcast_330* gst: man: 08: pad and capability

As we have seen in Elements, the pads are "the element's interface" to the
outside world. Data streams from one element's source pad to another element's
sink pad. The specific type of media that the element can handle will be
exposed by the pad's capabilities. 

8.1. Pads

A pad type is defined by two properties: its 'direction' and its
'availability'. GStreamer defines two pad directions: source pads and sink
pads. This terminology is defined from the view of within the element:
elements receive data on their sink pads and generate data on their source
pads. 

Schematically, sink pads are drawn on the left side of an element, whereas
source pads are drawn on the right side of an element. In such graphs, data
flows from left to right.

<pad-types>
A pad can have any of three availabilities: always, sometimes and on request. 

The meaning of those three types is exactly as it says: always pads always
exist, sometimes pad exist only in certain cases (and can disappear randomly),
and on-request pads appear only if explicitly requested by applications.

8.1.1. Dynamic (or sometimes) pads

Some elements might not have all of their pads when the element is created.
This can happen, for example, with an Ogg demuxer element. The element will
read the Ogg stream and create dynamic pads for 'each' contained elementary
stream (vorbis, theora) when it detects such a stream in the Ogg stream.
Likewise, it will delete the pad when the stream ends. This principle is very
useful for demuxer elements, for example.

You can see this in the pad template because there is an "Exists: Sometimes"
property. Depending on the type of Ogg file you play, the pads will be
created. We will see that this is very important when you are going to create
dynamic pipelines. You can attach a signal handler to an element to inform you
when the element has created a new pad from one of its "sometimes" pad
templates. The following piece of code is an example of how to do this:

<snippet>
Pad Templates:
  SINK template: 'sink'
    Availability: Always
    Capabilities:
      ANY

  SRC template: 'src_%u'
    Availability: Sometimes
    Capabilities:
      ANY

<pad-added>
The demuxers cannot produce any information until they have received some data
and have had a chance to look at the container to see what is inside. So no
source pads to which other elements can link.

The solution is to build the pipeline from the source down to the demuxer, and
set it to run (play). When the demuxer has received enough information to know
about the number and kind of streams in the container, it will start creating
source pads. This is the right time for us to finish building the pipeline and
attach it to the newly added demuxer pads.

/* Connect to the pad-added signal */
g_signal_connect (data.source, "pad-added", G_CALLBACK (pad_added_handler),
        &data);

In this line, are attaching to the "pad-added" signal of our uridecodebin
    element. When our element finally has enough information to start
    producing data, it will create source pads, and trigger the "pad-added"
    signal. At this point our callback will be called

note: source pad will be created from source element. This callback is to
'link' this new source to sink in a pipeline.

/* This function will be called by the pad-added signal */
static void pad_added_handler (GstElement *src, GstPad *new_pad, 
        CustomData *data) 
{
  GstPad *sink_pad = gst_element_get_static_pad (data->convert, "sink");
  GstPadLinkReturn ret;
  GstCaps *new_pad_caps = NULL;
  GstStructure *new_pad_struct = NULL;
  const gchar *new_pad_type = NULL;
   
  g_print ("Received new pad '%s' from '%s':\n", GST_PAD_NAME (new_pad), GST_ELEMENT_NAME (src));
   
  /* If our converter is already linked, we have nothing to do here */
  if (gst_pad_is_linked (sink_pad)) {
    g_print ("  We are already linked. Ignoring.\n");
    goto exit;
  }
   
  /* Check the new pad's type */
  new_pad_caps = gst_pad_get_caps (new_pad);
  new_pad_struct = gst_caps_get_structure (new_pad_caps, 0);
  new_pad_type = gst_structure_get_name (new_pad_struct);
  if (!g_str_has_prefix (new_pad_type, "audio/x-raw")) {
    g_print ("  It has type '%s' which is not raw audio. Ignoring.\n", new_pad_type);
    goto exit;
  }
   
  /* Attempt the link */
  ret = gst_pad_link (new_pad, sink_pad);
  if (GST_PAD_LINK_FAILED (ret)) {
    g_print ("  Type is '%s' but link failed.\n", new_pad_type);
  } else {
    g_print ("  Link succeeded (type '%s').\n", new_pad_type);
  }
   
exit:
  /* Unreference the new pad's caps, if we got them */
  if (new_pad_caps != NULL)
    gst_caps_unref (new_pad_caps);
   
  /* Unreference the sink pad */
  gst_object_unref (sink_pad);
}

note:
It is not uncommon to add elements to the pipeline only from within the
"pad-added" callback. If you do this, don't forget to set the state of the
newly-added elements to the target state of the pipeline using
gst_element_set_state () or gst_element_sync_state_with_parent ().


8.1.2. Request pads

An element can also have request pads. These pads are not created
automatically but are only created on demand. This is very useful for
multiplexers, aggregators and tee elements. Aggregators are elements that
merge the content of several input streams together into one output stream.
Tee elements are the reverse: they are elements that have one input stream and
copy this stream to each of their output pads, which are created on request.
Whenever an application needs another copy of the stream, it can simply
request a new output pad from the tee element.

// skipped the rest


8.2. Capabilities of a pad

Since the pads play a very important role in how the element is viewed by the
outside world, a mechanism is implemented to describe the data that can flow
or currently flows through the pad by using capabilities. Here, we will
briefly describe what capabilities are and how to use them, enough to get an
understanding of the concept. For an in-depth look into capabilities and a
list of all capabilities defined in GStreamer, see the Plugin Writers Guide.

Capabilities are attached to pad templates and to pads. For pad templates, it
will describe the types of media that "may stream over" a pad created from
this template. For pads, it can either be a list of possible caps (usually a
        copy of the pad template's capabilities), in which case the pad is not
yet negotiated, or it is the type of media that currently streams over this
pad, in which case the pad has been negotiated already.


8.2.1. Dissecting capabilities

note: capability is "media type"

A pad's capabilities are described in a GstCaps object. Internally, a GstCaps
(http://gstreamer.freedesktop.org/data/doc/gstreamer/stable/gstreamer/html/gstreamer-GstCaps.html)
will contain one or more GstStructure
(http://gstreamer.freedesktop.org/data/doc/gstreamer/stable/gstreamer/html/gstreamer-GstStructure.html)
that will describe 'one' media type. A negotiated pad will have capabilities
set that contain exactly one structure. Also, this structure will contain only
fixed values. These constraints are not true for unnegotiated pads or pad
templates.

Pad Templates:
   SRC template: ’src’
      Availability: Always
      Capabilities:
         audio/x-raw
            format: F32LE
            rate: [ 1, 2147483647 ]
            channels: [ 1, 256 ]

The source pad will be used to send raw (decoded) audio samples to the next element, with a raw
audio media type (in this case, "audio/x-raw"). The source pad will also contain 'properties' for the
audio samplerate and the amount of channels, plus some more.

// other example

Pad Templates:
  SINK template: 'sink'
    Availability: Always
    Capabilities:
      video/mpegts
           systemstream: true
      video/x-h264
          stream-format: byte-stream
              alignment: { au, nal }
      video/mpeg
            mpegversion: { 2, 4 }
           systemstream: false
      audio/mpeg
            mpegversion: 1
                  layer: { 1, 3 }
      audio/mpeg
            mpegversion: 4
          stream-format: { adts, loas }

<code>
  /* Check the new pad's type */
  new_pad_caps = gst_pad_get_caps (new_pad);
  new_pad_struct = gst_caps_get_structure (new_pad_caps, 0);
  new_pad_type = gst_structure_get_name (new_pad_struct);
  if (!g_str_has_prefix (new_pad_type, "audio/x-raw")) {
    g_print ("  It has type '%s' which is not raw audio. Ignoring.\n", new_pad_type);
    goto exit;
  }

gst_pad_get_caps() retrieves the capabilities of the pad wrapped in a GstCaps structure. A pad can
offer many capabilities, and hence GstCaps can contain many GstStructure, 'each' representing a
different capability.

Since, in this case, we know that the pad we want only had one capability (audio), we retrieve the
first GstStructure with gst_caps_get_structure().

Finally, with gst_structure_get_name() we recover the name of the structure, which contains the main
description of the format (its 'MIME' type, actually).


8.2.2. Properties and values

Properties are used to describe extra information for capabilities. A property consists of a key (a
        string) and a value. There are different possible value types that can be used:

* Basic types, this can be pretty much any GType registered with Glib. Those properties indicate a
specific, non-dynamic value for this property. Examples include:

** An integer value (G_TYPE_INT): the property has this exact value.
** A boolean value (G_TYPE_BOOLEAN): the property is either TRUE or FALSE.
** A float value (G_TYPE_FLOAT): the property has this exact floating point value.
** A string value (G_TYPE_STRING): the property contains a UTF-8 string.
** A fraction value (GST_TYPE_FRACTION): contains a fraction expressed by an integer numerator and
denominator.

// skipped the rest


8.3. What capabilities are used for

Capabilities (short: caps) describe the type of data that is streamed between two pads, or that one
pad (template) supports. This makes them very useful for various purposes:

* Autoplugging: automatically finding elements to link to a pad based on its capabilities. All
autopluggers use this method.

* Compatibility detection: when two pads are linked, GStreamer can verify if the two pads are
talking about the same media type. The process of linking two pads and checking if they are
compatible is called "caps negotiation".

* Metadata: by reading the capabilities from a pad, applications can provide information about the
type of media that is being streamed over the pad, which is information about the stream that is
'currently' being played back.

* Filtering: an application can use capabilities to limit the possible media types that can stream
between two pads to a specific subset of their supported stream types. An application can, for
example, use "filtered caps" to set a specific (fixed or non-fixed) video size that should stream
between two pads.

You will see an example of filtered caps later in this manual, in Section 19.2. You can do caps
filtering by inserting a capsfilter element into your pipeline and setting its "caps" property. Caps
filters are often placed after converter elements like audioconvert, audioresample, videoconvert or
videoscale to force those converters to convert data to a specific output format at a certain point
in a stream.

note: from possible caps, allowed and negotiated cap

Note that there is a distinct difference between the possible capabilities of a pad; ie. usually
what you find as caps of pad templates as they are shown in gst-inspect; the allowed caps of a pad;
can be the same as the pad's template caps or a subset of them, depending on the possible caps of
the 'peer' pad; and lastly negotiated caps. these describe the exact format of a stream or buffer
and contain "exactly one structure" and have no variable bits like ranges or lists, ie. they are
fixed caps.

<type-of-caps>
Caps are called simple caps when they contain only one structure, and fixed caps when they contain
only one structure and have no variable field types (like ranges or lists of possible values). Two
other special types of caps are ANY caps and empty caps.


8.4. Ghost pads

You can see from Figure 8-1 how a bin has no pads of its own. This is where
"ghost pads" come into play.

note: From 06 GstBin, bin is an element itself but do not have pad. Make sence
to have a ghost pad in thinking a data flow but perhaps in using APIs.


bin
+==============================================+
|           element 1            element 2     |
|           [sink   src]   ->    [sink       ] |
|                                              |
+==============================================+

A ghost pad is a pad from some element in the bin that can be accessed
directly from the bin as well. Compare it to a symbolic link in UNIX
filesystems. Using ghost pads on bins, the bin also has a pad and can
transparently be used as an element in other parts of your code.

bin
+==============================================+
|           element 1            element 2     |
|sink]------[sink   src]   ->    [sink       ] |
|                                              |
+==============================================+

Figure 8-2 is a representation of a ghost pad. The sink pad of element one is
now also a pad of the bin. Because ghost pads look and work like any other
pads, they can be added to any type of elements, not just to a GstBin, just
like ordinary pads.

int
main (int argc, char *argv[])
{
    GstElement *bin, *sink;
    GstPad *pad;

    /* init */
    gst_init (&argc, &argv);

    /* create element, add to bin */
    sink = gst_element_factory_make ("fakesink", "sink");
    bin = gst_bin_new ("mybin");
    gst_bin_add (GST_BIN (bin), sink);

    /* add ghostpad */
    pad = gst_element_get_static_pad (sink, "sink");
    gst_element_add_pad (bin, gst_ghost_pad_new ("sink", pad));
    gst_object_unref (GST_OBJECT (pad));
    [..]
}

In the above example, the bin now also has a pad: the pad called "sink" of the given element. The
bin can, from here on, be used as a substitute for the sink element. You could, for example, link
another element to the bin.


={============================================================================
*kt_dev_bcast_330* gst: man: 14: clocks and synchronization

GStreamer uses a GstClock object, buffer timestamps and a SEGMENT event to
synchronize streams in a pipeline

A GstClock returns the absolute-time according to that clock with
gst_clock_get_time (). The absolute-time (or clock time) of a clock is
monotonically increasing. From the absolute-time is a running-time calculated,
which is simply the difference between a previous snapshot of the absolute-time
    called the base-time. So:

running-time = absolute-time - base-time

A GStreamer GstPipeline object maintains a GstClock object and a base-time when
it goes to the PLAYING state. The pipeline gives a handle to the selected
GstClock to each element in the pipeline along with selected base-time. The
pipeline will select a base-time in such a way that the running-time reflects
the total time spent in the PLAYING state. As a result, when the pipeline is
PAUSED, the running-time stands still.

Because all objects in the pipeline have the same clock and base-time, they can
thus all calculate the running-time according to the pipeline clock.


14.2. Buffer running-time

To calculate a buffer running-time, we need a buffer timestamp and the SEGMENT
event that preceeded the buffer. First we can convert the SEGMENT event into a
GstSegment object and then we can use the gst_segment_to_running_time ()
function to perform the calculation of the buffer running-time.

Synchronization is now a matter of making sure that a buffer with a certain
running-time is played when the clock reaches the same running-time. Usually
this task is done by 'sink' elements. Sink also have to take into account the
latency configured in the pipeline and add this to the buffer running-time
before synchronizing to the pipeline clock.

<non-live>
Non-live sources timestamp buffers with a running-time starting from 0. After a
flushing seek, they will produce buffers again from a running-time of 0.

<live>
Live sources need to timestamp buffers with a running-time matching the pipeline
running-time when the first byte of the buffer was captured.


14.3. Buffer stream-time

The buffer stream-time, also known as the position in the stream, is calculated
from the buffer timestamps and the preceding SEGMENT event. It represents the
time inside the media as a value between 0 and the total duration of the media.

The stream-time is used in:
* Report the current position in the stream with the POSITION query.
* The position used in the seek events and queries.
* The position used to synchronize controlled values.

note: The stream-time is never used to synchronize streams, this is only done
with the running-time.


14.4. Time overview

The image below represents the different times in the pipeline when playing a
100ms sample and repeating the part between 50ms and 100ms.

Figure 14-1. GStreamer clock and various times


You can see how the running-time of a buffer always increments monotonically
along with the clock-time. Buffers are played when their running-time is equal
to the (clock-time - base-time). The stream-time represents the position in the
stream and jumps backwards when repeating.


14.5. Clock providers

A clock provider is an element in the pipeline that can provide a GstClock
object.

note: slaving

If an element with an internal clock needs to synchronize, it needs to estimate
when a time according to the pipeline clock will take place according to the
internal clock. To estimate this, it needs to slave its clock to the pipeline
clock.

If the pipeline clock is exactly the internal clock of an element, the element
can skip the slaving step and directly use the pipeline clock to schedule
playback. This can be both faster and more accurate.

Therefore, generally, elements with an internal clock like audio input or output
devices will be a clock provider for the pipeline.

When the pipeline goes to the PLAYING state, it will go over all elements in the
pipeline from sink to source and ask each element if they can provide a clock.
The last element that can provide a clock will be used as the clock provider in
the pipeline. 

note: audio sink?

This algorithm prefers a clock from an audio sink in a typical playback pipeline
and a clock from source elements in a typical capture pipeline.

note: bus message

There exist some bus messages to let you know about the clock and clock
providers in the pipeline. You can see what clock is selected in the pipeline by
looking at the NEW_CLOCK message on the bus.When a clock provider is removed
from the pipeline, a CLOCK_LOST message is posted and the application should go
to PAUSED and back to PLAYING to select a new clock.


14.6. Latency

The latency is the time it takes for a sample captured at timestamp X to reach
the sink. This time is measured against the clock in the pipeline. 

For pipelines where the only elements that synchronize against the clock are the
sinks, the latency is always 0 since no other element is delaying the buffer.

For pipelines with live sources, a latency is introduced, mostly because of the
way a live source works.  Consider an audio source, it will start capturing the
first sample at time 0. If the source pushes buffers with 44100 samples at a
time at 44100Hz it will have collected the buffer at second 1. Since the
timestamp of the buffer is 0 and the time of the clock is now >= 1 second, the
sink will drop this buffer because it is too late. Without any latency
compensation in the sink, all buffers will be dropped.


14.6.1. Latency compensation

note: latency query and maximum

'before' the pipeline goes to the PLAYING state, it will, in addition to
selecting a clock and calculating a base-time, calculate the latency in the
pipeline. It does this by doing a LATENCY query on all the sinks in the
pipeline. The pipeline then selects the maximum latency in the pipeline and
configures this with a LATENCY event.

All sink elements will delay playback by the value in the LATENCY event. Since
all sinks delay with the same amount of time, they will be relative in sync.

// void
// gst_query_set_latency (GstQuery *query,
//                        gboolean live,
//                        GstClockTime min_latency,
//                        GstClockTime max_latency);
// 
// 'answer' a latency query by setting the requested values in the given format.
// 
// live
// if there is a live element upstream
// 	 
// min_latency
// the minimal latency of the upstream elements
// 	 
// max_latency
// the maximal latency of the upstream elements
//
// typedef guint64 GstClockTime;
//
// Q: does this mean to set MAX UINT as type is gunit64?
// gst_query_set_latency (query, FALSE, 0, -1);


14.6.2. Dynamic Latency

Adding/removing elements to/from a pipeline or changing element properties can
change the latency in a pipeline. An element can request a latency change in the
pipeline by posting a LATENCY message on the bus. The application can then
decide to query and redistribute a new latency or not. Changing the latency in a
pipeline might cause visual or audible glitches and should therefore only be
done by the application when it is allowed.


={============================================================================
*kt_dev_bcast_330* gst: man: 15: buffering

Chapter 15. Buffering

The purpose of buffering is to accumulate enough data in a pipeline so that
playback can occur smoothly and without interruptions. It is typically done when
reading from a (slow) and non-live network source but can also be used for live
sources.

GStreamer provides support for the following use cases:

* Buffering up to a specific amount of data, in memory, before starting playback
so that network fluctuations are minimized. See the section called "Stream
buffering".

* Download of the network file to a local disk with fast seeking in the
downloaded data. This is similar to the quicktime/youtube players. See the
section called "Download buffering".

* Caching of (semi)-live streams to a local, on disk, ringbuffer with seeking in
the cached area. This is similar to tivo-like timeshifting. See the section
called "Timeshift buffering". 

GStreamer can provide the application with progress reports about the current
buffering state as well as let the application decide on how to buffer and when
the buffering stops.

In the most simple case, the application has to listen for BUFFERING messages on
the bus. If the percent indicator inside the BUFFERING message is smaller than
100, the pipeline is buffering. When a message is received with 100 percent,
buffering is complete. In the buffering state, the application should keep the
    pipeline in the PAUSED state. When buffering completes, it can put the
    pipeline (back) in the PLAYING state.

What follows is an example of how the message handler could deal with the
BUFFERING messages. We will see more advanced methods in the section called
"Buffering strategies". 

 [...]

  switch (GST_MESSAGE_TYPE (message)) {
    case GST_MESSAGE_BUFFERING:{
      gint percent;

      /* no state management needed for live pipelines */
      if (is_live)
        break;

      gst_message_parse_buffering (message, &percent);

      if (percent == 100) {
        /* a 100% message means buffering is done */
        buffering = FALSE;
        /* if the desired state is playing, go back */
        if (target_state == GST_STATE_PLAYING) {
          gst_element_set_state (pipeline, GST_STATE_PLAYING);
        }
      } else {
        /* buffering busy */
        if (!buffering && target_state == GST_STATE_PLAYING) {
          /* we were not buffering but PLAYING, PAUSE  the pipeline. */
          gst_element_set_state (pipeline, GST_STATE_PAUSED);
        }
        buffering = TRUE;
      }
      break;
    case ...

  [...]

Stream buffering

      +---------+     +---------+     +-------+
      | httpsrc |     | buffer  |     | demux |
      |        src - sink      src - sink     ....
      +---------+     +---------+     +-------+
    

In this case we are reading from a slow network source into a buffer element
(such as queue2).

The buffer element has a low and high watermark expressed in bytes. The buffer
uses the watermarks as follows:

* The buffer element will post BUFFERING messages 'until' the high watermark is
hit. This instructs the application to keep the pipeline PAUSED, which will
eventually block the srcpad from pushing while data is prerolled in the sinks.

* When the high watermark is hit, a BUFFERING message with 100% will be posted,
which instructs the application to continue playback.

* When during playback, the low watermark is hit, the queue will start posting
BUFFERING messages again, making the application PAUSE the pipeline again until
the high watermark is hit again. This is called the 'rebuffering' stage.

* During playback, the queue level will fluctuate between the high and the low
watermark as a way to compensate for network irregularities. 

note: push mode

This buffering method is usable when the demuxer operates in push mode. Seeking
in the stream requires the seek to happen in the network source. It is mostly
desirable when the total duration of the file is not known, such as in live
streaming or when efficient seeking is not possible/required.

The problem is configuring a good low and high watermark. Here are some ideas:

* It is possible to measure the network bandwidth and configure the low/high
watermarks in such a way that buffering takes a fixed amount of time.

* The queue2 element in GStreamer core has the max-size-time property that,
together with the use-rate-estimate property, does exactly that. Also the
    playbin buffer-duration property uses the rate estimate to scale the amount
    of data that is buffered.

* Based on the codec bitrate, it is also possible to set the watermarks in such
a way that a fixed amount of data is buffered before playback starts.  Normally,
the buffering element doesn't know about the bitrate of the stream but it can
    get this with a query.

* Start with a fixed amount of bytes, measure the time between rebuffering and
increase the queue size until the time between rebuffering is within the
application's chosen limits. 

The buffering element can be inserted anywhere in the pipeline. You could, for
example, insert the buffering element before a decoder. This would make it
possible to set the low/high watermarks based on time.

note: pull mode

The buffering flag on playbin, performs buffering on the parsed data. Another
advantage of doing the buffering at a later stage is that you can let the
demuxer operate in pull mode. When reading data from a slow network drive (with
        filesrc) this can be an interesting way to buffer. 


Download buffering

      +---------+     +---------+     +-------+
      | httpsrc |     | buffer  |     | demux |
      |        src - sink      src - sink     ....
      +---------+     +----|----+     +-------+
                           V
                          file
    
note: push or pull based?

If we know the server is streaming a fixed length file to the client, the
application can choose to download the entire file on disk. The buffer element
will provide a push or pull based srcpad to the demuxer to navigate in the
downloaded file.

This mode is only suitable when the client can determine the length of the file
on the server.

In this case, buffering messages will be emitted as usual when the requested
range is not within the downloaded area + buffersize. The buffering message will
also contain an indication that incremental download is being performed. This
flag can be used to let the application control the buffering in a more
intelligent way, using the BUFFERING query, for example. See the section called
"Buffering strategies". 


Timeshift buffering

      +---------+     +---------+     +-------+
      | httpsrc |     | buffer  |     | demux |
      |        src - sink      src - sink     ....
      +---------+     +----|----+     +-------+
                           V
                       file-ringbuffer
    

note: seek in the buffered data

In this mode, a fixed size ringbuffer is kept to download the server content.
This allows for seeking in the buffered data. Depending on the size of the
ringbuffer one can seek further back in time.

This mode is suitable for all live streams. As with the incremental download
mode, buffering messages are emitted along with an indication that timeshifting
download is in progress. 


Live buffering

In live pipelines we usually introduce some fixed latency between the capture
and the playback elements. This latency can be introduced by a queue (such as a
        jitterbuffer) or by other means (in the audiosink).

Buffering messages can be emitted in those live pipelines as well and serve as
an indication to the user of the latency buffering. The application usually does
not react to these buffering messages with a state change. 


Buffering strategies

What follows are some ideas for implementing different buffering strategies
based on the buffering messages and buffering query.  

No-rebuffer strategy

We would like to buffer enough data in the pipeline so that playback continues
without interruptions. What we need to know to implement this is know the total
remaining playback time in the file and the total remaining download time. If
the buffering time is less than the playback time, we can start playback without
interruptions.

We have all this information available with the DURATION, POSITION and BUFFERING
queries. We need to periodically execute the buffering query to get the current
buffering status. We also need to have a large enough buffer to hold the
complete file, worst case. It is best to use this buffering strategy with
download buffering (see the section called "Download buffering").

This is what the code would look like: 

note: skipped the code


={============================================================================
*kt_dev_bcast_330* gst: man: 18: autoplug and typefind

However, you would rather want to build an application that can automatically detect the media type
of a stream and automatically generate the best possible pipeline by looking at 'all' available
elements in a system. This process is called autoplugging, and GStreamer contains high-quality
autopluggers. If you're looking for an autoplugger, don't read any further and go to Chapter 20.
This chapter will explain the concept of autoplugging and typefinding. It will explain what systems
GStreamer includes to dynamically detect the type of a media stream, and how to generate a pipeline
of decoder elements to playback this media. The same principles can also be used for transcoding.
Because of the full dynamicity of this concept, GStreamer can be automatically extended to support
new media types without needing any adaptations to its autopluggers.

An element must associate a media type to its source and sink pads when it is loaded into the
system.  GStreamer knows about the different elements and what type of data they expect and emit
through the GStreamer registry. This allows for very dynamic and extensible element creation as we
will see.

note: media type means MIME type


18.2. Media stream type detection

Usually, when loading a media stream, the type of the stream is not known. This means that before we
can choose a pipeline to decode the stream, we first need to detect the stream type. GStreamer uses
the concept of 'typefinding' for this. 

Typefinding is a normal part of a pipeline, it will read data for as long as the type of a stream is
unknown. During this period, it will provide data to all plugins that implement a typefinder. When
one of the typefinders recognizes the stream, the typefind element will emit a signal and act as a
passthrough module from that point on. If no type was found, it will emit an error and further media
processing will stop.

Once the typefind element has found a type, the application can use this to plug together a pipeline
to decode the media stream.

Plugins in GStreamer can, as mentioned before, implement typefinder functionality. A plugin
implementing this functionality will submit a media type, optionally a set of file extensions
commonly used for this media type, and a typefind function. Once this typefind function inside the
plugin is called, the plugin will see if the data in this media stream matches a specific pattern
that marks the media type identified by that media type. If it does, it will notify the typefind
element of this fact, telling which mediatype was recognized and how certain we are that this stream
is indeed that mediatype. Once this run has been completed for all plugins implementing a typefind
functionality, the "typefind element" will tell the application what kind of media stream it thinks
to have recognized.

/* create file source and typefind element */
filesrc = gst_element_factory_make ("filesrc", "source");
g_object_set (G_OBJECT (filesrc), "location", argv[1], NULL);

typefind = gst_element_factory_make ("typefind", "typefinder");
g_signal_connect (typefind, "have-type", G_CALLBACK (cb_typefound), loop);
fakesink = gst_element_factory_make ("fakesink", "sink");

/* setup */
gst_bin_add_many (GST_BIN (pipeline), filesrc, typefind, fakesink, NULL);
gst_element_link_many (filesrc, typefind, fakesink, NULL);
gst_element_set_state (GST_ELEMENT (pipeline), GST_STATE_PLAYING);
g_main_loop_run (loop);

Once a media type has been detected, you can plug an element (e.g. a demuxer or decoder) to the
'source' pad of the typefind element, and decoding of the media stream will start right after.


={============================================================================
*kt_dev_bcast_400* gst: gst time and mpeg time

/**
 * GstSegment:
 * @flags: flags for this segment
 * @rate: the rate of the segment
 * @applied_rate: the already applied rate to the segment
 * @format: the format of the segment values
 * @base: the base of the segment
 * @offset: the offset to apply to @start or @stop
 * @start: the start of the segment
 * @stop: the stop of the segment
 * @time: the stream time of the segment
 * @position: the position in the segment
 * @duration: the duration of the segment
 *
 * A helper structure that holds the configured region of
 * interest in a media file.
 */
struct _GstSegment {
  /*< public >*/
  GstSegmentFlags flags;

  gdouble         rate;
  gdouble         applied_rate;

  GstFormat       format;
  guint64         base;
  guint64         offset;
  guint64         start;
  guint64         stop;
  guint64         time;

  guint64         position;
  guint64         duration;

  /* < private > */
  gpointer        _gst_reserved[GST_PADDING];
};

GST_DEBUG_OBJECT (sink, 
        "Segment event: %" GST_TIME_FORMAT " -> %" GST_TIME_FORMAT, 
        GST_TIME_ARGS(segment->start),GST_TIME_ARGS(segment->stop));

:gst_nexus_sink_event_locked: KT: Segment start: 32654987520000000
:gst_nexus_sink_event_locked:<nexussink1> Segment event: 9070:49:47.520000000 -> 99:99:99.999999999


<gst-time-vs-pts-time>
32654987520000000/90K = 362833194666 sec

>>> 362833194666/(60*60)
100786998 hours?

what happened? The reason is gst uses 'nanosecond' so usage of the GST_SECOND
or GST_MSECOND macros is highly recommended.

>>> 32654987520000000/(1000000000)
32654987
>>> 32654987/(60*60)
9070


/**
 * GST_SECOND:
 *
 * Constant that defines one GStreamer second.
 *
 * Value: 1000000000
 * Type: GstClockTime
 */
#define GST_SECOND  (G_USEC_PER_SEC * G_GINT64_CONSTANT (1000))


/* microseconds per second */
typedef struct _GTimer		GTimer;

#define G_USEC_PER_SEC 1000000

/**
 * GstClockTime:
 *
 * A datatype to hold a time, measured in nanoseconds.
 */
typedef guint64 GstClockTime;


/* timestamp debugging macros */
/**
 * GST_TIME_FORMAT:
 *
 * A string that can be used in printf-like format strings to display a
 * #GstClockTime value in h:m:s format.  Use GST_TIME_ARGS() to construct
 * the matching arguments.
 *
 * Example:
 * |[
 * printf("%" GST_TIME_FORMAT "\n", GST_TIME_ARGS(ts));
 * ]|
 */
#define GST_TIME_FORMAT "u:%02u:%02u.%09u"
/**
 * GST_TIME_ARGS:
 * @t: a #GstClockTime
 *
 * Format @t for the #GST_TIME_FORMAT format string. Note: @t will be
 * evaluated more than once.
 */
#define GST_TIME_ARGS(t) \
        GST_CLOCK_TIME_IS_VALID (t) ? \
        (guint) (((GstClockTime)(t)) / (GST_SECOND * 60 * 60)) : 99, \
        GST_CLOCK_TIME_IS_VALID (t) ? \
        (guint) ((((GstClockTime)(t)) / (GST_SECOND * 60)) % 60) : 99, \
        GST_CLOCK_TIME_IS_VALID (t) ? \
        (guint) ((((GstClockTime)(t)) / GST_SECOND) % 60) : 99, \
        GST_CLOCK_TIME_IS_VALID (t) ? \
        (guint) (((GstClockTime)(t)) % GST_SECOND) : 999999999


<gst-buffer>
GST_BUFFER_PTS(stream->buf);

/**
 * GstBuffer:
 * @mini_object: the parent structure
 * @pool: pointer to the pool owner of the buffer
 * @pts: presentation timestamp of the buffer, can be #GST_CLOCK_TIME_NONE when the
 *     pts is not known or relevant. The pts contains the timestamp when the
 *     media should be presented to the user.
 * @dts: decoding timestamp of the buffer, can be #GST_CLOCK_TIME_NONE when the
 *     dts is not known or relevant. The dts contains the timestamp when the
 *     media should be processed.
 * @duration: duration in time of the buffer data, can be #GST_CLOCK_TIME_NONE
 *     when the duration is not known or relevant.
 * @offset: a media specific offset for the buffer data.
 *     For video frames, this is the frame number of this buffer.
 *     For audio samples, this is the offset of the first sample in this buffer.
 *     For file data or compressed data this is the byte offset of the first
 *       byte in this buffer.
 * @offset_end: the last offset contained in this buffer. It has the same
 *     format as @offset.
 *
 * The structure of a #GstBuffer. Use the associated macros to access the public
 * variables.
 */
struct _GstBuffer {
  GstMiniObject          mini_object;

  /*< public >*/ /* with COW */
  GstBufferPool         *pool;

  /* timestamp */
  GstClockTime           pts;
  GstClockTime           dts;
  GstClockTime           duration;

  /* media specific offset */
  guint64                offset;
  guint64                offset_end;
};


={============================================================================
*kt_dev_bcast_400* gst: preroll

// from gnome mail archive

In GStreamer terminology this describes the process of preparing a GStreamer
pipeline for playback/recording. What happens is that audio/video data starts
flowing through the pipeline until all elements have data (so that you could
        start actually playing/outputting the video/audio immediately).

note: when see gst log, preroll happens in the early part of playback.


// copied from text about element state

* GST_STATE_PAUSED
an element has opened the stream, but is 'not' actively 'processing' it. An
element is allowed to modify a stream's position, read and process data and
such to 'prepare' for playback as soon as state is changed to PLAYING, but it
is not allowed to play the data which would make the clock run. In summary,
   PAUSED is the same as PLAYING but 'without' a running clock.  

* GST_STATE_PAUSED 
is the state in which an element is ready to accept and handle data. For most
elements this state is the same as PLAYING. The only exception to this rule are
sink elements. Sink elements only accept one single buffer of data and then
block. At this point the pipeline is 'prerolled' and ready to render data
immediately.

Used in sink element code:

rv = gst_base_sink_wait_preroll (GST_BASE_SINK(sink));

From
http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-libs/html/GstBaseSink.html

GstBaseSink will handle the prerolling correctly. This means that it will return
GST_STATE_CHANGE_ASYNC from a state change to PAUSED until the first buffer
arrives in this element. The base class will call the GstBaseSinkClass.preroll()
vmethod with this preroll buffer and will then commit the state change to the
next asynchronously pending state.

When the element is set to PLAYING, GstBaseSink will synchronise on the clock
using the times returned from GstBaseSinkClass.get_times(). If this function
returns GST_CLOCK_TIME_NONE for the start time, no synchronisation will be done.
Synchronisation can be disabled entirely by setting the object “sync” property
to FALSE.

After synchronisation the virtual method GstBaseSinkClass.render() will be
called. Subclasses should minimally implement this method.

Subclasses that synchronise on the clock in the GstBaseSinkClass.render() method
are supported as well. These classes typically receive a buffer in the render
method and can then potentially block on the clock while rendering. A typical
example is an audiosink. These subclasses can use gst_base_sink_wait_preroll()
to perform the blocking wait.

gst_base_sink_wait_preroll ()

GstFlowReturn
gst_base_sink_wait_preroll (GstBaseSink *sink);

If the GstBaseSinkClass.render() method performs its own synchronisation against
    the clock it must unblock when going from PLAYING to the PAUSED state and
    call this method before continuing to render the remaining data.

This function will block until a state change to PLAYING happens (in which case
        this function returns GST_FLOW_OK) or the processing must be stopped due
to a state change to READY or a FLUSH event (in which case this function returns
        GST_FLOW_FLUSHING).


={============================================================================
*kt_dev_bcast_400* gnome: glib

Basics of GObject
(http://library.gnome.org/devel/gobject/stable/) and glib
(http://library.gnome.org/devel/glib/stable/) programming.

For more information about GObject properties we recommend you read the GObject manual
(http://developer.gnome.org/gobject/stable/rn01.html) and an introduction to The Glib Object system
(http://developer.gnome.org/gobject/stable/pt01.html).

https://developer.gnome.org/glib/unstable/index.html


={============================================================================
*kt_dev_bcast_400* gst: log lines

{pipeline}
INFO            GST_PIPELINE gstparse.c:323:gst_parse_launch_full: parsing pipeline description 'souphttpsrc location=http://test-media.youview.co.uk/ondemand/bbb/avc3/1/2drm_manifest.mpd ! dashdemux name=dash dash. ! video/quicktime ! qtdemux ! queue ! cencdec sas-url=https://ms3.youview.co.uk/s/Big+Buck+Bunny+DASH+2#http://test-media.youview.co.uk/ondemand/bbb/avc3/1/2drm_manifest.mpd ! h264parse ! video/x-h264,stream-format=byte-stream ! nexussink dash. ! audio/x-m4a ! qtdemux ! cencdec ! aacparse ! nexussink '


{loading}
DEBUG GST_PLUGIN_LOADING gstplugin.c:709:gst_plugin_load_file: attempt to load plugin "/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstcenccrypto.so"
INFO  GST_PLUGIN_LOADING gstplugin.c:835:gst_plugin_load_file: plugin "/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstdashdemux.so" loaded


{caps}
DEBUG GST_CAPS gstpad.c:2114:gst_pad_link_check_compatible_unlocked:<nexussink1:sink> sink caps video/mpegts, systemstream=(boolean)true; video/x-h264, stream-format=(string)byte-stream, alignment=(string){ au }; video/mpeg, mpegversion=(int){ 2, 4 }, systemstream=(boolean)false; audio/mpeg, mpegversion=(int)1, layer=(int){ 1, 3 }; audio/mpeg, mpegversion=(int)4, stream-format=(string){ adts, loas }


={============================================================================
*kt_dev_bcast_500* gst: man: gstcap

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstCaps.html

g_object_set(uridecodebin.transferNone(), "caps",
    gst_caps_from_string(
        "application/dash+xml; "
        "application/sdp; "  
        "video/mpegts; "
        "application/hls; "
        "video/x-matroska; "
        "video/quicktime"
        ), NULL);



void
gst_caps_unref (GstCaps *caps);

Unref a GstCaps and and 'free' all its structures and the structures' values when the refcount reaches
0.


GstCaps *
gst_caps_from_string (const gchar *string);

Converts caps from a string representation.

The current implementation of serialization will lead to unexpected results when there are nested
GstCaps / GstStructure deeper than one level.

string
a string to convert to GstCaps
	 
Returns
a newly 'allocated' GstCaps. 


={============================================================================
*kt_dev_bcast_500* gst: gstbasesink

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-libs/html/GstBaseSink.html

GstBaseSink is the base class for sink elements in GStreamer, such as
xvimagesink or filesink. It is a layer on top of GstElement that provides a
simplified interface to plugin writers. GstBaseSink handles many details for
you, for example: preroll, clock 'synchronization', state changes, activation in
push or pull mode, and queries.

In most cases, when writing sink elements, there is no need to implement class
methods from GstElement or to set functions on pads, because the GstBaseSink
infrastructure should be sufficient.

GstBaseSink provides support for exactly one sink pad, which should be named
"sink". A sink implementation (subclass of GstBaseSink) should install a pad
template in its class_init function, like so:

GstBaseSink will handle the prerolling correctly. This means that it will return
GST_STATE_CHANGE_ASYNC from a state change to PAUSED until the first buffer
arrives in this element. The base class will call the GstBaseSinkClass.preroll()
vmethod with this preroll buffer and will then commit the state change to the
next asynchronously pending state.

When the element is set to PLAYING, GstBaseSink will synchronise on the clock
using the times returned from GstBaseSinkClass.get_times(). If this function
returns GST_CLOCK_TIME_NONE for the start time, no synchronisation will be done.
Synchronisation can be disabled entirely by setting the object “sync” property
to FALSE.

After synchronisation the virtual method GstBaseSinkClass.render() will be
called. Subclasses should minimally implement this method.

Subclasses that synchronise on the clock in the GstBaseSinkClass.'render'()
method are supported as well. These classes typically receive a 'buffer' in the
render method and can then potentially block on the clock while rendering. A
typical example is an audiosink. These subclasses can use
gst_base_sink_wait_preroll() to perform the blocking wait.

Upon receiving the EOS event in the PLAYING state, GstBaseSink will wait for the
clock to reach the time indicated by the stop time of the last
GstBaseSinkClass.get_times() call before posting an EOS message. When the
element receives EOS in PAUSED, preroll completes, the event is queued and an
EOS message is posted when going to PLAYING.

GstBaseSink will internally use the GST_EVENT_SEGMENT events to schedule
synchronisation and clipping of buffers. Buffers that fall completely outside of
the current segment are dropped. Buffers that fall partially in the segment are
rendered (and prerolled). Subclasses should do any subbuffer clipping themselves
when needed.

GstBaseSink will by default report the current playback 'position' in
GST_FORMAT_TIME based on the current clock time and segment information. If no
clock has been set on the element, the query will be forwarded upstream.

The GstBaseSinkClass.set_caps() function will be called when the subclass should
configure itself to process a specific media type.

The GstBaseSinkClass.start() and GstBaseSinkClass.stop() virtual methods will be
called when resources should be allocated. Any GstBaseSinkClass.preroll(),
GstBaseSinkClass.render() and GstBaseSinkClass.set_caps() function will be
    called between the GstBaseSinkClass.start() and GstBaseSinkClass.stop()
    calls.

The GstBaseSinkClass.event() virtual method will be called when an event is
received by GstBaseSink. Normally this method should only be overridden by very
specific elements (such as file sinks) which need to handle the newsegment event
specially.

The GstBaseSinkClass.unlock() method is called when the elements should unblock
any blocking operations they perform in the GstBaseSinkClass.render() method.
This is mostly useful when the GstBaseSinkClass.render() method performs a
blocking write on a file descriptor, for example.

The “max-lateness” property affects how the sink deals with buffers that arrive
too late in the sink. A buffer arrives too late in the sink when the
presentation time (as a combination of the last segment, buffer timestamp and
        element base_time) plus the duration is before the current time of the
clock. If the frame is later than max-lateness, the sink will drop the buffer
'without' calling the render method. 

This feature is disabled if sync is disabled, the GstBaseSinkClass.get_times()
method does not return a valid start time or max-lateness is set to -1 (the
        default). Subclasses can use gst_base_sink_set_max_lateness() to
configure the max-lateness value.

The “qos” property will enable the quality-of-service features of the basesink
which gather statistics about the real-time performance of the clock
synchronisation. For each buffer received in the sink, statistics are gathered
and a QOS event is sent upstream with these numbers. This information can then
be used by upstream elements to reduce their processing rate, for example.

The “async” property can be used to instruct the sink to never perform an ASYNC
state change. This feature is mostly usable when dealing with non-synchronized
streams or sparse streams.


{time}
// for video, nexussink0 and for audio nexussink1

0:00:46.502164232  2134   0x4a0230 DEBUG               basesink gstbasesink.c:3347:gst_base_sink_chain_unlocked:<nexussink0> got times start: 0:00:49.880000000, end: 0:00:49.890000000
0:00:46.502503454  2134   0x4a0230 DEBUG               basesink gstbasesink.c:1906:gst_base_sink_get_sync_times:<nexussink0> got times start: 0:00:49.880000000, stop: 0:00:49.890000000, do_sync 1

0:00:46.504891528  2134   0x4a0230 DEBUG               basesink gstbasesink.c:2479:gst_base_sink_do_sync:<nexussink0> reset rc_time to time 0:00:49.720000000

0:00:46.503935084  2134   0x5b8690 DEBUG               basesink gstbasesink.c:3347:gst_base_sink_chain_unlocked:<nexussink1> got times start: 0:00:45.354666666, end: 2962:58:31.854666666
0:00:46.505535677  2134   0x5b8690 DEBUG               basesink gstbasesink.c:1906:gst_base_sink_get_sync_times:<nexussink1> got times start: 0:00:45.354666666, stop: 2962:58:31.854666666, do_sync 1

0:00:46.505868676  2134   0x5b8690 DEBUG               basesink gstbasesink.c:2479:gst_base_sink_do_sync:<nexussink1> reset rc_time to time 0:00:45.194666666

0:00:46.506050492  2134   0x5b8690 DEBUG               basesink gstbasesink.c:2491:gst_base_sink_do_sync:<nexussink1> possibly waiting for clock to reach 0:00:45.354666666, adjusted 0:00:45.194666666

// note: debug line time is not in order.
0:00:46.505080269  2134   0x4a0230 DEBUG               basesink gstbasesink.c:2491:gst_base_sink_do_sync:<nexussink0> possibly waiting for clock to reach 0:00:49.880000000, adjusted 0:00:49.720000000

0:00:46.507354232  2134   0x4a0230 DEBUG               basesink gstbasesink.c:3429:gst_base_sink_chain_unlocked:<nexussink0> rendering object 0x5f9c90
0:00:46.508270343  2134   0x4a0230 DEBUG                default /concurrent_pes_writer.c:478:pes_data_source_produce_data: => 
KT: q:0x45ee60: venq: was_empty: 1, tb(13730), dts: 0:00:49.880000000 dts-diff: 0:00:00.040000000 dts-es-diff: 0:00:04.546666667 dts-abs: 0:00:00.251699371


<clear-stream-that-works>
// video
DEBUG basesink gstbasesink.c:3347:gst_base_sink_chain_unlocked:<nexussink0> got times start: 9074:24:57.720000000, end: 9074:24:57.730000000
DEBUG basesink gstbasesink.c:1906:gst_base_sink_get_sync_times:<nexussink0> got times start: 9074:24:57.720000000, stop: 9074:24:57.730000000, do_sync 1

// audio
DEBUG basesink gstbasesink.c:3347:gst_base_sink_chain_unlocked:<nexussink1> got times start: 9074:24:57.664000000, end: 99:99:99.999999999
DEBUG basesink gstbasesink.c:1906:gst_base_sink_get_sync_times:<nexussink1> got times start: 9074:24:57.664000000, stop: 99:99:99.999999999, do_sync 1


../../../../gtags-target/Zinc.3rdPartyStack/gstreamer-1.5.0/libs/gst/base/gstbasesink.c

// gets time from buf
  GST_DEBUG_OBJECT (basesink, "got times start: %" GST_TIME_FORMAT
      ", stop: %" GST_TIME_FORMAT ", do_sync %d", GST_TIME_ARGS (start),
      GST_TIME_ARGS (stop), *do_sync);


={============================================================================
*kt_dev_bcast_501* gst: gnome: glib: debug

gstreamer-1.5.0/docs/gst/running.xml

<formalpara id="G_DEBUG">
  <title><envar>G_DEBUG</envar></title>

  <para>
Useful GLib environment variable. Set G_DEBUG=fatal_warnings to make
GStreamer programs abort when a critical warning such as an assertion failure
occurs. This is useful if you want to find out which part of the code caused
that warning to be triggered and under what circumstances. Simply set G_DEBUG
as mentioned above and run the program in gdb (or let it core dump). Then get
a stack trace in the usual way.
  </para>


https://developer.gnome.org/glib/stable/glib-running.html

G_DEBUG.  
This environment variable can be set to a list of debug options, which cause GLib to print out
different types of debugging information.

fatal-warnings
Causes GLib to abort the program at the first call to g_warning() or g_critical().

fatal-criticals
Causes GLib to abort the program at the first call to g_critical().

gc-friendly
Newly allocated memory that isn't directly initialized, as well as memory being freed will be reset
to 0. The point here is to allow memory checkers and similar programs that use Boehm GC alike
algorithms to produce more accurate results.

resident-modules
All modules loaded by GModule will be made resident. This can be useful for tracking memory leaks in
modules which are later unloaded; but it can also hide bugs where code is accessed after the module
would have normally been unloaded.

bind-now-modules
All modules loaded by GModule will bind their symbols at load time, even when the code uses
%G_MODULE_BIND_LAZY.

The special value all can be used to turn on all debug options. The special value help can be used
to print all available options. 


==============================================================================
Copyright: see |ktkb|                              vim:tw=100:ts=3:ft=help:norl:
