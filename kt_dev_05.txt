*kt_dev_05*                                                                tw=100

KT KB. DEVELOPMENT. BROADCAST.

/^[#=]{
Use #{ for a group and ={ for a item

|kt_dev_bcast_001| OIPF
|kt_dev_bcast_002| CRB

|kt_dev_bcast_100| mpeg: time sync
|kt_dev_bcast_101| mpeg: ts view tool

|kt_dev_bcast_200| mpeg: streaming: terms
|kt_dev_bcast_201| mpeg: streaming: dash

|kt_dev_bcast_300| gst:
|kt_dev_bcast_301| gst: base elements
|kt_dev_bcast_302| gst: time and seek
|kt_dev_bcast_303| gst: pad cap. gstcap
|kt_dev_bcast_304| gst: multi-thread
|kt_dev_bcast_305| gst: buffers and interact with a pipeline
|kt_dev_bcast_310| gst: debug
|kt_dev_bcast_311| gst: tools
|kt_dev_bcast_312| gst: sdk, tutorials

|kt_dev_bcast_320| gst: plugin writer's guide


# ============================================================================
#{
={============================================================================
*kt_dev_bcast_001* OIPF, DAE

As of 16 June 2014, the Open IPTV Forum has transferred its technical activities to the HbbTV
Association.

http://www.oipf.tv/specifications/

The Open IPTV ForumÂ¿s (OIPF) Declarative Application Environment (DAE), which offers a browser
environment to network applications, is briefly reviewed. It is being implemented in many retail TVs
by major manufacturers. 

={============================================================================
*kt_dev_bcast_002* CRB

connected red button (CRB)


# ============================================================================
#{
={============================================================================
*kt_dev_bcast_100* mpeg: time sync

{master-clock}
STC(system time clock). The mpeg-2 encoder contains 27 MHz oscillator and 33 bits counter, called
the STC. STC is a 33 bits value driven by 90 KHz clock, obtained by dividing the 27 MHz by 300. It
belongs to a particular 'program' and is the master clock of the video and audio encoders for that
program.


{timing-model} 
At the input of the encoder, Point A, the time of occurrence of an input video picture or audio
block (and of the appearance of its coded version at the zero-delay encoder output) is noted by
sampling the STC. A constant quantity equal to the sum of encoder and decoder buffer 'delays' is
added, creating a Presentation Time Stamp (PTS), which is then inserted in the 'first' of the
packet(s) representing that picture or audio block, at Point B in the diagram.


<dts-and-pts>
Also entered into the bitstream under certain conditions is a Decode Time Stamp (DTS), which
represents the time at which the data should be taken instantaneously from the decoder buffer and
decoded. Since the System Target Decoder delay is zero, the DTS and PTS are identical except in the
case of picture reordering for B pictures. The DTS is only used where it is needed because of
reordering. Whenever DTS is used, PTS is also coded.

PTS (or DTS) is entered in the bitstream at intervals not exceeding 700 mS. ATSC further constrains
PTS (or DTS) to be inserted at the beginning of each coded picture ( access unit ). 

note: the both are generally the same, can decide whether to use PTS only or both in PES header.


<pcr-and-scr>
in addition, the output of the 'encoder' buffer (Point C) is time stamped with System Time Clock
(STC) values, called Program Clock Reference (PCR) if the stamp is at the 'transport' packet level,
    or System Clock Reference (SCR) at the PES level. PCR time stamps are required to occur at
    maximum 100 mS 'intervals'. SCR time stamps are required to occur at maximum 700mS intervals.

note: The PTS is stamped at encoder end and the PCR is stamped at packetizer end.

note: MPEG says there should be 10 PCR a second at least and DVB says 25 a second.


{clock-sync} clock-recovery, sync-decoders
The Program Clock Reference (PCR) and/or the System Clock Reference (SCR) are used to synchronize
the decoder STC with the encoder STC. (See Decoder STC Synchronization )

PCR is a clock recovery mechanism for MPEG programs. When a program is encoded, a 27 MHz STC drives
the encoding process. When the program is decoded (or multiplexed), the decoding process must be
driven by a clock which is locked to the encoder's STC. The decoder uses the PCR to regenerate a
local 27 MHz clock.

When a program is inserted into the TS (packetized), 27 MHz timestamp is inserted - PCR. At the
decoder end, it uses a Voltage Controlled Oscillator to generate 27 MHz clock. When PCR is received
via PCR PID in a program, it is compared to a local counter which is driven by the VCXO to ensure
that the 27 MHz clock is locked to the PCR. Get a diff and 'adjust' local clock.

The filtered difference (times a proportionality constant) is the control voltage for a crystal VCO.
This loop stabilizes with the correct frequency, but with an offset in STC that is proportional to
the offset in frequency between the encoder 27 Mhz oscillator and decoder 27 MHz oscillator
free-running frequency. This implies that the decoder should have a slightly larger buffer to
absorb the offset timing.

The PCR field is 42 bit field in the adaptation field of the TS. The PCR field consists of a 9 bit
part that increments at a 27 MHz rate and a 33 bit part that increments at a 90 KHz rate.

Not every TS packet containing the specific PID necessarily includes a PCR value. It is sufficient
to insert a value into a TS packet every 40/100 ms. For this reason, the PCR value is transmitted in
an optional field of the extendable header (adaptation field) in the TS packet.


pcr base: 33 bits          pcr extension: 9 bits    : 42 bits
0 to 2^33-1                0 to 299
90 KHz                     27 MHz
<---------- PTS --------->
<--------------------- PCR ---------------->

note:

Say when use 8 bits:

   7     6     5     4     3     2     1     0
   2^7   2^6   2^5   2^4   2^3   2^2   2^1   2^0

So the max is 2^8-1 and use 8 bits. Likewise, the max is 2^33-1 and use 33 bits. Needs one bit more
in addition to 32 bits type.

8589934591 (2^33-1) / 90K = 95443.717677778 sec 
                            95443.717677778 sec / 3600 = 26.5 hours

There is to be no problem longer than this and so enough to use it as PTS

<in-cdi>
Typedef struct
{
    Uint32_t high; // use LSB and if MSB is 1, invalid PTS
    Uint32_t low;
} PTS;

00:09:55:361 [pid=521,tid=18821664] IOCTL(48="clocksync0", CLOCK_SYNC_GET_VALUE)
PARAM =  (*debug_ptr_ClockValue) = {
   .high = 0
   .low = 108107180
 }

KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458528462]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458528462]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458532062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458928062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1459828062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1460728062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1460810862]

As can see, use 32th bit in the high.


{av-sync} sync-frames
This means that PTS is always higher than the current PCR. The difference between PCR and PTS
represents the data dwell time in the receiver and is thus closely related to the buffer size of the
receiver. According to MPEG-2, the dwell time must not exceed one second.


{problems-caused-by-pcr}
If decoding is too fast (pcr is faster) the buffer memory if the receiver might run empty because
the receiver wants to process the data faster than it arrives at the receiver. In the second case,
    the buffer might overflow because data is processed slower than it arrives at the receiver.


{pcr-example}
pcr base : 0x02B2E37AF  = 724449199
pcr ext  : 0x009B       = 155
pcr      : (724449199*300 + 155)/27 MHz = 8049.435550s = 2:14:09.435550


{pts-example-from-pes-header}
5 byte (40bits)
     0x23 :      0x9A :      0x0F :      0x08 :      0x19
0010 0011 : 1001 1010 : 0000 1111 : 0000 1000 : 0001 1001
        X                       X                       X

Remove the first 4 bits and each maker bit

     001    1001 1010   0000 111    0000 1000   0001 100

This becomes 33 bits:
     001100110100000111000010000001100 (33 bits)
     0-0110-0110-1000-0011-1000-0100-0000-1100 (0x6683840C)

Converts it to decimal, and / 90K

     19109.945022222222222222222222222 sec

5 hours and 18 minuts is 19080 sec. 19109-19080 = 29. So 5:18:29.945.


{pts-from-a-stream}
Frame #        PTS (hex)        PTS (dec)        Diff
1                29042690        688137872                
2                29042d98        688139672        1800    << Frame 1s interpolated..so this is expected.
3                29043ba8        688143272        3600    -> 0.04s = 40ms
4                290449b8        688146872        3600

Each 'frame' has a Presentation Time Stamp (PTS) note: can put PTS for each frame and GOP?

I can't see any drift more than 20 milli seconds, which is a field period. Where ever you see
INTERPOLTAED values, the PTS are 20 milli seconds ahead from the expected value, which shows that
this PTS was read from the next vsync period.

For example,for frame number 32,

PTS in 1st run is 0x2905d378  CODED
PTS is 2nd run is 0x2905da80 INTERP
PTS in 3rd run is 0x2905d378 CODED

The difference between 1st and 2nd run is 20 milli seonds. Similarly in all other cases the drift is
+/-20milli sec which is with in a frame period of 40 msec. This should not create issue of this
sort.


<interpolated-pts>
There is no relation between gop boundaries and interpolation. If a PTS is not available for a
frame/field, Display Manager (Software responsible for comparing PTS of frames and STC) willl
generate a PTS based on the previous coded PTS value so that the PTS-STC compariosn is more
accurate.


{lip-sync-problem}
The lip sync problem has nothing to do with the spec and is implementation issue. This happens when
do not check PTS from a decoder.


{clock-source}
Use CDI term. For live, clock source is "Clock Filter", set PCR PID, get PCR. This CF is attached to
"Clock Sync" device and CS is do clock recovery.

For playback, clock source is decoder, that is uses the first video or audio PTS, set CS with PTS
and do "free-running". That means there is no clock recovery (clock sync) for playback.

When playback a different program, discontinuity happens and set new PTS to CS and do free run.


{discontinuity}
Under special circumstances, the PCR may contain an unavoidable discontinuity, which may be caused
by a switchover from one decoder to another in the transmitter during program emission (contents are
        obtained from another source). PCR discontinuity of this type must be marked in the program
by means of discontinuity_indicator in the adaptation field.


={============================================================================
*kt_dev_bcast_101* mpeg: ts view tool

http://dvbsnoop.sourceforge.net/

// PID is 0x200, gives the filename, set ts format, show 10 items.
$ dvbsnoop.exe 0x200 -if FOSH_Stream20.TS -s ts -n 10

// -tssubdecode shows PES decoding and -ph 0 don't show data dump
// note: to see PES, should give big counts
$ dvbsnoop.exe 0x200 -ph 0 -if FOSH_Stream20.TS -s ts -tssubdecode -n 10000 > log.txt

        program_clock_reference:
            baseH: 0 (0x00)
            baseL: 224777 (0x00036e09)
            reserved: 63 (0x3f)
            extension: 79 (0x004f)
             ==> program_clock_reference: 67433179 (0x0404f2db)  [= PCR-Timestamp: 0:00:02.497525]

            PTS: 
               Fixed: 2 (0x02)
               PTS:
                  bit[32..30]: 0 (0x00)
                  marker_bit: 1 (0x01)
                  bit[29..15]: 7 (0x0007)
                  marker_bit: 1 (0x01)
                  bit[14..0]: 1579 (0x062b)
                  marker_bit: 1 (0x01)
                   ==> PTS: 230955 (0x0003862b)  [= 90 kHz-Timestamp: 0:00:02.5661]


={============================================================================
*kt_dev_bcast_200* mpeg: streaming: terms

o RTMP
Adobe's RTMP-based Dynamic Streaming uses Adobe's proprietary Real Time Messaging Protocol (RTMP),

o HLS
Apple HTTP Live Streaming (HLS).

What is MPEG DASH?
http://www.streamingmedia.com/Articles/Editorial/What-Is-.../What-is-MPEG-DASH-79041.aspx


={============================================================================
*kt_dev_bcast_201* mpeg: streaming: dash

MPEG DASH (Dynamic Adaptive Streaming over HTTP) is a developing ISO Standard (ISO/IEC 23009-1)

Adaptive streaming involves producing several instances of a live or on-demand source file and
making them available to various clients depending upon their delivery bandwidth and CPU processing
power. By monitoring CPU utilization and/or buffer status, adaptive streaming technologies can
change streams when necessary to ensure continuous playback or to improve the experience.


{media-presentation-description-data-model}
Figure 1. The Media Presentation Data Model. Taken from MPEG-DASH presentation at Streaming Media
West, 2011.

For DASH, the actual A/V streams are called the Media Presentation, while the manifest file is
called the Media Presentation Description.

The media presentation, <mpd> defines the video sequence with one or more consecutive <periods> that
break up the video from start to finish. Each period contains multiple <adaptation-sets> that
contain the content that comprises the audio/video experience. This content can be muxed, in which
case there might be one adaptation set, or represented in elementary streams, as shown in Figure 1,
     enabling features like multiple language support for audio. 

Each adaptation set contains multiple <representations>, each a single stream in the adaptive
streaming experience. In the figure, Representation 1 is 640x480@500Kbps, while Representation 2 is
640x480@250Kbps.

Each representation is divided into <media-segments>, essentially the chunks of data that all
HTTP-based adaptive streaming technologies use. Data chunks can be presented in discrete files, as
in HLS, or as byte ranges in a single media file. Presentation in a single file helps improve file
administration and caching efficiency as compared to chunked technologies that can create hundreds
of thousands of files for a single audio/video event.

note: there are five components in the model.

<MPD>
The DASH manifest file, called the Media Presentation Description, is an XML file that identifies
the various content components and the location of all alternative streams. This enables the DASH
player to identify and start playback of the initial segments, switch between representations as
necessary to adapt to changing CPU and buffer status, and change adaptation sets to respond to user
input, like enabling/disabling subtitles or changing languages.


{features}
Other attributes of DASH include:

o DASH is codec-independent, and will work with H.264, WebM and other codecs

o DASH supports both the ISO Base Media File Format (essentially the MP4 format) and MPEG-2
transport streams

o DASH does not specify a DRM method but supports all DRM techniques specified in ISO/IEC 23001-7:
Common Encryption

o DASH supports trick modes for seeking, fast forwards and rewind

o DASH supports advertising insertion


{spec}
http://mpeg.chiariglione.org/


={============================================================================
*kt_dev_bcast_300* gst

http://docs.gstreamer.com/display/GstSDK/Basic+tutorials

{pipeline}

<element-and-pipeline>
GStreamer is a framework designed to handle multimedia flows. Media travels from the "source"
elements (the producers), down to the âsinkâ elements (the consumers), passing through a series of
intermediate elements performing all kinds of tasks. The set of all the interconnected elements is
called a "pipeline".

<branch>
If a container embeds multiple streams (one video and two audio tracks, for example), the demuxer
will separate them and expose them through different output ports. In this way, different branches
can be created in the pipeline, dealing with different types of data.

<downstream>
The basic construction block of GStreamer are the elements, which process the data as it flows
downstream from the source elements (the producers) to the sink elements (the consumers), passing
through filter elements.


<pad>
The 'ports' through which GStreamer elements communicate with each other are called pads (GstPad).
There exists sink pads, through which data enters an element, and source pads, through which data
exits an element.

A demuxer contains one sink pad, through which the muxed data arrives, and multiple source pads, one
for each stream found in the container:

demux
+===================+
|[sink]     [audio] |
|           [video] |
+===================+

note: on demux
The demuxers cannot produce any information until they have received some data and have had a chance
to look at the container to see what is inside. So no source pads to which other elements can link.

The solution is to build the pipeline from the source down to the demuxer, and set it to run (play).
When the demuxer has received enough information to know about the number and kind of streams in the
container, it will start creating source pads. This is the right time for us to finish building the
pipeline and attach it to the newly added demuxer pads.


<signal>
GSignals are a crucial point in GStreamer. They allow you to be notified by means of a callback when
something interesting has happened. Signals are identified by a name, and each GObject has its own
signals.

/* Connect to the pad-added signal */
g_signal_connect (data.source, "pad-added", G_CALLBACK (pad_added_handler), &data);

In this line, we are attaching to the "pad-added" signal of our source uridecodebin element. To do
so, we use g_signal_connect() and provide the callback function to be used (pad_added_handler) and a
data pointer. GStreamer does nothing with this data pointer, it just forwards it to the callback so
we can share information with it. In this case, we pass a pointer to the CustomData structure we
built specially for this purpose.

When our source element finally has enough information to start producing data, it will create
source pads, and trigger the âpad-addedâ signal. At this point our callback will be called:


<status>
You can only move between adjacent ones, this is, you can't go from NULL to PLAYING, you have to go
through the intermediate READY and PAUSED states. If you set the pipeline to PLAYING, though,
        GStreamer will make the intermediate transitions for you.

There are 4 states in GStreamer:

NULL 
the NULL state or initial state of an element.

READY
the element is ready to go to PAUSED.

PAUSED
the element is PAUSED, it is ready to accept and process data. Sink elements however only accept one
buffer and then block.

PLAYING
the element is PLAYING, the clock is running and the data is flowing.


<want-to-message-from-pipeline>
Every element puts messages on the bus regarding its current state, so we filter them out and only
listen to messages coming from the pipeline.

case GST_MESSAGE_STATE_CHANGED:
  /* We are only interested in state-changed messages from the pipeline */
  if (GST_MESSAGE_SRC (msg) == GST_OBJECT (data.pipeline)) {
    GstState old_state, new_state, pending_state;
    gst_message_parse_state_changed (msg, &old_state, &new_state, &pending_state);
    g_print ("Pipeline state changed from %s to %s:\n",
        gst_element_state_get_name (old_state), gst_element_state_get_name (new_state));
  }
  break;


{build-pipeline}

<automatic>
Showed how to build a pipeline automatically. 

/* Initialize GStreamer */
gst_init (&argc, &argv);

/* Build the pipeline */
pipeline = gst_parse_launch ("playbin2 uri=http://docs.gstreamer.com/media/sintel_trailer-480p.webm", NULL);

/* Start playing */
gst_element_set_state (pipeline, GST_STATE_PLAYING);


<manually>
Now we are going to build a pipeline manually by instantiating each element and linking them all
together. 

1. create elements

/* Create the elements */
source = gst_element_factory_make ("videotestsrc", "source");
sink = gst_element_factory_make ("autovideosink", "sink");


2. create pipeline
All elements in GStreamer must typically be contained inside a pipeline before they can be used,
    because it takes care of some clocking and messaging functions.

/* Create the empty pipeline */
pipeline = gst_pipeline_new ("test-pipeline");

/* Build the pipeline */
gst_bin_add_many (GST_BIN (pipeline), source, sink, NULL);
if (gst_element_link (source, sink) != TRUE) {
    g_printerr ("Elements could not be linked.\n");
    gst_object_unref (pipeline);
    return -1;
}

A pipeline is a particular type of bin, which is the element used to contain other elements.
Therefore all methods which apply to bins also apply to pipelines. In our case, we call
gst_bin_add_many() to add the elements to the pipeline (mind the cast). This function accepts a list
of elements to be added, ending with NULL.

These elements, however, are not linked with each other yet. For this, we need to use
gst_element_link().


3. set properities
Most GStreamer elements have customizable properties: named attributes that can be modified to
change the element's behavior (writable properties) or inquired to find out about the element's
internal state (readable properties).

/* Modify the source's properties */
g_object_set (source, "pattern", 0, NULL);


4. start

/* Start playing */
ret = gst_element_set_state (pipeline, GST_STATE_PLAYING);
if (ret == GST_STATE_CHANGE_FAILURE) {
    g_printerr ("Unable to set the pipeline to the playing state.\n");
    gst_object_unref (pipeline);
    return -1;
}


5. check errors

/* Wait until error or EOS */
bus = gst_element_get_bus (pipeline);
msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE, GST_MESSAGE_ERROR | GST_MESSAGE_EOS);
   
/* Parse message */
if (msg != NULL) {
  GError *err;
  gchar *debug_info;
   
  switch (GST_MESSAGE_TYPE (msg)) {
    case GST_MESSAGE_ERROR:
      gst_message_parse_error (msg, &err, &debug_info);
      g_printerr ("Error received from element %s: %s\n", GST_OBJECT_NAME (msg->src), err->message);
      g_printerr ("Debugging information: %s\n", debug_info ? debug_info : "none");
      g_clear_error (&err);
      g_free (debug_info);
      break;
    case GST_MESSAGE_EOS:
      g_print ("End-Of-Stream reached.\n");
      break;
    default:
      /* We should not reach here because we only asked for ERRORs and EOS */
      g_printerr ("Unexpected message received.\n");
      break;
  }
  gst_message_unref (msg);
}

Can have while on message:

/* Listen to the bus */
bus = gst_element_get_bus (data.pipeline);
do {
    msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE,
            GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR | GST_MESSAGE_EOS);
    ...
} while (!terminate);


{bus-and-message}
GStreamer bus is the object responsible for delivering to the application the GstMessages generated
by the elements, in order and to the application thread. This last point is important, because the
actual streaming of media is done in another thread than the application.

Messages can be extracted from the bus synchronously with gst_bus_timed_pop_filtered() and its
siblings, or asynchronously, using signals (shown in the next tutorial). Your application should
always keep an eye on the bus to be notified of errors and other playback-related issues.


{bin}
GstBin â Base class and element that can contain other elements

Object Hierarchy

  GObject
   +----GstObject
         +----GstElement
               +----GstBin *
                     +----GstPipeline

Description

GstBin is an element that can contain other GstElement, allowing them to be managed as a group. Pads
from the child elements can be ghosted to the bin, see GstGhostPad. This makes the bin look like any
other elements and enables creation of higher-level abstraction elements.

A new GstBin is created with gst_bin_new(). Use a GstPipeline instead if you want to create a
toplevel bin because a normal bin doesn't have a bus or handle clock distribution of its own.

After the bin has been created you will typically add elements to it with gst_bin_add(). You can
remove elements with gst_bin_remove().

An element can be retrieved from a bin with gst_bin_get_by_name(), using the elements name.
gst_bin_get_by_name_recurse_up() is mainly used for internal purposes and will query the parent bins
when the element is not found in the current bin.

An iterator of elements in a bin can be retrieved with gst_bin_iterate_elements(). Various other
iterators exist to retrieve the elements in a bin.

gst_object_unref() is used to drop your reference to the bin.

The "element-added" signal is fired whenever a new element is added to the bin. Likewise the
"element-removed" signal is fired whenever an element is removed from the bin. 


={============================================================================
*kt_dev_bcast_301* gst: base elements

gst-plugins-base Elements
http://www.freedesktop.org/software/gstreamer-sdk/data/docs/2012.5/gst-plugins-base-plugins-0.10/index.html

http://docs.gstreamer.com/display/GstSDK/Basic+tutorial+14%3A+Handy+elements

{uridecodebin}
will internally instantiate all the necessary elements (sources, demuxers and decoders) to turn a
URI into raw audio and/or video streams. It does half the work that playbin2 does. Since it contains
demuxers, its source pads are not initially available and we will need to link to them on the fly.

This element decodes data from a URI into raw media. It selects a source element that can handle the
given URI scheme and connects it to a decodebin2 element. Like a demuxer, so it offers as many
source pads as streams are found in the media.

gst-launch-0.10 uridecodebin uri=http://docs.gstreamer.com/media/sintel_trailer-480p.webm ! ffmpegcolorspace ! autovideosink
gst-launch-0.10 uridecodebin uri=http://docs.gstreamer.com/media/sintel_trailer-480p.webm ! audioconvert ! autoaudiosink

Decodes data from a URI into raw media. It selects a source element that can handle the given "uri" scheme and connects it to a decodebin2. 


{decodebin2}
http://www.freedesktop.org/software/gstreamer-sdk/data/docs/2012.5/gst-plugins-base-plugins-0.10/gst-plugins-base-plugins-decodebin2.html#decodebin2
GstBin that auto-magically constructs a decoding pipeline using available decoders and demuxers via
auto-plugging.

decodebin2 is considered stable now and replaces the old decodebin element. uridecodebin uses
decodebin2 internally and is often more convenient to use, as it creates a suitable source element
as well. 


appsrc, appsink

The element used to inject application data into a GStreamer pipeline is appsrc, and its
counterpart, used to extract GStreamer data back to the application is appsink.


={============================================================================
*kt_dev_bcast_302* gst: time and seek

Here we modify this function to periodically wake up and query the pipeline for the stream position,
so we can print it on screen. This is similar to what a media player would do, updating the User
    Interface on a periodic basis.


{gstquery}
GstQuery is a mechanism that allows asking an element or pad for a piece of information.

msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE,
        GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR | GST_MESSAGE_EOS);

Previously we did not provide a timeout to gst_bus_timed_pop_filtered(), meaning that it didn't
return until a message was received. 

Now we use a timeout of 100 milliseconds, so, if no message is received, 10 times per second the
function will return with a NULL instead of a GstMessage. We are going to use this to update our
âUIâ. Note that the timeout period is specified in nanoseconds, so usage of the GST_SECOND or
GST_MSECOND macros is highly recommended.

msg = gst_bus_timed_pop_filtered (bus, 100 * GST_MSECOND,
        GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR | GST_MESSAGE_EOS | GST_MESSAGE_DURATION);


This leads to code that can do something on every timeout.

/* Parse message */
if (msg != NULL) {
    handle_message (&data, msg);
} else {
    /* We got no message, this means the timeout expired */
    if (data.playing) {

         /* We get here approximately 10 times per second, a good enough refresh rate for our UI. We
          * are going to print on screen the current media position, which we can learn be querying
          * the pipeline.
          */
    }
}

gst_element_query_position() is helper function and hides the management of the query object and
directly provides us with the result.


={============================================================================
*kt_dev_bcast_303* gst: pad cap. gstcap

Basic tutorial 6: Media formats and Pad Capabilities

Pads can support multiple Capabilities. In order for two elements to be linked together, they must
share a common subset of Capabilities (Otherwise they could not possibly understand each other) via
a process known as negotiation. This is the main goal of Capabilities.

You can use the gst-inspect-0.10 tool to learn about the Caps of any GStreamer element.

SINK template: 'sink'
  Availability: Always
  Capabilities:
    audio/x-raw-int
               signed: true
                width: 16
                depth: 16
                 rate: [ 1, 2147483647 ]
             channels: [ 1, 2 ]
    audio/x-raw-int
               signed: false
                width: 8
                depth: 8
                 rate: [ 1, 2147483647 ]
             channels: [ 1, 2 ]

SRC template: 'src'
  Availability: Always
  Capabilities:
    video/x-raw-yuv
                width: [ 1, 2147483647 ]
               height: [ 1, 2147483647 ]
            framerate: [ 0/1, 2147483647/1 ]
               format: { I420, NV12, NV21, YV12, YUY2, Y42B, Y444, YUV9, YVU9, Y41B, Y800, Y8  , GREY, Y16 , UYVY, YVYU, IYU1, v308, AYUV, A420 } 


={============================================================================
*kt_dev_bcast_304* gst: multi-thread

GStreamer is a multithreaded framework. This means that, internally, it creates and destroys threads
as it needs them, for example, to decouple streaming from the application thread. 

Moreover, <plugins> are also free to create threads for their own processing, for example, a video
decoder could create 4 threads to take full advantage of a CPU with 4 cores.


{queue}
An application can specify explicitly that a branch (a part of the pipeline) runs on a different
thread (for example, to have the audio and video decoders executing simultaneously).

This is accomplished using the queue element, which works as follows. The sink pad just enqueues
data and returns control. On a different thread, data is dequeued and pushed downstream. This
element is also used for buffering, as seen later in the streaming tutorials. The size of the queue
can be controlled through properties.


As seen in the picture, queues create a new thread, so this pipeline runs in 3 threads. Pipelines
with more than one sink usually need to be multithreaded, because, to be synchronized, sinks usually
block execution until all other sinks are ready, and they cannot get ready if there is only one
thread, being blocked by the first sink.


{pad-types}
We saw an element (uridecodebin) which had no pads to begin with, and they appeared as data started
to flow and the element learned about the media. These are called "Sometimes Pads", and contrast
with the regular pads which are always available and are called "Always Pads".

The third kind of pad is the "Request Pad", which is created on demand. The classical example is the
tee element, which has one sink pad and no initial source pads: they need to be requested and then
tee adds them. In this way, an input stream can be replicated any number of times. The disadvantage
is that linking elements with Request Pads is not as automatic.


={============================================================================
*kt_dev_bcast_305* gst: buffers and interact with a pipeline

Basic tutorial 8: Short-cutting the pipeline

Data 'travels' through a GStreamer pipeline in chunks called buffers. Since this example produces
and consumes data, we need to know about GstBuffers.

Source Pads produce buffers, that are consumed by Sink Pads; GStreamer takes these buffers and
passes them from element to element.

A buffer simply represents a piece of data, do not assume that all buffers will have the same size,
or represent the same amount of time. Neither should you assume that if a single buffer enters an
    element, a single buffer will come out. Elements are free to do with the received buffers as
    they please.

Every buffer has an attached 'GstCaps' structure that describes the kind of media contained in the
buffer. Also, buffers have an attached 'time-stamp' and 'duration', that describe in which moment
the content of the buffer should be rendered or displayed. Time stamping is a very complex and
delicate subject, but this simplified vision should suffice for now.

As an example, a filesrc (a GStreamer element that reads files) produces buffers with the "ANY" caps
and no time-stamping information. After demuxing (see Basic tutorial 3: Dynamic pipelines) buffers
can have some specific caps, for example âvideo/x-h264â. After decoding, each buffer will contain a
single video frame with raw caps (for example, âvideo/x-raw-yuvâ) and very precise time stamps
indicating when should that frame be displayed.


#define CHUNK_SIZE 1024   /* Amount of bytes we are sending in each buffer */
#define SAMPLE_RATE 44100 /* Samples per second we are sending */
#define AUDIO_CAPS "audio/x-raw-int,channels=1,rate=%d,signed=(boolean)true,width=16,depth=16,endianness=BYTE_ORDER"


/* Configure appsrc */
audio_caps_text = g_strdup_printf (AUDIO_CAPS, SAMPLE_RATE);
audio_caps = gst_caps_from_string (audio_caps_text);
g_object_set (data.app_source, "caps", audio_caps, NULL);
g_signal_connect (data.app_source, "need-data", G_CALLBACK (start_feed), &data);
g_signal_connect (data.app_source, "enough-data", G_CALLBACK (stop_feed), &data);

The first property that needs to be set on the appsrc is caps. It specifies the kind of data that
    the element is going to produce, so GStreamer can check if linking with downstream elements is
    possible 

We then 'connect' to the need-data and enough-data signals. These are fired 'by' appsrc when its
internal queue of data is running low or almost full, respectively. We will use these signals to
start and stop (respectively) our signal generation process.


/* Configure appsink */
g_object_set (data.app_sink, "emit-signals", TRUE, "caps", audio_caps, NULL);
g_signal_connect (data.app_sink, "new-buffer", G_CALLBACK (new_buffer), &data);
gst_caps_unref (audio_caps);
g_free (audio_caps_text);

Regarding the appsink configuration, we connect to the new-buffer signal, which is emitted every
    time the sink 'receives' a buffer. Also, the signal emission needs to be enabled through the
    emit-signals property, because, by default, it is disabled.

note: why not to set emit-signals for appsrc given that it is disabled by default?


/* Start playing the pipeline */
gst_element_set_state (data.pipeline, GST_STATE_PLAYING);
   

/* Create a GLib Main Loop and set it to run */
data.main_loop = g_main_loop_new (NULL, FALSE);
g_main_loop_run (data.main_loop);


<pushing>
/* This method is called by the idle GSource in the mainloop, to feed CHUNK_SIZE bytes into appsrc.
 * The ide handler is added to the mainloop when appsrc requests us to start sending data (need-data signal)
 * and is removed when appsrc has enough data (enough-data signal).
 */
static gboolean push_data (CustomData *data) {
  GstBuffer *buffer;
  GstFlowReturn ret;
  int i;
  gint16 *raw;
  gint num_samples = CHUNK_SIZE / 2; /* Because each sample is 16 bits */
  gfloat freq;
   
  /* Create a new empty buffer */                  note: alloc
  buffer = gst_buffer_new_and_alloc (CHUNK_SIZE);
   
  /* Set its timestamp and duration */             note: set and timestamp buffer and duration means size
  GST_BUFFER_TIMESTAMP (buffer) = gst_util_uint64_scale (data->num_samples, GST_SECOND, SAMPLE_RATE);
  GST_BUFFER_DURATION (buffer) = gst_util_uint64_scale (CHUNK_SIZE, GST_SECOND, SAMPLE_RATE);
   
  /* Generate some psychodelic waveforms */
  raw = (gint16 *)GST_BUFFER_DATA (buffer);        note: gst_buffer_'data'
  data->c += data->d;
  data->d -= data->c / 1000;
  freq = 1100 + 1000 * data->d;
  for (i = 0; i < num_samples; i++) {
    data->a += data->b;
    data->b -= data->a / freq;
    raw[i] = (gint16)(500 * data->a);
  }
  data->num_samples += num_samples;
   
  /* Push the buffer into the appsrc */ note:
  g_signal_emit_by_name (data->app_source, "push-buffer", buffer, &ret);
   
  /* Free the buffer now that we are done with it */ note:
  gst_buffer_unref (buffer);
   
  if (ret != GST_FLOW_OK) {
    /* We got some error, stop sending data */
    return FALSE;
  }
   
  return TRUE;
}

Once we have the buffer ready, we pass it to appsrc with the push-buffer action signal (see
        information box at the end of Playback tutorial 1: Playbin2 usage), and then
gst_buffer_unref() it since we no longer need it.

note: why need to free if a buffer travels down in a pipeline?
	

<action-signal>
This rather unintuitive way of retrieving the tag list is called an Action Signal. Action signals
are emitted by the application to a specific element, which then performs an action and returns a
result. They behave like a "dynamic function call", in which methods of a class are identified by
their name (the signal's name) instead of their memory address. These signals are listed In the
documentation along with the regular signals, and are tagged âActionâ. See playbin2, for example.

// appsrc signals
Signals
  "end-of-stream"                                  : Action
  "enough-data"                                    : Run Last
  "need-data"                                      : Run Last
  "push-buffer"                                    : Action
  "seek-data"                                      : Run Last


/* This signal callback triggers when appsrc needs data. Here, we add an idle handler
 * to the mainloop to start pushing data into the appsrc */
static void start_feed (GstElement *source, guint size, CustomData *data) {
  if (data->sourceid == 0) {
    g_print ("Start feeding\n");
    data->sourceid = g_idle_add ((GSourceFunc) push_data, data);
  }
}

This function is called when the internal queue of appsrc is about to starve (run out of data). The
only thing we do here is register a GLib idle function with g_idle_add() that feeds data to appsrc
until it is full again. "A GLib idle function" is a method that GLib will call from its main loop
whenever it is 'idle', this is, when it has no higher-priority tasks to perform. It requires a GLib
GMainLoop to be instantiated and running, obviously.

This is only one of the multiple approaches that appsrc allows. In particular, buffers do not need
to be fed into appsrc from the main thread using GLib, and you do not need to use the need-data and
enough-data signals to synchronize with appsrc (although this is allegedly the most convenient).


/* The appsink has received a buffer */
static void new_buffer (GstElement *sink, CustomData *data) {
  GstBuffer *buffer;
   
  /* Retrieve the buffer */
  g_signal_emit_by_name (sink, "pull-buffer", &buffer);
  if (buffer) {
    /* The only thing we do in this example is print a * to indicate a received buffer */
    g_print ("*");
    gst_buffer_unref (buffer);
  }
}

// appsink
Signals
  "eos"                                            : Run Last
  "new-buffer"                                     : Run Last
  "new-preroll"                                    : Run Last
  "pull-buffer"                                    : Action
  "pull-preroll"                                   : Action
  "new-buffer-list"                                : Run Last
  "pull-buffer-list"                               : Action

Finally, this is the function that gets called when the appsink receives a buffer. We use the
pull-buffer action signal to 'retrieve' the buffer and then just print some indicator on the screen.

We can retrieve the data pointer using the GST_BUFFER_DATA macro and the data size using the
GST_BUFFER_SIZE macro in GstBuffer. Remember that this buffer does 'not' have to match the buffer
that we produced in the push_data function, any element in the path could have altered the buffers
in any way.

note: no copying. what gst_buffer_unref() actually do?


={============================================================================
*kt_dev_bcast_306* gst: 

={============================================================================
*kt_dev_bcast_310* gst: debug

To enable debug output, set the GST_DEBUG environment variable to the desired debug level. All
levels below that will also be shown (i.e., if you set GST_DEBUG=2, you will get both ERROR and
        WARNING messages).

Furthermore, each plugin or part of the GStreamer defines its own category, so you can specify a
debug level for each individual category. For example, GST_DEBUG=2,audiotestsrc:5, will use Debug
Level 5 for the audiotestsrc element, and 2 for all the others.

The '*' wildcard is also available. For example GST_DEBUG=2,audio*:5 will use Debug Level 5 for all
categories starting with the word audio. GST_DEBUG=*:2 is equivalent to GST_DEBUG=2.

Use gst-launch-0.10 --gst-debug-help to obtain the list of all registered categories. Bear in mind
that each plugin registers its own categories, so, when installing or removing plugins, this list
can change.


The content of each line in the debug output is:

0:00:00.868050000  1592   09F62420 WARN   filesrc gstfilesrc.c:1044:gst_file_src_start:<filesrc0> 
                                          error: No such file "non-existing-file.webm"

0:00:00.868050000	Time stamp in HH:MM:SS.sssssssss format since the start of the program

1592	Process ID from which the message was issued. Useful when your problem involves multiple processes

09F62420	Thread ID from which the message was issued. Useful when your problem involves multiple threads

filesrc	Debug Category of the message

<filesrc0>	Name of the object that issued the message. It can be an element, a Pad, or something
else. Useful when you have multiple elements of the same kind and need to distinguish among them.

Naming your elements with the name property will make this debug output more readable (otherwise,
        GStreamer assigns each new element a unique name).


<in-source>
To do so, use the GST_ERROR(), GST_WARNING(), GST_INFO(), GST_LOG() and GST_DEBUG() macros. They
accept the same parameters as printf, and they use the default category (default will be shown as
        the Debug category in the output log).

To change the category to something more meaningful, add these two lines at the top of your code:

GST_DEBUG_CATEGORY_STATIC (my_category);
#define GST_CAT_DEFAULT my_category

And then this one after you have initialized GStreamer with gst_init():

GST_DEBUG_CATEGORY_INIT (my_category, "my category", 0, "This is my very own");

This registers a new category (this is, for the duration of your application: it is not stored in
        any file), and sets it as the default category for your code. See the documentation for
    GST_DEBUG_CATEGORY_INIT().


={============================================================================
*kt_dev_bcast_311* gst: tools

{gst-launch}
The command line for gst-launch consists of a list of options followed by a PIPELINE-DESCRIPTION.
Some simplified instructions are given next, see the complete documentation at the reference page for
gst-launch.

In simple form, a PIPELINE-DESCRIPTION is a list of element types separated by exclamation marks
(!). Go ahead and type in the following command:

gst-launch-0.10 videotestsrc ! ffmpegcolorspace ! autovideosink


<set-properties>
Properties may be appended to elements, in the form property=value (multiple properties can be
        specified, separated by spaces). Use the gst-inspect tool (explained next) to find out the
available properties for an element.

gst-launch-0.10 videotestsrc pattern=11 ! ffmpegcolorspace ! autovideosink


<name-elements>
Elements can be named using the name property, in this way complex pipelines involving branches can
be created. Names allow linking to elements created previously in the description, and are
indispensable to use elements with multiple output pads, like demuxers or tees, for example.

Named elements are referred to using their name followed by a dot.

gst-launch-0.10 videotestsrc ! ffmpegcolorspace ! tee name=t ! queue ! autovideosink t. ! queue ! autovideosink
                                                                                     ^^

The tee is named simply `t` (using the name property) and then linked to a queue and an
autovideosink. The same tee is referred to using ât.â (mind the dot) and then linked to a second
queue and a second autovideosink.


<cap-filters>
When an element has more than one output pad, it might happen that the link to the next element is
ambiguous: the next element may have more than one compatible input pad, or its input pad may be
compatible with the Pad Caps of all the output pads. In these cases GStreamer will link using the
first pad that is available, which pretty much amounts to saying that GStreamer will choose one
output pad at random.

Consider the following pipeline:

gst-launch-0.10 souphttpsrc location=http://docs.gstreamer.com/media/sintel_trailer-480p.webm ! matroskademux ! filesink location=test

This is the same media file and demuxer as in the previous example. The input Pad Caps of filesink
are ANY, meaning that it can accept any kind of media. Which one of the two output pads of
matroskademux will be linked against the filesink? video_00 or audio_00? You cannot know.

You can remove this ambiguity, though, by using named pads, as in the previous sub-section, or by
using Caps Filters:

gst-launch-0.10 souphttpsrc location=http://docs.gstreamer.com/media/sintel_trailer-480p.webm ! matroskademux ! video/x-vp8 ! matroskamux ! filesink location=sintel_video.mkv

A Caps Filter behaves like a pass-through element which does nothing and only accepts media with the
given Caps, effectively resolving the ambiguity. In this example, between matroskademux and
matroskamux we added a video/x-vp8 Caps Filter to specify that we are interested in the output pad
of matroskademux which can produce this kind of video.


{gst-inspect}
This tool has three modes of operation:

o Without arguments, it lists 'all' available elements types, this is, the types you can use to
instantiate new elements.

o With a file name as an argument, it treats the file as a GStreamer plugin, tries to open it, and
lists all the elements described inside.

o With a GStreamer 'element' name as an argument, it lists all information regarding that element.

The most relevant sections are:

o Pad Templates (line 25): This lists all the kinds of Pads this element can have, along with their
                           capabilities. This is where you look to find out if an element can link
                           with another one. In this case, it has only one sink pad template,
                           accepting only video/x-vp8 (encoded video data in VP8 format) and only
                           one source pad template, producing video/x-raw-yuv (decoded video data).

o Element Properties (line 70): This lists the properties of the element, along with their type and
                                accepted values.


{gst-discoverer}
This tool is a wrapper around the GstDiscoverer object shown in Basic tutorial 9: Media information
gathering. It accepts a URI from the command line and prints all information regarding the media
that GStreamer can extract. It is useful to find out what container and codecs have been used to
produce the media, and therefore what elements you need to put in a pipeline to play it.

Use gst-discoverer --help to obtain the list of available options, which basically control the
amount of verbosity of the output.


={============================================================================
*kt_dev_bcast_312* gst: sdk and tutorials

http://docs.gstreamer.com/display/GstSDK/Installing+on+Linux


={============================================================================
*kt_dev_bcast_320* gst: plugin writer's guide

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/pwg/html/index.html

GStreamer Plugin Writer's Guide (1.5.0.1)

<pluggable>
Its main advantages are that the pluggable components can be mixed and matched into arbitrary
pipelines so that it's possible to write a full-fledged video or audio editing application.

The framework is based on plugins that will provide the various codec and other functionality. The
plugins can be linked and arranged in a pipeline. This pipeline defines the flow of the data.

The GStreamer core function is to provide a framework for plugins, data flow, synchronization and
media type handling/negotiation. It also provides an API to write applications using the various
plugins. 


Preliminary Reading

This guide assumes that you are somewhat familiar with the basic workings of GStreamer. For a gentle
introduction to programming concepts in GStreamer, you may wish to read the GStreamer Application
Development Manual first. Also check out the other documentation available on the GStreamer web
site.

In order to understand this manual, you will need to have a basic understanding of the C language.
Since GStreamer adheres to the GObject programming model, this guide also assumes that you
understand the basics of GObject programming. You may also want to have a look at Eric Harlow's book
Developing Linux Applications with GTK+ and GDK. 


CH2.

<element-and-plugin>
Just writing a new element is not entirely enough, however: You will need to encapsulate your
element in a plugin to enable GStreamer to use it. 

A plugin is essentially a loadable block of code, usually called a shared object file or a
dynamically linked library. A single plugin may contain the implementation of several elements, or
just a single one. For simplicity, this guide concentrates primarily on plugins containing one
element. 

A filter is an important type of element that processes a stream of data. Producers and consumers of
data are called source and sink elements, respectively. Bin elements contain other elements. One
type of bin is responsible for synchronization of the elements that they contain so that data flows
smoothly. 

Another type of bin, called 'autoplugger' elements, automatically add other elements to the bin and
links them together so that they act as a filter between two arbitrary stream types.


<basic-type>
GStreamer already supports many basic media types. Following is a table of a few of the basic types
used for buffers in GStreamer. The table contains the name ("media type") and a description of the
type, the properties associated with the type, and the meaning of each property. A full list of
supported types is included in List of Defined Types.

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/pwg/html/section-types-definitions.html

note: no subtitles yet.


={============================================================================
*kt_dev_bcast_321* gst: plugin writer's guide: plugin template

Chapter 3. Constructing the Boilerplate

The first step is to check out a copy of the gst-template git module to get an important tool and
the source code template for a basic GStreamer plugin.

$ git clone git://anongit.freedesktop.org/gstreamer/gst-template.git

The following commands create the MyFilter plugin based on the plugin template and put the output
files in the gst-template/gst-plugin/src directory: 

$ cd gst-template/gst-plugin/src
$ ../tools/make_element myfilter


<element-metadata>
The element details are registered with the plugin during the _class_init () function, which is part
of the GObject system. The _class_init () function should be set for this GObject in the function
where you register the type with GLib.

static void
gst_myfilter_class_init (GstmyfilterClass * klass)
{
  GObjectClass *gobject_class;
  GstElementClass *gstelement_class;

  gobject_class = (GObjectClass *) klass;
  gstelement_class = (GstElementClass *) klass;

  ...

  gst_element_class_set_details_simple(gstelement_class,
    "myfilter",
    "FIXME:Generic",
    "FIXME:Generic Template Element",
    " <<user@hostname.org>>");

}


<constructor-functions>
Each element has two functions which are used for construction of an element. The _class_init()
    function, which is used to initialise the class only once (specifying what signals, arguments
            and virtual functions the class has and setting up global state); 

and the _init() function, which is used to initialise a specific instance of this type. 


<plugin-init>
This is a special function, which is called as soon as the plugin is loaded, and should return TRUE
or FALSE depending on whether it loaded initialized any dependencies correctly. Also, in this
function, any supported element type in the plugin should be registered.

/* entry point to initialize the plug-in
 * initialize the plug-in itself
 * register the element factories and other features
 */
static gboolean
myfilter_init (GstPlugin * myfilter)
{
  /* debug category for fltering log messages
   *
   * exchange the string 'Template myfilter' with your description
   */
  GST_DEBUG_CATEGORY_INIT (gst_myfilter_debug, "myfilter",
      0, "Template myfilter");

  return gst_element_register (myfilter, "myfilter", GST_RANK_NONE,
      GST_TYPE_MYFILTER);
}

/* PACKAGE: this is usually set by autotools depending on some _INIT macro
 * in configure.ac and then written into and defined in config.h, but we can
 * just set it ourselves here in case someone doesn't use autotools to
 * compile this code. GST_PLUGIN_DEFINE needs PACKAGE to be defined.
 */
#ifndef PACKAGE
#define PACKAGE "myfirstmyfilter"
#endif

/* gstreamer looks for this structure to register myfilters
 *
 * exchange the string 'Template myfilter' with your myfilter description
 */
GST_PLUGIN_DEFINE (
    GST_VERSION_MAJOR,
    GST_VERSION_MINOR,
    myfilter,
    "Template myfilter",
    myfilter_init,
    VERSION,
    "LGPL",
    "GStreamer",
    "http://gstreamer.net/"
)


Chapter 4. Specifying the pads

<chain-function>
In the element _init () function, you create the pad from the pad template that has been registered
with the element class in the _class_init () function. After creating the pad, you have to set a
_chain () function pointer that will 'receive' and process the input data on the sinkpad. You can
optionally also set an _event () function pointer and a _query () function pointer.

static void
gst_myfilter_init (Gstmyfilter * filter)
{
  filter->sinkpad = gst_pad_new_from_static_template (&sink_factory, "sink");
  gst_pad_set_event_function (filter->sinkpad,
                              GST_DEBUG_FUNCPTR(gst_myfilter_sink_event));
  gst_pad_set_chain_function (filter->sinkpad,
                              GST_DEBUG_FUNCPTR(gst_myfilter_chain));
  GST_PAD_SET_PROXY_CAPS (filter->sinkpad);
  gst_element_add_pad (GST_ELEMENT (filter), filter->sinkpad);

  filter->srcpad = gst_pad_new_from_static_template (&src_factory, "src");
  GST_PAD_SET_PROXY_CAPS (filter->srcpad);
  gst_element_add_pad (GST_ELEMENT (filter), filter->srcpad);

  filter->silent = FALSE;
}

/* chain function
 * this function does the actual processing
 */
static GstFlowReturn
gst_myfilter_chain (GstPad * pad, GstObject * parent, GstBuffer * buf)
{
  Gstmyfilter *filter;

  filter = GST_MYFILTER (parent);

  if (filter->silent == FALSE)
    g_print ("I'm plugged, therefore I'm in.\n");

  /* just push out the incoming buffer without touching it */
  return gst_pad_push (filter->srcpad, buf);
}

Remember, however, that buffers are not always writeable.

In more advanced elements (the ones that do event processing), you may want to additionally specify
an event handling function, which will be called when stream-events are sent (such as caps,
        end-of-stream, newsegment, tags, etc.). 


Chapter 6. The event function

The event function notifies you of special events that happen in the datastream (such as caps,
        end-of-stream, newsegment, tags, etc.). Events can 'travel' both upstream and downstream, so
you can receive them on sink pads as well as source pads.

Below follows a very simple event function that we install on the sink pad of our element. 

static gboolean
gst_my_filter_sink_event (GstPad *pad, GstObject *parent, GstEvent  *event)
{
  gboolean ret;
  GstMyFilter *filter = GST_MY_FILTER (parent);

  switch (GST_EVENT_TYPE (event)) {
    case GST_EVENT_CAPS:
      /* we should handle the format here */

      /* push the event downstream */
      ret = gst_pad_push_event (filter->srcpad, event);
      break;
    case GST_EVENT_EOS:
      /* end-of-stream, we should close down all stream leftovers here */
      gst_my_filter_stop_processing (filter);

      ret = gst_pad_event_default (pad, parent, event);
      break;
    default:
      /* just call the default handler */
      ret = gst_pad_event_default (pad, parent, event);
      break;
  }
  return ret;
}

It is a good idea to call the default event handler gst_pad_event_default () for unknown events.

'depending' on the event type, the default handler will forward the event or simply unref it. The CAPS
event is by default not forwarded so we need to do this in the event handler ourselves. 


Chapter 7. The query function

Through the query function, your element will receive queries that it has to reply to. These are
queries like position, duration but also about the supported formats and scheduling modes your
element supports. Queries can 'travel' both upstream and downstream, so you can receive them on sink
pads as well as source pads. 

As with event, it is a good idea to call the default query handler gst_pad_query_default () for
unknown queries. Depending on the query type, the default handler will forward the query or simply
unref it. 


Chapter 8. What are states?

Table of Contents

A state describes whether the element instance is initialized, whether it is ready to transfer data
and whether it is currently handling data. There are four states defined in GStreamer:

note: state is per element

which will from now on be referred to simply as "NULL", "READY", "PAUSED" and "PLAYING".

GST_STATE_NULL 
is the default state of an element. In this state, it has not allocated any runtime resources, it
has not loaded any runtime libraries and it can obviously not handle data.

GST_STATE_READY 
is the next state that an element can be in. In the READY state, an element has all default
resources (runtime-libraries, runtime-memory) allocated. However, it has not yet allocated or
defined anything that is stream-specific. 

When going from NULL to READY state (GST_STATE_CHANGE_NULL_TO_READY), an element should allocate any
'non'-stream-specific resources and should load runtime-loadable libraries (if any). When going the
other way around (from READY to NULL, GST_STATE_CHANGE_READY_TO_NULL), an element should unload
these libraries and free all allocated resources. Examples of such resources are hardware devices.
Note that files are generally streams, and these should thus be considered as stream-specific
resources; therefore, they should not be allocated in this state.

GST_STATE_PAUSED 
is the state in which an element is ready to accept and handle data. For most elements this state is
the same as PLAYING. The only exception to this rule are sink elements. Sink elements only accept
one single buffer of data and then block. At this point the pipeline is 'prerolled' and ready to
render data immediately.

GST_STATE_PLAYING 
is the highest state that an element can be in. For most elements this state is exactly the same as
PAUSED, they accept and process events and buffers with data. Only sink elements need to
differentiate between PAUSED and PLAYING state. In PLAYING state, sink elements actually render
incoming data, e.g. output audio to a sound card or render video pictures to an image sink.


<managing-state>
If at all possible, your element should derive from one of the new base classes (Pre-made base
        classes). There are ready-made general purpose base classes for different types of sources,
   sinks and filter/transformation elements. In addition to those, specialised base classes exist
   for audio and video elements and others.

If you use a base class, you will rarely have to handle state changes yourself. All you have to do
is override the base class's start() and stop() virtual functions (might be called differently
        depending on the base class) and the base class will take care of everything for you.

If, however, you do not derive from a ready-made base class, but from GstElement or some other class
not built on top of a base class, you will most likely have to implement your own state change
function to be notified of state changes. 

note: This is definitively necessary if your plugin is a demuxer or a muxer, as there are no base
classes for muxers or demuxers yet. 


==============================================================================
Copyright: see |ktkb|                              vim:tw=100:ts=3:ft=help:norl:
