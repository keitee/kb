*kt_dev_05*                                                                tw=100

KT KB. DEVELOPMENT. BROADCAST.

/^[#=]{
Use #{ for a group and ={ for a item

|kt_dev_bcast_001| OIPF
|kt_dev_bcast_002| CRB

|kt_dev_bcast_100| mpeg: time sync
|kt_dev_bcast_101| mpeg: ts view tool

|kt_dev_bcast_200| mpeg: streaming: terms
|kt_dev_bcast_201| mpeg: streaming: dash

|kt_dev_bcast_300| gst:

# ============================================================================
#{
={============================================================================
*kt_dev_bcast_001* OIPF, DAE

As of 16 June 2014, the Open IPTV Forum has transferred its technical activities to the HbbTV
Association.

http://www.oipf.tv/specifications/

The Open IPTV Forum¿s (OIPF) Declarative Application Environment (DAE), which offers a browser
environment to network applications, is briefly reviewed. It is being implemented in many retail TVs
by major manufacturers. 

={============================================================================
*kt_dev_bcast_002* CRB

connected red button (CRB)


# ============================================================================
#{
={============================================================================
*kt_dev_bcast_100* mpeg: time sync

{master-clock}
STC(system time clock). The mpeg-2 encoder contains 27 MHz oscillator and 33 bits counter, called
the STC. STC is a 33 bits value driven by 90 KHz clock, obtained by dividing the 27 MHz by 300. It
belongs to a particular 'program' and is the master clock of the video and audio encoders for that
program.


{timing-model} 
At the input of the encoder, Point A, the time of occurrence of an input video picture or audio
block (and of the appearance of its coded version at the zero-delay encoder output) is noted by
sampling the STC. A constant quantity equal to the sum of encoder and decoder buffer 'delays' is
added, creating a Presentation Time Stamp (PTS), which is then inserted in the 'first' of the
packet(s) representing that picture or audio block, at Point B in the diagram.


<dts-and-pts>
Also entered into the bitstream under certain conditions is a Decode Time Stamp (DTS), which
represents the time at which the data should be taken instantaneously from the decoder buffer and
decoded. Since the System Target Decoder delay is zero, the DTS and PTS are identical except in the
case of picture reordering for B pictures. The DTS is only used where it is needed because of
reordering. Whenever DTS is used, PTS is also coded.

PTS (or DTS) is entered in the bitstream at intervals not exceeding 700 mS. ATSC further constrains
PTS (or DTS) to be inserted at the beginning of each coded picture ( access unit ). 

note: the both are generally the same, can decide whether to use PTS only or both in PES header.


<pcr-and-scr>
in addition, the output of the 'encoder' buffer (Point C) is time stamped with System Time Clock
(STC) values, called Program Clock Reference (PCR) if the stamp is at the 'transport' packet level,
    or System Clock Reference (SCR) at the PES level. PCR time stamps are required to occur at
    maximum 100 mS 'intervals'. SCR time stamps are required to occur at maximum 700mS intervals.

note: The PTS is stamped at encoder end and the PCR is stamped at packetizer end.

note: MPEG says there should be 10 PCR a second at least and DVB says 25 a second.


{clock-sync} clock-recovery, sync-decoders
The Program Clock Reference (PCR) and/or the System Clock Reference (SCR) are used to synchronize
the decoder STC with the encoder STC. (See Decoder STC Synchronization )

PCR is a clock recovery mechanism for MPEG programs. When a program is encoded, a 27 MHz STC drives
the encoding process. When the program is decoded (or multiplexed), the decoding process must be
driven by a clock which is locked to the encoder's STC. The decoder uses the PCR to regenerate a
local 27 MHz clock.

When a program is inserted into the TS (packetized), 27 MHz timestamp is inserted - PCR. At the
decoder end, it uses a Voltage Controlled Oscillator to generate 27 MHz clock. When PCR is received
via PCR PID in a program, it is compared to a local counter which is driven by the VCXO to ensure
that the 27 MHz clock is locked to the PCR. Get a diff and 'adjust' local clock.

The filtered difference (times a proportionality constant) is the control voltage for a crystal VCO.
This loop stabilizes with the correct frequency, but with an offset in STC that is proportional to
the offset in frequency between the encoder 27 Mhz oscillator and decoder 27 MHz oscillator
free-running frequency. This implies that the decoder should have a slightly larger buffer to
absorb the offset timing.

The PCR field is 42 bit field in the adaptation field of the TS. The PCR field consists of a 9 bit
part that increments at a 27 MHz rate and a 33 bit part that increments at a 90 KHz rate.

Not every TS packet containing the specific PID necessarily includes a PCR value. It is sufficient
to insert a value into a TS packet every 40/100 ms. For this reason, the PCR value is transmitted in
an optional field of the extendable header (adaptation field) in the TS packet.


pcr base: 33 bits          pcr extension: 9 bits    : 42 bits
0 to 2^33-1                0 to 299
90 KHz                     27 MHz
<---------- PTS --------->
<--------------------- PCR ---------------->

note:

Say when use 8 bits:

   7     6     5     4     3     2     1     0
   2^7   2^6   2^5   2^4   2^3   2^2   2^1   2^0

So the max is 2^8-1 and use 8 bits. Likewise, the max is 2^33-1 and use 33 bits. Needs one bit more
in addition to 32 bits type.

8589934591 (2^33-1) / 90K = 95443.717677778 sec 
                            95443.717677778 sec / 3600 = 26.5 hours

There is to be no problem longer than this and so enough to use it as PTS

<in-cdi>
Typedef struct
{
    Uint32_t high; // use LSB and if MSB is 1, invalid PTS
    Uint32_t low;
} PTS;

00:09:55:361 [pid=521,tid=18821664] IOCTL(48="clocksync0", CLOCK_SYNC_GET_VALUE)
PARAM =  (*debug_ptr_ClockValue) = {
   .high = 0
   .low = 108107180
 }

KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458528462]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458528462]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458532062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458928062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1459828062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1460728062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1460810862]

As can see, use 32th bit in the high.


{av-sync} sync-frames
This means that PTS is always higher than the current PCR. The difference between PCR and PTS
represents the data dwell time in the receiver and is thus closely related to the buffer size of the
receiver. According to MPEG-2, the dwell time must not exceed one second.


{problems-caused-by-pcr}
If decoding is too fast (pcr is faster) the buffer memory if the receiver might run empty because
the receiver wants to process the data faster than it arrives at the receiver. In the second case,
    the buffer might overflow because data is processed slower than it arrives at the receiver.


{pcr-example}
pcr base : 0x02B2E37AF  = 724449199
pcr ext  : 0x009B       = 155
pcr      : (724449199*300 + 155)/27 MHz = 8049.435550s = 2:14:09.435550


{pts-example-from-pes-header}
5 byte (40bits)
     0x23 :      0x9A :      0x0F :      0x08 :      0x19
0010 0011 : 1001 1010 : 0000 1111 : 0000 1000 : 0001 1001
        X                       X                       X

Remove the first 4 bits and each maker bit

     001    1001 1010   0000 111    0000 1000   0001 100

This becomes 33 bits:
     001100110100000111000010000001100 (33 bits)
     0-0110-0110-1000-0011-1000-0100-0000-1100 (0x6683840C)

Converts it to decimal, and / 90K

     19109.945022222222222222222222222 sec

5 hours and 18 minuts is 19080 sec. 19109-19080 = 29. So 5:18:29.945.


{pts-from-a-stream}
Frame #        PTS (hex)        PTS (dec)        Diff
1                29042690        688137872                
2                29042d98        688139672        1800    << Frame 1s interpolated..so this is expected.
3                29043ba8        688143272        3600    -> 0.04s = 40ms
4                290449b8        688146872        3600

Each 'frame' has a Presentation Time Stamp (PTS) note: can put PTS for each frame and GOP?

I can't see any drift more than 20 milli seconds, which is a field period. Where ever you see
INTERPOLTAED values, the PTS are 20 milli seconds ahead from the expected value, which shows that
this PTS was read from the next vsync period.

For example,for frame number 32,

PTS in 1st run is 0x2905d378  CODED
PTS is 2nd run is 0x2905da80 INTERP
PTS in 3rd run is 0x2905d378 CODED

The difference between 1st and 2nd run is 20 milli seonds. Similarly in all other cases the drift is
+/-20milli sec which is with in a frame period of 40 msec. This should not create issue of this
sort.


<interpolated-pts>
There is no relation between gop boundaries and interpolation. If a PTS is not available for a
frame/field, Display Manager (Software responsible for comparing PTS of frames and STC) willl
generate a PTS based on the previous coded PTS value so that the PTS-STC compariosn is more
accurate.


{lip-sync-problem}
The lip sync problem has nothing to do with the spec and is implementation issue. This happens when
do not check PTS from a decoder.


{clock-source}
Use CDI term. For live, clock source is "Clock Filter", set PCR PID, get PCR. This CF is attached to
"Clock Sync" device and CS is do clock recovery.

For playback, clock source is decoder, that is uses the first video or audio PTS, set CS with PTS
and do "free-running". That means there is no clock recovery (clock sync) for playback.

When playback a different program, discontinuity happens and set new PTS to CS and do free run.


{discontinuity}
Under special circumstances, the PCR may contain an unavoidable discontinuity, which may be caused
by a switchover from one decoder to another in the transmitter during program emission (contents are
        obtained from another source). PCR discontinuity of this type must be marked in the program
by means of discontinuity_indicator in the adaptation field.


={============================================================================
*kt_dev_bcast_101* mpeg: ts view tool

http://dvbsnoop.sourceforge.net/

// PID is 0x200, gives the filename, set ts format, show 10 items.
$ dvbsnoop.exe 0x200 -if FOSH_Stream20.TS -s ts -n 10

// -tssubdecode shows PES decoding and -ph 0 don't show data dump
// note: to see PES, should give big counts
$ dvbsnoop.exe 0x200 -ph 0 -if FOSH_Stream20.TS -s ts -tssubdecode -n 10000 > log.txt

        program_clock_reference:
            baseH: 0 (0x00)
            baseL: 224777 (0x00036e09)
            reserved: 63 (0x3f)
            extension: 79 (0x004f)
             ==> program_clock_reference: 67433179 (0x0404f2db)  [= PCR-Timestamp: 0:00:02.497525]

            PTS: 
               Fixed: 2 (0x02)
               PTS:
                  bit[32..30]: 0 (0x00)
                  marker_bit: 1 (0x01)
                  bit[29..15]: 7 (0x0007)
                  marker_bit: 1 (0x01)
                  bit[14..0]: 1579 (0x062b)
                  marker_bit: 1 (0x01)
                   ==> PTS: 230955 (0x0003862b)  [= 90 kHz-Timestamp: 0:00:02.5661]


={============================================================================
*kt_dev_bcast_200* mpeg: streaming: terms

o RTMP
Adobe's RTMP-based Dynamic Streaming uses Adobe's proprietary Real Time Messaging Protocol (RTMP),

o HLS
Apple HTTP Live Streaming (HLS).

What is MPEG DASH?
http://www.streamingmedia.com/Articles/Editorial/What-Is-.../What-is-MPEG-DASH-79041.aspx


={============================================================================
*kt_dev_bcast_201* mpeg: streaming: dash

MPEG DASH (Dynamic Adaptive Streaming over HTTP) is a developing ISO Standard (ISO/IEC 23009-1)

Adaptive streaming involves producing several instances of a live or on-demand source file and
making them available to various clients depending upon their delivery bandwidth and CPU processing
power. By monitoring CPU utilization and/or buffer status, adaptive streaming technologies can
change streams when necessary to ensure continuous playback or to improve the experience.


{media-presentation-description-data-model}
Figure 1. The Media Presentation Data Model. Taken from MPEG-DASH presentation at Streaming Media
West, 2011.

For DASH, the actual A/V streams are called the Media Presentation, while the manifest file is
called the Media Presentation Description.

The media presentation, <mpd> defines the video sequence with one or more consecutive <periods> that
break up the video from start to finish. Each period contains multiple <adaptation-sets> that
contain the content that comprises the audio/video experience. This content can be muxed, in which
case there might be one adaptation set, or represented in elementary streams, as shown in Figure 1,
     enabling features like multiple language support for audio. 

Each adaptation set contains multiple <representations>, each a single stream in the adaptive
streaming experience. In the figure, Representation 1 is 640x480@500Kbps, while Representation 2 is
640x480@250Kbps.

Each representation is divided into <media-segments>, essentially the chunks of data that all
HTTP-based adaptive streaming technologies use. Data chunks can be presented in discrete files, as
in HLS, or as byte ranges in a single media file. Presentation in a single file helps improve file
administration and caching efficiency as compared to chunked technologies that can create hundreds
of thousands of files for a single audio/video event.

note: there are five components in the model.

<MPD>
The DASH manifest file, called the Media Presentation Description, is an XML file that identifies
the various content components and the location of all alternative streams. This enables the DASH
player to identify and start playback of the initial segments, switch between representations as
necessary to adapt to changing CPU and buffer status, and change adaptation sets to respond to user
input, like enabling/disabling subtitles or changing languages.


{features}
Other attributes of DASH include:

o DASH is codec-independent, and will work with H.264, WebM and other codecs

o DASH supports both the ISO Base Media File Format (essentially the MP4 format) and MPEG-2
transport streams

o DASH does not specify a DRM method but supports all DRM techniques specified in ISO/IEC 23001-7:
Common Encryption

o DASH supports trick modes for seeking, fast forwards and rewind

o DASH supports advertising insertion


{spec}
http://mpeg.chiariglione.org/


={============================================================================
*kt_dev_bcast_300* gst

http://docs.gstreamer.com/display/GstSDK/Basic+tutorials

{pipeline}

<element-and-pipeline>
GStreamer is a framework designed to handle multimedia flows. Media travels from the "source"
elements (the producers), down to the “sink” elements (the consumers), passing through a series of
intermediate elements performing all kinds of tasks. The set of all the interconnected elements is
called a "pipeline".

<branch>
If a container embeds multiple streams (one video and two audio tracks, for example), the demuxer
will separate them and expose them through different output ports. In this way, different branches
can be created in the pipeline, dealing with different types of data.

<downstream>
The basic construction block of GStreamer are the elements, which process the data as it flows
downstream from the source elements (the producers) to the sink elements (the consumers), passing
through filter elements.


<pad>
The 'ports' through which GStreamer elements communicate with each other are called pads (GstPad).
There exists sink pads, through which data enters an element, and source pads, through which data
exits an element.

A demuxer contains one sink pad, through which the muxed data arrives, and multiple source pads, one
for each stream found in the container:

demux
+===================+
|[sink]     [audio] |
|           [video] |
+===================+

note: on demux
The demuxers cannot produce any information until they have received some data and have had a chance
to look at the container to see what is inside. So no source pads to which other elements can link.

The solution is to build the pipeline from the source down to the demuxer, and set it to run (play).
When the demuxer has received enough information to know about the number and kind of streams in the
container, it will start creating source pads. This is the right time for us to finish building the
pipeline and attach it to the newly added demuxer pads.


<signal>
GSignals are a crucial point in GStreamer. They allow you to be notified by means of a callback when
something interesting has happened. Signals are identified by a name, and each GObject has its own
signals.

/* Connect to the pad-added signal */
g_signal_connect (data.source, "pad-added", G_CALLBACK (pad_added_handler), &data);

In this line, we are attaching to the "pad-added" signal of our source uridecodebin element. To do
so, we use g_signal_connect() and provide the callback function to be used (pad_added_handler) and a
data pointer. GStreamer does nothing with this data pointer, it just forwards it to the callback so
we can share information with it. In this case, we pass a pointer to the CustomData structure we
built specially for this purpose.

When our source element finally has enough information to start producing data, it will create
source pads, and trigger the “pad-added” signal. At this point our callback will be called:


<status>
You can only move between adjacent ones, this is, you can't go from NULL to PLAYING, you have to go
through the intermediate READY and PAUSED states. If you set the pipeline to PLAYING, though,
        GStreamer will make the intermediate transitions for you.

There are 4 states in GStreamer:

NULL 
the NULL state or initial state of an element.

READY
the element is ready to go to PAUSED.

PAUSED
the element is PAUSED, it is ready to accept and process data. Sink elements however only accept one
buffer and then block.

PLAYING
the element is PLAYING, the clock is running and the data is flowing.


<want-to-message-from-pipeline>
Every element puts messages on the bus regarding its current state, so we filter them out and only
listen to messages coming from the pipeline.

case GST_MESSAGE_STATE_CHANGED:
  /* We are only interested in state-changed messages from the pipeline */
  if (GST_MESSAGE_SRC (msg) == GST_OBJECT (data.pipeline)) {
    GstState old_state, new_state, pending_state;
    gst_message_parse_state_changed (msg, &old_state, &new_state, &pending_state);
    g_print ("Pipeline state changed from %s to %s:\n",
        gst_element_state_get_name (old_state), gst_element_state_get_name (new_state));
  }
  break;


{build-pipeline}

1. create elements

/* Create the elements */
source = gst_element_factory_make ("videotestsrc", "source");
sink = gst_element_factory_make ("autovideosink", "sink");


2. create pipeline
All elements in GStreamer must typically be contained inside a pipeline before they can be used,
    because it takes care of some clocking and messaging functions.

/* Create the empty pipeline */
pipeline = gst_pipeline_new ("test-pipeline");

/* Build the pipeline */
gst_bin_add_many (GST_BIN (pipeline), source, sink, NULL);
if (gst_element_link (source, sink) != TRUE) {
    g_printerr ("Elements could not be linked.\n");
    gst_object_unref (pipeline);
    return -1;
}

A pipeline is a particular type of bin, which is the element used to contain other elements.
Therefore all methods which apply to bins also apply to pipelines. In our case, we call
gst_bin_add_many() to add the elements to the pipeline (mind the cast). This function accepts a list
of elements to be added, ending with NULL.

These elements, however, are not linked with each other yet. For this, we need to use
gst_element_link().


3. set properities
Most GStreamer elements have customizable properties: named attributes that can be modified to
change the element's behavior (writable properties) or inquired to find out about the element's
internal state (readable properties).

/* Modify the source's properties */
g_object_set (source, "pattern", 0, NULL);


4. start

/* Start playing */
ret = gst_element_set_state (pipeline, GST_STATE_PLAYING);
if (ret == GST_STATE_CHANGE_FAILURE) {
    g_printerr ("Unable to set the pipeline to the playing state.\n");
    gst_object_unref (pipeline);
    return -1;
}


5. check errors

/* Wait until error or EOS */
bus = gst_element_get_bus (pipeline);
msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE, GST_MESSAGE_ERROR | GST_MESSAGE_EOS);
   
/* Parse message */
if (msg != NULL) {
  GError *err;
  gchar *debug_info;
   
  switch (GST_MESSAGE_TYPE (msg)) {
    case GST_MESSAGE_ERROR:
      gst_message_parse_error (msg, &err, &debug_info);
      g_printerr ("Error received from element %s: %s\n", GST_OBJECT_NAME (msg->src), err->message);
      g_printerr ("Debugging information: %s\n", debug_info ? debug_info : "none");
      g_clear_error (&err);
      g_free (debug_info);
      break;
    case GST_MESSAGE_EOS:
      g_print ("End-Of-Stream reached.\n");
      break;
    default:
      /* We should not reach here because we only asked for ERRORs and EOS */
      g_printerr ("Unexpected message received.\n");
      break;
  }
  gst_message_unref (msg);
}

Can have while on message:

/* Listen to the bus */
bus = gst_element_get_bus (data.pipeline);
do {
    msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE,
            GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR | GST_MESSAGE_EOS);
    ...
} while (!terminate);


{what-bus-and-message}
GStreamer bus is the object responsible for delivering to the application the GstMessages generated
by the elements, in order and to the application thread. This last point is important, because the
actual streaming of media is done in another thread than the application.

Messages can be extracted from the bus synchronously with gst_bus_timed_pop_filtered() and its
siblings, or asynchronously, using signals (shown in the next tutorial). Your application should
always keep an eye on the bus to be notified of errors and other playback-related issues.


{what-bin}
GstBin — Base class and element that can contain other elements

Object Hierarchy

  GObject
   +----GstObject
         +----GstElement
               +----GstBin *
                     +----GstPipeline

Description

GstBin is an element that can contain other GstElement, allowing them to be managed as a group. Pads
from the child elements can be ghosted to the bin, see GstGhostPad. This makes the bin look like any
other elements and enables creation of higher-level abstraction elements.

A new GstBin is created with gst_bin_new(). Use a GstPipeline instead if you want to create a
toplevel bin because a normal bin doesn't have a bus or handle clock distribution of its own.

After the bin has been created you will typically add elements to it with gst_bin_add(). You can
remove elements with gst_bin_remove().

An element can be retrieved from a bin with gst_bin_get_by_name(), using the elements name.
gst_bin_get_by_name_recurse_up() is mainly used for internal purposes and will query the parent bins
when the element is not found in the current bin.

An iterator of elements in a bin can be retrieved with gst_bin_iterate_elements(). Various other
iterators exist to retrieve the elements in a bin.

gst_object_unref() is used to drop your reference to the bin.

The "element-added" signal is fired whenever a new element is added to the bin. Likewise the
"element-removed" signal is fired whenever an element is removed from the bin. 


={============================================================================
*kt_dev_bcast_301* gst: available elements

uridecodebin 

will internally instantiate all the necessary elements (sources, demuxers and decoders) to turn a
URI into raw audio and/or video streams. It does half the work that playbin2 does. Since it contains
demuxers, its source pads are not initially available and we will need to link to them on the fly.


={============================================================================
*kt_dev_bcast_302* gst: time and seek

Here we modify this function to periodically wake up and query the pipeline for the stream position,
so we can print it on screen. This is similar to what a media player would do, updating the User
    Interface on a periodic basis.


{gstquery}
GstQuery is a mechanism that allows asking an element or pad for a piece of information.


msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE,
        GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR | GST_MESSAGE_EOS);

Previously we did not provide a timeout to gst_bus_timed_pop_filtered(), meaning that it didn't
return until a message was received. 

Now we use a timeout of 100 milliseconds, so, if no message is received, 10 times per second the
function will return with a NULL instead of a GstMessage. We are going to use this to update our
“UI”. Note that the timeout period is specified in nanoseconds, so usage of the GST_SECOND or
GST_MSECOND macros is highly recommended.

msg = gst_bus_timed_pop_filtered (bus, 100 * GST_MSECOND,
        GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR | GST_MESSAGE_EOS | GST_MESSAGE_DURATION);


This leads to code that can do something on every timeout.

/* Parse message */
if (msg != NULL) {
    handle_message (&data, msg);
} else {
    /* We got no message, this means the timeout expired */
    if (data.playing) {

         /* We get here approximately 10 times per second, a good enough refresh rate for our UI. We
          * are going to print on screen the current media position, which we can learn be querying
          * the pipeline.
          */
    }
}

gst_element_query_position() is helper function and hides the management of the query object and
directly provides us with the result.


={============================================================================
*kt_dev_bcast_303* gst: pad cap

Pads can support multiple Capabilities. In order for two elements to be linked together, they must
share a common subset of Capabilities (Otherwise they could not possibly understand each other) via
a process known as negotiation. This is the main goal of Capabilities.

You can use the gst-inspect-0.10 tool to learn about the Caps of any GStreamer element.

SINK template: 'sink'
  Availability: Always
  Capabilities:
    audio/x-raw-int
               signed: true
                width: 16
                depth: 16
                 rate: [ 1, 2147483647 ]
             channels: [ 1, 2 ]
    audio/x-raw-int
               signed: false
                width: 8
                depth: 8
                 rate: [ 1, 2147483647 ]
             channels: [ 1, 2 ]

SRC template: 'src'
  Availability: Always
  Capabilities:
    video/x-raw-yuv
                width: [ 1, 2147483647 ]
               height: [ 1, 2147483647 ]
            framerate: [ 0/1, 2147483647/1 ]
               format: { I420, NV12, NV21, YV12, YUY2, Y42B, Y444, YUV9, YVU9, Y41B, Y800, Y8  , GREY, Y16 , UYVY, YVYU, IYU1, v308, AYUV, A420 } 


={============================================================================
*kt_dev_bcast_304* gst: multi-thread

GStreamer is a multithreaded framework. This means that, internally, it creates and destroys threads
as it needs them, for example, to decouple streaming from the application thread. 

Moreover, <plugins> are also free to create threads for their own processing, for example, a video
decoder could create 4 threads to take full advantage of a CPU with 4 cores.


{queue}
An application can specify explicitly that a branch (a part of the pipeline) runs on a different
thread (for example, to have the audio and video decoders executing simultaneously).

This is accomplished using the queue element, which works as follows. The sink pad just enqueues
data and returns control. On a different thread, data is dequeued and pushed downstream. This
element is also used for buffering, as seen later in the streaming tutorials. The size of the queue
can be controlled through properties.


As seen in the picture, queues create a new thread, so this pipeline runs in 3 threads. Pipelines
with more than one sink usually need to be multithreaded, because, to be synchronized, sinks usually
block execution until all other sinks are ready, and they cannot get ready if there is only one
thread, being blocked by the first sink.


{pad-types}
We saw an element (uridecodebin) which had no pads to begin with, and they appeared as data started
to flow and the element learned about the media. These are called "Sometimes Pads", and contrast
with the regular pads which are always available and are called "Always Pads".

The third kind of pad is the "Request Pad", which is created on demand. The classical example is the
tee element, which has one sink pad and no initial source pads: they need to be requested and then
tee adds them. In this way, an input stream can be replicated any number of times. The disadvantage
is that linking elements with Request Pads is not as automatic.



==============================================================================
Copyright: see |ktkb|                              vim:tw=100:ts=3:ft=help:norl:
