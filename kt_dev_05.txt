*kt_dev_05*                                                                tw=100

KT KB. DEVELOPMENT. BROADCAST.

/^[#=]{
Use #{ for a group and ={ for a item

|kt_dev_bcast_001| OIPF
|kt_dev_bcast_002| CRB
|kt_dev_bcast_003| pdl
|kt_dev_bcast_004| marlin

|kt_dev_bcast_100| mpeg: clock and sync
|kt_dev_bcast_101| mpeg: ts view tool
|kt_dev_bcast_102| mpeg: locator
|kt_dev_bcast_103| mpeg: es
|kt_dev_bcast_104| mpeg: audio frame
|kt_dev_bcast_105| mpeg: aspect ratio

|kt_dev_bcast_200| mpeg: streaming: terms
|kt_dev_bcast_201| mpeg: streaming: dash
|kt_dev_bcast_202| mpeg: streaming: mpd file

|kt_dev_bcast_300| gst-git
|kt_dev_bcast_300| gst: tutorial: 02: concept
|kt_dev_bcast_300| gst: tutorial: 03: signal
|kt_dev_bcast_300| gst: tutorial: 04: seeking
*kt_dev_bcast_300* gst: tutorial: 06: pad cap negotiation
*kt_dev_bcast_300* gst: tutorial: 07: queue
*kt_dev_bcast_300* gst: tutorial: 08: buffers and interact with a pipeline
|kt_dev_bcast_302| gst: timer and query
|kt_dev_bcast_303| gst: registry
|kt_dev_bcast_304| gst: queue, branch
|kt_dev_bcast_305| gst: tutorial: 08 buffers and interact with a pipeline
|kt_dev_bcast_301| gst: base elements
*kt_dev_bcast_302* gst: elements: souphttpsrc
|kt_dev_bcast_302| gst-uridecodebin
*kt_dev_bcast_302* gst: elements: qtdemux
|kt_dev_bcast_302| gst: elements: h264parse
*kt_dev_bcast_302* gst: elements: tsparse
*kt_dev_bcast_302* gst-identity
*kt_dev_bcast_302* gst: elements: queue
*kt_dev_bcast_302* gst: elements: typefind
*kt_dev_bcast_302* gst-fakesink
*kt_dev_bcast_302* gst: elements: hlsdemux
*kt_dev_bcast_302* gst: elements: dashdemux

*kt_dev_bcast_310* gst-registry
*kt_dev_bcast_310* gst-scanner
|kt_dev_bcast_310| gst: debug
|kt_dev_bcast_311| gst: gst-launch
|kt_dev_bcast_311| gst: gst-inspect
|kt_dev_bcast_312| gst: sdk, tutorials

|kt_dev_bcast_319| gst: mime and types
*kt_dev_bcast_320* gst: plugin: 02 Foundations
*kt_dev_bcast_321* gst-plug: 03 Building a Plugin
*kt_dev_bcast_321* gst-plug: 04 Chain up
*kt_dev_bcast_321* gst-plug: 05 The chain function
*kt_dev_bcast_321* gst-plug: 06 The event function
*kt_dev_bcast_321* gst-plug: 07 The query function
*kt_dev_bcast_321* gst-plug: 08 What are states?
*kt_dev_bcast_321* gst-plug: 09 Adding Properties
*kt_dev_bcast_321* gst-plug: 10 Signals
*kt_dev_bcast_321* gst-plug: 11 Building a Test Application, gst-plugin-path
*kt_dev_bcast_321* gst-plug: 23 Pre-made base classes

*kt_dev_bcast_330* gst: man: 01: introduction
*kt_dev_bcast_331* gst: man: 03: foundations
*kt_dev_bcast_332* gst: man: 04: initialize gst
*kt_dev_bcast_333* gst-man: 05: elements, state
*kt_dev_bcast_334* gst-man: 06: bins
*kt_dev_bcast_335* gst-man: 07: bus
*kt_dev_bcast_330* gst-man: 08: pad and capability
*kt_dev_bcast_330* gst: man: 14: clocks and synchronization
*kt_dev_bcast_330* gst: man: 15: buffering
|kt_dev_bcast_330| gst: man: 18: autoplug and typefind

|kt_dev_bcast_330| gst: doc: segment, gstsegment
*kt_dev_bcast_330* gst: doc: synchronisation


*kt_dev_bcast_400* gst: gst time and mpeg time
|kt_dev_bcast_400| gst: ref: gstcap

*kt_dev_bcast_500* gst-object
*kt_dev_bcast_500* gst-childproxy
*kt_dev_bcast_500* gst-bin
*kt_dev_bcast_500* gst-cap
*kt_dev_bcast_500* gst: gstbasesink and preroll
*kt_dev_bcast_500* gst: gstbuffer
*kt_dev_bcast_500* gst: gstevent
*kt_dev_bcast_500* gst: gstvideo
*kt_dev_bcast_500* gst-element
*kt_dev_bcast_500* gst-ghostpad
*kt_dev_bcast_500* gst: gststructure
*kt_dev_bcast_500* gst-message
*kt_dev_bcast_500* gst: gstbus
*kt_dev_bcast_500* gst: gstutil
*kt_dev_bcast_500* gst-error
*kt_dev_bcast_500* gst: gstclock

*kt_dev_bcast_600* glib-model
*kt_dev_bcast_600* glib-gobject
*kt_dev_bcast_600* glib-property

*kt_dev_bcast_600* glib: gdebug
*kt_dev_bcast_600* glib-signal
*kt_dev_bcast_600* glib: gatomic
*kt_dev_bcast_600* glib: gthread
*kt_dev_bcast_600* glib: gmain
*kt_dev_bcast_600* glib-error
*kt_dev_bcast_600* glib-quark
*kt_dev_bcast_600* glib: gtime and gdate


# ============================================================================
#{
={============================================================================
*kt_dev_bcast_001* OIPF, DAE

As of 16 June 2014, the Open IPTV Forum has transferred its technical activities
to the HbbTV Association.

http://www.oipf.tv/specifications/

The Open IPTV Forum¿s (OIPF) Declarative Application Environment (DAE), which
offers a browser environment to network applications, is briefly reviewed. It is
being implemented in many retail TVs by major manufacturers. 


={============================================================================
*kt_dev_bcast_002* CRB

connected red button (CRB)


={============================================================================
*kt_dev_bcast_003* PDL

What is Progressive Download

* Download of Movies and TV program content to STB over the IP connection.
* VOD-like experience without allocating VOD bandwidths in the system.
* Supports variable available bandwidth – over Internet.
* Content servers (HTTP) much cheaper than VOD real time servers.
* Progressive Download is where content viewing occurs while content is still
being downloaded.
* STB will only download if it has enough disk space available for the download
content file.


PDL vs. VOD

VOD:
* Content is streamed over the IP connection. Trick modes are controlled by the
server according to the user request.

* Expensive VOD servers are required.
* Network Quality of Service is required.
* No local disk needed.

PDL:
* The content downloaded over the IP connection into the local disk and played
back like any other XTV content.

* No need of expensive VOD servers.
* No need of network Quality of Service.
* Local disk required.

PDL content is stored on the Content Distribution Network (CDN) in Network File
Format (NFF), a file format defined by NDS specifically to support PDL and
related applications. NFF defines a structure for content files including a
Table of Contents (TOC) and containers for content in MPEG TS format, and
optionally indexing and related metadata.


={============================================================================
*kt_dev_bcast_004* marlin

http://www.intertrust.com/technology/marlin-drm/

Marlin DRM is an open-standard content protection system for consumer devices
and services. It offers sophisticated copyrights management for playing
entertainment and media content (including, audio, video, ebooks, and games)
distributed over mobile, broadband, broadcast, and all other popular channels.
Since Marlin is not a proprietary DRM, it is able to deliver content over any
network or physical media. It seamlessly ports licensed content across devices
and services in a consumer's home or personal domain and supports a large and
flexible set of business models for content distribution, including
yet-to-be-defined business models.


note: MS3

Marlin is competitively priced. In fact, the Marlin Simple Secure Streaming
(MS3) technology is free to service providers who can interoperate with MS3
clients using simple off-the-shelf Web Server software. Free packaging tools for
formatting, encryption (including MPEG DASH), which can be used for
Video-on-Demand and Live, are also available. There are also options for service
providers who wish to build richer distribution models (e.g., offline use on
        mobile).


Device authentication using MS3
-------------------------------

In this mechanism, a compound URI is passed by the Application to the
MediaRouter interface. The compound URI includes an https: URL for an MS3
service (the S-URL), together with a URI template for an unencrypted delivery
mechanism with which to obtain the content (the C-URIT). The format of the
compound URI is specified in section 3.4.2 of the Marlin Simple Secure Streaming
(MS3) specification.  Note: YouView devices do not support MS3 Action Tokens or
MS3 Manifest Files.

When presented with a URI of this type, the implementation behind the
MediaRouter interface behaves as described by the MS3 specification. In summary:

The implementation first makes a secure connection to the MS3 service with
client authentication.  Server authentication is performed using the YouView set
of HTTPS root certificates.

note: SAS

The server responds with a Stream Access Statement (SAS) which can include an
authenticator element to be substituted into the media URL and which also
includes a set of output control requirements to be met whilst the content is
being played.

MediaRouter forms a URL for the content (the C-URL) using the C-URIT and the
authenticator.

The content is then streamed using the C-URL as if that URL had been provided
directly by the Application.

For the period that the content is being presented, the output control
mechanisms are configured to satisfy the Output controls requirements provided
by the SAS.

The SAS and the buffered data are retained until the relevant MediaRouter is
destroyed or its source is changed. If the device cannot retrieve or process the
SAS, a DrmEvent is raised.

Where the SAS contains an authenticator, it is possible that this authenticator
may expire after a period of time. If, after the initial request for the
content, the device needs to make a new request (for example to perform a seek,
        to resume after pausing or to re-try after a transient error condition),
it will first make one attempt to use the existing C-URL for the new request. If
    this fails, it will return to the MS3 service for a new SAS.


MS3 specifies the following 3 elements:

1.  a container (SAS) that contains a content encryption key and output control 
flags delivered over a secure channel. SAS's are typically discarded at 
completion of the playback session for the content item they authorize access 
to.


{output-control}
The output controls are:

HDCP (for the digital HDMI output)
CGMS-A (for the analogue output)
the option to disable the analogue output entirely

These options are set using the MS3 Stream Access Statement.


{sdk}
Wasabi Marlin Client SDK


# ============================================================================
#{
={============================================================================
*kt_dev_bcast_100* mpeg: clock and sync

{master-clock}
STC(system time clock). The mpeg-2 encoder contains 27 MHz oscillator and 33
bits counter, called the STC. STC is a 33 bits value driven by 90 KHz clock,
     obtained by dividing the 27 MHz by 300. It belongs to a particular
     'program' and is the master clock of the video and audio encoders for that
     program.


{timing-model} 
At the input of the encoder, Point A, the time of occurrence of an input video picture or audio
block (and of the appearance of its coded version at the zero-delay encoder output) is noted by
sampling the STC. A constant quantity equal to the sum of encoder and decoder buffer 'delays' is
added, creating a Presentation Time Stamp (PTS), which is then inserted in the 'first' of the
packet(s) representing that picture or audio block, at Point B in the diagram.


<dts-and-pts>
Also entered into the bitstream under certain conditions is a Decode Time Stamp
(DTS), which represents the time at which the data should be taken
instantaneously from the decoder buffer and decoded. Since the System Target
Decoder delay is zero, the DTS and PTS are identical except in the case of
picture 'reordering' for B pictures. The DTS is only used where it is needed
because of reordering. Whenever DTS is used, PTS is also coded.

PTS (or DTS) is entered in the bitstream at intervals not exceeding 700 mS. ATSC
further constrains PTS (or DTS) to be inserted at the beginning of each coded
picture (access unit). 

note: the both are generally the same, can decide whether to use PTS only or
both in PES header.


<pcr-and-scr>
in addition, the output of the 'encoder' buffer (Point C) is time stamped with
System Time Clock (STC) values, called Program Clock Reference (PCR) if the
stamp is at the 'transport' packet level, or System Clock Reference (SCR) at the
PES level. PCR time stamps are required to occur at maximum 100 mS 'intervals'.
SCR time stamps are required to occur at maximum 700ms intervals.

note: The PTS is stamped at encoder end and the PCR is stamped at packetizer
end.

note: MPEG says there should be 10 PCR a second at least and DVB says 25 a
second.


<mpeg-spec-says>
D.0.2 Audio and Video Presentation Synchronization

A PTS indicates the time that the PU(Presentation Unit) which results from
decoding the AU(Access Unit) which is associated with the PTS should be
presented to the user. 

So AU is coded stream and PU is decoded stream.

Since PTS and DTS values are not required for every AAU and VAU, the decoder may
choose to interpolate values which are not coded. PTS values are required with
intervals not exceeding 700ms in each elementary audio and video stream. These
time intervals are measured in presentation time, that is, in the same context
as the values of the fields, not in terms of the times that the fields are
transmitted and received. 


<dash-example>
Usually, the audio ES chunk comes first with later DTS and the video comes later
with sooner DTS. This varies to make sure there is no big difference between
them.

The difference between audio chunks : 21ms
The difference between video chunks : 40ms
The difference between video and audio chunks : 808ms

Shows that DTS and PTS are differenct for some chunks and believe that it is
owing to reordering. 

// from MPEG spec. So when I and P is too far from each other.
Since the audio and video elementary stream decoders are instantaneous in the
STD, the decoding time and presentation time are identical in most cases; the
only exception occurs with video pictures which have undergone re-ordering
within the coded bit stream, i.e. I and P pictures in the case of non-low-delay
video sequences. 


{clock-sync} clock-recovery, sync-decoders
The Program Clock Reference (PCR) and/or the System Clock Reference (SCR) are
used to synchronize the decoder STC with the encoder STC. (See Decoder STC
        Synchronization)

The device clock is synchronised to the value of the clock sync source. This
process is called "clock recovery".

PCR is a clock recovery mechanism for MPEG programs. When a program is encoded,
    a 27 MHz STC drives the encoding process. When the program is decoded (or
            multiplexed), the decoding process must be driven by a clock which
    is locked to the encoder's STC. The decoder uses the PCR to regenerate a
    local 27 MHz clock.

When a program is inserted into the TS (packetized), 27 MHz timestamp is
inserted - PCR. At the decoder end, it uses a Voltage Controlled Oscillator to
generate 27 MHz clock. When PCR is received via PCR PID in a program, it is
compared to a local counter which is driven by the VCXO to ensure that the 27
MHz clock is locked to the PCR. Get a diff and 'adjust' local clock.

The filtered difference (times a proportionality constant) is the control
voltage for a crystal VCO. This loop stabilizes with the correct frequency, but
with an offset in STC that is proportional to the offset in frequency between
the encoder 27 Mhz oscillator and decoder 27 MHz oscillator free-running
frequency. This implies that the decoder should have a slightly larger buffer to
absorb the offset timing.

The PCR field is 42 bit field in the adaptation field of the TS. The PCR field
consists of a 9 bit part that increments at a 27 MHz rate and a 33 bit part that
increments at a 90 KHz rate.

Not every TS packet containing the specific PID necessarily includes a PCR
value. It is sufficient to insert a value into a TS packet every 40/100 ms. For
this reason, the PCR value is transmitted in an optional field of the extendable
header (adaptation field) in the TS packet.


pcr base: 33 bits          pcr extension: 9 bits    : 42 bits
0 to 2^33-1                0 to 299
90 KHz                     27 MHz
<---------- PTS --------->
<--------------------- PCR ---------------->

note:

Say when use 8 bits:

   7     6     5     4     3     2     1     0
   2^7   2^6   2^5   2^4   2^3   2^2   2^1   2^0

So the max is 2^8-1 and use 8 bits. Likewise, the max is 2^33-1 and use 33 bits.
Needs one bit more in addition to 32 bits type.

8589934591 (2^33-1) / 90K = 95443.717677778 sec 
                            95443.717677778 sec / 3600 = 26.5 hours

There is to be no problem longer than this and so enough to use it as PTS

<in-cdi>
Typedef struct
{
    Uint32_t high; // use LSB and if MSB is 1, invalid PTS
    Uint32_t low;
} PTS;

00:09:55:361 [pid=521,tid=18821664] IOCTL(48="clocksync0", CLOCK_SYNC_GET_VALUE)
PARAM =  (*debug_ptr_ClockValue) = {
   .high = 0
   .low = 108107180
 }

KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458528462]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458528462]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458532062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1458928062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1459828062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1460728062]
KT: psrc(1) -MS_CDI_CONTROL- returning PTS [1,1460810862]

As can see, use 32th bit in the high.


{av-sync} sync-frames
This means that PTS is always higher than the current PCR. The difference
between PCR and PTS represents the data dwell time in the receiver and is thus
closely related to the 'buffer' size of the receiver. According to MPEG-2, the
dwell time must not exceed one second.

<stream-discontinuity>
The stream timestamp values from the source can be used to check that the Clock
Sync device clock is synchronised with the source of the stream timestamps (a
        decoder). If the Clock Sync device detects that the difference between
the latest stream timestamp and the previous stream timestamp is significantly
different to the difference between the Clock Sync device clock values at the
same points, this is considered a stream 'discontinuity' and the device shall
re-prime from the latest stream timestamp detected from the clock source.

The nature of the stream timestamp information provided by a Decoder device to
the Clock Sync device is stream dependent. For MPEG2 systems, the stream
timestamp is the video or audio PTS, the stream clock is the PCR and the device
clock is the system time clock (STC).


 stream clock              stream timestamp
 (PCR)                     (PTS)
 --------------------------------------------------------->

 | clock recovery          | av sync
 | (PCR discontinuity)     (stream/PTS discontinuity)

 device/reference clock
 (STC)
 --------------------------------------------------------->

The Clock Sync device usually uses clock recovery to keep its clock synchronised
with the transport stream source. The clock recovery has two modes: continuous
clock recovery when do clock recovery and free running when do not clock
recovery. 

For live, clock source is "Clock Filter", set PCR PID, get PCR. This CF is
attached to "Clock Sync" device and CS is do clock recovery.

For playback, clock source is decoder, that is uses the first video or audio
PTS, set CS with PTS and do "free-running". That means there is no clock
recovery (clock sync) for playback. When playback a different program,
         discontinuity happens and set new PTS to CS and do free run.

note: whether do clock recovery or not, av sync always happens

<underrun-overrun>
The consumption rate is determined by the local STB clock. If the local STB
clock is slower or faster than the data source clock it  may result in buffer
overrun or underrun in the attached decoders. 

<clock-source>
To decide which clock source use

// BRCM case

// TSM modes used in NEXUS_StcChannelSettings
typedef enum NEXUS_StcChannelMode
{  
    // Live TSM.
    // STC values are derived from the stream's PCR using the DPCR block. The
    // PCR_OFFSET block pipelines the offsets in the ITB to handle PCR
    // discontinuities. CDB/ITB buffer levels are determined by relative
    // PCR/PTS muxing.
    NEXUS_StcChannelMode_ePcr,

    // Playback TSM.
    // Decoders report PTS values and the StcChannel sets the STC. The
    // PCR_OFFSET block is not used and offsets are not pipelined. CDB/ITB
    // buffer levels are determined by the 'first' 'decoder' to fill its buffer
    // and by relative audio/video muxing.
    NEXUS_StcChannelMode_eAuto,

    // Playback TSM where the application is responsible for setting the STC.
    // This has all the same characteristics as eAuto, except the PTS interrupts
    // from the decoder do not set the STC.
    NEXUS_StcChannelMode_eHost, 

    NEXUS_StcChannelMode_eMax
} NEXUS_StcChannelMode;

// This describes the behavior for STC seeding during PVR when StcChannelMode is
// set to 'eAuto'
typedef enum NEXUS_StcChannelAutoModeBehavior
{
    // The STC will be driven by 'either' the video or audio PTS, depending on
    // stream muxing and error conditions.
    NEXUS_StcChannelAutoModeBehavior_eFirstAvailable, 

    // The video PTS will always drive the STC. Audio errors will be 'ignored'.
    NEXUS_StcChannelAutoModeBehavior_eVideoMaster,

    // The audio PTS will always drive the STC. Video errors will be ignored.
    NEXUS_StcChannelAutoModeBehavior_eAudioMaster,    
    NEXUS_StcChannelAutoModeBehavior_eMax
} NEXUS_StcChannelAutoModeBehavior;


<discontinuity>
Under special circumstances, the PCR may contain an unavoidable discontinuity,
which may be caused by a switchover from one decoder to another in the
transmitter during program emission (contents are obtained from another source).
PCR discontinuity of this type must be marked in the program by means of
discontinuity_indicator in the adaptation field.


{pcr-example}
pcr base : 0x02B2E37AF  = 724449199
pcr ext  : 0x009B       = 155
pcr      : (724449199*300 + 155)/27 MHz = 8049.435550s = 2:14:09.435550


{pts-example-from-pes-header}
5 byte (40bits)
     0x23 :      0x9A :      0x0F :      0x08 :      0x19
0010 0011 : 1001 1010 : 0000 1111 : 0000 1000 : 0001 1001
        X                       X                       X

Remove the first 4 bits and each maker bit

     001    1001 1010   0000 111    0000 1000   0001 100

This becomes 33 bits:
     001100110100000111000010000001100 (33 bits)
     0-0110-0110-1000-0011-1000-0100-0000-1100 (0x6683840C)

Converts it to decimal, and / 90K

     19109.945022222222222222222222222 sec

5 hours and 18 minuts is 19080 sec. 19109-19080 = 29. So 5:18:29.945.


{pts-from-a-stream}
Frame #        PTS (hex)        PTS (dec)        Diff
1                29042690        688137872                
2                29042d98        688139672        1800    << Frame 1s interpolated..so this is expected.
3                29043ba8        688143272        3600    -> 0.04s = 40ms
4                290449b8        688146872        3600

Each 'frame' has a Presentation Time Stamp (PTS) note: can put PTS for each frame and GOP?

I can't see any drift more than 20 milli seconds, which is a field period. Where ever you see
INTERPOLTAED values, the PTS are 20 milli seconds ahead from the expected value, which shows that
this PTS was read from the next vsync period.

For example,for frame number 32,

PTS in 1st run is 0x2905d378  CODED
PTS is 2nd run is 0x2905da80 INTERP
PTS in 3rd run is 0x2905d378 CODED

The difference between 1st and 2nd run is 20 milli seonds. Similarly in all other cases the drift is
+/-20milli sec which is with in a frame period of 40 msec. This should not create issue of this
sort.


<interpolated-pts>
There is no relation between gop boundaries and interpolation. If a PTS is not
available for a frame/field, Display Manager (Software responsible for comparing
        PTS of frames and STC) willl generate a PTS based on the previous coded
PTS value so that the PTS-STC compariosn is more accurate.


{lip-sync-problem}
The lip sync problem has nothing to do with the spec and is implementation
issue. This happens when do not check PTS from a decoder.


={============================================================================
*kt_dev_bcast_101* mpeg: ts view tool

http://dvbsnoop.sourceforge.net/

// PID is 0x200, gives the filename, set ts format, show 10 items.
$ dvbsnoop.exe 0x200 -if FOSH_Stream20.TS -s ts -n 10

// -tssubdecode shows PES decoding and -ph 0 don't show data dump
// note: to see PES, should give big counts
$ dvbsnoop.exe 0x200 -ph 0 -if FOSH_Stream20.TS -s ts -tssubdecode -n 10000 > log.txt

        program_clock_reference:
            baseH: 0 (0x00)
            baseL: 224777 (0x00036e09)
            reserved: 63 (0x3f)
            extension: 79 (0x004f)
             ==> program_clock_reference: 67433179 (0x0404f2db)  [= PCR-Timestamp: 0:00:02.497525]

            PTS: 
               Fixed: 2 (0x02)
               PTS:
                  bit[32..30]: 0 (0x00)
                  marker_bit: 1 (0x01)
                  bit[29..15]: 7 (0x0007)
                  marker_bit: 1 (0x01)
                  bit[14..0]: 1579 (0x062b)
                  marker_bit: 1 (0x01)
                   ==> PTS: 230955 (0x0003862b)  [= 90 kHz-Timestamp: 0:00:02.5661]


={============================================================================
*kt_dev_bcast_102* mpeg: locator

ETSI TS 102 812 V1.3.1 (2012-05)

14 System integration aspects
14.1 Namespace mapping (DVB Locator)


={============================================================================
*kt_dev_bcast_103* mpeg: es and pes

{ES}
sequence layer    : SH | Sequence(GOP) | SH | GOP | SH | GOP | ...
                         *
SH {picture width, picture height, aspect ratio, bit rate, picture rate }

GOP layer         : | I B B P B B P B B ... B P |
                        *
GOP { GOP header, FH, frame 1, FH, frame N }                    

FH { frame type, ..., extension start code, frame structure }

Picture layer     : slice | slice ...
                            *
Slice { SH(Slice Header), macro blocks 1 to N }

Slice layer       : MB MB MB ...

Macro block layer :

Block layer       :

The elementary stream is a 'continual' stream of encoded video frames. Though
all the data required to reconstuct frames exists here. 'no' timing information
or systems data is contained. Thats the job of the MPEG-2 multiplexer.


<pictures-or-frames>
I(Intra-coded)-picture 
P(Predictive-coded)-picture
B(Bi-directional Predictive-coded)-picture

I-frames: contain full picture information
P-frames: predicted from past I or P frames
B-frames: use past and future I or P frames
Transmit I frames every 12 frames or so.

<picture-order>
Pictures are coded and decoded in a different order than they are displayed.
This is due to bidirectional prediction for B pictures. See example below which
illustrates reordering for a 12 picture long GOP.

Source order and encoder input order:
I(1) B(2) B(3) P(4) B(5) B(6) P(7) B(8) B(9) P(10) B(11) B(12) I(13)
 
Encoding order and order in the coded bitstream: DTS
I(1) P(4) B(2) B(3) P(7) B(5) B(6) P(10) B(8) B(9) I(13) B(11) B(12)

In order for a decoder to reconstruct a B-frame from the preceding I and
following P frames, both these must arrive first. So the order of frame
transmission must be different to the order they appear on the tv screen.

Decoder output order and display order (same as input): PTS
I(1) B(2) B(3) P(4) B(5) B(6) P(7) B(8) B(9) P(10) B(11) B(12) I(13)


{PES}
The PES packets can be of variable length, typically upto 64 kbytes, but they
can be longer.

| start | stream | PES           | optional   | stuffing | PES data |
  code    id       packet length   PES header 

One of the most important parts of the structure, are the PTS and DTS, these
allow the decoder to reconstruct the video stream from the I, B, P frames sent
by the encoder.


https://www.uic.edu/classes/ece/ece434/chapter_file/chapter7.htm

The format of the PES header is defined by the stream ID (SID) used to identify
the type of ES. The PES packet length (PESPL) indicates the number of bytes in
the PES packet. The scrambling mode is represented by the scrambling control
(SC). The PES header data length (PESHDL) indicates the number of bytes in the
optional PES header (OPESH) fields, as well as stuffing bytes (SB) used to
satisfy the communication network requirements.

The PES header contains timestamps to allow for synchronization by the decoder.
Two different timestamps are used: presentation timestamp (PTS) and decoding
timestamp (DTS). The PTS specifies the time at which the access unit should be
removed from the decoder buffer and presented. The DTS represents the time at
which the access unit must be decoded. The DTS is optional and it is only used
if the decoding time differs from the presentation time.[2]

The elementary stream clock reference (ESCR) indicates the intended time of
arrival of the packet at the system target decoder (STD). The rate at which the
STD receives the PES is indicated by the elementary stream rate (ESR). Error
checking is provided by the PES cyclic redundancy check (PESCRC).

note: is it a way to reconstruct ES from PES?

The pack header field (PHF) is a PS pack header. The program packet sequence
counter (PPSC) indicates the number of system streams. The STD buffer size is
specified by the P-STD buffer (PSTDB) field.

Optional Fields 2
PESPD Packetized Elementary Stream Private Data
PHF   Pack Header Field
PPSC  Program Packet Sequence Counter
PSTDB P-STD Buffer
PESEF Packetized Elementary Stream Extension Field


{packetization}

VES |        I    | B | B |   P   | B | B |   P    | B | B | ...
AES | frame 1 | frame 2 | ...

    |        I    | B | B |   P   | B | frame 1  | B |   P    | B | B | ...
PES <-packet-><-packet-><-packet    -> <-packet-> <-packet-><-packet->

note: PES has audio and video? Maybe muxed PES stream from VPES and APES?

note: How construct ES from PES? Use time?

TS

<pes-usuage-for-video>
Except for ATSC, there is not defined mapping between an encoded picture and a
PES packet, nor is there any alignment requirement between PES and pictures

ATSC defines PES per Picture

Tandberg (and some others) use a PES per GOP (additional picture timestamps are
        inferred as required)

A few others use a fixed size PES; the timestamp then applies to the first
picture that starts in the PES packet, and is not always present

PTS (Presentation Time Stamp) and DTS (Decode Time Stamp), when present, are
used to synchronise streams, and control buffer usage


<non-transport-stream-playback> from CDI
The Audio and Video Decoder devices may be used for playing content that is
carried by containers other than a transport stream. In this case, the client
shall extract the encoded stream from the container and provide it separately to
the Video and Audio decoders, using a memory source and the write() system call.

The client shall configure the decoder with this stream format and provide the
Audio and Video streams as a bounded MPEG-2 PES format.  In addition, the stream
shall be structured according to the following rules:

1. The injection shall start at a beginning of a PES header.

2. The first PES header written to each decoder shall carry valid PTS info in
order to allow audio-video synchronisation, as described in section 6.2.1.1.

3. 'each' frame in the video stream shall start at a new PES packet. A frame may
be split into several PES packets, for example, if a frame exceeds the maximum
PES packet length (as defined in ISO/IEC 13818-1 - Generic Coding of Moving
        Pictures and Associated Audio Information: Systems).

4. In an audio stream, more than one frame may be aggregated into a single PES
packet.

Metadata that is required for decoding shall be provided to the decoder as part
of PES packet payload, for example, the sequence parameters set (SPS) and
picture parameters set (PPS) data in H.264-AVC encoded video streams.


<from-brcm-7401>

note: PES parser

The data transport processor is an MPEG-2/DIRECTV transport stream message/PES
parser and demultiplexer. It can simultaneously process 256 PID filters via 128
PID channels in up to five independent external transport stream inputs and two
internal playback channels, with decryption for all 128 PID channels. 

It supports message/PES parsing for 128 PID channels with storage to 128
external DRAM buffers, and it provides 512 4-byte generic section filters that
can be cascaded to provide effectively longer filters (up to 64-bytes or 128
        filters of 16-bytes each). 

The data transport module provides two sets of a two-channel remux output. 
    
The data transport module has a RAVE (record, audio and video interface engine)
    function, which can be configured to support eight record channels for PVR
    functionality and six AV channels to interface audio and video decoders.

* PES packet extraction for up to 128 PID channels.
* Supports parsing of Transport/PES data to ES and generate CDB/ ITBs for
audio/video decoders.


FUNCTIONAL OVERVIEW

The Data Transport Processor is an MPEG-2/DIRECTV transport stream message/PES
parser and demultiplexer. It is capable of simultaneously processing 256 PIDs
via 128 PID channels in up to six independent transport streams using the six
available parsers. These six streams are selected from five external serial
transport stream inputs, and four internal playback channels. The data transport
supports decryption for up to 128 PID channels in the six streams. All 128 PID
channels can be used by RAVE (record, audio and video interface engine), PCR
processors, message filter as well as for output via the high-speed transport or
remux module.

The data transport supports up to 128 PID channels for message or generic PES
processing and storage in up to 128 external DRAM message buffers. There are 512
4-byte generic filters supported for processing of MPEG/DVB sections or DIRECTV
messages. A special addressing mode filter is included for up to 32 PID channels
(PID channels 0-31), which filters MPEG and private stream messages.

The data transport module supports RAVE (record, audio and video interface
        engine) function, which can supports up to eight record channels with up
to total 12 SCDs (configured 1-12 per record channel) and six AV channels to
interface to the audio/video decoders.

The data transport also provides two PCR recovery blocks and one serial STC
broadcast block for transmitting the STC to the decoders

Input Band 
Refers to the five external transport stream inputs supported by this design
(IB0-4).

Parser Band 
Refers to the transport streams that are selected as inputs to the five front
end parsers or two playback channels to two playback parsers

PES Parser

The PES parser delineates PES packets and sends them to the message buffers. Any
number of the PID channels 0-127 can be enabled for PES processing. When a
complete PES packet is received, a data available interrupt is generated by the
message buffer manager. 

The PES parser checks for PES packet lengths and generates length error
interrupts. PES Padding streams (i.e., PES messages with stream_id of 0xBE) are
removed by default, or optionally retained. When stored to memory, padding bytes
(0x55) are optionally added at the end of each PES packet to word align to
32-bit boundaries in the message buffers. During data transport playback, the
padding bytes are removed.

The PES Parser checks for PES packet length and can generate length error
interrupts if enabled. It uses the payload_unit_start_indicator bit in the
transport packet to detect the beginning of the PES packet. A length error is
generated whenever the end of a PES packet does not coincide with the end of a
transport packet or the payload_unit start_indicator is received prior to the
end of the current PES packet. Only PID channels 0 to 127 can be routed to PES
parser.


{TS}
PUSI: Payload Unit Start Indicator which indicate if TS packet has PES header


={============================================================================
*kt_dev_bcast_104* mpeg: audio frame

http://www.datavoyage.com/mpgscript/mpeghdr.htm

MPEG Audio Frame Header

An MPEG audio file is built up from smaller parts called frames. Generally,
frames are independent items. Each frame has its own header and audio
    informations. There is no file header. Therefore, you can cut any part of
    MPEG file and play it correctly (this should be done on frame boundaries but
            most applications will handle incorrect headers). 
    

For Layer III, this is not 100% correct. Due to internal data organization in
MPEG version 1 Layer III files, frames are often dependent of each other and
they cannot be cut off just like that.


When you want to read info about an MPEG file, it is usually enough to find the
first frame, read its header and assume that the other frames are the same This
may not be always the case. Variable bitrate MPEG files may use so called
bitrate switching, which means that bitrate changes according to the content of
each frame. This way lower bitrates may be used in frames where it will not
reduce sound quality. This allows making better compression while keeping high
quality of sound.


The frame header is constituted by the very first four bytes (32bits) in a
frame. The first eleven bits (or first twelve bits, see below about frame sync)
of a frame header are always set and they are called "frame sync". Therefore,
you can search through the file for the first occurence of frame sync (meaning
        that you have to find a byte with a value of 255, and followed by a byte
        with its three (or four) most significant bits set). Then you read the
    whole header and check if the values are correct. You will see in the
    following table the exact meaning of each bit in the header, and which
    values may be checked for validity. Each value that is specified as
    reserved, invalid, bad, or not allowed should indicate an invalid header.
    Remember, this is not enough, frame sync can be easily (and very frequently)
    found in any binary file. Also it is likely that MPEG file contains garbage
    on it's beginning which also may contain false sync. Thus, you have to check
    two or more frames in a row to assure you are really dealing with MPEG audio
    file.


Frames may have a CRC check. The CRC is 16 bits long and, if it exists, it
follows the frame header. After the CRC comes the audio data. You may calculate
the length of the frame and use it if you need to read other headers too or just
want to calculate the CRC of the frame, to compare it with the one you read from
the file. This is actually a very good method to check the MPEG header validity.

Here is "graphical" presentation of the header content. Since at the time I
created this document there were no such information publicly available, I've
creted notation to explain content and meaning of data within mpeg file.
Characters from A to M are used to indicate different fields. In the table, you
can see details about the content of each field. 


={============================================================================
*kt_dev_bcast_105* mpeg: aspect ratio

https://en.wikipedia.org/wiki/Pixel_aspect_ratio

Pixel Aspect Ratio is often confused with different types of image aspect
ratios; the ratio of the image width and height. Due to non-squareness of pixels
in Standard-definition TV, there are two types of such aspect ratios: Storage
Aspect Ratio (SAR) and Display Aspect Ratio (abbreviated DAR, also known as
    Image Aspect Ratio and Picture Aspect Ratio).

note:
In short, PAR is conceptual model in digital video since there are difference
between coded size and display size.


http://www.lurkertech.com/lg/pixelaspect/#nonsqaspect

What Are They?

Pixels in the graphics world are square. A 100 pixel vertical line is the same
length as a 100 pixel horizontal line on a graphics monitor.

Some pixels in the standard-def video world (specifically, the digital
    electrical signals universally used in studios for 480i and 576i production,
    as defined by the infamous ITU-R BT.601-4 or "Rec. 601") are non-square. A
100 pixel vertical line may be longer or shorter than a 100 pixel horizontal
line on a video monitor, depending on the video system.

Pixels in the HD (e.g. 1080i and 720p) video world are, fortunately, square.

The term which describes this squareness or non-squareness is pixel aspect
ratio, expressed as a fraction of horizontal (x) pixel size divided by vertical
(y) pixel size. The pixel aspect ratio for square pixels is 1/1.


<case>

  16 (width)
  ================  9 (height)
  ================
  ================
  ================

  Sample(Source) AR

  SAR x PAR = DAR

  W     1     DW
  -   x -   = -
  H     1     DH


note: W, H from hardware

uint32_t displayHorizontalSize; 
uint32_t displayVerticalSize; 
/* 'intended' display height of active area needed to preserve correct aspect
 * ratio. this is 'not' the actual display height used. */

note: AR from hardware

NEXUS_AspectRatio aspectRatio;
/* Sample aspect ratio - aspect ratio of the source calculated as the ratio of
* two numbers reported by the 'decoder'.  This aspect ratio is applied to the
* picture's source size (i.e. coded size), not the picture's display size. */

uint16_t sampleAspectRatioX; 
uint16_t sampleAspectRatioY; /* See comments for sampleAspectRatioX */
/* Only valid if aspectRatio is NEXUS_AspectRatio_eSar.
 * NEXUS_AspectRatio_eSar is only used for some codecs. If used, the
 * source's aspect ratio is calculated as the ratio of
 * sampleAspectRatioX/sampleAspectRatioY. sampleAspectRatioX and
 * sampleAspectRatioY should not be used except as a ratio. */


:964 Returned W x H video size: 512 x 288
:965 Returned sampleAspectRatioX and sampleAspectRatioY : 1 / 1

// Calculate DAR based on PAR and video size.
// int displayWidth = W * sampleAspectRatioX;
// int displayHeight = H * sampleAspectRatioY;

// Divide display width and height by their GCD to avoid possible overflows.
// int displayAspectRatioGCD = greatestCommonDivisor(displayWidth, displayHeight);
// displayWidth /= displayAspectRatioGCD;
// displayHeight /= displayAspectRatioGCD;

:976 displayWidth: 16, displayHeight: 9

// if (!( H % displayHeight)) {
//     ERROR("Keeping video original height: sh: " << size.height << " dh: " << displayHeight );
//
//     288 x 16 / 9  = 4608 / 9 = 512
//
//     width = gst_util_uint64_scale_int(size.height, displayWidth, displayHeight);
//     height = static_cast<guint64>(size.height);
//
//     ERROR("Keeping video original height: w: " << width << " h: " << height );
// } else if (!(size.width % displayWidth)) {

:985 Keeping video original height: sh: 288 dh: 9
:988 Keeping video original height: w: 512 h: 288
:1009 Changed size: 512 x 288


512    1    16
 -  x  -  = -    -> 512(x) = 288 * 16 / 9
288    1    9

SAR x PAR = DAR(from SAR)

512    16    16
 -  x  -   = -    -> 352(x) = 288 * 11 * 16 / (16 *9)
288    11    9

This assumes that hardware reports pixel aspect ratio other than 1:1. Really? 

Yes, there are when play amazon dash stream.

:964 Returned video size: 512 x 216
:965 Returned AR: 1269 / 1280


:964 Returned video size: 512 x 216
:965 Returned AR: 1269 / 1280
:976 dw: 47 dh: 20


double ret_round1 = round(size.width*ratioNumerator/ratioDenominator);
double ret_round2 = 16*floor(size.width*ratioNumerator/ratioDenominator/16);
ERROR("Rounded size: " <<  ret_round1 << "," << ret_round2);

:1014 Rounded size: 507,496
:1016 Changed size: 507 x 216

:964 Returned video size: 640 x 272
:965 Returned AR: 1 / 1
:976 dw: 40 dh: 17
:1014 Rounded size: 640,640
:1016 Changed size: 640 x 272


={============================================================================
*kt_dev_bcast_200* mpeg: streaming: terms

o RTMP
Adobe's RTMP-based Dynamic Streaming uses Adobe's proprietary Real Time Messaging Protocol (RTMP),

o HLS
Apple HTTP Live Streaming (HLS).

What is MPEG DASH?
http://www.streamingmedia.com/Articles/Editorial/What-Is-.../What-is-MPEG-DASH-79041.aspx


={============================================================================
*kt_dev_bcast_201* mpeg: streaming: dash

MPEG DASH (Dynamic Adaptive Streaming over HTTP) is a developing ISO Standard
(ISO/IEC 23009-1)

Adaptive streaming involves producing several instances of a live or on-demand
source file and making them available to various clients depending upon their
delivery bandwidth and CPU processing power. By monitoring CPU utilization
and/or buffer status, adaptive streaming technologies can change streams when
necessary to ensure continuous playback or to improve the experience.


{media-presentation-description-data-model}

http://www.streamingmedia.com/Articles/Editorial/What-Is-.../What-is-MPEG-DASH-79041.aspx

Figure 1. The Media Presentation Data Model. Taken from MPEG-DASH presentation
at Streaming Media West, 2011.

For DASH, the actual A/V streams are called the Media Presentation, while the
manifest file is called the Media Presentation Description.

The media presentation, <mpd> defines the video sequence with one or more
consecutive <periods> that break up the video from start to finish. Each period
contains multiple <adaptation-sets> that contain the content that comprises the
audio/video experience. This content can be muxed, in which case there might be
one adaptation set, or represented in elementary streams, as shown in Figure 1,
    enabling features like multiple language support for audio. 

Each adaptation set contains multiple <representations>, each a single stream in
the adaptive streaming experience. In the figure, Representation 1 is
640x480@500Kbps, while Representation 2 is 640x480@250Kbps.

Each representation is divided into <media-segments>, essentially the chunks of
data that all HTTP-based adaptive streaming technologies use. Data chunks can be
presented in discrete files, as in HLS, or as byte ranges in a single media
file. Presentation in a single file helps improve file administration and
caching efficiency as compared to chunked technologies that can create hundreds
of thousands of files for a single audio/video event.

note: there are five components in the model.

<MPD>
The DASH manifest file, called the Media Presentation Description, is an XML
file that identifies the various content components and the location of all
alternative streams. This enables the DASH player to identify and start playback
of the initial segments, switch between representations as necessary to adapt to
changing CPU and buffer status, and change adaptation sets to respond to user
input, like enabling/disabling subtitles or changing languages.


{features}
Other attributes of DASH include:

o DASH is codec-independent, and will work with H.264, WebM and other codecs

o DASH supports both the ISO Base Media File Format (essentially the MP4 format)
    and MPEG-2 transport streams

o DASH does not specify a DRM method but supports all DRM techniques specified
in ISO/IEC 23001-7: Common Encryption

o DASH supports trick modes for seeking, fast forwards and rewind

o DASH supports advertising insertion


{spec}
http://mpeg.chiariglione.org/


={============================================================================
*kt_dev_bcast_202* mpeg: streaming: mpd file

<?xml version="1.0" encoding="UTF-8"?>
<MPD type="dynamic" xmlns="urn:mpeg:dash:schema:mpd:2011" profiles="urn:mpeg:dash:profile:isoff-live:2011,urn:dvb:dash:profile:dvbdash:2014" minBufferTime="PT1.11S" minimumUpdatePeriod="PT1H" timeShiftBufferDepth="PT35M" availabilityStartTime="2014-08-06T11:00:00Z">
<!-- MPEG DASH ISO BMFF test stream with avc3 -->
<!-- BBC Research & Development -->
<!-- For more information see http://rdmedia.bbc.co.uk -->
<ProgramInformation>
	<Title>Adaptive Bitrate Test Stream from BBC Research and Development - Full stream with separate initialisation segments</Title>
	<Source>BBC Research and Development</Source>
</ProgramInformation>
<UTCTiming schemeIdUri="urn:mpeg:dash:utc:http-xsdate:2014" value="http://time.akamai.com/?iso"/>
<Period start="PT0S">
	<AdaptationSet startWithSAP="2" segmentAlignment="true" id="1" scanType="progressive" mimeType="video/mp4" contentType="video">
		<Role schemeIdUri="urn:mpeg:dash:role:2011" value="main"/>
		<SegmentTemplate startNumber="1" timescale="1000" duration="3840" media="$RepresentationID$/$Number$.m4s" initialization="$RepresentationID$/IS.mp4"/>
		<Representation id="V1" codecs="avc3.4d4015" height="288" width="512" bandwidth="356296" />
		<Representation id="V2" codecs="avc3.4d401e" height="396" width="704" bandwidth="619088" />
		<Representation id="V3" codecs="avc3.64001f" height="504" width="896" bandwidth="1330608" />
		<Representation id="V4" codecs="avc3.640020" height="720" width="1280" bandwidth="2501216" />
		<Representation id="V5" codecs="avc3.640028" height="1080" width="1920" bandwidth="4487408" />
	</AdaptationSet>
	<AdaptationSet startWithSAP="2" segmentAlignment="true" id="2" audioSamplingRate="48000" lang="eng" mimeType="audio/mp4" contentType="audio">
		<AudioChannelConfiguration schemeIdUri="urn:mpeg:dash:23003:3:audio_channel_configuration:2011" value="2"/>
		<Role schemeIdUri="urn:mpeg:dash:role:2011" value="main"/>
		<SegmentTemplate startNumber="1" timescale="1000" duration="3840" media="$RepresentationID$/$Number$.m4s" initialization="$RepresentationID$/IS.mp4"/>
		<Representation id="A1" codecs="mp4a.40.2" bandwidth="95792" />
	</AdaptationSet>
</Period>
</MPD>


<ex>

<?xml version="1.0" encoding="UTF-8"?><MPD
xmlns="urn:mpeg:dash:schema:mpd:2011"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
mediaPresentationDuration="PT59.136S" minBufferTime="PT10S"
profiles="urn:mpeg:dash:profile:isoff-on-demand:2011" type="static"
xsi:schemaLocation="urn:mpeg:dash:schema:mpd:2011
http://standards.iso.org/ittf/PubliclyAvailableStandards/MPEG-DASH_schema_files/DASH-MPD.xsd">

  <Period>

    <AdaptationSet contentType="audio" group="1" lang="en"
    mimeType="audio/mp4" segmentAlignment="true" subsegmentAlignment="true"
    subsegmentStartsWithSAP="1">

      // note: see samping rate and codec
      <Representation audioSamplingRate="24000" bandwidth="32000" codecs="mp4a.40.5" id="audio_eng=32000">

        <AudioChannelConfiguration schemeIdUri="urn:mpeg:dash:23003:3:audio_channel_configuration:2011" value="2">

        </AudioChannelConfiguration>
        <BaseURL>c277aec7-53d2-4d22-bca3-6247c34b876d_audio_1.mp4</BaseURL>
        
            <SegmentList duration="48022" timescale="24000"><Initialization range="0-658"/>
               <SegmentURL d="48128" mediaRange="1051-9023"/>
               <SegmentURL d="48128" mediaRange="9024-17007"/>
               <SegmentURL d="48128" mediaRange="17008-24993"/>
               <SegmentURL d="48128" mediaRange="24994-32992"/>
               <SegmentURL d="48128" mediaRange="32993-40964"/>
               <SegmentURL d="48128" mediaRange="40965-48965"/>
               <SegmentURL d="48128" mediaRange="48966-56938"/>
               <SegmentURL d="47104" mediaRange="56939-64766"/>
               <SegmentURL d="48128" mediaRange="64767-72754"/>
               <SegmentURL d="48128" mediaRange="72755-80744"/>
               <SegmentURL d="48128" mediaRange="80745-88747"/>
               <SegmentURL d="48128" mediaRange="88748-96710"/>
               <SegmentURL d="48128" mediaRange="96711-104707"/>
               <SegmentURL d="48128" mediaRange="104708-112686"/>
               <SegmentURL d="48128" mediaRange="112687-120684"/>
               <SegmentURL d="47104" mediaRange="120685-128513"/>
               <SegmentURL d="48128" mediaRange="128514-136495"/>
               <SegmentURL d="48128" mediaRange="136496-144495"/>
               <SegmentURL d="48128" mediaRange="144496-152470"/>
               <SegmentURL d="48128" mediaRange="152471-160459"/>
               <SegmentURL d="48128" mediaRange="160460-168443"/>
               <SegmentURL d="48128" mediaRange="168444-176441"/>
               <SegmentURL d="48128" mediaRange="176442-184426"/>
               <SegmentURL d="47104" mediaRange="184427-192246"/>
               <SegmentURL d="48128" mediaRange="192247-200231"/>
               <SegmentURL d="48128" mediaRange="200232-208219"/>
               <SegmentURL d="48128" mediaRange="208220-216211"/>
               <SegmentURL d="48128" mediaRange="216212-224202"/>
               <SegmentURL d="48128" mediaRange="224203-232187"/>
               <SegmentURL d="26624" mediaRange="232188-236227"/>
         </SegmentList>
      </Representation>

      <Representation audioSamplingRate="48000" bandwidth="128000" codecs="mp4a.40.2" id="audio_eng=128000">
        <AudioChannelConfiguration schemeIdUri="urn:mpeg:dash:23003:3:audio_channel_configuration:2011" value="2">
        </AudioChannelConfiguration>
        <BaseURL>c277aec7-53d2-4d22-bca3-6247c34b876d_audio_3.mp4</BaseURL>

        ...


={============================================================================
*kt_dev_bcast_202* mpeg: streaming: cenc

The Common Encryption Scheme (CENC) specifies standard encryption and key
mapping methods that can be utilized by one or more digital rights and key
management systems (DRM systems) to enable decryption of the same file using
different DRM systems. The scheme operates by defining a common format for the
encryption related metadata necessary to decrypt the protected streams, yet
leaves the details of rights mappings, key acquisition and storage, DRM
compliance rules, etc. up to the DRM system or systems supporting the 'cenc'
scheme.


={============================================================================
*kt_dev_bcast_300* gst-git


http://cgit.freedesktop.org/gstreamer/gstreamer/tree/docs/design

http://cgit.freedesktop.org/gstreamer/

git clone git://anongit.freedesktop.org/gstreamer/modulename

git clone git://anongit.freedesktop.org/gstreamer/gst-plugins-good
git clone git://anongit.freedesktop.org/gstreamer/gst-libav
git clone git://anongit.freedesktop.org/gstreamer/gst-devtools
git clone git://anongit.freedesktop.org/gstreamer/common
git clone git://anongit.freedesktop.org/gstreamer/gst-integration-testsuites


={============================================================================
*kt_dev_bcast_300* gst: tutorial: 02: concept

http://docs.gstreamer.com/display/GstSDK/Basic+tutorials

<ex> basic-tutorial-2.c

#include <gst/gst.h>
  
int main(int argc, char *argv[]) {
  GstElement *pipeline, *source, *sink;
  GstBus *bus;
  GstMessage *msg;
  GstStateChangeReturn ret;

  /* Initialize GStreamer */
  gst_init (&argc, &argv);

  /* Create the elements */
  source = gst_element_factory_make ("videotestsrc", "source");
  sink = gst_element_factory_make ("autovideosink", "sink");

  /* Create the empty pipeline */
  pipeline = gst_pipeline_new ("test-pipeline");

  if (!pipeline || !source || !sink) {
    g_printerr ("Not all elements could be created.\n");
    return -1;
  }

  /* Build the pipeline */
  gst_bin_add_many (GST_BIN (pipeline), source, sink, NULL);
  if (gst_element_link (source, sink) != TRUE) {
    g_printerr ("Elements could not be linked.\n");
    gst_object_unref (pipeline);
    return -1;
  }

  /* Modify the source's properties */
  g_object_set (source, "pattern", 0, NULL);

  /* Start playing */
  ret = gst_element_set_state (pipeline, GST_STATE_PLAYING);
  if (ret == GST_STATE_CHANGE_FAILURE) {
    g_printerr ("Unable to set the pipeline to the playing state.\n");
    gst_object_unref (pipeline);
    return -1;
  }

  /* Wait until error or EOS */
  bus = gst_element_get_bus (pipeline);
  msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE, GST_MESSAGE_ERROR | GST_MESSAGE_EOS);

  /* Parse message */
  if (msg != NULL) {
    GError *err;
    gchar *debug_info;

    switch (GST_MESSAGE_TYPE (msg)) {
      case GST_MESSAGE_ERROR:
        gst_message_parse_error (msg, &err, &debug_info);
        g_printerr ("Error received from element %s: %s\n", GST_OBJECT_NAME (msg->src), err->message);
        g_printerr ("Debugging information: %s\n", debug_info ? debug_info : "none");
        g_clear_error (&err);
        g_free (debug_info);
        break;
      case GST_MESSAGE_EOS:
        g_print ("End-Of-Stream reached.\n");
        break;
      default:
        /* We should not reach here because we only asked for ERRORs and EOS */
        g_printerr ("Unexpected message received.\n");
        break;
    }
    gst_message_unref (msg);
  }

  /* Free resources */
  gst_object_unref (bus);
  gst_element_set_state (pipeline, GST_STATE_NULL);
  gst_object_unref (pipeline);
  return 0;
}


{pipeline}
All elements in GStreamer must typically be contained inside a pipeline before
they can be used, because it takes care of some clocking and messaging
functions.

A pipeline is a particular 'type' of bin, which is the element used to contain
other elements. Therefore all methods which apply to bins also apply to
pipelines. In our case, we call gst_bin_add_many() to add the elements to the
pipeline (mind the cast) which accepts a list of elements to be added, ending
with NULL.

These elements, however, are not 'linked' with each other yet. For this, we need
to use gst_element_link().

<element-and-pipeline>
GStreamer is a framework designed to handle multimedia flows. Media travels from
the "source" elements (the producers), down to the "sink" elements (the
        consumers), passing through a series of intermediate elements performing
all kinds of tasks. The set of all the interconnected elements is called a
"pipeline".

<branch>
If a container embeds multiple streams (one video and two audio tracks, for
        example), the demuxer will separate them and expose them through
different output ports. In this way, different branches can be created in the
pipeline, dealing with different types of data.

<downstream>
The basic construction block of GStreamer are the elements, which process the
data as it flows downstream from the source elements (the producers) to the sink
elements (the consumers), passing through filter elements.

<pad>
The 'ports' through which GStreamer elements communicate with each other are
called pads (GstPad). There exists sink pads, through which data enters an
element, and source pads, through which data exits an element.

A demuxer contains one sink pad, through which the muxed data arrives, and
multiple source pads, one for each stream found in the container:

demux
+===================+
|[sink]     [audio] |
|           [video] |
+===================+


{build-pipeline}

<automatic>
Showed how to build a pipeline automatically. 

/* Initialize GStreamer */
gst_init (&argc, &argv);

/* Build the pipeline */
pipeline = gst_parse_launch ("playbin2 uri=http://docs.gstreamer.com/media/sintel_trailer-480p.webm", NULL);

/* Start playing */
gst_element_set_state (pipeline, GST_STATE_PLAYING);

<gst-parse-launch>
In GStreamer you usually build the pipeline by manually assembling the
individual elements, but, when the pipeline is easy enough, and you do not need
any advanced features, you can take the shortcut: gst_parse_launch().

This function takes a textual representation of a pipeline and turns it into an
actual pipeline, which is very handy. In fact, this function is so handy there
is a tool built completely around it which you will get very acquainted with
(see Basic tutorial 10: GStreamer tools to learn about gst-launch and the
 gst-launch syntax).

GstElement *        
gst_parse_bin_from_description( const gchar *bin_description,
                                gboolean ghost_unlinked_pads,
                                GError **err);

This is a convenience wrapper around gst_parse_launch() to create a GstBin from
a gst-launch-style pipeline description. See gst_parse_launch() and the
gst-launch man page for details about the syntax. 

Ghost pads on the bin for unlinked source or sink pads within the bin can
automatically be created (but only a maximum of one ghost pad for each direction
    will be created; if you expect multiple unlinked source pads or multiple
    unlinked sink pads and want them all ghosted, you will have to create the
    ghost pads yourself).

bin_description :
	command line describing the bin

ghost_unlinked_pads :
   whether to automatically create ghost pads for unlinked source or sink pads
   within the bin

err :
	where to store the error message in case of an error, or NULL

Returns :
	a newly-created bin, or NULL if an error occurred. [transfer full]

<manually>
Now we are going to build a pipeline manually by instantiating each element and
linking them all together. 

/* Create the elements */
source = gst_element_factory_make ("videotestsrc", "source");
sink = gst_element_factory_make ("autovideosink", "sink");

GstElement *
gst_element_factory_make( const gchar *factoryname,
                          const gchar *name);

Create a new element of the type defined by the given element factory. note: If
name is NULL, then the element will receive a guaranteed unique name, consisting
of the element factory name and a number. If name is given, it will be given the
name supplied.


<error-handling>

/* Wait until error or EOS */
bus = gst_element_get_bus (pipeline);
msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE, GST_MESSAGE_ERROR | GST_MESSAGE_EOS);
   
/* Parse message */
if (msg != NULL) {
  GError *err;
  gchar *debug_info;
   
  switch (GST_MESSAGE_TYPE (msg)) {
    case GST_MESSAGE_ERROR:
      gst_message_parse_error (msg, &err, &debug_info);
      g_printerr ("Error received from element %s: %s\n", GST_OBJECT_NAME (msg->src), err->message);
      g_printerr ("Debugging information: %s\n", debug_info ? debug_info : "none");
      g_clear_error (&err);
      g_free (debug_info);
      break;
    case GST_MESSAGE_EOS:
      g_print ("End-Of-Stream reached.\n");
      break;
    default:
      /* We should not reach here because we only asked for ERRORs and EOS */
      g_printerr ("Unexpected message received.\n");
      break;
  }
  gst_message_unref (msg);
}

Can have while on message:

/* Listen to the bus */
bus = gst_element_get_bus (data.pipeline);
do {
    msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE,
            GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR | GST_MESSAGE_EOS);
    ...
} while (!terminate);


{bus-and-message}
GStreamer bus is the object responsible for delivering to the application the
GstMessages generated by the elements, in 'order' and to the application
'thread'. This last point is important, because the actual streaming of media
is done in another thread than the application.

Messages can be extracted from the bus 'synchronously' with
gst_bus_timed_pop_filtered() and its siblings, or 'asynchronously', using
signals. Your application should always keep an eye on the bus to be notified of
errors and other playback-related issues.


={============================================================================
*kt_dev_bcast_300* gst: tutorial: 03

Basic tutorial 3: Dynamic pipelines

This tutorial shows the rest of the basic concepts required to use GStreamer,
which allow building the pipeline "on the fly", as information becomes
  available, instead of having a monolithic pipeline defined at the beginning of
  your application.

As you are about to see, the pipeline in this tutorial is not completely built
before it is set to the playing state.

The main complexity when dealing with demuxers is that they cannot produce any
information until they have received some data and have had a chance to look at
the container to see what is inside. This is, demuxers start with no source pads
to which other elements can link, and thus the pipeline must necessarily
terminate at them.

The solution is to build the pipeline from the source down to the demuxer, and
set it to run (play). When the demuxer has received enough information to know
about the number and kind of streams in the container, it will start creating
source pads. This is the right time for us to finish building the pipeline and
attach it to the newly added demuxer pads.

<ex> basic-tutorial-3.c

#include <gst/gst.h>
   
/* Structure to contain all our information, so we can pass it to callbacks */
typedef struct _CustomData {
  GstElement *pipeline;
  GstElement *source;
  GstElement *convert;
  GstElement *sink;
} CustomData;
   
/* Handler for the pad-added signal */
static void pad_added_handler (GstElement *src, GstPad *pad, CustomData *data);
   
int main(int argc, char *argv[]) {
  CustomData data;
  GstBus *bus;
  GstMessage *msg;
  GstStateChangeReturn ret;
  gboolean terminate = FALSE;
   
  /* Initialize GStreamer */
  gst_init (&argc, &argv);
    
  /* Create the elements */

  // We create the elements as usual. uridecodebin will internally instantiate
  // all the necessary elements (sources, demuxers and decoders) to turn a URI
  // into raw audio and/or video streams. Since it contains demuxers, its source
  // pads are not initially available and we will need to link to them on the
  // fly.

  data.source = gst_element_factory_make ("uridecodebin", "source");
  data.convert = gst_element_factory_make ("audioconvert", "convert");
  data.sink = gst_element_factory_make ("autoaudiosink", "sink");
   
  /* Create the empty pipeline */
  data.pipeline = gst_pipeline_new ("test-pipeline");
    
  if (!data.pipeline || !data.source || !data.convert || !data.sink) {
    g_printerr ("Not all elements could be created.\n");
    return -1;
  }
   
  /* Build the pipeline. Note that we are NOT linking the source at this
   * point. We will do it later. */

  // Here we link the converter element to the sink, but we DO NOT link them
  // with the source, since at this point it contains no source pads. We just
  // leave this branch (converter + sink) unlinked, until later on.

  gst_bin_add_many (GST_BIN (data.pipeline),
      data.source, data.convert , data.sink, NULL);

  if (!gst_element_link (data.convert, data.sink)) {
    g_printerr ("Elements could not be linked.\n");
    gst_object_unref (data.pipeline);
    return -1;
  }
   
  /* Set the URI to play */
  g_object_set (data.source, "uri",
      "http://docs.gstreamer.com/media/sintel_trailer-480p.webm", NULL);
   
  /* Connect to the pad-added signal */
  g_signal_connect (data.source, "pad-added", G_CALLBACK (pad_added_handler),
      &data);
   
  /* Start playing */
  ret = gst_element_set_state (data.pipeline, GST_STATE_PLAYING);
  if (ret == GST_STATE_CHANGE_FAILURE) {
    g_printerr ("Unable to set the pipeline to the playing state.\n");
    gst_object_unref (data.pipeline);
    return -1;
  }
   
  /* Listen to the bus */
  bus = gst_element_get_bus (data.pipeline);
  do {
    msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE,
        GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR | GST_MESSAGE_EOS);
   
    /* Parse message */
    if (msg != NULL) {
      GError *err;
      gchar *debug_info;
       
      switch (GST_MESSAGE_TYPE (msg)) {
        case GST_MESSAGE_ERROR:
          gst_message_parse_error (msg, &err, &debug_info);
          g_printerr ("Error received from element %s: %s\n",
              GST_OBJECT_NAME (msg->src), err->message);
          g_printerr ("Debugging information: %s\n",
              debug_info ? debug_info : "none");
          g_clear_error (&err);
          g_free (debug_info);
          terminate = TRUE;
          break;

        case GST_MESSAGE_EOS:
          g_print ("End-Of-Stream reached.\n");
          terminate = TRUE;
          break;

        case GST_MESSAGE_STATE_CHANGED:
          /* We are only interested in state-changed messages from the pipeline */
          if (GST_MESSAGE_SRC (msg) == GST_OBJECT (data.pipeline)) {
            GstState old_state, new_state, pending_state;
            gst_message_parse_state_changed (msg, &old_state, &new_state, &pending_state);
            g_print ("Pipeline state changed from %s to %s:\n",
                gst_element_state_get_name (old_state), gst_element_state_get_name (new_state));
          }
          break;

        default:
          /* We should not reach here */
          g_printerr ("Unexpected message received.\n");
          break;
      }
      gst_message_unref (msg);
    }
  } while (!terminate);
   
  /* Free resources */
  gst_object_unref (bus);
  gst_element_set_state (data.pipeline, GST_STATE_NULL);
  gst_object_unref (data.pipeline);
  return 0;
}
   
/* This function will be called by the pad-added signal */
static 
void pad_added_handler (GstElement *src, GstPad *new_pad, CustomData *data) {
  GstPad *sink_pad = gst_element_get_static_pad (data->convert, "sink");
  GstPadLinkReturn ret;
  GstCaps *new_pad_caps = NULL;
  GstStructure *new_pad_struct = NULL;
  const gchar *new_pad_type = NULL;
   
  g_print ("Received new pad '%s' from '%s':\n",
      GST_PAD_NAME (new_pad), GST_ELEMENT_NAME (src));
   
  /* If our converter is already linked, we have nothing to do here */
  if (gst_pad_is_linked (sink_pad)) {
    g_print ("  We are already linked. Ignoring.\n");
    goto exit;
  }
   
  /* Check the new pad's type */
  new_pad_caps = gst_pad_get_caps (new_pad);
  new_pad_struct = gst_caps_get_structure (new_pad_caps, 0);
  new_pad_type = gst_structure_get_name (new_pad_struct);
  if (!g_str_has_prefix (new_pad_type, "audio/x-raw")) {
    g_print ("  It has type '%s' which is not raw audio. Ignoring.\n",
        new_pad_type);
    goto exit;
  }
   
  /* Attempt the link */
  ret = gst_pad_link (new_pad, sink_pad);
  if (GST_PAD_LINK_FAILED (ret)) {
    g_print ("  Type is '%s' but link failed.\n", new_pad_type);
  } else {
    g_print ("  Link succeeded (type '%s').\n", new_pad_type);
  }
   
exit:
  /* Unreference the new pad's caps, if we got them */
  if (new_pad_caps != NULL)
    gst_caps_unref (new_pad_caps);
   
  /* Unreference the sink pad */
  gst_object_unref (sink_pad);
}


<signal>
GSignals are a crucial point in GStreamer. They allow you to be notified by
means of a 'callback' when something interesting has happened. Signals are
identified by a 'name', and each GObject has its own signals.

/* Connect to the pad-added signal */
g_signal_connect (data.source, "pad-added", 
    G_CALLBACK (pad_added_handler), &data);

In this line, we are attaching to the "pad-added" signal of our source
uridecodebin element. To do so, use g_signal_connect() and provide the callback
function to be used and a data pointer. GStreamer does nothing with this data
pointer, it just forwards it to the callback so we can share information with
it. In this case, we pass a pointer to the CustomData structure we built
specially for this purpose.

When source element finally has enough information to start producing data, it
will create source pads, and trigger the "pad-added" signal. At this point our
callback will be called:


={============================================================================
*kt_dev_bcast_300* gst: tutorial: 04: seeking

Basic tutorial 4: Time management

GstQuery is a mechanism that allows asking an element or pad for a piece of
information. In this example we ask the pipeline if seeking is allowed (some
    sources, like live streams, do not allow seeking). If it is allowed, then,
once the movie has been running for ten seconds, we skip to a different position
  using a seek.

In the previous tutorials, once we had the pipeline setup and running, our main
function just sat and waited to receive an ERROR or an EOS through the bus. Here
we modify this function to periodically wake up and query the pipeline for the
stream position, so we can print it on screen. This is similar to what a media
player would do, updating the User Interface on a periodic basis.

Finally, the stream duration is queried and updated whenever it changes.  Here
we modify this function to periodically wake up and query the pipeline for the
stream position, so we can print it on screen. This is similar to what a media
player would do, 'updating' the User Interface on a periodic basis.


<ex> basic-tutorial-4.c

#include <gst/gst.h>
   
/* Structure to contain all our information, so we can pass it around */
typedef struct _CustomData {
  GstElement *playbin2;  /* Our one and only element */
  gboolean playing;      /* Are we in the PLAYING state? */
  gboolean terminate;    /* Should we terminate execution? */
  gboolean seek_enabled; /* Is seeking enabled for this media? */
  gboolean seek_done;    /* Have we performed the seek already? */
  gint64 duration;       /* How long does this media last, in nanoseconds */
} CustomData;
   
/* Forward definition of the message processing function */
static void handle_message (CustomData *data, GstMessage *msg);
   
int main(int argc, char *argv[]) {
  CustomData data;
  GstBus *bus;
  GstMessage *msg;
  GstStateChangeReturn ret;
   
  data.playing = FALSE;
  data.terminate = FALSE;
  data.seek_enabled = FALSE;
  data.seek_done = FALSE;
  data.duration = GST_CLOCK_TIME_NONE;
   
  /* Initialize GStreamer */
  gst_init (&argc, &argv);
    
  /* Create the elements */

  // However, playbin2 is in itself a pipeline, and in this case it is the only
  // element in the pipeline, so we use directly the playbin2 element.

  data.playbin2 = gst_element_factory_make ("playbin2", "playbin2");
   
  if (!data.playbin2) {
    g_printerr ("Not all elements could be created.\n");
    return -1;
  }
   
  /* Set the URI to play */
  g_object_set (data.playbin2, "uri",
      "http://docs.gstreamer.com/media/sintel_trailer-480p.webm", NULL);
   
  /* Start playing */
  ret = gst_element_set_state (data.playbin2, GST_STATE_PLAYING);
  if (ret == GST_STATE_CHANGE_FAILURE) {
    g_printerr ("Unable to set the pipeline to the playing state.\n");
    gst_object_unref (data.playbin2);
    return -1;
  }
   
  /* Listen to the bus */
  bus = gst_element_get_bus (data.playbin2);
  do {

    // Previously we did not provide a timeout to gst_bus_timed_pop_filtered(),
    // meaning that it didn't return until a message was received. 
    //
    // msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE, ... );
    //
    // Now we use a timeout of 100 milliseconds, so, if no message is received,
    // 10 times per second the function will return with a NULL instead of a
    // GstMessage.

    msg = gst_bus_timed_pop_filtered (bus, 100 * GST_MSECOND,
        GST_MESSAGE_STATE_CHANGED | GST_MESSAGE_ERROR |
        GST_MESSAGE_EOS | GST_MESSAGE_DURATION);
   
    /* Parse message */
    if (msg != NULL) {
      handle_message (&data, msg);
    } else {
      /* We got 'no' message, this means the timeout expired */

      // We get here approximately 10 times per second, a good enough refresh
      // rate for our UI. We are going to print on screen the current media
      // position, which we can learn be querying the pipeline.
          
      if (data.playing) {
        GstFormat fmt = GST_FORMAT_TIME;
        gint64 current = -1;
         
        /* Query the current position of the stream */
        if (!gst_element_query_position (data.playbin2, &fmt, &current)) {
          g_printerr ("Could not query current position.\n");
        }
         
        /* If we didn't know it yet, query the stream duration */
        if (!GST_CLOCK_TIME_IS_VALID (data.duration)) {
          if (!gst_element_query_duration (data.playbin2, &fmt, &data.duration)) {
            g_printerr ("Could not query current duration.\n");
          }
        }
         
        /* Print current position and total duration */
        g_print ("Position %" GST_TIME_FORMAT " / %" GST_TIME_FORMAT "\r",
            GST_TIME_ARGS (current), GST_TIME_ARGS (data.duration));
         
        /* If seeking is enabled, we have not done it yet,
         * and the time is right, 'seek' 
         */
        if (data.seek_enabled && !data.seek_done && current > 10 * GST_SECOND) {
          g_print ("\nReached 10s, performing seek...\n");

          gst_element_seek_simple (data.playbin2, GST_FORMAT_TIME,
              GST_SEEK_FLAG_FLUSH | GST_SEEK_FLAG_KEY_UNIT, 30 * GST_SECOND);

          data.seek_done = TRUE;
        }
      } // end if 
    }
  } while (!data.terminate);
   
  /* Free resources */
  gst_object_unref (bus);
  gst_element_set_state (data.playbin2, GST_STATE_NULL);
  gst_object_unref (data.playbin2);
  return 0;
}
   
static void handle_message (CustomData *data, GstMessage *msg) {
  GError *err;
  gchar *debug_info;
   
  switch (GST_MESSAGE_TYPE (msg)) {
    case GST_MESSAGE_ERROR:
      gst_message_parse_error (msg, &err, &debug_info);
      g_printerr ("Error received from element %s: %s\n",
          GST_OBJECT_NAME (msg->src), err->message);
      g_printerr ("Debugging information: %s\n",
          debug_info ? debug_info : "none");
      g_clear_error (&err);
      g_free (debug_info);
      data->terminate = TRUE;
      break;
    case GST_MESSAGE_EOS:
      g_print ("End-Of-Stream reached.\n");
      data->terminate = TRUE;
      break;

    case GST_MESSAGE_DURATION:
      /* The duration has changed, mark the current one as invalid */
      data->duration = GST_CLOCK_TIME_NONE;
      break;

    case GST_MESSAGE_STATE_CHANGED: {
      GstState old_state, new_state, pending_state;
      gst_message_parse_state_changed (msg, &old_state, &new_state,
          &pending_state);
      if (GST_MESSAGE_SRC (msg) == GST_OBJECT (data->playbin2)) {
        g_print ("Pipeline state changed from %s to %s:\n",
            gst_element_state_get_name (old_state), 
            gst_element_state_get_name (new_state));
         
        /* Remember whether we are in the PLAYING state or not */
        data->playing = (new_state == GST_STATE_PLAYING);
         
        // Seeks and time queries work 'better' when in the PAUSED or PLAYING
        // state, since all elements have had a chance to receive information
        // and configure themselves. Here we take note of whether we are in the
        // PLAYING state or not with the playing variable.

        if (data->playing) {
          /* We just moved to PLAYING. Check if seeking is possible */
          GstQuery *query;
          gint64 start, end;

          // gst_query_new_seeking() creates a new query object of the "seeking"
          // type, with GST_FORMAT_TIME format. This indicates that we are
          // interested in seeking by specifying the new time to which we want
          // to move. We could also ask for GST_FORMAT_BYTES, and then seek to a
          // particular byte position inside the source file, but this is
          // normally less useful.
          //
          // This query object is then passed to the pipeline with
          // gst_element_query(). The result is stored in the same query, and
          // can be easily retrieved with gst_query_parse_seeking(). It extracts
          // a boolean indicating if seeking is allowed, and the range in which
          // seeking is possible.

          query = gst_query_new_seeking (GST_FORMAT_TIME);
          if (gst_element_query (data->playbin2, query)) {

            gst_query_parse_seeking (query, NULL,
                &data->seek_enabled, &start, &end);

            if (data->seek_enabled) {
              g_print ("Seeking is ENABLED from %" GST_TIME_FORMAT
                  " to %" GST_TIME_FORMAT "\n",
                  GST_TIME_ARGS (start), GST_TIME_ARGS (end));
            } else {
              g_print ("Seeking is DISABLED for this stream.\n");
            }

          }
          else {
            g_printerr ("Seeking query failed.");
          }
          gst_query_unref (query);
        }

      }
    } break;
    default:
      /* We should not reach here */
      g_printerr ("Unexpected message received.\n");
      break;
  }
  gst_message_unref (msg);
}


Now we perform the seek, “simply” by calling gst_element_seek_simple() on the
pipeline. A lot of the intricacies of seeking are hidden in this method, which
is a good thing!

GST_FORMAT_TIME indicates that we are specifying the destination in time, as
opposite to bytes (and other more obscure mechanisms).

Then come the GstSeekFlags, let's review the most common:

GST_SEEK_FLAG_FLUSH: This discards all data currently in the pipeline before
doing the seek. Might pause a bit while the pipeline is refilled and the new
data starts to show up, but greatly increases the “responsiveness” of the
application. If this flag is not provided, “stale” data might be shown for a
while until the new position appears at the end of the pipeline.

GST_SEEK_FLAG_KEY_UNIT: Most encoded video streams cannot seek to arbitrary
positions, only to certain frames called Key Frames. When this flag is used, the
seek will actually move to the closest key frame and start producing data
straight away. If this flag is not used, the pipeline will move internally to
the closest key frame (it has no other alternative) but data will not be shown
until it reaches the requested position. Not providing the flag is more
accurate, but might take longer to react.

GST_SEEK_FLAG_ACCURATE: Some media clips do not provide enough indexing
information, meaning that seeking to arbitrary positions is time-consuming. In
these cases, GStreamer usually estimates the position to seek to, and usually
works just fine. If this precision is not good enough for your case (you see
    seeks not going to the exact time you asked for), then provide this flag. Be
warned that it might take longer to calculate the seeking position (very long,
    on some files).

And finally we provide the position to seek to. Since we asked for
GST_FORMAT_TIME, this position is in nanoseconds, so we use the GST_SECOND macro
for simplicity.


={============================================================================
*kt_dev_bcast_300* gst: tutorial: 06: pad cap negotiation

Through a process known as negotiation, two linked Pads agree on a common type,
and thus the Capabilities of the Pads become fixed. they only have one type and
do not contain ranges.

As the process evolves, actual Pads are instantiated and their Capabilities
  refined until they are fixed (or negotiation fails).

This tutorial instantiates two elements, shows their Pad Templates, links them
and sets the pipeline to play. On each state change, the Capabilities of the
sink element's Pad are shown, so you can observe how the negotiation proceeds
until the Pad Caps are fixed.


<ex> basic-tutorial-6.c

#include <gst/gst.h>
   
/* Functions below print the Capabilities in a human-friendly format */
static gboolean print_field (GQuark field, const GValue * value, gpointer pfx) {
  gchar *str = gst_value_serialize (value);
   
  g_print ("%s  %15s: %s\n", (gchar *) pfx, g_quark_to_string (field), str);
  g_free (str);
  return TRUE;
}
   
static void print_caps (const GstCaps * caps, const gchar * pfx) {
  guint i;
   
  g_return_if_fail (caps != NULL);
   
  if (gst_caps_is_any (caps)) {
    g_print ("%sANY\n", pfx);
    return;
  }
  if (gst_caps_is_empty (caps)) {
    g_print ("%sEMPTY\n", pfx);
    return;
  }
   
  for (i = 0; i < gst_caps_get_size (caps); i++) {
    GstStructure *structure = gst_caps_get_structure (caps, i);
     
    g_print ("%s%s\n", pfx, gst_structure_get_name (structure));
    gst_structure_foreach (structure, print_field, (gpointer) pfx);
  }
}
   
/* Prints information about a Pad Template, including its Capabilities */
static void print_pad_templates_information (GstElementFactory * factory) {
  const GList *pads;
  GstStaticPadTemplate *padtemplate;
   
  g_print ("Pad Templates for %s:\n", gst_element_factory_get_longname (factory));
  if (!factory->numpadtemplates) {
    g_print ("  none\n");
    return;
  }
   
  pads = factory->staticpadtemplates;
  while (pads) {
    padtemplate = (GstStaticPadTemplate *) (pads->data);
    pads = g_list_next (pads);
     
    if (padtemplate->direction == GST_PAD_SRC)
      g_print ("  SRC template: '%s'\n", padtemplate->name_template);
    else if (padtemplate->direction == GST_PAD_SINK)
      g_print ("  SINK template: '%s'\n", padtemplate->name_template);
    else
      g_print ("  UNKNOWN!!! template: '%s'\n", padtemplate->name_template);
     
    if (padtemplate->presence == GST_PAD_ALWAYS)
      g_print ("    Availability: Always\n");
    else if (padtemplate->presence == GST_PAD_SOMETIMES)
      g_print ("    Availability: Sometimes\n");
    else if (padtemplate->presence == GST_PAD_REQUEST) {
      g_print ("    Availability: On request\n");
    } else
      g_print ("    Availability: UNKNOWN!!!\n");
     
    if (padtemplate->static_caps.string) {
      g_print ("    Capabilities:\n");
      print_caps (gst_static_caps_get (&padtemplate->static_caps), "      ");
    }
     
    g_print ("\n");
  }
}
   
/* Shows the CURRENT capabilities of the requested pad in the given element */
static void print_pad_capabilities (GstElement *element, gchar *pad_name) {
  GstPad *pad = NULL;
  GstCaps *caps = NULL;
   
  /* Retrieve pad */
  pad = gst_element_get_static_pad (element, pad_name);
  if (!pad) {
    g_printerr ("Could not retrieve pad '%s'\n", pad_name);
    return;
  }
   
  /* Retrieve negotiated caps 
   * (or acceptable caps if negotiation is not finished yet) 
   */
  caps = gst_pad_get_negotiated_caps (pad);
  if (!caps)
    caps = gst_pad_get_caps_reffed (pad);
   
  /* Print and free */
  g_print ("Caps for the %s pad:\n", pad_name);
  print_caps (caps, "      ");
  gst_caps_unref (caps);
  gst_object_unref (pad);
}
   
int main(int argc, char *argv[]) {
  GstElement *pipeline, *source, *sink;
  GstElementFactory *source_factory, *sink_factory;
  GstBus *bus;
  GstMessage *msg;
  GstStateChangeReturn ret;
  gboolean terminate = FALSE;
   
  /* Initialize GStreamer */
  gst_init (&argc, &argv);
    
  // gst_element_factory_make() is really a shortcut for
  // gst_element_factory_find() + gst_element_factory_create().

  /* Create the element factories */
  source_factory = gst_element_factory_find ("audiotestsrc");
  sink_factory = gst_element_factory_find ("autoaudiosink");
  if (!source_factory || !sink_factory) {
    g_printerr ("Not all element factories could be created.\n");
    return -1;
  }
   
  /* Print information about the pad templates of these factories */
  print_pad_templates_information (source_factory);
  print_pad_templates_information (sink_factory);
   
  /* Ask the factories to instantiate actual elements */
  source = gst_element_factory_create (source_factory, "source");
  sink = gst_element_factory_create (sink_factory, "sink");
   
  /* Create the empty pipeline */
  pipeline = gst_pipeline_new ("test-pipeline");
   
  if (!pipeline || !source || !sink) {
    g_printerr ("Not all elements could be created.\n");
    return -1;
  }
   
  /* Build the pipeline */
  gst_bin_add_many (GST_BIN (pipeline), source, sink, NULL);
  if (gst_element_link (source, sink) != TRUE) {
    g_printerr ("Elements could not be linked.\n");
    gst_object_unref (pipeline);
    return -1;
  }
   
  /* Print initial negotiated caps (in NULL state) */
  g_print ("In NULL state:\n");
  print_pad_capabilities (sink, "sink");
   
  /* Start playing */
  ret = gst_element_set_state (pipeline, GST_STATE_PLAYING);
  if (ret == GST_STATE_CHANGE_FAILURE) {
    g_printerr ("Unable to set the pipeline to the playing state.\n");
  }
   
  /* Wait until error, EOS or State Change */
  bus = gst_element_get_bus (pipeline);
  do {
    msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE,
        GST_MESSAGE_ERROR | GST_MESSAGE_EOS |
        GST_MESSAGE_STATE_CHANGED);
   
    /* Parse message */
    if (msg != NULL) {
      GError *err;
      gchar *debug_info;
     
      switch (GST_MESSAGE_TYPE (msg)) {
        case GST_MESSAGE_ERROR:
          gst_message_parse_error (msg, &err, &debug_info);
          g_printerr ("Error received from element %s: %s\n",
              GST_OBJECT_NAME (msg->src), err->message);
          g_printerr ("Debugging information: %s\n",
              debug_info ? debug_info : "none");
          g_clear_error (&err);
          g_free (debug_info);
          terminate = TRUE;
          break;
        case GST_MESSAGE_EOS:
          g_print ("End-Of-Stream reached.\n");
          terminate = TRUE;
          break;

        case GST_MESSAGE_STATE_CHANGED:
          /* We are only interested in state-changed messages from the pipeline */
          if (GST_MESSAGE_SRC (msg) == GST_OBJECT (pipeline)) {
            GstState old_state, new_state, pending_state;
            gst_message_parse_state_changed (msg, &old_state, &new_state,
                &pending_state);
            g_print ("\nPipeline state changed from %s to %s:\n",
                gst_element_state_get_name (old_state),
                gst_element_state_get_name (new_state));

            /* Print the current capabilities of the sink element */
            print_pad_capabilities (sink, "sink");
          }
          break;

        default:
          /* We should not reach here because we 'only' asked for ERRORs, EOS
           * and STATE_CHANGED */
          g_printerr ("Unexpected message received.\n");
          break;
      }
      gst_message_unref (msg);
    }
  } while (!terminate);
   
  /* Free resources */
  gst_object_unref (bus);
  gst_element_set_state (pipeline, GST_STATE_NULL);
  gst_object_unref (pipeline);
  gst_object_unref (source_factory);
  gst_object_unref (sink_factory);
  return 0;
}


={============================================================================
*kt_dev_bcast_300* gst: tutorial: 07: queue

Basic tutorial 7: Multithreading and Pad Availability

GStreamer handles multithreading automatically but under some circumstances, you
might need to decouple threads manually. This tutorial shows how to do this.

GStreamer is a multithreaded framework. This means that, internally, it creates
and destroys threads as it needs them, for example, to decouple streaming from
the application thread. 

Moreover, 'plugins' are also free to create threads for their own processing,
for example, a video decoder could create 4 threads to take full advantage of a
    CPU with 4 cores.


{queue-and-branch}
An application can specify explicitly that a 'branch' (a part of the pipeline)
runs on a different 'thread'. for example, to have the audio and video decoders
executing simultaneously.

This is accomplished using the queue element, which works as follows. The sink
pad just enqueues data and returns control. On a different thread, data is
dequeued and pushed downstream. This element is also used for 'buffering', as
seen later in the streaming tutorials. The size of the queue can be controlled
through properties.

app source(thread1)    tee                     queue(thread2)       audio 
+=================+    +=================+     +=================+  resample
|           [src] | -> |[sink]     [src] | ->  |[sink]     [src] |  [sink] ->
|                 |    |           [src] |     |                 |
+=================+    +=================+     +=================+

                                               queue(thread3)       video 
                                               +=================+  convert
                                               |[sink]     [src] |  [sink] ->
                                               |                 |
                                               +=================+ 

As queues create a new thread, so this pipeline runs in 3 threads. Pipelines
with more than one sink usually need to be multithreaded because, to be
'synchronized', sinks usually 'block' execution until 'all' other sinks are
ready, and they cannot get ready if there is only one thread, being blocked by
the first sink.

<ex> basic-tutorial-7.c

#include <gst/gst.h>
   
int main(int argc, char *argv[]) {
  GstElement *pipeline, *audio_source, *tee, 
             *audio_queue, *audio_convert, *audio_resample, *audio_sink;
  GstElement *video_queue, *visual, *video_convert, *video_sink;
  GstBus *bus;
  GstMessage *msg;
  GstPadTemplate *tee_src_pad_template;
  GstPad *tee_audio_pad, *tee_video_pad;
  GstPad *queue_audio_pad, *queue_video_pad;
   
  /* Initialize GStreamer */
  gst_init (&argc, &argv);
   
  /* Create the elements */
  audio_source = gst_element_factory_make ("audiotestsrc", "audio_source");
  tee = gst_element_factory_make ("tee", "tee");
  audio_queue = gst_element_factory_make ("queue", "audio_queue");
  audio_convert = gst_element_factory_make ("audioconvert", "audio_convert");
  audio_resample = gst_element_factory_make ("audioresample", "audio_resample");
  audio_sink = gst_element_factory_make ("autoaudiosink", "audio_sink");
  video_queue = gst_element_factory_make ("queue", "video_queue");
  visual = gst_element_factory_make ("wavescope", "visual");
  video_convert = gst_element_factory_make ("ffmpegcolorspace", "csp");
  video_sink = gst_element_factory_make ("autovideosink", "video_sink");
   
  /* Create the empty pipeline */
  pipeline = gst_pipeline_new ("test-pipeline");
   
  if (!pipeline || !audio_source || !tee 
      || !audio_queue || !audio_convert || !audio_resample || !audio_sink ||
      !video_queue || !visual || !video_convert || !video_sink) {
    g_printerr ("Not all elements could be created.\n");
    return -1;
  }
   
  /* Configure elements */
  g_object_set (audio_source, "freq", 215.0f, NULL);
  g_object_set (visual, "shader", 0, "style", 1, NULL);
   
  /* Link all elements that can be automatically linked because they have
   * "Always" pads
   * /


  gst_bin_add_many (GST_BIN (pipeline), audio_source, tee, audio_queue,
      audio_convert, audio_resample, audio_sink,
      video_queue, visual, video_convert, video_sink, NULL);

  if (gst_element_link_many (audio_source, tee, NULL) != TRUE ||
      gst_element_link_many (audio_queue, audio_convert,
        audio_resample, audio_sink, NULL) != TRUE ||
      gst_element_link_many (video_queue, visual, video_convert,
        video_sink, NULL) != TRUE) {
    g_printerr ("Elements could not be linked.\n");
    gst_object_unref (pipeline);
    return -1;
  }
   
  // note: pad-link
  // Linking elements is one thing and linking pads is the other once linking
  // element is done.

  /* Manually link the Tee, which has "Request" pads */

  // To link Request Pads, they need to be obtained by “requesting” them to the
  // element. An element might be able to produce different kinds of Request
  // Pads, so, when requesting them, the 'desired' Pad Template must be provided.
  // Pad templates are obtained with gst_element_class_get_pad_template() and
  // are identified by their name. In the documentation for the tee element we
  // see that it has two pad templates named “sink” (for its sink Pads) and
  // “src%d” (for the Request Pads)

  // <audio-branch>
  tee_src_pad_template =
    gst_element_class_get_pad_template (GST_ELEMENT_GET_CLASS (tee), "src%d");

  tee_audio_pad =
    gst_element_request_pad (tee, tee_src_pad_template, NULL, NULL);

  g_print ("Obtained request pad %s for audio branch.\n",
      gst_pad_get_name (tee_audio_pad));

  queue_audio_pad = gst_element_get_static_pad (audio_queue, "sink");

  // <video-branch>
  tee_video_pad =
    gst_element_request_pad (tee, tee_src_pad_template, NULL, NULL);

  g_print ("Obtained request pad %s for video branch.\n",
      gst_pad_get_name (tee_video_pad));

  queue_video_pad = gst_element_get_static_pad (video_queue, "sink");

  if (gst_pad_link (tee_audio_pad, queue_audio_pad) != GST_PAD_LINK_OK ||
      gst_pad_link (tee_video_pad, queue_video_pad) != GST_PAD_LINK_OK) {
    g_printerr ("Tee could not be linked.\n");
    gst_object_unref (pipeline);
    return -1;
  }

  // WHY? The sink Pads we have obtained need to be released with
  // gst_object_unref().

  gst_object_unref (queue_audio_pad);
  gst_object_unref (queue_video_pad);
   
  /* Start playing the pipeline */
  gst_element_set_state (pipeline, GST_STATE_PLAYING);
   
  /* Wait until error or EOS */
  bus = gst_element_get_bus (pipeline);
  msg = gst_bus_timed_pop_filtered (bus, GST_CLOCK_TIME_NONE,
      GST_MESSAGE_ERROR | GST_MESSAGE_EOS);
   
  // gst_element_release_request_pad() releases the pad from the tee, but it
  // still needs to be unreferenced (freed) with gst_object_unref().

  /* Release the request pads from the Tee, and unref them */
  gst_element_release_request_pad (tee, tee_audio_pad);
  gst_element_release_request_pad (tee, tee_video_pad);
  gst_object_unref (tee_audio_pad);
  gst_object_unref (tee_video_pad);
   
  /* Free resources */
  if (msg != NULL)
    gst_message_unref (msg);
  gst_object_unref (bus);
  gst_element_set_state (pipeline, GST_STATE_NULL);
   
  gst_object_unref (pipeline);
  return 0;
}


={============================================================================
*kt_dev_bcast_300* gst: tutorial: 08: buffers and interact with a pipeline

Basic tutorial 8: Short-cutting the pipeline

http://docs.gstreamer.com/display/GstSDK/Basic+tutorial+8%3A+Short-cutting+the+pipeline

This tutorial shows:

How to inject external data into a general GStreamer pipeline.
How to extract data from a general GStreamer pipeline.
How to access and manipulate this data.


The element used to inject application data into a GStreamer pipeline is
appsrc, and its counterpart, used to extract GStreamer data back to the
application is appsink. To avoid confusing the names, think of it from
GStreamer's point of view: appsrc is just a regular source, that provides data
magically fallen from the sky (provided by the application, actually). appsink
is a regular sink, where the data flowing through a GStreamer pipeline goes to
die (it is recovered by the application, actually).

appsrc and appsink are so versatile that they offer their own API (see their
        documentation), which can be accessed by linking against the
gstreamer-app library. In this tutorial, however, we will use a simpler
approach and control them through signals.

appsrc can work in a variety of modes: in pull mode, it requests data from the
application every time it needs it. In push mode, the 'application' pushes
data at its own pace. Furthermore, in push mode, the application can choose to
be blocked in the push function when enough data has already been provided, or
it can listen to the enough-data and need-data signals to control flow. This
example implements the 'latter' approach. Information regarding the other
methods can be found in the appsrc documentation.


Data 'travels' through a GStreamer pipeline in chunks called buffers. Since this
example produces and consumes data, we need to know about GstBuffers.

Source Pads produce buffers, that are consumed by Sink Pads; GStreamer takes
these buffers and passes them from element to element.

A buffer simply represents a piece of data, do not assume that all buffers will
have the same size, or represent the same amount of time. Neither should you
assume that if a single buffer enters an element, a single buffer will come out.
Elements are free to do with the received buffers as they please.

'every' buffer has an attached 'caps' 'GstCaps' that describes the kind of media
contained in the buffer. Also, buffers have an attached 'timestamp' and
'duration', that describe in which moment the content of the buffer should be
rendered or displayed. Time stamping is a very complex and delicate subject, but
this simplified vision should suffice for now.

As an example, a filesrc (element that reads files) produces buffers with the
"ANY" caps and no time-stamping information. 'after' demuxing buffers can have
some specific caps, for example "video/x-h264". 'after' decoding, each buffer
will contain a single video frame with raw caps (for example, "video/x-raw-yuv")
and very precise time stamps indicating when should that frame be displayed.


application:
                  push_data()
                     + push_buffer

            start_feed()                        new_buffer()
               + push_data()                       + pull_buffer
            stop_feed()

pipeline signals:

need-data  : -> start_feed()                    new-buffer: -> new_buffer()
enough-data: -> stop_feed()

appsrc          ...                             appsink
+===========+                                   +===========+
|     [src] |                                   |[sink]     |
|           |                                   |           |
+===========+                                   +===========+


<code>
#include <gst/gst.h>
#include <string.h>
   
#define CHUNK_SIZE 1024   /* Amount of bytes we are sending in each buffer */
#define SAMPLE_RATE 44100 /* Samples per second we are sending */
#define AUDIO_CAPS "audio/x-raw-int,channels=1,rate=%d,signed=(boolean)true, \
   width=16,depth=16,endianness=BYTE_ORDER"
   
/* Structure to contain all our information, so we can pass it to callbacks */
typedef struct _CustomData {
  GstElement *pipeline, *app_source, *tee, *audio_queue, *audio_convert1, \
      *audio_resample, *audio_sink;
  GstElement *video_queue, *audio_convert2, *visual, *video_convert, \
      *video_sink;
  GstElement *app_queue, *app_sink;
   
  guint64 num_samples;   /* Number of samples generated so far 
                            (for timestamp generation) */
  gfloat a, b, c, d;     /* For waveform generation */
   
  guint sourceid;        /* To control the GSource */
   
  GMainLoop *main_loop;  /* GLib's Main Loop */
} CustomData;
   

/* This method is called by the idle GSource in the mainloop, to feed CHUNK_SIZE
 * bytes into appsrc. The idle handler is added to the mainloop when appsrc
 * requests us to start sending data (need-data signal) and is removed when
 * appsrc has enough data (enough-data signal).
 *
 * This is the function that feeds appsrc. It will be called by GLib at times
 * and rates which are out of our control, but we know that we will disable it
 * when its job is done (when the queue in appsrc is full).
 *
 * Its first task is to create a new buffer with a given size (in this example,
 * it is arbitrarily set to 1024 bytes) with gst_buffer_new_and_alloc(). 
 *
 * We count the number of samples that we have generated so far with the
 * CustomData.num_samples variable, so we can 'timestamp' this buffer using the
 * GST_BUFFER_TIMESTAMP macro in GstBuffer.  
 *
 * Since we are producing buffers of the same size, their duration is the same
 * and is set using the GST_BUFFER_DURATION in GstBuffer.
 *
 * gst_util_uint64_scale() is a utility function that scales (multiply and
 * divide) numbers which can be large, without fear of overflows. 
 *
 * The bytes that for the buffer can be accessed with GST_BUFFER_DATA in
 * GstBuffer (Be careful not to write past the end of the buffer: you allocated
 * it, so you know its size).  
 *
 * We will skip over the waveform generation, since it is outside the scope of
 * this tutorial (it is simply a funny way of generating a pretty psychedelic
 * wave).
 *
 * Once we have the buffer ready, we pass it to 'appsrc' with the push-buffer
 * action signal (see information box at the end of Playback tutorial 1:
 * Playbin2 usage), 
 *
 * and then gst_buffer_unref() it since we no longer need it.
 */

static gboolean push_data (CustomData *data) {
  GstBuffer *buffer;
  GstFlowReturn ret;
  int i;
  gint16 *raw;
  gint num_samples = CHUNK_SIZE / 2; /* Because each sample is 16 bits */
  gfloat freq;
   
  /* Create a new empty buffer */
  buffer = gst_buffer_new_and_alloc (CHUNK_SIZE);
   
  /* Set its timestamp and duration */
  GST_BUFFER_TIMESTAMP (buffer) = 
      gst_util_uint64_scale (data->num_samples, GST_SECOND, SAMPLE_RATE);
  GST_BUFFER_DURATION (buffer) = 
      gst_util_uint64_scale (CHUNK_SIZE, GST_SECOND, SAMPLE_RATE);
   
  /* Generate some psychodelic waveforms */
  raw = (gint16 *)GST_BUFFER_DATA (buffer);

  data->c += data->d;
  data->d -= data->c / 1000;
  freq = 1100 + 1000 * data->d;
  for (i = 0; i < num_samples; i++) {
    data->a += data->b;
    data->b -= data->a / freq;
    raw[i] = (gint16)(500 * data->a);
  }
  data->num_samples += num_samples;
   
  /* Push the buffer into the appsrc 
   *
   * <action-signal>
   * Do like a function call to pass buffer and "push-buffer" is implemented by
   * source base class?
   */
  g_signal_emit_by_name (data->app_source, "push-buffer", buffer, &ret);
   
  /* Free the buffer now that we are done with it */
  gst_buffer_unref (buffer);
   
  if (ret != GST_FLOW_OK) {
    /* We got some error, stop sending data */
    return FALSE;
  }
   
  return TRUE;
}
   
/* This signal callback triggers when appsrc needs data. Here, we add an idle
 * handler to the mainloop to start pushing data into the appsrc 
 *
 * This function is called when the internal queue of appsrc is about to starve
 * (run out of data). 
 *
 * The only thing we do here is register a GLib idle function with g_idle_add()
 * that feeds data to appsrc until it is full again. A GLib idle function is a
 * method that GLib will call from its main loop whenever it is "idle", this is,
 * when it has no higher-priority tasks to perform. It requires a GLib GMainLoop
 * to be instantiated and running, obviously.
 *
 * This is only one of the multiple approaches that appsrc allows. In
 * particular, buffers do not need to be fed into appsrc from the main thread
 * using GLib, and you do not need to use the need-data and enough-data signals
 * to synchronize with appsrc (although this is allegedly the most convenient).
 *
 * take note of the sourceid that g_idle_add() returns, so we can disable it
 * later.
 */

static void start_feed (GstElement *source, guint size, CustomData *data) {
  if (data->sourceid == 0) {
    g_print ("Start feeding\n");
    data->sourceid = g_idle_add ((GSourceFunc) push_data, data);
  }
}
   
/* This callback triggers when appsrc has enough data and we can stop sending.
 * We remove the idle handler from the mainloop 
 *
 * This function is called when the internal queue of appsrc is full enough so
 * we stop pushing data. Here we simply remove the idle function by using
 * g_source_remove() (The idle function is implemented as a GSource).  
 *
 * note:
 * feeding can be stopped either when push_data() returns false or stop_feed()
 * is called.
 */
static void stop_feed (GstElement *source, CustomData *data) {
  if (data->sourceid != 0) {
    g_print ("Stop feeding\n");
    g_source_remove (data->sourceid);
    data->sourceid = 0;
  }
}
   

/* The appsink has received a buffer 
 *
 * Finally, this is the function that gets called when the appsink receives a
 * buffer. 
 *
 * We use the pull-buffer action signal to retrieve the buffer and then just
 * print some indicator on the screen. 
 *
 * <action-signal> 
 * This rather unintuitive way of retrieving the tag list is called an Action
 * Signal. 
 *
 * Action signals are emitted 'by' the application to a specific element, which
 * then performs an action and 'returns' a result. They behave like a "dynamic
 * function call", in which methods of a class are identified by their 'name'
 * (the signal's name) instead of their memory address. These signals are listed
 * In the documentation along with the 'regular' signals, and are tagged
 * "Action". See playbin2, for example.
 *
 * // appsrc signals
 * Signals
 *   "end-of-stream"                                  : Action
 *   "enough-data"                                    : Run Last
 *   "need-data"                                      : Run Last
 *   "push-buffer"                                    : Action
 *   "seek-data"                                      : Run Last
 *
 * // appsink
 * Signals
 *   "eos"                                            : Run Last
 *   "new-buffer"                                     : Run Last
 *   "new-preroll"                                    : Run Last
 *   "pull-buffer"                                    : Action
 *   "pull-preroll"                                   : Action
 *   "new-buffer-list"                                : Run Last
 *   "pull-buffer-list"                               : Action
 *
 * note: "Run Last" means that it can be overrided?
 *
 * We can retrieve the data pointer using the GST_BUFFER_DATA macro and the data
 * size using the GST_BUFFER_SIZE macro in GstBuffer. Remember that this buffer
 * does not have to match the buffer that we produced in the push_data function,
 * any element in the path could have altered the buffers in any way (Not in
 * this example: there is only a tee in the path between appsrc and appsink, and
 * it does not change the content of the buffers). We then gst_buffer_unref()
 * the buffer.
 */

static void new_buffer (GstElement *sink, CustomData *data) {
  GstBuffer *buffer;
   
  /* Retrieve the buffer 
   *
   * <action-signal>
   * Do like a function call to pass buffer and "pull-buffer" is implemented by
   * sink base class?
   */
  g_signal_emit_by_name (sink, "pull-buffer", &buffer);

  if (buffer) {
    /* The only thing we do in this example is print a * to indicate 
     * a received buffer */
    g_print ("*");
    gst_buffer_unref (buffer);
  }
}
   
/* This function is called when an error message is posted on the bus */
static void error_cb (GstBus *bus, GstMessage *msg, CustomData *data) {
  GError *err;
  gchar *debug_info;
   
  /* Print error details on the screen */
  gst_message_parse_error (msg, &err, &debug_info);
  g_printerr ("Error received from element %s: %s\n", GST_OBJECT_NAME (msg->src), err->message);
  g_printerr ("Debugging information: %s\n", debug_info ? debug_info : "none");
  g_clear_error (&err);
  g_free (debug_info);
   
  g_main_loop_quit (data->main_loop);
}
   
int main(int argc, char *argv[]) {

  CustomData data;

  GstPadTemplate *tee_src_pad_template;
  GstPad *tee_audio_pad, *tee_video_pad, *tee_app_pad;
  GstPad *queue_audio_pad, *queue_video_pad, *queue_app_pad;
  gchar *audio_caps_text;
  GstCaps *audio_caps;
  GstBus *bus;
   
  /* Initialize cumstom data structure */
  memset (&data, 0, sizeof (data));
  data.b = 1; /* For waveform generation */
  data.d = 1;
   
  /* Initialize GStreamer */
  gst_init (&argc, &argv);
   
  /* Create the elements */
  data.app_source = gst_element_factory_make ("appsrc", "audio_source");
  data.tee = gst_element_factory_make ("tee", "tee");
  data.audio_queue = gst_element_factory_make ("queue", "audio_queue");
  data.audio_convert1 = gst_element_factory_make ("audioconvert", "audio_convert1");
  data.audio_resample = gst_element_factory_make ("audioresample", "audio_resample");
  data.audio_sink = gst_element_factory_make ("autoaudiosink", "audio_sink");
  data.video_queue = gst_element_factory_make ("queue", "video_queue");
  data.audio_convert2 = gst_element_factory_make ("audioconvert", "audio_convert2");
  data.visual = gst_element_factory_make ("wavescope", "visual");
  data.video_convert = gst_element_factory_make ("ffmpegcolorspace", "csp");
  data.video_sink = gst_element_factory_make ("autovideosink", "video_sink");
  data.app_queue = gst_element_factory_make ("queue", "app_queue");
  data.app_sink = gst_element_factory_make ("appsink", "app_sink");
   
  /* Create the empty pipeline */
  data.pipeline = gst_pipeline_new ("test-pipeline");
   
  if (!data.pipeline || !data.app_source || !data.tee || !data.audio_queue || 
      !data.audio_convert1 || !data.audio_resample || !data.audio_sink || 
      !data.video_queue || !data.audio_convert2 || !data.visual ||
      !data.video_convert || !data.video_sink || !data.app_queue || 
      !data.app_sink) {
    g_printerr ("Not all elements could be created.\n");
    return -1;
  }
   
  /* Configure wavescope */
  g_object_set (data.visual, "shader", 0, "style", 0, NULL);
   
  /* Configure appsrc
   *
   * The first property that needs to be set on the appsrc is caps. It specifies
   * the kind of data that the element is going to 'produce', so GStreamer can
   * check if linking with downstream elements is possible (this is, if the
   * downstream elements will understand this kind of data). This property must
   * be a GstCaps object, which is easily built from a string with
   * gst_caps_from_string().
   *
   * <signal-connect>
   * We then 'connect' to the need-data and enough-data signals. These are fired
   * by appsrc when its internal queue of data is running low or almost full,
   * respectively. We will use these signals to start and stop (respectively)
   * our signal generation process.
   *
   * note:
   * Provides "data" when connects signal.
   */

  audio_caps_text = g_strdup_printf (AUDIO_CAPS, SAMPLE_RATE);
  audio_caps = gst_caps_from_string (audio_caps_text);
  g_object_set (data.app_source, "caps", audio_caps, NULL);
  g_signal_connect (data.app_source, "need-data", G_CALLBACK (start_feed), &data);
  g_signal_connect (data.app_source, "enough-data", G_CALLBACK (stop_feed), &data);
   

  /* Configure appsink 
   *
   * Regarding the appsink configuration, we connect to the new-buffer signal,
   * which is emitted every time the sink 'receives' a buffer. Also, the signal
   * emission needs to be enabled through the emit-signals property, because, by
   * default, it is disabled 
   *
   * note: that appsrc do not need to be set emit-signals
   */
  g_object_set (data.app_sink, "emit-signals", TRUE, "caps", audio_caps, NULL);
  g_signal_connect (data.app_sink, "new-buffer", G_CALLBACK (new_buffer), &data);

  // see tidying up
  gst_caps_unref (audio_caps);
  g_free (audio_caps_text);
   
  /* Link all elements that can be automatically linked because they have
   * "Always" pads */

  gst_bin_add_many (GST_BIN (data.pipeline), data.app_source, data.tee,
          data.audio_queue, data.audio_convert1, data.audio_resample,
          data.audio_sink, data.video_queue, data.audio_convert2, data.visual,
          data.video_convert, data.video_sink, data.app_queue, data.app_sink,
          NULL);

  if (gst_element_link_many (data.app_source, data.tee, NULL) != TRUE ||
      gst_element_link_many (data.audio_queue, data.audio_convert1, 
          data.audio_resample, data.audio_sink, NULL) != TRUE ||
      gst_element_link_many (data.video_queue, data.audio_convert2, 
          data.visual, data.video_convert, data.video_sink, NULL) != TRUE ||
      gst_element_link_many (data.app_queue, data.app_sink, NULL) != TRUE) {
    g_printerr ("Elements could not be linked.\n");
    gst_object_unref (data.pipeline);
    return -1;
  }
   
  /* Manually link the Tee, which has "Request" pads */
  tee_src_pad_template = 
      gst_element_class_get_pad_template (GST_ELEMENT_GET_CLASS (data.tee), "src%d");
  tee_audio_pad = 
      gst_element_request_pad (data.tee, tee_src_pad_template, NULL, NULL);

  g_print ("Obtained request pad %s for audio branch.\n", 
          gst_pad_get_name (tee_audio_pad));

  queue_audio_pad = 
      gst_element_get_static_pad (data.audio_queue, "sink");
  tee_video_pad = 
      gst_element_request_pad (data.tee, tee_src_pad_template, NULL, NULL);

  g_print ("Obtained request pad %s for video branch.\n", 
          gst_pad_get_name (tee_video_pad));

  queue_video_pad = 
      gst_element_get_static_pad (data.video_queue, "sink");
  tee_app_pad = 
      gst_element_request_pad (data.tee, tee_src_pad_template, NULL, NULL);

  g_print ("Obtained request pad %s for app branch.\n",
          gst_pad_get_name (tee_app_pad));

  queue_app_pad = 
      gst_element_get_static_pad (data.app_queue, "sink");

  if (gst_pad_link (tee_audio_pad, queue_audio_pad) != GST_PAD_LINK_OK ||
      gst_pad_link (tee_video_pad, queue_video_pad) != GST_PAD_LINK_OK ||
      gst_pad_link (tee_app_pad, queue_app_pad) != GST_PAD_LINK_OK) {
    g_printerr ("Tee could not be linked\n");
    gst_object_unref (data.pipeline);
    return -1;
  }
  gst_object_unref (queue_audio_pad);
  gst_object_unref (queue_video_pad);
  gst_object_unref (queue_app_pad);
   
  /* Instruct the bus to emit signals for each received message, and connect to
   * the interesting signals */

  bus = gst_element_get_bus (data.pipeline);
  gst_bus_add_signal_watch (bus);
  g_signal_connect (G_OBJECT (bus), "message::error", (GCallback)error_cb, &data);
  gst_object_unref (bus);
   
  /* Start playing the pipeline */
  gst_element_set_state (data.pipeline, GST_STATE_PLAYING);
   
  /* Create a GLib Main Loop and set it to run */
  data.main_loop = g_main_loop_new (NULL, FALSE);
  g_main_loop_run (data.main_loop);
   
  /* Release the request pads from the Tee, and unref them */
  gst_element_release_request_pad (data.tee, tee_audio_pad);
  gst_element_release_request_pad (data.tee, tee_video_pad);
  gst_element_release_request_pad (data.tee, tee_app_pad);
  gst_object_unref (tee_audio_pad);
  gst_object_unref (tee_video_pad);
  gst_object_unref (tee_app_pad);
   
  /* Free resources */
  gst_element_set_state (data.pipeline, GST_STATE_NULL);
  gst_object_unref (data.pipeline);
  return 0;
}


={============================================================================
*kt_dev_bcast_306* gst: 


={============================================================================
*kt_dev_bcast_301* gst: base elements

gst-plugins-base Elements
http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-good-plugins/html/

http://docs.gstreamer.com/display/GstSDK/Basic+tutorial+14%3A+Handy+elements

http://gstreamer.freedesktop.org/documentation/

{uridecodebin}
will internally instantiate all the necessary elements (sources, demuxers and decoders) to turn a
URI into raw audio and/or video streams. It does half the work that playbin2 does. Since it contains
demuxers, its source pads are not initially available and we will need to link to them on the fly.

This element decodes data from a URI into raw media. It selects a source element that can handle the
given URI scheme and connects it to a decodebin2 element. Like a demuxer, so it offers as many
source pads as streams are found in the media.

gst-launch-0.10 uridecodebin uri=http://docs.gstreamer.com/media/sintel_trailer-480p.webm ! ffmpegcolorspace ! autovideosink
gst-launch-0.10 uridecodebin uri=http://docs.gstreamer.com/media/sintel_trailer-480p.webm ! audioconvert ! autoaudiosink

Decodes data from a URI into raw media. It selects a source element that can handle the given "uri" scheme and connects it to a decodebin2. 


{decodebin2}
http://www.freedesktop.org/software/gstreamer-sdk/data/docs/2012.5/gst-plugins-base-plugins-0.10/gst-plugins-base-plugins-decodebin2.html#decodebin2
GstBin that auto-magically constructs a decoding pipeline using available decoders and demuxers via
auto-plugging.

decodebin2 is considered stable now and replaces the old decodebin element. uridecodebin uses
decodebin2 internally and is often more convenient to use, as it creates a suitable source element
as well. 

appsrc, appsink

The element used to inject application data into a GStreamer pipeline is appsrc, and its
counterpart, used to extract GStreamer data back to the application is appsink.


{decodebin}
decodebin — Autoplug and decode to raw media

[root@HUMAX /]# gst-inspect-1.0 decodebin
Factory Details:
  Rank                     none (0)
  Long-name                Decoder Bin
  Klass                    Generic/Bin/Decoder
  Description              Autoplug and decode to raw media
  Author                   Edward Hervey <edward.hervey@collabora.co.uk>, Sebastian DrÃ¶ge <sebastian.droege@collabora.co.uk>

Plugin Details:
  Name                     playback
  Description              various playback elements
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstplayback.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gst-plugins-base
  Source release date      2015-06-02 10:11 (UTC)
  Binary package           GStreamer Base Plug-ins git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstBin
                         +----GstDecodeBin

Implemented Interfaces:
  GstChildProxy

Pad Templates:
  SINK template: 'sink'
    Availability: Always
    Capabilities:
      ANY

  SRC template: 'src_%u'
    Availability: Sometimes
    Capabilities:
      ANY


Element Flags:
  no flags set

Bin Flags:
  no flags set

Element Implementation:
  Has change_state() function: gst_decode_bin_change_state

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SINK: 'sink'
    Pad Template: 'sink'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "decodebin0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  async-handling      : The bin will handle Asynchronous state changes
                        flags: readable, writable
                        Boolean. Default: false
  message-forward     : Forwards all children messages
                        flags: readable, writable
                        Boolean. Default: false
  caps                : The caps on which to stop decoding.
                        flags: readable, writable
                           video/x-raw(ANY)
                           audio/x-raw(ANY)
                           text/x-raw
                           subpicture/x-dvd
                           subpicture/x-dvb
                           subpicture/x-pgs

  subtitle-encoding   : Encoding to assume if input subtitles are not in UTF-8 encoding. If not set, the GST_SUBTITLE_ENCODING environment variable will be checked for an encoding to use. If that is not set either, ISO-8859-15 will be assumed.
                        flags: readable, writable
                        String. Default: null
  sink-caps           : The caps of the input data. (NULL = use typefind element)
                        flags: readable, writable
                        Caps (NULL)
  use-buffering       : Emit GST_MESSAGE_BUFFERING based on low-/high-percent thresholds
                        flags: readable, writable
                        Boolean. Default: false
  low-percent         : Low threshold for buffering to start
                        flags: readable, writable
                        Integer. Range: 0 - 100 Default: 10 
  high-percent        : High threshold for buffering to finish
                        flags: readable, writable
                        Integer. Range: 0 - 100 Default: 99 
  max-size-bytes      : Max. amount of bytes in the queue (0=automatic)
                        flags: readable, writable
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  max-size-buffers    : Max. number of buffers in the queue (0=automatic)
                        flags: readable, writable
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  max-size-time       : Max. amount of data in the queue (in ns, 0=automatic)
                        flags: readable, writable
                        Unsigned Integer64. Range: 0 - 18446744073709551615 Default: 0 
  post-stream-topology: Post stream-topology messages
                        flags: readable, writable
                        Boolean. Default: false
  expose-all-streams  : Expose all streams, including those of unknown type or that don't match the 'caps' property
                        flags: readable, writable
                        Boolean. Default: true
  connection-speed    : Network connection speed in kbps (0 = unknown)
                        flags: readable, writable
                        Unsigned Integer64. Range: 0 - 18446744073709551 Default: 0 

Element Signals:
  "pad-added" :  void user_function (GstElement* object,
                                     GstPad* arg0,
                                     gpointer user_data);
  "pad-removed" :  void user_function (GstElement* object,
                                       GstPad* arg0,
                                       gpointer user_data);
  "no-more-pads" :  void user_function (GstElement* object,
                                        gpointer user_data);
  "unknown-type" :  void user_function (GstElement* object,
                                        GstPad* arg0,
                                        GstCaps* arg1,
                                        gpointer user_data);
  "autoplug-continue" :  gboolean user_function (GstElement* object,
                                                 GstPad* arg0,
                                                 GstCaps* arg1,
                                                 gpointer user_data);
  "autoplug-factories" :  GValueArray * user_function (GstElement* object,
                                                       GstPad* arg0,
                                                       GstCaps* arg1,
                                                       gpointer user_data);
  "autoplug-sort" :  GValueArray * user_function (GstElement* object,
                                                  GstPad* arg0,
                                                  GstCaps* arg1,
                                                  GValueArray* arg2,
                                                  gpointer user_data);
  "autoplug-select" :  GstAutoplugSelectResult user_function (GstElement* object,
                                                              GstPad* arg0,
                                                              GstCaps* arg1,
                                                              GstElementFactory* arg2,
                                                              gpointer user_data);
  "autoplug-query" :  gboolean user_function (GstElement* object,
                                              GstPad* arg0,
                                              GstElement* arg1,
                                              GstQuery* arg2,
                                              gpointer user_data);
  "drained" :  void user_function (GstElement* object,
                                   gpointer user_data);

Children:
  typefind


={============================================================================
*kt_dev_bcast_302* gst-plugin-docs

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/


={============================================================================
*kt_dev_bcast_302* gst: elements: souphttpsrc

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-good-plugins/html/
  gst-plugins-good-plugins-souphttpsrc.html

<http_proxy>
Set this environment variable to make all GStreamer HTTP request go via a proxy,
I used this to make all Dashdemux request go via my proxy so I could record the
  streams, it works on the STB.

export http_proxy=http://your.proxy.co.uk


gst-plugins-good/ext/soup/gstsouphttpsrc.c
22: * If the "http_proxy" environment variable is set, its value is used.
489:  proxy = g_getenv ("http_proxy");
492:        "The proxy in the http_proxy env var (\"%s\") cannot be parsed.",


={============================================================================
*kt_dev_bcast_302* gst-uridecodebin

/gst_plugins_base-1.5.0/ gst/playback/gsturidecodebin.c

[root@HUMAX /]# gst-inspect-1.0 uridecodebin
Factory Details:
  Rank                     none (0)
  Long-name                URI Decoder
  Klass                    Generic/Bin/Decoder
  Description              Autoplug and decode an URI to raw media
  Author                   Wim Taymans <wim.taymans@gmail.com>

Plugin Details:
  Name                     playback
  Description              various playback elements
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstplayback.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gst-plugins-base
  Source release date      2015-06-02 10:11 (UTC)
  Binary package           GStreamer Base Plug-ins git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstBin
                         +----GstURIDecodeBin

Implemented Interfaces:
  GstChildProxy

Pad Templates:
  SRC template: 'src_%u'
    Availability: Sometimes     note: 'sometimes'
    Capabilities:
      ANY


Element Flags:
  no flags set

Bin Flags:
  no flags set

Element Implementation:
  Has change_state() function: gst_uri_decode_bin_change_state

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  none

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "uridecodebin0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  async-handling      : The bin will handle Asynchronous state changes
                        flags: readable, writable
                        Boolean. Default: false
  message-forward     : Forwards all children messages
                        flags: readable, writable
                        Boolean. Default: false
  uri                 : URI to decode
                        flags: readable, writable
                        String. Default: null
  source              : Source object used
                        flags: readable
                        Object of type "GstElement"
  connection-speed    : Network connection speed in kbps (0 = unknown)
                        flags: readable, writable
                        Unsigned Integer64. Range: 0 - 18446744073709551 Default: 0 
  caps                : The caps on which to stop decoding. (NULL = default)
                        flags: readable, writable
                           video/x-raw(ANY)
                           audio/x-raw(ANY)
                           text/x-raw
                           subpicture/x-dvd
                           subpicture/x-dvb
                           subpicture/x-pgs

  subtitle-encoding   : Encoding to assume if input subtitles are not in UTF-8 encoding. If not set, the GST_SUBTITLE_ENCODING environment variable will be checked for an encoding to use. If
 that is not set either, ISO-8859-15 will be assumed.
                        flags: readable, writable
                        String. Default: null
  buffer-size         : Buffer size when buffering streams (-1 default value)
                        flags: readable, writable
                        Integer. Range: -1 - 2147483647 Default: -1 
  buffer-duration     : Buffer duration when buffering streams (-1 default value)
                        flags: readable, writable
                        Integer64. Range: -1 - 9223372036854775807 Default: -1 
  download            : Attempt download buffering when buffering network streams
                        flags: readable, writable
                        Boolean. Default: false
  use-buffering       : Perform buffering on demuxed/parsed media
                        flags: readable, writable
                        Boolean. Default: false
  expose-all-streams  : Expose all streams, including those of unknown type or that don't match the 'caps' property
                        flags: readable, writable
                        Boolean. Default: true
  ring-buffer-max-size: Max. amount of data in the ring buffer (bytes, 0 = ring buffer disabled)
                        flags: readable, writable
                        Unsigned Integer64. Range: 0 - 4294967295 Default: 0 

Element Signals:
  "pad-added" :  void user_function (GstElement* object,
                                     GstPad* arg0,
                                     gpointer user_data);
  "pad-removed" :  void user_function (GstElement* object,
                                       GstPad* arg0,
                                       gpointer user_data);
  "no-more-pads" :  void user_function (GstElement* object,
                                        gpointer user_data);
  "unknown-type" :  void user_function (GstElement* object,
                                        GstPad* arg0,
                                        GstCaps* arg1,
                                        gpointer user_data);
  "autoplug-continue" :  gboolean user_function (GstElement* object,
                                                 GstPad* arg0,
                                                 GstCaps* arg1,
                                                 gpointer user_data);
  "autoplug-factories" :  GValueArray * user_function (GstElement* object,
                                                       GstPad* arg0,
                                                       GstCaps* arg1,
                                                       gpointer user_data);
  "autoplug-sort" :  GValueArray * user_function (GstElement* object,
                                                  GstPad* arg0,
                                                  GstCaps* arg1,
                                                  GValueArray* arg2,
                                                  gpointer user_data);
  "autoplug-select" :  GstAutoplugSelectResult user_function (GstElement* object,
                                                              GstPad* arg0,
                                                              GstCaps* arg1,
                                                              GstElementFactory* arg2,
                                                              gpointer user_data);
  "autoplug-query" :  gboolean user_function (GstElement* object,
                                              GstPad* arg0,
                                              GstElement* arg1,
                                              GstQuery* arg2,
                                              gpointer user_data);
  "drained" :  void user_function (GstElement* object,
                                   gpointer user_data);
  "source-setup" :  void user_function (GstElement* object,
                                        GstElement* arg0,
                                        gpointer user_data);


={============================================================================
*kt_dev_bcast_302* gst: elements: qtdemux

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-good-plugins/html/gst-plugins-good-plugins-qtdemux.html

qtdemux — Demultiplex a QuickTime file into audio and video streams

Demuxes a .mov file into raw or compressed audio and/or video streams.

This element supports both push and pull-based scheduling, depending on the
capabilities of the upstream elements.


gst-inspect-1.0 qtdemux

Factory Details:
  Rank                     primary (256)
  Long-name                QuickTime demuxer
  Klass                    Codec/Demuxer
  Description              Demultiplex a QuickTime file into audio and video streams
  Author                   David Schleef <ds@schleef.org>, Wim Taymans <wim@fluendo.com>

Plugin Details:
  Name                     isomp4
  Description              ISO base media file format support (mp4, 3gpp, qt, mj2)
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstisomp4.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gst-plugins-good
  Source release date      2015-08-03 14:07 (UTC)
  Binary package           GStreamer Good Plug-ins git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstQTDemux

Pad Templates:
  SINK template: 'sink'
    Availability: Always
    Capabilities:
      video/quicktime
      video/mj2
      audio/x-m4a
      application/x-3gp

  SRC template: 'video_%u'
    Availability: Sometimes
    Capabilities:
      ANY

  SRC template: 'audio_%u'
    Availability: Sometimes
    Capabilities:
      ANY

  SRC template: 'subtitle_%u'
    Availability: Sometimes
    Capabilities:
      ANY


Element Flags:
  no flags set

Element Implementation:
  Has change_state() function: gst_qtdemux_change_state

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SINK: 'sink'
    Pad Template: 'sink'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "qtdemux0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"

Element Signals:
  "pad-added" :  void user_function (GstElement* object,
                                     GstPad* arg0,
                                     gpointer user_data);
  "pad-removed" :  void user_function (GstElement* object,
                                       GstPad* arg0,
                                       gpointer user_data);
  "no-more-pads" :  void user_function (GstElement* object,
                                        gpointer user_data);


// where seems to put pts from stream
//
gst_plugins_good-1.5.0/gst/isomp4/qtdemux.c

GST_DEBUG_OBJECT (demux, "stream : %" GST_FOURCC_FORMAT,
   GST_FOURCC_ARGS (stream->fourcc));

          dts = QTSAMPLE_DTS (stream, sample);
          pts = QTSAMPLE_PTS (stream, sample);


={============================================================================
*kt_dev_bcast_302* gst: elements: h264parse

$ gst-inspect-1.0 h264parse   
Factory Details:
  Rank                     primary + 1 (257)
  Long-name                H.264 parser
  Klass                    Codec/Parser/Converter/Video
  Description              Parses H.264 streams
  Author                   Mark Nauwelaerts <mark.nauwelaerts@collabora.co.uk>

Plugin Details:
  Name                     videoparsersbad
  Description              videoparsers
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstvideoparsersbad.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gst-plugins-bad
  Source release date      2015-08-03 14:19 (UTC)
  Binary package           GStreamer Bad Plug-ins git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstBaseParse
                         +----GstH264Parse

Pad Templates:
  SRC template: 'src'
    Availability: Always
    Capabilities:
      video/x-h264
                 parsed: true
          stream-format: { avc, avc3, byte-stream }
              alignment: { au, nal }

  SINK template: 'sink'
    Availability: Always
    Capabilities:
      video/x-h264


Element Flags:
  no flags set

Element Implementation:
  Has change_state() function: gst_base_parse_change_state

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SINK: 'sink'
    Pad Template: 'sink'
  SRC: 'src'
    Pad Template: 'src'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "h264parse0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  disable-passthrough : Force processing (disables passthrough)
                        flags: readable, writable
                        Boolean. Default: false
  config-interval     : Send SPS and PPS Insertion Interval in seconds 
  (sprop parameter sets will be multiplexed in the data stream when detected.) (0 = disabled)
                        flags: readable, writable
                        Unsigned Integer. Range: 0 - 3600 Default: 0 


={============================================================================
*kt_dev_bcast_302* gst: elements: mpegtsmux

gst-inspect-1.0 mpegtsmux

Factory Details:
  Rank                     primary (256)
  Long-name                MPEG Transport Stream Muxer
  Klass                    Codec/Muxer
  Description              Multiplexes media streams into an MPEG Transport Stream
  Author                   Fluendo <contact@fluendo.com>

Plugin Details:
  Name                     mpegtsmux
  Description              MPEG-TS muxer
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstmpegtsmux.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gst-plugins-bad
  Source release date      2015-08-03 14:19 (UTC)
  Binary package           GStreamer Bad Plug-ins git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----MpegTsMux

Pad Templates:
  SRC template: 'src'
    Availability: Always
    Capabilities:
      video/mpegts
           systemstream: true
             packetsize: { 188, 192 }

  SINK template: 'sink_%d'
    Availability: On request
      Has request_new_pad() function: 0x76e8ca70
    Capabilities:
      video/mpeg
                 parsed: true
            mpegversion: { 1, 2, 4 }
           systemstream: false
      video/x-dirac
      video/x-h264
          stream-format: byte-stream
              alignment: { au, nal }
      audio/mpeg
                 parsed: true
            mpegversion: { 1, 2 }
      audio/mpeg
                 framed: true
            mpegversion: 4
          stream-format: { raw, adts }
      audio/x-lpcm
                  width: { 16, 20, 24 }
                   rate: { 48000, 96000 }
               channels: [ 1, 8 ]
          dynamic_range: [ 0, 255 ]
               emphasis: { false, true }
                   mute: { false, true }
      audio/x-ac3
                 framed: true
      audio/x-dts
                 framed: true
      subpicture/x-dvb
      application/x-teletext


Element Flags:
  no flags set

Element Implementation:
  Has change_state() function: 0x76e8c804

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SRC: 'src'
    Pad Template: 'src'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "mpegtsmux0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  prog-map            : A GstStructure specifies the mapping from elementary streams to programs
                        flags: readable, writable
                        Boxed pointer of type "GstStructure"
  m2ts-mode           : Set to TRUE to output Blu-Ray disc format with 192 byte packets. FALSE for standard TS format with 188 byte packets.
                        flags: readable, writable
                        Boolean. Default: false
  pat-interval        : Set the interval (in ticks of the 90kHz clock) for writing out the PAT table
                        flags: readable, writable
                        Unsigned Integer. Range: 1 - 4294967295 Default: 9000 
  pmt-interval        : Set the interval (in ticks of the 90kHz clock) for writing out the PMT table
                        flags: readable, writable
                        Unsigned Integer. Range: 1 - 4294967295 Default: 9000 
  alignment           : Number of packets per buffer (padded with dummy packets on EOS) (-1 = auto, 0 = all available packets)
                        flags: readable, writable
                        Integer. Range: -1 - 2147483647 Default: -1 
  si-interval         : Set the interval (in ticks of the 90kHz clock) for writing out the ServiceInformation tables
                        flags: readable, writable
                        Unsigned Integer. Range: 1 - 4294967295 Default: 9000 


={============================================================================
*kt_dev_bcast_302* gst: elements: tsparse

$gst-inspect-1.0 tsparse   
Factory Details:
  Rank                     none (0)
  Long-name                MPEG transport stream parser
  Klass                    Codec/Parser
  Description              Parses MPEG2 transport streams
  Author                   Alessandro Decina <alessandro@nnva.org>, 
                           Zaheer Abbas Merali <zaheerabbas at merali dot org>

Plugin Details:
  Name                     mpegtsdemux
  Description              MPEG TS demuxer
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstmpegtsdemux.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gst-plugins-bad
  Source release date      2015-08-03 14:19 (UTC)
  Binary package           GStreamer Bad Plug-ins git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----MpegTSBase
                         +----MpegTSParse2

Pad Templates:
  SINK template: 'sink'
    Availability: Always
    Capabilities:
      video/mpegts
           systemstream: true

  SRC template: 'src'
    Availability: Always
    Capabilities:
      video/mpegts
           systemstream: true

  SRC template: 'program_%u'
    Availability: On request
      Has request_new_pad() function: 0x772c67c4
    Capabilities:
      video/mpegts
           systemstream: true


Element Flags:
  no flags set

Element Implementation:
  Has change_state() function: 0x772c0864

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SINK: 'sink'
    Pad Template: 'sink'
  SRC: 'src'
    Pad Template: 'src'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "mpegtsparse2-0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  parse-private-sections: Parse private sections
                        flags: readable, writable
                        Boolean. Default: false
  set-timestamps      : If set, timestamps will be set on the output buffers using PCRs and smoothed over the smoothing-latency period
                        flags: readable, writable
                        Boolean. Default: false
  smoothing-latency   : Additional latency in microseconds for smoothing jitter in input timestamps on live capture
                        flags: readable, writable
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  pcr-pid             : Set the PID to use for PCR values (-1 for auto)
                        flags: readable, writable
                        Integer. Range: -1 - 2147483647 Default: -1 


={============================================================================
*kt_dev_bcast_302* gst-identity

note: this affects performance of pipeline and have to use gst-launch -v to see
output. direct output to a file to minimize this.

note: use name to distinguish branches.

! identity name=video silent=false check-imperfect-timestamp=true
! identity name=audio silent=false check-imperfect-timestamp=true

/GstPipeline:pipeline0/GstIdentity:video: last-message = chain   ******* (video:sink) 
   (37076 bytes, dts: 0:02:57.640000000, pts:0:02:57.760000000, 
    duration: 0:00:00.040000000, offset: -1, offset_end: -1, flags: 00002000 delta-unit ) 0x5fbac0

/GstPipeline:pipeline0/GstIdentity:video: last-message = chain   ******* (video:sink) 
   (8259 bytes, dts: 0:02:57.680000000, pts:0:02:57.680000000, 
    duration: 0:00:00.040000000, offset: -1, offset_end: -1, flags: 00002000 delta-unit ) 0x69e728

/GstPipeline:pipeline0/GstIdentity:audio: last-message = chain   ******* (audio:sink) 
   (425 bytes, dts: 0:01:44.618666666, pts:0:01:44.618666666, 
    duration: 0:00:00.021333334, offset: -1, offset_end: -1, flags: 00004000 tag-memory ) 0x73e680

/GstPipeline:pipeline0/GstIdentity:audio: last-message = chain   ******* (audio:sink) 
   (415 bytes, dts: 0:01:44.640000000, pts:0:01:44.640000000, 
    duration: 0:00:00.021333333, offset: -1, offset_end: -1, flags: 00004000 tag-memory ) 0x603100


gst-inspect-1.0 identity

Factory Details:
  Rank                     none (0)
  Long-name                Identity
  Klass                    Generic
  Description              Pass data without modification
  Author                   Erik Walthinsen <omega@cse.ogi.edu>

Plugin Details:
  Name                     coreelements
  Description              GStreamer core elements
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstcoreelements.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gstreamer
  Source release date      2015-08-03 14:00 (UTC)
  Binary package           GStreamer git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstBaseTransform
                         +----GstIdentity

Pad Templates:
  SINK template: 'sink'
    Availability: Always
    Capabilities:
      ANY

  SRC template: 'src'
    Availability: Always
    Capabilities:
      ANY


Element Flags:
  no flags set

Element Implementation:
  Has change_state() function: gst_identity_change_state

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SINK: 'sink'
    Pad Template: 'sink'
  SRC: 'src'
    Pad Template: 'src'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "identity0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  qos                 : Handle Quality-of-Service events
                        flags: readable, writable
                        Boolean. Default: false
  sleep-time          : Microseconds to sleep between processing
                        flags: readable, writable
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  error-after         : Error after N buffers
                        flags: readable, writable
                        Integer. Range: -2147483648 - 2147483647 Default: -1 
  drop-probability    : The Probability a buffer is dropped
                        flags: readable, writable
                        Float. Range:               0 -               1 Default:               0 
  datarate            : (Re)timestamps buffers with number of bytes per second (0 = inactive)
                        flags: readable, writable
                        Integer. Range: 0 - 2147483647 Default: 0 
  silent              : silent
                        flags: readable, writable
                        Boolean. Default: true
  single-segment      : Timestamp buffers and eat segments so as to appear as one segment
                        flags: readable, writable
                        Boolean. Default: false
  last-message        : last-message
                        flags: readable
                        String. Default: null
  dump                : Dump buffer contents to stdout
                        flags: readable, writable
                        Boolean. Default: false
  sync                : Synchronize to pipeline clock
                        flags: readable, writable
                        Boolean. Default: false
  check-imperfect-timestamp: Send element messages if timestamps and durations do not match up
                        flags: readable, writable
                        Boolean. Default: false
  check-imperfect-offset: Send element messages if offset and offset_end do not match up
                        flags: readable, writable
                        Boolean. Default: false
  signal-handoffs     : Send a signal before pushing the buffer
                        flags: readable, writable
                        Boolean. Default: true

Element Signals:
  "handoff" :  void user_function (GstElement* object,
                                   GstBuffer* arg0,
                                   gpointer user_data);


={============================================================================
*kt_dev_bcast_302* gst: elements: queue

<element>
http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-plugins/html/gstreamer-plugins-queue.html

// to give a delay
! queue min-threshold-time=2000000000

// to flush data more quickly
! queue max-size-bytes=10000

The “max-size-bytes” property

“max-size-bytes”           guint

Max. amount of data in the queue (bytes, 0=disable).

Flags: Read / Write

note: Default value: 10485760


gst-inspect-1.0 queue

Factory Details:
  Rank                     none (0)
  Long-name                Queue
  Klass                    Generic
  Description              Simple data queue
  Author                   Erik Walthinsen <omega@cse.ogi.edu>

Plugin Details:
  Name                     coreelements
  Description              GStreamer core elements
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstcoreelements.so
  Version                  1.5.0.1
  License                  LGPL
  Source module            gstreamer
  Source release date      2015-08-03 14:00 (UTC)
  Binary package           GStreamer git
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstQueue

Pad Templates:
  SRC template: 'src'
    Availability: Always
    Capabilities:
      ANY

  SINK template: 'sink'
    Availability: Always
    Capabilities:
      ANY


Element Flags:
  no flags set

Element Implementation:
  Has change_state() function: gst_element_change_state_func

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SINK: 'sink'
    Pad Template: 'sink'
  SRC: 'src'
    Pad Template: 'src'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "queue0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  current-level-buffers: Current number of buffers in the queue
                        flags: readable
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  current-level-bytes : Current amount of data in the queue (bytes)
                        flags: readable
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  current-level-time  : Current amount of data in the queue (in ns)
                        flags: readable
                        Unsigned Integer64. Range: 0 - 18446744073709551615 Default: 0 
  max-size-buffers    : Max. number of buffers in the queue (0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer. Range: 0 - 4294967295 Default: 200 
  max-size-bytes      : Max. amount of data in the queue (bytes, 0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer. Range: 0 - 4294967295 Default: 10485760 
  max-size-time       : Max. amount of data in the queue (in ns, 0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer64. Range: 0 - 18446744073709551615 Default: 1000000000 
  min-threshold-buffers: Min. number of buffers in the queue to allow reading (0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  min-threshold-bytes : Min. amount of data in the queue to allow reading (bytes, 0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer. Range: 0 - 4294967295 Default: 0 
  min-threshold-time  : Min. amount of data in the queue to allow reading (in ns, 0=disable)
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Unsigned Integer64. Range: 0 - 18446744073709551615 Default: 0 
  leaky               : Where the queue leaks, if at all
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Enum "GstQueueLeaky" Default: 0, "no"
                           (0): no               - Not Leaky
                           (1): upstream         - Leaky on upstream (new buffers)
                           (2): downstream       - Leaky on downstream (old buffers)
  silent              : Don't emit queue signals
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Boolean. Default: false
  flush-on-eos        : Discard all data in the queue when an EOS event is received
                        flags: readable, writable, changeable in NULL, READY, PAUSED or PLAYING state
                        Boolean. Default: false

Element Signals:
  "underrun" :  void user_function (GstElement* object,
                                    gpointer user_data);
  "running" :  void user_function (GstElement* object,
                                   gpointer user_data);
  "overrun" :  void user_function (GstElement* object,
                                   gpointer user_data);
  "pushing" :  void user_function (GstElement* object,
                                   gpointer user_data);


={============================================================================
*kt_dev_bcast_302* gst: elements: typefind

http://gstreamer.freedesktop.org/
data/doc/gstreamer/head/gstreamer-plugins/html/gstreamer-plugins-typefind.html

Properties

GstCaps *   caps        Read
GstCaps *   force-caps  Read / Write
guint       minimum     Read / Write

Signals

void        have-type   Run Last

Types and Values

struct GstTypeFindElement

Object Hierarchy

    GObject
    ╰── GInitiallyUnowned
        ╰── GstObject
            ╰── GstElement
                ╰── GstTypeFindElement


Description

Determines the media-type of a stream. It applies typefind functions in the
order of their rank. Once the type has been detected it sets its 'src' pad caps
to the found media type.

Whenever a type is found the "have-type" signal is emitted, either from the
streaming thread or the application thread (the latter may happen when
    typefinding is done pull-based from the state change function).

Plugins can register custom typefinders by using GstTypeFindFactory.


Element Information

plugin coreelements
author Benjamin Otte <in7y118@public.uni-hamburg.de>
class Generic


Element Pads

name      sink
direction sink
presence  always
details   ANY

name      src
direction source
presence  always
details   ANY


Types and Values

struct GstTypeFindElement;

Opaque GstTypeFindElement data structure


Property Details

The "caps" property

  "caps"                     GstCaps *

detected capabilities in stream.

Flags: Read


The “force-caps” property

  “force-caps”               GstCaps *

force caps without doing a typefind.

Flags: Read / Write


The “minimum” property

  “minimum”                  guint

minimum probability required to accept caps.

Flags: Read / Write
Allowed values: [1,100] Default value: 1


Signal Details

The "have-type" signal

void
user_function (GstTypeFindElement *typefind,
               guint               probability,
               GstCaps            *caps,
               gpointer            user_data)

This signal gets emitted when the type and its probability has been found.

Parameters

typefind      the typefind instance
probability   the probability of the type found
caps          the caps of the type found
user_data     user data set when the signal handler was connected.

Flags: Run Last


={============================================================================
*kt_dev_bcast_302* gst-fakesink

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-plugins/html\
/gstreamer-plugins-fakesink.html

./gstreamer/plugins/elements/gstfakesink.c

Properties

gboolean                can-activate-pull   Read / Write
gboolean                can-activate-push   Read / Write
gboolean                dump                Read / Write  note:
gchar *                 last-message        Read
gint                    num-buffers         Read / Write
gboolean                signal-handoffs     Read / Write
gboolean                silent              Read / Write
GstFakeSinkStateError   state-error         Read / Write


={============================================================================
*kt_dev_bcast_302* gst: elements: hlsdemux

{hls}
http://www.iana.org/assignments/media-types/application/vnd.apple.mpegurl
vnd.apple.mpegurl application/vnd.apple.mpegurl

Additional information :

1. Magic number(s) : #EXTM3U
2. File extension(s) : m3u, m3u8
3. Macintosh file type code : none
4. Object Identifiers: none


/zinc-build-root/debug-humax-dtr_t1000/Zinc/Zinc.3rdPartyStack/src$ ag GST_STATIC_CAPS | ag hls
gst_plugins_bad-1.5.0/ext/hls/gsthlssink.c:68:    GST_STATIC_CAPS_ANY);
gst_plugins_bad-1.5.0/ext/hls/gsthlsdemux.c:50:    GST_STATIC_CAPS_ANY);
gst_plugins_bad-1.5.0/ext/hls/gsthlsdemux.c:55:    GST_STATIC_CAPS ("application/x-hls"));
gst_plugins_base-1.5.0/gst/typefind/gsttypefindfunctions.c:427:static GstStaticCaps hls_caps = GST_STATIC_CAPS ("application/x-hls");


<hlsdemux>
[root@HUMAX /]# gst-inspect-1.0 hlsdemux

Pad Templates:
  SRC template: 'src_%u'
    Availability: Sometimes
    Capabilities:
      ANY

  SINK template: 'sink'
    Availability: Always
    Capabilities:
      application/x-hls

Element Properties:

  num-retries         : Number of fragment fetch retries befor error
                        flags: readable, writable
                        Unsigned Integer. Range: 0 - 1000 Default: 5 
  retry-min-time      : Minimal time after a retry will be attempted in seconds
                        flags: readable, writable
                        Unsigned Integer. Range: 1 - 100 Default: 1 
  retry-max-time      : Maximum time between retry attemptes in seconds
                        flags: readable, writable
                        Unsigned Integer. Range: 1 - 100 Default: 5 

application%2Fx-hls


={============================================================================
*kt_dev_bcast_302* gst: elements: dashdemux

{dash}
dash+xml application/dash+xml
http://www.iana.org/assignments/media-types/application/dash+xml

/zinc-build-root/debug-humax-dtr_t1000/Zinc/Zinc.3rdPartyStack/src$ ag "application/dash"

gst_plugins_bad-1.5.0/ext/dash/gstplugin.c
74:/*** application/dash+xml typefind helper ***/
76:static GstStaticCaps dash_caps = GST_STATIC_CAPS ("application/dash+xml");
95:  if (!gst_type_find_register (plugin, "application/dash+xml",

gst_plugins_bad-1.5.0/ext/dash/gstdashdemux.c
171:    GST_STATIC_CAPS ("application/dash+xml"));

gst_plugins_base-1.5.0/gst/playback/gsturidecodebin.c
1293:  "application/dash+xml", NULL


<adaptivedemux>
note:
No adaptivedemux when use gst-inspect-1.0 but is in the logs:

adaptivedemux
gstadaptivedemux.c:623:gst_adaptive_demux_handle_message:<dashdemux0:audio_00>
Source posted error: 1480:5 Could not establish connection to server.
(gstsouphttpsrc.c(1565): gst_soup_http_src_parse_status ():
 /GstPipeline:pipeline/GstBin:VirtualBinSrc/GstDecodeBin:decodebin1/GstDashDemux:dashdemux0/GstSoupHTTPSrc:souphttpsrc1:


[root@HUMAX /]# gst-inspect-1.0 dashdemux
Factory Details:
  Rank                     primary (256)
  Long-name                DASH Demuxer
  Klass                    Codec/Demuxer/Adaptive
  Description              Dynamic Adaptive Streaming over HTTP demuxer
  Author                   David Corvoysier <david.corvoysier@orange.com>
                Hamid Zakari <hamid.zakari@gmail.com>
                Gianluca Gennari <gennarone@gmail.com>

Plugin Details:
  Name                     dashdemux
  Description              DASH demuxer plugin
  Filename                 /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstdashdemux.so
  Version                  1.6.0
  License                  LGPL
  Source module            gst-plugins-bad
  Source release date      2015-09-25
  Binary package           GStreamer Bad Plug-ins
  Origin URL               Unknown package origin

GObject
 +----GInitiallyUnowned
       +----GstObject
             +----GstElement
                   +----GstBin
                         +----GstAdaptiveDemux
                               +----GstDashDemux

Implemented Interfaces:
  GstChildProxy

Pad Templates:
  SRC template: 'audio_%02u'
    Availability: Sometimes
    Capabilities:
      ANY

  SRC template: 'video_%02u'
    Availability: Sometimes
    Capabilities:
      ANY

  SINK template: 'sink'
    Availability: Always
    Capabilities:
      application/dash+xml


Element Flags:
  no flags set

Bin Flags:
  no flags set

Element Implementation:
  Has change_state() function: 0x7764c1f0

Element has no clocking capabilities.
Element has no URI handling capabilities.

Pads:
  SINK: 'sink'
    Pad Template: 'sink'

Element Properties:
  name                : The name of the object
                        flags: readable, writable
                        String. Default: "dashdemux0"
  parent              : The parent of the object
                        flags: readable, writable
                        Object of type "GstObject"
  async-handling      : The bin will handle Asynchronous state changes
                        flags: readable, writable
                        Boolean. Default: false
  message-forward     : Forwards all children messages
                        flags: readable, writable
                        Boolean. Default: false
  num-lookback-fragments: The number of fragments the demuxer will look back to calculate an average bitrate
                        flags: readable, writable
                        Unsigned Integer. Range: 1 - 4294967295 Default: 3 
  connection-speed    : Network connection speed in kbps (0 = calculate from downloaded fragments)
                        flags: readable, writable
                        Unsigned Integer. Range: 0 - 4294967 Default: 0 
  bitrate-limit       : Limit of the available bitrate to use when switching to alternates.
                        flags: readable, writable
                        Float. Range:               0 -               1 Default:             0.8 
  max-buffering-time  : Maximum number of seconds of buffer accumulated during playback(deprecated)
                        flags: readable, writable, deprecated
                        Unsigned Integer. Range: 2 - 4294967295 Default: 30 
  bandwidth-usage     : Percentage of the available bandwidth to use when selecting representations (deprecated)
                        flags: readable, writable
                        Float. Range:               0 -               1 Default:             0.8 
  max-bitrate         : Max of bitrate supported by target decoder
                        flags: readable, writable
                        Unsigned Integer. Range: 1000 - 4294967295 Default: 24000000 

Element Signals:
  "pad-added" :  void user_function (GstElement* object,
                                     GstPad* arg0,
                                     gpointer user_data);
  "pad-removed" :  void user_function (GstElement* object,
                                       GstPad* arg0,
                                       gpointer user_data);
  "no-more-pads" :  void user_function (GstElement* object,
                                        gpointer user_data);


={============================================================================
*kt_dev_bcast_310* gst-registry

gst-init can use a gst-registry cache that can store information about existing
plugins. This greatly improves performance, as the discovery of plugins can be
only done when gst-init() is ran for the first time.  We configure this cache
using runBrowser.sh script, and if it does not exist, with the use of
gst-inspect tool we force it to be populted.

Unfortunately, gstreamer performs a limited validation of this cache, checking
only for it's version. This leads to potential problems, when plugins are
changed / renamed. Such a problem was discovered by CANTST-16517 Error 02100
when playing iPlayer programmes.  Between two affected releases name of one of
the gst-plugins has changed (DEVARCH-9248), namely: libgstfragmented became
ibgsthlsdemux.

It appears that gst plugin (gstregistry.bin) was not automatically rebuilt by
the gstreamer following the software upgrade, and non-existing plugins were
refered to in attempt to build the playback pipeline.

Ideally, the gstreamer could invalidate such a cache in such case, and it would
have been re-built automatically to match the changes performed during the
upgrade. As a quick solution, as part of DEVARCH-9643, runBrowser.sh script was
updated to reflect that there were changes by using a new name for the cachefile
(gstregistry.bin) appending a suffix with a version and deleting all of the
previous versions if a cache-file of the latest version did not exist. This,
however, would require updates to release validation process to make sure the
    version was increased whenever there was a need for it.

This change should make it less error-prone and fully automated.

# enable gstreamer plugin registy
GST_INSPECT_CMD="${GST_INSPECT_CMD:-$prefix/oss/bin/gst-inspect-1.0}"
gst_registry_file="$app_data_dir/gstregistry.bin"
if [ ! -s "${gst_registry_file}" ]; then
    GST_REGISTRY="$gst_registry_file" $GST_INSPECT_CMD &> /dev/null
fi


http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/gst-running.html

GST_REGISTRY, GST_REGISTRY_1_0.  

Set this environment variable to make GStreamer use a different file for the
plugin cache / registry than the default one. This is useful when operating in a
separate environment which should not affect the default cache in the user's
home directory. 


DEBUG           GST_REGISTRY gstregistry.c:476:gst_registry_add_plugin:<registry0> 
  adding plugin 0x44dd60 for filename 
  "/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstvideotestsrc.so"


<blacklist>

// when there is no lib file in the search path

gst-inspect-1.0 cencdec
No such element or plugin 'cencdec'

// when there is a lib file in the search path but failed to load

gst-inspect-1.0 cencdec
ERROR     GST_PLUGIN_LOADING gstpluginloader.c:277:plugin_loader_replay_pending: 
  Plugin file /opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstcenccrypto.so failed to load. Blacklisting
No such element or plugin 'cencdec'

// after blackedlisted

gst-inspect-1.0 cencdec
No such element or plugin 'cencdec'

// to reset blacklist, rm it.

<on-box>
ls .cache/gstreamer-1.0/registry.mipsel.bin 
.cache/gstreamer-1.0/registry.mipsel.bin


={============================================================================
*kt_dev_bcast_310* gst-scanner

./opt/zinc/oss/libexec/gstreamer-1.0/gst-plugin-scanner


={============================================================================
*kt_dev_bcast_310* gst-debug

http://docs.gstreamer.com/display/GstSDK/Basic+tutorial+11%3A+Debugging+tools

Basic tutorial 11: Debugging tools

0	none	No debug information is output.

1	ERROR
Logs all fatal errors. These are errors that do not allow the core or elements
to perform the requested action. The application can still recover if programmed
to handle the conditions that triggered the error.

2	WARNING
Logs all warnings. Typically these are non-fatal, but user-visible problems are
expected to happen.

3	INFO
Logs all informational messages. These are typically used for events in the
system that only happen once, or are important and rare enough to be logged at
this level.

4	DEBUG
Logs all debug messages. These are general debug messages for events that happen
only a limited number of times during an object's lifetime; these include setup,
teardown, change of parameters, ...

5	LOG
Logs all log messages. These are messages for events that happen repeatedly
during an object's lifetime; these include streaming and steady-state
conditions.

To enable debug output, set the GST_DEBUG environment variable to the desired
debug level. All levels below that will also be shown (i.e., if you set
    GST_DEBUG=2, you will get both ERROR and WARNING messages).

Furthermore, each plugin or part of the GStreamer defines its own category, so
you can specify a debug level for each individual category. For example,
GST_DEBUG=2,audiotestsrc:5, will use Debug Level 5 for the audiotestsrc element,
and 2 for all the others.

The '*' wildcard is also available. For example GST_DEBUG=2,audio*:5 will use
Debug Level 5 for all categories starting with the word audio. GST_DEBUG=*:2 is
equivalent to GST_DEBUG=2.

note:
Use gst-launch-0.10 --gst-debug-help to obtain the list of all registered
categories. Bear in mind that each plugin registers its own categories, so, when
installing or removing plugins, this list can change.


[root@HUMAX /]# gst-launch-1.0 --gst-debug-help
<VQEC-3-VQEC_ERROR> : CNAME: no mac addr found using defined input_ifname
<VQEC-3-VQEC_ERROR> : error loading configuration file
Channel summary: 0 out of 0 channels passed the validation

name                  level    description
---------------------+--------+--------------------------------
GST_BUFFER            0   no description
GST_BUFFER_LIST       0   no description
GST_BUS               0   no description
GST_CALL_TRACE        0   no description
GST_CAPS              0   no description
GST_CLOCK             0   no description
GST_CONTEXT           0   no description
GST_DATAFLOW          0   dataflow inside pads
GST_DEBUG             0   debugging subsystem
GST_ELEMENT_FACTORY   0   element factories keep information about installed elements
GST_ELEMENT_PADS      0   no description
GST_ERROR_SYSTEM      0   no description
GST_EVENT             0   no description
GST_INIT              0   no description
GST_LOCKING           0   locking
GST_MEMORY            0   memory
GST_MESSAGE           0   no description
GST_META              0   meta
GST_NEGOTIATION       0   no description
...

<envs>
You can send the gstreamer debug output to a file, rather than to the console by
adding the following two lines to the runBrowser.sh script:

GST_DEBUG_NO_COLOR=1 \
GST_DEBUG_FILE=/app-data/gst-log.txt \
GST_DEBUG=5


<logline>
The content of each line in the debug output is:

0:00:00.868050000  1592   09F62420 WARN   filesrc
  gstfilesrc.c:1044:gst_file_src_start:<filesrc0> error: No such file
  "non-existing-file.webm"

0:00:00.868050000	Time stamp in HH:MM:SS.sssssssss format since the start of the
program

1592	Process ID from which the message was issued. Useful when your problem
involves multiple processes

09F62420	Thread ID from which the message was issued. Useful when your problem
involves multiple threads

filesrc	Debug Category of the message

<filesrc0>	Name of the object that issued the message. It can be an element, a
Pad, or something else. Useful when you have multiple elements of the same kind
and need to distinguish among them.

Naming your elements with the name property will make this debug output more
readable (otherwise, GStreamer assigns each new element a unique name).


<in-source>
To do so, use the GST_ERROR(), GST_WARNING(), GST_INFO(), GST_LOG() and GST_DEBUG() macros. They
accept the same parameters as printf, and they use the default category (default will be shown as
        the Debug category in the output log).

To change the category to something more meaningful, add these two lines at the top of your code:

GST_DEBUG_CATEGORY_STATIC (my_category);
#define GST_CAT_DEFAULT my_category

And then this one after you have initialized GStreamer with gst_init():

GST_DEBUG_CATEGORY_INIT (my_category, "my category", 0, "This is my very own");

This registers a new category (this is, for the duration of your application: it is not stored in
        any file), and sets it as the default category for your code. See the documentation for
    GST_DEBUG_CATEGORY_INIT().


<graph>
Getting pipeline graphs

For those cases where your pipeline starts to grow too large and you lose track of what is connected
with what, GStreamer has the capability to output graph files. These are .dot files, readable with
free programs like GraphViz, that describe the topology of your pipeline, along with the caps
negotiated in each link.

This is also very handy when using all-in-one elements like playbin2  or uridecodebin, which
instantiate several elements inside them. Use the .dot files to learn what pipeline they have
created inside (and learn a bit of GStreamer along the way).

To obtain .dot files, simply set the GST_DEBUG_DUMP_DOT_DIR environment variable to point to the
folder where you want the files to be placed. gst-launch will create a .dot file at each state
change, so you can see the evolution of the caps negotiation. Unset the variable to disable this
facility. From within your application, you can use the GST_DEBUG_BIN_TO_DOT_FILE() and
GST_DEBUG_BIN_TO_DOT_FILE_WITH_TS() macros to generate .dot files at your convenience.

dot -Tjpeg gst-launch.PLAYING_PAUSED.dot -o gst-launch.PLAYING_PAUSED.jpg


={============================================================================
*kt_dev_bcast_311* gst: gst-launch

Basic tutorial 10: GStreamer tools
http://docs.gstreamer.com/display/GstSDK/Basic+tutorial+10%3A+GStreamer+tools

The command line for gst-launch consists of a list of options followed by a
PIPELINE-DESCRIPTION.  Some simplified instructions are given next, see the
complete documentation at the reference page for gst-launch.

In simple form, a PIPELINE-DESCRIPTION is a list of element types separated by
exclamation marks (!). Go ahead and type in the following command:

gst-launch-0.10 videotestsrc ! ffmpegcolorspace ! autovideosink


$ ./gst-launch-0.10 --help-all
Usage:
  gst-launch-0.10 [OPTION?] PIPELINE-DESCRIPTION

Help Options:
  -h, --help                        Show help options
  --help-all                        Show all help options
  --help-gst                        Show GStreamer Options

GStreamer Options
  --gst-version                     Print the GStreamer version
  --gst-fatal-warnings              Make all warnings fatal
  --gst-debug-help                  Print available debug categories and exit

  --gst-debug-level=LEVEL           Default debug level from 1 (only error) to 
                                    5 (anything) or 0 for no output

  --gst-debug=LIST                  Comma-separated list of category_name:level 
                                    pairs to set specific levels for the 
                                    individual categories. 
                                    Example: GST_AUTOPLUG:5,GST_ELEMENT_*:3

  --gst-debug-no-color              Disable coloured debugging output
  --gst-debug-disable               Disable debugging
  --gst-plugin-spew                 Enable verbose plugin loading diagnostics
  --gst-plugin-path=PATHS           Colon-separated paths containing plugins
  --gst-plugin-load=PLUGINS         Comma-separated list of plugins to preload 
                                    in addition to the list stored in environment 
                                    variable GST_PLUGIN_PATH
  --gst-disable-segtrap             Disable trapping of segmentation faults 
                                    during plugin loading
  --gst-disable-registry-update     Disable updating the registry
  --gst-disable-registry-fork       Disable spawning a helper process while 
                                    scanning the registry

Application Options:
  -t, --tags                        Output tags (also known as metadata)
  -v, --verbose                     Output status information and property notifications
  -q, --quiet                       Do not print any progress information
  -m, --messages                    Output messages
  -X, --exclude=TYPE1,TYPE2,...     Do not output status information of TYPE
  -o, --output=FILE                 Save xml representation of pipeline to FILE and exit
  -f, --no-fault                    Do not install a fault handler
  --no-sigusr-handler               Do not install signal handlers for SIGUSR1 and SIGUSR2
  -T, --trace                       Print alloc trace (if enabled at compile time)
  -e, --eos-on-shutdown             Force EOS on sources before shutting the pipeline down
  -i, --index                       Gather and print index statistics
  --version                         Print version information and exit


{set-properties}
Properties may be appended to elements, in the form property=value (multiple
        properties can be specified, separated by spaces). Use the gst-inspect
tool (explained next) to find out the available properties for an element.

gst-launch-0.10 videotestsrc pattern=11 ! ffmpegcolorspace ! autovideosink


{name-elements-named-pad}
Elements can be named using the name property, in this way complex pipelines
involving branches can be created. Names allow linking to elements created
previously in the description, and are indispensable to use elements with
multiple output pads, like demuxers or tees, for example.

Named elements are referred to using their name followed by a dot.

gst-launch-0.10 \
   videotestsrc ! ffmpegcolorspace ! tee name=t ! queue ! autovideosink t. ! queue ! autovideosink
                                                                                     ^^
   videotestsrc ! ffmpegcolorspace ! tee ! queue ! autovideosink 
                                     tee ! queue ! autovideosink

You should see two video windows, showing the same sample video pattern.

The tee is named simply 't' (using the name property) and then linked to a
queue and an autovideosink. The same tee is referred to using 't.' (mind the
        dot) and then linked to a second queue and a second autovideosink.


{cap-filters}
When an element has more than one output pad, it might happen that the link to
the next element is ambiguous: the next element may have more than one
compatible input pad, or its input pad may be compatible with the Pad Caps of
all the output pads. In these cases GStreamer will link using the first pad
that is available, which pretty much amounts to saying that GStreamer will
choose one output pad at random.

Consider the following pipeline:

gst-launch-0.10 
   souphttpsrc location=http://docs.gstreamer.com/media/sintel_trailer-480p.webm 
   ! matroskademux ! filesink location=test

This is the same media file and demuxer as in the previous example. The input
Pad Caps of filesink are ANY, meaning that it can accept any kind of media.
Which one of the two output pads of matroskademux will be linked against the
filesink? video_00 or audio_00? You cannot know.

You can remove this ambiguity, though, by using named pads, as in the previous
sub-section, or by using Caps Filters:

gst-launch-0.10 
   souphttpsrc location=http://docs.gstreamer.com/media/sintel_trailer-480p.webm 
   ! matroskademux ! video/x-vp8 ! matroskamux ! filesink location=sintel_video.mkv

A Caps Filter behaves like a pass-through element which does nothing and only
accepts media with the given Caps, effectively resolving the ambiguity. In
this example, between matroskademux and matroskamux we added a video/x-vp8
Caps Filter to specify that we are interested in the output pad of
matroskademux which can produce this kind of video.


{example}
gst-launch-1.0 
   souphttpsrc location=http://test-media.youview.co.uk/ondemand/bbb/avc3/1/2drm_manifest.mpd 
   ! dashdemux name=dash dash. 
   ! video/quicktime 
   ! qtdemux 
   ! queue 
   ! cencdec sas-url=https://ms3.youview.co.uk/s/Big+Buck+Bunny+DASH+2#http://test-media.youview.co.uk/ondemand/bbb/avc3/1/2drm_manifest.mpd 
   ! h264parse 
   ! video/x-h264,stream-format=byte-stream 
   ! nexussink dash. 
   ! audio/x-m4a '!' qtdemux '!' cencdec '!' aacparse '!' nexussink

   souphttpsrc 
   ! dashdemux name=dash dash. ! video/quicktime ! qtdemux ! queue ! cencdec ! h264parse ! video/x-h264,stream-format=byte-stream ! nexussink dash. 
                         dash. ! audio/x-m4a ! qtdemux ! cencdec ! aacparse ! nexussink

gst-launch-1.0 
   souphttpsrc location=http://test-media.youview.co.uk/ondemand/bbb/avc3/1/2drm_manifest.mpd 
   ! dashdemux name=dash dash. 
   ! video/quicktime 
   ! qtdemux 
   ! cencdec sas-url=https://ms3.youview.co.uk/s/Big+Buck+Bunny+DASH+2 
   ! h264parse 
   ! mpegtsmux name=m 
   ! tsnexusbin dash. 
   ! audio/x-m4a ! qtdemux ! cencdec ! aacparse ! m.

   souphttpsrc 
   ! dashdemux name=dash dash. ! video/quicktime ! qtdemux ! cencdec ! h264parse ! mpegtsmux name=m ! tsnexusbin dash. 
                         dash. ! audio/x-m4a ! qtdemux ! cencdec ! aacparse      ! mpegtsmux


={============================================================================
*kt_dev_bcast_311* gst: gst-inspect

This tool has three modes of operation:

o Without arguments, it lists 'all' available elements types, this is, the types you can use to
instantiate new elements.

o With a file name as an argument, it treats the file as a GStreamer plugin, tries to open it, and
lists all the elements described inside.

o With a GStreamer 'element' name as an argument, it lists all information regarding that element.

The most relevant sections are:

o Pad Templates (line 25): This lists all the kinds of Pads this element can have, along with their
                           capabilities. This is where you look to find out if an element can link
                           with another one. In this case, it has only one sink pad template,
                           accepting only video/x-vp8 (encoded video data in VP8 format) and only
                           one source pad template, producing video/x-raw-yuv (decoded video data).

o Element Properties (line 70): This lists the properties of the element, along with their type and
                                accepted values.


{gst-discoverer}
This tool is a wrapper around the GstDiscoverer object shown in Basic tutorial 9: Media information
gathering. It accepts a URI from the command line and prints all information regarding the media
that GStreamer can extract. It is useful to find out what container and codecs have been used to
produce the media, and therefore what elements you need to put in a pipeline to play it.

Use gst-discoverer --help to obtain the list of available options, which basically control the
amount of verbosity of the output.


={============================================================================
*kt_dev_bcast_312* gst: sdk and tutorials

http://docs.gstreamer.com/display/GstSDK/Installing+on+Linux


={============================================================================
*kt_dev_bcast_319* gst: mime and types

GStreamer already supports many basic media types. Following is a table of a few of the basic types
used for buffers in GStreamer. The table contains the name ("media type") and a description of the
type, the properties associated with the type, and the meaning of each property. A full list of
supported types is included in List of Defined Types.

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/pwg/html/section-types-definitions.html

note: no subtitles yet.

from gstreamer-1.5.0/docs/random/mimetypes

MIME types in GStreamer

What is a MIME type ?
=====================

A MIME type is a combination of two (short) strings (words)---the content type
and the content subtype. Content types are broad categories used for describing
almost all types of files: video, audio, text, and application are common
content types. The subtype further breaks the content type down into a more
specific type description, for example 'application/ogg', 'audio/raw',
'video/mpeg', or 'text/plain'.

So the content type and subtype make up a pair that describes the type of
information contained in a file. In multimedia processing, MIME types are used
to describe the type of information carried by a media stream. In GStreamer, we
use MIME types in the same way, to identify the types of information that are
allowed to pass between GStreamer elements. The MIME type is part of a GstCaps
object that describes a media stream. Besides a MIME type, a GstCaps object also
contains a name and some stream properties (GstProps, which hold combinations of
key/value pairs).

An example of a MIME type is 'video/mpeg'. A corresponding GstCaps could be
created using code:

GstCaps *caps = gst_caps_new_simple ("video/mpeg",
				     "width",  G_TYPE_INT, 384,
				     "height", G_TYPE_INT, 288,
				     NULL);

MIME types and their corresponding properties are of major importance in
GStreamer for uniquely identifying media streams. Therefore, we define them
per media type. All GStreamer plugins should keep to this definition.

Official MIME media types are assigned by the IANA. Current assignments are at
http://www.iana.org/assignments/media-types/.

The problems
============

Some streams may have MIME types or GstCaps that do not fully describe the
stream. In most cases, this is not a problem, though. For example, if a stream
contains Ogg/Vorbis data (which is of type 'application/ogg'), we don't need to
know the samplerate of the raw audio stream, since we can't play the encoded
audio anyway. The samplerate is, however, important for raw audio, so a decoder
would need to retrieve the samplerate from the Ogg/Vorbis stream headers (the
headers are part of the bytestream) in order to pass it on in the GstCaps that
belongs to the decoded audio (which becomes a type like 'audio/raw'). However,
other plugins might want to know such properties, even for compressed streams.
One such example is an AVI muxer, which does want to know the samplerate of an
audio stream, even when it is compressed.

Another problem is that many media types can be defined in multiple ways. For
example, MJPEG video can be defined as 'video/jpeg', 'video/mjpeg',
'image/jpeg', 'video/x-msvideo' with a compression of (fourcc) MJPG, etc.
None of these is really official, since there isn't an official mimetype
for encoded MJPEG video.

The main focus of this document is to 'propose' a standardized set of MIME types
and properties that will be used by the GStreamer plugins.

Different types of streams
==========================

There are several types of media streams. The most important distinction will be
container formats, audio codecs and video codecs. Container formats are
bytestreams that contain one or more substreams inside it, and don't provide any
direct media data itself. Examples are Quicktime, AVI or MPEG System Stream.
They mostly contain of a set of headers that define the media streams that are
packed inside the container, along with the media data itself.

Video codecs and audio codecs describe encoded audio or video data. Examples are
MPEG-1 video, DivX video, MPEG-1 layer 3 (MP3) audio or Ogg/Vorbis audio.
Actually, Ogg is a container format too (for Vorbis audio), but these are
usually used in conjunction with each other.

Finally, there are the somewhat obvious (but not commonly encountered as files)
raw data formats.

Container formats
-----------------

1 - AVI (Microsoft RIFF/AVI)
    MIME type: video/x-msvideo
    Properties:
    Parser: avidemux, ffdemux_avi
    Formatter: avimux

2 - Quicktime (Apple)
    MIME type: video/quicktime
    Properties:
    Parser: qtdemux
    Formatter:

3 - MPEG (MPEG LA)
    MIME type: video/mpeg
    Properties: 'systemstream' = TRUE (BOOLEAN)
    Parser: mpegdemux, ffdemux_mpeg (PS), ffdemux_mpegts (TS), dvddemux
    Formatter: mplex

4 - ASF (Microsoft)
    MIME type: video/x-ms-asf
    Properties:
    Parser: asfdemux, ffdemux_asf
    Formatter: asfmux

5 - WAV (Microsoft RIFF/WAV)
    MIME type: audio/x-wav
    Properties:
    Parser: wavparse, ffdemux_wav
    Formatter: wavenc

6 - RealMedia (Real)
    MIME type: application/vnd.rn-realmedia
    Properties: 'systemstream' = TRUE (BOOLEAN)
    Parser: rmdemux, ffdemux_rm
    Formatter:

7 - DV (Digital Video)
    MIME type: video/x-dv
    Properties: 'systemstream' = TRUE (BOOLEAN)
    Parser: gst1394, ffdemux_dv
    Formatter:

8 - Ogg (Xiph)
    MIME type: application/ogg
    Properties:
    Parser: oggdemux
    Formatter: oggmux

9 - Matroska
    MIME type: video/x-mkv
    Properties:
    Parser: matroskademux, ffdemux_matroska
    Formatter: matroskamux

10 - Shockwave (Macromedia)
     MIME type: application/x-shockwave-flash
     Properties:
     Parser: swfdec, ffdemux_swf
     Formatter:

11 - AU audio (Sun)
     MIME type: audio/x-au
     Properties:
     Parser: auparse, ffdemux_au
     Formatter:

12 - Mod audio
     MIME type: audio/x-mod
     Properties:
     Parser: modplug, mikmod
     Formatter:

13 - FLX video
     MIME type: video/x-fli
     Properties:
     Parser: flxdec
     Formatter:

14 - Monkeyaudio
     MIME type: application/x-ape
     Properties:
     Parser:
     Formatter:

15 - AIFF audio
     MIME type: audio/x-aiff
     Properties:
     Parser:
     Formatter:

16 - SID audio
     MIME type: audio/x-sid
     Properties:
     Parser: siddec
     Formatter:

Please note that we try to keep these MIME types as similar as possible to the
MIME types used as standards in Gnome (Gnome-VFS/Nautilus) and KDE
(Konqueror). Both will (in future) stick to a shared-mime-info database that
is hosted on freedesktop.org, and bases itself on IANA.

Also, there is a very thin line between audio codecs and audio containers
(take mp3 vs. sid, etc.). This is just a per-case thing right now and needs to
be documented further.

Video codecs
------------

For convenience, the fourcc codes used in the AVI container format will be
listed along with the MIME type and optional properties.

Optional properties for all video formats are the following:

width = 1 - MAXINT (INT)
height = 1 - MAXINT (INT)
pixel_width = 1 - MAXINT (INT, with pixel_height forms aspect ratio)
pixel_height = 1 - MAXINT (INT, with pixel_width forms aspect ratio)
framerate = 0 - MAXFLOAT (FLOAT)

1 - MPEG-1, -2 and -4 video (ISO/LA MPEG)
    MIME type: video/mpeg
    Properties: systemstream = FALSE (BOOLEAN)
                mpegversion = 1/2/4 (INT)
    Known fourccs: MPEG, MPGI
    Encoder: mpeg1enc, mpeg2enc
    Decoder: mpeg1dec, mpeg2dec, mpeg2subt

2 - DivX 3.x, 4.x and 5.x video (divx.com)
    MIME type: video/x-divx
    Properties:
    Optional properties: divxversion = 3/4/5 (INT)
    Known fourccs: DIV3, DIV4, DIV5, DIVX, DX50, DIVX, divx
    Encoder: divxenc
    Decoder: divxdec, ffdec_mpeg4

3 - Microsoft MPEG 4.1, 4.2 and 4.3
    MIME type: video/x-msmpeg
    Properties:
    Optional properties: msmpegversion = 41/42/43 (INT)
    Known fourccs: MPG4, MP42, MP43
    Encoder: ffenc_msmpeg4, ffenc_msmpeg4v1, ffenc_msmpeg4v2
    Decoder: ffdec_msmpeg4, ffdec_msmpeg4v1, ffdec_msmpeg4v2

4 - Motion-JPEG (official and extended)
    MIME type: video/x-jpeg
    Properties:
    Known fourccs: MJPG (YUY2 MJPEG), JPEG (any), PIXL (Pinnacle/Miro), VIXL
    Encoder: jpegenc
    Decoder: jpegdec, ffdec_mjpeg

5 - Sorensen (Quicktime - SVQ1/SVQ3)
    MIME types: video/x-svq
    Properties: svqversion = 1/3 (INT)
    Encoder:
    Decoder: ffdec_svq1, ffdec_svq3

6 - H263 and related codecs
    MIME type: video/x-h263
    Properties:
    Known fourccs: H263/h263, i263, L263, M263/m263, s263, x263, VDOW, VIVO
    Encoder: ffenc_h263, ffenc_h263p
    Decoder: ffdec_h263, ffdec_h263i

7 - RealVideo (Real)
    MIME type: video/x-pn-realvideo
    Properties: rmversion = "1"/"2"/"3"/"4" (INT)
    Known fourccs: RV10, RV20, RV30, RV40
    Encoder: ffenc_rv10
    Decoder: ffdec_rv10, ffdec_rv20

8 - Digital Video (DV)
    MIME type: video/x-dv
    Properties: systemstream = FALSE (BOOLEAN)
    Known fourccs: DVSD/dvsd (SDTV), dvhd (HDTV), dvsl (SDTV LongPlay)
    Encoder: ffenc_dvvideo
    Decoder: dvdec, ffdec_dvvideo

9 - Windows Media Video 1, 2 and 3 (WMV)
    MIME type: video/x-wmv
    Properties: wmvversion = 1/2/3 (INT)
    Encoder: ffenc_wmv1, ffenc_wmv2, none
    Decoder: ffdec_wmv1, ffdec_wmv2, none

10 - XviD (xvid.org)
     MIME type: video/x-xvid
     Properties:
     Known fourccs: xvid, XVID
     Encoder: xvidenc
     Decoder: xviddec, ffdec_mpeg4

11 - 3IVX (3ivx.org)
     MIME type: video/x-3ivx
     Properties:
     Known fourccs: 3IV0, 3IV1, 3IV2
     Encoder:
     Decoder:

12 - Ogg/Tarkin (Xiph)
     MIME type: video/x-tarkin
     Properties:
     Encoder:
     Decoder:

13 - VP3
     MIME type: video/x-vp3
     Properties:
     Encoder:
     Decoder: ffdec_vp3

14 - Ogg/Theora (Xiph, VP3-like)
     MIME type: video/x-theora
     Properties:
     Encoder: theoraenc
     Decoder: theoradec, ffdec_theora
     This is the raw stream that comes out of an ogg file.

15 - Huffyuv
     MIME type: video/x-huffyuv
     Properties:
     Known fourccs: HFYU
     Encoder:
     Decoder: ffdec_hfyu

16 - FF Video 1 (FFMPEG)
     MIME type: video/x-ffv
     Properties: ffvversion = 1 (INT)
     Encoder:
     Decoder: ffdec_ffv1

17 - H264
     MIME type: video/x-h264
     Properties:
     Known fourccs: VSSH
     Encoder:
     Decoder: ffdec_h264

18 - Indeo 3 (Intel)
     MIME type: video/x-indeo
     Properties: indeoversion = 3 (INT)
     Encoder:
     Decoder: ffdec_indeo3

19 - Portable Network Graphics (PNG)
     MIME type: video/x-png
     Properties:
     Encoder: pngenc
     Decoder: pngdec, gdkpixbufdec

20 - Cinepak
     MIME type: video/x-cinepak
     Properties:
     Encoder:
     Decoder: ffdec_cinepak

TODO: subsampling information for YUV?

TODO: colorspace identifications for MJPEG? How?

TODO: how to distinguish MJPEG-A/B (Quicktime) and lossless JPEG?

TODO: divx4/divx5/xvid/3ivx/mpeg-4 - how to make them overlap? (all
      ISO MPEG-4 compatible)

3c) Audio Codecs
----------------
For convenience, the two-byte hexcodes (as used for identification in AVI files)
are also given.

Properties for all audio formats include the following:

rate = 1 - MAXINT (INT, sampling rate)
channels = 1 - MAXINT (INT, number of audio channels)

1 - Alaw Raw Audio
    MIME type: audio/x-alaw
    Properties:
    Encoder: alawenc
    Decoder: alawdec

2 - Mulaw Raw Audio
    MIME type: audio/x-mulaw
    Properties:
    Encoder: mulawenc
    Decoder: mulawdec

3 - MPEG-1 layer 1/2/3 audio
    MIME type: audio/mpeg
    Properties: mpegversion = 1 (INT)
                layer = 1/2/3 (INT)
    Encoder: lame, ffdec_mp3
    Decoder: mad

4 - Ogg/Vorbis
    MIME type: audio/x-vorbis
    Encoder: rawvorbisenc (vorbisenc does rawvorbisenc+oggmux)
    Decoder: vorbisdec

5 - Windows Media Audio 1, 2 and 3 (WMA)
    MIME type: audio/x-wma
    Properties: wmaversion = 1/2/3 (INT)
    Encoder:
    Decoder: ffdec_wmav1, ffdec_wmav2, none

6 - AC3
    MIME type: audio/x-ac3
    Properties:
    Encoder: ffenc_ac3
    Decoder: a52dec, ac3parse

7 - FLAC (Free Lossless Audio Codec)
    MIME type: audio/x-flac
    Properties:
    Encoder: flacenc
    Decoder: flacdec, ffdec_flac

8 - MACE 3/6 (Quicktime audio)
    MIME type: audio/x-mace
    Properties: maceversion = 3/6 (INT)
    Encoder:
    Decoder: ffdec_mace3, ffdec_mace6

9 - MPEG-4 AAC
    MIME type: audio/mpeg
    Properties: mpegversion = 4 (INT)
    Encoder: faac
    Decoder: faad

10 - (IMA) ADPCM (Quicktime/WAV/Microsoft/4XM)
     MIME type: audio/x-adpcm
     Properties: layout = "quicktime"/"wav"/"microsoft"/"4xm"/"g721"/"g722"/"g723_3"/"g723_5" (STRING)
     Encoder: ffenc_adpcm_ima_[qt/wav/dk3/dk4/ws/smjpeg], ffenc_adpcm_[ms/4xm/xa/adx/ea]
     Decoder: ffdec_adpcm_ima_[qt/wav/dk3/dk4/ws/smjpeg], ffdec_adpcm_[ms/4xm/xa/adx/ea]

     Note: The difference between each of these four PCM formats is the number
           of samples packed together per channel. For WAV, for example, each
           sample is 4 bit, and 8 samples are packed together per channel in the
           bytestream. For the others, refer to technical documentation. We
           probably want to distinguish these differently, but I don't know how,
           yet.

11 - RealAudio (Real)
     MIME type: audio/x-pn-realaudio
     Properties: raversion ="1"/"2" (INT)
     Known fourccs: 14_4, 28_8
     Encoder:
     Decoder: ffdec_real_144 / ffdec_real_288

12 - DV Audio
     MIME type: audio/x-dv
     Properties:
     Encoder:
     Decoder:

13 - GSM Audio
     MIME type: audio/x-gsm
     Properties:
     Encoder: gsmenc, rtpgsmenc
     Decoder: gsmdec, rtpgsmparse

14 - Speex audio
     MIME type: audio/x-speex
     Properties:
     Encoder: speexenc
     Decoder: speexdec

15 - QDM2
     MIME type: audio/x-qdm2
     Properties:

16 - Sony ATRAC4 (detected inside realmedia and wave/avi streams, nothing to decode it yet)
     MIME type: audio/x-vnd.sony.atrac3
     Properties:
     Encoder:
     Decoder:

17 - Ensoniq PARIS audio
     MIME type: audio/x-paris
     Properties:
     Encoder:
     Decoder:

18 - Amiga IFF / SVX8 / SV16 audio
     MIME type: audio/x-svx
     Properties:
     Encoder:
     Decoder:

19 - Sphere NIST audio
     MIME type: audio/x-nist
     Properties:
     Encoder:
     Decoder:

20 - Sound Blaster VOC audio
     MIME type: audio/x-voc
     Properties:
     Encoder:
     Decoder:

21 - Berkeley/IRCAM/CARL audio
     MIME type: audio/x-ircam
     Properties:
     Encoder:
     Decoder:

22 - Sonic Foundry's 64 bit RIFF/WAV
     MIME type: audio/x-w64
     Properties:
     Encoder:
     Decoder:

TODO: adpcm/dv needs confirmation from someone with knowledge...

Raw formats
-----------

Raw formats contain unencoded, raw media information. These are rather rare from
an end user point of view since raw media files have historically been
prohibitively large ... hence the multitude of encoding formats.

Raw video formats require the following common properties, in addition to
format-specific properties:

width = 1 - MAXINT (INT)
height = 1 - MAXINT (INT)

1 - Raw Video (YUV/YCbCr)
    MIME type: video/x-raw-yuv
    Properties: 'format' = 'XXXX' (fourcc)
    Known fourccs: YUY2, I420, Y41P, YVYU, UYVY, etc.
    Properties:

    Some raw video formats have implicit alignment rules. We should discuss this
    more. Also, some formats have multiple fourccs (e.g. IYUV/I420 or
    YUY2/YUYV). For each of these, we only use one (e.g. I420 and YUY2).

    Currently recognized formats:

    YUY2: packed, Y-U-Y-V order, U/V hor 2x subsampled (YUV-4:2:2, 16 bpp)
    YVYU: packed, Y-V-Y-U order, U/V hor 2x subsampled (YUV-4:2:2, 16 bpp)
    UYVY: packed, U-Y-V-Y order, U/V hor 2x subsampled (YUV-4:2:2, 16 bpp)
    Y41P: packed, UYVYUYVYYYYY order, U/V hor 4x subsampled (YUV-4:1:1, 12 bpp)
    IUY2: packed, U-Y-V order, not subsampled (YUV-1:1:1, 24 bpp)

    Y42B: planar, Y-U-V order, U/V hor 2x subsampled (YUV-4:2:2, 16 bpp)
    YV12: planar, Y-V-U order, U/V hor+ver 2x subsampled (YUV-4:2:0, 12 bpp)
    I420: planar, Y-U-V order, U/V hor+ver 2x subsampled (YUV-4:2:0, 12 bpp)
    Y41B: planar, Y-U-V order, U/V hor 4x subsampled (YUV-4:1:1, 12bpp)
    YUV9: planar, Y-U-V order, U/V hor+ver 4x subsampled (YUV-4:1:0, 9bpp)
    YVU9: planar, Y-V-U order, U/V hor+ver 4x subsampled (YUV-4:1:0, 9bpp)

    Y800: one-plane (Y-only, YUV-4:0:0, 8bpp)

    See http://www.fourcc.org/ for more information.

    Note: YUV-4:4:4 (both planar and packed, in multiple orders) are missing.

2 - Raw video (RGB)
    MIME type: video/x-raw-rgb
    Properties: endianness = 1234/4321 (INT) <- use G_LITTLE_ENDIAN/G_BIG_ENDIAN
                depth = 15/16/24 (INT, color depth)
                bpp = 16/24/32 (INT, bits used to store each pixel)
                red_mask = bitmask (0x..) (INT)
                green_mask = bitmask (0x..) (INT)
                blue_mask = bitmask (0x..) (INT)

    24 and 32 bit RGB should always be specified as big endian, since any little
    endian format can be transformed into big endian by rearranging the color
    masks. 15 and 16 bit formats should generally have the same byte order as
    the CPU.

    Color masks are interpreted by loading 'bpp' number of bits using the given
    'endianness', and masking and shifting by each color mask. Loading a 24-bit
    value cannot be done directly, but one can perform an equivalent operation.

    Examples:
               msb .. lsb
      - memory: RRRRRRRR GGGGGGGG BBBBBBBB RRRRRRRR GGGGGGGG ...
                bpp        = 24
                depth      = 24
                endianness = 4321 (G_BIG_ENDIAN)
                red_mask   = 0xff0000
                green_mask = 0x00ff00
                blue_mask  = 0x0000ff

      - memory: xRRRRRGG GGGBBBBB xRRRRRGG GGGBBBBB xRRRRRGG ...
                bpp        = 16
                depth      = 15
                endianness = 4321 (G_BIG_ENDIAN)
                red_mask   = 0x7c00
                green_mask = 0x03e0
                blue_mask  = 0x003f

      - memory: GGGBBBBB xRRRRRGG GGGBBBBB xRRRRRGG GGGBBBBB ...
                bpp        = 16
                depth      = 15
                endianness = 1234 (G_LITTLE_ENDIAN)
                red_mask   = 0x7c00
                green_mask = 0x03e0
                blue_mask  = 0x003f

The raw audio formats require the following common properties, in addition to
format-specific properties:

rate = 1 - MAXINT (INT, sampling rate)
channels = 1 - MAXINT (INT, number of audio channels)
endianness = 1234/4321 (INT) <- use G_LITTLE_ENDIAN/G_BIG_ENDIAN/G_BYTE_ORDER

3 - Raw audio (integer format)
    MIME type: audio/x-raw-int
    properties: width = 8/16/24/32 (INT, bits used to store each sample)
                depth = 8 - 32 (INT, bits actually used per sample)
                signed = TRUE/FALSE (BOOLEAN)

4 - Raw audio (floating point format)
    MIME type: audio/x-raw-float
    Properties: width = 32/64 (INT)
                buffer-frames: number of audio frames per buffer, 0=undefined

Plugin Guidelines
=================

So, a short bit on what plugins should do. Above, I've stated that audio
properties like 'channels' and 'rate' or video properties like 'width' and
'height' are all optional. This doesn't mean you can just simply omit them and
everything will still work!

An example is the best way to explain all this. AVI needs the width, height,
rate and channels for the AVI header. So if these properties are missing, the
avimux element cannot properly create the AVI header. On the other hand, MPEG
doesn't have such properties in its header, so the mpegdemux element would need
to parse the separate streams in order to find them out. We don't want that
either, because a plugin only does one job. So normally, mpegdemux and avimux
wouldn't allow transcoding. To solve this problem, there are stream parser
elements (such as mpegaudioparse, ac3parse and mpeg1videoparse).

Conclusions to draw from here: a plugin gives info it can provide as seen from
its own task/job. If it can't, other elements might still need it and a stream
parser needs to be written if it doesn't already exist.

On properties that can be described by one of these (properties such as 'width',
'height', 'fps', etc.): they're forbidden and should be handled using filtered
caps.

Status of this document
=======================

Not all plugins strictly follow these guidelines yet, but these are the official
types. Plugins not following these specs either use extensions that should be
documented, or are buggy (and should be fixed).

Blame Ronald Bultje <rbultje@ronald.bitfreak.net> aka BBB for any mistakes in
this document.


={============================================================================
*kt_dev_bcast_320* gst: plugin: 02 Foundations
    
http://gstreamer.freedesktop.org/data/doc/gstreamer/head/pwg/html/index.html

GStreamer Plugin Writer's Guide (1.5.0.1)

Its main advantages are that the pluggable components can be mixed and matched
into arbitrary pipelines so that it's possible to write a full-fledged video or
audio editing application.

The framework is based on plugins that will provide the various codec and other
functionality. The plugins can be linked and arranged in a pipeline. This
pipeline defines the flow of the data.

The GStreamer core function is to provide a framework for plugins, data flow,
synchronization and media type handling/negotiation. It also provides an API to
    write applications using the various plugins. 


Preliminary Reading
-------------------

This guide assumes that you are somewhat familiar with the basic workings of
GStreamer. For a gentle introduction to programming concepts in GStreamer, you
may wish to read the GStreamer Application Development Manual first. Also check
out the other documentation available on the GStreamer web site.

In order to understand this manual, you will need to have a basic understanding
of the C language. Since GStreamer adheres to the GObject programming model,
this guide also assumes that you understand the basics of GObject programming.

You may also want to have a look at Eric Harlow's book Developing Linux
Applications with GTK+ and GDK. 


Elements and Plugins
--------------------
Just writing a new element is not entirely enough, however: You will need to
encapsulate your element in a plugin to enable GStreamer to use it. 

A plugin is essentially a loadable block of 'code', usually called a shared
object file or a dynamically linked library. A single plugin may contain the
implementation of several elements, or just a single one. For simplicity, this
guide concentrates primarily on plugins containing one element. 

A filter is an important type of element that processes a stream of data.
Producers and consumers of data are called source and sink elements,
respectively. Bin elements contain other elements. One type of bin is
  responsible for synchronization of the elements that they contain so that data
  flows smoothly. 

Another type of bin, called 'autoplugger' elements, automatically add other
elements to the bin and links them together so that they act as a filter between
two arbitrary stream types.


GstMiniObject, Buffers and Events
---------------------------------
All streams of data in GStreamer are chopped up into 'chunks' that are passed
from a source pad on one element to a sink pad on another element.
GstMiniObject is the structure used to hold these chunks of data.

GstMiniObject contains the following important types:

   An exact type indicating what type of data (event, buffer, ...) this
   GstMiniObject is.

   A reference count indicating the number of elements currently holding a
   reference to the miniobject. When the reference count falls to zero, the
   miniobject will be disposed, and its memory will be freed in some sense
   (see below for more details). 

For data transport, there are two types of GstMiniObject defined: 'events'
(control) and 'buffers' (content).

Buffers may contain any sort of data that the two linked pads know how to
handle. Normally, a buffer contains a chunk of some sort of audio or video
data that flows from one element to another.

Buffers also contain metadata describing the buffer's contents. Some of the
important types of metadata are:

   Pointers to one or more GstMemory objects. GstMemory objects are refcounted
   objects that encapsulate a region of memory.

   A timestamp indicating the preferred display timestamp of the content in
   the buffer. 

Events contain information on the state of the stream flowing between the two
linked pads. Events will only be sent if the element explicitly supports them,
else the core will (try to) handle the events automatically. Events are used
    to indicate, for example, a media type, the end of a media stream or that
        the cache should be flushed.

Events may contain several of the following items:

    A subtype indicating the type of the contained event.

    The other contents of the event depend on the specific event type. 


Buffer Allocation
-----------------
Buffers are able to store chunks of memory of several different types. The
most generic type of buffer contains memory allocated by malloc(). Such
buffers, although convenient, are not always very fast, since data often needs
to be specifically copied into the buffer.

Many specialized elements create buffers that point to special memory. For
example, the filesrc element usually maps a file into the address space of the
application (using mmap()), and creates buffers that point into that address
range. These buffers created by filesrc act exactly like generic buffers,
    except that they are read-only. 
        
The buffer freeing code automatically determines the correct method of freeing
the underlying memory. Downstream elements that receive these kinds of buffers
do not need to do anything special to handle or unreference it.

Another way an element might get specialized buffers is to request them from a
    downstream peer through a GstBufferPool or GstAllocator. Elements can ask
    a GstBufferPool or GstAllocator from the downstream peer element. If
    downstream is able to provide these objects, upstream can use them to
    allocate buffers. See more in Memory allocation.

Many sink elements have accelerated methods for copying data to hardware, or
have direct access to hardware. It is common for these elements to be able to
create a GstBufferPool or GstAllocator for their upstream peers. One such
example is ximagesink. It creates buffers that contain XImages. Thus, when an
upstream peer copies data into the buffer, it is copying directly into the
XImage, enabling ximagesink to draw the image directly to the screen instead
of having to copy data into an XImage first.

Filter elements often have the opportunity to either work on a buffer
in-place, or work while copying from a source buffer to a destination buffer.
It is optimal to implement both algorithms, since the GStreamer framework can
choose the fastest algorithm as appropriate. Naturally, this only makes sense
for strict filters -- elements that have exactly the same format on source and
    sink pads. 


={============================================================================
*kt_dev_bcast_321* gst-plug: 03 Building a Plugin

Chapter 3. Constructing the Boilerplate

The first step is to check out a copy of the gst-template git module to get an
important tool and the source code template for a basic GStreamer plugin.

$ git clone git://anongit.freedesktop.org/gstreamer/gst-template.git

The following commands create the MyFilter plugin based on the plugin template
and put the output files in the gst-template/gst-plugin/src directory: 

$ cd gst-template/gst-plugin/src
$ ../tools/make_element myfilter


Examining the Basic Code
------------------------

First we will examine the code you would be likely to place in a header file
(although since the interface to the code is entirely defined by the plugin
 system, and doesn't depend on reading a header file, this is not crucial.)

The code here can be found in
examples/pwg/examplefilter/boiler/gstexamplefilter.h.


Example 3.1. Example Plugin Header File

#include <gst/gst.h>

/* Definition of structure storing data for this element. */
typedef struct _GstMyFilter {

  GstElement element;

  GstPad *sinkpad, *srcpad;

  gboolean silent;

} GstMyFilter;

/* Standard definition defining a class for this element. */
typedef struct _GstMyFilterClass {
  GstElementClass parent_class;
} GstMyFilterClass;

/* Standard macros for defining types for this element.  */
#define GST_TYPE_MY_FILTER (gst_my_filter_get_type())
#define GST_MY_FILTER(obj) \
  (G_TYPE_CHECK_INSTANCE_CAST((obj),GST_TYPE_MY_FILTER,GstMyFilter))
#define GST_MY_FILTER_CLASS(klass) \
  (G_TYPE_CHECK_CLASS_CAST((klass),GST_TYPE_MY_FILTER,GstMyFilterClass))
#define GST_IS_MY_FILTER(obj) \
  (G_TYPE_CHECK_INSTANCE_TYPE((obj),GST_TYPE_MY_FILTER))
#define GST_IS_MY_FILTER_CLASS(klass) \
  (G_TYPE_CHECK_CLASS_TYPE((klass),GST_TYPE_MY_FILTER))

/* Standard function returning type information. */
GType gst_my_filter_get_type (void);
      

Using this header file, you can use the following macro to setup the GObject
basics in your source file so that all functions will be 'called'
appropriately:

#include "filter.h"

G_DEFINE_TYPE (GstMyFilter, gst_my_filter, GST_TYPE_ELEMENT);
    

Element metadata
----------------
The Element metadata provides extra element information. It is configured with
gst_element_class_set_metadata or gst_element_class_set_static_metadata which
takes the following parameters:

A long, English, name for the element.

The type of the element, see the docs/design/draft-klass.txt document in the
GStreamer core source tree for details and examples.

A brief description of the purpose of the element.

The name of the author of the element, optionally followed by a contact email
address in angle brackets. 

For example:

gst_element_class_set_static_metadata (klass,
  "An example plugin",
  "Example/FirstExample",
  "Shows the basic structure of a plugin",
  "your name <your.name@your.isp>");
    

The element details are registered with the plugin during the _class_init ()
function, which is part of the GObject system. The _class_init () function
should be set for this GObject in the function where you register the type with
GLib.

static void gst_my_filter_class_init (GstMyFilterClass * klass)
{
    GstElementClass *element_class = GST_ELEMENT_CLASS (klass);

    [..]
        gst_element_class_set_static_metadata (element_klass,
                "An example plugin",
                "Example/FirstExample",
                "Shows the basic structure of a plugin",
                "your name <your.name@your.isp>");

}


GstStaticPadTemplate
--------------------
A GstStaticPadTemplate is a description of a pad that the element will (or
        might) create and use. 

It contains:

  A short name for the pad.

  Pad direction.

  Existence property. This indicates whether the pad exists always (an "always"
      pad), only in some cases (a "sometimes" pad) or only if the application
  requested such a pad (a "request" pad).

  Supported types by this element (capabilities).

For example:

static GstStaticPadTemplate sink_factory =
GST_STATIC_PAD_TEMPLATE (
  "sink",
  GST_PAD_SINK,
  GST_PAD_ALWAYS,
  GST_STATIC_CAPS ("ANY")
);


Those pad templates are 'registered' during the _class_init () function with the
gst_element_class_add_pad_template (). For this function you need a handle the
GstPadTemplate which you can 'create' from the static pad template with
gst_static_pad_template_get (). See below for more details on this.

note: gst_pad_new_from_static_template()

Pads are created from these static templates in the element's _init ()
function using gst_pad_new_from_static_template(). In order to create a new
pad from this template using gst_pad_new_from_static_template (), you will
need to declare the pad template as a global variable. 

static GstStaticPadTemplate sink_factory = [..],
    src_factory = [..];

static void
gst_my_filter_class_init (GstMyFilterClass * klass)
{
    GstElementClass *element_class = GST_ELEMENT_CLASS (klass);
    [..]

    gst_element_class_add_pad_template (element_class,
      gst_static_pad_template_get (&src_factory));

    gst_element_class_add_pad_template (element_class,
      gst_static_pad_template_get (&sink_factory));
}

<pad-cap>
The last argument in a template is its type or list of supported 'types'. In
this example, we use 'ANY', which means that this element will accept all
input. In real-life situations, you would set a media type and optionally a
set of properties to make sure that only supported input will come in. This
representation should be a string that starts with a media type, then a set of
comma-separates properties with their supported values. In case of an audio
filter that supports raw integer 16-bit audio, mono or stereo at any
samplerate, the correct template would look like this:


static GstStaticPadTemplate sink_factory =
GST_STATIC_PAD_TEMPLATE (
  "sink",
  GST_PAD_SINK,
  GST_PAD_ALWAYS,
  GST_STATIC_CAPS (
    "audio/x-raw, "
      "format = (string) " GST_AUDIO_NE (S16) ", "
      "channels = (int) { 1, 2 }, "
      "rate = (int) [ 8000, 96000 ]"
  )
);

Values surrounded by curly brackets (“{” and “}”) are 'lists', values
surrounded by square brackets (“[” and “]”) are 'ranges'. Multiple sets of
types are supported too, and should be separated by a semicolon (“;”). 


Constructor Functions
---------------------
Each element has two functions which are used for construction of an element.

1. The _class_init() function, which is used to initialise the class only once
(specifying what signals, arguments and virtual functions the class has and
 setting up global state); 

2. The _init() function, which is used to initialise a specific 'instance' of
this type. 


The plugin_init function
------------------------
This is a special function, which is called as soon as the plugin is 'loaded',
     and should return TRUE or FALSE depending on whether it loaded
     initialized any dependencies correctly. Also, in this function, any
     supported element type in the plugin should be registered.


static gboolean
plugin_init (GstPlugin *plugin)
{
    return gst_element_register (plugin, "my_filter",
            GST_RANK_NONE,
            GST_TYPE_MY_FILTER);
}

GST_PLUGIN_DEFINE (
  GST_VERSION_MAJOR,
  GST_VERSION_MINOR,
  my_filter,
  "My filter plugin",
  plugin_init,
  VERSION,
  "LGPL",
  "GStreamer",
  "http://gstreamer.net/"
)

/**
 * GST_PLUGIN_DEFINE:
 * @major: major version number of the gstreamer-core that plugin was compiled for
 * @minor: minor version number of the gstreamer-core that plugin was compiled for
 * @name: short, but unique name of the plugin
 * @description: information about the purpose of the plugin
 * @init: function pointer to the plugin_init method with the signature of
 * <code>static gboolean plugin_init (GstPlugin * plugin)</code>.
 * @version: full version string (e.g. VERSION from config.h)
 * @license: under which licence the package has been released, e.g. GPL, LGPL.
 * @package: the package-name (e.g. PACKAGE_NAME from config.h)
 * @origin: a description from where the package comes from (e.g. the homepage URL)
 *
 * This macro needs to be used to define the entry point and meta data of a
 * plugin. One would use this macro to export a plugin, so that it can be used
 * by other applications.
 *
 * The macro uses a define named PACKAGE for the #GstPluginDesc,source field.
 * When using autoconf, this is usually set automatically via the AC_INIT
 * macro, and set in config.h. If you are not using autoconf, you will need to
 * define PACKAGE yourself and set it to a short mnemonic string identifying
 * your application/package, e.g. 'someapp' or 'my-plugins-foo.
 *
 * If defined, the GST_PACKAGE_RELEASE_DATETIME will also be used for the
 * #GstPluginDesc,release_datetime field.
 */

Note that the information returned by the plugin_init() function will be
cached in a central 'registry'. For this reason, it is important that the same
information is always returned by the function: for example, it must not make
element factories available based on runtime conditions. 

If an element can only work in certain conditions (for example, if the soundcard
        is not being used by some other process) this must be reflected by the
element being unable to enter the READY state if unavailable, rather than the
plugin attempting to deny existence of the plugin. 


/* entry point to initialize the plug-in
 * initialize the plug-in itself
 * register the element factories and other features
 */
static gboolean
myfilter_init (GstPlugin * myfilter)
{
  /* debug category for fltering log messages
   *
   * exchange the string 'Template myfilter' with your description
   */
  GST_DEBUG_CATEGORY_INIT (gst_myfilter_debug, "myfilter",
      0, "Template myfilter");

  return gst_element_register (myfilter, "myfilter", GST_RANK_NONE,
      GST_TYPE_MYFILTER);
}

/* PACKAGE: this is usually set by autotools depending on some _INIT macro
 * in configure.ac and then written into and defined in config.h, but we can
 * just set it ourselves here in case someone doesn't use autotools to
 * compile this code. GST_PLUGIN_DEFINE needs PACKAGE to be defined.
 */
#ifndef PACKAGE
#define PACKAGE "myfirstmyfilter"
#endif

/* gstreamer looks for this structure to register myfilters
 *
 * exchange the string 'Template myfilter' with your myfilter description
 */
GST_PLUGIN_DEFINE (
    GST_VERSION_MAJOR,
    GST_VERSION_MINOR,
    myfilter,
    "Template myfilter",
    myfilter_init,
    VERSION,
    "LGPL",
    "GStreamer",
    "http://gstreamer.net/"
)


={============================================================================
*kt_dev_bcast_321* gst-plug: 04 Chain up

Chapter 4. Specifying the pads

note: set functions >

In the element _init () function, you create the pad from the pad template
that has been registered with the element class in the _class_init ()
function. 

<chain-up>
After creating the pad, you have to set a _chain () function pointer that will
'receive' and process the input data on the sinkpad. You can optionally also set
an _event () function pointer and a _query () function pointer.

After that, you have to 'register' the pad with the element. This happens like
this:

static void
gst_myfilter_init (Gstmyfilter * filter)
{
  filter->sinkpad = gst_pad_new_from_static_template (&sink_factory, "sink");

  // note: event
  gst_pad_set_event_function (filter->sinkpad,
                              GST_DEBUG_FUNCPTR(gst_myfilter_sink_event));
  // note: chain
  gst_pad_set_chain_function (filter->sinkpad,
                              GST_DEBUG_FUNCPTR(gst_myfilter_chain));
  GST_PAD_SET_PROXY_CAPS (filter->sinkpad);

  gst_element_add_pad (GST_ELEMENT (filter), filter->sinkpad);

  filter->srcpad = gst_pad_new_from_static_template (&src_factory, "src");
  GST_PAD_SET_PROXY_CAPS (filter->srcpad);

  gst_element_add_pad (GST_ELEMENT (filter), filter->srcpad);

  filter->silent = FALSE;
}


={============================================================================
*kt_dev_bcast_321* gst-plug: 05 The chain function

The chain function is the function in which all data 'processing' takes place.
In the case of a simple filter, _chain () functions are mostly linear
functions - so for each incoming buffer, one buffer will go out, too. Below is
a very simple implementation of a chain function: 

/* chain function
 * this function does the actual processing
 */
static GstFlowReturn
gst_myfilter_chain (GstPad * pad, GstObject * parent, GstBuffer * buf)
{
    Gstmyfilter *filter;

    // note: parent is a filter element which has pads in it.
    filter = GST_MYFILTER (parent);

    if (!filter->silent)
        g_print ("Have data of size %" G_GSIZE_FORMAT" bytes!\n",
                gst_buffer_get_size (buf));

    /* just push out the incoming buffer without touching it */
    return gst_pad_push (filter->srcpad, buf);
}

Instead of printing that the data is in, you would normally process the data
there. Remember, however, that buffers are 'not' always writeable.

In more advanced elements (the ones that do event processing), you may want to
additionally specify an event handling function, which will be called when
stream-events are sent (such as caps, end-of-stream, newsegment, tags, etc.). 


={============================================================================
*kt_dev_bcast_321* gst-plug: 06 The event function

The event function notifies you of special events that happen in the
datastream (such as caps, end-of-stream, newsegment, tags, etc.). Events can
'travel' both upstream and downstream, so you can receive them on sink pads as
well as source pads.

Below follows a very simple event function that we install on the sink pad of
our element. 

static gboolean
gst_my_filter_sink_event (GstPad *pad, GstObject *parent, GstEvent  *event)
{
  gboolean ret;
  GstMyFilter *filter = GST_MY_FILTER (parent);

  switch (GST_EVENT_TYPE (event)) {
    case GST_EVENT_CAPS:
      /* we should handle the format here */

      /* push the event downstream */
      ret = gst_pad_push_event (filter->srcpad, event);
      break;

    case GST_EVENT_EOS:
      /* end-of-stream, we should close down all stream leftovers here */
      gst_my_filter_stop_processing (filter);

      ret = gst_pad_event_default (pad, parent, event);
      break;

    default:
      /* just call the default handler */
      ret = gst_pad_event_default (pad, parent, event);
      break;
  }
  return ret;
}

It is a good idea to call the 'default' event handler gst_pad_event_default ()
for unknown events.

'depending' on the event type, the default handler will forward the event or
simply unref it. The CAPS event is by default not forwarded so we need to do
this in the event handler ourselves. 


={============================================================================
*kt_dev_bcast_321* gst-plug: 07 The query function

Through the query function, your element will receive queries that it has to
reply to. These are queries like position, duration but also about the
supported formats and scheduling modes your element supports. Queries can
'travel' both upstream and downstream, so you can receive them on sink pads as
well as source pads. 

static gboolean
gst_my_filter_src_query (GstPad    *pad,
               GstObject *parent,
               GstQuery  *query)
{
    gboolean ret;
    GstMyFilter *filter = GST_MY_FILTER (parent);

    switch (GST_QUERY_TYPE (query)) {
        case GST_QUERY_POSITION:
            /* we should report the current position */
            [...]
                break;
        case GST_QUERY_DURATION:
            /* we should report the duration here */
            [...]
                break;
        case GST_QUERY_CAPS:
            /* we should report the supported caps here */
            [...]
                break;
        default:
            /* just call the default handler */
            ret = gst_pad_query_default (pad, parent, query);
            break;
    }
    return ret;
}

As with event, it is a good idea to call the default query handler
gst_pad_query_default () for unknown queries. Depending on the query type, the
default handler will forward the query or simply unref it. 


={============================================================================
*kt_dev_bcast_321* gst-plug: 08 What are states?

Table of Contents

A state describes whether the element instance is initialized, whether it is
ready to transfer data and whether it is currently handling data. There are four
states defined in GStreamer:

note: state is per element

which will from now on be referred to simply as "NULL", "READY", "PAUSED" and
"PLAYING".


GST_STATE_NULL 
is the default state of an element. In this state, it has not allocated any
runtime resources, it has not loaded any runtime libraries and it can obviously
not handle data.


GST_STATE_READY 
is the next state that an element can be in. In the READY state, an element has
all 'default' resources (runtime-libraries, runtime-memory) allocated. However,
it has not yet allocated or defined anything that is stream-specific. 

When going from NULL to READY state (GST_STATE_CHANGE_NULL_TO_READY), an element
should allocate any 'non'-stream-specific resources and should load
runtime-loadable libraries (if any). When going the other way around (from READY
        to NULL, GST_STATE_CHANGE_READY_TO_NULL), an element should unload these
libraries and free all allocated resources. Examples of such resources are
hardware devices. 

Note that files are generally streams, and these should thus be considered as
stream-specific resources; therefore, they should not be allocated in this
state.


GST_STATE_PAUSED 
is the state in which an element is ready to accept and handle data. For most
elements this state is the same as PLAYING. The only exception to this rule are
sink elements. Sink elements only accept one single buffer of data and then
block. At this point the pipeline is 'prerolled' and ready to render data
immediately.


GST_STATE_PLAYING 
is the highest state that an element can be in. For most elements this state is
exactly the same as PAUSED, they accept and process events and buffers with
data. Only 'sink' elements need to differentiate between PAUSED and PLAYING
state.  In PLAYING state, sink elements actually render incoming data, e.g.
output audio to a sound card or render video pictures to an image sink.


<managing-state>
If at all possible, your element should derive from one of the new base classes
(Pre-made base classes). There are ready-made general purpose base classes for
different types of sources, sinks and filter/transformation elements. In
addition to those, specialised base classes exist for audio and video elements
and others.

If you use a base class, you will rarely have to handle state changes yourself.
All you have to do is 'override' the base class's start() and stop() virtual
functions (might be called differently depending on the base class) and the base
class will take care of everything for you.

If, however, you do not derive from a ready-made base class, but from GstElement
or some other class not built on top of a base class, you will most likely have
to implement your own state change function to be notified of state changes. 

note: This is definitively necessary if your plugin is a demuxer or a muxer, as
there are no base classes for muxers or demuxers yet. 

An element can be notified of state changes through a virtual function pointer.
Inside this function, the element can initialize any sort of specific data
needed by the element, and it can optionally fail to go from one state to
another.

Do not g_assert for unhandled state changes; this is taken care of by the
GstElement base class.

static GstStateChangeReturn
gst_my_filter_change_state (GstElement *element, GstStateChange transition);

static void
gst_my_filter_class_init (GstMyFilterClass *klass)
{
  GstElementClass *element_class = GST_ELEMENT_CLASS (klass);

  element_class->change_state = gst_my_filter_change_state;
}

static GstStateChangeReturn
gst_my_filter_change_state (GstElement *element, GstStateChange transition)
{
  GstStateChangeReturn ret = GST_STATE_CHANGE_SUCCESS;
  GstMyFilter *filter = GST_MY_FILTER (element);

  switch (transition) {
    case GST_STATE_CHANGE_NULL_TO_READY:
      if (!gst_my_filter_allocate_memory (filter))
        return GST_STATE_CHANGE_FAILURE;
      break;
    default:
      break;
  }

  ret = GST_ELEMENT_CLASS (parent_class)->change_state (element, transition);
  if (ret == GST_STATE_CHANGE_FAILURE)
    return ret;

  switch (transition) {
    case GST_STATE_CHANGE_READY_TO_NULL:
      gst_my_filter_free_memory (filter);
      break;
    default:
      break;
  }

  return ret;
}

Note that upwards (NULL=>READY, READY=>PAUSED, PAUSED=>PLAYING) and downwards
(PLAYING=>PAUSED, PAUSED=>READY, READY=>NULL) state changes are handled in 'two'
separate blocks with the downwards state change handled only after we have
'chained' up to the parent class's state change function. This is necessary in
order to safely handle concurrent access by multiple threads.

The reason for this is that in the case of downwards state changes you don't
want to destroy allocated resources 'while' your plugin's chain function (for
        example) is still accessing those resources in another thread. Whether
your chain function might be running or not depends on the state of your
plugin's pads, and the state of those pads is closely linked to the state of the
element. Pad states are handled in the GstElement class's state change function,
    including proper locking, that's why it is essential to chain up before
    destroying allocated resources. 


={============================================================================
*kt_dev_bcast_321* gst-plug: 09 Adding Properties

The primary and most important way of controlling how an element behaves, is
through GObject properties. GObject properties are defined in the _class_init ()
function. The element optionally implements a _get_property () and a
_set_property () function. These functions will be notified if an application
changes or requests the value of a property, and can then fill in the value or
take action required for that property to change value internally.

You probably also want to keep an instance variable around with the currently
configured value of the property that you use in the get and set functions. Note
that GObject will not automatically set your instance variable to the default
value, you will have to do that in the _init () function of your element.


/* properties */
enum {
  PROP_0,
  PROP_SILENT
  /* FILL ME */
};

static void gst_my_filter_set_property (GObject      *object,
                   guint         prop_id,
                   const GValue *value,
                   GParamSpec   *pspec);

static void gst_my_filter_get_property (GObject      *object,
                   guint         prop_id,
                   GValue       *value,
                   GParamSpec   *pspec);

static void
gst_my_filter_class_init (GstMyFilterClass *klass)
{
  GObjectClass *object_class = G_OBJECT_CLASS (klass);

  /* define virtual function pointers */
  object_class->set_property = gst_my_filter_set_property;
  object_class->get_property = gst_my_filter_get_property;

  /* define properties */
  g_object_class_install_property (object_class, PROP_SILENT,

      // note: g_param_spec_boolean

      g_param_spec_boolean ("silent", "Silent",
        "Whether to be very verbose or not",
        FALSE, G_PARAM_READWRITE | G_PARAM_STATIC_STRINGS));
}

static void
gst_my_filter_set_property (GObject      *object,
             guint         prop_id,
             const GValue *value,
             GParamSpec   *pspec)
{
  GstMyFilter *filter = GST_MY_FILTER (object);

  switch (prop_id) {
    case PROP_SILENT:
      filter->silent = g_value_get_boolean (value);
      g_print ("Silent argument was changed to %s\n",
          filter->silent ? "true" : "false");
      break;
    default:
      G_OBJECT_WARN_INVALID_PROPERTY_ID (object, prop_id, pspec);
      break;
  }
}

static void
gst_my_filter_get_property (GObject    *object,
             guint       prop_id,
             GValue     *value,
             GParamSpec *pspec)
{
  GstMyFilter *filter = GST_MY_FILTER (object);
                                                                                
  switch (prop_id) {
    case PROP_SILENT:
      g_value_set_boolean (value, filter->silent);
      break;
    default:
      G_OBJECT_WARN_INVALID_PROPERTY_ID (object, prop_id, pspec);
      break;
  }
}


The above is a very simple example of how properties are used. Graphical
applications will use these properties and will display a user-controllable
widget with which these properties can be changed. This means that - for the
property to be as user-friendly as possible - you should be as exact as possible
in the definition of the property. Not only in defining ranges in between which
valid properties can be located (for integers, floats, etc.), but also in using
very descriptive (better yet: internationalized) strings in the definition of
the property, and if possible using enums and flags instead of integers. The
GObject documentation describes these in a very complete way, but below, we'll
give a short example of where this is useful. Note that using integers here
would probably completely confuse the user, because they make no sense in this
context. The example is stolen from videotestsrc.

typedef enum {
  GST_VIDEOTESTSRC_SMPTE,
  GST_VIDEOTESTSRC_SNOW,
  GST_VIDEOTESTSRC_BLACK
} GstVideotestsrcPattern;

[..]

#define GST_TYPE_VIDEOTESTSRC_PATTERN (gst_videotestsrc_pattern_get_type ())

static GType
gst_videotestsrc_pattern_get_type (void)
{
  static GType videotestsrc_pattern_type = 0;

  if (!videotestsrc_pattern_type) {
    static GEnumValue pattern_types[] = {
      { GST_VIDEOTESTSRC_SMPTE, "SMPTE 100% color bars",    "smpte" },
      { GST_VIDEOTESTSRC_SNOW,  "Random (television snow)", "snow"  },
      { GST_VIDEOTESTSRC_BLACK, "0% Black",                 "black" },
      { 0, NULL, NULL },
    };

    videotestsrc_pattern_type =
      g_enum_register_static ("GstVideotestsrcPattern",
          pattern_types);
  }

  return videotestsrc_pattern_type;
}

[..]

static void
gst_videotestsrc_class_init (GstvideotestsrcClass *klass)
{
  [..]
    g_object_class_install_property (G_OBJECT_CLASS (klass), PROP_PATTERN,

        // note: g_param_spec_enum

        g_param_spec_enum ("pattern", "Pattern",
          "Type of test pattern to generate",
          GST_TYPE_VIDEOTESTSRC_PATTERN, GST_VIDEOTESTSRC_SMPTE,
          G_PARAM_READWRITE | G_PARAM_STATIC_STRINGS));
  [..]
}
  

<property-proxying>
This is case when parent element forwards all property set request to child and
which is called property proxing. 

static void
gst_ts_nexus_bin_set_property (GObject * object,
  guint prop_id, const GValue * value, GParamSpec * pspec)
{
  GstTSNexusBin *ts_bin = GST_TS_NEXUSBIN_BIN (object);

  if (prop_id > PROP_LAST) {
    G_OBJECT_WARN_INVALID_PROPERTY_ID (object, prop_id, pspec);
  } else {
    if(ts_bin->sink &&
        g_object_class_find_property (G_OBJECT_GET_CLASS (ts_bin->sink), pspec->name)){
      g_object_set_property (ts_bin->sink, pspec->name, value);
    }
  }
}


={============================================================================
*kt_dev_bcast_321* gst-plug: 10 Signals

GObject signals can be used to notify applications of events specific to this
object. Note, however, that the application needs to be aware of signals and
their meaning, so if you're looking for a generic way for application-element
interaction, signals are probably not what you're looking for. In many cases,
however, signals can be very useful. See the GObject documentation for all
    internals about signals. 


={============================================================================
*kt_dev_bcast_321* gst-plug: 11 Building a Test Application

Often, you will want to test your newly written plugin in an as small setting as
possible. Usually, gst-launch-1.0 is a good first step at testing a plugin. If
you have not installed your plugin in a directory that GStreamer searches, then
you will need to set the plugin path. 

<gst-plugin-path>
Either set GST_PLUGIN_PATH to the directory containing your plugin, or use the
command-line option --gst-plugin-path. If you based your plugin off of the
gst-plugin template, then this will look something like gst-launch-1.0
--gst-plugin-path=$HOME/gst-template/gst-plugin/src/.libs TESTPIPELINE 

However, you will often need more testing features than gst-launch-1.0 can
provide, such as seeking, events, interactivity and more. Writing your own small
testing program is the easiest way to accomplish this. This section explains -
in a few words - how to do that. For a complete application development guide,
see the Application Development Manual.

At the start, you need to initialize the GStreamer core library by calling
gst_init (). You can alternatively call gst_init_get_option_group (), which will
return a pointer to GOptionGroup. You can then use GOption to handle the
initialization, and this will finish the GStreamer initialization.

You can create elements using gst_element_factory_make (), where the first
argument is the element type that you want to create, and the second argument is
a free-form name. The example at the end uses a simple filesource - decoder -
soundcard output pipeline, but you can use specific debugging elements if that's
necessary. For example, an 'identity' element can be used in the middle of the
pipeline to act as a data-to-application transmitter. This can be used to check
the data for misbehaviours or correctness in your test application. Also, you
can use a 'fakesink' element at the end of the pipeline to dump your data to the
stdout (in order to do this, set the dump property to TRUE). Lastly, you can use
valgrind to check for memory errors.

During linking, your test application can use filtered caps as a way to drive a
specific type of data to or from your element. This is a very simple and
effective way of checking multiple types of input and output in your element.

Note that during running, you should listen for at least the "error" and "eos"
messages on the bus and/or your plugin/element to check for correct handling of
this. Also, you should add events into the pipeline and make sure your plugin
handles these correctly (with respect to clocking, internal caching, etc.).

Never forget to clean up memory in your plugin or your test application. When
going to the NULL state, your element should clean up allocated memory and
caches. Also, it should close down any references held to possible support
libraries. Your application should unref () the pipeline and make sure it
doesn't crash. 


={============================================================================
*kt_dev_bcast_321* gst-plug: 23 Pre-made base classes

So far, we've been looking at low-level concepts of creating any type of
GStreamer element. Now, let's assume that all you want is to create an simple
audiosink that works exactly the same as, say, "esdsink", or a filter that
simply normalizes audio volume. Such elements are very general in concept and
since they do nothing special, they should be easier to code than to provide
your own scheduler activation functions and doing complex caps negotiation. For
this purpose, GStreamer provides base classes that simplify some types of
elements. Those base classes will be discussed in this chapter.


Writing a sink

Sinks are special elements in GStreamer. This is because sink elements have to
take care of preroll, which is the process that takes care that elements going
into the GST_STATE_PAUSED state will have buffers ready after the state change.

The result of this is that such elements can start processing data immediately
after going into the GST_STATE_PLAYING state, without requiring to take some
time to initialize outputs or set up decoders; all that is done already before
the state-change to GST_STATE_PAUSED successfully completes.

Preroll, however, is a complex process that would require the same code in many
elements. Therefore, sink elements can derive from the GstBaseSink base-class,
which does preroll and a few other utility functions automatically. The derived
  class only needs to implement a bunch of virtual functions and will work
  automatically.

The base class implement much of the synchronization logic that a sink has to
perform.

The GstBaseSink base-class specifies some limitations on elements, though:

  It requires that the sink only has one sinkpad. Sink elements that need more
  than one sinkpad, must make a manager element with multiple GstBaseSink
  elements inside. 

Sink elements can derive from GstBaseSink using the usual GObject convenience
macro G_DEFINE_TYPE ():

G_DEFINE_TYPE (GstMySink, gst_my_sink, GST_TYPE_BASE_SINK);

[..]

static void
gst_my_sink_class_init (GstMySinkClass * klass)
{
  klass->set_caps = [..];
  klass->render = [..];
[..]
}
    

The advantages of deriving from GstBaseSink are numerous:

  Derived implementations barely need to be aware of preroll, and do not need to
  know anything about the technical implementation requirements of preroll. The
  base-class does all the hard work.

  Less code to write in the derived class, shared code (and thus shared
      bugfixes). 

There are also specialized base classes for audio and video, let's look at those
a bit.


Writing an audio sink

Essentially, audio sink implementations are just a special case of a general
sink. An audio sink has the added complexity that it needs to schedule playback
of samples. It must match the clock selected in the pipeline against the clock
of the audio device and calculate and compensate for drift and jitter.

There are two audio base classes that you can choose to derive from, depending
on your needs: GstAudioBasesink and GstAudioSink. The audiobasesink provides
full control over how synchronization and scheduling is handled, by using a
ringbuffer that the derived class controls and provides. The audiosink
base-class is a derived class of the audiobasesink, implementing a standard
ringbuffer implementing default synchronization and providing a standard
audio-sample clock. Derived classes of this base class merely need to provide a
_open (), _close () and a _write () function implementation, and some optional
functions. This should suffice for many sound-server output elements and even
most interfaces. More demanding audio systems, such as Jack, would want to
implement the GstAudioBaseSink base-class.

The GstAudioBaseSink has little to no limitations and should fit virtually every
implementation, but is hard to implement. 

The GstAudioSink, on the other hand, only fits those systems with a simple open
() / close () / write () API (which practically means pretty much all of them),
    but has the advantage that it is a lot easier to implement. 
    
The benefits of this second base class are large:

  Automatic synchronization, without any code in the derived class.

  Also automatically provides a clock, so that other sinks (e.g. in case of
      audio/video playback) are synchronized.

  Features can be added to all audiosinks by making a change in the base class,
  which makes maintenance easy.

  Derived classes require only three small functions, plus some GObject
  boilerplate code. 

In addition to implementing the audio base-class virtual functions, derived
classes can (should) also implement the GstBaseSink set_caps () and get_caps ()
  virtual functions for negotiation.  
    

Writing a video sink

Writing a videosink can be done using the GstVideoSink base-class, which derives
from GstBaseSink internally. Currently, it does nothing yet but add another
compile dependency, so derived classes will need to implement all base-sink
virtual functions. When they do this correctly, this will have some positive
effects on the end user experience with the videosink:

  Because of preroll (and the preroll () virtual function), it is possible to
  display a video frame already when going into the GST_STATE_PAUSED state.

  By adding new features to GstVideoSink, it will be possible to add extensions
  to videosinks that affect all of them, but only need to be coded once, which
  is a huge maintenance benefit. 


={============================================================================
*kt_dev_bcast_330* gst: man: 01: introduction

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/manual/html/index.html

GStreamer is an extremely powerful and versatile framework for creating
streaming media applications.  Many of the virtues of the GStreamer framework
come from its modularity: GStreamer can seamlessly incorporate new plugin
modules. But because modularity and power often come at a cost of greater
complexity, writing new applications is not always easy.


2. Preliminary reading

In order to understand this manual, you need to have a basic understanding of
the C language. Since GStreamer adheres to the GObject programming model, this
guide also assumes that you understand the basics of GObject
(http://library.gnome.org/devel/gobject/stable/) and glib
(http://library.gnome.org/devel/glib/stable/) programming. Especially, 


* GObject instantiation 
* GObject properties (set/get) 
* GObject casting 
* GObject referecing/dereferencing 
* glib memory management 
* glib signals and callbacks 
* glib main loop


1. What is GStreamer?

Specifically, GStreamer core provides

* an API for multimedia applications
* a plugin architecture
* a pipeline architecture
* a mechanism for media type handling/negotiation
* a mechanism for synchronization
* over 250 plug-ins providing more than 1000 elements
* a set of tools

GStreamer plug-ins could be classified into

* protocols handling
* sources: for audio and video (involves protocol plugins)

GStreamer is packaged into

* gstreamer: the core package
* gst-plugins-base: an essential exemplary set of elements
* gst-plugins-good: a set of good-quality plug-ins under LGPL
* gst-plugins-ugly: a set of good-quality plug-ins that might pose
distribution problems
* gst-plugins-bad: a set of plug-ins that need more quality
* gst-libav: a set of plug-ins that wrap libav for decoding and encoding


See Figure 1-1. Gstreamer overview


2. Design principles

2.1. Clean and powerful

GStreamer provides a clean interface to:

* The application programmer who wants to build a media pipeline. The
programmer can use an extensive set of powerful tools to create media
pipelines without writing a single line of code.  Performing complex media
manipulations becomes very easy.

* The plugin programmer. Plugin programmers are provided a clean and simple
API to create self-contained plugins. 

2.2. Object oriented

GStreamer uses the mechanism of signals and object properties.

All objects can be queried at runtime for their various properties and
capabilities.

2.5. High performance

High performance is obtained by:

* extremely light-weight links between plugins. Data can travel the pipeline
with minimal overhead. Data passing between plugins only involves a pointer
dereference in a typical pipeline.

* providing a mechanism to directly work on the target memory. A plugin can
for example directly write to the X server's shared memory space. Buffers can
    also point to arbitrary memory, such as a sound card's internal hardware
        buffer.

* using a plugin registry with the specifications of the plugins so that the
plugin loading can be delayed until the plugin is actually used.


={============================================================================
*kt_dev_bcast_331* gst: man: 03: foundations

3.2. Pads

<caps-negotiation>

Pads are element's input and output, where you can connect other elements.
They are used to negotiate links and data flow between elements in GStreamer.
A pad can be viewed as a "plug" or "port" on an element where links may be
made with other elements, and through which data can flow to or from those
elements. Pads have specific data handling capabilities: a pad can restrict
the type of data that flows through it. Links are only allowed between two
pads when the allowed data types of the two pads are compatible. Data types
are negotiated between pads using a process called 'caps negotiation' Data
types are described as a GstCaps.

For the most part, all data in GStreamer flows one way through a link between
elements. Data flows out of one element through one or more source pads, and
elements accept incoming data through one or more sink pads. Source and sink
elements have only source and sink pads, respectively. 

Data usually means buffers (described by the GstBuffer object) and events
(described by the GstEvent)


3.3. Bins and pipelines

A pipeline is a top-level 'bin'. It provides a bus for the application and
manages the synchronization for its children. As you set it to PAUSED or
PLAYING state, data flow will start and media processing will take place. Once
started, pipelines will run in a separate 'thread' until you stop them or the
end of the data stream is reached.


3.4. Communication

GStreamer provides several mechanisms for communication and data exchange
between the application and the pipeline.

* buffers are objects for passing streaming data "between elements" in the
pipeline. Buffers always travel from sources to sinks (downstream).

* events are objects sent between elements or "from the application to
elements". Events can travel upstream and downstream. Downstream events can be
synchronised to the data flow.

* messages are objects posted by elements "on the pipeline's message bus",
where they will be held for collection by the application. Messages can be
    intercepted synchronously from the streaming thread context of the element
    posting the message, but are usually handled asynchronously by the
    application from the application's main thread. 

Messages are used to 'transmit' information such as errors, tags, state
changes, buffering state, redirects etc. "from elements to the application" in
a thread-safe way.

* queries allow applications to request information such as duration or
current playback position from the pipeline. Queries are always answered
synchronously. Elements can also use queries to request information from their
peer elements (such as the file size or duration). They can be used both ways
within a pipeline, but upstream queries are more common.

Figure 3-2. GStreamer pipeline with different communication flows

note: events is from app to elements and messages is from elements to app.


={============================================================================
*kt_dev_bcast_332* gst: man: 04: initialize gst

4.1. Simple initialization

Before the GStreamer libraries can be used, gst_init has to be called from the
'main' application.  This call will perform the necessary initialization of
the library as well as parse the GStreamer-specific command line options.


={============================================================================
*kt_dev_bcast_333* gst-man: 05: elements, state

The most important object in GStreamer for the application programmer is the
GstElement object. An element is the basic building block for a media pipeline.
All the different high-level components you will use are 'derived' from
GstElement. Every decoder, encoder, demuxer, video or audio output is in fact a
GstElement


5.1. What are elements?

For the application programmer, elements are best visualized as black boxes.
On the one end, you might put something in, the element does something with it
and something else comes out at the other side. For a decoder element, for
example, you'd put in encoded data, and the element would output decoded data.


5.1.1. Source elements

Source elements generate data for use by a pipeline, for example reading from
disk or from a sound card. We always draw a source pad to the 'right' of the
element. 

Source elements do not accept data, they only 'generate' data. A source pad can
only generate data.


5.1.2. Filters, convertors, demuxers, muxers and codecs

Filters and filter-like elements have both input and outputs pads. They operate
on data that they receive on their input (sink) pads, and will provide data on
their output (source) pads. Examples of such elements are a volume element
(filter), a video scaler (convertor), an Ogg demuxer or a Vorbis decoder.

Filter-like elements can have any number of source or sink pads. A video
demuxer, for example, would have one sink pad and several (1-N) source pads, one
for each elementary stream contained in the container format. Decoders, on the
  other hand, will only have one source and sink pads.

+====================+
|[sink]     [source] |
+====================+

+===================+
|[sink]     [audio] |
|           [video] |
+===================+


5.1.3. Sink elements

Sink elements are 'end' points in a media pipeline. They accept data but do not
produce anything. Disk writing, soundcard playback, and video output would all
be implemented by sink elements.

+===================+
|[sink]             |
+===================+


5.2. Creating a GstElement

The simplest way to create an element is to use gst_element_factory_make().
This function takes a factory name and an element name for the newly created
element. The name of the element is something you can use later on to look up
the element in a bin, for example. The name will also be used in debug output.
You can pass NULL as the name argument to get a unique, default name.

note: reference count

When you don't need the element anymore, you need to unref it using
gst_object_unref(). This decreases the reference count for the element by 1.
An element has a refcount of 1 when it gets created. An element gets destroyed
completely when the refcount is decreased to 0.  

The following example shows how to create an element named source from the
element factory named fakesrc. It checks if the creation succeeded. After
checking, it unrefs the element.

/* create element */
element = gst_element_factory_make ("fakesrc", "source");
if (!element) {
  g_print ("Failed to create element of type 'fakesrc'\n");
  return -1;
}

gst_object_unref (GST_OBJECT (element));


gst_element_factory_make is actually a shorthand for a combination of two
functions.


5.3. Using an element as a GObject

A GstElement can have several properties which are implemented using standard
GObject properties. The usual GObject methods to query, set and get property
values and GParamSpecs are therefore supported.

Every GstElement 'inherits' at least one property from its parent GstObject:
the "name" property. This is the name you provide to the functions.

gst_element_factory_make ()

GstElement *
gst_element_factory_make (const gchar *factoryname,
                          const gchar *name);

Create a new element of the type defined by the given element factory. If name
is NULL, then the element will receive a guaranteed unique name, consisting of
the element factory name and a number. If name is given, it will be given the
name supplied.

Parameters

factoryname   a named factory to instantiate
name          name of new element, or NULL to automatically create a unique
              name. 

int main (int argc, char *argv[])
{
  GstElement *element;
  gchar *name;

  /* init GStreamer */
  gst_init (&argc, &argv);

  /* create element */
  element = gst_element_factory_make ("fakesrc", "source");

  /* get name */
  g_object_get (G_OBJECT (element), "name", &name, NULL);
  g_print ("The name of the element is '%s'.\n", name);
  g_free (name);

  gst_object_unref (GST_OBJECT (element));

  return 0;
}

note: signal and properties

A GstElement also provides various GObject signals that can be used as a
flexible callback mechanism. Here, too, you can use gst-inspect to see which
signals a specific element supports. Together, signals and properties are the
most basic way in which elements and applications interact.


5.4. More about element factories

In the previous section, we briefly introduced the GstElementFactory object
already as a way to create instances of an element. Element factories are the
basic types retrieved from the GStreamer 'registry', they describe all plugins
and elements that GStreamer can create. This means that element factories are
useful for automated element instancing, such as what autopluggers do, and for
creating lists of available elements.

Tools like gst-inspect will provide some generic information about an element,
      such as the person that wrote the plugin, a descriptive name (and a
          shortname), a rank and a category. 
      
The category can be used to get the type of the element that can be created
using this element factory. Examples of categories include Codec/Decoder/Video
(video decoder), Codec/Encoder/Video (video encoder), Source/Video (a video
    generator), Sink/Video (a video output), and all these exist for audio as
well, of course. 

gst-inspect will give a list of all 'factories', and gst-inspect <factory-name>
will list all of the above information, and a lot more.


5.4.2. Finding out what pads an element can contain

Perhaps the most powerful feature of element factories is that they contain a
full description of the pads that the element can generate, and the capabilities
of those pads such as what types of media can stream over those pads, 'without'
actually having to load those plugins into memory. This can be used to provide a
codec selection list for encoders, or it can be used for autoplugging purposes
for media players. All current GStreamer-based media players and autopluggers
work this way.


5.5. Linking elements

By linking a source element with zero or more filter-like elements and finally a
sink element, you set up a media pipeline. Data will flow through the elements.
This is the basic concept of media handling in GStreamer.

// source -> filter -> sink

main (int argc, char *argv[])
{
  GstElement *pipeline;
  GstElement *source, *filter, *sink;

  /* init */
  gst_init (&argc, &argv);

  /* create pipeline */
  pipeline = gst_pipeline_new ("my-pipeline");

  /* create elements */
  source = gst_element_factory_make ("fakesrc", "source");
  filter = gst_element_factory_make ("identity", "filter");
  sink = gst_element_factory_make ("fakesink", "sink");

  /* must add elements to pipeline before linking them */
  gst_bin_add_many (GST_BIN (pipeline), source, filter, sink, NULL);

  /* link */
  if (!gst_element_link_many (source, filter, sink, NULL)) {
    g_warning ("Failed to link elements!");
  }

  [..]
}

note: 
you cannot directly link elements that are not in the same bin or pipeline; if
you want to link elements or pads at different hierarchy levels, you will need
to use ghost pads. more about ghost pads later, see Section 8.4.


5.6. Element States

After being created, an element will not actually perform any actions yet. You
need to change elements state to make it do something. GStreamer knows four
element states, each with a very specific meaning. Those four states are:

* GST_STATE_NULL 
default state. No resources are allocated in this state, so, transitioning to
it will 'free' all resources. The element must be in this state when its
refcount reaches 0 and it is freed.

* GST_STATE_READY 
an element has allocated all of its global resources, that is, resources that
can be kept within streams. You can think about opening devices, allocating
buffers and so on. However, the stream is not opened in this state, so the
stream positions is automatically zero. If a stream was previously opened, it
should be closed in this state, and position, properties and such should be
reset.

* GST_STATE_PAUSED
an element has 'opened' the stream, but is 'not' actively 'processing' it. An
element is allowed to modify a stream's position, read and process data and such
to 'prepare' for playback as soon as state is changed to PLAYING, but it is not
allowed to play the data which would make the clock run. In summary, PAUSED is
the same as PLAYING but 'without' a running clock.

Elements going into the PAUSED state should prepare themselves for moving over
to the PLAYING state as soon as possible. Video or audio outputs would, for
example, wait for data to arrive and queue it so they can play it right after
the state change. Also, video sinks can already play the first frame (since this
    does not affect the clock yet). Autopluggers could use this same state
transition to already plug together a pipeline. 

Most other elements, such as codecs or filters, do not need to explicitly do
anything in this state, however.

* GST_STATE_PLAYING
an element does exactly the same as in the PAUSED state, except that the clock
now runs.

You can change the state of an element using the function
gst_element_set_state(). If you set an element to another state, GStreamer will
internally traverse all intermediate states. So if you set an element from NULL
to PLAYING, GStreamer will internally set the element to READY and PAUSED in
between.

note: only for playing?

You can only move between adjacent ones, this is, you can't go from NULL to
PLAYING, you have to go through the intermediate READY and PAUSED states. If you
set the pipeline to PLAYING, though, GStreamer will make the intermediate
transitions for you.

note: see when threads start

When moved to GST_STATE_PLAYING, pipelines will process data automatically.
They do not need to be iterated in any form. Internally, GStreamer will start
threads that take this task on to them. GStreamer will also take care of
switching messages from the pipeline's thread into the application's own thread,
          by using a GstBus. 


When you set a bin or pipeline to a certain target state, it will usually
propagate the state change to 'all' elements within the bin or pipeline
automatically, so it's usually only necessary to set the state of the top-level
pipeline to start up the pipeline or shut it down. 

note: see when needs manually to set state

However, when adding elements dynamically to an already-running pipeline, e.g.
from within a "pad-added" signal callback, you need to set it to the desired
target state yourself using gst_element_set_state () or
gst_element_sync_state_with_parent ().


={============================================================================
*kt_dev_bcast_334* gst-man: 06: bins

A bin is a container element. You can 'add' elements to a bin. Since a bin is an
element 'itself', a bin can be handled in the same way as any other element.
Therefore, the whole previous chapter (Elements) applies to bins as well.


6.1. What are bins

Bins allow you to 'combine' a group of linked elements into one 'logical'
element. You do not deal with the individual elements anymore but with just one
element, the bin. We will see that this is extremely powerful when you are going
to construct complex pipelines since it allows you to break up the pipeline in
smaller chunks.

The bin will also 'manage' the elements contained in it. It will perform state
changes on the elements as well as collect and forward bus messages.


There is one 'specialized' type of bin available to the GStreamer programmer:

A pipeline: a generic container that manages the synchronization and bus
messages of the contained elements. The toplevel bin has to be a pipeline, every
application thus needs at least one of these.

note: pipeline is a special bin.


6.2. Creating a bin

Bins are created in the same way that other elements are created, i.e. using
an element factory. There are also convenience functions available
(gst_bin_new () and gst_pipeline_new ()). To add elements to a bin or remove
elements from a bin, you can use gst_bin_add () and gst_bin_remove (). Note
that the bin that you add an element to will take ownership of that element. 

If you destroy the bin, the element will be dereferenced with it. If you remove
an element from a bin, it will be dereferenced automatically.


6.4. Bins manage states of their children

Bins manage the state of all elements contained in them. If you set a bin (or a
    pipeline, which is a special top-level type of bin) to a certain target
state using gst_element_set_state(), it will make sure all elements contained
within it will also be set to this state. This means it's usually only necessary
to set the state of the top-level pipeline to start up the pipeline or shut it
down.

The bin will perform the state changes on all its children 'from' the sink
element 'to' the source element. This ensures that the downstream element is
ready to receive data when the upstream element is brought to PAUSED or PLAYING.
Similarly when shutting down, the sink elements will be set to READY or NULL
first, which will cause the upstream elements to receive a FLUSHING error and
stop the streaming threads before the elements are set to the READY or NULL
state.

Note, however, that if elements are added to a bin or pipeline that's already
running, e.g. from within a "pad-added" signal callback, its state will not
automatically be brought in line with the current state or target state of the
bin or pipeline it was added to. Instead, you have to need to set it to the
desired target state yourself using gst_element_set_state () or
gst_element_sync_state_with_parent () when adding elements to an already-running
pipeline.


={============================================================================
*kt_dev_bcast_335* gst-man: 07: bus

A bus is a simple system that takes care of forwarding messages from the
streaming threads to an application in its own thread context. The advantage of
a bus is that an application does not need to be thread-aware in order to use
GStreamer, even though GStreamer itself is heavily threaded.

'every' pipeline contains a bus by default, so applications do not need to
create a bus or anything. The only thing applications should do is set a message
handler on a bus, which is similar to a signal handler to an object. When the
mainloop is running, the bus will periodically be checked for new messages, and
the callback will be called when any message is available.


7.1. How to use a bus

There are two different ways to use a bus:

* Run a GLib/Gtk+ main loop or iterate the default GLib main context yourself
regularly and attach some kind of watch to the bus. 

This way the GLib main loop will check the bus for new messages and notify you
whenever there are messages. Typically you would use gst_bus_add_watch () or
gst_bus_add_signal_watch () in this case.

To use a bus, attach a message handler to the bus of a pipeline using
gst_bus_add_watch (). This handler will be called 'whenever' the pipeline emits
a message to the bus. In this handler, check the signal type (see next section)
and do something accordingly. The return value of the handler should be TRUE to
keep the handler attached to the bus, return FALSE to remove it. 


<ex>

#include <gst/gst.h>

static GMainLoop *loop;

static gboolean my_bus_callback (GstBus *bus, GstMessage *message, gpointer data)
{
  g_print ("Got %s message\n", GST_MESSAGE_TYPE_NAME (message));

  switch (GST_MESSAGE_TYPE (message)) 
  {
    case GST_MESSAGE_ERROR: {
                              GError *err;
                              gchar *debug;
                              gst_message_parse_error (message, &err, &debug);
                              g_print ("Error: %s\n", err->message);
                              g_error_free (err);
                              g_free (debug);
                              g_main_loop_quit (loop);
                              break;
                            }
    case GST_MESSAGE_EOS:
                            /* end-of-stream */
                            g_main_loop_quit (loop);
                            break;
    default:
                            /* unhandled message */
                            break;
  }

  /* we want to be notified again the next time there is a message
   * on the bus, so returning TRUE (FALSE means we want to stop watching
   * for messages on the bus and our callback should not be called again)
   */
  return TRUE;
}

gint main (gint argc, gchar *argv[])
{
  GstElement *pipeline;
  GstBus *bus;
  guint bus_watch_id;

  /* init */
  gst_init (&argc, &argv);

  /* create pipeline, add handler */
  pipeline = gst_pipeline_new ("my_pipeline");

  /* adds a watch for new message on our pipeline's message bus to
   * the default GLib main context, which is the main context that our
   * GLib main loop is attached to below
   */
  bus = gst_pipeline_get_bus (GST_PIPELINE (pipeline));
  bus_watch_id = gst_bus_add_watch (bus, my_bus_callback, NULL);
  gst_object_unref (bus);

  /*...*/

  /* create a mainloop that runs/iterates the default GLib main context
   * (context NULL), in other words: makes the context check if anything
   * it watches for has happened. When a message has been posted on the
   * bus, the default main context will automatically call our
   * my_bus_callback() function to notify us of that message.
   * The main loop will be run until someone calls g_main_loop_quit()
   */
  loop = g_main_loop_new (NULL, FALSE);
  g_main_loop_run (loop);

  /* clean up */
  gst_element_set_state (pipeline, GST_STATE_NULL);
  gst_object_unref (pipeline);
  g_source_remove (bus_watch_id);
  g_main_loop_unref (loop);

  return 0;
}


* Check for messages on the bus yourself. 

This can be done using gst_bus_peek () and/or gst_bus_poll ().


7.2. Message types

GStreamer has a few pre-defined message types that can be passed over the bus.
The messages are extensible, however. Plug-ins can define additional messages,
    and applications can decide to either have specific code for those or ignore
    them. All applications are strongly recommended to at least handle error
    messages by providing visual feedback to the user.

All messages have a message source, type and timestamp. The message source can
be used to see which element emitted the message. For some messages, for
example, only the ones emitted by the top-level pipeline will be interesting to
most applications (e.g. for state-change notifications). Below is a list of all
messages and a short explanation of what they do and how to parse
message-specific content.

note: what's the top-level pipeline?


* Error, warning and information notifications: 
those are used by elements if a message should be shown to the user about the
state of the pipeline. Error messages are fatal and terminate the data-passing.
The error should be repaired to resume pipeline activity. Warnings are not
fatal, but imply a problem nevertheless. Information messages are for
non-problem notifications. All those messages contain a GError with the main
error type and 'message', and optionally a 'debug' string. Both can be extracted
using gst_message_parse_error (), _parse_warning () and _parse_info (). Both
error and debug strings should be 'freed' after use.


* End-of-stream notification: 
this is emitted when the stream has ended. The state of the pipeline will not
change, but further media handling will stall. Applications can use this to skip
to the next song in their playlist. After end-of-stream, it is also possible to
seek back in the stream. Playback will then continue automatically. This message
has no specific arguments.


* Tags: 
emitted when metadata was found in the stream. This can be emitted multiple
times for a pipeline (e.g. once for descriptive metadata such as artist name or
        song title, and another one for stream-information, such as samplerate
        and bitrate). Applications should cache metadata internally.
gst_message_parse_tag () should be used to parse the taglist, which should be
gst_tag_list_unref () ’ed when no longer needed.


* State-changes: 
emitted after a successful state change. gst_message_parse_state_changed () can
be used to parse the old and new state of this transition.  


* Buffering: 
emitted during caching of network-streams. One can manually extract the progress
(in percent) from the message by extracting the “buffer-percent” property from
the structure returned by gst_message_get_structure (). See also Chapter 15.


* Element messages: 
these are special messages that are unique to certain elements and usually
represent additional features. The element's documentation should mention in
detail which element messages a particular element may send. As an example, the
qtdemux QuickTime demuxer element may send a redirect element message on certain
occasions if the stream contains a redirect instruction.

* Application-specific messages: 
any information on those can be extracted by getting the message structure (see
        above) and reading its fields. Usually these messages can safely be
ignored. Application messages are primarily meant for internal use in
applications in case the application needs to marshal information from some
thread into the main thread. This is particularly useful when the application is
making use of element signals (as those signals will be emitted in the context
        of the streaming thread).


<want-to-message-from-pipeline>
Every element puts messages on the bus regarding its current state, so we
'filter' them out and only listen to messages coming from the pipeline.

case GST_MESSAGE_STATE_CHANGED:
  /* We are only interested in state-changed messages from the pipeline */
  if (GST_MESSAGE_SRC (msg) == GST_OBJECT (data.pipeline)) {
    GstState old_state, new_state, pending_state;
    gst_message_parse_state_changed (msg, &old_state, &new_state, &pending_state);
    g_print ("Pipeline state changed from %s to %s:\n",
        gst_element_state_get_name (old_state), gst_element_state_get_name (new_state));
  }
  break;


={============================================================================
*kt_dev_bcast_330* gst-man: 08: pad and capability

As we have seen in Elements, the pads are "the element's 'interface'" to the
outside world. Data streams from one element's source pad to another element's
sink pad. The specific type of media that the element can handle will be exposed
by the pad's capabilities. 

8.1. Pads

A pad type is defined by two properties: its 'direction' and its 'availability'.
GStreamer defines two pad directions: source pads and sink pads. This
terminology is defined from the view of within the element: elements receive
data on their sink pads and generate data on their source pads. 

Schematically, sink pads are drawn on the left side of an element, whereas
source pads are drawn on the right side of an element. In such graphs, data
flows from left to right.

<pad-types>
A pad can have any of three availabilities: always, sometimes and on request. 

The meaning of those three types is exactly as it says: always pads always
exist, sometimes pad exist only in certain cases (and can disappear randomly),
and on-request pads appear only if explicitly requested by applications.

8.1.1. Dynamic (or sometimes) pads

Some elements might not have all of their pads when the element is created.
This can happen, for example, with an Ogg demuxer element. The element will read
the Ogg stream and create dynamic pads for 'each' contained elementary stream
(vorbis, theora) when it detects such a stream in the Ogg stream. Likewise, it
will delete the pad when the stream ends. This principle is very useful for
demuxer elements, for example.

You can see this in the pad template because there is an "Exists: Sometimes"
property. Depending on the type of Ogg file you play, the pads will be created.

We will see that this is very important when you are going to create 'dynamic'
pipelines. You can attach a signal handler to an element to inform you when the
element has created a new pad from one of its "sometimes" pad templates. The
following piece of code is an example of how to do this:

<snippet>
Pad Templates:
  SINK template: 'sink'
    Availability: Always
    Capabilities:
      ANY

  SRC template: 'src_%u'
    Availability: Sometimes
    Capabilities:
      ANY

<pad-added>
The demuxers cannot produce any information until they have received some data
and have had a chance to look at the container to see what is inside. So no
source pads to which other elements can link.

The solution is to build the pipeline from the source down to the demuxer, and
set it to 'run' (play). When the demuxer has received enough information to know
about the number and kind of streams in the container, it will start creating
source pads. This is the right time for us to finish building the pipeline and
attach it to the newly added demuxer pads.

/* Connect to the pad-added signal */
g_signal_connect (data.source, "pad-added", G_CALLBACK (pad_added_handler),
        &data);

In this line, are attaching to the "pad-added" signal of our uridecodebin
  element. When our element finally has enough information to start producing
  data, it will create source pads, and trigger the "pad-added" signal. At this
  point our callback will be called

note: source pad will be created from source element. This callback is to 'link'
this new source to sink in a pipeline.

/* This function will be called by the pad-added signal */
static void pad_added_handler (GstElement *src, GstPad *new_pad, 
        CustomData *data) 
{
  GstPad *sink_pad = gst_element_get_static_pad (data->convert, "sink");
  GstPadLinkReturn ret;
  GstCaps *new_pad_caps = NULL;
  GstStructure *new_pad_struct = NULL;
  const gchar *new_pad_type = NULL;
   
  g_print ("Received new pad '%s' from '%s':\n", GST_PAD_NAME (new_pad), GST_ELEMENT_NAME (src));
   
  /* If our converter is already linked, we have nothing to do here */
  if (gst_pad_is_linked (sink_pad)) {
    g_print ("  We are already linked. Ignoring.\n");
    goto exit;
  }
   
  /* Check the new pad's type */
  new_pad_caps = gst_pad_get_caps (new_pad);
  new_pad_struct = gst_caps_get_structure (new_pad_caps, 0);
  new_pad_type = gst_structure_get_name (new_pad_struct);
  if (!g_str_has_prefix (new_pad_type, "audio/x-raw")) {
    g_print ("  It has type '%s' which is not raw audio. Ignoring.\n", new_pad_type);
    goto exit;
  }
   
  /* Attempt the link */
  ret = gst_pad_link (new_pad, sink_pad);
  if (GST_PAD_LINK_FAILED (ret)) {
    g_print ("  Type is '%s' but link failed.\n", new_pad_type);
  } else {
    g_print ("  Link succeeded (type '%s').\n", new_pad_type);
  }
   
exit:
  /* Unreference the new pad's caps, if we got them */
  if (new_pad_caps != NULL)
    gst_caps_unref (new_pad_caps);
   
  /* Unreference the sink pad */
  gst_object_unref (sink_pad);
}

note:
It is not uncommon to add elements to the pipeline only from within the
"pad-added" callback. If you do this, don't forget to set the state of the
newly-added elements to the target state of the pipeline using
gst_element_set_state () or gst_element_sync_state_with_parent ().


8.1.2. Request pads

An element can also have request pads. These pads are not created
automatically but are only created on demand. This is very useful for
multiplexers, aggregators and tee elements. Aggregators are elements that
merge the content of several input streams together into one output stream.
Tee elements are the reverse: they are elements that have one input stream and
copy this stream to each of their output pads, which are created on request.
Whenever an application needs another copy of the stream, it can simply
request a new output pad from the tee element.

// skipped the rest


8.2. Capabilities of a pad

Since the pads play a very important role in how the element is viewed by the
outside world, a mechanism is implemented to describe the data that can flow
or currently flows through the pad by using capabilities. Here, we will
briefly describe what capabilities are and how to use them, enough to get an
understanding of the concept. For an in-depth look into capabilities and a
list of all capabilities defined in GStreamer, see the Plugin Writers Guide.

Capabilities are attached to pad templates and to pads. For pad templates, it
will describe the types of media that "may stream over" a pad created from
this template. For pads, it can either be a list of possible caps (usually a
        copy of the pad template's capabilities), in which case the pad is not
yet negotiated, or it is the type of media that currently streams over this
pad, in which case the pad has been negotiated already.


8.2.1. Dissecting capabilities

note: capability is "media type"

A pad's capabilities are described in a GstCaps object. Internally, a GstCaps
(http://gstreamer.freedesktop.org/data/doc/gstreamer/stable/gstreamer/html/gstreamer-GstCaps.html)
will contain one or more GstStructure
(http://gstreamer.freedesktop.org/data/doc/gstreamer/stable/gstreamer/html/gstreamer-GstStructure.html)
that will describe 'one' media type. A negotiated pad will have capabilities
set that contain exactly one structure. Also, this structure will contain only
fixed values. These constraints are not true for unnegotiated pads or pad
templates.

Pad Templates:
   SRC template: ’src’
      Availability: Always
      Capabilities:
         audio/x-raw
            format: F32LE
            rate: [ 1, 2147483647 ]
            channels: [ 1, 256 ]

The source pad will be used to send raw (decoded) audio samples to the next element, with a raw
audio media type (in this case, "audio/x-raw"). The source pad will also contain 'properties' for the
audio samplerate and the amount of channels, plus some more.

// other example

Pad Templates:
  SINK template: 'sink'
    Availability: Always
    Capabilities:
      video/mpegts
           systemstream: true
      video/x-h264
          stream-format: byte-stream
              alignment: { au, nal }
      video/mpeg
            mpegversion: { 2, 4 }
           systemstream: false
      audio/mpeg
            mpegversion: 1
                  layer: { 1, 3 }
      audio/mpeg
            mpegversion: 4
          stream-format: { adts, loas }

<code>
  /* Check the new pad's type */
  new_pad_caps = gst_pad_get_caps (new_pad);
  new_pad_struct = gst_caps_get_structure (new_pad_caps, 0);
  new_pad_type = gst_structure_get_name (new_pad_struct);
  if (!g_str_has_prefix (new_pad_type, "audio/x-raw")) {
    g_print ("  It has type '%s' which is not raw audio. Ignoring.\n", new_pad_type);
    goto exit;
  }

gst_pad_get_caps() retrieves the capabilities of the pad wrapped in a GstCaps structure. A pad can
offer many capabilities, and hence GstCaps can contain many GstStructure, 'each' representing a
different capability.

Since, in this case, we know that the pad we want only had one capability (audio), we retrieve the
first GstStructure with gst_caps_get_structure().

Finally, with gst_structure_get_name() we recover the name of the structure, which contains the main
description of the format (its 'MIME' type, actually).


8.2.2. Properties and values

Properties are used to describe extra information for capabilities. A property consists of a key (a
        string) and a value. There are different possible value types that can be used:

* Basic types, this can be pretty much any GType registered with Glib. Those properties indicate a
specific, non-dynamic value for this property. Examples include:

** An integer value (G_TYPE_INT): the property has this exact value.
** A boolean value (G_TYPE_BOOLEAN): the property is either TRUE or FALSE.
** A float value (G_TYPE_FLOAT): the property has this exact floating point value.
** A string value (G_TYPE_STRING): the property contains a UTF-8 string.
** A fraction value (GST_TYPE_FRACTION): contains a fraction expressed by an integer numerator and
denominator.

// skipped the rest


8.3. What capabilities are used for

Capabilities (short: caps) describe the type of data that is streamed between two pads, or that one
pad (template) supports. This makes them very useful for various purposes:

* Autoplugging: automatically finding elements to link to a pad based on its capabilities. All
autopluggers use this method.

* Compatibility detection: when two pads are linked, GStreamer can verify if the two pads are
talking about the same media type. The process of linking two pads and checking if they are
compatible is called "caps negotiation".

* Metadata: by reading the capabilities from a pad, applications can provide information about the
type of media that is being streamed over the pad, which is information about the stream that is
'currently' being played back.

* Filtering: an application can use capabilities to limit the possible media types that can stream
between two pads to a specific subset of their supported stream types. An application can, for
example, use "filtered caps" to set a specific (fixed or non-fixed) video size that should stream
between two pads.

You will see an example of filtered caps later in this manual, in Section 19.2. You can do caps
filtering by inserting a capsfilter element into your pipeline and setting its "caps" property. Caps
filters are often placed after converter elements like audioconvert, audioresample, videoconvert or
videoscale to force those converters to convert data to a specific output format at a certain point
in a stream.

note: from possible caps, allowed and negotiated cap

Note that there is a distinct difference between the possible capabilities of a pad; ie. usually
what you find as caps of pad templates as they are shown in gst-inspect; the allowed caps of a pad;
can be the same as the pad's template caps or a subset of them, depending on the possible caps of
the 'peer' pad; and lastly negotiated caps. these describe the exact format of a stream or buffer
and contain "exactly one structure" and have no variable bits like ranges or lists, ie. they are
fixed caps.

<type-of-caps>
Caps are called simple caps when they contain only one structure, and fixed caps when they contain
only one structure and have no variable field types (like ranges or lists of possible values). Two
other special types of caps are ANY caps and empty caps.


8.4. Ghost pads

You can see from Figure 8-1 how a bin has no pads of its own. This is where
"ghost pads" come into play.

note: From 06 GstBin, bin is an element itself but do not have pad. Make sence
to have a ghost pad in thinking a data flow but perhaps in using APIs.


bin
+==============================================+
|           element 1            element 2     |
|           [sink   src]   ->    [sink       ] |
|                                              |
+==============================================+

A ghost pad is a pad from some element in the bin that can be accessed
'directly' from the 'bin' as well. Compare it to a 'symbolic' link in UNIX
filesystems. Using ghost pads on bins, the bin also has a pad and can
transparently be used as an element in other parts of your code.


bin
+==============================================+
|           element 1            element 2     |
|sink]------[sink   src]   ->    [sink       ] |
|                                              |
+==============================================+

This is a representation of a ghost pad. The sink pad of element one is now
'also' a pad of the bin. Because ghost pads look and work like any other pads,
  they can be added to any type of elements, not just to a GstBin, just like
  ordinary pads.

int
main (int argc, char *argv[])
{
  GstElement *bin, *sink;
  GstPad *pad;

  /* init */
  gst_init (&argc, &argv);

  /* create element, add to bin */
  sink = gst_element_factory_make ("fakesink", "sink");
  bin = gst_bin_new ("mybin");
  gst_bin_add (GST_BIN (bin), sink);

  /* add ghostpad */
  pad = gst_element_get_static_pad (sink, "sink");
  gst_element_add_pad (bin, gst_ghost_pad_new ("sink", pad));
  gst_object_unref (GST_OBJECT (pad));
  [..]
}

In the above example, the bin now also has a pad: the pad called "sink" of the
given element. The bin can, from here on, be used as a substitute for the sink
element. You could, for example, link another element to the bin.


={============================================================================
*kt_dev_bcast_330* gst-doc: clocks and synchronization

note: from gst manual 14 which is the same as plugin guide

GStreamer uses a GstClock object, buffer timestamps and a SEGMENT event to
synchronize streams in a pipeline

A GstClock returns the absolute-time according to that clock with
gst_clock_get_time (). The absolute-time (or clock time) of a clock is
monotonically increasing. From the absolute-time is a running-time calculated,
which is simply the difference between a previous 'snapshot' of the
  absolute-time called the base-time. So:

running-time = absolute-time - base-time

A GStreamer GstPipeline object maintains a GstClock object and a base-time when
it goes to the PLAYING state. The pipeline gives a handle to the selected
GstClock to 'each' element in the pipeline along with selected base-time. The
pipeline will select a base-time in such a way that the running-time reflects
the total time 'spent' in the PLAYING state. As a result, when the pipeline is
PAUSED, the running-time stands still.

Because all objects in the pipeline have the same clock and base-time, they can
thus all calculate the running-time according to the pipeline clock.


14.2. Buffer running-time

To calculate a buffer running-time, we need a buffer timestamp and the SEGMENT
event that preceeded the buffer. First we can convert the SEGMENT event into a
GstSegment object and then we can use the gst_segment_to_running_time ()
function to perform the calculation of the buffer running-time.

// gst_segment_to_running_time ()
// 
// guint64
// gst_segment_to_running_time (const GstSegment *segment,
//                              GstFormat format,
//                              guint64 position);
// 
// Translate position to the total running time using the currently configured
// segment. Position is a value between segment start and stop time.
// 
// This function is typically used by elements that need to synchronize to the
// global clock in a pipeline. The runnning time is a constantly increasing
// value starting from 0. When gst_segment_init() is called, this value will
// reset to 0.
// 
// This function returns -1 if the position is outside of segment start and
// stop.
// 
// Parameters 
// segment     a GstSegment structure.  
// format      the format of the segment.  
// position    the position in the segment
// 	 
// Returns the position as the total running time or -1 when an invalid position
// was given.

Synchronization is now a matter of making sure that a buffer with a certain
running-time is played when the clock reaches the same running-time. Usually
this task is done by 'sink' elements. Sink also have to take into account the
latency configured in the pipeline and add this to the buffer running-time
before synchronizing to the pipeline clock.

<non-live>
Non-live sources 'timestamp' buffers with a running-time starting from 0. After
a flushing seek, they will produce buffers again from a running-time of 0.

note: source in a pipeline do timestamp on buffers. For example:

! dashdemux name=dash dash. ! video/quicktime ! qtdemux ! queue 
                                              ^1        ^2

When put identity element at 1, no PTS is printed.
When put identity element at 2, PTS is printed.


<live>
Live sources need to 'timestamp' buffers with a running-time matching the
pipeline running-time when the first byte of the buffer was captured.


14.3. Buffer stream-time

The buffer stream-time, also known as the position in the stream, is calculated
from the buffer timestamps and the preceding SEGMENT event. It represents the
time 'inside' the media as a value between 0 and the total duration of the media.

The stream-time is used in:
* Report the current position in the stream with the POSITION query.
* The position used in the seek events and queries.
* The position used to synchronize controlled values.

note: The stream-time is never used to synchronize streams, this is only done
with the running-time.


14.4. Time overview

The image below represents the different times in the pipeline when playing a
100ms sample and 'repeating' the part between 50ms and 100ms.

Figure 14-1. GStreamer clock and various times

               100 ms sample
               |-------------------------------|

stream time    10 20 30 40 50 60  70  80  90  100 60  70  80  90  ...
running time   10 20 30 40 50 60  70  80  90  100 110 120 130 140 ...
clock time     50 60 70 80 90 100 120 130 140 150 160 170 180 190 ...

               50 base time

You can see how the running-time of a buffer always increments monotonically
along with the clock-time. Buffers are 'played' when their running-time is equal
to the (clock-time - base-time). The stream-time represents the position in the
stream and jumps backwards when repeating.


14.5. Clock providers

A clock provider is an element in the pipeline that can provide a GstClock
object.

note: slaving

If an element with an internal clock needs to synchronize, it needs to estimate
when a time according to the pipeline clock will take place according to the
internal clock. To estimate this, it needs to slave its clock to the pipeline
clock.

If the pipeline clock is exactly the internal clock of an element, the element
can skip the slaving step and directly use the pipeline clock to schedule
playback. This can be both faster and more accurate.

Therefore, generally, elements with an internal clock like audio input or output
devices will be a clock provider for the pipeline.

When the pipeline goes to the PLAYING state, it will go over all elements in the
pipeline from sink to source and ask each element if they can provide a clock.
The last element that can provide a clock will be used as the clock provider in
the pipeline. 

note: audio sink?

This algorithm prefers a clock from an audio sink in a typical playback pipeline
and a clock from source elements in a typical capture pipeline.

note: bus message

There exist some bus messages to let you know about the clock and clock
providers in the pipeline. You can see what clock is selected in the pipeline by
looking at the NEW_CLOCK message on the bus.When a clock provider is removed
from the pipeline, a CLOCK_LOST message is posted and the application should go
to PAUSED and back to PLAYING to select a new clock.


14.6. Latency

The latency is the time it takes for a sample captured at timestamp X to reach
the sink. This time is measured against the clock in the pipeline. 

For pipelines where the only elements that synchronize against the clock are the
sinks, the latency is always 0 since no other element is delaying the buffer.

For pipelines with live sources, a latency is introduced, mostly because of the
way a live source works.  Consider an audio source, it will start capturing the
first sample at time 0. If the source pushes buffers with 44100 samples at a
time at 44100Hz it will have collected the buffer at second 1. Since the
timestamp of the buffer is 0 and the time of the clock is now >= 1 second, the
sink will drop this buffer because it is too late. Without any latency
compensation in the sink, all buffers will be dropped.


14.6.1. Latency compensation

note: latency query and maximum

'before' the pipeline goes to the PLAYING state, it will, in addition to
selecting a clock and calculating a base-time, calculate the latency in the
pipeline. It does this by doing a LATENCY query on all the sinks in the
pipeline. The pipeline then selects the maximum latency in the pipeline and
configures this with a LATENCY event.

All sink elements will delay playback by the value in the LATENCY event. Since
all sinks delay with the same amount of time, they will be relative in sync.

// void
// gst_query_set_latency (GstQuery *query,
//                        gboolean live,
//                        GstClockTime min_latency,
//                        GstClockTime max_latency);
// 
// 'answer' a latency query by setting the requested values in the given format.
// 
// live
// if there is a live element upstream
// 	 
// min_latency
// the minimal latency of the upstream elements
// 	 
// max_latency
// the maximal latency of the upstream elements
//
// typedef guint64 GstClockTime;
//
// Q: does this mean to set MAX UINT as type is gunit64?
// gst_query_set_latency (query, FALSE, 0, -1);


14.6.2. Dynamic Latency

Adding/removing elements to/from a pipeline or changing element properties can
change the latency in a pipeline. An element can request a latency change in the
pipeline by posting a LATENCY message on the bus. The application can then
decide to query and redistribute a new latency or not. Changing the latency in a
pipeline might cause visual or audible glitches and should therefore only be
done by the application when it is allowed.


={============================================================================
*kt_dev_bcast_330* gst: man: 15: buffering

Chapter 15. Buffering

The purpose of buffering is to accumulate enough data in a pipeline so that
playback can occur smoothly and without interruptions. It is typically done when
reading from a (slow) and non-live network source but can also be used for live
sources.

GStreamer provides support for the following use cases:

* Buffering up to a specific amount of data, in memory, before starting playback
so that network fluctuations are minimized. See the section called "Stream
buffering".

* Download of the network file to a local disk with fast seeking in the
downloaded data. This is similar to the quicktime/youtube players. See the
section called "Download buffering".

* Caching of (semi)-live streams to a local, on disk, ringbuffer with seeking in
the cached area. This is similar to tivo-like timeshifting. See the section
called "Timeshift buffering". 

GStreamer can provide the application with progress reports about the current
buffering state as well as let the application decide on how to buffer and when
the buffering stops.

In the most simple case, the application has to listen for BUFFERING messages on
the bus. If the percent indicator inside the BUFFERING message is smaller than
100, the pipeline is buffering. When a message is received with 100 percent,
buffering is complete. In the buffering state, the application should keep the
    pipeline in the PAUSED state. When buffering completes, it can put the
    pipeline (back) in the PLAYING state.

What follows is an example of how the message handler could deal with the
BUFFERING messages. We will see more advanced methods in the section called
"Buffering strategies". 

 [...]

  switch (GST_MESSAGE_TYPE (message)) {
    case GST_MESSAGE_BUFFERING:{
      gint percent;

      /* no state management needed for live pipelines */
      if (is_live)
        break;

      gst_message_parse_buffering (message, &percent);

      if (percent == 100) {
        /* a 100% message means buffering is done */
        buffering = FALSE;
        /* if the desired state is playing, go back */
        if (target_state == GST_STATE_PLAYING) {
          gst_element_set_state (pipeline, GST_STATE_PLAYING);
        }
      } else {
        /* buffering busy */
        if (!buffering && target_state == GST_STATE_PLAYING) {
          /* we were not buffering but PLAYING, PAUSE  the pipeline. */
          gst_element_set_state (pipeline, GST_STATE_PAUSED);
        }
        buffering = TRUE;
      }
      break;
    case ...

  [...]

Stream buffering

      +---------+     +---------+     +-------+
      | httpsrc |     | buffer  |     | demux |
      |        src - sink      src - sink     ....
      +---------+     +---------+     +-------+
    

In this case we are reading from a slow network source into a buffer element
(such as queue2).

The buffer element has a low and high watermark expressed in bytes. The buffer
uses the watermarks as follows:

* The buffer element will post BUFFERING messages 'until' the high watermark is
hit. This instructs the application to keep the pipeline PAUSED, which will
eventually block the srcpad from pushing while data is prerolled in the sinks.

* When the high watermark is hit, a BUFFERING message with 100% will be posted,
which instructs the application to continue playback.

* When during playback, the low watermark is hit, the queue will start posting
BUFFERING messages again, making the application PAUSE the pipeline again until
the high watermark is hit again. This is called the 'rebuffering' stage.

* During playback, the queue level will fluctuate between the high and the low
watermark as a way to compensate for network irregularities. 

note: push mode

This buffering method is usable when the demuxer operates in push mode. Seeking
in the stream requires the seek to happen in the network source. It is mostly
desirable when the total duration of the file is not known, such as in live
streaming or when efficient seeking is not possible/required.

The problem is configuring a good low and high watermark. Here are some ideas:

* It is possible to measure the network bandwidth and configure the low/high
watermarks in such a way that buffering takes a fixed amount of time.

* The queue2 element in GStreamer core has the max-size-time property that,
together with the use-rate-estimate property, does exactly that. Also the
    playbin buffer-duration property uses the rate estimate to scale the amount
    of data that is buffered.

* Based on the codec bitrate, it is also possible to set the watermarks in such
a way that a fixed amount of data is buffered before playback starts.  Normally,
the buffering element doesn't know about the bitrate of the stream but it can
    get this with a query.

* Start with a fixed amount of bytes, measure the time between rebuffering and
increase the queue size until the time between rebuffering is within the
application's chosen limits. 

The buffering element can be inserted anywhere in the pipeline. You could, for
example, insert the buffering element before a decoder. This would make it
possible to set the low/high watermarks based on time.

note: pull mode

The buffering flag on playbin, performs buffering on the parsed data. Another
advantage of doing the buffering at a later stage is that you can let the
demuxer operate in pull mode. When reading data from a slow network drive (with
        filesrc) this can be an interesting way to buffer. 


Download buffering

      +---------+     +---------+     +-------+
      | httpsrc |     | buffer  |     | demux |
      |        src - sink      src - sink     ....
      +---------+     +----|----+     +-------+
                           V
                          file
    
note: push or pull based?

If we know the server is streaming a fixed length file to the client, the
application can choose to download the entire file on disk. The buffer element
will provide a push or pull based srcpad to the demuxer to navigate in the
downloaded file.

This mode is only suitable when the client can determine the length of the file
on the server.

In this case, buffering messages will be emitted as usual when the requested
range is not within the downloaded area + buffersize. The buffering message will
also contain an indication that incremental download is being performed. This
flag can be used to let the application control the buffering in a more
intelligent way, using the BUFFERING query, for example. See the section called
"Buffering strategies". 


Timeshift buffering

      +---------+     +---------+     +-------+
      | httpsrc |     | buffer  |     | demux |
      |        src - sink      src - sink     ....
      +---------+     +----|----+     +-------+
                           V
                       file-ringbuffer
    

note: seek in the buffered data

In this mode, a fixed size ringbuffer is kept to download the server content.
This allows for seeking in the buffered data. Depending on the size of the
ringbuffer one can seek further back in time.

This mode is suitable for all live streams. As with the incremental download
mode, buffering messages are emitted along with an indication that timeshifting
download is in progress. 


Live buffering

In live pipelines we usually introduce some fixed latency between the capture
and the playback elements. This latency can be introduced by a queue (such as a
        jitterbuffer) or by other means (in the audiosink).

Buffering messages can be emitted in those live pipelines as well and serve as
an indication to the user of the latency buffering. The application usually does
not react to these buffering messages with a state change. 


Buffering strategies

What follows are some ideas for implementing different buffering strategies
based on the buffering messages and buffering query.  

No-rebuffer strategy

We would like to buffer enough data in the pipeline so that playback continues
without interruptions. What we need to know to implement this is know the total
remaining playback time in the file and the total remaining download time. If
the buffering time is less than the playback time, we can start playback without
interruptions.

We have all this information available with the DURATION, POSITION and BUFFERING
queries. We need to periodically execute the buffering query to get the current
buffering status. We also need to have a large enough buffer to hold the
complete file, worst case. It is best to use this buffering strategy with
download buffering (see the section called "Download buffering").

This is what the code would look like: 

note: skipped the code


={============================================================================
*kt_dev_bcast_330* gst: man: 18: autoplug and typefind

However, you would rather want to build an application that can automatically detect the media type
of a stream and automatically generate the best possible pipeline by looking at 'all' available
elements in a system. This process is called autoplugging, and GStreamer contains high-quality
autopluggers. If you're looking for an autoplugger, don't read any further and go to Chapter 20.
This chapter will explain the concept of autoplugging and typefinding. It will explain what systems
GStreamer includes to dynamically detect the type of a media stream, and how to generate a pipeline
of decoder elements to playback this media. The same principles can also be used for transcoding.
Because of the full dynamicity of this concept, GStreamer can be automatically extended to support
new media types without needing any adaptations to its autopluggers.

An element must associate a media type to its source and sink pads when it is loaded into the
system.  GStreamer knows about the different elements and what type of data they expect and emit
through the GStreamer registry. This allows for very dynamic and extensible element creation as we
will see.

note: media type means MIME type


18.2. Media stream type detection

Usually, when loading a media stream, the type of the stream is not known. This means that before we
can choose a pipeline to decode the stream, we first need to detect the stream type. GStreamer uses
the concept of 'typefinding' for this. 

Typefinding is a normal part of a pipeline, it will read data for as long as the type of a stream is
unknown. During this period, it will provide data to all plugins that implement a typefinder. When
one of the typefinders recognizes the stream, the typefind element will emit a signal and act as a
passthrough module from that point on. If no type was found, it will emit an error and further media
processing will stop.

Once the typefind element has found a type, the application can use this to plug together a pipeline
to decode the media stream.

Plugins in GStreamer can, as mentioned before, implement typefinder functionality. A plugin
implementing this functionality will submit a media type, optionally a set of file extensions
commonly used for this media type, and a typefind function. Once this typefind function inside the
plugin is called, the plugin will see if the data in this media stream matches a specific pattern
that marks the media type identified by that media type. If it does, it will notify the typefind
element of this fact, telling which mediatype was recognized and how certain we are that this stream
is indeed that mediatype. Once this run has been completed for all plugins implementing a typefind
functionality, the "typefind element" will tell the application what kind of media stream it thinks
to have recognized.

/* create file source and typefind element */
filesrc = gst_element_factory_make ("filesrc", "source");
g_object_set (G_OBJECT (filesrc), "location", argv[1], NULL);

typefind = gst_element_factory_make ("typefind", "typefinder");
g_signal_connect (typefind, "have-type", G_CALLBACK (cb_typefound), loop);
fakesink = gst_element_factory_make ("fakesink", "sink");

/* setup */
gst_bin_add_many (GST_BIN (pipeline), filesrc, typefind, fakesink, NULL);
gst_element_link_many (filesrc, typefind, fakesink, NULL);
gst_element_set_state (GST_ELEMENT (pipeline), GST_STATE_PLAYING);
g_main_loop_run (loop);

Once a media type has been detected, you can plug an element (e.g. a demuxer or decoder) to the
'source' pad of the typefind element, and decoding of the media stream will start right after.


={============================================================================
*kt_dev_bcast_330* gst: doc: segment

http://cgit.freedesktop.org/gstreamer/gstreamer/tree/docs/design/part-segments.txt

Segments
--------

A segment in GStreamer denotes a set of media samples that must be processed. A
segment has a start time, a stop time and a processing rate. 

note: -1 is unknown

A media stream has a start and a stop time. The start time is always 0 and the
stop time is the total duration (or -1 if unknown, for example a live stream).
We call this the complete media stream.

The segment of the complete media stream can be played by issuing a seek
on the stream. The seek has a start time, a stop time and a processing rate.
 

               complete stream
  +------------------------------------------------+
  0                                              duration
         segment
     |--------------------------|
   start                       stop


The playback of a segment starts with a source or 'demuxer' element 'pushing' a
segment event containing the start time, stop time and rate of the segment.


note:
The purpose of this segment is to inform downstream elements of the requested
segment positions. Some elements might 'produce' buffers that fall outside of
the segment and that might therefore be discarded or clipped.


Use case: FLUSHING seek
~~~~~~~~~~~~~~~~~~~~~~~

ex.

filesrc ! avidemux ! videodecoder ! videosink

When doing a seek in this pipeline for a segment 1 to 5 seconds, 'avidemux' will
perform the seek.

Avidemux starts by sending a FLUSH_START event downstream and upstream. This
will cause its streaming task to PAUSED because _pad_pull_range() and
_pad_push() will return FLUSHING. It then waits for the STREAM_LOCK, which will
be unlocked when the streaming task pauses. At this point no streaming is
happening anymore in the pipeline and a FLUSH_STOP is sent upstream and
downstream.

When avidemux starts playback of the segment from second 1 to 5, it 'pushes' out
a 'segment' with 1 and 5 as start and stop times. The stream-time 'in' the
segment is also 1 as this is the position we seek to.

The video decoder stores these values internally and forwards them to the next
downstream element (videosink, which also stores the values)

Since second 1 does not contain a keyframe, the avi demuxer starts sending data
from the previous keyframe which is at timestamp 0.

The video decoder decodes the keyframe but knows it should not push the video
frame yet as it falls outside of the configured segment.

When the video decoder receives the frame with timestamp 1, it is able to decode
this frame as it received and decoded the data up to the previous keyframe. It
then continues to decode and push frames with timestamps >= 1. When it reaches
timestamp 5, it does not decode and push frames anymore.

The video 'sink' receives a 'frame' of timestamp 1. It takes the start value of
the previous segment and aplies the following (simplified) formula:

render_time = BUFFER_TIMESTAMP - segment_start + element->base_time

note: this suggest that render_time uses absolute time and buffer and segment
time uses relative time starting from 0.

It then 'syncs' against the clock with this render_time. Note that
BUFFER_TIMESTAMP is always >= segment_start or else it would fall outside of the
configure segment.

Videosink reports its current position as (simplified):

current_position = clock_time - element->base_time + segment_time

See part-synchronisation.txt for a more detailed and accurate explanation of
synchronisation and position reporting.

Since after a flushing seek the stream_time is reset to 0, the new buffer will
be rendered immediately after the seek and the current_position will be the
stream_time of the seek that was performed.

The stop time is important when the video format contains B frames. The video
decoder receives a P frame first, which it can decode but not push yet. When it
receives a B frame, it can decode the B frame and push the B frame followed by
the previously decoded P frame. If the P frame is outside of the segment, the
decoder knows it should not send the P frame.

Avidemux stops sending data after pushing a frame with timestamp 5 and returns
GST_FLOW_EOS from the chain function to make the upstream elements perform the
EOS logic.


Use case: live stream
~~~~~~~~~~~~~~~~~~~~~

Use case: segment looping
~~~~~~~~~~~~~~~~~~~~~~~~~

Consider the case of a wav file with raw audio.

filesrc ! wavparse ! alsasink


http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstSegment.html

Description

This helper structure holds the relevant values for tracking the 'region' of
interest in a media file, called a segment.

The structure can be used for two purposes:

performing seeks (handling seek events)
tracking playback regions (handling newsegment events)

The segment is usually configured by the application with a seek event which is
propagated upstream and eventually handled by an element that performs the seek.

The configured segment is then propagated back downstream with a newsegment
event. This information is then used to clip media to the segment boundaries.

A segment structure is initialized with gst_segment_init(), which takes a
GstFormat that will be used as the format of the segment values. The segment
will be configured with a start value of 0 and a stop/duration of -1, which is
undefined. The default rate and applied_rate is 1.0.

The public duration field contains the duration of the segment. When using the
segment for seeking, the start and time members should normally be left to their
default 0 value. The stop position is left to -1 unless explicitly configured to
a different value after a seek event.

The current position in the segment should be set by changing the position
member in the structure.

For elements that perform seeks, the current segment should be updated with the
gst_segment_do_seek() and the values from the seek event. This method will
update all the segment fields. The position field will contain the new playback
position. If the start_type was different from GST_SEEK_TYPE_NONE, playback
continues from the position position, possibly with updated flags or rate.

For elements that want to use GstSegment to track the playback region, update
the segment fields with the information from the newsegment event. The
gst_segment_clip() method can be used to check and clip the media data to the
segment boundaries.

note:
For elements that want to synchronize to the pipeline clock,
    gst_segment_to_running_time() can be used to convert a timestamp to a value
        that can be used to synchronize to the clock. This function takes into
        account the base as well as any rate or applied_rate conversions.

For elements that need to perform operations on media data in stream_time,
    gst_segment_to_stream_time() can be used to convert a timestamp and the
        segment info to stream time (which is always between 0 and the duration
                of the stream).


={============================================================================
*kt_dev_bcast_330* gst: doc: synchronisation

http://cgit.freedesktop.org/gstreamer/gstreamer/tree/docs/design/part-synchronisation.txt

Synchronisation
---------------

This document outlines the techniques used for doing synchronised playback of
multiple streams.

Synchronisation in a GstPipeline is achieved using the following 3 components:

 - a GstClock, which is global for all elements in a GstPipeline.
 - Timestamps on a GstBuffer.
 - the SEGMENT event preceding the buffers.


A GstClock
~~~~~~~~~~

This object provides a counter that represents the current time in nanoseconds.
This value is called the absolute_time.

Different sources exist for this counter:

 - the system time (with g_get_current_time() and with microsecond accuracy)
 - monotonic time (with g_get_monotonic_time () with microsecond accuracy)
 - an audio device (based on number of samples played)
 - a network source based on packets received + timestamps in those packets (a
   typical example is an RTP source)
 - ...

In GStreamer any element can provide a GstClock object that can be used in the
pipeline. The GstPipeline object will select a clock from all the providers and
will distribute it to all other elements (see part-gstpipeline.txt).

A GstClock always counts time upwards and does not necessarily start at 0.

While it is possible, it is 'not' recommended to create a clock derived from the
contents of a stream (for example, create a clock from the PCR in an mpeg-ts
        stream).


Running time
~~~~~~~~~~~~

After a pipeline selected a clock it will maintain the running_time based on the
selected clock. This running_time represents the total time spent in the PLAYING
state and is calculated as follows:

  If the pipeline is NULL/READY, the running_time is undefined.

  In PAUSED, the running_time remains at the time when it was last PAUSED. When
  the stream is PAUSED for the first time, the running_time is '0'.

  In PLAYING, the running_time is the delta between the absolute_time and the
  base time. The base time is 'defined' as the absolute_time minus the
  running_time at the time when the pipeline is set to PLAYING.

  after a flushing seek, the running_time is set to 0 (see part-seeking.txt).
  This is accomplished by 'redistributing' a new base_time to the elements that
  got flushed.

This algorithm captures the running_time when the pipeline is set from PLAYING
to PAUSED and restores this time based on the current absolute_time when going
back to PLAYING. This allows for both clocks that progress when in the PAUSED
state (systemclock) and clocks that don't (audioclock).

The clock and pipeline now provide a running_time to all elements that want to
perform synchronisation. Indeed, the running time can be observed in each
element (during the PLAYING state) as:
  
  C.running_time = absolute_time - base_time

We note C.running_time as the running_time obtained by looking at the clock.
This value is monotonically increasing at the rate of the clock.


Timestamps
~~~~~~~~~~
 
The GstBuffer timestamps and the preceding SEGMENT event (See part-streams.txt)
'define' a 'transformation' of the buffer timestamps to running_time as follows:

The following notation is used:

note: PTS or DTS

 B: GstBuffer 
  - B.timestamp = buffer timestamp (GST_BUFFER_PTS or GST_BUFFER_DTS)

 S:  SEGMENT event preceding the buffers.
  - S.start: start field in the SEGMENT event. This is the lowest allowed
             timestamp.
  - S.stop: stop field in the SEGMENT event. This is the highers allowed
            timestamp.
  - S.rate: rate field of SEGMENT event. This is the desired playback rate.
  - S.base: a base time for the time. This is the total elapsed running_time of
            any previous segments.
  - S.offset: an offset to apply to S.start or S.stop. This is the amount that
              has already been elapsed in the segment.

Valid buffers for synchronisation are those with B.timestamp between S.start and
S.stop (after applying the S.offset). All other buffers outside this range
should be dropped or clipped to these boundaries (see also part-segments.txt).

The following transformation to running_time exist:

  if (S.rate > 0.0)
    B.running_time = (B.timestamp - (S.start + S.offset)) / ABS (S.rate) + S.base
  else
    B.running_time = ((S.stop - S.offset) - B.timestamp) / ABS (S.rate) + S.base

We write B.running_time as the running_time obtained from the SEGMENT event and
the buffers of that segment.

The first displayable buffer will yield a value of 0 (since B.timestamp ==
        S.start and S.offset and S.base == 0).

For S.rate > 1.0, the timestamps will be scaled down to increase the playback
rate. Likewise, a rate between 0.0 and 1.0 will slow down playback.

For negative rates, timestamps are received stop S.stop to S.start so that the
first buffer received will be transformed into B.running_time of 0 (B.timestamp
        == S.stop and S.base == 0).

This makes it so that B.running_time is always monotonically increasing starting
from 0 with both positive and negative rates.


Synchronisation
~~~~~~~~~~~~~~~

As we have seen, we can get a running_time:

 - using the clock and the element's base_time with:

    C.running_time = absolute_time - base_time

 - using the buffer timestamp and the preceding SEGMENT event as (assuming
   positive playback rate):

    B.running_time = (B.timestamp - (S.start + S.offset)) / ABS (S.rate) + S.base

We prefix C. and B. before the two running times to note how they were
calculated.

The task of synchronized playback is to make sure that we play a buffer with
B.running_time at the moment when the clock reaches the same C.running_time.

Thus the following must hold:

   B.running_time = C.running_time

expaning:

   B.running_time = absolute_time - base_time

or:

   absolute_time = B.running_time + base_time

The absolute_time when a buffer with B.running_time should be played is noted
with B.sync_time. Thus:

  B.sync_time = B.running_time + base_time

One then waits for the clock to reach B.sync_time 'before' rendering the buffer
in the sink (See also part-clocks.txt).

For multiple streams this means that buffers with the same running_time are to
be displayed at the same time. 

note:
A demuxer must make sure that the SEGMENT it emits on its output pads yield the
same running_time for buffers that should be played synchronized. This usually
means sending the same SEGMENT on all pads and making sure that the synchronized
buffers have the same timestamps.


Stream time
~~~~~~~~~~~
The stream time is also known as the position in the stream and is a value
between 0 and the total duration of the media file.

It is the stream time that is used for:

  - report the POSITION query in the pipeline
  - the position used in seek events/queries
  - the position used to synchronize controller values

Additional fields in the SEGMENT are used:

  - S.time: time field in the SEGMENT event. This the stream-time of S.start
  - S.applied_rate: The rate already applied to the stream.

Stream time is calculated using the buffer times and the preceding SEGMENT event
as follows:

    stream_time = (B.timestamp - S.start) * ABS (S.applied_rate) + S.time
 
For negative rates, B.timestamp will go backwards from S.stop to S.start, making
the stream time go backwards.

In the PLAYING state, it is also possible to use the pipeline clock to derive
the current stream_time.

Give the two formulas above to match the clock times with buffer timestamps
allows us to rewrite the above formula for stream_time (and for positive rates).

    C.running_time = absolute_time - base_time
    B.running_time = (B.timestamp - (S.start + S.offset)) / ABS (S.rate) + S.base

  =>
    (B.timestamp - (S.start + S.offset)) / ABS (S.rate) + S.base = absolute_time
    - base_time;

  =>
    (B.timestamp - (S.start + S.offset)) / ABS (S.rate) = absolute_time -
    base_time - S.base;

  =>
    (B.timestamp - (S.start + S.offset)) = (absolute_time - base_time - S.base)
    * ABS (S.rate)

  =>
    (B.timestamp - S.start) = S.offset + (absolute_time - base_time - S.base) *
    ABS (S.rate)

  filling (B.timestamp - S.start) in the above formule for stream time

  =>
    stream_time = (S.offset + (absolute_time - base_time - S.base) * ABS
            (S.rate)) * ABS (S.applied_rate) + S.time

This last formula is typically used in sinks to report the current position in
an accurate and efficient way.

Note that the stream time is never used for synchronisation against the clock.


={============================================================================
*kt_dev_bcast_400* gst: gst time and mpeg time

/**
 * GstSegment:
 * @flags: flags for this segment
 * @rate: the rate of the segment
 * @applied_rate: the already applied rate to the segment
 * @format: the format of the segment values
 * @base: the base of the segment
 * @offset: the offset to apply to @start or @stop
 * @start: the start of the segment
 * @stop: the stop of the segment
 * @time: the stream time of the segment
 * @position: the position in the segment
 * @duration: the duration of the segment
 *
 * A helper structure that holds the configured region of
 * interest in a media file.
 */
struct _GstSegment {
  /*< public >*/
  GstSegmentFlags flags;

  gdouble         rate;
  gdouble         applied_rate;

  GstFormat       format;
  guint64         base;
  guint64         offset;
  guint64         start;
  guint64         stop;
  guint64         time;

  guint64         position;
  guint64         duration;

  /* < private > */
  gpointer        _gst_reserved[GST_PADDING];
};

GST_DEBUG_OBJECT (sink, 
        "Segment event: %" GST_TIME_FORMAT " -> %" GST_TIME_FORMAT, 
        GST_TIME_ARGS(segment->start),GST_TIME_ARGS(segment->stop));

:gst_nexus_sink_event_locked: KT: Segment start: 32654987520000000
:gst_nexus_sink_event_locked:<nexussink1> Segment event: 9070:49:47.520000000 -> 99:99:99.999999999


<gst-time-vs-pts-time>
32654987520000000/90K = 362833194666 sec

>>> 362833194666/(60*60)
100786998 hours?

what happened? The reason is gst uses 'nanosecond' so usage of the GST_SECOND
or GST_MSECOND macros is highly recommended.

>>> 32654987520000000/(1000000000)
32654987
>>> 32654987/(60*60)
9070


/**
 * GST_SECOND:
 *
 * Constant that defines one GStreamer second.
 *
 * Value: 1000000000
 * Type: GstClockTime
 */
#define GST_SECOND  (G_USEC_PER_SEC * G_GINT64_CONSTANT (1000))


/* microseconds per second */
typedef struct _GTimer		GTimer;

#define G_USEC_PER_SEC 1000000

/**
 * GstClockTime:
 *
 * A datatype to hold a time, measured in nanoseconds.
 */
typedef guint64 GstClockTime;


/* timestamp debugging macros */
/**
 * GST_TIME_FORMAT:
 *
 * A string that can be used in printf-like format strings to display a
 * #GstClockTime value in h:m:s format.  Use GST_TIME_ARGS() to construct
 * the matching arguments.
 *
 * Example:
 * |[
 * printf("%" GST_TIME_FORMAT "\n", GST_TIME_ARGS(ts));
 * ]|
 */
#define GST_TIME_FORMAT "u:%02u:%02u.%09u"
/**
 * GST_TIME_ARGS:
 * @t: a #GstClockTime
 *
 * Format @t for the #GST_TIME_FORMAT format string. Note: @t will be
 * evaluated more than once.
 */
#define GST_TIME_ARGS(t) \
        GST_CLOCK_TIME_IS_VALID (t) ? \
        (guint) (((GstClockTime)(t)) / (GST_SECOND * 60 * 60)) : 99, \
        GST_CLOCK_TIME_IS_VALID (t) ? \
        (guint) ((((GstClockTime)(t)) / (GST_SECOND * 60)) % 60) : 99, \
        GST_CLOCK_TIME_IS_VALID (t) ? \
        (guint) ((((GstClockTime)(t)) / GST_SECOND) % 60) : 99, \
        GST_CLOCK_TIME_IS_VALID (t) ? \
        (guint) (((GstClockTime)(t)) % GST_SECOND) : 999999999


<gst-buffer>
GST_BUFFER_PTS(stream->buf);

/**
 * GstBuffer:
 * @mini_object: the parent structure
 * @pool: pointer to the pool owner of the buffer
 * @pts: presentation timestamp of the buffer, can be #GST_CLOCK_TIME_NONE when the
 *     pts is not known or relevant. The pts contains the timestamp when the
 *     media should be presented to the user.
 * @dts: decoding timestamp of the buffer, can be #GST_CLOCK_TIME_NONE when the
 *     dts is not known or relevant. The dts contains the timestamp when the
 *     media should be processed.
 * @duration: duration in time of the buffer data, can be #GST_CLOCK_TIME_NONE
 *     when the duration is not known or relevant.
 * @offset: a media specific offset for the buffer data.
 *     For video frames, this is the frame number of this buffer.
 *     For audio samples, this is the offset of the first sample in this buffer.
 *     For file data or compressed data this is the byte offset of the first
 *       byte in this buffer.
 * @offset_end: the last offset contained in this buffer. It has the same
 *     format as @offset.
 *
 * The structure of a #GstBuffer. Use the associated macros to access the public
 * variables.
 */
struct _GstBuffer {
  GstMiniObject          mini_object;

  /*< public >*/ /* with COW */
  GstBufferPool         *pool;

  /* timestamp */
  GstClockTime           pts;
  GstClockTime           dts;
  GstClockTime           duration;

  /* media specific offset */
  guint64                offset;
  guint64                offset_end;
};


={============================================================================
*kt_dev_bcast_400* gnome: glib

Basics of GObject
(http://library.gnome.org/devel/gobject/stable/) and glib
(http://library.gnome.org/devel/glib/stable/) programming.

For more information about GObject properties we recommend you read the GObject manual
(http://developer.gnome.org/gobject/stable/rn01.html) and an introduction to The Glib Object system
(http://developer.gnome.org/gobject/stable/pt01.html).

https://developer.gnome.org/glib/unstable/index.html


={============================================================================
*kt_dev_bcast_400* gst: log lines

{pipeline}
INFO            GST_PIPELINE gstparse.c:323:gst_parse_launch_full: 
  parsing pipeline description 'souphttpsrc
  location=http://test-media.youview.co.uk/ondemand/bbb/avc3/1/2drm_manifest.mpd
  ! dashdemux name=dash dash. ! video/quicktime ! qtdemux ! queue ! cencdec
  sas-url=https://ms3.youview.co.uk/s/Big+Buck+Bunny+DASH+2#http://test-media.youview.co.uk/ondemand/bbb/avc3/1/2drm_manifest.mpd
  ! h264parse ! video/x-h264,stream-format=byte-stream ! nexussink dash. !
  audio/x-m4a ! qtdemux ! cencdec ! aacparse ! nexussink '


{loading}
DEBUG GST_PLUGIN_LOADING gstplugin.c:709:gst_plugin_load_file: 
  attempt to load plugin "/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstcenccrypto.so"

INFO  GST_PLUGIN_LOADING gstplugin.c:835:gst_plugin_load_file: 
  plugin "/opt/zinc-trunk/oss/lib/gstreamer-1.0/libgstdashdemux.so" loaded


{caps}
DEBUG GST_CAPS gstpad.c:2114:gst_pad_link_check_compatible_unlocked:<nexussink1:sink> sink caps video/mpegts, systemstream=(boolean)true; video/x-h264, stream-format=(string)byte-stream, alignment=(string){ au }; video/mpeg, mpegversion=(int){ 2, 4 }, systemstream=(boolean)false; audio/mpeg, mpegversion=(int)1, layer=(int){ 1, 3 }; audio/mpeg, mpegversion=(int)4, stream-format=(string){ adts, loas }


={============================================================================
*kt_dev_bcast_500* gst-object

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstObject.html

GstObject — Base class for the GStreamer object hierarchy

Object Hierarchy

    GObject
    ╰── GInitiallyUnowned
        ╰── GstObject
            ├── GstAllocator
            ├── GstPad
            ├── GstPadTemplate
            ├── GstPluginFeature
            ├── GstElement
            ├── GstBus
            ├── GstTask
            ├── GstTaskPool
            ├── GstClock
            ├── GstControlBinding
            ├── GstControlSource
            ├── GstPlugin
            ├── GstRegistry
            ╰── GstBufferPool

Description

GstObject provides a root for the object hierarchy tree filed in by the
GStreamer library. It is currently a thin wrapper on top of GInitiallyUnowned.
It is an abstract class that is not very usable on its own.

GstObject gives us basic refcounting, parenting functionality and locking. Most
of the functions are just extended for special GStreamer needs and can be found
under the same name in the base class of GstObject which is GObject (e.g.
    g_object_ref() becomes gst_object_ref()).

Since GstObject derives from GInitiallyUnowned, it also inherits the floating
reference. Be aware that functions such as gst_bin_add() and
gst_element_add_pad() take ownership of the floating reference.

In contrast to GObject instances, GstObject adds a name property. The functions
gst_object_set_name() and gst_object_get_name() are used to set/get the name of
the object.


#define GST_TYPE_CHILD_PROXY               (gst_child_proxy_get_type ())


/**
 * G_TYPE_CHECK_INSTANCE_TYPE:
 * @instance: Location of a #GTypeInstance structure.
 * @g_type: The type to be checked
 * 
 * Checks if @instance is an instance of the type identified by @g_type.
 * 
 * This macro should only be used in type implementations.
 *
 * Returns: %TRUE on success. note: return true.
 */
#define G_TYPE_CHECK_INSTANCE_TYPE(instance, g_type)
  (_G_TYPE_CIT ((instance), (g_type)))


struct GTypeInfo {
  /* interface types, classed types, instantiated types */
  guint16                class_size;
  
  GBaseInitFunc          base_init;
  GBaseFinalizeFunc      base_finalize;
  
  /* interface types, classed types, instantiated types */
  GClassInitFunc         class_init;
  GClassFinalizeFunc     class_finalize;
  gconstpointer          class_data;
  
  /* instantiated types */
  guint16                instance_size;
  guint16                n_preallocs;
  GInstanceInitFunc      instance_init;
  
  /* value handling */
  const GTypeValueTable *value_table;
};


={============================================================================
*kt_dev_bcast_500* gst-childproxy

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstChildProxy.html

GstChildProxy — Interface for multi child elements.

Object Hierarchy

    GInterface
    ╰── GstChildProxy


This interface 'abstracts' handling of 'property' sets for elements with
children. Imagine elements such as mixers or polyphonic generators. They all
have multiple GstPad or some kind of voice objects. 

Another use case are 'container' elements like GstBin. The element implementing
the interface acts as a parent for those child objects.

By implementing this interface the child properties can be accessed 'from' the
'parent' element by using gst_child_proxy_get() and gst_child_proxy_set().

Property names are written as "child-name::property-name". The whole naming
scheme is recursive. Thus "child1::child2::property" is valid too, if "child1"
and "child2" implement the GstChildProxy interface.


The "child-added" signal

void
user_function (GstChildProxy *child_proxy,
               GObject       *object,
               gchar         *name,
               gpointer       user_data)

Will be emitted after the object was added to the child_proxy .

Parameters

child_proxy     the GstChildProxy
object          the GObject that was added
name            the name of the new child
user_data       user data set when the signal handler was connected.


#define GST_IS_CHILD_PROXY(obj)            
  (G_TYPE_CHECK_INSTANCE_TYPE ((obj), GST_TYPE_CHILD_PROXY))


<ex>
So calls this callback recursively to childs in a container.

g_signal_connect(elem.get(), "child-added",
  G_CALLBACK(GstMediaRouter::childAddedCallback),
  const_cast<std::string::value_type*>(userAgent.c_str()));

void Gst::childAddedCallback(
    GstChildProxy* /*proxy*/, GObject* obj, gchar* /*name*/, gpointer data)
{
  if (g_object_class_find_property(G_OBJECT_GET_CLASS(obj), "user-agent")) {
    g_object_set(obj, "user-agent", data, NULL);
  } else if (GST_IS_CHILD_PROXY(obj)) {
    g_signal_connect(obj, "child-added",
        G_CALLBACK(childAddedCallback), data);
  }
}


={============================================================================
*kt_dev_bcast_500* gst-bin

GstBin — Base class and element that can contain other elements

Object Hierarchy

  GObject
   +----GstObject
         +----GstElement
               +----GstBin *
                     +----GstPipeline

Description

GstBin is an element that can contain other GstElement, allowing them to be
managed as a group. Pads from the child elements can be 'ghosted' to the bin,
see GstGhostPad. This makes the bin look like any other elements and enables
  creation of higher-level abstraction elements.

A new GstBin is created with gst_bin_new(). Use a GstPipeline instead if you
want to create a toplevel bin because a 'normal' bin doesn't have a bus or
handle clock distribution of its own.

After the bin has been created you will typically add elements to it with
gst_bin_add(). You can remove elements with gst_bin_remove().

An element can be retrieved from a bin with gst_bin_get_by_name(), using the
elements name. gst_bin_get_by_name_recurse_up() is mainly used for internal
purposes and will query the parent bins when the element is not found in the
current bin.

An iterator of elements in a bin can be retrieved with
gst_bin_iterate_elements(). Various other iterators exist to retrieve the
elements in a bin.

gst_object_unref() is used to drop your reference to the bin.

The "element-added" signal is fired whenever a new element is added to the bin.
Likewise the "element-removed" signal is fired whenever an element is removed
from the bin. 


={============================================================================
*kt_dev_bcast_500* gst-cap

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstCaps.html

g_object_set(uridecodebin.transferNone(), "caps",
    gst_caps_from_string(
        "application/dash+xml; "
        "application/sdp; "  
        "video/mpegts; "
        "application/hls; "
        "video/x-matroska; "
        "video/quicktime"
        ), NULL);


void
gst_caps_unref (GstCaps *caps);

Unref a GstCaps and and 'free' all its structures and the structures' values
  when the refcount reaches 0.


GstCaps *
gst_caps_from_string (const gchar *string);

Converts caps from a string representation.

The current implementation of serialization will lead to unexpected results when
there are nested GstCaps / GstStructure deeper than one level.

string
a string to convert to GstCaps

Returns
a newly 'allocated' GstCaps. 


={============================================================================
*kt_dev_bcast_500* gst: gstbasesink and preroll

{preroll}

// from gnome mail archive
In GStreamer terminology this describes the process of preparing a GStreamer
pipeline for playback/recording. What happens is that audio/video data starts
flowing through the pipeline until all elements have data (so that you could
        start actually playing/outputting the video/audio immediately).

note: when see gst log, preroll happens in the early part of playback.

* GST_STATE_PAUSED
an element has opened the stream, but is 'not' actively 'processing' it. An
element is allowed to modify a stream's position, read and process data and
such to 'prepare' for playback as soon as state is changed to PLAYING, but it
is not allowed to play the data which would make the clock run. In summary,
   PAUSED is the same as PLAYING but 'without' a running clock.  

* GST_STATE_PAUSED 
is the state in which an element is ready to accept and handle data. For most
elements this state is the same as PLAYING. The only exception to this rule are
sink elements. Sink elements only accept one single buffer of data and then
block. At this point the pipeline is 'prerolled' and ready to render data
immediately.


http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer-libs/html/GstBaseSink.html

GstBaseSink is the base class for sink elements in GStreamer, such as
xvimagesink or filesink. It is a layer on top of GstElement that provides a
simplified interface to plugin writers. GstBaseSink handles many details for
you, for example: preroll, clock 'synchronization', state changes, activation in
push or pull mode, and queries.

In most cases, when writing sink elements, there is no need to implement class
methods from GstElement or to set functions on pads, because the GstBaseSink
infrastructure should be sufficient.

GstBaseSink provides support for exactly one sink pad, which should be named
"sink". A sink implementation (subclass of GstBaseSink) should install a pad
template in its class_init function, like so:

note: preroll
GstBaseSink will handle the prerolling correctly. This means that it will return
GST_STATE_CHANGE_ASYNC from a state change to PAUSED until the first buffer
arrives in this element. The base class will call the GstBaseSinkClass.preroll()
vmethod with this preroll buffer and will then commit the state change to the
next asynchronously pending state.

note: get_times
When the element is set to PLAYING, GstBaseSink will synchronise on the clock
'using' the times returned from GstBaseSinkClass.get_times(). If this function
returns GST_CLOCK_TIME_NONE for the start time, no synchronisation will be done.
Synchronisation can be disabled entirely by setting the object “sync” property
to FALSE.

'after' synchronisation the virtual method GstBaseSinkClass.render() will be
called. Subclasses should minimally implement this method.

Subclasses that synchronise on the clock in the GstBaseSinkClass.'render'()
method are supported as well. These classes typically receive a 'buffer' in the
render method and can then potentially 'block' on the clock while rendering. A
typical example is an audiosink. These subclasses can use
gst_base_sink_wait_preroll() to perform the blocking wait.

Upon receiving the EOS event in the PLAYING state, GstBaseSink will wait for the
clock to reach the time indicated by the stop time of the last
GstBaseSinkClass.get_times() call before posting an EOS message. When the
element receives EOS in PAUSED, preroll completes, the event is queued and an
EOS message is posted when going to PLAYING.

GstBaseSink will internally use the GST_EVENT_SEGMENT events to schedule
synchronisation and clipping of buffers. Buffers that fall completely outside of
the current segment are dropped. Buffers that fall partially in the segment are
rendered (and prerolled). Subclasses should do any subbuffer clipping themselves
when needed.

GstBaseSink will by default report the current playback 'position' in
GST_FORMAT_TIME based on the current clock time and segment information. If no
clock has been set on the element, the query will be forwarded upstream.

The GstBaseSinkClass.set_caps() function will be called when the subclass should
configure itself to process a specific media type.

The GstBaseSinkClass.start() and GstBaseSinkClass.stop() virtual methods will be
called when resources should be allocated. Any GstBaseSinkClass.preroll(),
GstBaseSinkClass.render() and GstBaseSinkClass.set_caps() function will be
    called between the GstBaseSinkClass.start() and GstBaseSinkClass.stop()
    calls.

The GstBaseSinkClass.event() virtual method will be called when an event is
received by GstBaseSink. Normally this method should only be overridden by very
specific elements (such as file sinks) which need to handle the newsegment event
specially.

The GstBaseSinkClass.unlock() method is called when the elements should unblock
any blocking operations they perform in the GstBaseSinkClass.render() method.
This is mostly useful when the GstBaseSinkClass.render() method performs a
blocking write on a file descriptor, for example.

The “max-lateness” property affects how the sink deals with buffers that arrive
too late in the sink. A buffer arrives too late in the sink when the
presentation time (as a combination of the last segment, buffer timestamp and
        element base_time) plus the duration is before the current time of the
clock. If the frame is later than max-lateness, the sink will drop the buffer
'without' calling the render method. 

This feature is disabled if sync is disabled, the GstBaseSinkClass.get_times()
method does not return a valid start time or max-lateness is set to -1 (the
        default). Subclasses can use gst_base_sink_set_max_lateness() to
configure the max-lateness value.

The “qos” property will enable the quality-of-service features of the basesink
which gather statistics about the real-time performance of the clock
synchronisation. For each buffer received in the sink, statistics are gathered
and a QOS event is sent upstream with these numbers. This information can then
be used by upstream elements to reduce their processing rate, for example.

The “async” property can be used to instruct the sink to never perform an ASYNC
state change. This feature is mostly usable when dealing with non-synchronized
streams or sparse streams.


gst_base_sink_wait_preroll ()
--------------------------
GstFlowReturn
gst_base_sink_wait_preroll (GstBaseSink *sink);

note: own synchronisation

If the GstBaseSinkClass.render() method performs its own synchronisation against
the clock it must unblock when going from PLAYING to the PAUSED state and call
this method before continuing to render the remaining data.

This function will block 'until' a state change to PLAYING happens (in which
    case this function returns GST_FLOW_OK) or the processing must be stopped
due to a state change to READY or a FLUSH event (in which case this function
    returns GST_FLOW_FLUSHING).

This function should only be called with the PREROLL_LOCK held, like in the
render function.

Parameters
sink
the sink
	 
Returns
GST_FLOW_OK if the preroll completed and processing can continue. Any other
return value should be returned from the render vmethod.

Used in sink element code:
rv = gst_base_sink_wait_preroll (GST_BASE_SINK(sink));


gst_base_sink_set_sync ()
--------------------------
void gst_base_sink_set_sync (GstBaseSink *sink, gboolean sync);

Configures sink to synchronize on the clock or not. When sync is FALSE, incoming
samples will be played as fast as possible. If sync is TRUE, the timestamps of
the incoming buffers will be used to schedule the exact render time of its
contents.  

Parameters
sink 
the sink

sync
the new sync value.

note:
If sync is FALSE, use "own" synchronisation?


GstElementClass 
---------------
struct GstBaseSinkClass {
  GstElementClass parent_class;

  /* get caps from subclass */
  GstCaps*      (*get_caps)     (GstBaseSink *sink, GstCaps *filter);
  /* notify subclass of new caps */
  gboolean      (*set_caps)     (GstBaseSink *sink, GstCaps *caps);

  /* fixate sink caps during pull-mode negotiation */
  GstCaps *     (*fixate)       (GstBaseSink *sink, GstCaps *caps);
  /* start or stop a pulling thread */
  gboolean      (*activate_pull)(GstBaseSink *sink, gboolean active);

  /* get the start and end times for syncing on this buffer */
  void          (*get_times)    (GstBaseSink *sink, GstBuffer *buffer,
                                 GstClockTime *start, GstClockTime *end);

  /* propose allocation parameters for upstream */
  gboolean      (*propose_allocation)   (GstBaseSink *sink, GstQuery *query);

  /* start and stop processing, ideal for opening/closing the resource */
  gboolean      (*start)        (GstBaseSink *sink);
  gboolean      (*stop)         (GstBaseSink *sink);

  /* unlock any pending access to the resource. subclasses should unlock
   * any function ASAP. */
  gboolean      (*unlock)       (GstBaseSink *sink);
  /* Clear a previously indicated unlock request not that unlocking is
   * complete. Sub-classes should clear any command queue or indicator they
   * set during unlock */
  gboolean      (*unlock_stop)  (GstBaseSink *sink);

  /* notify subclass of query */
  gboolean      (*query)        (GstBaseSink *sink, GstQuery *query);

  /* notify subclass of event */
  gboolean      (*event)        (GstBaseSink *sink, GstEvent *event);

  /* wait for eos or gap, subclasses should chain up to parent first */
  GstFlowReturn (*wait_event)   (GstBaseSink *sink, GstEvent *event);

  /* notify subclass of buffer or list before doing sync */
  GstFlowReturn (*prepare)      (GstBaseSink *sink, GstBuffer *buffer);
  GstFlowReturn (*prepare_list) (GstBaseSink *sink, GstBufferList *buffer_list);

  /* notify subclass of preroll buffer or real buffer */
  GstFlowReturn (*preroll)      (GstBaseSink *sink, GstBuffer *buffer);
  GstFlowReturn (*render)       (GstBaseSink *sink, GstBuffer *buffer);
  /* Render a BufferList */
  GstFlowReturn (*render_list)  (GstBaseSink *sink, GstBufferList *buffer_list);
};


parent_class; 
Element parent class
	 
get_caps () 
Called to get sink pad caps from the subclass
	 
set_caps () 
Notify subclass of changed caps
	 
fixate () 
Only useful in pull mode. Implement if you have ideas about what should be the
default values for the caps you support.
	 
activate_pull () 
Subclasses should override this when they can provide an alternate method of
spawning a thread to drive the pipeline in pull mode. Should start or stop the
pulling thread, depending on the value of the "active" argument. Called after
actually activating the sink pad in pull mode. The default implementation
starts a task on the sink pad.
	 
get_times ()
Called to get the start and end times for synchronising the passed buffer to
the clock

event ()
Override this to handle events arriving on the sink pad

note: Q: return TRUE or FALSE mean? Looks like return TRUE means it used up?


{time-log}
// for video, nexussink0 and for audio nexussink1

0:00:46.502164232  2134   0x4a0230 DEBUG               basesink gstbasesink.c:3347:gst_base_sink_chain_unlocked:<nexussink0> got times start: 0:00:49.880000000, end: 0:00:49.890000000
0:00:46.502503454  2134   0x4a0230 DEBUG               basesink gstbasesink.c:1906:gst_base_sink_get_sync_times:<nexussink0> got times start: 0:00:49.880000000, stop: 0:00:49.890000000, do_sync 1

0:00:46.504891528  2134   0x4a0230 DEBUG               basesink gstbasesink.c:2479:gst_base_sink_do_sync:<nexussink0> reset rc_time to time 0:00:49.720000000

0:00:46.503935084  2134   0x5b8690 DEBUG               basesink gstbasesink.c:3347:gst_base_sink_chain_unlocked:<nexussink1> got times start: 0:00:45.354666666, end: 2962:58:31.854666666
0:00:46.505535677  2134   0x5b8690 DEBUG               basesink gstbasesink.c:1906:gst_base_sink_get_sync_times:<nexussink1> got times start: 0:00:45.354666666, stop: 2962:58:31.854666666, do_sync 1

0:00:46.505868676  2134   0x5b8690 DEBUG               basesink gstbasesink.c:2479:gst_base_sink_do_sync:<nexussink1> reset rc_time to time 0:00:45.194666666

0:00:46.506050492  2134   0x5b8690 DEBUG               basesink gstbasesink.c:2491:gst_base_sink_do_sync:<nexussink1> possibly waiting for clock to reach 0:00:45.354666666, adjusted 0:00:45.194666666

// note: debug line time is not in order.
0:00:46.505080269  2134   0x4a0230 DEBUG               basesink gstbasesink.c:2491:gst_base_sink_do_sync:<nexussink0> possibly waiting for clock to reach 0:00:49.880000000, adjusted 0:00:49.720000000

0:00:46.507354232  2134   0x4a0230 DEBUG               basesink gstbasesink.c:3429:gst_base_sink_chain_unlocked:<nexussink0> rendering object 0x5f9c90
0:00:46.508270343  2134   0x4a0230 DEBUG                default /concurrent_pes_writer.c:478:pes_data_source_produce_data: => 
KT: q:0x45ee60: venq: was_empty: 1, tb(13730), dts: 0:00:49.880000000 dts-diff: 0:00:00.040000000 dts-es-diff: 0:00:04.546666667 dts-abs: 0:00:00.251699371


<clear-stream-that-works>
// video
DEBUG basesink gstbasesink.c:3347:gst_base_sink_chain_unlocked:<nexussink0> got times start: 9074:24:57.720000000, end: 9074:24:57.730000000
DEBUG basesink gstbasesink.c:1906:gst_base_sink_get_sync_times:<nexussink0> got times start: 9074:24:57.720000000, stop: 9074:24:57.730000000, do_sync 1

// audio
DEBUG basesink gstbasesink.c:3347:gst_base_sink_chain_unlocked:<nexussink1> got times start: 9074:24:57.664000000, end: 99:99:99.999999999
DEBUG basesink gstbasesink.c:1906:gst_base_sink_get_sync_times:<nexussink1> got times start: 9074:24:57.664000000, stop: 99:99:99.999999999, do_sync 1


../../../../gtags-target/Zinc.3rdPartyStack/gstreamer-1.5.0/libs/gst/base/gstbasesink.c

// gets time from buf
  GST_DEBUG_OBJECT (basesink, "got times start: %" GST_TIME_FORMAT
      ", stop: %" GST_TIME_FORMAT ", do_sync %d", GST_TIME_ARGS (start),
      GST_TIME_ARGS (stop), *do_sync);


DEBUG              GST_EVENT gstpad.c:5242:gst_pad_send_event_unchecked:<nexussink0:sink> have event type segment event: 0x5a51c8, time 99:99:99.999999999, seq-num 114, GstEventSegment, segment=(GstSegment)"GstSegment, flags=(GstSegmentFlags)GST_SEGMENT_FLAG_NONE, rate=(double)1, applied-rate=(double)1, format=(GstFormat)GST_FORMAT_TIME, base=(guint64)0, offset=(guint64)0, start=(guint64)0, stop=(guint64)18446744073709551615, time=(guint64)0, position=(guint64)0, duration=(guint64)18446744073709551615;";

DEBUG               basesink gstbasesink.c:3175:gst_base_sink_event:<nexussink0> received event 0x5a51c8 segment event: 0x5a51c8, time 99:99:99.999999999, seq-num 114, GstEventSegment, segment=(GstSegment)"GstSegment, flags=(GstSegmentFlags)GST_SEGMENT_FLAG_NONE, rate=(double)1, applied-rate=(double)1, format=(GstFormat)GST_FORMAT_TIME, base=(guint64)0, offset=(guint64)0, start=(guint64)0, stop=(guint64)18446744073709551615, time=(guint64)0, position=(guint64)0, duration=(guint64)18446744073709551615;";

DEBUG              nexussink /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexussink.c:979:gst_nexus_sink_event_locked:<nexussink0> Event segment

DEBUG              nexussink /home/kpark/builds/_virtual_/humax.2100/DEVARCH/Polonium/Polonium.GSTNexus/src/gstnexussink.c:982:gst_nexus_sink_event_locked:<nexussink0> Segment event: 0:00:00.000000000 -> 99:99:99.999999999


={============================================================================
*kt_dev_bcast_500* gst: gstbuffer

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstBuffer.html

Buffers are the basic unit of data transfer in GStreamer. They contain the
timing and offset along with other arbitrary metadata that is associated with
the GstMemory blocks that the buffer contains.

Buffers are usually created with gst_buffer_new(). After a buffer has been
created one will typically allocate memory for it and add it to the buffer. The
following example creates a buffer that can hold a given video frame with a
given width, height and bits per plane.


GstBuffer *buffer;
GstMemory *memory;
gint size, width, height, bpp;
...
size = width * height * bpp;
buffer = gst_buffer_new ();
memory = gst_allocator_alloc (NULL, size, NULL);
gst_buffer_insert_memory (buffer, -1, memory);
...

Alternatively, use gst_buffer_new_allocate() to create a buffer with
preallocated data of a given size.

Buffers can contain a 'list' of GstMemory objects. You can retrieve how many
memory objects with gst_buffer_n_memory() and you can get a pointer to memory
with gst_buffer_peek_memory()

A buffer will usually have timestamps, and a duration, but neither of these are
'guaranteed' (they may be set to GST_CLOCK_TIME_NONE). Whenever a meaningful value
can be given for these, they should be set. The timestamps and duration are
measured in nanoseconds (they are GstClockTime values).

The buffer DTS refers to the timestamp when the buffer should be decoded and is
usually monotonically increasing. The buffer PTS refers to the timestamp when
the buffer content should be presented to the user and is not always
monotonically increasing.

A buffer can also have one or both of a start and an end offset. These are
media-type specific. For video buffers, the start offset will generally be the
frame number. For audio buffers, it will be the number of samples produced so
far. For compressed data, it could be the byte offset in a source or destination
file. Likewise, the end offset will be the offset of the end of the buffer.
These can only be meaningfully interpreted if you know the media type of the
buffer (the preceding CAPS event). Either or both can be set to
GST_BUFFER_OFFSET_NONE.

gst_buffer_ref() is used to 'increase' the refcount of a buffer. This must be
done when you want to keep a handle to the buffer after pushing it to the next
element. The buffer refcount determines the writability of the buffer, a buffer
is only writable when the refcount is exactly '1', i.e. when the caller has the
only reference to the buffer.

To efficiently create a smaller buffer out of an existing one, you can use
gst_buffer_copy_region(). This method tries to share the memory objects between
the two buffers.

If a plug-in wants to modify the buffer data or metadata in-place, it should
first obtain a buffer that is safe to modify by using
gst_buffer_make_writable(). This function is optimized so that a copy will only
be made when it is necessary.

Several flags of the buffer can be set and unset with the GST_BUFFER_FLAG_SET()
and GST_BUFFER_FLAG_UNSET() macros. Use GST_BUFFER_FLAG_IS_SET() to test if a
certain GstBufferFlags flag is set.

Buffers can be efficiently merged into a larger buffer with gst_buffer_append().
Copying of memory will only be done when absolutely needed.

Arbitrary extra metadata can be set on a buffer with gst_buffer_add_meta().
Metadata can be retrieved with gst_buffer_get_meta(). See also GstMeta

An element should either unref the buffer or push it out on a src pad using
gst_pad_push() (see GstPad).

Buffers are usually freed by unreffing them with gst_buffer_unref(). When the
refcount drops to 0, any memory and metadata pointed to by the buffer is
unreffed as well. Buffers allocated from a GstBufferPool will be returned to the
pool when the refcount drops to 0.

The GstParentBufferMeta is a meta which can be attached to a GstBuffer to hold a
reference to another buffer that is only released when the child GstBuffer is
released.

Typically, GstParentBufferMeta is used when the child buffer is directly using
the GstMemory of the parent buffer, and wants to prevent the parent buffer from
being returned to a buffer pool until the GstMemory is available for re-use.
(Since 1.6)

struct GstBuffer

struct GstBuffer {
  GstMiniObject          mini_object;

  GstBufferPool         *pool;

  /* timestamp */
  GstClockTime           pts;
  GstClockTime           dts;
  GstClockTime           duration;

  /* media specific offset */
  guint64                offset;
  guint64                offset_end;
};

The structure of a GstBuffer. Use the associated macros to access the public
variables.  

Members

GstMiniObject mini_object;
the parent structure
	 
GstBufferPool *pool;
pointer to the pool owner of the buffer
	 
GstClockTime pts;
presentation timestamp of the buffer, can be GST_CLOCK_TIME_NONE when the pts is
not known or relevant. The pts contains the timestamp when the media should be
presented to the user.
	 
GstClockTime dts;
decoding timestamp of the buffer, can be GST_CLOCK_TIME_NONE when the dts is not
known or relevant. The dts contains the timestamp when the media should be
processed.
	 
GstClockTime duration;
duration in time of the buffer data, can be GST_CLOCK_TIME_NONE when the
duration is not known or relevant.
	 
guint64 offset;
a media specific offset for the buffer data. For video frames, this is the frame
number of this buffer. For audio samples, this is the offset of the first sample
in this buffer. For file data or compressed data this is the byte offset of the
first byte in this buffer.
	 
guint64 offset_end;
the last offset contained in this buffer. It has the same format as offset.


={============================================================================
*kt_dev_bcast_500* gst: gstevent

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstEvent.html

enum GstEventType
-----------------
GstEventType lists the standard event types that can be sent in a pipeline.

The custom event types can be used for private messages between elements that
can't be expressed using normal GStreamer buffer passing semantics. Custom
events carry an arbitrary GstStructure. Specific custom events are distinguished
by the name of the structure.


GST_EVENT_EOS

End-Of-Stream. No more data is to be expected to follow without a SEGMENT event.


={============================================================================
*kt_dev_bcast_500* gst: gstvideo

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-base-libs/html/gst-plugins-base-libs-gstvideo.html

gstvideo — Support library for video operations

This library contains some helper functions and includes the videosink and
videofilter base classes. 


struct GstVideoFormatInfo
-------------------------

struct GstVideoFormatInfo {
  GstVideoFormat format;
  const gchar *name;
  const gchar *description;
  GstVideoFormatFlags flags;
  guint bits;
  guint n_components;
  guint shift[GST_VIDEO_MAX_COMPONENTS];
  guint depth[GST_VIDEO_MAX_COMPONENTS];
  gint  pixel_stride[GST_VIDEO_MAX_COMPONENTS];
  guint n_planes;
  guint plane[GST_VIDEO_MAX_COMPONENTS];
  guint poffset[GST_VIDEO_MAX_COMPONENTS];
  guint w_sub[GST_VIDEO_MAX_COMPONENTS];
  guint h_sub[GST_VIDEO_MAX_COMPONENTS];

  GstVideoFormat unpack_format;
  GstVideoFormatUnpack unpack_func;
  gint pack_lines;
  GstVideoFormatPack pack_func;

  GstVideoTileMode tile_mode;
  guint tile_ws;
  guint tile_hs;

  gpointer _gst_reserved[GST_PADDING];
};


struct GstVideoInfo
-------------------

struct GstVideoInfo {
  const GstVideoFormatInfo *finfo;

  GstVideoInterlaceMode     interlace_mode;
  GstVideoFlags             flags;
  gint                      width;
  gint                      height;
  gsize                     size;
  gint                      views;

  GstVideoChromaSite        chroma_site;
  GstVideoColorimetry       colorimetry;

  gint                      par_n;
  gint                      par_d;
  gint                      fps_n;
  gint                      fps_d;

  gsize                     offset[GST_VIDEO_MAX_PLANES];
  gint                      stride[GST_VIDEO_MAX_PLANES];

  /* Union preserves padded struct size for backwards compat
   * Consumer code should use the accessor macros for fields */
  union {
    struct {
      GstVideoMultiviewMode     multiview_mode;
      GstVideoMultiviewFlags    multiview_flags;
    } abi;
};

Information describing image properties. This information can be filled in
from GstCaps with gst_video_info_from_caps(). The information is also used to
store the specific video info when mapping a video frame with
gst_video_frame_map().

Use the provided macros to access the info in this structure.

Members

gint width; the width of the video
gint height; the height of the video
gint par_n; the pixel-aspect-ratio numerator
gint par_d; the pixel-aspect-ratio demnominator
gint stride[GST_VIDEO_MAX_PLANES]; strides of the planes


GST_VIDEO_INFO_WIDTH()
#define GST_VIDEO_INFO_WIDTH(i)          ((i)->width)


={============================================================================
*kt_dev_bcast_500* gst-element

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstElement.html

Object Hierarchy
    GObject
    ╰── GInitiallyUnowned
        ╰── GstObject
            ╰── GstElement
                ╰── GstBin

gst_element_post_message ()
-----------------
gboolean
gst_element_post_message (GstElement *element, GstMessage *message);

Post a message on the element's GstBus. This function takes ownership of the
  message; if you want to access the message after this call, you should add an
  additional reference before calling.  
  
Parameters

element
a GstElement 'posting' the message

message
a GstMessage to post.  [transfer full]

Returns

TRUE if the message was successfully posted. The function returns FALSE if the
element did not have a bus.

MT safe.

note:
It turns out we don't have to execute gst_element_post_message() on the main
thread, as all it does is posting a message on GstBus, which does asynchronous
delivery anyway (apart from gst_bus_set_sync_handler() that we don't use).

<ex>

  GstStructure * str = gst_structure_new(
          "video-changed",
          "height", G_TYPE_UINT, sink->height,
          "width", G_TYPE_UINT, sink->width,
          "aspect", GST_TYPE_FRACTION, sink->aspect_ratio_n, sink->aspect_ratio_d,
          NULL);

  GstMessage *msg = gst_message_new_element(GST_OBJECT_CAST(sink), str);

  if(!gst_element_post_message(GST_ELEMENT_CAST(sink), msg)) {
      GST_ERROR_OBJECT (sink, "Unable to post message." );
  }

<log-when-post-message>

DEBUG                GST_BUS gstbus.c:309:gst_bus_post:<bus3> 
[msg 0x4d39c8] posting on bus element message: 0x4d39c8, time 99:99:99.999999999, 
seq-num 210, element 'nexussink0', video-changed, height=(uint)288,
width=(uint)512, aspect=(fraction)1/1;

DEBUG                GST_BUS gstbus.c:784:gst_bus_source_dispatch:<bus3> 
source 0x500550 calling dispatch with element message: 0x500178, time
99:99:99.999999999, seq-num 402, element 'nexussink0', video-changed,
  height=(uint)1080, width=(uint)1920, aspect=(fraction)1/1;


#define GST_ELEMENT_CAST(obj)           ((GstElement*)(obj))


GST_ELEMENT_ERROR()
-----------------
#define             GST_ELEMENT_ERROR(el, domain, code, text, debug)

Utility function that elements can use in case they encountered a fatal data
processing error. The pipeline will 'post' an error message and the application
will be requested to stop further media processing.

note: uses gst_element_message_full() to post

GST_ELEMENT_ERROR(demux, 
    RESOURCE, FAILED, ("Download error"), ("http error %d", http_status));


={============================================================================
*kt_dev_bcast_500* gst-ghostpad

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstGhostPad.html

Object Hierarchy

    GObject
    ╰── GInitiallyUnowned
        ╰── GstObject
            ╰── GstPad
                ╰── GstProxyPad
                    ╰── GstGhostPad


Description

GhostPads are useful when organizing pipelines with GstBin like elements. The
idea here is to create hierarchical element graphs. The bin element contains a
sub-graph. Now one would like to treat the bin-element like any other
GstElement. This is where GhostPads come into play. A GhostPad acts as a proxy
for another pad. Thus the bin can have sink and source ghost-pads that are
  associated with sink and source pads of the child elements.

If the target pad is known at creation time, gst_ghost_pad_new() is the function
to use to get a ghost-pad. Otherwise one can use gst_ghost_pad_new_no_target()
to create the ghost-pad and use gst_ghost_pad_set_target() to establish the
association later on.

Note that GhostPads add overhead to the data processing of a pipeline.


gst_ghost_pad_new ()
-----------------
GstPad *
gst_ghost_pad_new (const gchar *name,
                   GstPad *target);

Create a new ghostpad with target as the target. The direction will be taken
  from the target pad. target must be unlinked.

Will ref the target.

Parameters

name
the name of the new pad, or NULL to assign a default name.

target
the pad to ghost.

Returns
a new GstPad, or NULL in case of an error. 


={============================================================================
*kt_dev_bcast_500* gst: gststructure

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstStructure.html

A GstStructure is a collection of key/value 'pairs'. The keys are expressed as
GQuarks and the values can be of any GType.

In addition to the key/value pairs, a GstStructure also has a name. The name
starts with a letter and can be filled by letters, numbers and any of "/-_.:".

GstStructure is used by various GStreamer subsystems to store information in a
flexible and extensible way. A GstStructure does 'not' have a refcount because
it usually is part of a higher level object such as GstCaps, GstMessage,
GstEvent, GstQuery. It provides a means to enforce mutability using the refcount
  of the parent with the gst_structure_set_parent_refcount() method.

A GstStructure can be created with gst_structure_new_empty() or
gst_structure_new(), which both take a name and an optional set of key/value
pairs along with the types of the values.

Field values can be changed with gst_structure_set_value() or
gst_structure_set().

Field values can be retrieved with gst_structure_get_value() or the more
convenient gst_structure_get_*() functions.

Fields can be removed with gst_structure_remove_field() or
gst_structure_remove_fields().

Strings in structures must be ASCII or UTF-8 encoded. Other encodings are not
allowed. Strings may be NULL however.

Be aware that the current GstCaps / GstStructure serialization into string has
limited support for nested GstCaps / GstStructure fields. It can only support
one level of nesting. Using more levels will lead to unexpected behavior when
using serialization features, such as gst_caps_to_string() or
gst_value_serialize() and their counterparts.


GstStructure * str = gst_structure_new(
    "video-changed",
    "height", G_TYPE_UINT, sink->height,
    "width", G_TYPE_UINT, sink->width,
    "aspect", GST_TYPE_FRACTION, sink->aspect_ratio_n, sink->aspect_ratio_d,
    NULL);

GstMessage *msg = gst_message_new_element(GST_OBJECT_CAST(sink), str);

if (!gst_element_post_message(GST_ELEMENT_CAST(sink), msg)) {
  GST_ERROR_OBJECT (sink, "Unable to post video-changed message." );
}


gst_structure_has_field_typed ()
-----------------
gboolean
gst_structure_has_field_typed (const GstStructure *structure,
                               const gchar *fieldname,
                               GType type);

Check if structure contains a field named fieldname and with GType type.

Parameters
structure a GstStructure
fieldname the name of a field
type the type of a value

Returns
TRUE if the structure contains a field with the given name and type


gst_structure_get_uint ()
-----------------
gboolean
gst_structure_get_uint (const GstStructure *structure,
                        const gchar *fieldname,
                        guint *value);

Sets the uint pointed to by value corresponding to the value of the given field.
Caller is responsible for making sure the field exists and has the correct type.

Parameters
structure a GstStructure
fieldname the name of a field
value a pointer to a uint to set. [out]

Returns
TRUE if the value could be set correctly. If there was no field with fieldname
or the existing field did not contain a uint, this function returns FALSE.

note:
Is there case when gst_structure_get_uint fails when
gst_structure_has_field_typed succeed?

if (gst_structure_has_field_typed(str,"height",G_TYPE_UINT)) {
  guint height = size.height();
  if(gst_structure_get_uint(str,"height",&height)){
    size.setHeight(height);
  }
}


gst_structure_new ()
-----------------

GstStructure *
gst_structure_new (const gchar *name,
                   const gchar *firstfield,
                   ...);

Creates a new GstStructure with the given name. Parses the list of variable
arguments and sets fields to the values listed. Variable arguments should be
passed as field name, field type, and value. Last variable argument should be
NULL.

Free-function: gst_structure_free

Parameters

name name of new structure
firstfield name of first field to set
...  additional arguments

Returns
a new GstStructure. 


={============================================================================
*kt_dev_bcast_500* gst-message

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstMessage.html

Messages are implemented as a subclass of GstMiniObject with a generic
GstStructure as the content. This allows for writing custom messages without
requiring an API change while allowing a wide range of different types of
messages.

Messages are posted by objects in the pipeline and are passed to the
application using the GstBus.

The basic use pattern of posting a message on a GstBus is as follows:

gst_bus_post (bus, gst_message_new_eos());

A GstElement usually posts messages on the bus provided by the parent
  container using gst_element_post_message().

note: message type is different from domain or code of error or warning in a
message.

-----------------
struct GstMessage {
  GstMiniObject   mini_object;

  GstMessageType  type;
  guint64         timestamp;
  GstObject      *src;
  guint32         seqnum;
};


-----------------
GST_MESSAGE_SRC()

#define GST_MESSAGE_SRC(message)        (GST_MESSAGE_CAST(message)->src)

Get the object that posted message .


-----------------
GST_MESSAGE_SRC_NAME()

#define             GST_MESSAGE_SRC_NAME(message)

Get the name of the object that posted message. Returns "(NULL)" if the message
has no source object set.


note: type name and object name

element = gst_element_factory_make("queue","uri-decoder");

GstMessage *msg =
  gst_message_new_error(GST_OBJECT_CAST (element.get()),
    g_error_new_literal(GST_RESOURCE_ERROR,
    GST_RESOURCE_ERROR_NOT_FOUND, ""),
    "(no debug)");

type name from msg  : GstQueue 
object name from msg: uri-decoder

G_OBJECT_TYPE_NAME(msg->src)
GST_MESSAGE_SRC_NAME(msg)

glib-2.40.0/gobject/gobject.h #define G_OBJECT_TYPE_NAME(object)  
  (g_type_name (G_OBJECT_TYPE (object)))

gstreamer-1.6.0/gst/gstobject.h #define GST_OBJECT_NAME(obj)      
  (GST_OBJECT_CAST(obj)->name)


gst_message_new_element ()
-----------------
GstMessage *
gst_message_new_element (GstObject *src, GstStructure *structure);

Create a new element-'specific' message. This is meant as a generic way of
  allowing one-way communication from an element to an application, for example
  "the firewire cable was unplugged". The format of the message should be
  documented in the element's documentation. The structure field can be NULL.

Parameters
src The object originating the message.  [transfer none][allow-none]

structure
The structure for the message. The message will take ownership of the structure.
[transfer full] 

Returns
The new element message.

MT safe. [transfer full]


gst_message_new_error ()
-----------------
GstMessage *
gst_message_new_error (GstObject *src,
                       GError *error,
                       const gchar *debug);

Create a new error message. The message will copy error and debug. This
  message is posted by element when a fatal event occurred. The pipeline will
  probably (partially) stop. The application receiving this message should
  stop the pipeline.

Parameters
src     The object 'originating' the message.  [transfer none][allow-none]
error   The GError for this message.  [transfer none]
debug   A debugging string.

Returns
the new error message.

MT safe. 


gst_message_parse_error ()
-----------------
void
gst_message_parse_error (GstMessage *message,
                         GError **gerror,
                         gchar **debug);

note: GError comes from glib-error

Extracts the GError and debug string from the GstMessage. The values returned
  in the output arguments are copies; the caller must free them when done.

...
switch (GST_MESSAGE_TYPE (msg)) {
  case GST_MESSAGE_ERROR: {
    GError *err = NULL;
    gchar *dbg_info = NULL;
    
    gst_message_parse_error (msg, &err, &dbg_info);
    g_printerr ("ERROR from element %s: %s\n",
        GST_OBJECT_NAME (msg->src), err->message);
    g_printerr ("Debugging info: %s\n", (dbg_info) ? dbg_info : "none");
    g_error_free (err);
    g_free (dbg_info);
    break;
  }
  ...
}
...

MT safe.

Parameters

message A valid GstMessage of type GST_MESSAGE_ERROR.
gerror location for the GError. [out][allow-none][transfer full]
debug location for the debug message, or NULL. [out][allow-none][transfer full]


gst_message_new_warning ()
-----------------
GstMessage *
gst_message_new_warning (GstObject *src,
                         GError *error,
                         const gchar *debug);

Create a new warning message. The message will make copies of error and debug.

Parameters

src The object originating the message. [transfer none][allow-none]
error The GError for this message. [transfer none]
debug A debugging string.

Returns
The new warning message.

MT safe. 


gst_message_parse_state_changed ()
-----------------
void
gst_message_parse_state_changed (GstMessage *message,
                                 GstState *oldstate,
                                 GstState *newstate,
                                 GstState *pending);

Extracts the old and new states from the GstMessage.

Typical usage of this function might be:

...
switch (GST_MESSAGE_TYPE (msg)) {
  case GST_MESSAGE_STATE_CHANGED: {
    GstState old_state, new_state;
    
    gst_message_parse_state_changed (msg, &old_state, &new_state, NULL);
    g_print ("Element %s changed state from %s to %s.\n",
        GST_OBJECT_NAME (msg->src),
        gst_element_state_get_name (old_state),
        gst_element_state_get_name (new_state));
    break;
  }
  ...
}
...

MT safe.

Parameters

message
a valid GstMessage of type GST_MESSAGE_STATE_CHANGED

oldstate
the previous state, or NULL.

newstate
the new (current) state, or NULL.

pending
the pending (target) state, or NULL.


={============================================================================
*kt_dev_bcast_500* gst: gstbus

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstBus.html

Object Hierarchy
    GObject
    ╰── GInitiallyUnowned
        ╰── GstObject
            ╰── GstBus


The GstBus is an object responsible for delivering GstMessage packets in a
first-in first-out way from the streaming threads (see GstTask) to the
'application'.

Since the application typically only wants to deal with delivery of these
messages from one thread, the GstBus will marshall the messages between
different threads. This is important since the actual streaming of media is done
in another thread than the application.

The GstBus provides support for GSource based notifications. This makes it
possible to handle the delivery in the glib mainloop.

The GSource callback function gst_bus_async_signal_func() can be used to convert
all bus messages into signal emissions.

A message is posted on the bus with the gst_bus_post() method. With the
gst_bus_peek() and gst_bus_pop() methods one can look at or retrieve a
previously posted message.

The bus can be polled with the gst_bus_poll() method. This methods blocks up to
the specified timeout value until one of the specified messages types is posted
on the bus. The application can then gst_bus_pop() the messages from the bus to
handle them. 

<the-second-way>
Alternatively the application can register an 'asynchronous' bus function using
gst_bus_add_watch_full() or gst_bus_add_watch(). This function will 'install' a
GSource in the default glib main loop and will deliver messages a short while
after they have been posted. Note that the main loop should be running for the
asynchronous callbacks.

<synchronous>
It is also possible to get messages from the bus without any thread marshalling
with the gst_bus_set_sync_handler() method. This makes it possible to react to a
message in the same thread that posted the message on the bus. This should only
be used if the application is able to deal with messages from different threads.

Every GstPipeline has one bus.

Note that a GstPipeline will set its bus into flushing state when changing from
READY to NULL state.


gst_bus_add_watch ()
-----------------
guint
gst_bus_add_watch (GstBus *bus,
                   GstBusFunc func,
                   gpointer user_data);

Adds a bus watch to the default main context with the default priority
  (G_PRIORITY_DEFAULT). It is also possible to use a non-default main context
  set up using g_main_context_push_thread_default() (before one had to create a
      bus watch source and attach it to the desired main context 'manually').

This function is used to 'receive' asynchronous 'messages' in the main loop.
There can only be a single bus watch per bus, you must remove it before you can
set a new one.

The bus watch will only work if a GLib main loop is being run.

The watch can be removed using gst_bus_remove_watch() or by returning FALSE from
func. If the watch was added to the default main context it is also possible to
remove the watch using g_source_remove().

[skip]

Parameters
bus 
a GstBus to create the watch for

func 
A function to call when a message is received.

user_data 
user data passed to func.

Returns
The event source id or 0 if bus already got an event source.

MT safe.


gst_bus_add_signal_watch ()
-----------------
void
gst_bus_add_signal_watch (GstBus *bus);

Adds a bus signal watch to the default main context with the default priority
  (G_PRIORITY_DEFAULT). It is also possible to use a non-default main context
  set up using g_main_context_push_thread_default() (before one had to create a
      bus watch source and attach it to the desired main context 'manually').

After calling this statement, the bus will emit the "message" signal for each
message posted on the bus.

This function may be called multiple times. To clean up, the caller is
responsible for calling gst_bus_remove_signal_watch() as many times as this
function is called.

MT safe.

Parameters
bus
a GstBus on which you want to receive the "message" signal


={============================================================================
*kt_dev_bcast_500* gst: gstutil

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/gstreamer-GstUtils.html

gst_util_uint64_scale_int ()
-----------------
guint64
gst_util_uint64_scale_int (guint64 val,
                           gint num,
                           gint denom);

Scale val by the rational number num / denom, avoiding overflows and underflows
  and without loss of precision. num must be non-negative and denom must be
  positive.  
  
Parameters
val guint64 (such as a GstClockTime) to scale.
num numerator of the scale factor.
denom denominator of the scale factor.

Returns
val * num / denom . In the case of an overflow, this function returns
G_MAXUINT64. If the result is not exactly representable as an integer, it is
truncated.


gst_util_uint64_scale_round ()
-----------------
guint64
gst_util_uint64_scale_round (guint64 val,
                             guint64 num,
                             guint64 denom);

Scale val by the rational number num / denom, avoiding overflows and
  underflows and without loss of precision.

This function can potentially be very slow if val and num are both greater
than G_MAXUINT32.

Parameters

val the number to scale
num the numerator of the scale ratio
denom the denominator of the scale ratio

Returns

val * num / denom. In the case of an overflow, this function returns
G_MAXUINT64. If the result is not exactly representable as an integer, it is
rounded to the nearest integer (half-way cases are rounded 'up'). See also
gst_util_uint64_scale(), gst_util_uint64_scale_ceil(),
  gst_util_uint64_scale_int(), gst_util_uint64_scale_int_round(),
  gst_util_uint64_scale_int_ceil().

note:
This function seems to be odd since it's round the value up and
gst_util_uint64_scale_ceil () do the same.

954 Returned video size: 512 x 216
955 Returned AR: 1269 / 1280

width = static_cast<guint>(16*
  floor(size.width*ratioNumerator/ratioDenominator/16));

val = 
  gst_util_uint64_scale_round(size.width, ratioNumerator, ratioDenominator * 16);

xxwidth = static_cast<guint>(16*
  gst_util_uint64_scale_round(size.width, ratioNumerator, ratioDenominator * 16));

954 Returned video size: 512 x 216
955 Returned AR: 1269 / 1280

// 496 comes from floor(), 31x16 = 496
967 Changed size: 496 x 216 

// 512 comes from gst_ 32x16 = 512
968 gst util width size: 512, val: 32
954 Returned video size: 640 x 272

>>> 507.0/16.0
31.6875

31 or 32


gst_util_uint64_scale ()
-----------------
guint64
gst_util_uint64_scale (guint64 val,
                       guint64 num,
                       guint64 denom);

Scale val by the rational number num / denom , avoiding overflows and underflows
  and without loss of precision.

This function can potentially be very slow if val and num are both greater than
G_MAXUINT32.  

Parameters
val the number to scale
num the numerator of the scale ratio
denom the denominator of the scale ratio

Returns
val * num / denom . In the case of an overflow, this function returns
G_MAXUINT64. If the result is not exactly representable as an integer it is
truncated


={============================================================================
*kt_dev_bcast_500* gst-error

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/gstreamer-GstGError.html

GStreamer elements can throw non-fatal warnings and fatal errors. Higher-level
elements and applications can programmatically filter the ones they are
interested in or can recover from, and have a default handler handle the rest of
them.

The rest of this section will use the term "error" to mean both (non-fatal)
warnings and (fatal) errors; they are treated similarly.

Errors from elements are the combination of a GError and a debug string. The
GError contains:

a 'domain' type: CORE, LIBRARY, RESOURCE or STREAM

a code: an enum value specific to the domain

a translated, human-readable message

a non-translated additional debug string, which also contains file and line
information

Elements do not have the context required to decide what to do with errors. As
such, they should only inform about errors, and stop their processing. In short,
an element doesn't know what it is being used for.

It is the application or compound element using the given element that has more
context about the use of the element. Errors can be received by listening to the
GstBus of the element/pipeline for GstMessage objects with the type
GST_MESSAGE_ERROR or GST_MESSAGE_WARNING. The thrown errors should be inspected,
and filtered if appropriate.

An application is expected to, by default, present the user with a dialog box
(or an equivalent) showing the error message. The dialog should also allow a way
to get at the additional debug information, so the user can provide bug
reporting information.

A compound element is expected to forward errors by default higher up the
hierarchy; this is done by default in the same way as for other types of
GstMessage.

When applications or compound elements trigger errors that they can recover
from, they can filter out these errors and take appropriate action. For example,
an application that gets an error from xvimagesink that indicates all XVideo
  ports are taken, the application can attempt to use another sink instead.


enum GstStreamError

Stream errors are for anything related to the stream being processed: format
errors, media type errors, ... They're typically used by decoders, demuxers,
converters, ...  Members

GST_STREAM_ERROR_WRONG_TYPE
used when the element doesn't handle this type of stream.

GST_STREAM_ERROR_CODEC_NOT_FOUND
used when there's no codec to handle the stream's type.

GST_STREAM_ERROR_DECODE
used when decoding fails.

GST_STREAM_ERROR_DEMUX
used when demuxing fails.

GST_STREAM_ERROR_MUX
used when muxing fails.

GST_STREAM_ERROR_FORMAT
used when the stream is of the wrong format (for example, wrong caps).

GST_STREAM_ERROR_DECRYPT
used when the stream is encrypted and can't be decrypted because this is not
supported by the element.

GST_STREAM_ERROR_DECRYPT_NOKEY
used when the stream is encrypted and can't be decrypted because no suitable key
is available.


note: the header is a bit different from the doc

gstreamer-1.5.0/gst/gsterror.h

/**
 * GstCoreError:
 * @GST_CORE_ERROR_FAILED: a general error which doesn't fit in any other
 * category.  Make sure you add a custom message to the error call.
 * @GST_CORE_ERROR_TOO_LAZY: do not use this except as a placeholder for
 * deciding where to go while developing code.
 * @GST_CORE_ERROR_NOT_IMPLEMENTED: use this when you do not want to implement
 * this functionality yet.
 * @GST_CORE_ERROR_STATE_CHANGE: used for state change errors.
 * @GST_CORE_ERROR_PAD: used for pad-related errors.
 * @GST_CORE_ERROR_THREAD: used for thread-related errors.
 * @GST_CORE_ERROR_NEGOTIATION: used for negotiation-related errors.
 * @GST_CORE_ERROR_EVENT: used for event-related errors.
 * @GST_CORE_ERROR_SEEK: used for seek-related errors.
 * @GST_CORE_ERROR_CAPS: used for caps-related errors.
 * @GST_CORE_ERROR_TAG: used for negotiation-related errors.
 * @GST_CORE_ERROR_MISSING_PLUGIN: used if a plugin is missing.
 * @GST_CORE_ERROR_CLOCK: used for clock related errors.
 * @GST_CORE_ERROR_DISABLED: used if functionality has been disabled at
 *                           compile time.
 * @GST_CORE_ERROR_NUM_ERRORS: the number of core error types.
 *
 * Core errors are errors inside the core GStreamer library.
 */
/* FIXME: should we divide in numerical blocks so we can easily add
          for example PAD errors later ? */
typedef enum
{
  GST_CORE_ERROR_FAILED = 1,
  GST_CORE_ERROR_TOO_LAZY,
  GST_CORE_ERROR_NOT_IMPLEMENTED,
  GST_CORE_ERROR_STATE_CHANGE,
  GST_CORE_ERROR_PAD,
  GST_CORE_ERROR_THREAD,
  GST_CORE_ERROR_NEGOTIATION,
  GST_CORE_ERROR_EVENT,
  GST_CORE_ERROR_SEEK,
  GST_CORE_ERROR_CAPS,
  GST_CORE_ERROR_TAG,
  GST_CORE_ERROR_MISSING_PLUGIN,
  GST_CORE_ERROR_CLOCK,
  GST_CORE_ERROR_DISABLED,
  GST_CORE_ERROR_NUM_ERRORS
} GstCoreError;


/**
 * GstResourceError:
 * @GST_RESOURCE_ERROR_FAILED: a general error which doesn't fit in any other
 * category.  Make sure you add a custom message to the error call.
 * @GST_RESOURCE_ERROR_TOO_LAZY: do not use this except as a placeholder for
 * deciding where to go while developing code.
 * @GST_RESOURCE_ERROR_NOT_FOUND: used when the resource could not be found.
 * @GST_RESOURCE_ERROR_BUSY: used when resource is busy.
 * @GST_RESOURCE_ERROR_OPEN_READ: used when resource fails to open for reading.
 * @GST_RESOURCE_ERROR_OPEN_WRITE: used when resource fails to open for writing.
 * @GST_RESOURCE_ERROR_OPEN_READ_WRITE: used when resource cannot be opened for
 * both reading and writing, or either (but unspecified which).
 * @GST_RESOURCE_ERROR_CLOSE: used when the resource can't be closed.
 * @GST_RESOURCE_ERROR_READ: used when the resource can't be read from.
 * @GST_RESOURCE_ERROR_WRITE: used when the resource can't be written to.
 * @GST_RESOURCE_ERROR_SEEK: used when a seek on the resource fails.
 * @GST_RESOURCE_ERROR_SYNC: used when a synchronize on the resource fails.
 * @GST_RESOURCE_ERROR_SETTINGS: used when settings can't be manipulated on.
 * @GST_RESOURCE_ERROR_NO_SPACE_LEFT: used when the resource has no space left.
 * @GST_RESOURCE_ERROR_NOT_AUTHORIZED: used when the resource can't be opened
 *                                     due to missing authorization.
 *                                     Since: 1.2.4
 * @GST_RESOURCE_ERROR_NUM_ERRORS: the number of resource error types.
 *
 * Resource errors are for any resource used by an element:
 * memory, files, network connections, process space, ...
 * They're typically used by source and sink elements.
 */
typedef enum
{
  GST_RESOURCE_ERROR_FAILED = 1,
  GST_RESOURCE_ERROR_TOO_LAZY,
  GST_RESOURCE_ERROR_NOT_FOUND,
  GST_RESOURCE_ERROR_BUSY,
  GST_RESOURCE_ERROR_OPEN_READ,
  GST_RESOURCE_ERROR_OPEN_WRITE,
  GST_RESOURCE_ERROR_OPEN_READ_WRITE,
  GST_RESOURCE_ERROR_CLOSE,
  GST_RESOURCE_ERROR_READ,
  GST_RESOURCE_ERROR_WRITE,
  GST_RESOURCE_ERROR_SEEK,
  GST_RESOURCE_ERROR_SYNC,
  GST_RESOURCE_ERROR_SETTINGS,
  GST_RESOURCE_ERROR_NO_SPACE_LEFT,
  GST_RESOURCE_ERROR_NOT_AUTHORIZED,
  GST_RESOURCE_ERROR_NUM_ERRORS
} GstResourceError;

/**
 * GstStreamError:
 * @GST_STREAM_ERROR_FAILED: a general error which doesn't fit in any other
 * category.  Make sure you add a custom message to the error call.
 * @GST_STREAM_ERROR_TOO_LAZY: do not use this except as a placeholder for
 * deciding where to go while developing code.
 * @GST_STREAM_ERROR_NOT_IMPLEMENTED: use this when you do not want to implement
 * this functionality yet.
 * @GST_STREAM_ERROR_TYPE_NOT_FOUND: used when the element doesn't know the
 * stream's type.
 * @GST_STREAM_ERROR_WRONG_TYPE: used when the element doesn't handle this type
 * of stream.
 * @GST_STREAM_ERROR_CODEC_NOT_FOUND: used when there's no codec to handle the
 * stream's type.
 * @GST_STREAM_ERROR_DECODE: used when decoding fails.
 * @GST_STREAM_ERROR_ENCODE: used when encoding fails.
 * @GST_STREAM_ERROR_DEMUX: used when demuxing fails.
 * @GST_STREAM_ERROR_MUX: used when muxing fails.
 * @GST_STREAM_ERROR_FORMAT: used when the stream is of the wrong format
 * (for example, wrong caps).
 * @GST_STREAM_ERROR_DECRYPT: used when the stream is encrypted and can't be
 * decrypted because this is not supported by the element.
 * @GST_STREAM_ERROR_DECRYPT_NOKEY: used when the stream is encrypted and
 * can't be decrypted because no suitable key is available.
 * @GST_STREAM_ERROR_NUM_ERRORS: the number of stream error types.
 *
 * Stream errors are for anything related to the stream being processed:
 * format errors, media type errors, ...
 * They're typically used by decoders, demuxers, converters, ...
 */
typedef enum
{
  GST_STREAM_ERROR_FAILED = 1,
  GST_STREAM_ERROR_TOO_LAZY,
  GST_STREAM_ERROR_NOT_IMPLEMENTED,
  GST_STREAM_ERROR_TYPE_NOT_FOUND (4),
  GST_STREAM_ERROR_WRONG_TYPE,
  GST_STREAM_ERROR_CODEC_NOT_FOUND,
  GST_STREAM_ERROR_DECODE (7),
  GST_STREAM_ERROR_ENCODE,
  GST_STREAM_ERROR_DEMUX,
  GST_STREAM_ERROR_MUX,
  GST_STREAM_ERROR_FORMAT,
  GST_STREAM_ERROR_DECRYPT,
  GST_STREAM_ERROR_DECRYPT_NOKEY,
  GST_STREAM_ERROR_NUM_ERRORS
} GstStreamError;


/**
 * GST_LIBRARY_ERROR:
 *
 * Error domain for library loading. Errors in this domain will
 * be from the #GstLibraryError enumeration.
 * See #GError for information on error domains.
 */
#define GST_LIBRARY_ERROR   gst_library_error_quark ()
/**
 * GST_RESOURCE_ERROR:
 *
 * Error domain for resource handling. Errors in this domain will
 * be from the #GstResourceError enumeration.
 * See #GError for information on error domains.
 */
#define GST_RESOURCE_ERROR  gst_resource_error_quark ()
/**
 * GST_CORE_ERROR:
 *
 * Error domain for core system. Errors in this domain will
 * be from the #GstCoreError enumeration.
 * See #GError for information on error domains.
 */
#define GST_CORE_ERROR      gst_core_error_quark ()
/**
 * GST_STREAM_ERROR:
 *
 * Error domain for media stream processing. Errors in this domain will
 * be from the #GstStreamError enumeration.
 * See #GError for information on error domains.
 */
#define GST_STREAM_ERROR    gst_stream_error_quark ()

GQuark gst_core_error_quark (void);

Use:

g_quark_to_string(error->domain) returns "gst-resource-error-quark"

if (error->domain == GST_CORE_ERROR)


={============================================================================
*kt_dev_bcast_500* gst: gstclock

http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/GstClock.html

GstClock — Abstract class for global clocks


gst_clock_new_periodic_id ()
-----------------
GstClockID
gst_clock_new_periodic_id (GstClock *clock,
                           GstClockTime start_time,
                           GstClockTime interval);

Get an ID from clock to trigger a periodic notification. The periodic
  notifications will start at time start_time and will then be fired with the
  given interval. id should be unreffed after usage.

Free-function: gst_clock_id_unref

Parameters
clock
The GstClockID to get a periodic notification id from

start_time
the requested start time

interval
the requested interval

Returns
a GstClockID that can be used to request the time notification.

MT safe. 


={============================================================================
*kt_dev_bcast_600* glib-model

http://library.gnome.org/devel/gobject/stable/

GObject programming model

<background>
GObject, and its lower-level type system, GType, are used by GTK+ and most GNOME
libraries to provide:

  object-oriented C-based APIs and

  automatic transparent API bindings to other compiled or interpreted languages.
  cross-language interoperability.

It is useful to keep in mind that allowing access to C objects from other
interpreted languages was one of the major design goals.


The GLib Dynamic Type System

A type, as manipulated by the GLib type system, is much more generic than what
is usually understood as an Object type.

<register-new-type>
g_type_register_static, g_type_register_dynamic and g_type_register_fundamental
are the C functions, defined in gtype.h and implemented in gtype.c which you
should use to register a new GType in the program's type system.

Fundamental types are top-level types which do not derive from any other type
while other non-fundamental types derive from other types.


Copy functions

The major common point between all GLib types (fundamental and non-fundamental,
    classed and non-classed, instantiable and non-instantiable) is that they can
  all be manipulated through a single API to copy/assign them.

The GValue structure is used as an abstract 'container' for all of these types.
Its simplistic API (defined in gobject/gvalue.h) can be used to invoke the
value_table functions registered during type registration: for example
g_value_copy copies the content of a GValue to another GValue. This is similar
to a C++ assignment which invokes the C++ copy operator to modify the default
bit-by-bit copy semantics of C++/C structures/classes.

https://developer.gnome.org/gobject/stable/gobject-Generic-values.html#GValue

The GValue structure is basically a variable container that consists of a type
'identifier' and a specific 'value' of that type. The type identifier within a
GValue structure always determines the type of the associated value. To create a
undefined GValue structure, simply create a zero-filled GValue structure. To
initialize the GValue, use the g_value_init() function. A GValue cannot be used
until it is initialized. The basic type operations (such as freeing and copying)
are determined by the GTypeValueTable 'associated' with the type ID stored in
the GValue. Other GValue operations (such as converting values between types)
are provided by this interface.

The following code shows how you can copy around a 64 bit integer, as well as a
GObject instance pointer: 

static void test_int (void)
{
  GValue a_value = G_VALUE_INIT;
  GValue b_value = G_VALUE_INIT;
  guint64 a, b;

  a = 0xdeadbeef;

  g_value_init (&a_value, G_TYPE_UINT64);
  g_value_set_uint64 (&a_value, a);

  g_value_init (&b_value, G_TYPE_UINT64);
  g_value_copy (&a_value, &b_value);

  b = g_value_get_uint64 (&b_value);

  if (a == b) {
    g_print ("Yay !! 10 lines of code to copy around a uint64.\n");
  } else {
    g_print ("Are you sure this is not a Z80 ?\n");
  }
}

static void test_object (void)
{
  GObject *obj;
  GValue obj_vala = G_VALUE_INIT;
  GValue obj_valb = G_VALUE_INIT;
  obj = g_object_new (MAMAN_TYPE_BAR, NULL);

  g_value_init (&obj_vala, MAMAN_TYPE_BAR);
  g_value_set_object (&obj_vala, obj);

  g_value_init (&obj_valb, G_TYPE_OBJECT);

  /* g_value_copy's semantics for G_TYPE_OBJECT types is to copy the reference.
   * This function thus calls g_object_ref.
   *
   * It is interesting to note that the assignment works here because
   * MAMAN_TYPE_BAR is a G_TYPE_OBJECT.
   */
  g_value_copy (&obj_vala, &obj_valb);

  g_object_unref (G_OBJECT (obj));
  g_object_unref (G_OBJECT (obj));
}

The important point about the above code is that the exact semantics of the copy
calls is undefined since they depend on the implementation of the copy function.
Certain copy functions might decide to allocate a new chunk of memory and then
to copy the data from the source to the destination. Others might want to simply
increment the reference count of the instance and copy the reference to the new
GValue.

The value table used to specify these assignment functions is documented in
GTypeValueTable.

Interestingly, it is also very 'unlikely' you will ever need to specify a
value_table during type registration because these value_tables are 'inherited'
from the parent types for non-fundamental types. 


Unless your code has special requirements, you can use the G_DEFINE_TYPE macro
to define a class:

G_DEFINE_TYPE (MamanBar, maman_bar, G_TYPE_OBJECT)

'otherwise', the maman_bar_get_type function must be implemented manually:

GType maman_bar_get_type (void)
{
  static GType type = 0;
  if (type == 0) {
    const GTypeInfo info = {
      /* You fill this structure. */
    };
    type = g_type_register_static (G_TYPE_OBJECT,
                                   "MamanBarType",
                                   &info, 0);
  }
  return type;
}


/* --- GType boilerplate --- */
/**
 * G_DEFINE_TYPE:
 * @TN: The name of the new type, in Camel case.
 * @t_n: The name of the new type, in lowercase, with words 
 *  separated by '_'.
 * @T_P: The #GType of the parent type.
 * 
 * A convenience macro for type implementations, which declares a class
 * initialization function, an instance initialization function (see #GTypeInfo
 * for information about these) and a static variable named @t_n<!--
 * -->_parent_class pointing to the parent class. 
 *
 * Furthermore, it defines a *_get_type() function. See G_DEFINE_TYPE_EXTENDED()
 * for an example.
 * 
 * Since: 2.4
 */


Instantiable classed types: objects

Types which are registered with a class and are declared instantiable are what
most closely resembles an object. 

Although GObjects (detailed in The GObject base class) are the most well known
type of instantiable classed types, other kinds of similar objects used as the
base of an inheritance hierarchy have been externally developed and they are all
built on the fundamental features described below. 

For example, the code below shows how you could register such a fundamental
object type in the type system (using none of the GObject convenience API):

typedef struct {
  GObject parent;
  /* instance members */
  int field_a;
} MamanBar;

typedef struct {
  GObjectClass parent;
  /* class members */
  void (*do_action_public_virtual) (MamanBar *self, guint8 i);

  void (*do_action_public_pure_virtual) (MamanBar *self, guint8 i);
} MamanBarClass;

#define MAMAN_TYPE_BAR (maman_bar_get_type ())

GType 
maman_bar_get_type (void)
{
  static GType type = 0;
  if (type == 0) {
    const GTypeInfo info = {
      sizeof (MamanBarClass),           /* class size */
      NULL,                             /* base_init */
      NULL,                             /* base_finalize */
      (GClassInitFunc) foo_class_init,  /* class_init */
      NULL,                             /* class_finalize */
      NULL,                             /* class_data */
      sizeof (MamanBar),                /* instance size */
      0,                                /* n_preallocs */
      (GInstanceInitFunc) NULL          /* instance_init */
    };
    type = g_type_register_static (G_TYPE_OBJECT,
                                   "BarType",
                                   &info, 0);
  }
  return type;
}

<register-type>
Upon the first call to maman_bar_get_type, the type named BarType will be
'registered' in the type system as inheriting from the type G_TYPE_OBJECT.



<class-and-instance>

struct _GTypeClass
{
  /*< private >*/
  GType g_type;
};

struct  _GObjectClass
{
  GTypeClass   g_type_class;
  ...
};


struct _GTypeInstance
{
  /*< private >*/
  GTypeClass *g_class;
};

struct  _GObject
{
  GTypeInstance  g_type_instance;
  
  /*< private >*/
  volatile guint ref_count;
  GData         *qdata;
};

Every object must define two structures: its class structure and its instance
structure. All class structures must contain as first member a GTypeClass
structure. All instance structures must contain as first member a GTypeInstance
structure. The declaration of these C types, coming from gtype.h is shown below:

struct _GTypeClass
{
  GType g_type;
};
struct _GTypeInstance
{
  GTypeClass *g_class;
};

These constraints allow the type system to make sure that every object instance
(identified by a pointer to the object's instance structure) contains in its
first bytes a pointer to the object's class structure.

This relationship is best explained by an example: let's take object B which
inherits from object A:

/* A definitions */
typedef struct {
  GTypeInstance parent;
  int field_a;
  int field_b;
} A;
typedef struct {
  GTypeClass parent_class;
  void (*method_a) (void);
  void (*method_b) (void);
} AClass;

/* B definitions. */
typedef struct {
  A parent;
  int field_c;
  int field_d;
} B;
typedef struct {
  AClass parent_class;
  void (*method_c) (void);
  void (*method_d) (void);
} BClass;


={============================================================================
*kt_dev_bcast_600* glib-gobject

GObject is a fundamental classed instantiable type. It implements:

    Memory management with reference counting

    Construction/Destruction of instances

    Generic per-object properties with set/get function pairs

    Easy use of signals

All the GNOME libraries which use the GLib type system (like GTK+ and GStreamer)
inherit from GObject which is why it is important to understand the details of
how it works. 


Object instantiation

The g_object_new family of functions can be used to 'instantiate' any GType
which inherits from the GObject base type. All these functions make sure the
class and instance structures have been correctly initialized by GLib's type
system and then invoke at one point or another the constructor class method
which is used to:

    Allocate and clear memory through g_type_create_instance,

    Initialize the object's instance with the construction properties.

Objects which inherit from GObject are allowed to override this constructed
class method. The example below shows how MamanBar overrides the parent's
construction process:

#define MAMAN_TYPE_BAR maman_bar_get_type ()
G_DECLARE_FINAL_TYPE (MamanBar, maman_bar, MAMAN, BAR, GObject)

struct _MamanBar
{
  GObject parent_instance;

  /* instance members */
};

/* will create maman_bar_get_type and set maman_bar_parent_class */
G_DEFINE_TYPE (MamanBar, maman_bar, G_TYPE_OBJECT);

static void
maman_bar_constructed (GObject *obj)
{
  /* update the object state depending on constructor properties */

  /* Always chain up to the parent constructed function to complete object
   * initialisation. */
  G_OBJECT_CLASS (maman_bar_parent_class)->constructed (obj);
}

static void
maman_bar_class_init (MamanBarClass *klass)
{
  GObjectClass *object_class = G_OBJECT_CLASS (klass);

  object_class->constructed = maman_bar_constructed;
}

static void
maman_bar_init (MamanBar *self)
{
  /* initialize the object */
}

note:
Here pass maman_bar_get_type ()

MamanBar *bar = g_object_new (MAMAN_TYPE_BAR, NULL);


={============================================================================
*kt_dev_bcast_600* glib-property, glib-gparam, glib-value

Object properties

One of GObject's nice features is its generic get/set mechanism for object
properties. When an object is instantiated, the object's class_init handler
should be used to register the object's properties with
g_object_class_install_properties.

The best way to understand how object properties work is by looking at a real
example of how it is used:

/************************************************/
/* Implementation                               */
/************************************************/

enum
{
  PROP_MAMAN_NAME = 1,
  PROP_PAPA_NUMBER,
  N_PROPERTIES
};

static GParamSpec *obj_properties[N_PROPERTIES] = { NULL, };

static void
maman_bar_set_property (GObject      *object,
                        guint         property_id,
                        const GValue *value,
                        GParamSpec   *pspec)
{
  MamanBar *self = MAMAN_BAR (object);

  switch (property_id)
    {
    case PROP_MAMAN_NAME:
      g_free (self->priv->name);
      self->priv->name = g_value_dup_string (value);
      g_print ("maman: %s\n", self->priv->name);
      break;

    case PROP_PAPA_NUMBER:
      self->priv->papa_number = g_value_get_uchar (value);
      g_print ("papa: %u\n", self->priv->papa_number);
      break;

    default:
      /* We don't have any other property... */
      G_OBJECT_WARN_INVALID_PROPERTY_ID (object, property_id, pspec);
      break;
    }
}

static void
maman_bar_get_property (GObject    *object,
                        guint       property_id,
                        GValue     *value,
                        GParamSpec *pspec)
{
  MamanBar *self = MAMAN_BAR (object);

  switch (property_id)
    {
    case PROP_MAMAN_NAME:
      g_value_set_string (value, self->priv->name);
      break;

    case PROP_PAPA_NUMBER:
      g_value_set_uchar (value, self->priv->papa_number);
      break;

    default:
      /* We don't have any other property... */
      G_OBJECT_WARN_INVALID_PROPERTY_ID (object, property_id, pspec);
      break;
    }
}

static void
maman_bar_class_init (MamanBarClass *klass)
{
  GObjectClass *object_class = G_OBJECT_CLASS (klass);

  object_class->set_property = maman_bar_set_property;
  object_class->get_property = maman_bar_get_property;

  obj_properties[PROP_MAMAN_NAME] =
    g_param_spec_string ("maman-name",
                         "Maman construct prop",
                         "Set maman's name",
                         "no-name-set" /* default value */,
                         G_PARAM_CONSTRUCT_ONLY | G_PARAM_READWRITE));

  obj_properties[PROP_PAPA_NUMBER] =
    g_param_spec_uchar ("papa-number",
                        "Number of current Papa",
                        "Set/Get papa's number",
                        0  /* minimum value */,
                        10 /* maximum value */,
                        2  /* default value */,
                        G_PARAM_READWRITE));

  g_object_class_install_properties (object_class,
                                     N_PROPERTIES,
                                     obj_properties);
}

/************************************************/
/* Use                                          */
/************************************************/

GObject *bar;
GValue val = G_VALUE_INIT;

bar = g_object_new (MAMAN_TYPE_BAR, NULL);

g_value_init (&val, G_TYPE_CHAR);
g_value_set_char (&val, 11);

g_object_set_property (G_OBJECT (bar), "papa-number", &val);

g_value_unset (&val);


"g_object_set_property" first ensures a property with this name was 'registered'
in bar's class_init handler. If so it 'walks' the class hierarchy, 'from'
bottom-most most-derived type, 'to' top-most fundamental type to find the class
which registered 'that' property. It then tries to convert the user-provided
GValue into a GValue whose type is that of the associated property.

If the user provides a signed char GValue, as is shown here, and if the object's
property was registered as an unsigned int, g_value_transform will try to
transform the input signed char into an unsigned int. Of course, the success of
the transformation depends on the availability of the required transform
function. In practice, there will almost always be a transformation [3] which
matches and conversion will be carried out if needed. 

Here, the GParamSpec we provided in class_init has a validation function which
makes sure that the GValue contains a value which respects the minimum and
maximum bounds of the GParamSpec. In the example above, the client's GValue does
not respect these constraints (it is set to 11, while the maximum is 10). As
such, the g_object_set_property function will return with an error. 

If the user's GValue had been set to a valid value, g_object_set_property would
have proceeded with calling the object's set_property class 'method'. Here,
     since our implementation did override this method, execution would jump to
     maman_bar_set_property after having retrieved from the GParamSpec the
     param_id [4] which had been stored 'by' g_object_class_install_property.

Once the property has been set by the object's set_property class method,
     execution 'returns' to g_object_set_property which makes sure that the
     "notify" signal is emitted on the object's instance with the changed
     property as parameter unless notifications were frozen by
     g_object_freeze_notify. 


It sounds like a tedious task to set up GValues every time when one wants to
modify a property. In practice one will rarely do this. For application there is
an easier way and that is described next.


Accessing multiple properties at once

It is interesting to note that the g_object_set and g_object_set_valist
(variadic version) functions can be used to set multiple properties at once. The
client code shown above can then be re-written as:

MamanBar *foo;
foo = /* */;
g_object_set (G_OBJECT (foo),
              "papa-number", 2, 
              "maman-name", "test", 
              NULL);


void
g_object_set (gpointer object,
              const gchar *first_property_name,
              ...);

first_property_name 
name of the first property to set

...
value for the first property, followed optionally by more name/value pairs,
      followed by NULL


This saves us from managing the GValues that we were needing to handle when
using g_object_set_property. The code above will trigger one notify signal
emission for each property modified.

Equivalent _get versions are also available: g_object_get and
g_object_get_valist (variadic version) can be used to get numerous properties at
once.

These high level functions have one 'drawback' — they don't provide a return
value. One should pay attention to the argument types and ranges when using
them. A known source of errors is to pass a different type from what the
property expects; for instance, passing an integer when the property expects a
floating point value and thus shifting all subsequent parameters by some number
of bytes. Also forgetting the terminating NULL will lead to 'undefined'
behaviour.

This explains how g_object_new, g_object_newv and g_object_new_valist work: they
parse the user-provided variable number of parameters and invoke g_object_set on
the parameters only after the object has been successfully constructed. The
"notify" signal will be emitted for each property set.


<glib-gparam>
https://developer.gnome.org/gobject/stable/gobject-Standard-Parameter-and-Value-Types.html

GValue provides an abstract container structure which can be copied, transformed
and compared while holding a value of any (derived) type, which is registered as
a GType with a GTypeValueTable in its GTypeInfo structure. Parameter
specifications for most value types can be created as GParamSpec derived
instances, to implement e.g. GObject properties which operate on GValue
containers.

Parameter names need to start with a letter (a-z or A-Z). Subsequent characters
can be letters, numbers or a '-'. All other characters are replaced by a '-'
during construction.


g_param_spec_enum ()
--------------------------
GParamSpec *
g_param_spec_enum (const gchar *name,
                   const gchar *nick,
                   const gchar *blurb,
                   GType enum_type,
                   gint default_value,
                   GParamFlags flags);

Creates a new GParamSpecEnum instance specifying a G_TYPE_ENUM property.
See g_param_spec_internal() for details on property names.

Parameters

name
canonical name of the property specified

nick
nick name for the property specified

blurb
description of the property specified

enum_type
a GType derived from G_TYPE_ENUM

default_value
default value for the property specified

flags
flags for the property specified

Returns
a newly created parameter specification.


g_param_spec_boolean ()
--------------------------
GParamSpec *
g_param_spec_boolean (const gchar *name,
                      const gchar *nick,
                      const gchar *blurb,
                      gboolean default_value,
                      GParamFlags flags);

Creates a new GParamSpecBoolean instance specifying a G_TYPE_BOOLEAN property.
  In many cases, it may be more appropriate to use an enum with
  g_param_spec_enum(), both to improve code clarity by using explicitly named
  values, and to allow for more values to be added in future without breaking
  API.

See g_param_spec_internal() for details on property names.

Parameters

name
canonical name of the property specified

nick
nick name for the property specified

blurb
description of the property specified

default_value
default value for the property specified

flags
flags for the property specified

Returns
a newly created parameter specification. 


={============================================================================
*kt_dev_bcast_600* glib: doc

http://library.gnome.org/devel/glib/stable/


={============================================================================
*kt_dev_bcast_600* glib: debug

gstreamer-1.5.0/docs/gst/running.xml

<formalpara id="G_DEBUG">
  <title><envar>G_DEBUG</envar></title>

  <para>
Useful GLib environment variable. Set G_DEBUG=fatal_warnings to make
GStreamer programs abort when a critical warning such as an assertion failure
occurs. This is useful if you want to find out which part of the code caused
that warning to be triggered and under what circumstances. Simply set G_DEBUG
as mentioned above and run the program in gdb (or let it core dump). Then get
a stack trace in the usual way.
  </para>


https://developer.gnome.org/glib/stable/glib-running.html

G_DEBUG.  
This environment variable can be set to a list of debug options, which cause GLib to print out
different types of debugging information.

fatal-warnings
Causes GLib to abort the program at the first call to g_warning() or g_critical().

fatal-criticals
Causes GLib to abort the program at the first call to g_critical().

gc-friendly
Newly allocated memory that isn't directly initialized, as well as memory being freed will be reset
to 0. The point here is to allow memory checkers and similar programs that use Boehm GC alike
algorithms to produce more accurate results.

resident-modules
All modules loaded by GModule will be made resident. This can be useful for tracking memory leaks in
modules which are later unloaded; but it can also hide bugs where code is accessed after the module
would have normally been unloaded.

bind-now-modules
All modules loaded by GModule will bind their symbols at load time, even when the code uses
%G_MODULE_BIND_LAZY.

The special value all can be used to turn on all debug options. The special value help can be used
to print all available options. 


={============================================================================
*kt_dev_bcast_600* glib-signal

https://developer.gnome.org/gobject/unstable/gobject-Signals.html#

Signals — A means for customization of object behaviour and a general purpose
notification mechanism

https://developer.gnome.org/gobject/stable/signal.html

Signals

GObject's signals have nothing to do with standard UNIX signals: they connect
arbitrary application-specific events with any number of listeners. For example,
in GTK+, every user event (keystroke or mouse move) is received from the X
    server and generates a GTK+ event under the form of a signal emission on a
    given object instance.

note: type instance(object) and closure(callback)

Each signal is registered in the type system together with the type on which it
can be emitted: users of the type are said to connect to the signal on a given
type instance when they register a closure to be invoked upon the signal
emission. Users can also emit the signal by themselves or stop the emission of
the signal from within one of the closures connected to the signal.

When a signal is emitted on a given type instance, all the closures connected to
this signal on this type instance will be invoked. All the closures connected to
such a signal represent callbacks whose signature looks like:

return_type function_callback (gpointer instance, ... , gpointer user_data);


Signal connection
-----------------
If you want to connect to a signal with a closure, you have three possibilities:

* You can register a class closure at signal registration. this is a system-wide
* operation. i.e.: the class_closure will be invoked during each emission of a
* given signal on all the instances of the type which supports that signal.

* You can use g_signal_override_class_closure which overrides the class_closure
* of a given type. It is possible to call this function only on a derived type
* of the type on which the signal was registered. This function is of use only
* to language bindings.

<signal-connect>
* You can register a closure with the g_signal_connect family of functions. This
* is an *instance-specific* operation: the closure will be invoked only during
* emission of a given signal on a given instance.

It is also possible to connect a different kind of callback on a given signal:
emission hooks are invoked whenever a given signal is emitted whatever the
instance on which it is emitted. Emission hooks are used for example to get all
mouse_clicked emissions in an application to be able to emit the small mouse
click sound. Emission hooks are connected with g_signal_add_emission_hook and
removed with g_signal_remove_emission_hook. 


g_signal_connect()

#define             g_signal_connect(instance, detailed_signal, c_handler, data)

Connects a GCallback function to a signal for a particular object. The handler
will be called before the default handler of the signal.

Parameters

instance             the instance to connect to.
detailed_signal      a string of the form "signal-name::detail".
c_handler            the GCallback to connect.
data                 data to pass to c_handler calls.

Returns

the handler id (always greater than 0 for successful connections)


g_signal_emit_by_name ()

void
g_signal_emit_by_name (gpointer instance,
                       const gchar *detailed_signal,
                       ...);

Emits a signal. Note that g_signal_emit_by_name() resets the return value to the
    default if no handlers are connected, in contrast to g_signal_emitv().  

Parameters

instance
the instance the signal is being emitted on. [type GObject.Object]

detailed_signal
a string of the form "signal-name::detail".

...
parameters to be passed to the signal, followed by a location for the return
value. If the return type of the signal is G_TYPE_NONE, the return value
location can be omitted.


Signal emission
---------------
Signal emission is done through the use of the g_signal_emit family of
functions. 

Signal emission can be decomposed in 5 steps:

    RUN_FIRST: if the G_SIGNAL_RUN_FIRST flag was used during signal
    registration and if there exists a class closure for this signal, the class
    closure is invoked.

    EMISSION_HOOK: if any emission hook was added to the signal, they are
    invoked from first to last added. Accumulate return values.

    HANDLER_RUN_FIRST: if any closure were connected with the g_signal_connect
    family of functions, and if they are not blocked (with the
        g_signal_handler_block family of functions) they are run here, from
    first to last connected.

    RUN_LAST: if the G_SIGNAL_RUN_LAST flag was set during registration and if a
    class closure was set, it is invoked here.

    HANDLER_RUN_LAST: if any closure were connected with the
    g_signal_connect_after family of functions, if they were not invoked during
    HANDLER_RUN_FIRST and if they are not blocked, they are run here, from first
    to last connected.

    RUN_CLEANUP: if the G_SIGNAL_RUN_CLEANUP flag was set during registration
    and if a class closure was set, it is invoked here. Signal emission is
    completed here.


={============================================================================
*kt_dev_bcast_600* glib: gatomic

https://developer.gnome.org/glib/stable/glib-Atomic-Operations.html#

---------------
void
g_atomic_int_set (volatile gint *atomic, gint newval);

Sets the value of atomic to newval. This call acts as a full compiler and
    hardware memory barrier (after the set).

Parameters
atomic a pointer to a gint or guint
newval a new value to store


g_atomic_int_compare_and_exchange
---------------
gboolean
g_atomic_int_compare_and_exchange (volatile gint *atomic,
                                   gint oldval,
                                   gint newval);

Compares atomic to oldval and, if equal, sets it to newval . If atomic was not
    equal to oldval then no change occurs.

This compare and exchange is done atomically.

Think of this operation as an atomic version of 

{ 
    if (*atomic == oldval) 
    {
        *atomic = newval; return TRUE; 
    } else return FALSE; 
}

This call acts as a full compiler and hardware memory barrier.

Parameters
atomic a pointer to a gint or guint
oldval the value to compare with
newval the value to conditionally replace with
	 
Returns
TRUE if the exchange took place


={============================================================================
*kt_dev_bcast_600* glib: gthread

https://developer.gnome.org/glib/stable/glib-Threads.html

The aim of the thread-related functions in GLib is to provide a 'portable' means
for writing multi-threaded software. There are primitives for mutexes to protect
    the access to portions of memory (GMutex, GRecMutex and GRWLock). There is a
        facility to use individual bits for locks (g_bit_lock()). There are
        primitives for condition variables to allow synchronization of threads
        (GCond). There are primitives for thread-private data - data that every
        thread has a private instance of (GPrivate). There are facilities for
        one-time initialization (GOnce, g_once_init_enter()). Finally, there are
        primitives to create and manage threads (GThread).

Since version 2.32, the GLib threading system is automatically initialized at
the start of your program, and all thread-creation functions and synchronization
primitives are available right away.

Note that it is not safe to assume that your program has no threads even if you
don't call g_thread_new() yourself. GLib and GIO can and will create threads for
their own purposes in some cases, such as when using g_unix_signal_source_new()
or when using GDBus.

Originally, UNIX did not have threads, and therefore some traditional UNIX APIs
are problematic in threaded programs. Some notable examples are

C library functions that return data in statically allocated buffers, such as
strtok() or strerror(). For many of these, there are thread-safe variants with a
_r suffix, or you can look at corresponding GLib APIs (like g_strsplit() or
        g_strerror()).

The functions setenv() and unsetenv() manipulate the process environment in a
not thread-safe way, and may interfere with getenv() calls in other threads.
Note that getenv() calls may be hidden behind other APIs. For example, GNU
gettext() calls getenv() under the covers. In general, it is best to treat the
environment as readonly. If you absolutely have to modify the environment, do it
early in main(), when no other threads are around yet.

The setlocale() function changes the locale for the entire process, affecting
all threads. Temporary changes to the locale are often made to change the
behavior of string scanning or formatting functions like scanf() or printf().
GLib offers a number of string APIs (like g_ascii_formatd() or g_ascii_strtod())
that can often be used as an alternative. Or you can use the uselocale()
function to change the locale only for the current thread.

The fork() function only takes the calling thread into the child's copy of the
process image. If other threads were executing in critical sections they could
have left mutexes locked which could easily cause deadlocks in the new child.
For this reason, you should call exit() or exec() as soon as possible in the
child and only make signal-safe library calls before that.

The daemon() function uses fork() in a way contrary to what is described above.
It should not be used with GLib programs.

GLib itself is internally completely thread-safe (all global data is
        automatically locked), but individual data structure instances are not
automatically locked for performance reasons. For example, you must coordinate
accesses to the same GHashTable from multiple threads. The two notable
exceptions from this rule are GMainLoop and GAsyncQueue, which are thread-safe
and need no further application-level locking to be accessed from multiple
threads. Most refcounting functions such as g_object_ref() are also thread-safe.
g_thread_new ()



GThread * g_thread_new (const gchar *name, GThreadFunc func, gpointer data);

This function creates a new thread. The new thread starts by invoking func with
    the argument data. The thread will run until func returns or until
    g_thread_exit() is called from the new thread. The return value of func
    becomes the return value of the thread, which can be obtained with
    g_thread_join().

The name can be useful for discriminating threads in a debugger. It is not used
for other purposes and does not have to be unique. Some systems restrict the
    length of name to 16 bytes.

If the thread can not be created the program aborts. See g_thread_try_new() if
you want to attempt to deal with failures.

To free the struct returned by this function, use g_thread_unref(). Note that
g_thread_join() implicitly unrefs the GThread as well.  


Parameters

name an (optional) name for the new thread.  [allow-none]
func a function to execute in the new thread
data an argument to supply to the new thread
	 
Returns

the new GThread


union GMutex
------------

The GMutex struct is an opaque data structure to represent a mutex (mutual
        exclusion). It can be used to protect data against shared access.

Take for example the following function:

int give_me_next_number (void)
{
  static int current_number = 0;

  // now do a very complicated calculation to calculate the new
  // number, this might for example be a random number generator
  current_number = calc_next_number (current_number);

  return current_number;
}

It is easy to see that this won't work in a multi-threaded application. There
current_number must be protected against shared access. A GMutex can be used as
a solution to this problem:

int give_me_next_number (void)
{
  static GMutex mutex;
  static int current_number = 0;
  int ret_val;

  g_mutex_lock (&mutex);
  ret_val = current_number = calc_next_number (current_number);
  g_mutex_unlock (&mutex);

  return ret_val;
}

Notice that the GMutex is not initialised to any particular value. Its placement
in static storage ensures that it will be initialised to all-zeros, which is
appropriate.

If a GMutex is placed in other contexts (eg: embedded in a struct) then it must
be explicitly initialised using g_mutex_init().

A GMutex should only be accessed via g_mutex_ functions.


g_mutex_lock ()
------------
void
g_mutex_lock (GMutex *mutex);

Locks mutex. If mutex is already locked by another thread, the current thread
will block until mutex is unlocked by the other thread.

GMutex is 'neither' guaranteed to be recursive nor to be non-recursive. As such,
       calling g_mutex_lock() on a GMutex that has already been locked by the
       same thread results in undefined behaviour (including but not limited to
           deadlocks).


={============================================================================
*kt_dev_bcast_600* glib: gmain

https://developer.gnome.org/glib/stable/glib-The-Main-Event-Loop.html#

Includes

#include <glib.h>

note:
The source is the source of event or action. think source as functor and
mainloop as dispatcher?

Description
-----------
The main event loop manages all the available 'sources' of events for GLib and
GTK+ applications. These events can come from any number of different types of
sources such as file descriptors (plain files, pipes or sockets) and timeouts.
New types of event sources can also be added using g_source_attach().

To allow multiple independent sets of sources to be handled in different
threads, each source is associated with a GMainContext. A GMainContext can only
be running in a single thread, but sources can be 'added' to it and 'removed'
from it from other threads.

Each event source is assigned a priority. The default priority,
G_PRIORITY_DEFAULT, is 0. Values less than 0 denote higher priorities. Values
    greater than 0 denote lower priorities. Events from high priority sources
    are always processed before events from lower priority sources.

Idle functions can also be added, and assigned a priority. These will be run
whenever no events with a higher priority are ready to be processed.

The GMainLoop data type represents a main event loop. A GMainLoop is created
with g_main_loop_new(). 

After adding the initial event sources, g_main_loop_run() is called. This
continuously 'checks' for new events from each of the event sources and
dispatches them. Finally, the processing of an event from one of the sources
leads to a call to g_main_loop_quit() to exit the main loop, and
g_main_loop_run() 'returns'.

It is possible to create new instances of GMainLoop recursively. This is often
used in GTK+ applications when showing modal dialog boxes. Note that event
sources are associated with a particular GMainContext, and will be checked and
dispatched for all main loops associated with that GMainContext.

GTK+ contains wrappers of some of these functions, e.g. gtk_main(),
gtk_main_quit() and gtk_events_pending().


Creating new source types
-------------------------
One of the unusual features of the GMainLoop functionality is that new types of
event source can be created and used in addition to the builtin type of event
source. A new event source type is used for handling GDK events. A new source
type is created by "deriving" from the GSource structure. The derived type of
source is represented by a structure that has the GSource structure as a first
element, and other elements specific to the new source type. 

To create an instance of the new source type, call g_source_new() passing in the
size of the derived structure and a table of functions. These GSourceFuncs
determine the behavior of the new source type.

New source types basically interact with the main context in two ways. Their
prepare function in GSourceFuncs can set a timeout to determine the maximum
amount of time that the main loop will 'sleep' before checking the source again.
In addition, or as well, the source can add file descriptors to the set that the
main context checks using g_source_add_poll().  


Customizing the main loop iteration
--------------------------
Single iterations of a GMainContext can be run with g_main_context_iteration().
In some cases, more detailed control of exactly how the details of the main loop
work is desired, for instance, when integrating the GMainLoop with an external
main loop. In such cases, you can call the component functions of
g_main_context_iteration() directly. These functions are
g_main_context_prepare(), g_main_context_query(), g_main_context_check() and
g_main_context_dispatch().  


State of a Main Context
--------------------------
The operation of these functions can best be seen in terms of a state diagram,
as shown in this image.

On UNIX, the GLib mainloop is incompatible with fork(). Any program using the
mainloop must either exec() or exit() from the child without returning to the
mainloop.


g_main_loop_new ()
--------------------------
GMainLoop * g_main_loop_new (GMainContext *context, gboolean is_running);

Creates a new GMainLoop structure.

Parameters

context 
a GMainContext (if NULL, the default context will be used).  [allow-none]

is_running
set to TRUE to indicate that the loop is running. This is not very important
since calling g_main_loop_run() will set this to TRUE anyway.

Returns

a new GMainLoop.


g_main_loop_run ()
--------------------------
void g_main_loop_run (GMainLoop *loop);

Runs a main loop 'until' g_main_loop_quit() is called on the loop. If this is
    called for the thread of the loop's GMainContext, it will process events
    from the loop, otherwise it will simply wait.

Parameters

loop
a GMainLoop


g_main_loop_quit ()
--------------------------
void g_main_loop_quit (GMainLoop *loop);

Stops a GMainLoop from running. Any calls to g_main_loop_run() for the loop
    will return.

Note that sources that have already been dispatched when g_main_loop_quit() is
called will still be executed.

Parameters

loop
a GMainLoop


g_idle_add ()
--------------------------
guint g_idle_add (GSourceFunc function, gpointer data);

Adds a function to be called whenever there are no higher priority events
    pending to the default main loop. The function is given the default idle
    priority, G_PRIORITY_DEFAULT_IDLE. 
    
<function-return-value>
If the function returns FALSE it is automatically removed from the list of
event sources and will not be called again.

This 'internally' creates a main loop 'source' using g_idle_source_new() and
attaches it to the global GMainContext using g_source_attach(), so the callback
will be invoked in whichever thread is running that main context. You can do
these steps manually if you need greater control or to use a custom main
context.

Parameters
function    function to call
data        data to pass to function.

Returns
the ID (greater than 0) of the event source.


note:
The source is to be removed either when the function returns FALSE or when
g_source_remove() is called.


g_idle_add_full ()
--------------------------
guint
g_idle_add_full (gint priority,
                 GSourceFunc function,
                 gpointer data,
                 GDestroyNotify notify);

Adds a function to be called whenever there are no higher priority events
  pending. If the function returns FALSE it is automatically removed from the
  list of event sources and will not be called again.

See memory management of sources for details on how to handle the return value
and memory management of data .

This internally creates a main loop source using g_idle_source_new() and
attaches it to the global GMainContext using g_source_attach(), so the callback
will be invoked in whichever thread is running that main context. You can do
these steps manually if you need greater control or to use a custom main
context.

[rename-to g_idle_add]

Parameters

priority
the priority of the idle source. Typically this will be in the range between
G_PRIORITY_DEFAULT_IDLE and G_PRIORITY_HIGH_IDLE.

function
function to call

data
data to pass to function

notify
function to call when the idle is removed, or NULL. [allow-none]

Returns
the ID (greater than 0) of the event source.


GSourceFunc ()
--------------------------
gboolean
(*GSourceFunc) (gpointer user_data);

Specifies the type of function passed to g_timeout_add(), g_timeout_add_full(),
          g_idle_add(), and g_idle_add_full().
            
Parameters

user_data
data passed to the function, set when the source was created with one of the
above functions

Returns
FALSE if the source should be removed. G_SOURCE_CONTINUE and G_SOURCE_REMOVE are
more memorable names for the return value.


G_SOURCE_REMOVE

#define G_SOURCE_REMOVE         FALSE

Use this macro as the return value of a GSourceFunc to remove the GSource from
the main loop.

note:
gboolean (*)(void*)


g_source_remove ()
--------------------------
gboolean g_source_remove (guint tag);

Removes the source with the given id from the default main context.

The id of a GSource is given by g_source_get_id(), or will be returned by the
functions:

g_source_attach(), g_idle_add(), g_idle_add_full(), g_timeout_add(),
  g_timeout_add_full(), g_child_watch_add(), g_child_watch_add_full(),
  g_io_add_watch(), and g_io_add_watch_full().

See also g_source_destroy(). You must use g_source_destroy() for sources added
to a non-default main context.

It is a programmer error to attempt to remove a non-existent source.

More specifically: source IDs can be reissued after a source has been destroyed
and therefore it is never valid to use this function with a source ID which may
have already been removed. 

An example is when scheduling an idle to run in another thread with
g_idle_add(). the idle may already have run and been removed by the time this
function is called on its (now invalid) source ID. This source ID may have been
reissued, leading to the operation being performed against the wrong source.

Parameters
tag the ID of the source to remove.

Returns
For historical reasons, this function always returns TRUE


g_timeout_add_seconds ()
--------------------------
guint
g_timeout_add_seconds (guint interval,
                       GSourceFunc function,
                       gpointer data);

Sets a function to be called at 'regular' intervals with the default priority,
     G_PRIORITY_DEFAULT. The function is called repeatedly 'until' it returns
       FALSE, at which point the timeout is 'automatically' destroyed and the
       function will not be called again.

This internally creates a main loop source using g_timeout_source_new_seconds()
and attaches it to the main loop context using g_source_attach(). You can do
these steps manually if you need greater control. Also see
g_timeout_add_seconds_full().

Note that the first call of the timer may not be precise for timeouts of one
second. If you need finer precision and have such a timeout, you may want to use
g_timeout_add() instead.

See memory management of sources for details on how to handle the return value
and memory management of data .

The interval given is in terms of monotonic time, not wall clock time. See
g_get_monotonic_time().  

Parameters
interval    the time between calls to the function, in seconds
function    function to call
data        data to pass to function

Returns
the ID (greater than 0) of the event source.

Since: 2.14


={============================================================================
*kt_dev_bcast_600* glib-error

https://developer.gnome.org/glib/unstable/glib-Error-Reporting.html#GError

GLib provides a standard method of reporting errors from a called function to
the calling code. This is the same problem solved by exceptions in other
languages. 
    
It's important to understand that this method is both a data type (the GError
    struct) and a set of rules. If you use GError incorrectly, then your code
will not properly interoperate with other code that uses GError, and users of
your API will probably get confused.

First and foremost: 
GError should only be used to report recoverable 'runtime' errors, never to
report programming errors. If the programmer has screwed up, then you should
use g_warning(), g_return_if_fail(), g_assert(), g_error(), or some similar
facility. (Incidentally, remember that the g_error() function should only be
    used for programming errors, it should not be used to print any error
    reportable via GError.)

Examples of recoverable runtime errors are "file not found" or "failed to
parse input." Examples of programming errors are "NULL passed to strcmp()" or
"attempted to free the same pointer twice." These two kinds of errors are
fundamentally different: runtime errors should be handled or reported to the
user, programming errors should be eliminated by fixing the bug in the
program. This is why most functions in GLib and GTK+ do not use the GError
facility.

Functions that can fail take a return location for a GError as their last
argument. On error, a new GError instance will be allocated and returned to
the caller via this argument. For example:


The GError object contains three fields: domain indicates the module the
error-reporting function is located in, code indicates the specific error that
occurred, and message is a user-readable error message with as many details as
possible. 

Several functions are provided to deal with an error received from a called
function: g_error_matches() returns TRUE if the error matches a given domain and
code, g_propagate_error() copies an error into an error location (so the calling
    function will receive it), and g_clear_error() clears an error location by
freeing the error and resetting the location to NULL. 


--------------------------
struct GError {
  GQuark       domain;
  gint         code;
  gchar       *message;
};

The GError structure contains information about an error that has occurred.

Members

GQuark  domain; error domain, e.g. G_FILE_ERROR
gint    code; error code, e.g. G_FILE_ERROR_NOENT
gchar   *message; human-readable informative error message


g_error_new_literal ()
--------------------------
GError *
g_error_new_literal (GQuark domain,
                     gint 'code',
                     const gchar *message);

Creates a new GError; unlike g_error_new(), message is not a printf()-style
format string. Use this function if message contains text you don't have control
over, that could include printf() escape sequences.

Parameters
domain    error domain
code      error code
message   error message

Returns
a new GError


g_error_new ()
--------------------------
GError *
g_error_new (GQuark domain,
             gint code,
             const gchar *format,
             ...);

Creates a new GError with the given domain and code , and a message formatted
with format.

Parameters
domain  error domain
code    error code
format  printf()-style format for error message
...     parameters for message format

Returns
a new GError


={============================================================================
*kt_dev_bcast_600* glib-quark

https://developer.gnome.org/glib/unstable/glib-Quarks.html#GQuark

Quarks are associations between 'strings' and 'integer' identifiers. Given
either the string or the GQuark identifier it is possible to retrieve the other.

Quarks are used for both datasets and keyed data lists.

To create a new quark from a string, use g_quark_from_string() or
g_quark_from_static_string().


To find the string corresponding to a given GQuark, use g_quark_to_string().

g_quark_to_string ()
--------------------------
const gchar *
g_quark_to_string (GQuark quark);

Gets the string associated with the given GQuark.


To find the GQuark corresponding to a given string, use g_quark_try_string().

Another use for the string pool maintained for the quark functions is string
interning, using g_intern_string() or g_intern_static_string(). An interned
string is a canonical representation for a string. One important advantage of
interned strings is that they can be compared for equality by a simple pointer
comparison, rather than using strcmp().


Functions

#define G_DEFINE_QUARK()
GQuark g_quark_from_string ()
GQuark g_quark_from_static_string ()
const gchar *g_quark_to_string ()
GQuark g_quark_try_string ()
const gchar * g_intern_string ()
const gchar * g_intern_static_string ()


={============================================================================
*kt_dev_bcast_600* glib: gtime and gdate

https://developer.gnome.org/glib/stable/glib-Date-and-Time-Functions.html

The GDate data structure represents a day between January 1, Year 1, and
sometime a few thousand years in the future (right now it will go to the year
    65535 or so, but g_date_set_parse() only parses up to the year 8000 or so -
    just count on "a few thousand"). GDate is meant to represent everyday dates,
not astronomical dates or historical dates or ISO timestamps or the like. It
  extrapolates the current Gregorian calendar forward and backward in time;
there is no attempt to change the calendar to match time periods or locations.
  GDate does not store time information; it represents a day.

The GDate implementation has several nice features; it is only a 64-bit struct,
so storing large numbers of dates is very efficient. It can keep both a Julian
  and day-month-year representation of the date, since some calculations are
  much easier with one representation or the other. A Julian representation is
  simply a count of days since some fixed day in the past; for GDate the fixed
  day is January 1, 1 AD. ("Julian" dates in the GDate API aren't really Julian
      dates in the technical sense; technically, Julian dates count from the
      start of the Julian period, Jan 1, 4713 BC).

GDate is simple to use. First you need a "blank" date; you can get a dynamically
allocated date from g_date_new(), or you can declare an automatic variable or
array and initialize it to a sane state by calling g_date_clear(). A cleared
date is sane; it's safe to call g_date_set_dmy() and the other mutator functions
to initialize the value of a cleared date. However, a cleared date is initially
invalid, meaning that it doesn't represent a day that exists. It is undefined to
call any of the date calculation routines on an invalid date. If you obtain a
date from a user or other unpredictable source, you should check its validity
with the g_date_valid() predicate. g_date_valid() is also used to check for
errors with g_date_set_parse() and other functions that can fail. Dates can be
invalidated by calling g_date_clear() again.

It is very important to use the API to access the GDate struct. Often only the
day-month-year or only the Julian representation is valid. Sometimes neither is
valid. Use the API.

GLib also features GDateTime which represents a precise time.


g_get_monotonic_time ()
--------------------------
gint64
g_get_monotonic_time (void);

Queries the system monotonic time.

The monotonic clock will always increase and doesn't suffer discontinuities when
the user (or NTP) changes the system time. It may or may not continue to tick
during times where the machine is suspended.

We try to use the clock that corresponds as closely as possible to the passage
of time as measured by system calls such as poll() but it may not always be
possible to do this.

Returns

the monotonic time, in microseconds


==============================================================================
Copyright: see |ktkb|                              vim:tw=100:ts=3:ft=help:norl:
